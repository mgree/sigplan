boolean and quantitative synthesis using proof search rice university martin paris inria mit abstract we present a new technique for parameter synthesis under boolean and quantitative objectives the input to the technique is a sketch a program with missing numerical parameters and a probabilistic assumption about the programs inputs the goal is to automatically synthesize values for the parameters such that the resulting program satisfies a boolean specification which states that the program must meet certain assertions and a quantitative specification which assigns a real valued to every program and which the synthesizer is expected to optimize our method called proof search reduces this task to a sequence of unconstrained smooth optimization problems that are then solved by iteratively solving these problems we obtain parameter values that get closer and closer to the boolean specification at the limit we obtain values that provably meet the specification the approximations are computed using a new notion of for program abstractions where an abstract transformer is approximated by a function that is continuous according to a metric over abstract states we present a prototype implementation of our synthesis procedure and experimental results on two benchmarks from the embedded control domain the experiments demonstrate the benefits of proof search over an approach that does not meet the boolean and quantitative synthesis goals simultaneously categories and subject descriptors d verification correctness proofs f semantics of programming languages program analysis i automatic programming program synthesis i deduction and theorem proving uncertainty and probabilistic reasoning keywords synthesis probabilistic verification probabilistic programs program abstract interpretation the authors are ordered this work was supported by nsf permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than the authors must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ san diego ca usa copyright is held by the publication to acm acm introduction traditional reasoning tasks in formal methods are boolean we are to prove that a program satisfies a set of boolean properties program verification or to realize a given logical specification in the form of an implementation program synthesis however in many applications boolean reasoning alone is not enough we also need quantitative reasoning the need for combined boolean and quantitative reasoning is especially in program synthesis here a boolean specification is naturally used to set a lower bound on the of the synthesized implementation the program should at least meet the following safety properties however there can be many implementations that meet these properties and some of them are more desirable than others given this it is appropriate to consider synthesis tasks where the synthesized implementation must not only meet a boolean specification but also be optimal with respect to a quantitative objective a approach to solve such synthesis problems is to separately handle the boolean and quantitative goals for example by searching for candidates that are locally optimal with respect to the quantitative criterion and then to verify them against the boolean specification the of this approach is that if the search for a function that is good with respect to the quantitative objective happens without to the subsequent verification phase it may take a long time to find a solution that can be verified this suggests that a procedure for quantitative synthesis should aim to meet its boolean and quantitative objectives simultaneously the benefits of such an approach are by prior work on boolean synthesis which combines synthesis and verification to make the overall process more tractable in this paper we offer such a combined verification and synthesis procedure for the problem of synthesizing values for unknown program parameters specifically we introduce proof search as a new technique to combine quantitative synthesis and verification the key idea here is to reduce the synthesis problem to a sequence of unconstrained optimization problems where the objective function for each problem is a continuous approximation of the boolean and quantitative objectives as the sequence the approximate objective gets closer to the original objective and in the limit the method finds parameters that provably meet the boolean specification at the same time the continuity of the approximations lets us optimize them effectively using local numerical techniques which rely on assumptions and do on the functions that programs often represent we these ideas with a simple example suppose our goal is to find the value of the parameter c in the procedure double p double x c if x c y x else y return y while satisfying the safety invariant b c y c and a quantitative specification that states that the return value of the procedure should be as low as possible this is an example of a verified parameter synthesis problem the programmer has provided an implementation with missing numerical parameters also known as a sketch and the synthesis problem is to find values for these parameters such that the resulting program satisfies the combined boolean and quantitative specification the problem described above is not yet well defined as there are different ways to interpret the optimality requirement one interpretation is that the programmer to minimize the worstcase behavior of the system alternatively et al have argued that a more natural goal is to optimize the expected value of the output this is potentially a harder problem because it requires knowledge of the distribution of its inputs and it requires an analysis capable of deriving the distribution of the outputs from this input distribution in this paper we focus on this probabilistic view of the problem specifically we focus on problems where the input say x is drawn from a given probability distribution µx and the quantitative specification is that the expected return value of p on input x is minimal because the analysis is probabilistic the boolean assertion can also be generalized to a probabilistic one this new assertion call it is c x c with probability greater than or equal to a certain threshold our approach to this problem is a refinement of the following idea using existing ideas on probabilistic abstract interpretation we can symbolically represent the input distribution µx of x then compute an approximation of the actual distribution of outputs of the program on input x drawn from the distribution µx these outputs depend on c hence the subscript c in pc from p µx c we can compute a sound upper bound p on the probability with which violates the assertion b and a real interval i such that the expectation of is guaranteed to fall within i given this mapping from c p i our goal is to find a value of c that leads to low values of p as well as i we can frame this as a optimization problem where the goal is to find a c that p where is the least upper bound on i p is a function that is zero when p and a large positive value larger than any upper bound on i when p is larger than this function gives us a value of c that is desirable according to the quantitative criterion while satisfying the probabilistic assertion numerical search techniques like descent or search seem like the natural choice for solving this optimization problem because even though they do not provide any guarantees on the optimality of the result they are known to work well in practice when the function to optimize satisfies certain continuity requirements these techniques cannot be applied directly though because the results of abstract interpretation are highly to understand the source of the consider our example program from before let us consider an abstract interpretation where following prior work probability distributions are approximated by structures that are essentially of pairs ei wi where each ei is a set of possible values for a random variable a bin in the histogram and wi is an upper bound on the measure in ei ie the concrete abstract e e e e e figure example of probabilistic abstraction probability that the value will fall in that bin fig an example abstraction of a continuous distribution by such a structure now suppose the abstract state of x right before the conditional if x c in our example program is as in fig note that if c is exactly the probability that x c will be bounded from below by but for any making c will change the lower bound on the probability to this is because the abstract state tells us that the probability of in the range is but the abstract state has lost the information about the exact probability of between and this property of the abstract domain makes the function c p i highly our approach this difficulty via a novel notion of for abstract interpretations instead of using the abstract transformer to generate a target for optimization we use a series of approximations to the abstract transformer that are continuous in the sense because of the continuity of these approximations numerical optimization can be used to compute in objectives generated from them we define these approximations by first defining smooth of the classic operators of abstract interpretation such as join and widening the approximations are neither abstractions or refinements of the sound abstraction pc in fact they bear no to operations on abstract interpreters studied in the literature notably each of them is an unsound abstraction while this may sound like a violation of our stated goals it is not so each of our approximations is parameterized by a real value in the limit as approaches zero the approximations converge to pc therefore by iteratively finding local on objectives generated from approximations parameterized by lower and lower values of we can find parameters that are satisfy our boolean specification with maximal probability we call the above strategy for a combination of boolean and quantitative synthesis goals proof search we have implemented our algorithm in the form of a tool called built on top of the sketch program synthesizer we have used to do verified parameter synthesis on two benchmarks from the embedded control domain a model of a and a model of an controller our experiments show that significantly improves the quality of parameters synthesized through search and that searching simultaneously with respect to the boolean and quantitative objectives gives better results than applying the two kinds of search in sequence in summary this paper makes the following contributions · we introduce proof search a new way to boolean and quantitative reasoning in program synthesis the essence of the idea is to reduce a combination of proof and optimization tasks to a sequence of smooth optimization problems · we present a concrete proof search algorithm for verified parameter synthesis in programs with probabilistic inputs we prove several properties of our algorithm including a soundness and b the of the objectives that we generate for numerical optimization · we present a prototype implementation of our algorithm called and perform case studies on two benchmarks from the embedded control domain the rest of the paper is organized as follows in section we formulate our synthesis problem in section we present proof search as well as a concrete algorithm based on this strategy section presents the system and experimental results section discusses related work we conclude with some discussion in section finally we had to omit proofs for most of our theorems due to lack of space these can be found in an online technical report problem formulation in this section we formalize the verified parameter synthesis problem as outlined earlier we will be focusing on the probabilistic version of the problem where the goal is to meet the optimality criterion on the average input the key technical task of this section is the definition of the probabilistic semantics measures to define the semantics of our programs we need some definitions from probability theory for brevity we only give the most essential of these definitions a more treatment of this background material can be found in a such as definition sets a algebra over rn is a set of subsets of n r that contains and is closed under and countable union the collection of sets denoted b is the smallest algebra over rn containing the open sets examples of sets include the set n r the set of all rational vectors and the sets of real vectors satisfying conjunctions or of polynomial inequalities in practice all subsets of the reals of interest in program analysis definition measure a finite nonnegative measure µ over rn is a function µ b such that µ and if is a countable collection of disjoint subsets of n r then µ nn an the total weight of nn µ is µ if µ then µ is a probability measure if µ then µ is a measure a measure µ over rn is over a subset y rn if y the support of a measure µ is the least closed set y rn such that µ is on y intuitively if µ is a probability measure associated with a random variable x then is the probability that x y non probability measures µ formalize probability dis where the probability of the certain event does not have to be a function f rn rn is a measurable function if for all sets y rn f y is also a set for measurable functions f rn rn and random variables x with measure µ we can define the expectation x of f x by the standard integral x f all the functions that we consider in this paper are measurable programs now we define the language call it imp of programs that we want to synthesize imp is a core imperative language with standard control constructs programs here are allowed to update n memory locations containing real values for a fixed but arbitrary constant n hence the state of a program is described by a real vector of length n assignments in the program correspond to affine transformations applied to this vector let us fix a single variable x ranging over rn that stores the state of a program the syntax of boolean expressions b and programs s in imp are given by b · x b s skip x m · x c if b then s else s while b do s s s where bv c rn b r m is a real matrix and denotes the of bv we assign a probabilistic concrete semantics s to each imp program s we use whenever we need to refer to the standard semantics this semantics is defined as a transformation on finite measures µ ie measures with µ specifically is the output probability measure of s when applied to an input probability measure µ the semantics of a boolean expression b is a function b that maps the state vector to a boolean but notation we also use b to denote the set of vectors v that satisfy b we need some more notation given a matrix m and a set of vectors u rn we define m u vm v u to be the set of vectors that when times m produce a vector in u note that this operation is well defined even for matrices we also notation by using u a to refer to the set y ay u the probabilistic semantics of programs is now given by the following rules µ · x b x n r t bv · x b x m x u c s if b then s else b b to give semantics to loops let us define an operation b we use the notation to refer to the composition of n times with being the identity function now we define while b do b n the b corresponds to the probability that some initial state x will cause the loop to iterate exactly n times and after those n iterations the resulting state belongs to the set u for simplicity in this paper we only consider programs s that terminate on almost every input ie the set of inputs on which the program does not terminate is of measure example suppose we are trying to analyze the program from the introduction after having replaced c with if x y x else y now suppose the input x is uniformly distributed over the range the distribution can be represented by a probability measure which for any set a produces the probability that x a in particular for any interval a b with a b b b a for any set a that does not according to the semantic rules the distribution of the output y is given by the following function u now ma a for any a and since m is the zero matrix ma r if a and is the empty set otherwise thus on the range we have µ µ µ and on the range we have µ µ µ ie the probability of the output being between and is while the probability of the output being between and is lemma properties of concrete semantics the concrete semantics of imp satisfies the following properties for all programs s µ µ moreover where stands for the usual deterministic semantics of programs a sketch is an imp program with missing parameters formally let h be a special name for a variable storing an of missing control parameters we define an implementation sketch or simply a sketch to be a term sh with the syntax b · x · h b sh skip x m · x m · h c if b then sh else sh while b do sh sh sh where bv c bv rn b r and m m are real matrices by substituting h by a constant vector c in sh we obtain an imp program we denote this program by sh h c or sc note that as the sketch performs only linear operations in h we can treat the concatenation of h and x as a state vector the sketch then becomes a standard imp program sx h on that extended vector assertions a probabilistic assertion for a program s is a pair b where b is a boolean expression in imp and is a constant definition satisfaction a measure µ satisfies b if a program s satisfies on the input µ if µ satisfies intuitively if s satisfies under µ then the probability that the assertion b will hold on termination of s is greater than or equal to also note that if then is a assertion that is expected to be true for all inputs allowed by the distribution for simpler notation we only allow assertions as postconditions in the formal of our method however our method easily extends to requirements at intermediate labels within a program and such assertions are permitted in our implementation error value our programs compute an error value ie a real value how close the behavior was to the ideal in general we assume that there is an error function that computes the error value from the final state of the program usually the error value will just be stored as one of the components of the state vector for example if the program is storing the error value in the first component of the state vector · x where is the parameter synthesis now we define the parameter synthesis problem that we solve problem verified parameter synthesis given an implementation sketch p an input measure µ assumed to be of bounded support for technical convenience and a boolean requirement find a vector c rn such that boolean goal sc sh h c satisfies on input µ quantitative goal the expected error value produced by sc is minimal formally c c where c is the expected value of component zero of x according to the output distribution of sc if we use as a shorthand for ie the function that maps an input to its error value then it is easy to prove that so we will use the two notations example let us go back to th e example in the introduction if x c y x else y the boolean goal can be expressed as c y c as in example let us assume that the input x is distributed uniformly within the probability of passing the assertion at the end of the program is equal to c and the expected final value of y is y because our stated goal is to minimize the output y is just the identity function for this simple pro gram both of these functions can be computed from the definition of using integration we see that when c the expected value as a function of c is y c c so for example when c the expected value is and when c the expected value is from this we can see that in order to minimize the output y while preserving the invariant we need to set c any value of c lower than will reduce the probability of satisfying the invariant below on the other hand any value of c in will lead to a higher expected value of y by the above calculation and if c then the output will be the constant example let us now understand our problem statement using a real example a controller for a the has two inputs a target for the room and the value lin of the outside these two inputs are probabilistic because while we do not know what the outside or the target will be at a given time we can collect statistics from the local and users to determine an expected distribution of these input values the joint distribution µ on lin and is assumed to be given the output of the program is the difference between the real and the target over a period of time and the goal is to design a that will minimize the expected value of this error a natural partial implementation for our controller is in fig here the cc stand for missing parameters by using a like the above the programmer the additional insight that the parameter is likely to lie in the interval c c the code captures the simple highlevel insight that a is a system with two discrete modes one where the it is off and one where the is on when the goes above a certain threshold it is appropriate to turn off the when the room is below a certain the must come back on this code is written in sketch the language used in our is easily translated to imp double lin double double h double double double double k double lin i i i i h k lin else k lin error return error figure sketch of a abs is the absolute value function the synthesis problem is to find values for and as well the h given off by the in each time step such that the following two conditions are satisfied first after steps the of the room should be as close to the target as possible this is the error value computed by the sketch second the values of the parameters must be such that if the inputs follow a distribution µ then the controller provably satisfies each probabilistic assertion in the the property that the should be below with probability proof search in this section we describe the proof search approach to verified parameter synthesis of the two goals in the statement of the problem we view the boolean requirement as a minimum requirement on the solutions and guarantee that every solution satisfies it however the quantitative goal is met approximately using local optimization and this choice is to how in most program analyses one guarantees soundness but leaves completeness as an empirical consideration formally suppose we are given an instance of an optimal synthesis problem sh µ where the components of the tuple have meanings as before let b and for any c rn let sc sh h c now suppose we have a way to compute both the expected error value at the end of the execution as well as the probability that the execution value satisfies b both as a function of c c c satisfies b the goal is to find a c that c under the constraint that c we can also frame this as an unconstrained optimization problem by introducing a term when c specifically such a problem can have the form min c p c where is for p and an arbitrarily large positive value for p our proof search algorithm addresses two challenges in solving the optimization problem above the first challenge is that computing the true values of and is expensive so we need to have sound approximations that can be computed efficiently but can still guarantee that the result does indeed satisfy the probability bound second while local numerical search algorithms such as search are the only reasonable way of solving the problem these algorithms algorithm proof search initialize inc to a random value let m be a series of values where is a chosen real constant and i i for a constant and m is the first i below a certain threshold for in m a obtain representations of c and c let i c and i c b set g p c minimize g using local numerical search starting from the initial point inc let c gc d set inc c verify that sc satisfies if this fact can be proved then terminate report c as the optimal parameter value otherwise go to step rely very strongly on the continuity of the objective function due to the presence of control constructs this assumption does not hold in our setting the key idea behind our algorithm is to derive a set of continuous approximations to c and c the approximations c and c are continuous but unsound approximations of c and c parameterized by value that the degree of approximation specifically maps each control parameter value to an interval i and maps each control parameter value to an interval i for values of the mappings and are continuous and values of will lead to func tions that are easier to optimize in the limit as goes to zero on the other hand the intervals returned by the tions converge to sound bounds over the original functions and in other words · let c i then c i · let c i then c i thus values of make numerical optimization easier while small values of make the function closer to the sound but approximation we would like to optimize finally the function is also a source of so it is approximated by a smooth function p in particular we use an approximation of the form where is a smooth s function that increases in value with p has a single point at p and approaches the function as we exploit these properties in our proof search algo rithm algorithm note that while the abstraction of the error function returns an interval the algorithm the or least upper bound on this interval ie if i l h then we optimize h however in principle we could have also chosen to optimize a different real objective derived from the interval on the other hand it is important that we optimize the upper bound sup i on the probability of assertion failure because our goal is to guarantee that the probability of error is below the threshold the check in step of our algorithm is important because the bounds computed by approximation g in step are not sound ap the interval im may not bound the value of c and the interval im may not bound the the value of c however we can show that as approaches zero the approximation to sound bounds thus step can be seen to be a limit of the iteration in step now we show how to compute and using abstract interpretation in section we give a method to compute sound bounds for the expected value of a function on an input measure the procedure that computes sound bounds is so the parameter does not play a role here however the soundness of the procedure means that we can use it in the verification step step in the algorithm subsequently in section we present our method for smooth approximation of programs sound but domain our abstract interpretation for computing sound but bounds on the expected output of a program builds on the work of we improve upon this work by an assumption of structured control flow to handle conditionals more precisely abstract states an abstract state in our domain is a tuple of the form a i wi pi ei here i is a set of indices for each index i i there is exactly one weight wi one fraction pi and one subset ei of rn the meaning of each of these components is best understood by defining the concretization function which maps abstract states to sets of measures over rn i wi pi ei µ µ ii i i ei wi pi pi wi in other words the measure can be decomposed into a sum of component measures where each of these components is in the set ei the values pi carry information about abstraction precision and are one of the distinguishing features of our abstract domain compared to that of each pi lies between and when pi is or we know that the total weight of the corresponding component is respectively and the full weight wi an intermediate value means that we dont know the precise value of but we have a bound on it as we saw in the introduction when n and each ei is an interval the abstract state is essentially a histogram each ei corresponds to a bucket in the histogram and the value wi bounds the total area of the ith bar in the if p wi gives the exact area of that bar the abstract domain allows us to symbolically propagate this histogram through the program as this histogram passes through branches we lose about the weight in any given bucket ei to see why suppose the branch condition b has an intersection with ei given that we do not know how the measure on ei is distributed we do not know if the weight on b ei is the total initial weight on ei however note that by the time the abstract interpretation reaches the end of the branch this lost is once again this is because by this point all the paths into which ei could have been split within the have been thus we know that the total weight on ei at this point is the same as the initial weight on ei formally the partial order over the abstract states is defined as follows i ei i wi qi fi i i pi p qi ei fi where the relation p is defined as true if a b a p b true if b false otherwise it is important to note a few properties about the partial order first note that the partial order is only defined when the abstract states share the same sets i and wi in principle it is possible to provide a more general partial order that also relates abstract states with different index sets and weights but for the purpose of our analysis this partial order will suffice the second point to note is the behind the definition of p the main idea is that when pi it restricts the set of measures in the concretization but if pi has a fractional value it is example consider abstract states a and b below ia ib a wa pa ea b pb eb in this case both pa and pb are fractional so the concretization of both abstract states is the same and hence a p b even though they have different values of pi if pa were equal to the concretization of a would be a subset of the concretization of b so a p b would still hold however if pb was to equal or the partial order would only hold if pa was also made equal to pb abstraction of programs the abstract semantics of programs is given by the following rules · x m x ci wi pi ei i wi pi m ei c · s si wi pi ei wi pi ei if b then si wi pi ei · si wi si wi b here if b ei pi if ei b pi otherwise and is defined in an analogous way the set is some superset of ei b the operator on abstract states is defined as follows i wi i wi i wi where x x x finally the notation m ei c is used to describe the set of points m x c x ei note how the propagation of abstract states through a branch say with condition b tracks precision information if b ei is either empty or equal to ei we have not lost any precision by propagation through the branch condition however if these conditions not hold then we have lost precision and this is tracked by pi by half note also that the operator ensures that we will this lost precision at the end of the branch we observe that is not actually a join operation in the traditional sense because b pi and we could have pi p we refer to this operation as a because it produces a more precise result than the join however the result is still sound because of the relationship between the distributions from the two branches as we prove in the appendix the use of a operation and the use of the values pi to help track the precision of the approximation are the main distinguishing features between our method and prior work on probabilistic abstract interpretation by we also observe that is defined only when for all i it so happens that at any step where the operation is applied this condition holds specifically is used only on abstract states computed from two conditional branches as abstract interpretation of assignments preserves the values of pi the pi resulting from both branches will sum to the ones before the branch which was already lower than the behind the definition of the is illustrated by the following example example consider a simple code fragment if x x x now suppose the abstract state before the if statement is i w p e that means that x will fall between and but the abstract state has no information about the exact distribution of x in that range now inside the conditional the abstract state is i w p e ie x is now between and but we dont know the probability that the branch was taken all we know is that it is bounded by this uncertainty is reflected by the fractional value of p after the branch the new abstract state will be i w p e note that p is again because even though the probability of taking each side of the conditional is unknown we do know that the probabilities add up to w so after the two branches merge we know that the probability of x e is exactly w while loops let us now define the abstract semantics of first we define an abstract step operation wi pi ei si wi the transformations generate a sequence of abstract states i wi that are visited in in an abstract execution now we have while b do si wi pi ei i wi pi nn where nn is a superset of nn ¬b the concrete definition of relies on a widening policy see section we can prove the following properties of the abstract domain for proofs see the technical report version of the paper theorem let s be any imp program for each abstract state a and for µ a we have sa theorem consider an abstract state i wi pi ei such that for all i i we have pi and µ is a probability measure in the abstract states concretization then ii where k is the function mapping x to its kth component xk and is the of the kth component over the set ei theorem let a be an event and µ a probability measure in the concretization of an abstract state i wi pi ei then wi where ia i ei a the above theorems lead to a strategy for sound verification of probabilistic assertions b given an input measure µ let us abstract µ into an abstract state then use abstract interpretation to compute i wi pi ei let us the program as satisfying if wi where id i ei b from the above theorems this strategy is sound ie a program certified as satisfying does in fact satisfy this strategy is used to implement step of the proof search algorithm algorithm by the argument given above we have theorem soundness if algorithm returns a value c of the missing parameters then the implementation sc satisfies the assertion domain representing sets of points the abstract domain described above assumes we have a representation of the sets ei that can be manipulated efficiently our algorithm represents these sets as or of this choice is a matter of rather than the reason we use as opposed to other natural choices like is that they offer an tradeoff between efficiency and precision for instance with computing an essential step for us is whereas for volume computations are we note that efficiency considerations are especially important in our setting where an abstract interpretation is performed on every query from the toplevel numerical search routine and the total number of calls to the abstract interpreter can be in the formally we represent each set ei as a collection of n dimensional open ei each is represented by a pair o m t of an invertible matrix and a vector the pair represents a set of points defined as follows m t x r x m r t r we also have a special element for rn the abstract semantics requires the following operations on the sets ei computing the image of set under an affine transformation checking whether the intersection ei b for boolean tests b is empty and computing the sets and the operation and the operator for loops of these affine transformations of can be performed exactly in most cases the one exception is assignments which generate flat whose along some dimensions equal zero our algorithm such by ones where each axis is nonzero for the special case when an equals the transformation returns for testing intersections suppose b equals · x bo therefore b m t is nonempty iff r st r t bo r a solution for r in the above equation will exist if and only if we have tm bv · t bo by running this check we can determine if the intersection is empty or not as for the sets and we could retain soundness by setting them both to ei however in practice we achieve higher precision by setting and to the enclosing of the sets ei b and ei ¬b respectively we can compute these by symbolically transforming ei we skip the details the operation is accomplished by simply the lists of from the two abstract states if one of the arguments is then it returns finally the operator for loops is defined via loop unrolling we define the operator in such a way that it is equivalent to replacing the loop by the unrolling of its n first iterations and to return in the abstract for the branch corresponding to nontermination in n iterations a x x x b if x x x x else x x figure effect of different statements on domain fig illustrates abstract interpretation using the above domain part a shows the way an assignment transforms an abstract state with two fig b shows the effect of an the e that the boundary is split into two parts with each part translated by the assignment in the corresponding branch this means that at the join point the set e now two while the sets e and e still have one each continuous approximation the abstract domain described so far is sound but small changes to a programs inputs could lead to an arbitrarily large change to the expected output mostly come from appearing and from the sets ei for example in program b on figure fig a small change to x that caused to fall entirely below the x y line will cause to from final abstract state this will result in a of relative to x and therefore also in the bound computed for ex this is the same effect that was illustrated in the introduction with a one dimensional example where the become intervals now we give a domain called smooth that provides a continuous unsound approximation of the above domain we call the abstract semantics under the smooth domain the smooth semantics · as stated earlier the approximation is parameterized by a value that the degree of as approaches zero the domain to the sound domain of section compared to the original domain the smooth domain now associates with each in the set ei a measure ij that reflects how close each is from specifically each ei now corresponds to a multiset ei ij the smooth semantics · will modify as before the ij will be initialized to one for all and will be by assignments but modified by branches for example the rule for will now be as follows if b then si wi pi ij s i wi b ij s i wi ¬b ij the operation at the end of the will be computed as before by taking the union of the sets of for each ei for which pi is not zero as for it is a special continuous function because the arguments of are sets its continuity needs to be defined with respect to a metric on sets we choose this metric to be the metric defined below definition distance let a and b be two nonempty subsets of rn the distance a b between a and b is defined by a b inf a br b ar rr where ar is defined as xy a xy r in other words the distance between a and b is the smallest r such that if we draw a of width r around a the will contain b and if we draw a of width r around b the will contain a formally the function is a function with the following properties · is continuous with respect to the distance · b if b if b · as approaches zero should approximate the following function b if b otherwise · ox b b ox b b · for all invertible affine transformations f f ox b f ox b in other words is stable under affine transformation the last requirement is not needed by our proofs but expresses the fact that we do not want our abstraction to behave differently over programs that are affine transformations of each other the properties imply that if px is equal to zero in an abstract state i wi pi ij then all the xj will be equal to zero as well because the condition under which pi becomes zero are the same as those under which becomes zero there are many functions that satisfy the above criteria for a natural choice which we use in practice is o b min v f ob v o if o is finite f if o is the universal set where f min and is a constant parameter here v s stands for the volume of a subset s of rn initial distribution a sketch takes two different kinds of inputs x and c the initial distribution for inputs x is known and is given as part of the problem definition but c is unknown previous work on smooth interpretation showed that as the search algorithms tries different values ci for parameter c it can get better information about the function to be optimized by executing on a distribution at ci in our context this means that the initial distribution must be a product between the initial input distribution µ and a distribution at ci and with variance proportional to since we are interested in an abstraction of µ × we need a sound product operation over abstract states in general this is defined as × i × j i × j × however are not closed under cross product eg in the one dimensional case the product of two a a and b b is a rectangle instead of this rectangle with a we actually it by the largest by the rectangle this is unsound in general but it becomes sound in the limit as approaches zero as for for our experiments we use w p where is the at ci with continuity of · now we establish the continuity of our domain for space reasons we only offer a proof sketch of the central theorem behind this property full proofs are available in the technical report version of the paper in order to prove continuity we first need to define a distance metric ab between two abstract states aa ia and ab ib where the sets e are represented as weighted sets of as discussed earlier definition distance we define the distance to be between two abstract states with different index sets or different when index sets and are identical the distance ab between two abstract states is defined by the following equation max min i i j where the is are functions the distance between two weighted is defined in terms of the distance between a ob b oa ob a b b the definition assumes without loss of generality that matching ei are represented by the same number of if this is not the case we pad with extra with the choice of the extra does not affect the distance measure because when one of the say a equals zero the distance a ob b always equals the other alpha b of oa at a highlevel given two abstract states with matching index sets the definition above computes the distance for each index independently and returns the minimum over all of them for a given index the distance function tries to produce the best match between in one state and in the other for each pair of and their the distance is by the magnitude of the when the are far apart and by the distance between the when the are very close together our continuity theorem requires the introduction of an tional technical condition known as definition bounded states let b be an ball an abstract state is if b for all finite with nonzero measure ij that are part of the abstract state a set of abstract states is bounded if all the abstract states in the set are for some ball b the condition in the definition of continuity is there because of the previously mentioned property that when between are large the distance metric is dominated by alpha this means that for any it is possible to find states that are close to ae but whose are arbitrarily far and such states could violate the continuity property such states outside the however are not relevant from the point of view of proof search so this definition of continuity is sufficient for us now we establish that the abstract semantic function · is continuous over bounded sets theorem for any ball b any and any ae there exists a such that for all af af also the abstract semantics maps bounded sets to bounded sets proof sketch we only prove the theorem for programs the proof is by induction on the execution of the abstract interpretation the base case corresponds to assignments as assignments are linear the property clearly holds for them the interesting inductive case corresponds to conditionals let b be any ball any positive real and i wi pi ei be any abstract state we have if b then s else si wi pi ei si wi si wi where is defined as b b from ei by the induction hypothesis s and s are continuous maps from bounded sets to bounded sets so we only need prove this property for and the functions i wi pi ei i wi where b b ¬b let us first focus on the only possible source of comes from the fact that we only merge components with pi however when pi every ij so distance between union with and without this component is so it is equivalent to multiset union which is obviously continuous over the pairs of abstract states moreover if both abstract states are and then resulting abstract states is for any ball b enclosing both b and b since such a ball exists those properties are true for thus we only need to prove that the transformation i wi pi ei i wi maps bounded sets to bounded sets and is continuous over them the else branch case is the first property directly follows from properties of minimum volume enclosing for a convex set s n s s s since our re are of subsets of a common ball they are included inside this ball by a factor n around its center as for the second property we note that the intersection op between a and a is continuous with respect to the distance moreover one can show that the function that computes is continuous with respect to distance on sets so is continuous on b now let us restrict to a distance small enough such that any close to some that b also b a value satisfies this property if for every b the point of b is at least far from it so such a exists we then restrict this distance to be lower than one so index sets and weights must be identical let i wi qi fi be any abstract state with distance lower than from i wi pi ei by definition of distance we have di wi pi ei i wi qi fi max i j here we assume without loss of generality an indexing for sets ei and fi such that minimum over functions are reached at identity maps then di wi i wi fib j b b b b in the above in order to simplify notation is defined as any when do not b this is justified by corresponding b being zero to complete the proof for the case we only need to show how to construct a g such that for any such i wi qi fi di wi pi ei i wi qi fi g di wi i wi fib this construction is somewhat involved and hence we skip it interested readers will find the details in the technical report version of the paper smooth expectation and probability the measures ij are used to compute a smooth approximation of the expected value recall that a sound upper approximation of the expected error can be computed as follows ii in the smooth approximation of this expression we use the following expression in place of e k i wi pi ei k ij wi ii the function is defined as follows first let si i l be a list that contains the and of every with their respective measures ie is the and the of coordinate k of and they are both associated with ij the function is defined in terms of si as follows ei l i l e l i l e note that as approaches the function reduces to a weighted average of weighted by i in the limit as approaches zero on the other hand the function to the this can be seen by considering the equivalent formula ei l i l e l i l e where is the maximum of the ie the actual when is close to all the exponential coefficients will converge to zero except for the ones where so the result will converge to note that the elements should have special handling here instead of using e factors as for other on the we use f il where f is an increasing continuous function with the properties that f and f on the e are removed for universal elements a smooth approximation to an upper bound on the probability of an event specified as an affine boolean expression b is defined in a similar way p b wi ij b ii j from the definition of these bounds and from theorem we have theorem for each the expressions e and p are continuous over bounded sets of abstract states this in turn implies the continuity requirement that we in section theorem continuity of and assuming programs are abstracted using smooth and that the abstraction of the initial distribution is for some b the functions c and c in algorithm are continuous example to illustrate the effect of consider the following d example where become intervals consider two abstract states that are relatively close to each other a i w w p p e e a i w w p p e e now consider the following code if x x x else x x in the original domain the state at the end of the conditional would be p a i w w p p e e p a i w w p p e e that means that under a the upper bound on the expected value will be whereas the under the slightly different a the upper bound on the expected value is now by contrast under the continuous domain using the abstract states at the end of the program will be p a i w w p p e e p a i w w p p e e this means that the smooth upper bound over the expected value for p a will now be e e e e e e e e and for p a it will now be e e e e e e e e so whereas the original sound approximation was the approximation changes only slightly when we make a small change to the input distribution as decreases however the smooth approximation approaches the sound bound so for example with the approximate bounds will be and respectively conservative merges and widening one problem with the domain presented so far is that the representation of each ei as a set of can potentially double in size after every the partial order in the domain gives us some to define less precise merge functions that prevent some of this for example multiple can be replaced by a single one that cover all of them without soundness in the case of the smooth domain however such an operation can potentially introduce our approach described in this section follows the same strategy we followed to achieve continuity in the original semantics namely we introduce a smooth join operation and show that the abstract semantics using the parameterized join are continuous moreover as goes to zero and every to one the operation to a sound join operation in order to define join we define an operation oi i the functions input is a nonempty multiset of weighted each oi mi ci is represented by a pair as described earlier the output out is a single weighted with the following properties out min i i i oi in order to define we first define a function which the size and position of each based on its i ci weight the output oi is defined as follows let i i be i the center of of then oi i mi i ci i where i i max i note that the uniform center of of resulting center is now the produced by is then the minimum volume enclosing of the oi the computation of is performed using an adaptation of an algorithm by and fig illustrates the merge process here consider the labeled and for and therefore during the merge they are to the small does not as for it now we compute the of and leading to the dashed red this is the output of the merge a similar process applies to and the universal elements are handled differently they are combined in a single universal element with i combined as before now let us add to our language a statement abstract whose concrete semantics are equivalent to skip and whose abstract semantics is defined as follows abstract i wi pi ei i wi pi ei we can prove that abstract satisfies the inductive hypothesis of theorem furthermore this is obviously sound on the domain theorem continuity of conservative merge abstract maps bounded set of abstract states to bounded sets and is continuous over bounded set of abstract states in that case theorem is preserved even if we add the conservative merge operation abstract to our language now we do the same for loops to get loops to converge we need a widening strategy our strategy is very simple after a constant number of iterations we our representation to the universal element then apply abstract to be precise we replace every representation by the universal element before applying abstract figure merge process note that this strategy is equivalent to the first k iterations of the loop and then replacing the rest of the loop with the statement while b do s followed by abstract this transformation preserves the concrete semantics and is sound for the abstract semantics we show that it does not break the continuity of the smooth semantics by showing that theorem continuity of widening while b do s is continuous over abstract states and maps bounded sets to bounded sets implementation and evaluation we two case studies using our implementation of the case studies both come from the embedded control domain and include the controller introduced in section and a model of an uses the sketch program synthesis infrastructure to parse into an ast which is then translated into a c program that implements the smooth abstract semantics described earlier for numerical search uses the search available in scientific library as the underlying local optimization method our two case studies were designed to help answer the following questions does our algorithm really compute parameters ie parameters that lead to a lower upper bounds on error than those computed by regular numerical search note that the outcome of any local search routine depends on the starting point of the search and we cannot expect our algorithm to perform better than the approach on all starting points instead it is the distribution of the expected error value across starting points that should interest us the question is whether this distribution to lower expected error values in practice what are the proofs that the algorithm can find consider a probabilistic assertion b which states that b is violated with probability as becomes smaller one would expect the task of finding parameters that provably satisfy this assertion to get harder the question is to find the lowest value of an upper bound on the probability of assertion failure for which different synthesis algorithms can find a parameter this proof goal naturally this bound depends on the start point of the search the question is whether the distribution of the bounds computed using our algorithm is better than the distribution one would get running a less sophisticated technique in our method a search for optimal parameters is interleaved with a search for a boolean proof is this really needed or could number of experiments expectation upper bound expectation smooth search on abstraction proved expectation smooth search on abstraction expectation on abstraction proved expectation on abstraction expectation on samples proved expectation on samples figure for upper bound on the expected value of the function for the experiment number of experiments smooth search on abstraction on abstraction on samples bound on probability of assertion failure figure for upper bound on the probability of assertion failure for the experiment we first find optimal program parameters and then establish the boolean goal for these parameters for each of the two case studies we compared three different approaches for parameter synthesis · proof search is the algorithm described in this paper where the synthesizer is using numerical search to find parameters that allow it to optimize the expected and the probability of assertion failure · with abstract interpretation uses a standard numerical search to optimize the expected and the probability of assertion failure as computed by the abstract interpretation presented earlier · with sampling also tries to optimize the expected and the probability of assertion failure but computes these by running the program on a fixed sample of inputs drawn from the input distribution using each of these approaches we solved the parameter synthesis problem for the two problems different times from different random starting points in order to get a representative sample of the distribution of solutions that each of these three methods can produce the results of these experiments are summarized in figures and the rest of this section describes each of the benchmarks and discusses the results recall our example where the goal is to shift the of a room from an initial lin to a target and the sketch to be completed is as in fig here lin and target are both probabilistic inputs the distribution of lin is and with modes at and and ± the distribution of is and with mean and ± the parameters to be synthesized are the and at which the respectively switches off and on and the h released in a time step our safety property states that is lower than with high probability and sets limits on the of the room note that the code in the figure permits variables of discrete types this is syntactic sugar when such a variable depends on the inputs we just cast it to a real otherwise the compiler encodes it using control example the variable i in fig is eliminated by unrolling the main loop to a depth of fig shows the distribution function for the expected value of the function for the example for each of the three methods above for each of these methods we computed the expected value in two ways a for the lines labeled expectation we computed the expected by running the synthesized algorithms on a random sample of inputs and b for the lines labeled proved expectation we the upper bound of the expected value as computed with abstract interpretation fig shows the probability of assertion failure as proved by abstract interpretation for the solutions generated by the three methods together these two graphs allow us to answer the three questions as they apply to the benchmark the results are surprising at first fig shows that with sampling is able to produce parameters with better average than any of the methods that rely on abstract interpretation whether or not however while the parameters led to good behavior in practice the resulting implementations were difficult to analyze leading to the gap between their observed behavior and the probabilities that can be proved for these implementations this is by fig for the methods where the search used abstract interpretation the proved probability of assertion failure was close to zero for most instances by contrast of the implementations generated by there were that had probabilities of failure greater than and a full that had probabilities of failure higher than of the two methods that use the results of abstract interpretation we find that numerical proof was able to produce better results both in terms of their evaluated as well as in the bounds on the expected can prove the differences are not very big but they are significant using the test we computed a of for the for the expectation upper bound determined and a of less than for the proved bounds respectively supporting the hypothesis that us find better parameters in the case of the probability of assertion failure the difference between smooth and search was not significant of so in short we were able to find good bounds for the probability of failure question and the use of had an effect on the of the solutions we but not on the probability of assertion failure question in this problem we want to synthesize parameters for an that performs to avoid a with a second the controller has four control states called left straight and right normally our in the control state if it gets within a distance x y x y of the other then it starts left in the left state at an of for delay number of then it straight for a while after delay steps it turns right comes back to its original course and changes state back to see fig our error value here is the total delay as in an setting our returns to its course immediately our safety assertions set a minimum distance between the two and ensure that the does not start the too early or too late produced with a version of pdf number of experiments expectation upper bound smooth search on abstraction on abstraction on samples figure for upper bound on the function for the experiment number of experiments bound on probability of assertion failure smooth search on abstraction on abstraction on samples figure for upper bound on the probability of assertion failure experiment double v double double double delay double delay i i i i if stage x y x y v v y x y stage left steps left x y x y v v steps steps if delay steps stage straight steps if stage straight x y x y v v steps steps steps stage steps right x y x y v v steps steps steps stage assert x y xy return delay delay figure sketch of controller figure of safe and unsafe states lower values imply lower probability of assertion failure a sketch of this controller is provided in fig here the probabilistic inputs are the initial of the two the distribution of the v of our is with mean and ± that of the other is with modes at and and and respectively our goal is to synthesize values for the optimal distance from the other at which our programs should on this as well as the amount of time that this should in control states left right and straight the of expected error turns out to be fairly regular in this benchmark nonetheless the presence of assertions leads to a search for an optimizer that aims to find safe program parameters fig the probability of assertion failures at various points in the space of parameter values as by a sampling of the input space note that this probability changes in an highly manner because the function is a very simple linear function of the unknown values so the proved bound is the same as the determined bound for all methods which is why we only show one line per method in fig however we can see that for this benchmark had a big effect on the quality of the parameters we were able to obtain in terms of the probability of assertion failure using abstract interpretation as part of the search had a big effect in allowing us to find parameters for which we could prove good bounds for assertion failures but again the difference between search with and without were not significant related work symbolic of programs was previously studied by and ­ unlike the present paper these prior approaches did not allow any boolean requirement also goal of those papers was to find program parameters that are optimal for a fixed set of input values this question reduces to a function minimization problem of the form p c in contrast our present approach allows the program inputs to vary and this leads to a harder optimization problem quantitative synthesis has been studied in the past by the synthesis community ­ specifically the statement for our problem is derived from prior work by et al however the solutions provided by these papers are restricted to finitestate systems whereas our programs and specifications are also related is an literature at the interface of machine learning and program analysis in probabilistic programming the goal is to do probabilistic inference on statistical models written as programs and synthesis of missing parameters can be viewed as a form of probabilistic inference on programs however unlike our paper prior work on probabilistic programming is not concerned with proving satisfaction of boolean properties on the other hand machine learning techniques have also been used recently to prove program correctness however the goal there is verification with respect to a boolean objective as opposed to synthesis with respect to boolean and quantitative goals another thread of related work comes from the embedded software community the match to this paper is et als work on optimal switching logic synthesis for hybrid systems and synthesis of fixedpoint code from floatingpoint designs unlike our work both these approaches focus on systems and do not reason about system behavior earlier efforts on parameter synthesis in these areas include however these papers do not consider stochastic system models and do not have optimality as a goal while there is an literature on stochastic hybrid systems researchers here do not appear to have studied the synthesis problem of interest to us proof search assumes a procedure for sound analysis of probabilistic programs the particular abstract domain we use for this purpose builds on work on probabilistic abstract interpretation however sound analysis of probabilistic systems has also been explored by other researchers and it is that our technique could be combined with these other abstractions conclusion we have presented a new approach to combined boolean and quantitative reasoning in parameter synthesis our method has demonstrated that both reasoning tasks can be accomplished using numerical optimization the discrete task of finding a proof can be into a continuous optimization task while this process is unsound we have a way to make it sound in the limit although our approach has used abstract interpretation for proof search the idea of can apply to other proof search strategies as well we are especially interested in applying to approaches to verification and synthesis finally we are on exploring connections between the ideas of this paper and probabilistic programming probabilistic programming aims to allow bayesian inference on models written in a generalpurpose programming language this is accomplished using a combination of machine learning techniques that do the inference and program analysis that generate auxiliary information about how probabilities propagate through programs it is possible that the probabilistic abstraction and symbolic techniques introduced in this paper can be adapted to this end references a o j e and s static analysis of programs with probabilistic inputs in probability and measure john sons r k t henzinger and b better quality in synthesis through quantitative objectives in cav pages ­ p k t henzinger a and r quantitative synthesis for concurrent programs in cav pages ­ p and t henzinger from boolean to quantitative synthesis in k t henzinger b and r and synthesizing systems in probabilistic environments in cav pages ­ s m and a boolean and quantitative synthesis using proof search technical report rice university s and a smooth interpretation in pldi pages ­ s and a a program soundly and in cav pages ­ s and a a system for numerical optimization of programs in cav g s rajamani a a gordon and j bayesian inference using data flow analysis in pages ­ p cousot and m probabilistic abstract interpretation in esop pages ­ a b and a parameter synthesis for hybrid systems with an application to models in t henzinger and h using to synthesize control parameters for a in formal methods for applications pages ­ s and p a note on approximate minimum volume enclosing of in pages ­ s and s synthesis of optimal fixedpoint implementations of numerical software routines in s s and a synthesis of optimal switching logic for hybrid systems in pages ­ j a l and c morgan generation for probabilistic programs automated support for methods in sas pages ­ a k and s probabilistic programming via defined factor graphs in pages ­ d abstract interpretation of probabilistic semantics in sas pages ­ d backwards abstract interpretation of probabilistic programs in esop ja and r a method for function minimization the computer journal a and r termination proofs from tests in pages ­ a logical analysis of hybrid systems proving theorems for complex dynamics springerverlag s combining induction deduction and structure for verification and synthesis in pages ­ r s gupta b a aiken p and a a data driven approach for algebraic loop invariants in esop pages ­ r s gupta b a aiken and a verification as learning geometric concepts in sas pages ­ r and a synthesizing data structure from in pages ­ m smith probabilistic abstract interpretation of imperative programs using normal distributions notes theor comput sci ­ a program synthesis by phd thesis uc berkeley program ­ s s gulwani and j foster from program verification to program synthesis in popl pages ­ m and e deriving linearizable finegrained concurrent objects in pldi pages ­ m e and g synthesis of synchronization in popl pages ­ j and t probabilistic programming with infer net machine learning school lecture notes available at 