the power of in coinductive proof microsoft research university dreyer vafeiadis abstract coinduction is one of the most basic concepts in computer science it is therefore surprising that the accounts of the principles underlying coinductive proofs are in two key respects they do not support compositional reasoning ie proofs into separate pieces that can be developed in isolation and they do not support incremental reasoning ie developing proofs by starting from the goal and generalizing the coinduction hypothesis repeatedly as necessary in this paper we show how to support coinductive proofs that are both compositional and incremental using a dead simple construction we call the parameterized greatest fixed point the basic idea is to the greatest fixed point of interest over the knowledge of the proof so far while this idea has been proposed before by in and by in neither of the previous accounts suggests its general applicability to improving the state of the art in interactive coinductive proof in addition to presenting the foundations of parameterized coinduction its utility on representative examples and studying its composition with upto techniques we also explore its in proof like coq and unlike traditional approaches to coinduction eg which employ syntactic checking parameterized coinduction offers a semantic account of this leads to faster and more robust proof development as we demonstrate using our new coq library categories and subject descriptors d programming languages formal definitions and theory f mathematical logic and formal languages mathematical logic keywords coinduction simulation parameterized greatest fixed point compositionality lattice theory interactive theorem proving introduction coinduction is one of the most basic concepts in computer science coinductive proofs especially those based on simulation arguments are relevant in many settings where one to model properties or recursive behaviors it is therefore surprising that the accounts of this research was carried out primarily while the first author was a at the second author is currently by a european permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ italy copyright c acm the principles underlying coinductive proofs are in two key respects they do not support compositional reasoning ie proofs into separate pieces that can be developed in isolation and they do not support incremental reasoning ie developing proofs by starting from the goal and generalizing the coinduction hypothesis repeatedly as necessary the with compositionality consider for instance two possibly nonterminating mutually recursive but coupled functions f and g which we would like to show equivalent to another pair of recursive functions f and g we should be able to reason about each function separately that is we should be able to prove that f f entails g g and similarly that g g entails f f and derive from those two that f g f g the problem however is that this type of circular reasoning reasoning is unsound in general for example if f g and f g then we would be able to derive f f from a one way to avoid this kind of unsound circularity is by placing a syntactic restriction on the proofs of each of the ensuring that their hypotheses are only used after the definitions of the functions in their conclusions have been this is the approach taken for instance by tactic for coinduction unfortunately the limitation of using a syntactic criterion is that it is inherently it requires one to have access to the proof of each of the component in order to determine whether the whole coinductive argument is valid moreover as we explain in more detail in section the syntactic nature of checking makes proof checking inefficient and with other tactics what we would really like instead is a more semantic account of checking by which the condition is reflected directly in the statement of the entailment being proved rather than being to a syntactic property of the proof itself the for consider the transition system shown in figure and suppose we want to show that there is an infinite path starting from node a let us first try to a model checker and explore all paths starting from a in a depthfirst fashion for the first step there is only one choice the edge a b then at b we have two choices perhaps we can try b c but this will soon lead to a dead end at which point we will have to so let us follow the edge b d instead then we follow d b and we are back to a node we have already visited we have discovered a cycle and thus an infinite path reachable from a now let us try to do the same proof formally the set of nodes from which infinite paths can be defined as the greatest fixed point inf def step of the following monotone function def x node y x x y our goal then is to show that a inf of course in this small example we could just compute inf directly by iteration and then a gs b du e g c c x a a b d b a b d c a b d e d a b d f e a b d f a b c d e figure a simple transition system and of check if a is in it but suppose we do not want to do this because in practice the transition system may be or even infinite instead we may employ fixedpoint theorem which says that to show a inf it suffices to find a set of nodes x such that a x and x x y x x y for the given transition system a possible such set is x a b d which corresponds to the set of nodes that we followed to exhibit the cycle earlier the problem however is that this proof is rather different from the model checking one and actually more difficult because it forces us to figure out what x is up front what we would really like is a way to prove our goal by incrementally expanding the coinduction hypothesis from a to a b to a b d as we explore the transition system and see what nodes are reachable from a the validity of such an approach is intuitively obvious but what is the general proof principle that it contributions in this paper we show how to support coinductive proofs that are both compositional and incremental using a dead simple construction we call the parameterized greatest fixed point the basic idea is to the greatest fixed point of interest over the knowledge of the proof so far neither the idea nor the construction behind it is an original of ours per se in proposed the same idea for supporting local model checking in the modal µcalculus his construction which is slightly different from ours supports but not our sense of the it is straightforward to his core reduction lemma to derive a compositional version of his construction independently in presented a construction that is essentially the same as ours in a more abstract categorical setting however neither of these prior accounts suggests the general applicability of the parameterized greatest fixed point to improving the state of the art in interactive coinductive proof our goal in the present paper is to the idea of parameterized coinduction and explore its potential as a practically useful tool more specifically we make the following contributions · we present the parameterized greatest fixed point in simple terms and show that it several useful principles for compositional incremental proofs section we give representative examples to illustrate the utility of these proof principles sections and · we show how parameterized coinduction is complementary to the traditional approach to simplifying simulation proofs via upto techniques and we develop the basic theory of how these approaches compose section · we explore the issues that arise in the of parameterized coinduction in existing interactive theorem provers like coq and section fortunately several of these issues can be resolved through variations on a somewhat technique called recursion section · we describe a new coq library we have developed for parameterized coinduction compared to existing tactic enables faster and more robust proof development thanks to its support for semantic rather than syntactic checking section finally we conclude the paper in section with a detailed discussion of related work the technical development of this paper has been formalized in the coq proof assistant that formalization together with a for our coq library is available from the parameterized coinduction let us begin by the basic lattice theory underlying coinductive definitions and their associated standard proof principles consider a complete lattice c and a monotone ie function f c c strictly speaking for generality we do not require to be and we write for the intersection of and its inverse which corresponds to if we have we say that r is a point of f if f r r and r is a point of f if r f r further we write µf for f s least fixed point and f for its greatest fixed point which by fixedpoint theorem is equal to the join of all points of f f r c r f r principle we are concerned with proving statements of the form x f from theorem we directly get that points are included in the greatest fixed point x f x x f to prove that x f when x f x using this principle we have to determine a point of f larger than x up front x f r x r r f r this is clearly for doing interactive proofs as it forces one to construct the coinduction hypothesis r up front instead of allowing r to be generated naturally in the course of the proof recall that in the example of the introduction although we were only interested in showing that a step we had to pick r a b d up front in large proofs this quickly becomes a big problem for example the simulation proof in the verified compiler requires a simulation relation r that cases most of which relate intermediate execution states strong coinduction second there is a slight variant of sometimes called the strong coinduction principle lemma strong coinduction x f x f x f proof first we have f f f f x f the direction follows directly from for the direction from x f x f and we get x f f x f ie x f is a point of f so from x x f f this principle is strong in the sense that it is complete but it still does not offer us a very useful interactive proof technique the problem arises if in the course of proving that x f x f we ever need to generalize the coinduction hypothesis by adding some y to it the only the strong coinduction principle gives us at this point if we want to continue with the proof is to show that y f but of course the proof of that may cycle around forcing us to prove that x f in which case we are stuck we are therefore forced to the proof generalizing the coinduction hypothesis to x y ie showing that x y f parameterized coinduction our parameterized coinduction principle gives us a way to avoid the proof by making explicit the idea of knowledge in the course of the proof we remember the things that we have already are f and we can treat those as assumed knowledge in guarded where is enforced semantically not syntactically formally the idea is that instead of dealing with f directly we deal instead with some gf that is parameterized by knowledge that is gf c c and intuitively gf x represents our goal of proving that something is in f under or assumed knowledge x definition parameterized greatest fixed point we define g c c c c gf x def y f x y here and elsewhere we write ya for ya note that the fixed point in the definition exists because y f x y is monotone for monotone f the monotonicity of g and gf is easy to check one way of understanding g is in the transition system from the introduction describes the set of nodes that either have an infinite path ie are in step or else have a nonempty path leading to a node in x to illustrate figure lists the set for each node x of parameterized coinduction we begin with the trivial observation that the parameterized greatest fixed point coincides with the standard one if no knowledge has been lemma initialize f gf further by simply unfolding the fixed point we obtain the following analogue of the strong coinduction principle mentioned previously lemma except that this version can be stated directly as an equality on the parameterized greatest fixed point lemma unfold gf x f x gf x what gf allows us to do in addition that f does not support is to knowledge in the following sense theorem y gf x y gf x y proof the direction follows straight from the monotonicity of gf in the direction assume y gf x y then gf x y f x y gf x y fixed point equation f x gf x y f monotone and z f x z gf x y therefore as gf x y is a point of yf x y we obtain from that gf x y gf x which together with entails y gf x as required compositionality of parameterized coinduction our construction also admits a clean compositional rule for combining proofs in the circular relyguarantee style g gf r g gf r r r g r r g g g gf r compose the rule says that we can prove g and g are correct under assumptions r by proving that g is correct under the additional assumption that g is correct and similarly that g is correct under the additional assumption that g is in essence this rule is sound because gf r allows the use of the assumptions r only within a guarded context the fact that the paths to nodes in x must be nonempty is what ensures that x can be used as an assumption only within guarded contexts although we have motivated the for compositionality separately from the for it turns out that this compose rule is equivalent to the theorem theorem which we have already proved and in fact the two principles are under no assumptions about how gf is defined aside from the fact that it is monotone to see this let us first derive compose from theorem assuming the premises of compose hold it is clear by monotonicity of gf that g gf r g g and g gf r g g so g g gf r g g by theorem g g gf r as desired conversely we can derive theorem from compose as before the direction follows straight from the monotonicity of gf in the direction assume y gf x y then instantiate compose with g g y r x and r r x y the conclusion yields y y y gf x as desired a simple application of parameterized coinduction we now return to the model checking example presented in the introduction and show how to use parameterized coinduction to prove a inf incrementally here we instantiate the principle with the powerset lattice on states of the transition system thus showing x s is equivalent to showing x s in this lattice a inf step a y a y b b b y b b b y d b b d b y b b d y b b b initialize unfold pick y b unfold pick y d since d b unfold pick y b as you can see we can perform the same incremental proof as when model checking the example note that in the proof we do not necessarily have to the visited nodes at every step but rather only when we think that their addition to the knowledge will be useful in the remainder of the proof with the same example we can also illustrate a simple use of compositionality we can easily establish b d d b by unfolding y d by unfolding y b thus by compose b d inf full characterization of gf finally we observe that lemma and theorem uniquely determine gf up to proposition for any g such that i x g x f x g x and ii x y y g x y y g x we have g gf proof given x g x gf x follows by from assumption i to show gf x g x it suffices by ii to show gf x g x gf x which after unfolding on the left and rewriting using i on the right follows by monotonicity of f a simulation example in this section we illustrate parameterized coinduction on a slightly larger example that demonstrates the practical motivation for a compositional incremental coinduction principle consider the two recursive programs f and g shown in figure these programs the user for a new numerical input compute the double of the sum of all inputs seen so far def fix n if n then output n f n else f f def fix fn let v output n input in if v then f else f v n g def fix gm output m let v input in if v then g m else g v m figure recursive programs in the simulation example v n fix f x e e x v e e e e if e then e else e input output e k e · · v e · · v if · then e else e output · q in n out n x e def fix f x e where f fve let x e in e def x e e e e def let x e in e where x fve n n n n if n then e else e e where n if then e else e e fix f x e v f x ef vx input n output n ke q ke where e q e figure syntax and semantics of a programming language and report the current value of the sum if at any point the user provides zero as an input then the programs count down to zero and start again the two programs differ in the representation of the double of the sum as well as in their programming style as the programmer responsible for f followed a somewhat coding style nevertheless it should be relatively straightforward to see that gm is equivalent to fm formally these programs are written in the minimal programming language defined in figure this is a standard callbyvalue calculus with integers recursion primitives for and integer values and a evaluation order for applications and arithmetic operations we give its operational semantics as a labelled transition system e q e with the labels recording numerical inputs and outputs we follow the standard convention of writing instead of for internal transitions for this language we can define weak similarity as the greatest fixed point of the following function on expression relations def e e v e v e v q e e q e e e q e re e where is equal to the closure of and q is equal to q for q if e is a value we require e to evaluate to the same value otherwise if e reduces to some e we require that e can match that execution step and end up in a state note that we are working with the powerset lattice where and we chose this simulation definition for simplicity we can also handle more elaborate definitions that result in equivalences for values of function type our goal is to show that gm simulates fm that is r fm gm m z sim while this might appear trivial proving this formally using coinduction principle is we have to come up with a simulation relation r containing all the intermediate execution steps of f and g appropriately matched just before the output just after the output just before the input just after the input just before the evaluation of the condition just after the evaluation of the condition just after the choice of the condition and so on in total there are cases this relation must be fully defined before we can even start the proof we remark that upto techniques the standard for simplifying simulation proofs while very helpful in many cases are of limited use in this example as the programs strictly alternate internal and external execution steps the up to reduction technique can reduce the cases only by a factor of two similarly the up to context technique does not help us relate the intermediate states of the calls to f and g because the two functions have rather different internal structure it does however help in reasoning about as we will see in section proof sketch using parameterized coinduction we now show how to apply parameterized coinduction in order to avoid the explicit manual generalization of the coinduction hypothesis that is required with principle first let us introduce the following shorthand def if v then f else f v m def if v then g m else gv m we will now prove r which by lemma sim step by applying theorem followed by lemma we get as our goal r sim r r r this reduces to showing e gm e e e r r where e let v output m input in we pick e output m let v input in and proceed to show e e r step we now unfold the fixed point lemma to get as our goal e e r which means showing e e e e e r r where e let v input in now we pick e let v input in and proceed to show e e r next steps further steps using lemma eventually lead us to a point where we have to prove the following two fv m gv m r r f m g m r r regarding the former we observe that the terms are related by r and so we are done regarding the latter we proceed as follows to show that they are related by r step we realize that i we have to increase the knowledge r because calls itself recursively and ii that before doing so we should generalize the goal to r f n g n n z r to this we now apply the principle theorem and get as our new goal r r r final steps we proceed in the same manner as earlier using lemma to step through the code of when at the recursive call in the we use the new r part of the coinduction hypothesis and are done when reasoning about the on the other hand we conclude by to the original r part which fortunately is still around the benefit of our approach over the traditional approach here is that the simulation relation does not have to be defined up front instead the intermediate goals and the quantifier instantiations can be generated automatically by an interactive theorem prover using simple tactics the details for achieving this will be presented in section the proof since parameterized coinduction also supports compositionality we can factor out the reasoning about from the previous proof into a separate generic lemma that may then be reused in other proofs lemma for all values f and f we have f n f n n z f f its proof follows straightforwardly from the fact that applies its function argument only to instead of proving the previous goal r directly it now suffices to prove the following lemma r r then r is obtained from lemmas and by rule compose using empty initial assumptions the proof of lemma follows the same structure as the one sketched for r except that we are done after performing what were called above the next steps remark without we cannot split the proof into separate lemmas about and about f and g for example while we can prove the following statements r sim r sim r sim r sim we cannot combine the two to derive r sim this requires a sound form of circular reasoning such as the one provided by the compose rule combination with upto techniques the incremental proof sketched in the previous section despite being a improvement over the one based on is still quite tedious to do at least on paper most of the proof involved through f and g doing so for f and g is necessary because their structure is quite different but for the proof of lemma about where the structure is identical it seems unnecessary in this section we will see that with a bit of additional theory this can be avoided as well in traditional bisimulation proofs people often employ simplification techniques known as upto functions intuitively an upto function maps an element r c to an element r typically larger than r that is still valid as a coinduction hypothesis for proving r f definition c c is a sound upto function for f c c iff r f r implies r f for all r c the standard definition eg does not require to be monotone but this is a very natural condition which holds of all upto functions in the literature that we are aware of and is needed for taking fixed points involving this raises the question is parameterized coinduction compatible with the use of upto functions fortunately the answer is yes to see how upto functions and parameterized coinduction can be combined we observe that sound upto functions respect the greatest fixed point lemma if is a sound upto function for f c c then f f where f def r f r proof we have f f f f f which by definition implies f f thus to prove x f using parameterized coinduction we can instead show x gf for any sound upto function how does this interact with compositionality suppose we have two separate proofs using two upto functions and x gf y r and y gf x r if f and f happen to be equal then we can combine the proofs using compose as expected however in general and might be two arbitrarily different upto functions and thus we may not be able to apply the composition rule fortunately there is a solution to this if we ourselves to upto functions definition c c is a upto function for f c c iff for any r s c the following holds r s r f s r f s lemma if is a upto function for f then it is also a sound one the proof follows that in although stronger than soundness is still satisfied by most upto functions of interest and has better compositionality properties in particular proposition id def x x is a upto function for any f if u and u are upto functions for f then so is u u if each element of a set x is a upto function for f then so is x we can thus define the greatest upto function for a given f c c f def u c c u is a upto function for f using proposition it is easy to show that is a upto function and that it is the greatest such moreover we have lemma if is a upto function for f then gf gf proof follows from monotonicity of g and now using lemma we can bring the two proofs from above to a common x gf y r and y gf x r and then compose them to get x y gf r as with the definition of a sound upto function sangiorgi does not assume monotonicity of but here he requires a slightly weaker property r s r f s r s here and elsewhere we treat a function space a c as the complete lattice obtained by lifting c pointwise e e r e e e e refl e e r e e r e e r if e then e else e if e then e else e if v fix f x ef fix f x ef r e e r fix f x e e fix f x e e figure a upto function actually since the greatest upto function is so powerful we see no point in ever stating a proof components contribution involving a different upto function in other words state your goal in terms of the greatest one the following properties enable the use of zero or more particular upto functions inside the proof of such a goal lemma if is a upto function for f then for any r c r r r r proof follows from proposition and respectively we remark that the greatest upto function also allows us to use upto reasoning at any point in a proof by parameterized coinduction not just after unfolding theorem gf r gf r proof the direction holds by lemma the direction is more complicated by it suffices to show gf r f r gf r this follows from lemma if we can show gf r f r gf r by in turn it suffices to show gf r r gf r and gf r f r gf r both are not hard to show using lemma and for the second monotonicity of f and simulation example we return to the simulation example pre in section and show how to apply an up to context technique in order to simplify the proof to reason up to contexts one usually defines a context closure operation we could do this here as well but in combination with we can get away with something simpler figure defines a function ctx pexp pexp it is straightforward to verify that this is a upto function for sim the proof can be found in our coq formalization observe however that the definition is not recursive and thus ctx only adds atomic contexts ie contexts whose definition does not involve any form of recursion we will see in a moment how we can nevertheless get the full power of a proper context closure operation the final rule is the case for function application as sim relates values only if they are identical the simpler rule if e e r and e e r then e e e e while sound is useless because the first assumption requires e and e to evaluate to syntactically the same function therefore requires the functions to be already values and checks that their bodies are related whenever the functions are applied to the same arguments finally note that the following rule can be derived from e e r e e r e e e e seq with the help of this upto function we will now prove the same two lemmas as before lemmas and except that we replace by lemma for all values f and f we have f n f n n z f f lemma r r proof of lemma as in section we apply the principle and reason about the first two steps of execution with the help of lemma then we obtain the following proof obligation where e e r r ei if n then output n fi n else fi r f n f n n z f f at this point since ctx by lemma we may apply one of the rules from figure in order to simplify this goal we pick if and hence it remains to show the following three n n r r output n f n output n f n r r f f r r now by the same argument we may apply another of these rules in each case so effectively we can apply an arbitrary number of rules and thus did not lose any power by defining ctx in terms of atomic contexts only is solved by rule refl and is solved by lemma since the terms are related by r to show we first apply rules seq and refl and then use lemma to reduce the goal to f n f n r this is easily shown by unfolding lemma performing a step of computation which converts the two instances of n to n and then once again with lemma the reader may have that we never seem to need rule indeed this is a side effect of reasoning via lemma nevertheless the rule is necessary without it ctx would not be proof of lemma here reasoning up to contexts seems not to us anything so we just derive the goal from our old proof lemma via lemma finally we can as before use compose to deduce that r gf by lemmas and this implies r f parameterized coinduction in this section we discuss at a high level the issues raised by formalizing our parameterized coinduction principle from section in a proof assistant such as or coq details about our coq implementation as well as examples using it follow in section to establish some common terminology we say that a predicate of arity n is a dependent function of type aa an s where the sort s is impredicative prop in coq bool in if instead the sort s is predicative type in coq or set in agda we call such objects indexed sets predicates are normally used for writing proofs whose computational meaning is not of interest whereas indexed sets are used for writing programs whose computational meaning is their main point of interest and of these two actually only predicates form complete lattices therefore in this paper whose main focus is on coinductive proofs we shall largely ignore indexed sets and only briefly discuss them in section there are two ways in which a formalization can be done namely what we call the external and the internal approach they differ in the way in which the parameterized greatest fixed point is constructed · the external approach a library of complete lattices that uses construction for greatest fixed points and then uses that library to define g defining such a library however requires impredicative quantification and so this approach only works for and coq but not agda · the internal approach defines g directly using the proof primitive mechanism for defining coinductive types eg coinductive or constructor in and related systems where coinductive are not primitive one must instead follow the external approach in coq where both approaches are applicable the internal approach besides being more direct is also easier to use because automation works much better there the main problem is that in the external approach automation tactics do not know how to unfold the constructs and so the user has to coq manually to do so we now discuss the two approaches in more detail the external approach in this approach one defines a generic library of complete lattices and greatest fixed points of arbitrary monotone and uses that to construct g and prove its properties the library can then be instantiated to the application domain at hand this approach is as general and modular as it gets and works quite well for both coq and the implementation is actually simpler already has a complete lattice theory which only needs to be extended with our parameterized coinduction to apply the library definitions and lemmas to arbitrary predicates one has to prove that prop in coq or bool in forms a complete lattice and that the space of dependent functions to a complete lattice forms again a complete lattice the pointwise lifting using type classes in or coq or canonical structures only in coq one can easily that the appropriate lattice structure for a given predicate is automatically inferred the internal approach as mentioned above the internal approach depends on having primitive support for coinductive types but if it is available it can be more a convenient option however the applicability of the internal approach is somewhat limited for two further reasons the only objects that one can define with the primitive coinductive definition mechanism are predicates and indexed sets hence this approach does not work for arbitrary complete lattices however it is still very useful in practice because i predicates are already quite expressive and ii we found a trick that enables us to extend this approach to refined predicates explained in detail in section moreover the predicates themselves must have a certain syntactic form all recursive uses of a defined object in its definition must be strictly positive ie roughly speaking not occur on the left of an arrow while this syntactic condition is restrictive for predicates it is important for ensuring consistency in the case of indexed sets and is thus imposed for on all coinductive definitions what does this for the formalization consider the powerset lattice from section it satisfies condition above ie it is a predicate type but condition prevents one from the definition of gf over f recall its definition g pexp pexp pexp pexp gf x def y f x y this can be translated into coq as follows definition exp exp prop coinductive g f fm monotonic f x e e exp prop in f x g f fm x e e rejected this definition however is rejected because it violates strict in the type of the constructor argument in g occurs in the argument of a function application where the function f is a variable this is forbidden intuitively because f could be instantiated to a function that uses its argument on the left side of an arrow fortunately there is a that lets us work around the strict requirement as long as the function in question here yf x y is monotone this trick is based on recursion and explained in section consequently in the internal approach we can also define a library for parameterized greatest fixed points of arbitrary monotone predicates up to some fixed section recursion to the proposed a strongly normalizing calculus an treatment of inductive and coinductive types in this section we show how recursion in the style can be used to address the two issues mentioned in section that come up when parameterized greatest fixed points following the internal approach we first review the idea of recursion in the setting of complete lattices and show how it can be employed to overcome strict restriction in the case of predicates this observation is not novel and has been made before by second we generalize this theory in a way that yields a method for defining fixed points in complete in the internal approach this enables us to define refined predicates while in the external approach it in avoiding dealing with dependent types as far as we can tell both the generalization of the theory and its application to are new strict a convenient way to view fixed point constructions is as ordinary fixed points of functions given a function g c c where c is a complete lattice there are two canonical ways of this function slightly to make it monotone definition we define g g c c g def x gy y x g def x gy y x their monotonicity is easy to see given that for x x we have gy y x gy y x and gy y x gy y x it is also easy to verify that both and form a galois connection with the canonical embedding of c c in c c in the way depicted in figure this means that y c c gcc figure operators and their galois connections · f c c f g f g ie g is the greatest monotone function below g and · f c c g f g f ie g is the least monotone function above g and thus g g g which explains our choice of notation now if g is already monotone then all three are equivalent and consequently so are their least and greatest fixed points proposition if g c c then g g g and hence µ g µ g and g g g so why is this interesting at all the point is that an already monotone function using yields the same function in a form that translates to a strictly positive one in coq to see this let us apply it to the definition of g that we at the end of the previous section given a monotone function f there are actually two ways to apply proposition to gf x in the first we f gf x g f x z f y y x z in the second we z f x z gf x z f x z z f x y y z hence we have two possible definitions of g in coq coinductive g f x e e exp prop y le y x g f x in f y e e coinductive g f x e e exp prop y le y g f x in f x y e e note the similarity to the rejected definition in the previous section and the absence of the fm assumption these new definitions are wellformed for arbitrary functions f not necessarily monotone and fm makes them more convenient to work with of course we need to assume monotonicity in statements about g then instead we prefer the first definition because then monotonicity of f is required only for unfolding gf the direction of lemma and is not needed for the theorem theorem we remark that this trick of functions that are already monotone in order to obtain a strictly positive form applies to inductive predicates defined using inductive command as well but we do not exploit that observation in this development fixed points in the problem recall that in the internal approach coinductive definitions are limited to predicates and indexed sets here we show how to this to refined predicates by refined predicates we mean objects whose type has the form x aa an prop p x ie a regular predicate type refined by some property p this is best illustrated with an example imagine we want to define the greatest fixed point of a function f pexp pexp pexp pexp we can easily do this using coinductive mechanism since pexp pexp is naturally expressed as a predicate type we may have to bring f into strictly positive form of course now imagine we want to define the greatest fixed point of a different function g pexp pexp pexp pexp note that the complete lattice pexp pexp cannot be expressed as a predicate type due to the restriction of the function space instead it can be seen as a refined predicate type where p is monotonicity in the external approach there is no problem we can define the complete lattice structure for this type then use that to write down the faithful definition of g prove that it is monotone and finally just apply the greatest fixed point operator to it but in the internal approach this is not possible the best we can do there so it seems is take the greatest fixed point in the unrestricted function space however i this assumes that g is welldefined and monotone in the larger space and ii even if that fixed point exists it will not necessarily be the desired one the solution our solution is as follows we do indeed take the greatest fixed point in the unrestricted space but only after modifying the function in a way that ensures i that it is welldefined and monotone and ii that the result actually coincides with the desired greatest fixed point in the original restricted space the math behind this is a generalization of what we saw in section and is basically stated in terms of an arbitrary complete lattice and an arbitrary complete that preserves meets andor joins because the theory is so general it could actually also be used in the external approach where the benefit would be avoiding to work with dependent subset types the theory consider two complete lattices b and c we are interested in the scenario where there exists an embedding of b in c in the following sense definition a function i b c is an embedding of b in c written i b c iff b b b b b b ib c ib note that this implies in the case where b is a complete of c the canonical injection from b to c such an embedding with the help of an embedding we can now define generalized versions of the operators from section definition generalized for i b c and a function g b b we define g i g i c c g i def x y b x g i def x y b x now if i preserves meets and joins eg because c is a function space with a pointwise ordering and b its restriction to monotone functions then the galois connections from earlier generalize as well and if moreover g is monotone then the least and greatest fixed points of g g i and g i coincide modulo the embedding proposition if x i x ix the following hold for any f c c and g b b f g i f i g for f i def y f ix if g is monotone then µ g i and g i ig proposition if x i x ix the following hold for any f c c and g b b g i f g f i for f i def y f ix if g is monotone then µ g i and g i ig finally observe that the results in section are merely a special case of the results presented here namely where b c and i id in which case i i and i i id the example to see what this looks like in practice let us return to the example g from the beginning since pexp pexp is a complete of pexp pexp we have by propositions and for the canonical injection i g g i g i we can now either g i or g i because i has the added benefit of yielding a strictly positive form even if g does not have one we pick the latter to see how this translates into coq note that g i pexp pexp pexp pexp g i x y pexp pexp x and that g i by unfolding is equal to y pexp pexp g i accordingly we write coinductive r e e exp prop y ym monotonic y le r y r r in g y ym e e here g y ym e e may look a bit different depending on how g is defined if it is not defined explicitly one can just inline it here similar to the first part and as is evident from the theory all this applies to induction ie least fixed points as well remark the type that we used for illustration pexp pexp pexp pexp is highly of one that occurs in the metatheory of relation transition systems a new kind of semantic model that we recently introduced for compositional reasoning about program equivalences in higherorder stateful languages reasoning using this method very much like a regular bisimulation argument to show the equivalence of two functions one has to construct a local knowledge relating them and then prove its consistency in our mechanized proofs we found that explicitly defining this local knowledge up front was quite a experience for essentially the same reasons as defining the simulation relation was in the example in section it turns out that being a consistent local knowledge can be expressed as being a point of a certain monotone function of basically the above type pexp pexp pexp pexp by following the internal approach and using the trick presented here we were able to bring the benefits of parameterized coinduction to our framework while keeping changes to existing definitions to a minimum coq implementation and evaluation in this section we discuss our implementation of parameterized coinduction in coq and compare it to other approaches most notably builtin tactic technique lines proof s qed s no § no internal § yes external § yes table comparison of the approaches for a fair comparison we have carried out the simulation proof of the example from section using a number of different approaches and show the results in table in each case we report a the number of lines of proof and auxiliary not counting the lines of generic library lemmas and tactics as these can be defined once and for all b the time taken to execute the corresponding proof script and c the time taken to check qed that the constructed proof object from running the script is valid explicitly define a simulation relation prove that it is valid ie a point of sim and then apply this requires by far the most human effort which is partially reflected in lines of code needed to define the simulation relation the proof itself is quite short and hence is fast use builtin tactic to simulate an incremental proof we shall discuss this approach in detail in section but for the moment we remark that proof checking the qed column for is significantly slower than for all the other approaches and can become a serious in larger examples internal use a direct encoding of following the internal approach described in section see section for the details external use a definition of following the external approach see section for the details among these approaches it is quite clear that the internal and external approaches are roughly equivalent with the internal approach being somewhat more efficient by of using primitive definitions and avoiding the use of inference during type checking namely canonical structures or type classes that the external approach requires the two are much faster than the approach moreover they enable compositional proof developments unlike the other two approaches internal implementation of parameterized coinduction we now show what the internal approach to parameterized coinduction discussed in section looks like in practice by applying it to the example from section for the sake of a contrast with the external approach we do not g which we could with the help of the trick from section but directly define coinductive r e e exp prop val v val e v e v exp q e e q e e e q e r e e r e e observe how r corresponds to r next we prove the following property which we could in principle coq to generate and prove automatically theorem l r r r r l r l r l r this is a variant of the direction of theorem in form which avoids the explicit join operation and is thus a bit more convenient to use than its simpler counterpart also note that the unfold property lemma is implicit in the definition of and that we do not need to state lemma either because we simply work with directly we can now already state and prove the example from section theorem n m eq n m bot f n g m proof using subst do do m left fold f g generalize mm using do n do qed in this proof script all tactics except step and are standard ie part of coq the former matches one step of e with n steps of e the latter in applying parameterized coinduction rewrites the goal to a form suitable for applying the theorem applies the theorem and then simplifies the result for instance at the beginning of the proof when the goal is simply the theorem statement applying acc directly is not possible because coq cannot figure out how to instantiate its l parameter hence we invoke which first transforms the goal into the following equivalent statement fun x y n m f n x g m y n m bot after that it applies acc its argument which now matches the goal and finally simplifies the resulting proof state so that the user never this explicit function expression r exp exp prop n m n m r f n g m n m n m r f n g m note that the new goal essentially also appears as the coinduction hypothesis except that has been removed and so the hypothesis is semantically guarded the second use of corresponds to the step of the proof sketch and results in the following proof state r exp exp prop n m n m r f n g m n r f n g n n r f n g n it essentially added the additional assumption but to achieve this it had quite some up to do internally after applying the theorem a new r greater than r had been introduced so used transitivity of to convert all existing hypotheses involving r into statements about r which it then finally renamed to r again although we have demonstrated the use of our tactic on one example it is not to this particular it is a general tactic that applies to arbitrary coinductive predicates it is implemented in with the help of the library for handling dependent types external implementation of parameterized coinduction we implement g from section using a complete lattice library that provides a type of complete lattices and operations such as all with the obvious meaning definition g c f c c x c fun y f x y next we prove lemma here as two separate lemmas and the interesting direction of the property theorem again expressed in to make it easier to use like before there is no point in defining sim explicitly and proving lemma because we can just work with directly lemma fc c monotone f r c f g f r r g f r lemma fc c monotone f r c g f r f g f r r theorem fc c monotone f l r r r r l r l g f r l g f r to use this library we simply define the generating function sim inductive sim r e e exp prop val v val e v e v exp q e e q e e e q e r e e then g sim corresponds to where the implicit argument is automatically inferred from the type of sim with the help of canonical structure mechanism the statement of the example from section is the same as in the internal implementation fg simulated above except that is replaced by g sim in the corresponding proof script the argument to the tactic changes from acc to g acc sim similarly the step tactic is replaced by one that applies g fold sim instead of fold standard coinduction principle is rather syntactic and thus quite different from the principle it basically works as follows to show x f one invokes builtin tactic which adds the very same proposition as an assumption to the local context being an ordinary assumption it can be used at any point in the proof script however once the proof is finished coq runs a syntactic check on the proof term and accepts it only if the use of the coinductive assumption is guarded it is possible although not well known that one can nest uses of and thereby achieve a form of incremental coinduction for example if we take the proof script from section and simply replace the two occurrences of using acc with we obtain a valid proof despite its surprising in allowing a form of approach to coinductive proofs has several important due to its syntactic nature · it is inside a proof via the use of normal lemmas involving the coinduction hypothesis is not permitted by checking because they are treated one can of course mark these lemmas as transparent thereby further down proof checking but this does not yield proper compositionality from the statements of the transparent lemmas alone one cannot know whether their proofs can be composed together to yield a valid proof · it is inefficient checking can be very slow mainly because it has to reduce proof terms to normal forms which may be this problem is already apparent from table and another of syntactic checking can be found in et al one way to think of this is by the following analogy our compose rule corresponds to the relyguarantee parallel composition rule whereas transparent lemma application within a corresponds to the earlier rule with the syntactic noninterference side condition becomes in larger developments in our coq code we provide an example from an earlier project where proof checking takes seconds due to replacing with reduces this to seconds · it is not at all the user interface does not indicate when exactly in a proof it is safe to use the coinductive assumption coq provides a designated command for explicitly checking but due to the previous issue its repeated use during the proof is often · it with builtin automation tactics they do not know about and hence very frequently produce incorrect proofs usually this happens because automation applies constructors and hypotheses in the wrong order solving the goal but causing the proof to be rejected later by the checker to make worse as a consequence of the two previous issues it is very to such situations consequently one has to be extremely careful when using automation as a of automation leading to a dead end consider the following code definition monotone f prop prop p q prop f p p q f q coinductive a f prop p prop le p a f in f f p goal f prop prop monotone f p prop p f f p p a f proof qed rejected here although we explicitly apply the constructor first to construct an invalid proof term we can obtain a proper proof by manually applying the coinduction hypothesis before letting take over this is the result of and error apply alternatively we can just do the proof using our tactic instead of where a acc is the corresponding lemma using using a coq library for parameterized coinduction in order to make parameterized coinduction more easily applicable we have built the coq library for parameterized coinduction contains internal implementations of parameterized coinduction for predicates of arity up to with f for gf for any monotone function f from predicates of arity n to predicates of arity n besides the tactic that we have already seen in section the library provides tactics for folding and unfolding the definition of gf for proving monotonicity of predicates and for simplifying hypotheses by reducing occurrences of r to r it also provides multiplication lemmas of the form gf gf r gf r these follow easily from theorem and monotonicity of gf and show that guarded assumptions r are also simply guarded we have found these multiplication lemmas useful for composing parameterized coinduction proofs to illustrate the use of our library we build on the example of the introduction we represent a graph by a type of nodes and a relation r characterizing the edges between nodes variables node type r node node prop we now define infinite paths using parameterized coinduction we write and bot as the predicate is unary inductive step x node prop x node prop y r x y x y step x x constructors step definition step bot we also prove monotonicity of step and register the corresponding lemma in the database that is used by lemma monotone step proof qed resolve further we define the predicate path n x to say that there exists an outgoing path of length n from node x fixpoint path n x match n with o true s n y r x y path n y end then we can establish that if an infinite path from x then so do paths of length n for any n this is proved by induction on n and unfolding and the definition of infinite paths goal n x x path n x proof induction n h inversion h qed this shows that coinductive definitions can after unfolding be just as native coq coinductive definitions can we move on to a more interesting property involving transitive closure that was suggested to us by an anonymous the goal is to prove that if there is a predicate p holding of a node x in a graph and that whenever p holds of a node there is a nonempty path at the end of which p holds again then there is an infinite path starting from x the infinite path can be constructed by these nonempty paths which is formally done by an inner induction inside a coinductive proof goal p node prop x p x y r x y p y x p x x proof p m x px m px as y c py clear px induction c qed using the proof is straightforward the second assumption and the existential quantifier to expose the transitive closure upon which an induction is later performed whereas clear px simply px to avoid the later automation moreover ensures that the coinductive hypothesis is used in a semantically guarded way by construction thereby placing no further restrictions on the proof in contrast carrying out this proof using builtin tactic is surprisingly difficult because the inner inductive proof turns out to violate conservative syntactic notion of thus discussion and related work in this section we compare to some related forms of incremental coinduction that have appeared in the literature including the earlier versions of our construction due to and we conclude with some about coq local model checking and the reduction lemma to our knowledge the account of the parameterized greatest fixed point is in a paper by that paper building on prior work of and and walker is focused on the specific problem of local model checking in the modal deciding whether a particular state or process in a labelled transition system satisfies some assertion however in the course of this specific problem presents a generally useful construction on power sets but easily to lattices that with we can analyze and in a more abstract way key is a parameterized recursive assertion which he writes as one can understand this assertion as representing the greatest fixed point xa under the knowledge r but this is the key difference from our parameterized greatest fixed use of the knowledge is not guarded provides the following rules for checking recursive assertions incrementally where p a denotes that process p satisfies assertion a p r p p r p p due to the by the first of these rules parameterized assertion does not support compositional reasoning along the lines of our compose rule in particular from p xq ra and q xp ra one cannot conclude that p or q since the premises hold trivially if p q interestingly a subsequent paper by and building on parameterized assertions presents a compositional proof system for the modal µcalculus but they mean overloaded term in a very different sense from us their compositional concerns the structure of processes our parameterized greatest fixed point may be understood as a guarded version of formally model of is with xr a in the notation of our paper this suggests the following alternative to our gf x wf x def z x f z the connection to gf x provable using a few straightforward applications of principle is then very simple wf x x gf x gf x f wf x for the purpose of proving soundness of local model checking the lack of in wf x was not an issue but for compositional proof development it is that said the key technical result that in order to prove soundness of his local model checking algorithm namely a reduction lemma due to kozen is in fact with our theorem the reduction lemma states for monotone f y f y f wf y to see the connection with theorem first observe that since f wf y gf y the reduction lemma can be as y f y gf y and since f gf the lemma can thus be seen as an instantiation of theorem where x at the same time theorem can also be seen as an instantiation of the reduction lemma specifically if given x we instantiate the reduction lemmas f with f x it yields y z f x z y z f x y z this is precisely theorem incremental coinduction in and proposed a proof system for incremental coinduction towards bisimilarity in a process calculus and they established the soundness of their system by a global monolithic argument their judgment corresponds precisely to gf where f is the generating function of their process bisimilarity this suggests that account of parameterized coinduction might a offer a simpler alternative proof of their logics soundness and b support a shallow embedding of their logic in which is much more efficient than a deep embedding since it the proof underlying infrastructure finally we note that because their proof system like does not offer a way of expressing guarded it does not admit a circular compositional rule such as compose circular coinduction rather than developing a custom proof system another approach to incremental coinduction that has been tried is to a tactic that builds the full simulation relation on the as the interactive proof the idea is to use a unification metavariable during the proof and try to show that r r and r f r to solve the first goal we set r r r where r is a fresh metavariable each time we later a goal r r r where r r we instantiate r r r where r is another fresh metavariable and proceed if this exploration phase ever terminates then at the end there should be one rn metavariable left which we can instantiate to the empty relation in this way we will have constructed a valid point of f incrementally and coq can thus conclude that r f under this approach even if the construction of the simulation relation can be done incrementally using tactics the whole simulation has to eventually be constructed as a result this ap does not support compositional reasoning principle standard library theory inductive contains an interesting coinduction principle for sets attributed to martin generalized to complete lattices it reads x f f y x f x f this principle is strictly less useful than our parameterized coinduction while it lets us remember the initial x for as many of f as desired it does not let us further knowledge during the proof nor does it support compositional reasoning interestingly if we change the least fixed point into a greatest one then the result f y f y x f is equivalent to our gf parametric first introduced the construction that we use in this paper in a categorical setting calling it parametric and proving an analogue of theorem he was concerned with defining elements of coinductive sets rather than constructing proofs of coinductive predicates and thus did not observe the compositionality of this simple construction nor its practical utility in mechanized theorem proving more specifically in his work given sets x and y of variables a function of type x gf y models a set of equations of the form x xy xx where gf y is the coinductive set defined by the functor f y which corresponds to our gf constructor and xs are formulae of some particular form related to f then given a set of recursive equations of type x gf x y the variant of theorem gives its solution for x which is a set of nonrecursive equations of type x gf y in particular when y is the empty set x gf determines elements of gf the final coalgebra of f ie the coinductive set defined by f below we present an example showing that this construction can be useful for programming guarded functions take to be the set of parameterized infinite binary trees with nodes containing natural numbers defined as follows coinductive r type type a nat tl r r tr r r we can then define the combinator of the following type using the construction above see the for details l r l l r l r now we define the infinite tree containing on every left child and on every right child depicted below x Ô Ô using we can write the definition of without using thus avoiding checking definition false fun inr inl inl fun inr inr inl inr inl i i nice as these examples are we believe that the more important use of categorical construction is in building proofs incrementally and using tactics as we have to demonstrate in this paper application to agda we remark that the direct internal approach also works for agda and indexed sets because lemmas ­ and theorem can be generalized to a lattice where the greatest fixed point gf x exists for a particular f ie it does not have to exist for all f as in a complete lattice we can easily show that indexed sets form lattices as they have a natural order finite products and also for a strictly positive function f we can define gf x using constructor or coinductive which gives a greatest fixed point of f on this lattice and we can thus reason about statements of the form y gf x using our principle in this setting however we cannot use recursion because it requires impredicative quantification coinduction vs induction in coq it is to contrast support for induction and coinduction in both cases coq provides a builtin combinator that comes with a syntactic condition as we have seen in section it is possible but quite to use directly inside a proof via the tactic for each inductively defined type coq additionally generates induction lemmas proved using the recursion combinator which semantically enforce these lemmas can be applied using the induction tactic the use of the lowlevel recursion combinator inside proofs unnecessary for most users for defined types however coq does not generate any lemmas nor does it provide a coinduction tactic analogous to its induction tactic our work can be seen as this gap by providing lemmas such as g unfold and g acc and the tactic it is also worth pointing out that unlike the principles for induction ours are complete whatever can be proved using can also be proved using g acc co acknowledgements we would like to give special thanks to for us to the connection with work we would also like to thank pitts and for helpful discussions as well as the anonymous reviewers for their constructive feedback references h r c and g a compositional proof system for the modal µcalculus in lics pages ­ ieee computer society g m j e l and t typebased termination of recursive definitions mathematical structures in comp sci ­ e guarded definitions with recursive schemes in types for proofs and programs volume of lncs pages ­ springer a d gordon bisimilarity as a theory of functional programming theoretical computer science ­ d t and l iterative circular coinduction for in in volume of lncs pages ­ springer ck a coq library for heterogeneous equality presented at coq workshop ck d dreyer g and v vafeiadis the of bisimulations and kripke logical relations in popl c b jones specification and design of parallel programs in ifip pages ­ d kozen results on the propositional µcalculus theor comput sci ­ k g proof systems for logic with recursion in volume of lncs pages ­ springer r recursion on nested datatypes in dependent type theory in in volume of lncs pages ­ springer n p inductive types and type constraints in the secondorder lambda calculus of pure and applied logic ­ r milner communicating and mobile systems the picalculus cambridge university press l s parametric theor comput sci ­ june s s and d an axiomatic proof technique for parallel programs informatica ­ a and e l incremental coinduction for process algebra and its formalization in pages ­ d sangiorgi on the bisimulation proof method mathematical structures in comp sci ­ oct d sangiorgi introduction to bisimulation and coinduction cambridge university press d sangiorgi and j advanced topics in bisimulation and coinduction cambridge in theoretical computer science cambridge university press j s v vafeiadis f s and p sewell concurrency and verified compilation in popl c and d walker local model checking in the modal in vol volume of lncs pages ­ springer a a fixpoint theorem and its applications j math ­ g a note on model checking the modal calculus in icalp volume of lncs pages ­ springer 