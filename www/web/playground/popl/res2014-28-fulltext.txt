modular higherorder cardinality analysis in theory and practice software institute simon peyton jones microsoft research abstract since the mid s compiler writers for functional languages especially lazy ones have been writing papers about identifying and exploiting thunks and that are used only once however it has proved difficult to achieve both power and simplicity in practice we describe a new modular analysis for a higherorder language which is both simple and effective and present measurements of its use in a state of the art compiler the analysis finds many thunks and and enables a number of program optimisations categories and subject descriptors d programming techniques applicative functional programming f logics and meanings of programs semantics of programming languages program analysis operational semantics general terms languages theory analysis keywords compilers program optimisation static analysis functional programming languages haskell lazy evaluation thunks cardinality analysis types and effects operational semantics introduction consider these definitions written in a purely functional language like haskell int int int k sum map k k k f int int f xs let ys map costly xs in n sum map n ys here we assume that costly is some function that is expensive to compute and is either or if we replace ys by its definition we could transform f into f f xs n sum map n map costly xs a compiler like ghc can now use deforestation to the two maps into one eliminating the intermediate list and a substantial performance et al permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ san diego ca usa copyright c acm does this transformation make the program run faster or slower it depends on for example calls its function argument ten times so if function f would compute costly ten times for each element of xs whereas f would do so only once on the other hand if which calls its argument exactly once then f is just as efficient as f and fusion can improve it further the reverse is also true if the programmer writes f in the first place the full laziness transformation peyton jones et al will float the subexpression map costly xs out of the so that it can be shared that would be good for but bad for what is needed is an analysis that can provide a sound approximation of how often a function is called ­ we refer to such an analysis as a cardinality analysis an compiler can then use the results of the analysis to guide its transformations in this paper we provide just such an analysis · we two different useful forms of cardinality namely a how often a function is called and b how often a thunk is forced in a lazy language section of these the former is relevant under both callbyneed and callbyvalue while the latter is specific to callbyneed · we present a backwards analysis that can soundly and efficiently approximate both forms of cardinality for a nonstrict higherorder language section a significant is our use of call demands to model the usage of a function this makes the analysis both powerful and modular · we prove that our algorithm is sound for example if it claims that a function is called at most once then it really is section this proof is not at all straightforward because it must take account of sharing that is the whole point so we cannot use standard denotational techniques but instead must use an operational semantics that models sharing explicitly · we a number of program optimisations enabled by the results of the cardinality analysis prove them sound and what is more important improving section · we have implemented our algorithm by extending the glasgow haskell compiler ghc a state of the art compiler for haskell the implementation builds directly on current strictness and absence and is both simple and efficient section · we measured how often the analysis finds functions and thunks section and how much this knowledge improved the performance of real programs sections ­ the analysis proves quite effective in that many and thunks are detected in the range depending on the program improvements in performance are but consistent a few programs already by ghc are a challenging target we discuss related work in section features of our work are a the notion of call demands b a full implementation measured against a state of the art compiler and c the combination of simplicity with performance improvements what is cardinality analysis cardinality analysis answers three questions in the setting of a nonstrict pure functional language like haskell · how many times is a particular syntactic called section · which components of a data structure are never evaluated that is are absent section · how many times is a particular syntactic thunk evaluated section call cardinality we saw in the introduction an example where it is helpful to know when a function calls its argument at most once a lambda that is called at most once is called a lambda and they are extremely common in functional programming for example a continuation is usually so cardinality analysis can be a big when programs nor is that all as we saw in the introduction inlining under a lambda to transform f into f allows deforestation to two calls of map but deforestation itself introduces many calls of of the function build build forall b a b b b b a build g g you can see that build calls its argument exactly once and inlining ys in calls like build cn ys turns out to be crucial to making deforestation work in practice a section of his thesis to this point chapter an analysis for so his implementation still in ghc relies on a he to behave for build itself and a of other functions no userdefined function will have this good behaviour our analysis subsumes the by providing an analysis that the correct information for build as well as many other functions in a higher order language with functions we need to be careful about the details for example consider f a a t costly x in y ty a g g a g a a g sum map g a if was then in f it would be best to inline t at its use site thus f a a xy costly x y the transformed f is much better than f it avoids allocating a thunk for t and avoids allocating a function closure for the y but if f called instead such a transformation would be why because applies its argument g to one argument a and the function thus computed is applied to each of integers in f we will compute costly a once but f will compute it times which is arbitrarily bad so our analysis of must be able to report argument g is called once applied to one argument and the we will always use called to mean applied to one argument result is called many times we this by giving a usage signature to like this u c c u · u c c u · the notation c c u is a usage demand it describes how a function value is used the demand type u c c u · describes how a function uses its arguments therefore it gives a usage demand for each argument the · has no we are just used to something after the final arrow informally the c d means this argument is called once applied to one argument and the result is used with usage d whereas c d means this argument may be called many times with each result used with usage d the u means is used in some unknown way or even not used at all note that second argument usage is c c u not c c u that is in all cases the result of applying g to one argument is then called only once absence consider this function f x case x of pq a strictness can see that f is strict in x and so can use callbyvalue moreover rather than allocate a pair that is passed to f which immediately takes it apart ghc uses a transformation to pass the pieces separately thus f x case x of pq fw p q fw p q now f the wrapper is small and can be at fs call sites often eliminating the allocation of the pair fw the does the actual work strictness analysis and the transform to exploit its results are important to generating efficient code for lazy programs peyton jones and peyton jones and in general fs righthand side often does not have a syntactically visible case expression for example what if f simply called another function g that was strict in x fortunately the transform is easy to suppose the right hand side of f was just then we would transform to f x case x of pq fw p q fw p q let x pq in now we hope that the binding for x will with case expressions in and indeed it usually proves to be so peyton jones and but what if did not use q at all then it would be to pass q to fw we would rather transform to f x case x of pq fw p fw p let x p error in this turns out to be very important in practice programmers write functions with arguments but they frequently write functions that use only part of their argument and ignoring this point leads to large numbers of unused arguments being passed around in the program after the transformation absence analysis has therefore been part of ghc since its peyton jones and but it has never been in the framework of this paper we give f a usage signature like this f u u a · the u u a indicates that the argument is a product type that is a data type with just one constructor the a for absent indicates that f the second component of the product thunk cardinality consider these definitions f int int int f x c if x then c else if x then else c g y f y costly y since f is not strict in c g must build a thunk for costly y to pass to f in callbyneed evaluation thunks are that is when a thunk is evaluated at runtime it is with the value so that if it is evaluated a second time the value can be returned immediately but in this case we can see that f never evaluates its second argument more than once so the step is entirely we call these thunks is not expensive but it is certainly not free operationally a pointer to the thunk must be pushed on the stack when evaluation starts it must be to avoid space leaks jones and the update involves a memory write if cardinality analysis can identify thunks as well as that would be a good thing and so it can we give f the usage signature f u u · the says that f may evaluate its first argument more than once while the says that it evaluates its second argument at most once call vs evaluation for functions there is a difference between being evaluated once and called once because of haskells seq function for example f g g seq f u · f g g seq g f c u · f g g f c u · the function seq evaluates its first argument to form and returns its second argument if its first argument is a function the function is evaluated to a lambda but not called notice that fs usage type says that g is evaluated more than once but applied only once for example consider the call f x x y how many times is y evaluated for f equal to f the answer is zero for f and f it is one cardinality analysis we now present our analysis in detail the syntax of the language we is given in figure it is quite conventional just lambda calculus with pairs and nonrecursive constants include literals and primitive functions over literals as well as haskells builtin seq we use form and felleisen so that the issues concerning thunks show up only for let and not also for function arguments usage demands our cardinality analysis is a backwards analysis over an abstract domain of usage demands as with any such analysis the abstract domain a balance between the cost of the analysis and its precision our particular choices are expressed in the syntax of usage demands given in figure a usage demand d is one of the following · u d d applies to pairs the pair itself is evaluated and its first component is used as described by d and its second by d expressions and values e x v e x let x e in e case e of x x e v x e x x annotated expressions and values e x v e x let x m e in e case e of x x e v m x e x x usage demands and d c nd u d d u hu d a n d n m demand equalities c u u u u u u u a a hu usage types · d usage type expansion d d · u · usage environments x d auxiliary notation on environments x d when x d a otherwise usage signatures and signature environments k k z p x p transform k d if d c k fold c u otherwise figure syntax of terms values usage types and environments · c nd applies to functions the function is called at most n times and on each call the result is used as described by d call demands are to the best of our knowledge new · u or used indicating no information the demand can use the value in an arbitrary way · hu or is a special case it is the demand that seq places on its first argument seq hu u · a usage demand d always uses the root of the value exactly once it cannot express absence or multiple evaluation that is done by d which is either a absent or n d indicating that the value is used at most n times in a way described by d in both c nd and n d the multiplicity n is either or meaning many notice that a call demand c nd has a d inside it not a d if a function is called its body is evaluated exactly once this is different from pairs for example if we have let x e e in fst x fst x then e is evaluated twice both u and hu come with some equalities denoted by in figure and necessary for the proof of section for example u is equivalent to a pair demand whose components are used many times or a where the result is used in an arbitrary way similarly for pairs hu is equivalent to u a a while for functions hu is equivalent to c a if we had such a thing in the rest of the paper all definitions m d n d d d d d d ad d d a d n d n d d d a d d d a d n d n d n nd d d d d d d d du u u d u d hu d hu d d c n d c n d c d d u d d u d d u d d d d du u u d hu d hu d d c n d c n d c n n d d u d d u d d u d d d d x d d i x di x d d i x di d d d d · · n d d n n d d d d d · n d n d n n x n x x dom n n n n n ab a b a b b figure demands and demand operations p e d e x p transform d p x d x d x x p x d · x d x p e de e p x e c n de x n x lam n x e p x e c u e p x e u e p x e hu x e p e c d d r e p y d p e y d r e y p e c d · e p y u p e y d · e y p xi di i i p x x u d d · pair x x p x x u u u · p x x u · e e p x x hu · x x p er d r er p es u r x r y s es case p case es of x y er d r xy s case es of x y er p x d abs p x a p x d x p x nd n figure algorithmic cardinality analysis specification part and metatheory are modulo checking that all our definitions do respect is routine usage analysis the analysis itself is shown in figures and the main judgement form is written thus p e d e which should be read thus in signature environment p and under usage demand d the term e places demands on its components and to an annotated term e the syntax of each of these components is given in figure and their roles in the judgement are the following · the signature environment p maps some of free variables of e to their usage signatures section any free variables outside the domain of p have an signature · the usage demand d describes the degree to which e is evaluated including how many times its are evaluated or called · using p the judgement transforms the incoming demand d into the demands that e places on its arguments and free variables respectively the usage that e places on its argument is given by which gives a demand d for each argument the usage that e places on its free variables is given by its usage which is simply a finite mapping from variables to usage demands · we will discuss the expressions e in section for example consider the expression e x case x of p q p f true suppose we place demand c u on e so that e is called just once what demand does it then place on its arguments and free variables e c u u u a · f c u that is e will use its argument once its arguments first component perhaps many times but will ignore its arguments second component the a in the usage type moreover e will call f just once in short we think of the analysis as describing a demand transformer transforming a demand on the result of e into demands on its arguments and free variables pairs and case expressions with these definitions in mind we can look at some of the analysis rules in figure rule pair explains how to a pair under a demand u d d we simply the two components under d auxiliary with the multiplicity of the argument demands di the operator both is defined in figure and combines the and for the most part the definition is straightforward but there is a very important for call demands c nd c nd c d d the part is easy since n and n are both at least but note the switch from to the least upper bound to see why consider what demand this expression places on f ff each call gives a usage demand for f of c c u and if we use to combine that demand with itself we get c c u the inner is a consequence of the switch to and expresses the fact that no partial application of f is called more than once the other rules for pairs and case expressions case should now be readily r xy stands for the removal of x y from the domain of r lambda and application rule lam for expects the incoming demand to be a call demand c then it analyses the body e with demand de to give if n the lambda is called at most once so we can return but if n the lambda may be called more than once and each call will place a new demand on the free variables the n operation on the bottom line accounts for this multiplicity and is defined in figure rule handles an incoming demand of u by treating it just like c u while deals with the demand hu where the lambda is not even called so we do not need to the body and e is obtained from e by adding arbitrary annotations similarly the return type can be any type since the abstraction is not going to be applied but is only given an application e y rule analyses e with demand c d that e is here called once this returns the demand d on the context then we can the argument under demand d using yielding and combine and rule applies when e yields the usage type · usage signatures suppose we have the term let f xy x true in f p q we would like to determine the correct demands on p and q namely c u and a respectively the standard would be to fs righthand side at every call site that is to behave as if f were at each call site but that is not very modular with nested function definitions it can be exponentially expensive to each function body at each call site and it does not work at all for recursive functions instead we want to f its behaviour and then use that summary at each call site this summary is called fs usage signature remember that the main judgement describes how a term transforms a demand for the value into demands on its context so a usage signature must be a conservative approximation of this demand transformer there are many ways in which one might approximate fs demand transformer but rule figure uses a particularly simple one · look at fs right hand side y yk e where e is not a · e in demand u giving · record the triple k y y as fs usage signature in the environment p when the body of the let now at a call site of f rule calls transform d to use the recorded usage signature to transform the demand d for this occurrence of f what does transform k d do figure if the demand d on f is stronger than c c u where the call demands are nested k deep we can safely at the call site if not we simply treat the function as if it were called many times by both the demand type and the usage environment figure rule handles the case when the variable is not used in the body thunks the rule an approximation to the demands of the righthand side at each usage site this is good if the right hand side is a lambda but not good otherwise for two reasons consider let x y in x x how many times is y just once the thunk x is twice but xs thunk is so the y is evaluated only once so it is wrong to a demand on y at each of xs occurrence sites contrast the situation where x is a function let x v y v in x x here y really is twice and does that another reason that would be for thunks is shown here p e u e f y p f k f y e d f n c n c nk e p let f y yk e in e d f let f n n y nk yk e in e p e u e f y f a p f k f y e d e p let f y yk e in e d f let f y yk e in e p e d e n dx x p e dx e p let x e in e d x let x n e in e p e d e a x p let x e in e d x let x e in e figure algorithmic cardinality analysis specification part let x pq in case x of ab a the body of the let places usage demand u u a on x and if we xs righthand side in that demand we would see that q was unused so we get more information if we wait until we know the demand on x and use it to its righthand side this idea is in the rule used if does not apply ie the right hand side is not a lambda rule first analyses the body e to get the demand x on x then analyses the righthand side e using that demand notice that the multiplicity n of the demand that e places on x is ignored that is because the thunk is otherwise the rule is quite straightforward rule deals with the case when the bound variable is unused in the body elaboration how are we to take advantage of our analysis we do so by the term during analysis with annotations of two kinds see the syntax of annotated expressions in figure · carry an annotation m to indicate how often the let binding is evaluated · m x e carry an annotation m to indicate how often the lambda is called serves as an that the lambda is not to be called at all figure shows the terms after the the operational semantics section gets stuck if we use a thunk or lambda more often than its usage and the transformations section are by the same annotations a more realistic language the language of figure is to its our implementation handles all of haskell or rather the core language to which haskell is translated by ghc in particular · usage signatures for constants are · all data types with a single constructor ie simple products are treated analogously to pairs in the analysis · recursive data types with more than one constructor and cor case expressions with more than one alternative and hence also conditional statements are supported the analysis is more approximate for such types only usage demands that apply to such types are u and hu not u d d furthermore case expressions with multiple branches give rise to a least upper bound combination of usage types as usual · recursive functions and are handled using the standard kind of fixpoint iteration over a domain soundness of the analysis we establish the soundness of our analysis in a sequence of steps soundness means that if the analysis claims that say a lambda is then that lambda is only called once and similarly for thunks we this property as follows · we present an operational semantics written for the annotated language that counts how many times thunks have been evaluated and abstractions have been applied the semantics simply gets stuck when these counters reach zero which will happen only if the claims of the analysis are false section · our goal is to prove that if an expression e is to e by the analysis then e in the instrumented semantics behaves to e in a standard callbyneed semantics section for reasons of space we omit the rules for the callbyneed semantics which are completely standard and are identical to the rules of figure if one simply ignores all the annotations and the multiplicity we refer to this semantics as · we prove soundness by giving a type system for the annotated terms and showing that for welltyped terms the instrumented semantics simulates in a typepreserving way counting operational semantics we present a simple counting operational semantics for annotated terms in figure this is a standard semantics for callbyneed except for the fact that multiplicity annotations the terms stacks and heaps the syntax for heaps denoted with h contains two forms of bindings one for expressions x m and one for already evaluated expressions x m the multiplicity m denotes how many more times are we allowed to dereference this particular binding the stacks denoted with s are just lists of frames the syntax for frames includes application frames · y which store a reference y to an argument x y e which account for the execution of a and update frames of the form x m which take care of updating the heap when the active expression reduces to a value the first component of an update frame is a name of a variable to be updated and the second one is its thunk cardinality rule allocates a new binding on the heap the rules only if the cardinality annotation is nonzero it dereferences an binding and an update frame rules and are standard notice that also only if the s multiplicity m is nonzero note that the analysis does not assign to but we need them for the soundness result rule dereferences a binding for an expression x m and in a standard semantics would return v leaving the heap in our counting semantics however heaps h x m h x m h stacks s · y s x m s x y e s auxiliary definitions x e m x e m x e where v v otherwise h e s h e s h let x m e in e s h x m x s h x m e s h e x m s if m h x m x s h x m v s h v x m s st v h x m v s st v h m x e · y s h s if m he y s h e · y s h case es of x y er s h es x y er s h x x y y er s h er xy xy s figure a nondeterministic counting operational semantics the guards for counting restrictions are by grey boxes we need to account for two things first we decrease the multiplicity annotation on the binding from m to m in rule moreover the value v can in the future be used both directly since it is now the active expression and indirectly through a future dereference of x we express this by nondeterministically splitting the value v returning two values v and v whose toplevel annotations sum up to the original see split in figure our proof needs only ensure that among the nondeterministic choices there exists a choice that simulates rule is similar except that the heap gets updated by an update frame checking terms we would like to prove that if we a term e producing an annotated term e then if e executes for a number of steps in the standard semantics then execution of e does not get stuck in the instrumented semantics of figure to do this we need to prove preservation and progress lemmas showing that each step takes a term to a term and that terms do not get stuck figure says what it means to be using notation from figures and the rules look very similar to the analysis rules of figures except that we check an annotated term rather than producing one for example rule checks that the annotation on a abstraction m is at least as large as the call cardinality we press on this abstraction n as evaluation the situation so the annotations may become more conservative than the checker requires but that is fine a more substantial difference is that instead of holding concrete demand transformers as the analysis does figure the environment p holds generalised demand transformers a generalised demand transformer is simply a monotone function from a demand to a pair of a type and a usage environment figure in the rule we choose any such transformer which is sound for the rhs expression ­ denoted with p t e we still check that that e can be type checked with some demand d that comes from typechecking the body of the let x in rule we simply apply the transformer to get a type and environment rule imposes two conditions necessary for the soundness of the transformer first it has to be a monotone function on the demand argument second it has to soundly approximate any type and usage environment that we can attribute to the expression one can easily that the intensional representation used in the analysis satisfies both properties for the expressions bound with because these rules up functions out of and have universally quantified premises in they do not constitute an algorithm but for the very same reasons they are convenient to reason about in the metatheory and that is the only reason we need them in effect figure an elaborate invariant for the operational semantics soundness of the analysis the first result is almost trivial lemma analysis produces welltyped terms if p e d e then p e d we would next like to show that welltyped terms do not get stuck to present the main result we need some notation first definition heaps and stacks and erasure we use h and s to refer to an heap and stack respectively we use e e to mean that the erasure of all annotations from e is e and we define s s and h h analogously we can show that annotated terms run for at least as many steps as their would run in the semantics theorem safety for annotated terms if e hu and e e and e k h e s then there exist h e and s such that e k h e s h h s s and e e to prove this theorem we need to the statement to talk about a reduction of a configuration with arbitrary but heap and stack hence we introduce a configuration relation denoted h e s that extends the invariant of figure to configurations for reasons of space we only give the statement of the theorem below and defer the details of the relation to the extended version of the paper et al lemma safety assume that h e s if h e s h e s in the semantics then there exist h e and s such that h e s h e s h h e e and s s and moreover h e s notice that the counting semantics is nondeterministic so lemma simply ensures that there exists a possible transition in the counting semantics that always results in a welltyped configuration lemma relies on yet another property below p p x d p ed x p d p x d x d x p x d · x d d c n de m n p e de p m x e d x n x d hu p m x e d p e c d d r p y d p e y d r d u d d p x d p x d p x x d · p er d r p es u r x r y s p case es of x y er d r xy s m µx x n d p e d p t e p x e d p let x m e in e d x m n p e d n dx x p e dx p let x m e in e d x p e d a x p let x m e in e d x p x d p x a p x d p x nd n p te d dd d p d d d e d p te d figure terms lemma value demand splitting if p v d d then there exists a split v v such that p v d and p v d and moreover and why is lemma important consider the following let x v in case x of yz x h h h x h n h h e e h x n h x n h h h x h h h v v h x h x s s x s s s s x s x s s s · y s · y s e e s s x y e s x y e s figure auxiliary simulation relation heaps and stacks the demand on x from the body of the will be c u c u c u and hence the value v will be checked against this demand using the rule an environment however after substituting v in the body which is ultimately what callbyneed will do we will have checked it against c u and c u independently and in each call site lemma ensures that reduction never increases the demand on the free variables of the environment and hence safety is not it is precisely the proof of lemma that requires demand transformers to be monotone in the demand arguments by theorem safety of analysis if e hu e and e k h e s then there exist h e and s such that e k h e s h h s s and e e the proof is just a combination of lemma and theorem optimisations we discuss next the two optimisations enabled by our analysis allocation for thunks we show here that for annotated bindings there is no need to allocate an entry in the heap and for annotated ones we dont have to emit an update frame on the stack within the chosen operational model this optimisation is of dynamic so we express this by providing a new smallstep machine for the annotated expressions the new semantics is defined in figure we will show that programs that can be evaluated via the counting semantics figure can be also evaluated via the semantics in a smaller or equal number of steps the proof is a simulation proof hence we define relations between heaps heaps and stacks stacks that are preserved during evaluation h e s h e s h let x e in e s h let x n e in e s h x x s h e s hx n e s where n h e x s h x x s h e s h x x s h x v s h v x s h x v s h m x e · y s h s he y s h e · y s h case es of x y er s h es x y er s h x x y y er s h er xy xy s figure counting semantics definition auxiliary relations we write e e iff e and e differ only on the annotations h h and s s are defined in figure for this optimisation the annotations on abstractions play no role hence we relate any expressions that differ only on those figure tells us when a heap h is related with an heap with the relation h as we have described there are no bindings in the heap moreover notice that there are no bindings of the form x in either the or heap it is easy to see why every heap binding starts life as x m by the time has become a value we have already used x once hence if originally m then the value binding will also be in the or semantics if it was m then it can only be in the heap and in the heap if it was m then no such bindings would have in the heap the relation between stacks is given with s rule ensures that there are no frames x in the stack in fact during evaluation it is easy to observe that there are not going to be any update frames x in the original or stack we can now state the optimisation simulation theorem theorem semantics if and h e s h e s then there exists k st h e s k h e s and h e s h e s notice that the counting semantics may not be able to take a transition at some point due to the wrong nondeterministic choice but in that case the statement of theorem holds trivially finally we tie together theorems and to get the following result theorem analysis is safe for semantics if e hu e and e n h e s then e m h e s s t e e m n and there exist h and s st h h and s s and h h and s s theorem says that if a program e evaluates in n steps to e in the reference semantics then it also evaluates to the same e modulo annotation in the semantics in n steps or fewer and the heaps and stacks are consistent moreover the theorem has informative content on infinite sequences for example it says that for any point in the evaluation in the reference semantics we will have earlier reached a corresponding intermediate configuration in the semantics with consistent heaps and stacks floating into as discussed in section we are interested in the particular case of peyton jones et al moving the binder into the body of a this transformation is trivially safe given obvious syntactic side conditions and sands § however in general it is not here we describe the conditions under which floating makes things better in terms of the length of the program execution sequence we start by defining floating in a form of syntactic rewriting definition floating for let z m e in let f m x e in e let f m x let z m e in e in e for any m m and z fv e next we provide a number of definitions necessary to formulate the so called improvement result and sands the improvement is formulated for closed wellformed configurations for a configuration h e s to be closed any free variables in h e and s must be contained in a union domh doms where domh is a set of variables bound by a heap h and doms is a set of variables marked for update in a stack s a configuration is wellformed if domh and doms are disjoint definition convergence for a closed configuration h e s n def h v n h e s n h v h e s n def m h e s m and m n the following theorem shows that local floating into the body of a lambda does not make the execution longer theorem float improvement for any h and s if h let z m e in let f m x e in e s n and z fv e then h let f m x let z m e in e in e s n even though theorem gives a result its proof et al goes via a simulation argument hence it is possible to state the theorem in a more general way without requiring termination we also expect that the improvement result extends to arbitrary program contexts but have not carried out the exercise implementation we have implemented the cardinality by extending the demand analysis machinery of the glasgow haskell compiler available from its we briefly some implementation in this section analysis the implementation of the analysis was straightforward because existing strictness is already cast as a backwards analysis exactly like our new cardinality analysis so the existing unchanged all that was required was to the domains over which the works in total the increased from lines of code to lines an extremely change we run the analysis twice once in the middle of the optimisation pipeline and once the end the purpose of the first run is to expose which in turn enable a of subsequent transformations section the second analysis finds the thunks which are exploited by the code generator this second analysis is performed very late in the pipeline a so that it the result of all previous inlining and optimisation and b because the thunk information is not robust to certain other transformations section absence ghc exploits absence in the split as described in section absent arguments are not passed from the wrapper to the as shown in section there is no runtime for rather the information enables some important compiletime transformations specifically consider let x costly v in y x if the y is a lambda the binding for x can be inside the lambda without of the computation of costly once the binding for x is inside the y several other improvements may happen · it may be at x s use site perhaps entirely eliminating the allocation of a thunk for x · it may enable a rewrite rule eg fusion to · it may allow two to be replaced by one for example f v let x costly v in y x f v y costly v the latter produces one function with two arguments rather than a function that returns a lambda and peyton jones thunks the code that ghc for a thunk begins by an update frame on the stack which includes a pointer to the thunk then the code for the thunk is executed when evaluation is complete the value is returned and the update frame the thunk with an indirection to the values peyton jones it is easy to modify this mechanism to take advantage of thunks we do not generate the code for this claim is true in spirit but in practice we substantially the existing when adding usage program constraints gen integer life and more programs arithmetic mean rt table analysis results for of syntactic syntactic thunks and runtime entries into thunks rt thunks there is a code size fewer instructions generated and a runtime a few store instructions saved on thunk entry and a few more when evaluation is complete take care though the property is not robust to program transformation for example common subexpression elimination cse can combine two thunks into one one as can this sequence of transformations let y e in let x y in x x identity of let y e in let x y in x x inline x let y e in y y wrong this does not affect the formal results of the paper but it is the reason that our second run of the cardinality analysis is immediately before code generation evaluation to measure the accuracy of the analysis we counted the of a and b thunks in both cases these are of the syntactically occurring or thunks respectively measured over the code of the benchmark program only not library code table shows the results reported by our analysis for programs from the benchmark suite the numbers are quite account for of all while thunks are of all thunks the static syntactic frequency of thunks may be very different to their dynamic frequency in a program execution so we instrumented ghc to measure the latter we did not measure the dynamic frequency of because they no direct performance benefit the rt thunk column of table gives the dynamic frequency of thunks in the same programs note that these statistics include thunks from libraries as well as the benchmark program code the results vary widely most programs do not appear to use thunks much while a few use many up to for programs in the end of course we improved although the benefits are likely to be do not program no constraints gen integer life and more programs min max geometric mean runtime no table cardinality optimisations for any performance benefits directly rather they remove potential from other compiletime transformations thunks on the other hand give an immediate performance benefit by the code but it is a small one table the effect of cardinality analysis when running the suite is the change in how much heap was allocated when the program is run and runtime is a change in the actual program execution time in section we mentioned a used by in ghc in which he the information for three particular functions build and our analysis this redundant as now the same results can be soundly inferred we therefore report two sets of results relative to an and relative to a in both cases binary size of the statically linked slightly but consistently average which is this may be due to less code being generated considering allocation the numbers relative to the are quite but relative to the compiler the improvements are the was very effective otherwise only one program shows a significant reduction in allocation which turned out to be because a thunk was inside a lambda and up never being allocated exactly as a of suite is that tend to be short and very even with the execution key slow only programs from the suite run for longer than half second with a maximum of seconds for constraints among those the performance improvement is for integer with an average of one can notice that the new compiler sometimes performs worse than the versions in a very few benchmarks in in a highly compiler with many passes it is very hard to ensure that every optimisation always makes the program run faster and even if a pass does improve the program per se to ensure that every subsequent pass will carry out all the optimisations that it did before the earlier improvement was implemented the data show that we do not always succeed we leave for the future some detailed work to find out exactly why program rt rt s s s s s rt table optimisation of the programs from benchmarks game library binary benchmark benchmarks get all benchmarks alloc table analysis and optimisation results for libraries program parser loc ghc alloc no ghc rt no table compilation with ghc for more realistic numbers we measured the improvement in runtime relative to the compiler for several programs from the computer language benchmarks game the results are shown in table all programs were run with the settings except to which we gave a input value of on a intel core i os x machine with gb ram these are haskell programs to within an of their life by haskell there is no easy to be had and indeed the changes are so usually zero and at the most in the case of that we omit them from the table however we do get one result a speedup of in due to fewer thunk updates as you can see nearly half of its thunks entered at runtime are realworld programs to test our analysis and the optimisations on some realworld programs we chose four libraries from the a fast parser combinator library binary a lazy binary library a implementation of and a parsing and encoding library these libraries come with benchmark which we ran both for the compiler and the one table contains the of syntactic and thunks for the libraries as well relative improvement in memory allocation for particular benchmarks since we were interested only in the absolute improvement against the state of the art we made our comparison with respect to the version of ghc the results for are explained by its relatively high ratio of which is typical for parser combinator libraries ghc itself is a very large haskell program written in a variety of so we compiled it with and without optimisations and measured the allocation and runtime improvement when using the two variants to compile several programs the results are shown in table as in the other cases we get but consistent improvements related work abstract interpretation for usage and absence the goal of the traditional analyses is to figure out which parts of the programs are used and which are not peyton jones and this question was first studied in the late s when an elegant representation of the usage analysis in terms of projections was given by wadler and hughes wadler and hughes their formulation allows one to define a backwards analysis inferring the usage of arguments of a function from the usage of its result an idea that we adopted our work has important differences notably a call demands c nd which appear to be entirely new and b the ability to treat nested which requires us to capture the usage of free variables in a usage signature moreover our formal is quite different to their denotational approach because we must model sharing typebased approaches the notion of thunks and is of linear types girard turner and wadler a similarity that was very early launchbury et al linear types per se are far too restrictive see for example and peyton jones § for details but the idea of using a type system to express usage information inspired a series of once upon a type papers turner et al and peyton jones a promising idea turned out to lead step by step into a deep subtyping proved to be essential so that a function that used its argument once could have a type like int int but still be applied to an argument x that was used many times and had type int and peyton jones then usage polymorphism proved essential to with using the monomorphic system in the of the standard libraries just two thunks were annotated as bounded polymorphism to gain greater precision and while extended usage polymorphism to data types sometimes resulting in data types with many of usage parameters the interaction of ordinary type polymorphism with all these features was far from straightforward the inference algorithm for a polymorphic type system with bounds and subtyping is extremely complex and so on with these and implementation prototype in ghc around loc of code plus changes to of lines of code elsewhere turned out to be and never made it into the main our system these difficulties entirely by treating the problem as a backwards analysis like strictness analysis rather than as a type system this is what gives the simplicity to our approach but also prevents it from giving rich demand signatures to third and the title as so often is due to wadler higherorder functions our usage types can account uniformly only for the first and secondorder functions thanks to call demands for example what type might we attribute to the usage of x depends on the particular g in the call so usage polymorphism would be called for this is indeed more expressive but it is also more complicated we limit precision for very higherorder programs to gain simplicity at some level abstract interpretation and type inference can be seen as different sides of the same but there are some interesting differences for example our and rules are explicit about information flow in the former information flows from the definition of a function to its uses while in the latter the flow is type systems use unification variables to allow much richer information flow but at the cost of generating constraints involving subtyping and bounds that are to solve another difference is in the handling of free variables let f x y x in if b then f else y how many times is the free variable y evaluated in this expression obviously just once and this because we the demand on y at fs call site and lub the two branches of the if but type systems behave like compute the demand on f namely called once and from that compute the demand on y then combine the demand on y from the body of the let used at most once and from fs right hand side used at most once yielding the result that y is used many times we have lost the fact that the two uses come from different branches of the conditional the fact that our usage signatures include the component makes them more expressive than types unless we extend the type system yet further with an polymorphic effect system et al and moreover the analysis approach deals very naturally with absence and with product types such as pairs which are typebased approaches do not do so well here in short an approach has proved much simpler than the typebased one and far easier to implement one might if a type system might give better results in practice but results mostly zero change to allocation one program allocated more one less were no more than those we report our proof technique does however share much in common with and work all three being based on an operational semantics with an explicit heap one other typebased usage system is uniqueness types and notion of uniqueness is however different to ours in clean a argument places a restriction on the caller to pass the only copy of the value whereas for us a argument is a by callee to evaluate the argument at most once other related work call demands introduced in this paper appear to be related to the notion of employed in the recent work on typing and in particular means that an expression either guaranteed to be applied to an argument s or may not be applied to an argument l in this terminology s corresponds to a strong version of our demands c d which requires d u and l is similar to our u the evaluation of expressions corresponds to our demand hu however neither call nor are captured by the concept of abstract counting or sharing analysis conservatively determines which parts of the program might be used by several components or accessed several times in the course of execution early work employed a forward abstract interpretation framework hudak since the forward abstract interpreter makes assumptions about arguments of a function it the abstract interpretation can account for multiple combinations of those and may therefore be extremely expensive to compute recent development on the systematic construction of abstract static analyses for higherorder programs known as abstracted abstract machines makes it straightforward to derive an from an existing smallstep operational semantics rather than come up with an adhoc nonstandard one van horn and might this approach also greatly simplifies integration of the counting abstract domain to account for sharing might and shivers however the abstract interpreters obtained this way are forward which makes them it would be however an interesting future work to build a backwards analysis from conclusion cardinality analysis is simple to implement it added lines of code to a line compiler and it gives real improvements for serious programs not just for benchmarks for example ghc itself a very large haskell program runs faster in the context of a compiler a gain of this magnitude is a acknowledgements we are to for the to use libraries and the utility for the experiments in section we also thank the popl reviewers for their useful feedback references and uniqueness typing for functional languages with graph rewriting semantics mathematical structures in computer science ­ deforestation for nonstrict functional languages phd thesis university of glasgow department of computer john launchbury and simon peyton jones a short cut to deforestation in proceedings of the acm conference on functional programming languages and computer architecture pages ­ girard linear logic its syntax and semantics in proceedings of the workshop on advances in linear logic pages ­ cambridge university press benjamin detecting sharing of partial applications in functional programs in functional programming languages and computer architecture volume of lncs pages ­ springerverlag a type based sharing analysis for update and optimisation in proceedings of the third acm sigplan international conference on functional programming icfp pages ­ acm and a usage analysis with bounded usage polymorphism and subtyping in implementation of functional languages selected papers volume of lncs pages ­ springer and a generic usage analysis with qualifiers in proceedings of the th acm sigplan international conference on functional programming icfp pages ­ acm strictness analysis theoretical and practical aspects phd thesis university and making more relevant in proceedings of the acm sigplan workshop on partial evaluation and program manipulation pages ­ acm paul hudak a semantic model of reference counting and its abstraction in proceedings of the acm conference on lisp and functional programming pages ­ acm richard jones tail recursion without space leaks j program ­ john launchbury john hughes simon simon peyton jones and philip wadler avoiding unnecessary updates in proceedings of the glasgow workshop on functional programming in computing pages ­ springer simon and simon peyton jones making a fast curry vs for higherorder languages j program ­ matthew might and shivers improving flow analyses via cfa abstract garbage collection and counting in proceedings of the acm sigplan international conference on functional programming icfp pages ­ acm andrew and david sands improvement in a lazy context an operational theory for callbyneed in popl proceedings of the th annual acm sigplansigact symposium on principles of programming languages pages ­ acm will the benchmark suite of haskell programs in proceedings of the glasgow workshop on functional programming in computing pages ­ springer simon peyton jones implementing lazy functional languages on hardware the j program ­ simon peyton jones and will the effectiveness of a simple strictness in proceedings of the glasgow workshop on functional programming pages ­ springer simon peyton jones and a for haskell science of computer programming ­ simon peyton jones will and moving bindings to give faster programs in proceedings of the first acm sigplan international conference on functional programming icfp pages ­ acm and matthias felleisen reasoning about programs in continuationpassing style in proceedings of the acm conference on lisp and functional programming pages ­ acm and simon peyton jones modular higherorder cardinality analysis in theory and practice extended version technical report microsoft research available at id peter deriving a lazy abstract machine j program ­ david n turner and philip wadler operational interpretations of linear logic theor comput sci ­ david n turner philip wadler and once upon a type in proceedings of the acm conference on functional programming languages and computer architecture pages ­ acm david van horn and matthew might abstracting abstract machines in proceedings of the th acm sigplan international conference on functional programming icfp pages ­ acm philip wadler and john hughes projections for strictness analysis in functional programming languages and computer architecture volume of lncs pages ­ springerverlag simple polymorphic usage analysis phd thesis computer laboratory university of cambridge and simon peyton jones once upon a polymorphic type in popl proceedings of the th annual acm sigplansigact symposium on principles of programming languages pages ­ acm 