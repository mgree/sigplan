laws of order expensive synchronization in concurrent algorithms cannot be eliminated tu m michael ibm t j watson research center university martin ibm t j watson research center abstract building correct and efficient concurrent algorithms is known to be a difficult problem of fundamental importance to achieve efficiency designers try to remove unnecessary and costly synchronization however not only is this manual process adhoc time and but it often leaves designers the question of is it inherently impossible to eliminate certain synchronization or is it that i was unable to eliminate it on this attempt and i should keep trying in this paper we to this question we prove that it is impossible to build concurrent implementations of classic and specifications such as sets queues stacks mutual exclusion and operations that completely eliminate the use of expensive synchronization we prove that one cannot avoid the use of either i raw where a write to shared variable a is followed by a read to a different shared variable b without a write to b in between or ii atomic awar where an atomic operation reads and then writes to shared locations unfortunately enforcing raw or awar is expensive on all current processors to enforce raw memory called or instructions must be used to enforce awar atomic instructions such as are required however these instructions are typically substantially slower than regular instructions although algorithm designers frequently to avoid raw and awar their attempts are often our result characterizes the cases where avoiding raw and awar is impossible on the flip side our result can be used to guide designers towards new algorithms where raw and awar can be eliminated categories and subject descriptors d concurrent programming e data data structures general terms algorithms theory keywords concurrency algorithms lower bounds memory memory barriers permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm introduction the design of concurrent applications that avoid costly synchronization patterns is a programming challenge requiring consideration of algorithmic concerns and issues with implications to formal testing and verification two common synchronization patterns that frequently arise in the design of concurrent algorithms are read after write raw and atomic write after read awar the raw pattern consists of a process writing to some shared variable a followed by the same process reading a different shared variable b without that process writing to b in between the awar pattern consists of a process reading some shared variable followed by the process writing to a shared variable the write could be to the same shared variable as the read where the entire readwrite sequence is atomic examples of the awar pattern include operations such as a cas unfortunately on all processor architectures the raw and awar patterns are associated with expensive instructions modern processor architectures use relaxed memory models where raw order among accesses to independent memory locations requires the execution of memory ordering called memory or memory that enforce raw order the atomicity of awar requires the use of atomic instructions typically and atomic instructions are substantially slower than regular instructions even under the most caching conditions due to these high overheads designers of concurrent algorithms aim to avoid both raw and awar patterns however such attempts are often in many cases even after multiple attempts it turns out impossible to avoid these patterns while ensuring correctness of the algorithm this raises an interesting and important practical question can we discover and formalize the conditions under which avoiding raw and awar while ensuring correctness is in this paper we answer this question formally we show that implementations of a wide class of concurrent algorithms must involve raw or awar in particular we focus on two widely used raw order requires the use of explicit or atomic instructions even on strongly ordered architectures eg x and tso that automatically guarantee other types of ordering read after read write after read and write after write specifications linearizable objects and mutual exclusion our results are applicable to any algorithm to satisfy these specifications main contributions the main contributions of this paper are the following · we prove that it is impossible to build a linearizable implementation of a strongly method that satisfies a deterministic sequential specification in a way that sequential executions of the method are free of raw and awar section · we prove that common methods on and fundamental abstract data as sets queues queues stacks and strongly and are subject to our results section · we prove that it is impossible to build an algorithm that satisfies mutual exclusion is and avoids both raw and awar section practical implications our results have several implications · designers of concurrent algorithms can use our results to determine when looking for a design without raw and awar is conversely our results indicate when of these patterns may be possible · for processor our result indicates the importance of optimizing the performance of atomic operations such as and raw instructions which have received little attention for optimization · for synthesis and verification of concurrent algorithms our result is potentially useful in the sense that a synthesizer or a verifier need not generate or attempt to verify algorithms that do not use raw and awar for they are certainly incorrect the remainder of the paper is organized as follows we present an overview of our results with examples in section in section we present the necessary formal machinery we present our result for mutual exclusion in section and for linearizable objects in section in section we show that many widely used specifications satisfy the conditions outlined in section and hence are subject to our result we discuss related work in section and conclude the paper with section overview in this section we explain our results informally give an intuition of the formal proof presented in later sections and show concurrent algorithms that our result as mentioned already our result focuses on two practical specifications for concurrent algorithms mutual exclusion and linearizability informally our result states that if we are to build a mutual exclusion algorithm or a linearizable algorithm then in certain sequential executions of that algorithm we must use either raw or awar that is if all executions of the algorithm do not use raw or awar then the algorithm is incorrect mutual exclusion consider the classic mutual exclusion template shown in fig here we have n processes n with each process a lock entering the critical section and finally the lock the specification for mutual exclusion states that we cannot have multiple processes in their critical section at the same time the template does not show the actual code that each process must execute in its lock critical and unlock sections further the code executed by different processes need not be identical process process lock lock cs cs unlock unlock process n figure mutual exclusion template for n while ¬ figure awar a simplified of a lock acquire true while figure raw simplified from the lock section of way mutual exclusion algorithm here i our result states that whenever a process has sequentially executed its lock section then this execution must use raw or awar otherwise the algorithm does not satisfy the mutual exclusion specification and is incorrect informal proof explanation let us now give an intuition for the proof on a simplified case where the system is in its initial state ie all processes are just about to enter their respective lock sections but have not yet done so let us pick an arbitrary process i i n and let process i sequentially execute its section enter the critical section and then stop let us assume that process i did not perform a shared write when it executed its section now let us select another process j i j n as process i did not write to the shared state there is no way for process j to know where process i is therefore process j can fully execute its own section and enter the critical section now both processes are inside the critical section mutual exclusion therefore we have shown that each process must perform a shared write in its lock section let us now repeat the same exercise and assume that all processes are in the initial state where they are all just about to enter their respective lock sections but have not yet done so we know that each process must write to shared memory in the sequential execution of its lock section let us again pick process i to execute its section sequentially assume that process i writes to shared location named x now let us assume that the section is executed by process i sequentially without using raw and awar since there is no awar it means that the write to x cannot be executed atomically with a previous shared read be it a read from x or another shared location there could still be a shared read in that precedes the write to x but that read cannot execute atomically with the write to x let us now have process i execute until it is about to perform its first shared write operation and then stop now let process j perform a full sequential execution of its section this is possible as process i has not yet written to shared memory so process j is not process j now enters its critical section and stops process i now its section and immediately performs the shared write to x once process i writes to x it any changes to x that process j made this means that if process i is to know where process j is it must read a shared memory location other than x however we assumed that there is no raw which means that process i cannot read a shared location other than x without previously having written to that location in turn this implies that process i cannot observe where process j is that is process j cannot influence the execution of process i hence process i continues and completes its section and enters its critical section leading to a violation of mutual exclusion therefore any sequential execution of a lock section requires the use of either awar or raw examples here we show several examples of mutual exclusion algorithms that indeed use either raw or awar in their lock sections these examples are specific implementations that the applicability of our result namely that implementation of algorithms that satisfy the mutual exclusion specification cannot avoid both raw and awar one of the most common lock implementations is based on the atomic sequence its lock acquire operation down to an awar pattern by using an atomic operation eg cas to atomically read a lock variable check that it represents a free lock and if so replace it with an of a lock fig shows a simplified version of a similar pattern is used in all other locks that require the use of atomic operations in every lock acquire on the other hand a mutual exclusion lock algorithm that avoids awar must use raw for example fig shows a simplified from the lock section of algorithm for process mutual exclusion a process that succeeds in entering its critical section must first raise its own flag and then read the other flag to check that the other flag is not raised thus the lock section involves a raw pattern linearizability the second part of our result discusses linearizable algorithms intuitively an algorithm is linearizable with respect to a sequential specification if each execution of the algorithm is equivalent to some sequential execution of the specification where the order between the methods is preserved the equivalence is defined by comparing the arguments and results of method invocations unlike mutual exclusion where all sequential executions of a certain method ie the lock section must use either raw or awar in the case of linearizability only some sequential executions of specific methods must use either raw or awar we quantify these methods and their executions in terms of properties on sequential specifications any algorithm implementation that claims to satisfy these properties on the sequential specifications is subject to our results the two properties are · deterministic sequential specifications informally we say that a sequential specification is deterministic if a method executes from the same state will always produce the same result many classic abstract data types have deterministic specifications sets queues etc · strongly methods informally a method m is said to be strongly if there exists some state in the specification from which m executed sequentially by process p can influence the result of a method m executed sequentially by process q q p and vice versa m can influence the result of m from the same state note that m and m are performed by different processes s a ret k a s a s a ret k a s a k s a ret k a s a k figure sequential specification of a set s n denotes the contents of the set ret denotes the return value figure illustration of the reasoning for why raw is required in linearizable algorithms our result states that if we have an implementation of a strongly method m then there are some sequential executions of m that must use raw or awar that is if all sequential executions of m do not use raw or awar then the algorithm implementation is not linearizable with respect to the given sequential specification let us illustrate these concepts with an example a sequential specification of a classic set shown in fig where each method can be executed by more than one process first this simple sequential specification is deterministic if an add remove or contains execute from a given state they will always return the same result second both methods add and remove are strongly for add there exists an execution of the specification by a process such that add can influence the result of add which is executed by another process for example let us begin with s then if process p performs an add it will return true and a subsequent add will return false however if we change the order and the second add executes first then it will return true while the first add will return false that is add is a strongly method as there exists another method where both method invocations influence each others result starting from some state in this case it happens to be another add method but in general the two methods could be different similar reasoning shows why remove is strongly however contains is not a strongly method as even though its result can be by a preceding add or remove its execution cannot influence the result of any of the three methods add remove or contains regardless of the state from which contains starts executing for the set specification our result states that any linearizable implementation of the strongly methods add and remove must use raw or awar in some sequential execution of the implementation for example let us consider a sequential execution of starting from a state where k s then this sequential execution must use raw or awar however our result does not apply to the sequential execution of where k s in that case regardless of whether is performed the result of any other subsequent method performed right after is informal proof explanation the proof steps in the case of linearizable implementations are very similar to the ones already outlined in the case of mutual exclusion implementations intuitively if a method is strongly then any of its sequential executions must perform a shared write otherwise there is no way for the method to influence the result of any other method that is executed after it and hence the method cannot be strongly let us illustrate how we reason about why raw or awar should be present on our set example by contradiction let us assume that raw and awar are not present consider the concurrent execution in fig here some prefix of the execution marked as h has completed and at the end of h k s then process p invokes method and executes it up to the first shared write to a location called x and then p is then another process q performs a full sequential execution of for the same k which returns true after that p its execution and immediately performs the shared write and completes its execution of and also returns true the reason why both returned true is similar to the case for mutual exclusion the write to x by p any writes to x that q has made and as we assumed that raw is not allowed it follows that process p cannot read any locations other than x in its subsequent steps without having previously written to them hence both return the same value true now if the algorithm is linearizable there could only be two valid as shown in fig however it is easy to see that both are incorrect as they do not to the specification if k s at the end of h then according to the set specification executing two sequentially in a row cannot lead to both returning the same result therefore either raw or awar must be present in some sequential executions of more generally as we will see in section we show this for any deterministic specification not only for sets we will see that the central reason why both are not allowed is because the result of executed by process q is not by the preceding executed by process p the assumption that add is a strongly method practical implications while our result shows when it is impossible to eliminate both raw and awar the result can also be used to guide the search for linearizable algorithms where it may be possible to eliminate raw and awar by changing one or more of the following dimensions · deterministic specification change the sequential specification perhaps by considering nondeterministic specifications · strong focus on methods that are not strongly instead of add · restrict the specification such that a method can only be performed by a single process instead of multiple processes as we will see later technically this is also part of the strong definition bool ev val nv if ev nv return b l bx p if by goto figure adapted from et als cas algorithm · execution design efficient that can identify executions which are known to be commutative the first three of these to the specification and we illustrate two of them deterministic specification and in the examples that follow the last one is focused on the implementation as mentioned already for linearizability our result holds for some sequential executions however when implementing an algorithm it may be difficult to the sequential executions of a given method for which the result holds and those for which it does not however if a designer is able to come up with an efficient mechanism to identify these cases it may be possible to avoid raw and awar in the executions where it may not be required for instance if the method can check that k s before is performed then for those sequential executions of it may not need to use neither raw nor awar even though our result only about some sequential executions in practice it is often difficult to design efficient tests that sequential executions and hence it often ends up the case that raw or awar is used on all sequential executions of a strongly linearizable method examples linearizable algorithms next we illustrate the applicability of our result in practice via several wellknown linearizable algorithms compare and swap we begin with the universal cas construct whose sequential specification is deterministic and the method is strongly for a formal proof see section the sequential specification of o n says that it first compares m to o and if m o then n is assigned to m and cas returns true otherwise m is unchanged and cas returns false here we use the operator to denote address dereference the cas specification can be implemented trivially with a linearizable algorithm that uses an atomic hardware instruction also called cas and in that case the implementation inherently includes the awar pattern alternatively the cas specification can be implemented by a linearizable algorithm using reads writes and hardware cas with the goal of avoiding the use of the hardware cas in the common case of no contention such a linearizable algorithm is presented by et al fig shows an adapted code of the common path of that algorithm while the algorithm succeeds in avoiding the awar pattern in the common case the algorithm does indeed include the raw pattern in its common path to ensure correctness the write to bx in line must the read of by in line both examples our result awar or raw was necessary knowing that raw or awar cannot be avoided in implementing cas correctly is important as cas is a fundamental building block for many classic concurrent algorithms take b bottom a b b bottom b t top figure adapted from the take method of work algorithm take h head t tail if h t return empty task head h return task figure the take method from michael et als idempotent work fifo queue work structures concurrent work algorithms are popular algorithms for implementing load frameworks a work structure holds a collection of work items and it has a single process as its owner it supports three main methods put take and only the owner can insert and extract work items via methods put and take other processes may extract work items using in designing algorithms for work the priority is to optimize the owners methods especially the common paths of such methods as they are expected to be the most frequently executed parts of the methods examining known work algorithms that avoid the awar pattern ie avoid the use of complex atomic operations in the common path of the owners methods reveals that they all contain the raw pattern in the common path of the take method that succeeds in extracting work items the work algorithm by and is representative of such algorithms fig shows a code adapted from the common path of the take method of that algorithm with minor changes for the sake of consistency in presentation the variables bottom and top are shared variables and bottom is written only by the owner but may be read by other processes the key pattern in this code is the raw pattern in lines and the order of the write to bottom in line followed by the read of top in line is necessary for the correctness of the algorithm the order of these two instructions results in an incorrect algorithm in subsequent sections we will see why correct implementations of the take and methods must use either raw or awar from deterministic to nondeterministic specifications our result that in the standard case where we have the expected deterministic sequential specification of a structure it is impossible to avoid both raw and awar however as mentioned earlier our result can guide us towards finding practical cases where we can indeed eliminate raw and awar indeed the deterministic specification may allow us to come up with algorithms that avoid both raw and awar such a relaxation is by the idempotent work introduced by michael et al this concept the semantics of work to require that each inserted item is eventually extracted at least once data h head t tail next if head h goto if next null return empty if h t goto d if goto return d figure simplified of on lockfree fifo queue data if tail head return empty data data mod m head head mod m return data figure method adapted from fifo queue which does not use raw and awar instead of exactly once under this notion the authors to design algorithms for idempotent work that avoid both the awar and raw patterns in the owners methods our result explains the underlying reason of why the elimination of raw and awar was possible because the sequential specification of idempotent structures is necessarily nondeterministic our result now indicates that it may be possible to avoid raw and awar indeed this is by the algorithms in fig shows the take method of one of the idempotent algorithms note the absence of both awar and raw in this code the shared variables head tail and are read before writing to head and no reads need to be atomic with the subsequent write fifo queue example in examining concurrent algorithms for fifo queues one notes that either locking or cas is used in the common path of nontrivial methods that return a item however as we mentioned already our result proves that mutual exclusion locking requires each sequential execution of a successful lock acquire to include awar or raw all algorithms that avoid the use of locking in include a cas operation in the common path of each nontrivial execution fig shows the simplified code from the method of the classic michael and lockfree fifo queue note that every execution that returns an item must execute cas we observe that algorithms for include directly or indirectly at least one instance of the awar or raw patterns ie use either locking or cas from to our results suggest that if we want to eliminate raw and awar we can focus on restricting the processes that can execute a method for instance we can specify that can be executed only be a single process indeed when we consider fifo queues where no more than one process can execute the method we can obtain a correct implementation of which does not require raw and awar x y v ar m mid l lab b e n exp c com l x e l x ge l ge e l if b goto l l l l entry m x l exit m x c c p c c figure language syntax fig shows a method adapted from fifo queue note that the code avoids both raw and awar the variable head is private to the single consumer and its update is done by a regular write once again this example demonstrates a case where we used our result to guide the implementation in particular by changing the specification of a method of the abstract data namely from to enabled us to create an implementation of the method ie where we did not need raw and awar in this section we present the formal machinery necessary to specify and prove our results later language the language shown in fig is a basic assembly language with labeled statements assignments sequencing and conditional we do not elaborate on the construction of numerical and boolean expressions which are standard the language is also equipped with the following features · statements for beginning and ending of atomic sections using these one can implement various universal constructs such as · parallel composition of sequential commands · we use g to model global memory via a one dimensional array · two statements are used to denote the start ie entry statement and end of a method ie exit statement we use v ar to denote the set of local variables for each process mid to denote a finite set of method identifiers lab the set of program labels and pid a finite set of process identifiers we assume the set of values obtained from expression evaluation includes at least the integers and the booleans semantics a program state is a tuple pc g · p c × × × · p c pid lab · pid × v ar val · val val · pid the restriction or the lack of restriction on the number of concurrent does not affect the algorithm for the method a state tracks the program counter for each process pc a mapping from process local variables to values the contents of global memory g and whether a process executes atomically if no process executes atomically then is set to we denote the set of initial states as init initially is set to for all states in init transition function we assume standard smallstep operational semantics given in terms of transitions between states the behavior of a program is determined by a partial transition function t f × pid given a state and a process p t f p returns the unique state if it exists that the program will into once p executes its enabled statement when convenient we sometimes use the function t f as a relation for a transition t t f we denote its source state by its executing process by its destination state by its executing statement by a program transition represents the intuitive fact that starting from a state process can execute the statement and end up in a state that is t f we say that a transition t performs a global read resp write if reads from g resp writes to and use to denote the global memory location that the transition accesses if the transition does not read or write a global location then returns that is only in the case where a transition t accesses a global memory location does return a non value otherwise always returns we enforce strong atomicity semantics for any state process p can make a transition from only if or p for a transition t if then similarly if we use enabled to denote the set of processes that can make a transition from if then enabled otherwise enabled pid the statement entry m x is used to denote the start of a method invoked with a sequence of variables which contain the arguments to the method denoted by x the statement exit m x is used to denote the end of a method m these statements do not affect the program state except the program counter the meaning of the other statements in the language is standard executions an execution is a possibly infinite sequence of transitions where i i t f and j we use first as a for src ie the first state in the execution and last to denote the last state in the execution ie last dst if a transition t is performed in an execution then t is true otherwise it is false for a program prog we use prog to denote the set of executions for that program starting from initial states eg states in init next we define what it means for an execution prog to be atomic definition atomic execution we say that an execution is executed atomically by process p when · all transitions are performed by p i i p · all transitions are atomic or i i p we use ij to denote the substring of occurring between positions i and j including the transitions at i and j definition maximal atomic cover given an execution and a transition k the maximal atomic cover of k in is the unique substring ij of where · ij is executed atomically by where i k j · · intuitively we can understand the maximal atomic cover as taking a transition and extending it in both directions until we reach a leftmost state and a rightmost state where no process is inside an atomic section in either of these two states next we define executions definition read after write execution we say that a process p performs a in execution if i j i j such that · i performs a global write by process p · j performs a global read by process p · the memory locations are different · k i k j if p then intuitively these are executions where in the execution the process writes to global memory location a and then sometimes later reads a global memory location b that is different from a and the process does not access b note that there could be transitions in performed by processes other than p note that in this definition there is no restriction on whether the global accesses are performed atomically or not the definition only concerns itself with the ordering of accesses and not their atomicity we introduce the predicate raw p which evaluates to true if p performs a in execution and to false otherwise next we define atomic executions these are executions where a process first reads from a global memory location and then sometimes later writes to a global memory location and these two accesses occur atomically that is these two accesses no other process can perform any transitions note that unlike executions here the global read and write need not access different memory locations definition atomic write after read execution we say that a process p performs an atomic in execution if i j i j such that · process p performs a global read in i · process p performs a global write in j · ij is executed atomically by process p we introduce the predicate awar p which evaluates to true if process p performs an atomic in execution and to false otherwise specification histories a history h is defined as a finite sequence of actions ie h where an action denotes the start and end of a method p entry m a p exit m r where p pid is a process identifier m mid is a method identifier a is a sequence of arguments to the method and r is the return value for an action we use proc to denote the process kind to denote the kind of the action entry or exit and m to denote the name of the method we use hi to denote the action at position i in the history where i h for a process p h p is used to denote the subsequence of h consisting only of the actions of process p for a method m h m is used to denote the subsequence of h consisting only of the actions of method m a method entry p entry m a is said to be pending in a history h if it has no matching exit that is i i h such that p entry m and j i j h p or exit or m a history h is said to be complete if it has no pending calls we use to denote the set of histories resulting after extending h with matching exits to a subset of entries that are pending in h and then removing the remaining pending entries a history h is sequential if h is empty h or h starts with an entry action ie entry and if h entries and exits alternate that is i i h and each exit is matched by an entry that occurs immediately before it in h ie i i h if exit then entry and a complete sequential history h is said to be a complete invocation of a method m iff h m and m in the case where h is a complete sequential invocation we use to denote the entry action in h and to denote the exit action in h a history h is wellformed if for each process p pid h p is sequential in this work we consider only wellformed histories histories and executions given an execution we use the function hs to denote the history of hs takes as input an execution and produces a sequence of actions by over each transition t in order and extracting and if is an entry statement of a method m then the transition t the action entry m a where a is the sequence of values obtained from evaluating the variables used in the sequence in state similarly for exit statements if the transition t does not perform an entry or an exit statement then it for a program prog we define its corresponding set of histories as hs prog we use to denote the sequential histories in a transition t is said to be a method transition if it is performed method entry and exit that is there exists a preceding transition that performs an entry statement with such that does not perform an exit statement and t in we say that is a matching entry transition for t note that may be the same as t a transition that is not a method transition is said to be a client transition definition wellformed execution we say that an execution is wellformed if · hs is wellformed · any client transition t and · for any transition t if is an exit statement then · for any transition tr if tr is a method transition that reads a local variable other than the variables specified in the statement of its matching entry transition tm then there exists a transition tw performed by process tm and tr such that tw writes to that local variable that is a wellformed execution is one where its history is wellformed only method transitions are allowed to access global memory or perform atomic statements when a method exit statement is performed the should be and methods must initialize local variables which are not used for argument passing before using them we say that is a complete sequential execution of a method m by process p if is a wellformed execution and hs is a complete invocation of m by process p note that may contain client transitions both by process p and other processes a program prog is wellformed if prog contains only wellformed executions in this paper we only consider wellformed programs synchronization in mutual exclusion in this section we consider implementations that provide mutually exclusive access to a critical section among a set of processes we show that every mutual exclusion implementation either the raw or the awar pattern in certain executions a mutual exclusion implementation exports the following methods mid lock unlock where n pid in this setting we the definition of wellformed executions by requiring that each process p pid only invokes methods and in an alternating fashion that is for any execution prog and for any process p pid hs p is such that lock and unlock operations alternate ie given an execution prog and h hs p we say that p is in its trying section if it has started but not yet completed a operation ie and entry we say that p is in its critical section if it has completed but has not yet started that is and exit we say that p is in its exit section if it has started but has not yet finished it that is and entry otherwise we say that p is in the remainder section initially all processes are in their remainder sections a process is called active if it is in its trying or exit section for the purpose of our lower bound we assume the following weak formulation of the mutual exclusion problem in addition to the classical mutual exclusion requirement we only require that the implementation is ie if a number of active processes concurrently for the critical section at least one of them succeeds definition mutual exclusion a mutual exclusion implementation prog guarantees · safety for all executions prog it is always the case that at most one process is in its critical section at a time that is for all p q pid if p is in its critical section in hs p and q is in its critical section in hs q then p q · liveness in every execution in which every active process takes sufficiently many steps i if at least one process is in its trying section and no process is in its critical section then at some point later some process enters its critical section and ii if at least one process is in its exit section then at some point later some process enters its remainder section theorem raw or awar in mutual exclusion let prog be a mutual exclusion implementation for two or more processes pid then for every complete sequential execution of by process p · raw p true or · awar p true proof let base · prog such that is a complete sequential execution of by process p it follows that no process q pid q p is in its critical section in q otherwise mutual exclusion would be violated it also follows that p is not active in p by contradiction assume that does not contain a global write consider an execution such that process p does not perform transitions in and every active process takes sufficiently many steps in until some process q p completes its section ie q is in its critical section in · q the execution base · prog since prog is since p does not write to a shared location in further the local state of all processes other than p in is the same as their local state in · ie q pid var v ar if q p then var var also we know as transitions by process p do not access local variables of other processes hence we can build the execution nc base · · where is the execution with the same sequence of statements as ie process p does not perform transitions in hence q · q that is q is in its critical section in q but p is also in its critical section in p a contradiction thus contains a global write and let tw be the first global write transition in let f · w · l where w is the maximal atomic cover of tw in we proceed by contradiction and assume that raw p false and awar p false since awar p false and tw is the first write transition in it follows that tw is the first global transition in w since f contains no global writes applying the same arguments as before there exists an execution base · f · prog such that some process q q p is in its critical section in · f · q the assumption raw p false implies that no global read transition by process p in w · l accesses a variable other than without having previously written to it note that tw the only location that can be read by p in w · l thus applying the same arguments as before there exists an execution c base · f · · w · l in prog such that q is in its critical section in q and p is in its critical section in p a contradiction thus either raw p true or awar p true synchronization in linearizable algorithms in this section we state and prove that certain sequential executions of strongly methods of algorithms that are linearizable with respect to a deterministic sequential specification must use raw or awar linearizability following we define linearizable histories a history h induces an partial order h on actions in the history a h b if exit and entry and i j i j h such that hi a and b that is exit action a precedes entry action b in h a history h is said to be linearizable with respect to a sequential history s if there exists a history h such that p pid h p s p h s we can naturally extend this definition to a set of histories let spec be a sequential specification a set of sequential histories that is if s is a sequential history in spec then any prefix of s is also in spec then given a set of histories we say that is linearizable with respect to spec if for any history h there exists a history s spec such that h is linearizable with respect to s we say that a program prog is linearizable with respect to a sequential specification spec when is linearizable with respect to spec deterministic sequential specifications in this paper similarly to we define deterministic sequential specifications given two sequential histories s and s let s denote the longest common prefix of the two histories s and s definition deterministic sequential specifications a sequential specification spec is deterministic if for all s s spec s s and s s we have s or entry that is a specification is deterministic if we cannot find two different histories whose longest common prefix ends with an entry if we can find such a prefix then that would mean that there was a point in the execution of the two histories s and s up to which they but after they both performed the same entry they produced different results or one had no continuation strong we define a strongly method as follows definition strongly method we say that a method m is strongly in a sequential specification spec if there exists a method m possibly the same as m and there exist histories base s s s s such that s and s are complete invocations of m with and exits exits s and s are complete invocations of m with and exits exits base is a complete sequential history in spec base · s · s spec base · s · s spec in other words the method m is strongly if there is another method m and a history base in spec such that we can distinguish whether m is applied right after base or right after m which is applied after base similarly we can distinguish whether m is applied right after base or right after m which is applied after base note that m may be the same method as m in this work we focus on programs where the specification spec can be determined by the sequential executions of the program assumption spec raw and awar for linearizability next we state and prove the main result of this section theorem raw or awar in linearizable algorithms let m be a strongly method in a deterministic sequential specification spec and let p be a linearizable implementation of spec then there exists a complete sequential execution a of m by process p such that · p true or · p true proof from the premise that m is a strongly method and assumption we know that there exist executions base · a · c prog and base · b · d prog such that and are complete sequential histories a and d are complete sequential executions of m b and c are complete sequential executions of m from the fact that executions in the program are wellformed we know that if prog and hs hs are complete sequential invocations such that and it follows that hs hs and that is if a process completes the same method invocation from two program states with identical global memory the method will always produce the same result and global memory fact fact follows directly from the fact that transitions are deterministic processes cannot access the local state of another process arguments to both methods are the same and the starting global states are the same from fact and being complete sequential histories we can show that that is from f f both are initial states we can inductively show that any complete sequential invocation preserves the fact that the global state in the last states of the two executions are the same from it follows that let p and q we first prove that a method transition performed by process p must perform a global write in a let us assume the execution a does not contain a method transition where process p performs a global write as a is wellformed we know that any client transitions performed in a do not access global memory it then follows that however from the premise we know that and hence we know that from item above we know that b and c are complete sequential executions of m with item then it follows from fact that which contradicts with item therefore there must exist a method transition in a by process p that performs a global write let us proceed by contradiction and assume that both p false and p false let a f · w · where tw is the first method transition in a that writes to global memory and w is the maximal atomic cover of tw in a as a is wellformed we know that all transitions in w are method transitions since p false and tw is the first global write transition in a it follows that there can be no global read transitions in w that occur before tw otherwise we would awar this means that tw is the first global read or write transition in w as f does not contain global writes it follows that from the premise we know that and hence as w is a maximal atomic cover we know that from the fact that client transitions cannot they cannot execute atomic statements or access global memory and that a process cannot access the local variables of another process it follows that process q can execute m concurrently with m from state that is there exists an execution base · f · b prog where b is a complete sequential execution of m by process q such that as it follows that then by fact it follows that as p and b is a complete sequential execution by process q it follows that p process p can now continue execution of method m and build the execution base · prog where f · b · w · by assumption we know that p and p are false and it follows that w · does not contain a method transition which reads a global memory location other than without previously having written to it in both w and w process p the only global memory location that it can read without previously having written to it then w · and w · will contain the same sequence of statements with all global transitions accessing and identical values thus m given that the implementation is linearizable the two possible of base · are · m we already established that m and and hence by substitution we get · · from the premise we know that · · spec as the specification is deterministic it follows that a contradiction with item · · m we already established that m and and hence by substitution we get · · from the premise we know that · · spec and as the specification is deterministic it follows that a contradiction with item therefore p true or p true a note on commutativity and the notion of strongly method is related to traditional notions of methods and methods let us again consider definition method vs strongly method if it is the case that method m is the same as method m then the definition to methods that is given base if we apply m twice in a row the second invocation will return a different result than the first consider again the set specification in fig the method add is as discussed in the example in section we can start with s and base then if we perform two methods add in a row each one of the adds will return a different result classic vs strong in the classic notion of it is enough for one of the methods to not commute with the other while here it is required that both methods do not commute from the same prefix history in the classic case if two methods do not commute it does not mean that either of them is a strongly method however if a method is strongly then it is always the case that there exists another method with which it does not commute by definition consider again the set specification in fig although add and contains do not commute contains is not a strongly method that is add the result of contains but contains does not influence the result of add strongly specifications in this section we provide a few examples of wellknown sequential specifications that contain strongly methods as defined in definition stacks definition stack sequential specification a stack object s supports two methods push and pop the state of a stack is a sequence of items s v vk the stack is initially empty the push and pop methods induce the following state transitions of the sequence s v vk with appropriate return values · changes s to be v vk and returns · pop if s is nonempty changes s to be v vk and re turns vk if s is empty returns empty and s remains unchanged we let denote the sequential specification of a stack object as defined above lemma pop is strongly the pop stack method is strongly proof let base be a complete sequential history after which s v for some v let p and q be two processes let s and s be complete invocations of pop by p and let s and s be complete invocations of pop by q from definition base · s · s base · s · s v and empty the claim now follows from definition it also follows from definition that push methods are not strongly work as we now prove the work object discussed in section is an example of an object for which two different methods are strongly definition work sequential specification a work object supports three methods put take and the state of each process i is a sequence of items qi vi i all queues are initially empty the put and take methods are performed by each process i on its local queue qi and induce on it the following state transitions with appropriate return values · changes qi to be vi i and returns · take if qi is nonempty it changes qi to be vi i and returns vi if qi is empty it returns empty and qi remains unchanged the method is performed by each process i on some queue qj vj for j i if qj is nonempty it changes qj to be vj j and returns if qj is empty it returns empty and qj remains unchanged we let denote the sequential specification of a work object as defined above lemma take are strongly the take and methods are strongly proof let base be a complete sequential history after which qj v for some value v and process j let i j be some process other than j let s and s be complete invocations of by process i on qj and let s and s be complete invocations of take by process i from definition v and empty the claim now follows from definition it is easily shown that specifications for queues and sets have strongly methods the proofs are essentially identical to the proofs of lemmas and and are therefore omitted cas we now prove that cas is strongly definition sequential specification a object c supports a single method called cas and stores a scalar value over some domain v the method for exp new v induces the following state transition of the object if cs value is exp cs value is changed to new and the method returns true otherwise cs value remains unchanged and the method returns false we let denote the sequential specification of a object as defined above lemma cas is strongly the cas method is strongly proof let base be a complete sequential history after which cs value is v let i and j be two processes let s and s be complete invocations of by process i for some v v v and let s and s be complete invocations of by process j from definition base · s · s base · s · s true and false the claim now follows from definition it follows from lemma that any software implementation of cas is required to use either awar or raw proving a similar result for all nontrivial specifications such as swap and is equally straightforward related work numerous papers present implementations of concurrent data structures several of these are in section we refer the reader to and for many other examples modern architectures often execute instructions by a single process and provide or barrier instructions to order the execution cf there is a of and barrier instructions see for example dec alpha provides two different instructions a memory barrier mb and a write memory barrier provides a lightweight and a sync memory ordering instructions where sync is a full while guarantees all other orders except raw v provides several of instructions through a instruction that can be via encoding to order a combination of previous read and write operations with respect to future read and write operations supports load store and memory instructions the instruction can be used for enforcing the raw order proved that linearizable implementations of many concurrent datastructures such as counters stacks and queues must use awar these results do not mention raw and do not apply to implementations of such objects or to implementations of mutual exclusion however whereas our results do recently there has been a interest in formalizing memory models cf and model checking and synthesizing programs that run on these models our result is complementary to this direction it states that we may need to enforce certain order ie raw regardless of what weak memory model is used further our result can be used in with program testing and verification if both raw and awar are missing from a program that claims to satisfy certain specifications then that program is certainly incorrect and there is no need test it or verify it phd thesis also in papers the ability of weak consistency models to solve mutual exclusion with only reads and writes this work shows that many weak models coherence consistency weak ordering consistency and java consistency cannot solve mutual exclusion processor consistency can solve mutual exclusion but it requires registers for two processes solving mutual exclusion requires at least three variables one of which is in contrast we show that particular orders of operations or certain atomicity constraints must be enforced regardless of the memory model moreover our results apply beyond mutual exclusion and hold for a large class of important linearizable objects boehm studies when memory operations can be with respect to locks and shows that it is not safe to move memory operations into a locked region by delaying them past a lock call on the other hand memory operations can be moved into such a region by them to be before an unlock call however paper does not address the central subject of our paper namely the that certain ordering patterns raw or awar must be present inside the lock operations our proof technique the covering technique originally used by and to prove a lower bound on the number of registers needed for solving mutual exclusion this technique had many applications both with read write operations and with nontrivial atomic operations such as some steps of our proofs can be seen as a formalization of the arguments lamport uses to derive a fast mutual exclusion algorithm in terms of our result for mutual exclusion while one might guess that some form of raw should be used in the entry code of readwrite mutual exclusion we are not aware of any prior work that states and proves this claim and show that you need to have n registers and as part of their proof show that a process needs to write but they do not show that after it writes the process must read from a different memory location lamport also only to it these works neither state nor prove the claim we are making and they also do not discuss awar conclusion and future work in this work we focused on two common synchronization idioms raw and atomic write after read awar unfortunately enforcing any of these two patterns is costly on all current processor architectures we showed that it is impossible to eliminate both raw and awar in the sequential execution of a lock section of any mutual exclusion algorithm we also proved that raw or awar must be present in some of the sequential executions of strongly methods that are linearizable with respect to a deterministic sequential specification further we proved that many classic specifications such as stacks sets hash tables queues structures and operations have strongly operations making implementations of these specifications subject to our result finally as raw or awar cannot be avoided in most practical algorithms our result suggests that it is important to improve the hardware costs of and operations the instructions that enforce raw and awar an interesting direction for future work is taking advantage of our result by weakening its basic assumptions in order to build useful algorithms that do not use raw and awar acknowledgements we thank and the anonymous reviewers for valuable suggestions which improved the quality of the paper research is supported in part by the science foundation grants number and research is supported in part by the science foundation grants number and references v and shared memory consistency models a ieee computer ­ thomas e the performance of lock alternatives for ieee trans parallel syst ­ s robert d and c greg thread scheduling for in proceedings of the annual acm symposium on parallel algorithms and architectures pages ­ june and kaplan lower bounds for adaptive collect and related objects in proceedings of the annual acm symposium on principles of distributed computing pages ­ and computing in totally anonymous asynchronous shared memory systems information and computation ­ march and automatic discovery of mutual exclusion algorithms in proceedings of the th international conference on distributed computing pages ­ boehm reordering constraints for locks in proceedings of the acm sigplan symposium on principles and practice of parallel programming pages ­ and a complete and automatic linearizability checker in pldi proceedings of the acm sigplan conference on programming language design and implementation pages ­ new york ny usa acm james and bounds on shared memory for mutual exclusion information and computation ­ december david and dynamic circular in proceedings of the annual acm symposium on parallelism in algorithms and architectures pages ­ july w dijkstra solution of a problem in concurrent programming control acm and time lower bounds for implementations of journal of the acm and on the inherent of conditional primitives distributed computing ­ and on the space complexity of randomized synchronization journal of the acm ­ september mark and step complexity lockfree as an example in pages ­ e and h the implementation of the multithreaded language in proceedings of the acm sigplan conference on programming language design and implementation pldi pages ­ june james r cache consistency and sequential consistency technical report technical report and s synchronization algorithms for ieee computer ­ mark and a nonblocking work distributed computing ­ and nonblocking work queues in proceedings of the annual acm symposium on principles of distributed computing pages ­ july synchronization acm trans program lang syst ­ and the art of multiprocessor programming morgan and linearizability a correctness condition for concurrent objects acm trans program lang syst ­ and java memory consistency and process coordination in pages ­ and bounds for mutual exclusion with only processor consistency in pages ­ ibm system extended architecture principles of operation publication no sa and time and space lower bounds for nonblocking implementations siam journal on computing ­ limitations and capabilities of weak memory consistency systems phd thesis university of january michael martin and automatic inference of memory in formal methods in computer aided design lamport specifying concurrent program modules acm trans program lang syst ­ april lamport the mutual exclusion problem part ii statement and solutions j acm ­ lamport a fast mutual exclusion algorithm acm trans comput syst ­ lee compilation techniques for explicitly parallel programs phd thesis department of computer science university of illinois at mark and on the complexity of consensus in proceedings of the th international conference on distributed computing pages ­ october paul e memory barriers a hardware view for software linux technology center ibm june john m and michael l scott algorithms for scalable synchronization on acm trans comput syst ­ m michael and michael l scott simple fast and practical nonblocking and blocking concurrent queue algorithms in proceedings of the annual acm symposium on principles of distributed computing pages ­ may m michael martin t and idempotent work in proceedings of the acm sigplan symposium on principles and practice of parallel programming pages ­ february and concurrent counting journal of computer and system sciences ­ august scott sarkar and peter sewell a better x memory model in pages ­ l about the mutual exclusion problem inf process ­ a m michael and von a theory of memory models in proceedings of the th acm sigplan symposium on principles and practice of parallel programming pages ­ march sarkar peter sewell scott thomas o and jade the semantics of multiprocessor machine code in popl pages ­ william e concurrency control for abstract data types ieee trans computers ­ the formal semantics of programming languages mit press 