flow analysis with matthew might hall university of usa abstract we describe implement and benchmark an algorithm for higherorder controlflow analysis specifically cfa with a gpu ultimately our program transformations reductions and optimizations achieve a factor of speedup over an optimized cpu implementation we our investigation with the view that computations with a for branching taking that perspective to its limit we reduced cfa to an algorithm synthesized from operations central to this reduction were abstract church encodings and encodings of the syntax tree and abstract domains as vectors and matrices a straightforward implementation of performed slower than a fast cpu implementation ultimately data structures and operations turned out to be the critical because controlflow graphs are sparse in practice up to empty our controlflow matrices are also sparse giving the sparse matrix operations an space and speed advantage we also achieved by carefully data races the monotonicity of cfa makes it sound to perform analysis operations in parallel possibly using or even data categories and subject descriptors d programming languages processors and optimization general terms languages keywords abstract interpretation program analysis flow analysis lambda calculus gpu cps matrix introduction at obtaining for algorithms over continuous domains with kernels flow analyses on the other hand tend to be fixedpoint algorithms over discrete domains with kernels such as abstract interpretations at first seem to flow analyses yet with a shift in algorithmic perspective and the right data structures make the flow analysis for higherorder two orders of magnitude faster permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm motivation after nearly a higherorder controlflow analysis remains an important analysis for compilers of functional languages yet the analysis also remains expensive even one of its simplest cfa is still nearly in complexity on log n with cpu clock cycles no longer growing this complexity places a de upper bound on the size of programs that can be analyzed in a reasonable amount of time to analyze large programs higherorder controlflow analysis must exploit the parallelism available on modern systems toward that end we develop a algorithm for cfa achieves a factor of speedup over existing cpu techniques highlevel methodology to develop we the gpu as a platform for algorithms composed entirely of operations so we reduced shivers cfa to a algorithm composed entirely of matrix operations matrix multiplication matrix addition and matrix there are five key insights to this reduction to binary continuationpassing style to achieve good on a gpu we need controlflow among the gpu threads ie to avoid branching operations in flow analysis the key step to is the propagation of flow information at each call site to eliminate branching from this propagation routine we transform our programs into a canonical form binary continuationpassing style binary cps in binary cps every call site provides two arguments and every function accepts two arguments by eliminating the need to on the instruction type there is only one function call and on the number of arguments binary cps eliminates branching from the propagation subroutine abstract church encodings one could use church encodings to reduce every program construct eg if letrec set to binary continuationpassing style but these controlflow for instance set requires a global transform and the y combinator up recursion an abstract church encoding exploits the approximation in cfa so that the encoded program has the same abstract controlflow as the original program if no longer the same concrete behavior for example set x is equivalent as far as cfa is concerned to let x void encoding of the syntax tree we use binary continuationpassing style to provide to program syntax but we still need a way to encode the syntax tree of a program we encode the syntax tree of a program as a collection of selector functions which are themselves represented as matrices individual program terms are then encoded as vectors well write t to mean the vector that encodes term t for instance every call site has three components the procedure expression its first argument expression and its second argument expression so there are three selector matrices that operate on call sites fun arg and arg for example for a call site f e e f e e × fun f f e e × arg e f e e × arg e matrix encoding of the abstract store in cfa the abstract store also known as the abstract heap maps variable names to sets of values it is the primary data structure used during the execution of cfa so it must be encoded in a way fortunately it is straightforward to represent this data structure as a matrix one axis of the store matrix represents variables the other axis represents lambda terms if the entry for variable i lambda j is nonzero this indicates that a closure over lambda j may get bound to variable i thus if the matrix is an abstract store in matrix form and v is a variable then the vector v × describes the possible values of the variable v encoding of the transfer function once the syntax tree of the program is described in terms of selection matrices the next step is to describe the action of the smallstep transition relation for cfa in terms of operations of the smallstep transition relation section finds only three operations beyond syntactic selection function lookup join over functions and functional extension we reduce function lookup to multiplication join to matrix addition and functional extension to a combination of matrix addition matrix multiplication and matrix key insights for acceleration and races for the implementation there are two insights that lead to acceleration a representation of the abstract store and a of monotonic races that allows the analysis of call sites in parallel exploiting in practice most controlflow graphs are sparse in our matrix encoding of the store the of this matrix is linked to the of the controlflow graph thus algorithms have a performance advantage over algorithms for all but the most programs programs which tend toward completeness in their controlflow graphs programs in which any point may jump to any other point exploiting monotonicity to permit race conditions when analyzing call sites in parallel they may both attempt to read from andor write to the same location in the abstract store fortunately the monotonic growth of the abstract store during an abstract transition guarantees us that these races are if properly once an entry is set to a nonzero value in the abstract store no other thread will set it back to zero monotonicity also guarantees soundness when a thread works with a or partially updated store contributions · our primary contribution is the formulation of flow analysis to the best of our knowledge is the first such formulation · our claims are the reductions that made this formulation possible the encoding of the syntax tree as selection matrices abstract church encodings and the reduction to linear algebra of the abstract semantics for cfa outline theory and implementation this work in · theory section is a brief review of the smallstep formulation of shivers cfa for continuationpassing style section defines a formulation of cfa section describes abstract church transformations that are only for the abstract semantics of our analysis which more precisely and efficiently handle language constructs such as mutation recursion termination basic values and conditionals · implementation section describes how we map the algorithm for down to a gpu in fact this section describes three different approaches for performing the mapping two of these end up slower than or only as fast as the cpu version but the final implementation which uses sparse matrices achieves the desired speedup section gives the results of our empirical evaluation we tested two gpu implementations of against two cpu implementations of cfa and found a factor of speedup background binary cps and cfa we are going to original cfa for continuationpassing style cps to enable high performance on the gpu we are going to specialize it for a form binary continuationpassing style in this section well define binary cps and briefly review cfa readers familiar with cfa may wish to skip this section our definition of cfa is taken from might and recent smallstep we refer readers to for details such as abstraction maps and proofs of soundness binary cps is a variant of the continuationpassing style calculus in which every procedure accepts exactly two arguments we use binary as opposed to unary cps because the cps transform on lambda terms introduces an additional continuation cps cannot use to handle multiple arguments since it violates the no procedure may return principle we are using binary instead of cps because we want to eliminate branching from the smallstep transition relation were about to define branches interfere with operations in the gpu the in binary cps also has the benefit of making our matrix encodings of syntax trees structured this structure is amenable to the arithmetic optimizations and presented in section binary cps the grammar for binary cps contains calls and expressions and expressions are either lambda terms or variables call call f e e f e exp var lam v var is a set of identifiers lam lam v v call concrete semantics the simplest smallstep concrete semantics for binary cps uses just three domains in its call × env env var lam × env and one transition rule × f e e call where v v call ef ee ee v v where the function e exp × env evaluates expressions ev v lam abstract for cfa might and shivers cfa as an abstract interpretation of the concrete semantics just given the core of the abstraction is the elimination of environments from the semantics and the use of an abstract store to represent all environments so abstract states pair the current call site with the abstract store and closures lose their environments call × store store var l p lam we assume the natural partial orders on these sets lambda sets are ordered by inclusion abstract stores are ordered pointwise by inclusion and abstract states use a product ordering for instance v v v cfa abstract semantics the transition relation for the abstract semantics × that of the concrete semantics f e e call where v v call ef l ee l ee v l v l as does the argument evaluator e var × store p lam ev v lam a change in the abstract semantics is the nondeterminism that results from branching to the set of possible lambda terms for the procedure argument note the of computing cfa to compute classical flowinsensitive cfa with the smallstep transition relation we need a family of functions that computes the output store with respect to each call site call store store call then we can construct the pass function f store store which performs one full pass of the analysis by considering the effect on the store of every call site in the program call call n f · · · because the function f is continuous and monotonic and the height of the abstract store lattice is finite the result of cfa is least fixed point of the function f for some finite n where the bottom store maps everything to the empty set store v complexity if the function f adds one entry to the abstract store per application there can be at most var × lam applications before it the abstract store and must terminate since the cost of each application is proportional to call the complexity of this cfa is as expected a linear encoding of cfa in this section we discuss our encoding of both binary cps and cfa in brief individual program terms will be represented as vectors the structure of the syntax tree will be compiled into static selection matrices that operate on these term vectors finally the pass function f from cfa will be encoded as a function operating on stores represented as matrices running example throughout the remainder of this paper all of the examples will be with respect to this program the c c are explicitly labelled for future reference v v v v vc v v v v vc v v v v vc c and the same abstract store v v v v v v we do this to improve presentation and to emphasize the differences between each data representation strategy in section encoding terms as vectors we encode each program term a lambda term a variable or a call site as a vector over the set every term in the program will have a unique vector representation for convenience we write the vector encoding of term t as t well have vectors of two lengths expression vectors of length exp and call vectors of length call formally e exp exp call call call v var exp lam lam exp we can assign each expression a number from to exp so that the expression vector with at slot i is the expression vector for expression i we can do likewise for call sites more specifically variables are represented as vectors of size exp notably not of size var terms are also represented as vectors of size exp also notably not of size lam the first var entries of an expression vector represent variables and the last lam entries represent lambda terms running example here is the expression vector representing lambda term v v v v v v and here is the expression vector representing variable v v v v v v v its worth why we dont split expressions into two vector for variables and one for lambda terms we took this approach because it allows us to eliminate branching from argument evaluation later on as it is formulated in the abstract semantics the argument evaluator e must look up its expression argument if it is a variable and return its argument if it is a lambda term well be able to construct a single linear operator that has the effect of the argument evaluator branching necessary encoding the syntax tree as matrices to encode the syntax tree well use selectors encoded as static ma to syntax terms that is given a term vector t to figure out the syntactic children of term t well have a matrix for each type of child eg first formal parameter second ment that maps t to that child for instance the matrix call maps lambda terms to their call site so that v v call × call call there are six static syntax matrices fun call exp function applied in a call site arg arg call call exp the first argument in a call site call exp the second argument in a call site lam call the call site of a term var var lam exp the first formal of a term lam exp the second formal of a term these matrices are constructed once per program and remain constant through the of the analysis we could formulate the analysis more generally in terms of any orthogonal basis vectors for terms but there doesnt seem to be any performance advantage to doing so running example the fun matrix determines the function being applied at a call site for our running example c v v v v v v c c c running example in order to look up the function being applied at call site labelled c we multiply the vector representing c with the fun matrix the result is the vector representing the variable v c c c c v v v v v v × v v v v v v encoding flow sets as vectors cfa flow sets flow sets are sets of lambda terms we also need to represent these as values a natural encoding of flow sets is to use bit vectors of length lam l lam lam in some sense we appear to be with the uniform repre of expressions by creating a special terms however it is more accurate to think of lambda as rep sets of abstract closures than sets of lambda terms more can and values between the set lam and the set lam simply by adding or ignoring on the front of the vector for a set of lambda terms l we write their vector encoding as l and we expect the following property to hold lam lam · · · where the operator is encoding abstract stores as matrices now that we have a representation for syntactic domains and for flow sets we need a matrix representation for the abstract store fortunately the store is a map from variables to flow sets and a linear operator encoded as a matrix is a natural way of representing such a map we do have a design choice here and the right answer is not immediately obvious we could use a matrix to represent stores but well be able to eliminate a branch during argument evaluation if we make all stores into slightly larger matrices so formally store exp lam we write the of an abstract store as and we define the encoding by relating abstract and matrix stores lam × lam v × v where the operator × is actually boolean matrix multiplication according to these rules the lower portion of all stores the part that corresponds the lambda portion of expression vectors must be the identity matrix running example the matrix representation of the store is v v v under this encoding of stores the evaluation of an expression takes a single matrix multiplication theorem ee e × proof by analysis and the rules for the encoding running example the lookup of variable v v v v ··· ··· v ··· ··· v × v updates on the store during the abstract transition we make updates to stores of the form v l v l we need matrix operations that correspond to this update operation first we have to construct a store representing a single mapping v l fortunately matrix and matrix multiplication make this straightforward lemma v l v × l store its also the case that acts as join on stores lemma and these two lemmas give us store update theorem store update theorem v l v l v × l v × l proof by the prior two lemmas running example store update this example shows all the moving parts for store update running example to update with to variable v we multiply the vectors representing and v and add the result to the current store v v v × v v v v v v curr new linear encoding of the transfer function earlier we constructed a family of transfer functions which produced the effect of the transition relation on the store for a given call site we can construct an equivalent function store store l call × fun × l call × arg × l call × arg × v l × var v l × var v × l v × l soundness we can prove that the transfer function is equivalent to the traditional one under our encoding theorem soundness proof the proof proceeds first show that every entry in must be in and then vice versa in both directions its to split into cases the one in which the entry exists in and the one in which it is fresh the algorithm algorithm gives the highlevel algorithm for the algorithm assumes that the store is empty at the start while changes do foreach call do lookup function and arguments in call l call × fun × l call × arg × l call × arg × formal arguments of function l v l × var × v l × var × update store v × l v × l end end bind l to v bind l to v algorithm abstract church encodings its possible to transform any program into binary cps using mechanisms like church encodings and the y combinator some encodings like transforming the term let v e body into the term v body e are but because some encodings transform dataflow into controlflow they the controlflow of the original program even integers raise controlflow issues yet handling forms like set or letrec as special cases in the transfer function is not practical this would force conditional tests and branching the that we have carefully and protected to avoid branching but preserve precision we turn to abstract church encodings an abstract church encoding is a program transformation that is for an abstract semantics but not for the concrete semantics the abstract church encodings in this section work for the abstract semantics of cfa and the simple linear model of some of the gpu optimizations require of the program a constraint that these encodings violate where violations occur we will note how to adapt abstracting termination as nontermination we actually need abstract church encodings to handle program termination notice that our cps language has no halt form so to encode halt well use nontermination that is we use the nonterminating program as the abstract encoding for termination in other words we apply the following rewrite rule after conversion to a binary cps that contains a halt primitive halt a b f g f f f f g f f f f g f f f this works because once an abstract interpreter that branch contribute any more changes to the abstract store so the abstract interpretation can reach a fixed point abstracting mutation as binding to church encode a construct like set have to perform cell boxing on all mutable variables and then eliminate all cells with a transformation these kinds of global transforms alter the controlflow and dataflow behavior of the program yet cfa has a single abstract store that represents all program environments as a result let and set have exactly the same effect on the abstract store so we can apply a rewriting rule set v e let v e void from the abstract stores perspective these terms are equivalent encoding recursion as mutation now that we can handle mutation we can avoid using the y combinator or a mutually recursive variant to handle recursion letrec normally into lets and sets but since let has the same effect as set we can actually turn letrec into let with an abstract rewrite rule letrec v lam vn e let v lam vn e cfa does not distinguish their effects on the abstract store abstracting basic values as nontermination most encode all numbers as a single abstract value in fact most even convert all basic values into a single abstract value we can do the same using the halt function from before as the single abstract value any attempt to apply a basic value allow the program to continue normal execution all primitive operations that operate on basic values ignore their arguments and return this basic value abstractly encoding conditionals most do not attempt to evaluate conditionals their behavior is to always branch in both directions at an if we could church encode booleans and conditionals but this introduces a level of indirection as conditionals appear to flow through their condition or we could exploit the fact that flow sets merge in cfa to simulate the nondeterminism of the conditional with the nondeterminism of procedure call so after we cps transform if forms we can abstractly encode them with a rewrite if e call call let next call let next call next its safe to drop the conditional expression e because after the cps transform this expression is atomic and it makes no changes to the store by the time the analysis reaches the call site next both continuations will have been bound to next in the abstract store our gpu implementation in this section we discuss the details of mapping our highlevel algorithm for down to the and of a gpu implementation it took effort to discover which optimizations and data structures were the right ones so we will discuss both the right turns and the wrong ones on the to we present three iterations of our implementation and sparse we a in which transformed the input program and generated the static syntax matrices these matrices were passed to a c program which copied them into the gpu memory and the gpu kernels that performed the analysis for a brief summary of and the highlevel architecture of the gpu we see the appendix all three of our implementations up at this point attempt implementation with our first implementation of used library for linear algebra to perform the matrix operations this implementation was an almost of algorithm and it turned out to be slower than our cpu implementation we identified several problems · matrix computations although most of the matrices in our analysis were sparse the library is written for operations since the matrices are all quadratic in the size of the program they consumed all of the parallel processing facilities of the gpu even for small programs · memory requirements the matrices and vectors in only contain boolean values use of byte as required by the libraries for each element was as we describe in section using one bit per entry large and substantial memory · redundant operations the static syntax matrix each a large multiplication were repeated in each iteration eventually we were able to these matrices and simplify their application attempt matrix implementation to overcome some of the limitations of the approach we optimized the matrix and vector representations since the vectors and matrices contain boolean entries one obvious way to reduce the sizes of the matrices was to use for terms and values and for the abstract store and the syntactic matrices once encoded as data structures we also gain parallelism by from boolean to operations running example here is the same store lookup from earlier with matrices each column of the store has been compressed to make this example small but interesting we assume that the size of each word is bits the operator is that works on our scheme the multiplication and addition in conventional matrix multiplication algorithm become and in our implementation the word size is word size on the gpu on which we ran the analysis this means that all vectors are to bring their size to the multiple of this reduced the size of the matrices by almost a factor of the is why the reduction in size always exactly a factor of running example here is store update × the operator performs a static syntax matrices each static syntax matrix is quadratic in the size of the program them to and manipulating them on the gpu imposes significant time and memory costs fortunately we can exploit the of binary cps to them and optimize their application at the same time by choosing the numbering scheme for variables and lambda terms carefully we compressed the operators var and var into small explicit formulae specifically if the lambda term n is the nth lambda term then in an expression vector n lam is the expression number of its first formal and n lam is the expression number of it second formal now each expression vector has three distinct segments the lam segment for the lambda terms the var segment for formals in the first position and the var segment for formal in the second position with this scheme we can determine vk and vk from the representation of k without any matrix multiplication running example if vk and vk are respectively the first and second formal arguments for k these are the vectors representing v and v v v v v v v v v the call matrix after making the observation that there is exactly one call site for every term that is the same vector can represent both the term and its corresponding call site when we shift to a implementation we will be able to the remaining static syntax matrices abstract church encodings the compression strategy we have chosen implicitly the program so that each variable has a unique lambda term that binds it some of the abstract church encodings carefully exploit the behavior of cfa on code there are two ways to resolve this problem more sophisticated abstract church encodings or a lookup table in the gpu implementation the encoding approach adds a procedure for every variable v v q q all mutation of the variable v must pass through this procedure the approach creates a table in which entry n contains the expression number of the first formal and the entry n lam contains the expression number of the second formal for the nth lambda term store update optimization under the encoding described in the previous section the store is also logically divided into the same three segments the lam region the var region and the var region as a result we determined that store update which adds v × l can only modify a known third of the store the region in which the variables in the vector v live recall that in the abstract semantics updates for formals are carried out separately from updates for formals exploiting this knowledge the number of operations on update by optimizations a few optimizations in our second implementation apply only to the gpu · reducing the number of kernel calls the cpu parallel operations on the gpu through kernel invocations each kernel call has nontrivial overhead so kernels should be combined whenever synchronization between them is unnecessary we performed the for l l and l see algorithm within the same kernel combined with the static matrix compression and the store update optimization the total number of kernel calls per iteration from to · constant memory since we were over all call sites in the program there was strong locality in the syntax matrix lookup and this data was readonly on the gpu therefore it was a good candidate to use the constant memory which unlike the global memory is for low memory accesses although the constant memory on the gpu was not large enough to store the entire static matrix we could this locality and store only that part of the matrix which we would be used at any point of time this increased speed by compared to the scheme without constant memory · of the vectors to word boundaries in order to efficiently implement static syntax matrix compression section we aligned each segment of the expression var and word boundaries running example the figure below shows vectors where lam and the word size is d v ··· v ··· vd v v v ··· ··· ··· ··· ··· ··· v vd v v v ··· v ··· ··· ··· ··· ··· ··· ··· we also the store to take into account this change in the size of the vectors running example the corresponding store is below when creating the store we need to add rows corresponding to the pads d vd v vd v d · shared memory in our implementation of matrix multiplication each thread the value of exactly one cell in the result we the threads so that all threads within a block write to adjacent cells within the same row of the result this means that all of the threads will read from exactly the same part of the vectors being this yields significant data reuse so we copied the relevant segments of the vectors into shared memory and observed a reduction in the limitations of this approach to our these optimizations only the performance of our cpu implementation of cfa through debugging we the cause of the performance down to a few principal factors · unnecessary operations the first phase of the store update operation generates a new store which contains new bindings the second phase then adds this new store to the old store not only does this memory on a second store the update only a small number of locations yet we pay a cost proportional to the size of the · failure to exploit can do things cfa cant can represent multiple terms eg call sites in the same vector simply by setting additional bits this gives control over a different kind of ability to act on terms yet our implementation ignores this ability · parallelism since the analysis is flowinsensitive all of the call sites could be evaluated in parallel and their effects then merged however since we were using operations performing large matrix for each call site consumed the storage of the gpu to process each call site required a return to the cpu to the partial solution from the gpu and copy the input data for the next call site the computation success sparse matrix implementation before constructing our third implementation we observed that the abstract store to be sparse in fact of all entries in the final store matrix contained and on average each variable was bound to about two lambda terms using a matrix to encode the store is inefficient so we to using a sparse matrix representation of the store and this optimization turned out to be the critical for a sparse representation the sparse matrix representation we use is essentially form in which the sparse matrix has a fixed maximum number of columns each row has a header indicating how many cells in that row are active and an auxiliary array indicates the column for each nonzero element is best suited to sparse matrices that have roughly comparable numbers of nonzero elements and itself to computationally efficient access to the sparse matrix this scheme is not as as the more commonly used compressed sparse row representation but it has a smaller memory footprint than the representation and it for our purposes the performance advantages we allocate our initial store matrix by allocating a fixed number of columns to each row in the store initially we assume no flow set will hold more than of all lambda terms in the program if during the course of the analysis this number proves to be we save the store terminate the analysis and with a twice as many columns the header cell of each row is the index of the first free slot available in that row each entry in a flow set row is the assigned number of the lambda term in the flow set eg if row i contains n then n may flow to variable vi running example we sketch the sparse store representation for our running example below v v v in order to add to the flow set for v we append the number corresponding to to the row vector representing v now a operation its a operation because we check for membership before optimizing the static matrices in addition to the store we also compressed the remaining static syntax matrices since every variable and term has an assigned row in the store when evaluating the function and argument expressions of a given call site all needs is the corresponding row number in the store with this insight we reduced the fun arg and arg matrices to lookup vectors of size call this eliminates the need to perform a true matrix multiplication in the phase of algorithm running example the fun matrix becomes the vector c c c c callsite evaluation since the operation was simplified we up enough gpu resources to each iteration across call sites race conditions since global synchronization on a gpu is expensive its important to store update to behave correctly in the presence of races once call sites are analyzed in parallel it will likely be the case that one gpu thread a store has been only partially updated by another gpu thread fortunately abstract transfer functions in cfa are monotonic which means that flow sets only get larger more importantly it means that cfa is during each iteration working with data does not affect the soundness we ensure that the store grows by writing to it only when a gpu thread has new information to add and for overall efficiency we some a row may have more than one copy of the same lambda term if they race on trying to add the same lambda at the same time this prevents threads with data from any changes to the store without the cost of global synchronization for instance lets say that two call sites c and c are being evaluated in parallel and c reads from a row before c updates it if the evaluation of c does not yield any new store bindings nothing will be written to the store and the updates of c will be preserved in the next iteration these updates will be visible during the evaluation of c and the final result will still be correct against races from monotonicity we can argue more formally that cfa is robust in the presence of or partially completed stores and analysis for call sites suppose that the current abstract store is reset at some point to an arbitrarily chosen weaker store so that this represents on or partially completed data we can show that the analysis will still reach the most precise result theorem if then f proof by definition applying f to both sides yields f f by fixedpoint theorem f must also be a fixed point guarantees this fixed point will be equal to or greater than the least fixed point thus f we can also analyze calls in any on partially completed or data we show this by proving that applying the transfer function for any call site to a point below the least fixed point remains consistent with the least fixed point theorem if call site call then for any proof by contradiction and cases and and optimization memory a final led to additional speedup the selection matrices which had by now been reduced to a linear array were all stored in memory a way of readonly data in global memory so that it is for memory access we did this because data doesnt have the size restrictions of constant memory the use of memory in a roughly speedup over the use of global memory empirical evaluation in total we created four implementations to evaluate and compare in case it is not clear and cfa have exactly the same precision so time is the only relevant metric for comparison ultimately our gpu implementation was a factor of faster than our cpu implementation at scale we compared two cpu implementations against two gpu implementations · cpu s a fast cpu implementation of cfa in we implemented a traditional smallstep version of cfa according to best · cpu sp a cpu implementation of in c since many of our optimizations to the implementation were not and many good sparse matrix algorithms for the cpu exist we had to make sure that merely representing cfa as a sparse matrix algorithm was not the cause of the speedup we implemented this cpu version of the algorithm in order to show that the from the gpu in fact this matrix cpu version our cpu version in performance so we report our relative speedup against this cpu implementation instead · gpu d a implementation of on the gpu it performed we implemented this version as a point of comparison this implementation corresponds to attempt · gpu sp a implementation of on the gpu we implemented this version to measure the speedup over the cpu version which up being a factor of platform we evaluated our implementations on an gpu with gb of memory the host machine on which we also ran the cpu implementations was equipped with an intel i cpu running at benchmarks to benchmark our implementations across a range of program sizes we exploited van horn and recent work on the complexity of controlflow analyses because van horn and offer constructive proofs of complexity we can extract a benchmark generator from these proofs that a program of the size that will be worstcase to analyze for when k for cfa the programs it generates are difficult but not worstcase might and van recent work on the complexity of also used this generator and their empirical results provide a sense of its to benchmarks in short these benchmarks are about an order of magnitude harder to analyze than code written by measurements table presents the running time of the analysis for each implementation the first column is the number of terms in the program a time of indicates that the analysis took longer than to complete figure is a of the same data note the scale on the as programs grow larger the gpu implementation of has a significant advantage over the other implementations comparison of implementations time seconds number of terms figure comparison of implementations running times versus number of terms note that the time axis is the gpu implementation of clearly dominates terms gpu sp ms ms ms ms ms ms ms s s s m s cpu sp ms ms ms ms ms ms s s m s min hr m cpu s ms ms ms ms s s m s m s min hr m gpu d ms ms ms s s s s m s hr m table analysis running times versus number of terms means greater than gpu vs cpu for small programs time seconds number of terms figure comparison of gpu and cpu at smaller program sizes up to program terms the cpu implementation the gpu implementation in the interest of showing tradeoffs figure compares the performance of the implementation of the analysis running on the cpu and the gpu for smaller program sizes for small programs less than terms the cpu the gpu for the following reasons · kernel invocation cost there is a fixed cost associated with each invocation of a kernel on a gpu at smaller program sizes since the number of iterations and the time taken per iteration is small the cost becomes a significant percentage of the total running time · faster cpu clock the cpu clock runs twice as fast as the gpu against in small programs there as much parallelism that can be exploited by the gpu in this situation the slower clock of the gpu limits speed · fewer cpu iterations on larger program sizes the number of iterations is large enough that in the limit both the serial and parallel algorithms converge in exactly the same number of iterations for the smaller programs this true and the gpu often takes twice as many iterations to converge figure is a of the speedup obtained by the gpu implementation of over the cpu negative speedup values visible at the left side of the indicate a figure shows this of the in more detail speedup of gpu over cpu speedup speedup x number of terms figure speedup in of gpu over cpu versus the number of program terms related work our work in flow analysis from a long line of research beginning with the foundational work on abstract interpretation through jones work on controlflow analysis and most recently through shivers work on the literature on static analyses is sparse to the best of our knowledge there been any efforts to higherorder controlflow analyses very recent contributions include et als use of architectures to classical pointsto analysis and et als framework for distributed software model checking given that cfa can also be as an analysis techniques are likely applicable most of the prior work in static analyses have been focused on dataflow analysis for firstorder imperative programs dataflow analyses have been performed by iterative methods such as those originally proposed by and elimination methods or hybrid algorithms which combine both approaches most efforts at data flow analysis have involved these algorithms and gupta interval analysis ryder improved on the graph partitioning scheme from which led to a more effective parallel algorithm lee implemented the hybrid approach for architectures using message passing between the processors gupta et al moved away from existing algorithms to developing specific techniques for parallel program analysis in they convert the controlflow graph of a program into a dag and solve the dataflow problem for each node of the graph in parallel the gpu has been used to static analysis most notably by and who implemented the abstract domain on a gpu since computations are based on matrices it was easily mapped to the gpu future work there are at least two promising for future work our techniques to pointer analysis and exploiting within to achieve a new kind of flow analysis pointer analyses given recent results the connection between controlflow and pointer analyses we believe that the techniques presented here can be adapted to pointer analyses as well pointer analyses face additional such as the fact that its smallstep transition relation is much more complex and a reduction to binary cps seems out of the question but these problem do not seem exploiting using matrices to represent the store and vectors allows to be used as another potential source of parallelism this parallelism is implicit in the structure of the matrices themselves so it could be exploited in addition to the explicit parallelism that was described in section analysis it took a lot of syntactic normalization and abstract church encodings to impose controlflow on the abstract transfer function we that a better approach would be to allow different syntactic forms and then process each kind of form in process all function calls in parallel then all set statements in parallel then all conditionals and so on in each pass of the analysis a the gpu and in this section we provide a brief highlevel description of the architecture memory hierarchy and programming model of the gpu compute unified device architecture is a general purpose parallel computing architecture that the parallel compute engine in the parallel programming model provides three levels of hierarchy of thread groups shared memories and barrier synchronization threads on a gpu are organized into groups called blocks only threads within a block can be synchronized synchronization of threads across blocks would have to be done on the host which can be quite expensive threads are scheduled and executed in groups of parallel threads called divergence in execution paths of threads within a can have an impact on performance the gpu has different types of memories · global memory is large global readwrite · shared memory is small private to each block readwrite memory whose access time is potentially as low as register access time · constant memory is small global readonly and · memory is readonly and it is optimized for d spatial locality which means that certain memory access patterns can be very efficient data placement in memory and divergent execution in threads must be carefully controlled to performance references programming guide aug f e allen and j a program data flow analysis procedure acm s k v w d m l c b smith and h sparse matrices in users manual chapter pages ­ edition dec f and r a fast implementation of the abstract domain on hardware in h r nielson and g editors static analysis volume of lecture notes in computer science chapter pages ­ springer berlin berlin isbn n bell and m implementing sparse multiplication on processors in sc proceedings of the conference on high performance computing storage and analysis pages ­ new york ny usa acm isbn s algorithms for recursive state machines in proceedings of the th annual acm sigplansigact symposium on principles of programming languages popl pages ­ new york ny usa acm isbn p cousot and r cousot abstract interpretation a unified lattice model for static analysis of programs by construction or approximation of fixpoints in conference record of the fourth acm symposium on principles of programming languages pages ­ new york ny usa acm press p cousot and r cousot systematic design of program analysis frameworks in popl proceedings of the th acm symposium on principles of programming languages pages ­ new york ny usa acm r gupta l and m l data flow analysis m s flow analysis of computer programs science inc new york ny usa isbn n d jones flow analysis of lambda expressions preliminary version in proceedings of the th colloquium on automata languages and programming pages ­ london uk springerverlag isbn g a a unified approach to global program optimization in popl proceedings of the st annual acm symposium on principles of programming languages pages ­ new york ny usa acm r r gupta and m l the combining dag a technique for parallel data flow analysis ieee transactions on parallel and distributed systems ­ aug y f lee t j and b g ryder performing data flow analysis in parallel in proceedings of the conference on pages ­ ca usa ieee computer society press isbn y f lee b g ryder and m e region analysis a parallel elimination method for data flow analysis ieee trans ­ n p and a rybalchenko distributed and software model checking in proc of the th international conference on verification model checking and abstract interpretation jan t j and b g ryder an efficient hybrid algorithm for incremental data flow analysis in popl proceedings of the th acm sigplansigact symposium on principles of programming languages pages ­ new york ny usa acm isbn m a and k parallel pointsto analysis in proceedings of the acm international conference on object oriented programming systems languages and applications oopsla pages ­ new york ny usa acm isbn j and d van horn control flow analysis algorithm higherorder and symbolic computation to appear m might and o shivers improving flow analyses via cfa abstract garbage collection and counting in icfp proceedings of the acm sigplan international conference on functional programming pages ­ new york ny usa acm isbn m might y and d v horn and exploiting the functional vs objectoriented program analysis in pldi proceedings of the acm sigplan conference on programming language design and implementation pages ­ new york ny usa acm isbn j palsberg closure analysis in constraint form acm transactions on programming languages and systems ­ jan b g ryder and m c elimination algorithms for data flow analysis acm comput ­ o shivers control flow analysis in scheme in proceedings of the acm sigplan conference on programming language design and implementation volume pages ­ new york ny usa july acm isbn o g shivers controlflow analysis of higherorder languages phd thesis carnegie mellon university d van horn and h g relating complexity and precision in control flow analysis in icfp proceedings of the th acm sigplan international conference on functional programming pages ­ new york ny usa acm isbn d van horn and h g deciding is complete for in icfp of the th acm sigplan international conference on functional programming pages ­ new york ny usa acm isbn a parallel interval analysis of data flow equations volume ii the state university press aug for code running on the gpu this memory is readonly code running on the host ie the cpu can write to it 