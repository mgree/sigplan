quantitative interprocedural analysis institute of science and technology university abstract we consider the quantitative analysis problem for interprocedural controlflow graphs the input consists of an a positive weight function that assigns every transition a positive number and a labelling of the transitions events as good bad and neutral events the weight function assigns to each transition a numerical value that represents a measure of how good or bad an event is the quantitative analysis problem whether there is a run of the where the ratio of the sum of the numerical weights of good events versus the sum of weights of bad events in the is at least a given threshold or equivalently to compute the maximal ratio among all valid paths in the the quantitative analysis problem for can be solved in polynomial time and we present an efficient and practical algorithm for the problem we show that several problems relevant for static program analysis such as the worstcase execution time of a program or the average energy consumption of a mobile application can be modeled in our framework we have implemented our algorithm as a tool in the java framework we demonstrate the effectiveness of our approach with two case studies first we show that our framework provides a sound approach no false positives for the analysis of containers second we show that our approach can also be used for static profiling of programs which reasons about methods that are frequently invoked our experimental results show that our tool to relatively large benchmarks and relevant and useful information that can be used to optimize performance of the programs categories and subject descriptors d programming languages f logics and meanings of programs semantics of programming analysis general terms algorithms languages performance keywords interprocedural analysis quantitative objectives meanpayoff and ratio objectives memory static profiling this work has been supported by the science foundation under the rise s grant pn start grant graph games grant science foundation grant and microsoft permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than the authors must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ copyright is held by the publication to acm acm introduction static and interprocedural analysis static analysis techniques provide ways to obtain information about programs without actually running them on specific inputs static analysis the program behavior for all possible inputs and all possible executions for nontrivial programs it is impossible explore all the possibilities and hence static analysis uses approximations abstract interpretations to account for all the possibilities static analysis algorithms generally operate on the interprocedural controlflow graphs for brevity an consists of a collection of controlflow graphs cfgs one for each procedure of the program the cfg of each procedure has a unique entry node and a unique exit node and other nodes represent statements of the program and conditions in other words basic blocks of the program in addition there are call and return nodes for each procedure which represent invoking of procedures and return from procedures connect call nodes to entry nodes and connect exit nodes to return nodes algorithmic analysis of provides the mathematical framework for static analysis of programs interprocedural analysis with objectives such as reachability setbased information etc have been studied in the literature ­ quantitative objectives a or boolean objective assigns to every run of a program a boolean value accept or reject a quantitative objective assigns to every run of a program a real value that represents a quality measure of the run the analysis of programs with quantitative objectives is due to embedded systems with requirements on resource consumption of responses performance analysis etc quantitative objectives have been proposed in several applications such as for worstcase execution time see for survey power consumption prediction of cache behavior for timing analysis performance measures to name a few another important feature of quantitative objectives is that they are very for algorithms where algorithms generate answers quickly and proceed to construct better approximate solutions with refinements for a more elaborate discussion see meanpayoff and ratio objectives one of the most and elegant quantitative objectives is the meanpayoff objective where a weight is associated with every transition and the goal is to ensure that the average of the weights along a run is at least a given threshold for example consider a weight function that assigns to every transition the resource such as power consumption then the meanpayoff objective measures the average resource consumption along a run along with with meanpayoff objectives we also consider ratio objectives for ratio objectives the transitions events of the are labelled as good bad or neutral events and a positive weight function assigns a positive weight to every transition and the weight function represents how good or bad an event is the quantitative analysis problem if there is a run of the program such that the ratio of the sum of the weights of the good events versus the weights of the bad events in the is at least a given threshold for example consider a weight function that assigns weight to each transition and a labeling of events as follows whenever a request is made is a bad event whenever a request is pending is a good event and whenever no request is pending is a neutral event the ratio objective assigns the average time between requests and the corresponding grant per request for a run and measures of responses to requests finitestate systems or intraprocedural finitestate programs with meanpayoff objectives have been studied in the literature in for performance modeling and more recently applied in synthesis of reactive systems with quality guarantee and reliability requirements and resource bounds of reactive systems interprocedural quantitative analysis quantitative objectives such as meanpayoff and ratio objectives provide the appropriate framework to express several important system properties such as resource consumption and while finitestate systems with meanpayoff objectives have been studied in the literature the static analysis of with meanpayoff and ratio objectives has largely been ignored an interprocedural analysis is precise if it provides the solution a path is valid if it respects the fact that when a procedure it returns to the site of the most recent call in the quantitative setting the problem corresponds to finding the maximal value over all valid paths and to produce a witness symbolic path for that value in this work we consider precise interprocedural quantitative analysis for with meanpayoff and ratio objectives our contributions in this work we present a flexible and general modelling framework for quantitative analysis and show how it can be used to reason about quantitative properties of programs and about potential optimizations in the program we present an efficient polynomialtime algorithm for precise interprocedural quantitative analysis which is implemented as a tool we demonstrate the efficiency of the algorithm with two case studies and show that our approach to programs with of methods theoretical modeling we show that with mean and ratio objectives provide a robust framework that naturally captures a wide variety of static program analysis optimization and reasoning problems a detecting container usage an important problem for performance analysis is detection of runtime that significantly the performance and scalability of programs a common source of is inefficient use of containers we show that the problem of detecting usage of containers can be modeled as with ratio objectives a good use of a container corresponds to a good event and no use of the container is a bad event and a is represented as a low ratio of good vs bad events hence the container usage problem is naturally modeled as ratio analysis of while the problem of detecting container usage was already considered in our different approach has the following benefits see section for a comparison first our approach can handle recursion does not handle recursion second our approach is sound and does not yield false positives third the approach of ignored delete operations and we are able to take into consideration both add and delete operations thus provide a more refined analysis moreover our algorithmic approach for analysis of is polynomial whereas the algorithmic approach of in the worst case can be exponential b static profiling of programs we use our framework to model a conceptually new way for static profiling of pro grams for performance analysis a line in the code or a code segment is referred as hot if there exists a run of the program where the line of code is frequently executed for example a function is referred as hot if there exists a run of the program where the function is frequently invoked ie the frequency of calls to the function among all function calls is at least a given threshold again this problem is naturally modeled as ratio problem for and our approach statically detects methods that are more frequently invoked optimization of frequently executed code would naturally lead to performance improvements and reasoning about hot in the code can the to apply optimization such as function inlining and loop unrolling see subsection for more details c other applications we show the generality of our framework by that it provides an appropriate framework for theoretical modeling of applications such as interprocedural worstcase execution time analysis evaluating speedup in parallel computation and interprocedural average energy consumption analysis algorithmic analysis the quantitative analysis of with meanpayoff objectives can be achieved in polynomial time by a reduction to pushdown systems with meanpayoff objectives which can be solved in polynomial time however the resulting algorithm in the worst case has time complexity that is a polynomial of degree and space complexity that is a polynomial of degree six which is in practice we exploit the special theoretical properties of in order to improve the theoretical upper bound and get an algorithm that in the worst case runs in time and with quadratic space complexity in addition we exploit the properties of realworld programs and introduce optimizations that give a practical algorithm that is much faster than the theoretical upper bound when the relevant parameters the total number of entry exit call and returns nodes are small which is typical in most applications finally we present a linear reduction of the quantitative analysis problem with ratio objectives to meanpayoff objectives tool and experimental results we have implemented our algorithm and developed a tool in the java framework we show through two case studies that our approach to relatively large programs from wellknown benchmarks the details of the case studies are as follows a detecting container usage our experimental results show that our tool to relatively large benchmarks and discover relevant and useful information that can be used to optimize performance of the programs our tool could analyze all containers in several benchmarks like whereas could analyze them partially in only half of the containers were analyzed in before the time bound was our sound approach allows us to avoid false reports that were reported by and our simple mathematical modelling even allows us to report that were not reported by b static profiling of programs we run an analysis to detect hot methods for various our experimental results on the benchmarks report only a small fraction of the functions as hot for high threshold values and thus give useful information about potential functions to be optimized for performance gain in addition we perform a dynamic profiling and mark the top of the most frequently invoked functions as hot our experiments show a significant between the results of the static and dynamic analysis in addition we show that the sensitivity and of the static classification can be controlled by considering different where lower increase the sensitivity and higher increase the we in the tradeoff curve curve and demonstrate the prediction power of our approach thus we show that several conceptually different problems related to program optimizations are naturally modeled in our framework and demonstrate that we present a flexible and generic framework for quantitative analysis of programs moreover our case studies show that our tool to benchmarks with realworld programs definitions in this section we present formal definitions of interprocedural controlflow graphs and the quantitative analysis problems we will use an example program described in figure and figure to demonstrate each definition interprocedural controlflow graphs a pro gram p with m methods is modeled by an controlflow graph a which consists of a tuple a am of m modules where each module ai ni ex i i represents a method or the controlflow graph of a method in the program a module ai contains the following components · a finite set of nodes ni · an entry node which represents the first node of the method · an exit node ex i which represents termination of the method · a finite set that denotes the set of calls of the method and a finite set i that denotes the set of returns · a transition relation i defined as follows a transition in ai is either i between two nodes in the same module internal transitions or between a return node and a node in the same module ie u v such that u ni i and v ni or ii between a call node of a module ai and the entry node of a module aj which models the invocation of method j from method i or iii between the exit node of module ai and a return node of a module aj which models the case that method i terminated and the run of the program continues in method j which invoked method i we denote by n im ni and similarly en ex i calls im im and note all i in in a the sequel we use n resp ai and refer resp ni to the nodes of n calls en ex as internal nodes quantitative a quantitative for brevity consists of an a and a weight function w that assigns a weight we q to every transition e where q is the set of all example consider an example program shown in figure and figure in this example i modules a main a foo ii nodes n foo ret x iii entry and exit en and ex iv calls and returns calls foo and ret and v transition for example ret and ret configurations and paths a configuration consists of a sequence c r rj u where each ri is a return node ie ri and u n is a node in one of the modules intuitively when the module that u belongs to terminates the program will continue in rj a sequence of configurations is valid if it does not violate the transition relation and a path is a valid sequence of configurations we note that a path can be equivalently represented by the first configuration and a sequence of transitions for a path c c c we denote by i ni the node of configuration void main while x if y foo x else z z return i n t foo i n t x if x x return x ret foo z z figure main if x foo ret x figure foo ci ie ci r rj ni and ii i the stack string of ci ie i r rj for a path let i denote the prefix of length i of a run of the program is modeled by a path in a example consider the program and the corresponding shown in figure and figure an example of a run in the program modeled as a path in the is as follows if y foo ret ret if x ret foo ret ret where denotes the empty stack ratio analysis problem in this work we consider the ratio problem where every transition of a has a label from the set good bad neutral intuitively desirable events are la as good events are labelled as bad and other events are labelled as neutral the ratio analysis problem given a a labeling of the events and a threshold to determine whether there is a run where the ratio of sum of weights of good events vs the sum of weights of the bad events that is greater than the threshold formally we consider a positive function w that assigns a positive weight to every transition and for good and bad events the weight denotes how good or how bad the respective event is for a finite path we denote by good w resp bad w the sum of weights of the good resp bad events in in particular for the weight function w that assigns weight to every transition we have that good w resp bad w represents the number of good resp bad ratio of the events sum of we denote weights of good and bad good w the that in we have max bad w to remove the logical case of division by zero for an infinite path we denote inf i w i if has infinitely many good or bad events otherwise informally this represents the ratio as the number of relevant events goes to our analysis focuses on paths with unbounded number of relevant events and infinitely many events provide an elegant abstraction for hence we investigate the following problem given a with labeling of good bad and neutral events a positive integer weight function w and a threshold q such that determine whether there exists an infinite path such that remark our approach can be extended to reason about finite runs by a adding an auxiliary transition labeled as a neutral event from the final state of the program to its initial state also see section meanpayoff analysis problem in the meanpayoff analysis problem we consider a with a weight function w for a finite path in a we denote by w the total weight of the path ie the sum of the weights of the transitions in and by w the average of the weights where denotes the length of for an infinite path we de note i the mean analysis problem whether there exists an infinite path such that in section we show how the ratio analysis problem of reduces to the meanpayoff analysis problem of assigning and weights in our model the numerical weights are assigned to every transition of a first note that since we consider weight functions as an input and allow all weight functions the weights could be assigned in a dependent way second in general we can have an and a finitestate deterministic automaton such as a deterministic meanpayoff automaton that assigns weights the deterministic automaton can assign weights depending on different contexts or call strings of invocations or even independent of the context but dependent on the past few transitions ie ie the automaton has the stack alphabet and transition of the as input alphabet and assigns weights depending on the current state of the automaton and an input letter we call such a weight function regular weight function given a regular weight function specified by an automaton a and an we can obtain a that represents the weights by taking their synchronous product and hence we will focus on for algorithmic analysis the regular weight function can also be an abstraction of the real weight function eg the regular weight function is an overapproximation if the weights that it assigns to the good resp bad events are higher resp lower than the real weights if the original weight function is bounded then an overapproximation with a regular weight function can be obtained which can be refined to be more precise by allowing more states in the automaton of the regular weight function note that the new which is obtained from an and automaton a has a in the number of states of a and thus there is a tradeoff between the precision of a and the size of the new constructed applications theoretical modeling in this section we show that many problems for static analysis can be reduced to ratio analysis of we will present experimental results in section for the problems described in section and section container analysis the inefficient use of containers is the cause of many performance issues in java an of the problem with several practical is presented in the importance of accurate identification of of containers that and eliminates the number of false warnings was in and much effort was spent to avoid false warnings for realworld programs we show that the ratio analysis for provides a sound approach for the identification of inefficient use of containers two we aim to capture two common of containers following the definitions in the first inefficient use is an container that always holds very few number of elements the cause of is i a container is typically created with a default number of slots and much more memory is allocated than needed and ii the functionality that is associated with the container is typically not specialized to the case that it has only very few elements the second is caused by containers that are up though potentially they can have many elements this causes a memory and performance for every lookup thus we consider the following two cases of a container is if there exists a constant bound on the number of elements that it holds for all runs of the program for a threshold a container is if for all runs of the program the ratio of get vs add operations is less than we note that our approach is demanddriven where users can specify to check the of a specific container modeling the modeling of programs as is standard we describe how the weight function and the ratio analysis problem can model the problem of detecting we abstract the different container operations into get add and delete operations for this purpose we require the user to annotate the relevant class methods by get add or delete and by a weight function that corresponds to the number of get add or delete operations that the method does typically this number is for example in the class the add method is annotated by add the contains method is annotated by get and the remove operation is annotated by delete the clear operation which removes all elements from the set is annotated by delete but with a large weight if clear appears in a loop it dominates the add operations of the loop we note that the annotation can be automated with the approach that is described in when detecting containers we define add opera tions as good events and delete operations as bad events and check for threshold note that the relevant threshold is if the ratio of add vs delete is not greater than then the total number of elements in the container is bounded by a constant when detecting containers we define get operations as good events and add operations as bad events and check for the given threshold in addition since we wish to analyze heap objects the allocation of the container is a bad event with a large weight ie similar effect as of clear see example the container is iff the answer to the ratio analysis problem is no note that in the problem description for container analysis we have quantification over all paths and for ratio analysis of the quantification is existential the detection is demanddriven and done for an allocated container c details of modeling intuitively a transition in the call graph is good if it invokes a functionality that is annotated by a good operation ie add operation for the analysis and get operation for the analysis and the object that invokes the operation points to container c and it is bad if the invoked operation is annotated as bad formally for a given allocated container c if at a certain line a variable t that may point to c invokes a good functionality then we denote the transition as good if t must point to c and invokes a bad functionality then we denote it as a bad events all other transitions are neutral note that our modeling is conservative the is detected for the container c if all runs of the program have a ratio of good vs bad events that is below the threshold in other words the container is not if there exists a path where the ratio of good vs bad events is above the threshold and this exactly corresponds to the ratio analysis of example we illustrate some important aspects of the container analysis problem with an example consider the program shown in figure we consider the containers that are allocated in line and in line and analyze them for there exist runs that go through line and properly use the container that is allocated in line since method can add unbounded number of elements to the hash table due to its recursive call however the container in line is since in every run the number of elements is bounded by however note that if the delete operation is not handled then the container is reported as properly used we note that since we assign large weights to void e h i n t x h put x x i f x h x e bar i n t x r e t u r n new h a s h t a b l e xx v o i d f o o i n t x i f x h a s h t a b l e h b a r x h x e l s e f o r i n t y y x y h a s h t a b l e h new h a s h t a b l e y h p u t y x f o r i n t z z y z h p u t z y h remove z figure an example for container analysis the allocation of the container this prevents the analysis from that h properly uses the container that is allocated in line in summary the example illustrates the following important features the proper usage of the container should be tested also outside of its allocation site as opposed to the approach of sometimes the proper usage of a container is due to recursion and handling delete operations appropriately increases the precision of analysis while these important features are illustrated with the example such behaviors were also in the programs of the benchmarks see subsection for details soundness our ratio analysis approach for is both sound and complete with respect to the weighted abstracted since we use a conservative approach for assigning bad and good events the we obtain for the analysis of containers is sound and we get the following result theorem soundness the and container analysis through the ratio analysis problem on is sound do not report false positives ie any reported container is remark we remark about the of the soundness of our approach · the soundness criteria is a very important and desirable feature for container analysis for details see because a reported container needs to be analyzed manually and a substantial effort for optimization hence as argued in spurious warnings false positive of must be in our approach a is reported iff in every run a is detected and with a sound over approximation annotation of the weights our approach is sound · the soundness of our approach is with respect to a sound over approximation annotation of the add delete and get operations in addition for a given our ratio analysis is precise ie both sound and complete hence our container analysis is sound static profiling finding the most frequently executed lines in the code can help the programmer to identify the critical parts of the program and focus on the optimization of these parts it can also the compiler eg a c compiler to decide whether it should apply certain optimizations such as function inlining replacing a function call by the body of the called function and loop unrolling rewrite the loop as a repeated sequence of similar independent statements these optimizations can reduce the running time of the program but on the other hand they increase the size of the binary code hence knowing whether the function or loop is hot frequently eg an is allocated in bar function but the proper usage is done outside the allocation site namely after the termination of bar invoked is important when considering the time vs code size tradeoff in this subsection we present the model for profiling the frequency of function calls which allows finding hot functions and we note that our profiling technique is generic and can be to detect other hot in the code eg hot loops problem description given a program with several functions a function f is called hot if there exists a run of unbounded length of the program where the frequency of calls to f among all function calls in the run is at least formally for a run given a prefix of length i let f i denote the number of calls to f and ci denote the number of function calls in the prefix of length i the function is hot if there exists a run such that f i ci modeling the modeling of programs as is straightforward we describe the labeling of events and weight function in to determine if a function f is hot first we label to f as good events and assign weight then we label all other as bad events and assign them weight to ensure that the number of calls to f also appear in the in the total number of calls we label transitions from the entry node of f as bad events with weight the function f is hot iff the answer to the ratio analysis problem with threshold is yes worstcase execution time the approach of for worstcase execution time is also naturally captured by ratio analysis while the intraprocedural problem was considered in our approach allows the more general interprocedural analysis in this approach we consider as in that each program statement is assigned a cost that corresponds to its running time eg number of cpu cycles modeling the modelling of analysis of the program is as follows we add to the of the program a transition from every terminal node to the initial node and every such transition is a bad event with weight all the other transitions are good events and their weight is their cost running time the of the program is at most n cycles if and only if the answer to the ratio analysis problem with threshold n is no evaluating the speedup in a parallel computation the speed of a parallel computing is limited by the time needed for the sequential fraction of the program for example if a program runs for on a single processor core and a certain part of the program that takes to execute cannot be then the minimum execution time cannot be less than two regardless of how many processors are to a execution of this program hence the speedup is at most law states that the theoretical speedup that can be obtained by executing a given algorithm on a system capable of executing n the fraction threads of execution of the algorithm that is is at most strictly ra technique can be used to conservatively estimate the value of b and thus to evaluate the outcome of law modeling as in section we consider that the cost of every program statement is given and we add to the of the pro gram a transition from every terminal node to the initial node this time as a neutral event with weight all the transitions of the code that cannot be are defined as good events and the other transitions are defined as bad events we denote by p the fraction of the code that can be and by s the fraction of the code is at most if and only if the with threshold is no hence for which is no the answer to the ratio analysis average energy consumption in the case of many consumer devices especially mobile battery is restricted due to constraints on size and weight of the device this implies that energy well is in such devices since most mobile applications are nonterminating eg a web browser the most important metric for energy consumption is the average memory consumption per time unit eg per second modeling we consider that the running time and energy consumption of each statement in the application code is given or is approximated in our modeling we split each transition in the into two consecutive transitions the first is a good event and the next is a bad event the good event is assigned with a weight that corresponds to the energy consumption of the program statement and the bad event is assigned with a weight that corresponds to the running time of the statement the average energy consumption of the application is at most if and only if the answer to the ratio analysis problem is no algorithm for quantitative analysis of s in this section we present three results the meanpayoff analysis problem for can be solved in polynomial time this can be derived from first we present an algorithm that significantly improves the current theoretical bound for the problem for second we present an efficient algorithm that in most practical cases is much faster as compared to the theoretical upper bound finally we present a linear reduction of the ratio analysis problem to the meanpayoff analysis problem for improved algorithm for meanpayoff analysis in this section we first discuss the basic polynomialtime algorithm for meanpayoff analysis of that can be obtained from the results on pushdown systems shown in due to space constraints the technical proofs are to the material results of and reduction the results of show that pushdown systems with meanpayoff objectives can be solved in polynomial time given a pushdown system with state space q and stack alphabet the polynomialtime algorithm of can be described as follows the algorithm is iterative and in each iteration it constructs a finite graph of size · and runs a style algorithm on the finite graph from each vertex the algorithm on the finite graph from all vertices in each iteration requires · time and · space the number of iterations required is · thus the time and space requirement of the algorithm are · and · respectively a can be interpreted as a pushdown system where n corresponds to q and corresponds to theorem basic algorithm the meanpayoff analysis problem for can be solved in on · time and on · space respectively improved algorithm we will present an improved polynomialtime algorithm for the meanpayoff analysis of the improvement relies on the following properties of the transitions of a module are independent of the stack of a configuration while in pushdown systems the transitions can depend on the top symbol of the stack this enables to reduce the size of the finite graphs to be considered in every iteration every call node has only one corresponding return node therefore if a module a invokes a module a then the behavior of a after the termination of a is independent of a this enables us to reduce the number of iterations to to present the improved algorithm and its correctness formally we need a refined analysis and extensions of the results of we first describe a key aspect and present an overview of the solution remark lattice our algorithm will be an iterative algorithm some fixpoint is reached however for interprocedural analysis with lattices fixpoints are guaranteed to exist unfortunately in our case for meanpayoff objectives it is an lattice thus a fixpoint is not guaranteed for this reason the analysis for meanpayoff objectives is more involved and this is even in the case of finite graphs for example for reachability objectives in finite graphs algorithms exist whereas for finite graphs with meanpayoff objectives the algorithms for over three are quadratic solution overview in finite graphs the solution for the meanpayoff analysis is to check whether the graph has a cycle c such that the sum of weights of c is positive if such a cycle exists then a path that leads to the cycle and then follows the cyclic path forever has positive meanpayoff value for we show that it is enough to find either a loop in the program such that the sum of weights of the loop is positive or a sequence of calls and returns with positive total weight such that the last invoked module is the same as the first invoked module for this purpose we compute a summary function that finds the maximum weight according to the sum of weights path between every two statements of a method ie between every two nodes of a module the computation is an extension of the algorithm to we show that it is enough to compute a summary function for with a stack height that is bounded by some constant and then all that is left is to mark pairs of nodes such that the weight of a maximal weight path between them is unbounded in finite graphs the maximum weight between two vertices is unbounded only if the graph has a cycle with positive sum of weights ie a path with positive total weight that can be for it is also possible to special types of acyclic paths we first characterize these paths up to lemma we then show how to compute a bounded summary function lemma and the that follows it and example finally we show how to use the summary function to solve the meanpayoff analysis problem we start with the basic notions related to stack heights and paths and their properties which are crucial for the algorithm stack heights the configuration stack height of c r rj u denoted as is j for a finite path n n the stack height of the path denoted by sh is the maximal stack height of all the configurations in the path formally sh max the additional stack height of is the additional height of the stack in the segment of the path ie the additional stack height is sh max pair of paths let be a finite or infinite path where each ti is a transition in the a pair of paths for is a pair of nonempty sequences of transitions p p i i fo r the path a obtained by the pair p and p valid path ie for every j we have of paths j times ct ti ti is a valid path we illustrate the above definitions with the next example example consider the program from figure and figure and the corresponding a possible path in the program is foo foo x ret x ret and we denote this path with then and the pair of paths foo and ret x is a pair of paths in the next lemmas we first show that every path with large additional stack has a pair of paths and then establish the connection of additional stack height and the existence of pair of paths with positive weights in lemma the key intuition for the proof of the next lemma is that a path with calls must contain a recursive call that can be lemma let be a finite path with d calls then has a pair of paths lemma let c c be two configurations and j z let d n be the minimal additional stack height of all paths between c and c with total weight at least j if d calls then there exists a path from c to c with additional stack height d that has a pair p p with wp wp proof let us consider the set of paths between c and c with total weight at least j and let min be the subset of that has minimal additional stack height the proof is by induction on the length of paths in min consider a path from min that has the shortest length among all paths in min since d calls then by lemma it contains a pair let us consider the path obtained from by the pair zero times ie the pair is removed since we remove a part of the path we have that if w w then we obtain a path with weight at least j with either smaller additional stack height than or of length that is the shortest length minimal additional stack height path with weight at least j hence we must have w w and hence the pair has positive weight now for an arbitrary path in min we obtain that it has a pair either the pair has positive weight and we are done else removing the pair we obtain a length path of the same stack height and the result follows by inductive hypothesis on the length of paths our algorithm for the meanpayoff analysis problem is based on detecting the existence of certain paths with positive weight the maximal weights of such paths between node pairs are captured with the notion of a summary function and bounded summary functions with bounded additional stack height we now define them and establish the lemma related to the number of bounded summary functions to be computed local minimum and paths a configuration ci in a path c c is a local minimum if the stack height of ci is minimal in ie i min a path from configuration n to n is a path if n is a local minimum note that if a sequence of transitions is a path for some then the same sequence of transitions is a path for every hence we say that is a path if there exists such that is a path summary function given the a and we define a summary function s mn × n z as sn n z z iff the weight of the maximum weight path from configuration n to configuration n is z sn n iff for all j n there exists a path from n to n with weight at least j sn n iff there is no path from n to n we note that for every it holds that s s hence we consider only s s where is the empty string and corresponds to empty stack the computation of the summary function is done by considering stack height bounded summary functions defined below stack height bounded summary function for every d n the stack height bounded summary function sd mn × n z is defined as follows i sdn n z z iff the weight of the maximum weight path from n to n with additional stack height at most d is z ii sdn n iff for all j n there exists a path from n to n with weight at least j and additional stack height at most d and iii sdn n iff there is no path with additional stack height at most d from n to n facts of summary functions we have the following facts i for every d n we have sd sd monotonicity and ii sd is computable from sd and the by the above facts we get that if sd sd ie if a fix point is reached then s sd for interprocedural analysis with lattices fix points are guaranteed to exist unfortunately in our case the image of si is infinite and moreover it is an lattice thus a fix point is not guaranteed the next lemma shows that we can compute all the non values of s with the bounded summary function lemma let d calls for all n n n if sn n z then sn n sdn n by lemma we get that if sdn n sdn n for d calls then sn n hence the summary function s is obtained by the fix point of the following computation i compute si from si up to sd for d calls ii for i calls if sin n sin n then set sin n iii a fix point is reached after at most iterations say j iterations and then we set s sj this establishes that we require only iterations as compared to on · iterations the number of returns and calls are the same and thus we significantly improve the number of iterations required from the worstcase bound to linear bound we now describe the computation of every iteration to obtain si from si computation of si from si we first compute a partial func tion namely si en × ex z that satisfies sin n sin n for every n en and n ex we initialize sn n sn n for every module a we construct a finite graph of a and by adding a corresponding return node for every transition between a pair of nodes n n n calls we assign the weight according to the original weight in a for every transition between a call node that invokes module ap and a corresponding return node we assign the weight ex p to compute si for module n we have that the weight of the maximum weight path be is exactly i si n n the proof is by · si then si si follows from the first key hence to compute s we compute si from si until we get si si and then we compute all pairs maximum weight paths eg si time complexity and quadratic space complexity therefore the time complexity for computing every iteration of si is o n and the complexity of the last step is o n the space complexity of the last step is nm but to store si we require o n space summary graph given a with a summary function s we construct the summary graph v e of a with a weight function w e z as follows i v n ex and ii e where n n n n n for some and sn n contains the transitions in the same module and n n n calls and n en and n is a call to a module with entry node n contains the call transitions the weights of are according to the summary function s and the weights of are according to the weights of these transitions in a ie according to w a simple cycle in is a positive simple cycle iff one of the following conditions hold i the cycle contains an edge or ii the sum of the weights of the cycles according to the weights of the summary graph is positive lemma shows the equivalence of the meanpayoff analysis problem and positive cycles in the summary graph lemma a a has a path with iff the summary graph has a reachable positive cycle proof if does not contain a positive cycle then it follows that the weight of every path in a is bounded by the weight of the maximum weight path in hence for every infinite path we get that every prefix of is a path from the initial configuration with bounded weight sum of weights bounded from above and therefore conversely if has a positive cycle then it follows that there is a path in such that and are paths begins and ends in the same node possibly at higher stack height and w hence the path is a valid path and where the finite path the desired result follows algorithm and analysis algorithm solves the meanpayoff analysis problem for the computation of the summary function requires computations of the partial summary function si which requires m runs of algorithm each run over a graph of size n hence each run takes on time in addition the computation requires m runs of all pairs maximum weight path algorithm each run is over a graph of size on hence each run takes on time and on space finally we detect positive cycles by running algorithm once over the summary graph which takes on time and on space thus we obtain the following result theorem improved algorithm algorithm solves the meanpayoff analysis problem for in o calls · n n n time and o n space remark note that in the worst case the running time of algorithm is and the space requirement is quadratic the next example is an illustration of a run of algorithm example consider the that consists of modules f and g figures and and the entry of f is the initial entry of the program we now describe the run of algorithm over the for simplicity we denote the graph of f by f and the graph of g by g and not by g and g note that the number of call nodes is we first compute the summary function s and the first step is to compute s we have s and we construct node g a graph f to the node g with weight and find the maximum weight algorithm meanpayoff analysis for to m do compute s by i loop for to m do construct gi according to si si en ex compute si by running end for algorithm over gi if then end if if i calls then for to m do if ex ex then ex end for end if i i end loop s construct from l l si s run over if has a positive cycle then return yes else return no end if g fg ret fv figure module f g g f f figure module g g fv g figure summary graph of f and g f path from to in f we get s transition from and find the maximum f to f weight path with from to in g we get s since s s we continue to compute s we construct f and g in the same manner as we constructed f and g but take the values of s instead of s and get s s for i we get s s for i s s for i we get s and ss since i s and calls and s we assign assign s and in the iteration we get a fix point that is s s and exit the loop block from f and g we compute the summary function s for example and finally we construct the summary graph see figure and check whether a positive cycle exists the cycle contains an edge and thus it is a positive cycle hence algorithm returns yes efficient algorithm for meanpayoff analysis in this section we further improve the algorithm for the mean analysis problem for and the improvement de on the fact that typically the number of entry exit call and returns nodes is much smaller than the size of the for in most typical cases we have ex calls en n let x we present an ex en improvement that enables calls us to and x construct the function over graphs of size ox instead of graphs of size on of section and with at most iter hence the algorithm in most typical cases will be much faster and require much smaller space compact representation the key idea for the improvement is to represent the modules in compact form the compact form of a module a denoted by consists of the entry exit call and returns node of a there is transition between every node in and the weight of each transition is the maximum weight path between the nodes with additional stack height and if there is no such path then the weight is formally v e where v x e v × v and v sv v where s is the bounded height summary function of height if in there is a cycle with positive weight that is reachable from the entry node then we say that a is a positive meanpayoff witness the computation of the compact form for a module a requires ox · n time and on space running on each a and thus the compact form for all modules can be computed in o x · n time and n space note that the space can be reused witness in summary graph of compact forms after constructing the compact forms we compute a summary function for and a corresponding summary graph we say that there is a path with positive meanpayoff iff there exists a positive cycle in the summary graph or there exists a path to the entry node of a positive meanpayoff witness the correctness of the algorithm relies on the next lemma lemma let a a am be a let be its summary graph and let be the summary graph that is formed by the following assertions are equivalent has a reachable positive cycle has a reachable positive cycle or a positive meanpayoff witness the above lemma establishes the correctness of the computation on compact form graphs and gives us the following result the following result is obtained from theorem by replacing n with x and n by x and the additional x · n time and max n space are required for the compact form computation theorem efficient algorithm the meanpayoff analysis problem for can be solved in o calls · x x x x · n time and o x max n space where x ex en calls and x x reduction ratio analysis to meanpayoff analysis we now establish a linear reduction of the ratio analysis problem to the meanpayoff analysis problem given a a with labeling of good bad and neutral events a positive integer weight function w and rational threshold the reduction of the ratio analysis problem to the meanpayoff analysis problem is as follows we consider a a with weight function w for the meanpayoff objective defined as follows for a transition e we have we if e is labelled with good we · we if e is labelled with bad otherwise if e is labelled with neutral the next lemma establishes the correctness of the reduction lemma given a a with labeling of good bad and neutral events a positive integer weight function w and rational threshold let a be the with weight function w there exists a path in a with iff there exists a path in a with remark note that in our reduction from ratio analysis to meanpayoff analysis we do not change the but only change the weight function thus our algorithms from theorem and theorem can also solve the ratio analysis problem for moreover our proof of lemma shows that for all paths if we have then we also have ie any witness for the meanpayoff analysis is also a witness for the ratio analysis experimental results two case studies in this section we present our experimental results on two case studies described in section and section we run our case studies on several benchmarks in java including benchmarks and we use to for the construction of the controlflow graphs first we present some optimizations that proved useful for speedup in the benchmarks optimization for case studies we present four optimizations for the case studies the first two are general and the last two are specific to our case studies faster computation of stack height bounded summary func tion we note that if module a invokes only modules aj h ex ex for all hence set li iteration we algorithm only for the modules that invoke modules from li reducing the number of iterations for fix point we now present an optimization that allows us to reduce the number of bounded height summary functions from to a practically constant number we note that the theoretical bound is tight however only cases can reach even a fraction of this bound we note that in typical programs the average nest ing of function calls is practically constant say so if we do not get a fix point after iterations ie s s then it is probably because there is a recursive call with positive weight if this is the case then if we build the summary graph according to s we will get a positive cycle in the summary graph that is we will get a witness for a path with a positive meanpayoff and we can stop the computation since by definition s s we get that this witness is valid hence our optimized algorithm is to com the bounded i height then for a witness path if a path is found then we are done otherwise we continue and compute si removing redundant modules consider an a a am in which every node is reachable from the program entry the entry node of the main method we say that module ai is if i the module has nonzero weight transitions good or bad events or ii it invokes a module and is called redundant otherwise let ai be a redundant module for every path that contains a transition to an invocation of ai the segment of between that transition and the first transition to contains only neutral transitions because all nodes of a are reachable we can safely replace each call node that invokes ai by an internal node that leads to the corresponding return node and label it as a neutral event our optimization then consists of removing redundant modules as follows first we perform a interprocedural reachability from the program entry which requires linear time and discard all nodes in all modules then we perform a backwards reachability computation on the call graph of a starting from the set of all modules that contain nonzero weight transitions all detected redundant modules are discarded and calls to them are replaced according to the above description hence when computing the bounded height summary function the size of the graph is smaller and the algorithm takes less time additionally the number of calls calls decreases which reduces the number of iterations required in the main loop of algorithm in the first case study typically more than half of the methods are eliminated in this process incremental computation of summary functions we present the final optimization which is relevant for our second case study let a be a and let a be a that is obtained from a only by increasing some of the transitions weights let s be the summary function of a then we can compute the summary function of a by setting s s and by computing si from si in the usual way the correctness is almost trivial since the weights of a are at least as the weights of a we get that if we conceptually add a transition n n with weight sn n for every two nodes in the same module in a then the weights of the paths with the maximal weight in a remain we only add such conceptual follows we now describe how this optimization speed up the analysis of the second case study in the static profiling for function we need to build a summary graph for every function f and then run the meanpayoff analysis for every such graph given this optimization we can first compute only once a summary graph for the case that all method invocations are bad events we denote this by a and the corresponding summary function by s note that in a all weights are negative and the meanpayoff analysis answer is trivially no but still the summary function computation which computes the quantitative information about the maximum weight contextfree paths provides useful information and to determine the frequency of f we assign weights to a and get af and the difference between af and a is only in the weight that is assigned to the invocation of f we then compute the summary function sf for af programs can have s in practical cases but only small portion of them will have a path to f so along with the previous optimizations we get that only few runs are required to compute sf overall the computation of s is expensive and may take several for a large program but it is done only once and then the computation of each sf is much faster container analysis technical details about experimental results we discuss a few relevant details about our experiments and results · we use the pointsto analysis tool of this tool provides interprocedural on demand analysis for a relationship of two variables we say that a variable may point to an allocated container if it the container and a variable must point to an allocated container if it only one allocated container · for the containers the threshold is and for the analysis of containers we set a threshold of for our experimental results that is if the ratio between the number of added elements to the number of lookup operations is more than then the container is experimental results our experimental results on the benchmarks are reported in table in the table m and co represent the number of methods and containers that are reachable from the main entry of the program respectively op and uc represent the number of and containers discovered by our tool respectively and and represent the time required for alias analysis and the time required for the quantitative analysis of in seconds respectively and the entries of the respective columns represent the time for container analysis we now some interesting aspects of our experimental results first our approach for container analysis containers that are or while maintaining soundness second the cases that we identify reveal useful information for optimization for example in the first and the second benchmarks we identify containers that always have a small bounded number of elements benchmark java m co op uc table experimental results for container usage analysis comparison with the work of while the notion of and containers is the same in our work and in the concrete mathematical definition is different · conceptual difference in modeling the formal definitions in rely on properties of a specifically constructed inequality graph that in practice gives a good approximation on whether a container is properly used for details see definition in our formal definition is conceptually simpler and relies on a very general and flexible mathematical framework but on the benchmark m i t table experimental results for frequency of functions public void i f l i s t n u l l else object o remove id i f o t hi s not us put id o public void run while true if rc figure an example from benchmark the method run invokes in a loop and in every invocation one element of is removed and one element is added thus in this loop the total number of elements in is bounded figure the the left shows the results when all methods are analyzed the right shows the results when only the active methods are analyzed other hand it relies on the accuracy of the constructed for example our technique may not report a container as even if the witness path for proper is very complex eg has nested function calls and therefore it is to be realizable on practice · advantages and our approach has several advantages first our approach can handle recursion whereas does not handle recursion second we present a sound and complete approach for ratio analysis of and with a conservative modeling we have a sound analysis approach for detecting containers third our algorithm is polynomial time once the pointsto relation is computed whereas the algorithmic approach of in the worst case is exponential finally our approach also allows us to handle delete operations in loops with only add operations in add vs delete were identified as proper usage whereas we identify loops where the difference of the add and delete operations is positive as proper usage this subsumes the loops of and for example also loops with two add operations and one delete operation example illustrates the advantages of our approach one of our approach is that it is conservative while for containers analysis add vs delete our approach captures all cases of as explained above our approach for containers analysis is more conservative to obtain soundness · comparison of experimental outcomes with our approach we were able to fully analyze all the containers in all benchmarks whereas in the analysis for few benchmarks eg was not done for all containers since a timeout was reached below we present example of code from the benchmarks where our analysis gives different results from the analysis of the example in figure shows that handling delete operations leads to more refined analysis in the example if delete operations are not handled then the is not detected the example in figure shows that the proper of containers might depend on the recursive calls finally the example in figure illustrates that the proper use of containers can be outside its allocation site v o i d i f b a c k t r a c e n u l l b a c k t r a c e new v o r b a c k t r a c e i n s er t el em en t a t p u b l i c s e t r e s p o n s e f i n d r e c o r d s name name s h o r t t y p e i f t y p e type t y p e type any r r s e t g e t t y p e type z r f i n d r e c o r d s g e t t a r g e t t y p e z r r e t u r n z r figure an example from benchmark the method has a recursive call and method adds an element to vector a path with recursion depth n adds n elements to hence may have unbounded number of elements and it is not request request h a s h t a b l e a t t r s new h a s h t a b l e if query null s t r i n g t o k e n i z e r s t new s t r i n g t o k e n i z e r d e c o d e q u e r y while st a t t r s p u t key v a l u e r e t u r n a t t r s p u b l i c repl y r e c v r e p l y r e q u e s t r e q u e s t e l s e i f r e q u e s t g e t p a t h e q u a l s s e t h a s h t a b l e a t t r s c g i r e q u e s t f o r i n t i i e n a b l e d s i z e i p r e f s p u t key s t r i n g a t t r s g e t key r e t u r n r e p l y figure an example from benchmark the method allocates the container and potentially adds it many elements the method performs a get operation over in a loop since we analyze not only the operations that are nested in the allocation site we detect that is not the analysis of reports it as static profiling frequency of function calls experimental results we examined ten namely and for each threshold we say that a method is statically hot if it is hot according to the definition in subsection we compared the results to dynamic profiling from the benchmarks in the dynamic profiling we define the top of the most frequently invoked functions as dynamically hot for example if a program has functions and in the benchmark functions were invoked at least once then the most frequently invoked functions are dynamically hot we note that speaking the definitions of dynamic and static are incomparable basically for any but our experimental results show a good between the two notions to illustrate the we treat our static analysis as a classifier of hot methods and the and sensitivity of the classifier are controlled by the threshold the sensitivity of a classifier is measured by the true positive rate which is dynamically hot methods that are reported as statically hot dynamically hot methods the is determined by the false positive rate hot methods that are reported as statically hot methods for high values of the classifier is expected to capture only dynamically hot methods but it will miss most of the dynamically hot methods and thus it will have very high but very low for very low values of the classifier will report most of the methods as hot so most of the hot methods will be reported as hot and we will have very high but very low a fundamental metric for classifier evaluation is a receiver operating characteristic graph a graph is a with the false positive rate on the x axis and the true positive rate on the y axis the point is the perfect classifier and the area an curve can be used as a measure of accuracy of the classifier in our experimental evaluation we only considered application functions and not library functions and the results are presented in table in the table m represents the number of application methods that are reachable from the main entry of the program i represents the number of application methods that were actually invoked in the benchmark t represents the average running time for the static analysis of a single method ie to check whether a single method is hot for a fixed in seconds for each we present the and values of the we present an evaluation for two cases in the first case we statically analyze all the methods and calculate the and accordingly in the second case we consider only the active methods namely the methods that were invoked at least once in the program and we remove all the other methods from the program control flow graph this simulates a typical case where the programmer has prior knowledge on methods that are definitely not hot and can the static analysis to ignore them the are presented in figure where the most left points on the graph are for and the and increases as decreases until it finally reaches in general for most of the programs the static analysis gives useful and quite accurate information specifically the threshold captures more than half of the hot methods for most benchmarks ie except and and with a less than which means that if a method was statically reported as not hot then with probability it is really not hot we note that the analysis over gives quite results because only of the methods were active however when we analyzed only the active methods we get better results for see the right hand graph in figure when we only consider the active methods the threshold captures most of the dynamically hot methods while maintaining a less than for most programs remarks we run the experiments on a single thread intel for table results the alias analysis did not complete for some benchmarks eg in table we only show benchmarks for which we to obtain dynamic profiles for a few benchmarks eg the quantitative analysis took too long for the entire benchmark in such cases our tool could be used to focus on specific functions related work interprocedural analysis algorithms that operate on the interprocedural controlflow graphs provide the framework for static analysis of programs and have numerous applications precise interprocedural analysis is crucial for dataflow analysis and has been studied in several works the study of interprocedural analysis has also been extended to weighted pushdown systems where the weight domain is a bounded idempotent semiring analysis of such weighted pushdown systems has been used in many applications of program analysis our work is different because the objectives meanpayoff and ratio analysis we consider are very different from reachability and bounded domains the meanpayoff objective is a function that assigns a number to every path in contrast to bounded domain functions the range of a meanpayoff function is potentially we develop novel techniques to extend the summary graph approach for lattices to solve meanpayoff analysis of which requires computing fix points for lattices meanpayoff analysis meanpayoff objectives are quantitative metrics for performance modeling in many applications and very in the context of finitestate graphs and games finitestate graphs and games with meanpayoff objectives have been studied in for performance modeling and robust synthesis of reactive systems quantitative frameworks for finitestate systems with meanpayoff objectives have also been studied in while the meanpayoff objectives have been considered in depth for finitestate systems they have not been considered in depth for interprocedural analysis pushdown systems with meanpayoff objectives were considered in we significantly improve the complexity of the polynomialtime algorithm for interprocedural meanpayoff analysis that can be obtained by a reduction to the results of detecting containers detection and detecting used containers have been identified in many previous works as a major reason for program dynamic approaches for the problem were studied in many works such as a static approach to analyze the problem was first considered in which is the most closely related work to our case study the work of provides an of the problem with several practical it also describes the clear advantages of the static analysis tools and identifies that soundness in detecting used containers with no or low false positive is a very important feature our approach for the problem is significantly different from the approach of a big part of the contribution of is an automated annotation for the functionality of the containers operation the main algorithmic approach of is to use contextfree reachability to identify nesting loop and then use this information for detecting of containers our algorithmic approach is very different we use a quantitative analysis approach ie ratio analysis of to model the problem static profiling of programs static and dynamic profiling of programs is in the of program optimization static profiling are typically used in branch where the goal is to assign probabilities to branches and typically require some prior knowledge on the probability of inputs static profiling of programs for branch has been considered in dynamic profiling has also been used in many applications related to optimizations see for a collection of dynamic profiling tools two main of dynamic profiling are that they require inputs and they cannot be used for compiler optimizations we use static profiling to determine if a function is invoked frequently along some run of the program and do not require any prior knowledge on inputs the techniques used in involves solving linear equations with sparse matrix solvers whereas our solution method is different by quantitative analysis of conclusion in this work we considered the quantitative ratio and meanpayoff analysis for interprocedural programs we demonstrated how interprocedural quantitative analysis can aid to automatically reason about properties of programs and potential program optimizations we significantly improved the theoretical known for the polynomialtime solution and presented several practical optimizations that proved to be useful in real programs we have implemented the algorithm in java and showed that it to benchmarks of realworld programs this shows that interprocedural quantitative analysis is feasible and useful some possible directions of future works are as follows extend our framework with multiple quantitative objectives and study their applications and extend to have an framework for quantitative interprocedural analysis acknowledgments we thank sagiv for pointing us to reference for discussions on c optimizations and anonymous reviewers for their helpful comments references g m validity of the single processor approach to achieving large scale computing capabilities in proceedings of the april joint computer conference pages ­ acm t ball and j r branch prediction for free in pldi pages ­ h s and s a combinatorial strongly strategy improvement algorithm for mean games in pages ­ s m r c a m k s r a d d s z m a m jump h lee j e b a d t d von and b the benchmarks java development and analysis sigplan not ­ r k t a henzinger and b better quality in synthesis through quantitative objectives in cav pages ­ r k t a henzinger and b synthesizing robust systems in pages ­ e a j m and h reflection static analysis in the presence of reflection and custom class in pages ­ acm m s problem solving using dynamic programming in pages ­ u k t a henzinger and o temporal specifications with values in lics a j and t a generic approach to the static analysis of concurrent programs with procedures in popl a and g an analysis of power consumption in a in usenix p t a henzinger and a quantitative abstraction refinement in popl pages ­ k l and t a henzinger quantitative languages acm trans comput log k and y meanpayoff pushdown games in lics pages ­ t h c e r l and c introduction to algorithms the mit press rd edition p cousot and r cousot abstract interpretation a unified lattice model for static analysis of programs by construction or approximation of fixpoints in popl pages ­ m and i describing average and by weighted mso logics in pages ­ b b g ryder and g a scalable technique for characterizing the usage of temporaries in java applications in pages ­ a and j strategies for mean games int journal of game theory ­ c f martin r and m cache behavior prediction by abstract interpretation sci comput program j and k markov decision processes springerverlag j l hennessy and d a computer architecture fourth edition a quantitative approach morgan r a characterization of the minimum cycle mean in a discrete mathematics ­ a t w reps and g extended weighted pushdown systems in cav pages ­ o and l j scaling java pointsto analysis using in cc pages ­ t a and s a stochastic games with perfect information and time average siam review ­ n mitchell and g the causes of the limits of in oopsla pages ­ n mitchell g and h modeling runtime behavior in applications in ecoop m and h precise interprocedural analysis through linear algebra in popl pages ­ g e d and b g efficiently and precisely memory leaks and in pldi pages ­ t w reps s horwitz and s sagiv precise interprocedural dataflow analysis via graph reachability in popl pages ­ t w reps a and n program analysis using weighted pushdown systems in pages ­ t w reps s s and d weighted pushdown systems and their application to interprocedural dataflow analysis sci comput program ­ o m t and e adaptive selection of collections in pldi pages ­ a m and r lightweight dynamic analysis and removal of object in oopsla m and r contextsensitive pointsto analysis for java in pldi pages ­ v s and a power analysis of embedded software a first step towards software power minimization ieee trans vlsi syst ­ r p co e l p lam and v a java bytecode optimization framework in t a v s l graham and m a accurate static for program optimization in pldi list of performance analysis tools r j a n s d b g c r t f i p p j and p the worstcase problem overview of methods and survey of tools acm trans embedded comput syst y wu and j r static branch frequency and program profile analysis in pages ­ acm y m b and a aiken soundness and its role in bug detection systems in proc of the workshop on the evaluation of software detection tools g h xu n mitchell m a e and g finding data structures in pldi g h xu and a detecting containers to avoid in pldi pages ­ u and m the complexity of mean games on graphs theoretical computer science ­ 