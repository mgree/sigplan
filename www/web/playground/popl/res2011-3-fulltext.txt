learning minimal abstractions uc berkeley university intel berkeley abstract static analyses are generally parametrized by an abstraction which is chosen from a family of abstractions we are interested in flexible families of abstractions with many parameters as these families can allow one to increase precision in ways to the client without scalability for example we consider pointsto analyses where each call site and allocation site in a program can have a different k value we then ask a natural question in this paper what is the minimal abstraction in a given family which is able to prove a set of client queries in addressing this question we make the following two contributions i we introduce two machine learning algorithms for efficiently finding a minimal abstraction and ii for a static race by a pointsto analysis we show that minimal abstractions are actually quite it suffices to provide sensitivity to a very small fraction ­ of the sites to yield equally precise results as providing sensitivity uniformly to all sites categories and subject descriptors d software engineering verification general terms measurement verification keywords heap abstractions static analysis concurrency machine learning introduction static analyses typically have parameters that control the tradeoff between precision and scalability for example in a or pointsto analysis ­ the parameter is the k value which determines the amount of context sensitivity and object sensitivity increasing k yields more precise pointsto information permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm but the complexity of the analysis also grows exponentially with k shape analysis and model checkers based on predicate abstraction are parametrized by some number of predicates these analyses also exhibit this tradeoff in many analyses these tradeoffs are controlled by a small number of parameters for instance a single k value past studies eg and demanddriven approaches have shown that it is often not necessary to provide context sensitivity to each call site or object sensitivity to each allocation site this working with a larger family of abstractions parametrized by a separate k value for each site to the parametric framework of et al more generally we represent an abstraction as a binary vector eg component j of the vector specifies whether site j should be treated but how much sensitivity is needed and where is it needed in this paper we formulate and the following problem given a family of abstractions find a minimal abstraction sufficient to prove all the queries provable by the abstraction in the family studying this problem is important for two reasons i a minimal abstraction provides insight into which aspects of a program need to be modeled precisely for a given client and ii reducing the complexity of the abstraction along some components could enable us to increase the complexity of the abstraction along other components more than before for example keeping the k values of most sites at zero enables us to use higher k values for a select subset of sites to find these minimal abstractions we introduce two machine learning algorithms both treat the static analysis as a black box which takes an abstraction and a set of client queries as input and produces the set of proven queries as output the first algorithm starts with the abstraction runs the static analysis on randomly chosen abstractions and from these training examples detects statistical between components of the abstraction and whether a query is proven components highly with proven queries are added section the second algorithm starts with the abstraction and samples abstractions at random incrementally reducing the abstraction to a minimal one section we also provide a theoretical analysis of our algorithms let p be the number of components of the abstraction family and s be the number of components in the largest minimal abstraction we show that although the number of abstractions considered is exponential in p we only need os log p calls to the static analysis to find a minimal abstraction the of this result is that when a very small abstraction suffices to prove the queries s p our algorithms are much more efficient than a approach which would require op calls we found that minimal abstractions are indeed very small this is an instance of an important property in machine learning and statistics that very few components of an unknown vector are nonzero our approach represents a significant from traditional program analysis where iterative refinement techniques are the norm in particular our methods exploit to generate information in the form of statistical also note that iterative refinement is in general not guaranteed to find a minimal abstraction whereas our techniques do have this guarantee we present one iterative refinement technique in section and show that it refines ­ of the components whereas the minimal abstractions found by our approach only refine ­ section all our empirical results are for a static race detection client by a pointsto analysis pointsto information is used extensively by the race to determine which statements may be reachable which statements may access the same memory locations which statements may access memory locations and which statements may happen in parallel our pointsto analysis is both contextsensitive and see section for details problem formulation let a be a poset corresponding to a family of abstractions and let q be a set of queries that we would like to prove we assume we have access to a static analysis f a q which maps an abstraction a a to a binary vector fa where component q is if query q is proven and if it is not we assume that f is monotone with respect to the abstraction family a that is if a a then fa fa in other words refining an abstraction increasing a can only enable us to prove more queries decrease fa the focus of this paper is on the following problem definition minimal abstraction problem given a family of abstractions a with a unique top element the most precise output an abstraction a such that a is is fa f and a is is such that a a fa fa a in general there could be multiple minimal abstractions but we are content with choosing any one of them furthermore we would like to find a minimal abstraction efficiently ie the number of calls to f binary abstractions we now specialize to abstractions representable by binary vectors that is a j for some index set of components j where the partial order is inequality a a iff aj aj for all j j the idea is that for an abstraction a a aj denotes whether component j j has been refined or not it will also be convenient to treat a a directly as a set of components namely j j aj so that we can use set notation eg a j we use and j to denote the and abstractions in the family and we denote the size of abstraction a by a many abstraction families are binary in predicate abstraction j is the set of candidate abstraction predicates and an abstraction a would specify a subset of these predicates to include in the analysis shape analysis uses predicates on the heap graph eg reachability from a variable similar to predicate abstraction aj if predicate j is to be treated as an abstraction predicate abstractions in this paper we focus on abstractions let h be the set of allocation sites and i be the set of call sites in a program we use s h i to denote the set of sites of both types consider the family of abstractions defined by setting a nonnegative integer k for each site s s in the case of k cfa this integer specifies that the k most recent elements of the call stack should be used to distinguish method calls and objects allocated at various sites we denote this abstraction family as ak where is the largest allowed k value at first it may not be evident that the abstraction family ak is a binary abstraction family how ever we can represent ak using binary vectors as follows let j s × be the set of components we map each element a a j in the binary tion family to a unique element ak ak in the original abstraction family by ask k ask a a is a unary encoding of the k values of ak ak note that multiple binary vectors a map onto the same ak ie and both represent k but this is not important it is crucial however that the mapping from a to ak respects the partial ordering within each family deterministic approaches in this section we discuss two deterministic approaches for finding small abstractions conceptually there are two ways to proceed start with the abstraction and refine or start with the abstraction and we present two algorithms section and section which operate in these two directions refinement via datalog analysis we first present the algorithm which assumes that static analysis f is expressed as a datalog program p the basic idea behind the algorithm is as follows run the static analysis with the abstraction and look at queries which were not proven then p to find all the components of the abstraction which could affect the queries and refines exactly these components a datalog program p consists of i a set of relations r eg r where o denotes whether variable v may point to abstract object o ii a set of input tuples i for example o i and iii a set of rules of the following form rw rw where for each i m we have a relation ri r and a tuple of variables wi of the appropriate arity eg when r w v o given a datalog program we derive new tuples from the input tuples i by using the rules formally let a derivation be a sequence t tn of tuples satisfying the following two conditions i for each i n either ti is an input tuple ti i or there exist indices j jm all smaller than i such that ti tj is an instantiation of a rule and ii for each j n tuple tj appears on the righthand side of an instantiation of a rule with some ti i j on the lefthand side in this formalism each query q q is a tuple in race detection the set of queries is q p p p p where p is the set of program points for each query q we define if and only if there exists a derivation of q in other words if and only if q is proven the abstraction a determines the input tuples i more specifically let at j denote whether the value aj of component j affects the input tuple t i for example o j for any j s k and abstract object o where o can represent a concrete object allocated at allocation site s h given a let ja denote the components which are involved in a derivation of some query ja j j t i q q at j t q where t q denotes that there exists some derivation t tn where ti t for some i and tn q the key point is that a component not in ja cannot eliminate existing derivations of q which would be necessary to prove q such a component is therefore irrelevant and not refined computation via a datalog program transformation we now define the algorithm which computes and returns j a as the abstraction works by taking a datalog program p as input and transforming it into another datalog program p whose output contains ja having this general transformation allows us to existing datalog solvers to efficiently compute the set of relevant components ja to refine the new program p contains all the rules of p plus additional ones for each datalog rule of p taking the form given in p will contain m rules one for each i m rt rt is true iff q for any query q q we also add the following rule for each r r which the relevant components r t at j it can be verified that is true exactly when j ja where ja is defined in note that is correct in that it outputs an abstraction a which is guaranteed to prove all the queries that can but a will most likely not be minimal abstractions we now describe how we use for abstractions recall that in our binary representation of abstractions section the components are s k where s s is a site and k we start with the abstraction a corresponding to cfa we then iterate k where at each iteration we use ak to construct the input tuples and call to produce a set ja we refine the sites specified by ja setting ak ak s k s k ja k k in this way in each iteration we increase the k value of each site by at most one because we always refine all relevant components after k iterations ak is guaranteed to prove the same subset of queries as the approach is the same for via scanning in the previous section we started with the abstraction and refined it we now introduce a simple algorithm which does the opposite it takes the most refined abstraction a and it preserving correctness definition along the way to simplify presentation suppose we have one query we will the issue of multiple queries in section the idea behind the algorithm is quite simple for each component try removing it from the abstraction if the resulting abstraction no longer proves the query add the component back the of the algorithm is given in figure the algorithm maintains the invariant that al is a via scanning au if al au return au choose any component j if try j return dont need j else return j au need j figure algorithm that finds a minimal abstraction subset of some minimal abstraction and au is a superset sufficient to prove the query the algorithm requires j calls to f and therefore is only practical when the number of components under consideration is small theorem properties of the algorithm j returns a minimal abstraction a with oj calls to f proof let a be the returned abstraction it suffices to show that for all j a that is we fail to prove the query with removal of any j take any j a since j was kept where au corresponds to the value when j was considered however we also have a au so by monotonicity of f machine learning approaches we now present two machine learning algorithms for finding minimal abstractions which is the main theoretical contribution of this paper the two algorithms are which refines an abstraction by iteratively adding components section and which an abstraction by removing components section at a high level these two algorithms parallel their deterministic counterparts and presented in the previous section however there are two important worth noting i the machine learning algorithms find a minimal abstraction much more effectively by exploiting the property that a minimal abstraction contains a small fraction of the full set of components and ii is used to exploit this for clarity of presentation we again focus on the case where we have a single query section addresses the setting refinement via statistical learning we call a component j j dependent if j appears in any minimal abstraction let d j be the set of dependent components and let d d note that d is the union of all minimal abstractions define s to be the size of the largest minimal abstraction observing that s d identifies dependent components by sampling n independent refinement via statistical learning parameters refinement probability s size of largest minimal abstraction n number of training examples per iteration sample al au a al for each component j aj with probability return a s r e fi n if or al s return al for i n create training examples ai sample al j for each j al compute a score for each component nj i j nj choose best component return j figure algorithm for finding a minimal abstraction by iteratively adding dependent components determined via statistical learning random abstractions and running the static analysis f on them the component j associated with the most number of proven queries is then added to the abstraction and we iterate the of the algorithm is given in figure while the datalog program f to compute the set of relevant components relies instead on with the output of f to find dependent components as theorem will show with high probability a dependent component can be found with n calls to f where n is only in the total number of components j we must also ensure that n depends only on s and d the main technical challenge is to set the refinement probability properly to achieve this to this problem suppose that s d so that fa consists of a simple conjunction fa iff aj for each j in the minimal abstraction if we set to a constant then it would take an exponential number of examples s in expectation to even see an example where fa fortunately the following theorem shows that if is set properly then we obtain the desired polynomial dependence see appendix a for the proof theorem properties of let d be the num of dependent components in j and s be the size of the largest minimal abstraction suppose we set the refinement probability d d d and obtain n j note that dependent components are a subset of relevant components via active learning parameters refinement probability s size of largest minimal abstraction ac t i v e c oa r s e if au s return au a sample au if fa run static analysis return reduced else return try again figure returns a minimal abstraction a by iteratively removing a random fraction of the components from an upper bound sample is defined in figure training examples from f each iteration then with probability outputs a minimal abstraction with j total calls to f via active learning we now present our second machine learning algorithm like it starts from the abstraction j and tries to remove components from j but instead of doing this one at a time tries to remove a random constant fraction of the at once as we shall see this allows us to in on a minimal abstraction much more quickly the of the algorithm is given in figure it maintains an upper bound au which is guaranteed to prove the query it repeatedly tries random abstraction a au until a can prove the query fa then we set au to a and repeat recall that which removes one at a time requires an oj calls to the static analysis the key idea behind is to re move a constant fraction of components each iteration then we would hope to need only j iterations however the only is that it might take a lot of to sample an a that proves the query fa to the of this problem suppose fa ¬a a for some unknown set a with a s that is we prove the query if all the components in a are refined by a then there is only a s probability of sampling a random abstraction a that proves the query the expected number of until we prove the query is thus s which has an exponential dependence on s on the other hand when we succeed we reduce the number of components by a factor of there is therefore a tradeoff here setting too small results in too many per iteration but setting too large results in too many iterations fortunately the following theorem shows that we can balance the two to yield an efficient algorithm see appendix a for the proof theorem properties of let s be the size of the largest minimal abstraction if we set the refinement probability es the expected number of calls to the analysis f made by is os log j the refinement probability until now we have assumed that the size of the largest minimal abstraction s is known and indeed theorems and depend on setting properly in terms of s in practice s is unknown so we a mechanism for setting without this knowledge the intuition is that setting properly ensures that queries are proven with a probability bounded away from by a constant indeed in fol the setting of we get d d d in we have e interestingly d d d is lower bounded by e and exactly to e as d the preceding discussion a method that keeps e t which we call the target we can this by as we get new examples from f the adaptive strategy we will derive is simple if fa we decrease otherwise we increase but by how much to avoid boundary conditions we e which maps to for convenience let us define g now consider the following function o g t clearly the value zero is obtained by setting so that g t we can optimize o by updating its do do g t dg d d d where is the step size of course we cannot evaluate g but the key is that we can obtain samples of g by evaluating fa which we needed to do e fa g we can therefore replace the with a stochastic a classic technique with a rich theory we note that dg d so we it into the step size this leaves us with the following rule for updating given a random a f a t note that we have not verified the step size conditions that guarantee convergence instead we simply set for our experiments which well in practice multiple queries and parallelization so far we have presented all our algorithms for one query given multiple queries we could just solve each query independently but this is quite since the information obtained from one query is not used for other queries we therefore adopt a lazy splitting strategy where we initially place all the queries in one group and partition the groups over time as we run either or more specifically we maintain a partition of q into a collection of groups g where each g g is a subset of q we run the algorithm independently for each g g after each call to fa we create two new groups g q g and g q g and set g to gg g g away empty groups in g we take the fa branch of the algorithm and in g we take the fa branch we thus maintain the invariant that for any two queries q q g we have for any a that we have run f on for g or any of gs groups conceptually from the point of view of a fixed q q it is as if we had run the algorithm on q alone but all of the calls to f are shared by other queries when the algorithm terminates all the queries in one group share the same minimal abstraction in section we will see that the number of groups is much smaller than the number of queries our algorithms have been presented as sequential algorithms but parallelization is possible is trivial to because the n training examples are generated independently is slightly more because of the sequential dependence of calls to f with one processor we set so that the target probability is e when we have m processors we set the target probability to em so that the expected time until a reduction is approximately the same the of this is that related to t is now smaller and thus we obtain larger reductions discussion of algorithms table summarizes the properties of the four algorithms we have presented in this paper one of the key advantages of the approaches and is that they have a dependence on j since they take advantage of the property that a minimal abstraction has at most s components both algorithms sample random abstractions by including each component with probability and to avoid an exponential dependence on s it is important to set the probability so that the profile of an irrelevant component is sufficiently different from that of a relevant component for so that the probability of obtaining a successful reduction of the abstraction is sufficiently large the algorithms are also complementary in several respects is a algorithm the run time is fixed but there is some probability that it does not find a minimal abstraction whereas is a algorithm the running time is random but we are guaranteed to find a minimal abstraction note that has an extra factor of d because it implicitly tries to reason globally about all possible minimal abstractions which involve d dependent components whereas tries to in on one minimal abstraction in practice we found to be more effective and thus used it to obtain our empirical results pointsto analysis we now present the static analysis fa in our general notation for the abstraction family ak defined in section which allows each allocation and call site to have a separate k value figure describes the basic analysis each node in the controlflow graph of each method m m is associated with a simple statement eg v v we omit statements that have no effect on our analysis eg operations on data of primitive type for simplicity we assume each method has a single argument and no return value our actual implementation is a straightforward extension of this simplified analysis which handles multiple arguments return values class and objects allocated through reflection our analysis uses sequences of call sites in the case of or allocation sites in the case of to represent method contexts in either case abstract objects are represented by an allocation site plus the context of the containing method in which the object was allocated our abstraction a maps each site s s to the maximum length as of the context or abstract object to maintain for example with heap specialization is represented by ah k for each allocation site h h and ai k for each call site i i is represented by ah k for each allocation site h h the abstraction determines the input tuples c c where s to c and at length as yields c for example if ah then we have i i h i our analysis computes the reachable methods reachable statements and pointsto sets of local variables each with the associated context the pointsto sets of static fields and heap graph heap and a contextsensitive call graph cg we briefly describe the analysis rules in datalog rule states that the main method is reachable in a distinguished context rule states that a target method of a reachable call site is also reachable rule states that every statement in a reachable method is also reachable rules through implement the transfer function associated with each kind of statement rules a and b the call graph while rules a and b propagate the pointsto set from the argument of a call site to the formal argument of each target method rules a and a are used in the case of whereas rules b and b algorithm minimal no yes prob yes correct yes yes prob yes calls to f o oj j os log j in expectation table summary showing the two properties of definition for the four algorithms we have presented in this paper note that the two machine learning algorithms have only a dependence on j the total number of components and a linear dependence on the size of the largest minimal abstraction s are used in the case of as by rule b analyzes the target method m in a separate context o for each abstract object o to which the distinguished this argument of method m points and rule b sets the pointsto set of the this argument of method m in context o to the singleton o race detection we use the pointsto information computed above to answer queries of the form presented in where we include pairs of program points corresponding to statements of the same field in which at least one statement is a write we implemented the static race of which declares a p p pair as if both statements may be reachable may access data may point to the same object and may happen in parallel all four components rely on the pointsto analysis experiments in this section we apply our algorithms sections and to the analysis for race detection section to answer the main question we started out with how small are minimal abstractions our experiments were performed using ibm jvm on bit linux machines all the analyses the basic analysis and the race were implemented in an extensible program analysis framework for java bytecode the machine learning algorithms simply use the race as a black box the experiments were applied to five multithreaded java benchmarks an implementation of the problem a discrete event simulation program a web a and tool and a text search tool table provides the number of classes number of methods number of of methods and number of sites reachable by cfa in these benchmarks table shows the number of races queries reported by the and abstractions classes methods k k k k k h i table benchmark characteristics h is the number of allocation sites and i is the number of call sites together these determine the number of components in the abstraction family j for j i for j k h a cfa j cfa q obj j obj q table number of races queries reported using the abstraction and the abstraction j for and for the difference is the set of queries under consideration those provable by j but not by their difference is the set of queries q that we want to prove with a minimal abstraction results table summarizes the basic results for and while both find abstractions which prove the same set of queries obtains this precision using an abstraction which is minimal and an order of magnitude smaller than the abstraction found by which is not guaranteed to be minimal and is in fact far from minimal in our experiments algorithms aside it is in itself that very small abstractions exist for example on we can get the same precision as cfa by essentially using a cfa analysis indeed our static analysis using this minimal abstraction was as fast as using cfa whereas cfa took significantly longer domains method m m local variable v v global variable g g object field f f method call site i i allocation site h h site s s h i statement p p method context c c k sk abstract object o o h × c abstraction a ak s input relations body m × p method contains statement i × m call site to method i × v call sites argument variable m × v methods formal argument variable ext s × c × c extend context with site s c s c c s s c c output relations c × m reachable methods c × p reachable statements c × v × o pointsto sets of local variables g × o pointsto sets of static fields heap o × f × o heap graph cg c × i × c × m call graph p v new h v v g v v g vf v v vf iv rules m cg c m p m p v o v o o v o f o v o v new h c o v v v o g v v o v g o vf v v o v o v vf v o f o i c m i o m v o v c i m c c a i m v v o b i c m v v v o a m v b figure datalog implementation of our pointsto analysis with construction our abstraction a affects the analysis solely through ext which specifies that when we s to c we the resulting sequence to length as if we use rules a and a we get if we use b and b we get query groups recall from section that to deal with multiple queries we partition the queries into groups and find one minimal abstraction for each group the abstraction sizes reported so far are the union of the abstractions over all groups we now take a closer look at the abstractions for individual queries in a group first table shows that the number of groups is much smaller than the number of queries which means that many queries share the same minimal abstraction this is intuitive since many queries depend on the same data and control properties of a program next figure shows a histogram of the abstraction sizes across queries most queries required a abstraction only requiring a of sites to be refined for example for on over of the queries require just a single allocation site to be refined even the most query requires only of the sites to be refined recall that refining sites suffices to prove all the queries table for comparison refines sites related work one of the key algorithmic tools that we used to find minimal abstractions is a powerful idea has been previously applied in program analysis eg in random testing and random interpretation queries cfa a obj a queries queries cfa a obj a queries queries cfa a obj a queries figure a histogram showing for each abstraction size a the number of queries that have a minimal abstraction of that size for three of the five benchmarks note that most queries need very small abstractions some need only one site to be refined cfa obj cfa obj cfa obj cfa obj cfa obj j minimal table this table shows our main results j is the number of components the size of the abstraction the next two columns show abstraction sizes absolute and fraction of j for the abstraction found by and a minimal abstraction found by all abstractions prove the same set of queries refines anywhere between ­ of the components while the minimal abstraction is an order of magnitude smaller ­ of the components our approach is perhaps more closely associated with machine learning although there is an important difference in our goals machine learning as by the learning model is largely concerned with prediction that is an algorithm is evaluated on how accurately it can learn a function that well on future inputs we are instead concerned with finding the smallest input on which a function evaluates to as a result many of the results for example on learning monotone dnf formulae are not directly applicable though many of the bounding techniques used are similar groups min mean max cfa obj cfa obj cfa obj cfa obj cfa obj table recall that our learning algorithms group the queries section so that all the queries in one group share the same minimal abstraction the minimum mean and maximum size of a group is reported in all cases there is large of the number of queries in a group one of the key properties that our approach exploited was only a small subset of the components of the abstraction actually for proving the desired query this enabled us to use a rather than linear number of examples is one of the main in machine learning signal processing and statistics for example in the area of compressed one also needs a number of linear measurements to recover a sparse signal past research in program analysis and pointer analysis specifically has proposed various ways to reduce the cost of the analysis while still providing accurate results frameworks provide a mechanism for the user to control the tradeoff between cost and precision of the analysis approaches are capable of computing an exhaustive solution of varying precision while demand driven approaches are capable of computing a partial solution of fixed precision below we expand upon each of these topics in relation to our work et al present a parametrized framework for a pointsto analysis where each local variable can be separately treated or and different k values can be chosen for different allocation sites instantiations of the framework using k and k are evaluated on sideeffect analysis construction and resolution and are shown to be significantly more precise than cfa while being comparable to cfa in performance if not better and present a parametrized framework for alias analysis they evaluate various instantiations of including conventional and with heap on the and clients as well as using traditional metrics and show that is to other approaches both in performance and in precision and use a algorithm to determine the concrete types of objects in programs written in the concurrent objectoriented language when in the analysis causes a type conflict the algorithm can improve context sensitivity by performing function splitting and object sensitivity through container splitting which object creation sites and thus enables the creation of objects of different types at a single site in a more recent study and present a demanddriven alias analysis for java and apply it to a client their algorithm computes an overapproximation of the pointsto relation which is refined in response to client demand at each stage of refinement the algorithm simultaneously refines handling of heap accesses and method calls along paths establishing the pointsto sets of variables that are relevant for evaluating the clients query and lin present a pointer analysis for c that its precision in response to in the client analysis their analysis is a algorithm in the first pass a pointer analysis is run to detect which statements lead to in the client analysis based on this information a finegrained precision policy is built for the second pass which treats these statements with greater context and flow sensitivity this is similar to our in spirit but is more general in contrast to analyses which compute an exhaustive solution of varying precision demanddriven approaches compute a partial solution of fixed precision heintze and demanddriven alias analysis for c performs a provably optimal computation to determine the pointsto sets of variables by the client their analysis is applied to construction in the presence of function pointers a more recent alias analysis developed by and uses a demanddriven algorithm that is capable of alias queries without constructing pointsto sets reps shows how to automatically obtain demanddriven versions of program analyses from their exhaustive counterparts by applying the transformation developed in the and the exhaustive analysis is expressed in datalog to that in our approach but unlike us they are interested in specific queries of interest in the program being analyzed as opposed to all queries for instance while we are interested in finding all pairs of points p p in a given program that may be involved in a race they may be interested in finding all points that may be involved in a race with the statement at point p in the given program the transformation takes as input a datalog program which performs the exhaustive analysis along with a set of specified queries it produces as output the demanddriven version of the analysis also in datalog but which eliminates computation from the original analysis that is unnecessary for evaluating the specified queries in contrast our transformation takes as input a parametrized analysis expressed in datalog and produces as output another analysis also in datalog whose goal is to compute all possible parameters that may affect the output of the original analysis for instance all possible sites in a given program whose small k values may be responsible for a set of races being reported by the original analysis conclusion we started this study with a basic question what is the minimal abstraction needed to prove a set of queries of interest to answer this question we developed two machine learning algorithms and applied them to find minimal k values of sites for a static race the key in this work is the property that very few components of an abstraction are needed to prove a query the are we show that our algorithms are efficient under we found that the minimal abstractions are quite ­ of the sites are needed to prove all the queries of interest a proofs proof of theorem note that the algorithm will run for at most s iterations where s is the size of the largest minimal abstraction if we set n so that chooses a dependent component each iteration with probability at least s then the algorithm will succeed with probability at least by a union bound let us now focus on one iteration the main idea is that a dependent component j is more with proving the query fa than one that is independent this enables it out with high probability given sufficiently large n recall that d is the set of dependent components with d d fix a dependent component j d let bj be the event that nj nj and b be the event that bj holds for any independent component j jd note that if b does not happen then the algorithm will correctly pick a dependent component possibly j thus the main focus is on showing that p b s first by a union bound we have p b p bj j max j p bj j for each training example ai define xi observe that bj happens exactly when n nj nj n n i xi we now bound this using inequality where the mean is aj aj and the bounds are a and b setting we get en j d j d substituting into and terms we can solve for n s n log j now it remains to lower bound which intuitively repre the gap in the amount of with proving the query between a dependent component and an independent one note that for any j j also j is independent of fa so aj using these two facts we have aj let c be the set of minimal abstractions of f we can think of c as a set of clauses in a dnf formula fa c ¬ cc jc aj where we explicitly mark the dependence of f on the clauses c for example c corresponds to fa ¬a a a next let cj c c j c be the clauses containing j rewrite as the sum of two parts one that depends on j and one that does not cj fa computing aj is similar the only difference due to on aj is that the first inequality if x xn are random variables with a xi b then p n pn i xi exp n n ba o term has an additional factor of because by the second term is unchanged because no c cj depends on aj these two results back into yields cj fa now we want to lower bound over all possible f equivalently c where j is allowed to depend on c it turns out that the worst possible c is obtained by either having d disjoint clauses c j j d or one clause c d if s d the intuition is that if c has d clauses there are many opportunities d of them for some c cj to making it hard to realize that j is a dependent component in this case d if c has one clause then it is very hard probability d to even this clause in this case d let us focus on the case where c has d clauses we can with respect to by setting the derivative d d and solving for doing this yields d as the optimal value this value back into the expression for we get that d d d d the second factor can be lower bounded by e so od combining this with completes the proof proof of theorem let t au be the expected number of calls that makes to f the recursive computation of this the of figure t au au if au s au otherwise where a sample au is a random binary vector by assumption there exists an abstraction a au of size s that proves the query define ga ¬a a which is when all components in a are active under the random a we have pa a s note that ga fa as a suffices to prove the query we assume t a t au t is monotonic so we get an upper bound by replacing f with g and performing some algebra t au e a au set a a a st au et a a a s overloading notation we write t n t a to be the maximum over abstractions of size n note that a given a a is s plus a random variable n with expectation n s using the bound t n nt n nt ns we see that t n s n · n in t n is moreover t n is for large enough n so we can use inequality to swap t and e t n t es n s t s n s s solving the recurrence we obtain s log n t n log s from we can see the tradeoff between reducing the number of iterations by increasing log versus reducing the number of by decreasing s we now set to minimize the upper bound with respect to x and set the derivative to zero log x xs log x solving this equation yields es this value back into yields t n es log n s os log n references d queries and concept learning machine learning ­ t ball and s rajamani the slam project debugging system software via static analysis in proceedings of acm symp on principles of programming languages popl pages ­ t ball r majumdar t and s rajamani automatic predicate abstraction of c programs in proceedings of acm conf on programming language design and implementation pldi pages ­ d compressed ieee trans on information theory ­ s and h construction of abstract state graphs with pages ­ s gulwani program analysis using random interpretation phd thesis uc berkeley s and c lin pointer analysis in proceedings of intl static analysis symposium pages ­ d random testing in of software engineering pages ­ n heintze and o demanddriven pointer analysis in proceedings of acm conf on programming language design and implementation pldi pages ­ o and l contextsensitive pointsto analysis is it worth it in proceedings of intl conf on compiler construction pages ­ o and l evaluating the benefits of contextsensitive pointsto analysis using a implementation acm transactions on software engineering and methodology ­ a a and b ryder parameterized object sensitivity for pointsto and sideeffect analyses for java in proceedings of acm intl symp on software testing and analysis pages ­ a a and b ryder parameterized object sensitivity for pointsto analysis for java acm transactions on software engineering and methodology ­ m a aiken and j effective static race detection for java in proceedings of acm conf on programming language design and implementation pldi pages ­ j and a precise concrete type inference for objectoriented languages in proceedings of acm conf on objectoriented programming systems languages and applications pages ­ t w reps demand interprocedural program analysis using logic databases in workshop on programming with logic databases pages ­ t w reps solving demand versions of interprocedural analysis problems in proceedings of intl conf on compiler construction pages ­ h and s a stochastic approximation method of mathematical statistics ­ m sagiv t w reps and r parametric shape analysis via valued logic acm transactions on programming languages and systems ­ o shivers controlflow analysis in scheme in proceedings of acm conf on programming language design and implementation pldi pages ­ m and r contextsensitive pointsto analysis for java in proceedings of acm conf on programming language design and implementation pages ­ m d l and r demanddriven pointsto analysis for java in proceedings of acm conf on objectoriented programming systems languages and applications pages ­ l a theory of the communications of the acm ­ m j for and recovery of using constrained quadratic programming ieee transactions on information theory ­ j contextsensitive pointer analysis using binary decision diagrams phd thesis stanford university j and m lam contextsensitive pointer alias analysis using binary decision diagrams in proceedings of acm conf on programming language design and implementation pldi pages ­ x and r demanddriven alias analysis for c in proceedings of acm symp on principles of programming languages popl pages ­ 