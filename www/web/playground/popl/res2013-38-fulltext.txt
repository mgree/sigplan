checking nfa equivalence with bisimulations up to congruence de abstract we introduce bisimulation up to congruence as a technique for proving language equivalence of nondeterministic finite automata exploiting this technique we an optimisation of the classical algorithm by hopcroft and we compare our approach to the recently introduced algorithms by and relating the two underlying coinductive proof methods we give concrete examples where we exponentially improve over experimental results moreover show non improvements categories and subject descriptors f mathematical logic decision problems f models of computation automata d program verification model checking keywords language equivalence automata bisimulation coinduction upto techniques congruence introduction checking language equivalence of finite automata is a classical problem in computer science which finds applications in many fields ranging from compiler construction to model checking equivalence of deterministic finite automata dfa can be checked either via or through hopcroft and algorithm which exploits an instance of what is called a coinduction proof principle two states the same language if and only if there exists a bisimulation relating them in order to check the equivalence of two given states hopcroft and algorithm creates a relation containing them and tries to build a bisimulation by adding pairs of states to this relation if it succeeds then the two states are equivalent otherwise they are different on the one hand algorithms have the advantage of checking the equivalence of all the states at once while hopcroft and algorithm only check a given pair of states on the other hand they have the of the whole automata from the beginning while hopcroft and algorithm can be executed on a lazy dfa whose transitions are computed on demand work partially by the project and by the project permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ italy copyright c acm this difference is fundamental for our work and for other recently introduced algorithms based on indeed when starting from nondeterministic finite automata nfa the powerset construction used to get deterministic automata induces an exponential factor in contrast the algorithm we introduce in this work for checking equivalence of nfa as well as those in usually does not build the whole deterministic automaton but just a small part of it we write usually because in few bad cases the algorithm still needs exponentially many states of the dfa our algorithm is on a simple observation on nfa for all sets x and y of states of the original nfa the union written of the language by x written x and the language by y y is equal to the language by the union of x and y x y in symbols x y x y this fact leads us to introduce a sound and complete proof technique for language equivalence namely bisimulation up to context that exploits both induction on the operator and coinduction if a bisimulation r both the sets of states x y and x y then x y and x y and by we can immediately conclude that also x x and y y are language equivalent intuitively bisimulations up to context are bisimulations which do not need to relate x x and y y when x resp x and y resp y are already related to illustrate this idea let us check the equivalence of states x and u in the following nfa final states are labelled edges represent transitions a xo a a zf y a a a wo a v a the automaton is depicted below x a y a z a x y a y z a y z u a v w a u w a u v w a a each state is a set of states of the nfa final states are they contain at least one final state of the nfa the lines show a relation which is a bisimulation containing x and u actually this is the relation that is built by hopcroft and algorithm the numbers express the order in which pairs are added the dashed lines by form a smaller relation which is not a bisimulation but a bisimulation up to context the equivalence of states x y and u v w could be immediately from the fact that x is related to u and y to v w without the need of further exploring the automaton bisimulations upto and in particular bisimulations up to context have been introduced in the setting of concurrency theory as a proof technique for bisimilarity of ccs or calculus processes as far as we know they have never been used for proving language equivalence of nfa among these techniques one should also mention bisimulation up to equivalence which as we show in this paper is implicitly used in the original hopcroft and algorithm this technique can be briefly explained by noting that not all bisimulations are equivalence relations it might be the case that a bisimulation relates for instance x and y y and z but not x and z however since x y and y z we can immediately conclude that x and z the same language analogously to bisimulations up to context a bisimulation up to equivalence does not need to relate x and z when they are both related to some y the techniques of upto equivalence and upto context can be combined resulting in a powerful proof technique which we call bisimulation up to congruence our algorithm is in fact just an extension of hopcroft and algorithm that attempts to build a bisimulation up to congruence instead of a bisimulation up to equivalence an important consequence when using up to congruence is that we do not need to build the whole deterministic automata but just those states that are needed for the bisimulation upto for instance in the above nfa the algorithm stops after z and u v and does not build the remaining four states despite their use of the up to equivalence technique this is not the case with hopcroft and algorithm where all accessible subsets of the deterministic automata have to be visited at least once the ability of only a small portion of the automaton is also the key feature of the algorithm and its optimisation exploiting similarity the two algorithms are designed to check language inclusion rather than equivalence but we can relate these approaches by observing that the two problems are equivalent x y iff x y and y x and x y iff x y y iff x y y in order to compare with these algorithms we make explicit the coinductive upto technique underlying the algorithm we prove that this technique can be seen as a restriction of up to congruence for which symmetry and transitivity are not allowed as a consequence the algorithm usually needs to explore more states than our algorithm moreover we show how to integrate the optimisation proposed in in our setting resulting in an even more efficient algorithm in summary the contributions of this work are the observation that hopcroft and implicitly use bisimulations up to equivalence an efficient algorithm for checking language equivalence and inclusion based on a powerful up to technique and a comparison with algorithms by them into our coinductive framework outline section hopcroft and algorithm for dfa showing that it implicitly exploits bisimulation up to equivalence section describes the novel algorithm based on bisimulations up to congruence we compare this algorithm with the one in section and we show how to exploit similarity in section section is to benchmarks sections and discuss related and future works omitted proofs can be found in notation we denote sets by letters x y s t and functions by lower case letters f g given sets x and y x × y is their cartesian product x y is the disjoint union and xy is the set of functions f y x finite iterations of a function f x x are denoted by f n formally f x x f nx f f nx the collection of subsets of x is denoted by px the iteration of a function f px px is denoted by f formally f y n f ny for a set of letters a a denotes the set of all finite words over a the empty word and ww the concatenation of words w w a we use for the set and a for the set of all languages over a hopcroft and algorithm for dfa a deterministic finite automaton dfa over the alphabet a is a triple s o t where s is a finite set of states o s is the output function which determines if a state x s is final ox or not ox and t s sa is the transition function which returns for each state x and for each letter a a the next state for a a we write x a x to mean that x for w a we write x w x for the least relation such that x x and x aw x iff x a x and x w x for any dfa there exists a function s a mapping states to languages defined for all x s as follows x ox the language x is called the language accepted by x given two automata s o t and s o t the states x s and x s are said to be language equivalent written x x iff they accept the same language remark in the following we will always consider the problem of checking the equivalence of states of one single and fixed automaton s o t we do not generality since for any two automata s o t and s o t it is always possible to build an automaton s s o o t t such that the language accepted by every state x s s is the same as the language accepted by x in the original automaton si oi ti for this reason we also work with automata without explicit initial states we focus on the equivalence of two arbitrary states of a fixed dfa proving language equivalence via coinduction we first define bisimulation we make explicit the underlying notion of which we need in the sequel definition bisimulation given two relations r r s × s on states r to r denoted r r if whenever x r y then ox and for all a a r a bisimulation is a relation r such that r r as expected bisimulation is a sound and complete proof technique for checking language equivalence of dfa proposition coinduction two states are language equivalent iff there exists a bisimulation that relates them naive algorithm figure shows a naive version of hopcroft and algorithm for checking language equivalence of the states x and y of a deterministic finite automaton s o t starting from x and y the algorithm builds a relation r that in case of success is a bisimulation in order to do that it the set of pairs of states todo which intuitively at any step of the execution contains the pairs x y that must be checked if x y already belongs to r then it has already been checked and nothing else should be done otherwise the algorithm checks if x and y have the same outputs ie if both are final or not if ox then x and y are different if ox then the algorithm inserts x y in r and for all a a the pairs in todo y r i s e m p t y todo i s e m p t y i n s e r t x y i n todo w h i l e todo i s n o t empty d o e x t r a c t x y f r o m todo i f x y r t h e n c o n t i n u e i f ox t h e n r e t u r n f for all a a i n s e r t i n todo i n s e r t x y i n r r e t u r n true figure naive algorithm for checking the equivalence of states x and y of a dfa s o t r and todo are sets of pairs of states the code of y is obtained by replacing step with if x y er then continue x a a z a u a vh a w a x ab y ab zd vh ab w ua ab b figure checking for dfa equivalence ab proposition for all x y s x y iff y proof we first observe that if y returns true then the relation r that is built before to step is a bisimulation indeed the following proposition is an invariant for the loop corresponding to step r r todo this invariant is preserved since at any iteration of the algorithm a pair x y is removed from todo and inserted in r after checking that ox and adding for all a a in todo since todo is empty at the end of the loop we eventually have r r ie r is a bisimulation by proposition x y we now prove that if y returns false then x y note that for all x y inserted in todo there exists a word w a such that x w x and y w y since ox then x y and thus x y that is x y since both hopcroft and algorithm and the one we introduce in section are simple variations of this naive one it is important to illustrate its execution with an example consider the dfa with input alphabet a a in the lefthand side of figure and suppose we want to check that x and u are language equivalent during the x u is inserted in todo at the first iteration since ox x u is inserted in r and y v in todo at the second iteration since y v is inserted in r and z w in todo at the third iteration since ow z w is inserted in r and y v in todo at the fourth iteration since y v is already in r the algorithm does nothing since there are no more pairs to check in todo the relation r is a bisimulation and the algorithm terminates returning true these iterations are described by the dashed lines in figure the line i means that the connected pair is inserted in r at iteration i in the sequel when iterations we ignore those where a pair from todo is already in r so that there is nothing to do remark unless it finds a counterexample naive constructs the smallest bisimulation that relates the two starting states see proposition in on the contrary algorithms are designed to compute the largest bisimulation relation for a given automaton for instance taking automaton on the left of figure they would the states x and w which are language equivalent while u does not relate them hopcroft and algorithm the naive algorithm is quadratic a new pair is added to r at each nontrivial iteration and there are only n such pairs where n s is the number of states of the dfa to make this algorithm almost linear hopcroft and actually record a set of equivalence classes rather than a set of visited pairs as a consequence their algorithm may stop earlier when an encountered pair of states is not already in r but in its reflexive symmetric and transitive closure for instance in the righthand side example from figure we can stop when we the dotted pair y w since these two states already belong to the same equivalence class according to the four previous pairs with this optimisation the produced relation r contains at most n pairs two equivalence classes are merged each time a pair is added formally and ignoring the concrete data structure to store equivalence classes hopcroft and algorithm consists in simply replacing step in figure with i f x y er t h e n c o n t i n u e where e ps × s ps × s is the function mapping each relation r s × s into its symmetric reflexive and transitive closure we refer to this algorithm as hk bisimulations upto we now show that the optimisation used by hopcroft and corresponds to exploiting an upto technique definition bisimulation upto let f ps × s ps × s be a function on relations on s a relation r is a bisimulation up to f if r f r ie whenever x r y then ox and for all a a f r with this definition hopcroft and algorithm just consists in trying to build a bisimulation up to e to prove the correctness of the algorithm it suffices to show that any bisimulation up to e is contained in a bisimulation we use for that the notion of compatible function definition compatible function a function f ps × s ps × s is compatible if it is monotone and it preserves for all r r s × s r r entails f r f r proposition let f be a compatible function any bisimulation up to f is contained in a bisimulation proof suppose that r is a bisimulation up to f ie that r f r using compatibility of f and by a simple induction on n we get n f nr f nr therefore we have f nr f nr nn in other words f r n f nr is a bisimulation this latter relation trivially contains r by taking n we could prove directly that e is a compatible function we however take a to ease our correctness proof for the algorithm we propose in section lemma the following functions are compatible id the identity function f g the composition of compatible functions f and g f the pointwise union of an arbitrary family f of compatible functions f r ff f r f the iteration of a compatible function f lemma the following functions are compatible · the constant reflexive function rr x x x s · the converse function sr y x x r y · the function tr x z y x r y r z intuitively given a relation r s is the symmetric closure of r r s is its reflexive and symmetric closure and r s t is its symmetric reflexive and transitive closure e r s t id another way to understand this decomposition of e is to recall that for a given r er can be defined inductively by the following rules x er y x er y y er z rs t id x er x y er x x er z x er y theorem any bisimulation up to e is contained in a bisimulation proof by proposition it suffices to show that e is compatible which follows from lemma and lemma corollary for all x y s x y iff y proof same proof as for proposition by using the invariant r er todo we deduce that r is a bisimulation up to e after the loop we conclude with theorem and proposition returning to the righthand side example from figure hopcroft and algorithm constructs the relation x u y v z w z v which is not a bisimulation but a bisimulation up to e it contains the pair x u whose lead to y w which is not in but in its equivalence closure algorithm for nfa we now move from dfa to nondeterministic automata nfa we start with standard definitions about and language equivalence for nfa a semilattice x consists of a set x and a binary operation x × x x which is associative commutative idempotent and has x as identity given two x and x an homomorphism of is a function f x x such that for all x y x f x y f x f y and f the set is a semilattice when taking to be the ordinary boolean or also the set of all languages a carries a semilattice where is the union of languages and is the empty language more generally for any set x px is a semilattice where is the union of sets and is the empty set in the sequel we use to denote the element the empty language in a and the empty set in px similarly we use to denote the boolean or in the union of languages in a and the union of sets in px a nondeterministic finite automaton nfa over the input alphabet a is a triple s o t where s is a finite set of states o s is the output function as for dfa and t s is the transition relation which assigns to each state x s and input letter a a a set of possible successor states the powerset construction transforms any nfa s o t in the dfa ps o t where o ps and t ps are defined for all x ps and a a as follows ox o x o x o x if x x with x s if x if x x x if x x with x s if x if x x x observe that in ps o t the states form a semilattice ps and o and t are by definition homomorphisms these properties are fundamental for the upto technique we are going to introduce in order to the difference with generic dfa which usually do not carry this structure we introduce the following definition definition a nfa is a dfa ps o t obtained via the powerset construction of some nfa s o t we use a new notation for representing states of nfa in place of the singleton x we just write x and in place of x xn we write x · · · xn for an example consider the nfa s o t depicted below left and part of the nfa ps o t right x g a y o a bz a a x a y z a x y a x f y z a in the nfa x makes one single going into y z this state is final o y z o y o z it makes an into z x y the language accepted by the states of an nfa s o t can be defined via the powerset construction the language accepted by x s is the language accepted by the singleton x in the dfa ps o t in symbols x therefore in the following instead of considering the problem of language equivalence of states of the nfa we focus on language equivalence of sets of states of the nfa given two sets of states x and y in ps we say that x and y are language equivalent x y iff x y this is exactly what happens in standard automata theory where nfa are equipped with sets of initial states extending coinduction to nfa in order to check whether two sets of states x and y of an nfa s o t are language equivalent we can simply employ the bisimulation proof method on ps o t more explicitly a bisimulation for an nfa s o t is a relation r ps × ps on sets of states such that whenever x r y then o x o y and for all a a r since this is just the old definition of bisimulation definition applied to ps o t we get that x y iff there exists a bisimulation relating them remark linear time vs branching time it is important not to these bisimulation relations with the standard bisimulations which strictly imply language equivalence in a standard bisimulation r if the following states x and y of an nfa are in r a x x a xn a y y a ym then each xi should be in r with some yj and here instead we first transform the transition relation into x a x · · · xn y a y · · · ym using the powerset construction and then we require that the sets x · · · xn and y · · · ym are related by r bisimulation up to congruence the semilattice structure ps carried by nfa makes it possible to introduce a new upto technique which is not available with plain dfa up to congruence this technique relies on the following function definition congruence closure let u × ps × ps be the function on relations on sets of states defined for all r ps × ps as ur x x y y x r y and x r y the function c r s t u id is called the congruence closure function intuitively cr is the smallest equivalence relation which is closed with respect to and which includes r it could alternatively be defined inductively using the rules r s t and id from the previous section and the following one x cr y x cr y u x x cr y y we call bisimulations up to congruence the bisimulations up to c we report the explicit definition for the sake of clarity definition bisimulation up to congruence a bisimulation up to congruence for an nfa s o t is a relation r ps × ps on sets of states such that whenever x r y then o x o y and for all a a cr we then show that bisimulations up to congruence are sound using the notion of compatibility lemma the function u is compatible proof we assume that r r and we prove that ur ur if x ur y then x x x and y y y for some x x y y such that x r y and x r y by assumption we have o x o y o x o y and for all a a r and r since o and t are homomorphisms we deduce o x x o y y and for all a a x ur y theorem any bisimulation up to congruence is contained in a bisimulation proof by proposition it suffices to show that c is compatible which follows from lemmas and in the introduction we already gave an example of bisimulation up to context which is a particular case of bisimulation up to congruence up to context corresponds using just the function r u id without closing under s and t x g a y o a ez a a du x a y z a x y a x f y z a du aa figure bisimulations up to congruence on a single letter nfa y r i s e m p t y todo i s e m p t y i n s e r t x y i n todo w h i l e todo i s n o t empty d o e x t r a c t x y f r o m todo i f x y r t h e n c o n t i n u e i f o x o y t h e n r e t u r n f for all a a i n s e r t i n todo i n s e r t x y i n r r e t u r n true figure naive algorithm for checking the equivalence of sets of states x and y of an nfa s o t the code for y is obtained by replacing the test in step with x y er the code for y is obtained by replacing this test with x y cr todo a more involved example the use of all of the congruence closure function c is given in figure the relation r expressed by the dashed lines formally r x u y z u is neither a bisimulation nor a bisimulation up to equivalence since y z a xy and u a u but xy u er however r is a bisimulation up to congruence indeed we have x y u cr x y cr u y cr y z y yz cr u x u r y z u r y z u r in contrast we need four pairs to get a bisimulation up to e containing x u this is the relation depicted with both dashed and dotted lines in figure note that we can deduce many other equations from r in fact cr defines the following partition of sets of states y z x u xy xz and the remaining subsets algorithm for nfa algorithms for nfa can be obtained by computing the nfa starting from the algorithms for dfa figure it suffices to work with sets of states and to inline the powerset construction the corresponding code is given in figure the naive algorithm naive does not use any up to technique hopcroft and algorithm hk reasons up to equivalence in step and the algorithm referred as hkc in the sequel relies on up to congruence step becomes i f x y cr todo t h e n c o n t i n u e observe that we use cr todo rather than cr this allows us to skip more pairs and this is safe since all pairs in todo will eventually be processed corollary for all x y ps x y iff y proof same proof as for proposition by using the invariant r cr todo for the loop we deduce that r is a bisimulation up to congruence after the loop we conclude with theorem and proposition the most important point about these three algorithms is that they compute the states of the nfa this means that only accessible states need to be computed which is of practical importance since the nfa can be exponentially large in case of a negative answer the three algorithms stop even before all accessible states have been explored otherwise if a bisimulation possibly upto is found it depends on the algorithm · with naive all accessible states need to be visited by definition of bisimulation · with hk the only case where some accessible states can be avoided is when a pair x x is encountered the algorithm this pair so that the successors of x are not necessarily computed this situation happens in actually never happens when starting with disjoint automata in the other cases where a pair x y is then x and y are necessarily already related to some other states in r so that their successors will eventually be explored · with hkc only a small portion of the accessible states is built check the experiments in section to see a concrete example let us execute hkc on the nfa from figure after two iterations r x u y z u since x y cr u the algorithm stops without building the states x y and x y z similarly in the example from the introduction hkc does not construct the four states corresponding to pairs and this ability of hkc to ignore parts of the nfa comes from the up to congruence technique which allows one to infer properties about states that were not necessarily encountered before as we shall see in section the efficiency of algorithms also comes from their ability to skip large parts of the nfa computing the congruence closure for the algorithm to be effective we need a way to check whether some pairs belong to the congruence closure of some relation step we present here a simple solution based on set rewriting the key idea is to look at each pair x y in a relation r as a pair of rewriting rules x xy y xy which can be used to compute normal forms for sets of states indeed by idempotence x r y entails x cr x y definition let r be a relation on sets of states we define r ps × ps as the smallest relation that satisfies the following rules x ry x r xy x ry y r xy z rz u z r u z lemma for all relations r the relation r is in the sequel we denote by xr the normal form of a set x wrt r intuitively the normal form of a set is the largest set of its equivalence class the example from figure the common normal form of x y and u can be computed as follows r is the relation x u y z u xy s w xu u theorem for all relations r and for all x y ps we have xr y r iff x y cr thus for checking x y cr todo we only have to compute the normal form of x and y with respect to note that each pair of may be used only once as a rewriting rule but we do not know in advance in which order to apply these rules therefore the time required to find one rule that applies is in the worst case rn where r r todo is the size of the relation and n s is the number of states of the nfa assuming linear time complexity for settheoretic union and containment of sets of states since we cannot apply more than r rules the time for checking whether x y cr todo is bounded by rn we tried other solutions notably by using binary decision diagrams we have chosen to keep the presented rewriting algorithm for its simplicity and because it behaves well in practice complexity the complexity of naive hk and hkc is closely related to the size of the relation that they build we use v a to denote the number of letters in a lemma the three algorithms require at most iterations where r is the size of the produced relation moreover this bound is reached whenever they return true therefore we can reason about r lemma let and denote the relations produced by the three algorithms we have m m where m n is the number of accessible states in the nfa and n is the number of states of the nfa if the algorithms return true we moreover have as shown below in section can be exponentially smaller than notice however that the problem of deciding nfa language equivalence is and that none of the algorithms presented here is in pspace all of them store a set of visited pairs and in the worst case this set can become exponentially large with all of them this also holds for the algorithms which we describe in section instead the standard pspace algorithm does not store any set of visited pairs it checks all words of length smaller than n while this can be done in polynomial space this systematically requires exponential time using hkc for checking language inclusion for nfa language inclusion can be reduced to language equivalence in a rather simple way since the function ps a is a semilattice homomorphism see theorem in for any given sets of states x and y xy y iff xy y iff x y therefore it suffices to run y to check the inclusion x y in such a situation all pairs that are eventually manipulated by hkc have the shape x y y for some sets x y step of hkc where it checks whether the current pair belongs to the congruence closure of the relation can thus be simplified first the pairs in the current relation can only be used to rewrite from right to left second the following lemma allows one to avoid unnecessary normal form computations lemma for all sets x y and for all relations r we have xy cr y iff x y r proof we first prove that for all x y xr y r iff x y r and y xr using the fact that the function r x xr is monotone and idempotent the result follows by theorem since y x y r is always true and xy y r iff x y r at this point the reader might whether checking the two is more convenient than checking the equivalence directly we show that this is not the case lemma let x y be two sets of states let r and r be the relations computed by y and x respectively if r and r are bisimulations up to congruence then the following relation is a bisimulation up to congruence r x y x y y r or x y x r on the contrary checking the equivalence directly actually allows one to skip some pairs that cannot be when reasoning by double inclusion as an example consider the dfa on the right of figure the relation computed by u contains only four pairs because the fifth one follows from transitivity instead the relations built by xu and u would both contain five pairs transitivity cannot be used since our relations are now oriented from y v z v and z w we cannot deduce y w another example where we get an exponential factor by checking the equivalence directly rather than through the two can be found in section in a sense the behaviour of the coinduction proof method here is similar to that of standard proofs by induction where one often has to the induction predicate to get a proof algorithm in de et al have proposed the approach for checking language inclusion of nfa we show that this approach can be explained in terms of simulations up to that in turn can be seen as a special case of bisimulations up to congruence before doing so we recall the standard notion of and we describe the algorithm ac given a partial order x an is a subset y x containing only incomparable elements that is for all y y y y y and y y ac exploits over the set s × ps where the ordering is given by x y x y iff x x and y y in order to check x y for two sets of states x y of an nfa s o t ac maintains an of pairs x y where x is a state of the nfa and y is a state of the automaton more precisely the automaton is explored nondeterministically via t for obtaining the first component of the pair and via t for the second one if a pair such that x is accepting ox and y is not o y is encountered then a counterexample has been found otherwise all derivatives of the pair along the automata transitions have to be inserted into the so that they will be explored if one of these pairs p is larger than a previously encountered pair p p p then the language inclusion corresponding to p is subsumed by p so that p can be otherwise if p p pn for some pairs p pn that are already in the then one can safely remove these pairs they are subsumed by p and by doing so the set of visited pairs remains an remark an important difference between hkc and ac consists in the fact that the former inserts pairs in todo without checking whether they are redundant this check is performed when the pair is processed while the latter removes all redundant pairs whenever a new one is inserted therefore the cost of an iteration with hkc is merely the cost of the corresponding congruence check while the cost of an iteration with ac is merely that of inserting all successors of the corresponding pair and simplifying the note that the above description corresponds to the forward algorithm as described in instead the original algorithm as first described in is backward in the sense that the automata are traversed in the way from accepting states to initial states the two versions are dual and we could similarly define the backward counterpart of hkc and hk we however to the forward versions for the sake of clarity coinductive presentation leaving apart the concrete data structures used to manipulate we can this algorithm using a coinductive framework like we did for hopcroft and algorithm first define a notion of simulation where the lefthand side automaton is executed nondeterministically definition simulation given two relations t t s t to t denoted t s t if whenever x t y then ox o y and for all a a x x t a simulation is a relation t such that t s t as expected we obtain the following coinductive proof principle proposition coinduction for all sets x y we have x y iff there exists a simulation t such that for all x x x t y note that like for our notion of bisimulation the above notion of simulation is weaker than the standard one from concurrency theory which strictly entails language to account for the algorithm where we can discard pairs using the preorder it suffices to define the upward closure function ps × ps ps × ps as t x y x y t st x y x y a pair belongs to the upward closure t of a relation t s × ps if and only if this pair is subsumed by some pair in t in fact rather than trying to construct a simulation ac attempts to construct a simulation up to upward closure like for hk and hkc this method can be justified by defining the appropriate notion of function showing that any simulation up to an function is contained in a simulation and showing that the upward closure function is theorem any simulation up to is contained in a simulation corollary for all x y ps x y iff y comparing hkc and ac the efficiency of the two algorithms strongly depends on the number of pairs that they need to explore in the following sections and we show that hkc can explore far fewer pairs than ac when checking language inclusion of automata that share some states or when checking language equivalence we would also like to formally prove that a hkc never more than ac and b when checking inclusion of disjoint automata ac never more than hkc unfortunately the validity of these statements highly depends on numerous assumptions about the two algorithms eg on the exploration strategy and their potential proofs seem complicated and not really informative for these reasons we to investigate the formal correspondence at the level of the coinductive proof techniques where it is much language inclusion hkc can ac as explained in section we can check the language inclusion of two sets x y by executing y we now show that for any simulation up to upward closure that proves the inclusion x y there exists a bisimulation up to congruence of the same size which proves the same inclusion for t s × ps let t ps × ps denote the relation x y y x t y lemma we have t ct proof if x y y t then there exists y y such that x y t by definition x y y t and y y ct by the rule u x y y y y ct and since y y x y y ct proposition if t is a simulation up to then t is a bisimulation up to c proof first observe that if t if t s t then t ct s t then t ut t by lemma t note that transitivity and symmetry are not used in the above proofs the constructed bisimulation up to congruence is actually a bisimulation up to context r u id the relation t is not the one computed by the former contains pairs of the shape x y y while the latter has pairs of the shape x y y with x possibly not a singleton however note that manipulating pairs of the two kinds does not change anything since by lemma x y y cr iff for all x x x y y cr inclusion ac can hkc on disjoint automata as shown in section below hkc can be faster than ac thanks to the up to transitivity technique however in the special case where the two automata are disjoint transitivity cannot help and the two algorithms actually match each other suppose that the automaton s o t is built from two disjoint automata s o t and s o t as described in remark let r be the relation obtained by running y with x s and y s all pairs in r are necessarily of the shape xy y with x s and y s let r s × ps denote the relation x y x x x and xy r y lemma if s and s are disjoint then cr r proof suppose that x cr y ie x x with x y cr y by lemma we have x y r and hence x y r by definition of r the pairs it contains can only be used to rewrite from right to left moreover since s and s are disjoint such rewriting steps cannot enable new rewriting rules so that all steps can be performed in parallel we have y r x y r y y x therefore there exist some x y with x x x y r y and y y it follows that x y r hence x y r proposition if s and s are disjoint and if r is a bisimulation up to congruence then r is a simulation up to upward closure proof first observe that for all relations r r if r r then r s r therefore if r cr then r s cr we deduce r s r by lemma ab x a x ab · · · ab xn ab y b y ab · · · ab yn ab z ab z ab · · · ab zn figure family of examples where hkc exponentially improves over ac and hk we have x y z inclusion ac cannot hkc on merged automata the containment of lemma does not hold when s and s are not disjoint since c can exploit transitivity while cannot for a concrete take r x y y y z z and observe that x z cr but x z r this difference makes it possible to find bisimulations up to c that are much smaller than the corresponding simulations up to and for hkc to be more efficient than ac an example where hkc is exponentially better than ac for checking language inclusion of automata sharing some states is given in language equivalence ac cannot hkc ac can be used to check language equivalence by checking the two underlying however checking equivalence directly can be better even in the disjoint case to see this on a simple example consider the dfa on the righthand side of figure if we use ac twice to prove x u we get the following t x u y v y w z v z w t u x v y w y v z w z containing five pairs each instead four pairs are sufficient with hk or hkc thanks to up to symmetry and up to transitivity for a more interesting example consider the family of nfa given in figure where n is an arbitrary natural number taken together the states x and y are equivalent to the state z they the language ab alone the state x resp y the language ab resp ab for i n let xi xx xi yi yy yi and zi zz zi for n i furthermore set x xj jn y n i y yj in the nfa x y can reach all the states of the shape y n i for i n and n i for instance for ni we have xy aa xy ab xy ba and xy bb instead z reaches only n distinct states those of the form zi the smallest bisimulation relating x y and z is r y n i zi i n n i which contains n pairs this is the relation computed by y and up to equivalence technique alone does not help in hk with ac we obtain the tx ty for x y z and tz for x y z where tx xi zi i n ty yi zi i n tz zi y n i i n n i note that tx and ty have size n and tz has size n the language by x or y are known for having a minimal dfa with n states so checking x y z via eg would also require exponential time this is not the case with hkc which requires only polynomial time in this case indeed z builds the relation r x y z x yi yi zi i n x yi xi zi i n which is a bisimulation up to congruence and which only contains n pairs to see that this is a bisimulation up to congruence consider the pair z obtained from xy z after reading the word ba this pair does not belong to r but to its congruence closure indeed we have cr zy r z cr r z cr z r z check lemma in for a complete proof exploiting similarity looking at the example in figure a natural idea would be to first the automaton by graph isomorphism by doing so we would merge the states xi yi zi and we would obtain the following automaton for which checking xy z is much easier ab x a ab y b m ab · · · ab mn ab z ab as shown in one can actually do better with the algorithm by exploiting any preorder contained in language inclusion eg similarity in this section we this technique for in our coinductive framework and we show how this idea can be embedded in hkc resulting in an even stronger algorithm ac with similarity ac for the sake of clarity we fix the preorder to be similarity which can be computed in quadratic time or even less definition similarity similarity is the largest relation on states s × s such that x y entails ox and for all a a x s such that x a x there exists some y such that y a y and x y one extends similarity to a preorder ps × ps on sets of states and to a preorder s × ps × s × ps on pairs as x y if x x y y x y x y x y if x x and y y the new algorithm which we call ac is similar to ac but the is now taken wrt the new preorder formally let ps × ps ps × ps be the function defined for all relations t s × ps by x t y if x y or x y x y for some x y t while ac consists in trying to build a simulation up to ac tries to build a simulation up to ie it a pair x y if either a it is subsumed by another pair of the or b x y theorem any simulation up to is contained in a simulation corollary the algorithm proposed in is sound and complete for all sets x y x y iff y optimisation a and optimisation b in are simply a and b as discussed above another optimisation called optimisation is presented in if y y and y y y for some pair x y then y can be safely removed from y note that while this is useful to store smaller sets it does not allow one to explore less since the pairs encountered with or without optimisation are always equivalent wrt the ordering y y y and for all a a y hkc with similarity hkc although hkc is primarily designed to check language equivalence we can also extend it to exploit the similarity preorder it suffices to notice that for any similarity pair x y we have xy y let denote the relation xy y x y let r denote the constant function to and let c r accordingly we call hkc the algorithm obtained from hkc figure by replacing x y cr todo with x y c r todo in step notice that the latter test can be reduced to rewriting thanks to theorem and the following lemma lemma for all relations r c r cr in other words to check whether x y c r todo it suffices to compute the normal forms of x and y wrt the rules from r todo plus the rules x y y for all x y theorem any bisimulation up to c is contained in a bisimulation proof consider the constant function r × ps mapping all relations to since language equivalence is a bisimulation we immediately obtain that this function is compatible thus so is the function c r we have that is contained in so that any bisimulation up to c is a bisimulation up to c since c is compatible such a relation is contained in a bisimulation by proposition note that in the above proof we can replace by any other relation contained in intuitively bisimulations up to c correspond to classical bisimulations up to bisimilarity from concurrency corollary for all sets x y we have x y iff y relationship between hkc and ac like in section we can show that for any simulation up to there exists a corresponding bisimulation up to c of the same size lemma for all relations t s × ps t c t proposition if t is a simulation up to then t is a bisimulation up to c however even for checking inclusion of disjoint automata ac cannot hkc because now the similarity relation allows one to exploit transitivity to see this consider the example given in figure where we want to check that z x y and for which the similarity relation is shown on the righthand side since this is an inclusion of disjoint automata hkc and ac which do not exploit similarity behave the same cf sections and actually they also behave like hk and they require n pairs on the contrary the use of similarity allows hkc to prove the inclusion with only n pairs by computing the ab x a x k ab · · · ab xn a ab y b y k ab · · · ab yn c ab z ab z k ab · · · ab zn ab xz x z xn zn figure family of examples where hkc exponentially improves over ac for inclusion of disjoint automata we have z xy following bisimulation up to c lemma in r xy i n i n where xi xx xi and zi zz zi like in section to see that this is a bisimulation up to c where we do exploit similarity consider the pair obtained after reading the word ab this pair does not belong to r or cr but it does belong to c r indeed by lemmas and this pair belong to c r iff z and we have r r r x z r r xy on the contrary ac is not able to exploit similarity in this case and it behaves like ac both of them compute the same tz as in the example from section which has n elements in fact even when considering inclusion of disjoint automata the use of similarity to merge states so that hkc can use the up to transitivity technique which ac and ac lack a short figure the relationship the presented algorithms in the general case and in the special case of language inclusion of disjoint automata in this diagram an arrow xy from an algorithm x to y means that a y can explore less states than x and b y can x ie the proof technique of y is at least as powerful as the one of x the labels on the arrows point to the sections showing these relations arrows are not illustrated in this paper they are easily inferred from what we have shown experimental to get an intuition of the average behaviour of hkc on various nfa and to compare it with hk and ac we provide some benchmarks on random automata and on automata obtained from modelchecking problems in both cases we the experiments on a pro intel core i with gb of memory running os x we use our ocaml implementation for hk hkc and hkc while we use the c library for ac and ac to our knowledge is the most efficient implementation currently available for these algorithms even though this library was designed for tree automata rather than plain nfa random automata for a given size n we generate a random nfa with n states and two letters according to we use a linear more precisely the upward algorithms provided by default in general case h hkc f ac hk e ac naive disjoint inclusion case hkc o ac hk naive figure relationship between the various algorithms hk ac ac hkc hkc number of checked nfa e number of processed pairs figure distributions of the number of processed pairs for the nfa with states and letters from table tion density of which means that the expected of each state and with respect to each letter is and showed that one gets more challenging nfa with this particular value we generate nfa without accepting states by doing so we make sure that the algorithms never a counterexample so that they always continue until they find a bisimulation up to these runs correspond to their worst cases for all possible choices of accepting states for the given nfa we run all algorithms on these nfa starting from two distinct singleton sets to measure the required time and the number of processed pairs for hk hkc and hkc this is the number of pairs put into the bisimulation up to r for ac and ac this is the number of pairs inserted into the the for hkc and ac do not include the time required to compute similarity we report the values the last the last and the maximum values in table for instance for n of the examples require less than ms with hk equivalently of the examples require more than ms for a few tests ran out of memory the symbols in the table we also on figure the distribution of the number of processed pairs when n hkc and ac are several orders of magnitude better than hk and hkc is usually two to ten times faster than ac moreover for the first four lines hkc is much more than ac ie the last and maximal values are of the same order as the value ac seems to become more for larger values of n the same relative behaviour can be observed between hkc and ac moreover hkc alone is faster than ac also recall that the size of the relations generated by hk is a lower bound for the number of accessible states of the to get this behaviour for ac and ac we actually had to trick which otherwise starts by removing states and thus reduces any of these nfa to the empty one n s algo required time seconds number of processed pairs size hk ac hkc ac hkc hk ac hkc ac hkc hk ac hkc ac hkc ac hkc ac hkc ac hkc ac hkc ac hkc ac hkc ­ ­ ­ table running the five presented algorithms to check language equivalence on random nfa with two letters nfa lemma one can thus see in table that hkc usually an extremely small portion of these dfa eg less than one per for n the last column reports the size of the minimal dfa for the corresponding parameters as given in hk usually much more states than what would be necessary with a minimal dfa while hkc and ac need much less automata from regular modelchecking checking language inclusion of nfa can be useful for modelchecking where one sometimes has to compute a sequence of nfa by iteratively applying a transducer until a fixpoint is reached to know that the fixpoint is reached one typically has to check whether an nfa is contained in another one et al use such benchmarks to test their algorithm ac against the plain algorithm ac we reuse them to test hkc against ac in a concrete scenario we take the sequences of automata provided by l which come from those used in and arise from the model checking of various programs the algorithm sort and a system for all these sequences we check the of consecutive pairs in both directions we separate the results into those for which a counterexample is found and those for which the inclusion holds the results are given in table as expected hkc and ac roughly behave the same we have of disjoint automata hkc is however quite faster than ac up to transitivity can be exploited thanks to similarity pairs like in table the do not include the time required to compute similarity which is given separately computed using also note that over the positive answers are obtained immediately by similarity related work a similar notion of bisimulation up to congruence has already been used to obtain decidability and complexity results about contextfree processes under the name of introduced this concept to give a and proof of the result by et al bisimilarity is decidable for contextfree processes et al then generalised the result to all contextfree processes also by using et al used a refinement of this notion to get a polynomial algorithm for bisimilarity in the case there are two main differences with the ideas we presented here first the above papers focus on bisimilarity rather than language equivalence recall that although we use bisimulation relations we check language equivalence since we work on the second we consider a notion of bisimulation up to congruence where the congruence is taken with respect to nondeterminism union of sets of states are also bisimulations up to congruence but the congruence is taken with respect to word concatenation we cannot consider this operation in our setting since we do not have the corresponding monoid structure in plain nfa other approaches that are independent from the algebraic structure eg monoids or and the behavioural equivalence eg bisimilarity or language equivalence are shown in these propose very general frameworks into which our up to congruence technique as a very special case to our knowledge bisimulation up to congruence has never been proposed as a technique for proving language equivalence of nfa conclusions and future work we showed that the standard algorithm by hopcroft and for checking language equivalence of dfa relies on a bisimulation up to equivalence proof technique this allowed us to design a new algorithm hkc for the nondeterministic case where we exploit a novel technique called up to congruence we then compared hkc to the recently introduced algorithms ac when checking the inclusion of disjoint automata the two algorithms are equivalent in all the other cases algorithm ac hkc ac hkc sim time pairs counterexamples pairs table in seconds for language inclusion of disjoint nfa generated from modelchecking hkc is more efficient since it can use transitivity to a larger portion of the the difference between these two approaches becomes even more when considering some optimisation exploiting similarity indeed as shown with ac the approach can widely benefit from the knowledge one gets by first computing similarity inspired by this work we showed that both our proof technique bisimulation up to congruence and our algorithm hkc can be easily modified to exploit similarity the resulting algorithm hkc is now more efficient than ac even for checking language inclusion of disjoint automata we provided concrete examples where hkc and hkc are exponentially faster than ac and ac sections and and we proved that the coinductive techniques underlying the are at least as powerful as those exploited by the propositions and we finally compared the algorithms by running them on both randomly generated automata and automata resulting from model checking problems it appears that for these examples hkc and hkc perform better than ac and ac our implementation of the presented algorithms is available online together with coq proof scripts and with an making it possible to run the algorithms on examples as future work we plan to extend our approach to tree automata in particular it seems promising to investigate if further upto techniques can be defined for regular tree expressions for instance the algorithms proposed in exploit some optimisation which suggest us coinductive upto techniques acknowledgments we are to m j and a for the discussions that eventually led to this work and to l p l and b for their help with algorithms references p a yf chen l r and t when simulation meets in proc tacas vol of lncs pages ­ springer a v aho j e hopcroft and j d ullman the design and analysis of computer algorithms addisonwesley a aiken and b r implementing regular tree expressions in vol of lncs pages ­ springer j c m j a and j w decidability of bisimulation equivalence for processes generating contextfree languages in proc ii vol of lncs pages ­ springer f on generalized coinduction and probabilistic specification formats phd thesis amsterdam f and d extended version of this abstract with omitted proofs f and d web appendix for this paper a p and t abstract regular model checking in proc cav vol of lncs springer r e algorithms for boolean function manipulation ieee trans computers ­ d de ­ s h and c bisimulation equivalence is decidable for all contextfree processes information and computation ­ l and algorithms for finite automata in proc tacas vol of lncs springer jc l c and t verification of finite transition systems formal methods in system design ­ m r henzinger t a henzinger and p w computing simulations on finite and infinite graphs in proc pages ­ ieee computer society y m and f a polynomial algorithm for deciding bisimilarity of contextfree processes theoretical computer science ­ l and j s optimizing an algorithm computing and ­ j e hopcroft an n log n algorithm for in a finite automaton in proc international symposium of theory of machines and computations pages ­ academic press j e hopcroft and r m a linear algorithm for testing equivalence of finite automata tr cornell univ december j e hopcroft and j d ullman introduction to automata theory languages and computation addisonwesley h j and b c pierce regular expression types for xml acm trans program lang syst ­ o j and t a library for efficient manipulation of nondeterministic tree automata in tacas vol of lncs pages ­ springer m from settheoretic coinduction to coinduction some results some problems ­ d and g circular coinduction with special contexts in proc vol of lncs pages ­ springer a meyer and l j word problems requiring exponential time in proc pages ­ acm r milner communication and concurrency prentice hall d complete lattices and upto techniques in proc vol of lncs pages ­ springer j automata and coinduction an exercise in coalgebra in proc concur vol of lncs pages ­ springer d sangiorgi on the bisimulation proof method mathematical structures in computer science ­ d sangiorgi introduction to bisimulation and coinduction cambridge university press d and m experimental evaluation of classical automata constructions in proc vol of lncs pages ­ springer m d l t a henzinger and a new algorithm for checking of finite automata in proc cav vol of lncs pages ­ springer 