a kripke logical relation between ml and assembly dreyer max institute for software systems abstract there has recently been great progress in proving the correctness of compilers for realistic languages with realistic runtime systems most work on this problem has focused on proving the correctness of a particular compiler leaving open the question of how to verify the correctness of assembly code that is or linked together from the output of multiple compilers this has led benton and other researchers to propose more abstract compositional notions of when a lowlevel program correctly a highlevel one however the state of the art in socalled compositional compiler correctness has only considered relatively simple highlevel and lowlevel languages in this paper we propose a novel extensional notion of equivalence between highlevel programs in an expressive impure mllike calculus and lowlevel programs in an only slightly assembly language we define this equivalence by means of a stepindexed kripke logical relation which enables us to reason quite about assembly code that uses local state in a different manner than the highlevel code it implements eg code in contrast to prior work we factor our relation in a symmetric fashion which helps to simplify and the formal presentation and we also show how to account for the presence of a garbage collector our approach relies on recent developments in kripke logical relations for mllike languages in particular the idea of possible worlds as state transition systems categories and subject descriptors d programming languages formal definitions and theory d programming languages language constructs and features f logics and meanings of programs specifying and verifying and reasoning about programs general terms languages theory verification keywords stepindexed kripke logical relations compositional compiler correctness garbage collection code this work was while the first author was at paris supported by project hd and engineering research center of program of of science and technology national research foundation of grant r permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm introduction while compiler verification is an problem there has been progress in the last several years in proving the correctness of compilers for realistic languages with realistic runtime systems of particular note is compcert project in which he used the coq proof assistant to both program and verify a optimizing compiler from cminor a intermediate language to assembly has adapted the compcert framework to a compiler for a pure miniml language and et al have extended it to support with a garbage collector independently has developed verified compilers for both pure and impure functional core languages the former with a focus on using custom coq tactics to provide significant automation of verification that said all of the work has focused on proving the correctness of a particular compiler leaving open the question of how to verify the correctness of assembly code that is or linked together from the output of multiple compilers the issue is that compiler correctness results are typically established by a fairly close simulation relation between source and target code but code produced by another compiler may an entirely different simulation relation with the source program and code might not closely simulate the source program at all thus existing correctness proofs depend on the assumption that one has control over how the whole source program is compiled in order to lift the assumption benton and suggest that what is needed is a more abstract extensional notion of what it means for a lowlevel program to correctly implement a highlevel notion that is not to a particular compiler and that moreover offers as much flexibility in the lowlevel representation of highlevel features as possible when reasoning strictly about highlevel programs the canonical extensional notion of when one program implements the same functionality as another is observational or contextual equivalence which says that the two programs exhibit the same termination behavior when placed into the context of an arbitrary enclosing welltyped highlevel program however it is not clear how to define such a contextual notion of equivalence between high and lowlevel programs because there is no way to run both programs under the same would need to quantify over equivalent high and lowlevel program contexts but when are two contexts equivalent we are back to the original question benton and solution is to define a logical relation between the high and lowlevel languages actually two relations one for each direction of semantic approximation a denotational semantics to represent the highlevel side logical relations are inherently two functions are logically related iff they map related arguments to related results regardless of their private implementation guarantee equivalent termination behavior under arbitrary contexts that are themselves logically related while not as canonical as contextual equivalence logical equivalence is nevertheless useful as long as one can establish that the logical relations are sufficiently that they relate enough of interest in the traditional setting where one is defining equivalence of programs in the same language this sufficient population property is by the fundamental theorem of logical relations which states that all welltyped programs and thus all welltyped contexts are logically for mixed relations benton and demonstrate sufficient population by providing a simple compilation translation from their high to their lowlevel language and proving that all welltyped highlevel programs are logically equivalent to their they also use the logical relations to show the of some simple lowlevel code with corresponding highlevel programs benton and present their work as the first step towards compositional compiler correctness however the source language they simplytyped calculus with recursion is purely functional and the target language they relatively highlevel has subsequently proposed a more syntactic approach to proving compositional compiler correctness applicable to a richer impure untyped source language but the target language he considers is also highlevel namely a cps variant of the source contributions in this paper we study compositional equivalence of high and lowlevel programs in a more realistic setting our highlevel language is an expressive mllike cbv calculus supporting abstract types general recursive types and general mutable references our lowlevel language is an only slightly assembly language furthermore our logical relation is designed to be sound in the presence of garbage collection under some fairly abstract assumptions about the behavior of the garbage collector that are satisfied by both and copying collectors following benton and we define our equivalence using a stepindexed logical relation is useful when reasoning about programs such as lowlevel ones whose behavior is contextsensitive and is useful in reasoning about semantically cyclic features like recursive types and higherorder state we from prior work though in that our relation is also a kripke logical it is indexed by possible worlds that specify assumptions about the machine state possible worlds are useful in enforcing invariants about lowlevel data structures eg that a representation of a closure is immutable they are also helpful in encoding a variety of runtime system invariants such as the convention concerning registers and the notion of data liveness last but not least possible worlds enable us to reason quite about assembly code that uses local state in a different manner than the highlevel code it implements an interesting example of this is assembly code whose correctness proof involves reasoning about lowlevel internal state changes to the code itself that clearly have no highlevel counterpart this is the essence of what we mean when we say that our relation is extensional technically our approach relies closely on recent developments in kripke logical relations for mllike languages in particular the idea of possible worlds as state transition systems this idea which we review in section was proposed originally by ahmed et al in a somewhat different form as a way to reason about representation independence for socalled generative abstract data types whose private state a controlled series of state transitions during the execution of the program dreyer et al have subsequently generalized the idea in order to reason about state changes by our logical relations on this most recent work we are able to model a variety of state transition systems that arise naturally in lowlevel code eg in code a novel feature of our logical relation is that while it is defined by induction on an mllike type structure it is also defined in a fashion that is it may be instantiated to form an equivalence relation between any two languages lowlevel that are capable of implementing various relevant function application a continuation with a value etc factoring the relation in this way helps to simplify and the formal presentation moreover it has the advantage that the relation becomes inherently symmetric and thus easier to use in proving equivalences than benton and asymmetric approximation relations high and low the highlevel language high is a system polymorphic calculus extended with existential product and types as well as general references higherorder state figure shows the syntax and the typing and evaluation judgments the inference rules for typing and evaluation are standard so we omit them see the companion technical appendix for details the lowlevel language low is an assembly language in two ways its word and memory sizes are infinite and its instructions are represented by abstract objects rather than by physical words its memory consists of four entities code memory register file stack and heap a code memory is a map from physical addresses represented by natural numbers to instructions a register file is a map from registers to words words in turn are represented by natural numbers with an extra bit indicating whether the word is a pointer to a heap cell or not useful for garbage collection purposes there are registers half of which sv are specified as registers by our calling convention both a stack and a heap are memories ie maps from addresses to words the instruction set includes standard instructions jmp move plus supporting different addressing modes via lvalues and the nonstandard operations include halt for normal termination fail for raising a runtime error for testing whether a value is a pointer or not for marking a pointer bit and for and updating instructions in code memory note that as instructions are not represented by words we employ a bijection e and its inverse d for encode and to convert back and between instructions and words the dynamic semantics of low is standard and is given in figure a machine configuration pc is a pair consisting of a memory and a program counter it halts fails or to another configuration pc by executing the instruction stored at pc in the code memory code the key ideas in this section we present the key ideas behind our work through the lens of an challenging example we will through the code of this example suggest intuitively how to reason about it and then explain how our kripke logical relation formalizes this intuition while we will initially ignore the question of how garbage collection affects we will return to this issue at the end of the section and discuss how our logical relation enables reasoning in the presence of a garbage collector this section is intended to be accessible to a broad and to serve as a useful guide to the high ­ syntax semantics b × ref e x e e e e x e e e e e pack e as e as x in e roll e e ref e e e e e e v x v v x e e pack v as roll v k · k e v k k k k e v k k roll k k pack k as k as x in e ref k k e v k k k e v k · with ftv · · x static semantics e def v def h fin dynamic semantics h e h e low ­ syntax def pc × def code reg stk hp × × × def instruction def register def def def a n def w × n r register sp sv sv wk wk lv r a s r o s a h r o h rv lv w instruction fail halt jmp rv rv rv rv rv rv rv rv lv move lv rv plus lv rv rv lv rv rv lv rv rv rv low ­ semantics w def w def w n def n a def a w def w r def a a s h def def ro ro s h def def stk r hp r o o r w def code w stk hp a s w r o s w a h w r o h w def def def def code reg w hp code reg stk r o w hp code reg stk w code reg stk hp r o w fail pc def fail halt pc def halt jmp rv pc def rv rv rv pc def if rv then rv else pc rv rv rv pc def if rv rv rv rv pc def then rv else pc if move lv rv pc then rv else pc def lv rv pc lv pc def lv lv pc plus lv rv rv pc def lv rv rv pc lv rv rv pc def lv rv rv pc lv rv pc def lv pc rv rv pc def code rv reg stk hp pc where e instruction n is a bijection and d e dynamic semantics pc pc figure syntax and semantics for the high and low languages of readers who are not familiar with recent developments in kripke logical relations a motivating example our motivating example is based on pitts and example their original example is almost simple prove that the following high terms are contextually equivalent e def let x ref in f unit unit x f x e def f unit unit f the first term e allocates a fresh memory location x initially set to and then returns a higherorder function when called the latter will set x to invoke its callback argument f and then return the contents of x the second term e is similar except that it does not allocating or updating any memory and the function it defines always returns proving that e and e are contextually equivalent is to showing that in the former whenever the callback invocation f returns x points to after a thought it should be intuitively clear why the equivalence holds the pointer x in e is initially set to but once the function that e returns is applied for the first time x will be set to and will never be set back to this is because e x as a piece of local state and the only thing e ever does to x after first allocating it is to set it to thus the example serves as an elegant of the ability of code to control some local state and impose arbitrary constraints on it and the ability of that local state to over time in an or monotone way such changes to local state arise in a variety of realworld in generative whose sets of grow over time and in data structure initialization it is thus rather that only recently have methods been developed for proving an equivalence as simple as the example more on that in section we are now ready to present our motivating example we want to prove a variant of the example namely that the high term e is implemented correctly by a low program p where ps implementation follows the second high term e fairly closely by itself that would already be an interesting would demonstrate the extensional equivalence of a highlevel program with an optimized lowlevel program where the optimization is based on the of highlevel program contexts to observe es manipulation of local state but we will make it more interesting still with an added the code of the function that p evaluates to will be using a primitive form of encryption and when first applied the function will first and itself via code the low program p is shown in figure before we through the code let us first note that p is parameterized by alloc a code pointer to the memory allocation routine and bg the location in the code segment where ps code will be loaded and where its execution will parameters will be instantiated as part of linking and loading we will define the formal semantics of linking and loading for low programs in section bg create and return a closure the evaluation of p is very simple it does no interesting computation except to immediately create and return a closure value just as the high term e does the first instructions starting at bg allocate a fresh closure on the heap by invoking the alloc routine passing it the size parameter in register wk and the return address bg in register wk we only need one word for the closure because the function were implementing e is closed so all we need to store in the closure is the code pointer the alloc routine is assumed to return the pointer to a fresh cell in wk without modifying the contents of any registers except wk and e def let x ref in f unit unit x f p def alloc bg bg move wk bg move jmp wk alloc bg move jmp wk h bg wk x bg move wk bg wk wk wk plus wk bg bg bg bg bg wk h bg sp sp s wk wk wk h bg wk sp bg sp s bg wk wk wk wk wk bg bg bg sp wk wk bg sp figure motivating example wk we then store in that cell the code pointer bg before to the return address which we assume the had passed to p originally in register wk the has essentially the same calling convention as for ordinary functions which is different from the one for alloc and is described below see bg although our calling convention is that return values are passed back to the caller in wk our return value was already in wk after the call to alloc so we need not explicitly move anything into wk before returning we now describe the implementation of the closure returned in wk by the evaluation of p initially this closure contains just a code pointer to bg but eventually that code pointer will be updated see below bg and the code the code from bg on is by the addition of to the machine representation of each instruction eventually once the code is the function will be executable starting at the address bg before that time however the function must begin executing at bg because the first step will be to and the instructions the reader can easily verify manually that the code starting at bg will use register wk to loop through the instructions bg through bg for each it will use to read the instruction stored at wk into wk from it and then write the instruction back to the code segment at address wk when this loop is finished the program counter will be at bg bg the first instruction of the function having the code we do not want future calls to this function to perform the decryption again we therefore the first instruction at bg with a jump to bg bg update the code pointer the low calling convention is that a function is passed its return address in wk its argument in wk and its own closure in wk the decryption code starting at bg did not touch any of these registers so at bg we know that wk still stores a closure with a code pointer to bg having the code we can now safely update this code pointer wk point to bg the address where the function begins its computation in an important point why did we both the bg instruction to jump to bg and updating the code pointer in the function closure would the latter alone not have been sufficient the answer is that it depends if p were only evaluated once in which case only one closure for this function were ever generated then yes just updating the code pointer would be sufficient because bg would become effectively dead code but we would like our notion of program equivalence to be preserved under a rich set of program contexts including those that evaluate the p and than once repeated evaluation of p will result in the creation of multiple closures for the function p defines and merely updating the code pointer for one closure will not change the fact that other closures may still point to bg so it is necessary to the bg instruction as well bg implement f f this is the implementation of the function proper we first push our return address wk onto the stack we then invoke the callback argument f in the high code by moving a pointer to f s closure into wk moving the return address bg into wk and to f s code pointer wk h note f s argument type is unit so there is no need to pass anything in the argument register wk when control is returned to bg we store the result in the result register wk pop the return address off the stack and jump to it discussion of the motivating example why does p implement e intuitively the reason is that the aspects of p are not visible to ps clients because they do not affect its extensional behavior and ultimately once p has itself it behaves essentially the same as the high term e which we have already argued is equivalent to e of course this the question how exactly do we know that ps clients cannot observe its interestingly the answer is similar to the argument for why e and e are equivalent namely that what p does to its own code takes the form of changes to local state specifically we take it as a given that p its own code and since the evaluation of p will allocate a fresh memory cell for the closure it returns p that closure as well thus in reasoning about p we can place restrictions on how its code and closure may over time much as the local variable x in e starts out pointing to and eventually points to the code of p starts out in form and any closure it returns is first applied it changes to form in both cases it is critical that we never to the earlier state similarly the closure returned by the evaluation of p starts out with its code pointer set to bg but the closure is ever applied it will be set to bg in this case it is not so essential for correctness that the code pointer never back to bg but it is essential that the closures code pointer only be set to bg when the code is in the state given these restrictions on how the local state of p and e may it is but a short distance to a proof before that proof let us first review the recent work on kripke logical relations that will put our reasoning on a kripke logical relations and state transition systems logical relations are a technique for reasoning about equivalence of higherorder programs a logical relation is defined inductively on the type structure of the language at base type the logical relation coincides with observable two programs of type int are logically related if they produce the same at higher type the relation is defined by interpreting each type operator by the appropriate logical connective eg two functions are related at type if of their arguments at type implies of their results at type the important feature about logical relations for our purposes is that they give considerable to how related functions are im so long as they produce related results as benton and put it the ends justify the means in the presence of state we cannot talk about the of two programs without making some assumptions and ing some restrictions on how they manipulate state this is where kripke logical relations come in kripke logical relations are in by possible worlds which represent a set of restrictions on the memories of the two programs under which the programs are guaranteed to behave equivalently when we want to prove the re of two programs under a world w we suppose we are given arbitrary initial memories that are related by ie satisfy the restrictions of w and we proceed typically by showing that when evaluated under those memories the programs either both di dont terminate or else produce values and final mem that are related under some future world w of w what does it mean for w to be a future world of w if in the course of evaluation the programs allocate fresh pieces of memory w may extend the initial world w with new restrictions the use of the allocated memory this approach allows us to establish whatever constraints we want on allocated state that is kept local if the state is made globally by being passed to the context at a ref the state will have to the usual invariants by the ref type in traditional kripke logical relations such as those of pitts and possible worlds essentially take the form of simple memory relations ie memory invariants as we have seen in the example however memory invariants are not enough we need additionally the ability to describe assumptions about state that may change in a controlled and monotone way it is thus not a that pitts and put the example as an example for which their method was to address this limitation ahmed et al proposed possible worlds to include the ability for a memory rela tion to dreyer et al later and extended ahmed et als approach in various ways and cast ahmed et als possible worlds as collections of state transition systems in the case of the example one can understand the re placed on es local variable x according to the following sts x x when this sts is first added to the initial world w it starts out in the x state because after x is first allocated it points to however under dreyer et als model future worlds of w may not only place additional restrictions on fresh pieces of memory but also update the state of existing in w thus in some future world w the above sts may be to the x state and memories satisfying w would have to map x to furthermore any future world of w would have to remain in the x state as there is no transition out of it this corresponds to the intuitive reasoning about the example that we described in section state transition systems for the motivating example using kripke logical relations based on state transition systems we can now roughly sketch the proof of equivalence of p and e we will prove that p and e are logically related in some initial world w that includes some basic assumptions in the form of about registers the stack etc see section for details first since we can assume p has just been loaded into memory we can think of its code as a allocated piece of memory and we are therefore given the to extend w with an sts ps code for most programs we would extend w at this point with a sts representing the simple invariant that the code of the program never changes for our motivating example we instead extend w with an sts of the form when the sts is in the left state the associated memory relation will require that ps code be in its initial form and when the sts is in the right state the memory relation will require that ps code be in form once always when p and e are executed the former allocates a fresh closure on the heap setting its constituent code pointer to bg and the latter allocates the local ref cell x setting its contents to since both the closure and x are allocated we may at that point also extend the world with a new sts both of them bg we have the assumptions about them into one sts because they change in when the functions returned by p and e are applied for the first time x gets updated to and the closures code pointer gets updated to bg simultaneously as noted in section it should never be the case that the code is still while the closure returned by p points to bg the closure would behave in an unspecified manner if it were called in such a state when adding the second sts it is therefore important that we this possibility up front so that we will not have to consider it later fortunately dreyer et als model on which our logical relation is based allows us to define the second sts in such a way that we can only be in its right state if the first sts is also in its right state let w be the world resulting from extending w with the above two what remains to be shown is that the functions returned by p and e are related in world w so suppose that w is a future world of w and that we begin executing the high and lowlevel functions in some corresponding high and lowlevel memories that are related by w there are three cases to consider depending on the states of the two of interest in w the fourth case was as described above case code is x closure bg in this case we first the code and then set x to and the closures code pointer to bg case code is x closure bg in this case we set x to and the closures code pointer to bg case code is x closure bg in this case we set x to and dont touch the closures code pointer in all three cases we end up to a future world w in which both of interest are in the right state if they already there in w after making the state transition both the high and lowlevel functions invoke their callback arguments assuming they return they will do so with memories that satisfy some future world of w but in the of interest there is to transition to so we know that x must still point to the high and lowlevel functions must therefore both return the same result namely along with memories that satisfy a future world of the starting world w state changes and private transitions for simplicity we have over many details in the above proof sketch one important detail is how we reason about the stack when the functions invoke their unknown callback arguments we need to know that the callback will return the stack ie the contents of the stack segment up to the stack pointer sp as it found it one approach would be to this condition into our logical relation for functions however as we explained in the introduction we have set out to define our relation in a largely fashion and it would not make sense to a lowlevel property about the stack into a relation instead we wish to build this condition into the initial world under which we relate high and lowlevel programs but the property we of the stack is not expressible in terms of as we have described them so far to account for behavior we employ another aspect of dreyer et als possible worlds namely the idea of private vs public transitions private transitions were introduced in order to reason about socalled state changes of which the behavior of the stack is a perfect example the basic idea is to label local state transitions as either public or private functions may make either private or public transitions internally but viewed ie they must appear to make a public transition in the sts that we use to reason about the stack the states of the sts correspond to the possible states of the stack every state is accessible from every other state by a private transition but the only public transitions are this grants functions of flexibility in how they manipulate the stack but requires that when they return they leave the stack exactly how they found it we also use private transitions to reason about registers which functions are expected to return as they found them even if they modify them internally reasoning in the presence of garbage collection another important detail we have over is how the presence of a garbage collector affects our proof of equivalence of p and e let us assume that we are using a standard or copying collector the main effect such a collector has on our reasoning is that whenever we pass control to the allocator or to an unknown function that may call the allocator we need to make sure that all data that we care about being able to access in the future is in the reachable portion of the heap and there are no dangling pointers in the reachable portion of the heap typically these two conditions is straightforward in our example there is one call to the allocator at bg and one call to an unknown function at bg in the case of the former there is nothing interesting to show but in the case of the latter it is important that when we increment the stack pointer before invoking the callback we set the contents at the top of the stack to a value from which no dangling pointer may be reached here we store in that stack slot the return address wk which trivially satisfies this requirement so at an abstract level reasoning in the presence of garbage collection is no big deal what is more interesting is the technical question of how we actually implement this reasoning in the context of our logical relation the central difficulty is that kripke logical relations traditionally a monotonicity property meaning that when two values are related in a world w they are related in any future world of w monotonicity is essential when reasoning about unknown functions such as the callback argument in our example there we were given the assumption that the callback argument was logically related in the world w but we did not actually invoke it until we had to the future world w this step requires a use of monotonicity to show that the callback argument is still related in w unfortunately garbage collection seems to throw a into monotonicity for instance in our logical relation we want to be able to relate a high pair value v v with a memory location pointing to a representation of that pair value on the heap how can we expect those two value representations to be related in all future worlds if at some point in the future the memory location may get deallocated or its contents moved by a moving collector and later reused for storing something else our solution is to employ logical memories which form a layer of abstraction over physical memories pointers in a logical memory are never moved or deallocated their connection to is established by a component of the logical memory called the lookup table which specifies for any given logical pointer whether it is live and if so what physical pointer it corresponds to we say that a logical memory m represents a physical memory only if ms lookup table describes a bijection between the reachable portions of m and we maintain a global invariant on the logical memory which must hold before and after calls to the allocator requiring that all reachable data be live and thus not dangling according to the lookup table together with the definition of what it means for a logical memory to represent a physical one this invariant guarantees the precondition there are no reachable dangling pointers after the allocator returns the lookup table of the logical memory may have completely due to a the only other change to the logical memory will be its extension with a allocated logical pointer thus if any data we care about was reachable prior to will still be reachable and by the global invariant it will also still be live with logical memories in hand we can adapt our logical relations accordingly so that they relate values in high with logical values ie logical pointers or data in low we also define our possible worlds to impose invariants on logical memories not physical ones in this way we monotonicity as well as a clean abstract account of memory locations that gives the garbage collector significant flexibility in how it implements them and allows us to essentially ignore how it does so a kripke logical relation figure defines a kripke logical relation between two languages that is parameterized by abstract specifications for those languages as well as by a specification for the possible worlds relating the memories of those languages language specifications a language specification upper left of figure must provide sets of values val computations com continuations cont memories mem and configurations conf together with a number of operations on these sets note that we are assuming here a standard on sets and that val com etc are smaller than most of these operations take elements of some of these sets and return a predicate on another of the sets in most cases this is because there may be a number of different representations of the same in low a pair value v v is represented by a pointer that satisfies some conditions but many pointers may satisfy those conditions forms a configuration by a value into a continuation under a given memory does the same but a computation instead of a value step executes a configuration for one step of computation resulting in either a new configuration termination halt or failure fail returns the domain of a memory and takes two memories and returns a memory that contains their disjoint union if it exists determines whether a value is considered syntactically to have type under a certain memory in our specification for high we include heap typings in memories in order to define this predicate at ref type for low there is no notion of syntactic typing but we find convenient for expressing assumptions about the syntactic structure of closures see section def ftv def val com cont mem conf step pair app pack roll ref val com cont mem conf set val × cont × mem com × cont × mem step conf conf fail halt mem pval mem × mem pval × mem b pval × mem pair val × val pval × mem app val × val val × pack × val pval × mem roll val pval × mem ref val pval × mem mem × val × val mem m m m m for l l def world m b o world set world n m world × b world × o world × world world × world × world are w w w w w w w w w w w w w w w def w w w w def r × × rw r r def def def v v w v v r w v v w v v w w w v w v v v r r r w def w v v w w w v v r r r def w v v m m v m r v m r for r × r × def r r def def def v def w v v w v v r v b def w v v x b v × def w v v w v v × u u v w u u v w v def w v v w v v u u w w u u v w v def e w v u e v u w e w w r e e e e v def w w e e v v e r r u u v rw v ref def w v v w v v l pack u u w w m m v v bw u u v w v m v m u u v w v u v u v def f f def r w v v u u v rw f w def f w v v f w w k def w k k world × × w w v v v w m m c k m c k m c c ow e def w e e world × × k k k w m m c k m c k m c c ow figure language specifications world specifications and a kripke logical relation the remaining operations define the various syntactic language forms that are referenced in the definition of the logical relation one point of note determining whether a value represents a particular canonical form may require one to consider an memory it is easy to see one cannot determine if a pointer to a pair of heap cells represents v v without the heap determining whether a computation represents a function application also may require the memory but we have found it technically more convenient to assume that any memory inspection is built into the notion of computation see section to see how this works in the low specification world specifications a world specification lower left of figure defines a set of possible worlds relating the memories of two languages specified by l and l together with a variety of operations on and relations indexed by those worlds in order to understand some of these it is necessary to first say a word about our use of appel and introduced in order to model recursive types in foundational proofcarrying code the basic idea is to use a natural number index step level to what would otherwise be a circular construction has also proven useful in modeling other semantically circular notions like higherorder state which is how we use them here due to space considerations since our stepindexed construction follows closely that of and we refer the interested reader to those previous works for the relevant background in the world specification the important bits are the function which returns the step level of a given world and the later operator which returns an approximated version of the given world at one lower step level we use to ensure of the logical relation is the memory relation associated with w which specifies when two memories from l and l satisfy the constraints of the in w bw is a bijection on values representing memory locations in l and l this is used in defining the logical relation for ref type ow is an observation relation on configurations for the possible worlds we employ in this paper ow actually only depends on and it is defined to relate configurations that either both terminate without failure or that both run for at least steps of computation when we prove that two programs are logically related we will prove it in starting worlds of an arbitrary step level thus ensuring that the programs are equivalent for arbitrarily many computation steps finally defines the general future world relation between worlds and defines a restricted public version of that relation if w w then for any sts in w the new state of that sts in w must be accessible from its old state in w only by public transitions see section both and are kripke logical relation the right side of figure our kripke logical relation whose definition is parametric wrt l l and an instance of in the definition we adopt the convention that the entities values continuations etc from l appear in v k etc and the entities from l appear in v k etc the of the notation for l entities with the notation for the corresponding entities from high is for in the next section we will instantiate l and l with our specifications for low and high respectively we notation in this way in order to avoid the of more than two our logical relation is based very closely on dreyer et als with the principal difference being that the relevant forms have been abstracted away in the language specifications l and l for instance in the logical relation for arrow types we do not construct the applications vu and vu directly since l and l may not include an explicit application construct rather we quantify over arbitrary computations e and e drawn from u and u respectively the logical relation consists of a relation for values v one for continuations k and one for computations e here we assume that is a relational interpretation of the free variables of mapping them to arbitrary value relations for v is defined as the restriction of to triples w v v where v and v are welltyped according to and continue to be related in all future worlds of w this last part which is specified using the r operator defined at the top right of the figure is key to ensuring monotonicity of the value relation the pair and existential cases of the value relation also use the r operator in order to ensure monotonicity of data that if v represents a pair of u and u it will continue to do so in all future worlds as in appel et al the interpretation of recursive types is defined by induction on the strictly future world relation this relation is wellfounded because w w implies that w has a lower step level than w by defining the recursive type case this way we can relate high programs where roll and are explicit coercions to low programs where they have been erased the ref type case relates two memory locations if dereferencing assigning and testing them for pointer equality will always produce related results the condition on pointer equality testing is guaranteed by the requirement that the locations be in the bijection of the world in which they are related the value relation is lifted to a relation on computations by the technique of closure the idea is to define two computations to be related if they behave in an equivalent manner when into related continuations two continuations are in turn related if they behave in an equivalent manner when with related values by only over public future worlds in the definition of k we ensure that computations may only make public transitions when viewed as per the discussion in section this technique is to languages where the evaluation of a computation is contextsensitive in the sense that it cannot be performed in of its continuation such is the case with low where a computation always ends with a jump to its return address as we shall see the fact that the return address really is a valid return address which is part of the contract between computation and continuation will be encoded in the low implementation of that we define in section implementing the specifications in order to instantiate our logical relations to relate low and high entities we must first show how to implement the abstract interface for both languages for high the implementation of the interface is almost entirely straightforward as all the required entities values computations have direct in the high language the only slightly bit is that we define high memories to be pairs of heaps and heap typings the inclusion of the heap typing is necessary for defining for the remaining details see the companion technical appendix lowlevel entities the implementation of for low figure is much more interesting as described in section we employ a notion of logical values v which are either non pointer words w or logical pointers l logical lvalues are similar to physical except that logical lvalues include logical heap locations l o h in place of physical ones a h we include the offset o because the logical heap is for convenience modeled as a list of blocks low computations e are tuples data where is the code address where the computation begins is the return address is the lvalue where the return value will be stored and data is a memory predicate that must be satisfied in order for the computation to be correctly executed low continuations k are pairs where is the code address where the continuation begins and is the lvalue where the input to the continuation should be placed it is worth noting that unlike and the in k must be a code address not an because when we an e into a k we need to know that the value of will be the same before and after the execution of e were an we would not know that logical memories m are tuples code reg stk hp where code is the code segment reg is the register file sp since it is determined by the size of the stack stk is the stack hp is the heap is the lookup table described in section and is the system heap which is a separate portion of the heap controlled by the runtime system the lookup table maps each logical pointer to a physical pointer and the size of the memory block starting at that address the pointer is live iff the size is note that reg stk and hp are all maps from various lvalues to logical values whereas the and are maps to physical values in the proofs we end up treating and as essentially black boxes since they can be changed at by the allocator whereas the allocator should not around with the logical portion of the memory represented by reg stk and hp note that reg stk and may all be undefined undef this is a useful technical device for defining the disjoint union of several partial memories it enables us to specify that only one of those partial memories contains information about say the stack see the definition of below lowlevel representations of highlevel constructs the bottom left of figure defines low representations of various highlevel constructs as required by the interface a pair of v and v is represented as a pointer to a pair of cells containing v loc word def l n def w n v val w l lv lvalue r a s r o s l o h r o h rv lv v com def e data cont list x stack heap table mem × × lvalue × def k × lvalue def def fin instruction register sp val undef def def x xn n n x xn x list val undef def def def loc fin list val loc fin n × undef word undef def m code reg stk hp × × stack × heap × table × conf def def v m val × mem v def w if v w l if v l mv def v m r def m m a s lo h def def m m r r o o s h def def o mt s def t s ml o def v vo v vo vn if v vn o n w if v w def a undef if v l n a otherwise def a n m def code reg j hp l w v l w l n a n a n n l w v l w def v m val × mem v is a representation of x v def v m val × mem l v l k m def pc conf m pc v k m def pc conf m m v v pc v def e com l v l l h wk wk m mem v v step pc def r with pc r def l val l def e com l v l m def m mem l h wk wk pack v def m mem v m val × mem v v v def v m val × mem v v def v m val × mem l v l v v v def ml undef if v l otherwise x x def x undef xx x undef xx figure the implementation of for low and v pack v and are represented the same as v references are represented directly as pointers we also use to enforce that values of arrow and universal type are represented by pointers to closures whose first cell is not a logical pointer this ensures that we can jump to the code address directly the most interesting bit is the representation of v and which is similar in order for a computation e to represent this application v is assumed to be a pointer l to a closure the starting address of the function application is thus taken to be the code address stored in the first cell of the closure ie l h our calling convention is that when the function is called the return address should be stored in wk and when the function returns the return value should be stored in wk so and reflect this convention finally the memory predicate requires that when control is passed to the function v and argument v are stored in wk and wk connecting logical and physical memories the right side of figure defines the remaining elements of along with a number of auxiliary operations the operations at the top right give shorthand for various lookup and update operations on logical memories returns the physical interpretation of v according to ms lookup table if one exists and returns the live portion of the physical heap according to ms lookup table note that the definition of demands that the physical representations of logical memory blocks with distinct head pointers be disjoint thus ensuring a proper bijection between the reachable parts of the physical and logical heaps m says that m is a valid logical abstraction of the definition is fairly straightforward making use of and as one would expect the fourth line of the definition guarantees that the reachable heap is disjoint from the system heap and the fifth condition just checks that the block sizes specified for live data in are correct note that may contain arbitrary other in the code stack and heap segments that is not described by m continuations using m it is easy to specify how to continuations with values and computations a configuration pc belongs to k m if m is a valid abstraction of the program counter pc is set to the starting address of the continuation and the value v is stored in the location where the continuation is it a configuration pc belongs to k m if m is a valid abstraction of m satisfies the memory constraints by e the program counter pc is set to the starting address of the computation the starting address of the continuation is stored in the place where the computation is to find its return address and the place where e will store its return value is the same place where k is to find its input value the remaining definitions of step and are fairly as mentioned earlier the definition of bg def bg c code def c undef undef undef undef mem h heap def undef undef h undef undef mem v live in m def if v w n a n a n if v l def l r register l l j l def l l j l j def in def a init alloc i × × list instruction × pc × code pc m pc code code m code m m m m pc n m m m pc n wn pc m m m m mt l reg l w def m mem l l live in m def m mem figure abstract specification of the memory allocator uses the undef option for reg stk and to ensure that if a memory is split into disjoint pieces each of these components can only appear in one of the pieces possible worlds the model of possible worlds that we use to implement is based very closely on dreyer et al for space reasons we will therefore not present the details of the model in this paper and instead refer the reader to the appendix see section for a more detailed discussion of how our model relates to dreyer et als that said there are a few aspects of the model that are needed for understanding the definition of compositional program equivalence that we will give in section in the model worlds w are tuples k gr where k is w s step level is a finite set of state transition systems of the sort described in section and gr is a global invariant the entire memory we call the islands because they disjoint pieces of memory two memories are related by w if they satisfy its global invariant gr and they can be split into disjoint memories one for each island such that the nth pair of memories satisfies the local memory relation determined by the current state of the nth island in in our definition of program equivalence we will use gr to enforce the property that all reachable data is live we need to use a global invariant since reachability cannot be determined by looking at a local we use islands to express all other assumptions about memory assumptions about the memory allocator figure shows the assumptions we make about memory allocation and garbage collection in the form of the specification given as input a starting physical address the runtime system represented by a will return a tuple init alloc code i where init is the starting address of the initialization routine that sets up the runtime system alloc is the starting address of the allocator code is the list of instructions defining the runtime system which are assumed to be loaded at address and i is a private invariant of the runtime system which describes when a logical lookup table is in sync with its system heap the assumption about code is together with the private invariant i to form the memory predicate mr defined at the bottom of the figure assuming init is invoked with the runtime system code in the right place and with a return address placed in wk its specification says it will return control in a memory that satisfies along with the global invariant that all reachable data is live assuming that alloc is invoked in a physical memory represented abstractly by the logical memory m that m satisfies the mr and gr properties that the number of cells to be allocated n is stored in wk and that the return address is stored in wk the specification of alloc says that it will return a pointer to a fresh block in wk and that the memory it returns m will continue to satisfy all the invariants moreover while the lookup table and system heap of m may be completely different from those of m the contents of m must remain otherwise unchanged this does not of course prevent the allocator from having performed a gc any logical pointer that was not reachable in m before the call to alloc may very well be marked as dead in the lookup table of the m but any pointer that was reachable in m will still be reachable in m and thus by the definition of gr still be live our specification of the runtime system provides considerable example it should be satisfied by either a or a copying collector because the specification says nothing about the private invariant of the runtime system however it does assume that the collector places no restrictions such as read or write barriers on what the mutator does to live data we believe it should be possible to adapt our approach to a range of collectors but we leave that to future work compositional program equivalence the logical relation e characterizes what it means for two computations to be logically equivalent but ultimately what we really care about is whether a pair of high and low programs are logically equivalent what one may is the difference between computations and programs in short a program is what you write and a computation is what you run that is a program is a piece of code that must be linked with other programs and loaded into memory before it can be executed whereas a computation describes the next thing to be executed in a running machine configuration for the high language the distinction between computations and programs can be easily over because the operational semantics of high is defined directly on high programs for the low language however especially given the ability to write code it is important to distinguish the two notions in this section we explain what a low program is and how to define logical equivalence between high and low programs and we then present our key technical results equivalence of high and low programs as can already be seen from our motivating example in section we define a low program p to be a function from two code pointers to a list of instructions the first of the code pointer inputs is assumed to be the address of the memory allocation routine and the second is assumed to be the address where the list of instructions returned by the program will be loaded into memory def e def p × list instruction d · def d def r d r g · def w v w world v g x def w v x v v v w v v × w v v v w v g def k bg w e def w w d v g w bg wk wk m v e e w where e p e def e a bg k w m m m m m bg w w m m bg w e figure program equivalence when listing the code of a program eg in figure we write line numbers on selected lines of code eg bg to indicate the physical addresses where we expect the code to be loaded but note that these addresses are always relative to the second parameter of the program typically named bg so that the code is always and the notation is merely line numbers are not part of the actual program now concerning equivalence of high and low programs it is possible to define a notion of logical equivalence strictly between closed programs but we will find it useful when reasoning about compiler correctness to generalize this relation to one on open pro grams on the high side an open program is simply an expres sion e with free variables and no free locations but what is an open program on the low side in order to answer this we need to pick an convention specifying what is an lowlevel representation of a highlevel environment and where the lowlevel program expects to get its environment data in figure we give a logical relation g between lowlevel values and highlevel environments this relation specifies that a highlevel environment should be represented as a linked list of lowlevel values that are related to the highlevel values in the range of we assume that the environment data is passed in the register assumption will be in the definition of the program equivalence relation see below before we define program equivalence we must also specify the invariants on memories that are required for executing programs these conditions are the k in wk represented by the simply determines initial worlds wk in step level but does not otherwise affect the definition the initial worlds consist of three islands and one global invariant the register file and the stack in the low memory and requires that reg and stack be preserved before and after function calls this is accomplished using a combination of private and public tions as discussed in section requires that the heap typing in the high memory should only grow in future worlds the lookup table and the system heap and en forces the private invariant of the runtime system figure and the global invariant specifies that all reachable blocks in the low memory are live in figure and that the high memory satisfies its heap typing all these components of wk are formally defined in the appendix we are now ready to define the program equivalence relation the judgment p e says that a low program p and a high program e are equivalent if the following is true first let k be any starting step level and let w be any future world of wk that does not already impose any invariants on the code segment of p this latter condition is guaranteed by the assumption that we are given initial memories m and m related by w but where m does not contain the code segment of p we use the notation m m here to denote the smallest memory in m in a sense defined formally in the appendix under these assumptions we must be able to extend w to a future world w of the same step level such that w relates m and m where m is m extended with the code of p intuitively this step p the to own its own code segment by extending w with an island it typically this island will take the form of an invariant stating that ps code must never be modified although in the case of code we would instead define the island to be a state transition system as described in section show that the high computation e is related under the world w constructed in the previous step to the low computation e that starts at the beginning bg of ps code segment this is in the open computation equivalence judgment bg w e figure this judgment says that for any future world w of w for any relational interpretation of the type variables in and for any environments v and related under w by g the high computation e is logically related by e under w to the low computation starting at address bg the other three components of the low computation respectively that the computation expects to find its return address stored in wk the computation will store its resulting value in wk and the computation expects to find its environment stored in sv one can think of this last assumption about sv as having the effect of closing p in much the same way that and close e compiler correctness and other technical results our first result is an adequacy theorem for our program equivalence relation the statement of the theorem refers to the following simple for the low language p first runs the initialization routine of the memory allocator a then executes p and finally halts as soon as it gets control back from p p let init alloc a move wk jmp init move wk jmp halt in code the adequacy theorem states that closed high and low programs that are equivalent according to must when loaded by the above theorem adequacy for all p e a pc p h both pc and h e diverge or both halt without fail one might think that this adequacy result is weak because the does not the result returned by the program p how p alloc bg let bg c bg c c in bg plus sp sp move move move sp s sp s wk wk bg c bg c move move sp s wk wk bg c c bg c c move move move wk wk wk sp s sp s wk sp sp jmp wk h figure compilation of function application ever together with the compositionality result below one can link the program p with arbitrary wellbehaved test programs and the linked programs are guaranteed to behave the same in order to show that our logical relations are sufficiently important as we explained in the have written a very compiler from high to low and proved that every source program is related to its compiled lowlevel program by our program equivalence specifically we have implemented a lowlevel construct corresponding to each highlevel construct and then defined the compiler purely inductively on the structure of source programs using these lowlevel constructs for each construct we have shown a corresponding compatibility lemma following pitts terminology meaning that program equivalence is preserved under said construct this implies that equivalent programs behave the same under arbitrary wellbehaved contexts one example of a highlevel construct is function application figure shows its lowlevel realization as a simple linking program p we assume here that p is some program computing a value of function type and p is some program computing a value of the argument type the program begins by up the stack pointer twice the return address stored in wk onto the first new stack slot and the second one with the instruction at bg is needed because the stack slot at sp s might otherwise contain a dangling pointer thus in order to maintain the global invariant that all reachable data is live we must clear sp s before passing control to p when p returns we assume it returns a pointer to a function closure in wk we store that pointer in the stack slot and we proceed to execute p after p returns we move ps closure pointer into wk the argument value returned by p into wk and the original return address of p into wk we then pop off two stack slots and make a tail call to the code pointer stored in closure the compatibility result for can be seen as a compositionality result for our program equivalence relation lemma compatibility app p e p e p e e another related construct of course is abstraction figure shows its lowlevel realization as the program we assume here that p is a program implementing the body of a abstraction under the assumption that the argument of that abstraction is the first element in the linked list environment stored at sv alloc bg let code bg c code in bg move wk bg move wk jmp alloc bg move move jmp wk h wk h wk bg sv bg bg bg c plus move move move move jmp move move move move code move jmp sp sp s sp s wk wk alloc wk h wk h sv wk sv sp sp s sp wk sv bg wk wk h wk bg c sp s sp figure compilation of abstraction the actual computation of the program is quite simple because it merely allocates a closure representing the abstraction and returns it just like the example in section the closure consists of a code pointer to bg and an environment pointer which is of course just whatever environment was passed to in sv whenever the closure is invoked by to bg it first the return address stored in wk as well as the register sv by them on the stack it then pushes the argument value wk onto the front of the environment by wk h which requires allocating two new memory cells and stores a pointer to this extended environment in sv before executing p finally when p returns it the stack twice and the original contents of the sv before to the return address that was stored on the stack the compatibility result for is as follows lemma compatibility abs x p e x e using and all the other low program constructors we can define a compiler e in a simple syntaxdirected fashion eg e e e e x e x e and then establish the following compiler correctness result theorem compiler correctness for e e e the theorem is easily provable by induction on e using the appropriate compatibility lemma in each case finally we prove the program is equivalent to the highlevel program from figure theorem unit unit int as a corollary we can see that for any e unit unit both e and e similarly for any e unit unit int both e and e detailed proofs of all these results appear in the companion technical appendix related and future work there is a body of work on compiler correctness and semantics for lowlevel code we focus on the most closely related work compositional compiler correctness as explained in the introduction the overall motivation of our work is very similar to that of benton and and our use of logical relations to build an extensional compositional notion of equivalence between high and lowlevel languages is inspired directly by their work however there are significant differences between our work and first they define a relation between a purely functional language and an machine whereas we relate a more expressive impure mllike highlevel language to an assembly language that is significantly more lowlevel and realistic than reasoning about compositional equivalence in our setting is significantly more complex not least because we must deal with reasoning about the heap and the presence of a garbage collector we make essential use of kripke logical relations for this purpose that said there is a sense in which our setting makes the problem easier one of benton and goals was to develop a model of lowlevel programs that would admit program equivalences such as commutativity of addition whose validity depend on the purely functional nature of the source language toward this end they related lowlevel programs to denotations of highlevel programs so that one could use reasoning to establish the purely functional equivalences of interest these relations were of asymmetric in particular they employ only on the lowlevel side of the well as only in defining one direction of approximation in the other direction they use an admissible closure operation as a result proving in their setting that a high and lowlevel program are equivalent really involves doing two proofs one for each direction of approximation using very different technical machinery in our work we this problem because the mllike lack of effect encapsulation in our highlevel language causes it to have a relatively weak equational theory that simply does not admit the kinds of purely functional equivalences that benton and were interested in nevertheless as our motivating example illustrates there are still of interesting equivalences in our setting particularly involving uses of local state moreover our logical relations being entirely operational and defined in fashion are inherently symmetric making them easier to use more recently benton and have generalized their technique to a compiler for a polymorphic yet still purely functional language but their logical relations are still asymmetric proposes a syntactic approach to proving compositional compiler correctness his idea is to establish a set of criteria for compilation relations such that the results of different compilers can be correctly linked so long as their compilation relations satisfy these criteria however the criteria are still syntactic and thus he cannot reason for instance about our motivating example the equivalence depends on semantic reasoning about local state moreover only considers a fairly highlevel target language that is a cps version of the source and propose an alternative approach to compositional compiler correctness based on type preservation instead of proving compiler correctness directly they prove that the compiler is typepreserving but their source language has such a rich type system with dependent refinement types that this effectively implies correctness like benton and they compile a purely functional language to an machine but their correctness result only applies to terminating programs none of the work considers a target language that supports garbage collection kripke logical relations our logical relation is based closely on dreyer and which was in turn a refinement and generalization of ahmed dreyer and of course these are but the in a long line of work on kripke logical relations spanning loc as well as pitts and for further pointers to the literature main goal was to show how a kripke model based on state transition systems could be extended in orthogonal ways to exploit the absence of certain features namely higherorder state andor control effects for the high language considered here in which there are no control operators showed how to extend their model with private transitions section and demonstrated the utility of private transitions in reasoning about a variety of challenging contextual equivalences involving local state for our purposes private transitions have proved useful in formalizing the assumptions about the stack and registers assumptions which indeed rely on the absence of control operators extending the high language with control operators would thus a significant change to our kripke model precisely because the compilation strategy for high would need to change as well we leave this problem to future work despite the close connection our logical relation diverges from in several ways first and whereas relation was only designed to reason about highlevel programs the whole point of our model is to allow us to relate high and lowlevel programs as a result we have no fundamental theorem of logical relations because one cannot even state such a theorem for a relation between two languages instead we prove a compiler correctness result whose proof that of the usual fundamental theorem our consideration of lowlevel programs has also led us to make a clear distinction between programs and computations and between the notions of equivalence this seems to us an interesting and important distinction that is worth further dealing with lowlevel programs introduces significant technical complexity in order to this complexity we factor the presentation of our relation wrt a language specification interface this helps to the structure of our kripke logical relation into its essentially symmetric highlevel structure as well as the components of the model that are namely the two implementations of where most of the complexity lies although we have in this paper only instantiated our model so as to relate high and low programs one may also instantiate the model to relate high and high programs or low and low programs the former model would be largely similar to dreyer et als the latter would enable one to reason about equivalence of lowlevel programs directly which may prove useful in reasoning about correctness of lowlevel optimizations although this remains to be explored as far as possible worlds are concerned although ours are largely similar to we have extended their worlds in one relatively straightforward way in addition to local invariants expressed in our possible worlds by islands our worlds also permit one to express a global invariant we exploit this added functionality to encode the invariant that all reachable data are live sections and as far as the highlevel structure of the logical relation is concerned ours is also quite similar to except that our interpretation of reference types is somewhat different various approaches to interpreting reference types have been proposed in the literature our present interpretation is in a more extensional style than either or ahmed et als in the sense that it avoids dependence on too many details of how possible worlds are structured this enables us to present it as we have in a more fashion among existing accounts our present formulation is fairly close to the denotational one given by birkedal and but the is still out on which of these is most although the primary benefit of presenting our logical relation language and is to its complex structure it also enables us to prove a few structural lemmas as well most notably monotonicity that we can prove monotonicity should not be surprising given that it is essentially into the logical relation via the quantification over future worlds in certain cases and the use of the operator in others most of the interesting theorems however cannot be stated let alone proven without about the details of the for high and low garbage collection et al prove the correctness of a copying collector using separation logic they specify the behavior of a correct garbage collector in terms of an isomorphism between the reachable portions of the initial and final heaps our specification is similar in that we express the reachable portion of the heap in our logical memories we construct our entire for low around these logical memories building on the work of et al et al develop a garbage collector interface that is general enough to characterize a variety of different collectors including incremental copying collectors with read and write barriers they prove in coq that various collectors implement this interface and that various mutator programs respect it in more recent work et al extend compcert compiler with support for garbage collection by building the interface into the design of a new intermediate language they prove semantics preservation mostly for a compiler from a purely functional typed but as in compcert not compositional correctness code et al provide one of the only formal accounts of how to verify code they use reasoning to enable local reasoning about modifications to code in much the same way that standard separation logic enables local reasoning about the heap we adopt a similar approach using possible worlds to impose local invariants more generally to establish local state transition systems the heap and the code segment ours is the first relational model for reasoning about compositional equivalence of programs future work given our ability to reason about code one direction for future work is to adapt this functionality to reason about more practical applications such as a compiler or a dynamic we have shown compositionality of our relation by showing that it is closed under the linking constructs expressible at the level of our high language such as function application in future work we would like to prove that our relation is also compositional wrt a more realistic linking language the concrete logical relation we have presented here assumes a uniform data representation it is possible in principle to define the meaning of language forms like v in a to enable it is not currently possible in our framework to define such language forms in a manner we leave a serious of this issue to future work it is how to scale our techniques to reason about compositional correctness of a compiler because the stepindexed logical relations we define are not obviously tive this is a wellknown problem with stepindexed logical relations and it seems a fresh idea is needed to it acknowledgments we would like to thank for pointing out a very subtle technical in an earlier draft of this paper references a ahmed stepindexed syntactic logical relations for recursive and quantified types in esop a ahmed d dreyer and a representation independence in popl a appel and d an indexed model of recursive types for foundational proofcarrying code toplas ­ a appel pa c and j a very modal model of a modern major general type system in popl n benton and ck and compiler correctness in icfp n benton and ck realizability and compositional compiler correctness for a polymorphic language technical report microsoft research l birkedal k and j a relational realizability model for higherorder stateful submitted for publication n advances in reasoning principles for contextual equivalence and termination phd thesis it university of copenhagen h z shao and a certified code in pldi a a certified typepreserving compiler from lambda calculus to assembly language in pldi a syntactic proofs of compositional compiler correctness submitted for publication a a verified compiler for an impure functional language in popl z phd thesis paris july d dreyer g and l birkedal the impact of higherorder state and control effects on local relational reasoning in icfp ck and d dreyer technical appendix for this paper url g and n realizability for compiler correctness in jl classical logic storage operators and secondorder lambdacalculus of pure and applied logic ­ x leroy a formally verified compiler journal of automated reasoning ­ a t and a a certified framework for compiling and executing languages in icfp a z shao c lin and l li a general framework for garbage collectors and their mutators in pldi a pitts typed operational reasoning in b c pierce editor advanced topics in types and programming languages chapter mit press a pitts and i operational reasoning for functions with local state in e a complete characterization of observational equivalence in polymorphic lambdacalculus with general references in csl n l birkedal and j c reynolds local reasoning about a copying garbage collector toplas 