a scalable correct stack university of york university of m university of abstract concurrent datastructures such as stacks queues and often implicitly enforce a total order over elements in their underlying memory layout however much of this order is unnecessary linearizability only requires that elements are ordered if the insert methods ran in sequence we propose a new approach which uses to avoid unnecessary ordering pairs of elements can be left unordered if their associated insert operations ran concurrently and order imposed as necessary at the eventual removal we our approach in a new nonblocking datastructure the ts stack using the same approach we can define corresponding queue and datastructures in experiments on x the ts stack and all its ­ for example it the stack by factor of two in our approach more concurrency translates into less ordering giving removal and thus higher performance and scalability despite this the ts stack is linearizable with respect to stack semantics the weak internal ordering in the ts stack presents a challenge when establishing linearizability standard techniques such as linearization points work well when there exists a total internal order we present a new stack theorem in which the orderings su to establish stack semantics by applying our stack theorem we show that the ts stack is indeed linearizable our theorem a new generic proof technique for concurrent stacks and it the way for future weakly ordered datastructure designs categories and subject descriptors d programming languages programming techniques ­ concurrent e data structures lists stacks and queues f logics and meanings of programs specifying and verifying and reasoning about programs keywords concurrent stack linearizability timestamps verification permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ copyright acm b ts stack a insert an element with a timestamp b ts stack c a b ts stack pop c find and remove the element with the timestamp c a b ts stack a figure ts stack push and pop operations introduction this paper presents a new approach to building ordered concurrent datastructures a of this approach in a stack and a new proof technique required to show that this algorithm is linearizable with respect to sequential stack semantics our general approach is at datastructures eg stacks queues and the key idea is for insertion to timestamps to elements and for these timestamps to determine the order in which elements should be removed this idea can be instantiated as a stack by removing the element with the timestamp or as a queue by removing the element with the timestamp both kinds of operation can be combined to give a for most of this paper we will focus on the ts stack variant the ts queue variants are discussed briefly in § the ts stack push and pop are illustrated in figure one might assume that generating a timestamp and adding an element to the datastructure has to be done together atomically this intuition is wrong linearizability allows concurrent operations to take e in any order within method boundaries ­ only sequential operations have to keep their order therefore we need only order inserted elements if the methods inserting them execute sequentially we exploit this fact by splitting timestamp generation from element insertion and by allowing unordered timestamps two elements may be in a di order than they were inserted or they may be unordered but only when the surrounding methods overlap meaning the elements could be removed in either order the only constraint is that elements of sequentially executed insert operations receive ordered timestamps by separating timestamp creation from adding the element to the datastructure our insert method can avoids two expensive synchronisation patterns ­ awar and raw we take these patterns from and refer to them as strong synchronisation can be done by a stuttering counter or a hardware instruction like the x in neither of which require strong synchronization elements can be stored in such also do not require strong synchronization in the insert operation thus stack insertion avoids strong synchronization reducing its cost the lack of synchronization in the insert operation comes at the cost of contention in the remove operation indeed proves that stacks queues and cannot be implemented without some strong synchronisation perhaps surprisingly this problem can be by reducing the ordering between timestamps intuitively less ordering results in more opportunities for parallel removal and thus less contention to weaken the element order we associate elements with intervals represented by pairs of timestamps interval timestamps allow our ts stack to achieve performance and scalability better than concurrent stacks for example we believe the stack is the current world in our experiments on x the ts stack it by a factor of two establishing correctness for the ts stack presents a challenge for existing proof methods the standard approach would be to linearization points syntactic points in the code which fix the order that methods take e this simply does not work for structures because the order of overlapping push operations is fixed by the order of future pop operations in the absence of pop operations elements can remain entirely unordered we solve this with a new theorem in the proof assistant which builds on henzinger et als aspectoriented technique rather than a total order we need only generate an order from push to pop operations and vice versa which avoids certain violations this order can be generated from syntactic points in the ts stack code allowing us to show that it is correct our stack theorem is generic not to the ts stack by away from linearization points it the way for future concurrent datastructures which weaken internal ordering contribution to our contributions are · a new class of datastructure based on as a stack queue and · a new optimisation strategy interval which exploits the weak ordering permitted by datastructures · a new proof technique for establishing the linearizability of concurrent stacks and a of the core theorem in · a detailed application of this proof technique to show that the ts stack is linearizable with respect to its sequential specification · an experimental evaluation showing our ts stack the best existing concurrent stacks we have produced two research · the ts stack itself implemented in c along with queue and variants and benchmark code used to test it · the of our stack theorem both are included with the material on the acm digital library and are also available from the ts stack t top t top t top at c d next next next next b taken next next e ts stack next f tn top figure the ts stack datastructure paper structure § describes the key ideas behind the ts stack then in § we describe the algorithm in detail in § we describe our proof technique while in § we use it to establish that the ts stack is linearizable in § we discuss our experiments § discusses ts queue and variants § the related work § concludes longer proofs and other auxiliary material are included in available on the acm digital library appendix a discusses the proof of our core stack theorem appendix b gives further details about our ts stack linearizability proof appendix c gives a proof of correctness for our intermediate ts bu er datastructure key ideas algorithm structure the ts stack marks elements with timestamps recording the order they were pushed elements are according to this timestamp order figure shows the stacks internal structure each thread tn which is accessing the stack has an associated pool implemented as a linked list we call these sp these are linked from a common array every element on the stack is stored in the sp pool of the thread that pushed it to the ts stack involves adding a node to the head of the threads sp pool generating a new timestamp and the timestamp to the node thus nodes are visible to other threads ­ we write these in figure as the maximal value as nodes are only added by one thread to each sp pool elements in a single pool are totally ordered by timestamp and no synchronisation is needed when from the ts stack involves searching all the sp for an element with a maximal timestamp and to remove it this process until an element is removed successfully as each sp pool is ordered searching only requires reading the head of each pool in turn timestamps in di may be mutually unordered ­ for example when two timestamps are equal thus more than one element may be maximal and in this case either can be chosen to remove a node the thread writes to a flag in the node marking it as taken multiple threads may try to remove the same node so an atomic cas instruction ensures at most one thread succeeds accessing the heads of multiple imposes a cost through cache contention however our experiments show that this can be less expensive than contention on a single location with an approach in our experiments we contention and thereby improve performance by introducing a small nop delay to the pop search loop however even without this optimisation the ts stack the eb stack by a factor of two we have with various implementations for itself most straightforwardly we can use a counter we can avoid unnecessary increments by using a to detect when the counter has already been we can avoid strong synchronisation entirely by using a vector of threadlocal counters meaning the counter may many elements get the same timestamp we can also use a hardware operation ­ for example the instruction which is available on all modern x hardware our benchmarks show that hardware provides the best push performance however the picture is more complicated in the presence of optimisation see § for our experiments optimisations enables several optimisations of the ts stack most importantly elimination a standard strategy in the literature and interval a contribution of this paper in a stack a concurrent push and pop can always soundly eliminate each other of the state of the stack therefore a thread can remove any concurrently inserted element not just the stack top unlike our mechanism for detecting elimination exploits the existence of timestamps we read the current time at the start of a pop any element with a later timestamp has been pushed during the current pop and can be eliminated surprisingly it is not optimal to insert elements as quickly as possible the reason is that removal is when there are many unordered maximal elements reducing contention and avoiding failed cases to exploit this we define timestamps as intervals represented by a pair of start and end times overlapping interval timestamps are considered unordered and thus there can be many top elements in the stack to implement this the algorithm includes a delay for a interval after generating a start timestamp then generates an end timestamp allows us to o the performance of push and pop an increasing delay in insertion can reduce the number of in pop for evidence see § though may appear as an unnecessary overhead to a push our experiments show that optimal delay times are actually than eg an atomic on a memory location by weakening the order of stored elements interval can substantially increase overall and decrease the of similarly although interval increases the nondeterminism of removal ie the variance in the order in which pushed elements are this need not translate into greater overall nondeterminism compared to other stacks a major source of nondeterminism in existing concurrent datastructures is in fact contention while interval increases the potential for nondeterminism in one respect it decreases it in another performance vs stack to the best of our knowledge the eb stack is the stack previously proposed in our experiments § the ts stack with elimination and interval the eb stack by a factor of two several design decisions contribute to this success the lack of and of contention in the pr transitive ir ins val ir val pop a rem pop b pr transitive ir figure behaviour forbidden by stack theorem remove makes our stack fast even without elimination also allows us to integrate elimination into normal stack code rather than in separate code algorithm correctness intuitively the ts stack is correct because any two push operations that run sequentially receive ordered timestamps and are therefore removed in order elements arising from concurrent push operations may unordered timestamps and may be removed in either order but this does not a correctness to formally prove that a stack is correct ie linearizable with respect to stack semantics we need to show that for any execution there exists a total linearization order however proving this directly is challenging because the order between parallel push operations can be fixed by the order on later pop operations while the order between parallel pop operations can likewise be fixed by earlier pushes instead we use a new stack theorem which removes the need to find a total linearization order intuitively our theorem requires that if two elements are on the stack then the element is first ie ordering it is important that all operations take a consistent view on which elements are on the stack to express this our stack theorem require a relation which states whether a push takes logical e before or after a pop we call this ir for the ir relation is the of the linearization order surprisingly our theorem shows that it is the only part that thus the stack theorem us of the need to resolve problematic ordering between parallel push or parallel pop operations the stack theorem also uses two other relations precedence pr which relates methods that run in sequence and value val which relates a push to the pop removing the associated value the theorem has the following form if for every execution there exist ir pr val relations that are then the algorithm is linearizable with respect to sequential stack semantics rules out behaviour intuitively the situation shown in figure is forbidden if and are ordered and is related to pop a in ir then the two cannot also be ordered pop a before pop b ­ this would violate ordering as pr and val can easily be extracted from an execution establishing linearizability amounts to showing the existence of a consistent ir this is analogous to finding the linearization order but ir can be constructed much more easily for the ts stack we use a modified version of the linearization point method but rather than a single order we identify two and combine them to build ir · vis for visibility a and pop are ordered by vis if the value a inserted by push was in a sp pool when the pop started and so could have been visible to it additionally the full theorem requires the algorithm is linearizable with respect to sequential set semantics this guarantees properties such as absence of duplication · rr for two operations pop a and pop b are ordered in rr if elements a and b are removed in order from the underlying sp building ir is more than just merging these two relations because they may one another instead ir is built by taking vis as a core and using rr to correct cases that order our proof of correctness § shows that this is always possible which establishes that the ts stack is linearizable the ts stack in detail we now present our ts stack algorithm in detail listing shows the ts stack code this code the collection of sp linked from the array we factor the sp pool out as a separate datastructure supporting the following operations · insert ­ insert an element without a timestamp and return a reference to the new node · ­ return a reference to the node in the pool with the timestamp together with the top pointer of the pool · remove ­ tries to remove the given node from the pool return true and the element of the node or false and null depending whether it succeeds we describe our implementation of the sp pool in § listing also assumes the function ­ various implementations are discussed in § we can now describe listing to push an element the ts stack inserts an element into the current threads pool line generates a fresh timestamp line and sets the new elements timestamp line a pop iteratively over all sp line and searches for the node with the timestamp in all sp line the binary operator ts is timestamp comparison this is just integer comparison for timestamps ­ see § if removing the identified node succeeds line then its element is returned line otherwise the iteration for simplicity in listing threads are associated statically with slots in the array to support a dynamic number of threads this array can be replaced by a linked list for iteration in pop and by a or threadlocal storage for fast access in push elimination and emptiness checking code in gray in listing handles elimination and emptiness checking elimination is an essential optimisation in making our stack e it is permitted whenever a push and pop execute concurrently to detect opportunities for elimination a pop reads the current time when it starts line when searching through the sp any element with a later timestamp must have been pushed during the current pop and can be eliminated immediately lines to check whether the stack is empty we reuse an approach from when scanning the sp if a pool indicates that it is empty then its top pointer is recorded lines if no candidate for removal is found then the sp are scanned again to check whether their top pointers have changed lines if not the must have been empty between the first and second scan the linearizability of this emptiness check has been proved in listing ts stack algorithm the sp pool is defined in listing and described in § timestamps are discussed in § the gray code deals with the emptiness check and elimination node element element timestamp timestamp node next bool taken v o i d push element element pool node node pool insert element timestamp timestamp node timestamp timestamp element pop elimination timestamp bool success element element do success element w h i l e success r e t u r n element bool element timestamp node null timestamp timestamp pool node top node empty f o r each current in node node node node current emptiness check i f node null empty current id c o n t i n u e timestamp node timestamp elimination i f ts n o d e t i m e s t a m p r e t u r n current remove node i f timestamp ts n o d e t i m e s t a m p node timestamp pool current top emptiness check i f null f o r each current in i f current top empty current id r e t u r n false null r e t u r n true empty r e t u r n pool remove top listing sp pool algorithm the gray code deals with the of taken nodes node top int id the id of the owner thread init node element null taken true next top node insert element element node element element taken false next top top node next next w h i l e next next next next taken next next next next next r e t u r n node node node top node result w h i l e true i f result taken r e t u r n result e l s e i f result next result r e t u r n null result result next bool element remove node node node i f cas node taken false true cas top node nodes before node in the list i f node next node nodes after node in the list node next node next w h i l e next next next next taken next next next node next next r e t u r n true node element r e t u r n false null sp pool the sp pool listing is a linked list of nodes accessed by a top pointer a node consists of a next pointer for the linked list the element it stores the timestamp assigned to the element and a taken flag the linked list is closed at its end by a node pointing to itself line initially the list contains only the node the taken flag of the is set to true indicating that the does not contain an element the top pointer is annotated with an to avoid the elements are inserted into the sp pool by adding a new node line at the head of the linked list line to remove an element the taken flag of its node is set atomically with a cas instruction line iterates over the list line and returns the first node which is not marked as taken line if no such node is found returns line taken nodes nodes marked as taken are considered removed from the sp pool and are therefore ignored by and remove however to memory and to reduce the overhead of over taken nodes nodes marked as taken are eventually either in insert line or in remove line to we the next pointer from a node a to a node b previously connected by a sequence of taken nodes line line and line in insert the nodes between the new node and the next node are and in remove the nodes between the old top node and the removed node and between the removed node and the next node are additionally remove tries to all nodes between top and the removed node line by using cas we guarantee that no new node has been inserted between the top and the removed node algorithms this algorithm takes a timestamp from a global counter using an atomic instruction such instructions are available on most modern processors ­ for example the lock instruction on x this algorithm uses the x instruction to read the current value of the register the register counts the number of processor cycles since the last reset was not originally intended for so an obvious concern is that it might not be across in this case e could lead to violations we believe this is not a problem for modern et al have tested on various x systems as part of their transactional memory system our understanding of and the intel x architecture guide is that should provide su synchronisation on recent and machines we have also observed no violations of stack semantics in our experiments across di machines aside from we use c sequentially consistent atomics throughout all other relaxed behaviours however we have reports that is not on x systems furthermore memory order guarantees o by are often and ­ see eg sewell et als work on a formalized x model which does not cover should test before using it to generate timestamps on substantially di hardware we hope the ts stack will further research into and hardware timestamp generation more generally this algorithm uses threadlocal counters which are synchronized by algorithm to generate a new timestamp a thread first reads the values of all threadlocal counters it then takes the maximum value increments it by one stores it in its threadlocal counter and returns the stored value as the new timestamp note that the algorithm does not require strong synchronization may return the same timestamp multiple times but only if these timestamps were generated concurrently listing algorithm the gray code is an optimisation to avoid unnecessary cas i n t counter timestamp i n t timestamp counter delay optimisation i n t timestamp counter i f timestamp timestamp r e t u r n timestamp timestamp i f cas counter timestamp timestamp r e t u r n timestamp timestamp r e t u r n timestamp counter this algorithm does not return one timestamp value but rather an interval consisting of a pair of timestamps generated by one of the algorithms above let a b and c d be two such interval timestamps they are ordered a b ts c d if and only if b c that is if the two intervals overlap the timestamps are unordered the algorithm is correct because for any two interval timestamps a b and c d if these intervals are generated sequentially then b is generated before c and therefore b c as discussed above in our experiments we use the algorithm ie the x instruction to generate the start and end of the interval because it is faster than and adding a delay between the generation of the two timestamps increases the size of the interval allowing more timestamps to overlap and thereby reducing contention during element removal the e of adding a delay on overall performance is analyzed in section this algorithm is an optimisation of combined with interval timestamps it exploits the insight that the shared counter needs only be by some thread not necessarily the current thread in a highly concurrent situation many threads using will increment the counter the algorithm instead uses cas failure to detect when the counter has been cas failure without is so this scheme is fast despite using strong synchronisation for is given in listing the algorithm begins by reading the counter value line if the cas in line succeeds then the timestamp takes the counters original value as its start and end line if the cas fails then another concurrent call must have the counter and does not have to instead it returns an interval starting at the original counter value and ending at the new value one line this interval will overlap with concurrent calls to but will not overlap with any intervals created later similar to adding a small delay between reading the counter value and the cas can improve performance here this not only increases the number of overlapping intervals but also reduces contention on the global counter contention is reduced further by line a standard cas optimisation if the value of counter changed during the delay then the cas in line is guaranteed to fail instead of executing the cas we can im return an interval timestamp our experiments show that in scenarios the performance of with a delay is up to x faster than without a delay correctness theorem for stacks linearizability is the de standard correctness condition for concurrent algorithms it ensures that every behaviour observed by an algorithms calling context could also have been produced by a sequential ie atomic version of the same algorithm we call the ideal sequential version of the algorithm the specification eg below we define a sequential stack specification interactions between the algorithm and calling context in a given execution are expressed as a history note that our formulation is to datastructures because the val relation the connection between an operation which inserts a value and the operation that receives it eg a push and corresponding pop definition a history h is a tuple pr where a is a finite set of operations for example push and pr val tm a a are the precedence and value relations respectively a history is sequential if pr is a total order a history is extracted from a trace t the interleaved sequence of events that took place during an execution of the algorithm to extract the history we first generate the set a of executed operations in the trace as is standard in linearizability assume that all calls have corresponding returns a pair x y is in pr if the return event of operation x is ordered before the call event of y in t a pair x y is in val if x is an insert y a remove and the value inserted by x was removed by y note that we assume that values are unique linearizability requires that algorithms only interact with their calling context through call and return events therefore a history captures all interactions between algorithm and context we thus define a datastructure specification as just a set of histories eg stack is the set of histories produced by an ideal sequential stack linearizability is defined by relating implementation and specification histories definition a history pr is linearizable with respect to some specification s if there exists a linearization order such that pr tm and oe s an implementation c is linearizable with respect to s if any history h arising from the algorithm is linearizable with respect to s the problem with linearization points proving that a concurrent algorithm is linearizable with respect to a sequential specification amounts to showing that for every possible execution there exists a total linearization order the standard strategy is a proof where the implementation and specification histories are constructed in the points where the specification takes e are known as linearization points ­ to simplify the proof our formulation of linearizability di from the classic one rather than have a history record the total order on calls and returns we convert this information into a strict partial order pr likewise linearizability between histories is defined by inclusion on orders rather than by reordering call and return events this approach taken from is convenient for us because our stack theorem is defined by constraints on orders however the two are equivalent these are often associated with points in the tions syntax conceptually when a linearization point is reached the method is to the linearization order it has long been understood that linearization points are a limited approach simulation arguments work for many nonblocking algorithms because the specification his is not precisely determined by the implementation al may have linearization points by complex interactions between methods or by nondeterministic fu ture behaviour the ts stack is a particularly example of this problem two push methods that run concurrently may insert elements with unordered timestamps giving no information to choose a linearization order however if the elements are later sequentially an order is imposed on the earlier pushes worse ordering two pushes can im order other methods leading to a of lin back in time consider the following history lines represent execution time h represents calls and n returns pop a pop c pop b this history induces the precedence order pr represented by lines in the following graph pr pr pr pr pop a pr pop c pr pop b first consider the history immediately before the return of pop c ie without order in the graph as and run concurrently elements b and c may have unordered timestamps at this point there are several consistent ways that the history might even given access to the ts stacks internal state now consider the history after pop b dotted edges represent linearization orders forced by this operation as c is before b order requires that has to be before ­ order transitivity then implies that has to be ordered before ­ order furthermore ordering before requires that pop c is ordered before pop a ­ order thus a methods linearization order may be fixed long after it returns any attempt to identify linearization points conditions aspects for a given sequential specification it may not be necessary to find the entire linearization order to show that an algorithm is linearizable a example is the specification which contains all possible sequential histories in this case we need not find a linearization order because any order consistent with pr will do one alternative to linearization points is thus to conditions for particular sequential specifications henzinger et al have just such a set of conditions for queues they call this approach aspectoriented one property of their approach is that their queue conditions are mostly expressed using precedence order pr in other words most features of queue behaviour can be checked without linearization points at all the exception is emptiness checking which also requires special treatment in our approach ­ see below stack and set specifications our theorem makes use of two sequential specifications stack and a weaker specification set that does not respect order we define the set of permitted histories by defining updates over abstract states assume a set of values val abstract states are finite sequences in let oe be an arbitrary state in stack push and pop have the following sequential behaviour · means sequence concatenation · ­ update the abstract state to · v · pop ­ if return empty otherwise must be of the form Õ · update the state to Õ return in set push is the same but pop behaves as follows · pop ­ if return empty otherwise must be of the form Õ update the state to Õ return the stack theorem we have developed stack conditions su to ensure linearizability with respect to stack unlike our conditions are not expressed using only pr indeed we believe this would be impossible ­ see § rather we require an auxiliary relation ir which relates pushes to and vice versa but that does not relate pairs of pushes or pairs of in other words our theorem shows that for stacks it is su to identify just part of the linearization order we begin by defining the orders ins and rem over push operations and pop operations respectively informally ins and rem are fragments of the linearization order that are imposed by the combination of ir and the precedence order pr in all the definitions in this section assume that h pr is a history below we write a b c etc for push operations and a b c etc for pop operations definition derived orders ins and rem assume an relation ir · for all a b oe a a b if either a b or there exists an operation c oe a with a c b · for all a b oe a a b if either a b or there exists an operation c oe a with a c b the order ins expresses ordering between pushes imposed either by precedence or by likewise rem expresses ordering between using ins and rem we can define which expresses the conditions necessary to achieve ordering in a stack in our formulation ins is weaker than rem ­ note the pr rather than ir in the final clause however our stack theorem also holds if the definitions are with rem weaker than ins the version above is more convenient in verifying the ts stack definition alternating we call a relation r on a alternating if every pair a b oe a consisting of one push and one nonempty pop is ordered and no other pairs are ordered definition we call h if there exists an alternating relation ir on a and derived orders ins and rem such that ir fi pr is and let a a b oe a with a a and a a if a b a then there exists b oe a with b b and a b condition is at the of our proof approach it the behaviour illustrated in figure only imposes ordering it does not guarantee correctness properties for a stack these are elements should not be lost elements should not be duplicated elements should come from a corresponding push and pop should report empty correctly the last is subtle as it is a global rather than pairwise property pop should return empty only at a point in the linearization order where the abstract stack is empty fortunately these properties are also orthogonal to ordering we just require that the algorithm is linearizable with respect to set simple to prove for the ts stack for properties it is trivial why this is su for emptiness checking any history satisfying set can be split into free of as a can only occur when no elements are in the datastructure any such partitioning is also valid in stack thus the correctness of emptiness checking can be established separately from ordering theorem stack correctness let c be a concurrent algorithm if every history arising from c is and c is linearizable with respect to set then c is linearizable with respect to stack proof here we only sketch five stages of the proof for full details see appendix a order all pop operations which do not return empty and which are ordered with their matching push operation in the precedence order the ir relation to deal with the definition of ins discussed above again we ignore all pairs with overlapping execution times order all push operations which remain unordered after the first two stages and show that the resulting order is within stack show that pairs with overlapping execution times can always be added to a correct linearization order without stack show that also pop operations which return empty can always be added to a correct linearization order as long as they are correct with respect to set for the ts stack the advantage of theorem is that problematic orderings need not be resolved in the example discussed above and can be left unordered in ir removing the need to decide their eventual linearization order likewise pop a and pop c as we show in the next section the ir relation can be extracted from the ts stack using an adapted version of the linearization point method our stack theorem is generic not to the ts stack it the internal ordering su for an algorithm to achieve stack semantics as well as sound it is complete ­ for any linearizable stack ir can be trivially from the linearization order for stacks such as nonblocking stack it is simple to see intuitively why the theorem applies if two pushes are ordered then their cases are ordered as a result their elements will be ordered in the stack representation and removed in order intuitively our theorem seems close to the lower bound for stack ordering the next section § provides evidence for this by out the class of weaker without ir intuitively we would expect any concurrent stack to enforce orders as strong as the ones in our theorem thus theorem points towards fundamental constraints on the structure of concurrent stacks pop a pop b pop c pop d pr val pr pop a lin pr val pr pop b lin pr pr val pr pop c lin val pop d figure top example execution nonlocal behaviour bottom corresponding graph out pr val and lin relations we have theorem in the theorem prover the source files for this proof are provided in file appendix a discusses the structure of our with an informal proof of the theorem why the relation is necessary theorem builds on a similar theorem for queues proved by henzinger et al as in our definition of definition their theorem certain bad orderings between operations however their conditions are defined purely in terms of precedence pr and value val ­ they do not require the auxiliary relation ir we believe that any stack theorem similar in structure to ours must require some additional information like ir and as a corollary that checking linearizability for stacks is harder than for queues by similar we mean a local theorem defined by a finite number of bad orderings it is this locality that makes our theorem and so it reduces datastructure correctness from global ordering to out a number of specific bad cases our key evidence that the relation is needed is the execution shown in figure top this execution as a whole is not linearizable ­ this can be seen more clearly in corresponding graph in figure bottom which projects out the pr and val relations here lin is the linearization order forced by ordering the pr edges form a cycle the requirement that linearization order is acyclic and includes pr however if for any i oe a b c d the corresponding pair is deleted the execution becomes linearizable intuitively doing this breaks the cycle in lin fi pr that appears above thus any condition based on precedence that is smaller than this whole execution cannot it ­ otherwise it would executions worse we can make arbitrarily large bad executions of this form thus no theorem based on condition can define linearizability for stacks our relation introduces just enough extra structure to let us define a local stack theorem this kind of execution is not a problem for queues because ordering an pair cannot constrain the or order of any other pair proving the ts stack correct we now prove the ts stack correct we use a twolevel argument to separate concerns in the proof by verifying a structure first we hide the of the datastructure from the higherlevel proof prove the linearizability of an intermediate structure called the ts bu er this shows that the sp combine to form a single consistent pool but does not enforce ordering use our stack theorem theorem to prove the ts stack is linearizable with respect to stack semantics linearizability lets us use the ts bu er in terms of its sequential specification ts bu er linearizability the ts bu er is a virtual intermediate datastructure ie a proof convenience that does not exist in the algorithm syntax it would be easy to add but would make our code more complex the ts bu er methods are the lines in push and pop which modify the array and proving the ts bu er linearizable means these lines can be treated as atomic we name the ts bu er operations as follows ­ line numbers refer to listing note that where possible these names coincide with names in listing · ins ­ inserts an element into an sp pool line · ­ generates a new timestamp line · ­ assign a timestamp to a sp pool element line · ­ record the current time at the beginning of a pop line · ­ search through the sp and try to remove the element with the timestamp line note that and have the same underlying implementation but di abstract specifications this is because they play di roles in the ts stack respectively generating timestamps for elements and con the abstract state of the ts bu er individual sp by merging all the elements into a single pool as elements may be eliminated depending on when the method started the abstract state also records representing particular points in the bu history as with stack and set we define the sequential by tracking updates to abstract states for we assume a set of bu er identifiers id representing individual bu er elements and a set of timestamps ts with strict partial order ts and top element a abstract state is a tuple b s b oe buf is a partial map from identifiers to tuples representing the current values stored in the bu er s oe is a partial map from timestamps to buf representing of the bu er at particular timestamps buf id Ô val ts ts Ô buf we implicitly assume that all timestamps in the bu er were previously generated by are used to support globally consistent re to remove from the bu er pop first calls to generate a timestamp t ­ abstractly t b is added to the library of when pop calls elements that were present when t was generated may be re moved normally while elements added or more recently than t may be eliminated out of order the stored st determines which element should be removed or eliminated the ts bu er functions have the following specifications assuming b s is the abstract state before the operation · ­ pick a timestamp t such that for all already in b ts t return t note that this means many elements can be the same timestamp if the thread is before writing it into the bu er · ­ pick an id i oe update the state to bi v s and return i · ­ assume that t was generated by if bi v then update the abstract state to bi v t s if bi do nothing · ­ pick a timestamp t such that t oe doms or t oe doms and st b if t oe doms update the state to b st b return t · ­ assume t oe doms there are four possible behaviours failure nondeterministically fail and return this corresponds to a failed sp pool remove by another thread emptiness check if the map is empty ie then return normal removal pick an id i with i oe fl and bi vi ti such that ti is maximal with respect to other elements from the ie oe fl · · ti ts update the abstract state to bi s and return note that there may be many maximal elements that could be returned elimination pick an id i such that i oe and either i oe and bi v or v update the abstract state to bi s and return this corresponds to the case where v was inserted or after pop called and v can therefore be removed using elimination theorem ts bu er operations are linearizable with respect to the specification proof the concrete state of the ts bu er consists of the array where each slot points to a sp pool ie a linked list of nodes for the abstract state the mapping buf is easily built by taken nodes we build by examining the preceding trace are generated from the state of the bu er at any point is called ins and are sp pool operations which take e atomically because they build on atomic operations ie the assignment to top in line and to timestamp in line respectively and both build on the same operation only concurrent timestamp requests can generate overlapping timestamps as timestamps have to be generated and then added to the bu er separately at the call of and an overlapping timestamp cannot be in the bu er for the is correctly constructed automatically as a consequence of the mapping from concrete to abstract state the most complex proof is for where the linearization point is in the call to remove always removes a valid element because any element in the is guaranteed to be contained in one of the sp before starts its search for the element any removed element not in the must have been added since the start of the search and thus satisfies the elimination case of the specification further details are given in appendix c ts stack linearizability we now prove that the ts stack is correct we first define two orders vis and rr on push and pop operations these orders are extracted from executions using a method analogous to linearization points except that we generate two possibly conflicting orders the points chosen correspond to ts bu er operations in § we proved that the ts bu er is linearizable so we can treat these points as atomic · vis visibility ­ the element inserted by a push was visible to a pop a push and nonempty pop are ordered in vis if sp pool insertion in the push line is ordered before recording the current time in the pop line · rr ­ two removed elements in order two nonempty pop operations are ordered in rr if their final successful operations line are similarly ordered in the execution as with ins rem in the stack theorem it is useful to define a order ts timestamp on push operations this order is imposed by precedence and vis transitivity informally if two push operations are ordered in ts their elements are ordered in ts definition derived order ts assume a history h pr and order vis on a two operations a b oe a are related a b if a b or a c b for some c oe a or a d c b for some c d oe a to apply theorem and to show that the ts stack is correct we need to show that any history arising from the ts stack is definition the following lemma vis rr and ts to this notion of lemma let h pr be a history assume vis an alternating order on a and rr a total order on nonempty pop operations in a assume the derived order ts if pr fi vis and pr fi rr are and for all a a b oe a such that a a a a and a b a there exists b oe a such that b b and b a then h is according to definition proof the proof works by using vis rr and ts to construct a relation ir that h is either vis is such a witness or vis can be locally such that it becomes a witness works iteratively by identifying triples of operations which violate then one of the operations earlier or later in the relation to remove the violation the detail of the proof con of a case analysis showing that for any execution such are always possible and eventually terminate further details are given in appendix b lemma ts stack is linearizable with respect to set proof straightforward from the fact that the ts bu er is linearizable with respect to we take the linearization point for push as the call to ins and the linearization point for pop as the call to correctness follows from the specification of theorem ts stack is linearizable with respect to stack proof follows by applying our stack theorem lemma deals with the first clause of theorem the other clause requires the existence of an ir relation that satisfies it su to show that vis rr and ts satisfy the preconditions of lemma the first requirement that and are follows from the fact that the instructions used to define vis and rr are linearization points of the ts bu er the second requirement for the lemma follows from the fact that ordering in ts implies ordering in ts and the fact that the ts stack removes elements in an order that respects ts further details are given in appendix b theorem the ts stack is lockfree proof straightforward from the structure of removal can only fail when another thread succeeds performance analysis our experiments compare the performance and scalability of the ts stack with two concurrent stacks the stack because it is the standard lockfree stack implementation and the eb stack because it is the concurrent stack we are aware of we the and eb stacks to perform as well as possible on our test machines see below for the parameters used we ran our experiments on two x machines · an server with four core intel processors per core mb shared and gb of memory running linux and · an server with four core processors mb shared and gb of memory running linux measurements were done in the framework to avoid measurement the framework uses a custom memory allocator which performs cyclic allocation in threadlocal bu for objects smaller than bytes larger objects are allocated with the standard allocator of all memory is allocated when it is to avoid cache of course other stacks exist we decided against the stack because no implementation is available for our platform and according to their experiments in performance it is no better than a flat combining stack we decided against the flat combining stack because the eb stack it when to access the array before the stack itself stack stack stack stack core machine core machine table benchmark delay times for the framework is written in cc and compiled with gcc and o optimizations provides implementations of the stack and of the eb stack unlike the description of the eb stack in we access the elimination array before the stack ­ this improves scalability in our experiments we the eb stack such that the performance is optimal in our benchmarks when with threads on the core machine or with threads on the core machine these configurations may be for lower numbers of threads similarly the ts stack configurations we discuss later are selected to be optimal for and threads on the core and core machine respectively on the core machine the elimination array is of size with a delay of in the benchmark and of size with a delay of in the low contention benchmark on the core machine the elimination array is of size with a delay of in the benchmark and of size with a delay of in the low contention benchmark on the core machine the stack benefits from a strategy which delays the of a failed cas on this machine we the stack with a constant delay of in the experiments and a constant delay of in the experiments which is optimal for the benchmark when with threads on the core machine performance decreases when a delay is added so we it we compare the datastructures in where threads are split between which insert elements into the datastructure and which remove elements from the datastructure we measure performance as total execution time of the benchmark figures show the total execution time in successful operations per to make scalability more visible all numbers are over executions to avoid empty removal operations that do not return an element are not counted the contention on the datastructure is controlled by a computational load which is calculated between two operations of a thread in the scenario the computational load is a in iterations in the scenario fi is calculated in iterations on average a computational load of iterations corresponds to a delay of on the core machine performance and scalability results figures a and b show performance and scalability in a benchmark where half of the threads are and half of the threads are these figures show results for the scenario results for the scenario are similar but less ­ see figure in the material for and we use the optimal delay when with threads on the core machine and with threads on the core machine derived from the experiments in section the delay thus depends on the machine and benchmark the delay times we use in the benchmarks are listed in table the impact of di delay times on performance is discussed in section comparison between implementations is faster than the other algorithms in the benchmarks with an increasing number of threads interestingly the stack is faster than the stack in the benchmark the reason is that since the push operations of the stack are so much faster than the push operations of the stack elimination is possible for more pop operations of the stack eg more elimination on the core machine see table in the appendix which results in a factor of less of operations than in the stack on the core machine the stack is significantly slower than the stack while on the core machine the stack is faster the reason is that on the core machine is significantly slower than see figure c on the core machine the stack is much faster than the stack stack and stack on the core machine it is slightly faster the reason is that on the core machine a cas is slower in comparison to other instructions than on the core machine comparison with other datastructures with more than threads all ts stacks are faster than the stack on both machines the stack and the stack the eb stack in the benchmark with a maximum number of threads on the core machine also the stack and the stack are faster than the eb stack we believe and performance increase with respect to the eb stack comes from three sources a more elimination b faster elimination c higher performance without elimination as shown in and experiments the lack of and of contention in pop makes our stack fast even without elimination additional experiments show that for example the stack eliminates and more elements than the eb stack in scenarios on the core and on the core machine respectively thus we improve on eb in both a and c b is di to measure but we integrating elimination into the normal code path introduces less overhead than an elimination array and is thus faster push performance we measure the performance of push operations of all datastructures in a benchmark where each thread pushes element into the stack the stack and the stack use the same delay as in the benchmark see table figure c and figure d show the performance and scalability of the datastructures in the benchmark the push performance of the stack is significantly better than the push of the other stack implementations with an increasing number of threads the push operation of the stack is faster than the stack eb stack stack stack stack stack stack operations per ms more is better number of threads a benchmark core machine operations per ms more is better number of threads b benchmark core machine operations per ms more is better operations per ms more is better number of threads c benchmark core machine number of threads d benchmark core machine operations per ms more is better operations per ms more is better number of threads e benchmark core machine number of threads f benchmark core machine figure ts stack performance in the scenario on core machine left and core machine right push operations of the stack and the stack which means that the delay in the is actually than the execution time of the and the perhaps surprisingly which does not require strong synchronisation is slower than which is based on an atomic instruction pop performance we measure the performance of pop operations of all datastructures in a benchmark where each thread from a stack note that no elimination is possible in this benchmark the stack is concurrently which means in case of the stack and stack that some elements may have unordered timestamps again the stack uses the same delay as in the benchmark figure e and figure f show the performance and scalability of the datastructures in the benchmark the performance of the stack is significantly higher than the performance of the other stack implementations except for low numbers of threads the performance of is close to the performance of the stack is faster than the and stack due to the fact that some elements share timestamps and therefore can be removed in parallel the stack and stack show the same performance because all elements have unique timestamps and therefore have to be removed sequentially also in the stack and the eb stack elements have to be removed sequentially depending on the machine removing elements sequentially from a single list stack is sometimes less and sometimes as expensive as removing elements sequentially from multiple lists ts stack operations per ms more is better number of less is better performance stack stack performance stack stack delay in ns figure benchmark using and with increasing delay on the core machine and analysis of interval figure shows the performance of the stack and the stack along with the average number of calls needed in each pop one call is optimal but contention may cause these figures were collected with an increasing interval length in the high contention benchmark on the core machine we used these results to determine the delays for the benchmarks in section initially the performance of the stack increases with an increasing delay time but beyond the performance decreases again after that point an average push operation is slower than an average pop operation and the number of pop operations which return empty increases for the stack the high performance strongly with a drop in we conclude from this that the performance we achieve with interval arises from reduced contention in for the optimal delay time we have calls to per pop ie less than of pop calls need to scan the sp array more than once in contrast without a delay the average number of per pop call is more than the performance of the stack increases initially with an increasing delay time however this does not decrease the number of significantly the reason is that without a delay there is more contention on the global counter therefore the performance of with a delay is actually better than the performance without a delay however similar to with a delay time beyond the performance decreases again this is the point where an average push operation becomes slower than an average pop operations ts queue and ts variants in this paper we have on the stack variant of our algorithm however stored timestamps can be removed in any order meaning it is simple to change our ts stack into a queue doing this requires three main changes change the timestamp comparison operator in change the sp pool such that returns the rightmost leftmost element for the ts queue remove elimination in for the ts enable it only for removal the ts queue is the second queue we know of in our experiments the queue the queue and the queue but the lack of elimination means it is not as fast as the the is the we know of although it is slower than the corresponding stack queue however it still the and queues and the and eb stacks related work our approach was initially inspired by et als laws of order paper which proves that any linearizable stack queue or necessarily uses the raw or awar patterns in its remove operation while to extend this result to insert operations we were to discover a counterexample the ts stack we believe the queue was the first algorithm to exploit the fact that need not take e in order of their atomic operations although unlike the ts stack it does not avoid strong synchronisation when inserting and use in their queue as in our stack elements are and stored in bu aside from the obvious di in kind our ts stack di in several respects the uses ­ that is a thread merges timestamps into a total order as a result the queue is blocking the ts stack avoids enforcing an internal total order and instead allows nonblocking parallel removal removal in the queue depends on the expensive process and as a result their benchmark shows remove performance significantly worse than other queues interval lets the ts stack insertion and removal cost avoiding this problem timestamps in the queue are lamport clocks not intervals we also experiment with lamport clocks ­ see in § finally queue elements are before being inserted ­ in the ts stack this is this trivial di enables elimination which is important to the ts stacks performance the queue and the sp queue both index elements using an atomic counter however operations do not look for one of the elements as in our ts stack but rather for the element with the index that matches the index exactly both approaches fall back to a slow path when the counter becomes higher than the counter in contrast to indices timestamps in the ts stack need not be unique or even ordered and the performance of the ts stack does not depend on a fast path and a slow path but only on the number of elements which share the same timestamp our use of the x instruction to generate hardware timestamps is inspired by work on testing fifo queues there the instruction is used to determine the order of operation calls note the distinction between the and has since been used in the design of an stm by et al who investigate the instructions multiprocessor synchronisation behaviour correctness our stack theorem lets us prove that the ts stack is linearizable with respect to sequential stack semantics this theorem builds on henzinger et al who have a similar theorem for queues their theorem is defined almost entirely in terms of the sequential order on methods ­ what we call precedence pr that is they need not generate a linearization order in contrast our stack theorem requires a relation between inserts and removes we it is impossible to define such a theorem for stacks without an auxiliary relation see § a stack must respect several correctness properties elements should not be lost or duplicated and pop should correctly report when the stack is empty henzinger et al build these properties into their theorem making it more complex and harder to use furthermore each that returns empty requires a partition before and after the operation e a partial linearization order however these correctness properties are orthogonal to ordering and so we simply require that the algorithm also respects set semantics implementation features our ts stack implementation concepts from several previous datastructures storing elements in multiple partial datastructures is used in the distributed queue where insert and remove operations are distributed between partial queues using a load one can view the sp as partial queues and the ts stack itself as the load the ts stack emptiness check also from the distributed queues however the ts stack the performance of distributed queues while preserving sequential stack semantics elimination in the stack however in the ts stack elimination works by comparing timestamps rather than by accessing a array as a result in the ts stack a pop which eliminates a concurrent push is faster than a normal pop in the stack such an eliminating pop is slower as synchronization on the array requires at least three successful cas operations instead of just one conclusions and future work we present a novel approach to implementing ordered concurrent datastructures like queues stacks and a concurrent algorithm the ts stack and a new proof technique required to show the ts stack is correct the broad messages that we draw from our work are · in concurrent datastructures total ordering on internal data imposes a performance cost and is unnecessary for linearizability · however internal ordering makes establishing correctness more challenging theorems such as our stack theorem can solve this problem our work represents an initial step in designing and verifying datastructures in future work we plan to experiment with other internal ordering constraints with dynamically the level of order in response to contention with correctness conditions weaker than linearizability and with the underlying memory model acknowledgments we thank for feedback frank for help with the formalization and michael for help with we also thank the popl for their comments this work has been supported by the national research network rise on rigorous systems engineering science sn references y and a fast concurrent queues for x processors in acm h r d p m michael and m laws of order expensive synchronization in concurrent algorithms cannot be eliminated in popl g d and a a dynamic stack algorithm in m m and a library abstraction for cc concurrency in popl computational systems group university of framework url m and d brief an asymmetric based queue algorithm in a c m and h how fifo is your concurrent fifo queue in races acm a t henzinger c m h a and a distributed queues in shared performance and scalability through quantitative relaxation in cf acm d n and l a scalable lockfree stack algorithm in acm d i n and m flat combining and the in t henzinger h and a replacing with to achieve scalable lockfree fifo queues technical report t a henzinger a and v vafeiadis aspectoriented linearizability proofs in concur m and n the art of multiprocessor programming morgan inc m and j linearizability a correctness condition for concurrent objects toplas m ho o and n the queue in springer intel intel and ia architectures software developers manual volume b system programming guide part url l lamport time clocks and the ordering of events in a distributed system communications acm july m michael and m scott simple fast and practical nonblocking and blocking concurrent queue algorithms in acm h h and m rinard detecting and eliminating memory leaks using cyclic memory allocation in acm w y and m transactional memory by exploiting hardware cycle counters in p sewell s sarkar s f z and m o a rigorous and usable programmers model for x acm r systems programming with parallelism technical report rj ibm research center april 