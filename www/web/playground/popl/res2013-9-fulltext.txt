static and dynamic semantics of languages france paris paris paris france ibm watson research ny usa abstract we present a calculus for processing data that differences of application area among several novel query languages as this calculus lets users define their own operators capturing a range of data processing capabilities providing a typing precision so far typical only of primitive operators the type inference algorithm is based on semantic type checking resulting in type information that is both precise and flexible enough to handle structured and data we illustrate the use of this calculus by encoding a large fragment of including operations and iterators over embedded sql expressions and and show how the encoding directly yields a typing discipline for as it is namely without the addition of any type definition or type annotation in the code categories and subject descriptors d programming languages formal definitions and theory f logics and meanings of programs semantics of programming semantics f logics and meanings of programs studies of program structure h information storage and information search and query formulation keywords computing type inference introduction the of computing and the ever growing importance of data in applications has given to a of new data models and languages whether they are developed under the of for for computing or as domain specific languages embedded in a host language most of them share a common subset of sql and the ability to handle data while there is no consensus yet on the precise boundaries of this class of languages they all share two common i an on sequence operations eg through the popular paradigm and ii a lack of types for both data and programs contrary to say xml programming or relational databases where data schemas are in that such languages can greatly benefit from formal foundations and suggests as a unifying model although we agree with for the need to provide unified formal this work was partially supported by the project n and by a grant of permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ italy copyright c acm to those new languages we argue that such foundations should account for novel features critical to various application domains that are not captured by also most of those languages provide limited type checking or ignore it we believe type checking is essential for many applications with usage ranging from error detection to optimization but we understand the designers and programmers of those languages who are to any kind of type definition or annotation in this article we propose a calculus which is expressive enough to capture languages that go beyond sql or we show how the calculus to various data models while a precise type checking that can exploit in a flexible way limited type information information that is directly from the structure of the program even in the absence of any explicit type declaration or annotation example we use a language over developed for to illustrate how our proposed calculus works our reason for using is that it all the features found in the previously query languages and includes a number of original ones as well like it supports sequence iteration and operations on queries like and it features nested queries furthermore uses a rich data model that allows arbitrary nesting of data it works on generic sequences of records whose fields can contain other sequences or records while other languages are limited to flat data models such as whose is similar to the standard relational model used by sql databases tuples of scalars and of lists of scalars includes sql as an embedded for relational data for these reasons although in the present work we focus almost exclusively on we believe that our work can be adapted without effort to a wide array of sequence processing languages the following program illustrates some of those features it performs between one input containing information about and one relational input containing information about the query returns for each department its name and id from the first input and the number of from the second input a sql expression is used to select the with above a given value while a filter is used to access the set of and the elements of these two collections are processed by the group expression in denotes the current element group filter each x by g as ds select from where by g dept as es into dept g the query expressions eg filter which selects in the collection with a size of more than and the itself with a sql statement select ing in a relational table for which the is more than relations are naturally in as collections of records in our example one of the key difference is that field access in sql requires the field to be present in the record while the same operation in does not actually field selection in is very expressive since it can be applied also to collections with the effect that the selection is recursively applied to the components of the collection and the collection of the results returned and similarly for filter and other iterators in other words the expression filter each x above will work as much when x is bound to a record with or without a size field in the latter case the selection returns null as when x is bound to a collection of records or of arbitrary nested collections this accounts for the nature of compared to the relational model our calculus can express both in a way that illustrates the difference in both the dynamic semantics and static typing in our calculus the selection of all records whose field is greater than is defined as let sel nil nil x as y tail if x then else collections are encoded as lists à la lisp while the among records or arbitrary nested collections of records of those where the optional size field is present and larger than is let nil nil size x as if x then else tail the terms above show nearly all the basic building blocks of our calculus only composition is missing building blocks that we filters filters can be defined recursively eg is a recursive call they can perform pattern matching as found in functional languages the filter p f executes f in the environment resulting from the matching of pattern p they can be composed in alternation ff tries to apply f and if it fails it applies f they can over the structure of their argument eg ff of which is an instance requires an argument of a product type and applies the corresponding fi for instance the filter collections encoded as lists à la lisp ie by right associative pairs with nil denoting the empty list if its argument is the empty list then it returns the empty list if it is a list whose head is a record with a size field and possibly other fields matched by then it captures the whole record in y the content of the field in x the tail of the list in tail and keeps or y ie the record according to whether x ie the field is larger than if the head is also a list then it recursively applies both on the head and on the tail if the head of the list is neither a list nor a record with a size field then the head is discarded the encoding of the whole query is given in section our aim is not to propose yet another query language but rather to show how to express and type such languages via an encoding into our core calculus each such language can in this way preserve its execution model but obtain for free a formal semantics a type inference system and as it happens a prototype implementation the type information is via the encoding without the need of any type annotation and can be used for early error detection and debugging purposes the encoding also yields an executable system that can be used for both possibilities are critical in most typical usage scenarios of these languages where is very expensive both in time and in resources as observed by the of big data makes it more important than ever for programmers and we add for language and system designers to have a single abstraction that allows them to process transform query analyze and compute across data presenting both in volume and in structure yielding a number of new data models query languages and execution the framework we present here we claim them all a goal is that the compilers of these languages could use the type information inferred from the encoding and the encoding itself to further optimizations types have all been by considering just the execution model the type or schema of the manipulated data did not play any role in their design as a consequence these languages are untyped and when present types are optional and clearly added as an differences in data model or type discipline are particularly important when embedded in a host language since they yield the socalled the reason why types in such languages may in an between type inference and data on the one hand these languages are to work with collections of data that are weakly or partially structured on the other hand current languages with type inference such as haskell or ml can work only on collections typically lists of elements of the same type in this work we show that the two can we type data by semantic subtyping a type system for data and describe computations by our filters which are untyped combinators that thanks to a technique of weak typing introduced in can type the results of data query and processing with a high degree of precision the of filters is driven by the schema of the data rather than the execution model and we use them i to capture and give a uniform semantics to a wide range of structured data processing capabilities ii to give a type system that the types defined for such languages if any notably and but also xml query and processing languages see section iii to infer the precise result types of queries written in these languages as they are so without the addition of any explicit type or new construct and iv to show how minimal of the current syntax of these languages can bring improvements in the precision of the inferred types the types we propose here are extensible record types and heterogeneous lists whose content is described by regular expressions on types as defined by the following grammar types t v t t t t r int char any empty null tt tt singleton closed record open record sequences base special union difference r t r r r r r rr where denotes the empty word the semantics of types can be expressed in terms of sets of values values are either constants such as true false null the latter denoting the character records of values or lists of values so the singleton type v is the type that contains just the value v in particular null is the singleton type containing the value null the closed record type contains all record values with exactly two fields a and b with integer values while the open record type contains all record values with at least two fields a and b with integer values the sequence type r is the set of all sequences whose content is described by the regular expression r so for example char contains all sequences of characters we will use string to denote this type and the standard double quote notation to denote its values while denotes nonempty lists of even length containing record values of type the union type st contains all the values of s and of t while the difference type st contains all the values of s that are not in t we shall use bool as an abbreviation of the union of the two singleton types containing true and false any and empty respectively contain all and no values recursive type definitions are also used see section for formal details these types can express all the types of and all xml types and much more so for instance includes only lists of type t that can be expressed by our types as t in documentation one can find the type boolean which is the type of arrays whose first element is the second is the string a and all the other are booleans this can be easily expressed in our types as a bool but while only allows a limited use of regular expressions kleene star can only appear in tail position our types do not have such restrictions so for example char char f ri t is the type of all strings ie sequences of that denote email addresses ending by either fr or it we use some syntactic sugar to make terms as the previous one more readable eg likewise we use at to denote that the field a of type t is optional this is just syntactic sugar for stating that either the field is undefined or it contains a value of type t for the formal details of this encoding see the full version of this work available on line coming back to our initial example the filter defined before expects as argument a collection of the following type type size int that is a possibly empty arbitrary nested list of records with an optional size field of type int notice that it is important to specify the optional field and its type since a size field of a different type would make the expression x raise a runtime error this information is just from the structure of the filter since does not contain any type definition or annotation we define a type inference system that any argument of that has not type and for arguments of type size int addr string sec int which is a subtype of the result type size int addr so it does not the field addr but the field sec and by replacing for that the test may fail by encoding primitive operations into a formal core calculus we shall provide them a formal and clean semantics as well as precise typing so for instance it will be clear that applying the following dot selection a a a the result will be and we shall be able to deduce that a applied to arbitrary nested lists of records with an optional integer a field ie of type t t yields arbitrary nested lists of int or null values ie of type u int null u finally we shall show that if we accept to extend the current syntax of or of some other language by some minimal filter syntax eg the pattern filter we can obtain a improvement in the precision of type inference contributions the main contribution of this work is the definition of a calculus that structural operators over languages and that some characteristics that make it unique in the of current data processing languages in particular it is parametric though fully in a host language it uniformly handles both width and deep nested data recursion while most languages offer just the former and limited forms of the latter finally it includes firstclass arbitrary deep composition while most languages offer this operator only at top level whose power is nevertheless by the type system an important contribution of this work is that it directly compares a programming language approach with the tree transducer one our calculus implements transformations typical of topdown tree transducers but has several advantages over the transducer approach the transformations are expressed in a formalism immediately to any functional programmer our calculus in its untyped version is turing complete its transformations can be statically typed at the of turing completeness without any annotation yielding precise result types even if we restrict the calculus only to welltyped terms thus turing completeness it still is strictly more expressive than wellknown and widely studied deterministic topdown tree transducer the technical contributions are i the proof of turing completeness for our formalism ii the definition of a type system that with records with computable labels iii the definition of a static type system for filters and its correctness iv the definition of a static analysis that ensures the termination and the proof of the type inference algorithm with complexity bounds expressed in the size of types and filters and iv the proof that the terms that pass the static analysis form a language strictly more expressive than topdown tree transducers outline in section we present the syntax of the three components of our system namely a minimal set of expressions the calculus of filters used to program userdefined operators or to encode the operators of other languages and the core types in which the types we just presented are to be encoded section defines the operational semantics of filters and a declarative semantics for operators the type system as well as the type inference algorithm are described in section in section we present how to handle a large subset of section reports on some design choices of our system and compare with related work for space constraints proofs results encodings some formal definitions in particular the definition of the static analysis for termination and the interpretation of record values as functions and further extensions can be found only in the full version available online syntax in this section we present the syntax of the three components of our system a minimal set of expressions the calculus of filters used to program userdefined operators or to encode the operators of other languages and the core types in which the types presented in the introduction are to be encoded the core of our work is the definition of filters and types the key property of our development is that filters can be to any host language that satisfies minimal requirements by simply adding filter application to the expressions of the host language the minimal requirements of the host language for this to be possible are quite simple it must have constants typically for types int char string and bool variables and either pairs or record values not necessarily both on the static side the host language must have at least basic and products types and be able to assign a type to expressions in a given type environment ie under some typing assumptions for variables by the addition of filter applications the host language can acquire or increase the capability to define polymorphic userdefined iterators query and processing expressions and be with a powerful and precise type system expressions in this work we consider the following set of expressions definition expressions e c constants x variables e e pairs ee ee records ee record concatenation e field deletion e builtin operators fe filter application where f ranges over filters defined later on c over generic constants and over string constants intuitively these expressions represent the syntax supplied by the host language though only the first two and one of the next two are really needed that we extend with the missing expressions and the expression of filter application expressions are formed by constants variables pairs records and operation on records record concatenation gives priority to the expression on the right so if in r r both records contains a field with the same label it is the one in r that will be taken while field deletion does not require the record to contain a field with the given label though this point is not important the metavariable op ranges over operators as well as functions and other constructions belonging to or defined by the host language among expressions we single out a set of values intuitively the results of computations that are formally defined as follows v c v v v v we use foo for character string constants c for characters and so on for integers and words such as foo for atoms ie userdefined constants we use three distinguished atoms nil true and false double can be omitted for strings that are labels of record fields thus we write rather than sequences heterogeneous lists ordered collections arrays are encoded à la lisp as nested pairs where the atom nil denotes the empty list we use e en as syntactic sugar for e en nil types definition types types t b basic types v singleton types tt products t t closed records t t open records tt union types tt intersection types negation type empty empty type any any type recursive types t recursion variable opt t type calls where every recursion is guarded that is every type variable is separated from its binder by at least one application of a type constructor ie products records or op most of these types were already explained in the introduction we have basic types int bool ranged over by b and singleton types v denoting the type that contains only the value v record types come in two closed record types whose values are records with exactly the fields specified by the type and open record types whose values are records with at least the fields specified by the type product types are standard and we have a complete set of type connectives that is finite unions intersections and we use empty to denote the type that has no values and any for the type of all values sometimes denoted by when used in patterns we added a term for recursive types which allows us to encode both the regular expression types defined in the introduction and more generally the recursive type definitions we used there finally we use op to distinguish it from expression operators to denote the host languages type operators if any thus when filter applications return values whose type belongs just to the language eg a list of functions we suppose the typing of these functions be given by some type operators for instance if succ is a user defined successor function we will suppose to be given its type in the form and similarly for its application say we will be given the type of this expression int here arrow is a type operator and apply an expression operator the denotational semantics of types as sets of values that we informally described in the introduction is at the basis of the definition of the subtyping relation for these types we say that a type t is a subtype of a type t noted t t if and only if the set of values denoted by t is contained in the settheoretic sense in the set of values denoted by t for the formal definition and the decision procedure of this subtyping relation the reader can refer to the work on semantic subtyping patterns filters are our core untyped operators all they can do are three different things they can structurally decompose and transform the values they are applied to or they can be sequentially composed or they can do pattern matching in order to define filters thus we first need to define patterns definition patterns patterns p t type x variable pp pair p p closed rec p p open rec pp pp where the forming pairs records and intersections have distinct capture variables and those forming unions have the same capture variables patterns are essentially types in which capture variables ranged over by x y may occur in every position that is not under a negation or a recursion a pattern is used to match a value the matching of a value v against a pattern p noted vp either fails noted or it returns a substitution from the variables occurring in the pattern into values the substitution is then used as an environment in which some expression is evaluated if the pattern is a type then the matching fails if and only if the pattern is matched against a value that does not have that type otherwise it returns the empty substitution if it is a variable then the matching always succeeds and returns the substitution that assigns the matched value to the variable the pair pattern pp succeeds if and only if it is matched against a pair of values and each succeeds on the corresponding projection of the value the union of the two substitutions is then returned both record patterns are similar to the product pattern with the that in the open record pattern matches all the fields that are not specified in the pattern an intersection pattern pp succeeds if and only if both patterns succeed the union of the two substitutions is then returned the union pattern pp first tries to match the pattern p and if it fails it tries the pattern p for instance the pattern succeeds only if the matched value is a pair of values v v in which v is an integer in which case it returns the substitution xv and fails otherwise finally notice that the notation p as x we used in the examples of the introduction is syntactic sugar for px this informal semantics of matching see for the formal definition explains the reasons of the restrictions on capture variables in definition in intersections pairs and records all patterns must be matched and thus they have to assign distinct variables while in union patterns just one pattern will be matched hence the same set of variables must be assigned alternative is selected the strength of patterns is their connections with types and the fact that the pattern matching operator can be typed exactly this is by the following theorems both proved in theorem accepted type for every pattern p the set of all values v such that vp is a type we call this set the accepted type of p and note it by p the fact that the exact set of values for which a matching succeeds is a type is not obvious it states that for every pattern p there exists a syntactic type produced by the grammar in definition whose semantics is exactly the set of all and only values that are matched by p the existence of this syntactic type which we note p is of importance for a precise typing of pattern matching in particular given a pattern p and a type t contained in ie subtype of p it allows us to compute the exact type of the capture variables of p when it is matched against a value in t theorem type environment there exists an algorithm that for every pattern p and t p returns a type environment tp types such that v t filters definition filters a filter is a term generated by filters f e expression pf pattern f f product f f record ff union recursion xa recursive call f f composition o declarative operators operators o f f filter filter ordering arguments a x c aa a a variables constants pairs record such that for every subterm of the form f g no recursion variable is free inf filters are like transducers that when applied to a value return another value however unlike transducers they more constructs like the ability to test an input and capture subterms an result from captured values and a composition operator we first describe informally the semantics of each construct the expression filter e always returns the value corresponding to the evaluation of e and its argument the filter p f applies the filter f to its argument in the environment obtained by matching the argument against p provided that the matching does not fail this rather powerful feature allows a filter to perform two critical actions i an input with regular patternmatching before exploring it and ii capture part of the input that can be reused during the evaluation of the f if the argument application of fi to vi returns vi then the application of the product filter ff to an argument v v returns v v otherwise if any application fails or if the argument is not a pair it fails the record filter is similar it applies to each specified field the corresponding filter and as by the leaves the other fields unchanged it fails if any of the applications does or if any of the specified fields is absent or if the argument is not a record the filter ff returns the application of f to its argument or if this fails the application of f the semantics of a recursive filter is given by standard unfolding of its definition in recursive calls the only real restriction that we introduce for filters is that recursive calls can be done only on arguments of a given form ie on arguments that have the form of values where variables may occur this restriction in practice amounts to recursive calls on the result of another recursively defined filter all other cases can be easily encoded the reason of this restriction is technical since it greatly simplifies the analysis of section which ensures the termination of type inference without expressiveness filters are turing complete even with this restriction see theorem filters can be composed the filter ff applies f to the result of applying f to the argument and fails if any of the two does the condition that in every subterm of the form f g f does not contain free recursion variables is not strictly necessary indeed we could allow such terms the point is that the analysis for the termination of the typing would then reject all such terms apart from trivial ones in which the result of the recursive call is not used in the composition but since this restriction does not restrict the expressiveness of the calculus theorem proves turing completeness with this restriction then the addition of this restriction is just a design rather than a technical choice we prefer to the programmer to write recursive calls on the lefthand side of a composition than systematically reject all the programs that use them in a nontrivial way finally we out some specific filters specifically we chose and whose semantics is generally specified in a declarative rather than operational way these do not bring any expressive power to the calculus the proof of turing completeness theorem does not use these declarative operators and actually they can be encoded by the remaining filters but it is interesting to single them out because they yield either simpler encodings or more precise typing semantics the operational semantics of our calculus is given by the reduction semantics for filter application and for the record operations since the former is the only of our work we save space and omit the latter which are standard we define a big step operational semantics for filters the definition is given by the inference rules in figure for judgments of the form eval f a r and describes how the evaluation of the application of filter f to an argument a in an environment yields an object r where r is either a value or the latter is a special value which represents a runtime error it is raised by the rule error either because a filter did not match the form of its argument eg the argument of a filter product was not a pair or because some pattern matching failed ie the side condition of did not hold notice that the argument a of a filter is always a value v unless the filter is the unfolding of a recursive expr eval ev r r eval e prod eval fv r eval fv r eval v r r if r and r vp eval f v r eval p f v r if vp comp eval fv r eval fr eval r r if r union eval fv r eval r if r union eval fv eval fv eval r r rec x f eval f v r eval v r eval xa r eval r error eval f a if no other rule applies eval fv r · · · eval rn eval f v r if i ri figure dynamic semantics of filters call in which case variables may occur in it cf rule environment is used to store the body of recursive definitions the semantics of filters is quite straightforward and inspired by the semantics of patterns the expression filter its input and evaluates rather the host language to evaluate the expression e in the current environment expr it can be thought of as the righthand side of a branch in a construct the product filter expects a pair as input applies its and returns the pair of the results prod this filter is used in particular to express sequence mapping as the first component f transforms the element of the list and f is applied to the tail in practice it is often the case that f is a recursive call that iterates on arbitrary lists and stops when the input is nil if the input is not a pair then the filter fails rule error applies the record filter expects as input a record value with at least the same fields as those specified by the filter it applies each to the value in the corresponding field leaving the contents of other fields unchanged if the argument is not a record value or it does not contain all the fields specified by the record filter or if the application of any fails then the whole application of the record filter fails the pattern filter matches its input value v against the pattern p if the matching fails so the filter does otherwise it evaluates its in the environment augmented by the substitution vp the alternative filter follows a standard policy if the filter f succeeds then its result is returned union if f fails then f is evaluated against the input value union this filter is particularly useful to write the alternative of two or more pattern filters making it possible to continue a computation based on the shape of the input the composition allows us to pass the result of f as input to f the composition filter is of importance indeed without it our only way to iterate an input value is to use a product filter which always a pair as result finally a recursive filter is evaluated by recording its body in and evaluating it rec while for a recursive call we replace the recursion variable by its definition this concludes the presentation of the semantics of filters ie without and these form a turing complete formalism complete proof in the full version theorem turing completeness the language formed by constants variables pairs equality and applications of filters is turing complete proof sketch we can encode untyped callbyvalue calculus by first applying continuation passing style cps transformations and encoding cps term reduction rules and substitutions via filters thanks to cps we the restrictions on composition to conclude the presentation of the semantics we have to define the semantics of and we prefer to give the semantics in a declarative form rather than operationally in order not to tie it to a particular order of keys or of the execution f applied to a sequence v vm reduces to a sequence k l kn ln such that i i m j j n st kj f vi j j n i i m st kj f vi j j n lj is a sequence vj j j j n k k nj f kj ki kj i j l ln is a partition of v vm f applied to v vn reduces to v vn such that v vn is a permutation of v vn i j st i j n f vi f vj since the semantics of both operators is connected to a notion of equality and order on values of the host language we give them as builtin operations however we will illustrate how our type algebra allows us to provide very precise typing rules specialized for their particular semantics it is also possible see full version to encode or on several input sequences with a combination of and filters syntactic sugar the reader may have that the productions for expressions definition do not define any destructor eg projections label selection just constructors the reason is that destructors as well as other common expressions can be encoded by filter applications e def x xe def xe def xe let p e in e def p ee if e then e else e def true ee match e with p en def p e pn these are just a possible choice but others are possible for instance in dot selection is overloaded when is applied to a record returns the content of its field if the field is absent or the argument is null then returns null and fails if the argument is not a record when applied to a list array in terminology it recursively applies to all the elements of the list so is precisely defined as µx x x null null ht besides the syntactic sugar above in the next section we will use t t to denote the record type formed by all field types in t and all the field types in t whose label is not already present in t similarly t will denote the record types formed by all field types in t apart from the one labeled by if present finally we will also use for expressions types and patterns the syntactic sugar for lists used in the introduction so for instance p p pn is matched by lists of n elements provided that their ith element matches pi type inference the type inference system assign types to expressions variables constants and pairs are typed by standard rules while we suppose that the typing of expressions is provided by the host language so we omit the corresponding rules they can be found in the full version the core of our type system starts with records typing of records the typing of records is novel and challenging because record expressions may contain string expressions in label position such as in ee while in all type systems for record we are aware of labels are never computed it is difficult to give a type to ee since in general we do not statically know the value that e will return and which is required to form a record type all we can and must ask is that this value will be a string to type a record expression ee thus we distinguish two cases according to whether the type t of e is finite ie it contains only finitely many values such as say bool or not if a type is finite of regular types seen as tree automata can be decided in polynomial time then it is possible to write it as a finite union of values actually of singleton types so consider again ee and let t be the type of e and t the type of e first t must be a subtype of string since record labels are strings so if t is finite it can be expressed as · · · n which means that e will return the string i for some i n therefore ee will have type i t for some i n and thus the union of all these types as expressed by the rule below if t is infinite instead then all we can say is that it will be a record with some unknown labels as expressed by rule e · · · n e t ee t · · · nt et e t ee t string t is infinite ee t · · · tn ee t · · · tn e t e t t e e t t t et e t t records with multiple fields are handled by the rule which merges the result of typing single fields by using the type operator as defined in which is a record concatenation defined to take into account undefined and unknown fields for instance unknown fields in the righthand side may notice that our expressions whereas they include filter applications do not include applications of expressions to expressions therefore if the host language provides function definitions then the applications of the host language must be dealt as expressions as well cf apply in § known fields of the lefthand side which is why for instance we have likewise for every record type t ie for every t subtype of we have t finally and deal with record concatenation and field deletion respectively in a straightforward way the only constraint is that all expressions must have a record type ie the constraints of the form see the full version for formal definitions of all these type operators notice that these rules do not ensure that a record will not have two fields with the same label which is a runtime error detecting such an error needs sophisticated type systems eg dependent types beyond the scope of this work this is why in the rule we used type operator which in case of multiple occurring labels since records are unordered corresponds to randomly choosing one of the types bound to these labels if such a field is selected it would yield a runtime error so its typing can be ambiguous we can fine the rule so that when all the ti are finite unions of record types then we require to have pairwise disjoint sets of labels but since the problem would still for infinite types we prefer to retain the current simpler formulation typing of filter application filters are not firstclass they can be applied but not passed around or computed therefore we do not assign types to filters but as for any other expression we assign types to filter applications the typing rule for filter application et f t s fe s relies on an auxiliary deduction system for judgments of the form m f t s that states that if in the environments m explained later on we apply the filter f to a value of type t then it will return a result of type s to define this auxiliary deduction system which is the core of our type analysis we first need to define f the type accepted by a filter f intuitively this type gives a necessary condition on the input for the filter not to fail definition accepted type given a filter f the accepted type of f written f is the set of values defined by e p f ff ff ff any p f f f f f f xa f f any f any any f f n f it is easy to show that an argument included in the accepted type is a necessary but not sufficient because of the cases for composition and recursion condition for the evaluation of a filter not to fail lemma let f be a filter and v be a value such that v f for every if eval f v r then r the last two auxiliary definitions we need are related to product and record types in the presence of unions the most general form for a product type is a finite union of products since intersections on products for instance consider the type this type denotes the set of pairs for which either both projections are int or both projections are string a type such as is less precise since it also allows pairs whose first projection is an int and second projection is a string and vice versa we see that it is necessary to manipulate finite unions of products and similarly for records and therefore we introduce the following notations lemma product decomposition let t types such that t a product decomposition of t denoted by t is a set of types t tt such that t ti for a given product decomposition we say that n is the rank of t noted and use the notation j i t for the type there exist several suitable whose details are out of the scope of this article we refer the interested reader to and for practical algorithms that compute such for any subtype of or of these notions of decomposition rank and projection can be generalized to records lemma record decomposition let t types such that t a record decomposition of t denoted by t is a finite set of types tr rn where each ri is either of the form i ti i ni i or of the form i ni i and such that t ri for a given record decomposition we say that n is the rank of t noted and use the notation for the type of label in the jth component of t in our calculus we have three different sets of variables the set vars of term variables ranged over by x y introduced in patterns and used in expressions and in arguments of calls of recursive filters the set of term recursion variables ranged over by x y and that are used to define recursive filters the set of type recursion variables ranged over by t u used to define recursive types in order to use them we need to define three different environments vars types denoting type environments that associate term variables with their types filters denoting definition environments that associate each filter recursion variable with the body of its definition m × types denoting memoization environments which record that the call of a given recursive filter on a given type the introduction of a fresh recursion type variable our typing rules thus work on judgments of the form m f t t stating that applying f to an expression of type t in the environments m yields a result of type t this judgment can be derived with the set of rules given in figure these rules are straightforward when put side by side with the dynamic semantics of filters given in section it is clear that this type system simulates at the level of types the computations that are carried out by filters on values at runtime for instance rule calls the typing function of the host language to determine the type of an expression e rule applies a product filter recursively on the first and second projection for each member of the product decomposition of the input type and returns the union of all result types rule for records is similar recursively applying for each member of the record decomposition and returning the union of the resulting record types as for the pattern filter rule its f is typed in the environment augmented by the mapping tp of the input type against the pattern cf theorem the typing rule for the union filter reflects the first match policy when typing the second branch we know that the first was not taken hence that at runtime the value will have a type that is in t but not in f notice that this is not by the definition of accepted type which is a approximation that errors but as we right after its definition is not sufficient to ensure that evaluation of f will not fail but by the type system itself the premises check that ft is welltyped which by induction implies that f will never fail on values of type t and that these values will never reach f also we discard from the output type the contribution of the branches that cannot be taken that is branches whose accepted type have an empty intersection with the input type t composition rule is straightforward in this rule the restriction that f is a filter with no open recursion variable ensures that its output type s is also a type without free recursion variables and therefore that we can use it as input type for f the next three rules work together the first introduces for a recursive filter a fresh recursion variable for its output type it also in that the recursive filter x is associated with a body f and in m that for an input filter x and an input type t the output type is the newly introduced recursive type variable when dealing with a recursive call x two situations may arise one possibility is that it is the first time the filter x is applied to the input type t we therefore introduce a fresh type variable t and replacing x by its definition f otherwise if the input type has already been encountered while typing the filter variable x we can return its memoized type a type variable t finally rule and rule handle the special cases of and filters their typing is explained in the following section typing of and while the structural filters simple compositional typing rules the adhoc operations and need rules indeed it is well known that when transformation languages have the ability to compare data values typechecking and also type inference becomes undecidable eg see we therefore provide two typing approximations that yield a good between precision and decidability first we define an auxiliary function over sequence types definition item set let t types such that t any the item set of t denoted by is defined by item ti if t the first and second line in the definition ensure that item returns the empty set for sequence types that are not products namely for the empty sequence the third line handles the case of nonempty sequence type in this case t is a finite union of products whose first components are the types of the head of the sequence and second components are recursively the types of the note also that this definition is wellfounded since types are regular trees the number of distinct types by item is finite we can now defined typing rules for the and operators f the filter uses its argument filter f to compute a key from each element of the input sequence and then returns the same sequence of elements sorted with respect to their key therefore while the types of the elements in the result are still known their order is lost we use item to compute the output type of an application ti ti f the typing of can be used to give a approximation of the typing of as stated by rule in words we obtain a list of pairs where the key component is the result type of f applied to the items of the sequence and use to the order of the list a far more precise typing of that keeps track of the relation between list elements and their via f is given in the full version soundness termination and complexity the soundness of the type inference system is given by the property of subject reduction for filter application m et type e tp m f t s t p f m p f t s j m fj i j t m si si jm m fj ij t m f t si i m si m si empty t f f t t f t f m ft s m m s fs s x f m x t t f t s t fresh m t s m x t t xt t m t t type a x t t fresh m m x t t type a x t ti m f ti si m f t t any is ordered ti m f ti si t any m f t i figure type inference algorithm for filter application theorem subject reduction if f t s then for all v t eval f v r implies r s whose proof is given in the full version it is easy to write a filter for which the type inference algorithm that is the deduction of does not terminate the deduction of m f t s simulates an abstract execution of the filter f on the type t since filters are turing complete then in general it is not possible to decide whether the deduction of for a given filter f will terminate for every input type t for this reason we define a static analysis for filters that ensures that if f passes the analysis then for every input type t the deduction of m f t s terminates for space reasons the formal definition of is available in only the full version but its behavior can be easily explained imagine that a recursive filter f is applied to some input type t the algorithm tracks all the recursive calls occurring in f next it performs one step of reduction of each recursive call by unfolding the body finally it checks in this unfolding that if a variable occurs in the argument of a recursive call then it is bound to a type that is a subtree of the original type t in other words the analysis verifies that in the execution of the derivation for f t every call to sp for some type s and pattern p always yields a type environment where variables used in recursive calls are bound to subtrees of t this implies that the rule will always for a given x types that are obtained from the arguments of the recursive calls of x by replacing their variables with a subtree of the original type t memoized by the rule since t is regular then it has finitely many distinct subtrees thus can only finitely many distinct types and therefore the algorithm terminates more precisely the analysis proceeds in two passes in the first pass the algorithm tracks all recursive filters and for each of them it i marks the variables that occur in the arguments of its recursive calls ii assigns to each variable an abstract identifier representing the subtree of the input type to which the variable will be bound at the initial call of the filter and iii it returns the set of all types obtained by replacing variables by the associated abstract identifier in each argument of a recursive call the last set intuitively represents all the possible ways in which recursive calls can and the subtrees forming the initial input type the second phase of the analysis first abstractly reduces by one step each recursive filter by applying it on the set of types collected in the first phase of the analysis and then checks whether after this reduction all the variables marked in the first phase ie those that occur in ar of recursive calls are still bound to subtrees of the initial input type if this checks fails then the filter is rejected it is not difficult to see that the type inference algorithm if and only if for every input type there exists a integer n such that after n recursive calls the marked variables are bound only to subtrees of the initial input type or to something that does not depend on it of course since deciding whether such an n exists is not possible our analysis checks whether for all possible input types a filter satisfies it for n that is to say that at every recursive call its marked variables satisfy the property otherwise it the filter theorem termination if then for every type t the deduction of f t s is in furthermore if t is given as a nondeterministic tree automaton then f t s is in where the size of the problem is f × t this complexity result is in line with those of similar for instance in it is shown that typechecking non deterministic topdown tree transducers is in when the input and output types are given by a all filters defined in this article pass the analysis as an example consider the filter that applied to a list returns the same list with the first element moved to the last position and the empty list if applied to the empty list µx xyz w w the analysis succeeds on this filter if we denote by x the abstract subtree bound to the variable x then the recursive call will be executed on the abstract argument xz so in the unfolding of the recursive call x is bound to x whereas y and z are bound to two distinct subtrees of z the variables in the recursive call x and z are thus bound to subtrees of the original tree even though the argument of the recursive call is not a subtree of the original tree therefore the filter is accepted in order to the precision of the inference algorithm consider the type int bool that is the type of lists formed by some integers at least one followed by some booleans at least one for the application of to an argument of this type our algorithm statically infers the most precise type that is int bool int if we apply it once more the inferred type is int bool int int bool generic filters are turing complete however requiring that check holds meaning that the filter is by our system restricts the expressive power of our filters by preventing them from a new value before doing a recursive call for instance it is not possible to typecheck a filter which the elements of a sequence determining the exact class of transformations that filters can express is challenging however it is possible to show see the full version for the proof that filters are strictly more expressive than topdown tree transducers with regular lookahead a formalism for tree transformations introduced in for an intuition of this result consider the tree au vm that is a tree whose root is labeled a with two children each being a monadic tree of height n and m respectively it is not possible to write a topdown tree transducer with regular lookahead that creates the tree au vm which is just the concatenation of the two children of the root seen as sequences a transformation that can be easily by filters the key difference in expressive power comes from the fact that filters are evaluated with an environment that binds capture variables to subtrees of the input this feature is essential to encode sequence concatenation and sequence two operations when dealing with sequences that cannot be expressed by topdown tree transducers with regular lookahead in this section we show how filters can be used to capture some popular languages for processing data on the we consider a query language for developed by ibm we give translation rules from a subset of into filters definition expressions we use the following simplified grammar for where we distinguish simple expressions ranged over by e from core expressions ranged over by k e c constants x variables current value e e arrays ee ee records el field access function call e k k filter each x e filter transform each x e transform expand each x e expand group each x by x e as x into e in order to ease the presentation we extend our syntax by adding filter definitions already informally used in the introduction to filters and filter calls to expressions e let filter f f fn f in e filter f f f f call where f ranges over filter names the mapping for most of the language we consider rely on the following builtin filters let filter filter f µx nil nil x xx f x true x let filter transform f µx nil nil x xx f x let filter expand µx nil nil x tl expressions are mapped to our expressions as follows where is a distinguished expression variable interpreting c c x x e e en en el e l en op e en e en nil e k e core expressions are mapped to filters as follows filter e f filter each e f filter each x e f filter x e transform e f transform each e f transform each x e f transform x e expand each x e f expand f transform each x e f expand f expand group into e f group by into e f group by ye into e f group each by ye into e f group each x by ye into e f group each x by y e as into e f group each x by y e as g into e f x e transform e this translation defines the first in our knowledge formal semantics of such a translation is all that is needed to define the semantics of a language and as a it with the type inference system we described without requiring any modification of the original language no further action is since the machinery to exploit it is all developed in this work as for typing every expression is encoded into a filter for which typechecking is to terminate check holds for filter transform and expand provided it holds also for their arguments since they only perform recursive calls on of subtrees of their input by its definition the encoding does not introduce any new recursion and hence it always yields a composition and application of filters for which check holds examples to show how we use the encoding let us encode the example of the introduction for the sake of the we will use filter definitions rather than expanding them in details we use and sel defined in the introduction expand and transform defined at the beginning of the section the encoding of field selection as defined in section and finally head that returns the first element of a sequence and a family of recursive filters with i n both defined below let filter head nil null x let filter nil nil x tail tail then the query in the introduction is encoded as follows sel x x expand dept l l es ds dept g ds in words we perform the selection on and filter the lines we tag each element by if it comes from and by if it comes from line we merge the two collections line we group the heterogeneous list according to the corresponding key line then for each element of the result of we capture in g the key line split the group into and line capture each into the corresponding variable ie es and ds line and return the expression specified in the query after the into lines the general definition of the encoding for the can be found in the full version let us now illustrate how the above composition of filters is typed consider an instance where · has type where dept int · has type where name string size int name string this type is a subtype of dept as defined in the introduction the global input type is therefore line which becomes after selection and line note how all occurrences of are ignored by tagging with an integer line and line yields which illustrates the precise typing of products coupled with singleton types ie instead of int while the line introduces an approximation the dependency between the tag and the corresponding type is kept int the transform is typed exactly yielding the final type note how null is in the output type since there may be without a department then head may be applied to an empty list returning null and the selection of name of null returns null for instance suppose to the defined in the introduction into the following expression in order to produce a representation of the records of the result transform each x where denotes string concatenation and is a conversion operator from any type to string the composition is illtyped for three reasons the field dept is as is of type int so it must be applied to before concatenation and the programmer did not account for the fact that the value stored in the field may be null the encoding produces the following lines to be to the previous code transform x in which all the three errors are detected by our type system a example of error is given by the following alternative code transform dept d e n d e null invalid department which all the previous errors but adds a new one since as detected by our type system the last branch can be never selected as we can see our ensures soundness forcing the programmer to handle situations as in the null example above but is also precise enough to detect that some code paths can never be reached in order to focus on our contributions we kept the language of types and filters simple however there already exists several contributions on the types and expressions used here two in particular are worth in this context recursive patterns and xml definition defines patterns inductively but alternatively we can consider the possibly infinite regular trees generated by these productions and on the lines of what is done in use the recursive patterns so obtained to encode regular expressions patterns see although this does not expressiveness it greatly improves the writing of programs since it makes it possible to capture distinct of a sequence by a single match for instance when a sequence is matched against a pattern such as int as x bool as y then x captures the list of all integer elements capture variables in regular expression patterns are bound to lists y captures all boolean elements while the remaining elements are ignored by such patterns can be encoded without the for instance the transform in lines can be more as dept g for what concerns xml the types used here were originally defined for xml so it comes as a no that they can express xml types and values for example uses the very same types used here to encode both xml types and elements as triples the first element being the tag the second a record representing attributes and the third a heterogeneous sequence for the content of the element furthermore we can adapt the results of to encode forward queries in filters therefore it requires little effort to use the filters presented here to encode languages such as designed to integrate and xml or to precisely type regular expressions the of xml data or queries embedded in programs the description of these encodings can be found in the long version of this article where we also argue that it is better to extend languages with xml primitives directly derived from our system rather than to use our system to encode languages such as as a matter of fact existing approaches tend to xml and operators thus yielding to stratified ie not integrated systems which have several eg does not allow xml nodes to contain objects and arrays such restrictions are absent from our approach since both xml and operators are encoded in the same basic building blocks and as such can be freely nested and combined extensions we used filters only to encode primitive operators of some languages in particular however it is possible to add filters to other languages so as to have userdefined operators typed as precisely as primitive ones from a point of view this is a it suffices to add filter application to the expressions of the host language however such an extension can be problematic from a computational viewpoint since it may the execution model especially for what concerns aspects of parallelism and distribution a good is to add only filters that have local effects which can already bring increases in expressiveness and type precision without the distributed compilation model for instance one can add just pattern and union filters as in the following extended program transform ax as y y y y with the convention that a filter occurring as an expression de notes its application to the current argument with this syntax our inference system is able to deduce that this expression with an argument of type returns a result of type this precision comes from the of our inference system to between the two branches of the filter and deduce that a sum field will be added only if the a field is present similarly by using pattern matching in a filter expression we can deduce that filter int true false with any sequence of elements always returns a possibly empty list of integers an even greater precision can be obtained for expressions when the generation of the key is performed by a filter that on types the result type can keep a precise correspondence between keys and the corresponding groups finally let us explain some design choices for our system filter design the reader may whether products and record filters are really necessary since at first the filter ff could be encoded as x y fx fy and similarly for records the point is that fx and fy are expressions and thus their pair is a filter only if the are closed ie without free term recursion variables without an explicit product filter it would not be possible to program a filter as simple as the identity map t since xt is not an expression x is a free term recursion variable similarly we need an explicit record filter to process recursively defined record types such as likewise one can why we put in filters only the open record variant that copy extra fields and not the closed one the reason is that if we want a filter to be applied only to records with exactly the fields specified in the filter then this can be simply obtained by a pattern matching so the filter f ie without the can be simply introduced as syntactic sugar for any f constructors the syntax for constructing records and pairs is exactly the same in patterns types expressions and filters the reader may why we did not distinguish them by using say × for product types or instead of in record values this combined with the fact that values and have the same syntax is a critical design choice that greatly reduces the confusion in these languages since it makes it possible to have a unique representation for constructions that are semantically equivalent consider for instance the pattern with our syntax nil denotes both the product type of two and nil or the value nil or the singleton that contains this value according to the interpretation we choose the pattern can then be interpreted as a pattern that matches a product or a pattern that matches a value if we had the syntax of from that of values eg v and that of pairs from products then the pattern above could have been written in five different ways the point is that they all would match exactly the same sets of values which is why we chose to have the same syntax for all of them record types in order to type records with computed labels we distinguished two cases according to whether the type of a record label is finite or not although such a distinction is simple it is not labels with singleton types cover the most common case of records with statically fixed labels the dynamic choice of a label from a statically known list of labels is a usage pattern seen in javascript when building an object which must to some interface based on a runtime value labels with infinite types cover the fairly common usage scenario in which records are used as dictionaries we deduce for the expression computing the label the type string thus forcing the programmer to insert some code that checks that the label is present before accessing it the behind the typing of records was first and in this work we to avoid type annotations at all costs since there is not even a notion of schema for records and collections only the notion of basic type is defined we cannot expect the programmer to put any kind of type information in the code more sophisticated type systems such as dependent types would probably type reconstruction dependent types need a lot of annotations and this does not fit our requirements second we the to be simple yet precise making the distinction increases typing precision at no cost we do not need any extra machinery since we already have singleton types adding heuristics or complex analysis just to gain some precision on records would have the main focus of our article which is not on typing records but on typing transformations on records we leave such additions for future work record polymorphism the reader will have that we do not use row variables to type records and nevertheless we have a high degree of polymorphism row variables are useful to type functions or transformations since they can keep track of record fields that are not modified by the transformation in this setting we do not need them since we do not type transformations ie filters but just the application of transformations filters are not firstclass terms we have polymorphic typing via filters see how the first example given in section keeps track of the c field and therefore open records suffice related work in the nested relational and sql context many works have studied the integration of algebra or sql into general purpose programming languages among the first attempts was the integration of the relational model in pascal or in smalltalk also monads or have been successfully used to design and implement query languages including a way to embed queries within host languages significant efforts have been done to those languages with type systems and type checking disciplines and more recently for integration and typing aspects however these approaches only support sequences of records in the context of specific classes of queries practically equivalent to a nested relational algebra or calculus they do not account for records with computable labels and therefore they are not easily to a setting where sequences are heterogeneous data are and queries are much more expressive while the present work is inspired and from previous works on the xml iterators languages made the filter calculus presented here substantially different from the one of xml filters in what follows as well in syntax as in dynamic and static semantics in xml filters behave as some kind of topdown tree transducers termination is enforced by heavy syntactic restrictions and a less constrained use of the composition makes type inference challenging and requires sometimes type annotations while xml filters are allowed to operate by composition on the result of a recursive call and thus simulate bottomup tree transformations the absence of explicit arguments in recursive calls makes programs only to programmers in contrast the main focus of the current work was to make programs immediately to any functional programmer and make filters effective for the typing of sequence transformations sequence iteration element the last two are especially difficult to write with xml filters and require type annotations also the integration of filters with record types absent in and just sketched in is novel and much needed to encode transformations conclusion our work addresses two very practical problems namely the typing of languages and a definition of their semantics these languages add to list and sql operators the ability to work on heterogeneous data sets and are based on instead of tuples typing precisely each of these features using the best techniques of the literature would probably yield quite a complex row polymorphism for records parametric polymorphism some form of dependent typing and we are that this could be achieved without using any explicit type annotation therefore we explored the formalization of these languages from by defining a calculus and a type system the thesis we is that all operations typical of current languages as long as they operate structurally ie without on term equality or relations amount to a combination of more basic our filters on the structural side the claim is that combining recursive records and pairs by unions intersections and suffices to capture all possible structuring of data covering a ranging from to heterogeneous lists typed and untyped data through regular expressions types and xml schemas therefore our calculus not only provides a simple way to give a formal semantics to compare and combine operators of different languages but also offers a means to these languages in they current definition ie without any type definition or annotation with precise type inference this type inference yields and in precision systems using parametric polymorphism and row variables the price to pay is that transformations are not first class we do not type filters but just their applications however this seems an deal in the world of languages where selects are never passed around at least not explicitly but early error detection is critical especially in the view of the cost of code the result are filters a set of untyped terms that can be easily included in a host language to complement in a framework existing operators with userdefined ones the requirements to include filters into a host language are so minimal that every modern typed programming language satisfies them the interest not in the fact that we can add filter applications to any language rather that filters can be used to define a smooth integration of calls to domain specific languages eg sql into general purpose ones eg java c python ocaml so as both can share the same set of values and the same typing discipline likewise even though filters can provide a platform for queries they cannot currently be used as a final compilation stage for languages their operations rely on a encoding of sequences and this makes the correspondence with optimized operations on lists whether we can derive an efficient compilation from filters to the semantics of the highlevel language is a challenging open question future include practical of our technique we intend to benchmark our type analysis against existing collections of programs the amount of code that is ill typed and verify on this how frequently the programmer adopted programming to with the potential type errors references a g and r a programming language for object databases the journal ­ n t f d and v xml with data values typechecking in acm only operators are not first class if the host language provides say higherorder functions then they higherorder and are typed by embedding the host type system if any via type calls n t f d and v typechecking xml views of relational databases acm trans comput logic ­ july a et al towards a scalable data platform for models ­ v g and a an general purpose language in icfp acm k et al a language for large scale data analysis ­ s d m f d j and j an xml query language wc rec p l d v and l syntax record ­ p r and r a practical functional programming system for databases in proc conference on functional programming and architecture acm g and k typed iterators for xml in icfp acm h m r f c d s and m tree automata techniques and applications g and d making smalltalk a database system in acm conf j topdown tree transducers with regular lookahead mathematical systems theory ­ dec a et de à xml phd thesis paris a g and v semantic subtyping dealing with function union intersection and negation types journal of the acm ­ javascript object notation w and f typechecking topdown uniform tree transducers in springer e the world according to acm queue e and g a model of data for large shared data communications of the acm ­ k language of combinators for xml typing implementation phd thesis a and p type inference in a database programming language in lisp and functional programming a p and v database programming in polymorphic language with static type inference in acm conf a and k making standard ml a practical database programming language in icfp c b u r and a a language for data processing in acm conf f et al in the data connecting and db in acm conf j editor j and m report technical report de a scala and for with databases with minimum and maximum type safety v p and l naturally embedded query languages in pages ­ p and p wadler improving list database queries in th ieee region conference 