qualified data flow problems l ibm cambridge scientific center technology square cambridge ma rosen computer sciences department j watson research heights ny center abstract it k known that not all paths are possible in the run time control flow of many programs it is also known that data flow analysis cannot restrict attention to exactly those paths that are possible it is therefore usual for analytic methods to consider all paths information can be obtained by considering a recursive set of paths that is large enough to include all possible paths but small enough to many of the impossible ones this paper presents a simple uniform methodology for data flow information by considering certain recursive path sets of practical importance associated with each control flow arc there is a relation on a finite set q the paths that to be considered are essentially those for which the composition of the relations encountered k nonempty for example q might be the set of all assignments of values to each of several bit variables used by a program to remember some facts about the past and branch accordingly in the future given any data flow problem together with relations on q associated with the control flow arcs we construct a new problem considering all paths in the new problem is equivalent to considering only paths in the old one preliminary experiments with a small set of real programs indicate that qualified analysis is feasible and substantially more informative than ordinary analysis the methodology also has a feedback effect on the task of passing from programs to meaningful data flow analysis problems even when all paths information can be obtained by passing from programs to problems in ways suggested by our theorems introduction consider the following program segment written level slightly higher than that of assembly language language level is not important here at a the r work or fee are the to prior a is by permission of machinery to copy otherwise or to republish requires a fee and or specific permission acm test assign test x y assign test goto assign y next red red the equivalent flowchart appears as figure there are four paths from test to assign but only one of them can actually occur at run time along this one path the value of a b is already in x when assign is reached so assign can be changed to x x y beyond such optimizations explicit understanding of control flow constraints can help in or maintaining programs variables like color above are especially important in operating systems where a routine will branch on the value of a global variable set by another routine the extensive literature on data flow analysis contains many algorithms for detecting available expressions and all of them miss the availability of a b at assign because they are by spurious paths this paper presents a simple uniform methodology for data flow information by considering certain recursive path sets of practical importance these sets are large enough to include all possible paths but small enough to many of the impossible ones in the example above the two techniques in our methodology both detect the availability of a b at assign to solve a data flow problem qualified by restrictions on the paths to be considered both techniques construct a new problem all paths are considered after has been solved by essentially any of the standard methods a solution to the original qualified problem is obtained by an easy calculation we show that any data flow algorithm in large family can be used to solve qualified problems in this way unlike an adaptation of any single algorithm to consider this methodology allows qualified data flow analysis to automatically keep with progress in ordinary all paths considered analysis section the algebraic framework here section considers qualified problems each control flow arc there is a relation on a finite set q the paths that be considered are essentially those for which the composition of the relations encountered is nonempty for example q might be the set of all relevant values for color in figure the relation on q associated with the yes arc from test k red red the relation associated with the arc from assign is with only pairs of the form q blue the composition is the empty relation which shows that the path from assign to test to assign is spurious section introduces our first technique called context implementation issues are considered in section the call string approach to interprocedural data flow analysis proposed by and pnueli sp is essentially a special case of context but is less robust call strings are discussed in section an alternative technique called flow k considered in section an benefit to the study of data flow tracing is that some of the results in we are refined and simplified the choice between context and data flow tracing in qualified analysis seems to depend on essentially the same factors the choice between elimination and iteration in ordinary analysis finally section the practical of our methodology including feedback effects on the task of passing from programs to meaningful data flow analysis problems data from preliminary experiments are presented at appropriate places in sections and the results are at least for the small sample of real programs that has been studied analysis is clearly feasible and is substantially more informative than ordinary analysis this paper is a of a technical report available from the second author for the sake of brevity we have only sketched the theory qualified analysis and the techniques for an ordinary algorithm to run well on problems constructed by our methodology to facilitate of the full report we have the original numbering except for some figures in the numbering here are of not errors we thank frank for and many discussions during the early stages of this work we thank for his comments especially regarding the optimization of the general data flow tracing procedure for solving qualified data flow problems we thank for a copy of sp and comments on its relation to this work finally we thank and mark wegman for comments on the presentation algebraic framework though the intuition about data flow analysis is fairly well we cannot simply a few widely read papers and say that basic ideas are as in this one or that one why not this paper is not toward any one or two analysis algorithms it shows how to solve a problem by constructing a problem and then solving s by any of a host of known methods it is therefore important that our notion of what it is to be a problem be free of assumptions in constructing we should not time obtaining conditions by only some or sometimes even none of the known algorithms a careful formulation of the important of data flow analysis will soon be available in sec our algebraic framework is summarized below to avoid parentheses the value of a function f at an argument x is fx rather than fx if fx is itself a function then is the result of applying fx to y the usual s and z symbols are used for arbitrary partial orders as well as for the usual order among integers a function from a partially ordered set to a poset is iff x y implies fx fy maps are sometimes called monotonic in the literature a meet lattice is a poset with a binary operation a such that x a y is the greatest lower bound of the set x y a meet semilattice every subset has a greatest lower bound is complete such a semilattice is also a complete lattice a fact that is interesting but does not happen to be used in this paper the greatest lower bound of a set x will be denoted a x in particular the empty subset has a greatest lower bound a t in l and t is then the maximum element in l when l is clear from context we will write just t here note the distinction between our meet and the join of we where least upper bounds are considered instead of greatest lower bounds for us strong assertions about a programs data flow are high in the semilattice this is somewhat more common than the use of join in the literature the algebraic context of a data flow problem is a pair l m consisting of a complete meet semilattice l and a set m of maps u l l intuitively maps in m can be represented in implementing data flow algorithms even if no convenient representation for arbitrary maps on l is available because this paper is not toward any one algorithm or family of algorithms for solving data flow problems we do not restrict l or m any further except as explicitly stated in the results in particular we do not assume that m includes the identity map and the constant with value t we do not assume that m k closed under composition and meet contexts that satisfy these assumptions are said to be closed as in r the framework here is as in r except that the fundamental definitions r defs ­ do not a closed context for a graph g with arc set ag the obvious extension of a map f ag m from arcs in g to paths in g may now take values outside m but its values are still maps from l to l for a given algebraic context l m a y is a g f e e with f ag m the local information e ng the entry nodes and e e l the entry information a problem is by any map i ng l no more at any node than can be propagated along paths from entry nodes transforming entry information by the local information associated with the arcs along the paths like an acceptable assignment gw p a good solution i dominates any fixpoint to illustrate the framework we return to figure and the question of availability of a b to indicate whether a b is available we use the semilattice l o with the usual ordering we need in addition to the bit values o and in order to avoid a ordinary availability analysis deals with several expressions simultaneously by manipulating a vector of bits it is for all the expressions of interest in a program to be available at any one point so it is for t to be assigned to a node when the analysis is complete because the simple example deals with only one expression we need o to avoid having in l play two roles the starting point t for iteration and the assertion that a b is available having chosen l we need a set m of maps to complete the algebraic context to solve the example availability problem by iteration it will suffice to use the set m y t where is the identity map on l and y summarizes the effects of instructions that generate the availability of a b because they compute a b without later changing a or b values of y are as follows the constant tm with value tl is not needed yet but we put it into m now for later convenience a more elaborate example would require a member of m for instructions that kill the availability of a b by changing a or b the global flow problem is shown in figure there is just one ordinary solution rd with o for all n the spurious path from assign to test to assign is responsible for the o bit at assign where a bit could safely be used the next section will formalize this intuition but first we must review the two major kinds of algorithm used for ordinary analysis in many examples l is wellfounded there are no infinite chains what have come to be called iterative algorithms can be used in any algebraic context where l is wellfounded the only operation on m used by these algorithms is eval hl x l l defined by x ux it is wellknown ki that these algorithms find the maximum solution whenever l is wellfounded and the context k distributive p v u e mv x y e a y ux a instead of assuming that l is wellfounded what have come to be called elimination algorithms assume that l m is a closed context and use the composition and pointwise meet operations on m as well as eval m x l l examples include ac gw r ta u most if not all of which need only assume as much about l m as is assumed in r to deal with loops we need to assume more than a closed context the effects of infinitely many paths must be summarized in finitely many steps the weakest assumption that is known to suffice is a concept whose general definition avoids assuming distributivity under a strong form of distributivity k more characterized as follows an algebraic context is strongly distributive iff v u q mv nonempty x l x note that is equivalent to with x restricted to nonempty subsets of l the general definition r def def of implies the following lemma lemma let l m be a strongly distributive closed algebraic context then l m is iff for each u in u is m and can be computed from u in at most t steps where t is independent of u and any composition or meet operation on m is counted as a single step our t corresponds to t in the more general definition instead of assuming that l is wellfounded elimination algorithms assume that l m is a closed context distributivity is often assumed but it is easy to many algorithms to avoid it as soon as one has distinguished good solutions from maximum solutions because is equivalent to in the common situation where l is wellfounded we are not really assuming more than is usually assumed in distributive contexts when we assume strong distributivity paths let there be given a finite set q whose members will be called we may think of q as a set of mutually exclusive and exhaustive assertions at any moment during the actual execution of a program just one holds now suppose that a program has been represented as a graph when control is at a node n and a q holds flow along some of n may not be possible if flow along c is possible then a r will hold when control reaches the target of c thus we have a relation pc on q with q pc r iff control might flow along c when q holds with the result that r holds the case where flow along c is impossible under q is covered by the condition q pc where for any relation p on q associated with each node m is a set of that might hold on with q the safe assumption in the absence of particular information on thk point a path c for consideration if so far as can be determined from this partial information about the program it seems possible specifically a path c cl ck starting a node m in e if there are qk for k o k such that q is in and qk for k k this is equivalent to saying pc where pc is the composition of the relations encountered along the path the identity relation on q in the special case k o of null paths and the notation is extended from members of q to subsets of q by up rc pv and we have explained the notations and sketched the for the following definition definition a qualified global data flow problem is a problem g f e e together with additional information a finite set q a relation pc on q for each arc c in g and a subset of q for each node m in e a path c in g from a node m in e to a node n is said to iff pc a solution for is any i ng l such that whenever m is in e and c m n is a path in g in s for example the problem from figure can be by the scheme shown in figure the set q is red blue where stands for whatever values color m test might have other than red and blue for we have q in the absence of information about the context of test in a larger program the relations pc on q assigned to arcs here are identity red red blue blue red red blue blue red blue blue blue blue we now have a problem with a qualified solution that is different from the ordinary solution only for n assign with i in this case to obtain data flow information we would like to solve problems in a way that considers only paths one approach would be to take a paper developing an analysis method and work through the entire development replacing each reference to all paths c by a reference to all paths c and checking that everything is still meaningful and correct this would be tedious hence and would have to be repeated for each analysis method to be considered a more promising approach is to use old methods in new ways given a qualified problem we construct an ordinary problem with the same graph but a different algebraic in section this is context alternatively we construct an ordinary problem with the same algebraic context but a different graph in section this is an application of execution called flow tracing the basic result is the same in both cases any good solution to the new problem with all paths considered defines a good qualified solution to the original qualified problem this section concludes with some auxiliary considerations common to both techniques definition let g f e e be by q pc for each arc c and for each m the auxiliary algebraic context consists of a semilattice and a closed monoid of maps on where is the set of all subsets of q with the partial ordering u v iff v g u and where f is in iff there is a relation p on q such that fu u p for all u s q it is easy to check that really is a closed monoid of maps we use containment rather than inclusion for because larger sets are less informative thus is t ux while q itself is l ux some easily checked properties of the auxiliary context will be important here is finite is strongly distributive and ut aux t ux for all u in definition as in def the auxiliary flow problem g e inherits g and e from but has the auxiliary algebraic context and v c c u u pc v m q the member qn of assigned to a node n in the maximum solution for the auxiliary problem is said to be the set of relevant at n any of the usual data flow analysis algorithms can be used to find the relevant at each node the utility of this information is more apparent when we relate qn back to the original situation with a qualified data flow problem m s e q c m nq pc r thus qn tells what can hold at n when reached by a path the size of qn over all the nodes n in the graph will be more important to the efficiency of our techniques than is the size of q itself the pair g e is called a flow scheme in r here it will be convenient to call the g e q pc i c c ag i m e a fw scheme where by m c e we mean the function with domain e whose value at m is and so on note that aux and therefore the relevant at each node depends only on the qualified flow scheme associated with if several qualified problems have the same qualified scheme then only one auxiliary problem needs to be considered definition as in def a qualified fixpoint is any j ng l such that for all m in e with and all c in ag with pc n x o jm em and s a good qualified solution is any qualified solution i such that for each qualified fixpoint j and each n in ng in z jn comparing this definition with r def we see that every ordinary fixpoint is also a qualified fixpoint but not conversely qualified fixpoints are only constrained for relevant choices of m and c there are new and possibly larger fixpoints to be dominated an ordinary good solution may fail to be a good qualified solution even though it is trivially a qualified solution this may sound but it is consistent with the usage of words like good for example a programmer is a person but a good programmer may fail to be a good person from it follows that the relation pc n x could also be described by the similar expression with q in place of this of pc will be mentioned frequently so we abbreviate pc pc n x pc n x q context intuitively we may think of the original context l m as a family l u m of scalars subject to addition and multiplication operations that are only defined for certain pairs of scalars the sum of x and y is x a y provided x and y are in l the product of u and x is ux provided u is in m and x is in l some will be needed because the usual laws of numerical arithmetic do not hold but potential technical difficulties may be ignored in this intuitive sketch for qualified analysis we use a new algebraic context lq mq members of lq are vectors whose components are scalars from l for k i q the vectors have length k because we want to be able to associate various members of l with each graph node one member of l for each in q as will be seen in section doing this is not as costly as it seems at first what set mq lq lq of maps should be used here we rely on the analogy with linear algebra a simple way to transform vectors to vectors is to apply linear operators and linear operators from to correspond to matrices each map in mq will be represented by a matrix q when vectors q have q the component qr can be found by the products as q ranges over the indices because our sums and products do not the laws of numerical arithmetic very few of the theorems of linear algebra can be applied here in particular the matrix representation of an operator is not unique here some is needed in writing up the idea as the precise context construction with the algebraic structures actually available but everything does work out given a map with domain q x q it will be convenient to denote the value q r at the argument q r by q the order of q and r when we put them in subscript position this reversal is a little trick to maintain a close analogy with ordinary notation despite the fact that composition of functions is backwards when compared to composition of relations pos ax c vu ua v bc let l m be an algebraic context let q be any finite set and let lq be the set of all f c l with eq rather than our usual q for the value of at q by analogy with vector notation so that lq is partially ordered by n if f fq s qq for all q let mq be the set of all lq lq such that for some q x q m with value rq at q r o lemma then lq mq is an algebraic wellfounded if l is wellfounded context and lq is theorem let l m be the algebraic context of a problem g f e e that is qualified by q pc for each arc c in g and for each m c e suppose m includes the constant t with value tl let be the problem g fk e e with algebraic context lq mq where g and e are inherited from and fc corresponds to with rq if q pc r then fc else tm if q then em else t suppose is is a good solution for then a good qualified solution for is obtained by setting for each node n in g in a ir i r qn in the running example from figure figure and figure the problem and a good solution are as in figure for n assign the good qualified solution determined by i has a a the path from assign to test to assign does not trick qualified analysis into here and even could be solved by inspection if theorem is to be of more general utility we need some that is not more difficult to solve than the new problem can be solved in various ways the available algorithms differ in what they about their inputs as was noted in section iterative algorithms a wellfounded semilattice but need no operations on maps other than evaluating them at specific arguments theorem has the following direct corollary corollary as in theorem any iterative algorithm that can solve alone can also solve with by first solving practical global flow problems almost always involve wellfounded but the chains can be long this is one reason for considering elimination algorithms even in distributive contexts where the solutions they find are no better than those found by iterative algorithms as was noted in section elimination algorithms a closed context the next lemma shows that lq mq is indeed closed under certain conditions conclusions are only under the corresponding hypotheses lemma let l m be a strongly distributive closed algebraic such that each u q m has ut t let q be any finite set then lq mq is a strongly distributive closed algebraic context such that each p mq has t t elimination algorithms can represent members of mq with i q i by i q i matrices of representations of members of m if each u in m has ut t then the representation is determined by in lemma if not then several matrices may represent the same member of mq for example suppose m includes a constant map u with value w t many matrices represent the constant map in mq with w w w in lq the rules for representing the basic operations on mq are given by of the rules in linear that represent operations on linear operators by operations on their matrix representations given representations of and v we can find representations of and a of course members of lq can be represented by i q i vectors of representations of members of l at first this seems costly if it costs fl to store a member of l and pm to store a member of m then for k i q i it costs to store a member of lq and to store a member of mq in the obvious way an way is therefore considered in section for the present all that is if we can represent members of l and m then we can also represent members of lq and mq for we use some ideas from sec simplified for strongly distributive contexts if l m is a strongly distributive closed context then lq mq can be shown to be as well as by lemma strongly distributive and closed at present we do not know how to obtain without strong distributivity corollary as in theorem any elimination algorithm that can solve alone can also solve with by first solving provided that the original algebraic context is strongly distributive implementation of context the obvious implementation of theorem would multiply costs by k or k for k i q the example from figure illustrates the properties that a slightly more implementation can exploit each relation pc is only about k pairs rather than the up to k pairs possible in a relation on q therefore each fc matrix in theorem is it contains only about k nontrivial entries where t is in numerical computation a trivial entry is normally here it is tm the principle is the same instead of storing q x q m as an array one can store some means for testing whether is trivial and some means for rq when the test says that it is nontrivial in practice one may wish to use a test that is fast and simple even if it is slightly pessimistic if the test reports that is nontrivial when actually yields rq tm then the pairs q r that produce this behavior are processed but correctly the costs of may be less than the costs of applying an exact test to all pairs q r two sparse matrix representations can be one for each of the broad data flow analysis strategies for iteration each q q x q m to be represented is a matrix for in theorem testing whether q pc r is a good way to test whether is nontrivial if q pc r then is found by finding fc this scheme is pessimistic for the arc c with fc tm for elimination one may matrices derived from the simpler c matrices by the o a operations on mq one could use a hash table see au pp for an introduction to hashing if q r to a table entry then rq is nontrivial and can be found by looking in the table moreover the table can store pointers to members of m rather than members of m when corresponds to in theorem every q r with q nontrivial has the same member of m as every other q r with rq nontrivial in general we expect the matrices encountered by any elimination algorithm to be quite sparse and to have fewer nontrivial entry values than they have nontrivial entries and indirection are therefore for representing members of mq even though they do not improve the worstcase time bounds trivial vector entries q tl are not so common we do have eq tl whenever is associated with a node n that has q in q qn the average value of i qn in our example is and most q values are nontrivial moreover even iterative algorithms distinct nontrivial entry values as happens at four nodes in figure so far we have discussed representations that exploit one could also a given flow analysis algorithm to the characteristics of problems as constructed in theorem this is not the place to report on a implementation of context with a fast but subtle basic algorithm no such implementation is available the experience of building and using an data flow analysis package would a paper in its own right to use of qualified analysis in such we will remark on some of the possibilities for a particularly simple algorithm the algorithm is essentially that of ki in current notation with maps assigned to arcs rather than nodes at this point the full report states the basic algorithm propagate and discusses ways to it so as to run well when the algebraic context has the form lq mq here we present the minimum needed to understand the experimental results the version q ga te is much longer to write down but at least it is derived from propagate by fairly simple transformations both propagate and use a worklist w of arcs c that need attention because the current information at the target of c might not be dominated by what can be propagated along c from the current information at the source of c each execution of the statement work takes an arc c from w and does whatever is necessary which may adding arcs to w the first author has compared the costs of ordinary and qualified analysis with programs called ext and they are modules from a large operating system for each program a semilattice l of bit vectors is considered with bit vector positions corresponding to pairs reg n where reg is a register and n is a node corresponding to a basic block of program text that assigns to reg a bit in the position corresponding to reg n indicates that reg received its present value at node n each is an assignment of label values to each of several registers that are used by the program in order to branch to whatever label is indicated by a certain register this kind of is discussed more fully in section for the present all that is the size of q and the average size of i i over the nodes of the graph table summarizes the basic characteristics of the four programs the last column shows what percentage of the arcs c have pc s aq where table compares the costs of ordinary and qualified analysis the ordinary work column in table tells how often work was executed by propagate in solving the ordinary problem with ignored the qualified work column tells how often work was executed by in solving the problem constructed by theorem in general qualified analysis more though not by much the program is very and has so many spurious paths that convergence is faster with qualified analysis of course the number of executions of work is not the whole each execution of work in costs more than a similar execution in propagate we therefore consider a statement comp in that computes a member of l and has a cost comparable to the cost of work in propagate the c column counts executions of comp after an intermediate degree of while the comp column counts executions of comp after all the done in the full report comparing this column with ordinary work gives a idea of the cost expansion involved in qualified analysis under our working assumption that calls on eval and on a are much more expensive than of course this is only the expansion involved when the basic algorithm is propagate for a prediction of the costs of one can multiply the average i qn i by the number of work executions in propagate the last column in table lists the which are rather pessimistic these results do not reflect several other possibilities such as of the high percentage of arcs that have pc aq the preliminary experiment indicates that analysis will be feasible for real programs despite the factors of k and k for k q i that enter into worstcase bounds on the costs of steps in data flow analysis the preliminary experiments implemented w by the arcs in a circular list and associating a bit with each arc to indicate whether the arc was currently in w the operation take c from w moved a cursor along the list as soon as an arc with a bit was found then the bit was set to o and the arc the value of c for take operations the cursor was left where it in the simplest implementation only take would move the cursor in the experiments however give d to w compared d with the current cursor position and moved the cursor back to d when d was earlier in the list this slight of the should convergence the arcs were ordered by an ordering of their sources with arbitrary when two arcs had the same source the node ordering was as used in hu the net effect of implementing w with is much like the net effect of beginning with the iteration in hu and then optimizing in a natural way to avoid members of l when nodes whose predecessors have not acquired new members of l since the last visit this optimization has been before hu p but it seems not to have been that one gets to essentially the same place by an easier start with propagate from ki and implement w with rather than the implementations considered in ki two other natural implementations are as a queue the first thing in is the first thing out and as a stack the last thing in is the first thing out the full report shows that fifo and were to in their performance on the programs analyzed in the preliminary experiments addresses and call strings assembly languages commonly have some kind of load address instruction such as lab where reg is a register and lab is an instruction label performing this instruction puts the address indicated by lab into reg a branch register instruction such as the instruction counter to whatever address is held in reg in data flow analysis passes control to any instruction whose address might be held in reg there is usually more than one such address since programmers use branch register instructions when direct branch instructions will do the same job if reg can hold either al or a then a block ending with will have two in the usual graph representation one to the block beginning at al and the other to the block beginning at az along some control flow paths will put al into reg and eventually will pass control to a even though reg has not been changed since the load address instruction these spurious paths are much like the ones in the example program from section the preliminary experiments discussed in section use qualified analysis to avoid spurious paths that branch the wrong way at each member of the set q q is a map from certain registers that figure in instructions to certain address values that figure in instructions of course it is possible in assembly language to treat the result of an arbitrary computation as an address but real programs tend to very conservative in manipulating the register values they may later branch to if a register reg appears in a instruction then there are few relevant instructions that can change reg the instructions that can change reg with an effect on are usually of two forms lab or a memory fetch that puts the contents of memory cell mem into reg such can restore reg to hold an address that was saved by a memory store instruction just before a computation that used reg for some other purpose the instruction types just load address branch register branch memory fetch and memory store are enough to illustrate how qualified analysis may be applied to assembly language programs with a slight modification they will also illustrate the relation of qualified analysis to interprocedural analysis consider a simple way to implement nonrecursive a single register reg will suffice provided we also use a memory cell mem for each procedure the code for a call on a procedure with a cell mem for holding return addresses will for parameter transmission and then continue in the following way where lab indicates the entry point of the called procedure lab lab the called procedure can return simply by executing of course is not the only way to code the control aspects of procedure but it is typical enough and easy to write down the same information contained in the choice q q of in the preliminary experiments could be encoded by strings of addresses each lab in adds the address indicated by lab to the end of the current string and each removes the last address from the current string the length of address strings is bounded by the bound on lengths of call chains in a nonrecursive program for such programs the call strings approach to interprocedural analysis in sp sec is essentially the same as context with q taken to be this encoding q of q for example our paths with q q correspond to valid sp sec as is apparent from sp lemma the theorems in sp are restricted to strongly distributive algebraic contexts l m in such contexts sp is a corollary of theorem when nonrecursive programs are considered because sp considers recursive programs too it is important to consider what happens when our theory is generalized to avoid the assumption that q is finite with infinite q all cases of sp would come under theorem with q allowed to be infinite section is still meaningful but the relevant at each node can no longer be found by straightforward iteration in finitely many steps theorem is still true and even has the same proof like sp theorem does not immediately yield algorithms when q is infinite at least when q q and l is finite section of sp shows how to restrict attention to a finite subset of q without loss of information the are such that it is difficult to under which call strings would be to the functional approach in sp sec however call strings are of interest here for two reasons first they yield a simple definition of the goals of interprocedural analysis as a special case of qualified analysis even though one may prefer the functional approach for doing the work second the ideas in sp may generalize to other situations with infinite q but finite sets at least for finite l returning to the case of nonrecursive procedures and finite q q we note that context is more robust than use of call strings sections ­ of sp can only be applied to programs with a strict discipline expressed either by explicit calls or by lowlevel code that obviously simulates them as in the from procedure calling will all the questions about call strings from the general to the particular we proved theorem and its long before considering context is applicable to lowlevel code that uses instructions like mostly in the manner of but in some other way perhaps because of error exits the worstcase assumption pc q x q is available for the very when a program treats the result of a nontrivial computation as a address data flow tracing a special kind of symbolic execution has been called valuation si or expansion we we will use a longer but more phrase tracing given a global data flow problem with a finite semilattice we construct a new graph h with some useful properties the nodes of h are pairs n x where n is a node from the original graph g and x is in the semilattice there is an arc cx from n x to p y if there is an arc c in g from n to p with y in the full report h is constructed in the natural way by a program appropriate for any algebraic context with a semilattice in the common special case where ut t for all u in m a smaller graph h is just as good because nodes of the form p tl are unnecessary for this special case we can replace with i a program that leaves out ch and p y when y tl the following theorem implicit in we sec b refines and simplifies results theorem let g f e e be a global data flow problem with l finite and let h be the output from or in the special case with input then i no by in xc is the maximum solution for data flow tracing is of interest for several reasons when l is finite and m is not distributive it provides a possibly slow but definitely simple way to find the maximum solution for a problem when fragments of program text are associated with the nodes in g the expanded graph h can be used to specify an expanded program that is more amenable to optimization our interest in data flow tracing comes from the case where is aux from def for a qualified flow problem optimizing for this application to qualified data flow problems we let the nodes of h be pairs n q with q q rather than pairs n u with u s q the following program results begin g e q pc i c e ag i m e qualified flow scheme w subset of ng x q h graph n p nodes in g c arc in g q r in q get g e h graph with node set w and arc set while w do take n q from w for c in do p for all r in q pc do if p r then add node p r to h give p r to w put h add arc cx from n q to p r in h end theorem let g f e e be qualified by q pc for each arc c in g and for each m e e let g be the output of with input g e and so on let be the problem g f e e where fc for c the arc in g that makes em em q em add c to g suppose is a good solution for then a good qualified solution for y is obtained by setting for each node n in g in a in q i n q is a node in gs corollary as in theorem any iterative algorithm that can solve alone can also solve with by first solving d corollary as in theorem any elimination that can solve alone can also solve with by first solving provided that the algorithm can with arbitrary graphs two questions are important in studying the efficiency of data flow tracing for qualified problems first is g much larger than g the ratio i ng ng is precisely the average of i qn i over all n in ng and the ratio i ao i i ag i is likely to be a little less one can to have nearly i q i nodes in g for each node in g but this may only happen in programs written to establish worstcase bounds on the size of g in any case the implementation strategy outlined in section can be applied to data flow tracing also second is gi less tractable for analysis than is g for example consider the class of reducible hu graphs in practice the graph g is usually reducible kz kn but g is another the report illustrates how easy it is to lose when g is constructed thus we are with the of applying algorithms with quadratic worstcase bounds to graphs substantially larger than the original graphs worstcase bounds are not the whole however the practical experience with iteration has been much better than worstcase bounds would suggest as by mr p as well as our experience when w is with the number of executions of work is usually less than i ag i and often less than i ag i at least for relatively simple traditional problems like available expressions data flow tracing was applied to of the programs from section with being too large for the experimental implementation table compares computational effort under the two techniques context with and data flow tracing with propagate applied to the graph g the work and comp columns come from table while the work after tracing column counts executions of work by propagate graph g by with input the preliminary experiments suggest that context is somewhat more efficient than data flow tracing when iteration is used for ordinary analysis but the difference is small and might well be out by costs with a amount of data flow tracing becomes essentially as fast as context under the conditions of the experiment the programming effort required to implement context is greater to code is more work than to code code propagate and finally code a simple from output of to input of propagate the modularity of qualified analysis with data flow tracing is a major advantage on the other hand some applications may have more of nontrivial vector components inr tl for q r for such applications the more elaborate in may be it must be that the comparison here deals with lowlevel programs and simple traditional bit vectors it must also be that we have dealt with exhaustive analysis of entire programs as is usual in the literature on data flow analysis the practical cost considerations are very different and largely when one deals with highlevel programs with more analysis eg static type checking or subscript range analysis ha or with demand analysis bj of large programs that sometimes small changes in short our comparison data flow tracing over context has been under the same conditions that iteration over elimination the only general data flow algorithm designed for conversion to demand analysis of highlevel programs is the elimination algorithm of and elimination algorithms are generally more for these purposes than iterative algorithms further discussion of the choice between iteration and elimination is in sec and the references there the choice between context and data flow tracing seems to reduce to the choice between elimination and iteration for ordinary analysis having chosen elimination or iteration one should choose context or data flow tracing for qualified analysis practical the kind of is suggested by the in sections and given a qualified problem we might ignore the and find an ordinary good solution most of the algorithms that could be used for this purpose could instead be used to find a good qualified solution at greater cost we can ask whether the additional information is worth what it costs the answer depend on the program from which the is abstracted a program that has a long lifetime and is expected to be reliable or efficient will a analysis the most extreme examples of thk kind of program are operating systems the first author has applied qualified analysis to modules from a large operating system with costs that have been discussed in sections and table shows the increase in bits over the nodes in the graph when is compared with the last column shows the increase in bits when the average over q in qn of the number of bits in is compared with and then over the nodes to obtain we first obtain with values in lq and these can be saved for future reference if i has nontrivial q f then i tells us more than merely by accessing tuples we can obtain a answer to a question of the form suppose control reaches thk node with that what is sure to be true with nothing but or even to we would either have to the program or reply with only what is sure to be true regardless of the in the question for the four programs analyzed in table the last column shows that i can be expected to be or more informative than while a few programs like will yield increases to determine whether the information is worth what it would cost to obtain with a implementation of qualified analysis we would need to perform an elaborate experiment that the costs of making do without the information as well as the costs of obtaining it a view should be taken precisely because the mathematical theory by considering data flow problems abstracted from the concrete programs that pose them it is important to remember that practical data flow analysis has a program not a problem as the input passing from a program to a problem is a task if one wants meaningful information about the program for example consider availability of expressions involving variables because i and j can have the same value assignments to ai k must be taken as aj k b on the other hand because i and j can have different values computations of ai k b must not be taken as making aj k b available to meaningful availability information specifies f ag e m is to assume precisely the opposite of whatever is desirable and neither i j nor i j is desirable throughout when one that a program instruction the assumption i j is followed by one i j without any changes to i or j one begins to feel that there must be a better way to proceed there is let q i j i j and consider how the truth of either assumption is affected as control flows along each arc in the graph chosen to represent control flow in the program we get a qualified flow scheme and an auxiliary problem as in def the relevant at each node can be found now suppose c is an arc from n to p if q in qn holds at n and r in qp holds at p then it is relatively easy to specify a member of m that transforms information about the availability of ai k b correctly for this particular pair q r doing so we get q in m considering all pairs q r and using t for irrelevant pairs we get a matrix that represents a map in mq forming would be a first step in trying to choose a safe fc in m to cover all possibilities since we could then take fc to be a q i q r e q rather than choose fc in this way and then solve a qualified problem we should solve the ordinary problem with unlike the problem from theorem constructed from a qualified problem with no reference to the actual program this problem is constructed from the program in a way suggested by context except for the fact that even iteration algorithms will sometimes distinct nontrivial entries in the same matrix the implementation considerations from section apply solving need not be as costly as a at i q i would suggest in this simple example i q i is only because the matrix q is much more informative than l q i q r c q solving will be much more informative than applying theorem without being applied in any strict logical sense context has a effect on the way we formulate problems in the first place context may be more useful because of its feedback effect on problem formulation than because of results like theorem in this setting a further gain in the of data flow information can be had by allowing the entry information ex e lq to have nontrivial q fr with our example q i j i j this may not be important but consider call strings as discussed in section a compiler could analyze a routine in isolation from the routines that call it but without failing to any fact abstracted from the callers for example there might be two expressions such that one is available on entry when the caller is while the other is available on entry when the caller is the routine can be analyzed with varying according to whether q indicates that the caller is or without a theory of the relationship between programs and problems we cannot summarize the feedback benefits of our methodology in mathematical theorems for the present we make do with examples references au aho a v and unman jd of design addisonwesley reading ma ac allen f e and j a program data flow analysis procedure comm acm bj w a and m the method of attributes for data flow analysis part ii demand analysis acts gw graham s l and wegman m a fast and usually linear algorithm for global flow analysis j acm ha wh compiler of the value ranges for variables ieee trans on software engineering hu m s and unman jd a simple algorithm for global data flow analysis problems siam j computing jm jones n d and muchnick ss eds program w analysis theory and applications prenticehall nj to appear j b and unman jd global data flow analysis and iterative algorithms j acm kz kennedy k w and l applications of a graph grammar for program control flow analysis proc th acm symp on principles of programming languages january ki ga a unified approach to global program optimization proc acm symp on principles of programming languages october kn knuth de an empirical study of fortran programs software practice and experience mr e and c global optimization by of partial comm acm r rosen bk monoids for data flow analysis proc th ann acm symp on principles of programming languages january rosen bk monoids for data flow analysis siam j computing to appear an earlier version with different numbering appears as r rosen bk of availability as an introduction to the general theory of data flow analysis to appear in jm sp m and pnueli a two approaches to interprocedural data flow analysis report computer science department new york university new york september to appear in jm si m calculating properties of programs by valuations on specific models proc acm symp on proving assertions about programs sigplan notices january ta tarjan re solving path problems on directed graphs computer sci dept stanford u november u unman jd fast algorithms for the elimination of common subexpressions we b property extraction in wellfounded property sets ieee trans on engineering figures and tables next three pages m yes assign color red color blue yes i figure of a whenever assign path from o figure bits whether a appear at each of the nodes ordinary analysis put different assign but the same bits at other nodes the entry information is all but two arcs for arc from assign to test and for c the arc from assign to next fc y q v assign v figure qualified flow scheme for the example from figure arcs c have pc the identity relation on q v i figure each tuple f is displayed f for c the arc from assign to test and for c the arc from assign to next f corresponds to the matrix q with qq if r red q then y else tm other arcs c have fc in mq with each matrix entry being or t program vs ng i bit vector length iq i average i i percentage pc s ao table the average number of relevant at a node n is less than more than of the arcs c have only pairs q r with q r in their relevant relations program ext average i i ordinary work qualified work comp comp prediction table cost comparison between ordinary and qualified analysis program ext qualified work comp work after tracing table cost comparison between context and data flow tracing program ext bit vector length average i i ­ ­ table percentage increases in number of bits the number of bits in i is compared with the number of bits in and the percentage is over all nodes n the average over q in qn of the number of bits in iq is compared with the number of bits in and the percentage is over all nodes n 