a sound and complete abstraction for reasoning about parallel prefix sums f college london abstract prefix sums are key building blocks in the implementation of many concurrent software applications and recently much work has into efficiently implementing prefix sums to run on parallel processing units because they lie at the of many applications the correctness of prefix sum implementations is of prime importance we introduce a novel abstraction the interval of that allows scalable reasoning about implementations of prefix sums we present this abstraction as a monoid and prove a soundness and completeness result showing that a generic sequential prefix sum implementation is correct for an array of length n if and only if it computes the correct result for a specific test case when instantiated with the interval of monoid this allows correctness to be established by running a single test where the input and result require on space this improves upon an existing result by where the input requires on space and the result on space and is more feasible for large n than a method by that uses on space for the input and result but requires running on tests we then extend our abstraction and results to the context of programs developing an automated verification method for gpu implementations of prefix sums our method uses static verification to prove that a generic prefix sum implementation is data after which functional correctness of the implementation can be determined by running a single test case under the interval of abstraction we present an experimental evaluation using four different prefix sum algorithms showing that our method is highly automatic to large thread counts and significantly method when applied to large arrays categories and subject descriptors f logics and meanings of programs specifying verifying reasoning about programs keywords parallel prefix sum computation abstraction formal verification this work was supported by the eu fp project project number the psl project and an first grant permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ san diego ca usa copyright © acm introduction the prefix sum operation which given an array a computes an array b consisting of all sums of prefixes of a is an important building block in many high performance computing applications a key example is stream compaction suppose a group of n threads has calculated a number of data items in parallel each thread t t n having computed dt items now the threads must write the resulting items to a shared array out in a compact manner ie thread t should write its items to a series of dt indices of out starting from position d · · · dt the data stream by the threads so that thread writes its results followed by thread etc would be slow instead compaction can be performed in parallel via a prefix sum each thread t writes its count dt to an array count at position t then the threads perform an exclusive parallel prefix sum defined formally in section on count to yield an array index the prefix sum sets the elements of index to d d d d · · so that a thread t can write its results to out starting from position stream compaction is just one example the prefix sum operation is defined for any binary associative operator and prefix sums using various operators have found wide application in computationally tasks a selection of examples are in table for several the design of efficient parallel prefix sums has been an active research area parallel prefix sums were first implemented in hardware circuits as logic for propagating carry bits in the design space for these circuits has been explored and important circuits known in the literature are due to and and fischer and and more recent work by has further explored the design space in software prefix sum algorithms have been used in the context of evaluating polynomials in parallel and has proposed the use of prefix sums as a primitive parallel operation giving numerous examples of their application prefix sums are now primitives in parallel programming models such as and recently there has been a great deal of interest in implementing efficient prefix sums for acceleration on parallel processing units implemented using gpu programming models such as and the major benchmark for generalpurpose gpu programming and all include a variety of prefix sum implementations the and data parallel primitives libraries implement efficient prefix sum primitives for we present a method for formally verifying that a generic parallel prefix sum to work for any data prefix sum application stream compaction sorting algorithms polynomial interpolation calculation binary addition finite state machine simulation datatype int int float int transition functions operator a max carry operator function composition a floating point multiplication is not actually associative but is often treated as such in applications where some error can be table some applications of parallel prefix sums stream compaction requires an exclusive prefix sum the other applications employ regular prefix sums the carry operator and functional composition are examples of operators type with an associative correct because prefix sum implementations are at the of many parallel applications the correctness of these implementations is the challenges of concurrent programming make it especially hard to correctly implement prefix sums for modern architectures such as this a verification method by observing that a generic prefix sum algorithm may only exploit the property of associativity we have a novel abstraction which we call the interval of abstraction for i j the interval of abstraction represents a interval of input elements ini · · · inj with respect to any data type and associative operator abstractly as a pair i j abstract intervals can be added together if they the abstract sum of i j and k l is i l if j k otherwise the addition results in a special value which represents all sums of input elements that are not necessarily any monoid element to yields modelling the fact that using only the property of associativity a of inputs cannot be made by adding more inputs we present the interval of abstraction as a monoid with an identity element representing an empty this makes it possible to run a generic prefix sum implementation with respect to the data type and operator defined by the interval of monoid our first main contribution is a theorem showing that a sequential generic prefix sum on n elements is correct for all data types and operators if and only if when instantiated using the interval of monoid and applied to the input sequence n n it computes the correct result sequence n this theorem shows that the interval of abstraction is sound and complete by running a single test we can establish either that the generic prefix sum is correct if the test passes or that it is incorrect for one specific pair namely that of the interval of monoid itself if the test fails our result provides a highly scalable method for verifying sequential prefix sums elements of the interval of monoid capable of representing intervals of up to length n can be encoded using bits allowing a generic prefix sum to be verified by running a single test case requiring on space for both the input and result this is an improvement on a previous result by which allows a sequential program implementing a prefix sum to be verified by running one test case requiring on space for the input and on space for the result for large values of n this space requirement becomes infeasible our method is also more practically feasible than an alternative approach of that uses only on space for the input and result but requires running on tests void t in t out out in i i n i ini figure a sequential prefix sum for inputs of length n our second main contribution is an extension of our method and theoretical results to the case of programs the programming model of gpu kernels this is a contribution over previous work on the correctness of parallel prefix sums which applies to synchronous parallel hardware described as sequential haskell programs but not to asynchronous concurrent programs we show that if a program implementing a generic prefix sum can be proved free from data races then correctness of the prefix sum can be established by running a single test case using the interval of monoid as in the sequential case we use this result to design and implement a highly automatic method for verifying parallel prefix sum implementations at the level of gpu kernel source code using the tool to prove data we present a large experimental evaluation using four different prefix sum algorithms implemented as kernels we show that of these kernels can be proven efficiently for all element sizes up to and that verifying the kernels by running a single test is very fast using two two intel for which is also supported and an arm gpu prefix sums on vectors of up to length can be checked in a matter of seconds we argue that once has been established the method of can also be used to prove correctness of prefix sum implementations using testing but show that our method approach which requires running on tests because we can efficiently prove data of gpu kernels implementing prefix sums for all useful sizes and because verification using our method involves running a single test case taking no longer than using the prefix sum in practice we claim that we have made a major step towards solving the problem of verifying gpu implementations of generic prefix sums background on prefix sums we briefly review the definitions of a prefix sum and an exclusive prefix sum and give examples of sequential and parallel prefix sum implementations given a set s with an associative binary operation ie a the prefix sum of a list s s sn of elements of s is the list s s s s s · · · sn consisting of all sums of prefixes in increasing order of length for example if we consider the set of integers under addition the prefix sum of is if s has an identity element so that s is a monoid then the exclusive prefix sum of s s sn is defined as s s s s s · · · sn for example if s is the set of all binary numbers and is the operator with identity then the exclusive prefix sum of is it is trivial to implement a sequential prefix sum the program of figure computes the prefix sum of in into out kernel void local t in local t out barrier for unsigned offset offset n offset t temp if tid offset temp barrier if tid offset temp barrier figure prefix sum implemented in o set o set o set figure evolution of the output array for the kernel of figure with n for an example input where t is some data type with associative binary operator an exclusive prefix sum can be implemented similarly figure shows the parallel prefix sum implemented in and designed to be executed by a single of n threads the function is marked kernel to indicate that it is the entry point for a gpu kernel and arrays in and out have the local qualifier to indicate that they are allocated in gpu memory local to the threads execute and a thread can use tid to access its unique thread identifier in the range n threads by calling barrier which causes all threads to wait until every thread reaches the barrier statement on each iteration of the loop every thread whose tid is greater than or equal to offset sums the value it has already computed with the value previously computed by the thread offset places to the left figure illustrates how the output array when the kernel is instantiated using integer addition and applied to the input with n sequential computational model we present a sequential imperative programming language and define what it means to implement a correct prefix sum in this language this provides a simple foundation on which to clearly introduce our interval of abstraction which we do in section we extend our language abstraction and theoretical results to apply to programs and in particular gpu kernels in section syntax and typing the syntax for our language is shown in figure where c ranges over literal values v and a over scalar and array variable names respectively and op over an unspecified set of binary operators our presentation can be easily extended to for operators of other for ease of presentation we assume throughout that the input length n is into the program under consideration in practice n would be supplied as a parameter for readability we a little from the precise syntax of expr e c literal v variable ae array element e op e operator stmt s v e variable assignment ae e array element assignment if e ss else ss conditional while e ss loop ss empty sequence s ss sequence figure syntax all variables arrays and literals are typed assuming some standard syntax for variable and type declarations which we omit types for scalar variables and literals are drawn from a set of base types t ranged over by t which includes at least integers and booleans denoted int and bool with the usual literals and the type unit array variables have type which denotes all maps of type int t for ease of presentation we assume no errors occur and we do not allow arrays of arrays the language also features such as pointers procedures and control flow in section we argue that our technique extends to languages with these features the typing rules of the language are straightforward and are depicted in figure as usual there is a context which specifies the types of variables operational semantics let var be a set of variables and arr a set of arrays all of which are assumed to be typed a variable store v is a mapping var t t t such that if v var is of type t then vv is of type t an array store a is a mapping arr t t such that if a is of type then aa is of type expressions are evaluated under a variable store and an array store denoting the evaluation of an expression e by e v a we define c v a c v v a vv ae v a aa e v a e op e v a e v a op e v a figure gives the operational semantics for our language the semantics is defined over program states s v a ss with v a variable store a an array store and ss a sequence of statements in the figure ss · ss denotes the concatenation of sequences of statements ss and ss the rules are standard for an imperative language like ours except that we have both a variable and an array store instead of just a single store although this split is not strictly needed here it the extension to programs in section where we shall regard variables as threadlocal and arrays as shared among all threads let a program p be a sequence of statements an initial state of p is any program state with p as the sequence of statements given an initial state s of p an execution of a program p is a finite or infinite sequence s s s · · · s si s si s · · · with each si i a program state an execution is maximal if it a cannot be extended by applying one of the rules from the c of type t ct vt vt a e int ae t op of type t × t t e t e op e t a typing rules for expressions e t top vt et v e unit a e int ae e unit e t e bool ss unit ss unit if e ss else ss unit e bool ss unit while e ss unit unit s unit ss unit s ss unit b typing rules for statements figure typing rules of our sequential programming language v vv e v a v a v e ss s v a ss e v a n a e v a a aa a v a ae e ss s v a ss e v a v a if e ss else ss ss s v a ss · ss ¬ e v a v a if e ss else ss ss s v a ss · ss e v a v a while e ss ss s v a ss · while e ss ss ¬ e v a v a while e ss ss s v a ss figure operational semantics of our sequential programming language operational semantics or b is infinite we say p terminates for an initial state s if all maximal executions starting from s are finite note that the maximal execution is unique in the current case as execution is deterministic this will no longer be so in the case of section the proof that our type system and operational semantics satisfy the usual safety types are preserved under execution and progress can always be made unless termination has occurred section is standard prefix sum algorithms we now define what it means for a program in our language to implement a prefix sum algorithm recall from section that a prefix sum can be defined with respect to a and an exclusive prefix sum with respect to a monoid because most prefix sums of interest in practice are over monoids we shall consider prefix sums over monoids all the results we present restrict easily to the case of we shall write m to refer to a monoid with elements sm t binary operator m which we assume is a programming language operator and identity m sm moreover we say that a variable v respectively an array a is readonly in p if no assignment of the form v respectively ae occurs in p definition let m be a monoid p be a program n a natural number and in out arrays of type such that in is readonly in p the program p computes an m prefix sum of length n from in to out for an initial state s if p terminates for s and for each final array store a it holds that m ik k n the program p implements an m prefix sum of length n from in to out if p computes an m prefix sum of length n from in to out for every initial state whether a program computes a prefix sum for a given input can be established by running the program determining whether a program implements a prefix sum amounts to functional verification implementation of an exclusive prefix sum is defined analogously the array in as readonly is for ease of presentation making it sufficient to consider only the final array store in the above definition and avoiding the need to relate final and initial array stores we observe that definition over all possible final array stores to be able to cover the case of section in the current sequential case the final array store is unique as execution is deterministic we shall assume that a prefix sum is always from in to out and shall simply talk about a program implementing an m prefix sum of length n or computing an m prefix sum of length n from an initial state the interval of abstraction we now turn our attention to proving the correctness of generic prefix sums prefix sums that are designed to be polymorphic so that they work for any type and operator that together have the properties of a monoid we present our main theoretical result that correctness of a generic prefix sum of length n can be established by showing that the prefix sum is correct for one particular monoid the interval of monoid for one particular input generic prefix sums let us extend our programming language with a fresh generic type sx a new operator x sx sx and a distinguished literal value x of type sx we intend this generic type to represent an arbitrary monoid x sx x with identity x we call a program that makes use of sx a generic program to a generic method in java a template method in c or a function with a type class context in haskell a generic program cannot be directly executed it must first be instantiated with respect to a specific type definition let p be a generic program and m a monoid we write p m to denote the program that is identical to p except that every occurrence of sx x x is replaced by sm m m respectively we refer to the process of obtaining p m from p as a monoid substitution if m already occurs in p then we cannot tell from p m alone which uses of m were already present in p or from uses of x we handle this issue as follows to simplify our formal treatment if m occurs in p then we choose a monoid m isomorphic to m such that m does not occur in p and we define p m to be p m for ease of presentation we still refer to the monoid m as m it is easy to see that monoid substitution is welldefined and that if the abstract program p is welltyped according to the rules of figure then applying a monoid substitution to p leads to a welltyped program definition generic prefix sums let p be a generic program then p implements a generic prefix sum of length n if in and out are of type and for every monoid m p m implements an m prefix sum of length n the interval of monoid the key insight behind our result is the observation that a generic prefix sum can only rely on the properties of a monoid associativity and the existence of an identity element relying on any additional properties specific to a particular operator such as commutativity idempotence or distributivity with respect to some other operator would the prefix sum in general suppose we wish to compute a prefix sum of length n from array in into array out with respect to an arbitrary monoid m thus in and out have type and the prefix sum operator is m if the prefix sum is correctly implemented then at the end of the computation each element of out must be the sum of a sequence of elements of in for example out should be equal to in and out to in m in m in m in as computation these are built up using the m operator initially starting with individual elements of in consider for any k n how could be computed to give the main intuition in a simple manner we ignore possible uses of the identity element either a the existing value of a variable or array element could be copied into or b could be derived by two values of sm c and d say using m in the latter case · c and d are of elements of in c m ji inj and d m inj for some i and i where an empty is defined to be m and · the c and d must we must have i i if c and d did not have these forms then using only the laws of associativity and identity it would not be possible to rewrite c m d into the form inj the required value for by a similar argument if c similarly d is in turn constructed by two values e and f then e and f must both be of elements of in that otherwise it would not be possible using just the monoid laws to e m f into the form of c this argument we can see that a correct generic prefix sum must compute its result by partial sums of input elements that if a sum is ever constructed then this sum cannot contribute to the final prefix sum result we introduce a monoid the interval of monoid which captures abstractly the notion of intervals and the operation of pairs of intervals definition the interval of monoid i referred to as the interval monoid has the elements si i i int × int i i i and a binary operator i defined by i i x x i i x for all x si i x x i for all x si i i i i i i i if i i otherwise it is easily checked that i defines a monoid with identity i the interval monoid abstractly describes of ele ments of in with respect to an arbitrary monoid say m a pair i i abstractly describes the m iii ini the identity i represents an empty interval of thus is an abstraction of m finally the element represents all nonempty of elements in with respect to m that are not known to correspond to the i operator precisely captures the effect of two for i i i m iii ini m m iii ini m iii ini and i i i i i i i the way i treats i expresses the fact that adding an empty to either side of a interval has no effect finally the treat ment of by the operator captures the argument above using only the properties of a monoid it is not possible to transform a non into a one by applying the m operator notice that is an element or el of i once a has become there can be no return it may appear that the interval monoid is similar to the abstract domain of intervals that is commonly used in abstract interpretation the domains are in fact very different an ele ment i j of our interval of domain represents a sin concrete value that can be expressed as a whereas an element i j of the traditional domain of intervals represents the set of values i i j we now define a condition on initial program stores which for a given natural number n ensures that the first n elements of in are abstracted in the interval monoid by appropriate singleton sum mation intervals and that all other array elements and variables of type si have values that are not known to be intervals definition singleton condition for sequential programs let p be a generic program with in of type an initial state of p i with variable store v and array store a satisfies the singleton condition for n if for all v of type sx in p vv for all a of type in p and k int k k if a in and k n otherwise we can now state our main theorem which shows that with respect to generic prefix sums the interval monoid is a sound and complete abstraction a program implements a generic prefix sum if and only if it implements a prefix sum when instantiated with the interval monoid in fact our result is stronger than this it shows that correctness of a generic prefix sum can be established by considering the interval monoid instantiation only for those initial stores that satisfy the singleton condition theorem soundness and completeness let p be a generic program and n a natural number then p i computes an sum of length n for every initial state satisfying the singleton condition for n p implements a generic prefix sum of length n given a generic program p the main proof idea is to simulate the execution of p m for any monoid m by p i and vice versa relating the states encountered in the executions to this end we first define a reification function reification let m be a monoid and let denote the type of array stores define si × i i a m iii i a m a sm thus maps a represented abstractly in the interval monoid to the set containing the corresponding concrete in the monoid m and maps an unknown represented by in the interval monoid to the full set of elements of m let p be a generic program and for a monoid m let m denote the set of all program states of p m we lift to map states of p i to sets of states of p m formally i m is defined by v a ss v a ss if and only if · for all v of type t in p vv vv vv vv a if t sx if t sx · for all a of type in p and k int a if t sx if t sx · there exists a generic program q such that ss qi and ss qm observe that the generic program q in the final condition is unique even if i or m is used in p due to the discussion following definition simulation we can now prove our simulation result lemma let p be a generic program and m a monoid if si is a program state of p i and sm si is a program state of p m then · if si s si there exists a program state sm si of p m such that sm s sm and · if sm s sm there exists a program state si of p i such that si s si and sm si proof let si be a program state of p i and sm si be a program state of p m moreover let q be the generic pro gram such that qi and qm are the program components of si and sm respectively observe that monoid substitution definition satisfies the following algebraic laws for any monoid m v em v em ae em ae em if e ss else if e else while e while e m s sm where it is immediate by the typing rules and the definition of generic programs that neither x nor x can occur in either the expression e of ae e or the expression e of a conditional or loop for the same reason it follows that if the first statement in q is a loop or conditional with guard e then e will evaluate in both si and sm thus whatever the form of q the same rule from the operational semantics applies in both si and sm let the resulting program states after application of the rule be si and sm by the algebraic laws it is now immediate that there exists a unique generic program q such that q i is the sequence of statements of si and q m is the sequence of statements of sm it remains to show that the conditions on the stores of si and sm are satisfied if the applied rule was one of site f this is immediate as the stores are identical before and after the steps for there are two cases to consider if v is not of type sx in q it is immediate by the typing rules and the of q that e evaluates in both si and sm if v is of type sx in q it follows by the typing rules and associativity of x that e is an expression of the form x x · · · x xk where for all xi with i k either a the identity element x b a variable or c an array element for each xi ae the expression e evaluates in both si and sm by the typing rules and of q the result is now immediate by the definition of i and the fact that a for each xi x we have m i a b for each variable v among xi we have by assumption that vv vv ain and c for each ae among xi we have aa e v a aa e v a a in where v a are the stores of si and v a those of sm for observe that e evaluates in both si and sm by the typing rules and of q hence the same array element is updated in each application of the rule there are now two cases to consider ie a is or is not of type in q these cases are identical to those of lemma let p be a generic program m a monoid and n a natural number if sm is an initial state of p m then there exists an initial state si of p i satisfying the singleton condition for n such that sm si and · if si s si there exists a program state sm of p m such that sm s sm and si · if sm s sm there exists a program state si of p i such that si s si and sm si proof let sm be an initial state of p m the lemma is immediate by induction on the number of steps applying lemma once we show that an initial state si of p i exists that satisfies the singleton condition for n such that sm si to this end write sm as v a p m and define si as v a p i with · for all v of type t in p vv v v if t sx if t sx · for all a of type in p and k int k k if a in and k n if a in and k n if a in and t sx otherwise that si satisfies the singleton condition for n and that we have sm si is now immediate proof theorem the direction is trivial as i is a monoid for the direction let m be a monoid and sm an initial state of p m by lemma there exists an initial state si of p i such that si satisfies the singleton condition and sm all executions starting from si are terminating because p i im an sum by lemma for every terminating execution and thus every execution si s si there exists a corresponding terminating execution sm s sm such that sm si moreover these are the only executions of sm if there would exist a nonterminating or another terminating execution starting from sm then the induction argument from lemma would yield an additional execution starting from si because p i computes an sum we have for the array store a of si that i ik k for all k n hence by definition of cation for the array store a of sm we have that m ik for all k n extension to programs we next consider programs or kernels in which threads by means of barriers syntax typing and semantics we extend the language of figure with a barrier synchronisation statement as follows stmt s · · · barrier the typing rule for barrier is straightforward barrier unit the typing rules for expressions and all other statements are as before see figure given a finite set of thread identifiers d int a kernel program state is a tuple a k with a an array store and k a map from d to variable store sequence of such that for each t d and variable store v of kt we have t the array store a represents arrays that are shared among all threads and k represents the local variables and instruction sequence for each thread the variable tid represents the identity of a thread and must occur readonly in every program p our theoretical presentation does not depend on the existence of tid but the prefix sum implementations we evaluate in section rely on each thread having a unique identifier figure gives the operational semantics of our kernel programming language where s is as defined in figure the semantics is a standard interleaving semantics see rule with an additional rule for barrier synchronisation rule rule checks whether all threads are either at a barrier or have terminated the additional condition that at least one thread must be at a barrier ensures that rule and termination are mutually exclusive the semantics assumes that statements are executed atomically so that for example a thread can execute a shared state update such as ai aj in a single step this assumption is not valid in practice such a statement would involve separate and possibly multiple load and store instructions between which other threads could even if we refined the semantics to reflect this we would still need to account for the weak memory models of modern architectures however if a program is free from data races which we define formally below the effects of this assumption are not visible the verification technique for gpu implementations of parallel prefix sums which we present in section depends on proving that a gpu kernel is free from data races our rules for barrier synchronisation follow the programming model in which it is valid for parallel processes to at syntactically distinct barriers in gpu programming models such as and the rules for barrier synchronisation are requiring that all threads at the same barrier and that if a barrier is inside a loop all threads must have executed the same number of loop iterations on reaching the barrier precise semantics for barrier synchronisation in gpu kernels have been formally specified our results for the barrier synchronisation model of makes our technique more widely applicable adding conditions for barrier synchronisation does not affect our theoretical results and the tool used as part of our verification method checks the conditions required in the gpu setting given a kernel program p an initial state of p is any kernel state a k where for every thread t the sequence of statements of kt is p an execution of a program p starts from an initial state maximal executions and termination for an initial state are defined as for sequential programs note that the interleaving nature of the semantics means that there may be multiple maximal executions contrary to the sequential case prefix sums and generic prefix sums computation and implementation of a prefix sum is defined as in the sequential case see definition with p interpreted as a kernel program a generic kernel program and generic prefix sum are also defined as in the sequential case definition interval monoid the interval monoid as is definition we extend the definition of the singleton condition to kernel programs taking into account that there is now a variable store per thread definition singleton condition for kernel programs let p be a generic program with in of type an initial state a k of p i satisfies the singleton condition for n if for all t d and v of type sx in p vv variable store of kt condition of definition is satisfied with v the soundness and completeness our soundness and completeness theorem for sequential programs theorem can now be stated for kernel programs the proof strategy is the same using reification of abstract states to concrete states to set up a simulation using lemmas analogous to lemma and lemma we explain the differences in how the proof is set up the function is adapted to map states of kernel programs p i to sets of states of kernel programs p m the function treats the array store as before but must take into account the fact that each thread now has its own variable store and program thus while reification of array stores still operates at the level of the whole program state reification of the variable store and program component now operates at the level of each individual thread the definition is left unchanged otherwise the proof of lemma translates to the case of kernel programs by observing that barrier t v kt v ss v a ss s v a ss a k k a k ss kt v barrier ss k t v ss kt v k t v a k k a k k kt v ss t v ss kt v barrier ss figure operational semantics of our kernel programming language extending the sequential rules of figure and that by this algebraic law and the algebraic laws from the proof of lemma we have that the same rule will be applied in both the case of si and sm where the proof of lemma is embedded in the treatment of the case in proving lemma for kernel programs we must relate an initial state sm a k of p m to an initial state si of p i that satisfies the singleton condition otherwise the proof is identical to that of lemma the definition of the initial state si is easily adapted from the definition in the proof of lemma by applying the definition for the variable store to the variable store of each individual thread the definition is left unchanged otherwise data race freedom we now define what it means for a kernel program to exhibit a data race we show that if a kernel program is data then due to the properties of barrier synchronisation the program computes a deterministic result the verification technique we present in section depends on this guarantee of determinism and as discussed above establishing of a kernel avoids the need to reason about weak memory semantics or the of memory accesses say that we read from an array element ai in an execution step if is referenced during the evaluation of any of the expressions occurring in the step likewise say that we write to an array element ai in an execution step if is updated in the step an array element ai is accessed if it is either read from or written to in the execution step observe that an array element can be accessed multiple times in a single execution step eg when evaluating m m definition let s k sn be an execution of a kernel program p the execution is said to have a data race if there are steps si k si and sj k sj along the execution such that · distinct threads are responsible for these steps · a common array element is accessed in both execution steps · at least one of the accesses writes to the array element · no application of occurs in between the accesses a program p is data if for every initial state s of p and execution starting from s it holds that the execution does not have a data race theorem let p be a generic kernel program and let m be a monoid then p m is data p m is data for all monoids m proof the direction is trivial as m is a monoid for the direction observe that neither the controlflow nor the array accesses can be by the choice of m by the typing rules and of p we now argue that data kernel programs behave say that a barrier synchronisation occurs during execution of a program whenever the rule of figure applies suppose that a kernel program is and consider executing the program from an initial state s means that the execution of one thread cannot depend upon the actions of another until all threads in a barrier synchronisation thus while the threads may their individual execution is deterministic and when the first barrier synchronisation occurs assuming all threads reach a barrier without the program state s is deterministic by a similar argument individual thread execution until the next barrier synchronisation is deterministic leading to a deterministic program state s when the second barrier synchronisation occurs applying this argument and using induction on program executions we can prove the following lemma let p be a data kernel program and let s be an initial state of p if there exists a finite maximal execution starting from s with final array store a then all executions starting from s are finite and for all maximal executions the final array store is a an automated verification technique our theoretical results show that to verify functional correctness of a generic parallel prefix sum of length n implemented as a program it suffices to prove that the program is free from data races and then test that the program behaves correctly with respect to the interval monoid for every initial state with in n n in practice a library routine for computing a prefix sum takes no input except for in so that there is just a single input to test gpu kernels are barrier programs that are required to be data the semantics of an or kernel is undefined if the kernel data races thus our results suggest the following method for verifying functional correctness of gpu kernels that claim to implement generic prefix sums with respect to a single input array prove that the gpu kernel is data run the interval monoid test to check functional correctness steps and can be independently but the functional correctness guarantee provided by step is conditional on the result of step we now discuss how we have implemented a practical verification method for generic prefix sums written in based on this approach representing a generic prefix sum the programming model does not support generic functions directly however it is easy to describe a generic prefix sum by writing a kernel that uses a symbol type and a macro operator as for the concrete type and operator with respect to which the prefix sum should be executed a concrete choice of type and operator can be chosen by including a header file that specifies type and operator race analysis step above can be to any sound verifier for gpu kernels capable of proving for gpu kernels this includes the and tools which rely on smt solvers and the tool described in which performs verification using a combination of dynamic and static analysis in our experiments section we use the tool because it is maintained and available and is the only tool supporting the programming model which allows us to evaluate step above on a number of platforms by theorem we can prove of a generic prefix sum using any choice of type and operator in our experiments we use the interval monoid running the interval monoid test case step above requires an encoding of the interval monoid observe that to check a prefix sum of length n we need only encode elements i j of the interval monoid for i j n as well as the i and elements an element can thus be encoded using bits meaning that on space is required for the test input and result with this encoding the interval monoid test case can be executed as a regular kernel soundness completeness and automation our verification strategy is sound due to the guarantees provided by the version of theorem for kernel programs together with theorem and lemma and the requirement that a sound method for verifying is used as with any dynamic technique soundness of the testing phase of our approach depends on the of the compiler driver and hardware implementation of the architecture on which the test is executed in our experiments we guard against this source of by testing with respect to multiple platforms our verification strategy is only complete if the technique used to prove race freedom is complete the method which we employ is not complete the technique uses both thread abstraction and loop invariant abstraction which can lead to false positive data race reports a small amount of manual effort is required in order to prove of prefix sum kernels tries to perform automatic inference of invariants required to prove but often this inference is not strong enough and manual invariants must be supplied by the programmer however these invariants are checked by are not taken on trust by the tool in the examples we consider in section we automatic invariant inference and supplied a small number of relatively simple loop invariants by hand the dynamic analysis phase of our verification method is fully automatic handling extended programming language features our sequential language and its extension omit realworld language features such as procedures control flow and pointers with associated aliasing issues we are that our results apply directly in this extended setting under the condition that data of generic type sx are never accessed via pointers with different element types this is important for the testing part of our approach without this restriction it would be possible to pass our test case by the out array to a char pointer and writing the desired final result for the interval monoid proving in the presence of pointer manipulation can be challenging but is possible with the right invariants and is supported by because operates at the level of control flow graphs using the techniques of control flow is also supported nevertheless the prefix sum implementations we have seen in practice do not manipulate pointers in complex ways call auxiliary procedures or exhibit control flow as discussed in section the requirement that the input array in be readonly is for ease of presentation only and can be in practice allowing our technique to be used in verifying prefix sums that operate in place algorithm sequential depth n lg n lg n lg n lg n size n n lg n n n lg n n lg n n n table characteristics of the prefix sum algorithms considered in this paper where n is the number of elements experimental evaluation we demonstrate the effectiveness of our method for prefix sum verification in practice by applying it to four different algorithms implemented as kernels all required to our experimental results including implementations all kernels are available online details of prefix sum kernels we evaluate our technique using four different wellknown prefix sum algorithms and the algorithm is an exclusive prefix sum figure provides a circuit diagram description for each algorithm there is a for each input and data flows topdown through the circuit each node · performs the binary associative operator on its two inputs and produces an output that passes downward and also across the circuit through a diagonal table gives a comparison of these algorithms as well as the straightforward sequential implementation in terms of circuit characteristics depth the number of levels size the number of nodes and the maximum number of per node as the size of the input or width n of the circuit the depth of a circuit indicates how long it takes for the result to propagate through the circuit and thus the time taken for prefix sum computation size indicates how many would be required to build the circuit in hardware while indicates the required of the output gate a software implementation of a prefix sum is work efficient if the number of operations required is within the same order of magnitude as the sequential version the size column of figure indicates that and are while and are not a more detailed discussion of these characteristics can be found in through a survey of several gpu code the app the and the and benchmarks we found to be the most widely used gpu implementation of a prefix sum in practice to be used where an exclusive prefix sum is required and employed several times in one large kernel that computes of a matrix is the prefix sum algorithm we consider we did not see it used in practical gpu kernels experimental as discussed in section we use the tool to prove that each kernel is free from data races these experiments were performed on a linux machine with a b core processor using a version of from the tool web page on june we then verify functional correctness for each kernel by running the interval of test case we give results for five different platforms two ­ a a b c d figure circuit representations of the prefix sum algorithms for n elements and a m two intel ­ a core e and a core x and one arm gpu ­ a core t these devices exhibit a range of power and performance characteristics the offer a large number of parallel processing elements running at a relatively low the intel run at a higher but exhibit less parallelism and the arm gpu is designed for high we chose to run on multiple platforms to guard against possible unsound results as a result of a particular compiler driver or hardware configuration each experiment was run with a timeout of all the timing results we present are over three runs in shows for sequential programs that a generic prefix sum is correct for all input lengths if it can be shown to behave correctly for all input lengths with respect to two binary operators over a set of three elements he shows further that it is sufficient to consider on test inputs nn tests using the first operator and n tests using the second operator the result of considers all possible n but also restricts so that a prefix sum of a specific length n can be shown correct by testing the corresponding set of inputs our lemma allows us to lift this method to programs thus by way of comparison we also tried dynamically verifying the prefix sum kernels by running for each element size the set of tests we discuss paper further in the related work section verification using the interval of test case and the tests both require to be established thus the overhead of race checking applies to both approaches results for proving table presents verification times in seconds for proving of each of our prefix sum kernels using for a selection of element counts up to times for the thread counts not shown are in the same order of magnitude the results show that verification time is independent of the number of threads executing the kernel this is because checks for a kernel with respect to an arbitrary pair of threads and reasons about loops using invariants rather than by unrolling this scalability demonstrates that the data race analysis aspect of our verification method is practical for very large arrays as noted in section we automatic invariant inference in as it did not allow fully automatic verification of our examples instead we provided relatively simple loop invariants for each of our kernels in order for verification to succeed for example for the loop of the kernel shown in figure we provided the following invariant no further invariants were necessary no no which specifies that no access to the out array is at the head of the loop the most invariant we had to specify for the kernel was d d offset d n d offset n here d and offset are loop counters with d increasing in from to n and offset decreasing in from n to and finally reaching zero the conjunct d d uses the operator to express that d must be a or zero counting each toplevel conjunct of a loop invariant as a separate invariant the number of invariants required for verification of each kernel was as follows the invariants did not have to be for individual thread counts results for dynamic analysis the i rows of table present for each prefix sum and platform the time in seconds taken to run the single interval of test for array sizes from to and to results in between follow a similar for each array size we also show in the tests row the total number of tests that would need to be run to verify the kernel using method and the v rows show how long these tests took to run the times are taking into account device creation of memory buffers compilation of the kernel copying data into memory buffers running the test and the result for the tests we used a wrapper so that device creation of memory buffers and kernel compilation is performed only once for each array size we note that while in theory an array of length n can be processed by n concurrent threads or n threads in the case of it is up to the runtime to decide how to schedule threads across hardware resources and that in practice a large number of threads will be in a series of from the i rows it can be seen that running the interval of test case is very fast on all platforms even for arrays of length it may seem surprising that the runtime does not increase significantly as the array size increases this is because a computing with the interval monoid is very requiring only integer equality testing and addition by a constant and b the of online compilation of the kernel which dominates the runtime of these simple tests the v rows show that running a quadratic number of tests per array size does not scale well on all platforms we found that sizes of n our time limit of showing that the method will not scale to large arrays related work relationship to results by and the work to this paper is work by which proves two interesting results for sequential generic prefix sums in the model a kernel is compiled at runtime using an online compiler so that it need not be for any given device n table time in seconds taken to prove race freedom for prefix sum kernels for increasing array lengths an array of length n is processed by n threads except in the case of where n threads are used n tests × × × × × × × v i v i v i v i v i v i v i v i v i v i v i v i v i v i v i v i v i v i v i v i m intel x intel e arm t table time in seconds taken to establish correctness of prefix sum implementations for increasing array lengths on gpu intel cpu and arm gpu architectures the number of test cases associated with method for each array length is also shown first shows using relational parametricity that a generic prefix sum is correct if and only if it behaves correctly for each length n with respect to integer lists and list concatenation when applied to the input n that is it yields the output n states that this result was inspired by earlier unpublished work by this claim when exploiting the result in later work on the design of prefix sum algorithms we refer to this as the result the result also holds for fixed lengths n which means the result is similar to our result for sequential prefix sums with respect to the interval monoid however the result cannot be used for practical verification of prefix sums for large n because on space is required to represent the result our interval of monoid avoids this problem by exploiting the fact that as argued in section a correct generic prefix sum should never compute a the interval of abstraction thus allows a to be represented precisely and as an interval i j or as i in the case of an empty and all other into the element letting l denote the monoid of integer lists under list concatenation employed by the method there is an obvious homomorphism l i defined by i x xm x xm if m if m and xi xi i m otherwise this homomorphism exactly those that cannot contribute to a correct generic prefix sum secondly presents an elegant proof of the principle for parallel prefix computation in the spirit of the principle of knuth the principle states that a prefix sum algorithm is correct on all sequences with every associative operator defined over the set if and only if it is correct on any input sequence with an associative operator in fact carefully tracing his proof that it suffices to consider just two operators and for every length n a particular input set of sequences of size on and on for his first and second operator respectively as with the result this result also holds for fixed lengths n does not present an experimental evaluation of his method in we believe that our work is the first to do so as we demonstrated in the experiments of section verifying a prefix sum by running set of tests does not scale to large array sizes the focus of is on sequential haskell programs describing parallel synchronous hardware in contrast we have presented a method for verifying prefix sums implemented as asynchronous programs leading to a practical method for verifying gpu kernel implementations due to our imperative setting we present proofs using a direct simulation argument we are not aware of any work on using relational parametricity to reason about programs and believe this would be challenging prefix sums an alternative approach by is to construct prefix sums that are correct by derivation in work rather than verifying candidate implementations for functional correctness new prefix sum circuits are built from a set of basic primitives and combinators the correctness of these circuits is by a set of algebraic laws derived for these combinators similar to the work of prefix sums are given as haskell programs describing circuit layouts we are not aware of any work that translates this approach to a setting formal verification of gpu kernels recently a number of techniques for formal or analysis of gpu kernels have been proposed ­ and formal semantics for gpu kernels have been studied the and methods aim to statically prove while the the space complexity is on for correct algorithms an incorrect algorithm could apply the concatenation operator arbitrarily many times requiring unbounded space if an sorting algorithm is correct on all boolean valued input sequences then it is correct on input sequences over any totally ordered set method described in uses a combination of dynamic and static analysis for verification we employed to prove of prefix sum kernels in our experimental evaluation section the and methods employ dynamic symbolic execution based on the tool to find data races and array bounds errors in and kernels respectively a recent technique for functional verification of kernels using separation logic could in principle be applied to generic prefix sum kernels however tool support for automation of this method is not yet available in prior work we introduced barrier invariants to enable reasoning about gpu kernels and used barrier invariants to statically prove functional properties of and prefix sum kernels the challenges in making this approach scale provided insights which ultimately led to the interval of abstraction and we used an early formulation of the abstraction to verify the kernel in the method for prefix sum verification presented here significantly the technique of but the concept of barrier invariants has applicability conclusions we have introduced the interval of abstraction investigated theoretical guarantees provided by this abstraction for verifying prefix sum algorithms and shown that it can be used to design a practical and scalable verification method for gpu kernel implementations of prefix sums our experimental results demonstrate that we can prove of prefix sum implementations for all useful array sizes after which checking functional correctness of a prefix sum down to running a single test case taking no longer than the time required to run the prefix sum in practice we believe this our claim that we have made a major step towards solving the problem of functional verification for gpu implementations of generic prefix sums our approach depends on the ability to prove that a program behaves for models of computation where barriers provide the only means of synchronisation this down to proving lemma our approach thus extends to prefix sums written in programming models such as and as long as these implementations use barriers exclusively for synchronisation in of locks or atomic operations our method cannot be applied to gpu kernel implementations where threads communicate using atomics in practice we have not seen the use of atomics in prefix sum kernels we have considered verification of prefix sums with respect to fixed array sizes to prove that a prefix sum is correct for all array sizes it would be necessary to prove for arbitrary array sizes and also to prove that the prefix sum would compute the correct result for our interval of test case for every array size while the former is beyond the scope of current tools such as and the latter obviously cannot be achieved through testing one could attempt to establish both through manual or partially automated mathematical proof we designed the interval of abstraction to capture the specific case of prefix sums due to the importance of this class of algorithms the abstraction can be used to any algorithm that operates over an abstract monoid as long as the results computed by the algorithm are required to be we are aware of one further class of such algorithms reduction operations the reduction of an array s s sn is the sum s s · · · sn indeed a reduction is performed during the first phase of the and prefix sum algorithms a natural question is to ask if there are prefix sum algorithms that gain efficiency by exploiting other properties of operators such as commutativity or idempotence we are aware of recent work by which prefix sum circuits for the operator exploiting the fact that this operator satisfies the identity x y y x beyond prefix sums there is scope for abstractions to allow reasoning via a canonical test case in less restricted settings for example potentially of monoid elements can be represented using lists and commutativity can be by switching to a bag representation however these representations lose the by the interval of abstraction which allows our approach to scale acknowledgments we are to berdine alan john and our popl reviewers for their comments on various of this work and to for helpful discussion we thank daniel college arm and for their with our experimental evaluation finally we thank and for their work which inspired our efforts references a n a f s qadeer and p a verifier for gpu kernels in oopsla pages ­ m o and u efficient stream compaction on wide architectures in pages ­ g e as primitive parallel operations ieee trans comput ­ g e prefix sums and their applications in j h reif editor synthesis of parallel algorithms morgan r p and ht a regular layout for parallel ieee trans computers ­ c d and d r and automatic generation of tests for complex systems programs in pages ­ s et al a benchmark suite for heterogeneous computing in characterization pages ­ n a f p h j and s qadeer barrier invariants a shared state abstraction for the analysis of gpu kernels in oopsla pages ­ p c and p h j symbolic testing of code in pages ­ p a f j and s qadeer interleaving and semantics for analysis and verification of gpu kernels in esop pages ­ p cousot and r cousot abstract interpretation a unified lattice model for static analysis of programs by construction or approximation of fixpoints in popl pages ­ a et al the scalable heterogeneous computing benchmark suite in pages ­ o e and c a parallel method for fast and practical interpolation bit numerical mathematics ­ w e and a using portable parallel programming with the message passing interface mit press nd edition a and a on the correctness of the execution model of in esop pages ­ m s and j parallel prefix sum scan with in h editor gpu addisonwesley r an algebra of in pages ­ m and m specification and verification of programs using separation logic in bytecode working group the specification version d e knuth the art of computer programming volume addisonwesley nd edition p m and h s a parallel algorithm for the efficient solution of a general class of recurrence equations ieee trans computers r e and m j fischer parallel prefix computation j acm ­ a m gupta y et al verifying gpu kernels by test in pldi pages ­ g li and g scalable verification of gpu kernel functions in pages ­ g li p li g g i and s p verification and test generation for in pages ­ d and a parallel scan for stream architectures technical report cx department of computer science university of c programming guide version b c pierce types and programming languages mit press p and j l parallel prefix scan algorithms for in pages ­ n m and m designing efficient sorting algorithms for in pages ­ s m y and j d scan primitives for gpu computing in gh pages ­ i on the complexity of parallel prefix circuits technical report tr electronic colloquium on computational complexity m functional and dynamic programming in the design of parallel prefix networks j of program ­ j addition logic trans electronic computers b so a m and y wu optimizing data parallel operations on platforms in in first workshop on software tools for systems h s parallel processing with the perfect ieee trans comput ­ j et al a revised benchmark suite for scientific and commercial computing technical report impact university of illinois at j much about two a on parallel prefix computation in popl pages ­ 