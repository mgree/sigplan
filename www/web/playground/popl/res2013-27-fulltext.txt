quantitative relaxation of concurrent data structures thomas a m ali university of abstract there is a tradeoff between performance and correctness in implementing concurrent data structures better performance may be achieved at the of correctness by the semantics of data structures we address such a of data structure semantics and present a systematic and formal framework for obtaining new data structures by existing ones we view a data structure as a sequential specification containing all legal sequences over an alphabet of method calls the data structure corresponds to defining a distance from any sequence over the alphabet to the sequential specification the sequential specification contains all sequences over the alphabet within distance k from the original specification in contrast to other existing work our are semantic distance in terms of data structure states as an instantiation of our framework we present two simple yet generic relaxation schemes called and stuttering relaxation along with several ways of computing we show that the relaxation when further instantiated to stacks queues and priority queues amounts to bounded behavior which cannot be captured by a purely syntactic relaxation distance in terms of sequence manipulation eg edit distance we give concurrent implementations of relaxed data structures and demonstrate that bounded provide the means for correctness for performance in a controlled way the are monotonic which further the tradeoff increasing k increases the number of permitted sequences which as we demonstrate can lead to better performance finally since a relaxed stack or queue also implements a pool we obtain new concurrent pool implementations that the ones categories and subject descriptors d programming languages formal definitions and e data structures lists stacks and queues d programming languages programming general terms theory algorithms design performance keywords concurrent data structures relaxed semantics quantitative models costs permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ italy copyright c acm introduction concurrent data structures may be a performance and scalability and thus prevent effective use of parallel hardware there is a tradeoff between scalability performance and correctness in implementing concurrent data structures a to the scalability problem is to relax the semantics of concurrent data structures the semantics is given by some notion of equivalence with sequential behavior the equivalence is determined by a consistency condition most commonly linearizability and the sequential behavior is inherited from the sequential version of the data structure eg the sequential behavior of a concurrent stack is a regular stack therefore the semantics of a concurrent data structure amounts to either weakening the consistency condition linearizability being replaced with sequential consistency or consistency or its sequential specification in this paper we present a framework for sequential specifications in a quantitative manner for an example of a relaxation imagine a in which each pop removes one of the most recent k elements and an operation size which returns a value that is at most k away from the correct size it is intuitively clear that such a a regular stack but current theory does not provide means to quantify the relaxation our framework does it provides a way to formally describe and such we view a data structure as a sequential specification s consisting of all semantically correct sequences of method calls we identify the sequential specification with a particular labeled transition system lts whose states are sets of sequences in s with future behavior and transitions are labeled by method calls a sequence is in the sequential specification if and only if it is a finite trace of this lts our framework for quantitative relaxation of concurrent data structures amounts to specifying costs of transitions and paths in the lts only correct transitions are allowed eg a transition labeled by is only possible in a state of a stack with a as top element in a relaxation we are exactly interested in allowing the wrong transitions but they will have to cost our framework makes this possible in a controlled quantitative way the framework is instantiated through specifying two cost functions a local function transition cost that assigns a to each wrong transition and a global function path cost that the local costs using eg maximum sum or average to obtain the overall distance of a sequence via this we are able to achieve a separation of concerns modularity and flexibility different transition costs can be used with the same path cost or vice versa leading to different once the distance of a sequence from the original sequential specification s is defined in this way a of the data structure becomes the set of all sequences within distance k from s returning to the stack example above we can set the transition cost of a pop transition at a state to be the number of elements that are between the element and the top of the stack we can define the path cost to be the maximum transition cost that occurs along a sequence then the corresponding precisely captures what we intuitively described we instantiate the framework on two levels on the abstract level we present two generic called and stuttering relaxation which provide a way to assign transition costs together with several different path cost functions for any data structure on the concrete level we instantiate the relaxation to stacks queues and priority queues we out the effects of the relaxation in these concrete cases and prove that they indeed correspond to the intuitive idea of bounded relaxed behavior we also instantiate the stuttering relaxation to a cas object and a shared counter and prove correspondence results as well we show that the relaxation framework is indeed of practical value we give an efficient new implementation of an stack and fit an existing efficient implementation of an queue in our framework as well the experimental results demonstrate ideal behavior linear scalability and performance in particular the relaxed stack implementation we present and stack queue and pool algorithms on various we also present implementations for a stuttering cas a stuttering shared counter using this stuttering cas and a different stuttering shared counter all of which further demonstrate increased scalability and performance the main contributions of this paper are the framework for quantitative relaxation of data structures and efficient concurrent implementations the way to the framework is by formally capturing the semantics of a data structure other contributions made possible by the framework are the generic and stuttering of data structures of the relaxation in concrete terms for stacks queues and priority queues characterization of the stuttering relaxation in concrete terms for cas and shared counters the structure of the paper is as follows in the remainder of this section we provide motivation for the main features of our work we present the formal view on data structures in section followed by the framework for quantitative relaxation in section throughout the formal part we use a stack as running example we present the two generic instances and stuttering in section and instantiate them further to concrete data structures in section and section respectively we discuss related work in section in section we present implementation details and in section experimental results our original scalability and performance goal we wrap up with remarks in section in the related work survey we put special on the only other work we are aware of that also the problem of sequential data structures for better performance in the concurrent setting as opposed to our semantic statebased approach in assigning to sequences the relaxation of is syntactic we argue that the semantic approach is more expressive than the syntactic one and it allows the designer of a data structure to formally capture the of a specific relaxation more easily and naturally relaxation improves performance a relaxation of the sequential specification of a data structure can lead to a distribution of contention points the need for and thus the cost of synchronization for instance instead of requiring that each pop operation updates the top pointer of a concurrent stack allowing a relaxation which sets the size of the window from which a removal is acceptable to some k most recent elements effectively reduces contention for the top pointer in section we show that even such a simple relaxation for stacks with k on a core per core server machine can lead to an increase in performance compared to the existing implementations of strict stacks note that a larger sequential specification increases the potential for better performance since our are monotonic increasing k increases the performance potential however the extent to which this potential can be in practice depends on many factors among which is the choice of hardware generality consider three different sequences belonging to three different data structures stack queue and priority queue respectively where for the priority queue b has top priority followed by a and c that have the same and d has low priority if these sequences are extended with a removal operation pop rem respectively the expected return values are d element at the top of the stack a element at the head of the queue and b element with the priority imagine instead that the removal operation returns c for all of these three sequences at first that c is returned seems to be arbitrary however a careful reveals a common pattern in each sequence c is not the current but next possible value to be removed that is in the stack it is the element immediately below the top element in the queue it is the element immediately after the head element in the priority queue it is an element with the second priority it then seems natural to view all these as an instantiation of a common relaxation scheme our framework allows one to precisely express this and other common types of for instance our generic relaxation provides exactly this for data structures in which information is according to some order temporal in the case of queue and stack logical in the case of a priority queue the generic relaxation removes the need of each data structure separately modularity let us now consider the situation immediately following the removal of c from the stack as was depicted above we have the following sequence the stack now contains the elements a b and d the last of which is on top one might a particular relaxation where two consecutive are not allowed and hence the next removal has to return d yet another might find it acceptable that at all times one of the top two elements are removed it does not matter how long the top element remains on the stack our framework allows one to express both each transition a transition cost observe that in both the same cost removal cost is assigned to each transition each sequence of transition costs a path cost and this is what distinguishes the two the first will require that there are no two consecutive transitions with nonzero cost the second will require that the maximum of any transition cost is at most we thus obtain a modular framework in which existing can be by modifying transition costs path costs or both the code given in figure a represents a strict shared counter the shared variable c is a counter and each thread tries to increment the value of the counter representative of many concurrent implementations this code leads to scalability as all threads trying to increment the counter will for access to c while true x c i f return x a single counter while true xv ft i f return x b distributed counter figure shared counters next consider a modified version of this shared counter given in figure b unlike the strict implementation here we use an array c of k counters and the logical value of the shared counter is taken to be the maximum value among all the counters in c each thread t can write only to the slot with index ft each attempt of t the counter starts by reading the value contained in ft stored in v and the maximum value of all the counters in c stored in x then it tries to update its counter to x provided that ft is not updated by a concurrent thread the behavior of this code depends on the value chosen for the size k of the array for instance if k then this implementation will be equivalent to the single counter code for other values of k it is evident that there will be a between the behaviors of the two codes we go beyond this notion existence vs absence of relaxation and provide a measure for any relaxation defined in our framework the distributed counter given in figure b is in fact a relaxation of the shared counter of figure a this way an application using a relaxed data structure can evaluate the gain in performance for vs the effort of modifying an application that uses it and try to optimize k for instance if the relaxed shared counter is used as a performance counter counting the occurrence of a given event eg context switches in a multiprocessor scheduler not all occurrences of events will be knowing that the number of event occurrences within one counter increment can not k the size of c is a crucial information for the application designer now consider the code given in figure this code is very similar to the strict shared counter code of figure a except for the call to the method instead of cas the is a relaxed version of cas such that up to at most k many concurrent threads trying to update the value of the cas object can complete with false positive see section and section for details although the and the dis counter of figure b take different approaches in the strict semantics of a counter they both implement a k x c if return x stuttering relaxation this illustrates another use of our framework it can figure counter be used as a simpler way to establish abstract equivalence thus providing for a higherlevel application if one shows that two implementations implement the same relaxation then a client application using either implementation will observe the same behavior regardless of the differences in actual tation details data structures specifications states let be a set of methods including input and output values we will refer to as the sequential alphabet a sequential history s is an element of ° ie a sequence over as usual by we denote the empty sequence in ° a data structure is a sequential specification s which is a set of sequential histories s ° example the set of methods of a stack with data in a set d is s d p du y d p d y the sequential specification ss consists of all sequences ie sequences in which each pop the top of the stack and each push pushes an element at the top for instance the sequence ss is in the sequential specification ss whereas the sequence ts is not the following definition is the core of our way of capturing semantics let s be a sequential specification definition two sequential histories s t p s are written s s t if for any sequence u p ° su p s if and only if tu p s it is clear that s is an equivalence relation by we denote the class of s intuitively two sequences in the sequential specification are if they lead to the same state the following simple property follows directly from the definition of lemma if s s t and su p s then su s tu the intuition about states is made explicit in the next definition in addition we point out particular minimal of a state definition a state of a data structure with sequential specification s is an equivalence class with respect to s for a state q the kernel of q is the set t t p t has minimal length u a sequence s p s is a kernel sequence if s p example one can easily show that kernel sequences of a stack are all sequences in d p moreover for any a stack implies there is a unique sequence that different sequences in in ie s p d p represent different states having identified states a data structure corresponds to a labeled transition system lts that we define next definition let s be a sequential specification of a data structure its corresponding lts is pq Ñ qq with · set of states q s s t s p su · set of labels · transition relation Ñ q q given by if and only if sm p s and · initial state q note that the transition relation is well defined independent of the choice of a well defined since representative due s is prefix closed to lemma we write q also q if there is is an transition from q to some state q if there is no transition from q we also write q if there is a u labeled path of transitions starting from q and q if it is not the case that q the following immediate observation provides the exact correspondence between the sequential specification of a data structure and its lts s is the set of finite traces of the initial state of lt lemma let s be a sequential specification with pq Ñ q qq then for any u p ° we have u p s if and only if example since different sequences represent different states cf example the transitions of lt are fully described by rs ¨ if s s ¨ and if s where s is a kernel sequence in d p note that if s s ¨ then rs ¨ framework for quantitative we are now ready to present the framework for relax ing data structures let s ° be a data structure with socalled providing the bound giving a relaxation for a data structure s amounts to the follow ing three steps completion from pq Ñ qq we construct the completed labeled transition system pq q q qq with transitions from any state to any other state by any method transition costs from a quantitative labeled transition system pq q q qc is constructed here c is a cost domain hence it has a minimum that we denote by and cost q q Ñ c is the transition cost function satisfying m qq if and only if q q in we write q q for the quantitative transition with m qq k a quantitative path of is a sequence q q q qn qn the sequence pm p p is the quantitative trace of notation and the sequence u m mn is the trace of the quantitative path and of the quantitative trace notation u by we denote the set of all quantitative traces of quantitative paths starting in the initial state with trace u and by the set of all quantitative traces of quantitative paths starting in the initial state path cost function we choose a monotone path cost function Ñ c monotonicity here is with respect to prefix order if a quantitative trace is a prefix of a quantitative trace then having performed these three steps we can define the specification definition the specification sk for k p c contains all sequences that have a distance at most k from s sk tu p ° where is the distance of u to the sequential specification s given by p remark both the distance ds and the relaxed specification sk are actually parametric in the transition cost function as well as in the path cost function for simplicity we prefer a light overloaded notation that does not explicitly mention these parameters also for some applications one may wish for two different cost domains one for the transition one for the path cost of which only the second one needs to be well ordered again for simplicity we restrict the presentation to a single cost domain some obvious properties of the quantified resulting from our framework are · s s by the condition on the transition cost function · every relaxation sk is prefix closed by the monotonic ity of the path cost function · the are monotone ie if k m then sk sm to conclude in order to relax a data structure all that one needs is a cost domain c a transition cost for each transition in the completed lts item above and a path cost function item above remark the current framework does not allow for relax that leave the original state space of lt an example of such is a relaxation of eg stack where sequences with preceding need to be assigned finite distance this can be done by slightly changing the definition of lt instead of keeping the original ° where states s s one can take as set of states is an equivalence that coincides with the s when restricted to s for simplicity and since we do not use such in this paper our current definition of lt keeps the states unchanged generic in this section we illustrate the relaxation framework on two generic examples the value and generality of these particular examples become evident in section and section when we instantiate them to concrete data structures let s ° be a data structure with pq Ñ qq we first fix the cost domain to c n y tu relaxation for the generic relaxation we define a transition cost function q q Ñ c called segment cost and mention two other related transition cost functions definition let t pq m qq be a transition in let v be a sequence with minimal length satisfying one of the following two conditions there exist sequences u w such that p and uw is a kernel sequence and either i and q or ii and q there exist sequences u w such that uw p and is a kernel sequence and either i and q or ii and q then the segment cost is given by the length of v v if such a sequence v does not exist for t then intuitively segment cost of a relaxed transition is the length of the shortest v whose removal or insertion into the kernel sequence enables a transition observe that the transition can be taken in if and only if its segment cost is obtained by setting v we will see in the next section that this cost updates or observations such as returning an element other than the top element in a stack or removing an element other than the head of a queue we note that segment cost just as any transition cost can also be used per method ie some methods may be relaxed some not stuttering relaxation for the stuttering generic relaxation we define the socalled stuttering cost definition let t pq m qq be a transition in then the stuttering cost is defined as if q q m qq if q q q q q otherwise where Ñ is the transition relation of lt intuitively the stuttering relaxation allows for already enabled transitions to have no effect on the state if in the specification s q goes to q with method m and q q then the stuttering cost of applying m at q and at q after the transition is all other transitions which are not part of the original specification are set to have infinite cost an example of an unbounded except in the maximal size of the queue stuttering relaxation is presented in where are allowed to work on the same task by a relaxed queue semantics and an element in the queue can be a number of times up to the maximal size of the queue in of bounded stuttering we note that typically implementations can benefit from rather than mutator method calls when there is too much contention and have the client handle false positives path cost functions let s ° be a data structure and pm a quantitative trace in we define the following generic path cost functions to be used with any transition cost · the maximal cost maximal transition tu maps to the i nu · the interval cost Ñ for a binary predicate first order formula with two free variables making statements about positions in the quantitative trace maps to the length of a maximal consecutive quantitative that satisfies hence we have j ´ i pi and i j nu · the interval restricted maximal cost Ñ n y tu for as in the interval cost is given by j pi and i j nu where li j pr ´ i q i r we instantiate the relaxation along with the maximal interval and interval restricted maximal cost on stacks queues and priority queues in section the interval restricted maximal cost is more complex and less intuitive than the other path cost functions but when instantiated it provides valuable relaxation examples that are efficiently implementable in section we apply the stuttering relaxation along with the interval cost on a cas object and on a shared counter note that the other two cost functions do not make much sense together with the stuttering cost the maximal cost is and the interval restricted maximal cost amounts to the interval cost plus one figure the ranges of elements which may be returned by a pop operation of a with restricted and relaxation with k the element a is already removed stacks queues and priority queues in this section we apply the relaxation of section to stacks fifo queues and priority queues due to lack of space here we leave out some common methods eg top for stack head for queue size for all inclusion of these methods does not change the results in particular propositions presented in this section stack we have already given the set of methods of a stack its states and its lts in example example and example let us recall that the sequential specification ss consists of all sequences ie sequences in which each pop the top of the stack and each push pushes an element at the top let s be a kernel sequence a kernel sequence s is · from s if s u ¨ ¨ v where s uv v is minimal and v k · from s if s uv where s u ¨ ¨ v v is minimal and v k · from s if s s and s k by all cases we can show the following proposition proposition let s and s be two kernel sequences of a stack rs only if sis in the relaxation from s with segment as mentioned in section the can be applied we implemented stacks with only push and pop methods of which only pop is relaxed according to the segment interpretation of the path cost functions from section and the corresponding are as follows · the maximal cost represents the maximal distance from the top of a element leading to an hence in an each pop an element that is at most k away from the top let pi be the following first order formula with free variables i and j r p ri js kr · the interval cost represents ie the maximal number of consecutive needed to pop the top leading to a hence in a at most the kth consecutive pop the top · the interval restricted maximal cost represents the maximal size of a window starting from the top from which elements can be leading to a restricted in a restricted each pop removes an element at most k ´ l away from the top where l is the current of the top figure presents a of a relaxed stack in each of the three it shows a state of a stack in which the element a marked in grey has been removed after the last removal of the top or the last push had the ranges show which elements may be returned by a pop operation applied to this state in each relaxed version for k fifo queue we now briefly describe the relaxation of a set of methods for a fifo queue with data set d is q d p du y d p d y the sequential specification sq consists of all se ie sequences in which each the head of the queue and each enq at the tail of the queue for instance the following sequence sq is in the sequential specification sq whereas the sequence tq is not one can easily show that kernel sequences of a fifo queue are all sequences in d p moreover also here for any state in there is different a unique in s p d p represent different states as a consequence the transition relation of can be described in a concise way let s be a kernel sequence of a queue we have rs ¨ if s ¨ s and if s in a similar way as for stack we can define when a queue kernel sequence is from another kernel sequence for m being a queue method furthermore the analogue of proposition obtained by replacing stack by fifo queue holds for queues as well which we state below proposition let s and s be two kernel sequences of a queue then segment cost with the maximal path cost function leads to analogous for and restricted we need to employ slightly different path cost functions priority queue the data set of a priority queue needs to be since data items carry priority as well we take the data set to be n the smaller the number the higher the priority the set of methods is p n p nu y n p n y the sequential specification sp consists of all sequences ie sequences in which each rem removes an element with available priority kernel sequences of a priority queue are all sequences in n p unlike for stack and queue there may be more than one sequence representing a state of a priority queue for a state q if s p then also any permutation of s is in nevertheless the order provides a canonical representative of a state the unique kernel sequence ordered in priority let s be a canonical kernel sequence the transitions of lt are fully described by rs ¨ if s ¨ s and the canonical representative is a matter of choice equally justified is using the unique kernel sequence ordered in priority in which case the transitions of a priority queue more the transitions of a stack the duality between fifo queues and stacks if s again we define when a canonical kernel sequence is from another canonical kernel sequence where m is a priority queue method we have the following result proposition let s and s be two kernel sequences of a with analogous are again possible only the path cost functions are more complex since they need to capture when an element with higher priority than all existing elements in the priority queue is inserted stuttering relaxed cas and shared counter in this section we instantiate the relaxation from section to two concrete examples cas the set of methods for a cas object with a data set d and an initial data value init p d can for our purposes be modeled as cas d d d p d b p tt the sequential specification is defined inductively as follows the empty sequence is in and any sequence of length one and shape d tq is the maximal prefix of s such in that s for d p u d if s p for m then s ¨ m p if either m d tq or m d fq and d d let that s s p and let ¨ u for as before t m be d the tq maximal prefix of s such then it is not difficult to show that s d tq hence there is a unique kernel sequence in each equivalence class and it has length one the transitions of lt are given by d d d and d d if d d intuitively the state of the cas object is given by one data value d initially set to init in such a state a transition by method d tq is enabled since the comparison of the first argument and the current value succeeds returns t true leading to the new state value d that is a successful comparison results in a value a transition by method d fq in which the comparison fails returns f false is enabled if indeed d d after which no swap happens and the state value remains d the state with data value d is formally represented by the equivalence class of a sequence with a single method d tq now let us formalize the notion of allowing failures for cas object updates for this purpose we define another object called over the same set of methods with a somewhat different set of legal sequences we call a sequence d tq ¨ y ¨ d tq over cas a a sequence y is d and d d then s s sn p s f if there exists a set of positions i i ir n for s such that each sequence r is either a false count of x p s f positive sequence or is in be the maximum number of consecutive false positive sequences x contains the corresponding relaxation in our framework is obtained by using stuttering cost on the cas object and interval cost for the predicate pi given by r p ri dd d p d leads to a in which up to k methods may at the same state fail to perform a swap even though the data values match it is then easy to show the following correspondence result proposition a sequence x p s f has failure count k if and only if x is in the specification of shared counter the set of methods of a shared counter is sc n p nu the sequential specification of a shared counter contains the empty sequence and a sequence s of length n is in if and only if for all i n one can easily show that each state of a shared counter is a singleton ie for s t p we have s t if and only if s t the unique sequence representing a state is automatically a kernel sequence the transitions of lt are obviously given by and rs ¨ if for all i n we define a failing shared counter analogously to a failing cas object a sequence s is a behavior of if either s p t or t u ¨ is a behavior of and s t ¨ aq for a p t u the failure count of s p s f is one less than the length of a maximal subsequence of identical symbols in s the corresponding relaxation of the shared counter is obtained by the stuttering cost and the interval cost for pi r p ri thus we get the shared counter in which a method can fail to produce a new by one value at most k times for instance the sequence is in the sequential specification of the stuttering shared counter sc we again have the following correspondence result proposition a sequence x p s f has failure count k if and only if x is in the specification of related work the general topic of this paper is part of a recent towards scalable but semantically weaker concurrent data structures we first discuss work related to our framework and then focus on work related to our implementations framework the relaxation framework generalizes previous work on socalled and queues which correspond to restricted queues here our work is closely related to the semantics of concurrent data structures through just like we provide quantitative of concurrent data structures unlike which uses syntactic our are based on from a sequence to the sequential specification we briefly present the approach identify two main issues and how our method these we call two sequences x x both of length n permutation equivalent written x x if there exists a permutation p on t nu such that for all i n we write x a p x case to emphasize the permutation the permutation distance between x and x x in such x is given as ´ i nu let s be a sequential specification over in the distance of a sequence x p ° to s is defined via a collection d of subsets of let y p ° be a sequence such that z xy has a permutation equivalent z p s then for a p d the of obtaining z from z is the permutation distance between za and za where denotes restriction let ka denote the minimal over all y then x is with factor q d Ñ n if for all a p d ka observe that the distance for x is obtained by over all possible extensions of x whose permutations are in s we now show that this definition fails to capture desired relaxation not precise consider the following sequence in order to assign a relaxation cost of to this sequence belonging to an queue a scheme where only operations are allowed to commute formally uses d with k and where enq symbols however with resp this scheme w removes elements from an empty queue will always be in any k relaxation of the queue because setting z z will give independent of the value n this means that the following implementation is a relaxation of queue return random this implementation is clearly not implementing a queue nor any intended bounded relaxation of a queue but all the sequences it generates will have zero distance relative to d as given above thus cannot exactly capture intended and might allow wrong behaviors observe that we have already shown in proposition that can never generate such erroneous behavior not general for a stack consider the sequence x where all symbols a b have distinct values prior to the removal of a the stack contains a and b with the latter at the top position the distance in the relaxation induced by maximal path cost and segment cost in this case is since the element is immediately after the top entry however with factor q it is impossible to precisely capture for data structures like stacks first consider the case where we pick z x which we can do since x has a permutation equivalent valid stack sequence in order to get a permutation x of x such that x is a valid sequence of a stack either one of or has to move over m copies of push and pop operations or one of or has to move over n copies of push and pop operations so either d is empty which allows for any sequence to be in the relaxation or it is always possible to pick the values for n and m such that the is arbitrarily large second consider the case where we extend x with y such that xy has a permutation equivalent valid stack behavior but because of the suffix of x a similar reasoning as in the previous case applies to this case as well similarly a stuttering relaxation will not have a finite distance since no permutation of an extension of a stuttering sequence is in the original sequential specification consistency conditions as opposed to the sequential specification of a concurrent data structure one may also relax the consistency condition eg consistency instead of linearizability we note that linearizable relaxation of a stack is incomparable to a consistent stack to see this first consider a concurrent history c with two threads t and t the followed by history c starts with a sequence by t this history is consistent for stack because the reordering of methods even those that do not overlap in time is allowed as long as they are not separated by a state on the other hand any linearization of c will have to observe pop operations since the operations of t do not overlap so for each k there exists a history which is consistent for stack but is not in the specification of an second consider the sequential history which has an out relaxation distance of since the history is sequential consistency will not allow any reordering thus for any k there exists a history which is in the specification of an out but not consistent a overview of variants of weaker and stronger consistency conditions than linearizability can be found in implementations work related to our implementations and experiments is discussed in more detail in section and section here we briefly refer to all the work considered our relaxed stack implementation is closely related to the very recent efficient lockfree implementation of a relaxed queue by some of us and a third but the change from queue to stack semantics imposes a significant difference as well the queue is in turn related to implementations of relaxed fifo queues such as the random and segment queue as well as queues both the random queue and the segment queue implement the restricted relaxation the segment queue and the queue implement a queue of segments however the implementations are quite different with significant impact on performance see section our relaxed stack implementation implements a stack of segments queues are relaxed queues with in general unbounded relaxation since any relaxed stack or queue implementation also implements a pool we compare our work also to pool implementations in the authors show that implementing deterministic data structure semantics requires expensive synchronization mechanisms which may scalability in high contention scenarios we agree with that and show in our implementations and experiments that the nondeterminism introduced in the sequential specification provides scalability and performance benefits in the authors present a queue with relaxed semantics where queue elements may be returned any number of times instead of just once in comparison to other queues with semantics this may provide better performance and scalability again the introduced nondeterminism off overview of different on hardware and software level is presented in implementations of relaxed data structures in this section we present the new implementation of a restricted stack for short and present the two new implementations of a stuttering shared counter it is interesting to note that the restricted relaxation seems to be crucial for obtaining performance which is why we focus on it the top pointer of a concurrent stack may become a scalability under high contention the main idea behind our implementation is to reduce contention on the top pointer by maintaining a stack of socalled we implemented the stack that holds the similarly to the lockfree stack of with the difference that there is always at least one even if it is empty on the stack this avoids unnecessary removal and adding of a eg in the empty state a or just segment when no confusion arises contains k atomic values see next which may either point to null indicating an empty slot or may hold a socalled item both push and pop operations are by the top segment hence up to k stack operations may be performed in parallel a push operation tries to insert an element in the top segment it adds a new segment to the stack if the top segment is full a pop operation tries to remove an element from the top segment it removes the top segment from the stack if it is empty and is not the only segment on the stack additionally each segment contains an atomic counter remove that counts how many threads are trying to remove it from the stack the counter is initially set to zero the code of the lockfree algorithm with k is depicted in figure the occurrence of the problem is made through version numbers we refer to values with version numbers as atomic values hence an atomic value has two fields the actual value val and its version number the methods init try add new and try remove implement the stack of segments in the latter the atomic counter remove is updated and the method empty that performs an empty check is called we discuss the method empty within the pop method as it is also called there let item represent an element to be pushed on the the push method first tries to find an empty slot for the item using the find empty slot method line the find empty slot method randomly selects an index in the top and then linearly searches for an empty slot starting with the selected index and around at index k then the push method checks if the state has been consistently observed by testing whether top changed in the line which would trigger a if an empty slot is found line the method tries to insert the item at the location of the empty slot using a cas operation line if the insertion is successful the method verifies whether the insertion is also valid by calling the committed method line as discussed below if any of these steps fails a is performed if no empty slot is found in the current top segment the push method tries to add a new segment to the stack of segments line and then the committed method line an insertion it ensures that the inserted element is really inserted on the stack this method is the core and the main of the algorithm it returns true when the insertion is valid and false when it is not valid an insertion is if a concurrent thread removes the segment to which the element was inserted before the effect of the for an efficient implementation one needs a sequential specification that the properties of the hardware that it will run on the same highlevel idea is used in the segment queue and the queue discussed in the next subsection global top void init top void i f top next cas top v o i d i f top i f next null remove i f empty next i f cas top r e t u r n remove b o o l committed index i f s index r e t u r n true e l s e i f remove r e t u r n true e l s e empty i f top i f cas s index r e t u r n true e l s e val i f cas top r e t u r n true i f cas s index r e t u r n true r e t u r n false v o i d push item w h i l e true top index i f top i f val empty item i f cas s index i f committed index r e t u r n true e l s e item pop w h i l e true top index i f top i f val empty empty i f cas s index r e t u r n val e l s e i f i f empty i f top r e t u r n null e l s e figure lockfree algorithm insertion took place therefore an insertion is valid if the inserted item already at validation time by a concurrent thread line or the segment where the item was inserted was not removed by a concurrent thread line a remove counter larger than zero indicates that the segment has been removed or concurrent threads are trying to remove the segment from the stack line if the current top segment is not equal to the segment where the item was inserted we have to conservatively assume that the segment was removed from the stack line and the insertion line if the current top segment is equal to the segment where the item was inserted a race with concurrent threads may occur which may not have observed the insertion of the item and may try to remove the from the stack in the this would result in loss of the inserted item to prevent that the method tries to increment the version number in the top atomic value using cas line forcing threads that concurrently try to remove that to if this fails a concurrent pop operation may have changed top line which would make the insertion potentially invalid hence in case of the race the method tries to the insertion using cas line the pop method returns an item if the is not empty otherwise it returns null similar to the push method the pop method first tries to find an item in the top segment using the find item method line the find item method randomly selects an index in the top and then linearly searches for an item starting with the selected index and around at index k then the pop method checks if the state has been consistently observed by checking whether top changed in the line which would trigger a if an item was found line the method tries to remove it using cas line and returns it if the removal was successful line otherwise a is performed if no item is found and the current segment is the only segment on the stack line an empty check is performed using the method empty line this method stores the values of the k slots of the segment in a local array if they are empty and subsequently checks in another pass over the segment slots whether the values in the slots changed in the if a nonempty slot was found the empty method immediately returns false if the empty check and the top did not change in the line null is returned line otherwise if no item is found in the current segment and there is more than one segment in the stack the method tries to remove the segment line and performs a correctness we now prove that the implementation is correct for the relaxed stack semantics proposition the algorithm is linearizable with respect to restricted proof without loss of generality we assume that each item pushed on the stack is unique a segment s is reachable from a segment s if either ss or s is reachable from an item i is on the stack if has already committed and there exists a segment reachable from the top segment containing a slot whose value is i note that reachability is important ie only having a slot containing the item is not enough to guarantee that the item is logically on the stack because the slot could be in a segment to be removed by a concurrent pop operation we begin by identifying a linearization point of each method call the goal is to show that the sequential history obtained from a concurrent history by ordering methods according to their linearization points is in the specification of a restricted the linearization point of push is the reading of the empty slot line in the last iteration successful insertion of the main loop the linearization point of pop that does not return null is the reading of a nonempty slot line in the last iteration successful removal of the main loop the linearization point of a pop is the point after the first pass of the segment in the call to empty method line which returns true the correctness argument is based on the following facts an item is pushed on the stack exactly once this is a con sequence of our assumption and the control flow of the only method that can modify a slot to contain i an item is at most once if an item i is on the stack then it can only be removed once because of and the existence of a unique statement which replaces i with empty if i is in some slot but not on the stack then will erase i and insertion before we have to show that while i is in some slot but not on the stack no pop operation can return i clearly the call to method committed by must return false this implies that until committed completes the slot where i is not modified by any other thread otherwise either after the first if statement line or following failed cas attempts lines and of replacing i with empty will lead to returning true furthermore when control reaches the only exit point for returning false it is guaranteed that there is no slot containing i thus if i is not on the stack no pop operation could have replaced it with empty if a pop operation returns null then during its execution there must be a state at which there are no items on the stack since returning null is without any sideeffect it suffices to prove the existence of a state which corresponds to a logically empty stack the call to empty is only done when the top segment is the only segment in the stack in the empty method the value of top is checked at the beginning and after the first pass to ensure that the pointer is not updated by concurrent operations hence the stack is indeed logically empty at the linearization point since the second pass succeeds an item j cannot be before an item i if they are both on the stack and i j are in segments s s respectively with s reachable from s and s s the segment s can become a top segment only after the segment s has been removed by some pop operation moreover if a segment becomes unreachable from the top segment it remains unreachable these two observations imply that the linearization point of must the linearization point of which can only happen when s is a top segment an item i on the stack is only if it is one of the k ´ l items on the stack where l is the current of the item by we mean most recently pushed recall that is the number of that were performed after the pop of the previous item or the push of the current one assume i is from the stack at the current moment in time and at that point the item is t with current l this means that ever since t is the item on the stack no push operation was performed and there have been l pop operations performed none of which removed t let j be any of these l items since t is the last item pushed and it is still on the stack t is in the top segment since j is removed before t by it must have also in the top segment for the same reason also i is in the top segment prior to its removal hence at the moment in which happens there are at most k ´ l items in the top segment now shows that the sequential behavior obtained by order ing methods according to their linearization points satisfies the moreover shows an even behavior a linearizable empty check thus any concurrent tion generated by the given algorithm is linearizable for restricted observe that already and show that the has pool semantics l we could easily relax the linearizable empty check to fit the restricted specification by removing line in the code however a linearizable empty check is a valuable feature of a concurrent implementation struct x bool y v bool c d a cas struct xk bool yk v bool ck d b figure cas and state structures without any particular difficulty but with a somewhat argument one can show that the algorithm is lockfree by showing that whenever a thread an operation another thread completes its operation ensuring progress of at least one thread shared counters we implemented the two versions of a stuttering counter as discussed in the introduction the first version is based on a stuttering version of a software cas operation for short it uses a structure original shown in figure a to keep track of the state of concurrent cas operations the atomic value is located in field v and the cas operation uses the decision fields x y and c to determine which thread gets permission to change v we modified the original structure so that the fields x y and c are arrays of size k depicted in structure modified in figure b we keep the main cas operation but use a function that maps threads to array indices i smaller than k the thread id modulo k a thread determines the state of its cas operation by just accessing position i in the arrays x y and c on success a thread writes the new value into v hence up to k concurrent threads may perform the operation in parallel and change v resulting in a loss of at most k ´ state changes which further results in at most k ´ lost shared counter updates the second version of the counter is the counter depicted in figure b it is not difficult to show that both implementations are linearizable with respect to the shared counter and they are lockfree experiments we evaluate the performance and scalability of the several existing relaxed fifo queues and the shared counter implementations all experiments ran on an server machine with four core intel processors per core mb shared and gb of memory running linux we implemented a framework to analyze our and counter implementations as well as the implementations of relaxed queues the framework can be for a different number of threads n number of operations each thread performs o and the computational load performed between each operation c the computational load between two consecutive operations is created by iteratively calculating and c is the number of iterations performed we use c which takes a total of ns on average in our experiments the framework uses static for memory used at runtime with each page before running the benchmark to avoid paging issues we compare our implementation with a standard stack ls which acquires a global lock for each stack operation and a nonblocking stack ns which uses a cas operation to manipulate the top pointer of the stack moreover we compare our with different the lockfree bag more is better more is better more is better number of threads ls bag rp ns ed k number of threads lb rd r ed ms sq s rp fc bag k threads cas k k distributed counters k a stack b fifo queue c shared counter figure benchmarks on a core per core server with an increasing number of threads more is better more is better more is better k thread threads threads threads threads a stack k threads threads threads threads threads b fifo queue k threads k distributed counters threads c shared counter figure benchmarks on a core per core server with increasing k pool is based on threadlocal lists of elements threads put elements on their local list and take elements from their local list if it is not empty if it is empty they take elements from the lists of other threads the lockfree pool ed uses a set of fifo queues to store elements access to these queues is balanced using elimination arrays and a tree the synchronous pool rp implements a single elimination array using a ring buffer a get operation marks a slot identified by its thread id and for a take operation to insert an element take operations scan the array for pending get operations figure a the performance analysis of our we k which is a good k on average for a broad range of thread combinations and on our server machine this is no since the server machine has logically the analysis is done on a where half of the threads are and half are the and all considered stack and pool implementations figure a shows the effect of k on performance and scalability there exists an optimal k with respect to performance which is also robust in the sense that there exists only a single range of k for large k performance decreases due to higher sequential overhead eg scanning for elements in almost empty note that an increase in performance above k is not to be expected on the given architecture relaxed fifo queues we also evaluate the existing implementations of a queue and different fifo queues fifo queues and the introduced in the previous section the queue implements a restricted as a lockfree linked list of an operation is by the tail and a operation is by the head hence up to k and k operations may be performed in parallel the queue is empty if head and tail point to the same which does not contain any elements the strict fifo queue lb locks a global lock for each queue operation the lockfree strict fifo queue ms uses cas operations to change head tail and next pointer in a linked list of elements the strict fifo queue fc is based on the approach that a single thread performs the queue operations of multiple threads by locking the whole queue collecting pending queue operations and applying them to the queue the random queue rd implements a fifo with r where r defines the range r r ´ s of a random number it actually implements a restricted queue rd is based on ms where the operation was modified in a way that the random number determines which element is returned starting from the element the segment queue sq is a fifo queue with s it is logically similar both implement a queue of segments and hence a restricted queue to the queue but does not provide a linearizable empty check ie it may return null in the state also sq comes with a different segment management strategy than the queue which results on average in significantly more cas operations figure b the performance analysis of the queues we k r s which turns out be a good k on average for a broad range of thread combinations and on our server machine this is no since then the level of possible parallelism is k the number of logical we use a where half of the threads are and half the queue and all considered fifo queue fifo queue and pool implementations figure b shows the effect of k on performance and scalability again there exists a robust and optimal k with respect to performance also here for large k performance decreases due to larger sequential overhead counter we compare our shared counter and distributed shared counter implementations with a shared counter implementation based on a regular cas operation depicted in figure c the threads perform in total one counter increment operations in each benchmark run the cas version performs best until threads after that the shared counter and the distributed shared counter version it figure c shows the effect of k on performance and scalability in the version performance increases with larger k whereas in the distributed shared counter version decreases until k but increases after that our guess is that this is caused by the tradeoff between two possible sources of contention cas on the same memory location and bad caching ie accessing many different locations in memory the distributed shared counter decreases but increases however except for small values of k we observe that the gain is larger than the loss final remarks we have presented a framework for quantitative relaxation of concurrent data structures together with generic as well as further concrete instances of it our main motivation is the that relaxed data structures may decrease contention and thus provide the potential for scalable and implementations indeed the potential advantage which we demonstrate is the learned can be summarized as follows the way from a sequential implementation to efficient concurrent implementation is always hard just because a sequential specification is relaxed it does not necessarily mean that an efficient implementation immediately follows however efficient implementations that benefit from quantitative are possible as we demonstrate in this paper in our the framework provides a formal ground for quantitative relaxation of concurrent data structures and the to designing efficient concurrent implementations our current results open up several directions for future work one important issue is the applicability of relaxed data structures applicability can either be achieved by exploring applications that a relaxation eg provide less accurate but nevertheless acceptable results or showing that quality may remain the same despite the actual relaxation of semantics in the latter case do not influence correctness in the sense of another evident but difficult goal would be to synthesize implementations from as a first step we believe it is important to study the main principles that lead to good performance this is another line of future work that we plan to in small steps acknowledgements this work has been supported by the european research advanced grant the national research network rise on rigorous systems engineering science sn and an science v we thank the anonymous for their constructive and comments and suggestions to thank kozen and in particular had they not saved her life she would have a lot of the fun involved in working on this paper and it finished references y g m and n scalable based on trees in proc conference on parallel processing pages ­ springer y g and e relaxed consistency for improved concurrency in proc conference on principles of distributed systems pages ­ springer y m and a fast and scalable in proc international conference on distributed computing pages ­ berlin springerverlag j m and n counting networks journal of the acm ­ h r d p m michael and m laws of order expensive synchronization in concurrent algorithms cannot be eliminated in proc of principles of programming languages popl pages ­ acm m and n the art of multiprocessor programming morgan inc m and j linearizability a correctness condition for concurrent objects acm transactions on programming languages and systems toplas ­ d h i n and m flat combining and the tradeoff in proc symposium on parallelism in algorithms and architectures pages ­ acm c and h incorrect systems its not the problem its the solution in proc design automation conference acm c h h and a brief scalability versus semantics of concurrent fifo queues in proc symposium on principles of distributed computing acm c m and h fast and scalable queues technical report department of computer sciences university of june c h h and a performance scalability and semantics of concurrent fifo queues in proc international conference on algorithms and architectures for parallel processing pages ­ lncs v m m and n on the complexity of consensus in proc international symposium on distributed computing pages ­ springerverlag m michael and m scott simple fast and practical nonblocking and blocking concurrent queue algorithms in proc symposium on principles of distributed computing pages ­ acm m michael m and v idempotent work in proc principles and practice of parallel programming pages ­ acm s s h and m c rinard quality of service profiling in proc nd international conference on software engineering volume pages ­ acm a w e d l and d approximate data types for safe and general computation in proc nd acm sigplan conference on programming language design and implementation pldi pages ­ acm n data structures in the age communications acm ­ march h a m and p a lockfree algorithm for concurrent in proc symposium on parallelism in algorithms and architectures pages ­ new york ny usa acm r systems programming with parallelism technical report rj ibm research center april 