sound compilation of reals abstract writing accurate numerical software is hard because of many sources of including finite numerical precision of implementations we present a programming model where the user writes a program in a implementation and specification language that explicitly includes different types of we then present a compilation algorithm that generates a implementation that is guaranteed to meet the desired precision with respect to real numbers our compilation performs a number of verification steps for different candidate it generates verification conditions that treat all sources of in a unified way and encode reasoning about errors into reasoning about real numbers such verification conditions can be used as a format for verifying the precision and the correctness of numerical programs due to their nonlinear nature precise reasoning about these verification conditions remains difficult and cannot be handled using art smt solvers alone we therefore propose a new procedure that combines exact smt solving over reals with approximate and sound affine and interval arithmetic we show that this approach scalability limitations of smt solvers while providing improved precision over affine and interval arithmetic our implementation gives promising results on several numerical models including systems functions and controller implementations categories and subject descriptors d software engineering program verification keywords error floatingpoint arithmetic fixedpoint arithmetic verification compilation embedded systems numerical approximation scientific computing sensitivity analysis introduction writing numerical programs is difficult in part because the programmer needs to deal not only with the correctness of the algorithm but also with different forms of program inputs may not be exactly known because they come from physical experiments or were measured by an embedded the computation itself from errors at each step because of the use of arithmetic in addition resources like energy may permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ san diego ca usa copyright c acm be so that only a certain number of bits are available for the numerical data type at the same time the computed results in these domains can have consequences if used to control for example a or a power it is therefore to develop tools that improve in numerical code one of the first challenges in doing this is that most of our automated reasoning tools work with real arithmetic whereas the code is implemented with many current approaches to verify numerical programs start with the implementation and then try to verify the absence of runtime errors not only are such verification results specific to a given representation of numbers but the absence of runtime errors does not guarantee that program behavior matches the desired specification expressed using real numbers the source code semantics is currently expressed in terms of lowlevel data types such as this is problematic not only for developers but also for compiler optimizations because eg the associativity law is unsound with respect to such source code semantics in this paper we a natural but alternative source code programs should be expressed in terms of mathematical real numbers in our system the programmer writes a program using a real data type states the desired postconditions and specifies explicitly the as well as the desired target precision it is then up to our compiler to check taking into account all and their propagation that the desired precision can be soundly realized in a implementation if so the compiler chooses and one such implementation selecting from a range of software or hardware floatingpoint or fixedpoint arithmetic representations a key question that such a compiler needs to answer is whether a given representation remains close enough to an ideal implementation in terms of real numbers to answer this question we present a method to generate verification conditions that encode reasoning about errors into reasoning about real numbers our verification conditions explicitly model the ideal program without external and the actual program which is executed in finite precision with possibly inputs and the relationship between the two solving such verification conditions is one of the key tasks of a sound compiler for reals our approach is parametric in the of the representations and can thus be used on different platforms from embedded without floatingpoint units where fixedpoint implementations are needed to platforms that expose floatingpoint arithmetic in their instruction set architecture using libraries we can also emit code that uses precision twice or four times that of the double data type when multiple representations are available the compiler can select eg the smallest representation needed to the desired number of trusted significant to summarize source code as operating on real numbers has many advantages · programmers can reason about correctness using real arithmetic instead of arithmetic we achieve separation of the design of algorithms which may still be approximate for other reasons from their realization using computations · we can verify the ideal meaning of programs using techniques developed to reason over real numbers which are more scalable and better understood than techniques that directly deal with arithmetic · the approach allows us to quantify the of implementation outputs from ideal ones instead of merely proving eg range bounds of floatingpoint variables which is used in simpler static analyses · the compiler for reals is free to do optimizations as long as they preserve the precision requirements this allows the compiler to apply for example associativity of arithmetic or even select different approximation schemes for functions · in addition to errors the approach also allows the to quantify program behavior in the face of external such as input measurement errors using our verification conditions the correctness and the precision of compilation for small programs can in principle be directly verified using an smt solver such as z see section the capabilities of such solvers are likely continue to improve as the solvers advance our approach however the complexity of the generated verification conditions for larger programs is currently out of reach of such solvers and we believe that specialized techniques are and will continue to be necessary for this task this paper presents two specialized techniques that improve the feasibility of the verification task the first technique performs local approximation and is effective even in benchmarks containing nonlinear arithmetic the second technique specifically handles conditional expressions solving nonlinear constraints forward propagation nonlinear arithmetic a significant challenge for verification because it cannot directly be handled using algorithms embedded inside smt solvers although interesting relevant fragments are decidable and are supported by modern solvers the complexity of solving such constraints is much higher in terms of both worstcase complexity and the experience in practice unfortunately nonlinear arithmetic is in numerical software furthermore our verification conditions add error terms to arithmetic expressions so the resulting constraints grow further in complexity often out of reach of solvers an alternative to encoding into smt solver input is to use a sound and arithmetic model such as interval or affine arithmetic however when used by itself on nonlinear code these approaches yield too pessimistic results failing to establish any bounds on precision in a number of useful benchmarks we show that we can combine range arithmetic computation with smt solving to overcome the limitations of each of the individual techniques from the point of view of the logical encoding of the problem range arithmetic becomes a specialized method to perform approximate quantifier elimination of bounded variables that describe the we obtain a sound precise and somewhat scalable procedure during range computation our technique also checks for common problems such as overflow division by zero or square root of a negative number the corresponding warnings because the procedure is a forward computation it is suitable for automatically generating function summaries containing output ranges and errors of a function this is a feature that smt solvers do not solve by themselves because their primary functionality is formula satisfiability questions sound compilation of conditionals in the presence of conditional branches become another verification challenge namely the ideal execution may follow one branch but because of input or errors the actual execution follows another this behavior may be acceptable however if we can show that the error on the output remains within required bounds our approach could benefit from modular automated analysis of continuity which was previously because we are interested in concrete bounds we present a new method to check that different paths taken by and versions of the program still preserve the desired precision specification our check does not require continuity which can be difficult to prove for nonlinear code instead it directly checks that the difference between the two values on different branches meets the required precision this technique extends our method for handling nonlinear arithmetic so it benefits from the combination of range arithmetic and smt solving implementation and evaluation we have implemented our compilation and verification procedure including the verification condition generation analysis of possibly nonlinear expressions and the handling of conditionals our system is implemented as an extension of the verifier for functional scala programs the implementation relies on a range arithmetic implementation for scala as well as on the z smt solver we have evaluated the system on a number of benchmarks obtaining promising results our implementation and the benchmarks are available from to support programming of larger code fragments our system also supports a basic modular verification technique which handles functions by replacing calls with function postconditions or by inlining bodies of called functions we thus expect that our technique is applicable to larger code bases as well possibly through code into multiple smaller and annotated functions even on the benchmarks that we release we are aware of no other available system that would provide the same guarantees with our level of automation summary of contributions our overall contribution is an approach for sound compilation of real numbers into representation specifically · we present a implementation and specification language for numerical programs with we define its semantics in terms of verification constraints that they induce we believe that such verification conditions can be used as a format for verifying the precision and the correctness of numerical programs · we develop an approximation procedure for computing precise range and error bounds for nonlinear expressions which combines smt solving with interval arithmetic we show that such an approach significantly improves computed range and error bounds compared to standard interval arithmetic and better than smt solving alone our procedure can also be used independently as a more precise alternative to interval arithmetic and thus can perform forward computation without requiring function postconditions to be provided · we describe an approach for soundly computing error bounds in the presence of branches and which ensures soundness of compilation in case the function defined by a program with conditionals · we have implemented our framework and report our experience on a set of benchmarks including benchmarks from and control systems the results show that our technique is effective and that it achieves a of the techniques on which it relies example we demonstrate some aspects of our system on the example written in the scala programming language in figure the methods and compute the area of a with side lengths a b and c we consider a particular application where the user may have two side lengths given and may vary the third she has two functions available to do the computation and wants to determine whether either or both satisfy the precision requirement of e on line require and ensuring give the pre and postconditions of functions which are written in scala notation res e denotes that the return value res should have an absolute error of at most e compared to the ideal computation over reals our tool determines that such requirement needs at least double floatingpoint precision and the corresponding code in general the challenge is to establish that this precision is sufficient to ensure the required bounds given that errors in code and grow without an a priori bound our tool verifies fully automatically that the method indeed satisfies the postcondition and generates the source code with the double data type which also includes a more precise and complete postcondition on main res res res e to achieve this result our tool first checks that the precondition of the function call is satisfied using the z solver then it the body of the function and computes a sound bound on the results uncertainty with our approximation procedure it uses the computed bounds to show that the postcondition of main is satisfied the error computation takes into account in a sound way the input uncertainty here an initial error on the inputs its propagation and errors committed at each arithmetic operation additionally due to the error the comparison on line may give a different truth value in boundary cases certain floatingpoint computations will therefore take a different branch than their corresponding computation more precisely the total error when computing the condition is e as computed by our tool that is floatingpoint values that satisfy a b e may take the else branch even though the corresponding real values would follow the then branch and similarly in the opposite direction our tool verifies that despite this the difference in the computed result in two branches remains within the precision requirement intuitively the values of computed branches are close for the interval where the truth value of the condition changes and these values in and implementation are close to each other finally our tool uses our novel range computation procedure to also find a more precise output range than we could have obtained in eg interval arithmetic or any static analysis method that does not error variations from the fact that a program can be ran over an interval of inputs our tool computes the range for both methods but shows a difference in the absolute error of the computation for the method the verification fails because the computed error e the def real b real c real real require a a val b val c val area b c val area b c area res e def real b real c real real require a a b b c c a b c e a c b e b c a e val s a b c s a s b s c def real b real c real real require a a b b c c a b c e a c b e b c a e a c b c if a b else figure computing the area of a with a given precision required precision bound this result is formula for is known to from for flat which is somewhat in the method on the other hand our tool proves that using the desired precision of on the result programs with reals each program to be compiled consists of one toplevel object with methods written in a functional subset of the scala programming language all methods are functions over the real data type and the user them with pre and postconditions that ex talk about real represents ideal real num without any uncertainty we allow arithmetic reals with the standard arithmetic operators together with conditionals and function calls they form the body of methods our tool also supports immutable variable declarations as val x this language allows the user to define a computation over real numbers note that this specification language is not the precondition allows the user to provide a specification of the environment a complete environment specification consists of lower and upper bounds for all method parameters and an upper bound on the uncertainty or range bounds are expressed with regular comparison operators uncertainty is expressed with a predicate such as x e which denotes that the variable x is only known up to e alternatively the programmer can specify the relative error as x e x if no except for is present errors are automatically added to input variables the postcondition can specify constraints on the output and in particular the range and the maximum accepted uncertainty in ad to the language allowed in the precondition the tion may reference the errors on inputs directly in the following way res x which says that the maximum acceptable error on the output variable res is bounded from above by times the initial error on x whereas the precondition may only talk about the ideal values the postcondition can also reference the actual value directly via x this allows us to assert that runtime values will not a certain range for instance floatingpoint arithmetic our tool and technique support in the generated target code any floatingpoint precision and in par single and double floatingpoint precision as defined by the ieee floatingpoint standard we assume mode and that basic arithmetic operations correctly which means that the re from any such operation must be the representable floatingpoint number hence provided there is no overflow and the numbers are not in range the result of a binary ation f satisfies x f y x r y m where r is the ideal operation in real numbers and m is the machine epsilon that determines the upper bound on the relative error this model provides a basis for our error when there is a possibility of an overflow or values our analysis reports an error an extension of the analysis to handle is straightforward by adding an additional error term in equation fixedpoint arithmetic our tool and technique also support standard fixedpoint arithmetic for more details see our precision analysis supports any our code generator generates code for and bit fixedpoint arithmetic which are the most common choices using integers and other representations of the techniques described in this paper are general in that they are also applicable to other arithmetic representations as long as errors can be computed at each computation step from the ranges of variables examples include floatingpoint implementations with a different number of bits for the and or redundant arithmetic compiling reals to finite precision given a specification or program over reals and possible target data types our tool generates code over floatingpoint or fixedpoint numbers that satisfy the given pre and postconditions and thus meet the target precision figure presents a highlevel view of our compilation algorithm our tool first analyses the entire specification and generates one verification condition for each postcondition to be proven to obtain a modular algorithm the tool also generates verification conditions that check that at each function call the precondition of the called function is satisfied the methods are then sorted by occurring function calls this allows us to reuse already computed postconditions of function calls in a modular analysis if the user specifies one target data type the remaining part of the compilation process is performed with respect to this data types precision if not or in the case the user specified several possible types our tool will perform a binary search over the possible types to find the least in the list that satisfies all specifications the user of our tool can provide the list of possible data types manually and sort them by her individual preference currently the analysis is performed separately for each data type which is not a big issue due to the relatively small number of alternatives we did identify certain shared computations between iterations we can exploit them in the future for more efficient compilation in order for the compilation process to succeed the specification has to be met with respect to a given arithmetic thus the principal part of our algorithm is spent in verification which we describe in section we that in the future the compilation task will also include automatic code optimizations but in input spec specification over reals candidate for f f while and precision for for vc while precision output floatingpoint or fixedpoint code figure compilation algorithm this paper we concentrate on the challenging of verifying the precision of code our tool can currently generate scala code over · fixedpoint arithmetic with a or bit width or · floatingpoint arithmetic in single bit double bit bit and bit precision we currently do not support of data types in one program but plan to explore this in the future for and precision which were implemented in software by we provide a scala interface to the library with the generated code in case the verification part of compilation fails our tool a failure report with the best postconditions our tool was able to compute the user can then use the generated specifications to gain insight why and where her program does not satisfy the requirements while we have implemented our tool to accept specifications in a domain specific language embedded in scala and generate code in scala all our techniques apply equally to all programming languages and hardware that follow the floatingpoint abstraction we assume equation verifying real programs we will now describe the verification part of our compilation algorithm in the following we will call the ideal computation the computation in the absence of any and implemented in a real arithmetic and the actual computation the computation that will finally be executed in and with potentially inputs verification conditions for programs for each method with a precondition p and a postcondition q our approach considers the following verification condition x res y p x y res qx res where x res y denote the input output and local variables respectively table summarizes how verification constraints are generated from our specification language for floatingpoint arithmetic each variable x in the specification corresponds to two variables x x the ideal one in the absence of and errors and the actual one computed by the compiled program note that the ideal and actual variables are related only through the error bounds in the pre and postconditions which al a x x b x a b x k x x k k x m x x x mx mx x x x x y ideal part x y x y actual part x y ideal part actual part val z x z x z x if cx ex cx ex ex else ex gx gx gx m i i m all are fresh cond and e denote functions with errors at each step table semantics of our specification language for the ideal and actual executions to take different paths in the method body we have to take into account errors from be in principle extended to elementary functions for instance by encoding them via taylor note that the resulting verification conditions are parametric in the machine epsilon for fixedpoint arithmetic constraints such as above can also be generated although the translation is more complex due to the fact that fixedpoint formats have to be statically determined error can thus no longer be encoded by a formula such as equation one possible translation is to assign fixedpoint formats including the position of the fixed point based on ranges and then verify that the resulting constraint with the assigned formats still holds in the presence of errors however because a direct attempt to solve verification conditions using an solver alone is not satisfactory see section our tool directly uses the approximation procedure from section and computes sound ranges for fixedpoint implementation allocating fixedpoint formats on the specification generation in order to give feedback to developers and to facilitate automatic modular analysis our tool also provides automatic specification generation by this we mean that the programmer still needs to provide the environment specification in form of preconditions but our tool automatically computes a precise postcondition formally we can rewrite the constraint as x res y p x y res qx res where q is now unknown we obtain the most precise postcondition q by applying quantifier elimination qe to p x y res and eliminate y the theory of arithmetic over reals admits qe so it is possible to use this approach we do not currently use a full qe procedure for specification generation as it is expensive and it is not clear whether the returned expressions would be of a suitable format instead we use our approximation approach which computes ranges and maximum errors in a forward fashion and computes an over approximation of a postcondition of the form res a b res ± u when proving a def precision inlining postcondition inlining full function functions merging paths error error range arithmetic first approximation figure approximation pipeline postcondition our tool automatically generates these specifications and provides them as feedback to the user difficulty of simple encoding into smt solvers for small functions we can already prove interesting properties by using the exact encoding of the problem just described and the verification constraints with z consider the following code a programmer may write to implement the third basic function which is commonly used in signal processing def real real require u u u e ensuring res res res res e functions and the corresponding verification conditions of this complexity are already within the possibilities of the nonlinear solver within z for more complex functions however z does not yet provide an answer in a reasonable time or returns unknown whether alternative techniques in smt solvers can help in such cases remains to be seen we here provide an approach based on approximation that addresses the difficulty of generalpurpose constraint solving verification with approximations to soundly compile more interesting programs we have developed an approximation procedure that computes a sound overapproximation of the range of an expression and of the uncertainty on the output this procedure is a forward computation and we also use it to generate specifications automatically we describe the approximation procedure in detail in section for now we will assume that it exists and given a precondition p and an expression expr computes a sound bound on the output range and its associated uncertainty a b err expr x x res resp x x res expr x res expr x a res res b res res err we have identified three possibilities for approximation nonlinear arithmetic function calls and paths due to conditionals and each can be approximated at different levels we have observed in our experiments that one size does not fit all and a combination of different approximations is most successful in proving the verification conditions we encountered for each verification condition we thus construct approximations until z is able to prove one or until we run out of approximations where we report the verification as failed we can thus view verification as a stream of approximations to be proven we illustrate the pipeline that computes the different approximations in figure the routines and are described in the following sections in more detail the first approximation indicated by the long arrow in figure is to use z alone on the entire constraint constructed by the rules in table this is indeed an approximation as all function calls are treated as uninterpreted functions in this case as noted before this approach only works in very simple cases or when no and no functions are present then taking all possible combinations of in our pipeline we obtain the other approximations which are accordingly depending on whether the constraint contains function calls or conditional branches function calls if the verification constraint contains function calls and the first approximation failed our tool will attempt to inline postconditions and pass on the resulting constraint down the approximation pipeline we support inlining of both postconditions and postconditions computed by our own specification generation procedure if this still is not precise enough we inline the entire function body postcondition inlining is implemented by replacing the function call with a fresh variable and it with the postcondition thus if verification succeeds with inlining the postcondition we avoid having to consider each path of the function separately and can perform modular verification avoiding a potential path explosion problem such modular verification is not feasible when postconditions are too and we plan to explore the generation of more precise postconditions in the future one step in this direction is to allow postconditions that are parametric in the initial errors for example with the operator x introduced in section while our tool currently supports postcondition inlining with such postconditions we do not yet generate these automatically arithmetic the arithmetic part of the verification constraints generated by table can be essentially divided into the ideal part and the actual part which includes errors at each computation step the ideal part determines whether the ideal range constraints in the postcondition are satisfied and the actual part determines whether the uncertainty part of the postcondition is satisfied we can use our procedure presented in section to compute a sound approximation of both the results range as well as its uncertainty based on this our tool first constructs an approximation which leaves the ideal part unchanged but replaces the actual part of the constraint by the computed uncertainty bound this effectively removes a large number of variables and is many times a sufficient simplification for z to succeed in verifying the entire constraint if z is still not able to prove the constraint our tool constructs the next approximation by also replacing the ideal part this time with a constraint of the results range which has been computed by our approximation procedure previously note that this second approximation may not have enough information to prove a more complex postcondition as information is lost we note that the computation of ranges and errors is the same for both approximations and thus trying both does not affect efficiency significantly in our experiments z is able to prove the ideal part in most cases so this second approximation is used paths in the case of several paths through the program we have the option to consider each path separately or to merge results at each join in the control flow graph this introduces a tradeoff between efficiency and precision since on one hand considering each path separately leads to an exponential number of paths to consider on the other hand merging at each join information between variables which may be necessary to prove certain properties our approximation pipeline chooses merging first before to a verification in case of failure we believe that other techniques for exploring the path space could also def real real require x x val z val z z z res res e def real real require x x val z val z z z res res e def real real require x x x res res res e def real real require x x x res res res e figure different polynomial approximations of be integrated into our tool another possible improvement are heuristics that select a different order of approximations depending on particular characteristics of the verification condition example we illustrate the verification algorithm on the example in figure using double floatingpoint precision as the target the functions and are verified first since they do not contain function calls verification with the full verification constraint fails next our tool computes the errors on the output and z succeeds to prove the resulting constraint with the ideal part from this approximation our tool directly computes a new more precise postcondition in particular it can narrow the resulting errors to e and e respectively next our tool considers the function inlining only the postcondition is not enough in this case but computing the error approximation on the functions succeeds in verifying the postcondition note that our tool does not approximate the portion of the constraint ie z is used directly to verify the constraint z z this illustrates our separation of the real reasoning from the implementation with our separation we can use a real arithmetic solver to deal with algorithmic reasoning and verify with our error computation that the results are still valid within the error bounds in the implementation finally the tool verifies that the preconditions of the function calls are satisfied by using z alone verification of the function fails with all approximations our tool is able to determine that the ideal constraint alone z z is not valid reports a counterexample x and returns invalid as the verification result soundness our procedure is sound because our constraints the actual errors furthermore even in the full constraint as generated from table errors are since we assume the worstcase error bound at each step while this ensures soundness it also introduces incompleteness as we may fail to validate a specification because our overapproximation is too large this implies that counterexamples reported by z are in general only valid if they the ideal part of the verification constraint our tool checks whether this is the case by constructing a constraint with only the part and reports the counterexamples if such are returned from z loops and recursion in principle our techniques can be applied to programs with loops via recursion however because of of errors only systems can be expected to have simple inductive invariants and such systems can to some extent also be addressed using runtime verification techniques for systems the depend on the number of iterations which makes specifications of such functions very complex note that for ensuring the stability of certain embedded control systems it has been shown that it is sufficient to consider the body of the control loop only solving nonlinear constraints having given an overview of the approximation pipeline we now describe the computation of the approximation for nonlinear arithmetic which corresponds to the last box in figure for completeness of presentation we first review interval and affine arithmetic which are common choices for performing sound arithmetic computations and which we also use as part of our technique we then present our novel procedure for computing the output range of a nonlinear expression given ranges for its inputs that can be a more precise substitute for interval or affine arithmetic finally we continue with a procedure that computes a sound overapproximation of the uncertainty on the result of a nonlinear expression one possibility to perform guaranteed computations is to use standard interval arithmetic interval arithmetic computes a bounding interval for each basic operation as x y y y and analogously for square root affine arithmetic was originally introduced in and ad the difficulty of interval arithmetic in handling between variables affine arithmetic represents possible values of variables as affine forms n x x xi i i where x denotes the central value of the represented interval and each symbol i is a formal variable denoting a from the central value intended to range over the maximum magnitude of each term is given by the corresponding xi note that the sign of xi does not matter in isolation it does however reflect the relative dependence between values eg take x x x then x x x x x x x x x x if we x x x instead the resulting interval would have width x and not zero the range represented by an affine form is computed as x x x n xi i a general affine operation x y consists of addition addition of a constant or multiplication by a constant expanding the affine forms x and y we get n x y x y xi yi i i def precondition precision lower bound if a precision a b while ba precision mid a b a mid match case sat b mid case a mid case unknown break a else upper bound return figure algorithm for computing the range of an expression an additional motivation for using affine arithmetic is that different contributions to the range it represents remain at least separated this information can be used for instance to help identify the major of a results uncertainty or to separate contributions from external from errors range computation the goal of this procedure is to perform a to determine the range of a nonlinear arithmetic expression given ranges for its inputs two common possibilities are interval and affine arithmetic but they tend to the resulting range especially if the input intervals are not sufficiently small order affine arithmetic improves over interval arithmetic somewhat by tracking linear but in the case of nonlinear expressions the results can become actually worse than for interval arithmetic eg x y where x y gives in affine arithmetic and in interval arithmetic observation a nonlinear theorem prover such as the one that comes with z can decide with fairly good precision whether a given bound is sound or not that is we can check with a prover whether for an expression e the range a b is a sound interval this observation is the basis of our range computation the input to our algorithm is a nonlinear expression expr and a precondition p on its inputs which specifies among possibly other constraints ranges on all input variables x the output is an interval a b which satisfies the following a b expr x resp x res expr x a res res b the algorithm for computing the lower bound of a range is given in figure the computation for the upper bound is symmetric for each range to be computed our tool first computes an initial sound estimate of the range with interval arithmetic it then performs an initial quick check to test whether the computed first approximation bounds are already tight if not it uses the first approximation as the starting point and then down the lower and upper bounds using a binary search at each step of the binary search our tool uses z to or reject the newly proposed bound the search stops when either z fails ie returns unknown for a query or cannot answer within a given timeout the difference between subsequent bounds is smaller than a precision threshold or the maximum number of iterations is reached this criterion can be set dynamically additional constraints in addition to the input ranges the precondition may also contain further constraints on the variables for example consider again the method in figure the precondition bounds the inputs as a b c but the formula is useful only for valid ie when every two sides together are longer than the third if not we will get an error at the when we try to take the square root of a negative number in approaches we can only consider input intervals that satisfy this constraint for all values and thus have to check several and possibly many cases in our approach since we are using z to check the soundness of bounds we can assert the additional constraints and then all subsequent checks are performed with respect to all additional and initial constraints this allows us to avoid interval due to or problem specific constraints such as those in the example this becomes especially valuable in the presence of multiple variables where we may otherwise need an exponential number of error approximation we now describe our approximation procedure which for a given expression expr and a precondition p on the inputs computes the range and error on the output more formally our procedure satisfies the following a b err expr x x res resp x x res expr x res expr x a res res b res res err where expr represents the expression evaluated in arithmetic and x x are the ideal and actual variables the precondition specifies the ranges and of initial variables and other additional constraints on the ideal variables the uncertainty specification is necessary as it relates the ideal and actual variables the idea of our procedure is to execute a computation while keeping track of the output range of the current expression and its associated errors at each arithmetic operation we propagate existing errors compute an upper bound on the error and add it to the overall errors since the error depends on the range of values we need to keep track of the ranges as precisely as possible our procedure is build on the abstraction that a computation is an ideal computation plus or some uncertainty the abstraction of floatingpoint errors that we choose also follows this separation f lx y x y x y x y for m m and this allows us to treat all in a unified manner for fixedpoint arithmetic the situation is similar but we first determine the fixedpoint format from the current range and only from this compute the error our procedure builds on the idea of the data type which uses affine arithmetic to track both the range and the errors for nonlinear operations however the so computed ranges become quickly very pessimistic and the error computation may also from this we observed that since the errors tend to be relatively small this does not affect the error propagation itself to such an extent if the initial errors are small less than one nonlinear terms tend to be even smaller whereas if the affine terms are larger than one the nonlinear terms grow we thus concentrate on improving the ideal range of values and use our novel range computation procedure for this part and leave the error propagation with affine arithmetic as in in our adaptation we represent every variable and intermediate computation result as a datatype with the following components x range interval err where range is the range of this variable computed as described in section and err is the affine form representing the errors the overapproximation of the actual range including all is then given by range err where err denotes the interval represented by the affine form error computation errors for floatingpoint arithmetic are computed at each computation step as where is the machine epsilon and added to err as a fresh term note that this error computation makes our error computation parametric in the floatingpoint precision for fixedpoint arithmetic errors are computed as where the function returns the best fixedpoint format that can accommodate the range this computation is also parametric in the error propagation for affine operations addition and multiplication by a constant factor the propagated errors are computed and thus as for standard affine arithmetic we refer the reader to for further details and describe here only the propagation for nonlinear arithmetic for multiplication division and square root the magnitude of errors also depends on the ranges of variables since our ranges are not affine terms themselves propagation has to be in the following we denote the range of a variable x by x and its associated error by the affine form when we write x we mean that the interval x is converted into an affine form and the multiplication is performed in affine arithmetic multiplication is computed as x y x x y x y where is the new error thus the first term to the ideal range and the remaining three to the error affine form the larger the factors x and y are the larger the finally computed errors will be in order to keep the overapproximation as small as possible we evaluate x and y with our new range computation division is computed as x y x y x x y x err y y err y for square first compute an affine approximation of square root as in x x and then perform the affine multiplication term and our procedure allows us to detect potential division by zero and square root of a negative value as our tool computes ranges of all intermediate values we currently report these issues as warnings to the user limitations the limitation of this approach is clearly the ability of z to check our constraints we found its capabilities satisfactory although we expect the performance to still significantly improve to emphasize the difference to the constraints that are defined by table the constraints we use here do not add errors at each step and thus the number of variables is reduced significantly we also found several transformations helpful such as rewriting eg x x x to x out products and avoiding nonstrict comparisons in the precondition although the benefits were not entirely consistent note that at each step of our error computation our tool computes the current range thus even if z fails to the bound for some expressions we still compute more precise bounds than interval arithmetic overall in most cases as the ranges of the remaining subexpressions have already been computed more precisely conditional statements in this section we consider the difference between the ideal and actual computation due to on computing branch conditions and the resulting different paths taken we note that the full constraint constructed according to section automatically includes this error recall that the ideal and actual computations are independent except for the initial conditions so that it is possible that they follow different paths through the program in the case of using approximation however we compute the error on individual paths and have to consider the error due to paths separately we propose the following algorithm to explicitly compute the difference between the ideal and the actual computation across paths note that we do not assume continuity ie the algorithm allows us to compute error bounds even in the case of functions for simplicity we present here the algorithm for the case of one conditional statement if cx fx else fx it generalizes readily to more complex expressions we assume that the condition is of the form cx indeed any conditional of the form cx would yield different results for the ideal and actual computation for nearly any input so we do not allow it in our specification language the actual computation commits a certain error when computing the condition of the branch and it is this error that causes some executions to follow a different branch than the corresponding ideal one would take consider the case where the ideal computation evaluates f but the actual one evaluates f the algorithm in figure gives the computation of the path error in this case the idea is to compute the ranges of f and f but only for the inputs that could be the final error is then the maximum difference of these value the algorithm extends naturally to several variables in the case of several paths through the program this error has to be in principle computed for each pair of paths we use z to rule out infeasible paths up front so that the path error computation is only performed for those paths that are actually feasible our tool implements a refined version of this approach which merges paths to avoid having to consider an exponential number of path combinations it also uses a higher default precision and number of iterations threshold during the binary search in the range computation as this computation requires in general very tight intervals for each path we identify two challenges for performing this computation as soon as the program has multiple variables the inputs for the different branches are not intervals which makes an accurate evaluation of the individual paths difficult def input pre x a b x ± n program if fx else fx val cond f f val ¬ cond f f return max def c f f c err c c err float cx err c f cx err c f return max err float figure computing error due to paths denotes by x are intervals the inputs for the two branches are thus simply evaluating the two branches with inputs that are in the correct ranges but are not yields pessimistic results when computing the final difference line we overcome the first challenge with our range computation which takes into account additional constraints for the second challenge we use our range computation as well unfortunately z fails to the final range to a satisfactory precision due to we still obtain much better error than with interval arithmetic alone as the ranges of values for the individual paths are already computed much more precisely we report in section on the type of programs whose verification is within our reach experiments the examples in figure and and section provide an idea of the type of programs our tool is currently able to verify fully automatically the example from section is the largest meaningful example we were able to find that z alone could verify in the presence of for all other cases it was necessary to use our approximation methods we have chosen several nonlinear expressions commonly used in and as benchmark functions as well as benchmarks used in control systems and suitable benchmarks from experiments were performed on a computer running with a i processor and gb of ram compiling programs which data type is suitable for a given program depends on the parameter ranges the code itself but also the precision required by the application using the code for example take the functions in figure depending on which precision on the output the user needs our tool will select different data types for the requirement res e as specified double is be a suitable choice for and however for the example this is not sufficient and thus would be selected by our tool the user can influence which data types are by a list to our tool which is ordered by her preference figure illustrates the tradeoff between the precision achieved by different data types against the runtime of the compiled code generated by our tool we used the framework for the running times benchmark order our range e e interval arithmetic e e simulated range e e table comparison of ranges computed with out procedure against interval arithmetic and simulation simulations were performed with random inputs ranges are affine arithmetic does not provide better results than interval arithmetic def real v real t real real require u u v v t t val t t t v t ut u res e def real x real real require x x x x val t xx x x x xx x xx x res e def real w real r real real require v v w w r r rr res e figure benchmark functions from and control systems for our benchmarks with their limited input ranges bit fixedpoint implementations provide better precision than single floatingpoint precision because single precision has to accommodate a larger dynamic range which reduces the number of bits available for the that said fixedpoint implementations run slower at least on the jvm than the more precise double floatingpoint arithmetic with its hardware support however the choice for fixedpoint rather than floatingpoint may be also due to this hardware being our tool can thus support a wide variety of applications with different requirements we also note that across the three not selected benchmarks the results are very consistent and we expect similar behavior for other applications as well bit bit bit bit float double engine bit float double bit float double max abs error analyzed e e e e e e e e e e e e e e e e e e e e figure of benchmarks compiled for different in left vs abs error bounds computed for that precision by our tool right evaluating effectiveness on nonlinear expressions range computation of errors depends on the estimate of the ranges of variables the strength of using a constraint solver such as z is that it can perform such while taking into account the precise dependencies between variables in preconditions and path conditions table compares results of our range computation procedure described in section against ranges obtained with standard interval arithmetic interval arithmetic is one of the methods used for range tion an alternative being affine arithmetic which we found to give more pessimistic results in our experiments we believe that this is due to in computing nonlinear operations note however that we still use affine arithmetic to estimate errors given the computed ranges for our range computation we set the default precision threshold to e and maximum number of iterations for the binary search to to obtain an idea about the true ranges of our functions we have also computed a lower bound on the range using simulations with random inputs and with exact rational arithmetic evaluation of expressions we observe that our range computation can significantly improve over standard interval bounds the benchmark is a example where interval arithmetic yields the bound but our procedure can still provide bounds that are quite close to the true range error computation table compares computed by our tool against maximum obtained through extensive simulation with random inputs for different to obtain of error bounds we ran the simulation in parallel with rational and their corresponding floatingpoint or fixedpoint values and obtained the error by taking the difference in the result selected benchmarks marked with also have added on the input parameters to our knowledge this is the first quantitative comparison of an error computation precision with an approximation of the true errors on such benchmarks our computed are mostly within about an order and many times even closer to the underapproximation of the true errors provided by simulation in the case of the benchmark we believe that the is mainly due to its complexity and subsequent failures of z in the range computation the values in parentheses in the second column indicate errors computed if ranges at each arithmetic operation are computed using interval arithmetic alone while we have not to improve the affine error computation from we can see that in some cases a more precise range computation can gain us improvements the full effect of the of standard range computation appears when due to this we obtain possible errors such as or square root of a negative number errors the first case happens in the case of the nonlinear benchmark so with interval arithmetic alone we would therefore not obtain any meaningful result similarly for the example from section without being able to constrain the inputs to form valid we cannot compute any error bound because the becomes possibly negative table presents another relevant experiment evaluating the ability to use additional constraints during our range computation for this experiment we use double precision and the example from section with additional constraints allowing flat by setting the threshold on line a b c e to the different values given in the first column as the become we observe an expected increase in uncertainty on the output since the formula becomes more to errors at threshold e our range computation fails to provide the necessary precision and the becomes possibly negative using our tool the can therefore go beyond rules of and informal and be that the computed area is accurate up to even for that whose difference a b c is as small as compilation running time running times for compilation are below seconds for all benchmarks from table and for the example from figure except for which runs in about two due to from z for some intermediate ranges examples that require computation of the path error are much more computationally challenging verifying the benchmark bit bit bit fl bit fl bit order fl order our error ia only e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e simulated error e e e e e e e e e e e e e e e e e e e e e e e e e e e e table comparison of errors computed with our procedure against simulated errors simulations were performed with random inputs indicates that inputs have external associated and fl denotes double and single floatingpoint precision respectively or bit is the of the fixedpoint implementation benchmark e e e e e e e e e area e e e e e e e max abs error e e e e e e e e e table area computed and error on the result for flat all values are interval arithmetic alone fails to provide any result def real real require x x if xx x x else xx res res res def real real require x x x e if x e x else x res e def real b real c real real require a a b b c c bb a c val bb a c ac c b else b else b else b res e figure path error computation examples tion on the main function from the initial example takes about one running times depend highly on whether z fails to intermediate ranges and the timeout used for z our default setting is one second we did not find much improvement in the success rate above this threshold evaluating errors across program paths figure presents several examples to evaluate our error computation procedure across different paths from section the first method cav has been used before as a benchmark function for computing the output range our tool can verify the given postcondition immediately note that the error on the result is actually as large as the result itself since the method is an aspect that has been ignored in previous work but that our tool detects automatically the method is also a function that computes the square root of x using an approximation for small values and the regular library method otherwise note the additional uncertainty on the input which could occur for instance if this method is used in an embedded controller our tool can verify the given specification if we change the condition on line to x e however verification fails in this fashion we can use our tool to determine the appropriate branch condition to meet the precision requirement these two examples verify in under seconds finally the method computes one root of a quadratic equation using the wellknown more precise method from our tool compares the values error across different paths in real and implementation and succeeds in verifying the postcondition under s related work current approaches for verifying code include abstract interpretation interactive theorem proving and decision procedures which we survey in this section we are not aware of work that would automatically integrate reasoning about into a programming model abstract interpretation ai abstract domains that are sound with respect to floatingpoint computations can prove bounds on the ranges of variables the only work in this area that can also quantify errors is the tool these techniques use interval or affine arithmetic and together with the required join and meet operations may yield too pessimistic results improves the precision of by refining the input domains with a constraint solver our approach can be viewed as the problem from a different end starting with an exact constraint and then using approximation until the solver succeeds unlike ai tools in general our system currently does not perform widening to ensure convergence if the user can provide inductive postconditions then we can still prove the code correct but we do not in general discover these postconditions ourselves our focus is on proving precise bounds on the ranges in the presence of nonlinear computations and the quantification of errors and other theorem proving the tool generates a proof by the interactive theorem prover coq from source code with specifications it can reason about properties that can be reduced to reasoning about ranges and errors but targets very precise properties of specialized functions such as software implementations of elementary functions the specification itself requires and the proofs human a similar approach is taken by which generate verification conditions that are by various theorem provers has also done significant work on proving floatingpoint programs in the hol light theorem prover our approach makes a different on the precision vs automation tradeoff by being less precise but automatic the and interactive theorem provers can be used as to our tool if our tool detects that more precision is needed interactive tools can be employed by an user on selected methods the results can then used by our tool in the context of the overall program range computation the tool and most constraint solvers internally use interval arithmetic for sound range computations whose limitations are wellknown describes an arithmetic based on function and use an arithmetic based on taylor series as an alternative this approach is useful when checking a constraint but is not suitable for a forward computation of ranges and errors decision procedures an alternative approach to verification via range computation are floatingpoint decision procedures constraints however become very large quickly addresses this problem by using a combination of over and present an alternative approach in combining interval constraint solving with a algorithm and is a decision procedure for nonlinear real arithmetic combining interval constraint solving with an smt solver for linear arithmetic formalizes the for the format while these approach can check ranges on numeric variables they do not handle errors or other and cannot compute specifications automatically use a floatingpoint decision procedure to detect stability issues and while their approach provides witnesses of if such exist it is not able to prove sound error bounds with respect to a semantics prove fixedpoint constraints with a combination of bit vectors and reals but such an encoding is only possible for and less efficient than reals alone an alternative to our approach is using linear approximations to solve polynomial constraints we believe that such advances are largely orthogonal to our use of range arithmetic and complement each other testing symbolic execution is a wellknown technique for generating test inputs use a combination of search and interval constraint solving to solve the floatingpoint constraints that arise whereas combine random search and techniques test numerical code for precision by bits of values and rewriting expressions the idea is to initial errors and thus make more visible probabilistic arithmetic is a similar approach but it does the by using different modes also propose a testing produce to detect accuracy problems by code to perform a computation side by side with the regular computations while these approaches are sound with respect to floatingpoint arithmetic they only generate or can check individual inputs and are thus not able to verify or compute output ranges or their errors analysis combines abstract interpretation with model checking to check programs for stability by tracking the evolution of the width of the interval representing a single input use execution to find inputs which given maximum on inputs the on the outputs these two works however use a testing approach and cannot provide sound guarantees presents a framework for continuity analysis of programs along the mathematical definition of continuity and builds on this work and presents a sound analysis this framework provides a syntactic proof of for programs over reals and thus does not consider our approach describes a quantitative measure of for nonlinear programs with floatingpoint numbers and other and we believe that it can complement the framework conclusion we have presented a programming model for numerical programs that the mathematical problem description from its realization in finite precision the model uses a real data type that corresponds to mathematical real numbers the specifies the program using reals and indicates the target precision the compiler chooses a representation while checking that the desired precision targets are met we have described the soundness criteria by translating programs with precision requirements into verification conditions over mathematical reals the resulting verification conditions while a natural description of the problem being solved are difficult to solve using a art smt solver z we therefore developed an algorithm that combines smt solving with range computation our notion of soundness full inputoutput behavior of functions taking into account that due to conditionals small differences in values can lead to different paths being taken in the program for such cases our approach a sound upper bound on the total error of the computation we have evaluated our techniques on a number of benchmarks from the literature including benchmarks from and control systems we have found that invocation of an smt solver alone is not sufficient to handle these benchmarks due to scalability issues whereas the use of range arithmetic by itself is not precise enough by combining these two techniques we were able to show that a version of the code to the version with reasonable precision requirements we believe that our results indicate that it is reasonable to introduce reals as a data type following a list of previously introduced mathematical abstractions in programming languages such as unbounded integers and algebraic data types the feasibility of verified compilation of our benchmarks suggests that it is realistic to the verification of executable mathematical models over reals from their sound compilation we therefore expect that this methodology will help advance rigorous formal cation of numerical software and enable us to focus more on highlevel correctness properties as opposed to runtime errors alone furthermore we expect that having real numbers as a data type automatic reordering of computations as well as highlevel optimizations such as replacing one version of a numerical algorithm with another to achieve the desired combination of efficiency and rigorous worstcase bounds on precision references open source framework a and p to sample or not to sample control for nonlinear systems ieee transactions on automatic control a r majumdar i and p automatic verification of control system implementations in a and c verification of floatingpoint programs in d h y x s li and b and package f a and s a dynamic program analysis to find floatingpoint accuracy problems in pldi r e v and p an overview of the verification system verification by translation to recursive functions in scala workshop b p cousot r cousot j l a d and x a static analyzer for large software in pldi pages ­ m m s d and c s symbolic execution with interval solving and search in c s a e and a sat modulo linear arithmetic for solving polynomial constraints j automated reasoning a d and t mixed abstractions for floatingpoint arithmetic in pages ­ s s gulwani and r continuity analysis of programs in popl s s gulwani r and s proving programs robust in l chen a j wang and p cousot interval polyhedra an abstract domain to infer interval linear relationships in sas e and v numerical computation in scala in oopsla e and v solutions for numerical constraints in rv e v r majumdar and i synthesis of fixedpoint programs in f de c and g the floatingpoint implementation of an elementary function using ieee trans comput l h de and j numerical methods and applications l de and n z an efficient smt solver in tacas d e s j k and f towards an use of on software in v l d and m numeric bounds analysis with learning in tacas j a and m polynomial function and floating point software verification in j static analysis of digital filters in esop s m f a gupta s and e clarke integrating and solvers for deciding nonlinear real arithmetic problems in k e and s a logical product approach to intersection in cav d what every computer should know about floatingpoint arithmetic acm comput e s and f modular static analysis with in sas l a m and d deciding floatingpoint logic with systematic abstraction in j floatingpoint verification using theorem proving in formal methods for hardware verification f m s and a gupta numerical stability analysis of floatingpoint computations using software model checking in b and a a library of numerical abstract domains for static analysis in cav j w and d computation of in d and l de solving nonlinear arithmetic in w area and of a technical report university of california berkeley v j s and g efficient state merging in symbolic execution in pldi k n m and j de floating point constraint solving for symbolic execution in testing software and systems springer berlin x leroy verified does critical software verified tools in popl m d m ho d l t h and g p towards program optimization through automated analysis of numerical precision in r majumdar i and z wang systematic testing for control applications in k and m taylor models and other validated functional inclusion methods international journal of pure and applied mathematics a relational abstract domains for the detection of floatingpoint runtime errors in esop r moore interval analysis prenticehall j d mathematical springer m odersky l and b programming in scala a guide g and w verifying in floatingpoint programs by increasing precision using smt solving in o c and m refining abstract interpretation based value analysis with constraint programming techniques in cp a f and p scientific computing with and springer rd edition p and t an theory of binary floatingpoint arithmetic in informal proceedings of smt at n scott f c and jm numerical check for scientific codes the approach computer communications i c society ieee standard for floatingpoint arithmetic ieee e e x li and z su numerical for statistical analysis of floatingpoint program in e m and s a semantics for approximate program transformations c and c numerical methods with examples volume nd springer 