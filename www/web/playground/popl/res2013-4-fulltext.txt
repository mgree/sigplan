cache and io efficient functional algorithms e robert harper carnegie mellon university abstract the widely studied io and models were developed to account for the large difference in costs to access memory at different levels of the memory hierarchy both models are based on a two level memory hierarchy with a fixed size primary memory cache of size m an unbounded memory organized in blocks of size b the cost measure is based purely on the number of block transfers between the primary and memory all other operations are free many algorithms have been analyzed in these models and indeed these models predict the relative performance of algorithms much more accurately than the standard ram model the models however require specifying algorithms at a very low level requiring the user to carefully out their data in arrays in memory and their own memory allocation in this paper we present a cost model for analyzing the memory efficiency of algorithms expressed in a simple functional language we show how some algorithms written in standard forms using just lists and trees no arrays and requiring no explicit memory layout or memory management are efficient in the model we then describe an implementation of the language and show provable bounds for mapping the cost in our model to the cost in the ideal cache model these bound imply that purely functional programs based on lists and trees with no special attention to any details of memory layout can be as asymptotically as efficient as the carefully designed imperative io efficient algorithms for example we describe an o n b n b cost sorting algorithm which is optimal in the ideal cache and io models categories and subject descriptors d programming languages formal definitions and theory f analysis of algorithms and problem complexity tradeoffs and complexity measures f logics and meanings of programs semantics of programming languages general terms algorithms design languages performance theory keywords cost semantics io algorithms introduction on computers there is a difference in cost for accessing different levels of the memory hierarchy whether it be registers permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ italy copyright c acm one of many levels of cache the main memory or a disk on current processors for example there is over a factor of a between the time to access a register and main memory and another factor of a or so between main memory and disk even a state disk this variance in costs is contrary to the standard random access machine ram model which assumes that the cost of accessing memory is uniform to account for non several cost models have been developed that assign difference costs to different levels of the memory hierarchy the widely used io and models both assume a two level memory hierarchy with a fixed size primary memory cache of size m an unbounded memory partitioned into blocks of size b cost is measured in terms of the number of block transfers between primary and other operations are considered free the parameters m and b are considered variables for the sake of analysis and therefore show up in asymptotic bounds algorithms that do well in these models are often referred to as io efficient or cache this paper we will use the term cache efficient the theory of cache efficient algorithms is now well developed see eg the and the models indeed much more accurately capture the relative cost of algorithms on real machines than the ram model does this is true both in the context of algorithms that must run off disk when there is not enough main memory and also in the context of algorithms that can fit in main memory but not in various levels of the cache for example the models properly indicate that a blocked or hierarchical multiply is much more efficient than the nested loop n vs n in the ram bm they have equal costs the models also indicate that properly implemented versions of and quicksort are reasonably cache efficient but that and are more efficient and in fact optimal all the disk sorts indeed use some variant of or as the theory although the study of cache efficient algorithms has been very successful in identifying algorithms that are fast in practice not surprisingly designing and programming algorithms for these models requires a careful layout of memory and careful management of space both temporal and spatial locality is critical in achieving good bounds spatial locality is important since memory is moved in blocks of size b corresponding to either cache lines or memory pages for example although merging two arrays of integers is reasonably efficient the cost of merging two linked lists will depend on how the links are out in memory and needs to be considered with care care is also needed when allocating and memory since unused memory a cache miss it is therefore important to reuse space immediately rather than returning it to a pool which might be by the time it is generic memory allocator or garbage collection scheme will likely not do the right thing to properly this problem memory is typically and fully by the designer to say this form of programming is inconsistent with functional programming especially when using recursive data types such as lists or trees however it is known that by using certain standard memory allocation schemes purely functional programs no side effects can be reasonably cache efficient with to both spatial and temporal locality we give two examples consider applying map with some simple function eg increment over a list of integers and then applying the same map to the output if the allocator keeps a pointer that gets on each allocation then after the first map all the cells of the list will be allocated on the second map since the allocations are adjacent reading the whole list will only cache misses where b is the block size and the newly generated blocks will also only misses this gives cost which asymptotically matches the cost of an optimal array version in an imperative setting if the list were in an arbitrary order the cost would be on all we have done is noted that the temporal locality of the allocations will lead to spatial locality of how they are out in memory secondly consider a block recursive matrix multiply on two n × n matrices such an algorithm will never require more than on live space but if recursion stops at problems of a constant size it will allocate a total of on space assuming that the maximum live space within the cache we should be able to run our matrix multiply with only cache misses needed for loading the two matrices and storing the result but this would require being careful about space that is already in cache fortunately generational garbage collectors have approximately this effect in particular if we make the first generation smaller than the size of the cache m then we will the memory whenever the allocation area and reuse memory that is already in cache this does not quite work in general since what is live at the time of the minor collection might get from cache but it gives some that it is not to make the natural recursive matrix multiply algorithm as well as similar recursive algorithms cache efficient we show that one can indeed implement algorithms in a callbyvalue functional setting using recursive data types and get provably efficient bounds on cache complexity in particular we show that one can express algorithms at a high level using standard techniques and achieve optimal asymptotic performance when implemented on the ideal cache of course we do not expect the algorithm designer to understand the the garbage collector works in order to analyze their algorithm instead our approach consist of providing a reasonably highlevel cost semantics that abstracts away from implementation details such as the garbage collection method but still admits precise analysis of the cost of an algorithm on a twolevel memory architecture we then describe a implementation of the language on the model we show that by using this implementation the costs analyzed in the highlevel cost model asymptotically match the number of cache misses in the underlying model the general idea of using highlevel cost models based on a cost semantics along with a provable efficient implementation that maps the cost onto a lower level machine model has previously been used in the context of parallel cost models our highlevel cost model consists of an operational semantics for a callbyvalue variant of pcf in which we make explicit the allocation of and access to data objects the store consists of three parts a main memory an allocation cache and a read cache both caches have size m and the memory is organized in blocks of size b both measured in terms of abstract data objects data can from the allocation cache to memory and from memory to the read cache always in blocks of size b allocations are made in the allocation cache and if the number of live objects in the cache m then the b locations are to memory as a block having unit cost the read cache contains a subset of the memory blocks a read has no cost if its location is in the read or allocation cache otherwise it requires loading a block from memory into the read cache having unit cost and possibly an existing block hence the only costs are for a block from the allocation cache or loading a block into the read cache since we are only concerned with the traffic between main memory and cache memory garbage collection for main memory is not modeled but we do account for the detection of live objects and their migration to main memory when the cache limit is the provable implementation uses a generational collector to maintain the allocation cache it uses a of size m and al until the space runs out it then traces the for the live data if there is l m live data then l m locations are written to memory in blocks of b leaving the with at most m tions the implementation allocates the stack in the heap and must the cost of loading old stack frames against other opera tions since they are not modeled in the highlevel cost semantics we emphasize that the algorithm designer need not know anything about the garbage collector or how the stack is to their algorithm these concepts are only part of the provable implementation the cost model is described in section and the provable implementation is described in sections and to demonstrate the utility of our approach in section we describe some general techniques for analyzing the cost of algo in our model and show three examples of how to analyze the cost of algorithms in the model and matrix multiply importantly our results on sorting and matrix multiply match the bounds for algorithms implemented directly in the model o n b n b and o n bn the bounds for sorting are optimal because of our provable implementation bounds these results imply that on the model our algorithms written in a functional style using lists and trees are asymptotically as efficient as the lowlevel imperative pro grams to analyze the algorithms we introduce the notion of a data structure being compact with respect to a traversal order this is the way we capture the spatial locality of data structures in a language that has no explicit way to express memory layout related work although there has been a large amount of experimental work on showing how good garbage collection can lead to efficient use of caches and and many references in we know of none that try to prove bounds for algorithms for functional programs when manipulating recursive data types such as lists or trees et al show how a functional style can be used to design cache efficient graph algorithms they however assume that data structures are in arrays called lists and that primitives for operations such as sorting map filter and reductions are supplied and implemented with optimal asymptotic cost at a lower level using imperative code their goal is therefore to design graph algorithms by composing these highlevel operations on collections they do not explain how to deal with garbage collection or memory management background io and caching models the twolevel io model of and assumes a memory hierarchy consisting of main memory of size m and an un bounded memory both memories are partitioned into blocks of size b of consecutive memory locations all computation must be performed from main memory which is treated like a ram but there is an additional instruction for moving a block of memory from memory to main memory and one for moving the other way the cost of an algorithm is analyzed in terms of the number of block cost of operations within the main memory is ignored many algorithms can be analyzed in this model and it is perhaps surprising how accurately it is able to capture the relative performance of algorithms in their original work for example and showed tight upper and lower bounds for sorting n keys with io cost n b n b the two algorithm that match this bound are a merge sort and a distribution sort which are the standard algorithms used for disk based sorting and they both perform significantly better than quicksort or standard these algorithms are more efficient since they do not need to pass over the data as many times the io model can capture either the distinction between cache and main memory or between main memory and disk in the first case the memory size corresponds to the cache size and the block size to the size and in the second case the memory size corresponds to the main memory size and the block size to the page size or whatever the transfer size between the disk and main memory is one might note however that while the io model assumes two address spaces and the user explicitly moves data between them a machine with caches assumes a single address space and makes its own decisions about what gets from cache eg using a least recently used policy the model can be used to better model a cache it is similar to the io model but assumes the primary memory is treated as a cache with an ideal policy in particular the programmer only accesses one address space and a block is into the cache when a memory location is accessed whose block is not already in cache in a block might require another block from the cache the model assumes that the best decision is always made which is to the line used in the future the optimal offline replacement policy since in practice we dont know the future this is not possible online but it is proved by and work on paging that an policy is always with the optimal strategy within constant factors in time and cache size therefore from a theoretical point of view the models are asymptotically the same in this paper we will be using the ideal cache model for simplicity although the results are also apply to the io model the model is often used in the context of cache algorithms these are simply algorithms for which the algorithm does not make any decisions based on the cache m and b although of course the analysis of cache complexity will depend on m and b the advantage of algo is that since they are to the cache parameters they work across multiple levels of a cache hierarchy simultaneously most of the algorithms in this paper are cache but our is not we leave it as an open question whether it is possible to develop an sorting algo rithm in our model cache efficient algorithms we now review some basic well known results on cache efficient algorithms in the imperative setting the functional algorithms we present in section are based on the algorithms described here and also considered a version of the model with parallel disk access but most interesting results are explained with the single disk version but do not require arrays or explicitly memory management we first consider throughout our discussion we assume that the elements being sorted each fit in a single machine word all cache efficient algorithms we know of for sorting store the input and output elements directly in arrays first consider merging two arrays of keys a and b into an output array c of length n as usual we assume the inputs are sorted in a and b in increasing order the standard sequential algorithm for merging starts at the beginning of each array keeping a on each finding the the of the two keys at the copying this key to c and the appropriate this algorithm has a cache complexity as long as m b this can be seen by noting that at any given time we only need one block from each of a b and c in cache and that we fully process the block before the next block therefore every block is only needed once for we assume the standard version which recursively sorts each half of the array and then merges the result since merging as described cannot be done in place we have to be specific on how to memory in particular allocating a new array for the result and then the two old arrays using a general purpose memory allocator will likely not lead to the desired bounds unless one can ensure special properties of the memory allocator instead the algorithm needs to a temporary array of length n and pass parts of this array to all in particular could take as arguments both the input array and an equal length temporary array the result is returned in the input array and the temporary array is used to merge into although these optimizations are relatively obvious and standard to programmers of imperative code we bring them up to emphasize the care that needs to be taken to ensure the cache is not simply an issue of reducing the number of calls to the memory allocator it can actually asymptotically affect the cache bounds the cache complexity of this can be analyzed by considering two cases the first is when the full computation in cache in this case the two arrays need only be loaded into cache once and all the work can be done in cache the problem in cache as long as n log n m where the log n accounts for the stack size the second case is when the problem does not fit in cache in this case we have to pay for the cache misses on the two recursive calls plus the cache misses of the merge this gives the following recurrence for the cache complexity qn qn o n b n log n m otherwise the solution to this recurrence can be derived by noting that the top levels of the recursion do not fit in memory while the lower levels do the total cache complexity across each of the upper levels is so the total overall cache complexity is we note that this does not match the optimal cache complexity for sorting but is significantly better than simply assuming every access is a cache a factor of b log n log m better for sorting words in a memory with words and a block size of words it is about a factor of about better quicksort has basically the same com as although in the expected case this is because scanning the input array to split it into the and larger ele ments can be done using two like in merging so again each block only needs to be loaded once we now describe a sort that is optimal for the io model the idea is instead of partitioning the input array into two and recursively calling sort on each to partition the input array into k parts sort each part and then merge all the parts since instead of having just two arrays to merge we have k arrays we require a merge without going into too much detail such a merge can be implemented using one block of memory for each of the inputs to be merged as well as one block for the output we keep a on each input and on each step select the minimum key at the move it to the output buffer and increment that as long as all input blocks the output block and any data for maintaining the fit in cache then the merge will run with cache complexity which is the same as the binary merge since there will be k input blocks and output blocks the space needed for the blocks is k b therefore for overheads everything will fit in cache as long as m or equivalently k for some constant c we therefore pick k to be as large as possible giving k as in the two way merge we need to be careful about allocation and temporary arrays to copy the output we again can analyze the algorithm by considering the case when the problem in memory and when it does not this gives the recurrence qn m b q n o n b nc m o n b otherwise where c and c are constants but n b and m are variables this solves to this bound matches the lower bound for sorting in the io model and hence also the model the is therefore asymptotically optimal cost semantics in this section we define an evaluation dynamics that assigns a cost to a complete execution of a program following the io model the cost measures the cache complexity which is defined to be the traffic caused by the transfer of objects between the main memory and the memory cache accesses to objects in cache are considered to be whereas migration of objects from cache into memory and from memory into the cache are unit cost the dynamics is based on a twolevel model of storage that includes a allocation cache and a read cache together with a main memory of unbounded size the evaluation dynamics provides the basis for both the correctness and the cache complexity of programs it is formulated at a sufficiently abstract level to free the programmer from having to reason directly about the compiler and runtime system but is sufficiently concrete as to admit an implementation with a provable bound on its cache complexity thus we may achieve the same overall results as are obtained using only lowlevel machine models in previous work on io algorithms while working at the much more practical level of abstraction by functional programming languages we give the dynamics of a callbyvalue variant of pcf language the syntax of expressions is summarized by the following grammar e x z se e xe ye e the conditional tests whether a number is zero or not and passes the predecessor to the nonzero case functions are equipped with a name for themselves to allow for recursion the typing rules are standard and are omitted here for the sake of see for example chapter of for purposes natural numbers are treated as data structures of unbounded size as will become evident it is straightforward to extend the language to account for a richer variety of data structures including sum product finite sequence and recursive types and to account for typical concepts such as machine words and floating point numbers storage model following morrisett et al the dynamics distinguishes large from small values with large values being allocated in memory and represented by a location and small values being those that are manipulated directly in the present case the only small values are locations but it is also possible to consider for example numbers as forms of small value all other forms of value numbers and functions are large we also allocate stack frames which the control state of evaluation in memory a memory object is either a large value of a stack frame the twolevel memory model is parameterized by two constants the block size b and the cache size m c × b determined by some constant c representing the number of blocks in the cache a memory µ is a finite mapping assigning a memory object to each of a finite set of abstract locations the memory may grow without bound we do not consider here the separate problem of garbage collection for main memory for which see morrisett et al as a technical convenience we assume that locations are divided into two classes value locations l and stack locations s and require that a memory map value locations to large values and stack locations to stack frames when the distinction is we simply of locations and objects in memory a memory µ comes equipped with an equivalence relation l µ l over specifying that l and l are neighbors in µ additionally we require that each equivalence class in the domain of a memory is of size b a memory whose domain consists of a single equivalence class of size b is called a block the l of a location l is the restriction of µ to the neighbors of l in µ a single block the expansion µ of a memory µ by a block such that dom is the memory µ that with µ and on their respective domains and for which l µ l iff l µ l or l l there are two forms of cache access to memory a read cache for a memory µ is the restriction of µ to a finite set of locations of size at most m the contraction of a read cache by a block is the read cache such that a is a finite mapping that associates an object to each a finite set dom of locations a comes equipped with a linear ordering l l of dom called the allocation ordering if l l we say that l is than l and that l is than l in the extension l o of a binding a location l to an object o is the such that l o and l l for each l dom and l l for every l dom the contraction of a by a block is the restriction of to dom dom the live locations in a relative to a subset r dom consists of those locations in dom that are reachable from locations in r the scan of a with respect to a subset r dom is the block of consisting of the b live locations in see morrisett et al for formal definitions of these standard concepts it will be an invariant of the dynamics that the contains at most m live objects relative to the roots of the computation a store is a triple µ consisting of a memory µ a read cache for µ and a such that the domain of a store is defined by dom dom an initial store is a store in which the main memory contains only large values and in which the read cache and allocation area are empty evaluation dynamics the overall goal of the evaluation dynamics is to define the evaluation of a closed expression by an inductive definition of a relation between an expression and its value which is always small and its cost a nonnegative integer the cost is computed by tracking the z nr l z nr l a s rn s e l sl rn l se n l e xe s e l l n z e nr l e xe n n n r n l e xe s e l l n sl l xe rn l e xe n n n r n l ye nr l ye nr l b c d e app e s e l l n ye rn s e l l lx ye rn l e n n n r n n n l f figure cost dynamics l dom µ l µ l l dom µ l µ l l dom dom dom m b µ l µ l l dom dom dom m µ l µ l a b c d m l dom µ o r µ l o l a m l dom µ o r µ l o l b figure reading and allocation movement of objects among the components of the store one unit of cost whenever a block of objects must be moved to or copied from main memory and zero cost otherwise so for example a computation that runs entirely in cache will be assigned zero cost consistently with the io model to account for the memory traffic involving values the dynamics makes explicit the allocation of objects in the their to main memory when the of the is and their movement into the read cache as they are required by the computation to account for the memory traffic to the implicit control stack the dynamics also allocates but does not otherwise use stack frames and ensures that any data that would appear in the stack is kept live by the dynamics these considerations lead to the evaluation judgment e nr l stating that the expression e when evaluated with respect to a store such that dom and to roots r dom results in a modified store a location l representing the large value of the expression and a cost n representing the cache complexity of the execution the modifications to the store consist of allocations in the of objects from the to the main memory and copying of objects from the main memory to the read cache all memory traffic occurs in blocks of b objects corresponding to loading a cache line or reading a block from disk the roots r represent locations that are to be kept live by of their being present in the implicit control stack or expression under evaluation the evaluation judgment is defined by the rules in figure making use of two auxiliary judgments for reading and allocating objects defined in figure it may be helpful to read through the rules once while ignoring all but the evaluation judgments to see that the rules define a conventional eager dynamics for a functional language on such a reading the root set plays no role and can be ignored moreover the cost assignment has no under such a simplification next let us consider the roles of the read judgments l n o and the allocate judgments v rn l where v is a value in the dynamics the read judgment states that the result of reading location l in store results in the object o and the modified store and has cost n or n the cost is nonzero only if the read causes a block to be loaded into the read cache the modified store represents the possible effect of loading a block into the read cache the write judgment states that allocating the large value v in store results in a modified store and location l dom and has cost n or n the cost is nonzero only if the allocation causes the of a block from the in order to maintain the invariant the read and allocate operations in the dynamics record the memory traffic by the creation and of values during computation it remains to consider the role of the allocation judgments of the form s rn f which represent the allocation of a stack frame in the store at stack location s the purpose of allocating these frames is purely to ensure that the cost assigned to a computation is accurate with respect to the underlying implementation although an evaluation semantics has no explicit control stack it is nevertheless the case that an implementation must allocate space for the representation of the control state and this space allocation does influence the cache behavior of the computation it may not therefore be ignored our method for for the memory effects of the control stack is to allocate explicitly frames that would appear in the control stack to ensure that space usage is properly for and that required liveness information to be detailed is properly maintained the frames are denoted as app e and in the cost dynamics with this in mind let us examine in detail rule f in figure we are to evaluate and determine the cost of e in store with given roots r first we allocate a stack frame s representing the pending evaluation of e during the evaluation of e this frame is now considered live even though it does not appear in any expression under consideration accordingly we evaluate e relative to the store containing this frame treating the stack pointer to be live as indicated by rs this results in a location l which we then read from the store to obtain a function abstraction as would be guaranteed by the static type discipline omitted here we then create another frame s corresponding to the suspended application of the function at location l and evaluate e with this stack pointer considered live as indicated by rs to obtain location l finally we evaluate the function body replacing the self variable by l and the argument by l the overall cost of the computation is the sum of the costs of each of these steps which are given either inductively or by the uses of the read and allocate judgments observe that this rule properly accounts for tail recursion in that no extra space is held during tail recursive calls as indicated by r it remains to explain the read and allocate judgments defined in figure the read judgment assigns cost zero to any read from a location in either the or the read cache rules b and a such reads have no effects and hence induce no cache traffic a read of a location that is only in main memory induces a load of the of that location a block of memory into the read cache if there is sufficient room for it in the read cache the block is added to the cache and the contents is returned at a cost of one unit rule c if there is room in the read cache a block is selected nondeterministically to be replaced by the required block and once again a unit cost is to the read rule d at the end of the section we discuss the use of nondeterministic the allocate judgement defines the procedure for creating new objects in the store of course new objects are considered in the allocation ordering than the objects already present in the if the new object within the it is allocated there at zero cost rule a if the new object will not fit within the then the block consisting of the b live objects in the is to main memory making room for the newly allocated object such an allocation is unit cost rule b it is important to our method that the objects be from the cache as a block forming the of each of its locations whether an object within the is determined as follows the is full if the number of live objects in it is exactly m it is for the sake of liveness that the allocation judgment is parameterized by a root set of a block reduces this to at most m b objects so that the next b allocations will not cause an thus we are in effect at most b units of cost to each allocation less if objects before to be it is essential to our results that the liveness of objects in the may be without accessing main memory given roots r we need only trace objects in the itself and need never consider locations outside of it this is by two properties of the dynamics first since the model is purely functional the dependency graph of objects in the is acyclic an object may only refer to objects allocated earlier in the computation as defined by the allocation ordering second implicit stack frames are explicitly allocated in the to ensure that liveness may be solely by examining the itself starting from the root set put another way an object in the cannot be live solely because of a pointer from main memory back to the this property is a consequence of and the explicit allocation of stack frames in the semantics in section we will make use of a deep copy operation on values of certain types in the language considered here this operation is definable on natural numbers as follows z x x calling this function on a number n has the effect of creating a fresh copy of n in the heap no such operation is definable or required for function types deep copying is easily extended to product sum and inductive types but would need to be provided as a primitive for base types such as fixed precision integers or floating point numbers discussion we briefly discuss some of the motivation for the decisions we made in the dynamic semantics the overall goal is to allow a simple analysis for the algorithm while capturing all the costs needed to prove asymptotic implementation bounds the separate allocation cache is important both for convenience of analysis and properly for costs it ensures that all short allocations never need to be allocated to memory for a in which the maximum footprint of live data allocated in the allocation cache the user need not about any costs for any temporary memory in a block matrix multiply on n × n matrices for example once kn m for some small constant k the only cost that needs be considered is the cost of reading the input and the output this is the case even though the multiply will allocate a total of n space it is also important that the partitioning of locations into blocks is not decided until locations are from the allocation cache which ensures that only live data is ever to memory if blocking were to be decided on allocation for example then by the time the objects are most of the objects in a block may no longer be live this would break the bounds we give in section the cost semantics accounts for the allocation of stack frames in order to account for the space required to the control state of evaluation this is particularly important in the case that no allocation is associated with the creation of a frame for then there is no possibility to the space required for the frame against the allocated object note that the semantics only models the space taken by the frames in the allocation cache and the cost of them it does not model any costs associated with them into the read cache as described in the next section in a lower level model this can be against the cost of the frames in the first place it is important that the stack frames be heap allocated a crucial invariant we require is that all live data in the allocation cache can be determined solely through the caches if we had a separate stack cache it could allow for the of a stack frame that references data in the allocation cache the required invariant there are other techniques to handle this problem but we found that allocating the stack frames in the heap is the our model is nondeterministic in the choice of what block is from the read cache in the case of a read miss in our provable implementation bounds we show that if there is a nondeterministic execution that gives certain cache complexity then we can guarantee those bounds on the ideal cache model within constant factors when analyzing an algorithm this allows one to consider any policy for this is possible because the ideal cache makes the optimal decisions and will therefore be at least as good as the policy the user assumes the justification for the ideal cache model is given in section abstract machine the abstract cost of a computation assigned by the evaluation dynamics given in the preceding section is validated in two stages first in this section we define an abstract machine with an explicit control stack and show that the evaluation dynamics accurately the behavior of the abstract machine with respect to both the outcome and the cost of the computation second in section we show how to implement the basic operations of the abstract machine with only a small overhead taken together these two arguments demonstrate that the evaluation semantics provides an accurate model of the cache complexity of a program when implemented as described in these two steps the abstract machine takes the form of a labeled transition system between states of two different forms evaluation state k e where k dom is a stack pointer and dom stating that e is to be evaluated on stack k relative to store return state k l where k l dom stating that small value l is to be returned to stack k relative to store the control stack is represented by a stack location k that refers to a linked list of frames either the empty stack written · or a frame together with another stack location written f k the label on a transition is either or and specifies the amount of work to be for that transition the rules given in figure define the abstract machine their overall form is standard see for example chapter of with the main differences being that allocation and reading of values is made explicit just as in the evaluation dynamics and that the stack is explicitly represented as a linked data structure in the store the transition judgment s n s means that there is a finite possibly empty sequence of transitions from s to s whose labels sum to n theorem correctness of evaluation dynamics let be an initial store let e be a closed expression such that dom let the abstract machine be equipped with one additional block in the read cache and let k be a stack location not used in the evaluation dynamics if e n l then there is an evaluation k · k e m k · k l such that the results are isomorphic l l and the cost m is at most n the relation l l states that the reachable graph from l in is isomorphic to the reachable graph from l in theorem states that the outcome of a computation on the abstract machine is the same up to choice of locations as the outcome of the same computation according to the evaluation dynamics moreover the total cost of the machine execution measured in with the io model described earlier is at most a small constant factor larger than the cost assigned by the evaluation dynamics the content of the theorem amounts to a proof that the space required by the control stack in a computation may be so as not to interfere with space usage of the computation itself the correctness proof may be decomposed into three major components the first obligation is to relate the outcome of the evaluation dynamics to that of the abstract machine for the moment the cost the required correspondence is proved k l k l z nk l k z n k l sk k k se n k e a b c k n sk sl nk l k l nn k l d e k k e xe n k e e k n e l n z k l nn k e f k n e l n sl k l nn k l xe ye nk l k ye n k l g h app ek k k e n k e i k n app ek k k k l nn k e j k n k l n ye k l nn k l lx ye k figure abstract machine by induction on the derivation of evaluation judgment specifically we prove that if e nr l then for any stack pointer k k e k l the proof proceeds along standard lines as described for example in chapter of the second authors the same choice of locations may be made in the machine derivation as were made in the evaluation derivation because the sequence of value allocations is precisely the same in both forms of dynamics the next step of the proof is to show that the abstract machine performs the same sequence of value reads in the same order as specified by the evaluation dynamics this may be proved along with the correspondence described in the preceding the argument relies on two important properties of the evaluation dynamics any read of a value location is either a read of a location in the initial store or a location that was allocated earlier in the evaluation stack frames are allocated but never read in order to ensure that of blocks from the occurs in exactly the order imposed by the abstract machine a deterministic policy is required to ensure that the memory reads correspond exactly between the evaluation dynamics and the abstract machine we can assume whatever policy is used the the dynamic semantics is also used by the abstract machine it remains to show that the stack reads employed by the abstract machine do not impose an asymptotically significant cost beyond what is by the evaluation dynamics without special access to the control stack would interfere with the allocation of data in the read cache the cost given to the computation by the evaluation dynamics to avoid this we make use of a read cache block in the abstract machine which we will call the stack cache block and explicitly this cache block as follows whenever a stack location is read from main memory its is loaded into the stack cache block the block that currently it we will argue that the cost of loading the stack cache can be across the execution sequence even if the same block is loaded into the stack cache more than once a possibility that will be detailed the validity of the argument depends on two special properties of the runtime stack namely that each allocated frame is read exactly once in a complete computation and that the preceding stack frame is always than the current one given such an it is then clear that the overall cost of execution on the abstract machine is bounded by a small constant factor of the cost to it by the evaluation dynamics establishing the theorem to complete the proof we describe the of the cost of stack management in more detail as an invariant we put a on every memory block that contains a stack frame except if it is the such block and in the stack cache block in which case it has no associated with it when the abstract machine a block containing a stack frame from the allocation cache we three for the itself one to put a on the block and one to put a on the block that is in the cache stack block this third might be needed to maintain our invariant since that block if there is one will no longer be the memory block containing a stack frame now when the abstract machine loads a block into the stack cache block from memory we its for the load all blocks with frames have a on them already by the invariant so the invariant is maintained in summary we block transfers worst case per block that is from the allocation cache we finally note that there is no need to explicitly maintain the stack cache block in the abstract machine semantics since we are assuming an ideal cache therefore as long as the cache has an extra block available then the cache policy will do at least as well as the one we described provable implementation in this section we describe an implementation of the abstract machine given in section in the ideal cache model with the same asymptotic cost the efficiency proof for the implementation takes account of two issues that are treated abstractly in the evaluation semantics and in the abstract machine the main issue is how to implement the allocation judgment defined in figure rules a and b make reference to liveness of the data and a block consisting of the b live objects in the to ensure that the costs are realized in practice we must argue that these conditions can be met by an implementation the second issue is that we must account for the size of the stored objects values and frames that may appear in a computation and account for the cost of handling these objects in an implementation define the size of a machine state k e be the sum of the size of e and the size of any function in this may be thought of as the size of the program including any abstractions that may be present in the initial store theorem fix an initial state k e of size s and consider a complete computation k e m k l with s objects in the final store this computation be simulated in the ideal cache model with cache complexity for some constant c provided that words are of size at least d s for some d and that the cache has at least m b × s words the constants c and d are independent of k and e theorem states that the implementation asymptotically the work attributed to the computation by the evaluation semantics and hence the algorithm analysis performed using that semantics the requirement on the word size in theorem ensures that all objects are by a pointer and accounts for the sizes of the objects themselves in storage a closure can be as large as the initial program the requirement on the cache size in theorem ensures that we may implement the abstract memory hierarchy with no more than a small constant factor of overhead in a manner that we now describe the b × s additional words account for the stack cache described in section it remains to discuss the implementation of allocation the allocation judgment defined in figure relies on an of the live size of the and on the of blocks from the to ensure that the contains no more than m live objects as we note earlier the liveness of data in the may be without reference to the main memory the liveness computation takes place entirely within the cache rather than liveness possibly a block on each allocation we instead these costs across multiple allocations according to the following strategy we m × s words of cache memory for the allocation area to accommodate at least m objects objects are allocated by maintaining a pointer into the area it on each allocation until m objects have been allocated at which point the space is when this occurs we perform a garbage collection that preserves the allocation order of objects simultaneously as many blocks as necessary to obtain a live size of m objects in the after compaction allocation continues as before until the is again as long as there is sufficient space allocation takes constant time when a garbage collection is required the cost may be attributed to the allocations of the live data in the so that in an sense garbage collection is it is easy to see that no object is to main memory using this implementation that would not have been in the abstract sense however the will in general happen later than by the semantics as a result fewer objects may be live at the time of and so fewer blocks overall may be moved to main memory as a result of this compression effect two locations that were neighbors in the evaluation semantics may be in two different blocks in the implementation to account for this two blocks must be loaded to ensure that objects in the semantics are loaded into the read cache together thus we require m × s words of cache in the ideal cache model to account for the m objects in the read cache with to the policy from the read cache we note that an ideal cache will always choose an optimal policy in the future it will therefore do at least as well as any policy assumed by the abstract machine this completes the proof of the implementation bound stated in theorem io efficient algorithms we now describe algorithms analyzed in the model and prove bounds on their cache complexity in particular we will consider and a recursive block multiply we will show that the and the multiply analyzed in our model match the best bounds for the furthermore the implementations are completely natural functional programs using lists and trees instead of arrays before describing these algorithms we discuss some general issues and techniques that will be important in the analysis for simplicity the semantics described in section only defines natural numbers in the discussion in this section we assume the semantics have been augmented with some basic types including base types that fit in a machine word machine integers and boolean sum types product types and recursive types made from products and sums as with the ram and io models we assume that for an input of size n that machine words can store n bits hence a machine integer will be bounded by nk for some constant k since sorting and matrix multiply can be defined as higher order polymorphic functions we need to be careful about the size of the element type and cache complexity of the element function when analyzing overall cache complexity for this purpose we define the notion of a finite hf values at the base any value of basic types that fit in words are hf inductively any sums or products of hf values are hf a value of function type is hf iff for every hf argument of the domain type the function yields a hf argument of the range type using only constant space in the process in sorting we assume the elements themselves and the comparison function are finite in matrix multiply we assume the elements addition and multiplication functions are finite it is important that the data structures that are traversed by our algorithms are out in an order that makes accessing them efficient in our model the only way to control the layout of data structures is to allocate them in the desired order we could try to define the notion of a list being in a good order in terms of how the list is represented in memory this is and low level we could also try to define it with respect to the specific code that allocated the data again this is instead we define it directly in terms of the cache complexity of traversing the structure by traversing we mean going through the structure in a specific order and reading all data in the structure since types such as trees might have many traversal orders the definition is with respect to a particular order eg preorder for this purpose we define the notion of compact definition a data structure of size n is compact with respect to a given traversal order if traversing it in that order has cache complexity in our cost semantics with m kb for some constant k we can now argue that certain code will generate data structures that are compact with respect to a particular order to keep data structures compact not only do the top level links need to be accessed in approximately the order they were allocated but anything that is by the algorithm during traversal also needs to be accessed in a similar order for example if we are sorting a list it is important that in addition to the cons cells the keys are allocated in the list order or as we will argue reverse order is fine to ensure that the keys are allocated in the appropriate order they need to be copied whenever placing them in a new list this copy needs to be a deep copy that copies all components if the keys are machine words then in practice these might be into the cons cells by a fact fun hr let val touch h in r end fun hr let val r r val touch h in r end fun map f map f hr r fun map f map f hr let val r map r in end figure traversing a list in two orders and examples of map that use each of the orders fun less less a a less a let fun case ab of b b a a if then else val lh split a in end figure code for optimizing compilers such as can even inline product and sum types however to ensure that objects are copied we will use the copy operation described in section writing a to indicate copying of a although it might seem that there is only one canonical way to traverse a list there are actually two in the first all the elements of the list are visited on the way down the recursion and in the second the elements are visited on the way back up the order in which the elements are visited is the two versions are illustrated by the code in figure fortunately if an algorithm is compact for one traversal it is compact for the other this is because to be compact under either traversal requires that adjacent elements are allocated in the same block in memory and hence will be efficient in both directions the fact that the orders are effectively equivalent is important since it means the model is quite robust relative to programming for example the two implementations of the map function shown in figure will both be efficient if the list is compact with respect to either traversal furthermore the output is compact with respect to either traversal this is true even though in the first case all elements are allocated first and then all cons cells while in the second case they are interleaved sorting we now consider analyzing sorting in our cost model we first consider we assume a purely functional version of on lists as shown in figure we will not cover the definition of split since it is similar to merge note the only difference from a standard is the use of the copy before the ah and bh as discussed earlier this is important to ensure the result of the merge is compact we note that for a list to be compact all its elements must be constant size the interesting aspect of the code is that the cache complexity of this code in our model and hence when mapped onto the ideal cache using the implementation in section matches the bounds for the array version discussed in section to analyze the we first analyze the merge theorem for a hf function less and compact lists a and b the evaluation of merge less a b starting with any cache state will have cache complexity o n b n a b and will return a compact list as a result proof we consider the cache complexity of going down the recursion and then coming back up since a and b are both compact we need only put aside a constant number of cache blocks to traverse each one by definition recall that in the cost model we have a that maintains both live allocated values and place for stack frames in the order they are created in merge nothing is allocated from one recursive call to the next the cons cells are created on the way back up the recursion so only the stack frames are placed in the after m recursive calls the will fill and blocks will have to be to the memory µ rule b in figure the merge will invoke at most such since only n frames are created on the way back up the recursion we will generate the cons cells for the list and copy each of the keys using the note that copying the keys is important so that the result remains compact the cons cells and copies of the keys will be interleaved in the allocation order in the and to memory once the once again these will be in blocks of size b and hence there will be at most such furthermore the resulting list will be compact since adjacent elements of the list will be in the same block we now consider as a whole theorem for a hf function less and compact list a the evaluation of less a starting with any cache state will have cache complexity o n b log n m n a and will return a compact list as a result proof as with the array version we consider the two cases when the input in cache and when it does not the routine never requires more than on live allocated data therefore when kn m for some small constant k all allocated data in the furthermore since the input list is compact for k n m the input in the read cache for some constant k therefore the cache complexity for is at most the time to on items out of the allocation cache that it might have contained at the start and to load the read cache with the input this cache complexity is bounded by when the input does not fit in cache we have to pay for the merge as analyzed above plus the recursive calls this gives the same recurrence as for the array version section equation and hence solves to the result we now consider a using lists and a tree based heap for the merging the code is shown in figure the sort partitions the list into k parts sorts each part recursively builds a priority queue pq out of the resulting parts and keys one by one out of the pq adding them to the output list the only slightly part is maintaining the priority queue the idea is each of the sorted lists is placed at a leaf when elements from the root of the pq the value is removed from the root and the datatype a pq leaf of a list node of a a pq a pq fun a a less k l let fun leaf none leaf a node a fun case of none r none l if then else and leaf r node lr fun merge h case of none let val r in ar end fun a a let val lh partition a in end val ll partition k l val sl map less k ll val hl sl in merge hl end figure merge sort two children are which recursively the value from the child with the smaller root we dont show the code for partition which simply partitions a list into k equal length parts within although the code is somewhat involved the analysis of the cache complexity is relatively simple since most of the data allocated for the priority queue becomes unreachable before it needs to be to memory theorem for a hf function less and compact list a the evaluation of less k a starting with any cache state and with an appropriate k mb will have cache complexity o n b n b n a and will return a com list as a result proof as usual once the input size is less than nc for some constant the whole problem in cache and we just pay to load the input and write the output which will have cache complexity if the input and output are compact when the problem size does not fit in cache we note that with k c m b for some constant c we can fit the head of each recursively solved list in the read cache assuming each is compact therefore traversing all lists will use cache complexity furthermore the size of the priority queue is proportional to k so the live part easily within the allocation cache we have to be careful however since the datatype a m leaf of a node of a m a m a m a m fun leaf a leaf b let fun leaf a leaf b val mm in end figure matrix multiply allocation cache is shared with the stack frames which could some of the data allocated by the pq but since at any given time much less than half of the allocation cache only k of it is used by the pq we can all such against the cache frames we for every cache frame we can also reading them back into read cache against the cache frames going down the recursion of the merge therefore requires cache complexity to account for loading the k recursively solved lists and the n cache frames coming back up the recursion again requires cache complexity for the list and the copied keys the resulting list is compact for list traversal since it is allocated in list traversal order tail of the list first this gives us the recurrence in equation from section which solves to the desired result matrix multiply our final example is matrix multiply the code is shown in figure we have left out checks for matching sizes this is a block recursive matrix multiply with the matrix out in a tree it is therefore an interesting example of a tree data structure we define with respect to a preorder traversal of this tree we therefore say the matrix is compact if traversing in this order can be done with cache complexity for an n × n matrix n leaves we note that if we generate a matrix in a preorder traversal allocating the leaves along the way the resulting array will be compact theorem for hf functions and and compact n × n matrices a and b the evaluation of a b starting with any cache state will have cache complexity o n and bm will return a compact matrix as a result proof matrix addition has cache complexity and gen a compact result since we traverse the two input matrices in preorder traversal and we generate the output in the same order since the live data is never larger than on the problem will fit in cache for n mc for some constant c once it in cache the cost is needed to the load the input matrices and write out the result when it does not fit in cache we have to do recursive calls and four calls to matrix addition this gives the recurrence qn o n b n mc otherwise this solves to o n the output is compact since each of bm the four calls to in allocate new results in preorder with respect to the they generate and the four calls are made in preorder therefore the overall matrix returned is allocated in preorder conclusion the idea of distinguishing the abstract cost semantics of language from its concrete implementation with and work on parallel programming the benefit of their approach is that it provides a useful abstraction to the programmer that accounts for the complexity of a program while simultaneously providing a guide to the for how to achieve the complexity bound with stated overhead this work extends that methodology to account for the io complexity of a program in terms of two parameters the cache block size measured in objects and the number of cache blocks the programmer reasons at the level of the evaluation semantics the makes use of the provable implementation strategy to realize the complexity in the present case the essence of the proof is to argue that conventional implementation techniques which rely on a runtime control stack and copying garbage collection can be to meet the abstract bounds given by the semantics of a functional language the separation between the semantics and its implementations allows the programmer to work at the level of the code itself and avoids having to reason in terms of the details of the compiler and runtime system or even worse to be forced to drop down to a level in which the programmer explicit storage allocation for each application using the approach we are able to express algorithms in a standard highlevel functional style using recursive data types lists and trees analyze them using a model that captures the idea of a fixed size read and allocation stack but no details of the run time system and yet match the asymptotic bounds for the ideal cache achieved by designing them using arrays and explicit and careful memory management in the imperative setting for sorting the bounds are optimal one direction for further research is to integrate deterministic parallelism with the present work based on previous work we expect that the evaluation semantics given here will provide a good foundation for specifying parallel as well as sequential complexity one is that the explicit consideration of storage considerations in the cost model given here would have to take account of the interaction among parallel threads the arguments would also have to be to account for parallelism another direction is suggested by the special treatment of the runtime stack described in section the stack is after all a particular data structure that is used implicitly by each program this use could be made explicit in which case it would be useful to understand more generally what properties of it allow for its efficient in terms of cache complexity implementation these might well generalize to other data structures and we it may be useful to develop a type system to capture these special properties finally although we are able to generate an optimal sorting algorithm it is whether it is possible to generate an optimal sorting algorithm in our model acknowledgments this work is partially supported by the national science foundation under grant number and by intel academic research for the parallel algorithms for computing program references j a l and j a functional approach to external graph algorithms ­ a and j s the inputoutput complexity of sorting and related problems acm ­ a w appel garbage collection can be faster than stack allocation inf process ­ l m a e d c e and k editors and algorithms volume of proceedings germany g e and j parallelism in sequential functional languages in pages ­ yj m t e f r d e and j s graph algorithms in k l editor pages ­ isbn t m and j r using generational garbage collection to implement data placement in s l p jones and r e jones editors pages ­ acm isbn r improving locality of reference in a memory management system acm ­ m c e h and s algorithms in pages ­ ieee computer society m t jj d e and j s computational geometry preliminary version in pages ­ ieee computer society j and g e a provably parallel implementation of full acm trans program lang syst ­ d b g and r improving the cache locality of memory allocation in r cartwright editor pldi pages ­ acm isbn r harper practical foundations for programming languages cambridge university press draft available at r jones and r garbage collection algorithms for automatic dynamic memory management u meyer p and j f editors algorithms for memory hierarchies advanced research march volume of lecture notes in computer science springer isbn j g morrisett m felleisen and r harper abstract models of memory management in pages ­ k and a g of graph algorithms in r e tarjan and t editors pages ­ isbn g d plotkin considered as a programming language theor comput sci ­ m p and j scalable external sorting in f li m m s j r g m j f e y i s u and v j editors pages ­ ieee isbn d d and r e tarjan efficiency of list update and paging rules acm ­ d g e r harper and p b space profiling for parallel functional programs in j and p editors icfp pages ­ acm isbn j s algorithms and data structures for external memory foundations and in theoretical computer science ­ p r m s lam and t g caching considerations for generational garbage collection in lisp and functional programming pages ­ 