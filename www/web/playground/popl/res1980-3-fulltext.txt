with an applicative processing language string james h xerox palo alto research university of berkeley philip wadler carnegie university experience using and implementing the language is described the major conclusions are applicative programming can be made more natural through the use of builtin iterative operators and notation evaluation strategies such as lazy evaluation can make applicative programming more efficient pattern matching can be performed in an applicative framework many problems remain milner the properties of applicative to define semantics mathematical primarily to a by analogy with a does not make his by programming but rather by studying programming only a few people suggest very that the applicative style is good per se this paper attempts to explore the question introduction will programmers ever applicative programs applicative programming is a style that assignment statements or other operations that have the effect of changing the values of variables it deals with the of sideeffects by them out pure lisp is probably the best known applicative language the calculus church and systems of recursion equations are languages in the of logic that can be as applicative languages many people have developed applicative languages or their use eg requires a fee and or specific permission is an experimental language for text and list it has been used for testing some ideas for extending the of interactive text editors it has several aspects but the one we shall emphasize here is the use of the applicative style in more realistic situations than considered we designed applicative programming and tried to use it in that spirit a for it might read start with pure lisp replace atoms with strings add snobol pattern matching iteration over lists with ideas add powerful primitives like sorting fold into an syntax and until half done in by the first two authors and has recently been by the wadler had designed language lt has received use few pages of program written by about acm programmers and computer they had a good text editor but no language like snobol or any unix facility like aho or it has received most of its use from people with a task that is regular to be tedious but not recurrent enough to justify a big effort in a more conventional language a typical comment has been in a of i was able to learn and use it to solve a problem that would have taken much longer otherwise a few people more serious programs a report generation system for software projects a family budget a correspondence management system for academic journal editors a order management system large portions of some of these projects have been written a b b n is written as li rather than lt a negative subscript i yields the list with its first i elements removed a b c d c d lists can be with the inf operator the familiar y operation of lisp can be accomplished with the idiom x y which places x in a list of length one and it with y functions are denoted by lambda expressions except that instead of ax one writes x an expression like xy x y is an abbreviation for z z zj the application of to parameters is n in notation using the operator of values and functions strings are primitive values and are written in eg a string and concatenation of strings is denoted by as in snobol a number is simply a string of the can be omitted addition and can be written as infix operations the special primitive value fail plays the role normally by boolean values in algol conditional expressions test their parameters for being fail or not rather than true or false if p then x else y if then x else y values are either lists or functions lists are written like a list and lists may be t t t t there is a standard assignment statement x e it is used mostly for defining functions at the top level the precedence of is such that one can use functions as a sort of assignment statement lx xx lx xx and is equivalent to x l xx equality assertions to make programs readable there is a checked comment facility any function definition can be with a set of assertions which constitute a test evaluation of the function for example given the x one can equality assertions to produce x reverse foo which says input is foo the be and the final value will be this idea has out well it is much easier to what a program is doing if a is interleaved with it the fact that the example is makes it more than a normal comment in practice one needs mechanical to generate examples because of all the details eg how many spaces are in which escape the reader but not the checker a list of length of lists may be a b c d e a d b e c fl is important because it allows one to generalize a f to work on lists via the idiom the combination of builtin iterators and notation was very successful applicative programs to do complicated things could be written easily without using recursion furthermore writing such programs a simple natural process rather than a challenge to the syntax and builtin iteration since applicative programming has been employed mostly by rather than programmers many of the syntactic like are absent from applicative languages the applicative style usually requires the use of many recursive function definitions one for every loop to this situation several builtin iterative operators string concatenation and the arithmetic operations extend to lists of strings three iterative functional are infix operators reduction operator and an operator similar to the p operator of recursive theory like function application these three operators are written with the function second rather than first a b cf af bf cfl a b cf cf reduce xf if xf then else x a sequence of numbers can be generated by the notation as an example consider the problem discussed by given a list of like time was lost generate an list useful for looking up specific key words time was time was lost time was lost the procedure is as follows break the text up into lines break each line up into words for each line generate a list of pairs one for each word consisting of the word and a reconstruction of the line with brackets around the merge all these lists into one big one sort the list by the words discard the words all the lines to form tie final text figure shows the program to do this and figure shows the same program with equality assertions the major steps correspond to the informal steps above the character stands for the functions lines and words are patterns to be discussed later that split text into lines and words respectively append pairs of lists cone pairs of strings the phrase is a shorthand for xx besides using a nontrivial recursion the function g makes heavy uses of the implicit iteration of concatenation the subexpression w puts a space at the beginning of before they are by cone the subexpression wl prefixes the current word wi to every string in the list that the recursive call of g returns figure the program annotated with equality assertions s was lost time was lost words time was lost list time time was lost was time was lost lost time was lost append time time was was time was lost lost time was lost figure a program for s words append sort cone g q w if then n else wi wi notice that the informal description j of this procedure consists of quite imperative statements while the program itself is entirely applicative this is the advantage of post fix syntax the key to this is that at any point in the program there is only one thing being dealt with and it plays the role normally by the state of the machine in an program many programs have been written in this style often the system allows one to type something like f as a command and function f is applied to the last thing out and then the result is a is kept and the user may edit this to produce a program sort lost time was lost time time was lost was time was lost time was lost time was lost time was lost cone time was lost time was lost time was lost g w time was if then g else wi j wl was was lost time was lost time was lost time was lost this program is rather but we believe that translating it to a more conventional notation makes it worse in figure the program appears written in an style of syntax ie changed to a prefix notation with all the and reduce operations explicit to make the nesting we introduced many assignment statements imagine how the program would look if we eliminated them by of course the assignment statements give one the to a identifier to describe the intermediate result thus the of the program is due to the style of expression rather than w syntax of the language figure the program written in syntax even if one it has problems for functions that were binary one has an to place one of the arguments after the function name this syntactic style has in smalltalk if one a function producing function like f x y x y a call of the function did not look right it was not obvious how to programs in something like the function name would get lost instead of providing ifthenelse expressions as this paper suggests we used two binary operations and i with the following definitions x y if x fail then fail else y x i y if x fail then y else x procedure begin words q append return cone end this allows one to write things like u rather than tie more t if t fail then l else t the conventional if p then x else y could almost be achieved by p x i y in this syntax caused more confusion than it was worth procedure gw begin if then return w ax x cone rest ax w x return rest end problems with the syntax users and potential users had mixed about th syntax even aspects we consider successful were not no one was ever sure what the precedence rules were or should be a better set of operators although the iterators were successful in general we now have a better idea of what they should be the operator had the feature that if a value in the output list was fail it was omitted this was but it to errors one would like to discover there should be a separate operator to this perhaps split defined as follows where is the list of items on list for which p is true ie not fail and is a list of all the others the reduce operator was to use when the function was not associative the following definition would have been more useful x y z f it processes the list from right to left and includes the empty list in the enumeration this would allow us easily to solve the problem of eliminating adjacent from a list xy if and x yl then y else xy of course this reduce would have not have for functions like plus and cone which expect to see the null list as an argument but such could be extended to take lists of parameters as a matter of course making their use with reduce unnecessary the general iteration operator was not very useful perhaps we should have built in the list iterator described by which is something like evaluation is essential the program is inefficient by standards every line seems to create a large new structure which the following line great improvements in this algorithms performance can be made by a little in the evaluation strategy we recently changed the implementation to use the lazy evaluation strategy described in so these operations are merged the essence of the technique is that nothing is evaluated until it must be under this lists often behave like streams because their remain until they are needed in the case of the first operator that forces evaluation is sort which demands that it receive a list of lists each of whose components is a fully evaluated string this causes the append operation to be completed but the second component of each pair remains until the reduction using cone thus in principle this program requires only enough space to create a list of all the individual words and does not require space proportional to its output which approximates the square of the input lit hx the need was for ways other than the sequence operator to generate lists from whole for example the following function might be useful notice that the revised definition of the reduce operator works much better under lazy evaluation for example the beginning of the value of can before l has been completely traversed a if a fail then u else a af an infix functional composition operator eg x would have been used frequently since lists are never fully one can even deal with infinite the numbers may be described by the recursively defined list fib fib fib suppose one want to find the first number that is by he can say fib where search can be defined in terms of split the obvious way this will not involve computing any more elements than a more conventional program would in general any while loop could be written in this way s a while ps do s fs can be simulated by a our implementation of lazy evaluation has not been a complete success for reasons which we shall discuss latter but it has allowed us to be that this style of programming may be more practical pattern there are two aspects to the design of pattern matching the parsing of strings and the of successful we most of our effort to the second of these on the theory that a great deal is known about the first in essence the matching is the language of regular expressions a primitive pattern is either a string or the which matches anything like larger patterns may be constructed from smaller ones by using four combination rules if p and q are patterns then so are the following pq pp l concatenation alternation etc iteration optional a simple parsing algorithm causes problems rather than use a general parsing algorithm like we chose an ad hoc matching algorithm of the variety we it would be a lot of work to implement a general parsing algorithm that would run as fast as an ad hoc one furthermore it was not clear what to do with multiple some of the advantage of having a formally correct parser would be lost if the programmer had to understand the matching algorithm in order to decide which parse would come out first nevertheless in we feel that a better algorithm is called for because even the found he made in writing patterns for example he would write something like a i an even though the manual stated that this would not work because the would not back up to try the an alternative after matching the a in a string like an the the pattern which was very to in practice raises some problems we dont know how to solve even with a fully general parser because it gives rise to a considerable amount of ambiguity the pattern x can match the string in two different ways we chose the approach so that the string would parse into b x and however in more complex situations things do not work out well no matter what rule one consider the following description of text in which spaces and are used to describe the twolevel that appears in the example the kleene star pattern p can be written as p every pattern is in since patterns can be assigned to variables it is possible to create recursive patterns for example j j there are many possible of the string e i e e and we cannot think of any consistent rule which will produce the parse one wants it seems clear that in this context one to mean any characters other than space and snobol has an expression break that means precisely that and now we it in practice this difficulty has been we use a process described below break up the text at all then break the at spaces applicative is how can one make pattern matching an applicative operation specifically how does the language make the results of parsing available to without using side effects for exam the snobol pattern p x y assigns the parts of the string which fall before the to the variables x and y as a side effect of the matching process this is because a reader who only the name p in a matching operation cannot easily discover what variables if any will be changed the basic idea in is that a pattern is a function which can be applied to a string the result can be fail or something derived from the string by a set of pattern composition rules as the default the simply the pieces matched so that however by the pattern appropriately one can for different things to happen a component with causes whatever it matches to be discarded one can replace pieces by the phrase x one can make lists out of the pieces by inserting brackets and in the pattern aa conceptually it is best to think of a process first the string is parsed then one computes the result from the parse tree using the various signals attached to the pattern although it can be syntactically to these two processes it the fact that any division of the two phases can lead to them inconsistent experience suggests a slightly different design for patterns might be better first one is always writing after string constants to indicate that they should be discarded the default should be the other way around second including names for the interesting the pattern has great value once there are more than two or three interesting parts of a pattern one begins to lose track of the order the design alternative we now was the one chosen by wadler introduce pascal records into the language and allow the result of a match to be a record for example the value of a match using x y would be a record with components x and y this the applicative nature of pattern matching while the of conditional value assignment notation a more significant is associated with iterated patterns like pi a snobol programmer can not use the equivalent pattern if he wants to do anything with the result of the parse if he wants to apply the procedure f to each substring p matched he must write an explicit loop that off a prefix of the string matching p applies f and starts over this is too bad there is a nice construct that can describe iterated structures but one must to traditional programming to actually process them the first solution to this problem is to introduce a new for list processing let us now consider the problem of writing the two patterns lines and words that appeared in the example lines is relatively easy operator that things just like but produces a list of the items matched rather than them then the operation f can be applied to each element on the list using thus one says lines notice that the are discarded words is harder because one has to get the piece immediately following the last space and with the case in which there are no spaces a second answer is given by a very general method for words letter jy processing the outcome of a pattern match a element result of matching that element one says where letter is a pattern matching any letter if lazy evaluation methods were extended to pattern matching this method would with a lefttoright parser unfortunately we found that the semantics we and the result match would apply ing each of the which matched p and choose for pattern matching are not quite right for lazy evaluation for example the value of the results this method is applicable in more general cases y the recursive patterns without functional such patterns are not useful if one wants to process the recursive structure for example to parse an expression its value one sp z is fail if s does not end with z thus one cannot begin to process a long file of ps for that the file will not end in z can write because up text is a very common operation e i e e which is if nothing else functional was used extensively to build powerful patterns which and our patternmatching language doesnt seem to do it very we adding it as a primitive s simultaneously matched and transformed their input parsing is conceptually but needs help is defined as returning two lists the first is the list of separated objects and the second is the list of for example experience has shown that the method is usually easier to use than the function method it seems simpler to because it is less in general the apl style of processing seems just as for parsing as ac u lines s j words s notice that always succeeds so it is amenable to lazy evaluation implementation notes an interpreter for was implemented on the alto using the language mitchell it is organized so that there is no distinction made between expressions and values what one normally of as a value is simply an expression that the evaluator will not reduce any further an expression is represented by a node and may be one of a variety of different types a string an empty list or fail a list node with pointers to the first element and the rest of the list a specific operator with one or two associated operands eg plus with a pointer to each or with a pointer to the function and a pointer to the list a a closure a pointer to an environment list of pairs and a pointer to an expression the evaluator is a simplifier passed an expression it returns a new expression which is a simplified version of the first after normal evaluation an expression will be in one of the following three forms a string or a closure of a a list composed of the nd recursively lists to convert the evaluator to be lazy in the manner described in we made two changes arguments of a function are not evaluated until needed components of a list structure are not evaluated until needed in each of these cases the expression is put in a closure wi the current environment an outcome of this rule is that the final result of evaluation may be a list node whose components are closures the of needed means that the value is to be or treated as the subject of a pattern match we did not make the concatenation of strings or pattern matching lazy but have chosen a representation of strings both arguments of a concatenation are fully evaluated but if the resulting string is more than characters long the result is represented as a node with pointers to the two strings thus in general a string is represented by a binary tree of such nodes the terminal nodes point at pieces of files which are in as needed immediately before or pattern matching this tree is converted to be e each left son is a terminal node this scheme was at after some and seems to work well most of the time garbage collection we implemented a garbage collector for both nodes and strings temporary string storage was and files were closed if garbage collection that no string pointed to them we set up strict programming conventions to avoid bugs we made it our policy that each procedure would register the address of any local variable of type node it did not have to register parameters because they were the callers locations were kept in a stack which and in parallel with the runtime stack when garbage collection was necessary only those nodes accessible from locations were saved we used the algorithm instead of reference counts because it gave us explicit control over the memory and no programming errors ever caused us to lose memory since we explicitly its use every time a garbage collection if a procedure failed to register a value the subsequent garbage collection would the values about to be used of this sort were not too hard to find since the collector gave nodes on the free list a special type and subsequent access usually checked the nodes type evaluation and problems as expected lazy evaluation required a larger constant overhead than normal evaluation a lot of time is spent contexts in the form of closures and them we that this would cause a by a factor of three in those computations where one must eventually evaluate everything completely it appears that the factor was to two another problem is that the saved closures can tie a lot of space to avoid this one can scan the expression part of a closure to determine what variables are free in it and include only these in the environment list for the closure we dont know whether this would be worth the there was an in the lazy evaluator design it sometimes requires twice as deep an evaluation stack as the normal evaluator consider factorial x fl if x o then f else xl ones intuition suggests that this is efficient because the recursive call can be replaced by a simple jump an optimization that most compilers and some interpreters detect unfortunately under lazy evaluation this program is somewhat less efficient the problem is that the expression fx at each level remains thus when the evaluator gets to the call at which x o it begins to work on the expression f to produce a number at this level f is bound to a closure whose expression is nx and whose environment binds x to and f to a closure whose expression is px etc in other words to come with a numerical value for f the evaluator is going to get into a recursion precisely as deep as the one we thought we were avoiding this second recursion is not in general because one doesnt know that is associative and one is also required to all those closures with the numerical values on the way back furthermore if the evaluator cannot avoid the recursion in the first place we will need twice as much stack as under a normal evaluator in practice this problem is not because a programming style with no recursion in it one should write x which is and as efficient under lazy evaluation as factorial is under normal evaluation even if we defined lx by a recursive procedure a way of avoiding some of these difficulties has been suggested by turner his implementation avoids closures and environment lists entirely by translating the expression into combinators however some hand simulations indicate that the size of his combinator expressions may grow large in the same situations that generate many closures under our implementation his implementation avoids checking each value to see if it is evaluated it also solves the problem of deeper nesting by expanding the functions inline the first time they are called a more fundamental problem is that lazy evaluation is not as a method of improving performance as one might imagine consider the following function j evaluation of does not require the entire list to exist at any one time nor does evaluation of but since both are to be calculated the entire list will evaluating one forces the list into existence and it cannot be garbage collected because the other still needs it there is no mechanism to the evaluation in general this problem may occur whenever a list is generated that needs to be traversed by two different functions another example is list problems of this type will often be associated with the reduction operator because it reduces a list to a single value making greater space possible writing a few special functions to handle reduction might solve some of these problems for example consider reduce f lf lf although its use is not completely natural one could a compiler generating it lists and strings should be it never occurred to us at first to unify the concepts of strings and lists we thought of strings as lisp atoms however it clear that this division forced the language into two pieces as in snobol the pattern and the general language the of this clear when to a parsing operation by a lexical analysis that produced a list of strings the pattern language could not be used on the list this was avoided in lisp we now an alternate design in which the base data type is character and a string is just a list all of whose elements are characters the is how to generalize the patternmatching language so that it works on lists now we are required to say what it is about the pattern matching language that makes it so nice other than that it is just like regular expressions one thing makes it powerful is that it is basically a second order language like in that expressions in the language tend to denote functions than values for example x y in the conventional language assumes that x and y denote strings and the value is another string in the pattern matching language x and y are functions and the result is another should have a powerful compiler if we are really going to write programs as as the example lazy evaluation is not powerful enough to recover all the efficiency that is needed the approach demonstrated by is more promising and is being studied by the third author who claims that for any function written in a lazy programming style there is an equivalent and equally efficient program that may be written in the normal style one can imagine a that at compile time performs a transformation that converts a lazy program to its equivalent this would avoid the problems discussed above furthermore the compiletime analysis could be used to detect type errors that are especially difficult to with when things happen in an order the programmer doesnt expect deep problems about applicative programming a more serious bar to applicative programming is by the following problem one to process all the elements of a list some of which may cause conditions one writes l x if then else exceptions exceptions x if the machine is it is probably harder to understand especially if it various ad hoc heuristics based upon of what sort of programs people write the problem is that one wants to use a side channel to some information which is to the main computation in general if a process has multiple output streams which receive data at very different it is difficult to retain an applicative approach beyond this technical problem there are basic long questions with applicative languages which our experience has to the surface how does one meaningful even if ones computation has no bugs and is the order in which things are done can be relevant when ones computation takes a long time he would like to save intermediate states that have meaning to the programmer for example in a correspondence management system we found it desirable to produce a letter and record the fact that it had been sent as an atomic action typically one might request the system to send many letters and expect how should interaction with a user be carried out in our environment it is the norm to write programs that interact with a person through a screen and pointing device to describe such things one can describe each program as a function that maps each input into its output response or better an input stream into an output stream as have done this model doesnt fit very well with making random changes to a display however how does one a program with a surprising evaluation order our attempts to programs submitted to the lazy implementation have been quite the thing in our experience to it was debugging a system but in this case every parameter to a procedure represents a new process it was difficult to predict when something was going to happen the best strategy seems to be to print out welldefined intermediate results clearly labelled that one or two requests would cause for reasons ranging from hardware errors to software errors to requests also one to interrupt the process to do something else with the machine since there is no between these requests and because the operation takes a nontrivial amount of time one would like all but the requests to be completed we to solve this problem through the use of explicit writes on highly operation if one to describe the operation as a whole control of what happens to the system any forces one to start over entirely to summarize the potential practical benefit of an applicative language is that its implementation has much more running room in which to be since the order in which operations are performed is constrained only by the data flow examples of such are lazy evaluation compiletime loop integration and parallel processing on how does one predict performance never mind that lazy evaluation or any other strategy will make the program perform better than it would have one depends upon his understanding of the machine to design things so that they run reasonably the other hand computing is an activity which goes on in time and space in situations where one about the time and space aspects of an operation as much as the result applicative programming is less applicable furthermore the personal interactive mode of computing situations to increase the frequency of these acknowledgements alan has our exploration of this programming style robert and have written programs that they upon in paul has made many comments about the and this paper references john recursive functions of symbolic expressions and their computation by machine comm acm april church a church the calculi of conversion of mathematics studies no princeton university press princeton n j kleene s c kleene introduction to d van princeton n j christopher towards a semantics in formal language description languages for computer programming tb cd northholland amsterdam pj the next programming languages comm acm march friedman dp and ds cons should not evaluate its arguments in automata languages and programming and milner eds edinburgh university press milner m gordon r milner l m and c a metalanguage for interactive proof in in proc th annual acm symposium on principles of programming languages william h recursive programming techniques addisonwesley reading mass john can programming be from the von style a functional style and its algebra of programs comm acm aug j morris and e language xerox internal philip c wadler conversion xerox internal directed data re and jp the snobol programming language prenticehall aho a v aho b w and p j a pattern scanning and processing language bell internal hill n j m e and e a lexical analyzer generator internal hill n j d on the criteria to be used in systems into modules comm acm a and alan smalltalk instruction manual xerox palo alto research center report morris peter and james h morris a lazy evaluator third symposium on principles of programming languages j an efficient contextfree parsing algorithm comm acm c p e m b w r f and d r alto a personal computer in computer structures second edition bell and eds to appear mitchell j mitchell w r language manual version xerox palo alto research center report csl d a turner a new implementation technique for applicative languages software practice and experience l h d smith the lisp pattern matching system proceedings of the international joint conference on artificial intelligence stanford j and r burstall a transformation system for developing recursive programs jacm january pp 