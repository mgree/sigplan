safe nondeterminism in a parallel language robert l jr v s carnegie mellon university university of illinois at systems intel abstract a number of deterministic parallel programming models with strong safety guarantees are but similar support for nondeterministic algorithms such as branch and bound search remains an open question we present a language together with a type and effect system that supports nondeterministic computations with a guarantee nondeterminism must be explicitly via special parallel constructs marked nd and any deterministic construct that does not execute any nd construct has deterministic inputoutput behavior moreover deterministic parallel constructs are always equivalent to a sequential composition of their constituent tasks even if they or are by nd constructs finally in the execution of nd constructs interference may occur only between pairs of accesses guarded by atomic statements so there are no data races either between atomic statements and accesses strong isolation or between pairs of accesses stronger than strong isolation alone we enforce the guarantees at compile time with modular checking using novel extensions to a previously described effect system our effect system extensions also enable the compiler to remove unnecessary transactional synchronization we provide a static semantics dynamic semantics and a complete proof of soundness for the language both with and without the barrier removal feature an experimental evaluation shows that our language can achieve good scalability for realistic parallel algorithms and that the barrier removal techniques provide significant performance categories and subject descriptors d software concurrent programming d software formal definitions and theory d software language distributed and parallel languages d software language languages d software language constructs and programming structures general terms languages verification performance introduction widely used parallel programming models java c are based on a lowlevel and concept of threads these models provide few or no guards against parallel program ming errors such as data races deadlocks or atomicity violations some higherlevel programming models are available or are that prevent these kinds of errors however these models achieve their safety guarantees by greatly restricting side effects either through functional programming eg stm haskell or through dataflow or of programming eg concurrent collections ct there is much recent interest in supporting deterministic algorithms within general imperative languages via static type systems language mechanisms or largely transparent runtime techniques for algorithms that have deterministic inputoutput behavior such models can provide major benefits compared with traditional programming there are also important algorithms however that do not have deterministic inputoutput behavior and are not supported by these techniques some examples include clustering algorithms optimization algorithms like solvers and graph algorithms like mesh refinement a common feature of such algorithms is that they permit any of multiple possible outputs to be produced for a given input such outputs must usually be derived from a controlled set of choices typically from different orders of evaluating parallel tasks eg evaluating different groups of points in clustering algorithms importantly the nondeterminism should not simply derive from behavior due to data races and atomicity violations such behavior is not only potentially erroneous but can also can make executions difficult to reason about eg by producing results furthermore realworld applications are composed of a potentially large number of different algorithms likely to be a of deterministic and nondeterministic ones therefore it is essential to be able to compose deterministic and nondeterministic algorithms in a way that is easy to reason about these observations pose two challenges for a safe and realistic parallel programming model how do we express nondeterminism itself in a manner that simplifies reasoning about program behavior how do we allow nondeterministic and deterministic computations to be composed without weakening the deterministic guarantees for the latter in this work we present a parallel language that supports both deterministic and nondeterministic parallel code in a manner our language has the following major features permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm deterministic parallel operations we provide operations that describe deterministic parallel composition of tasks a deterministic parallel operation enforces noninterference between its component tasks ie there are no conflicting reads or writes in any pair of tasks ensuring that the whole operation behaves like a sequential and therefore deterministic composition of its component tasks the noninterference property is enforced at compile time this part is derived from previous work nondeterministic parallel operations we provide operations that describe potentially nondeterministic parallel composition of tasks these operations allow interference between tasks but any such interference is subject to the following guarantees again enforced at compile time this part is entirely new a race freedom and sequential consistency no execution of a valid program in our language can ever produce a data race this property is very important even for nondeterministic codes because in the java memory model race freedom implies sequential consistency which makes parallel programs much easier to reason about b strong isolation our language provides an atomic statement atomic s that executes the statement s in isolation ie as if there were no interleaving with concurrently executing tasks the isolation is strong ie isolation is provided with respect to all concurrent operations not just other ones occurring in atomic statements novel effect system features enable our language to be built on top of an off the runtime such as software transactional memory that provides only weak isolation previous work has also used effects to enforce strong isolation but as discussed in section our language is less restrictive and our guarantees are stronger c composition of deterministic and nondeterministic operations a deterministic parallel operation always behaves as an isolated and sequential composition of its component tasks even if the operation or is in a nondeterministic parallel operation this property allows local compositional reasoning about deterministic operations which we view as essential for a language that supports both deterministic and nondeterministic operations d determinism by default nondeterminism occurs only where by an explicit nondeterministic operation and cannot occur by specifically if a deterministic construct does not any nondeterministic construct for a given input heap state in some execution then it has deterministic inputoutput behavior ie it produces the same output heap state and other results in all executions for that input heap state for the deterministic parallel operations we build on deterministic parallel java in the programmer partitions the heap into regions and writes effect summaries on methods that describe the methods read and write operations on the regions the compiler uses the regions and effect summaries to enforce noninterference of parallel tasks at compile time however by design completely nondeterministic parallel algorithms to provide the nondeterministic operations and their associated guarantees we must extend for the isolation guarantee b we build on software transactional memory stm stm is not the only choice here but it is a good one as it runs on all platforms as opposed to hardware transactions which require special hardware and provides relatively strong guarantees isolation and deadlock freedom with very low programming overhead however stm alone is for our purposes first stm implementations usually provide only weak isolation and we want strong isolation second even strong isolation is not enough it allows data races between accesses outside of transactions and we want to such data races finally stm introduces significant runtime overheads including scalar overhead and false conflicts due to which can cause scalability to solve these technical challenges we extend the effect system in several ways first we add a new kind of effect called an atomic effect for tracking when memory accesses occur inside an atomic statement the atomic effects allow the compiler to guarantee both race freedom property a and strong isolation b by conflicting memory operations unless each operation is in an atomic statement second we introduce new effect checking rules to enforce composition of operations c and determinism by default d for composition of operations the extended effect system interference between a deterministic operation and any other concurrent operation unless the whole deterministic operation is in an atomic statement for determinism by default the interference is for deterministic parallel operations but allowed for nondeterministic parallel operations third to reduce stm overhead we introduce atomic regions so that the programmer can identify which regions may be accessed in an manner for operations to other regions the compiler can remove or simplify the stm synchronization because such operations never cause conflicts overall this work makes the following contributions we present a language that provides the compiletime guarantees through d stated above to our knowledge no previous language or system has provided all these properties for parallel programs through any mechanisms static or dynamic our language includes novel extensions to the effect system as discussed above for enforcing race freedom strong isolation and determinism by default and for reducing the runtime overhead of the underlying stm implementation we formalize our ideas using three formal languages the first has only deterministic parallel operations the second adds nondeterministic parallel operations and the third adds atomic regions we have developed a full syntax static semantics and dynamic semantics for all three languages further we have formally stated the soundness properties given informally above and proved that the properties follow from the semantic definitions here we summarize the key features of the formal language and the essential soundness results the full details including proofs may be found in the lead authors phd thesis we describe our experience using our language to implement three nondeterministic algorithms mesh refinement from the benchmarks the problem and oo a database benchmark our experience shows that these algorithms form pure java into our language was relatively straightforward and required neither of existing data structures nor of the algorithms themselves the language naturally expresses all these algorithms although the achieved vary depending on the inherent parallelism in the algorithms and performance limitations of the underlying stm additionally use of atomic regions eliminated a large fraction of the overhead in two out of three benchmarks background in this work we build on a language called deterministic parallel java uses an effect system to enforce deterministic semantics for explicitly parallel programs via compiletime type checking this section briefly explains the key constructs of the details may be found in in the rest of this paper we refer to the language as basic provides a parallel model the programmer creates parallel tasks using either a foreach statement for a parallel loop or cobegin block for a group of mutually parallel state ments effect system guarantees that in a welltyped parallel program any two parallel tasks have effects an effect is a set of operations on memory two effects interfere if they both access a common memory location and at least one of them writes to that location the noninterference guarantee for parallel tasks implies deterministic inputoutput semantics for the computation the effect system works as follows the programmer assigns every object field and array cell to a region and every method with a method effect summary stating a superset of the reads or write operations performed by the method in terms of regions the compiler checks two things that the effect summaries are a superset of the actual effects in the method body and that no two parallel statements are the effect summaries on method definitions enable modular checking of effects class p region l r double mass in p left in pl right in pr void mass writes p mass writes p void mass writes p cobegin if left null writes pl if right null writes pr figure some features of basic for deterministic parallelism figure illustrates the use of regions and effects in basic in line we declare class node to have one region parameter p line declares field mass in region p the actual region of the field is determined when the class is instantiated into a type as shown in lines and line declares names l and r that have static scope ie they are shared by all instances of class node lines and declare fields left and right and place them in regions pl and pr respectively the form pl is called a region path list or and it expresses the hierarchical structure of regions intuitively pl and pr are both nested under p the use of l and r puts the two fields in different regions while the use of p allows different node objects instantiated with different bindings to p to have their fields in different regions because l and r are distinct names pl and pr are guaranteed to refer to different regions for any common binding to p lines and illustrate the use of method effect summaries method line has declared effects writes p while line has declared effects writes p where the is a wildcard representing any sequence of names if an effect declaration is omitted it to most general effect writes the whole heap the compiler performs checks and stated above by the effects of a method foreach statement or cobegin statement the analysis is simple and local because at each call site the declared effects of the invoked method provide the effects of the invocation after substituting actual for formal region parameters for example the effect of in line is writes pl obtained by substituting pl from the type of left for the class parameter p in the declared method effect writes p the read of field left is subsumed because in write effects imply read effects similarly the compiler infers the effect of a field access or assignment to ef by substituting the region named in the type of e for the parameter in the declared region of the field f as an example of check correct method effects the effect of is legal because the method body writes to field mass in region p and has no other heap effects as an example of check parallel noninterference the compiler infers that the effect of lines and are writes pl and writes pr respectively because pl and pr must be disjoint regions for any common binding to p the effects are although this example is somewhat these and other features of can express a range of realistic parallel idioms including parallel updates on arrays of objects parallel traversals and updates of a tree on arrays and commutative operations within parallel tasks language support for nondeterminism we now informally describe the language mechanisms for nondeterministic parallel control parallel safety guarantees and optimization support we illustrate the new language features with a running example of the problem section describes the language more formally the computation the problem or is the wellknown problem of finding a shortest cycle in a weighted graph that all the nodes once ie a cycle can be solved by branch and bound search a common algorithm for solving optimization problems and a classical example of a nondeterministic computation figures ­ show simplified for the global data lines ­ include a weighted graph that is the input to the program a priority queue for storing the paths being explored and a best ie shortest which is refined as the computation eventually storing the answer two regions are used to hold the data readonly for fields that will not be modified during the computation and mutable for those that will be the priority queues type mutable indicates that it contains objects of type and that the internal data used to represent the queue itself is in region mutable the main computation loop lines ­ iterates in parallel over several tasks each task generates a prefix to search using the in figure and adds it to a priority queue when all prefixes have been generated the tasks remove prefixes from the priority queue and search them using the in figure until there are no more prefixes to search nondeterministic parallel control to express nondeterministic parallel computations we introduce a parallel loop denoted foreach nd where nd stands for nondeterministic line in figure shows an example this construct is identical to foreach in basic except it says explicitly that conflicting accesses and therefore potential nondeterminism are allowed between the loop iterations of foreach nd we also introduce a cobegin nd construct corresponding to cobegin in basic we refer to these four foreach foreach nd cobegin and cobegin nd as parallel constructs the resulting parallel control structure is just parallelism and can be represented as a static task graph where each node or task is a single iteration of a parallel loop foreach or foreach nd or a single statement in a cobegin or cobegin nd all four parallel constructs have an implicit join synchronization at the end of the construct for the tasks of the construct the directed edges in the task graph represent either program order or at the start of a parallel construct or the join synchronization at the end of a parallel construct two tasks are concurrent if they are not ordered in the task graph two memory accesses are concurrent if they occur in concurrent tasks regions for partitioning data region readonly atomic mutable graph we are working on immutable graph in readonly the graph priority queue for prefix paths final mutable new mutable the answer in mutable infinite path i in prefix null do atomic prefix if prefix null while prefix null figure global data and main computation for the problem path reads readonly writes mutable while prefix if return prefix else for each edge edge that can be added to prefix while under new edge return null figure generating the next prefix void prefix reads readonly writes atomic mutable for each cycle in graph with prefix prefix atomic if figure searching all with a given prefix the specific parallel constructs used to fork and join tasks are not fundamental to our work the language mechanisms used to enforce safety properties described next can be applied directly to other parallel programming languages eg a large subset of and potentially other parallel languages in which the compiler can identify all groups of concurrent tasks distinguishing the constructs that permit interference ie may be nondeterministic from those that do not and so are deterministic is a useful property but again not necessary for any of our other guarantees safety properties for nondeterministic code as stated in the introduction the goal of our language design and type system is to achieve four safety guarantees for nondeterministic and deterministic code i data race freedom ii strong tion for nondeterministic parallel constructs iii sequential equivalence for deterministic parallel constructs and iv a property we call determinism by default defined below these four properties give programmers a simple elegant execution model for reasoning about nondeterministic programs below we discuss the language mechanisms for expressing synchronization the effect system features for enforcing the properties and the resulting execution model seen by programmers synchronization to ensure correctly synchronized accesses in the presence of interference defined in section we add an atomic statement to the language this construct is similar to previous work except that in conjunction with our effect system discussed below our atomic statements provide stronger guarantees a statement atomic s indicates that s is to be run as if all other concurrent execution were suspended while s is executing this is called strong isolation with reference to the example in figure each call to is by an atomic statement that the accesses to the shared priority queue note that while the calls to are effectively each can start its call to as soon as its call to is done in a manner this pattern can achieve good because most of the work in this code is done in in figure an atomic statement the concurrent updates to an atomic statement may appear inside any of the four parallel constructs as well as inside other atomic statements two nested atomic statements in the same task are that is the inner atomic becomes a and atomicity is enforced entirely at the outer atomic if a parallel task created in an atomic statement contains a nested atomic statement then the nesting behaves in the standard way the inner atomic enforces isolation with regard to other tasks created by the immediately enclosing parallel construct while the outer atomic enforces isolation as to tasks created by any outer enclosing parallel construct effect system we now discuss how our effect system enforces the four safety properties stated at the of this section data race freedom and strong isolation we use the following strategy to ensure both data race freedom and strong isolation first a transactional runtime guarantees at least weak isolation of atomic statements ie isolation between different atomic statements but not between atomic statements and code second the effect system ensures that for any pair of conflicting memory accesses each access occurs inside an atomic statement for example in figure any two concurrent accesses to are both in instances of the atomic block at line the concurrency is created by the foreach nd at line of figure this requirement ensures strong isolation because no conflicts between memory accesses and atomic statements are allowed it also ensures race freedom because no conflicts between pairs of accesses are allowed notice two things about our strategy first our language may be built on top of a standard software transactional memory stm implementation which typically guarantees only weak isolation for performance reasons second our strategy all data races even tm systems with strong isolation generally allow data races between pairs of accesses occurring outside any transaction to the effect checking we extend the effect system to distinguish effects that are atomic meaning the effect occurred inside an atomic statement from effects that are nonatomic meaning the effect occurred outside any atomic statement the compiler ensures that interference occurs only between atomic effects to enable sound modular reasoning about method invocations we make atomic effects explicit in method effect summaries for example the effect writes atomic mutable in the summary for figure says that any possible writes to region mutable occur inside atomic blocks in the body of the method or its in checking method effect summaries our system is sound but conservative it is correct to summarize a write to region r occurring inside an atomic block as either writes atomic r or simply writes r the latter is more conservative than necessary but is correct however it is not correct to summarize an access occurring outside any atomic section as an atomic effect because such an effect would report a transactional guard when in fact there is none for example the effect system can verify that all accesses within the foreach nd in figure are atomic effects first the variable prefix is local to each task and so generates no conflicts across tasks second according to the effect summary for figure the method invocation in line produces conflicting effects on region mutable these effects are within the atomic statement starting at line and so are recorded as atomic effects that may interfere third the call to is not within an atomic statement but according to its effect summary it generates only read effects which do not interfere with themselves and atomic write effects which are allowed to interfere with themselves and are to a different region from the read effects sequential equivalence for deterministic constructs an important property we wish to preserve from basic is sequential equivalence for deterministic constructs that is foreach and cobegin are equivalent to the sequential execution of their constituent tasks in program order to enforce this property we obviously need to interference between cobegin or foreach branches even if the effects are atomic for example this program is not allowed cobegin atomic x atomic x for this we just have a simple typing rule that interference between atomic effects is allowed only inside foreach nd or cobegin nd however that is not enough because interference can also occur between a deterministic task and a concurrent nondeterministic task for example consider the following program z cobegin atomic x z atomic y z s s atomic z s this program could produce the result x y by executing s s s this result violates sequential equivalence of cobegin because it does not correspond to any sequentially consistent execution of the program where the cobegin block is executed in program order instead we wish to ensure that a foreach or cobegin executes in isolation even if it appears inside foreach nd or cobegin nd our solution to this problem is to convert atomic effects occurring inside a deterministic construct to nonatomic effects when propagating them to the outer context in the example above when checking interference the compiler nonatomic reads to z in the first cobegin nd branch those reads occurred in atomic statements but nonatomic when passing across the cobegin on the other hand the second branch has atomic writes to z therefore the cobegin nd branches have illegal readwrite interference ie not both guarded by atomic on z to write this program in our language the programmer could put the whole cobegin in an atomic statement determinism by default finally by of the isolation of deterministic constructs and the noninterference between their internal tasks both discussed above we have the following property if a deterministic construct does not dynamically execute any nondeterministic construct then the execution of the deterministic construct is in fact deterministic that is a given input heap state to the deterministic construct always produces a fixed result value and fixed output heap state we refer to this property as determinism by default nondeterministic inputoutput behavior may be introduced only by the execution of an explicit nondeterministic construct implications for programmers the properties discussed above and treated more formally in the next section provide two key benefits for programmers first concurrency errors such as data races or nondeterminism will be detected via compiletime type checking this benefit in base for deterministic programs and has now been extended to nondeterministic ones second once a program has been type checked the above properties greatly simplify how programmers can reason about the possible nondeterministic execution behaviors with regard to the second point many programmers and testing tools analyze program behavior by reasoning about the possible interleavings or schedules of parallel operations the above properties simplify this reasoning in several important ways we focus on cobegin here without loss of generality foreach is analogous we only need to consider interleavings of isolated atomics and accesses because of strong isolation and sequential equivalence of cobegin the nd constructs do not constrain interleavings we can reason about the tasks of a cobegin sequentially the first task can be fully evaluated without any accesses from elsewhere immediately followed by a complete evaluation of the second task cobegin nd provides the only source of nondeterminism even within such a construct the effect system guarantees that any block of code that is outside an atomic statement and does not execute any atomic statement call this an section cannot interfere with any concurrent task therefore programmers need not consider interactions between any sections when reasoning about program behavior put together these observations mean that the only source of multiple interleavings is from different orderings of atomic sections thereby significantly reducing the number of interleavings that programmers must consider furthermore programmers can control the of the atomic sections to control the number of possible interleavings the following example illustrates these observations assume the s terms are all statements s s atomic s s atomic s s even if all the statements are primitive operations reads or writes if sequential consistency is not guaranteed then up to different interleavings are possible if sequential consistency holds then there are still up to different interleavings in our language however we may consider only two sequentially consistent interleavings one with atomic s appearing before atomic s and vice versa for example any execution generated by our language is equivalent to executing the entire first cobegin nd branch before the entire second branch or vice versa performance removing barriers we use a software transactional memory stm runtime system to implement the atomic construct because stm provides weak atomicity better than locks and potentially better scalability because of optimistic rather than pessimistic synchronization one key of is the overhead due to transactional read and write barriers for every load or store to shared data eg see these barriers are of code often automatically inserted by a compiler that invoke the stm runtime to implement some transactional concurrency control protocol the barriers can either read and write shared memory directly socalled update stm and all transactional operations when a transaction aborts or they can buffer updates into a private data structure socalled write stm and apply all the changes into shared memory when a transaction successfully commits in both cases barriers can significant overhead and them is essential for performance we observe that we can use the region and effect system to remove unnecessary stm barriers where there is no interference however the effect system as described so far does not carry enough information to perform this analysis locally for example suppose a method m reads a variable x inside an atomic section then the read needs a barrier if and only if m is invoked in some context where there is interference on x there is no information in the method body that enables the compiler to make that judgment interprocedural analysis would be required however with a slight extension to the effect system we can enable local reasoning about this kind of noninterference specifically we have the effect system distinguish two kinds of regions those that may interfere and so need barriers everywhere and those that cannot and so do not need read barriers anywhere we call the first kind atomic regions the programmer can declare a region to be atomic such as region mutable on line of figure the key benefit is that for a nonatomic region the compiler can remove read barriers entirely and assuming stm using updates it can turn write barriers into barriers synchronization is not needed because there is no interference but transactions must still log the old value on writes in case the transaction aborts this completely eliminates the barrier overheads for readonly shared data it also substantially reduces the barrier overheads for data and shared data to enable sound reasoning about atomic regions and barrier elimination we require some constraints on the use of these regions any region declaration field region local region or region parameter may be declared to be atomic we impose the following requirements · when instantiating a type an atomic respectively nonatomic region parameter may only be passed an atomic respectively nonatomic region name as the argument this is straightforward to enforce using the region declarations · a region that is involved in effects must be declared atomic this is enforced by the compiler as described below the barrier elimination also requires a refinement in the semantics of atomic effects described in the previous section an effect in an atomic statement is marked atomic only if it operates on an atomic region for example the read of region readonly in figure due to the operation does not generate an atomic effect even though it is inside the atomic block at line the write to region mutable does generate an atomic effect if region mutable had not been declared atomic the write to would generate a nonatomic effect the compiler would then flag the effect declaration at line of figure as an programs p r c e classes c class c f m region names r region r fields f t f in r methods m t mt x e e regions r r types t cr effects e reads r writes r e e expressions e e v new t variables v this x figure core language syntax c f m and x are identifiers error because an atomic effect does not cover a nonatomic effect as noted earlier we can now explain how the last rule above is enforced if a region is not marked atomic but has an effect that causes interference in some parallel construct the compiler will detect an error either at the parallel construct or at the method effect summary for example if region mutable were not marked atomic the write to would generate a normal effect writes mutable this would cause the effect summary at line of figure to be as an error as noted above if the effect summary were changed to not mark the write effect atomic then the call to at line of figure would generate a nonatomic effect and the compiler would report the interference there formal semantics and soundness to make precise the ideas discussed in the previous section we have studied three variants of the same formal language each one building on the last the first variant which we call the deterministic language is a simple expression language with regions effects and deterministic parallel composition it is a version of core simplified to focus on the key elements for this work the second variant which we call the language adds nondeterministic parallel composition atomic expressions and atomic effects to the deterministic language the third variant which we call the atomic regions language adds atomic regions for removing or simplifying transactional barriers without loss of generality we only include cobegin and cobegin nd in these simple languages the treatment for foreach and foreach nd is similar overview of language variants we first explain the syntactic structure of all three languages and we summarize the soundness guarantees that each one provides in the following we explain the formal semantics of each language variant state the soundness guarantees more formally and sketch how the guarantees follow from the semantic definitions the full details including all the semantics rules and proofs of all the claims may be found in the lead authors phd thesis deterministic language figure gives the syntax of the deterministic language a program p consists of zero or more region declarations zero or more class definitions and an expression to evaluate a class c consists of a class name c a region parameter zero or more field declarations and zero or more method declarations a field f specifies a type a field name and a region a method m consists of a return type a method name a formal parameter type a formal parameter an effect and an expression to effects e atomic reads r atomic writes r expressions e cobegin atomic e figure syntax of the language extends figure evaluate a region r is either a region name r or a region parameter a type t is a class instantiated with a region parameter cr an effect e is a possibly empty union of read effects and write effects on regions for expressions e we model field access field assignment method invocation variables new objects sequential composition seq and deterministic parallel composition cobegin a variable v is this or a method formal parameter x the operational semantics of the first five expressions in figure is exactly as in java the last two expressions evaluate both component expressions either sequentially or in parallel and return the value of the second component as the value of the entire expression the deterministic language provides the following semantic guarantees stated more formally as theorems ­ in section they follow from the fact that the executions of the two branches of any cobegin expression are required to be equivalence of cobegin and seq in terms of the final result final value produced and final heap state there is no difference between executing and as a consequence the entire program is guaranteed to behave like a sequential program the one that results by replacing cobegin everywhere with seq determinism if an expression e evaluates to completion then the value it produces is deterministic moreover if e is evaluated in a sequential context ie not inside a cobegin then the final heap state is deterministic in particular the final heap produced by a terminating execution of the whole program is deterministic language figure shows the additional syntax for the language we extend the syntax of effects to record atomic effects we also add cobegin nd which is the same as cobegin except that it allows interference guarded by atomic expressions and expressions atomic e which signal that expression e should be executed in isolation that is as if it were executed all at once with no interleavings from the rest of the execution the language provides the following semantic guarantees stated more formally as theorems ­ in section race freedom and sequential consistency program execution contains no data race this result follows because the effect system requires that all parallel interference occur between pairs of accesses guarded by atomic expressions further in the java memory model race freedom implies sequential consistency ie one can reason about execution as a interleaving of memory operations strong isolation for the same reason that the program is race free expressions atomic e execute e in isolation even if the underlying implementation guarantees only weak isolation moreover the effect system any interference between the cobegin and concurrent operations that would violate isolation of the cobegin therefore every cobegin expression executes in isolation together race freedom and strong isolation imply that execution is a sequentially consistent interleaving of isolated expressions regions r atomic region r classes c class f m figure syntax of the atomic regions language extends figure equivalence of cobegin and seq because executes in isolation it is equivalent to an isolated execution of seq ie atomic as discussed in section for the language we make cobegin behave like atomic seq and not just seq to guarantee that cobegin executes even inside a cobegin nd determinism by default both atomic and cobegin expressions execute in the same sense as discussed for the deterministic language even inside a cobegin nd unless they contain a dynamic instance of cobegin nd atomic regions language the third variant of the formal language allows some regions to be marked atomic and only operations on those regions generate atomic effects operations on nonatomic regions never generate atomic effects even in an atomic expression figure shows the new syntax the execution semantics of this language variant is identical to that of the language except that the compiler can distinguish and potentially optimize operations within an atomic expression that never interfere with concurrent tasks in section we discuss a prototype compiler that uses these rules to optimize our stm by or simplifying barriers inside an atomic expression for such operations deterministic language static semantics the typing is done with respect to an environment which consists of elements v t stating that variable v has type t the key rule for ie when one effect conservatively summarizes another written e e is that a write effect on region r covers a read effect on the same region r reads r writes r the rules for typing programs classes fields etc are straightforward the rule for typing methods enforces effect subsumption that is that a methods actual effect must be a of its declared effect method tr tx e x tx e tr e e e tr x e e the key rules for effects ie effects that may safely go in parallel with deterministic composition written e e are that reads never interfere with reads and writes never interfere with reads or writes to different regions reads r reads r rr writes r writes r rr reads r writes r as in core every expression has a type and an effect the rules for typing expressions e with type t and effect e e t e are straightforward the most important rule says that in parallel composition the effects of the expressions being evaluated in parallel must be cobegin e t e e t e e e t e e dynamic semantics we give a smallstep operational semantics describing the recursive reduction of expressions execution state the execution state is e h consisting of an expression to evaluate and a heap a heap h is a partial function from object references to pairs o t where o is an object and t is the type of o an object o is a mapping from field names f to object references o null is a special reference that is in domh but does not map to an object to invoke a method of null causes the execution to fail we extend the static syntax of expressions to represent computations e o e e ei the additional expressions have the following meanings object references o are the values produced by reducing expressions a local execution state e e records an expression e to evaluate an environment containing the bindings for the free variables in e and an effect e of reducing e and the indices i enable us to say which expression is reduced in a given execution step as explained below a program execution is a sequence of steps ep i null ei h for some i e and h where ep is the main program expression i is an arbitrary index denoting the toplevel expression in the reduction ei is the evolution of expression ep i and h is the heap represented as a domain containing null plus all object references o added during the execution a terminating execution has ei o ei where o is the answer computed by the program and e is the union of all effects on h done in the execution expression semantics field access and assignment work in the standard way except that we track dynamic effects to state and prove the soundness results as an example of the effect tracking we give the rule for field access dy n fi e l d ac c e s s this o ho o cr f c f t f in r h of reads h the function cr the region argument r for the parameter of class c the rule for field assignment is similar except that the subexpression is evaluated first the heap is updated and the effect is a write instead of a read for evaluation of subexpressions we use the following standard rule dy n su b e x p e h e h e h e ei ei h it says that if we can reduce expression e to e starting with heap h and e appears with index i as a subexpression of e then we can reduce e by rewriting the subexpression ei in place and updating the heap the rules for cobegin illustrate subexpression evaluation dy n c o b e g i n e va l h ie j h dy n c o b e g i n ac c u m u l at e e j h o e e h creates two new subexpressions with fresh indices i and j for evaluation in environment the evaluation steps of ei and ej can be arbitrarily interleaved via rule this interleaving models the parallelism when both are done the results into the toplevel expression field assignment method invocation and sequential composition are similar except that the subexpressions are evaluated in a fixed sequential order so there is no parallelism except inside a cobegin the rules are entirely standard and are stated in full in the thesis soundness results as summarized in section there are two main soundness results for the core deterministic language to state the results we call out the reduction of a particular expression inside the evolution of the whole program we write e i h p ei h to denote a reduction of expression i occurring in program execution that means p is welltyped with main expression ep and there is a program execution ep j null ej h ej h such that ej contains e i which is the first of expression i in the execution and ej contains ei ej h is called the initial state of the reduction and ej h is called the final state our first result states that there is no semantic difference in this language between seq and cobegin at any point in the execution that is an initial state for a cobegin reduction we can replace cobegin with seq and get exactly the same results the proof follows directly from the noninterference property guaranteed by the static and dynamic semantics theorem equivalence of cobegin and seq e i h p o ei h if and only if e i h p o ei h with the same initial state except that expression i is as shown the second result states that expression evaluation is inputoutput deterministic up to the choice of object reference names the proof follows from the fact that once cobegin is replaced by seq everywhere according to theorem the only nondeterminism left in the rules is the choice of reference names and expression indices theorem inputoutput determinism if e j h p o ej h and e j h p o e j h with the same initial state then o o where denotes equivalence up to renaming object references and e e moreover if e j is not a subexpression of any cobegin expression then h h language static semantics figure gives the static semantics of atomic effects rule formally expresses the idea that nonatomic effects cover atomic effects that is if e occurred in an atomic expression then we can summarize the effect as either atomic e or e note that the converse is not true because we cannot soundly report an atomic effect where there is no atomic expression rule provides the relation for two atomic effects rule says that an atomic effect is if the underlying effect is the judgment e states that it is safe to run expressions with effects e and e nondeterministically in parallel inside a cobegin nd figure gives the key rules for making the judgment figure gives the rules for typing nondeterministic parallel composition and atomic expressions rule cobegin nd is similar to cobegin except that effects are allowed if they ee ee atomic e e ee atomic e atomic e ee n m i c ee atomic e e e ee e n o n d et at o m i c e atomic e figure static semantics of atomic effects selected rules e t e cobegin nd e t e e t e e cobegin t e e atomic e t e e atomic e t e cobegin e t e e t e e e t e e mi c r e a d s r atomic reads r n o mi c at o m i c e e figure static semantics of cobegin nd and atomic expressions selected rules the judgment e says that e is the effect obtained after adding atomic from all reads and writes in e and is the reverse are both guarded by atomic expressions rule atomic collects the effect e of the expression e then marks all the constituent read and write effects atomic to reflect the fact that e is occurring in an atomic expression finally rule cobegin has changed in addition to checking noninterference as in the basic language the new rule converts all atomic effects occurring inside the cobegin to ordinary effects using the judgment e this ensures that no atomic effects are ever propagated from inside a cobegin the last rule is key to ensuring that cobegin executes in isolation as discussed in section dynamic semantics we describe the dynamic semantics of the nondeterministic language in two parts the first operational and the second the first operational part is just the same semantics as for the basic language section with a few minor to accommodate the new features the second part describes a weak isolation constraint on execution histories generated by the operational part the overall dynamic semantics all execution histories described by the operational semantics that also satisfy weak isolation in practice weak isolation would be enforced by a runtime implementation such as software transactional memory operational semantics of expressions the operational semantics is identical to the one described in section with three changes first we add a rule to execute cobegin nd it is identical to the rule for cobegin shown in section ie in the operational semantics there is no difference between executing cobegin nd and cobegin the difference is all in the static semantics second we add a rule for executing an expression atomic e we execute e then mark all its effects atomic for purposes of effect tracking dy n m i c e va l atomic e h atomic e i h dy n m i c m a r k e ff e c t s e atomic o ei h o e h finally we modify the rule shown in section to mark the effects of a cobegin expression nonatomic dy n c o b e g i n ac c u m u l at e e e e j h o e h weak isolation constraint to state the weak isolation constraint we need the concept of a reduction history h which is a sequence of program execution steps e i h p ei h if e i h is the initial program state then we call h a program execution history and write hp two histories occur in parallel under cobegin or cobegin nd if they each occur in reducing different branches of the same cobegin or cobegin nd expression in the same program execution definition conflict relation on atomic expressions fix a program execution history hp and let i be the set of expression indices appearing in hp that label atomic expressions ie expressions introduced by rule the conflict relation on atomic expressions in hp is the transitive closure of the following relation on i × i i j is in the relation if i j and there are conflicting memory accesses ai and aj such that a ai occurs in the reduction of an atomic expression ei b aj occurs in the reduction of an atomic expression ej c the reductions of ei and ej occur in parallel under cobegin nd and d ai precedes aj in hp notice that we put the relation only on operations under cobegin nd not under cobegin and we do not include any conflicts occurring outside of atomic expressions that is because the type system will ensure there are no conflicts between cobegin tasks or outside of atomic expressions this is the soundness result that we state below now we can define the weak isolation constraint on executions in the language for the remainder of this section we assume an implementation that guarantees weakly isolated program execution histories hp definition weakly isolated histories let h be a history if the conflict relation on atomic expressions in h is a partial order then we say that h is weakly isolated soundness results as summarized in section there are three main soundness results for the nondeterministic language race freedom strong isolation and determinism by default race freedom the first result says that the language is race free assuming that pairs of memory accesses in different atomic expressions are which is true of any transactional implementation the proof follows from the fact that the type system parallel interference except for pairs of accesses both occurring in atomic expressions theorem race freedom if p then a history hp that has synchronization orderings consistent with the conflict relation stated in definition contains no data race strong isolation to state the strong isolation result formally we use the wellknown concept of histories we say that a history h e i h p ei h is serial with respect to expression i if every step in the history transforms expression i or a subexpression of expression i we say that h is with respect to expression i if it is possible to generate a history h with the same initial and final states as h that contains a serial history e i h p ei h for some heaps h and h in other words an expression reduction is if we could have done it with the same results the following theorem says that a history is if it does not occur in a cobegin nd or it does not reduce any atomic expression or it reduces a cobegin or atomic expression the proof follows from the type systems guarantees of noninterference for cobegin tasks and noninterference for cobegin nd tasks except where guarded by atomic expressions together with the weak isolation assumption for atomic expressions theorem strong isolation suppose p let hp be a weakly isolated history executing p and let h be a history e i h p ei h contained in hp then h is with respect to expression i if e i is not a subexpression of any cobegin nd expression or no atomic expression appears in h or e is a cobegin or atomic expression determinism by default finally we show that the nondeterministic language is deterministic by default first we show the equivalent of theorem for the nondeterministic language there is no difference between cobegin e and atomic seq e note that we need the atomic here because in general a seq occurring under a cobegin nd can interfere with the other branch of the cobegin nd the result follows directly from theorem and theorem initial state is defined before theorem theorem semantic equivalence of cobegin and atomic seq e i h p o ei h if and only if atomic e i h p o ei h with the same initial state except that expression i is as shown second we show the equivalent of theorem for the nondeterministic language for the expressions called out by theorem as having reductions the execution of such expressions is also inputoutput deterministic unless there is explicit nondeterminism via cobegin nd the proof follows from theorem together with the definition of a history theorem determinism by default suppose p and let h be a history e i h p o ei h that is with respect to expression i where no cobegin nd and e cobegin e or moreover if e i is not a cobegin nd expression then h subexpression h of any atomic regions language static semantics first the rules for constructing types require that atomic regions bind only to region parameters declared atomic and nonatomic regions bind only to region parameters not declared atomic this requirement ensures that memory regions are treated consistently across method invocation second we refine the judgment e so that an atomic expression makes an effect atomic only if the effect is on an atomic region this rule ensures that in applying rule cobegin nd to check a cobegin nd expression the compiler will never allow effects on nonatomic regions to interfere the rules are stated in full in the thesis dynamic semantics and soundness the dynamic semantics of this language is exactly as given in the previous section with two changes first marking effects in rule happens according to the refined definition of e that is effects are marked atomic only if they operate on atomic regions second we the conflict relation on atomic expressions definition so that only conflicts involving accesses to statically atomic regions ie region names r or region parameters marked atomic are synchronized by the implementation the soundness result says that theorems ­ hold for this language variant the proofs given in the thesis are entirely straightforward prototype implementation to implement our new mechanisms we extended the compiler used for basic it is a modified version of which translates code to standard java the compiler implements parallelism constructs by generating calls to the library which schedules tasks onto a pool of threads further details are available in in this work we extended the compiler to implement atomic blocks using the stm library we used the transactional locking ii tl algorithm tl is a ie lazy algorithm with optimistic reads supports concurrency control at the object field level and uses a lightweight custom reflection mechanism to access object fields inside transactions we selected this stm system for pragmatic reasons of ease of implementation and because it implements a wellknown stm algorithm we have not to absolute performance in our implementation it could be improved significantly by using a different stm system such as one integrated with the jvm our method is applicable to other types of stm systems and algorithms including those updates for each atomic block the compiler generates code to execute the body of the atomic block as a transaction until the transaction commits successfully nested atomic blocks are methods that are within atomic blocks are versions containing barriers are used when they are called within atomic blocks within atomic blocks the compiler inserts normal read and write barriers for accesses to fields in atomic regions as discussed in section the compiler barriers for read accesses to nonatomic regions and it generates barriers for write accesses we modified the tl implementation in to support these optimized barriers however because tl is a algorithm we would have to use to obtain correct values in the cases to avoid read barriers entirely we modified the algorithm to perform updates for these locations and we maintain a separate log to effects of such updates in case the transaction aborts reads to such locations do not need barriers because they can now obtain their values directly from the original memory location evaluation the ideas presented in this paper raise four key questions for experimental investigation can the language express nondeterministic algorithms in a natural way can the algorithms expressed in the language give good performance how effective is the optimization of stm barriers what is the annotation overhead of the language we used four nondeterministic algorithms to evaluate these questions two different versions of mesh from the benchmarks and oo a database benchmark that has been used in previous studies of parallel performance these codes are discussed further below benchmarks and expressing parallelism problem we studied two versions of the algorithm which we call and is the algorithm described in section as discussed there the algorithm proceeds in two phases the first phase breaks the problem up into and adds them to a priority queue and the second phase concurrently removes items from the queue and processes each one using sequential recursive search the priority queue orders the work so that more promising subtrees are explored first is a variant that eliminates the priority queue and uses recursion to express the entire algorithm at each level of the tree the algorithm computes a bound for each subtree and compares the bound against the global current best bounds that are definitely no better than the current best are while bounds that may be better are explored recursively the recursion occurs in parallel until a specified depth of the tree in our studies we used a depth varying with the log of the number of threads is a simpler algorithm than but it potentially from more contention as the global best must be read before every recursive descent into a subtree to avoid exploring too many bad paths by contrast because uses a priority queue to order the paths it can read the global best less often once per tree level we adapted both versions of from code that was used in previous studies of stm performance our code uses the identical algorithm to the original code and expresses the parallelism in the same way the original code had a data race and we added one extra atomic block to eliminate that race our code is a transformation of that eliminates the priority queue checks the bound at each level of the tree and the recursion mesh refinement this code uses algorithm to find and eliminate bad ie those that do not satisfy some quality constraint from a of a mesh of points the program is nondeterministic since different orders of processing of bad elements lead to different although all such satisfy the quality constraints the program uses a foreach nd loop and each iteration of the loop a new thread at most one per core each thread has a private worklist of bad in each iteration of the worklist loop the selects one bad from the work list forms a around it the and adds any new bad back to the worklist finding and code sections access the shared mesh data structure and are in atomic blocks oo oo simulates a number of clients each performing a fixed number of queries on an database each query is in an atomic block the performance metric is the queries per unit time and we measure how this by varying the number of clients while keeping the number of queries performed by each one constant the program uses a foreach nd loop with one iteration corresponding to each client we it to use a number of clients equal to the number of threads so there is always one thread per client thus the total amount of work performed is proportional to the number of threads expressing parallelism we successfully expressed all the parallelism that did not use data races in these four nondeterministic algorithms as discussed above we eliminated a race in that was there to avoid synchronization we could have also written with a similar race the four codes do not use any deterministic algorithms but such algorithms do not any runtime performance overheads in our language such overheads are dominated by that of atomic sections in nondeterministic components the performance and expressivity of the language for deterministic algorithms were studied previously performance to evaluate performance we measured the speedup ie the speedup compared to running the transactional code on one thread achieved by the three codes we focused on speedup rather than absolute speedup because a optimizing the code generation for atomic statements has not been a focus of this paper and b the stm although using a good algorithm lacks many many essential performance features of a high performance java stm have the effect of factoring out some of the performance impact of the stm implementation while capturing the scalability of the benchmarks we ran and measured the codes on a core system using four intel e processors each with six running windows server figure shows the with barrier optimizations enabled using running times for and and scaling for oo because the are nondeterministic we ­ runs for each data point using an method to a few extreme for both variants we used the version of which was the faster of the two as the both versions of show good scaling and oo shows good scaling throughout the range of numbers of threads t we examined shows better speedup for smaller t this is because the parallel algorithm is very efficient in that range it rules out subtrees quickly and so only about of the tree nodes at t compared to t however the scaling curve for out as t increases most likely due to higher contention than the speedup curve for is it out and reaches only x on threads we the code to understand the source of this behavior and traced it to the method in the jvm this standard java function is extensively used in to index into lock tables we observed that the time spent in this function grows with the number of threads in which has large transactions this overhead affected the speedup curve this problem can by solved by modifying the jvm but we leave that and other optimizations for atomic to future work oo speedup number of threads figure for oo we the amount of work with the number of threads and measured speedup based on scaling number of queries done per unit time the barrier optimization was enabled for all of these benchmarks impact of barrier elimination we compared the performance of two versions of the parallel code for each benchmark with and without the barrier simplification optimization for nonatomic regions figure shows the improvement in running time for the optimized code compared to the code figure shows the reduction in the number of barriers due to our optimizations time oo figure ratio of optimized with barrier elimination to without barrier elimination a value lower than means the optimization increased performance of total barriers oo eliminated simplified remaining figure reduction in barriers due to optimizations showing the of barriers from the version that are eliminated entirely simplified to write barriers or that remain as full barriers in the optimized version for each of the three benchmarks with and threads the optimization has a substantial impact on performance for three of the four benchmarks and oo the performance improvements well with the barrier reductions the optimizations give essentially no improvement for because the transactions are very short reads and operations on the best as a result there are few if any barriers to remove and transactional overhead is not a significant component of the overall runtime on the other hand oo and use longer transactions providing more opportunities for reducing overhead our optimizations can eliminate barriers both by actually removing barrier operations on certain statements and also by reducing the number of times that transactions must be the latter effect occurs because removing unnecessary barriers reduces the number of false conflicts by the stm system as shown in table this effect is more with larger numbers of threads so our optimizations not only reduce scalar overheads but also improve scalability for example in the optimization changed this ratio from to on threads but from to on threads threads opt oo opt table ratio of committed transactions to started transactions for and oo lower numbers indicate more aborted transactions for both versions of all numbers are program oo total total annotated region effect table annotation counts for the four benchmarks in the middle columns the numbers in parentheses represent the number of annotations marked atomic in the last column xy means of y total method definitions in the program x were annotated with effect summaries annotation overhead table provides a quantitative measure of the annotation overhead of writing the four benchmarks in our language column after the vertical bar shows the total number of lines of source code counted by column gives the count of annotated lines as an absolute number and as a percentage of the total lines the following three columns show the number of region declarations including arguments to in arguments to types and methods and arguments to effect summaries and region parameters the number of annotations marked atomic is shown in parentheses after the main number the last column shows the number of effect summaries before the and the number of method definitions after the while the average number of annotated lines is nontrivial we believe it is not high given the strong safety properties of the programming model as in our prior work most of the annotations were arguments to types the overhead could be reduced by inferring some of the annotations but we leave that for future work our approach does impose the limitation that if a programmer to use a class region parameter as an atomic region in some context and a nonatomic region in some other context then the class must be the programmer must create two copies of the class one with the atomic parameter and one with the nonatomic parameter the is required because different barriers must be generated for methods of the class that operate on the parameter depending on whether the region bound to the parameter is atomic the could be done automatically by the compiler similarly to what c does for templates while we have not implemented this approach we believe it does not raise any significant technical issues in the benchmarks we studied only required class in we needed both atomic and nonatomic versions of the list and map structures used in the benchmark related work type and effect systems several researchers have described effect systems for enforcing a locking discipline in nondeterministic programs that prevents data races and deadlocks or guarantees isolation for critical sections et al have recently proposed a type system that guarantees for locks and other synchronization constructs using a construct called an interval for expressing parallelism while there is some overlap with our work in the guarantees provided race freedom deadlock freedom and isolation the mechanisms are very different explicit synchronization vs atomic statements supported by stm further these systems do not provide determinism by default finally there is no other effect system we know of that provides both race freedom and strong isolation together stm correctness language stm haskell provides an isolation guarantee but for a pure functional language that uses monads to limit effects to the transactional store unlike our imperative language moore and and abadi et al use types and effects to guarantee strong isolation for an imperative language but their languages permit races where neither access occurs in a transaction finally none of these languages allows both transactional and effects to the same memory as our language does et al show how to use a form of alias control called access permissions to verify that the placement of atomic blocks in a program respects the invariants of a specification written by the programmer for example that a condition is checked and upon atomically this approach is complementary to ours we provide guarantees of race freedom strong isolation and determinism by default for all programs in our language on top of that one could check that additional invariants are satisfied stm correctness compiler and runtime several guarantee strong isolation by preventing interference between transactions and accesses at runtime most of these systems use a combination of sophisticated static analysis runtime optimizations and other runtime techniques like page protection to optimize strong isolation while these techniques can significantly reduce the cost of strong isolation they cannot completely eliminate it in contrast our languagebased approach provides strong isolation without extra runtime overhead reducing stm overheads much research has been to reducing the cost of stm barriers on transactional memory accesses early work showed how to eliminate several classes of transactional overhead including redundant barriers barriers for accesses to provably immutable memory locations and certain barriers for accesses to objects allocated in a transaction recent work by et al uses the logic of program reads and writes within a transaction to reduce stm overhead for example a shared variable that is read several times can be be read once and locally these optimizations complement ours as they target different kinds of stm overhead from our work et al show how to use access permissions to remove stm synchronization overhead while the goals are the same as ours the mechanisms are different alias control vs type and effect annotations the two mechanisms have different tradeoffs in expressivity and power for example et als method can eliminate write barriers only if an object is accessed through a unique reference whereas our system can eliminate barriers for access through shared references so long as the access does not cause effects however alias restrictions can express some patterns such as unique references in a data struc ture that our system cannot as future work it would be interesting to explore these tradeoffs further finally several researchers have eliminated stm overhead for accesses to threadlocal data using static escape analysis and programmer annotations to specify code blocks that do not require instrumentation unlike our work this work either requires analysis or it relies on unverified programmer annotations nondeterministic parallel programming several research efforts are developing parallel models for nondeterministic codes with data access patterns such as mesh refinement galois provides a form of isolation but with iterations of parallel loops instead of atomic statements as the isolated computations concurrency is increased by detecting conflicts at the level of method calls instead of reads and writes and using semantic commutativity properties et al have proposed object as an alternative model for expressing computations these models are largely orthogonal to our work in galois strong isolation holds if all shared data is accessed through welldefined but this property is not enforced either statically or at runtime we believe that our type and effect mechanisms could be applied to galois to ensure this property the object model may have stronger isolation guarantees than galois but it is very specialized to graph computations in contrast to the more general model we present here et al have recently proposed task types as a way of enforcing a property they call atomicity this work shares with ours the broad goal of reducing the number of concurrent interleavings the programmer must consider however et al adopt an approach in which data is by default and sharing occur through special task objects this is in contrast to our approach of allowing familiar patterns of programming but using effect annotations to enforce safety properties finally none of the work discussed above provides any guarantee conclusion we have shown how to design a type and effect system that together with a weakly atomic runtime system achieves our stated goals of providing and safe nondeterminism including race freedom strong isolation of atomic operations and deterministic parallel operations compositional reasoning about deterministic and nondeterministic operations and determinism by default we have also shown how to the system to remove unnecessary barriers from the transactional implementation thereby performance acknowledgements this work was supported by the national science foundation under grants and and by intel microsoft and the university of illinois through illinois an anonymous us to study and provided helpful suggestions on a draft of this paper and the supporting proofs references application program interface version m abadi et al types for safe locking static race detection for java toplas m abadi et al semantics of transactional memory and automatic mutual exclusion in popl m abadi et al transactional memory with strong atomicity using memory protection hardware in ar et al compiler and runtime support for efficient software transactional memory in pldi y et al stm overhead with static analysis in m d allen et al sets a dynamic parallel execution model in a et al efficient deterministic parallelism n e et al verifying correct usage of atomic blocks and typestate in oopsla n e et al reducing stm overhead with access permissions in t et al a compiler and runtime system for deterministic multithreaded execution in intl conf on support for programming and operating e d et al safe multithreaded programming for cc in oopsla r d et al an efficient multithreaded runtime system r l et al a type and effect system for deterministic parallel java in oopsla r l et al parallel programming must be deterministic by default in r l jr an effect system and language for parallel programming phd thesis university of illinois il c et al ownership types for safe programming preventing data races and deadlocks in oopsla j checking interference with fractional permissions sas n g et al barrier optimization in a strongly isolated stm in popl z et al implementations of the concurrent collections programming model in s et al concurrent programming with and isolation types in oopsla m j et al a status report on the oo effort in oopsla l p mesh generation for in j et al deterministic shared memory in d et al transactional locking ii in c flanagan et al types for atomicity static checking and inference for java toplas a et al ct a flexible parallel programming model for architectures intel white paper t et al composable memory transactions in t et al optimizing memory transactions in pldi t and k fraser language support for lightweight transactions in oopsla b et al a programming model for concurrent objectoriented programs toplas a et al task types for atomicity in oopsla m et al optimistic parallelism requires abstractions in pldi j and r transactional memory synthesis on computer architecture morgan e a lee the problem with threads computer r et al parallel programming with object in oopsla m martin c and e lewis of transactional memory atomicity semantics ieee comp letters n d and t r a type system for protection and guaranteed initialization in oopsla k f moore and d highlevel smallstep operational semantics for transactions in popl m et al efficient deterministic in software in c the theory of database concurrency control computer science press inc m c rinard and m s lam the design implementation and evaluation of jade toplas f t v t and ar dynamic optimization for efficient strong atomicity in oopsla t et al enforcing isolation and ordering in stm in pldi m et al inferring method effect summaries for deterministic parallel java technical report u illinois c von et al implicit parallelism with ordered transactions in a et al safe futures for java in oopsla a et al revocation techniques for java concurrency concurrency and computation practice and experience r m et al the of software transactional memory why the going gets in 