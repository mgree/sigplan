pick your contexts well understanding the making of a precise and scalable pointer analysis department of computer science university of massachusetts ma usa and department of university of martin inc two ga usa david r school of computer science university of on nl g abstract has as an context abstraction for pointsto analysis in objectoriented languages despite its practical success however is understood for instance for a context depth of or higher past scalable implementations significantly from the original definition of an analysis the reason is that the analysis has many of freedom relating to which context elements are at every method call and object creation we offer a clean model for the analysis design space and discuss a formal and informal understanding of and of how to create good analyses the results are surprising in their extent we find that past implementations have made a choice of contexts to the of precision and performance we define a analysis that results in significantly higher precision and often performance for the exact same context depth we also introduce as an explicit approximation of that preserves high context quality at substantially reduced cost a pointsto analysis makes an use of types as context the context types are not dynamic types of objects involved in the analysis but instead upper bounds on the dynamic types of their allocator objects our results expose the influence of context choice on the quality of pointsto analysis and demonstrate to be an idea with major impact it advances the with a of analyses that simultaneously speed several times faster than an analogous analysis scalability comparable to analyses with much less and precision comparable to the best analysis with the same context depth categories and subject descriptors f logics and meanings of programs semantics of programming analysis d programming languages formal definitions and general terms algorithms languages performance permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm introduction pointsto analysis or pointer analysis in our context is one of the most fundamental static program analyses pointsto analysis consists of computing a static abstraction of all the data that a pointer expression or just a variable without loss of generality can point to during program runtime the analysis forms the basis for practically every other program analysis and is closely with mechanisms such as construction since the values of a pointer determine the target of dynamically resolved calls such as objectoriented dynamically method calls or functional lambda applications by nature the entire challenge of pointsto analysis is to pick approximations behind any attempt to track program control or dataflow precisely furthermore the global character and complicated nature of the analysis make it hard to determine how different analysis decisions interact with various language features for objectoriented and functional languages is a general approach that achieves tractable and high precision consists of local program variables and possibly heap object abstractions with context information the analysis information eg what objects this method argument can point to over all possible executions that result in the same context while separating all information for different contexts two main kinds of have been explored callsite sensitivity and ever since the introduction of by et al there has been evidence that it is a context abstraction for objectoriented programs yielding high precision relative to cost the success of has been such that in current practice analyses have almost completely traditional callsite analyses for objectoriented languages this paper is concerned with understanding in depth formalizing it and exploring design choices that produce even more scalable and precise analyses than current practice what is at a high level perhaps the way to describe the concept is by analogy and contrast to the callsite sensitivity a callsite analysis uses method ie labels of instructions that may call the method as context elements that is in oo terms the analysis information on local variables eg method arguments per ie sequence of k of method invocations that led to the current method call similarly the analysis information on heap objects per of method invocations that led to the objects allocation for instance in the code example below a callsite sensitive analysis unlike a analysis will distinguish the two of method foo on lines and this means that the analysis will treat foo separately for two cases that of its argument o pointing to anything may point to and that of o pointing to anything may point to class a void o class client void a a a in contrast uses object allocation sites ie labels of instructions containing a new statement as context elements hence a better name for might have been sensitivity that is when a method is called on an object the analysis the inferred facts depending on the allocation site of the receiver object ie the object on which the method is called as well as other allocation sites used as context thus in the above example a analysis will analyze foo separately depending on the allocation sites of the objects that a and a may point to it is not apparent from the above fragment neither whether a and a may point to different objects nor to how many objects the allocation site of the receiver object may be remote and to the method call itself similarly it is not possible to compare the precision of an and a callsite sensitive analysis in principle in this example it is not even clear whether the object sensitive analysis will examine all calls to foo as one case as two or as many more since this depends on the allocation sites of all objects that the analysis itself computes to flow into a and a note that our above description has been the analysis facts depending on the allocation site of the receiver object as well as other allocation sites what are these other allocation sites the first contribution of our paper consists of that there is confusion in the literature regarding this topic the original definition of see fig p defines the context of a method call to be the allocation site of the receiver object obj the allocation site of the allocator object obj of obj ie the receiver object of the method that made the allocation of obj the allocation site of the allocator object of obj and so on nevertheless subsequent analyses eg among many maintain the fundamental premise of using allocation sites as context elements yet differ in which allocation sites are used for instance the analysis in the framework uses as method context the allocation site of the receiver object and the allocation site of the caller object ie an object of class client and not a in our example in this paper we offer a unified formal framework that captures the analyses defined in the literature and allows a deeper understanding of their differences additionally we implement an array of analyses and draw insights about how the choice of context relates to scalability and precision we discover that the simple difference of how an analysis context is chosen results in large differences in precision and performance we use the name sensitivity to refer to a slight generalization of the original statement of by et al we argue that sensitivity is an choice in the design space while the choice of context made in past actual implementations is and results in substantial loss of precision concretely a practical outcome of our work is to establish a analysis with a object sensitive heap to as an analysis that is often though not always feasible with current technology and precise perhaps even more importantly our understanding of the impact of context on the effectiveness of an analysis leads to defining a new variant that combines scalability with good precision namely we introduce the idea of a analysis which is defined to be directly analogous to an analysis yet approximates some context elements using types instead of full allocation sites in contrast to past uses of types in pointsto analysis eg and see ryder for several examples we demonstrate that the types used as contexts should not be the types of the corresponding objects instead the precision of our analysis is due to replacing the allocation site of an object o which would be used as context in an analysis with an of the dynamic type of os allocator object the result is a substantial improvement that establishes a new in the practical tradeoff of pointsto analysis precision and performance in summary our work makes the following contributions · we offer a better understanding of the concept and variations of pointsto analyses our understanding relies on a precise formalism that captures the different analyses as well as on informal insights · we identify the differences in past analyses and analyze the influence of these differences on precision and performance we argue that is a substantially better choice than others used in actual practice we validate the impact of for the case of a context depth of the difference is significant in terms of precision and scalability our results help establish a analysis as the for precision in objectoriented programs among analyses that are often practically feasible · we introduce as a of the context information of an analysis in order to improve scalability we discuss what is a good context and show that the straightforward option of replacing an object by its type is bad instead we identify an choice of type context and demonstrate that it yields a surprisingly ideal combination of precision and scalability a analysis for a context depth of is several times x to high faster than a corresponding analysis while keeping almost the same precision in fact the runtime performance and scalability of a analysis often those of a analysis of a lower context depth while yielding more precision formalizing and variations we formalize analyses using an abstract interpretation over a simplified base language that closely captures the key operations that practical pointsto analyses perform for this we use java in form java is identical to ordinary java except that arguments to a function call must be atomically for example the body return becomes b f return f this shift does not change the expressive power of the language or the nature of the analysis but it simplifies the semantics and the language closer to the intermediate languages that practical pointsto analysis implementations operate on our formalism is an imperative variant with a call stack instead of continuations of the corresponding formalism of might and van horn which attempts a unified treatment of controlflow analysis in functional languages and pointsto analysis in languages the grammar below describes java some of the java conventions a a class declaration always names a superclass and lists fields dis from those in the superclass followed by a single constructor and a list of methods constructors are always taking in as many parameters as total fields in the class and and consisting of a call to the superclass constructor and assignment of the rest of the fields all in order class class c extends c cf k m k c cf m method c m c v cv s s stmt v e return v e exp v v f new c v cv v var is a set of variable names f is a set of field names c is a set of class names m is a set of method invocation sites lab is a set of labels every statement has a label to provide a convenient way of program points the function succ lab stmt yields the subsequent statement for a statements label concrete semantics we express the semantics using a smallstep state machine figure contains the state space a state consists of a statement a data stack for local variables a store a recording for each active method invocation the statement to return to the context to restore and the location that will hold the return value and a current context following earlier work our concrete semantics abstraction in that variables and fields are mapped to contextsensitive addresses and the store maps such addresses to objects the purpose by the concept of a contextsensitive address is to introduce an extra level of indirection through the store in the usual concrete semantics every dynamic instance of a variable or field will have a different context making contextsensitive addresses nevertheless the existence of the store makes it easy to information from different program paths as long as variables or fields map to the same contextsensitive address in addition to being a map from fields to contextsensitive addresses an object also stores its creation context there are two different kinds of context in this state space a context for local method variables and a heap context for object fields at a first approximation one can think of the two contexts as being the same sets any infinite sets can play the role of context by specific context sets we can simplify the mapping from concrete to abstract as well as capture the essence of the semantics are encoded as a smallstep transition relation × shown in figure there is one transition rule for each expression type plus an additional transition rule to account for return evaluation consists of finding all states reachable from an initial state typically a single call statement with an empty store and binding environment we use standard functions cons car cdr and to construct and for to prevent we note that the extra level of indirection is the only purpose of the store in our semantics specifically our store is not intended for modeling the java heap and our stack is not modeling the java local variable stack although it has similar structure as it is a map over local variables for instance the store is used to also map local variables to actual values which is the purpose of a java stack stmt × stack × store × × context st stack var store obj o obj × stmt × context × a var × context × c context is an infinite set of contexts hc is an infinite set of heap contexts figure for java variable reference v v st c succ st c where return return v st c s c where s c field reference v v f st c succ st c where f af af method invocation v st c s st c where m c m cv cv s mo m o oi hc o c merge hc c ai vi c aj vj c ai oi c st ai vj aj st object allocation v new c v st c succ st c where oi hc record c f f c o hc fi ai ai fi hc o ai oi v c v st c succ st c where figure concrete semantics for java convenience we define a lookup of a variable in a stack to mean a lookup in the top component of the stack eg means we also use functions m obj × method f the former takes a method invocation point and an object and returns the objects method that is called at that point which is not known without knowing the receiver object due to dynamic dispatch the latter returns a list of all fields in a class definition including superclass fields our semantics is parameterized by two functions that manipulate contexts record lab × context merge lab × × context context the record function is used every time an object is created in order to store a creation context with the object the merge function is used on every method invocation its first argument is the current call statement label while the second and third arguments are the context of allocation of the methods receiver object and the current callers context respectively the key for different of context sensitivity is to specify different record and merge functions for contexts for a simple understanding of the concrete semantics that yields the natural behavior one expects we can define contexts as just natural numbers context n in that case we need to ensure that the record and merge functions never return a duplicate context for instance if we consider labels to also map to we can capture the entire history of past context creation by defining record c · c and merge hc c · hc · c a different choice of context that is closer to infinite context semantics consists of defining a context as a list of labels context lab yielding a straightforward record function while the merge function can ignore its second argument record c cons c merge hc c cons c these definitions enable every variable to have a different address for different invocations and every object field to have a different address for each object allocated as expected abstract semantics it is now straightforward to express pointsto analyses by abstract interpretation of the above semantics for an abstract domain that maintains only finite context the abstract state space is shown in figure the main from the concrete are that the set of contexts is finite and that the store can return a set of objects instead of a single object generally the abstract semantics closely the concrete the abstract semantics are encoded as a smallstep transition relation × shown in figure there is one abstract transition rule for each expression type plus an additional transition rule to account for return we assume the usual properties for of a map to sets ie merging of the sets for the same value we similarly define abstract versions of the functions record lab × context merge lab × × context context technically this is true only because the fj calculus has no iteration so object allocations from the same statement can only occur after a recursive call thus the string of labels for method calls is enough to ensure that objects have a unique context stmt × stack × store × × context st stack var store p obj o obj × stmt × context × a var × context × c context is a finite set of contexts hc is a finite set of heap contexts figure analysis for java variable reference v v st c succ st c where return return v st c s c s c where field reference v v f st c succ st c where f a f a f method invocation v st c s st c where m c m cv cv s mo m o oi hc o c merge hc c ai vi c aj vj c ai oi c st ai vj aj st object allocation v new c v st c succ st c where oi f f c o hc fi ai hc record c ai fi hc o ai oi v c v st c succ st c where figure abstract semantics for java the abstract record and merge functions capture the essence of an analysis is distinguished by its storing a context together with every allocated object via record and by its that context and using it as the basis for the analysis context of every method on the object via merge analysis variations and observations with the above framework we can characterize all past analyses as well as discuss other possibilities all variations consist of only modifying the definitions of context record and merge original object sensitivity et al gave the original definition of but did not implement the analysis for context greater than taken the original definition that for an analysis the regular and the heap context consist of n labels context while record just keeps the first n elements of the context defined in our concrete semantics and merge everything but the receiver object context record c cons merge hc c hc in practical terms this definition has several consequences · the only labels used as context are labels from an object allocation site via the record function · on a method invocation only the heap context of the receiver object · the heap context of a newly allocated object is derived from the context of the method doing the allocation ie from the heap context of the object that is doing the allocation in other words the context used to analyze a method consists of the allocation site of the methods receiver object the allocation site of the object that allocated the methods receiver object the allocation site of the object that allocated the object that allocated the methods receiver object and so on in the next section we discuss whether this is a good choice of context for scalability and precision past implementations of object sensitivity for an analysis with context depth the context choice is obvious the merge function has to use some of the receiver object context or the analysis would not be just and the receiver object context consists of just the objects allocation site for analyses with context depth n however the definitions of merge and record can vary significantly actual implementations of such analyses from the et al definition notably the framework which provides the such implementation available merges the allocation site of the receiver object with multiple context elements from the caller object context when analyzing a method this results in the following functions record c cons merge hc c the practically interesting case is of n higher values are well outside current technology if the analysis is applied as defined ie the context depth applies to all program variables the above definition then means that every method is analyzed using as context a the allocation site of the receiver object and b the allocation site of the caller object heap context naming and sensitivity the above analyses defined the heap context and the context context to be the same set namely there are other interesting possibilities however since the merge function has access to a heap context and to a regular context and needs to build a new regular context the heap context can be for instance exploration of analyses studies in depth analyses where the heap context is always just a single allocation site lab context record c merge hc c in fact the above definition is what is most commonly called an analysis in the literature the analyses we saw earlier are called analyses with a contextsensitive or heap that is the pointsto analysis literature by convention uses context to apply only to methods while heap objects are represented by just their allocation site adding more context to object fields than just the object allocation site is designated with such as contextsensitive heap or heap in an analysis description thus one needs to be very careful with naming conventions we summarize ours at the end of this section another interesting possibility is that of keeping a deeper context for heap objects than for methods the most meaningful case in practice is the one where the heap object keeps one extra context element context record c cons c the merge function can vary as per our earlier discussion for n this is analysis which is currently considered the best tradeoff between scalability and precision and to which we refer repeatedly in our experimental evaluation of section this variation naming even more in detailed naming scheme the above analysis would be an analysis with an heap while our standard analysis with context is called an analysis with an n heap the reason for the convention is it is considered in the pointsto analysis community that the static abstraction for a heap object will consist of at least its allocation site label therefore the heap context is considered to be n labels when the static abstraction of an object consists of n labels in total in the rest of this paper we adopt the standard terminology of the pointsto analysis literature that is we talk of an analysis with an n heap to mean that context furthermore to distinguish between the two definitions of a merge function et als as opposed to that of the framework we refer to a analysis vs a analysis a analysis is characterized by a merge function merge hc c hc that is the full object abstraction of the receiver object is used as context for the method invoked on that object in contrast a analysis merges information from the receiver and the caller objects record c cons merge hc c thus the original definition by et al is a analysis while the practical analysis of the framework is we abbreviate analyses for different context and heap context to and analyses to when the two analyses coincide we use the abbreviation discussion finally note that our theoretical framework is not limited to traditional but also callsite sensitivity namely the merge function takes the current callsite label as an argument none of the actual analyses we saw above use this argument thus our framework suggests a generalized interpretation of what an analysis we believe that the essence of is not in only using labels as context but in storing an objects creation context and using it at the site of a method invocation ie the functionality of the merge function we expect that future analyses will explore this direction further possibly combining callsite and information in interesting ways our framework also allows the same highlevel structure of an analysis but with different context information preserved section a specific direction in this design space but generally we can produce analyses by using as context any abstractions computable from the arguments to functions record and merge with appropriate changes to the context and sets for instance we can use as context program location information module identifiers packages user annotations since these are uniquely identified by the current program statement label similarly we can make different context choices for different allocation sites or call sites by defining the record and merge functions on the supplied label in fact there are few examples of contextsensitive pointsto analyses that cannot be captured by our framework and simple extensions would suffice for most of those for instance a contextsensitive pointsto analysis that as context a static abstraction of the arguments of a method call and not just of the receiver object is currently not directly expressible in our formalism this approach has been in different static analyses eg for type inference nevertheless it is a simple matter to the form of the merge function and the method invocation rule in order to allow the method call context to also be a function of the heap contexts of argument objects insights on context choice with the benefit of our theoretical map of context choices for we next discuss what makes a scalable and precise analysis in practice vs the first question in our work is how to select which context elements to what is the best definition of the merge function for practical purposes we already saw the two main in the form of vs analyses consider the standard case of a analysis with a heap ie per the standard naming convention context lab the analysis will examine every method using as context the allocation site of the methods receiver object and the allocation site of the allocator of this receiver object recall that all information for method invocations under the same context will be merged while information under different contexts will be kept separate in contrast the analysis every method using as context the allocation site of the receiver object and the allocation site of the caller object the analysis has not been implemented or evaluated in practice before yet with an understanding of how context is employed there are strong conceptual reasons why one may expect to be to the insight is that context serves the purpose of yielding extra information to classify dynamic paths at high extra cost for added context depth thus for context to serve its purpose it needs to be a good classifier splitting the space of possibilities in roughly uniform partitions when context elements allocation site labels from the receiver and the caller object as in the analysis the two context elements are likely to be high means that a analysis is effectively reduced to a one in a simple case of context an object calls another method on itself the receiver object and the caller object are the same there are many more patterns of common object that we are executing a method of object p almost always yields significant information about which object q is the receiver of a method call wrapper patterns patterns the design pattern etc all have pairs of objects that are allocated and used together for such cases of related objects one can see the effect in intuitive terms the traditional mixed context of a analysis method calls by objects where were you and where was your sibling a analysis where were you and where was your parent the latter is a better since locations of are more context depth and analysis complexity the of context element and its effect on precision is generally important for analysis design the rule of is that context elements should be as as possible for an analysis with high precision to see this consider how context depth affects the scalability of an analysis there are two forces when context depth is increased on the one hand increased precision may help the analysis avoid combinatorial explosion on the other hand when will occur a deeper context analysis will be significantly less scalable for the first point consider the question when can an analysis with a deeper context one with a context concretely are there cases when a analysis will be faster or more scalable than a analysis for otherwise the same analysis logic ie both or much of the cost of evaluating an analysis is due to propagating matching facts consider for instance the variable reference rule from our abstract semantics in figure which results in an evaluation of the form for this evaluation to be faster under a deeper context the lookup should return substantially fewer facts ie the analysis context should result in much higher precision specifically two conditions need to be satisfied first the more detailed context should partition the facts well the redundancy should be minimal between partitions in that would project to the same partition with context depth intuitively adding an extra level of context should not cause all or many facts to be replicated this case can be detected statically by the analysis and context can be avoided this simple fix alone is not sufficient for reducing the of a analysis however for all or many extra context elements second fewer facts should be produced by the rule evaluation at context depth relative to depth when compared after projection down to depth facts in other words going to context depth should be often enough to tell us that some object does not in fact flow to a certain contextsensitive variable because the assignment is invalid given the more precise context knowledge in case of on the other hand the deeper context will almost always result in a combinatorial explosion of the possible facts and highly inefficient computation when going from a analysis to a or we have every fact analyzed in up to n times more contexts where n is the total number of context elements ie allocation sites as a rule of every extra level of context can multiply the space consumption and runtime of an analysis by a factor of n and possibly more since the collections of facts need to be used to index in other collections with indexing mechanisms and possibly results that may not be linear it is therefore expected that an analysis with deeper context will perform quite well when it to keep precise facts while in runtime complexity when the context is not sufficient to maintain precision unfortunately there are sources of in any real programming include reflection which is impossible to handle soundly and precisely static fields which arrays exceptions etc when this is not and affects large parts of a realistic program the pointsto analysis will almost certainly fail within any reasonable time and space bound this produces a scalability effect a given analysis on a program will either terminate quickly or will fail to terminate ever in practical terms input characteristics of the program eg size metrics are almost never good of whether the program will be easy to analyze as this property depends directly on the induced analysis the challenge then becomes whether we can maintain high precision while reducing the possibility for a combinatorial of analysis facts due to deeper context we next introduce a new approach in this direction if an analysis with a deeper context by nature results in a combinatorial explosion in complexity then a natural step is to reduce the base of the exponential function context elements in an analysis are object allocation sites and typical programs have too many allocation sites making the product of the number of allocation sites too high therefore a simple idea for scalability is to use approximations of objects as context instead of complete allocation sites this would the possible combinations down to a more space yielding improved scalability the most straightforward static abstraction that can approximate an object is a type which leads to our idea of a analysis definition of analysis a analysis is almost identical to an analysis but whereas an analysis would keep an allocation site as a context element a analysis keeps a type instead consider for instance a analysis with a heap the method context for this analysis consists of two types for now we do not care which types we discuss later what types yield high precision despite the name similarity our pointsto analysis has no relationship to controlflow analysis which uses types to filter control flow facts in a analysis expressed in our framework the analysis has lab × context note again the contrast of the standard convention of the pointsto analysis community and the structure of abstractions the analysis also includes an allocation site label in the static abstraction of an aspect is considered so essential that it is not reflected on the analysis name the allocation site of the object itself by a type would be possible but for precision generally and can be merged at any level as long as the merge function can be defined an interesting choice is an analysis that merely replaces one of the two allocation sites of with a type while leaving all the rest of the context elements we call this a analysis with a heap and the name to that is a analysis has lab context lab × the and the analyses are the most practically promising analyses with a context depth of their functions can be described with the help of an auxiliary function t lab which a type from an allocation site label for the context functions become record c c c c merge hc c c t c while for the two functions are record c c merge hc c t in other words the two analyses are variations of and not of with some of the context information to be types instead of allocation site labels the function t makes opaque the method we use to produce a type from an allocation site which we discuss next choice of type contexts just having a type as a context element does not tell us how good the context will be for ensuring choice of type is of importance the essence of understanding what a good type context is the question what does an allocation site tell us about types after all we want to use types as a approximation of allocation sites so we want to maintain most of the information that allocation sites imply regarding types the identity of an allocation site ie an instruction new a inside class c gives us · the dynamic type a of the allocated object · an upper bound c on the dynamic type of the allocator object since the allocation site occurs in a method of class c the allocator object must be of type c or a subclass of c that does not the method containing the allocation site we use common list and patternmatching notation to avoid long expressions eg record c c c means when the second argument c of record is a list of two elements c and c a straightforward option would be to define the t function to return just the type of the allocation site ie type a above this is an design decision however to see why consider first the case of a analysis when we analyze a method and the first element of the context is the type of the receiver object we are effectively most of the potential of the context the reason is that the method under analysis already gives us enough information about the type of the receiver object ie the identity of the method and the type of the receiver are closely if for instance the method being analyzed is ie method foo defined in class b then we already have a tight upper bound on the dynamic type of the receiver object the receiver objects type has to be either b or a subclass of b that does not method foo since we want to pick a context that is less with other information and yields meaningful a analysis should have its t function return the type in which the allocation takes place ie class c above a similar argument applies to a analysis in this analysis the method context consists of the receiver object as well as a type we want to be a good approximation of which would keep two allocation sites instead that of the receiver object and that of the receiver objects allocator object thus the two allocation sites of give us the following information about types · the dynamic type of the receiver object · an upper bound on the dynamic type of the receiver objects allocator object · the dynamic type of the receiver objects allocator object · an upper bound on the dynamic type of the receiver objects allocator object the first two pieces of information above come from the identity of the receiver objects allocation site and thus are kept in a analysis the question is which of the last two types we would like to keep the high of the second and third point above upper bound of a type and the type itself makes it clear that we want to keep the type of the last that is in all scenarios the function t l when l represents an instruction new a inside class c should return type c and not type a we validate this understanding as part of the results of the next section implementation and evaluation we implemented and evaluated several analyses for a context depth up to which meets the limit of for realworld programs the questions we want to answer relate to the main new ideas presented so far · is compared to in terms of precision and performance as argued in section recall that analyses had not been implemented in the past for context depth greater than · does the definition of function t matter as in section · does achieve higher scalability than regular analyses while maintaining most of the precision setting our implementation is in the context of the framework uses the datalog language to specify analyses additionally an explicit representation of relations listing all the elements of tuples of related elements explicitly as opposed to binary decision diagrams bdds which have often been used in pointsto analysis as we showed in earlier work bdds are only useful when the selected context abstractions introduce high redundancy while analyses that take care to avoid unnecessary are significantly faster and scalable in an explicit representation is a highly scalable framework and implements very efficiently the most complex and precise contextsensitive analyses in current use achieves functional equivalence identical results with and system which is another framework for precise analyses but based on an entirely different architecture using analysis specifications and bdds to represent relations this equivalence of results is useful for establishing that an analysis is correct and meaningful which is a property far from granted for complex pointsto analysis algorithms we use a bit machine with a e cpu only one thread was active at a time the machine has gb of ram but we have found no analysis that terminates within two but also allowing up to after more than gb of memory most analyses require less than gb with only the longest running analyses over seconds run time more memory this is of the scalability described earlier the explosion of contexts without a corresponding increase in precision makes an analysis quite we analyzed the benchmark programs with these benchmarks are the largest in the literature on contextsensitive pointsto analysis we analyzed all benchmarks except and with the full functionality and support for language features including native methods reflection a refinement of et als algorithm with support for invoked methods and constructors and precise exception handling generally our settings are a superset ie more complete feature support than prior published benchmarks on the language feature support is among the most complete in the literature as in detail in the past and could not be analyzed with reflection analysis cannot even be analyzed and cannot even be analyzed with the obj analysis this is due to introduced when reflection methods are not in any way by constant strings for classes fields or methods and the analysis infers a large number of reflection objects to flow to several variables eg in the worst case sites in can be inferred to allocate over abstract objects for these two applications our analysis has reflection reasoning since in the benchmark code has its main functionality called via reflection we had to its entry point manually vs figure shows the precision comparison of a and a analysis for a subset of the benchmarks for reference we also include a and analysis and indicate how the metrics change from an analysis to the next more precise one the metrics are a of core pointsto statistics and client analysis metrics the methodology of and for ease of reference we in some of the most important metrics the number of methods inferred to be reachable including both application methods and methods in the standard java library the average set ie how many allocation sites a variable can refer to the total number of call sites that are found to be polymorphic ie for which the analysis cannot a single method as the target of the dynamic dispatch and the total number of casts that edges reachable methods total reachable virtual call sites total polymorphic call sites application reachable virtual call sites application polymorphic call sites total reachable casts total casts that may fail application reachable casts application casts that may fail average average application edges reachable methods total reachable virtual call sites total polymorphic call sites application reachable virtual call sites application polymorphic call sites total reachable casts total casts that may fail application reachable casts application casts that may fail average average application edges reachable methods total reachable virtual call sites total polymorphic call sites application reachable virtual call sites application polymorphic call sites total reachable casts total casts that may fail application reachable casts application casts that may fail average average application edges reachable methods total reachable virtual call sites total polymorphic call sites application reachable virtual call sites application polymorphic call sites total reachable casts total casts that may fail application reachable casts application casts that may fail average average application edges reachable methods total reachable virtual call sites total polymorphic call sites application reachable virtual call sites application polymorphic call sites total reachable casts total casts that may fail application reachable casts application casts that may fail average average application obj figure precision metrics for and for a subset of the benchmarks the last two metrics average are in absolute numbers the rest are given relative to the immediately preceding column not relative to the numbers in the column all metrics are ie metrics is the main relation of a pointsto analysis linking a variable to the allocation sites it may be referring to the average is over variables reachable methods is the same as nodes hence the first two metrics show how precise is the inferred polymorphic are those for which the analysis cannot statically determine a unique receiver method casts that may fail are those for which the analysis cannot statically determine that they are safe may fail at runtime ie for which the analysis cannot statically determine that the cast is always safe these metrics a fairly complete picture of relative analysis precision although the rest of the metrics are useful for eg for examining how the statistics vary between application classes and system libraries for comparing to the total number of reachable call sites etc as can be seen in figure is almost always significantly more precise than even though both analyses have the same context depth the difference in precision is quite substantial for several metrics and programs eg multiple metrics for or reduction in total polymorphic virtual call sites for many programs the difference between and is as large as any other increment in precision eg from obj to perhaps most this precision is by substantially improved performance figure shows the running time of the analyses together with two key internal complexity metrics the number of edges in the contextsensitive ie how many methods are inferred to call how many other methods and the size of the contextsensitive set ie the total number of facts inferred that relate a variable with a allocation site the running time of is almost always much lower than that of the case of is most with in well under a third of the time of while achieving the much higher precision shown in figure the internal metrics show that makes use of its context and has substantially lower internal complexity than note that the statistics for contextsensitive are quite low why the analysis is faster since each variable needs to be examined only in a smaller number of contexts a second reason why such internal metrics are important is that the performance of an analysis depends very much on algorithmic and data structure implementation choices such as whether bdds are used to represent large relations internal metrics on the other hand are invariant and indicate a complexity of the analysis that often representation choices it is easy to see from the above figures that makes a much better choice of context than resulting in both increased precision and better performance in fact we have found the analysis to be a in the current set of analyses in terms of precision adding an extra level of context sensitivity for object fields yielding a analysis adds extremely little precision to the analysis results while greatly increasing the analysis cost in fact for the benchmarks shown is even significantly faster than the much less precise nevertheless the same is not true universally of the benchmarks in our evaluation set handles with ease the shown plus which has very similar behavior to but its running time for the other the analysis at least as for the benchmarks but a analysis handles out of and a obj analysis handles all of them in short the performance and internal complexity of a but analysis is when precision is maintained the analysis performs when however significant in the analysis does since the number of contexts increases in combinatorial fashion therefore achieves precision but is not a good point in the design space in terms of scalability in the past the only fully scalable uses of with depth have applied deep context to a small carefully selected subset of allocation sites we are interested in scalability when the whole program is analyzed with the precision of deep context this is the that we expect to address with analyses importance of type context choice in section we argued that for use of the extra context element function t has to be defined so that it returns the enclosing type of an allocation site and not the type that is being allocated this is very clearly the case figure demonstrates this for two of our benchmark programs the first and last which are representative of the rest is shown as a to the difference with the wrong type context a analysis is far more expensive and more precise than while with the right type context the analysis is scalable and precise very close to as we show later as can be seen the impact of a good context is highly significant both for scalability in terms of time and internal metrics and for precision in our subsequent discussion we assume that all analyses use the context as defined above precision and performance we found that fully meets its stated goal it yields analyses that are almost as precise as ones while being highly scalable in fact analyses seem to clearly other current both and seem better than in both precision and performance for most of our benchmarks and metrics our experiment space consists of the four precise analyses that appear feasible or with current capabilities and figure shows the results of our evaluation for of the benchmark programs there is some replication of numbers compared to the previous tables but this is limited to columns included as we omit for layout reasons since it behaves almost to we discuss the final benchmark in text but do not list it on the table because is the only of the four analyses that terminates on it note that the first two analyses are semantically incomparable in precision but every other pair has a provably more precise analysis so the issue concerns the amount of extra precision obtained and the running time cost specifically is guaranteed to be more precise than the other three analyses and is guaranteed to be more precise than either or the from our experiments are quite clear · although there is no guarantee is almost always more precise than hence the precision metrics reported in the table relative to the preceding column ie are showing negative numbers ie an improvement additionally is almost always for out of programs the analysis in our set in all but one case is several times faster than x faster or more for of the benchmarks the clear improvement of over is perhaps the most important of our experimental recall that is currently considered the of precision and scalability in practice a highly precise analysis that is still feasible for large programs without in complexity · achieves great scalability for fairly good precision it is the only analysis that terminates for all our benchmark programs it typically produces quite tight pointsto sets with being a significant exception that requires more in terms of client analyses and metrics the increase in pre time sec contextsensitive edges contextsensitive time sec contextsensitive edges contextsensitive time sec contextsensitive edges contextsensitive time sec contextsensitive edges contextsensitive time sec contextsensitive edges contextsensitive obj figure performance and complexity metrics for analyses is almost always faster than some cases are additionally makes good use of context and has often substantially lower internal metrics than and typically even than edges reachable reachable poly reach in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k bad context good context edges reachable reachable poly reach in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k bad context good context figure precision performance and internal complexity metrics for a analysis with a good and a bad choice of context the entries are the same as in figures and with metric names as in figure all but the last two precision metrics are reported as differences relative to the immediately preceding column ie we are showing how much more precision the good context yields over the already higher precision of the bad context not over the going from to is often greater than that of going from to overall is an approximation of given its low cost · although not shown on the table is not just faster than the three shown analyses but also faster than obj for out of benchmark programs the difference in precision between the two analyses is however a good example is the benchmark omitted from figure since is the only analysis with a contextsensitive heap that terminates for it processes slightly faster than obj sec instead of sec at the same time all precision metrics are better the pointsto sets are almost half the size total vs and for application vars only vs on other precision metrics the difference between and obj is much greater than that between obj and a analysis for application casts that may fail alone eliminates instances relative to obj · is highly precise and its difference from is significant it is to add the two columns and compare the difference of from relative to the difference of the former from at the same time avoids many of the scalability problems of it terminates on of benchmarks instead of out of and is always faster than eg by a significant factor conclusions in this paper we for a better understanding of the concept of in pointsto analysis our exploration led to a precise formal modeling to a complete mapping of past analyses in the literature as well as to insights on how context affects the precision and scalability of an analysis one concrete outcome of our work is to establish and especially a analysis as a choice of context compared to others in past literature additionally we have introduced the concept of and applied our insights to pick an appropriate type to use as context of a pointsto analysis the result is a range of analyses especially and that have very good to scalability while maintaining most of the precision of a much more expensive analysis the new analyses we introduced are current in the design space and represent a significant of the in pointsto analysis edges reachable reachable poly in app poly in app reachable casts casts that may fail reach casts in app app casts may fail app time sec cs edge k cs k edges reachable reachable poly in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k edges reachable reachable poly reach in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k edges reachable reachable poly reach in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k type h type full h edges reachable reachable poly reach in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k edges reachable reachable poly reach in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k edges reachable reachable poly reach in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k edges reachable reachable poly reach in app poly in app reachable casts casts that may fail reach casts in app casts in app may fail app time sec cs edge k cs k type h type full h figure precision performance and internal complexity metrics for precise analyses the entries are the same as in figures and with metric names as in figure all but the last two precision metrics are reported as differences relative to the immediately preceding column empty entries are due to nontermination after of running time some of the most interesting metrics are acknowledgments we thank the anonymous reviewers for their helpful comments as well as might and david van horn for interesting discussions this work was by the national science foundation under grants and as well as by the natural sciences and engineering research of references the cartesian product algorithm simple and precise type inference of parametric polymorphism in proceedings of the th european conference on objectoriented programming pages ­ london uk springerverlag martin and exception analysis and pointsto analysis better together in editor proceedings of the international symposium on software testing and analysis new york ny usa july martin and strictly declarative specification of sophisticated pointsto analyses in oopsla th annual acm sigplan conference on object oriented programming systems languages and applications new york ny usa acm cousot and cousot abstract interpretation a unified lattice model for static analysis of programs by construction or approximation of fixpoints in proceedings of the th acm symposium on principles of programming languages popl pages ­ new york ny usa acm g and effective typestate verification in the presence of aliasing in proceedings of the international symposium on software testing and analysis pages ­ new york ny usa acm benjamin c pierce and philip wadler java a minimal core calculus for java and gj acm trans program lang syst ­ program analysis using binary decision diagrams phd thesis university january and evaluating the benefits of contextsensitive pointsto analysis using a implementation acm trans ­ and relations as an abstraction for program analysis acm trans program lang syst ­ and evaluating the impact of on algorithm for java programs in michael d and thomas p editors pages ­ acm benjamin john and s lam reflection analysis for java in yi editor proceedings of the rd symposium on programming languages and systems volume springerverlag november matthew might and david van horn and exploiting the functional vs objectoriented program analysis in conf on programming language design and implementation pldi acm june and g ryder parameterized object sensitivity for pointsto analysis for java acm trans ­ aiken and john effective static race detection for java in proceedings of the acm sigplan conference on programming language design and implementation pldi pages ­ john and andrew a precise concrete type inference for objectoriented languages in proceedings of the annual conference on objectoriented programming systems language and applications oopsla pages ­ new york ny usa acm john controlflow analysis in proceedings of the acm sigplan workshop on ml pages ­ september g ryder dimensions of precision in reference analysis of objectoriented programming languages in editor compiler construction th international conference cc volume of lecture notes in computer science pages ­ springer and pnueli two approaches to interprocedural data flow analysis in s muchnick and d jones editors program flow analysis pages ­ nj prenticehall inc shivers controlflow analysis of higherorder languages phd thesis carnegie mellon university may and contextsensitive pointsto analysis for java in pldi proc of the acm sigplan conf on programming language design and implementation pages ­ new york ny usa acm lam and practical virtual method call resolution for java in proceedings of the conference on objectoriented programming systems languages and applications pages ­ acm press john michael and s lam using datalog with binary decision diagrams for program analysis in yi editor volume of lecture notes in computer science pages ­ springer john and s lam contextsensitive pointer alias analysis using binary decision diagrams in pldi proc of the acm sigplan conf on programming language design and implementation pages ­ new york ny usa acm 