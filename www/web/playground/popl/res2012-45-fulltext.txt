a type theory for probability density functions institute of technology new york university richard gray institute of technology abstract there has been great interest in creating probabilistic programming languages to simplify the coding of statistical tasks however there still does not exist a formal language that simultaneously provides continuous probability distributions the ability to naturally express custom probabilistic models and probability density functions this collection of features is necessary for fundamental statistical techniques we formalize the first probabilistic language that these features and it serves as a foundational framework for extending the ideas to more general languages particularly novel are our type system for continuous ac distributions those which permit and our pdf calculation procedure which for a large class of ac distributions our formalization the way toward the rigorous encoding of powerful statistical categories and subject descriptors f logics and meanings of programs semantics of programming analysis f logics and meanings of programs specifying and verifying and reasoning about verification g probability and statistics statistical computing general terms theory languages keywords continuous probability probability density functions introduction in the face of more complex data analysis needs both the machine learning and programming languages have recognized the need to express probabilistic and statistical computations this has led to a of probabilistic programming languages ­ ­ program transformations on probabilistic programs are crucial many techniques for converting statistical problems into efficient executable algorithms are syntactic in nature a rigorous language definition reasoning about the correctness of these program transformations however several fundamental statistical techniques cannot currently be encoded as program transformations because current languages have weak support for probability distributions on continuous or hybrid spaces in particular no existing language supports expressing the probability density function pdf of custom probability distributions this is an to statistics continuous distributions and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ pa usa copyright c acm their are in statistical theory and applications techniques such as maximum l le and kernel methods are all formulated in terms of the pdf specifically we want the ability to naturally express a probabilistic model over a discrete continuous or hybrid space and then mechanically obtain a usable form of its pdf usage of the pdf may direct numerical evaluation of the pdf or symbolic manipulation of the pdf and its derivatives continuous spaces pose some unique however first the existence of the pdf is not guaranteed unlike the discrete case second stating the conditions for existence involves the language of measure theory an area of mathematics for results that may not be straightforward notably obtaining a pdf from its distribution is a operation in the general case in light of these issues we make the following new contributions · we present a formal probability language with classical semantics which allows naturally expressing a variety of useful probability distributions on discrete continuous and hybrid spaces as well as their when they exist section the language is a core calculus which functions and mutation · we define a type system for continuous probability distributions ie those which permit a pdf the type system does not require algebras null sets the measure or other complex constructions from measure theory the key insight is to analyze a distribution by how it transforms other distributions instead of using the obvious induction on the monadic structure of the distribution section · we define a procedure that for a large class of distributions accepted by our type system the design permits adding knowledge about individual distributions with known but which cannot be calculated from enabling the procedure to proceed with programs that use these distributions as section we believe this is the first general treatment of in a language we omit features that are not essential to the current investigation eg expectation sampling finally we discuss the relation to existing and future work sections and in particular we save a treatment of in the context of conditional probability for future work background and motivation we first introduce probability in the context of countable spaces to emphasize the complications that arise when moving to continuous spaces we focus only on issues surrounding we from standard probability notation to in the standard notation and to create a notation throughout the paper in this section we present a specialized ac figure the and pdf of a standard normal distribution and the of a distribution that does not have a pdf count of probability for ease of we discuss the rigorous and generalized definitions in section we use the term discrete distribution for distributions on discrete spaces countable sets continuous distribution for distributions on the continuous spaces r and rn and hybrid distribution for distributions on products of discrete and continuous spaces that are themselves neither discrete nor continuous such as r × z probability on countable spaces consider a set of outcomes a for now let a be countable it is meant to represent the possible states of the world we are modeling such as the set of possible outcomes of an experiment or measure ments of a an event is a subset of a also understood as a predicate on elements of a events denote some occurrence of in and partition the outcomes into those that exhibit the property and those that do not a probability distribution p or simply on a is a function from events to such that px for all events x pa and p i xi i for countable sequences of mutually disjoint events xi distributions tell us the probability that different events will occur it is more convenient to work with a distributions probability mass function instead defined f x px which tells us how likely an individual outcome is it satisfies px f x xx for all events x on a for example if p is the distribution char the outcome of a fair its is given by f x where x a and a the ity an even number is is p moving to continuous spaces a probability density function pdf is the continuous of the unfortunately although every distribution on a countable set has a not every distribution on a continuous space has a pdf consider distributions on the real line we say that a function f is a pdf of a distribution p on r if for all events x px f x dx x which states that the probability of x is the integral of f on x in the simplest case x is an interval this idea can be extended to more general spaces this equation does not determine f uniquely but any two solutions f and f are equal almost everywhere see section and give identical results under integration thus we often refer to a pdf as the pdf for the spaces we consider in this paper the property of having a pdf is equivalent to being continuous ac roughly speaking a distribution is ac if it never assigns positive probability to events that have size zero in the underlying space for in the standard normal distribution is continuous and has the pdf x on the other hand the distribution of y in the model z x normal y x if z heads if z does not have a pdf we have used random variables to write the model this is a commonly used informal notation that is shorthand for a more rigorous expression that defines the model the model represents the following process flip a fair return if it is heads and sample from the standard normal distribution otherwise we can see that it is not ac the event occurs with probability whenever the comes up heads but has an interval length of zero we use the distribution function to vi each distribution figure the f of a distribution p on r is f x p x and gives the probability that a sample from the distribution takes a value less than or equal to x from equation we know that p has a pdf if and only if there ex a function f such that f x x f t dt clearly no such function exists for the of y due to the jump discrete and continuous types is not the only consider the following process sample a number u uniformly randomly from and return the vector x u u r the distribution of x a distribution on r is not ac the event x u u u has probability but x is a line segment and thus has zero area likewise there is no pdf on r we could integrate to give positive mass on this zero area line segment applications of the pdf the pdf is often used to compute and probabilities which are a special case of expectation expectation is a fundamental operation in probability and is used in defining such as mean and variance the expectation operation e of a distribution p on r is a higherorder function that satisfies eg gx · f x dx when p has a pdf f and the integral exists another application is maximum which addresses the problem of choosing the member of a family of distributions that best explains observed data let p · · be a parameterized family of distributions where p · is the distribution for a given parameter the estimate of p for observed data x is given by arg max f x where f · is the pdf of p · for example x could be a set of points in rn we wish to cluster and could be the estimate of the locations of the cluster p would be the family of distributions we believe generated the clusters a family parameterized by the positions of the cluster such as a model more details are available in challenges for language design probability distributions form a monad this structure forms the basis of many probabilistic languages because it is minimal elegant and presents many features first it provides the look and feel of informal random variable notation allowing us to express models as we normally would while remaining rigorous the monad structure probability as an embedded domain specific language or as a mathematical theory in a proof assistant additionally many proofs about distributions expressed in the probability monad are greatly simplified by the monadic structure we feel it is desirable to structure a language around the probability monad and we investigate supporting specifically in such languages the probability monad consists of monadic return and monadic bind as usual monadic return corresponds to the point mass distribution we also provide the uniform distribution as a monadic value these three combinators can be used to express a variety of distributions the main issue when designing a type system for absolute continuity is that return creates distributions on continuous types a core member of the monadic appears in the specification of nearly every distribution even those that are ac the obvious induction along the monadic structure is difficult to use to prove absolute continuity in the cases of interest consider for instance the joint distribution of two independent uniform random variables written in our language as var x random in var y random in return x y it is ac even though the subexpressions return x y and var y random in return x y are both not ac where we treat x and y as real numbers as by the typing rule for bind also as we have found it difficult to the rules for absolute continuity for example only the first of these distributions is ac even though they are all nearly identical to equation var x random in var y random in return x x y var x random in var y random in return x y y var x random in var y random in return x y x y clearly what is needed is a analysis we provide this in section a natural is to remove return to create a language in which only ac distributions are expressible we feel this is without return we would not be able to express something as simple as adding two random variables consider x y instead of x y in equation essentially return allows us to express random variables as transformations of other random fundamental modeling tool we feel should be supported allowing users to write down models that most naturally capture their domain without return we must extend the core calculus for each transformation we wish to use on random variables and we must do so carefully if we want to ensure that distributions remain this extension of the core from and and it developing the theory in a verification environment such as coq one of our eventual goals finally in addition to checking for existence we would like to also calculate a usable form for the pdf many current probabilistic languages focus on distributions with only finitely many alternatives which allows for implementing distributions as weighted lists of outcomes the probability monad in this case is similar to the list monad with some added logic describing how bind should propagate the weights the weighted lists correspond directly to the but no such straightforward computational strategy exists for the pdf we explore this further in section variables base types types expressions distributions programs contexts substitution x y z u v literals l bool z r × t dist x l op n if then else op inv · · fst snd exp log sin r of z e random return var x e in e p pdf e x x e ex free variables f v · figure the abstract syntax the language in this section we present the abstract syntax type system and semantics for our probabilistic language except for the parts related to which we cover in section abstract syntax figure contains the syntax definitions in addition to the standard letters for variables we also use u and v when we want to emphasize that a random variable is distributed according to the uniform distribution the syntactic category for literals includes boolean bool integer z and real number r literals types are stratified to ensure that distributions dist are only over base types integers are a distinct type from the reals there is no subtyping in the language we also terms to simplify analysis expressions and primitive operations take their standard mathematical meaning unless noted otherwise for simplicity we addition multiplication negation and integer literals on the integers and reals but there is a for integers and a separate for reals etc inversion inv denotes the operation and log is the natural we give our semantics in terms of classical mathematics so we do not concern ourselves with the issue of computation on the reals equality is defined on all base types in the usual way and is defined only on the numeric types we write n n as shorthand for n n the function r of z an integer into the reals the distribution random corresponds to the uniform distribution the next two constructs correspond to monadic return and bind for the probability monad the distribution return is the point mass distribution which assigns probability to the event a random variable distributed according to return is in fact deterministic there is no variation in the value it can take the bind construct var x e in e is used to build complex distributions from simpler ones it can be read introduce random variable x distributed according to the distribution e with scope in the distribution e it is the only binding construct in the language for simplicity we have chosen to omit and functions from our language but we use both in our examples we can use standard substitution rules to reduce such examples to the syntax of figure examples include if then else to booleans into the reals inv and if then else false finally free variables substitution and typing contexts are defined in the usual way the probability context is used to additionally keep track of the distributions that random variables are bound to when we use in places is expected the understanding is that the extra information carried by is ignored examples of expressible distributions with just random return and bind we can already construct a wide variety of distributions we might care to use in practice though we do not have a formal proof of this expressivity existing work on sampling suggests that this is the case random generation is concerned with generating samples from arbitrary distributions using only samples from uniform we can see the connection with our language if we view the constructs by a sampling analogy which understanding a distribution by its generating process the phrase var x e in e samples a value x from the sampling function e which is used to create a new sampling function e random samples from the uniform distribution return always returns as its sample for instance the standard normal distribution can be defined in our language using the sampling method normal var u random in var v random in return log u v where exp log in particular our language is amenable to inverse transform sampling likewise we can express other common continuous distributions uniform var u random in return u var u random in return log u normal var x normal in return x exponential var u random in return log u these are the standard standard exponential and normal distributions we the normal distribution by its standard instead of its variance for simplicity we define it as a transformation of a standard normal random variable and require we can also express discrete distributions such as the flip distribution flip var u random in return u which takes the value true with probability this is equivalent to the distribution in fact we can express any distribution with finitely many outcomes var u random in return if u then else if u then else a more satisfying definition would be possible if we had lists in the language the reason we do not is that although others have addressed recursion and iteration in the context of defining probability distributions we have not yet fully recursion with the absence of recursion also means that we do not support distributions in the style of sampling methods which values until a criterion is met furthermore we do not support infinite discrete distributions in the core language many of which are naturally described using recursion however in section we describe how to add special support for any distribution with a known pdf we also define some higherorder concepts the following functions are used to create joint distributions and models join e e var x e in var x e in return x x mix e e var z flip in var x e in var x e in return if z then x else x the model is created by a with the specified probability to determine which component distribution to ple from for instance a simple is given by mix normal normal we have defined discrete and continuous distributions and now we can use join to define nontrivial hybrid distributions as well such as join flip random which has type dist bool × r essentially enable models and tuples enable joint models these two concepts are special cases of hierarchical models which are models that are defined in stages distributions defined using nested instances of bind correspond to hierarchical models these examples are all ac but we can also express distributions such as the example from section written mix return normal in our language will successfully type check as a distribution but the program pdf will be it should is not continuous the ability to represent distributions even though they cannot be used in programs is in of future language features such as expectation and sampling which can be used with distributions measure theory measure theory is the basis of modern probability theory and the concepts of discrete and continuous probability distributions it is a precise way of defining the notion of volume we develop our formalization within this framework we give only a brief overview of the necessary concepts details are available in let a be a set we wish to measure a algebra m on a is a subset of the powerset pa that contains a and is closed under complement and countable union the pair a m is a measurable space a subset x of a is if x m in the context of probability a is the set of outcomes and m is the set of events for a function f a b the f image of a subset x of a written f x denotes the set f x x a and the f of a subset y of b written f y denotes the set x a f x y when f is on measurable spaces we say f is ma when the f of any set is measurable functions are closed under composition we drop the prefix and say measurable for functions or sets when it is clear what the algebras are the algebra machinery is needed to ensure a consistent theory there are spaces which contain sets that violate intuition about volume eg the ball measure theory these issues by measurable sets and functions as much as possible when a is countable no such problems arise and we can always take pa for m measures a nonnegative function m r is a sure if x for all x in m and i xi for all sequences of mutually disjoint xi i xi countable the triple a m is a measure space if addition a then is a probability measure written p and the triple is a probability space we use the terms probability measure probability distribution and distribution inter we use c to denote the counting measure which uses the number of elements of a set as the sets measure we use l to denote the measure on r which assigns the length b a to an open interval a b the sizes of other sets can be un by and countable unions of intervals the product measure a b of two measures a and b on measurable spaces a ma and b mb is the measure on a × b and the product algebra ma mb such that x × y ax · by for x ma and y mb the measure is unique when a and b are finite the condition is a technical condition that is satisfied by all measures we will consider in this paper and requires that the space can be covered by a countable number of pieces of finite measure null sets a measurable set x is null if x x is said to have measure zero the empty set is always null the only set is the empty set and all countable subsets of r are a propositional function holds almost everywhere ae if the set of elements for which the proposition does not hold is null for instance two functions of type r r are equal everywhere if they differ at only a countable number of points a measure space a m is complete if all subsets of any null set are completion is an operation that takes any measure space a m and produces an equivalent complete measure space a m such that x x for x m null sets are in measure theory so it will be to work in spaces that support null sets as much as possible thus completion makes measure spaces to work with the measure ln is the completed product of l for measures and on a measurable space is continuous with respect to if each null set is also null integration a fundamental operation involving measures is the abstract integral a generalization of the integral that avoids some of its the abstract integral of a measurable function f a r wrt a measure on a is written f d the integral is always defined for nonnegative f the integral for arbitrary f is defined in terms of the positive and negative parts of f and may not exist if it does we say f is we write x f d as shorthand for x x x · f x d which restricts the integral to the subset x we write x for the function on x expectation refers to abstract integration wrt a distribution the abstract integral satisfies x x d for all measurable x in terms of probability it says that the probability of x is the expectation of x another consequence is that null sets cannot affect integration two functions that are equal ae give the same results under integration wrt abstract integration wrt c and l is ordinary possibly infinite and the ordinary integral respectively the integral with the integral on functions to conclude that a distribution such as var x e in return f x is wellformed we are to verify that f is a measurable function however sets and functions are actually quite and constructing them requires the axiom of choice none of the constructs in our language are as powerful as the axiom of choice though we do not have a formal proof of this thus all expressions represent measurable functions this the obligation and we do not make any further mention of checking for spaces for most applications we often have a standard idea of how spaces are measured we now formalize this practice a space a is a space if it comes equipped with a complete measure space a ma a which is the measure space of a we call ma the algebra of a and a the measure of a the abstract integral wrt a is the integral of a we define measure spaces for the spaces b true false z r and product spaces between spaces as follows mb b pb c mz z pz c mr r the sets l mb a b this definition matches what is used in practice eg c becomes the measure for countable spaces and ln becomes the measure for rn for the rest of the paper we assume spaces are unless random dist r return dist e dist var x x e in e e dist dist figure standard monadic typing rules for distributions explicitly noted otherwise we say that a distribution on a is ac if it is ac with respect to a a function f is a pdf of a distribution p on a if px x f da for all measurable x expectation can be written using the pdf g dp x gx · f x da a joint pdf is the pdf of a joint distribution which is simply a distribution on a product space we later use the fact that the joint pdf f of a model such as x p x p · x can be written as the product of the individual parameterized f x x fx · fx x type system and semantics for distributions we now discuss the type system and semantics for syntactic categories besides programs the type system for expressions is ordinary we assume an external mechanism for enforcing the preconditions necessary to ensure totality of functions such as an automated theorem prover or the programmer for instance log must be applied to only positive real numbers distributions standard monadic typing rules figure the random variables introduced by bind are really just normal variables and are typed as such calling them random variables is a about the role they play the typing rules ensure that random variables are never used outside a probabilistic context we give our language a semantics based in classical mathematics with total functions base types have the usual meaning the denotation of dist is the set of distributions possible on t dist p t m p is a probability space we measure space notation for types thus m and are shorthand for mt and t let ee be the denotation of a distribution e under the environment also overloaded for expressions expressions have the semantics of their corresponding forms from classical mathematics as stated before random is the uniform distribution x lx which says that the probability of an event x is its interval size on return is the point mass distribution x x e which gives an event x probability as long as it includes the outcome bind expresses the law of total probability x e in e y x f x y dp where f x x and p ee the family of distributions e is parameterized by the variable x in essence the probability of an event y is the average the of what each member of the family is the probability of y the integral exists because it is the expectation of a bounded function type system and semantics for programs the program pdf e is wellformed if the distribution e permits a pdf the following theorem gives us a sufficient condition theorem for any two finite measures and on the same measurable space such that is continuous wrt there is a function f such that x x f d we call f a derivative of with respect to denoted dd pdf corresponds to the operator the condition is also necessary given a satisfying f is trivially ac all measures we define and all distributions are finite so for our purposes absolute continuity is equivalent to a pdf though not necessarily unique derivatives are equal almost everywhere when is the counting measure the derivative is a for hybrid spaces it is a function which must be along one dimension and integrated along the other to obtain as probabilities derivatives unify and of the two for this reason we refer to all of these as we use when we want to emphasize its discrete nature defining a type system for absolute continuity in terms of the straightforward induction on distribution terms proves suppose we want to check if the distribution in equation is ac we must verify that the probability of any set z is zero a straightforward induction leads us to trying to show px var y random in return x y z where p is the uniform distribution and we have notation slightly by object language syntax with ordinary mathematics this states that the body of the outermost bind assigns z probability zero always it is how to proceed from here or how to remove concepts like null sets from the we take an alternate approach based on the insight that we can reason about a distribution by examining how it transforms other distributions our approach and outline of the following is as follows · we introduce the new notion of a function and prove a transformation theorem stating that when a random variable is transformed the output distribution is ac if the input distribution is ac and the transformation is we also prove some results about functions · we define the random variables transform of any distribution written in our language and show that for a large class of distributions the transformation theorem is applicable · we present a type system which defines absolute continuity of a distribution in terms of whether its rv transform is as we have found it easier to come up with the rules for functions concepts like algebras null sets and the measure while present in the metatheory do not need to be for implementing the type checker also due to the foundation we correctly handle cases that are not typically explained such as on hybrid spaces we conclude the section with the semantics of programs absolute continuity and functions a function h a b is if the of each set is of null sets are always null and forward of nonnull sets are always nonnull a function that fails to be is called the next theorem establishes the link between absolute continuity and theorem transformation for a function h a b and an ac distribution p on a the distribution on b is ac if h is proof let y be a set by the of h the set hy is by the absolute continuity of p we have that y is also this style of defining q may seem odd but it actually the use of random variables as a modeling language for instance the model x p y hx the relationship where q is the distribution of y in general the reverse direction does not hold h can be even if q is ac this happens when h has behavior only in regions of the space where p is assigning zero probability this will be a source of incompleteness in the type system lemma discrete domain a function h a r is if a is nonempty and countable proof let x be an element of a the set x has positive counting measure while its which is a singleton set is this implies r of z is meaning that when we view an integer random variable as a real random variable it its ability to have a pdf this is desirable behavior different spaces have different ideas of what it means to be a pdf we would not want to mark an integer random variable as ac and later attempt to integrate its in a context a real random variable lemma discrete codomain a function h a b is if b is countable proof the of the empty set the only set is the empty set which is always null this reasoning the fact that distributions on countable spaces always have a lemma interval a function h r r is if it is constant on any interval proof let h be constant on a b a b is not but its a singleton set is one way to how this leads to a distribution is to observe that the transformation h takes all the probability mass along a b and it onto a single point in the target space lemma inverse an invertible function h r r is if its inverse h is an continuous function proof we have discussed absolute continuity of measures the absolute continuity of functions is a related idea it is a stronger notion than continuity and uniform continuity continuous functions are well in many ways in particular the of null sets are also null sets coupled with the fact that an is an this proves the claim more details on continuous functions can be found in this result shows that log exp and linear functions are we believe the idea can be extended without much difficulty to show that functions with a countable number of invertible pieces such as the functions and polynomials are also lemma for functions c a b and f g h a b where hx if cx then f x else gx h is if f and g are proof let y be a set the set hy is a subset of f y gy and is thus by of f and g and the countable and completeness of a lemma composition the set of functions is closed under function composition proof let f a b and g b c be functions and h let h z f g gf set the z is given by of f and g lemma projection the function hx y x of type a × b a is proof let x be a set its is x × b by the properties of product measure we have that × b ax · bb · bb even if bb the definition of multiplication on extended nonnegative reals defines · along these lines we can show that returning a permutation of a subset of tuple components is also a function the last two results permit us to ignore arguments when reasoning about the of the body of a function distributions and rv transforms a large class of distributions in our language can be understood by equation from the syntax we know that a distribution e must take the form of zero or more nested binds terminating in a body that is either random or return we focus on the latter nontrivial case the expression represents a transformation of the random variables xi introduced by the binds the function x xn is the random variables transform rv transform of the distribution e where we use tuple pattern matching shorthand to name the components of a tuple argument the correspondence between distributions in our language and theorem is as follows let q be the denotation of e let h be the rv transform of e and let p be the joint distribution of the random variables introduced on the spine of e the class of distributions for which the theorem is applicable is given by the set of distributions for which each ei is ac wrt the random variables preceding it where ei is the distribution corresponding to xi in other words the distribution for ei must be ac while treating free occurrences of x xi as fixed unknown constants this ensures that the joint distribution is also ac the joint pdf can be written as the product of the individual parameterized this is a commonly used implicit assumption in practice for example the distribution var u random in var z flip u in return u z has the rv transform u z u z which has type r × b r and is transforming the joint distribution of random and e flip u the variable u appears free in e making e parametric in u the restriction requires that e is ac for all possible values of u which is the case here two equivalent distributions may have different rv transforms and because of different representations to show that this choice of p q and h satisfies equation we to the semantics of distributions defined in section consider the general case random ac nn return ac e ac e dist x e x e ac var x e in e ac figure the absolute continuity judgment e ac e var xi ei in return where we have used the bar as shorthand for nested binds the denotation q of e under an environment is given by xi y xi where pi is the denotation of ei extending as necessary and we have again used the bar notation to denote iterated expectation and the repeated extension of the environment with variable mappings we can now rewrite the to use their corresponding fi and then replace the iterated with a single product integral using their joint pdf f di xi x xi · y xi d x f x · y hx dp x y hx px hx y where x x xn hx xi i is the type of each xi and is their product we have also used the fact that the expectation of the function on a set is the probability of that set the set here is x hx y not y replacing an iterated integral with a product integral is not always legal but is possible here because the integral is of a nonnegative function wrt independent measures see theorem type system for programs all judgments are defined modulo conversion a program pdf e is wellformed if e is an ac distribution e dist holds for some and e ac holds if the judgment e ac figure holds then e is an ac distribution under the probability context and the active variable context where is given by the grammar x variables in are currently active and should be understood in a probabilistic sense while those not in are and should be treated as fixed parameters the contexts the following invariant is always the prefix of ie the variables in correspond directly to the n most recent entries added to where n is the length of rule asserts that the uniform distribution is ac the main action of rules and is to a call to the judgment for theorem to be applicable a distribution along the spine must be ac wrt the random variables preceding it thus in we check that e is ac without marking any current random variables as active we reach the body of the rv transform in roughly speaking pointing into and correspond to p and h in theorem next is the judgment figure if nn holds then represents the body of a function under and the variables in are the arguments to the rv transform throughout this discussion we implicitly use the composition and projection lemmas lemmas and to ignore arguments during analysis for example in rule we could x x nn countable nn nn op inv log exp sin op nn nn nn if then else nn nn op fst snd op nn nn nn nn xi x xn are distinct x xn nn nn nn f v nn nn nn nn l l nn nn figure the judgment nn be analyzing a function with multiple inputs but we can drop all of them but x leaving us to analyze the function x x which is trivially under the what we are actually doing is representing the original transform as the composition of a function that selects a single components of a tuple with the identity function x x the composition lemma is also the justification for being able to into subexpressions rule is merely an application of lemma the types bool z and products define the countable types note that this covers the cases of integer and and boolean and integer literals rules and are direct translations of lemmas and the injection from integers into the reals is lemma so there is no rule for r of z rule expresses the idea that the joint distribution of independent ac distributions is ac if holds then and represent independent distributions under and its definition is f v f v where x xx x it states that and must not have any ancestors in common the function x computes the ancestors of a random variable x a random variable y is the parent of a random variable x if y appears free in the distribution that x is bound to rule corresponds to the corollary of lemma that states that you can drop and tuple components the requirement that the variables are distinct is important the distribution var u random in return u u is not ac as we saw in section we have multiple rules for addition because they each capture a different usage of plus rule states that if the formation of the pair is then is also because it is the composition of tuple formation with r × r r where the latter is by corollary to lemma rule represents the idea of composing with the function x x c where c is a constant wrt the arguments of the rv transform there is an analogous rule for when the constant appears as the left operand rules and are analogous note that is slightly weaker than its counterpart only because it needs to prove that the scaling is nonzero discussion we believe our type system is sound the only remaining case to prove is the soundness of the reduction to is given by theorem and the soundness of the other cases in the judgment are covered by the lemmas in section stating the needed lemma for essentially requires formalizing the idea that the conditional distribution of the second component on the first component should be ac in terms the second component should still have a degree of freedom even after the first stating this involves conditional probability putting it outside the scope of the current work there are few sources of incompleteness in our type system for instance conservatively requires and to be independent the distribution var x random in var y random in return exp x x y is ac despite the fact that the tuple components are not independent even if we know the value of exp x the residual in the x y is still ac the joint pdf is given by the pdf of the first component by the conditional pdf of the second component on the first this is a similar issue as the parametric ac requirement on spine distributions this generalization of is interesting future work likewise conservatively requires both branches of an to be the distribution var x normal in x then min x else max x is not accepted as ac because both branches x min x and x max x are even though the distribution is equivalent to var x normal in return x which is ac we define min and max in the usual way using if finally is sufficient but not necessary for absolute continuity to hold for instance the rv transform of var x random in return if x then x else is x if x then x else which is due to the constant portion thus our type system does not accept this distribution as ac however x only takes values on so the second branch is never entered and thus the distribution is equivalent to the ac distribution var x random in return x semantics of programs the denotation of a program pdf e is that it is a member of the set of derivatives of the distribution e pdf e f x px f d x where p ee is the denotation of e under the empty environment and e has type dist the procedure discussed in the next section a member of this set calculating density functions the previous sections have defined a language in which it is possible to express our goal now is to mechanically obtain a usable form of the pdf for a given distribution but what a usable form we are motivated by applications of the pdf and the need to interface with existing software for instance we may want to use numerical optimization software to perform where the pdf appears in the objective function we may also want to symbolically derive information to improve the search or we may want to use the pdf to calculate an expectation using a numerical roughly speaking we call a term usable if we can map it onto the capabilities of existing software in with common practice for example the term x x is target types target terms x typing r r semantics e e d figure the target language usable in practice real addition is mapped to floating point tion likewise x dx is usable the integral is and in a form accepted by computer algebra systems cas and on the other hand terms like g dp and make use of operations such as abstract tion and the derivative current software do not handle these operations though progress on measure theory has been made thus the basic plan is to eliminate concepts during pdf calculation this means the constructs random return bind and pdf should not appear in a pdf term because they involve measure theory it will take some to remove the derivative pdf it has been shown that the tive is a operator given a distribution there is no general computable procedure for computing its pdf the dis case at least the fact that the has a straightforward definition in terms of its distribution if p is an executable imple of a discrete distribution an executable implementation of its is given by x px in general however we will need to the calculation of with a collection of tech our basic approach is as follows first we define a target language that defines what a usable form second we provide a procedure that converts many distributions accepted as ac by our type system into expressed in the target language some rv transforms are so we will not be able to calculate certain from in particular dependence between random variables makes the general case however the design permits adding knowledge about individual distributions with known enabling the pro to calculate for programs that use these distributions as this allows us to handle many useful cases the target language the target language extends expressions with abstraction application and the integral figure we treat functions in a standard way we skip specifying in abstractions when the choice of is clear computing solutions for is not always feasible or possible so cannot be completely eliminated from the target language the integral is wellformed if its is and a function f is if f d is finite we require users of the target language compiler writers to manually ensure this is reasonable for a language we have verified for each use of integration in the compilers presented in this section although a concept integration is close enough to the notion of integration used by numerical and symbolic solvers to be useful as a compilation target recall integration over c and l is ordinary and integration respectively for most applications integration will coincide with integration random x r x x return e e x var x e in e figure the probability compiler e random x r x e dist x e x var x e in e return e figure the e the probability compiler we need to calculate probabilities as a subroutine of pdf calculation we achieve this by translating distributions into terms the probability compiler e performs this translation figure it takes a distribution e of type dist and a function from to and returns the expectation of wrt e when is the function on a set x is the of x for instance suppose we want to know the probability that a sample from flip is true we invoke the probability compiler with e flip and z bool z producing x r x u z z u x for which is equivalent to x dx as expected likewise to derive the probability that a standard normal random variable within a standard of its mean we would invoke the probability compiler with e normal and x x details on how this computes probabilities are given by kozen and can also be understood by the expectation monad we also need the judgment which invokes the probability compiler on the distribution corresponding to the rv transform body in the context the pdf calculation procedure we structure the pdf calculation procedure as we did the type system the judgment on distributions a call to the judgment on rv transforms the pdf of a wellformed program pdf e is given by the satisfying e the judgment e the pdf of the distribution e under and figure rule gives the pdf of uniform the function on rules and build the contexts and invoke the next compiler the real work begins in the judgment which computes the pdf corresponding to the rv transform body under and we present this judgment in two parts one each for univariate and multivariate transforms the multivariate transforms must deal with the issue of dependence between inputs or between outputs of the transform univariate transforms we use univariate for rv transforms between spaces that are not product spaces the correctness of rules pexp and is given by the following lemma log x r exp x exp x exp x r log x x pexp f v x r x l l x r xl l x r x inv x r x x x figure the univariate cases lemma for continuous distributions p and q on r and a function h r r such that if h is strictly increasing and invertible then the function gy f hy · d hy dy is a pdf of q where f is the derivative of the f of p proof the derivative of a is a pdf the g of q is gy q y ph y p hy f hy where we have used the fact that the of y is hy because h is strictly increasing and invertible the claim follows from the fact that g is the derivative of g the lemma is easily modified for and also an extra sign appears because they consist of strictly decreasing components it is possible to define a version of for negative literals as well as integer versions of and with these rules and discussed below we can already compute some continuous consider the standard exponential from section we derive its pdf with exponential which builds the contexts u and u r random and invokes the chain log u x x log u x exp x exp x u x exp x exp x we reduce for clarity the chain ends with which gives the pdf of uniform for then and produce and the latter is equivalent to x x exp x which is easily seen to be the pdf of the standard exponential likewise the pdf of uniform is correctly calculated to be x x which is equivalent to x x we do not provide rules for sin and because we are of any simple expression for the corresponding multivariate transforms we use multivariate for rv transforms to or from a product space the presence of multiple dimensions introduces the issue of dependence between the inputs or between the outputs of the transform making it difficult to provide rules that work in the general case as a result some of the following rules introduce specific independence requirements l l countable x x l bool x bool x bool x if x then else ii i if then else ii x true x false x x y ym j x x y ym x xn x xn fst x y ym x xn j y ym y x y x x i i i x x x r t i r i i x t x figure the multivariate cases j e j j x e x x figure the joint pdf body constructor j rule states that the of a point mass distribution on l is simply the function on l the transforms corresponding to the rules in this section tend to be less obvious the transform in question for is the constant function on l whose argument may be a tuple rule the of a boolean random variable which is a simple expression of the probability that the random variable is true we thus invoke the probability compiler in the current context to compute this probability this rule covers the cases for and the ability to represent the of a boolean random variable allows us to encode arbitrary probability queries rule computes the pdf of a which is a weighted combination of the component where the probability is the probability the is true for this to be valid the must be independent of its branches as required for instance the pdf of var x random in var y uniform in return if x then x else y is not equivalent to x x x as would be calculated without the restriction there should be no probability mass on rule is a special case of the transform corresponding to is a function that returns a permutation of a subset of components of its tuple argument we assume x xn and y ym are distinct and we use to denote disjoint union the resulting pdf is a pdf the pdf of a joint pdf f on a × b is given by gx y f x y db g is a pdf on a whose density at x is given by adding up the contribution of the joint pdf along the other dimension b the corresponding process is one which generates tuples but then the second component returning the first we generalize to higher dimensions by integrating out random variables not appearing in the result tuple when this set is empty m the integral re to the resulting pdf may be computationally inefficient due to a large number of nested more efficient schemes that take advantage of the graphical structure of the probabilistic model such as variable elimination are possible the judgment j constructs the body of the joint pdf of the active random variables figure rule first computes the pdf of e parametric in all of the preceding random variables thus invoking the with no active random variables it then constructs the product with the of the remaining active variables the product of these parametric is the joint pdf the terms and in have type r and r respectively the judgment returns an open term and relies on the fact that the free variables will be bound appropriately by the invoking judgment rule is analogous to we ask for a pdf and compute the pdf of the first component we define an analogous rule for snd rules and state the well known results that the joint pdf and the pdf of the sum of independent random variables is the product of and of their individual respectively on the face of it these rules handle models and joint models but where they really is on general hierarchical models for example the pdf of var x random in var y uniform x in return y is not immediately obvious the process is generated by sampling a value x uniformly from and then sampling uniformly from x x we calculate the pdf with which builds y r uniform x x r random and y x for y rule then produces y x yx x x for where we have reduced for clarity the body of the inner abstraction is generated by the joint pdf body constructor the two nontrivial are the parametric pdf of uniform x and the pdf of random respectively with some manipulation we can show corresponds to f y y x dx for y and zero otherwise the rules do not perform algebraic simplifications but the benefit of automation can still be clearly modularity some rv transforms are to work with preventing us from calculating certain for example we cannot calculate the pdf of normal from because its specification uses which we do not handle however the design allows us to address cases like this where we want to handle the pdf for a specific distribution we can add the rule normal where x is the pdf of the standard normal this new rule is used by the joint body constructor whenever normal appears on the spine of a distribution enabling the calculation of for hierarchical models using normal that were previously not for example the pdf of normal can now be calculated as x x x x x x x x x using the rules and where x and x r normal we can see is equivalent to the classic formula for the normal pdf f x exp x likewise we can now handle distributions like the and to support an infinite discrete distribution with a known pdf such as the distribution we can add a new primitive to the core calculus and handle it in the related work our work builds on a long of probabilistic functional languages most connected to the probability monad in some way they work by semantics into a functional language so that one can express values which represent a distribution over possible outcomes the distribution can either be manifest available to the programmer or implicit existing only in the metatheory an early of the latter was given by kozen in in which he provides the semantics for an imperative language with a random number primitive samples from uniform values of type a in the object language are given semantics in functions of type a in the metatheory these functions represent distributions over a and satisfy the expected laws for measures work is and will continue to future languages it can accommodate continuous and hybrid distributions it handles unbounded iteration general recursion a traditionally issue for probabilistic languages and it even provides a treatment of distributions on function types however are not addressed at all though not explicitly cast as functional or monadic approach forms the basis for and monadic development for reasoning about randomized algorithms in coq their focus is on verification and they define the probability monad from first principles modulo an axiomatization of arithmetic on the interval whereas we provide it we hope to a of ideas between the efforts as we bring our theory of into coq while suitable for semantics and verification representation is not ideal for direct use in computing certain operations for instance it is how to sample or compute general efficiently given a term of type a more recent works explore alternate concrete of the probability monad ramsey and discuss some of the possibilities a popular choice is to represent distributions as weighted lists or trees this has the that only distributions with finitely many outcomes are expressible out essentially all commonly used continuous distributions and are the only supported form of on the other hand distributions can occur on arbitrary types expectation and computing the is straightforward and the approach works well as an embedded domainspecific language probability monads in haskell languages like or church offer more scope for program analysis which is crucial for the limitations of an embedded approach and some of the fundamental of the representation ultimately however these languages do not support continuous or hybrid distributions nor their in a general sense sampling functions are a fun alternative representation they are used by to support continuous and hybrid distributions in a true sense and also allow distributions on arbitrary types distributions are represented by sampling functions that return a sample from the distribution when sampling and routines are the only supported operations thus are not another recent work also supports continuous and hybrid distributions by providing a measure transformer semantics for a core functional calculus the work does not provide but is novel for its ability to support conditional probability in the presence of zero probability events in continuous spaces a feature necessary in many machine learning applications their formalization is similar to ours as both are based in standard measure theory they have independently recognized the importance of analyzing distributions by their transformations doing so in the context of conditional probability whereas we have developed the idea for this that reasoning via transforms may be a technique that is more applicable to other program analyses for probabilistic languages the hierarchical compiler is a for implementing hierarchical bayesian models its specification language represents a different point in the design space essentially it removes return while adding a set of standard distributions with to the core calculus this guarantees that all models are ac many powerful models used in machine learning are expressible in however something as basic as adding two random variables is not furthermore if a distribution outside of the provided set is required it must be added to the core this is the fundamental surrounding return with it the core is minimal expressivity is high and are nontrivial without it are easily supported but the core becomes large and expressivity is is not formally defined an entirely different probabilistic semantics into logic programming languages markov logic these languages are well suited for probabilistic knowledge engineering and statistical relational learning in markov logic for instance programmers associate higher weights with logical clauses that are more strongly to hold the semantics of a set of clauses is given by graphical models with the weights determining the potential functions eg by certain continuous distributions can be supported by manipulating the potential function calculation supporting in this context should not be problematic the potential functions essentially always exist by design however like it appears these languages are not quite as expressive as is possible in a probabilistic functional language the system shares a key feature with our language in that are manifest in the object language the derivation of maximum and bayesian for a significant class of statistical models with a focus on code generation and can express continuous distributions and however despite their focus on the language is not formally defined furthermore it is how general the language actually is ie how custom the models can be our work could serve as a formal basis for their system conclusion we have presented a formal language capable of expressing discrete continuous and hybrid distributions and their our novel contributions include a type system for continuous distributions and a modular pdf calculation procedure the type system uses the new ideas of rv transforms and functions there are several interesting for future work the first is to address in the context of conditional probability perhaps by our formalization of with the ideas presented in secondly to provide a complete account of continuous probability one must support expectation supporting expectation requires a treatment of or reasoning via the rv transform may be a finally combining this work with a formal language for optimization such as would create a true formal language for statistics which would be able to express statistical problems in the object language itself current languages express probability any notion of statistics is outside the language acknowledgments we thank christopher for valuable input on the idea of functions we also thank the anonymous reviewers whose suggestions have greatly improved the paper references a s a gray and i e mathematical program transformations in practical aspects of declarative languages p and c proofs of randomized algorithms in coq in mathematics of program construction pages ­ springer c pattern recognition and machine learning springer j a d gordon m j and j v measure transformer semantics for bayesian machine learning in european symposium on programming pages ­ h iii hierarchical compiler url l random generation m and s functional probabilistic functional programming in haskell journal of functional programming ­ m a categorical approach to probability theory categorical aspects of topology and analysis ­ n v d k and j church a language for generative models in uncertainty in artificial intelligence a g gray b fischer j and w automatic derivation of statistical algorithms the em family and beyond in advances in information processing systems m c and k the operator is not computable in complexity in analysis k and l de bayesian logic programming theory and tool in introduction to statistical relational learning o and c embedded probabilistic programming in working conference on domain specific languages springer d kozen semantics of probabilistic programs journal of computer and system sciences ­ t o and s on the formalization of the integration theory in hol interactive theorem proving pages ­ b b s russell d d and a probabilistic models with unknown objects in international joint conference on artificial intelligence volume o an introduction to integration and measure theory s park f pfenning and s a probabilistic language based upon sampling functions in principles of programming languages pages ­ acm new york ny usa a a probabilistic rational programming language in international joint conference on artificial intelligence n ramsey and a stochastic lambda calculus and monads of probability distributions volume pages ­ acm m and p markov logic networks machine learning ­ t and y a modeling language in international joint conference on artificial intelligence pages ­ d scott parametric statistical modeling by minimum integrated square error ­ b density for statistics and data analysis r a model of in which every set of reals is measurable of mathematics pages ­ l all of statistics a concise course in statistical inference springer 