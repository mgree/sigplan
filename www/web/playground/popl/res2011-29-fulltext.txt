multivariate resource analysis jan martin hofmann abstract we study the problem of automatically analyzing the worstcase resource usage of procedures with several arguments existing automatic analyses based on or types bound the resource usage or result size of such a procedure by a sum of unary functions of the sizes of the arguments in this paper we generalize this to arbitrary multivariate polynomial functions thus allowing bounds of the form mn which had to be by m n before our framework even bounds like where the mi are the sizes of the entries of a list of length n this allows us for the first time to derive useful resource bounds for operations on matrices that are represented as lists of lists and to considerably improve bounds on other operations on lists such as longest common subsequence and removal of from lists of lists furthermore resource bounds are now closed under composition which improves accuracy of the analysis of composed programs when some or all of the components exhibit resource or size behavior the analysis is based on a novel multivariate resource analysis we present it in form of a type system for a simple firstorder functional language with lists and trees prove soundness and describe automatic type inference based on linear programming we have validated the automatic analysis on a wide range of examples from functional programming with lists and trees the obtained bounds were compared with actual resource consumption all bounds were asymptotically tight and the constants were close or even identical to the optimal ones categories and subject descriptors f logics and meanings of programs semantics of programming analysis f logics and meanings of programs specifying and verifying and reasoning about programs general terms performance languages theory reliability keywords functional programming static analysis analysis resource consumption quantitative analysis introduction a primary feature of a computer program is its quantitative performance characteristics the amount of resources like time memory and power the program needs to perform its task permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm it should be possible for an programmer to from the source code of a program to its asymptotic worstcase behavior but it is often to determine the asymptotic behavior of program only a conservative of the resource consumption for a specific input or a comparison of two programs with the same asymptotic behavior require instead concrete upper bounds for specific hardware that is to say closed functions in the sizes of the programs inputs that bound the number of clock cycles or memory cells used by the program for inputs of these sizes on a given system concrete worstcase bounds are particularly useful in the development of embedded systems and hard realtime systems in the former one wants to use hardware that is just good enough to a task in order to produce a large number of units at lowest possible cost in the latter one needs to guarantee specific worstcase running times to ensure the safety of the system the manual determination of such bounds is very cf eg the careful analyses carried out by knuth in the art of computer programming where he close attention to the concrete and best possible values of constants for the mix architecture not commands the mathematical ease of knuth and even he would run out of if he had to do these over and over again while going through the debugging loops of program development in short derivation of precise bounds by hand appears to be in practice in all but the simplest cases as a result automatic methods for static resource analysis are highly desirable and have been the subject of extensive research on the one hand there is the large field of worstcase execution time analysis that is focused on yet not limited to the runtime analysis of sequential code without loops taking into account lowlevel features like hardware caches and instruction on the other hand there is an active research community that type systems and abstract interpretation to deal with the analysis of loops recursion and data structures in this paper we continue our work on the resource analysis of programs with recursion and inductive data structures our approach is as follows we consider resource aware ml a firstorder fragment of ocaml that features integers lists binary trees and recursion we define a bigstep operational semantics that formalizes the actual resource of programs it is parametrized with a resource metric that can be directly related to the compiled assembly code for a specific system architecture we describe an type system whose type judgments establish concrete worstcase bounds in terms of closed easily understood formulas the type system allows for an efficient and completely automatic inference algorithm that is based on linear programming we prove the nontrivial soundness of the derived resource bounds with respect to the big see § for a detailed overview of the state of the art to obtain bounds for atomic steps one has to employ tools step operational semantics we verify the of our approach with a available implementation and a experimental evaluation as by hofmann and to analyze the consumption of firstorder functional programs our type system relies on the of analysis to take into account the interactions between different parts of a computation this technique has been successfully applied to objectoriented programs to generic resource metrics to polymorphic and higherorder programs and to bytecode by means of separation logic the main limitation shared by these analysis systems is their restriction to linear resource bounds which can be efficiently reduced to solving linear constraints a recently discovered technique yields an automatic analysis for polynomial bounds while still relying on linear constraint solving only the resulting extension of the linear system efficiently computes resource bounds for firstorder functional programs that are sums of univariate polynomials pi for instance it automatically infers bounds for the sorting algorithms quick sort and insertion sort that exactly match the measured worstcase behavior of the functions the computation of these bounds takes less then a second this analysis system for polynomial bounds has however two that the automatic computation of bounds for larger programs first many functions with multiple arguments that appear in practice have multivariate cost characteristics like m · n secondly if data from different sources is in a program then multivariate bounds like m n arise even if all functions have a univariate resource behavior in these cases the analysis fails or the bounds are by m n to overcome these this paper presents an automatic typebased analysis for multivariate polynomial resource bounds we three main challenges in the development of the analysis the identification of multivariate polynomials that accurately describe the resource cost of typical examples it is necessary that they are closed under natural operations to be suitable for local typing rules moreover they must handle an unbounded number of arguments to with nested data structures the automatic relation of sizes of data structures in function arguments and results even if data that is over different locations like n n n in the partitioning of quick sort the smooth integration of the inference of size relations and resource bounds to deal with the interactions of different functions while keeping the analysis technically feasible in practice to address challenge one we define multivariate resource polynomials that are a generalization of the resource polynomials that we used earlier to address challenges two and three we introduce a multivariate analysis § and § the local type rules emit only simple linear constraints and are considering the variety of relations between different parts of the data that are taken into account our experiments with a prototype implementation see § show that our system automatically infers tight multivariate bounds for complex programs that involve nested data structures such as trees of lists additionally it can deal with the same wide range of linear and univariate programs as the previous systems as representative examples we present in § the analyses of the dynamic programming algorithm for the length of the longest common subsequence of two lists and an implementation of insertion sort that sorts a list of lists note that the latter see for a web interface example programs and the source code example a worstcase running time of the form where n is the length of the outer list and m is the maximal length of the inner lists the reason is that each of the on comparisons performed by insertion sort needs time linear in m we also implemented a more involved case study on matrix operations were matrices are lists of lists of integers it demonstrates interesting capabilities like the precise automatic tracking of data sizes when matrices or the automatic analyses of complex functions like the multiplication of lists of matrices of different dimensions details are available on the web the main contributions we make in this paper are as follows the definition of multivariate resource polynomials that generalize univariate resource polynomials in § the introduction of type annotation that correspond to global polynomial potential functions for analysis which depend on the sizes of several parts of the input in § the presentation of local type rules that modify type annotations for global potential functions in § the implementation of an efficient type inference algorithm that relies on linear constraint solving only background and informal presentation analysis analysis with the potential method has been introduced to manually analyze the efficiency of data structures the key idea is to incorporate a nonnegative potential into the analysis that can be used to pay costly operations to apply the potential method to statically analyze a program one has to determine a mapping from machine states to for every program point then one has to show that for every possible evaluation the potential at a program point suffices to cover the cost of the next transition and the potential at the program point the initial potential is then an upper bound on the resource consumption of the program linear potential one way to achieve such an analysis is to use linear potential functions inductive data structures are statically annotated with a positive rational numbers q to define nonnegative n q · n as a function of the size n of the data then a sound incomplete typebased analysis of the program text statically verifies that the potential is sufficient to pay for all operations that are performed on this data structure during any possible evaluation of the program the analysis is best explained by example consider the function that removes the of a given integer from a list of integers match l with nil nil let xs in if x mod p then xs else assume that we need two memory cells to create a new list cell then the usage of an evaluation of is at most to infer an upper bound on the usage we the type of filter with a priori unknown potential annotations qi pi q q q p p the intuitive meaning of the resulting type is as follows to evaluate one needs q memory cells per element in the list and q additional memory cells after the evaluation there are p memory cells and p cells per element of the returned list left we say that the pair p has potential we use the naming scheme of the that arises from the more general method introduced in this paper p q q q q · and that has potential p p p p · a valid potential annotation would be for instance q p p and q another valid annotation would be q p p and q it can be used to type the inner call of filter in an expression like to infer the potential annotations one can use a standard type inference in which simple linear constraints are collected as each type rule is applied for the consumption of filter the constraints would state that q p and q p univariate polynomials an automatic analysis can be also used to derive potential functions of the form ik qi n i with qi while still relying on solving linear inequalities only these potential functions are attached to inductive data structures via type annotations of the form q q qk with qi q for instance the typing defines the potential the use of the coefficients rather than of variables has several advantages in particular the identity ik qi n i ik qi n i ik qi n i gives rise to a local typing rule for list match which allows to type naturally both recursive calls and other calls to func tions in branches of a pattern match this identity forms the mathematical basis of the additive shift of a type annotation which is defined by q qk q q qk qk qk for example it appears in the typing q q of the function tail that removes the first element from a list the potential resulting from the con q of a list q usually in a pattern match suffices to pay for three common purposes i to pay the constant costs q after and before the recursive calls ii to by q qn calls to auxiliary functions and iii to pay by q qn for the recursive calls to see how the polynomial potential annotations are used con the function that implements the of it calls the function filter to delete of the first element from the input list if is called with a list of the form n then it computes the list of p with p n l match l with nil nil note that it is possible in our system to implement the function filter with a destructive pattern match just replace match with that would result in a filter function that does not consume heap cells and in a linear consumption of but to the use of quadratic potential we use the filter function with linear consumption from the first example in an eval of the function filter is called once for every of the input list in the worst case then the calls of filter cause a worstcase consumption of this is for exam ple the case if is a list of pairwise distinct additionally there is the creation of a new list element for every recursive call of thus the total worstcase consumption of the function is n n if n is the size of the input list to bound the consumption of our analysis system automatically computes the following type since the typing assigns the initial potential n n to a function argument of size n the analysis computes a tight bound it is just more convenient to argue about heap space than to argue about evaluation steps for in the pattern match the additive shift assigns the type to the variable xs the constant potential is then used to pay for the cons operation i the potential is shared between the two occurrences of xs in the following expression by using to pay the cost of ii and by using to pay for the recursive call of iii to infer the typing we start with an unknown potential annotation as in the linear case q q q p p p the syntaxdirected type analysis then computes linear inequalities which state that q p q p and q p this analysis method works for many functions that admit a worstcase resource consumption that can be expressed by sums of univariate polynomials like n m however it often fails to compute types for functions whose resource consumption is bounded by a mixed term like the reason is that the potential is attached to a single data structure and does not take into account relations between different data structures multivariate bounds this paper extends typebased analysis to compute mixed resource bounds like n · m to this end we introduce a global polynomial potential annotation that can express a variety of relations between different parts of the input to give a of the basic ideas we informally introduce this global potential in this section for pairs of integer lists the potential of a single integer list can be expressed as a vector q q qk that defines a of the form k i qi n i to represent mixed terms of degree k for a pair of integer lists we use a matrix q then q defines a of the form n i m j where m and n are the lengths of the two lists this definition has the same advantages as the univariate version of the system particularly we can still use the additive shift to as sign potential to to generalize the additive shift of the uni system we use the identity n i m j n i m j n i m j it is re by two additive shifts q and q where if i j k the shift operations can be used like in the univariate case for example we derive the typing tail q q for the function to see how the mixed potential is used consider the function that computes the product of two lists match l with nil nil match l with nil nil similar to previous examples n heap cells if n is the length of input this exact bound is represented by the typing int that states that the potential is n n before and after the evaluation of if is a list of length n the function n nm heap cells if n is the length of first argument and m is the length of the second argument this is why the following typing represents a tight bound for the function int to verify this typing of the additive shift is used in the pattern matching this results in the potential that is used as in the function the constant potential is used to pay for the cons operation i the linear potential is used to pay the cost of ii the rest of the potential is used to pay for the recursive call iii multivariate potential is also needed to assign a potential to the result of a function like append this is for example needed to type an expression like here append would have the type append the correctness of the bound follows from the formula nm n m nm and from the fact that append n resources if n is the length of the first argument the respective initial potential n m n m mn a tight bound on the worstcase consumption of the evaluation of where n m resource aware ml resource aware ml is a firstorder functional language with syntax booleans integers pairs lists binary trees recursion and pattern match in the implementation of we already included a destructive pattern match that we could handle using the methods described here syntax to simplify typing rules and semantics we define the following expressions of to be in let normal form in the implementation we transform unrestricted expressions into a let normal form with explicit sharing before the type analysis e true false n x x x f x xn let x e in e if x then et else ef x x nil xt leaf x x match x with x x e match x with nil e xt e match x with leaf e x x e mod div and or we skip the standard definitions of integer constants n z and variable identifiers x for the resource analysis it is which ground operations are used in the definition of in fact one can use here every function that has a constant worstcase resource consumption simple types we define the welltyped expressions of by assigning a simple type a usual ml type without resource annotations to welltyped expressions simple types are data types and firstorder types as given by the grammars below a unit bool int la t a a a f a a to each simple type a we assign a set of semantic values a in the obvious way for example t int int is the set of finite binary trees whose nodes are labeled with pairs of integers it is convenient to identify tuples like a a a a with the pair type a a a a a typing context is a partial finite mapping from variable identifiers to data types a signature is a finite partial mapping of function identifiers to firstorder types the typing judgment e a states that the expression e has type a under the signature in the context the typing rules that define the typing judgment are standard and a subset of the typing rules from § if the resource annotations are omitted programs each program consists of a signature and a family ef yf of expressions with a distinguished variable identifier such that yf a ef b if f a b we write f y yk ef as an abbreviation to indicate that f a a ak · · · b and ya ef b in this case f is defined by ef match yf with y yf match yf with y yf ef of course one can use such function definitions also in the implementation operational semantics to prove the correctness of our analysis we define a bigstep operational semantics that measures the resource consumption of programs it is parametric in the resource of interest and can measure every whose usage in a single evaluation step can be bounded by a constant the actual constants for a step on a specific system architecture can be derived by analyzing the translation of the step in the compiler tation for that architecture the semantics is formulated with respect to a stack and a heap as usual a value v val is either a location loc a boolean constant b an integer n a null value null or a pair of values v v a heap is a finite partial mapping h loc val from locations to values a stack is a finite partial mapping v val from variables to values the operational evaluation rules define an evaluation judgment of the form v h e v h q q expressing the following if the stack v and the initial heap h are given then the expression e evaluates to the value v and the new heap h to evaluate e one needs at least q q resource units and after the evaluation there are q q resource units available the actual resource consumption is then q q the is negative if resources become available during the execution of e fig shows the evaluation rules of the bigstep semantics there is at most one pair q q such that v h e v h q q for a given expression e a heap h and a stack v the non negative number q is the high of resources that are used simultaneously during the evaluation it is to view the pairs q q in the evaluation judgments as elements of a monoid q q × q · the neutral element is which means that resources are neither used nor the operation q q · p p defines how to account for an eval consisting of evaluations whose resource are defined by q q and p p respectively we define q q · p p q p q p if q p q p q p if q p if resources are never as with time then we can restrict to elements of the form q and q · p is just q p we identify a rational number q with an element of q as follows q denotes q and q denotes q this notation avoids case in the evaluation rules since the constants k that appear in the rules might be negative a feature of classical bigstep semantics is that it does not provide evaluation judgments for nonterminating evaluations in a companion paper we describe a bigstep operational semantics for partial evaluations that with the usual bigstep semantics on terminating computations it inductively defines statements of the form v h e q for a stack v a heap h q q and an expression e the meaning is that there is a partial evaluation of e with the stack v and the heap h that q resources this allows for a smooth extension of the soundness theorem theorem to nonterminating evaluations see x v h x vx h v h null h nz v h n n h b true false v h b b h vx v yf v h ef v h q q v h f x v h · q q · x x v vx v h x op x v h vx true v h et v h q q v h if x then et else ef v h q vx false v h ef v h q q v h if x then et else ef v h q v h e v h q q vx v h e v h p p v h let x e in e v h · q q · · p p · x x v vx vx v h x x v h vx v v vx v x v h e v h q q v h match x with x x e v h · q q · v h nil null h xt v l domh v h xt l hl v vx null v h e v h q q v h match x with nil e xt e v h · q q · vt h e v h q q v h match x with nil e xt e v h · q q · v h leaf null h x x x v vx vx vx l domh v h x x l hl v vx null v h e v h q q v h match x with leaf e x x e v h · q q · v v xv xv h e v h q q v h match x with leaf e x x e v h · q q · figure evaluation rules of the bigstep operational semantics the resource metric the type rules in § make use of the resource metric this is the metric in which all constants k that appear in the rules are instantiated to zero it follows that if v h e v h q q then q q we will use the metric in § to pass on potential in the typing rule for let expressions wellformed environments if h is a heap v is a value a is a type and a a then we write h va a to mean that v defines the semantic value a a when pointers are followed in h in the obvious way we a formal definition of this judgment note that if h va a then v may well point to a data structure with some aliasing but no circularity is allowed since this would require values a we do not include them because in our functional language there is no way of generating such values in principle our method can circular data we also write h v a to indicate that there exists a necessarily unique semantic value a a so that h va a a stack v and a heap h are wellformed with respect to a context if h vx x holds for every x dom we then write h v formal definitions can be found in the literature resource polynomials a resource polynomial maps a value of some data type to a nonnegative rational number potential functions are always given by such resource polynomials in the case of an inductive data type a resource polynomial will only depend on the list of entries of the data structure in preorder thus if da is such a data type with entries of type a eg binary trees and v is a value of type da then we write a an for this list of entries an analysis of typical polynomial computations operating on a data structure v with a an shows that it consists of operations that are executed for every ai with i · · · ik n the simplest examples are linear map operations that perform some operation for every ai another example are common sorting algorithms that perform comparisons for every pair ai aj with i j n in the worst case base polynomials for each data type a we now define a set pa of functions p a n that map values of type a to natural numbers the resource polynomials for type a are then given as nonnegative rational linear combinations of these base polynomials we define pa as follows pa a if a is an atomic type pa a a a pa · pa pi k v k n pi pa i in the last clause a an every set pa contains the constant function v in the case of da this arises for k one element sum empty product for example the function k is in for ev k n simply take p pk in the def of the function k · k is in lb for every k k n and n i k · j k for every k k n resource polynomials a resource polynomial p a q for a data type a is a nonnegative linear combination of base polynomials ie p qi · pi im for qi q and pi pa we write ra for the set of resource polynomials for a an but not exhaustive example is given by rn the set rn is the set of linear combinations of products of coefficients over variables x xn that is rn m i qi n xj j qi q m n n these expressions naturally generalize the polynomials used in our univariate analysis and meet two conditions that are im to efficiently manipulate polynomials during the analysis first the polynomials are nonnegative and secondly they are closed under the discrete difference operators i for every i the discrete derivative i p is defined through i px xn px xi xn px xn as in it can be shown that rn is the largest set of als these closure properties it would be interesting to have a similar of ra for arbitrary a so far we know that ra is closed under sum and product see lemma and are compatible with the construction of elements of data structures in a very natural way see lemmas and this provides some justification for their choice and an abstract character would have to take into account the fact that our resource polynomials depend on an unbounded number of variables eg sizes of inner data structures and are not invariant under tion of these variables it seems that some generalization of infinite symmetric polynomials to of the symmetric group could be useful but this would not serve our immediate goal of accurate multivariate resource analysis annotated types the resource polynomials described in § are nonnegative linear combinations of base polynomials the rational coefficients of the linear combination are present as type annotations in our type sys tem to relate type annotations to resource polynomials we sys describe base polynomials and resource polynomials for data of a given type if one considers only univariate polynomials then their tion is straightforward every inductive data of size n admits a po of the form ik qi n i so we can describe the potential function with a vector q q qk in the corresponding type for instance can we write for annotated list types since each annotation refers to the size of one input part only uni annotated types can be directly composed for example an annotated type for a pair of lists has the form see for details here we work with multivariate potential functions ie func tions that depend on the sizes of different parts of the input for a pair of lists of lengths n and m we have for instance a potential function of the form n i m j which can be described by the coefficients but we also want to describe potential func tions that refer to the sizes of different lists inside a list of lists etc that is why we need to describe a set of ia that the basic resource polynomials pi and the corresponding coefficients qi for a data type a these type annotations can be in a straight forward way automatically transformed into usual easily understood polynomials this is done in our prototype to present the bounds to the user at the end of the analysis names for base polynomials to assign a unique name to each base polynomial we define the index set ia to denote resource polynomials for a given data type a interestingly but as we find ia is essentially the meaning of a with every atomic type replaced by unit ia if a int bool unit ia a i i i ia and i ia it b i ik k ij ib the degree of an index i ia is defined as follows i ik k · · · define i ia k the i are an enumeration of the base pi pa of degree at most k for each i ia we define a base polynomial pi pa as follows if a int bool unit then pv if a a a is a pair type and v v v then pi v · pi v if a db in our type system d is either lists or binary trees is a data structure and v vn then pi vj · · · j n we use the notation a or just for the index in ia such that pa a for all a we have int and aa a a and db if a db for b a data type then the index ia of length n is denoted by just n we identify the index i i i i with the index i i i i for a list i i ik we write ii to denote the list i i ik furthermore we write ii for the concatenation of two lists i and i lemma if p p ra then pp ra and p and by linearity it suffices to show this lemma for base polynomials this is done by induction on a corollary for every p ra a there exists p ra with and p a pa a for all a a this follows directly from lemma that base polynomials p pa a take the form pi · pi lemma let a a and la let i ik ia and k then and pi a · pa · to prove this one the sum in the definition of into two one corresponding to the case where the first position j equals one thus a and where it is greater than one thus a is not considered note that pa this factor is there to achieve the format of the resource polynomials for types like a la lemma characterizes of lists written as as they will occur in the construction of data note that eg t t lemma let la then la and k t pi it · this can be proved by induction on the length of using lemma or else by a decomposition of the defining sum according to which indices the first list and which ones the second annotated types and potential functions we use the and base polynomials to define type annotations and resource polynomials we then give examples to illustrate the definitions a type annotation for a data type a is defined to be a family qa qi ii a with qi q we say qa is of degree at most k if qi for every i ia with k an annotated data type is a pair a qa of a data type a and a type annotation qa of some degree k let h be a heap and let v be a value with h va a for a data type a then the type annotation qa defines the potential qa qi · ii a usually we define type annotations qa by only stating the values of the nonzero coefficients qi however it is sometimes to write annotations q qn for a list of atomic types just as a vector similarly we write annotations q q q q for pairs of lists of atomic types sometimes as a matrix if a a and q is a type annotation for a then we also write a a q for i examples the simplest annotated types are those for atomic data types like integers the for int are and thus each type annotation has the form int q for a q q it defines the constant potential function q q similarly tuples of atomic types feature a single index of the form and a constant potential function defined by some q q more interesting examples are lists of atomic types like eg the set of of degree k is then where the last list contains k unit el since we identify a list of i unit elements with the integer i we have k consequently annotated types have the form q qk for qi q the de potential function is a q qn ik qi n i the next example is the type of pairs of lists the set of of degree k is i j i j k if we identify lists of units with their lengths as usual annotated types are then of the from q for a k × k matrix q with nonnegative rational en tries if a an b bm are two lists then the potential function is n i m j finally consider the type a of lists of lists of integers the set of of degree k is then i im m k ij n jm ij k m k k · · · let a am an be a list of lists and q be a corresponding type annotation the defined potential function is then q i il ik a qj n i il mj i ··· il in practice the potential functions are usually not very complex since most of the qi are zero note that the resource polynomials for binary trees are identical to those for lists the potential of a context for use in the type system we need to extend the definition of resource polynomials to typing contexts we treat a context like a tuple type let xa be a typing context and let k n the index set ik is defined through ik i in ij aj mj k jn a type annotation q of degree k for is a family q with qi q we denote a context with q let h be a heap and v be a stack with h v where h xj the potential of q with respect to h and v is n vh q qi j in particular if then ik and vh q q we sometimes also write q for q type rules if f a b is a function computed by some program and ka is the cost of the evaluation of f a then our type system will essentially try to identify resource polynomials p ra and rb such that pa a ka the key aspect of such cost is that it well with composition proposition let p ra p rb rc f a b g b c k a q and k b q if pa pf a ka and pb kb for all a b c then pa a ka a for all a notice that if we merely had pa ka and pb kb then no bound could be directly obtained for the composition interaction with parallel composition ie a c f a c is more complex due to the presence of mixed multiplicative terms in the resource polynomials proposition let p ra c rb c f a b and k a q for each j ic let pj ra and rb be such that pa c j and c j c if pa a ka and a holds for all a and j then pa c a c ka in fact the situation is more complicated due to our for high as opposed to merely additive cost and also due to the fact that functions are recursively defined and may be partial furthermore we have to deal with contexts and not merely types to gain an intuition for the development to come the above simplified view should however prove helpful type judgments the declarative type rules for expressions see fig define a typing judgment of the form q e a q where e is a expression is a signature see below q is a context and a q is a data type the intended meaning of this judgment is that if there are more than q resource units available then this is sufficient to evaluate e in addition there are more than va q resource units left if e evaluates to a value v programs with annotated types firstorder types have the form a q b q for annotated data types a q and b q a signature is a finite partial mapping of function identifiers to sets of firstorder types a program with types consists of a signature and a family of expressions with variables identifiers ef yf such that yf a q ef b q for every function type a q b q f notations families that describe type and context annotations are denoted with upper case letters q p r with optional super scripts we use the convention that the elements of the families are the corresponding lower case letters with corresponding ie q q and qx let q q be two annotations with the same index set i we write q q if qi qi for every i i for k q we write q q k to state that q q k and qi qi for i i let be a context let i i ik i and j j jl i we write i j to denote the index i ik j jl i we write q cf e a q to refer to type judgments where all constants k in the rules from fig are zero we use it to assign potential to an extended context in the let rule more will follow later let q be an annotation for a context for j i we define the projection j q of q to to be the annotation q with qi the essential properties of the projections are stated by propositions and they show how the analysis of functions can be broken down to individual components proposition let xa q be an context h v xa and h a then it is true that vh xa q vh · pj a additive shift a key notion in the type system is the additive shift that is used to assign potential to typing contexts that result from a pattern match or from the application of a constructor of an inductive data type we first define the additive shift then illustrate the definition with examples and finally state the soundness of the operation let be a context and let q be a context annotation of degree k the additive shift for lists lq of q is an annotation lq of degree k for a context xa that is defined through qi j j let tt a be a context and let q a be a context annotation of degree k the additive shift for binary trees t q of q is an annotation t q of degree k for a context xa a a that is defined by qi j j the definition of the additive shift is short but substantial we be by its effect in some example cases to start with consider a context with a single integer list that features an annotation q qk q q the shift opera tion l for lists produces an annotation for a context of the form xint namely lq qk q qk such that qi qi qi for all i k and qk qk this is exactly the additive shift that we introduced in our previous work for the univariate system we use it in a context where points to a list of length n and xs is the tail of it reflects the fact that ik qi n i ik qi n i ik qi n i now consider the annotated context tt int q qk with a single variable t that points to a tree with n nodes the additive shift t produces an annotation for a context of the form xint tt int tt int we have t q qk where if i j k and if i j k the intention is that t and t are the subtrees of t which have n and n nodes respectively n n n the definition of the additive shift for trees the nm k n i m j for it is true that ik qi n i qi n i qk n k k i qi n j n j q i n j n j as a last example consider the context q where q l is a list of length m and l is a list of length n the additive shift results in an annotation for a context of the form xint and the intention is that xs is the tail of l ie a list of length n from the definition it follows that lq where if i j k and if i j k the soundness follows from the fact that for every i k it is true that ki j m i n j m i ki j n i n k lemmas and state the soundness of the shift operations lemma let la q be an annotated context h v la h v and let v v xt then h v and vh la q v h lq this is a consequence of lemma one takes the linear combination of instances of its second equation and the right hand side according to the base polynomials for the resulting context lemma let tt a q be an annotated context h v tt a ht v t t and v vx v x t x t if xa xt a xt a then h v and vh tt a q v h t q we remember that the potential of a tree only depends on the list of nodes in preorder so we can think of the context splitting as done in two steps first the head is separated as in lemma and then the list of remaining elements into two lists lemma is then proved like the previous one by terms using lemma for the first separation and lemma for the second one sharing let xa xa q be an annotated context the sharing operation q defines an annotation for a context of the form xa it is used when the potential is split between multiple occurrences of a variable the following lemma shows that sharing is a linear operation that does not lead to any loss of potential lemma let a be a data type then there are nonnegative rational numbers for i j k ia and j such that the following holds for every context xa xa q and every h v with h v xa it holds that vh xa q v h xa xa q where v vx x vx and q k ij q q xa q x a q q q q unit q nz q q q n int q b true false q q b bool q q op mod div q q xint xint q x op x int q p q p q a p a p f xa q f x a q op or and q k op q q x op x bool q p e a p xa r e b r p q p j i pj cf e a pj pj j q pj q let x e in e b q r q p r et a p p q p q ef a r q r q q if x then et else ef a q xa xa p e b p p q p q xa q match x with x x e b q q q xa xa q x x a a q q q q nil la q q q q leaf t a q q lq q xt la q q t q xa xt a xt a q x x t a q r e b r r q r q p e b p p lq p q q match x with nil e xt e b q r q r e b r r q xa xt a xt a p e b p p t q xt a q match x with leaf e x x e b q p q xa ya p e b q q p za q zy b q p e b p q p q e b q q p p e b p q p c q p c q e b q p e b p i i pi qi xa q e b q figure type rules for annotated types lemma is a consequence of corollary moreover the coefficients can be computed effectively and are natural numbers for a context xa xa q we define q to be the q from lemma type rules fig shows the annotated type rules for expressions we assume a fixed global signature that we omit from the rules the last four rules are structural rules that apply to every expression the other rules are and there is one rule for every construct of the syntax in the implementation we incorporated the structural rules in the ones the most interesting rules are explained below has to be applied to expressions that contain a variable twice z in the rule the sharing operation p transfers the annotation p for the context xa ya into an annotation q for the context za without loss of potential lemma this is crucial for the accuracy of the analysis since instances of are quite in typical examples the remaining rules are affine linear in the sense that they assume that every variable occurs at most once assigns potential to a list the additive shift lq transforms the annotation q for a list type into an annotation for the context lemma shows that potential is neither nor lost by this operation the potential q of the context has to pay for both the potential q of the resulting list and the resource cost for list cons shows how to treat pattern matching of lists the initial potential defined by the annotation q of the context has to be sufficient to pay the costs of the evaluation of e or e depending on whether the matched list is empty or not and the potential defined by the annotation q of the result type to type the expression e of the nil case we use the projection q that results in an annotation for the context since the matched list is empty in this case no potential is lost by the of the annotations of q where j to type the expression e of the cons case we rely on the shift operation lq for lists that results in an annotation for the context again there is no loss of potential see lemma the equalities relate the potential before and after the evaluation of e or e to the potential before the and after the evaluation of the match operation by the respective resource cost for the matching and are similar to the corresponding rules for lists but use the shift operator t for trees see lemma essentially an application of proposition with f e and c followed by an application of tion with f being the parallel composition of e and the identity on and g being e of course the rigorous soundness proof takes into account and additional constant costs for dis a let it is part of the inductive soundness proof for the entire type system theorem the derivation of the type judgment q let x e in e b q can be explained in two steps the first starts with the derivation of the judgment p e a p for the subexpression e the annotation p corresponds to the potential that is exclusively attached to by the annotation q plus some resource cost for the let namely p q now we derive the judgment xa r e b r the potential that is assigned by r to xa is the potential that from the judgment for e plus some cost that might occur when binding the variable x to the value of e namely p the potential that is assigned by r to is essentially the potential that is assigned by to by q namely q r the second step of the derivation is to relate the annotations in r that refer to mixed potential between xa and to the annotations in q that refer to potential that is mixed between and to this end we remember that we can derive from a judgment s e a s that s va s if e evaluates to v this inequality remains valid if with a potential for t ie s · va s · to relate the mixed potential annotations we thus derive a cost free judgment pj cf e a pj for every j i we use judgments to avoid multiple times for the evaluation of e then we pj to the corresponding annotations in q and pj to the corresponding annotations in r ie pj j q and pj the intuition is that j corresponds to note that we use a fresh signature in the derivation of each judgment for e soundness the main theorem of this paper states that type derivations establish correct bounds an annotated type judgment for an expression e shows that if e evaluates to a value v in a wellformed environment then the initial potential of the context is an upper bound on the of the resource usage and the difference between initial and final potential is an upper bound on the consumed resources note that it is possible to prove that the bounds also hold for nonterminating evaluations as we did for the univariate system in a companion paper see the discussion in § theorem soundness let h v and q ea q if v h e v h p p then p vh q and p p vh q h va q theorem is proved by a nested induction on the derivation of the evaluation judgment v h e v h p p and the type judgment q ea q the inner induction on the type judgment is needed because of the structural rules there is one proof for all possible instantiations of the resource constants it is technically involved but conceptually compared to earlier works further complexity arises from the new rich potential annotations it is mainly dealt with in lemmas and and the concept of projections as explained in propositions and type inference and experiments type inference the algorithm for extends the algorithm that we have developed for the univariate polynomial system it is not complete with respect to the type rules in § but it works well for the example programs we tested its basis is a classic type inference generating simple linear con for the annotations that are collected during the inference and that can be solved later by linear programming in order to ob a finite set of constraints one has to provide a maximal degree of the resource bounds if the degree is too low then the generated linear program is the maximal degree can either be specified by the user or can be after an analysis a main challenge in the inference is the handling of resource polymorphic recursion which we believe to be of very high com if not undecidable in general to deal with it practically we employ a heuristic that has been developed for the univariate system in a a function is allowed to invoke itself recursively with a type different from the one that is being justified recursion provided that the two types differ only in lower degree terms in this way one can derive polymorphic type schemes for higher and higher for details see the of this approach to the multivariate setting no extra difficulties the number of multivariate polynomials our type system takes into account eg nm n m n m m n m n n m for a pair of integer lists if the max degree is grows exponentially in the maximal degree thus the number of inequalities we collect for a fixed program grows also exponentially in the given maximal degree moreover one often has to analyze function applications with respect to the call stack recall eg the expression from § where we had to use two different types for filter experimental evaluation in our prototype implementation we the cycles in the call graph and analyze each function once for every path in the resulting graph for larger programs this can lead to large linear constraint systems if the maximal degree is high sometimes they are infeasible for the lp solver we use our was on correctness of the prototype not on performance there certainly is room for improvement either by the configuration of the current lp solver or by with alternative solvers further improvement is possible by finding a suitable heuristic that is in between the maybe too flexible method we use here and the inference for the univariate system that also works efficiently with high maximal degree for large programs for example we could set certain coefficients qi to zero before even generating the constraints alternatively we could limit the number of different types for each function however we are satisfied with the performance of the prototype on the example programs that do not require high for instance we successfully analyzed longer examples with up to degree multiplication of a list of matrices table shows a compilation of the computation of bounds for several example functions all computed bounds are asymptotically tight the runtime of the analysis from to seconds on an intel core with gb ram depending on the needed degree and the complexity of the source program our experiments show that the constant factors in the computed bounds are generally quite tight and even match the measured lp solve version currently we use the standard configuration with no additional function computed bound simplified computed bound act run time int mi n n mi n n in min ix yi nx x n n n n n nx x n n n nn on on s s s s s s s s table the computed bounds the actual worstcase time behavior and the run time of the analysis in seconds all computed bounds are asymptotically tight and the constant factors are close to the worstcase behavior in the bounds n is the size of the first argument mi are the sizes of the elements of the first argument x is the size of the second argument yi are the sizes of the elements of the second argument m mi and y yi worstcase running times of many functions the univariate analysis infers identical bounds for the functions subtrees and in contrast it can infer bounds for the other functions only after manual transformations even then the resulting bounds are not asymptotically tight we present the experimental evaluation of two functions below the source code and the experimental validation for the other examples is available online it is also possible to the source code of the prototype and to analyze user generated examples directly on the web example sorting of lists of lists the following code implements the wellknown sorting algorithm insertion sort that sorts lists of lists to compare two lists one needs linear time in length of the one since insertion sort does quadratic many comparisons in the worstcase it has a running time of if n is the length of the outer list and m is the maximal length of the inner lists ll match l with nil true match l with nil false xy or x y and insert xl match l with nil x if then else l match l with nil nil insert xs below is the analysis output for the function when instantiated to bound the number of needed evaluation steps the computation needs less then a second on typical computers positive annotations of the argument the number of evaluation steps consumed by is at most nm n nm n where n is the length of the input m is the length of the elements of the input the more precise bound implicit in the positive annotations of the argument is presented in mathematical notation in table we manually identified inputs for which the worstcase behavior of namely sorted lists with similar inner lists then we measured the needed evaluation steps and compared the results to our computed bound fig shows a of this comparison our experiments indicate that the computed bound exactly matches the actual worstcase behavior example longest common subsequence an example of dynamic programming that can be found in many is the computation of the length of the longest common subsequence of two given lists sequences if the sequences a an and b bm are given then an n × m matrix here a list of lists a is filled such that ai j contains the length of the of a ai and b bj the following recursion is used in the computation if i or j ai j ai j if i j and j ai j if i j and the run time of the algorithm is thus below is the implementation of the algorithm let m in match m with nil l match l with nil len len ll match l with nil l let m in match m with nil nil match l with nil nil match with nil nil let nl in let right nl in let right in let elem if x y then else in match l with nil nil xs right l match l with nil x the analysis of the program takes less then a second on a usual computer and produces the following output for the function figure the computed bound lines compared to the actual worstcase number of for sample inputs of various sizes used by on the left and on the right int positive annotations of the argument the number of evaluation steps consumed by is at most mn m n where n is the length of the first component of the input m is the length of the second component of the input fig shows that the computed bound is close to the measured number of evaluation steps needed in the case of the run time exclusively depends on the lengths of the input lists related work most closely related is the previous work on automatic analysis see § this paper describes the first system that can compute multivariate polynomial bounds other resource analyses that can in principle obtain polynomial bounds are approaches based on by and in those systems an a priori unknown resource bounding function is introduced for each function in the code by a straightforward intraprocedural analysis a set of recurrence equations or inequalities for these functions is then derived even for relatively simple programs the resulting are quite complicated and difficult to solve with standard methods in the project progress has been made with the solution of those in an automatic complexity analysis for higherorder terms uses to solve the generated recurrence equations the size measures used in these approaches like the length of the longest path in the input data are less precise for nested data structures than our resource polynomials which the sizes of all inner data structures as a result our method can deal with compositions of functions more accurately and is able to express a range of relations between parts of the input we also find that yields better results in cases where resource usage of intermediate functions depends on factors other than input size eg sizes of partitions in quick sort a successful method to estimate time bounds for c procedures with loops and recursion was recently developed by gulwani et al in the speed project they annotate programs with counters and use automatic invariant discovery between their values using program analysis tools which are based on abstract interpretation a recent for nonrecursive programs is the combination of disjunctive invariant generation via ab interpretation with proof rules that employ in contrast to our method these techniques can not fully analyze iterations over data structures instead the user needs to define numerical quantitative functions this seems to be less modular for nested data structures where the user needs to specify an owner predicate for inner data structures it is also if quantitative functions can represent complex mixed bounds such as n n for moreover our method infers tight bounds for functions such as insertion sort that admit a worstcase time usage of the form in i in contrast indicates that a nested loop on i n and j i is with the bound n a difference to techniques based on abstract in is that we infer using linear programming an abstract potential function which indirectly yields a function the approach may be in the presence of compositions and data over different tions partitions in quick sort as any type system our approach is naturally compositional and itself to the smooth integration of components whose implementation is not available moreover type derivations can be seen as certificates and can be translated into formalized proofs in program logic on the other hand our method does not model the interaction of integer arithmetic with resource usage other related works use type systems to validate resource bounds crary and weirich presented a monomorphic type system capable of specifying and resource consumption provided a library based on dependent types and manual cost annotations that can be used for complexity analyses of purely functional data structures and algorithms in contrast our focus is on the inference of bounds another related approach is the use of types which provide a general framework to represent the size of the data in its type types are a very important concept and we also employ them indirectly our method adds a certain amount of data dependency and with the explicit manipulation of symbolic expressions in of numerical potential annotations polynomial resource bounds have also been studied in that addresses the derivation of polynomial size bounds for functions whose exact growth rate is polynomial besides this strong tion the efficiency of inference remains conclusion and directions for future work we have introduced a quantitative analysis for firstorder functions with multiple arguments for the first time we have been able to fully automatically derive complex multivariate resource bounds for recursive functions on nested inductive data structures such as lists and trees our experiments have shown that the analysis is sufficiently efficient for the functions we have tested and that the resulting bounds are not only asymptotically tight but are also surprisingly precise in terms of constant factors the system we have developed will be the basis of various future projects a challenging problem we are interested in is the computation of precise bounds in the presence of automatic memory management we have first ideas for extending the type system to derive bounds that contain not only polynomial but also involve and exponential functions the extension of linear analysis to polymorphic and higherorder programs seems to be compatible with our system and it would be interesting to integrate it finally we plan to investigate to what extent our multivariate analysis can be used for programs with cyclic data structures following and recursion including loops on integers for the latter it might be to merge the method with successful existing techniques on abstract interpretation another very interesting and piece of future work would be an adaptation of our method to imperative languages without builtin inductive types such as c one could try to employ discovery of inductive data structures as is done eg in separation logic references e p s g and d cost analysis of java bytecode in th symp on prog esop pages ­ e p s and g automatic inference of upper bounds for recurrence relations in cost analysis in th static analysis symp sas pages ­ e p s m g d g and d termination and cost analysis with and its user interfaces notes theor comput sci ­ r resource analysis with separation logic in th symp on prog esop pages ­ r automated higherorder complexity analysis theor comput sci ­ l m hofmann a and o automatic certification of heap consumption in log f prog ai and th conf pages ­ b memory analysis using the depth of data structures in th symp on prog esop pages ­ wn and sc calculating types and comp ­ k crary and s weirich resource bound certification in th acm symp on principles of prog popl pages ­ n a lightweight time complexity analysis for purely functional data structures in th acm symp on principles prog popl pages ­ p b and p automatic analysis of algorithms comput sci ­ b cost for dml programs in th int conf on prog icfp pages ­ b s and s gulwani a numerical abstract domain based on expression abstraction and max operator with application in timing analysis in comp aid verification th int conf cav pages ­ s gulwani and f the problem in conf on prog lang design and pldi pages ­ s gulwani k k and t m speed precise and efficient static of program computational complexity in th acm symp on principles of prog popl pages ­ j and m hofmann resource analysis with polynomial potential in th symp on prog esop pages ­ j and m hofmann resource analysis with polymorphic recursion and partial bigstep operational semantics in th symp on prog to appear m hofmann and s static prediction of heap space usage for firstorder functional programs in th acm symp on principles of prog popl pages ­ m hofmann and s typebased analysis in th symp on prog esop pages ­ m hofmann and d efficient typechecking for analysis in th conf on comp science logic csl lncs j hughes and l recursion and dynamic datastructures in bounded space towards embedded ml programming in th int conf on prog icfp pages ­ j hughes l and a proving the correctness of reactive systems using types in th acm symp on principles of prog popl pages ­ s k n and m hofmann for computations using analysis in th symp on form meth fm pages ­ s k and m hofmann static determination of quantitative resource usage for higherorder programs in th acm symp on principles of prog popl pages ­ o r van and m c van polynomial size analysis of firstorder functions in typed lambda pages ­ r e tarjan computational complexity siam j algebraic discrete methods ­ r et al the worstcase problem overview of methods and survey of tools acm trans embedded comput syst 