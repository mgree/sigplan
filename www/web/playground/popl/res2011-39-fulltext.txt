space overhead bounds for dynamic memory management with partial compaction computer science department computer science department abstract dynamic memory allocation is in runtime environments allocation and deallocation of objects during program execution may cause and the programs ability to allocate objects has shown that a worst case scenario can create a space overhead within a factor of log n of the space that is actually required by the program where n is the size of the largest possible object compaction can eliminate but is too costly to be run frequently many runtime systems employ partial compaction in which only a small fraction of the allocated objects are moved partial compaction reduces some of the existing at an acceptable cost in this paper we study the effectiveness of partial compaction and provide the first rigorous lower and upper bounds on its effectiveness in reducing at a low cost categories and subject descriptors d objectoriented programming memory management d language constructs and features dynamic storage management d processors memory management garbage collection d storage management garbage collection general terms languages performance algorithms theory keywords runtime systems memory management storage allocation dynamic storage allocation compaction partial compaction introduction the study of the theoretical foundations of memory management has not been very extensive in particular not much is known about the theoretical potential and limitations of memory management previous work that we are aware of includes a study of a study of memory placement and a study of the space limitations of conservative garbage collection and lazy reference counting in this work supported by the science foundation grant no permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm we attempt to extend these foundations and study the potential and limitations of partial compaction modern software dynamic memory allocation to sup port its memory needs allocation and deallocation of memory create holes between the allocated objects in the memory may be too small to further satisfy future allocation creates a space overhead since the memory consumed may become larger than the memory required to satisfy the tion requests when no exists studied the amount of space overhead that may be caused by when no objects are moved he showed that in the worst case causes quite a large space overhead in particular he presented a program or an allocation and deallocation sequence that never keeps more than m words allocated simultaneously but any allocator that attempts to satisfy this sequence would require a space of almost m log n words the parameter n stands for the largest possible allocated size in the system also provided a simple allocation strategy that can handle any allocation sequence in approximately m log n words compaction eliminates completely if one the heap after each deallocation then no tion appears at all and m words of space always suffice however full compaction is costly since a substantial fraction of the objects may be moved and all references need to be updated therefore modern systems tend to either use compaction or employ partial compaction moving some objects in an attempt to reduce and keep its cost acceptable of course the larger the space that is being com the less the heap becomes on the other hand the overhead that is on the executing program increases the question that arises is what is the tradeoff between the amount of space the can be moved and the space overhead that may occur in the heap in this work we provide the first lower and upper bounds for such a scenario since the amount of moved space makes a difference we need to bound the amount of space we choose to study a scenario in which a percentage of all allocated space can be moved one could generalize the question to allow a of bs movement for an arbitrary function b of the allocated space s other decisions are also possible for example one could limit the according to the amount of live space or the deleted space all these make sense as well but we that by a fraction of the allocated space is interesting as the amount of allocation typically represents allocation time in the memory management literature most notably for generational garbage collection in this work we present general results that bound the heap size required for allocation in the presence of partial compaction let the compaction budget at any point in the execution be c of the space allocated so far by the program to provide an upper bound we present an allocation and compaction strategy we show that for programs that never keep more than m words allocated simultaneously it suffices to use a heap whose size is the minimum between m · c and m · log n where n is the largest possible object that can be allocated when the compaction budget is high ie c is small then the heap size can be significantly smaller than the heap size obtained when no compaction is used the above is formally in theorem in section to show a lower bound on the space overhead we present a program that a large space overhead for any allocator whose compaction budget is limited by c of the allocated objects this program never keeps more than m words allocated simultaneously and it makes any allocator use a heap whose size is at least the minimum of m · c and m · log n log c words when the compaction budget is large c is small the minimum is obtained with m · c when the compaction budget is the heap size is at least m · log n log c thus these asymptotic bounds show that partial compaction can reduce the heap size but only to a limited extent which depends on the compaction this result is stated as theorem in section the above results hold for any possible allocator we this investigation with a study of a specific widely used memory allocator the free list allocator used for example in a specific allocator allows a more accurate analysis and yields better bounds we first examine this allocator when no compaction is allowed and improve bounds it turns out that the bounds depend on the number of used by the allocation scheme one extreme case is that the allocator maintains a for any possible object size the required space becomes almost n in this this case is much higher than the m log n words presented in papers the other extreme case is that a small number of free lists is allowed specifically one for each power of in this case we show a lower bound of m log n words which is two times stronger than the lower bound for the general allocator in practice the number of free lists kept is in between and our analysis can be applied to any specific choice of sizes to yield the corresponding lower bound these results are formalized in theorems and in section finally we examine the effects of adding partial compaction to the free list allocator we show that a heap size of m · c k · n suffices in this case for any program where k is the number of different free lists so when the compaction budget is large ie c is small partial compaction helps reducing see theorem in section for the lower bound the number of free lists employed is important we first show that when using free at least the minimum overhead is large if we dont allow a substantial compaction budget if we only keep free lists for objects sizes that are of then the heap size required is at least the minimum between · log n and · m · c see theorems and in section this work a study of the cost and effectiveness of par tial compaction new techniques are developed and novel bounds are shown the results are asymptotic they hold for any possible object size limit n any possible live space bound m and any pos compaction budget ratio c however although they represent the known bounds their applicability for specific realistic parameters of modern systems is limited we hope that these tech can be extended in future work to provide bounds and help us better understand the behavior of memory in practice organization in section we survey previous results on when no compaction is allowed and formally define the compaction budget model used in this paper in section we present and prove a simple upper bound on the effectiveness of partial compaction in section we state and prove the lower bound on the effectiveness of partial compaction we divide the proof into two parts first we prove the lower bound for the restricted case of aligned objects this proof contains most of the ideas and is easier to follow the full proof of the lower bound with all the details follows in section we study the specific free list allocator and prove all the results stated above we conclude in section problem description framework and previous work dynamic storage allocation that does not move objects can from j m provided lower and upper bounds on the space overhead for a memory management system that allocates and but cannot move objects the lower bound demonstrates the existence of a bad program that makes any allocator use a lot of space overhead the upper bound provides an allocator whose space overhead is limited for any program including the bad program that demonstrates the lower bound clearly if the program allocates a lot and does not space then a lot of space is required to satisfy the request it is thus interesting to ask how much space is required to satisfy the allocation requests of a program that never keeps more than m words simultaneously if compaction is frequently used a space of m words suffices the memory manager can simply compact the heap after each deallocation however when no compaction is allowed some allocation sequences demand a large space overhead in order to satisfy the allocation requests an allocator should be able to handle any allocation and deallocation sequence and for each allocator we can ask what is the smallest heap that can handle all allocation sequences that never allow more than m words of memory simultaneously allocated the bounds that are going to be derived depend on the size of the largest possible object in the system we denote by n the size of such an object and assume that no allocation in the sequence requests more than n words for the allocated object some can move allocated objects in the memory this is not always the case as moving objects requires updating all pointers them and some runtime systems cannot always distinguish pointers from integers that hold the same values such systems do not move objects whose references cannot be identified other runtime systems avoid compaction because of its costs the interaction between the program and the memory allocator is divided into stages each stage consists of three parts deletion the program removes objects from the heap compaction the memory allocator moves objects in the heap allocation the program makes allocation requests and the memory allocator returns the corresponding object addresses in the heap for the upper and lower bounds the program and the memory allocator can be viewed as the program objective is to make the memory allocator use as much space as possible the memory objective is to allocate space in a way that will require as little space as possible the upper and lower bounds for the case when is no compaction occurs were studied by he showed a tight connection between the upper and lower bounds we define pm n as the set of all programs that never keep more than m words of space allocated simultaneously and never allocate an object larger than n words the function p is defined as the heap size necessary for allocator a to answer the allocation requests of a program p so that p pm n the upper and lower bounds showed demand an overhead of × log n of the actual allocated space m ie · upper bound for all m n such that nm there exists an allocator ao such that for all programs p pm n it holds that p m · log n m · lower bound for all m n such that nm there exists a program po pm n such that for all a it holds that h sa po m · log n m n in this work we investigate the effect of moving objects during the run we allow the allocator to move a fraction of the objects ie to perform partial compaction we explore how partial compaction affects these bounds the compaction budget our objective in this work is to explore upper and lower bounds on the space required by that apply partial compaction we now define partial compaction if there were no limit on how much space can be moved the total heap size required for the allocation would be m the largest amount of live space throughout the application run one simple method of compaction that uses only m words could be allocate sequentially using a pointer and compact all of the memory once the m boundary is reached the problem with this approach is that the memory a high computation overhead and is in real systems practical systems consider tradeoffs between the amount of compaction executed and the space overhead therefore we introduce a bound on the amount of space that can be moved this space is denoted by bs which is the partial compaction function specifically after the program allocates a space s the may move bs c space c is a constant larger than the bs function is calculated incrementally the is added to on every allocation and reduced with every compaction for example bs s means that after every allocation of s the can move of the space allocated in this example if there were allocations of words and maybe some without any space of size of · could be moved if only words are moved a of words remains we define an algorithm that works within the bs limit as the partial compaction in this document we always use the compaction threshold function bs c · s for some constant c the setting to present our results we first specify what a program is what an allocator is in what way they are limited and how we measure their interaction we consider a program p that executes a sequence of allocations and the programs execution is adaptive in the sense that it may choose its allocation according to the locations in which the allocator chose to place its previous allocation however to save further quantification over the inputs we assume that p already has some fixed embedded input so that the allocation sequence depends only on p and the allocation decisions of the allocator but not on any additional inputs of p we also consider an allocator a that receives the sequence of allocations and one by one sequentially or online from the program and must satisfy each allocation request as it given a specific pair of a program p and an allocator a their joint execution is well defined and we measure the size of the heap that the allocator a uses to satisfy the requests of p we denote this heap size by p of course for the same program p there could be a allocator a that requires a large heap size p and there could be a better allocator a for which p is much smaller for a lower bound we look for a program p for which p is high for all for an upper bound we look for an allocator a that can serve all programs with a low p finally we define pm n as the set of all programs that never keep more than m words of space allocated simultaneously and never allocate an object larger than n words the upper bound we start with the upper bound which is simpler to show formally we state and prove the following theorem theorem upper bound for any real number c there exists a memory manager ac that satisfies the compaction bound c s and for all m n such that nm and all programs p pm n p min mc m · log n m proof for any real number c we present a specific memory manager ac that never the compaction c and that uses a heap size of at most m · c words ie p m · c for any program p pm n the memory manager ac allocates objects sequentially by a pointer when the pointer reaches the mc boundary the live space is fully to the beginning of the heap a for the memory allocator ac is presented in algorithm algorithm memory manager ac initially the heap is empty f while true receive an allocation request for an object of size if f mc then compact all allocated objects to the beginning f first word after the allocated space end if allocate the object at address f f f clearly algorithm never uses more than m · c words however we need to show that there is enough compaction budget to execute step which all live objects to the begin of the heap the required compaction for this step is at most m consider the time interval between any two denoted c and c since p pm n there are at most m allocated words at any point in time and therefore after the compaction c there are at most m words allocated in the beginning of the heap and f is smaller than m after executing c and before starting to execute c the allocator receives requests for allocations and until it fails to allocate words ­ when the f pointer at a point beyond mc in the heap the total size of the allocations executed between c and c must be at least mc m since this is at least the number of words that the free pointer has advanced between the two therefore the compaction at step is at least c · mc m m c according to our as c and and therefore m c m thus enough budget is available in the so that all of the m al located words can be moved to the beginning of the heap this memory manager is effective when c log n other c is large ie the compaction budget is small and then it is possible to obtain a good heap size without moving objects at all we can use the allocator presented in paper not use com at all and consume at most m log n m words for any pos program p pm n this concludes the proof of theorem the lower bound in this section we provide a specific program that can force any memory allocator to a space overhead to simplify the presentation we start with a program that assumes that the respect some alignment restriction we later extend this proof to eliminate the alignment restriction using a similar program we provide a proof for theorem due to paper size limitation the proof details appear only in the full version of the paper bounding the space consumption with aligned objects we start by defining what aligned object allocation means aligned object allocation places a limitation on the memory allocator which is not allowed to allocate objects in an arbitrary location but only in an aligned manner this constraint simplifies the proofs and has fewer corner cases for the lower bound we only consider objects whose size is a power of two because the program we build only allocates such sizes thus it is fine to define alignment only with respect to objects of size i for some integer i without loss of generality we assume that the smallest object is of size otherwise one could think of the object sizes as of the smallest object size definition we say that an object of size i is aligned if it is located at address k · i for some integer k definition we say that a memory allocator uses aligned allocation if all the objects it allocates are aligned we present a specific program called and de noted paw and prove that any memory allocator that satisfies the compaction bound and uses aligned allocation must a space overhead when supporting the allocation requests of paw the program paw receives the compaction bound c the largest object size n and the bound on the live space m as its input and for any c m n it holds that paw c m n pm n we will show that the program paw satisfies the following lemma lemma for all c and all m n there exists a program paw pm n such that for all a that use aligned allocation and satisfy the compaction bound c s the following holds paw m · min c log n log c n m m · log n log log n n if c log n if c log n note that this lemma is very much like theorem except that it is stated for aligned allocation and therefore the leading constants are better a instead of a we start with some intuition for constructing paw and then provide its algorithm paw works in phases in each phase it generates a series of allocation requests and lets the allocator allocate the requests once it the locations of the allocated objects it then which objects to delete the algorithm follows the idea is that at each phase i for i log n requests allocation of objects of size i it allocates as many of them as possible while keeping the allocated space below m it then where the allocator places the objects and on object the deletion is intended to prevent space reuse in the next phase phase i in the aligned setting an allocated object of size i must be placed on a full aligned interval of size i each such interval and if it is not empty makes sure that some object is kept there to avoid of a new object on this interval in the next phase however not leaving an aligned interval of length i empty is not enough to prevent its reuse in the presence of partial com the allocator may choose to move the objects on such an interval and clear it for therefore ap a policy when deciding which objects to delete in par it attempts to keep enough allocated space in each interval so that it will not be for the allocator to its com budget and move all these objects out of the interval given the compaction bound ratio c the algorithm paw sets a density d c and when deleting objects paw attempts to keep a density of d in each of the aligned interval the algorithm fol algorithm program input m n and c compute d as a function of c to be determined for i to log n do deletion step divide the memory into consecutive areas of size i remove as many objects as possible from each area subject to leaving at least id space allocation step request allocation of as many objects as possible of size i not the overall allocated size m end for by definition paw pm n since it never allocates an object larger than n and its overall allocation space never m words we note that intuitively it does not make sense to set d c if the allocated space that remains in an interval is of size ic or less then it makes sense for the allocator to move these allocated objects away and make space for reuse this costs the compaction budget ic but this budget is completely when the i object is later allocated on the interval so if we want to make the allocator pay for a reuse we must set d c or in other words let paw s deletion step leave more allocated space on each interval we leave the setting of the density d as a parameter but one possible good setting sets d to be c in this case the deletion step of paw leaves enough allocated space on each interval so that not much reuse is possible within the given allocation budget if reuse cannot be applied then the allocator has to get more fresh space and increase the heap in order to satisfy the allocation requests of paw the rest of this section provides a rigorous analysis of paw showing that it outputs allocation and deallocation requests that make any allocation strategy a space overhead as in lemma section extends this algorithm and analysis for the case extending the analysis to obtain the proof of theorem to bound the space required to satisfy the allocation requests we bound on one hand the amount of space allocated by paw in the execution and on the other hand the amount of space that the allocator to reuse during the execution let sa paw denote the total space allocated during an execution of the program removed or but there was no object placed on top of them between their and until the allocation of object o namely the was used for the reuse of o and not for any previous reuse qo is the total size of objects that were located in the space later by object o these objects were but there was no object placed on top of them until the allocation of o figure an example of deleting objects in with parameter d and phase i paw with an allocator a also let ra paw denote the total reused space a space is considered reused if some object is allocated on it the object is then deleted or by the compaction and later the same space is used for allocation of another object the same space can be counted as reused more than once if more than two objects are placed on it during the execution clearly during the execution of paw and a the size of the heap required to allocate all objects is at least the total size of the allocated objects the size of reuse that the allocator to make formally we can state this claim as follows claim for any program p and any allocator a the space p required by a to satisfy the allocation requests of p during their joint execution satisfies p sa p ra p we that claim holds not only for the specific program paw or in the aligned setting it holds for any program and any allocator to prove lemma we use claim and show that a lot of space is allocated and not much space is reused in other words we bound sa paw from below and ra paw from above for any possible allocator a we start with an upper bound on the space reuse ra paw the intuition is that reuse can typically be done only after the objects out of this area for there are not too many objects that can be moved out as discussed earlier nonetheless the actual proof is more complicated than this simple intuition because as phases advance different areas converge into a single one an area of phase i consists of areas from phase i some of these areas may be empty while others may be still creating a sparse area of size i let qa paw denote the total size of space ie objects throughout the run of the program paw against a memory manager a that allocates in an aligned manner and satisfies the compaction threshold c s let d be the deletion threshold of we show that the reuse ra paw is bounded according to the following claim claim for any allocator a that allocates objects in an aligned manner and satisfies the compaction bound c s and a deletion factor d employed by paw the following bound holds ra paw qa paw · d in the proof of this claim we use several definitions of areas and compaction that hold in the run of program definition if o is an object that is placed on the heap during the run of program then o is its size ro is the space size reused by the allocation of the object o ro is defined as the total size of objects and parts of objects that in the space that is later by object o these objects were figure an example the definitions o ro and qo in the execution of with parameter d and phase i an example of these definitions is in figure in this figure a fraction of the heap of size words is shown this fraction of the heap is an empty aligned area upon which an object o is placed in phase in the run of program the gray represent words in the heap that the last operation on this word was a compaction of an object the light gray represent words in the heap that the last operation on this word was a deletion of an object the white are words on the heap that were empty since the beginning of the run of the program in this example qo equals to the total size of the objects that were last ie to the total size of the words words ro equals to the total size of the deleted objects in this area ie words proof of claim fix an allocator a and consider a specific ex of paw with a we break the amount of compaction on qa paw and the amount of reuse ra paw into small increments in particular consider the space reuse in the execution a reuse occurs when a new object is placed upon an empty aligned area that previously contained allocated objects consider each such allocation of an object o j on an aligned area a of size i just before the placement of o j we check the size of and that occurred in order to make space for the allocation we show in claim below that · d for any allocated object o j now over all allocations during in the execution of we get ra paw · d qa paw · d jj the second inequality follows from claim the last inequality follows since any that is later used for space reuse is part of the entire set of executed by a during the execution it remains to show that claim holds claim consider any phase i during the execution of observe any object o just before it is being allocated in the heap during phase i it holds ro qo · d proof we look at the area a where the object o is placed just before the allocation of o in phase i the space of a is empty at that time this space can be into aligned of size k for any k st k i using the algorithm the result of algorithm is a list l that contains of the area a the are distinct their size is k for k n k i the union of the is exactly the area a for every sub area that is the output of algorithm one of the following holds algorithm division of an area a into input an area a of size i list l a new empty list stack s a new empty stack i while s is not empty do x k if k then k else look at the removal step in phase k during the run of check if there were any in area x during that step if there was at least one removal then k else divide x into its two equal sub areas x x k k end if end if end while return l the was either was always empty or was by an object that was away the size is the size is k st k and last time objects were deleted from this during the deletion step of phase k in algorithm the statement above follows directly from the behavior of algorithm an area of size k is only if it had no in phase k therefore when an area reaches size it never had any and statement is true since algorithm each aligned area of size k to its two equal the resulting are the exact same areas that algorithm considered in phase k the algorithm actually goes back from phase i back to phase looking for the last deletion for each for all resulting from algorithm we examine cases and in case it holds that there were no at all from the the reuse if larger than occurs since an object was allocated and later possibly more than once in this case the reuse in phase i of the equals to the size of the object that was last away from this since d it holds that · d in case for each phase k captures the time of the last deletion from the sub area if the was not reused between phases k and i the first allocation on after phase k occurred in phase i then we compute the reuse based on the following according to the definition of the program behavior for each such area after each removal at least d of the area remains we denote these objects by q these objects must be later away from this so that the space could be reused therefore if we multiply the size of this compaction by d we get the size of the the reuse size in this is smaller or equal to the size therefore in this case · d another option is that the area was already reused fully or partially an object was already placed on the and was later away in this case it holds that the size that was but never reused is at least d of the size this statement is true since after the last deletion at least d of the area if we notice only these locations in the heap we can see that objects were and possibly partially reused by other objects between phases k and i but these objects were later as the is eventually empty therefore in this case also there is always d of the area that was away and not yet reused so we get that · d this statement is true for all the sum of where k denotes the k of a holds ra and the sum of where k denotes the k of a holds qa since the area a is exactly the location of the newly placed object o it holds that ro qo · d this concludes the proof of claim claim let a be any allocator that satisfies the compaction bound c s such that c let the deletion threshold of paw be d and let the total space allocated during the execution of paw with a be sa paw it holds that paw sa paw d c proof by claim it holds that paw sa paw ra paw by claim it holds that ra paw qa paw · d where q is the total space furthermore by the bound on the partial compaction we know that only c of the allocated space can be ie qa paw c · sa paw using equations and we get paw sa paw sa paw · d c as required and we are done with the proof of claim we now return to lemma which asserts the lower bound for the aligned case we break the argument into its two different cases according to the compaction bound range we first state the two key claims then derive the lower bound from them as a conclusion and finally provide the proofs of these two claims the first claim considers the case of a small compaction in particular the compaction ratio c and the density parameter d are larger than the of the largest object size n claim for all a such that a satisfies the compaction bound c s st c d log n and m n it holds that paw m log n log log n n d c the second key claim considers the general case in which more compaction can be executed and only requires that c d claim for all a such that a satisfies the com bound c s for all m n and for all c d it holds that paw min md m log n n log d · d c having stated these two key claims we now show that they imply lemma which is the main focus of this section to this end we choose the density parameter d for paw to be d c claim holds for d log n and therefore for c log n in this case we get paw log n log log n n for the case that c log n we can use claim and deduce that paw · min c log n n log c m these two equations yield the lower bound of lemma exactly as required to finish the proof of the lower bound we need to prove claims and we start with some properties of areas as induced by the activity of the program paw claim consider any execution of paw with any allocator a and any phase i i log n let d be the deletion threshold just after the deletion step in phase i any nonempty aligned area of size i has one of the following two possible configurations the allocated space in the area is smaller than i d · and each object size is smaller than i d the area contains exactly one object that is larger or equal to i d proof recall that the deletion in phase i attempts to delete as much space as possible while still leaving a space of at least i d words allocated suppose that after the deletion we have an object whose size is at least i d then before the deletion this object in the area and if the deletion left this object allocated then it must delete all other objects in the area and this object must be the only object alone in this area satisfying the second case otherwise after the deletion all objects are of size smaller than i d let the smallest of the remaining objects be of size j for some j i removing this object was not possible in the deletion step therefore the space in this area can be at most i d j which is smaller than i d · the above claim shows that small objects must be allocated after a deletion step in particular objects smaller than i d must be allocated on an area with density smaller than d we generalize this of small objects in the next claim let i be the phase number and let k d be a size threshold ie we consider objects of size i k as small objects denote by the total space consumed by small objects after phase i the following claim asserts that the heap size must be large enough to accommodate these small objects claim consider an execution of paw against any allocator a let k d be a size threshold and be the total space consumed by objects that are smaller than ik after the deletion step in phase i after the deletion step of any phase i in the execution it holds that · k paw · · k if k d if d k d proof consider the state of the heap after the deletion step in any phase i in this phase the areas considered for allocation and deletion are of size i all objects that are smaller than i k belong to consider any area that holds an object in according to claim either there is only one object in the area and its size lies between i d and i k or the size of all remaining objects in the area is at most i d · if k d then the size of the area is larger by a factor of at least k than the space on this area with objects from if d k d then the size of the area is larger by a factor of at least k than the space on this area with objects from the size of the heap must be at least the sum of all areas containing objects in therefore we get paw · k or paw · k as required note that the heap size is monotone because the heap size required for phases i i is at least the space required for phases i therefore the obtained bound holds for the heap size of the entire execution we now prove the key claims intuitively it can be argued that either there is a point in the run of the program with many small objects that are so allocated that the lower bound follows easily or there is no such point in the latter case there must be many memory allocations during the execution of paw with a ie sa paw is large and therefore claim implies the correctness of the lower bound proof of claim we need show that if d log n then paw · m · log n log log n n d c we use claim and set the size bound k to be log n which is smaller than d this means that in phase i we consider an object to be small if it is smaller than i log n we divide the analysis into two cases in one case there is a phase in which a lot of small objects are allocated and in the other case all phases do not have a large number of small objects allocated so let be some fraction to be set later and consider the case in which the execution has a phase i for which small objects smaller than i log n more than m words then by claim the heap size at this point and on is larger than m · log n otherwise in each of the phases i log n after every allocation phase i the total space in the heap is at least m i it holds that more than · m i of the heap consists of objects that are larger than i log n the size of the object tells us when it was allocated objects smaller than i log n words were allocated in phases that are at least log log n earlier than the current phase whereas large objects were created in the last log log n phases this means that at least · m i of the live space must consist of objects that were created in the last log log n phases an execution consists of log n phases we divide the execution into discrete sections of log log n phases in the last phase of each such section at least · m i of the allocated space must have been allocated during this section this is true for all sections therefore if we compute the amount of space allocated in all log n log log n sections we get that that the amount of space allocated in the entire execution sa paw satisfies sa paw m log n log log n log n log i n m log n log log n n according to the relation between the amount of allocation and the heap size shown in claim we get that paw sa paw · d c m · log n log log n n · d c setting d c and using the fact that d c we get that paw m · log n in the first case and that paw m · log n log log n n d c in the second case and we are done with the proof of claim proof of claim we need to show that paw min md m log n n log d · d c similarly to the proof of claim we use claim this time we set k d that in phase i objects are considered small if they are smaller than i d if there is a phase i where more than m of the live space consists of objects smaller than i d then the total heap size is larger than m · d md otherwise in every phase i in the execution it holds that more than m i of the live space consists of objects that are larger than i d partitioning the execution into disjoint consecutive sections of log d phases we get that in each such section at least m i space was allocated therefore sa paw m · log n log d log d i log d m · log n log d n and again according to claim we get that paw m · log n n log d · d c one of these cases must hold and therefore the heap size satisfies paw min md m log n n log d · d c as required and we are done with the proof of claim bounding the space consumption when the objects are not necessarily aligned in the general form of the allocation problem there are no alignment constraints on the memory allocator and the objects can be allocated anywhere this requires a modification of the program and an extension of the proofs the details become more and are omitted due to space limitation the proofs are available in the full version of this paper the main problem that comes up in the proof is that a new allocation does not necessarily cover an aligned area and so we cannot analyze reuse in a clean manner as we did above the solution is to consider smaller areas in the delete step if we consider aligned areas whose size is a of the allocated object size then we can be sure that at least three of them are completely covered by the newly allocated object the proof details appear in the full version of the paper here we only state the main theorem theorem lower bound for all c and all m n there exists a program pw pm n such that for all a that satisfy the compaction bound c s the following holds pw m · min c log n log c n m m · log n log log n n if c log n if c log n free list allocator in the previous section we presented a specific program that to create a large space overhead for any possible memory allocation method in practice systems implement specific that can be specifically studied it is possible that a program can cause more overhead to a given specific allocator one very common memory allocator is the free list allocator and in particular the one that is eg in this section we study limits of this specific allocator and prove lower bounds based on its specific behavior we start by defining this free list allocation method we then show bounds on the space requirement when no compaction is allowed and finally in section we show bounds on the space requirements when partial compaction is used to aid in reducing for a free list allocation definition of a free list allocator a free list allocator the free list into several subsets according to the size of the free chunks each subset forms a list of free chunks of the same size or a small range of sizes and an array of pointers is used to index the various free lists a object is placed on the appropriate list according to its size an allocation request is from the appropriate list free lists are typically implemented in a block oriented manner the heap is partitioned into blocks typically of a page size ie kb and each block may only contain objects of one size whenever a full block is it is returned to the pool of free blocks whenever an allocation is for an object whose free list is empty a free block is from the block pool it is partitioned into as many free chunks as possible in an aligned manner and then the new chunks are added to the appropriate free list to allow allocation the free lists are known as each bucket is characterized by the chunk size or range of chunk sizes that can be allocated in it the allocation within each bucket is executed in a manner in the first free chunk in the list that can satisfy the allocation the simplicity of this method comes with a cost this method is to high external as shown below in the following sections we simplify the discussion by assuming that the maximal object size n equals the block size this is close to what happens in practice and small eg if the block size is n have small impact on the results as before we denote by m the total space that can be allocated simultaneously memory usage without compaction let us start with the case that no compaction is used the general lower bound of holds in this case that an overhead factor of at least × log n can occur in practice for the worst program we extend this bound for the specific free list allocator denote by s sk the different maximal object sizes inside each bucket and assume for simplicity that the program only allocates objects whose sizes are one of s sk when proving a lower bound it is enough to show that there exists one bad program a block contains objects of the same size but this can be any size as long as sk n as stated earlier for simplicity we also assume that n sk different free list have different tion policies which are determined by the size vector s sk we study two rather extreme cases by looking at an allocator that keeps a bucket for each possible object size ie a lot of and an allocator that has for exponential object sizes ie sizes that increase by a factor of two typical implementations use something in between these extreme cases and the tools developed here can be used to explore each specific set of sizes we first look at the allocator that implements the free list method with a full of bucket sizes s s s s sn n let the class pm s sn be the class of all programs that always keep the allocated space smaller or equal to m and allocates objects only of the sizes s sn the theorems below show that with no compaction the space overhead is quite large it is of the order of the square root of n this is much higher than the factor that was shown for the general allocator theorem there exists a program p pm s sn and the heap size required for allocator to execute the allocation requests of p satisfies p mn n the result for the smaller set of is very different par let be an allocator that implements the free list method with a of bucket sizes s s s s n let pm s be a set of programs that always keep the live space smaller or equal to m and allocates objects only of the sizes s theorem there exists a program p pm s and the heap size required for allocator to execute the allocation requests of p satisfies h p m log n m n note that theorems and improve on the general results obtained by could only show an overhead of log n since he needed to work with a general allocator unlike the free list allocator that we assume in this section the proofs of the theorems are omitted for lack of space and are available in the full version of this paper below we only present the bad program that causes large space overheads for any allocator that the free list allocation method algorithm program with bucket sizes s sk for i to k do do allocate as many object as possible of size si as many objects as possible subject to leaving exactly one object in each allocated block end for according to the definition of program after each deallocation step there will be a single object in each allocated block moreover this single object will not be deleted until the end of the execution this property makes the algorithm use a large heap in the full version of this paper we prove that the heap size in phase k depends on the total space size available for allocation in each of the phases mi and on the choice for bucket sizes si in the following way k i mi si n si · n equation implies that the more phases there are during the program execution and the more memory remaining for allocation in every one of them the larger the heap is the total space size available for allocation in each of the phases mi satisfies mi mi mi si n · si si equation represents the connection between object sizes in the different and the memory remaining for allocation in every phase of the program execution the larger the range of object sizes is the more memory remains for future phases consequently according to equation the total heap size becomes larger simplification of the above equations and assignment of the si parameter yields the proofs of theorems space overheads with compaction in section we built that many blocks by leaving a single object in each with no compaction that program created a lot of modern collectors employ partial compaction especially for such scenarios and they clear sparse blocks by moving their elements to a different block the blocks can then be used for further allocations in this section we first discuss a simple upper bound on the heap usage by looking at a simple compaction strategy which is similar to what is used in actual systems in practice we then provide a program that makes the allocator blocks by leaving enough allocated space on them so that the compaction budget will not allow effective the upper bound our memory manager allocates in a free list manner as before but it also has a strategy we call this memory manager a free list allocator and denote it by we specify the compaction strategy and later show that using this compaction strategy the following upper bound on the heap size holds theorem let c be the compaction threshold k be the number of and n be the block size the maximal heap size required for to execute any program p that never uses more than m words of allocated space simultaneously and never allocates an object larger than n satisfies p m · c k · n in order to prove this upper bound we first present the compaction strategy of show that it is consistent always has enough budget for compaction and finally prove the bound in theorem the definition below assumes a last block in any bucket between all blocks that are allocated for the bucket the last block is the one that was most recently allocated for this bucket by the behavior of the free list allocator all slots in other blocks were before the last one was allocated definition compaction strategy we denote by b a block that contains objects of size i will compact all objects within block b to other blocks if block b holds the following constraints block b is not the last block for objects of size i at most c of the space in block b is allocated there is enough free space in blocks containing objects of size i to contain all of the objects that are currently placed in block b claim with the compaction strategy of definition has enough compaction budget to execute all required compaction proof we show that enough space was allocated on this block alone to provide the budget for it at the appropriate time in the execution by the allocation strategy of the free list any block that is not the last block was filled with objects at some point in the execution otherwise the next block would not have been allocated there were no made on this block since it was taken from the pool according to the algorithm by which every compaction an entire block and returns it to the blocks pool since this block was taken from the blocks pool the compaction due to allocations on this block is at least c of the block size it can be larger if space on this block was reused at the execution point in which compaction is on this block at most c of the block is allocated therefore all this space can be moved using only budget in the that from allocations on this block after all of the objects in this block are away the block is returned to the block pool and can be reused in the future for other bucket sizes proof of theorem according to claim all blocks except maybe the last block are at least c full otherwise the block would have been using compaction therefore the total heap size required for these blocks is at most c · m where m is the total space in all blocks except the last ones in each bucket size therefore the total heap size required is at most c · m k · n where k is the number of different possible sizes and n is the block size the lower bound we now construct a program that creates a lot of for the free list allocator as before we look at a that uses a large number of and at a that uses a small number of we provide lower bounds for these two extreme cases and the same techniques can be used to work with any specific bucket sizes employed in any practical system denote by pm s sk the set of programs that never al more than m words simultaneously and allocate only objects whose size is in the set s sn the following theorems assert the two lower bounds theorem let be an allocator that implements the free list method and keeps the compaction bound of c for c let its bucket object sizes be the complete of s s s s sn n for n then there exists a program p pm s sn such that the heap size required for allocator to execute the allocations and of p p satisfy p · m · n m · m · c m if c n if c n theorem let be an allocator that implements the free list method and keeps the compaction bound of c · s let its bucket object sizes be the of bucket sizes s s s s n then there exists a program p pm s such that the heap size required for the allocator to execute all allocations and of p p satisfy p · m · log n m · m · c m if c log n if c log n we now present the program denoted that creates a large for the free list allocator algorithm program for the free list allocator compute d as a function of the input compaction threshold c for i to k do do allocate as many objects as possible of size si in each block as many objects as possible subject to leaving at least d of the space allocated in this block end for algorithm is similar to algorithm presented in section in its allocation steps but differs in its deallocation steps the difference is that algorithm keeps the allocated space remaining in each block larger than d of the space allocated within this block leaving more allocated space in a block makes it difficult to compact away all of the objects in it a block can be used for allocating objects of a different size only when all of the objects in it are moved away below we investigate the joint run of and and provide claims that lead to the proof of the theorems and note the difference between the execution of against a program p with compaction enabled and the execution of against it with compaction the difference is the fact that in the scenario space can be reused after objects on a block are away this same block can be reused for objects of larger sizes therefore it will be necessary to calculate the heap size depending on compaction as well as allocation the compaction is the reason why in algorithm leave d of the space in a block where d is chosen according to the value of c this is done in order to make the compaction more costly but it has another effect less deletion leaves less space for future allocations below we present some claims that lead to the proof of theorems and presented above claim consider the execution of a allocator with the program for any phase i in the execution let mi denote the space available for allocation in phase i and let si denote the object size allocated in phase i let k denote the number of phases in the execution the heap size required for the execution of with satisfies k mi i k si i proof according to the definition of the program run consists of phases in each phase the space available for allocation is mi the program requests allocations of as many objects as it can of size si the number of allocation requests is therefore mi si and the total space allocated in phase i is mi si the total space allocated from the beginning of the run until including phase k is k sk i mi si kk · si mi si i i the total for compaction is sk · c in each phase the program removes as many objects as it can subject to leaving at least d of the space already allocated in each block in order to free a bucket for reuse all the objects in this block must be moved to another block therefore the total space reuse during the execution is at most d times the space which is at most d · sk · c setting d c in we get that the space reuse is at most sk the total heap size necessary is at least the space allocated sk the space reused which is at most sk now using the inequality for sk gets the desired bound claim consider the execution of a allocator with the program let k be the number of phases in the execution let mi be the total space size available for allocation in phase i let si be the size of objects allocated in phase i let n be the block size and let d be the density threshold then in all phases i k it holds that mi mi d si n si we omit the proof of this claim due to the constraints of the paper length the proof appears in the full version of the paper claim consider the execution of a allocator with the program let mi be the size of the space available for allocation in phase i let si be the objects size in phase i let n be the block size and let d be the density threshold in all phases i k it holds that mi m m · i d i j n sj s j proof by removing the recursion from the result of claim we get i mi m j d n si si we use the fact that for any positive i it holds that ii j j j j to simplify the equation above the result is mi m i j si d n si which is what we to prove claim consider the execution of a allocator with the program let be the total heap size required for the allocations of until the end of phase k the heap size can be bounded by mk m · kk d m k i k i si n sk k si i proof according to claim the total heap size until phase k is k mi i k si i substituting the value of mi with the result of claim we get k m m i i d i j n sj s j k si i mk m · kk d k m i k i si n si k si i by substituting si in the with the maximal value of si which is sk we get the claim we now use the above claims to prove theorems and proof of theorem the object sizes in this case are s s s s sn n we set these sizes in the expression of claim mk m kk d m k i k i i nk k i i using the closed sum for k i i kk k and some algebra we get mk m kk d m k k k n k kk the above bound is correct for any phase k k n but the quality of the bound is best at where the computation actually stops we divide into two the first case is when d n in this case we choose k n and get h s n mn m · n n d m n n n n n n n since we assume that d n we can bound n d from above by note also that for n n n n n n n therefore h s n mn m n m n n n according to our definition m n therefore we can replace the last n n with m and get that h s n mn m the other case is when d n in this case we choose k d therefore hs d m d m · d m d d d n d dd since n d we can replace n with d we also use the fact that for d d d d d d d therefore hs d md d m d d d since m n d we can replace the last d d with m and get that hs d md m setting d c the claim holds proof of theorem the object sizes in this case are s s s s sn n setting the object sizes in the expression of claim we get mk m · kk d m k i k i i n k k i i using the following equation k ii kk i and some algebra we get mk m · m · kk k we split the analysis into two possible cases first assume that d in this case we look at the phase k log n in this case we get m log n m · log n n n log n d n m log n m · log n d m m · log n n n since d log n we can bound log n d from above by also we use the fact that m n to get m log n m · log n m m · log n n n m log n m the other case is that d log n in this case we choose k d and obtain m we replace n with d and use the fact that m n d to get md m d m · d d d d d md m and again setting d c the claim holds conclusion in this work we studied the effectiveness of partial compaction for reducing the space overhead of dynamic memory allocation we developed techniques for showing lower bounds on how much space must be used when the amount of compaction is limited by a given budget it was shown that partial compaction can reduce but up to a limit determined by the compaction budget we also studied the effectiveness of partial compaction for a specific common allocator the free list allocator bounds have been shown based on the specific behavior of this allocator this work extends our understanding of the underlying theory behind memory management with compaction we hope that the techniques developed can be further extended in future work to achieve even bounds and improve our understanding of the effectiveness of partial compaction for modern systems references and an efficient parallel heap compaction algorithm in proceedings of the acm sigplan conference on objectoriented programming systems languages and applications acm sigplan notices pages ­ october david f and vt a realtime garbage collector with low overhead and consistent in conference record of the annual acm symposium on principles of programming languages acm sigplan notices pages ­ new la usa january and an algorithm for parallel incremental compaction in boehm and david editors proceedings of the third international symposium on memory management june acm sigplan notices pages ­ berlin germany february and space overhead bounds for dynamic memory management with partial compaction a full version of this paper is available at boehm bounding space usage of conservative garbage collectors in popl pages ­ boehm the space cost of lazy reference counting in proceedings of the annual acm symposium on principles of programming languages acm sigplan notices pages ­ italy january boehm and mark garbage collection in an environment software practice and experience ­ and michael the gc algorithm in michael and jan editors proceedings of the first acm international conference on virtual execution environments pages ­ il usa june david and garbage collection in david f and editors proceedings of the fourth international symposium on memory management pages ­ october acm press k lewis e and implementing an garbage collector for java in craig chambers and l editors proceedings of the second international symposium on memory management acm sigplan notices pages ­ mn october richard e jones garbage collection algorithms for automatic dynamic memory management july with a chapter on distributed garbage collection by r and the concurrent incremental and parallel compaction in michael i and thomas ball editors proceedings of the acm sigplan conference on programming language design and implementation acm sigplan notices pages ­ june and the of cache data placement in popl pages ­ daniel and a realtime garbage collector for in greg morrisett and sagiv editors proceedings of the international symposium on memory management pages ­ october acm press and a study of concurrent realtime garbage collectors in gupta and p editors proceedings of the acm sigplan conference on programming language design and implementation acm sigplan notices pages ­ az usa june conference record of the annual acm symposium on principles of programming languages acm sigplan notices portland or usa january j m an estimate of the store size necessary for dynamic storage allocation journal of the acm ­ july j m bounds for some functions concerning dynamic storage allocation journal of the acm ­ july 