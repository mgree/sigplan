randomized program transformations for efficient approximate computations allen a martin rinard mit abstract despite the fact that approximate computations have come to many areas of computer science the field of program transformations has focused almost exclusively on traditional transformations that do not attempt to exploit the available in many computations to off accuracy for benefits such as increased performance and reduced resource consumption we present a model of computation for approximate computations and an algorithm for optimizing these computations the algorithm works with two classes of transformations substitution transformations which select one of a number of available implementations for a given function with each implementation a different combination of accuracy and resource consumption and sampling transformations which randomly discard some of the inputs to a given reduction the algorithm produces a randomized approximation to the optimal randomized computation which resource consumption subject to a probabilistic accuracy specification in the form of a maximum expected error or maximum error variance categories and subject descriptors d programming languages g probability and statistics probabilistic algorithms f analysis of algorithms and problem complexity numerical algorithms and problems general terms algorithms design performance theory keywords optimization tradeoff probabilistic introduction computer science was on exact computations with discrete logical correctness requirements examples include compilers and traditional relational databases but over the last approximate computations have come to many fields in contrast to exact computations approximate computations only to produce an accurate approximation to an exact but in many cases inherently output examples include machine learning information analysis and and and image processing permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ pa usa copyright c acm despite the of approximate computations the field of program transformations has focused on techniques that are guaranteed not to change the output and therefore do not affect the accuracy of the approximation this situation leaves the solely responsible for the approximation the result is computations with approximation choices directly embedded in the implementation transformations we investigate a new class of transformations transformations for approximate computations given a computation and a probabilistic accuracy specification our transformations change the computation so that it operates more efficiently while satisfying the specification because transformations have the freedom to change the output within the bounds of the accuracy specification they have a much scope and are therefore able to a much range of benefits the field of transformations is in its only very recently have researchers developed general transformations that are designed to manipulate the accuracy of the computation examples include task loop approximate function memoization and substitution of multiple alternate implementations when successful these transformations programs that can operate at multiple points in an underlying consumption tradeoff space users may select points that minimize resource consumption while satisfying the specified accuracy constraints accuracy while satisfying specified resource consumption constraints or dynamically change the computation to adapt to changes such as load or clock rate in the underlying computational platform standard approaches to understanding the structure of the tradeoff spaces that transformations induce use training executions to derive empirical models potential include models that may not accurately capture the characteristics of the transformed computation between the behaviors of the computation on training and production inputs a resulting to find optimal points in the tradeoff space for production inputs and an absence of guaranteed bounds on the magnitude of potential accuracy our result we present a novel analysis and optimization algorithm for a class of approximate computations these computations are expressed as a tree of computation nodes and reduction nodes each computation node is a directed acyclic graph of nested function nodes each of which applies an arbitrary function to its inputs a reduction node applies an function such as min max or mean to its inputs we consider two classes of transformations substitution transformations replace one implementation of a function node with another implementation each function has a propagation specification that characterizes the sensitivity of the function to in its inputs each implementation has resource consumption and accuracy specifications resource consumption specifications characterize the resources such as time energy or cost each implementation to compute the function accuracy specifications characterize the error that the implementation introduces sampling transformations cause the transformed reduction node to operate on a randomly selected subset of its inputs simultaneously eliminating the computations that produce the discarded inputs each sampling transformation has a sampling rate which is the ratio between the size of the selected subset of its inputs and the original number of inputs together these transformations induce a space of program configurations each configuration identifies an implementation for every function node and a sampling rate for every reduction node in this paper we work with randomized transformations that specify a probabilistic choice over configurations our approach focuses on understanding the following technical question what is the optimal consumption tradeoff curve available via our randomized transformations understanding this question makes it possible to realize a variety of optimization goals for example resource consumption subject to an accuracy specification or accuracy subject to a resource consumption specification the primary technical result in this paper is an optimization algorithm that produces a approximation to the optimal randomized computation which resource consumption subject to a probabilistic accuracy specification in the form of a maximum expected error or maximum error variance we also discuss how to realize a variety of other optimization goals challenges and solutions finding optimal program configurations presents several algorithmic challenges in particular · exponential configurations the number of program configurations is exponential in the size of the computation graph so a search for the best configuration is computationally · randomized combinations of configurations a transformed program that over multiple configurations may substantially one that chooses any single fixed configuration we thus optimize over an even larger space of probability distributions over the configuration space · global error propagation effects local error allocation decisions propagate globally throughout the program the optimization algorithm must therefore work with global accuracy effects and interactions between errors introduced at the nodes of the computation graph · nonlinear optimization problem the running time and accuracy of the program depend on the optimization variables the resulting optimization problem is nonlinear and we show that in the absence of reduction nodes one can formulate the optimization problem as a linear program which allows us to obtain an exact optimization over the space of probability distributions of configurations in polynomial time the question becomes much more involved when reduction nodes come to the picture in this case we approximate the optimal tradeoff curve but to a precision for an arbitrarily small constant our algorithm has a running time that is poly dependent on it is therefore a fully polynomialtime approximation scheme our algorithm reduction nodes one by one for each re node it the tradeoff curve achieved by the sub program that generates the inputs to the reduction node this dis uses a special technique that is specifically designed for such tradeoff problems we next show how to extend this to obtain a corresponding tradeoff curve that includes the reduction node the fi step is to recursively combine the to obtain a dy programming algorithm that approximates the optimal off curve for the entire program we note that the optimization algorithm produces a weighted combination of program configurations we call such a weighted combination a randomized configuration each execution of the final randomized program chooses one of these configurations with probability proportional to its weight the transformed program provides several in comparison with a deterministic program the randomized program may be able to substantially reduced resource con for the same accuracy specification furthermore ran also simplifies the optimization problem by replacing the discrete search space with a continuous search space we can therefore use linear programs which can be solved efficiently to model regions of the optimization space instead of integer pro grams which are in general potential applications a precise understanding of the consequences of transformations will enable the field to beyond its current focus on transformations that do not change the output this increased scope will enable researchers in the field to attack a much range of problems some potential examples include · computations on big data sampling transformations enable the optimization algorithm to automatically find computations that process only a subset of the inputs to provide an accurate output over the past researchers have developed many algorithms transformations hold out the of the development of many of these algorithms · and online computations many algorithms can be viewed as towards an optimal exact solution as they process more inputs because our model of computation supports such computations our techniques make it possible to characterize the accuracy of the current result as the computation incrementally processes inputs this capability the to the automatic development of computations which incrementally sample available inputs until the computation produces an accurate result and online computations which characterize the accuracy of the current result as the computation incrementally processes dynamically inputs · selection networks require low power low cost transformations may allow developers to specify a network computation with as the initial function nodes in the computation an optimization algorithm can then select that minimize power consumption or cost while still providing acceptable accuracy · data representation choices data representation choices can have consequences on the amount of resources time area power required to manipulate that data giving an optimization algorithm the freedom to the ac within specified bounds may enable an automatic selection of less accurate but more appropriate data representations for example a compiler may automatically replace an expensive floating point representation with a more efficient but less accurate fixed point representation we the application of this technology in both standard compilers for as well as hardware synthesis systems · dynamic adaptation in large data the amount of computing power that a large data center is able to to individual computations can vary dynamically depending on factors such as load available power and the operating within the data center a rise in may force reductions in power consumption via clock rate by computations that can operate at multiple points in the underlying consumption tradeoff space transformations open up new strategies for to for example a data center may to load or by running applications at less accurate but more efficient operating points · successful use of mostly correct components many components operate correctly for almost all inputs by inputs and computations with small amounts of random it is possible to ensure that with very high probability no two executions of the computation operate on the same values given a way to check if a fault occurred during the execution it is possible to the computation until all components happen to operate on values that no understanding the accuracy consequences of these can make it possible to employ this approach successfully the scope of traditional program transformations has been largely confined to standard compiler optimizations as the above examples illustrate appropriately transformations that exploit the to manipulate accuracy within specified bounds can increase the impact and of the field of program analysis and transformation contributions this paper makes the following contributions · model of computation we present a model of computation for approximate computations this model supports arbitrary compositions of individual function nodes into computation nodes and computation nodes and reduction nodes into computation trees this model enough computational structure to enable approximate optimization via our two transformations · transformations we consider two classes of transformations function substitutions and reduction sampling together these transformations induce a space of transformed programs that provide different combinations of accuracy and resource consumption · tradeoff it shows how to use linear programming dynamic programming and a special technique to obtain a approximation to the underlying optimal consumption tradeoff curve available via the transformations if the program contains no reduction nodes the tradeoff curve is exact · optimization algorithm it presents an optimization algorithm that uses the tradeoff curve to produce randomized programs that satisfy specified probabilistic accuracy and resource consumption constraints in comparison with approaches that attempt to a deterministic program en the last author would like to thank pat for an interesting discussion on this topic log sin log sin log sin average output figure a numerical integration program ables our optimization algorithm to programs with better combinations of accuracy and resource consumption and avoid a variety of issues · accuracy bounds we show how to obtain statically probabilistic accuracy b for a general class of approximate computations the only previous static accuracy bounds for transformations exploited the structure present in a set of computational patterns example we next present an example computation that a univariate function f x over a fixed interval a b the computation a b into n each of length x ba n let x x xn where xi x the value of the numerical integral i is equal to i x · n f xi n n b a · f xi i i say for instance f x x · sin is the function that we want to integrate and a b our model of computation as illustrated in figure in our model of computation we have n input edges that carry the values of into the computation and an additional edge that carries the value of ba for each xi a computation node the value of b a · f xi the output edges of these nodes are connected to a reduction node that computes the average of these values we call such a node an node as the final integral i program transformations the above numerical integration program presents multiple opportunities to accuracy of the result i in return for increased performance specifically we identify the following two transformations that may improve the performance · substitution it is possible to substitute the original of the and functions that f x with alternate implementations that may compute a less rate output in less time · sampling it is possible to discard some of the n inputs of the node and the computations that produce these in puts by taking a random sample of s n inputs here we call s the reduction factor roughly speaking this transformation introduces an error proportional to s but decreases the run time of the program to s n tradeoff space in this numerical integration problem a program configuration specifies which implementation to pick for each of the functions and in principle although we do not configuration weight c c sn error speedup table the optimal randomized program configuration for and do so in this example × the configuration also specifies the reduction factor s for the node if we assume that we have two alternate implementations of and each program configuration provides the following information indicating whether we choose the ith implementation of the function u log sin and i and s indicating the reduction factor for the node we choose a randomized program configuration is a probabilistic choice over program configurations function specifications we impose two basic requirements on the implementations of all functions that f x the first requirement is that we have an error bound and time complexity specification for each implementation of each func tion in this example we will use the following model the implementation of executes in time with error the original implementation of executes in time with error we have two alternate tions of and where the ith implementation given function u runs in time i tu with error i · and i · i the second requirement is that the error propagation of the entire computation is bounded by a linear function this require ment is satisfied if the functions that the computation are continuous in our example the function is continuous since its derivative is bounded by the function is also continuous when x finally the product function × is continuous when the two in puts are bounded we remark here that this second requirement en that an error introduced by an approximate implementation propagates to cause at most a linear change in the final output finding the optimal program configuration given performance and accuracy specifications for each function we can run our optimization algorithm to approximately calculate the optimal tradeoff curve for each point on the curve our algorithm can also produce a randomized program configuration that achieves this tradeoff given a target expected error bound we use the tradeoff curve to find a randomized program configuration that executes in expected time the approximation ensures that this expected running time is at most times the optimal expected running time for the expected error bound in this example we use so that our optimized program will produce a approximation in addition we define · the number of inputs n · the overall expected error and · the running times and for this example our optimization algorithm identifies the point t on the tradeoff curve where t is the running time of the original program this indicates that the optimized program achieves a speedup of over the original program while keeping the expected value below the bound table presents the randomized program configuration that achieves this tradeoff this a univariate function is continuous if for any it follows that f x f x as a special case a function is continuous if f x this definition extends to multivariate functions randomized program configuration consists of two program configurations c and c each configuration has an associated weight which is the probability with which the randomized program will execute that configuration the table also presents the error and speedup that each configuration produces the configuration c selects the less accurate approximate versions of the functions and and uses all inputs to the reduction node the configuration c on the other hand selects more accurate approximate versions of the functions and and at the same time samples of the original inputs note that neither c nor c can achieve the desired tradeoff the configuration c produces a more accurate output but also executes significantly slower than the optimal program the configuration c executes much faster than the optimal program but with expected error greater than the desired bound the randomized program selects configuration c with probability and c with probability the randomized program has expected error and expected running time t we can use the same tradeoff curve to obtain a randomized program that the expected error subject to the execution time constraint in our example if the time bound t the optimization algorithm will produce the program configuration from table with expected error more generally our optimization algorithm will produce an efficient representation of a probability distribution over program configurations along with an efficient procedure to sample this distribution to obtain a program configuration for each execution model of approximate computation we next define the graph model of computation including the constraints for function nodes and present the substitution and sampling transformations definitions programs in our model of computation programs consist of a directed tree of computation nodes and reduction nodes each edge in the tree a stream of values the size of each edge indicates the number of transmitted values the multiple values transmitted along an edge can often be understood as a stream of numbers with the same purpose for example a from an image or a samples from a figure presents an example of a program under our definition reduction nodes each reduction node has a single input edge and a single output edge it reduces the size of its input by some multiplicative factor which we call its reduction factor a node with reduction factor s has an input edge of size r · s and an output edge of size r the node the r · s inputs into blocks of size s it produces r outputs by applying an sto function such as min max or mean to each of the r blocks for clarity of and to avoid a of notation we primarily focus on one specific type of reduction node which we call an node an node with reduction factor s will output the average of the first s values as the first output the average of the next s values as the second output and so on the techniques that we present are quite general and apply to any reduction operation that can be approximated well by sampling section describes how to extend our algorithm to work with other reduction operations output edge of size computation node reduction node figure an example program in our model of computation computation nodes a computation node has potentially ple input edges and a single output edge a computation node of size r has · a single output edge of size r · a nonnegative number of input edges each of size either which we call a edge or some multiple tr of r which we call a edge each edge carries a single global constant edges carry a stream of values which the computation node partitions into r chunks the computation node executes r times to produce r outputs with each execution processing the value from each edge and a block of t values from each edge the executions are independent for example consider a computation node of size with two input edges one edge of size denoted by a a a and one edge of size denoted by b then the function that outputs the vector b b b i i i is a computation node we remark here that a reduction node is a special kind of com node we treat computation and reduction nodes separately because we optimize computation nodes with substitution transformations and reduction nodes with sampling transformations see section inner structure of computation nodes a computation node can be further decomposed into one or more function nodes connected via a directed acyclic graph dag like computation nodes each function node has potentially multiple input edges and a single output edge the size of each input edge is either or a multiple of the size of the output edge the functions can be of arbitrary complexity and can contain language constructs such as conditional statements and loops for example the computation node in eq can be further decomposed as shown in figure a although we require the computation nodes and edges in each program to form a tree the function nodes and edges in each computation node can form a dag see for example figure b in principle any computation node can be represented as a single function node but its decomposition into multiple function nodes allows for and more transformation choices when optimizing entire program sin sum a log sin average output b c fig ab a closer look at two computation nodes and c numerical integration example example section presented a integration program example figure c presents this example in our model of tation tation n of c the log function node with input and output edges of size n runs n times each run a single inp ut and produces a single output the × function node with input edges of size n and runs n times each execution produces as the product of an xi with the common value b a from the control edge transformations in a program configuration we specify th e following two kinds of transformations at function and reduction nodes · substitution for each function e fu of size r we have a polynomial number of implementations fu the function runs r times we require h implementation to have the following properties each run of is in expected time giving a total expected running time of r · and each run of produces an ex absolute additive error of at most ie x the expectation is over the the of and fu we assume that all pairs are known in advance they are constants or depend only on control inputs · sampling for each reduction node r with reduction factor sr we can decrease this factor sr to a smaller factor sr sr at the of introducing some additive error for example for an node instead of all sr inputs we would randomly select sr in puts without replacement and output the average of the samples for convenience we denote the sampling rate of node r as r sr sr if the output edge is of size r the computation selects sr · r inputs instead of all sr · r inputs the values for the reduction node inputs which are not selected need not be computed dis the computations that would otherwise produce these discarded inputs produces a speedup factor of r sr sr for all nodes above r in the computation tree the following lemma provides a bound on the sampling error b a sr sr sr sr for an node the proof is available in the full version of the paper lemma given m numbers x x xm a b ran sampling s of the numbers xi without replace ment and computing the sample average gives an approximation to x m with the following expected error guarantee ei is xi · · · x · · · xm sm b a ms sm error propagation the errors that the transformations induce in one part of the computation propagate through the rest of the computation and can be or in the process we next provide constraints on the form of functions that characterize this error propagation these constraints hold for all functions in our model of computation regardless of whether they have alternate implementations or not we assume that for each function node xm with m inputs if each input xj is replaced by some approximate input xj such that xj j the propagation error is bounded by a linear error propagation function eu e xm xm eu m we assume that all of the error propagation functions eu for the functions fu are known a priori e xm xm j j j this condition is satisfied if all functions fu are continuous with parameters furthermore if xm is we can let i fu x xm xi if fu is itself probabilistic we can take the expected value of such is substitute implementations for functions with multiple implementations the overall error when we choose the ith implementation is bounded by an error propagation function eu and the local error induced by the ith implementation defined in the previous subsection e xm eu m this bound follows immediately from the inequality we remark here that the for the expectation in eq comes from the of its input x xm caused by errors from previous parts of the computation and random choices in the possibly probabilistic implementation these two sources of are mutually independent reduction node the function is a continuous function with all i m so in addition to lemma we have corollary consider an node that selects s random samples from its m inputs where each input xj has bounded error xj j then ei is x xm xi · · · x · · · xm sm m m j b a ms sm j if all input values have the same error bound xj then m m j j approximation questions we focus on the following question question given a program p in our model of computation and using randomized configurations what is the optimal tradeoff curve that our approximate computations induce here the time and error refer to the expected running time and error of the program we say that the expected error of program p is if for all input x ep x p x the tradeoff curve is a pair of functions t · such that et is the optimal expected error of the program if the expected running time is no more than t and t e is the optimal expected running time of the program if the expected error is no more than e the substitution and sampling transformations give rise to an exponentially large space of possible program configurations we optimize over arbitrary probability distributions of such tions a naive optimization algorithm would therefore run in time at least exponential in the size of the program we present an al that approximately solves question within a factor of in time polynomial in the size of the computation graph and polynomial in the algorithm uses linear programming and a novel technique called which we present in section a successful answer to the above question leads directly to the following additional consequences consequence optimizing time subject to error question given a program p in our model and an overall error what is the optimal possibly randomized program p available within our space of transformations with expected error no more than we can answer this question approximately using the optimization algorithm for question this algorithm will produce a randomized program with expected running time no more than times the optimal running time and expected error no more than the algorithm can also answer the symmetric question to find a approximation of the optimal program that the expected error given a bound on the expected running time consequence from error to variance we say that the overall variance ie expected error of a randomized program p is if for all input x ep x p x a variant of our algorithm for question approximately answers the following questions question given a program p in our model of computation what is the optimal tradeoff curve that our approximate computations induce question given a program p in our model and an overall variance what is the optimal possibly randomized program p available within our space of transformations with variance no more than section presents the algorithm for these questions consequence probabilities of large errors a bound on the expected error or variance also provides a bound on the probability of observing large errors in particular an execution we say that we approximately obtain the curve within a factor of if for any given running time t the difference between the optimal error et and our et is at most et and similarly for the time function t e our algorithm is a fully polynomialtime approximation scheme section presents a more precise definition in which the error function et is also subject to an additive error of some arbitrarily small constant of a program with expected error will produce an absolute error greater than t with probability at most t this bound follows immediately from inequality similarly an execution of a program with variance will produce an absolute error greater than t with probability at most t optimization algorithm for question we next describe a recursive dynamic programming optimization algorithm which exploits the tree structure of the program to compute the approximate optimal tradeoff curve for the entire program the algorithm computes and combines the approximate optimal tradeoff for the we stage the presentation as follows · computation nodes only in section we show how to compute the optimal tradeoff curve exactly when the tion consists only of computation nodes and has no reduction nodes we reduce the optimization problem to a linear program which is efficiently solvable · in section we intro our technique which con a of any tradeoff curve t · such that there are only o segments on the curve and at the same time the approximates t · to within a multiplicative factor of · a single reduction node in section we show how to compute the approximate tradeoff curve when the given pro gram consists of computation nodes that produce the input for a single reduction node r see figure we first work with the curve when the reduction factor s at the reduction node r is constrained to be a single integer value given an expected error e for the entire tion each randomized configuration in the optimal randomized program allocates part of the expected error to the transformation on the reduction node and the remaining expected error e to the substitution on the subprogram with only computation nodes one inefficient way to find the optimal randomized tion for a given expected error e is to simply search all possible integer values of s to find the optimal allocation that the running time this approach is inefficient because the num of choices of s may be large we therefore the tradeoff curve for the input to the reduction node into a small set of linear pieces it is straightforward to compute the optimal integer value of s within each linear piece in this way we ob an approximate optimal tradeoff curve for the output of the reduction node when the reduction factor s is constrained to be a single integer we next use this curve to derive an approximate optimal off curve when the reduction factor s can be determined by a probabilistic choice among multiple integer values we would use the convex envelope of the original curve to obtain this new curve but because the original curve has an infinite number of points it is infeasible to work with this convex directly we therefore perform another to ob a curve that we can represent with a small number of points we work with the convex envelope of this new curve to obtain the final approximation to the optimal tradeoff curve for the output of the reduction node r this curve the effect of both the substitution trans on the computation nodes and the sampling trans formation on the reduction node output figure example to illustrate the computation of time and error · the final dynamic programming m in section we provide an algorithm that computes an approximate tradeoff c for an arbitrary program in o ur model of computation each step uses the algorithm from section to compute the approximate tradeoff curve for a subtree rooted at a reduction node this subtree includes the computation nodes that produce the input to the reduction node it then uses this tradeoff curve to replace this subtree with a single function node it then recursively applies the algorithm to the new program terminating when it computes the approximate tradeoff curve for the output of the final node in the program stage computation nodes only we start with a base case in which the program consists only of computation nodes with no reduction nodes we show how to use linear programming to compute the optimal tradeoff curve for this case variables x for each function node fu the variable indicates the probability of running the ith implementation we also have the constraint that i running time since there are no reduction nodes in the program each function node fu will run ru times recall that ru is the number of values carried on the output edge of fu the running time is simply the weighted sum of the running times of the function nodes where each weight is the probability of selecting each corresponding implementation · · ru ui here the u is over all function nodes and i is over all implementations of fu total error the total error of the program also ad a linear form for each function node fu the ith implementation a local error on each output value by the linear error propagation assumption this is by a constant factor u which depends on the program structure it is possible to compute the u with a traversal of the program backward against the flow of values consider for example for function node f in the program in figure let be the linear error propagation factor for the univariate function the function · · is with propagation factors we similarly define for the function f and for f any error in an output value of f will be by a factor the total expected error of the program is · · u ui optimization given a fixed overall error the following linear program defines the minimum expected running time variables constraints minimize x i u i u by the roles of and it is possible to obtain a linear program that defines the minimum expected error for a given expected maximum running time tradeoff in the previous section we use linear programming to obtain the optimal tradeoff curve since there are an infinite number of points on this curve we define the curve in terms of functions to avoid unnecessary when doing we define the curve using two related functions and t · definition the tradeoff curve of a program is a pair of functions t · such that et is the optimal expected error of the program if the expected running time is no more than t and t e is the optimal expected running time of the program if the expected error is no more than e we say that a tradeoff curve is efficiently computable if both functions e and t are efficiently computable the following prop is important to keep in mind lemma in a tradeoff curve e t both e and t are convex functions proof t is always because when the allowed error increases the minimum running time does not increase and similarly for e we prove by contradiction assume et et et t for some then choose the optimal program for et with probability and the optimal program for et with probability the result is a new program p in our probabilistic transformation space this new program p has an expected running time less than the optimal running time et t the optimality of e a similar proof establishes the of t we remark here that given a running time t one can compute e and be sure that et t is on the curve but one cannot write down all of the infinite number of points on the curve we therefore introduce a technique that allows us to approximate e t within a factor of this technique uses a linear function with roughly o segments to approximate the curve our technique see figure ap e in the bounded range where is an upper bound on the expected error and approximates t in the bounded range t t we assume that we are given the maximum acceptable error for example by a user of the program it is also possible to conservatively compute an by analyzing the possible execution of the program in the remainder of the paper we refer to the function simply as e and to the function t · as t figure an example of definition given a tradeoff curve e t where e and t are both along with constants and e we define the curve of e t to be the curve defined by the following set of see figure · the two black points t t · the red points ei t ei where ei ei for some i and e i and · the blue points ti where ti t i for some i and t i t note that there is some in the of the two for the vertical time axis we know that the minimum running time of a program is t which is always greater than zero since a program always runs in a positive amount of time however we the error axis proportional to of i for values above e this is because the error of a program can indeed reach zero and we cannot forever the following claim follows immediately from the definition claim if the original curve e t is and convex the curve e t is also and convex accuracy of we next define notation for the tradeoff curve definition a curve e t is an to e t if for any error e te t e t e and for any running time t t t et et et e we say that such an approximation has a multiplicative error of and an additive error of e lemma if e t is an of e t then it is an of e t proof sketch the idea of the proof is that since we have the vertical time axis in an exponential manner if we compute te for any value e the result does not differ from t e by if instead we know that the minimum expected error is greater than zero ie for some maximum possible running time then we can define e just like our axis is curve for each value on this edge it to using parameter solve univariate optimization problem to get it to using parameter compute its convex envelope output is exact curve for single choice of our claims is is is to to to is exact curve for probabilistic choice of figure algorithm for stage more than a factor of similarly since we have the axis in an exponential manner if we compute et for any value t the result does not differ by more than a factor of except when et is smaller than e when we stop the but even in that case the value remains smaller than e because every point on the new curve e t is a linear combination of some points on the original curve e t te t e and et et because e t is convex recall lemma the approximation will always lie above the original curve complexity of the number of segments that the approximate tradeoff curve has in an is at most np def log e log log t t log o log log e log log where is a lower bound on the expected execution time and is an upper bound on the expected execution time our algorithm only needs to know in advance while and are values that we will need later in the complexity analysis on an approximate curve the above analysis does not rely on the fact that the original tradeoff curve e t is exact in fact if the original curve e t is only an to the exact tradeoff curve and if e t is the of e t then one can verify by the inequality that e t is a linear curve that is an e e approximation of the exact tradeoff curve stage a single reduction node we now consider a program with exactly one reduction node r with original reduction factor s at the end of the computation the example in figure c is such a program we describe our optimization algorithm for this case step by step as illustrated in figure we first define the tradeoff curve for the subprogram without the reduction node r to be section describes how to compute this curve lemma ensures that it is and convex in other words for every input value to the reduction node r if the allowed running time for computing this value is t then the optimal expected error is and similarly for note that when computing as described in section the size of the output edge ri for each node i must be divided by s as the curve characterizes each single input value to the reduction node r if at reduction node r we choose an actual reduction factor s s the total running time and error of this entire program is time × s error this is because to obtain s values on the input to r we need to run the subprogram s times with a total time × s and by corollary the total error of the output of an reduction node is simply the sum of its input error and a local error by the sampling let e t be the exact tradeoff curve e t of the entire program assuming that we can choose only a single value of s we start by describing how to com this e t approximately e t single choice of s by definition we can write e t in terms of the following two optimization problems te min × s ss se and et min ss where the first optimization is over variables s and and the second optimization is over variables s and we emphasize here that this curve e t is by definition because is but may not be convex because these optimization problems may not be convex they may be difficult to solve in general but thanks to the linear defined in section we can approximately solve these optimization problems efficiently specifically we produce a that as illustrated in figure we then solve the following two optimization problems te min × s ss se and et min ss we remark here that e and t are both since and are using claim each of these two problems can be solved by comput ing the optimal value within each linear segment defined by and and returning the smallest optimal value across all linear segments suppose that we are computing te given an error e in the linear piece of b here a and b are the and of the linear segment we have e the objective that we are therefore becomes univariate with respect to s × s b × s ae b × s here we have ignored the running time for the sampling procedure in the reduction node as it is often in comparison to other computations in the program it is possible to add this sampling time to the formula for time in a straightforward manner we extend this analysis to other types of reduction nodes in section the calculation of s is a simple univariate optimization problem that we can solve quickly using our expression for com the optimal answers from all of the linear pieces gives us an efficient algorithm to determine te and similarly for et this algorithm runs in time linear in the number of pieces np re call eq in our this the computation of e t in figure we next show that e t accurately approximates e t claim e t is an to e t proof because approximates we know that for any and this gives te min × s ss se min × s te ss se similarly this also gives that te te using using a similar technique we can also prove that et et and et et e therefore the e t curve the exact tradeoff curve e t we next further the curve e t that we obtained into e t using the same parameter e a of an approximate curve is still ap see section section we therefore conclude that claim e t is a to e t e t probabilistic choice of s now we define e t to be the exact tradeoff curve e t of the entire program assuming that we can choose s we claim claim e t is the convex envelope of e t proof we first prove the claim that t is the convex envelope of t the proof for e is similar one side of the proof is straightforward every weighted combination of points on t should lie on or above t because this weighted combination is one candidate randomized configuration that chooses s and t is defined to be the optimal curve that takes into account all such randomized configurations for the other side of the proof we need to show that every point on t is a weighted combination of points on t let us take an arbitrary point e t e suppose that t e is achieved when the optimal probabilistic choice of s is si at reduction node r where we choose si with probability pi and when si is chosen the overall is ei ti therefore we have e i and t e i because t e is the exact optimal tradeoff curve each ti is also with respect to ei and the fixed choice of si this is equivalent to saying that ei ti lies on the curve e t ie ti this implies that t is the convex envelope of t in general computing the convex envelope of an arbitrary function e t may be hard but thanks to our we can compute the convex envelope of e t easily let us denote the convex envelope of e t by e t in fact e t can be computed in time log np because e t contains only np note that for each different type of reduction code the optimization procedure for this univariate optimization can be since e t is a to e t by claim we should expect the same property to hold for their convex claim e t is a to e t proof by the definition of convex envelope for all time t there exists some such that et et et and t t t then e t et et e et et e et where the first inequality uses the fact that e is the convex envelope of e and the second uses the fact that e approximates e at the same time there exists some such that e t et et and t t t then e t et et et et et where the first inequality uses the fact that e approximates e and the second uses the fact that e is the convex envelope of e we can derive the two similar inequalities for the time function t and conclude that e t is a to e t so far we have finished all steps described in figure we have with a tradeoff curve e t that the exact tradeoff curve e t taking into account the probabilistic choices at this reduction node as well the final dynamic programming algorithm we next show how to compute the approximate tradeoff curve for any program in our model of computation we first present the algorithm then we discuss how to choose the parameters and e and how the errors compose if the program has no reduction nodes we can simply apply the analysis from section otherwise there must exist at least one reduction node r whose input is computed from computation nodes only assume e t is the exact tradeoff curve for the output of r applying stage section to the subprogram rooted at r we can efficiently find some curve e t that accurately approximates e t recall that this curve e t is convex since it is a convex envelope if we pick all of its at most np on the curve p then every point on the curve can be by at most two points in p because the function is linear and all points that can be by p lie above the curve because the function is convex the two observations and above indicate that we can replace this reduction node r along with all computation nodes above it by a single function node fr such that its ith substitute implementation gives an error of and a running time of observation indicates that every tradeoff point on e t can be implemented by a probabilistic of and observation indicates that every tradeoff point that can be implemented is no better than the original curve e t in sum this function node achieves the same tradeoff as the curve e t this completes the description of the algorithm we can recursively choose a reduction node with only computation nodes above it replace the subtree rooted at the reduction node with a function node continue until we have no reduction nodes left and compute the final tradeoff curve we next consider the accuracy and running time of this recursive algorithm maximum error propagation factor we use a value to bound the additive error of the optimized program specifically we choose so that a local additive error e at some node in the computation will produce at most a e × additive error at the final output specifically we set to be the maximum u over all function nodes fu in the program accuracy our algorithm repeatedly approximates with their this typically occurs twice during the analysis of each reduction node if the inputs of a reduction node come from some computation node the first approximates the tradeoff curve for that computation node we do not need this step if inputs come from some other reduction node the second approximates the tradeoff curve for the outputs of the reduction node by bounding the total error introduced by all of these approximations we show that our algorithm produces a good approximation to the exact curve we remark that there are two very different types of errors being discussed here errors that we off against time in our approximate computation and errors that our optimization algorithm in computing the optimal tradeoff curve for the approximate computation for clarity we shall refer to the former as computation error and to the latter as optimization error if we choose parameters e for both of a single reduction node r the argument in section shows that we obtain a to its tradeoff curve a computation node may the computation error which will result in a corresponding increase in the optimization error of our approximation to it however we can bound the total effect of all computation nodes on our optimization error using our linear error propagation assumption in particular we can study the effect of a single reduction nodes on the programs overall additive and multiplicative optimization error · the additive error e is by the computation nodes parameters the result is an overall additive optimization error of at most · the multiplicative error is not affected since both the actual computation error and our approximation to it are by the same factors the overall multiplicative optimization error introduced is thus at most this bounds the optimization errors introduced by the dis in our analysis of a single reduction node we can add these up to bound the total optimization error we fix parameters n and e e for all of the where n is the number of nodes in the original program since we have at most n reduction nodes this sums up to a total multiplicative error of and additive error of e both of these errors are optimization errors in the final approximation of the exact tradeoff curve of the whole program our over all optimization algorithm thus produces a curve e t that the exact tradeoff curve if we choose e to be the smallest unit of error that we care about then this essentially becomes an multiplicative approximation to the actual tradeoff curve time complexity the most expensive operation in our tion algorithm is solving the multiple linear programs that the algo rithm generates we next bound the number of linear programs that the algorithm generates recall by eq that np is the number of pieces in our by our choices of and e np o log log e log log n o log log e log log for each reduction node we solve a linear program for each of the np points in its we therefore run the lp solver on × np number of times the number of variables in these linear program is on × np since each node may have np implementations which occurs when we replace a reduction node with its this yields theorem our proposed recursive algorithm calls the lp solver on × np times each time with on × np variables the algorithm produces a approximation to the exact error time tradeoff curve and thus approximately solves question note that in practice we can find constants that appropriately bound and e in addition all those numbers within log functions we can therefore assume that in practice np o n log n we also note that the worstcase time complexity of linear programming is polynomial and efficient linear programming algorithms exist in practice optimization algorithm for question at this point we have approximately solved the tradeoff curve problem in question given an overall error we have an efficient algorithm that approximates the optimal running time t up to a multiplicative factor we next show that our algorithm is constructive it uses substitution and sampling transformations to obtain a randomized program p with expected error bound that runs in expected time no more than t the algorithm therefore answers question approximately our proof first considers the two simpler cases that we introduced in section stage computation nodes only if the program has no reduction nodes then the solution x to the linear program gives explicitly the probability of choosing each implementation at each function node which in turn gives us a randomized configuration for the randomized program p stage a single reduction node if the program consists of computation nodes that generate the input for a single reduction node this reduction node is therefore the root of the program recall that we have perform two as illustrated in figure specifically let e t be the exact tradeoff curve of the final output which we have approximated by a curve e t in the last step of figure we want to obtain a program p whose pair is t because t is guaranteed to be a approximation to t for example we may lower bound with the clock cycle time of the machine running the computation e with the smallest unit of error we care about upper bound with the lifetime of the machine running the computation with the largest value representable on the machine running the computation and with the ratio between the largest and smallest value representable on the machine running the computation by linearity the point t lies on some linear segment of e t t t e t e e e where e e are the two of that segment to achieve an pair t we can let the final program p run with probability some randomized program p whose pair is e t e and with probability some other randomized program p whose pair is e t e we next verify that both p and p can be constructed explicitly we focus on the construction of p the goal is to construct p with pair p e t e note that p is an endpoint of e t and thus is also an endpoint of e t since e t is a of e t p is also a point on the curve e t we can therefore write p e te recall that we obtained te by exactly solving the univariate optimization problem defined in eq because this solution is constructive it provides us with the optimal reduction factor s to use at the reduction node substituting this optimal value s into eq we obtain the pair that we should allocate to the subprogram without the reduction node we therefore only need to construct a program for the subprogram whose pair is because is a of we can obtain a program whose pair is by combining at most two points on the curve the linear program described above implements this curve this completes the construction of p we have therefore shown that as part of the algorithm for stage see section we can indeed construct a program p with the desired error and time bounds t putting it all together we prove by induction that for a program with an arbitrary number of reduction nodes we can obtain a randomized program p with expected error bound and expected running time no more than t we are done if there is no reduction node by stage otherwise suppose that we have nr reduction nodes we will substitute one of the reduction nodes r along with all computations above it by a function node fr with tradeoff e t this function node has a set of implementations and in our stage we have actually shown that each being a point on the e t curve is by some explicit randomized program pi in other words this new function node fr is a real function node every approximate implementation has a corresponding randomized program pi that can be constructed by our algorithm this reduces the problem into a case with nr reduction nodes using our induction hypothesis we conclude that for any we can construct a program p that runs in expected time no more than t optimization algorithm for question we next describe how to modify our algorithm if one instead is interested in the tradeoff changes to function nodes for each function node fu the algorithm for question works with an pair for each implementation now we instead work with a pair where is the variance ie the expected error x where the expectation is over the the of and fu we also assume that the variance propagation function is linear with coefficients m in other words if fu has an arity of m and each input xi is approximated by some input xi such that xi vi then the final variance e xm j vj j we next check that a large class of meaningful functions the above definition note that if fu is a deterministic function with respect to vector l l lm e e e e e i xi me i li xi x m i li vi so let i in eq if fu is probabilistic and each de function in its support is perhaps for different ls then we can similarly set the final i using the expectation changes to reduction nodes recall that for a reduction node r with reduction factor sr we can decrease this factor to a smaller value sr sr at the of introducing some additive sampling error in our question we have a different variance bound b a sr sr sr sr whose proof can be found in the full version of this paper lemma given m numbers x x xm a b ran sampling s of the numbers xi without replace ment and computing the sample average gives an approximation to x m with the following variance guarantee ei is xi · · · x · · · xm sm b a ms sm similar to corollary we also verify that an node satisfies our linear variance propagation assumption due to its continuity we defer the proof to the full version of this paper corollary consider an node that will pick s ran dom samples among all m inputs where each input xj has bounded variance xj vj then ei is x xm xi · · · x · · · xm sm b a m s sm m m vi i because the within fu is independent of the for input error xi xi we have xi xi xi putting things together as before if there is no reduction node we can write the variance in the following linear form · · u ui the factors u can be using the linear variance propagation factors i the expressions for the reduction node are slightly changed instead of eq we now have time × s error one can use almost the same arguments as in section to show that the optimization problem with respect to one reduction node can be reduced to optimizing a univariate function this yields a fully polynomialtime approximation scheme that solves question the solution for question can be derived similarly if one follows the arguments in section other reduction nodes we have stated our main theorems for programs that contain only reduction nodes more generally our algorithm supports any reduction node that can be approximated using sampling this includes for example p and order statistics functions in this section we discuss the analysis of some additional reduction nodes reduction nodes for a node with re factor sr ie sr numbers to sum up the sampling pro randomly selects sr numbers then outputs the partial sum by sr sr the output is therefore an of the exact sum the derivation of the expressions for the expected error and the variance of the error closely follows the derivation for nodes the final expressions are the corresponding expressions by sr minimization and reduction nodes note that all reasonable sampling techniques may with some probability discard the smallest input value we therefore consider minimization nodes which have two parameters q and b if the output is any of the smallest qm numbers in the input here m is the number of inputs the error is otherwise the error is b we also define m qm lemma for a minimization or node r with parameters q and b sampling sr of m input elements gives an expected error of m sr m sr b if sr m if sr m which is a convex function it is also possible to define a simpler approximate bound if m is large or sampling is done without replacement b proof there are a total of m sr ways to take samples of size sr m sr of these samples will contain no element in the smallest q fraction of the original sr inputs the probability that a sample contains no element that lies in this smallest q fraction is therefore m sr m sr the expected error is this probability by b one can verify that this is a convex function by taking the second order discrete derivative when m is large the probability can be approximated by which is obviously convex also the error propagates linearly with a constant factor of for such nodes in other words if all inputs to a minimization or node are subject to an error of xi the output will have error now if we replace in eq with this function we just derived all of the analysis in section goes through unchanged it is also straightforward to verify that if one uses the variance of the error and reduction nodes we can similarly define and nodes where the output value is a pair i v in which i is the index for the element and v is the actual minimum or maximum value the error depends exclusively on the element v we can output the index i but it can not be used as a value in arithmetic expressions related work empirical tradeoffs accuracy for performance is a well known practice often done at an algorithmic or system level for many computationally expensive problems researchers have developed approximation randomized or iterative algorithms which produce an approximate solution with guaranteed error bounds researchers have also proposed a number of techniques that specifically aim to accuracy for performance energy or fault the proposed techniques operate at the level of hardware system software and user applications previous research has explored the tradeoff space by running and comparing the results of the original and transformed programs on either training inputs or online for chosen production inputs the result of this exploration is often an empirical approximation of the tradeoff curve this approximation may be valid only for inputs similar to those used to construct the empirical curve in some cases the approximation comes with empirical statistical accuracy bounds in other cases there are no accuracy bounds at all this paper in contrast statically analyzes the computation to produce a approximation to the exact tradeoff curve this approximation provides guaranteed probabilistic accuracy bounds that are valid for all legal inputs probabilistic accuracy bounds for single loops researchers have recently developed static analysis techniques for characterizing the accuracy effects of loop which transforms loops to execute only a subset of their iterations or loops with approximate function memoization which transforms functions to return a previously computed value these techniques analyze single loops and do not characterize how the accuracy effects propagate through the remaining computation to affect the output this paper in contrast presents a technique that characterizes and exploits the curve available via substitution and sampling transformations applied to complete computations unlike previous research the techniques presented in this paper capture how global interactions between multiple approximate transformations propagate through the entire computation instead of just a single loop and does not require specification of probability distributions of the inputs analytic properties of programs researchers have developed techniques to identify continuous or programs identified applications include differential privacy and robust functions for embedded systems this paper in contrast presents techniques that apply transformations to obtain new computations that more desirable points on the underlying tradeoff curve that the transformations induce smooth interpretation uses a descent based method to synthesize control parameters for imperative computer programs the analysis returns a set of parameters that minimize the difference between the expected and computed control values for programs that control interactions approximate queries in database systems modern databases often enable users to define queries that operate on some subset of the records in a given table such queries come with no accuracy or performance guarantees researchers have explored multiple directions for supporting approximate queries with probabilistic guarantees approximate aggregate queries let a user specify a desired accuracy bound or execution time of a query the database then generates a sampling strategy that satisfies the specification or uses a sample when applicable online queries compute the exact answer for the entire but provide intermediate results and bounds probabilistic databases operate on inherently data the accuracy bounds of all queries including depend on the uncertainty of data these systems work with specific classes of queries defined in a relational model they sample data but do not consider multiple function implementations they also do not provide general mechanisms to achieve optimal tradeoffs for sampling when processing complex nested queries conclusion despite the central role that approximate computations play in many areas of computer science there has been little research into program optimizations that can off accuracy in return for other benefits such as reduced resource consumption we present a model of computation for approximate computations and an algorithm for applying transformations to optimize these approximate computations the algorithm produces a randomized program which randomly selects one of multiple weighted alternative program configurations to performance subject to a specified expected error bound given the growing importance of approximation in computation we expect to see more approximate optimization algorithms in the future we that these algorithms may share many of the key characteristics of the computations and optimizations that we present in this paper program transformations that off accuracy in return for performance the ability to optimize programs in the presence of errors that propagate globally across multiple composed programming constructs and to improve performance and acknowledgements we thank michael yang and the anonymous reviewers for useful feedback on previous versions of this paper this research was supported in part by the national science foundation grants and the states department of energy grant and a references s p v and s join for approximate query in j c chan y m q a and s a language and compiler for algorithmic choice in pldi w and t a framework for supporting programming using controlled approximation in pldi l k a j and k highly energy and performance efficient embedded computing through approximately correct arithmetic a mathematical foundation and preliminary experimental validation in cases s s gulwani and r continuity analysis of programs in popl s s gulwani r and s proving programs robust in s and a smooth interpretation in pldi s and a a program soundly and in cav j and m adaptation for mobile applications in r w qadeer m o a b lee s c and m understanding sources of in generalpurpose in j p and h wang online in h s m s a and m rinard dynamic for computing in h m m d a and a a general and extensible framework for computing technical report h s s a and m rinard using code to improve performance reduce energy consumption and to failures technical report wc g and b k processing aggregate relational queries with hard time constraints y hu s and j supporting sql queries in oracle s k t and b through critical data partitioning r majumdar and i symbolic analysis in r majumdar i and z wang systematic testing for control applications in j s and a parallel execution framework for recognition and applications in j a and s b s exploiting the nature of applications for scalable parallel execution in s d and m rinard probabilistic and statistical analysis of patterns technical report january s d and m rinard accurate program transformations in sas s s h and m rinard quality of service profiling in r and p randomized algorithms cambridge university press j and b c pierce distance makes the types grow stronger a calculus for differential privacy in icfp m rinard probabilistic accuracy bounds for computations that discard tasks in m rinard using early phase termination to eliminate load at barrier synchronization points in oopsla r time algorithms in intl of a w e d l and d approximate data types for safe and general computation in pldi s s h and m rinard performance vs accuracy tradeoffs with loop in k d and t networks technology protocols and applications j a m m m d corner and e d a language and runtime system for systems in p and d computation in concurrent hardware technical report university of technology d d c and c probabilistic databases 