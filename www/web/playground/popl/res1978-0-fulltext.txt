conference record of the fifth acm symposium on principles of programming languages compilation and delayed in apl evaluation by j and k xerox palo alto center hill palo alto cal introduction most existing apl implementations are in nature that is each time an apl statement is encountered it is executed by a body of code that is general ie capable of evaluating any apl expression and is in no way to the statement on hand this costly generality is said to be justified because apl variables are and thus can vary arbitrarily in type shape and size during the execution of a program what this argument is that the operational semantics of an apl statement are not modified by the varying storage requirements of its variables the first implementation proposal a non fully was the thesis of p ii in which a high level interpreter can defer performing certain operations by compiling code which a low level interpreter must later be called upon to execute the benefit thus is that intelligence from a context can be to bear on the evaluation of a subexpression thus on evaluating b only the addition b will be performed more recently a and several of his at yale have presented a scheme by which a apl compiler can be written the compiled code generated can then be very efficiently executed on a specialized hardware processor a similar scheme is used in the newly released hp apl this paper builds on and extends the above several directions we start by studying in some depth the two key notions all this work has in common namely compilation and delayed evaluation in the context of apl by delayed evaluation we mean the strategy of the computation of intermediate results until the moment they are needed thus large intermediate expressions are not built in storage instead their elements are time delayed evaluation for apl was probably first proposed by see many apl operators do not correspond to any real data operations instead their effect is to rename the elements of the array they act upon a wide class of such operators which we will call the grid selectors can be handled by essentially them down the expression tree and their effect into the leaf semantically this is equivalent to the transformations described by performing this optimization will be shown to be an integral part of delayed evaluation in order to focus our attention make a number of simplifying on the above issues we assumptions we our attention to code compilation for single apl expressions such as might occur in an apl where user defined functions are not allowed of course we will be concerned with the of the compiled code for future evaluations we also ignore the assume that type we simplifying elsewhere among the various apl primitive types and all our arrays are of one uniform numeric have studied the situation without these assumptions but plan to report on this the following paper is a list of the main contributions of this o we present an algorithm for the selector operators into the for the leaves of the expression tree the algorithm runs in time proportional to the size of the tree as opposed to its path length which is the case for the algorithms of and although arbitrary above algorithm an a if pb is a suffix of cannot especially important the a be handled by the case can that of is called o by using we can eliminate inner and outer products from the expression tree and replace them with scalar operators and reductions along the last dimension we do this by introducing appropriate selectors on the product arguments then eventually these selectors into the leaf the same mechanism handles scalar extension the convention of making scalar operands of scalar operators to arbitrary arrays o once products scalar extensions and selectors have been eliminated what is left is an expression tree consisting entirely of scalar operators and reductions along the last dimension as a consequence during execution the dimension currently being on a strict discipline this implies that we can generate extremely efficient code that is independent of the of the arguments several several require apl operators use the times a pure delayed multiple elements evaluation of their operands strategy would o we introduce a general mechanism called slicing which allows portions of a subexpression that will be repeatedly needed to be saved to avoid future slicing is well integrated with the evaluation on demand mechanism for example when operators that break the streaming are encountered slicing is used to determine the minimum size buffer required between the order in which a subexpression can its result and the order in which the full expression needs it o the compiled code is very efficient a minimal number of loop variables is maintained and are shared among as many expression atoms well as possible suited for finally the code generated is execution by an ordinary such as a or a data general we have implemented this compiler on the alto computer at xerox the plan of the paper is this we start with a general discussion of compilation the structures and delayed evaluation then we and algorithms we need to introduce by showing how to handle a and class of the primitive apl operators we discuss various ways of an evaluator for a particular expression some of this is possible based only expression itself while other optimizations on the require knowledge of the sizes of the atom bindings in the expression the reader should always be to the kind of knowledge being used for this affects the validity of the compiled code across of a statement the representation of expressions apl many other very high level languages is characterized by its ability to manipulate large objects thus apl deals with arrays setl with sets lisp deals with lists etc the word large refers to a comparison between the size of the primitive objects of the language and the complexity operations on them with of its primitive the size and complexity of the objects manipulated inside the processor of a computer thus an array typically several storage locations and the evaluation of an array sum ab in apl requires the execution of a number of machine instructions proportional to the size of the arrays a or b note that the semantics of apl although they completely determine the meaning of an expression in the language do not fully specify how that expression is to be computed for example the semantics in evaluating ab to add and in order of the language the corresponding we thus leave us free elements of a we can regard apl expressions more as a specification of the result we to compute rather than as a detailed for evaluation operators on a serial computer for the of apl the between what the semantics of the language require and what the evaluator is free to choose along the following lines the semantics specify what data operations are to be performed ie how each element of the result array depends on some of the elements of the operand arrays the order in which the result elements are to be evaluated however that is the control computation is usually left unspecified of the we can often use this freedom in sequencing to advantage by matching the order in which the result may be required eg for display according to standard apl conventions with the orders in which the operands most be traversed in the apl evaluators an operation is executed only after its operands have been fully evaluated this has the advantage assuming the usual convention of storing arrays in row major format that at any moment there is a very efficient way of traversing the arguments of an operation ie the row major order however this is not the only possibility suppose for example that we wish to display the result of where a and b are evaluated matrices then if we traverse a and b in column major order we can display the result without ever having to generate the intermediate array ab at the of slightly more traversal we have avoided generating a possibly large intermediate array furthermore we can optimize our freedom in sequencing over the entire expression we wish to evaluate there is a simple way of sequencing through xc so that elements of the result can begin to be displayed before any of the implied subexpressions have been fully evaluated thus we come to the other extreme that of evaluation on demand or delayed evaluation such evaluation strategies have been discussed previously in the context of very high level languages see for example in the techniques above description of equivalent evaluation we have not dealt with the issue of side effects the equivalence is valid only as long as al operations return proper values this unfortunately is not always so in apl because of undefined forms such as or the traditional evaluation strategy would report an error in computing because of the division by o however delayed evaluation would return since the division by o was never required so it never occurred this raises numerous issues which we will not discuss in this paper the access modes one way to evaluation on demand is to regard each apl expression as an object capable of to certain questions some of the questions we may want to ask are how many dimensions do you have what is your ith dimension what is your l j kth element this us to an object oriented view of expressions analogous to that of or smalltalk classes forms or clu clusters naturally we can that the ability to to the above messages is obtained through recursion assuming that fully evaluated arrays such as the atoms of an expression can in the obvious way our task is simply to associate with each apl operator procedures for to the above questions given that the operator can ask these same questions of its operands for example in ab the subexpression ab can to the request for an element by having issue requests for the appropriate elements to a and and then use its local to perform the addition in the above scheme we have essentially regarded each apl expression as a random access storage device it is clear that keeping each subexpression in a state of to provide an arbitrary element will involve very substantial overhead furthermore this ability to access elements in random order is not frequently needed in the evaluation of apl expressions much more common is the situation in which we need all elements of an expression one at a time in the order in which they would occur if the expression had been evaluated and the corresponding array stored in row major form order we will name this important way of accessing an expression mode in this mode we wish to regard an expression as a coroutine which upon successive calls will successive elements of the array it represents by restricting ourselves to highly access modes such as access we have a much better of an efficient implementation in order to understand what access modes are useful we have to understand in detail how the various apl operators use the elements of their operands to produce the elements of the result for example for the compression operator it will certainly be to have its argument be able to to the message skip as well as to the message generate the element next the argument may in fact being and just throw it away or it may be able to propagate the skip message further down the computation expression to as another example a real in the we may wish to break the message next of mode into two distinct advance and fetch the reason for this is that several evaluated atoms eg in may be able to share the same further explained later and thus we can get by with a single advance message for all three atoms the perspective by the above discussion is that of associating with each node in the expression tree an access mode the access modes are determined from the top down an operator is that the subtree it heads needs to be accessed in a certain way then by knowing how the elements of its result depend on the operand elements it in which modes its arguments must be accessed thus access modes correspond to inherited attributes in the sense of knuth the compilation of streams in this section we limit ourselves to apl expressions containing only scalar operators as the reader may handling such expressions is relatively trivial however to a domain where the task is well understood will allow us to focus our attention on setting the context for the following developments we will further limit the present discussion by as not have identical scalar expressions shapes we will where all atoms do deal with the very important special of scalar atoms which to any array according to the apl rules in section consider how to evaluate a clean way of obtaining the delayed evaluation effect is by implementing each scalar operator such as or x as a coroutine a different instance of the coroutine is used for each occurrence of the operator in the expression naturally all subexpressions including the atoms are accessed in mode unfortunately interpreting via is only as long as the cost of a coroutine call and return is small compared to the processing performed between successive invocations of the coroutine assuming costs for machine operations such aa are common then in for example each element of the result generated requires one one multiplication and coroutine control transfer instructions there are other hidden costs as well each instance of the atom accessing coroutine if implemented in the obvious way will be maintaining its own local copy of a counter and an offset into the atom array when clearly these variables can be shared and thus updated only once we have here the classical argument for compilation before we can discuss compilation in detail however we need to say a few more words about the machine model we have in mind we assume a stack machine with all apl scalar operators as primitives in addition the execution environment contains certain data structures specifying how arrays are to be traversed called the notion of an was first introduced by in where it is called a a detailed discussion of these structures will be given in the next section in the context of the current section an can be thought of simply as the index of the array element we are currently accessing thus in the evaluation of c all three atoms can clearly share the same our instruction will include the instructions which advances to the next array position and which pushes on the stack the element referenced by in atom a it will become clear in the next section that the above two operations can be implemented with a few machine instructions on most computers compilation is now straightforward assume that we have formed the expression tree during the lexical analysis of the expression in a first pass the dimensions pass the of the atoms is checked and storage for the result can be allocated if we are executing an assignment eg next in the push pass an is created to be shared by all atoms and initialized to point to the first element in the code generation pass a traversal of the expression tree in suffices to generate a performing the scalar computations for the example the code would be multiply add z advance in the above denotes the shared of all atoms the last instruction advances this in preparation for the next iteration note that this code is correct of the and size of the atoms not of their type this information has been confined within note also that we have obtained the effect of optimization with no extra work finally the above code needs to be by an appropriate p and we are ready to execute operators that break the streaming in the previous section we saw how simple it is to stream the evaluation of an expression composed solely of scalar operators we now take a brief look at the other end of the streaming namely operators that cause any reasonable mechanism to break down such operators include rotation a q and arbitrary the evaluation of these operators requires either a random access mode or partial evaluation in temporary storage there other cases where evaluation is necessary for example the argument of monadic i and the left argument of compression must be fully evaluated before even the can be checked finally may be useful even when they are not necessary such storage time will be taken up in section thus the compiler generates a number of code streams corresponding to broken subexpressions at run time these are invoked to replace a broken subexpression by possibly portions of an evaluated atom the universal selector in this section we discuss compilation of expressions involving a subset of the selection operators of apl as well as scalar operators the selection operators we will handle are reversal and el e where the e are arithmetic ie expressions equivalent to c for some integer scalars a b and c we will name the above selection operators the grid selectors for reasons that will become clear the grid selectors operate on an array argument the right argument except for by extracting andor renaming a portion of it this is done according to a second argument the control argument one may consider monadic b and o to have default control arguments the control argument must always be fully evaluated in order to check with operations higher up in the tree thus it will be convenient to think of the control argument as being part of the selector and not an object to which delayed evaluation is applicable we can think of the elements of an array a as lattice points in a space of dimensions within a bounding box of size pa note that each of the grid selectors when applied to a results in an object that a of the original lattice in other words moving across any coordinate of the result array can be viewed as moving along some set of of a using equal size steps let us a generalized selector called the selector that can represent any such selection operation then multiple selectors applied to the same array can be composed into a single instance of the universal selector involving thus if we start only scalar operators with an apl and grid selectors we can think of a null universal selector starting at each atom and traversing the path to the root of the expression tree in this process the universal selector into itself any grid selectors it we will of the stage when this processing is done as the push pass when this process is complete our expression will have only scalar operators left an instance of the universal selector will be associated with each leaf indicating the composition of all selectors that must be applied to that evaluated array although the composition of selectors can be most naturally thought of as from the bottom up we will in fact carry out this process top down the reasons for this are the cost of the push pass now becomes proportional to the size of the expression tree as opposed to the path length of the tree secondly we will be able to leave with each node an instance of the universal selector which represents the composition of all selectors above that node as we will see in section this will provide us with essential information needed in off decisions at that node the end result is quite similar to the normal form for select expressions first described by however the implementation of the transformations is much less than solution the data structure that represents a universal selector is called a a u will be associated with every node of the expression tree u will represent the state of the universal selector before the selector represented by the node if any has been a node n itself into the if it is a selector node and passes this on to its the newly formed u is characterized by n the rank of ns and four arrays q s d and defined over the interval ln these arrays encode the way in which elements of the current node in the formation of the final result qi is an integer in and denotes which coordinate of the result array the ith coordinate of the current node corresponds to the set of all qi with qi j is the set of of the current node which have been into the jth coordinate of the result recall that a or may reduce the number of dimensions si denotes the index along the ith coordinate of the element of the current node which to tile first element of the result eg the oo element of the result in di indicates by how much to move along the ith coordinate of the current node in order to arrive at the next element of the result along coordinate gi it can be negative li indicates the size of the result along the coordinate note that li lj if qi w initially for a null at the root have n qi i si di and li i for all i let us now see how to incorporate structure when various grid selectors using the u is about to selector s which in turn is applied to a node n we will let p denote and the array r denote pn indicate the new monadic a monadic can be in u by the following simple program n e n for i in in do begin ail ji zi end control argument how to this let ci i in the following into lp denote the program shows u n p for i in ln do begin qi e si sci di li end take as above let ci denote argument we have the control n n for i in ln do begin qi gi si if ci then si else si reversal reversal along be implemented as follows the kth coordinate can n n for i in ln do begin qi qi si if i k then di if i k then di zi li end else si else di the above the itself examples u can should be sufficient the various to illustrate grid selectors how into once all the have propagating by reaching the leaves of the tree the grid selectors can be completely removed from the expression we know from section how to compile code for a tree of scalar operators so the remaining issue is how to use the to compile code for accessing the evaluated atoms of the expression note that each element of an atom is used at most once in computing some element of the final result for each atom a there is an associated u we will use u to compute a new data structure called an which will allow us to step through the elements of a in the proper order a itself is assumed to be stored in order the t obtained from u consists of n the current position into the stored representation of the array a a the starting value of and two arrays yi and defined for i in max qj intuitively yi denotes the distance by which we have to increment t to obtain the next element of a needed for computing the next element of the result along the ith dimension the related i denotes the distance by which has to be to the same goal as above but now assuming that we have completely through all dimensions higher than i in the result more formally let the shape pa be and define hi have hn then for i max qj we yi and i w and i jn a ln in order to understand the meaning of we now describe how they are used in the code compilation observe that once the grid selectors have been removed from the expression tree the shapes resulting from each this follows selector to its atom from the requirement must be on the of scalar operator scalar extension let arguments again this common shape which is also the shape of the expression result be this shape is used to form a data structure the expression called the coordinate the coordinate is described by arrays and currently for i in om during execution the array indicates the of the result element being produced the array is initialized by ci i in and is constant throughout execution also included is a indicating the coordinate currently the coordinate an following operations global variable being on using t then implements the tv ta using field extraction the notation for push on the stack the contents of base address of a m it may seem redundant to specify both t and l we are the sharing of discussed below tw tv t tn this operation arises in the implementation of compression and will not be treated further in the current section we are now ready to describe the compiled code for our expression let t ts denote the list of generated compiled during the elimination code has the form of grid selectors the loop advance while do begin o end code for scalar operations as described in section advance advance if then go to loop else if o then done else begin go to advance end we will instructions call the code following the universal all the advance there are two important optimization we perform on the above code they are called sharing and coordinate compression we deal with them in turn by sharing we refer to the fact that the same can often be shared by several atoms in the expression we can this sharing as follows an atom a which is a descendant of some selector s in the tree will be called visible from s if there is no other selector on the path from s to add a dummy grid selector to the top of the expression tree an is generated not by each leaf but rather by each selector that has a nonempty set of atoms visible below it when a reaches this selector it can be used to generate an that will be shared by the set of atoms in question the next optimization coordinate compression is important because it frequently happens that the applied grid selectors affect only a few of the of the atoms involved thus the order in which these atoms are stored in memory corresponds to a large extent with the order in which they need to be accessed so as to produce the result specifically if for some coordinate c it is true that all generated have tic o and is not needed ie there is no compression along that coordinate then coordinate c can be merged into coordinate cl for practical apl expressions the above optimizations are very important consider for example all three leaves a b and c are visible from the dummy selector at the root thus they can all share the same t coordinate compression will then the loops implicit in t and the coordinate into just one loop that goes around times this code is certainly the best we can hope to generate for the above expression and in general these optimizations allow us to get by with the smallest number of and loops possible note that the push pass must happen every of the expression atoms change however code previously generated will still be valid can be reused with the newly generated described above time the shapes the compiled the same code as reduction reduction has two novel aspects it must generate its own looping code which is not part of the universal secondly it has a number of special cases which will be briefly mentioned at the end of the section what happens when a goes through a reduction node in the expression tree assume the reduction is along the dimension of node n the newly formed will have another dimension added to it the semantics of apl require that this new dimension be traversed in the reverse direction an additional variable m the depth of the coordinate at the current point in the tree must also be maintained in the previous section m was constant it was always the rank of the final result a in going through a reduction in effect also ensures that the reduced coordinate h has become the last coordinate of the reductions argument here is the reduction code n nl m ml for i in lk do begin qi qi si e st di e di zi li end comment add a new coordinate recall that the semantics of apl require that it be traversed in the reverse direction qk m sk dk lk for i in k in do begin qi e si di li end a reduction node must compile a loop that applies the appropriate binary operation between all elements along the reduced coordinate the length of the reduced coordinate is saved in the expression frame at runtime the compiled code pushes that length on the coordinate and a loop starting with the appropriate identity element and repeatedly advances and operates on the next element of the argument subexpression when the coordinate is the coordinate is ie is and the result returned the details of these operations are straightforward and will not be described note that the coordinate gets used as a stack the compiled code always the global pointer and thus is never aware of the dimension number of the coordinate being on as a consequence the compiled code remains valid for of the same expression as long as the types of the expression atoms do not change the and dimension vectors can change without the code figure illustrates the various transformations described in this and the previous section the reader is to study this example in detail as a final note boundary conditions for reduction give rise to many complications consider a a aa the expressions i i o due to of space we do not discuss techniques for handling these complications p top t p s dll note o dll pw p j el c q sz ci l d l ps s fig propagation of a specialized with application and outer product and scalar extension to inner in this section we illustrate the power of the universal selector mechanism introduced in section we show how this mechanism can handle a special case of which we will call using this as our tool we can then transform expressions containing inner or outer products into equivalent expressions containing only scalar operators grid selectors reductions and these are expressions we already know how to compile the same can be done with the scalar extension problem we have until this section that is the problem of scalar operators with one scalar and one argument a pb will suffix of a equivalently be called pb l if pb is a note that if b is scalar this is always the case such a preserves the structure of it only adds dummy copies along the incorporated introduced o new dimensions a can be into a by marking the by the as dummy setting their ds to it turns out that by introducing appropriate and on the arguments we can transform an outer product into a scalar operator and an inner product into a scalar operator followed by a reduction how this is done is in fact most expressed in apl itself let and be any scalar operators equivalent to outer ao b stands for pa inner note p wa ea va a ta p kb vb tb pb ab we have broken the inner product transformation up into a series of subexpressions for the sake of clarity the reader can verify that each argument of the product is on by a and possibly a note that if we were of evaluating an apl expression in the straightforward way the above transformations would be extremely expensive as we are in effect creating may copies of the arguments of each inner or outer product since the above transformations show that these operators are redundant one that they were introduced into the language in order to provide efficient implementations of certain common operations with our delayed evaluation strategy the multiple copies will of course never be generated and they introduce no overhead at runtime scalar extension can be handled in an entirely analogous way in the pass scalar operators with one scalar and one argument can make a note of this fact later during the push pass these operators can just in effect introduce a on to the scalar argument that will make it to the one using the same principle of dummy we can easily implement more general kinds of than apl allows slicing in this section we introduce a general technique for portions of an array as its elements are computed which we will call slicing this technique is an integral part of our compilation with delayed evaluation strategy slicing gets used to store subexpressions whose value will be required many times thus it also gets used to the effects of operators that break the streaming the results of such operators are often not needed in their but only certain slices an appropriate scheme the full expression and the subexpression by the operator can then save space a of array or subexpression a is defined as i where il i are valid indices for array a with n in other words a is a array obtained from a by arbitrarily a value for all but the last k then letting these k vary through all their allowed values we will call inner higher note that for each k as we traverse a in order we will generate a complete set of of a our scheme will work by always computing and a slice of appropriate size for a given subexpression there are numerous situations in evaluating apl expressions in which a subexpression of size should be saved in order to avoid consider as examples a where a is a very complex scalar or ao xb where again a is complex and b is large note that we already have the tools to discover when these situations arise in both of the above a was introduced during the processing of the expression this leads to with ds equal to o along certain to be called the dummy thus the reuse of certain elements such a indicates the need to save a slice of its selected result by selected we mean that only that portion of the true slice need be generated which will eventually in the production of the final result the slice size can be determined once the has been into the let s be the coordinate just lower than the outermost dummy coordinate take s if the outermost dummy coordinate is coordinate o storage will be allocated for all of the which are higher that s coordinate s itself will be called the slicing coordinate the slice naturally acts as a buffer between the full expression and the subexpression below the the code for the subexpression is placed in a separate the main and subexpression communicate data via the slice control is accomplished via a in the main code and a producing in the subexpression code the is built from the in the usual way except that along dimension s to the origin and along any dimension lower that s is a the which the subexpression receives has all dummy removed this modified is then propagated down the subexpression in the usual way finally the producing is built from a trivial for the subexpressions selected result except again that along dimension s to the origin how does control pass back and between the two let us first note that each will be responsible for its own yet we want all to share the global coordinate for obvious efficiency reasons it turns out that the following simple policy solves the coordination problem every time a slice producing or is advanced control passes to the if the coordinate being advanced is lower or equal to the slicing dimension this elegant rule also subsumes initialization difficulties at the beginning we set and start by the main along that dimension of course slicing may recursively happen within the subexpression and so on in general there will be several separate one for each piece of the entire expression that was introduced by slicing this may be smaller than the number of in the expression but this is a further optimization we do not discuss here the above coordination rule works in the general case as well for instance each scalar which is needed many times will be computed exactly once no matter where it appears in the entire expression happens because the for a scalar always consists entirely of dummy dimensions and thus the scalar becomes available through a slice with slicing coordinate equal to therefore namely when the scalar will be computed and the various exactly once are advanced at the beginning of time the same idea can be used to save space when operators that break the streaming such operators stop the propagation of a s coming down from the root however rather than evaluating the entire subexpression we can often proceed by only having the subexpression a slice at a time thus for example can easily be evaluated a row at a time etc the smallest required slice is a with k the smallest integer such that all but the last k of the subexpression correspond to order traversal this addition of memory to our delayed evaluation strategy is not entirely without cost at runtime if the runtime bindings of the atoms are such that the slice is only once then we are clearly doing unnecessary memory references this however is a somewhat event and furthermore to come into effect only when expressions are small in which case we can the the benefits of generality and overall efficiency for the compiled code seem well worth the price figure shows the runtime environment execution of the expression si xa with scalars the subexpression s s has been buffer for the s s using a coordinate expression t ss xa pt pa si scalars count o slice at n a slice n data slice code stream s s main local data pa g storage pool u fig the runtime environment conclusion we have seen how to compile good code for a dynamic language the generated code must be by a stating the assumptions for its validity in our case these assumptions consist mostly of assertions about the expressions atom types in ordinary apl usage it is extremely that these assumptions will be violated during multiple executions of the expression if that should happen then the compiler must be on the expression note that if our machine were able to interpret relative to a type specification even that step would not be necessary acknowledgements the authors would like to thank alan j l alan and peter for valuable comments on the paper references philip report an apl machine stanford university february and begin a p on the essence of compilation proceedings of ifip conference on formal description of programming concepts august pp peter and james h morris jr a lazy evaluator proceedings of the rd acm symposium on principles of programming languages january pp alan et al smalltalk instruction manual xerox technical report e knuth semantics of context free languages math sys th liskov and abstraction mechanisms in clu proceedings of acm conference on language design for reliable software march pp w m language design university an approach to computer phd dissertation stanford phd thesis compiling a dynamic language yale university alan j updated science steps research department toward an apl report yale university compiler computer march t schwartz report on the computer institute of february on programming an setl project part i science department new york university j van a dynamic compiler for an journal july incremental language pp london and in methodology abstraction introduction university and and and information sciences institute technical report 