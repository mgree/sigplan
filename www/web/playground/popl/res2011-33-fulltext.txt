scheduling michael paris france qadeer microsoft research wa usa university of abstract we provide a new characterization of scheduling nondeterminism by allowing deterministic to delay their task in limiting the delays an scheduler is allowed we discover concurrency bugs exploring few independent of the number of tasks context switches or events our characterization applies to any systematic exploration eg testing model checking of concurrent programs with dynamic additionally we show that certain delaying admit efficient reductions from concurrent to sequential program analysis categories and subject descriptors d software engineering verification d software engineering testing and debugging general terms algorithms reliability testing verification keywords concurrency asynchronous programs delay introduction a concurrent program is a transition system combined with a nondeterministic scheduler the programs semantics is easy to describe the scheduler repeatedly chooses an enabled transition to execute executes it then chooses another transition and so on this inherently nondeterministic semantics is the root cause of programming errors that manifest and are hard to and repair a class of techniques known as model checking systematically explore this nondeterminism in order to discover or prove the absence of such bugs although systematically exploring or searching a concurrent programs behavior is a simple and intuitively idea the exploration is computationally expensive for programs in which the only source of nondeterminism is the scheduler the combinatorial cost is determined by two factors the maximum number i of scheduler invocations and the maximum number c of choices available to the scheduler at each invocation given these two factors the cost of exploration is while i naturally corresponds to the length of program executions c corresponds to the number of concurrently executing tasks both c and i grow in realistic programs causing a combinatorial explosion in the exploration cost permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm search is a basic strategy to the explosion in general one characterizes a subset of the search space by a bounding parameter p more behaviors are explored as p is increased and in the limit all behaviors are explored a is effective when useful information eg the presence of bugs is obtained by examining few behaviors ie low values of p in this work we introduce an effective search strategy for concurrent programs that handles both and tasks we begin by scheduler nondeterminism with the concept of delay our main insight is captured by the premise a deterministic scheduler is made sufficiently nondeterministic with the ability to delay its task in other words we can thus nondeterminism by a finite more concretely the search space by a deterministic scheduler m and a k when k exploration is limited to the unique execution produced by m when k the scheduler may from its usual schedule a total of k times over an entire execution for instance a delaying scheduler executes all of its scheduled tasks to completion in a given round i before to the next round where the tasks delayed in round i are again scheduled as the total number of delays is bounded by k the number of is bounded by k the search space for any deterministic scheduler and k is bounded by ik is with many properties · is a canonical characterization and a means of limiting scheduling nondeterminism since the bound is chosen independently of the number of tasks the approach naturally handles both and tasks · the cost of exploration is polynomial in i our preliminary experiments discover previously unknown bugs in real programs with small that search does provide adequate coverage in practice · the choice of a deterministic scheduler is independent of the and every reasonable scheduler any given bug at some cost to minimize exploration cost it is even possible to perform parallel exploration using various deterministic even chosen at random · it is possible with certain deterministic to capture a concurrent programs semantics as a sequential program the scheduling complexity of these is reduced from undecidable with tasks or with tasks to npcomplete with or without in practice the reduction allows us to the numerous existing tools and techniques for verification including symbolic model checking and symbolic exploration with smt solvers contributions our contributions are summarized as follows · we introduce a canonical characterization of scheduling nondeterminism allowing a simple elegant and unified approach for exploration of concurrent programs with dynamic · we show that delaying deterministic can efficiently discover both known and bugs by systematic exploration eg testing · we give program translations using encodings of certain delaying the encodings rely on extensions of and reps s guess and constrain methodology and ultimately lead to practical verification algorithms · we identify an npcomplete analysis problem under a particular delaying scheduler the complexity is lower than related bounded reachability problems comparison to related search represents a convergence of a over the last of search techniques for concurrent programs given a number i of scheduler invocations and c of choices available to the scheduler at each the cost of complete search is limits the number of scheduler invocations by a d reducing the search complexity to though this approach is taken by it is clearly not effective as d approaches i in practical terms errors in a program execution remain difficult to discover labels the choices available to the scheduler by distinct task identifiers and bounds the number of label changes in executions by a c here the complexity is reduced to the polynomial dependence on execution length to a large extent the problem with low values are sufficient for finding many bugs in real programs however in many cases bugs are only after each of the say c tasks have a to execute in practice this dependence arises in eg initialization patterns protocols etc consequently the exponential dependence on c in is practically an exponential dependence on c as well worse yet when tasks are created dynamically the same ill as errors in a chain are discovered only for high values of c also labels scheduler choices with distinct identifiers but only bounds the number of label changes by p label changes due to blocking or completion are not search addresses the main problem with as each task gets the to complete its execution unfortunately the number of label changes is expected to depend on c the search complexity for large values of c as we can see when tasks are numerous or dynamically created the existing exploration techniques are these problems by allowing an unbounded number of tasks to execute and bounding instead the degree of variation from a deterministic scheduling order the exploration cost using k delays is ik which is independent of the number of tasks asynchronous programs so that our approach handles dynamic by design we begin with a model of simple asynchronous programs corresponding to the style of singlethreaded programming the style is typically used as a lightweight technique for adding to singlethreaded applications by up computations into a collection of tasks in this model control begins with a nonempty task buffer of pending tasks from which a a single task to execute the transfers control to the task which is essentially a sequential program that can read from and write to global storage and post additional tasks to the task buffer when a task completes its execution control returns to the which another task from the task buffer and so on when the has control and the task buffer is empty the program terminates this model forms the basis of web applications and has been shown useful for building fast servers and embedded networks in what follows we generalize the usual notion of dispatch ie choosing any pending task by a scheduler by which we a simple program semantics to capture programs with tasks or alternatively concurrently running tasks on sequentially consistent we extend the simple asynchronous model to permit arbitrary task with the although the extension may seem the resulting language is powerful enough to model concurrent programs with arbitrary and synchronization eg by inserting a before every shared variable access the model particularly typical kernel code and device drivers which are often implemented as a collection of the scheduler for the moment let tasks and be uninterpreted sets and blocked tasks × b be an uninterpreted predicate intuitively each element w tasks is a task to be scheduled each element g is a global state and g holds when w is not enabled in the global state g a scheduler m d empty give take consists of a datatype d of scheduler objects m d a scheduler constructor empty d and scheduler update functions give d to receive tasks and take d × d × tasks to determine which tasks can be scheduled next the scheduler m is deterministic when for all m d and g g contains at most one element and is nonblocking when for all w tasks g and m m d g implies m w g in general nonblocking must be aware of the program state hence the argument g to take fix m d and g a task w is pending in m if there is a sequence wj mj such that m m wj w and for i j mi wi g in other words w is pending when w can eventually be taken from m a task w is by m when there exists m such that m w g scheduler the bag scheduler the scheduler bag is defined on the multiset domain of tasks as def w def m w g def m w w w m the bag scheduler is nondeterministic since all pending tasks are returned by each application of here and are the multiset union and difference operators p h s x var g t h proc p var l t s s s x e skip assume e if e then s else s while e do s call x p e return e post p e gl figure the grammar of simple asynchronous programs we consider only m that refine the bag scheduler ie satisfying the property property any sequence of give and take operations allowed by m are also allowed by bag note that a scheduler can be can drop pending tasks though is not allowed to tasks out of ie schedule tasks that have not been the following is an example of a scheduler that refines the bag scheduler scheduler the bounded bag scheduler the bag scheduler bb is defined on the multiset domain of tasks as def w def m w m if k otherwise g def m w w w m the bounded bag scheduler is nondeterministic since all pending tasks are returned by each application of it is since it will drop tasks if its bound k has been reached program syntax let be a set of procedure names a set of values containing true and false and t the type of values the grammar of figure describes our language of simple asynchronous programs where p ranges over procedure names we leave the syntax of expressions e unspecified though we do the set of expressions contains and the choice operator a simple synchronous program or sequential program is a simple asynchronous program which does not contain each program declares a single global variable g and a sequence of procedures p pn each procedure p has single parameter l and a toplevel statement denoted sp the set of program statements s is denoted we assign the statements of simple asynchronous programs their usual meaning in particular post p e is an asynchronous call to procedure p with argument e which returns immediately with the expectation that p is invoked at a later time the assume e statement proceeds only when e evaluates to true later on we use the to block executions in a reduction to sequential programs program semantics for the remainder of this section we fix a predicate blocked tasks × b and a scheduler m d empty give take a frame s is a valuation to the variable l with a statement s a task w is a sequence of frames and the set × of tasks is denoted tasks a configuration c g w m is a valuation g of the global variable g with a task w tasks and a scheduler object m d we say a task w is blocked in a configuration g w m alternatively in a global state g when g true for expressions without program variables we assume the existence of an evaluation function · such that for convenience we define e g s w m def eg def l since g and l are the only variables the expression l has no free variables a statement context s is a term derived from the grammar s s s we write ss for the statement obtained by substituting a statement s for the unique occurrence of in s and write g s w m s to denote the configuration g ss w m figure gives the semantics of simple asynchronous programs as a set of operational steps on configurations the choice operator is used in the call and rules only as a for an value the assume rule restricts the set of valid executions a step is only allowed when the expression e evaluates to true this confined to intermediate crucial for our reduction to sequential programs in section a task w in a is said to be and a task w in a is said to be we refer to the semantics instantiated by a scheduler m as the m semantics the natural see scheduler corresponds to the usual asynchronous program semantics we call the semantics of synchronous programs ie those without synchronous semantics which is in fact independent of the scheduler a configuration g s return empty where s does not contain is called m initial an m execution to cj is a configuration sequence h cc cj where · c is m initial and · ci ci for i j the initial statement s begins the execution by tasks if no tasks are by s the execution ends when s completes we say a configuration c g w m alternatively the global value g is m reachable when there exists an m execution to c and is m final when in addition w and for a scheduler m and program p the m value set on p is the set of values gc of the global variable g such that c is m final the m semantics on p is to the m semantics on p when the m value set on p is equal to the m value set on p although we have not made task identifiers explicit we may assume that a set u n of task identifiers is defined by an execution h cc cj such that u u is the task identifier of a task by cu cu with an execution in mind there is no ambiguity when referring to the frame or execution of a task by its identifier we say u is resp in h when there exists i j such that the task identified by u is resp in ci ci in the presence of the only the values of completed executions are guaranteed to be valid we could also define task identifiers by their tasks in h skip s cs assume true ec e true ec e then s else s cs false ec e then s else s cs true ec e do s cs while e do s false ec e do s g eg g sg e w m g w m eg g sl e w m g w m call eg g x pe w m g sp sx w m g return e v eg sx w m g sx v w m post eg w sp g pe w m g w w dispatch m w g g g m g w m g return e m g m figure the operational semantics of simple asynchronous programs parameterized by the scheduler m d empty give take programs with the language of asynchronous programs extends the grammar of figure with the production s yield and the semantics of figure with the rule yield w w g w m g w allowing arbitrary task simply single be added to and later from the task buffer synchronization and blocking our model of asynchronous programs ie with the can express arbitrary synchronization disciplines example locking mutual exclusion can be modeled by adding an additional global variable lock critical sections are with the lock acquire operation encoded by while lock true do yield lock true and the lock release operation encoded by lock false the g predicate is defined to hold if and only if w lock true do yield w and true ie w is waiting for a lock that is in use although the presence of these primitives does not imply that critical sections are mutually exclusive ensuring mutual exclusion in their presence is a and orthogonal problem since exploration deadlocks when blocked tasks are scheduled we are generally interested in nonblocking example nonblocking lock scheduling a deterministic nonblocking scheduler for the mutual exclusion encoding of example can be defined by between ready tasks and tasks waiting for the lock the scheduler must not pick a waiting task when true scheduling to limit the nondeterminism present in a scheduler and thus the number of executions at the same time the ability to consider interesting allow deterministic to exercise a limited and parameterized number of from their deterministic schedules we define a delaying scheduler as a tuple m d empty give take delay where d empty give and take are defined as before for and the delay d × tasks d function is intended to the scheduler of the definitions of deterministic nonblocking pending and remain unchanged we extend the semantics of figure with a rule delay m w g g m g w which we refer to as a delay operation and we say w is delayed note that the delay operation occurs at the point when tasks are usually not the at point when they are we say an execution h is when the number of delay operations in h is at most k a delaying scheduler m is limit sound when for any global value g there exists k n such that g is m reachable in a execution limit soundness has an operational characterization when h is an execution to c we say w is blocked resp pending in h when w is blocked resp pending in c a delaying scheduler m is when for any m execution h and task w in h w is either pending or blocked in h otherwise we say m is a delaying scheduler m is when for every reachable configuration c with pending task w there exists a sequence c cj of such that w is in cj lemma a delaying scheduler m is limit sound if m is and here we assume w is uniquely identified by its task identifier intuitively a and scheduler can always delays to access any task pending in the bag scheduler since tasks are also pending in a detailed proof appears in our extended technical report as a consequence of lemma our approach will discover any bug with some using any wellbehaved ie deterministic the required to a given bug depends on the scheduler since the number of explored schedules is exponential in the in practice one may want to the scheduler to fit a particular program or alternatively it is also possible to run several with different in parallel with the same small or even to choose at random here we a of simple deterministic delaying to compare with existing exploration techniques and as a basis for defining practical exploration algorithms scheduling one simple scheduler cycles through tasks in order the scheduler advances to the next task when the current task either i completes execution ii yields and is blocked or iii is delayed scheduler the scheduler let rr be the delaying scheduler tasks × n give take delay where the give m i w operation is defined by if w sp for some procedure p then append w to m else w is a yielding task insert w into m at position i the delay m i w operation is defined by insert w into m at position i increment i modulo m and the take m i g operation is defined by if mj is blocked in g for all j m return while mi is blocked in g increment i modulo m remove w from m at position i return w it is easy to see that rr is deterministic and nonblocking the scheduler demonstrates that scheduling captures scheduling example in the each delay operation simulates a to the next task to any of n tasks generally requires n successive delay operations given a k and a program with n tasks the set of contains the set of executions from the perspective of limiting scheduling choice does not give a direct bound the number of choices nk is also dependent on the number of tasks in contrast exploration requires only k choices depthfirst scheduling another simple scheduler schedules all tasks by a given task u before scheduling tasks that were pending when u was in a discipline this scheduler is particularly since as we show in section for any k and asynchronous program p we can encode the depthfirst semantics of p as a sequential program we define the give delay and take operations by and returning the scheduler object u u × u × u u u u u u × u u × u × u u u u u u u × u × u × u u u u u figure depthfirst traversals of asynchronous call with task identifiers u u arrows indicate dotted arrows indicate delays and the dotted line the is u u u scheduler the depthfirst scheduler let dfs be the scheduler tasks × tasks × tasks give take delay for a scheduler object qr qd we call qr and qd the handler round and the give qr qd w operation is defined by push w to the delay qr qd w operation is defined by push w to qd and the take qr qd g operation is defined by empty into qr if qr then empty qd into qr if qr then return pop w from qr return w figure shows the of a simple asynchronous program it is easy to see that dfs is deterministic and though is not generally nonblocking to understand the order in depthfirst scheduling we view executions as trees where nodes are tasks and the parent relation corresponds to the relation on tasks let h be an execution and u a set of task identifiers from h we define the asynchronous call forest as the ordered forest f with nodes labeled by the identifiers u u of tasks in h when u was by a task u in h u is a child of u in f children are ordered by the order they are we capture yielding by as tasks note that synchronous calls are not explicit in f the tasks along the synchronous execution of u appear directly as children of u the of a program proceeds in a sequence of ending when a take operation both and qr empty with this we schedule each delayed task w in the round i following the round i where w is delayed after all pending tasks of round i have been scheduled a round partitioning of an asynchronous call forest f with nodes u is a labeling r u n on the nodes of f such that ru ru whenever u is an ancestor of u given an execution h we can think of the round partitioning as a partition of f into asynchronous call f f where each task u is in the forest corresponding to the round in which it executes the depthfirst scheduler traverses the asynchronous call forest in a depthfirst preorder see figure in this order yielding tasks u are only after the tasks that u has before yielding have been scheduled we consider rather than trees since more than one task may be as we at the beginning of each round in certain delaying var g t proc l t s post p e s post p e s proc l t s proc l t s gy s g s g s g g g push pe on push pe on return g p p p pp pp gy s g g return g g g p p gy s g g return g g g figure the of a simple asynchronous program the scheduler objects qr since there are no delays qd is given below the corresponding points in the execution which are labeled with the global values lines indicate procedure control and dotted lines indicate control note that the bag scheduler allows one additional execution where p executes before p testing in concurrency analyses which explicitly enumerate execution schedules eg systematic testing using deterministic has a clear scalability advantage over existing bounded exploration approaches for instance the number of executions of a program using n tasks is exponential in n since a complete execution must apply at least p label changes the number of executions does not depend on n here we demonstrate that although much fewer schedules than with the same bound existing bugs with p are also with p delays furthermore exploration is able to discover bugs which have not been by bounding to the exponential increase in schedules with respect to the number of tasks to demonstrate this advantage we have implemented the delaying scheduler scheduler in the concurrency testing tool to directly compare and on three programs ccr futures both written by microsoft product groups and region ownership written by peter of we feel the scheduler is appropriate for comparison with since it we observe every bug found with p is also found with p delays significantly fewer schedules before discovering a execution can discover at least one bug that cannot be found with under similar as a systematic testing tool repeatedly executes each test case to completion until all possible each test case have been explored during the exploration has no control over the number of tasks created nor the number of steps taken by the input program thus each test case is expected to the program to termination under any schedule ccr concurrency and coordination runtime provides a concurrent programming model with highlevel primitives for data and without the use of explicit and synchronization we evaluate a suite of test cases various parts of ccr each test case takes between ­ steps to complete and creates at most tasks before a known bug found with of these bugs were discovered with and the remaining one with the delaying each of these bugs with the same budget of delays but fewer schedules figure compares the number of schedules explored to discover each bug discovering the remaining bug required exploring schedules using and only schedules using delays schedules w bounding xy schedules w delay bounding figure comparing the number of schedules explored between and before discovering known bugs in tests of concurrency and coordination runtime ccr found each of these bugs with futures futures library provides a synchronization primitive based on a value for a computation whose result is not yet known we find two with bugs also with delay both the and exception are exposed after the program has tasks bug steps schedules pb schedules db exception region ownership peter region ownership library concurrency and coordination based on objects into regions that communicate with each other via asynchronous procedure calls the library is by a single test case a system during testing at most tasks are created and at most execution steps taken with and terminates without finding a bug with generates schedules over several without terminating after which we manually it bounding delays to and did not expose any bugs though a of discovered a previously unknown bug after exploring only schedules to determine whether this bug can be discovered with we focusing to the methods in our error trace after exploring schedules without discovering the bug thus this is the first bug has ever found in a realworld program that requires at least of depthfirst scheduling the depthfirst scheduling order ie of scheduler has a rather nice property the stack of pending tasks used for a depthfirst traversal of the asynchronous call forest see section can be combined with a synchronous programs call stack ie of activation records in this section we exploit that fact to encode a programs dfs semantics as a sequential program a similar encoding is possible for reverse depthfirst scheduling ie where each tasks children are traversed in reverse order in section we reveal the most basic encoding of depthfirst scheduling for simple asynchronous programs in sections and we extend the basic encoding to handle delaying and to improve clarity we the syntactic extensions of appendix a which each reduce to the original syntax of asynchronous programs depthfirst scheduling we begin by defining the function · dfs which translates a simple asynchronous program p into a synchronous program p dfs which encodes the depthfirst scheduler var g t h dfs def var g t t h dfs proc p var l t s dfs def proc p var l t s dfs s s dfs def s dfs s dfs x e dfs def x e skip dfs def skip assume e dfs def assume e if e then s else s dfs def if e then s dfs else s dfs while e do s dfs def while e do s dfs call x p e dfs def call x p e return e dfs def return e post p e dfs def let t g in let t in g call p e assume g g the initial configuration is translated as g s return e m def dfs g s return e m with the initial global value g def g g and the initial statement s given by s def let in s dfs assume g g the key mechanism enabling the encoding is the introduction of global values in the form of unconstrained symbolic constants these are essentially variables whose values will be available only later in the sequential execution once the appropriate global values are available the corresponding are constrained in the translation we replace with and introduce the following variables to ensure the correct global values are observed along the · caches the global value of a task so that the task can observe its future global value in g and the task can resume from its current global value without interference · stores the global value reached when each task completes · stores the global value reached after all tasks that have been thus far ie that have appeared on the have completed example the of the program in figure is var g t t proc l t s let t g in let t in g post p e dfs call p e assume g g g g s let t g in let t in g post p e dfs call p e assume g g g g s proc l t s proc l t s where we assume do not contain to the right of the we indicate the values stored in the variables and which are the same values used below this program has the following unique sequential execution s g g s g g call p e return call p e return g s g g g s g g g s g g where the double lines indicate part of a post ie up to the call here each global value gi matches that of the asynchronous of figure execution begins with an initial guess for of g ie the global value after p executes the latter values for of g and g are the global values after the execution of p and p the final global value is g lemma let p be a simple asynchronous program the syn semantics of p dfs is to the dfs semantics of p proof sketch let h be a of p and consider the asynchronous call forest f of h with tasks u u u uj furthermore without loss of generality suppose the dispatch order of tasks in h is u u uj since the corresponds to a preorder depthfirst traversal of the forest f each task us children u are executed before any other pending task or task to be later in us execution in this way the order corresponds exactly to the execution of executed the order in which tasks would execute had they been called instead of that the tasks would observe not the global state at the end of the current tasks execution but an intermediate global state to ensure that tasks observe the global state we rely on the following invariant of the synchronous semantics of p dfs the value of at the beginning of execution for each task ui is equal to the final value of g at the end of execution for the task ui immediately preceding ui in the order combining the invariant with the of order in the of p and the synchronous program p dfs we have the sequence gj of global values during dispatch points of h ie when control is given to the is equal to the sequence of global values before the synchronous execution of each task and after all tasks have executed finally the last statement to be executed in p dfs sets the global value to ensuring the final global states of p and p dfs are equal note that in general for an arbitrary deterministic scheduler algorithmic exploration is not possible for example even scheduling with the scheduler scheduler is undecidable by reduction from the problem for queue machines from an point of view depthfirst scheduling remains decidable since the asynchronous is a stack which can be combined with the synchronous activation stack delaying depthfirst scheduling the function · k dfs translates a simple asynchronous program p into a synchronous program p k dfs which encodes the k delay bounded depthfirst scheduler var g t h k dfs def var g t t k n h k dfs proc p var l t s k dfs def proc p var l t n s k dfs call x p e k dfs def call x p e post p e k dfs def let t g in let t in var k n while and do k k g call p e k assume g g where the omitted statements are translated exactly as in the translation · dfs the initial configuration is translated as g s return e m k def dfs g s return e m where the initial global and local values g and are g def g g k and def l and the initial statement s is given by s def let n in let t k in s k dfs assume g assume assume g the encoding extends the encoding in two important ways first the schedule proceeds in k we must note which round each task executes in and separately the global values in each round second each time a task is there is the possibility of delay we must keep track of how many delays have been spent to this we introduce the following auxiliary variables · and are used exactly as before to cache the tasks observed global value and store the global value reached at the end of the task · is the extension to each stores the global value reached after all tasks that have been thus far and are executed in round i have completed · stores the remaining budget of delays · indicates which round a given task executes in · k is once per delay of the task then passed as the note that a task may be delayed more than once and we simulate all the delays of a given task example the of the program of figure allows the following sequential execution s g g call p e return s g g g s g g call p e return g s g g g s g g where p executes before p this execution two separated by a dashed line p though second executes with p in the first round while p executes alone in the second thus takes the values g g and takes the values g g the final global value is g lemma let p be a simple asynchronous program the syn semantics of p k dfs is to the k delay dfs semantics of p proof sketch as in the proof sketch of lemma there is a between the order of tasks in p and p k dfs except in this case the order is a round depthfirst preorder here we adapt the previous invariant to the case the value of at the beginning of execution for each task ui of round k is equal to the value of g at the end of execution for the task ui immediately preceding ui in the order for executed tasks ui ui in ka and kb the resulting global value of ui is guaranteed to be in which is in turn guaranteed to be equal the initial global value of task ui var g t h dfs def var g t t h dfs proc p var l t s dfs def proc p var l t s dfs call x p e dfs def call x p e return e dfs def return e post p e dfs def let t g in let t in g call p e assume g g yield dfs def assume g g figure the symbolic encoding of depthfirst scheduling with yields extending the symbolic encoding of section the translation of the missing controlflow statements and initial configuration is identical to the encoding depthfirst scheduling with perhaps surprising is the fact that our symbolic encodings of delaying depthfirst can be extended to asynchronous programs the encoding of figure extends the depthfirst scheduler encoding of section to handle the key difference is that the guess of the of each handler should not be verified only at the end of a handlers execution instead the guess is validated if a handler yields at which point a new guess is made and the new guess will be validated either at the next yield one at the end of the handlers execution to allow multiple throughout a handlers execution we simply ensure the is in scope throughout by making it a parameter to every procedure the extension to delaying depthfirst scheduling is extending to multiple is orthogonal to handling yields and is done exactly according to the extension of section in particular occurrences of are replaced by their and the round counters also happens at we omit the full definition of p k dfs for asynchronous programs p since it is redundant lemma let p be a asynchronous program the synchronous semantics of p k dfs is to the of p complexity of depthfirst scheduling thus far we have given that reduce depthfirst semantics to sequential semantics since the number of program variables in the resulting sequential program is ok the worstcase complexity of exploration using this reduction for programs with domains is exponential in k what remains is the question of whether the exploration via this reduction is asymptotically optimal here we find that a algorithm is the proofs of these results are quite technical and can be found in our extended technical report for the remainder of this section we assume the of asynchronous programs ie the set is finitestate we show that depthfirst is an npcomplete problem though this exploration corresponds to an underapproximation of the program semantics the complexity is lower than the precise of and and jhala and majumdar for asynchronous programs additionally our underapproximation has a lower complexity than the underapproximation used by jhala and majumdar s algorithm which corresponds to our bounded bag semantics see scheduler note in the case of programs the analysis problem is generally undecidable interestingly depthfirst exploration has the same npcomplete complexity as for a finite number of tasks even though scheduling an unbounded number of tasks problem depthfirst scheduling for a given k n initial configuration c and global value g of an asynchronous program p does there exist a to g by reduction from the circuit satisfiability problem we show our exponential algorithm for depthfirst scheduling is likely to be asymptotically optimal theorem depthfirst scheduling is nphard to show membership in np for the case we give an algorithm that a execution witness to the target global value given by the sequence of tasks delayed in and a global value reached at the end of each round for a k we validate the guess by applying k polynomialtime sequential program analyses to sequential encodings of each round of initial conditions of each round are determined by the delayed tasks and final global state of the previous round validation ensures that each round can indeed reach the global value while delaying exactly the delayed tasks theorem depthfirst scheduling for simple asynchronous programs is in np in fact we can extend the proof of theorem to the case the presence of yields an additional technical challenge a extension of the execution witnesses to record delayed does not work because the activation stacks have no bound we solve this problem essentially by the activation stacks of tasks need not be stored across instead we may tasks to their stacks from same socalled lazy technique by la et al theorem depthfirst scheduling for asynchronous programs is in np thus we achieve tight on depthfirst scheduling corollary depthfirst scheduling is npcomplete since we are interested in the scheduling complexity rather than complexity arising from program data we have restricted the program syntax so that a fixed number of variables is in scope at any moment indeed the reachability problem is in the number of variables even with only a single recursive task due to the encoding of states in the corresponding pushdown system we assume the k is written in unary verification the of section allows any sequential analysis algorithm to be lifted immediately to a concurrent analysis algo rithm a sequential analysis of p k dfs is exposed to all concurrent behaviors of p with a depthfirst scheduler an underapproximation of p s concurrent semantics the additional implementation effort is minimal since only the translation is required as a we have implemented a symbolic encoding of the delaying depthfirst scheduler in the concurrent checker analyzes closed concurrent software modules ie each module is closed from below using for external procedures and closed from above using a test driver with symbolic inputs while concretely executes a closed concurrent program with a single input vector see section symbolically verifies a closed concurrent program on a potentially unbounded set of input vectors to analyze the input program precisely loops and recursive procedure calls up to a bound prior to this work implemented cation for programs with a finite number of statically declared tasks our experience applying to realistic programs indicated that the theoretical exponential complexity in the number of contexts does manifest in practice furthermore many concurrency errors in realistic programs require a large ie number of tasks to be executed and the number of tasks required to manifest a concurrency error is a lower bound on the number of re contexts in these cases discovery becomes expensive in contrast scheduling these errors with very few or the reason depthfirst scheduling works well despite the fact that it is an artificial ordering is that the of these tasks need not in any interaction they simply need to be executed in order to reach a program point which the bug thus our implementation of scheduling improves in two important ways we enable to handle programs with dynamic and to find a class of bugs at low computational cost the implementation works in three phases in the first phase we translate a concurrent c program into a concurrent was originally intended as an intermediate language for representing the semantics of simple imperative sequential programs we have extended the syntax and semantics to express concurrent behavior in the second phase we transform the concurrent program into a sequential program encoding the depthfirst semantics it is that our algorithm was very easy to implement with simple minimal extensions to finally we verify the resulting sequential program using the of field abstraction generation and smt solving we have applied our implementation to symbolic exploration of over device drivers with lines of code in the process we have found previously unknown bugs with a maximum of the developers of these drivers have the accuracy of these bugs more importantly with the ability to handle dynamic we can precisely model the in the device drivers execution environment eg interrupts deferred procedure calls driver request and completion etc thereby extending the applicability and precision of related work the programming models considered in this paper have received of attention from researchers interested in abstractions of concurrent programs the model has not been so studied primarily because the reachability problem is known to be undecidable even with a finite number of to interference between multiple stacks more attention has been to the model of asynchronous programs and introduced the model explicitly to reason about programs and showed that reachability is and majumdar this result to show that the problem is to this high worstcase theoretical complexity jhala and majumdar suggest a scheme that combines an and an computation of the reachable states verification only those executions of a concurrent program in which the number of context switches is bounded globally by a value however the idea of does not make intuitive sense for programs with large or unbounded number of tasks this problem is wellknown and other researchers have proposed various et al suggested stratified that allows an unbounded number of context switches without decidability la et al exploit the nondeterministic scheduling scheme of and reps to achieve unbounded number of context switches for parameterized concurrent programs exploiting sequential for concurrent program verification is also an active area the verifier this approach by providing a transformation from multithreaded programs into sequential programs that the set of behaviors of the original program and reps achieved a by providing the first translation that computes a underapproximation for any this approach also the idea of and them later at an appropriate control point in the execution we exploit this idea in our transformation as well a key of and reps s socalled eager approach is that control states unreachable in the original concurrent program may be explored in the transformed sequential program la et al s lazy technique addresses this by repeatedly to the control points where values would have been used et al compared the two approaches in the paradigm where as opposed to modelchecking benefits of laziness are since the eager approach in fact the lazy one et al have introduced a reduction from concurrent programs with to sequential programs though the construction requires a bound on the number of tasks although none of these handle dynamic la et al have recently introduced a of their parameterized modelchecking algorithm which does handle an unbounded number of tasks finally such as and introduce delays by adding sleep statements introduces delays by randomly task these techniques share with the ability to scale to many tasks in fact a version of the algorithm provides a scheduling complexity that is independent of the number of program tasks the mechanism for achieving this scalability is based on their characterization of a bugs depth as the minimum number of events that must occur in a certain order to reveal the bug attempts like to the effort required to discover a given bug but is defined with respect to ordering constraints rather than from a deterministic scheduler conclusion we have introduced a canonical characterization of scheduling nondeterminism by considering deterministic with the ability to delay their task we demonstrate that is an effective search strategy for concurrent programs by extending the applicability of existing techniques furthermore we identify a concurrent analysis problem via depthfirst scheduling and we show that our depthfirst delaying admit practical sequential reductions allowing us to lift existing sequential analyses to concurrent analyses our approach is generally applicable to concurrent programs with dynamic and arbitrary and synchronization acknowledgments we thank ahmed majumdar and for providing helpful insight and the anonymous reviewers for their numerous comments and suggestions references m abadi and l lamport the existence of refinement mappings theor comput sci ­ m f a and s qadeer analysis for concurrent programs with dynamic creation of threads in tacas proc th international conference on tools and algorithms for the construction and analysis of systems volume of lncs pages ­ springer t ball s k e m and s qadeer sealing for efficient concurrency testing in tacas proc th international conference on tools and algorithms for the construction and analysis of systems volume of lncs pages ­ springer m and k r m leino of programs in proc acm workshop on program analysis for software tools and engineering pages ­ acm a and r majumdar personal communication july s and m personal communication november s p m and s a randomized scheduler with probabilistic guarantees of finding bugs in proc th international conference on support for programming languages and operating systems pages ­ acm e m clarke and e a emerson design and synthesis of synchronization skeletons using temporal logic in logic of programs volume of lncs pages ­ springer j a and g linux device drivers inc rd edition l m de and n z an efficient smt solver in tacas proc th international conference on tools and algorithms for the construction and analysis of systems volume of lncs pages ­ springer r and k r m leino a typed procedural language for checking objectoriented programs technical report microsoft research o e e y g and s ur framework for testing multithreaded java programs concurrency and computation practice and experience ­ m s qadeer and z scheduling a canonical characterization of scheduler nondeterminism technical report microsoft research http p and r majumdar algorithmic verification of asynchronous programs abs j j a new approach to web applications february n a j hu and z translations for concurrent software an empirical evaluation in proc th international workshop on model checking software volume of lncs pages ­ springer p model checking for programming languages using in popl proc th acm sigplansigact symposium on principles of programming languages pages ­ acm j l hill r a s d e and k s j system architecture directions for in proc th international conference on support for programming languages and operating systems pages ­ acm r jhala and r majumdar interprocedural analysis of asynchronous programs in popl proc th acm sigplansigact symposium on principles of programming languages pages ­ acm p m cs park and k an extensible active testing framework for concurrent programs in cav proc st international conference on computer aided verification volume of lncs pages ­ springer n s and j one stack to run them all reducing concurrent analysis to sequential analysis under priority scheduling in proc th international workshop on model checking software volume of lncs pages ­ springer e r morris b chen j and m f the modular acm trans comput syst ­ s la p and g reducing concurrent reachability to sequential reachability in cav proc st international conference on computer aided verification volume of lncs pages ­ springer s la p and g modelchecking parameterized concurrent programs using linear interfaces in cav proc nd international conference on computer aided verification volume of lncs pages ­ springer s la p and g parameterized programs under s k s qadeer and z static and precise detection of concurrency errors in systems code using smt solvers in cav proc st international conference on computer aided verification volume of lncs pages ­ springer a and t w reps reducing concurrent analysis under a context bound to sequential analysis formal methods in system design ­ l lamport proving the correctness of programs ieee trans software ­ m and s qadeer iterative context bounding for systematic testing of multithreaded programs in pldi proc acm sigplan conference on programming language design and implementation pages ­ acm m s qadeer t ball g p a and i finding and in concurrent programs in proc th usenix symposium on operating systems design and implementation pages ­ usenix association w programming the microsoft windows driver model microsoft press nd edition v s p and w an efficient and portable web server in usenix proc general track of the usenix annual technical conference pages ­ usenix c h computational complexity s qadeer and j model checking of concurrent software in tacas proc th international conference on tools and algorithms for the construction and analysis of systems volume of lncs pages ­ springer s qadeer and d wu keep it simple and sequential in pldi proc acm sigplan conference on programming language design and implementation pages ­ acm g contextsensitive analysis is undecidable acm trans program lang syst ­ k and m model checking multithreaded programs with asynchronous atomic methods in cav proc th international conference on computer aided verification volume of lncs pages ­ springer a syntactic sugar the following syntactic extensions are reducible to the original syntax of asynchronous programs of section here we freely assume the existence of various type and this does not present a problem since our program semantics does not restrict the language of types nor expressions multiple types multiple type labels t tj can be encoded by systematically replacing each ti with the t j i ti this allows local and global variables with distinct types multiple variables additional variables x t xj tj can be encoded with a single variable x t where t is the record type f t fj tj and all occurrences of xi are replaced by when combined with the extension allowing multiple types this allows each procedure to declare any number and type of local variable parameters distinct from the number and type of global variables local variable declarations additional local variable declarations var l t to a procedure p can be encoded by adding l to the list of parameters and systematically adding an initialization expression eg the choice expression or false to the corresponding position in the list of arguments at each call site of p to ensure that l begins correctly uninitialized unused values call assignments call x p e where x is not subsequently used can be written as call p e where t is an additional local variable or simpler yet as call p e let bindings let bindings of the form let x t e in can be encoded by x as a local variable var x t immediately followed by an assignment x e this construct is used to that the value of x remains constant once initialized the binding let x t in is encoded by the binding let x t in where is the choice expression tuples assignments x xj e to a tuple of variables x xj are encoded by the sequence let r f t fj tj e in x rf xj where r is a fresh variable a tuple expression x xj occurring in a statement s is encoded as let r f t fj tj f x fj xj in xj where r is a fresh variable and see replaces all occurrences of e in s with e when a xi on the lefthand side of an assignment is eg from the return value of a call we may replace the occurrence of xi with the the unused values arrays finite t with j elements of type t can be encoded as records of type t f t fj t where f fj are fresh names occurrences of terms ai are replaced by and e ej are replaced by f e fj ej 