well easy to popl consist program properties from big code department of computer science martin department of computer science department of computer science abstract we present a new approach for program properties from big code our approach first a probabilistic model from existing data and then uses this model to predict properties of new programs the key idea of our work is to transform the input program into a representation which allows us to phrase the problem of inferring program properties as structured prediction in machine learning this formulation enables us to powerful probabilistic graphical models such as conditional random fields in order to perform joint prediction of program properties as an example of our approach we built a scalable prediction engine called for solving two kinds of problems in the context of javascript syntactic names of identifiers and semantic type annotations of variables correct names for of name identifiers and its type annotation are correct in of the cases in the first since its release was used by more than developers and in only few has become a popular tool in the javascript community by the problem of inferring program properties as structured prediction and showing how to perform both learning and inference in this context our work up new possibilities for a wide range of difficult problems in the context of big code including invariant generation synthesis and others introduction the increased amounts of freely available high quality programs in code such as a situation big code by a recent creates a unique for new kinds of programming tools based on statistical reasoning these tools will extract useful information from existing and will use that information to provide likely solutions to problems that are difficult or impossible to solve with traditional rule based techniques permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ copyright c acm § input program dependency network relating unknown with known properties properties output program training data learned model learning § figure statistical prediction of program properties in this work we focus on the general problem of inferring program properties we introduce a novel statistical approach which properties of a given program by learning a probabilistic model from existing already annotated with such properties we use the term program property to both classic semantic properties of programs eg type annotations as well as syntactic program elements eg identifiers or code our approach is fairly general and can serve as a basis for various kinds of statistical invariant and statistical code the core technical insight of our work is transforming the input program into a representation that enables us to formulate the problem of inferring program properties be it semantic or syntactic as structured prediction with conditional random fields a powerful graphical model successfully used in a wide variety of applications including computer information and natural language processing to our knowledge this is the first work which shows how to in the context of programs by connecting programs to our work also enables immediate reuse of learning and inference algorithms to the domain of programs fig illustrates our structured prediction approach in the prediction phase shown on top we are given an input program for which we are to infer properties of interest in the next step we convert the program into a representation which we call a dependency network the essence of the dependency network is to capture relationships between program elements whose properties are to be with elements whose properties are known once the network is obtained we perform structured prediction and in particular a query referred to as maximum a map inference this query makes a joint prediction for all elements together by optimizing a function based on the learned model making a joint prediction which takes into account structure and dependence is particularly important as properties of different elements are often related a useful analogy is the ability to make joint in image processing where the prediction of a label is by the of to achieve good performance for the map inference we developed a new algorithmic variant which targets the domain of programs existing inference algorithms cannot efficiently deal with the combination of unrestricted network structure and number of possible per element finally we output a program where the newly properties are incorporated in the learning phase we find a good function by learning a model from a large training set of programs here because we deal with and map inference queries we are able to training methods name and type inference for javascript as an example of our approach we built a system called which addresses two important challenges in javascript syntactic identifier names and semantic type annotations of variables we focused on javascript for three reasons first in terms of type inference recent years have seen extensions of javascript that add type annotations however these extensions rely on traditional type inference which does not scale to realistic programs that make use of dynamic evaluation and complex libraries eg our work likely type annotations for real world programs which can then be provided to the programmer or to a standard type checker second much of javascript code found on the web is making it difficult to understand what the code is doing our approach likely identifier names thereby making the code readable again finally javascript programs are readily available in source code eg meaning that we can obtain a large set of high quality training programs main contributions the contributions of this paper are · a new approach for inferring likely program facts based on structured prediction with conditional random fields · a new framework consisting of fast and approximate inference and learning algorithms to structured prediction tasks in programs · a new system which is an instance of our approach focusing on names and type annotations of javascript programs a after its release was used by developers and has since become a popular tool in the javascript community · an evaluation on a range of realworld javascript programs the experimental results indicate that successfully correct names for of program identifiers and of the type annotations are correct paper structure the paper is organized as follows section illustrates our approach on an example section introduces the general prediction framework while section describes an instantiation of that framework section and section discuss our structured prediction and learning procedures section presents a detailed experimental evaluation of in section we elaborate on our design choices and explain why and how we at using structured prediction with finally in section we discuss related work in detail and conclude in section overview in this section we provide an informal description of our statistical inference approach on a running example consider the javascript program shown in fig a this is a program which has short identifier names such names can be produced by both a programmer or by an automated process known as a form of which replaces identifier names with names in the case of javascript is a common process on the web and is used to reduce the size of the code being over the network andor to prevent users from understanding what the program is actually doing in addition to names variables in this program also lack annotated type information the net effect is that it is difficult to understand what the program actually does which is that it partitions an input string into chunks of given sizes and stores those chunks into consecutive entries of an array given the program in fig a our system produced the program in fig e the output program has new identifier names and is annotated with types for the parameters the local variables and the return statement overall it is easier to understand what that program does when compared to the input program next we provide an overview of the prediction procedure we focus on names but the process for types is identical determine known and unknown properties given the program in fig a we first determine the set of program elements for which we would like to infer properties these are elements for which the properties to be inferred are currently unknown for example in the case of name inference this set of elements includes the local variables of the input program e t n r and i we also determine the set of elements whose properties are known one such element is the name of the field length in the input program or the names of the methods both kinds of elements are shown in fig b the goal of our prediction task is to predict the unknown properties based on i the obtained known properties and ii the relationship between various elements discussed below build dependency network next we build a dependency network capturing various kinds of relationships between program elements the dependency network is key to capturing structure when performing and intuitively captures how properties which are to be influence each other for example the link between known and unknown properties allows us to the fact that many programs use common eg common such as meaning that the unknown we aim to predict are by the way the known elements are used by the program further the link between two unknown properties that the prediction for the two properties is related in some way dependencies are of the form where n and m are program elements and rel is the particular relationship between the two elements in our work all dependencies are but in general they can be extended to other more complex relationships in fig c we show three example dependencies between the program elements for instance the statement i t generates a dependency because i and t are on the left and right side of a expression similarly the statement var r generates several dependencies including which that the left part of the relationship denoted by l appears before the dereference of the right side denoted by r we elaborate on the different types of relationships later in the paper for clarity in fig c we include only some of the relationships map inference after obtaining the dependency network of a program the next step is to infer the most likely values according to a probabilistic model learned from data for the nodes of the network a query referred to as map inference as illustrated in fig d for the network of fig c our system infers the new names step and len it also inferred that the previous name i was most likely let us consider how we the names step and len consider the network in fig d this is the same network as in fig c but with additional tables we elaborate on now these tables are produced as an output of the learning phase each table is a function that the assignment of properties for the nodes connected by the corresponding edge the function takes as input function t var n var r var i for i r i t if i t r i t else r return n a javascript program with identifier names str string step number return array function step var array var len var i i number for i len i step if i step len i step else len return e javascript program with new identifier names and types unknown properties variable names known properties constants length push t lr i lr r length lr b known and unknown name properties c dependency network lr score i step jj t ij step i i length lr score r l r score i len i length len length length len length d result of map inference figure a javascript program with new names and type annotations along with an overview of the name inference procedure two properties and returns the score for the pair intuitively how likely is the particular pair in fig d each table shows possible functions for the three kinds of relationships we have let us consider the table the first row says that the assignment of i and step is with the map inference tries to find an assignment of properties to the nodes so that the assignment a particular function for the two nodes i and t the inference ends up selecting the score from the table ie the values i and step similarly for the nodes i and r however for nodes r and length the inference does not select the row but selects values from the second row the reason is that if it had selected the row then the only choice in order to match the value length for the remaining relationship is the second row of that table with value however the assignment leads to a lower combined overall score that is the map inference must take into account the structure and dependencies between the nodes and cannot simply select the maximal score of each function and then stop output program finally after the new names are inferred our system transforms the original program to use these names the output of the entire inference process is captured in the program shown in fig e notice how in this output program the names tend to accurately capture what the program does type annotations even though we illustrated the inference process for variables names the overall flow for type annotations is identical first we define the program elements with unknown properties to infer type annotations for then we define elements with known properties such as api names or variables with known types next we build the dependency network some of the relationships overlap with those for names and finally we perform map inference and output a program annotated with the type annotations one can then run a standard type checker to check whether the types are valid for that program in our example program shown in fig e the type annotations are indeed valid in general when automatically trying to predict semantic properties such as types where soundness is required the approach presented here will have value as part of a loop a note on name inference we note that our name inference process is independent of what the names are in particular the process will return the same names regardless of which was used to the original program provided these always rename the same set of variables structured prediction for programs in this section we introduce our approach for program properties the key idea is to formulate the problem of inferring program properties as structured prediction with conditional random fields we first introduce then show how the is done in a manner and finally discuss the of inference and learning in the context of programs the prediction framework presented in this section is fairly general and can potentially be instantiated to many different kinds of challenges we instantiate it for two challenges in section notation programs labels let x x be a program as with standard program analysis we will infer properties about program statements or expressions referred to as program elements for a program x each element eg a variable is identified with an index a natural number we will usually need to separate the elements into two kinds i elements for which we are interested in inferring properties and ii elements for which we already know their properties eg these properties may have been obtained via standard program analysis or via manual annotation we use two functions n m x n to return the appropriate number of program elements for a given program x nx returns the total number of elements of the first kind and mx returns the total number of elements of the second kind for convenience we assume that elements of the first kind are indexed in the range nx and elements of the second kind are indexed in the range nx nx mx to avoid when x is clear from the context we write n instead of nx and m instead of mx we use the set to denote all possible values that a property can take for instance in type prediction contains all possible types eg number string etc then for a program x we use the notation y y to denote a vector of program properties here y y where y that is each entry yi in the vector y ranges over and denotes that program element i has a property yi problem definition let d xj yj tj denote the training data a set of t programs each with corresponding program properties our goal is to learn a model that captures the conditional probability p ry x once the model is learned we can predict properties of new programs by the following query also known as map or maximum a query given a new program x find y x p ry x that is for a new program x we aim to find the most likely assignment of program properties y according to the probabilistic distribution here x y describes the set of possible assignments of properties y for the program elements of x the set x is important as it allows restricting the set of possible properties and is useful for encoding constraints conditional random fields we now describe a particular model for representing the conditional probability p ry x we consider the case where the factors are positive in which case without loss of generality any conditional probability of properties y given a program x can be encoded as follows p ry x x z x where score is a function that returns a real number indicating the score of an assignment of properties y for a program x assignments with higher score are more likely than assignments with lower score zx called the partition function ensures that the above expression does in fact encode a conditional distribution it returns a real number depending only on the program x such that the probabilities over all possible assignments y sum to ie zx x yx we consider score functions that can be expressed as a composition of a sum of k feature functions fi associated with weights wi k x x wt f y x i here f is a vector of functions fi and w is a vector of weights wi the feature functions fi y × x r are used to score assignments of program properties this representation of score functions is particularly suited for learning as the weights w can be learned from data based on the definition above we can now define a conditional random field definition conditional random field a model for the conditional probability of labels y given observations x is called conditional random field if it is represented as p ry x f y x z x a note on feature functions feature functions are key to controlling the of an assignment of properties y for a program x for instance a feature function can be defined in a way which or the score of say if x the feature function fi with weight wi an assignment yb thus resulting in p x we discuss how the feature functions are defined in the next subsection note that feature functions are defined independently of the program being and are only based on the particular prediction problem we are interested in for example when we predict a programs types we define one set of feature functions and when we predict identifier names we define another set once defined the feature functions are reused for the particular kind of property we are interested in for any input program making for programs we next describe a step by step process for program properties using where program elements are related with pairwise functions we first show how to build a network between elements then describe how to build the feature functions fi based on that network and finally illustrate how to score a prediction step build dependence network gx the first step in defining x is to build what we refer to as a dependency network gx v x ex from the input program x this network captures dependencies between the made for the program elements of interest here v x denotes the set of program elements eg variables and consists of elements for which we would like to predict properties and elements whose properties we already know the set of edges ex v x × v x × denotes the fact that there is a relationship between two program elements and describes what that relationships is for a program x we define the vector zx zx to capture the set of properties that are already known that is each element in is assigned a property from zx here denotes the property of program element n i each ranges over a set of properties which could potentially differ from the properties that we use for inference for example if the known properties are integer constants will be all valid integers to avoid where x is clear from the context we use z instead of zx we use labels to denote the set of all properties step define feature functions once the network gx for a program x is obtained we use it to define the shape of the feature functions we define the assignment vector a y z which is a concatenation of two assignments the unknown properties y and the known properties z as usual we access the property of the jth element of the vector a via aj we define a feature function fi as the sum of the applications of its corresponding pairwise feature function i over the set of network edges obtained from step that is the formula below allows us to compute the value of a particular feature function for a given prediction y x i y za y rel x here each i labels × labels × r is a pairwise feature function relating a pair of program properties as opposed to a larger number like fi recall that each edge a b rel e x represents a pair of elements a and b and the kind of relationship rel between them then every time we an edge between two program elements we apply the pairwise feature function passing in the appropriate relationship rel as an argument conversely if two program elements are there is no need to invoke the pairwise feature function for these two elements focusing on pairwise feature functions allows us to define fi directly on the network obtained from the program note again that pairwise feature functions i are once and for all independently of the program for which we are properties we will see particular instantiations of pairwise feature functions in later sections unknown properties known properties y z yi y zi z z y y y y y z z prediction y x p ry x figure a general schema for building a network for a program x and finding the best assignment of program properties y although in this work we use pairwise feature functions there is nothing specific in our approach which us from using functions with higher arity step score a prediction y based on the above definition of a feature function we can now define how to obtain a total score for a prediction y by substitution we obtain k x za y rel x i that is for a program x and its dependence network gx by using the pairwise functions i and the learned weights wi associated with each i we can obtain the score of a prediction y example let us illustrate the above steps as well as some key points on the simple example in fig here we have program elements for which we would like to predict program properties we also have program elements whose properties we already know each program element is a node with an index shown outside the the edges indicate relationships between the nodes and the labels inside the nodes are the program properties or the already known properties as explained earlier the known properties z are fixed before the prediction process begins in a structured prediction problem the properties y y of program elements are such that p ry x is maximal key points let us note three important points first for a node eg from all other nodes in the network can be made independently of the made for the other nodes second nodes and are connected but only via nodes with known therefore the properties for nodes and can be assigned independently of one another that is the prediction y of node will not affect the prediction y of node with respect to the total score and vice versa the reason why this is the case is due to a property in known as conditional independence we say that the prediction for a pair of nodes a and b is independent given a set of nodes c if the for the nodes in c are fixed and all paths between a and b go through a node in c this is why the for nodes and are independent of node conditional independence is an important property of and is by both the inference and the learning algorithms we do not discuss conditional independence further but refer the reader to a standard reference finally a path between two nodes not involving known nodes means that the for these two nodes may and generally will be dependent on one another for example nodes and are connected without going through known nodes meaning that the prediction for node can influence the prediction for node and vice versa map inference recall that the key query we perform is map inference given a program x find y x p ry x in a this amounts to the query y x y x zx where zx x y x note that zx does not depend on y and as a result it does not affect the final choice for the prediction y this is an important observation because computing zx is generally very expensive as it may need to sum over all possible assignments y therefore we can zx from the formula next we take into account the fact that exp is a increasing function enabling us to remove exp from the equation this leads to an equivalent simplified query y x y x this means that an algorithm the map inference query must ultimately the score function for instance for the example in fig once we fix the labels zi we need to find labels yi such that score is in principle at this stage one can use any algorithm to answer the map inference query for instance a but inefficient way to solve this query is by trying all possible outcomes y x and each of them to select the one other exact and inference algorithms exist if the network gx and the outcomes set x have certain restrictions eg gx is a tree of programs unfortunately the problem with existing inference algorithms is that they are too slow to be usable for our problem domain ie programs for example in typical applications of it is to have more than a of possible assignments for an element eg while in our case there could potentially be of possible assignments per element towards that in section we present a fast and approximate map inference algorithm that is to the of dealing with programs the shape of the feature functions the unrestricted nature of gx and the set of possible assignments learning we briefly discuss how we learn the weights w that describe the function score to learn w we use an advanced learning technique that generalizes support vector machines given the training data d xj yj tj of t samples our goal is to find w such that the given assignments yj are the assignments in as many training samples as possible subject to additional learning constraints we discuss the learning procedure in detail in section names and type annotations for javascript in this section we present an example of using our structured prediction approach presented in section for inferring two kinds of properties i names of local variables and ii type annotations of function arguments we investigate the above challenges in the context of javascript a popular language where addressing the above two questions is of significant importance we do note however that much of the machinery discussed in this section applies almost to other languages presentation flow recall that in section we defined a process to obtaining a total score for a prediction where the first step is to define the network gx v x ex and the second step is to define the pairwise feature functions i the combination of these two fully defines the feature functions fi in what follows we first present the probabilistic name prediction and define v x for that problem we then present the probabilistic type prediction and define v x in that context then we define ex which program elements from v x are related as well as how they are related that is some of these relationships are similar for both prediction problems and hence we discuss them in the same section finally we discuss how to obtain the pairwise feature functions i probabilistic name prediction the goal of our name prediction task is to predict the most likely names of local variables in a given program x the way we proceed to solve this problem in our framework is as follows first as outlined in section we identify the set of known program elements referred to as as well as the set of unknown program elements for which we will be new names referred to as for the name prediction problem we take to be all constants objects properties methods and global variables of the program x each program element in can be assigned values from the set where is a set of all valid identifier names and is a set of possible constants we note that object property names and api names are modeled as constants as the dot operator takes an object on the lefthand size and a string constant on the righthand size we define the set to contain all local variables of a program x here a variable name belonging to two different leads to two program elements in finally ranges over to ensure the newly names are semantic preserving we ensure that the prediction satisfies the following constraints all references to a renamed local variable must be renamed to the same name the identifier names must not be keywords the prediction must not suggest the same name for two different variables in the same scope the first property is naturally enforced in the way we define where each element corresponds to a local variable as opposed to having a unique element for every variable occurrence in the program the second property is enforced by making sure the set from which names are drawn does not contain keywords finally we enforce the third constraint by restricting x so that with conflicting names are probabilistic type annotation prediction our second application involves probabilistic type annotation inference of function parameters focusing on function parameters is particularly important for javascript a language type annotations without knowing the types of function parameters a forward type inference analyzer will fail to derive precise and meaningful types except the types of constants and those returned by common such as dom as a result realworld programs using libraries cannot be analyzed precisely instead we propose to predict the type annotations of function parameters here our training data consists of a set of javascript programs that have already been annotated with types for function parameters in javascript these annotations are provided in a comments known as any type string number boolean function other objects array eg element event etc no type figure the lattice of types over which prediction occurs the simplified language over which we predict type annotations is defined as follows expr val var expr expr expression val var expr n value here n ranges over constants n var is a meta variable ranging over the program variables ranges over the standard binary operators etc and ranges over all possible variable types that is l where l is a set of types we discuss how to instantiate l below and denotes the unknown type to be explicit we use the set where we use the function x expr to obtain the type of a given expression in a given program x this map can be manually provided or built using program analysis when the program x is clear from the context we use e as a for xe defining known and unknown program elements as usual our first step is to define the two sets of known and unknown elements we define the set of unknown program elements as follows e e is var e that is contains variables whose type is unknown we between the type and the unknown type in order to allow for control over which types we would like to predict for instance a type may be if a classic type inference algorithm fails to infer more precise types usually standard inference only types of constants and values returned by common but fails to infer types of function parameters a type may be denoted as unknown ie if the type inference did not even attempt to infer types for the particular expression eg function parameters of course in the above definition of we could also include and use our approach to potentially refine the results of classic type inference next we define the set of known elements note that can contain any expression not just variables like above e e is expr e n n is constant that is contains both expressions whose types are known as well as constants currently we do not apply any global restriction on the set of possible assignments x that is x recall that n is a function which returns the number of elements whose property is to be this means that we rely entirely on the learning to discover the rules that will produce types the only restriction discussed below that we apply is when performing defining so far we have not discussed the exact contents of the set except to state that l where l is a set of types the set l can be instantiated k ij lr ij lr lr k i lr j lr lr k lr ij a b c figure a the ast of expression and two dependency networks built from the ast relations b for name and c for type in various ways in this work we chose to define l as l pt where t is a complete lattice of types with t and as defined in fig in the figure we use to denote a potentially infinite number of userdefined object types key points we note several important points here first the set is built during training from a finite set of possible types that are already manually provided or are inferred by the classic type inference therefore for a given training data is necessarily a finite set second because may contain a subset of types o specific to a particular program in the training data it may be the case that when we are considering a new program whose types are to be the types found in o are simply not relevant to that new program for instance the types in o refer to names that do not appear in the new program therefore when we perform prediction we filter irrelevant types from the set this is the only restriction we consider when performing type finally because l is defined as a powerset lattice it encodes in this case a finite number of that is a variable whose type is to be ranges over exponentially many subsets allowing many choices for the type for example a variable can have a type string number which for convenience can also be written as string number relating program elements we next describe the relationships that we introduce between program elements these relationships define how to build the set of edges ex of a program x since the program elements for both prediction tasks are similar eg they both contain javascript constants variables and expressions we discuss the relationships we use for each task together if a relationship is specific to a particular task we explicitly state so when describing it relating expressions the first relationship we discuss is syntactic in nature it relates two program elements based on the their syntactic relationship in the programs abstract syntax tree ast let us consider how we obtain the relationships for the expression first we build the ast of the expression shown in fig a suppose we are interested in performing name prediction for variables i j and k denoted by program properties with indices and respectively that is then we build the dependency network as shown in fig b to indicate that the prediction for the three elements are dependent on one another with the particular relationship shown over the edge for example the edge between and represents the relationships that these nodes in an expression lr where l is a node for and r is a node for the relationships are defined using the following grammar l r all relationships are part of that is here as discussed earlier ranges over binary operators all relationships derived using the above grammar have exactly one occurrence of l and r for a relationship r let e denote the expression where x is substituted for l y is substituted for r and the expression e is substituted for then given two program elements a and b and a relationship r a match is said to exist if br expr here expr denotes all possible expressions in the programming language and is all expressions of program x an edge a b r ex between two program elements a and b exists if there exists a match between a b and r note that for a given pair of elements a and b there could be more than one relationship which matches that is both r r match where r r therefore there could be multiple edges between a and b with different relationships the relationships described above are useful for both name and type inference in the case of names the expressions being related are always variables while for type annotations the expressions need not be restricted to variables for example in fig c there is a relationship between the types of k and ij via lr note that our rules do not directly capture relationships between i and ij but they are dependent still many useful and interesting direct relationships for type inference are present for instance in classic type inference the relationship lr implies a constraint rule l r where is the supertype relationship indicated in fig interestingly our inference model can learn such rules instead of providing them explicitly aliasing relations another kind of semantic relationship we introduce is that of aliasing let denote the set of expressions that may alias with the expression e this information can be determined via standard alias analysis we introduce the relationship which relates arguments of a function invocation the arguments can be arbitrary expressions with parameters in the function declaration variables whose names or types are to be inferred let ee be an invocation of the function captured by the expression e then for all possible declarations of e those are an overapproximation we relate the argument of the call e to the parameter in the declaration that is for any v p p e we add the edge e v to ex in the case of names e is always a variable while with types e is not restricted to variables transitive aliasing second we introduce a transitive aliasing relationship referred to as r alias between variables which may alias this is a relationship that we introduce only when types let a and b be related via the relationship r where r ranges over the grammar defined earlier then for all c where c is a variable we include the edge a c r alias function name relationships we also introduce two relationships referred to as and these relationships are only used when names and are particularly useful for function names the reason is that in javascript many of the local variables are function declarations the relationship relates a function name f with names of other functions g that f may call this semantic information can be obtained via program sis that is if a function f may call function g we add the edge f g to the set of edges ex similarly if in a func tion f there is an access to an object field named f ld we add the edge f f ld to the set ex naturally f and g are allowed to only range over variables as when names the nodes represent variables and not arbitrary expressions and the name of an object field f ld is a string constant pairwise feature functions finally we describe how to obtain and define the pairwise feature functions we obtain these functions as a preprocessing step before the training phase begins recall that our training set d xj yj consists of t programs where for each program x we are given the corresponding properties y for each tuple x y d we define the set of features as follows f y y za y rel a b rel e x then for the entire training set we obtain all features as follows t f yj j we then define the pairwise feature functions to be functions of each feature triple li li rel i all il l rel if l li and l li and rel rel i otherwise in addition to functions we have features for equality of program properties l l rel that return if and only if the two related labels are equal our feature functions are fully inferred from the available training data and the network gx of each pro gram x after the feature functions are defined in the training phase discussed later we learn their corresponding weights k is the number of pairwise functions note that the weights and the feature functions can vary depending on the training data d but both are independent of the program for which we are trying to predict properties based algorithms in their results they show that advanced approximate algorithms may result in higher precision for the inference and the learning however they also come at the cost of significantly more computation their experiments that more advanced techniques such as propagation are consistently at least an order of magnitude slower than greedy algorithms as our focus is on performance we with a greedy approach also known as iterated conditional modes our algorithm is to the nature of our prediction task especially when names where we have a number of possible assignments for each element in order to significantly improve the computational complexity over a greedy approach in particular our algorithm the shape of the feature functions discussed in section in essence the approach works by selecting candidate assignments from a of possible labels leading to significant in performance at the of slightly higher of obtaining assignments algorithm greedy inference algorithm input network gx v x ex of program x initial assignment of n unknown properties y x known properties z pairwise feature functions i and their learned weights wi output y x x begin y y for pass do for each node with unknown property in the graph gx for v n do ev v ex v ex ev y z for l candidates v y z ev do l get current label of v l change label of v in y ev y z if y x then else l no score improvement label prediction algorithm in this section we present our inference algorithm for making also referred to as map inference recall that properties y of a program x involves finding a y such that y p ry x x wt f y x y x y x y x when designing our inference algorithm a key objective was optimizing the speed of prediction there are two reasons why speed is critical first we expect prediction to be done as part of a program development environment or as a service eg via a public web site such as this requirement any inference algorithm that takes more than a few seconds second as we will see later the prediction algorithm is part of the innermost loop of training and hence its performance directly an already costly and training phase exact algorithms exact inference in is generally nphard and computationally in practice this problem is well known and hard specifically for networks with no shape like the ones we obtain from programs approximate algorithms previous studies for map inference in networks of arbitrary shapes discuss propagation greedy algorithms combination approaches or return y greedy inference algorithm algorithm illustrates our greedy inference procedure the inference algorithm has four inputs i a network gx obtained from a program x ii an initial assignment of properties for the unknown elements y iii the obtained known properties z and iv the pairwise feature functions and their weights the way these inputs are obtained was already described earlier in section the output of the algorithm is an approximate prediction y which also to the desired constraints x the algorithm also uses an auxiliary function called defined as follows k a ab rel i the a function is the same as score defined earlier except that works on a subset of the network edges e ex given a set of edges e and an assignment of elements to properties a iterates over e applies the appropriate feature function to each edge and sums up the results the basic idea of the algorithm is to start with an initial assignment y line and to make a number of passes over all nodes in the network to improve the score of the current prediction y the algorithm works on a node by node basis it selects a node v n and then finds a label for that node which improves the score of the assignment that is once the node v is selected the algorithm first obtains the set of edges ev in which v shown on line and computes via the contribution of the edges to the total score then the inner loop starting at line tries new labels for the element v from a set of candidate labels and accepts only labels that lead to a score improvement time complexity the time complexity for one iteration of the prediction algorithm depends on the number of nodes the number of adjacent edges for each node and the number of candidate labels since the total number of edges in the graph ex is a product of the number of nodes and the number of edges per node then one iteration of the algorithm has time complexity where d is the total number of possible candidate assignment labels for a node obtained on line obtaining candidates our algorithm does not try all possible labels for a node instead we define the function a e which suggests candidate labels given a node v assignment a and a set of edges e recall that is a large set of triples l l r obtained from the training data d relating labels l and l via r further our pairwise feature functions where k defined earlier are functions meaning there is a one to one correspondence between a triple l l r and a pairwise function recall that in the training phase discussed later we learn a weight wi associated with each function i and because of the mapping with each triple l l r we use these weights in order to restrict the set of possible assignments we consider for a node v let be a function which given a set of features triples returns the top s triples based on the respective weights let for convenience f then we define the following auxiliary functions rel t tl rel t f rel t tr rel t f the above functions can be easily for a fixed size s and all triples in the training data f finally we define a e l l l r rel e l l l r rel e the meaning of the above function is that for every edge adjacent to v we consider at most s of the triples according to the learned weights this results in a set of possible assignments for v used to the inference algorithm the parameter s a tradeoff between precision and running time lower values of s decrease the of a good candidate label while higher s make the algorithm consider more labels and run longer our experiments show that good candidate labels can be obtained with fairly low values of s thanks to this observation the prediction runs orders of magnitude faster than a greedy algorithms that tries all possible labels monotonicity at each pass of our algorithm we iterate over the nodes of gx and update the label of each node only if this leads to a score improvement at line since we always increase the score of the assignment y after a certain number of iterations we reach a fixed point assignment y that can no longer be improved by the algorithm the local however is not guaranteed to be a global since we cannot give a complete optimality guarantee to achieve further speed we also cap the number of algorithm passes at a constant additional improvements to further decrease the computation time and possibly increase the precision of our algorithm we made two improvements first if a node has more than a certain number of adjacent nodes we decrease the size of the s in our implementation we decrease the size by a factor of if a node has more than adjacent nodes at almost no computation cost we also perform optimizations on pairs of nodes in addition to individual nodes in this case for each edge in gx we use the s best features on the same type of edge in the training set and attempt to set the labels of the two elements connected by the edge to the values in each triple learning in this section we discuss the learning procedure we use for obtaining the weights w of the model p ry x from a data set we assume that there is some underlying joint distribution p y x from which the data set d xj yj of t programs is drawn independently and distributed in addition to programs xj we assume a given assignment of labels yj names or type annotations in our case is provided as well we perform training ie estimate p ry x directly rather than generative training ie estimate p ry x and deriving p ry x from this joint model since latter requires a distribution over programs x ­ a challenging and for our purposes unnecessary task the goal of learning is to then estimate the parameters w to achieve generalization we wish that for a new program x drawn from the same distribution p ­ but generally not contained in the data set d ­ its properties y are accurately using the prediction algorithm from section several approaches to this task exist and can potentially be used one approach is to fit parameters in order to the conditional of the data that is try to choose weights such that the model p ry x accurately the true conditional distribution p y x associated with the distribution p unfortunately this task requires computation of the partition function zx which is a task instead we perform what is known as training we learn weights w such that the training data is classified correctly subject to additional constraints like and for this task powerful learning algorithms are available in particular we use a variant of the structured support vector machine and we it efficiently with the scalable descent algorithm proposed in structured support vector machine the goal of learning is to find w such that for each training sample xj yj j t the assignment found by the classifier ie the score is equal to the given assignment yj and there is a between the correct classification and any other classification j y xj xj xj yj y here labels × labels r is a distance function nonnegative and satisfying inequality one can interpret yj y as a safety between the given assignment yj and any other assignment y wrt the score function is chosen such that slight eg incorrect prediction of few properties require less than major for our structured support vector machines generalize classical support vector machines to predict many labels at once as necessary when analyzing programs we took to return the number of different labels between the reference assignment yj and any other assignment y generally it may not be possible to find weights achieving the above constraints hence attempt to find weights w that minimize the violation of the ie the of fit to the data at the same time control model complexity via the use of large weights this is done to facilitate better generalization to test programs learning with stochastic descent achieving the balance of and model complexity leads to a natural optimization problem t w w xj yj st w w w j where w max wt f y yj y y xj is called the structured loss this nonnegative loss function measures the violation of the constraints caused for the jth program when using a particular set of weights w thus if the objective reaches zero all constraints are ie accurate labels are returned for all training programs furthermore the set w encodes some constraints on the weights in order to control model complexity and avoid in our work we by requiring all weights to be nonnegative and bounded by hence we set w w wi for all i the optimization problem is convex since the structured loss is a pointwise maximum of linear functions and w is convex the use of descent optimization in particular we use a technique called stochastic descent which is known to converge to an optimal solution while being extremely scalable for structured prediction problems the algorithm proceeds iteratively in each iteration it a random program with index j t from d computes the wrt w of the loss function w xj yj and takes a step in the negative direction if it ends up outside the feasible region w it projects w to the feasible point in order to compute the g w w xj yj at w wrt the jth program we must solve the problem xj yj y y xj resulting in the g g f xj f yj xj hence computing the requires solving the inference problem this problem can be approximately solved using the algorithm presented in section after the computation the weights w used by score are updated as follows w w g where is a learning rate constant and is a projection operation determined by the described below the function rk rk projects its arguments to the point in w that is in terms of distance this operation is used to place restrictions on the weights w rk such as and the goal of this procedure known as is to avoid a problem known as ­ a case where w is good in training data but fails to generalize to data in our case we perform inf which can be efficiently done in closed form as follows w w such that wi max min wi this projection ensures that the learned weights are nonnegative and never a value limiting the ability to learn too strong feature functions that may not generalize to data our choice of inf has the additional benefit that it operates on vector components independently this allows for efficient implementation of the learning where we only vector components that changed or components for which the g is nonzero this enables us to use a sparse representation of the vectors g avoiding iteration over all components of the vector w when complete training phase in summary our training procedure first iterates once over the training data and extracts features we initialize each weight with wi then we start with a learning rate of and iterate in multiple passes to learn their weights with stochastic descent in each pass we compute via inference and apply as described before additionally we count the number of wrong labels in the pass and compare it to the number of wrong labels in the previous pass if we do not observe improvement we decrease the learning rate by one half in our implementation we iterate over the data up to times to speed up the training phase we also the stochastic descent on multiple threads as described in at each pass we randomly split the data to threads where each thread ti updates its own version of the weights at the end of each pass the weights are to obtain the final weights w implementation and evaluation we implemented our approach in an production quality interactive tool called which targets name and type annotation prediction for javascript is integrated within the closure compiler a tool which takes javascript with optional type annotations and it it then returns an optimized and javascript with annotations to implement our system we added a new mode to the compiler that aims to reverse its operation given an optimized javascript code generates javascript code that is well annotated with types and as as possible with useful identifier names our two applications for names and types were implemented as two models that can be run separately impact on developers a after was made available it was used by more than developers with the of feedback left in and being very positive those can be found by a simple web search we believe the combination of high speed and high precision achieved by the structured prediction approach were the main reasons for this positive experimental evaluation we next present a detailed experimental evaluation of our statistical approach and demonstrate that the approach can be successfully applied to the two prediction tasks we described further we evaluate how various of our system affect the overall performance and precision of the we collected two disjoint sets of javascript programs to form our training and evaluation data for training we javascript projects from for evaluation we took the javascript projects with the number of commits system all training data of training data of training data all data no structure no names accuracy types precision types recall table precision and recall for name and type reconstruction of javascript programs evaluated on our test set from by taking projects from different we decrease the of overlap between training and evaluation data we also in to check that the projects in the evaluation data are not included in the training data finally we implemented a simple checker to detect and filter out and files from the training and the evaluation data after files we up with training data consisting of files and evaluation data of files next we discuss how we and evaluated our system first we discuss parameter selection section then precision section and model sizes section and finally the running times section parameter selection we used fold to select the best learning parameters of the system only based on the training data and not by any test set works by splitting the training data into equal pieces called folds and evaluating the error rate on each fold by training a model on the data in the other folds then we and evaluated on a set of different training parameters and selected the parameters with the lowest error rate we the values of two parameters that affect the learning constant and presence of higher values of mean that we more ie add more restrictions on the feature weights by limiting their maximal value to a lower value the parameter determines if the function see section should return zero or the number of different labels between the two assignments to reduce computation since we must and test a large number of parameters we performed on only sample of the training data the procedure determined that the best value for is for names for types and should be applied to both tasks precision after choosing the parameters we evaluated the precision of our system for names and type annotations our experiments were performed by the names and types in isolation on each of the testing files to evaluate precision we first all files with the process local variable identifiers to short names and removes and type annotations each program is semantically equivalent except when using with or eval to the original program then we used to name and type information we compared the precision of the following configurations · the most powerful system works with all of the training data and performs structured prediction as described so far · two systems using a fraction of the training data ­ one on and one on of the files · to evaluate the effect of structure when making we relationships between unknown properties and performed on that network the learning phase still uses structure · a which does no prediction it keeps names the same and sets all types to the most common type string name to evaluate the accuracy of name we took each of the programs and used the name inference in to rename its local variables then we compared the new names to the original names before for each of the tested programs the results for the name reconstruction are summarized in the second column of table overall our best system produces code with of identifier names exactly equal to their original names the systems on less data have significantly lower precision showing the importance of the amount of training data not using structured prediction also the accuracy significantly and has about the same effect as an order of magnitude less data finally not changing any identifier names produces accuracy of ­ this is because the code may not rename some variables eg global variables in order to guarantee semantic preserving transformations and local variable names the same eg induction variable of a loop type annotation out of the test programs have type annotations for functions in a for these we took the version with no type annotations and tried to all types in the function signatures we first ran the closure compiler type inference which produces no types for the function parameters then we ran and evaluated on inferring these function parameter types does not always produce a type for each function parameter for example if a function has an empty body or a parameter is not used we often cannot relate the parameter to any known program properties and as a result we make no prediction and return the unknown type to take this effect into account we present two metrics for types recall and precision recall is the percentage of function parameters in the evaluation for which made a prediction other than precision refers to the percentage of cases ­ among the ones for which made a prediction ­ where it was exactly equal to the manually provided annotation of the test programs we note that the manual annotations are not always correct and as a result precision is not necessarily a desired outcome we present our evaluation results for types in the last two columns of table since we evaluate on production javascript applications that typically have short methods with complex relationships the recall for program types is only for our best system however we note that none of the types we infer can be inferred by regular forward type analysis since the total number of commonly used types is not as high as the number of names the amount of training data has less impact on the system precision and recall to increase the precision and recall of type prediction adding more semantic relationships between program elements will be of higher importance than adding more training data structure increases the precision of the types slightly but at the cost of a significantly reduced recall the reason is that some types are related to known properties only via other types ­ relationships that approaches cannot capture on the other end of the is a prediction system that suggests the most likely type input programs typecheck with type error programs programs fixed programs programs output programs typecheck with type error figure evaluation results for the number of typechecking programs with manually provided types and with types in javascript programs ­ string such a system produces a type for every variable recall but its precision is only of type annotations to see if the type annotations are useful we compared them to the original types provided in the evaluated programs first we note that our evaluation data has type annotations for function parameters in programs after removing these annotations and them with the number of annotations that are not increased to for the same programs the reason produces more types than originally present despite having only recall is that not all functions in the original programs had manually provided type annotations despite annotating more functions than in the original code the output of has fewer type errors we summarize these in fig for each of the programs we ran the typechecking pass of closure compiler to discover type errors among others this pass checks for incompatible types calling into a conflicting and missing types and properties on objects for our evaluation we kept all checks except the property check which fails on almost all even valid programs because it depends on annotating all properties of types ­ annotations that almost no program when we ran typechecking on the input programs we found the to have typechecking errors while surprising this can be explained by the fact that javascript developers typically do not typecheck their annotations among others we found the original code to have type names most typecheck errors occur due to missing or conflicting types in a number of cases the types provided were interesting for documentation but were semantically wrong eg a parameter is a string that denotes function name but the manual annotation its type to be function in contrast the types by make the of the programs typecheck in of the programs that originally did not typecheck was able to infer correct types on the other hand introduced type errors in programs we investigated some of these errors and found that not all of them were due to wrong types ­ in several cases the types were rejected due to of the type system model sizes our models contain features for names and features for types each feature is stored as a triple along with its weight as a result we need only bytes per feature resulting in a mb model for names and mb model for types the dictionary which stores all names and types requires mb as we do not data our model the memory requirements for query processing are about as much as the model size running times we performed our performance evaluation on a core machine with four processors and running with bit java the training phase for name parameter b greedy no name prediction accuracy time ms ms ms ms ms ms ms s type prediction precision time ms ms ms ms ms ms ms ms table tradeoff between precision and runtime for the name and type depending on search parameter s prediction took around to compile the input code and generate networks for the input programs and per sub descent optimization pass similarly for types the compilation and network construction phase took and then we needed and seconds per descent optimization pass for all our training we ran descent passes on the training data all the training passes used threads to the of our machine running times of prediction we evaluated the effect of changing the size s of our map inference algorithm from section and the effect s has on the prediction time the average prediction times per program are summarized in table each query is performed on a single core of our test machine as expected increasing s improves prediction accuracy but requires more time removing the and running greedy iterated conditional modes leads to running times of around two per program for name prediction for an interactive tool such as also its precision some of the systems because it does not perform optimization per pair of nodes but only a node at a time due to the requirements for high performance in our main evaluation and for our live server we chose the value s this value provides a good balance between performance and precision suitable for our live system evaluation data metrics our evaluation data consists of lines of javascript code with the largest file being lines for each of the evaluated files the constructed for name prediction has on average arcs and random variables for the type prediction evaluation tasks each has on average arcs and random variables design decisions the problem of effectively learning from existing code and precisely interesting questions on new programs is nontrivial and requires careful between programs and sophisticated probabilistic models as the overall machine can be fairly complex here we state the important design choices that we made in order to arrive at the solution presented in the paper from programs to random variables when program properties one needs to account for both the program elements whose properties are to be and the set of available properties from which we draw in the elements are local variables for name prediction and function parameters for type prediction in general however one could instantiate our approach with any program element that ranges over program properties for which sufficient amount of training data is available we note that for our instantiation a program property always exists in the training data in the case of names large amounts of training data still allow us to predict meaningful and useful names because of the assumption that the property exists in the training data we create one random variable per local variable of a program with the name to predict and the feature functions as described in section however the framework from section can be instantiated with different feature functions to cases where a variable name does not exist in the training data eg is a concatenation of several existing words the need for structure when facts and properties of programs it is important to observe that these properties are usually dependent on one another this means that any of these properties should be done and not independently in isolation graphical models over directed a family of probabilistic models able to capture complex structural dependencies are graphical models such as bayesian networks directed models and markov networks models in graphical models nodes represent random variables and edges capture dependencies between these random variables therefore graphical models are a natural fit for our problem domain where program elements are random variables and the edges capture a particular dependency between the properties to be inferred for these program elements while we do need to capture dependence between two or more random variables it is often not possible to decide a priori on the exact order direction of the edges of that dependence in turn this means that models are a better match for our setting as they do not require specifying a direction of the dependence inference map inference over for a given program x and a probabilistic model p we are interested in finding the most likely properties which should be assigned to program elements given the known fixed values for the elements what is the right query for computing this most likely assignment should we try to find the value v of each random variable ri which its probability p ri v separately or should we try to find the values v v vn for all random variables r rn together so that the joint probability is that is p r v r v rn vn called map inference we decided to use map inference over for several reasons first we ultimately aim to find the best joint assignment and not make independent potentially second it is easy to show that the probability of each variable separately does not lead to the assignment computed by map inference third when computing it is difficult to enforce deterministic constraints such as a b it is often easier to incorporate such constraints with map inference finally and as we discuss below the decision to perform map inference over a substantial benefit when it comes to training the corresponding probabilistic model an interesting point is that standard map inference algorithms are computationally expensive when the number of possible properties is large hence we had to develop new algorithmic variants which can effectively deal with of possible properties for a given random variable eg many possible names training a markov network over generative an important question when dealing with probabilistic models is deciding how the model should be one approach is to an model in a generative way thus obtaining a joint probability distribution over both and this model is sometimes referred to as markov random field while possible in theory this has a serious practical it requires placing prior probabilities on the known variables providing such a prior distribution can however be very difficult in practice however recall that our map inference query is in fact conditional on an existing assignment of the known elements this means that the underlying probabilistic model need only capture the conditional probability distribution of given and not the joint distribution an graphical model able to capture such conditional distributions is referred to as a conditional random field the model admits training where on are no longer necessary a key reason for why this model is so effective and popular in practice training a over maximum after deciding to use we still need to find a scalable method for training and obtaining such models from available data eg big code in our setting training these models say via maximum is possible but can be computationally expensive see ch recall that we are mainly interested in map inference queries where we do not need the exact probability value for the assignment of because of that we are now able to recent advances in scalable training methods for and in particular training a method towards map inference queries on the model summary in summary based on the above reasoning we arrive at using map inference queries with conditional random fields a probabilistic graphical model which we learn from the available data via an efficient training we do note however that the translation of programs to networks and the feature functions we provide are directly if one is interested in performing queries and the uncertainty associated with the solutions clustering vs probabilistic models it is to understand that our approach is not based on clustering given a program x we do not try to find a similar for some definition of similarity program s in the training and then extract useful information from s and integrate that information into x such an approach would be limiting as often there is not even a single program in the training which contains all of the information we need to predict for program x in contrast with the approach presented here it is possible to build a probabilistic model from multiple programs and then use that information to predict properties about a single program x related work we next discuss some of the work most closely related to ours probabilistic models for code recently there has been interest in creating probabilistic models of programs based on large in contrast to such generative models in this work we model a conditional distribution over likely properties for any given program such approaches are often more scalable in terms of their data requirements and computational efficiency and enable us to capture complex relationships between program properties while there are other probabilistic models they typically serve different purposes as we use here are particularly well suited for making multiple dependent for complex structured data like programs graphical models in programming several works have used graphical models in the context of programs all of these works phrase the prediction problem as computing as we already discussed in section we find map inference to be the conceptually problem formulation over except for none of these works learn the probabilistic models from data if one is to learning in their context then this would essentially require new features which keep information common among programs need some form of further because they frame the problem as computing probabilities these approaches do not allow for learning with loss functions meaning that one has to model probabilities and compute the partition function at learning time this would be expensive for large such as ours and does not allow for advanced methods for map inference based structured prediction where one need not compute the partition function indeed the learning method of is extremely inefficient and from the need to compute wrt to the model which is generally very expensive and requires sampling a procedure that is a lot less scalable than map inference in terms of particular applications aims to infer ownership values for pointers which range over a very small domain of values ro co and their negation in contrast we handle efficiently variables with of possible assignments for names and even for types the size of our domain is much greater further their selection of feature functions make the inference process extremely slow the reason is that one of the features called the check factor critical to the precision of their approach requires running program analysis essentially on every iteration of the inference this is in an interactive setting even with optimizations indeed their work section that the authors would like to find a better solution to this problem unfortunately this feature is also practically infeasible due to the large numbers of possible combinations of similarly focuses on inferring likely tags for string methods eg source sink regular sanitizer where values range over a small domain the basic idea is to convert a graph extracted from a program called the propagation graph into a factor graph and to then perform inference on that graph this work does a little more than computing it selects the best and conditions on them for of the rest meaning that it can potentially encode constraints by on what is already computed this approach is computationally very inefficient it essentially requires running inference n times for n variables instead of once further the approach cannot compute optimal map inference the paper is similar to in the sense that it infers permission annotations again ranging over a small set of values and both build factor graphs from programs however the inference algorithm in their paper just computes directly and is simpler than the one in as it does not even iterate overall we believe that the problems these works address may benefit from considering map inference instead of computing also it seems that even with computing these works are not using state of the art inference methods eg methods instead of the basic propagation which has no guarantees on convergence conclusion we presented a new statistical approach for program properties by learning from big code the core idea is to formulate the problem of property inference as structured prediction with conditional random fields enabling joint of program properties as an example of our approach we built a system called which is able to predict names and type annotations for javascript is practically effective it was used by more than developers a after its release and has now become a popular tool in the javascript community we believe the structured prediction approach presented in this work can serve as a basis for exploring approximate solutions to a variety of analysis and synthesis tasks in the context of big code references m and c source code at scale using language modeling in d a b and x statistical debugging using latent topic models in n e and a v probabilistic modular and scalable inference of typestate specifications pldi pp ­ j on the statistical analysis of dirty journal of the statistical society series b ­ d and j topic models in text classification clustering and applications closure compiler big code to improve software reliability and construction t and t training structural when exact inference is in pp ­ gulwani s and n program verification as probabilistic inference popl acm pp ­ he x r s and m a conditional random fields for image labeling s h a and p type analysis for javascript in sas j h et al a study of modern inference techniques for discrete energy minimization problems s v and m statistical translation of programming languages d and friedman n probabilistic graphical models principles and techniques mit press t ng a y and d a factor graph model for software bug finding t p back g ng a and d from uncertainty to inferring the specification within usenix association pp ­ j d a and f c n conditional random fields probabilistic models for and labeling sequence data pp ­ b a v rajamani s k and a specification inference for explicit information flow problems pldi acm pp ­ c j and d structured generative models of natural source code k p machine learning a probabilistic perspective cambridge ma d a x and w b table extraction using conditional random fields pp ­ a m and t conditional random fields for object recognition in pp ­ n d j a and m approximate methods for structured prediction in pp ­ v m and e code completion with statistical language models pldi acm pp ­ b pointsto analysis in almost linear time popl pp ­ b c and d markov networks in i t hofmann t and y large methods for structured and output variables journal of machine learning research ­ typescript language m m li l and a j stochastic descent in pp ­ 