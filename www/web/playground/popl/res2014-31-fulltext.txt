optimal dynamic partial order reduction department of information technology university abstract model checking is a powerful technique for program verification which however from an exponential growth in the number of explored executions a successful technique for reducing this number while still maintaining complete coverage is dynamic partial order reduction we present a new algorithm which is the first to be provably optimal in that it always the minimal number of executions it is based on a novel class of sets called source sets which replace the role of persistent sets in previous algorithms first we show how to modify an existing algorithm to work with source sets resulting in an efficient and simple to implement algorithm second we extend this algorithm with a novel mechanism called trees that allows to achieve optimality we have implemented both algorithms in a model checking tool for programs experiments show that source sets significantly increase the performance and that trees only a small overhead in both time and space categories and subject descriptors d software engineering verification d software engineering testing and debugging f logics and meanings of programs specifying and verifying and reasoning about programs general terms algorithms verification reliability keywords dynamic partial reduction software model checking systematic testing concurrency source sets trees introduction verification and testing of concurrent programs is difficult since one must consider all the different ways in which can interact model checking addresses this problem by systematically exploring the state space of a given program and verifying that each reachable state satisfies a given property applying model checking to realistic programs is problematic however since it requires to capture and store a large number of global states model checking avoids this problem by exploring the state space of the program without explicitly storing global states a special runtime scheduler the program execution making decisions on scheduling whenever such decisions may affect the interaction between processes model checking has been successfully implemented in tools such as and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than the authors must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ san diego ca usa copyright is held by the publication to acm acm while model checking is applicable to realistic programs it from combinatorial explosion as the number of possible interleavings grows exponentially with the length of program execution there are several approaches that limit the number of explored interleavings such as and context bounding among them partial order reduction por stands out as it provides full coverage of all behaviours that can occur in any interleaving even though it only a representative subset por is based on the observation that two interleavings can be regarded as equivalent if one can be obtained from the other by adjacent independent execution steps in each such equivalence class called a trace por at least one interleaving this is sufficient for checking most interesting safety properties including race freedom absence of global deadlocks and absence of assertion violations existing por approaches are essentially based on two techniques both of which reduce the set of process steps that are explored at each scheduling point · the persistent set technique that only a provably sufficient subset of the enabled processes this set is called a persistent set variations are sets and sets · the sleep set technique that maintains information about the past exploration in a socalled sleep set which contains processes whose exploration would be provably redundant these two techniques are independent and complementary and can be combined to obtain increased reduction the construction of persistent sets is based on information about possible future conflicts between threads early approaches analyzed such conflicts statically leading to and therefore limiting the reduction dynamic partial order reduction improves the precision by recording actually occurring conflicts during the exploration and using this information to construct persistent sets by need guarantees the exploration of at least one interleaving in each trace when the explored state space is acyclic and finite this is the case in model checking in which only executions of bounded length are analyzed challenge since is suited as a reduction technique several variants improvements and for different computation models have appeared the obtained reduction can however vary significantly depending on several factors eg the order in which processes are explored at each point of scheduling for a particular implementation of with sleep sets up to an order of magnitude of difference in the number of explored interleavings has been observed when different strategies are used for specific communication models specialized algorithms can achieve better reduction heuristics for choosing which next process to explore have also been investigated without results let us explain one fundamental reason for the above variation in obtained reduction in the combination of persistent set and sleep set techniques guarantees to explore at least one complete interleaving in each trace moreover it has already been proven that the use of sleep sets is sufficient to prevent the complete exploration of two different but equivalent interleavings at first this seems to imply that sleep sets can give optimal reduction what it actually implies however is that when the algorithm tries an interleaving which is equivalent to an already explored one the exploration will begin but it will be blocked or later by the sleep sets in what we call a blocked exploration when only sleep sets are used for reduction the exploration effort will include an arbitrary number of blocked it is here where persistent sets enter the picture and limit the number of computation of smaller persistent sets leads to fewer blocked however as we will show in this paper persistent sets are not powerful enough to completely prevent blocked exploration in view of these variations a fundamental challenge is to develop an optimal algorithm that i always the minimum number of interleavings regardless of scheduling decisions ii can be efficiently implemented and iii is applicable to a variety of computation models including communication via shared variables and message passing contributions in this paper we present a new algorithm called which is provably optimal in that it always exactly one interleaving per trace and never any sleep exploration our optimal algorithm is based on a new theoretical foundation for partial order reduction in which persistent sets are replaced by a novel class of sets called source sets source sets are often smaller than persistent sets and are provably minimal in the sense that the set of explored processes from some scheduling point must be a source set in order to guarantee exploration of all traces when a minimal persistent set contains more elements than the corresponding source set the additional elements will always blocked we will use a approach to describe our optimal algorithm in the first step we develop a simpler algorithm called which is based on source sets it is derived by modifying the classical algorithm by flanagan and so that persistent sets are replaced by source sets the power of source sets can already be observed in the algorithm it achieves significantly better reduction in the number of explored interleavings than the classical algorithm in fact the minimal number of interleavings for a large number of our benchmarks in section although often achieves optimal reduction it may sometimes blocked therefore in the second step we combine source sets with a novel mechanism called trees thus deriving the algorithm trees control the initial steps of future that never any blocked ie redundant exploration an important feature of trees is that they are simple data structures that are constructed from already explored interleavings hence they do not increase the amount of exploration on the other hand they allow to reduce the number of explored executions in our benchmarks of the trees reduces total exploration time when blocked and it never requires more than of additional time in the cases where there are none or only a few blocked memory consumption is practically always the same between the algorithms and the space cost of maintaining trees is very small in our experience we show the applicability of our algorithms to a wide range of computation models including shared variables and message passing by them in a general setting which only assumes that we can compute a happensbefore relation also called a ordering between the events in an execution for systems with shared variables the happensbefore relation can be based on the variables that are accessed or modified by events for message passing systems the happensbefore relation can be based on the transmission of a message with the corresponding our approach allows to make leading to better reduction than many other approaches that define a happensbefore relation which is based on program statements possibly taking into account the local state in which they are executed for instance we allow a send transition to be dependent with another send transition only if the order in which the two messages are received is significant we have implemented both and as extensions for a model checking tool for programs concurrency model focuses primarily on message passing but it is also possible to write programs which manipulate shared data structures our evaluation shows that on a wide selection of benchmarks including benchmarks from the literature but more importantly on real applications of significant size we obtain optimal reduction in the number of interleavings even with therefore significantly the original algorithm not only in number of interleavings but in total execution time as well organization in the next section we illustrate the basic new ideas of our technique we introduce our computational model and formulation of the partialorder framework in section in section we introduce source sets the algorithm is described in section we formalize the concept of trees in section before describing the algorithm in section implementation of the algorithms is described in section and experimental evaluation in section the paper ends by related work and some remarks basic ideas in this section we give an informal introduction to the concepts of source sets and trees and their improvement over existing approaches using some small examples p write x q read y read x r read z read x example code source sets in example the three processes p q and r perform dependent accesses to the shared variable x two accesses are dependent if they access the same variable and at least one is a write the accesses to y and z are not dependent with anything else for this program there are four traces ie equivalence classes of executions each characterized by its sequence of accesses to x three accesses can be ordered in six ways but two pairs of orderings are equivalent since they differ only in the ordering of adjacent reads which are not dependent any por method selects some subset of p q r to perform some first step in the set of explored executions it is not enough to select only p since then executions where some read access happens before the write access of p will not be explored in assume that the first execution to be explored is we denote executions by the sequence of scheduled process steps a p x q y initially x y z r m y if m then z s n z l y if n then if l then x example program with control flow algorithm will detect the dependency between step by p and step by q and note that it seems necessary to explore sequences that start with a step of q the algorithm will also detect the dependency between and and possibly note that it is necessary to explore sequences that start with a step of r existing methods guarantee that the set of processes explored from the initial state is a persistent set in short a set p of processes is persistent in the initial state if in any execution from the initial state the first step that is dependent with the first step of some process in p must be taken by some process in p in this example the only persistent set which contains p in the initial state is p q r to see this suppose that eg r is not in the persistent set p ie p p q then the execution rr contains no step from a process in p but its second step is dependent with the first step of p which is in p in a similar way one can see that also q must be in p in contrast our source setbased algorithms allow s p q as the set of processes explored from the initial state the set s is sufficient since any execution that starts with a step of r is equivalent to some execution that starts with the first local step of q the set s is not a persistent set but it is a source set intuitively a set s of processes is a source set if for each execution e from the initial state there is some process proc in s such that the first step in e that is dependent with proc is taken by proc itself to see that p q is a source set note that when e is rr then we can choose q as proc noting that rr is not dependent with q any persistent set is also a source set but as shown by this example the converse is not true our algorithm combines source sets with sleep sets and will explore exactly four interleavings whereas any algorithm based on persistent sets will explore at least five if the first explored execution starts with p some of which will be blocked if sleep sets are used if we extend the example to include n reading processes instead of just two the number of blocked increases significantly see table in section sleep sets were introduced to prevent redundant exploration they are manipulated as follows i after exploring interleavings that begin with some process p the process p is added to the sleep set and ii when a process step that is dependent with p is executed p is removed from the sleep set the effect is that the algorithm need never explore a step of a process in the sleep set in example for instance after having explored executions starting with p the process p is added to the sleep set when exploring executions that start with q the process p is still in the sleep set after the first step of q and should not be explored next since executions that start with qp are equivalent to executions that start with pq trees as mentioned by source sets will explore a minimal number of executions for the program of example there are cases however where blocked exploration we illustrate this by example a program with four processes p q r s two events are dependent if they access the same shared initial state p r q r q r r s s s q s s other traces s q traces p q traces figure explored interleavings for example variable ie xy or z variables are local each statement accessing a global variable has a unique label eg process s has three such statements labeled and statements that operate on local variables are assumed to be part of the previous labeled statement for example label marks the read of the value of y together with the assignment on l and the condition check on n if the value of n is the condition check on l is also part of which ends just before the assignment on x that has the label similar assumptions are made for the other local statements consider a algorithm that starts the exploration with p the interleaving marked in figure with an arrow from top to bottom and then detects the race between events and it must then explore some interleaving in which the race is ie the event occurs before the event note that event will occur only if it is by the sequence and not by a step of process q thus an interleaving that this race must start with the sequence such an interleaving is shown in figure between the two chunks labeled traces having detected the race in adds r to the source set at the initial state however it does not remember that r must be followed by to reverse the race after exploring r it may therefore continue with q however after rq any exploration is to blocking meaning that the exploration reaches a state in which all enabled processes are in the sleep set to see this note that p is in the sleep set when exploring r and will remain there forever in any sequence that starts with rq as explained above it is removed only after the sequence this corresponds to the left chunk of traces in figure solves this problem by replacing the set with a structure called a tree this tree contains initial fragments of executions that are guaranteed not to sleep set blocking in the example algorithm will handle the race between and by adding the sequence to the tree the point is that after the process p has been removed from the sleep set and so sleep set blocking is avoided framework in this section we introduce the technical background material first we present the general model of concurrent systems for which the algorithms are formulated the assumptions on the happensbefore relation and finally the notions of independence and races abstract computation model we consider a concurrent system composed of a finite set of processes or threads each process executes a deterministic program statements act on the global state of the system which is made up of the local states of each process and the shared state of the system we assume that the state space does not contain cycles and that executions have bounded length we do not restrict to a specific mode of process interaction allowing instead the use of shared variables messages etc let be the set of global states of the system the system has a unique initial state s we assume that the program executed by a process p can be represented as a partial function which moves the system from one state to a subsequent state each such application of the function represents an atomic execution step of process p which may depend on and affect the global state we let each execution step or just step for short represent the combined effect of some global statement together with the following finite sequence of local statements that only access and affect the local state of the process ending just before the next global statement this avoids consideration of interleavings of local statements of different processes in the analysis such an optimization is common in tools such as the execution of a process is said to block in some state s if the process cannot continue ie is undefined for example trying to receive a message in a state where the message queue is empty to simplify the presentation we assume in sections ­ that a process does not another process ie if p is enabled and another process q performs a step then p is still enabled this assumption is valid for programs note that a process can itself eg after a step such that the next statement is a receive statement an execution sequence e of a system is a finite sequence of execution steps of its processes that is performed from the initial state s since each execution step is deterministic an execution sequence e is uniquely characterized by the sequence of processes that perform steps in e for instance denotes the execution sequence where first p performs two steps followed by a step of q the sequence of processes that perform steps in e also uniquely determine the global state of the system after e which is denoted se for a state s let denote the set of processes p that are enabled in s ie for which is defined we use to denote concatenation of sequences of processes thus if p is not blocked after e then ep is an execution sequence an event of e is a particular occurrence of a process in e we use p i to denote the ith event of process p in the execution sequence e in other words the event p i is the ith execution step of process p in the execution sequence e we use dome to denote the set of events p i which are in e ie p i dome iff e contains at least i steps of p we will use e e to range over events we use proce to denote the process p of an event e p i if ew is an execution sequence obtained by e and w then denotes dome ie the events in ew which are in w as a special case we use to denote we use e to denote the total order between events in e ie e e e denotes that e occurs before e in e we use e e to denote that the sequence e is a prefix of the sequence e event dependencies a central concept in algorithms is that of a happensbefore relation between events in an execution sequence also called a relation we denote the happensbefore relation in the execution sequence e by e intuitively for an execution sequence e and two events e and e in dome e e e means that e happens before or precedes e for instance e can be the transmission of a message that is received by e or e can be a write operation to a shared variable that is accessed by e our algorithms assume a function called a happensbefore assignment which assigns a happensbefore relation to any execution sequence in order not to restrict to a specific computation model we take a general approach where the happensbefore assignment is only required to satisfy a set of natural properties which are collected in definition as long as it satisfies these properties its precision can vary for instance the happensbefore assignment can let any transmission to a certain message buffer be related with a from the same buffer however better reduction can be if the assignment does not make the transmission of a message dependent with a of a different one in practice the happensbefore assignment function is implemented as expected by relating accesses to the same variables and of the same messages etc typically using vector clocks in section we describe such an assignment suitable for programs definition a happensbefore assignment which assigns a unique happensbefore relation e to any execution sequence e is valid if it satisfies the following properties for all execution sequences e e is a partial order on dome which is included in e the execution steps of each process are totally ordered ie p i e p i whenever p i dome if e is a prefix of e then e and e are the same on dome any linearization e of e on dome is an execution sequence which has exactly the same happensbefore relation e as e this means that the relation e induces a set of equivalent execution sequences all with the same happensbefore relation we use e e to denote that e and e are of the same happensbefore relation and e to denote the equivalence class of e if e e then se se for any sequences e e and w such that ew is an execution sequence we have e e if and only if ew e w if p q and r are different processes then if and then the first six properties should be obvious for any reasonable happensbefore relation the only one would be the last intuitively if the next step of p happens before the next step of r after the sequence e then the step of p still happens before the step of r even when some step of another process which is not dependent with p is inserted between p and r this property holds in any reasonable computation model that we could think of as examples one situation is when p and q read a shared variable that is written by r another situation is that p sends a message that is received by r if an process q is independent with p it cannot affect this message and so r still receives the same message properties and together imply as a special case that if e and e are two consecutive events in e with e e e then they can be and the global state after the two events remains the same independence and races we now define independence between events of a computation if ep and ew are both execution sequences then denotes that is an execution sequence such that e for any e in other words e pw states that the next event of p would not happen before any event in w q ry p wx r rz ew e q rx r rx figure a sample run of the program in example is shown to the left this run is annotated by a happensbefore relation the dotted arrows to the right the happensbefore relation is shown as a partial order notice that e qr since q and r are not happensbefore related in e rq we also observe that ie w q as q is the only process in w and its first occurrence has no predecessor in the dotted relation in w furthermore w q r since r is not happensbefore related to any event in w algorithm algorithm initially explore sleep if p sleep then p while p sleep do foreach e dome such ep do let e e let v ep if ie v then add some q ie v to let sleep q sleep sleep add p to sleep in the execution sequence intuitively it means that p is independent with w after e in the special case when w contains only one process q then e pq denotes that the next steps of p and q are independent after e we use to denote that does not hold for a sequence w and p w let w p denote the sequence w with its first occurrence of p removed and let w p denote the prefix of w up to but not including the first occurrence of p for an execution sequence e and an event e dome let e denote the prefix of e up to but not including the event e for an execution sequence e and an event e e let e be the subsequence of e consisting of the events that occur after e but do not happen after e ie the events e that occur after e such that e e e a central concept in most algorithms is that of a race intuitively two events e and e in an execution sequence e where e occurs before e in e are in a race if · e happensbefore e in e and · e and e are concurrent ie there is an equivalent execution sequence e e in which e and e are adjacent formally let e e e denote that proce proce that e e e and that there is no event e dome different from e and e such that e e e e e whenever a algorithm detects a race then it will check whether the events in the race can be executed in the reverse order since the events are related by the happensbefore relation this may lead to a different global state therefore the algorithm must try to explore a corresponding execution sequence let e e e denote that e e e and that the race can be formally if e e and e occurs immediately before e in e then proce was not blocked before the occurrence of e in figure there are two pairs of events e e such that e e e namely p q and p r it also holds for both these pairs that e e e since both q and r are enabled before p in other words both the races in the program are reversible source sets in this section we define the new concept of source sets intuitively source sets contain the processes that can perform first steps in the possible future execution sequences let us first define two related notions of possible first steps in a sequence · for an execution sequence ew let denote the set of processes that perform events e in that have no happensbefore predecessors in more formally p if p w and there is no other event e with e ew · for an execution sequence ew define as the union of and the set of processes p such that p and the point of these concepts is that for an execution sequence ew · p if and only if there is a sequence w such that ew and · p if and only if there are sequences w and v such that definition source sets let e be an execution sequence and let w be a set of sequences such that ew is an execution sequence for each w w a set p of processes is a source set for w after e if for each w w we have p the key property is that if p is a source set for w after e then for each execution sequence of form ew with w w there is a process p p and a sequence w such that for some sequence v therefore when an exploration algorithm to cover all in w after e the set of processes that are chosen for exploration from se must be a source set for w after e in this section we present the algorithm it is shown in algorithm as mentioned in the introduction it is the first step towards our optimal algorithm and is derived from the classical algorithm by replacing persistent sets with source sets performs a depthfirst search using the recursive procedure sleep where e is the stack ie the past execution sequence explored so far and sleep is a sleep set ie a set of processes that need not be explored from se the algorithm maintains for each prefix e of e a set of processes that will eventually be explored from e sleep to consist of an arbitrary enabled process which is not in sleep for each process p in which is not in sleep the algorithm performs two phases race detection lines ­ and state exploration lines ­ in the race detection phase the algorithm first finds the events e in e that are in a race with the next step of p and where the race can be line for each such event e dome the algorithm must explore an execution sequence in which the race is using the notation of the algorithm such an execution sequence is equivalent to a sequence of form e where v is obtained by p after the sequence e of events that occur after e in ep but do not happen after e and z is any continuation of the execution note that we insert all of e before since e includes all events that follow e in e that happen before which must be performed before when the race is the events in e that happen after e should still occur after e in the sequence z a sequence equivalent to e can be performed by taking a step of a process in ie v immediately after e the algorithm therefore at line checks whether some process in ie v is already in if not then a process in ie v is added to this ensures that a sequence equivalent to e has been or will be explored by the algorithm in the exploration phase exploration is started recursively from ep using an appropriately initialized sleep set sleep sets are manipulated as follows i after exploration of ep the process p is added to sleep set at e and ii when the exploration of ep is started recursively from e the sleep set sleep of ep is initialized to be the set of processes currently in the sleep set of e that are independent with p after e ie sleep q sleep e qp the effect is that the algorithm need never explore ep for any process p sleep since that would always lead to a sequence which is equivalent to one that has been explored from a prefix of e for processes p that were added according to case i above this is obvious to see why a process q in the initial sleep set sleep of ep need not be explored note that any execution sequence of form is equivalent to the execution sequence by since the algorithm guarantees to have explored some sequence in whenever q is in the sleep set of e it need not explore correctness algorithm is correct in the sense that for all maximal execution sequences e the algorithm some execution sequence in e for lack of space the proof is omitted the main part of the proof establishes that when sleep returns the set sleep will be a source set for w after e where w is the set of w such that ew is an execution sequence on source sets and persistent sets the mechanism by which produces source sets rather than persistent sets is the test at line in persistent setbased algorithms this test must be stronger and at least guarantee that contains a process q such that q performs some event in v which happensbefore in ep such a test guarantees that the first event in v which is dependent with some process in is performed by some process in thus making a persistent set in contrast our test at line does not require the added process to perform an event which happensbefore in e v consider for instance that v is just the sequence qp where q is independent with p after e then since the event of q does not the event of p there is an execution sequence e pq in which p is dependent with the process proce in but need not be in on the other hand since q ie pq the set together with the initial sleep set at e is still a source set for the possible continuations after e trees as we described earlier may still lead to blocked we therefore present an algorithm called which is provably optimal in that it always exactly one interleaving per trace and never blocking is obtained by combining source sets with a novel mechanism called trees which control the initial steps of future trees can be motivated by looking at lines ­ of algorithm at these lines it is found that some execution sequence starting with e v should be performed in order to reverse the detected race however at lines ­ only a single process from the sequence v is entered into thus information about how to reverse the race since the new exploration after e q does not remember this sequence v it may explore a completely different sequence which could potentially lead to sleep set blocking to prevent such a situation we replace the set by a socalled tree the tree contains initial fragments of the sequences that are to be explored after e each fragment guarantees that no sleep set blocking will be encountered during the exploration to define trees we first generalize the relations p and p to the case when p is a sequence let e be an execution sequence and let v and w be sequences of processes · let v e w denote that there is a sequence v such that and ew are execution sequences with ew intuitively v e w if after e the sequence v is a possible way to start an execution that is equivalent to w · let v e w denote that there are sequences v and w such that and are execution sequences with intuitively v e w if after e the sequence v is a possible way to start an execution that is equivalent to an execution sequence of form as special cases for a process p we have p iff p e w and p iff p e w as examples in figure we have qr e but qq e rr we also have qq e rr since e e note that e is not transitive the relation v e w can be checked using the following recursive definition lemma the relation v e w holds if either v or v is of form pv and either · p and v ep w p or · and v ep w the following lemma states some useful properties lemma let e be an execution sequence and let v w and w be sequences then ew ew implies that i v e w iff v e w and ii w e v iff w e v and iii v e w iff v e w v e w and w e w imply v e w p and w e w imply p p and e pq and e imply p the above properties follow from the definitions let us define an ordered tree as a pair b where b the set of nodes is a finite set of sequences of processes with the empty sequence being the root the children of a node w of form wp for some set of processes p are ordered by the ordering in b such an ordering between children has been extended to the total order on b by letting be the induced relation between the nodes in b this means that if the children wp and wp are ordered as wp wp then wp wp w in the induced definition tree let e be an execution sequence and p be a set of processes a tree after e p is an ordered tree b such that the following properties hold p whenever w is a leaf of b whenever up and uw are nodes in b with up uw and uw is a leaf then p intuitively a tree after e p is intended to consist of initial fragments of sequences that should be explored after e to avoid sleep set blocking when p is the current sleep set at e to see this note that if q p then by the way sleep sets are handled q for any sequence w that is explored after e if in addition then q is still in the sleep set at ew to prevent this we therefore require q which is the same as property ie p property implies that if a process p is added to the sleep set at eu after exploring then by the same reasoning as above it will have been removed from the sleep set when we reach the empty tree is the tree which consists only of the root we state a useful property of trees lemma if b is a tree after e p and w w b and w is a leaf which satisfies w e w then w w the lemma states that any leaf w is the smallest wrt node in the tree which is consistent with w after e proof we prove the lemma by contradiction assume w w then there are u p v v such that w and w uv such that up uv since uv is a leaf we have p by property of definition hence p eu v which implies up e uv which implies e uv ie w e w for a tree b and a process p b define subtree b p to denote the subtree of b rooted at p ie subtree b p b where b w pw b and is the restriction of to b let b be a tree after e p for any sequence w such that ew is an execution sequence with p we define the operation b with the properties b is also a tree after e p any leaf of b remains a leaf of b and b contains a leaf u with u e w a simple construction of b is the following let v be the smallest wrt to sequence v in b such that v e w if v is a leaf b can be taken as b and we are done otherwise let w be a shortest sequence such that w e vw and add vw as a new leaf which is ordered after all already existing nodes in b of form vw as an illustration using example assume that a tree b after contains p as the only leaf then the operation insert qq b adds qq as a new leaf with p qq if we perform insert rr b then the tree remains the same since qq rr and qq is already a leaf in this section we present the optimal algorithm shown in algorithm the algorithm performs a depthfirst search using the recursive procedure sleep where e and sleep are as in algorithm and is a tree after e sleep containing extensions of e that are guaranteed to be explored in order by sleep if is empty then sleep is free to explore any extension of e like algorithm the algorithm runs in two modes race detection lines ­ and state exploration lines ­ but it is slightly differently organized instead of analyzing races at every invocation of explore races are analyzed in the entire execution sequence only when a maximal execution sequence has been generated the reason for this is that the test at line is precise only when the used sequence v which is defined at line includes all events in the entire execution that do not happen after e also those that algorithm algorithm initially explore sleep if then foreach e e dome such that e e e do let e e let v if v then v else if then else choose p p sleep while p do let p let sleep q let p sleep add p to remove all sequences of form pw from occur after e therefore v can be defined only when e is a maximal execution sequence in the race detection phase algorithm must be able to access the current sleep set for each prefix e of the currently explored execution sequence e for each such prefix e the algorithm therefore maintains a set of processes which is the current sleep set at e in a similar way for each prefix e of e the algorithm maintains which is the current tree at e let us now explain the race detection mode which is entered whenever the exploration reaches the end of a complete sequence ie in this mode the algorithm all races that can be in the just explored sequence e such a race consists of two events e and e in e such that e e e let e e and let v ie the subsequence of e consisting of the events that occur after e but do not happen after e followed by proce this notation is introduced at lines ­ the reversible race e e e indicates that there is another execution sequence which performs v after e and in which the race is ie the event e happens before the event e since e v is incompatible with the currently explored computation the algorithm must now make sure that it will be explored if it was not explored previously if some p is in v then some execution equivalent to one starting with e v will have been explored previously if not we perform the operation v to make sure that some execution equivalent to one starting with e v will be explored in the future in the exploration mode which is entered if exploration has not reached the end of an execution sequence first the tree is initialized to if is empty then as we will state in lemma the sleep set is empty and an arbitrary enabled process is entered into the sleep set is initialized to the sleep set that is passed as argument in this call to explore each sequence in is subject to exploration we find the first ie minimal branch p in and call explore recursively for the sequence ep in this call the associated sleep set sleep is obtained from in the same way as in algorithm the associated tree is obtained as the corresponding subtree of explore is called recursively for the sequence ep with the modified sleep set sleep and tree after sleep has returned the sleep set is extended with p and all sequences beginning with p are removed from correctness let us now prove the correctness of the algorithm throughout we assume a particular completed execution of this execution consists of a number of terminated calls to sleep for some values of the parameters e sleep and let e denote the set of execution sequences e that have been explored in some call of form · · define the ordering on e by letting e e if · · returned before · · intuitively if one were to draw an ordered tree that shows how the exploration has then e would be the set of nodes in the tree and would be the between nodes in that tree we begin by establishing some useful invariants lemma if ep ew then p proof after the call to · · has returned we have that p it follows from the rules for manipulating sleep sets that if ew e and ep ew then p lemma whenever algorithm is inside a call of form sleep then is a tree after e if is empty then sleep is empty the following lemma captures the relationship between trees and e lemma let e be the tree of explored execution sequences consider some point in the execution and the tree at that point for some e e if w for some w then ew e if w w for w w then ew ew proof the properties follow by noting how the exploration from any e e is controlled by the tree at lines we can now give the proof of correctness for the algorithm theorem whenever a call to sleep returns during algorithm then for all maximal execution sequences of form ew the algorithm has explored some execution sequence e which is in ew since initially the algorithm is called with explore theorem implies that for all maximal execution sequences of form e the algorithm some execution sequence e which is in e proof of theorem by induction on the set of execution sequences e that are explored during the considered execution using the ordering ie the order in which the corresponding calls to explore returned base case this case is the first sequence e for which the call · · returns by the algorithm e is already maximal so the theorem trivially holds inductive step consider an arbitrary execution sequence e e that is explored by the algorithm let denote the value of when sleep returns let denote sleep ie the set of processes that are explored from se as inductive hypothesis we assume that the theorem holds for all execution sequences e with e e the inductive step is proven by contradiction assume that ew is a maximal execution sequence such that the algorithm has not explored any execution sequence e in ew we first prove that this implies the proof is by contradiction assume that there is a p with p ie p at some point during the exploration let e be the longest prefix of e such that e p e and define w by e w e by the handling of sleep sets we have e pw it follows by the definition of that there is a w such that ew e w pw e pw w by the inductive hypothesis applied to e p the algorithm has explored some execution sequence in e pw w ew which gives a contradiction for each process p let ep be the prefix of e such that is the last wrt execution sequence of form ep p with ep being a prefix of e that precedes e wrt note that if p then ep e but if p sleep then ep is a strict prefix of e let wp be defined by e note that let wp be the longest prefix of w such that and let ep be the first event in which is not in wp such an event ep must exist since otherwise wp w which implies which implies p which contradicts let q be such that is a longest prefix among the prefixes wp for p if there are several processes p such that wp is the same longest prefix then let q be the process among these such that is minimal wrt let wr be consider the exploration of which happens in the call sleep · with sleep as the sleep set argument since is an execution sequence and it follows that is an execution sequence since is an execution sequence and q does not eq since q and are different we conclude that and hence is an execution sequence we next establish that sleep using a proof by contradiction as follows assume that some process p is in sleep by the construction of sleep at line the process p must be in just before the call to sleep · and satisfy eq pq from p and property of lemma we infer p from this eq pq and property of lemma it follows that p which using which follows from eq implies p which by property of lemma imply p from this and the property that p just before the call to sleep · we have by the handling of sleep sets that p which together with p implies eq this and the fact that p just before the call to sleep · implies that p ie p since eq we have by construction that ep eq but since among the processes p with ep eq we chose q to be the first one for which a call of form · · was performed we have that p just before the call to sleep · p sleep thus we have a contradiction let z be any sequence that makes maximal such a z can always be found since is an execution sequence from sleep proven in the preceding follows sleep hence no execution sequence in was explored before the call to sleep · otherwise there would be a call p · · with e a prefix of eq and p sleep and defining w by e w eq we would have e pw and p thus sleep by the inductive hypothesis for applied to the algorithm then some sequence of form in by the construction of wr we have eq from it follows that the same race between and eq will also occur in ie we have eq since the sequence is actually explored by the algorithm it will the race eq when handling it · e in the algorithm will correspond to in this proof · e in the algorithm will correspond to in this proof and · e in the algorithm will correspond to eq in this proof let v be the sequence v at line in the algorithm let x be and let x be we note that is a prefix of x from and the definitions of x and x it follows that and hence that let x be obtained from x by adding just after the prefix ie in the same place that it has in since eq happens after in it follows that no events in x happen after eq in hence since is a prefix of x we have eq x which by implies eq x which by implies eq v by properties of sleep sets and the construction of wr it fol that this implies by eq v that thus the test at line will succeed and after performing line the tree will by the specification of insert contain a leaf y such that y eq v since at this point q and q we have by definition of insert that and by property of lemma we then have e from eq v and y eq v it follows by property of lemma that y eq furthermore from which follows from and it follows that y is not a prefix of let u be the longest common prefix of y and we claim that is a strict prefix of u otherwise there are different processes p p and a sequence v such that up v and up is a prefix of y from y eq and property of lemma we infer y eq up if up when y is inserted we infer by lemma and property of lemma that if up when y is inserted we also infer since then y will be explored before up thus which implies v which by handling of sleep sets implies p v by y eq up eq up v we have p p v which is the same as p v by p v this implies v this implies that p ie p hence by the construction of wr we have p v wr which together with v implies p v wr which implies up eq up v wr which implies y eq which contradicts the construction of y thus is a strict prefix of u since and hence is explored by the algorithm we have moreover since u is a prefix of we infer that is a prefix of this means that there is a sequence w such that ew it follows by the inductive hypothesis applied to that the algorithm has explored some maximal sequence in and hence in ew this contradicts the assumption at the beginning of the inductive step this concludes the proof of the inductive step and the theorem is proven optimality in this section we prove that is optimal in the sense that it never two different but equivalent execution sequences and never sleep set blocking the following theorem which is essentially the same as theorem of et al establishes that sleep sets alone are sufficient to prevent exploration of two equivalent maximal execution sequences theorem never two maximal execution sequences which are equivalent proof assume that e and e are two equivalent maximal execution sequences which are explored by the algorithm then they are both in e assume that e e let e be their longest common prefix and let e and e ev by lemma we have p which contradicts e e and the of e and e we will now prove that algorithm is optimal in the sense that it never blocking let us first define this precisely definition sleep set blocking during the execution of algorithm a call to sleep is sleep set blocked if and sleep now let us state and prove the corresponding optimality theorem theorem during any execution of algorithm no call to sleep is ever sleep set blocked proof consider a call sleep during the exploration then any sequence in is enabled after e by lemma is a tree after e sleep thus if sleep then contains a sequence w such that sleep letting p be the first process in w this implies p sleep that p is enabled and thus sleep implementation in this section we describe our implementation in the context of a model checking tool for is an relevant programming language based on the actor model of concurrency in actors are realized by processes implemented by the runtime system instead of being directly mapped to os threads each process has its own private memory area stack heap and and with other processes via message passing a call to the spawn function creates a new process p and returns a process identifier pid that can be used to send messages to p messages are sent using the or send function messages get placed in the of the receiving process in the order they arrive a process can then consume messages using selective pattern matching in receive expressions which are blocking operations public writer fun x end reader i x end end i n receive after deadlock end figure program in when a process does not contain any matching message a receive may contain an after clause which specifies a timeout value either an integer or the special value and a value to be returned if the timeout time in ms is processes do not share any memory by default still the implementation comes with a store mechanism called term storage that allows processes to create memory areas where terms shared between processes can be inserted up and updated such areas are the tables that are explicitly declared public the runtime system automatically accesses to these tables when this is necessary each table is owned by the process that created it and its memory is by the runtime system when this process exits has all the needed for concurrency via message passing and most of the eg reads and writes to shared data etc needed for concurrent programming using shared memory programs are to the usual errors associated with concurrent execution although the of them around message passing and of builtin primitives implemented in c figure shows example written in generalized to n instead of just two readers a public table shared between n processes n readers and one writer the writer inserts a pair using x as a key each of the n readers tries to read two entries from this table some entry with a different key in each process an integer in the range n and the entry by x the receive expression on line forces the process executing the readers code to get stuck at this point ensuring that the process the table which in turn preserves the table is a systematic testing tool for finding concurrency errors in programs or verifying their absence given a program and a test to run uses a search algorithm to systematically explore the execution of the test under conceptually all process interleaving to achieve this the tool a transformation that inserts instrumentation at points ie points where a context switch is allowed to occur in the code under execution this instrumentation allows to take control of the scheduler when the program is run without having to modify the vm in any way in the current vm a context switch may occur at any function call inserts points only at process actions that interact with ie or update shared state supports the complete language and can programs of any size including any libraries they use the tool two techniques to reduce the number of explored traces i an optimization which avoids exploring traces that involve processes blocking on receive expressions and ii a technique that restricts the number of explored traces with respect to a parameter which calls bound in this respect is similar to the tool our implementation we extended with three algorithms i the algorithm presented by flanagan and with the sleep set extension ii and iii to implement these we had to encode rules for dependencies between operations that constitute points these rules are shared between all variants for and inserts to tables ie reads and writes the rules are standard two operations conflict if they act on the same key and at least one is an insert for sending and receiving operations the happens before relation e is the following · two sends are ordered by e if they send to the same process even if the messages are the same note that if we would not order two sends that send the same message then when we them the corresponding receive will not happen after the same send statement · a send happens before the receive statement that receives the message it sent a race exists between these statements only if the receive has an after clause · a receive which executes its after clause happens before a subsequent send which sends a message that it can consume there are also other primitives in but it is beyond the scope of this paper to describe how they interact uses a vector clock for each process at each state to calculate the happens before relation for any two events the calculation of the vector clocks uses the ideas presented in the original paper the only special case is for the association of a send with a receive where we the message itself with the vector clock of the sending process experiments we report experimental results that compare the performance of the three algorithms which we will refer to as classic for the algorithm of source and optimal we run all benchmarks on a with an i cpu gb of ram running linux the machine has four physical but uses only one of them in all benchmarks was started with the option p inf which the tool to use an infinite bound ie verify these programs performance on two standard benchmarks first we report performance on the two benchmarks from the paper and these are benchmarks that have been used to evaluate another variant and a technique based on as both programs use locks we had to a locking mechanism using to make this translation we used particular language features this benchmark uses two primitives called acquire and release the assumptions made for these primitives are that an acquire and a release operation on the same lock are never and should therefore not tried to be interleaved in a different way than they occur thus acquires are the only operations that can be if possible to get a different interleaving we implemented the lock objects in as separate processes to acquire the lock a process sends a message with its identifier to the lock process and for a reply upon receiving the message the lock process uses the identifier to reply and then for a release message other acquire messages are left in the locks upon receiving the release message the lock process loops back to the start the next acquire message and the next process this behavior can be implemented in using two selective receives traces explored time benchmark classic source optimal classic source optimal s s s s s s ms s s ms s s s s s ms s s table performance of algorithms on two benchmarks this benchmark uses a cas primitive instruction to check whether a specific entry in a matrix is and set it to a new value the way to do this is to try to execute an operation on an table if another entry with the same key exists the operation returns false otherwise the operation returns true and the table now contains the new entry both benchmarks are parametric on the number of threads they use for we used and threads for we used and threads table shows the number of traces that the algorithms explore as well as the time it takes to explore them it is clear that our algorithms which in these benchmarks explore the same optimal number of interleavings classic with sleep sets by a that becomes as the number of threads increases as a check et al report that their method is also able to explore only paths for while their prototype implementation of extended with sleep sets and support for commutativity of reads and writes between and paths with as value the numbers we report for classic and for our algorithms are very similar performance on two benchmarks next we compare the algorithms on two benchmarks that expose differences between them the first is the readers program of figure the results for and readers are shown in table for classic the number of explored traces is on here while only explore n traces both numbers are exponential in n but as can be seen in the table for eg n source and finish in about one and a half while the algorithm with the sleep set extension two orders of magnitude more mostly blocked traces and needs almost one and a half to complete traces explored time benchmark classic source optimal classic source optimal readers readers readers s s s s s s m s ms ms s s s ms s s ms m s ms table performance of algorithms on more benchmarks variables int i thread for i n i thread j j n figure the of the benchmark traces explored time benchmark classic source optimal classic source optimal ms ms ms m s ms ms m s ms ms ms ms ms table performance of algorithms on four real programs classic source optimal table memory consumption in mb for selected benchmarks the second benchmark is the program whose is shown in figure its n threads operate on an array of n elements which are all initially zero in this program thread searches the array for the zero element with the index while the other n threads read one of the array elements and update the next one the final state of the program is uniquely defined by the values of i and what happens here is that thread has control flow that depends on data that is exposed to races and represents a case when may blocking that the algorithm avoids as can be seen in table about twice as many traces than and naturally even if it uses a test takes almost twice as much time to complete performance on real programs finally we evaluate the algorithms on four applications the programs are i a parallel static code analyzer included in the distribution ii an extended process a pool and iv a program that uses processes and tables to solve the in parallel the last program is complex but lines of code the first three programs besides their code call many modules from the libraries which also the total number of lines of instrumented code for testing the first three programs is and respectively table shows the results here the performance differences are not as as in benchmarks still some general conclusions can be drawn both source and explore less traces than classic from up to times fewer and require less time to do so from up to times faster even in real programs the number of blocked is significant regarding the number of traces explored is quite close to optimal but to completely avoid blocked executions in only one program in is faster overall but only slightly so compared to even though it uses a test in fact its maximal performance difference from is a bit less than in again although due to space limitations we do not include a full set of memory consumption measurements we mention that all algorithms have very similar and quite low memory needs table shows numbers for the real program which requires most memory and for all benchmarks where the difference between source and optimal is more than one mb from these numbers it can also be that the size of the tree is small in fact the average size of the trees for these programs is less than three nodes and related work in early approaches to model checking it was observed that reduction was needed to the explosion in number of explored interleavings several reduction methods have been proposed including partial order reduction and context bounding since early persistent set techniques on static analysis sleep set techniques were used in it was observed that sleep sets are sufficient to prevent the complete exploration of different but equivalent interleavings but additional techniques were needed to reduce blocked exploration dynamic partial order reduction showed how to construct persistent sets by need leading to better reduction similar techniques have been applied in testing and symbolic execution eg to testing where new test runs are in response to detected races several variants improvements and of for model checking and testing have appeared all based on persistent sets our algorithms can be applied to all these contexts to provide increased or optimal reduction in the number of explored interleavings a related area is reachability testing in which test executions of concurrent programs are by the test and present a technique for exploring all traces in a setting with a restricted set of primitives message passing and monitors for process interaction the scheduling of new test executions explicitly pairs message with and could potentially require significant memory compared to the more lightweight approach of software model checking the technique of and guarantees to avoid of different but equivalent maximal executions corresponding to theorem but reports blocked executions et al present a normal form for executions of concurrent programs and prove that two different executions are not in the same trace this normal form can be exploited by sat or bounded model checkers but it can not be used by model checkers that enumerate the execution sequences by exploration et al use which can also obtain optimal reduction in number of interleavings however this technique has significantly larger overhead than techniques and also needs an additional step for checking nonlocal properties such as races and deadlocks a technique for using partial order reduction for programs without moving to an formulation is to refine the concept of dependency between transitions to that of conditional dependency conclusion we have presented a new algorithm which is the first to be provably optimal in that it is guaranteed both to explore the minimal number of executions and to avoid sleep set it is based on a novel class of sets called source sets source sets make existing algorithms significantly more efficient and can be extended with trees to achieve optimality in the derivation of the optimal algorithm we have first presented a simpler algorithm which maintains less information than the optimal algorithm on the other hand the extra overhead of maintaining trees is very in practice never more than in our experiments which is a good tradeoff for having an optimality guarantee and the possibility to run arbitrarily faster we intend to further explore the ideas behind source sets and trees not only for verification but also for new ways of testing programs acknowledgments this work was carried out within the centre of programming for architectures research center and was supported in part by the eu fp project release and the research references j comm of the acm ­ m a and k systematic testing for detecting concurrency errors in programs in e m clarke o m and d state space reduction using partial order techniques ­ c flanagan and p dynamic partialorder reduction for model checking software in popl pages ­ acm c flanagan and p to dynamic partialorder reduction for model checking software available at http p partialorder methods for the verification of concurrent systems an approach to the problem phd thesis university of also volume of lncs springer p model checking for programming languages using in popl pages ­ acm press p software model checking the approach formal methods in system design ­ p and d refining dependencies improves partialorder verification methods in cav volume of lncs p g j and d caching formal methods in system design ­ k o and k using in automated testing of multithreaded programs in pages ­ acm v c wang and a gupta monotonic partial order reduction an optimal symbolic partial order reduction technique in cav volume of lncs pages ­ springer s and d defining conditional independence using theoretical computer science ­ s r d and g evaluating ordering heuristics for dynamic partialorder reduction techniques in volume of lncs pages ­ springer y and r reachability testing of concurrent programs ieee trans ­ f virtual time and global states of distributed systems in m editor proc workshop on parallel and distributed algorithms pages ­ ch de france a trace theory in advances in petri nets k a technique of a state space search based on unfolding formal methods in system design ­ m and s qadeer iterative context bounding for systematic testing of multithreaded programs in pldi pages ­ m s qadeer t ball g p and i finding and in concurrent programs in pages ­ usenix association d all from one one for all on modelchecking using in cav volume of lncs pages ­ o k and k improving dynamic partial order reductions for testing in ieee k and g automated systematic testing of open distributed programs in volume of lncs pages ­ k and g a and algorithm for automated testing of multithreaded programs in verification conference volume of lncs pages ­ springer s et al a novel dynamic partialorder reduction technique for testing actor programs in volume of lncs pages ­ springer a sets for reduced state space generation in advances in petri nets volume of lncs pages ­ 