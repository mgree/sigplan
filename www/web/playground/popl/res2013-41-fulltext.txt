optimizing data structures in highlevel programs new directions for extensible compilers based on staging k j lee martin odersky oracle stanford university abstract high level data structures are a of modern programming and at the same time stand in the way of compiler optimizations in order to reason about user or data structures compilers need to be extensible common mechanisms to extend compilers fall into two categories frontend macros staging or partial evaluation systems can be used to remove abstraction and specialize programs before they enter the compiler alternatively some compilers allow extending the internal by adding new transformation passes at different points in the compile chain or adding new intermediate representation ir types none of these mechanisms alone is sufficient to handle the challenges by high level data structures this paper shows a novel way to combine them to yield benefits that are greater than the sum of the parts instead of using staging merely as a front end we implement internal compiler passes using staging as well these internal passes back to program execution to construct the transformed ir staging is known to simplify program generation and in the same way it can simplify program transformation defining a transformation as a staged ir interpreter is simpler than implementing a lowlevel ir to ir transformer with custom ir nodes many optimizations that are expressed as from ir nodes to staged program fragments can be combined into a single pass phase ordering problems speculative rewriting can preserve optimistic assumptions around loops we demonstrate several powerful program optimizations using this architecture that are particularly towards data structures a novel loop fusion and deforestation algorithm array of struct to struct of array conversion object and code generation for heterogeneous parallel devices we validate our approach using several non trivial case studies that exhibit order of magnitude in experiments categories and subject descriptors d programming languages processors ­ code generation optimization runtime environments d programming techniques concurrent programming ­ parallel programming general terms design languages performance keywords staging code generation data structures extensible compilers permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ italy copyright c acm vectors object vector def new vector val data a def int abstract class val data def that matrices abstract class complex numbers case class double im double def that complex im def that complex type class implementation figure skeleton of a highlevel scala linear algebra package introduction compiling highlevel programs to efficient lowlevel code is hard particularly because such programs use and define highlevel ab the compiler cannot see through abstractions tion and it cannot reason about domainspecific prop general purpose among the most important abstractions are data structure and collection operations and those also commonly present the most difficulties to an optimizing com let us consider an example of high level programming in scala we would like to implement a linear algebra package fig shows a skeleton implementation of vectors and matrices as a layer over arrays using highlevel collection operations fill internally vectors and matrices contain numeric values type class numeric we also define complex numbers as a new numeric type with these definitions at hand we can write programs like the following def k val m vv val m if scale km else no need to compute m this code is elegant and high level how fo on abstraction and generalization increases development pro but unfortunately the code will run very one or two orders of magnitude slower than a lowlevel tation using just arrays and while loops see section what exactly is going wrong some of the reasons are neither the scala compiler nor the compiler inside the jvm can apply generic optimizations like common subexpression or dead code elimination cse to nontrivial matrix or vector computations the involved compilers have no notion of domainspecific laws like where m is a matrix and id the identity matrix which could be used for optimization programming in a functional programming style creates of intermediate objects the uniform object representation on the jvm is inefficient for complex numbers in order to enable a compiler to reason about programs with highlevel abstractions we need mechanisms to extend the compiler so that it is able to resolve those abstractions there are two common approaches the first is to translate the abstractions away before the program reaches the compiler proper so that the compiler does not need to be aware of them this is the idea behind macro systems and staging partial evaluation pe can also be used to specialize programs before they reach the actual compiler the second option is to the compiler new domainspecific rules usually compiler is understood as a means to add new phases some extensible compilers also allow adding new ir types but often it is not clear how new nodes interact with existing generic optimizations however neither of these approaches alone is sufficient to handle the challenges by highlevel data structures and abstractions going back to our example if all we have is staging or macros then the expression m id which is equivalent to m will be expanded into while loops before it even reaches the compiler so no simplification to m can take place in general limited forms of simplification can be added see c expression templates but to be fully effective the full range of generic compiler optimizations cse constant propagation etc would need to be duplicated too if on the other hand all we have is a facility to add new compiler passes then we can add an optimization pass that simplifies mid to m but we need another pass that expands matrix into loops these passes need to be implemented as lowlevel ir to ir transformations which is much more difficult than the macro or staging approach that can use regular computation to express the desired target code implementing optimizations as separate passes also leads to phase ordering problems if multiple optimizations are added independently we argue that what we really want is the best of both worlds on the one hand we want to treat operations symbolically so that they can be matched by transformation rules but on the other hand we also want the power of staging to define the result of a transformation in addition to that we need a way to define transformations independently but avoid phase ordering problems when optimizations are applied contributions in this paper we present an extensible compiler architecture that solves this challenge while still keeping the programming effort required to express optimizations and transformations low our approach achieves large on highlevel programs by collection operations changing data layout and applying further optimizations on highlevel objects enabled by intermediate languages with staging and a facility to combine independently specified optimizations without phase ordering issues to illustrate at a high level how our system works consider again the linear algebra case we use staging to obtain an intermediate representation from the initial program in order to avoid the problems of the pure staging or macro approach we apply optimizations at different levels and we combine as many optimizations as possible together in a single pass to avoid many of the traditional problems linear algebra optimizations are implemented by the library author as rewrite rules that rewrite symbolic expressions into staged program fragments the system applies rewriting using optimistic assumptions and back intermediate transformations when they later turn out to be unsound this strategy eliminates phase ordering problems that are due to one optimization module having to make pessimistic assumptions about the outcome of another one once no further simplification is possible on the linear algebra level we want to switch to a lower level representation which consists of arrays and loops we implement this kind of transform as a staged interpreter over the intermediate program since the interpreter again uses staging it constructs a new program and thus acts as a program transformer by default this interpreter maps each expression to a structurally equivalent one the library author extends it to map linear algebra operations to their lower level representation on this lower level the system applies another set of optimizations eg loop fusion in a combined pass global optimizations to take advantage of new opportunities exposed by changing the representation this process can be repeated for any desired number of steps in particular we make the following contributions · we use staging to build extensible compilers that can also combine modular optimizations into single passes staged ir interpreters act as ir transformers and speculative rewriting allows combining independently specified optimization while keeping optimistic assumptions · we use a intermediate representation that may contain structured compound expressions splitting and merging compound expressions allows existing optimization on their pieces eg to remove unused parts of a data structure this approach extends to powerful data format conversions eg to · we present a novel data parallel loop fusion algorithm that uniformly handles and vertical fusion and also includes asymmetric traversals · we demonstrate how this compiler architecture can solve optimization problems related to data structures in a number of nontrivial case studies we build on our previous work on lightweight modular staging and and differences to previous work as we go along the speculative rewriting approach is based on earlier work by and chambers organization we start out by partial evaluation and staging § insights from this section will help understand how we use staging for program transformation § where we first present symbolic optimizations using speculative rewriting § before into passes as staged ir interpreters § a third transformation we explore is the splitting and merging of compound expressions to reuse existing optimizations § we then present how these techniques can be used to perform data structure optimizations § our generic staged struct abstraction § which extends to unions and inheritance § an array of struct to struct of array transform § and loop fusion § we then present a set of case studies where our transformations are particularly § linear algebra § regular expression matching § collection and query operations § and string § finally we discuss our results § review related work § and conclude § background many computations can naturally be separated into stages distinguished by frequency of execution or availability of information programming staging for short as established by and make the stages explicit and allows programmers to delay evaluation of a program expression to a later stage thus staging an expression the present stage effectively acts as a code generator that when run produces the program of the next stage staging is closely related to partial evaluation which programs to statically known parts of their input for the purpose of this paper we can treat partial evaluation and in particular bindingtime analysis as automatic staging and staging as partial evaluation a key difference is that partial evaluation strictly programs and usually comes with soundness guarantees whereas adding staging annotations to a program in an language such as provides more freedom for composing staged fragments but requires some care so as not to change the computed result much of the research on staging and partial evaluation was driven by the to simplify compiler development for example an interpreter to a particular program yields a compiled program with the interpreter overhead removed the first projection self applicable partial evaluators can generate compilers from interpreters and compiler generators the in this paper uses scala and lightweight modular staging a staging approach contrary to languages based on uses only types to distinguish the computational stages expressions belonging to the second stage have type in the first stage when yielding a computation of type t in the second stage expressions of a plain type t will be evaluated in the first stage and become constants in stage two the plain scala type system propagates information about which expressions are staged and thus performs a local thus shares some of the benefits of automatic pe and manual staging example traversal abstractions arrays in scala are jvm arrays which need to be traversed using while loops and indices the scala standard library provides an that adds a foreach method array foreach i adding foreach is achieved using an implicit conversion implicit def new def t unit unit var i while i alength i this implementation has abstraction overhead closure allocation interference with jvm inlining etc we would like to tell the compiler whenever it a foreach invocation to just put the while loop there instead this is a simple case where macros or staging can help using we just change the method argument types figure shows a set of staged array and vector operations the framework provides overloaded variants of many operations that lift those operations to work on rep types ie staged expressions rather than actual data it is important to note the difference between types a staged function object and a function on staged values by using the latter foreach ensures that the function parameter is always evaluated and at staging time macro systems that only allow lifting expression trees support only types this limits expressiveness and there are no guarantees that higher order functions are evaluated at staging time in addition to the framework we use the compiler which several core language features as method calls and thus makes them as well for example the code array implicit def new def var i while i alength i def min i bi vector extends struct val data implicit def new def b companion objects define and figure staged array and vector ops var i while i n i i will be as follows val i n i methods assign while are overloaded to work on rep types these methods need to be defined and made available in scope also provides overloaded field access and object construction methods in the declaration of vectors or complex numbers extending struct serves as a marker to automatically lift object construction and field accesses to the domain of rep types this means that staged field accesses such as are available on values and in this case would return values similarly new vector val data type will return a object generic programming with type classes figure uses the type class numeric to abstract over particular numeric types the type class pattern which data objects from generic dispatch naturally with a staged programming model we can define a staged variant of the standard numeric type class and with addition on numeric vectors defined in figure make vectors themselves instance of numeric class def implicit def new def b a b this allows us to pass say a staged to any function that works over generic types such as vector addition itself the same holds for without staging type classes are implemented by passing an implicit dictionary the type class instance to generic functions here type classes are a purely concept all generic code is specialized to the concrete types and no type class instances exist and hence no virtual dispatch occurs when the staged program is run maintaining evaluation order compared to staging or macro systems based on preserves program semantics in more cases in particular adding rep types does not change the relative evaluation order of statements within a stage in compute foreach i the staged foreach implementation from figure will evaluate compute only once whereas purely syntactic expansion would produce this target code while i i performs conversion similar to earlier work on pe with effects systems based on could use the same method to maintain evaluation order but we are of any that does instead most other systems leave it up to the programmer to insert in the right places which can easily lead to subtle errors limitations of frontend staging and macros despite the given benefits for top performance it is often not sufficient to use staging or macros or partial evaluation as a front end let us consider a simple yet nontrivial example val v v staging will replace the zero vector creation and the subsequent addition with arrays and loops what we would like instead however is to apply a symbolic simplification rule to remove the zero addition before expansion takes place let us imagine that our system would allow us to implement the vector plus operation in such a way that it can its arguments to look for invocations of this would cover the simple case above but we would still run into problems if we the use case slightly val v val v v v to handle programs like this it is not sufficient to just the syntactic arguments we need to integrate the staging expansion with some form of forward data flow propagation otherwise the argument to plus is just an identifier the deeper problem is that we are forced to commit to a single data representation even if we combine staging with an extensible compiler we need to make a decision should we treat vectors as symbolic entities with algebraic laws implemented as ir nodes amenable to optimization or should we stage them so that the compiler just loops and arrays without abstraction overhead the following sections will discuss mechanisms to integrate these approaches and for treating data structures in a more abstract way program transformation via staging staging usually is a method for generating programs a program builds an object program which is then compiled normally again we show that staging is also useful as a tool for transforming programs any transformation can be broken down into a traversal and a generation part not surprisingly staging helps with the generation part in our case internal compiler passes are ir interpreters that happen to be staged so that they return a new program as the result this way they can back to program execution to build the result of a program transformation staging for program transformation has a number of benefits first building a staged ir interpreter is far simpler than building a nontrivial ir to ir transformer second optimizations can be added to a staged program starting eg with the code from figure third the program or library itself is in control of the translation and can influence what kind of code is generated one of the key aspects of our approach is to distinguish two kinds of transforms optimizations and translate programs into a representation eg linear algebra operations into arrays and loops have a natural ordering so they can be in separate passes optimizations by contrast have no clear ordering and are to phase ordering problems thus they need to be combined for maximum effectiveness also most are whereas optimizations are usually optional of course the distinction is not always clear cut but many transforms fall into only one of the categories in any case it is important that all applicable optimizations are applied before takes place otherwise highlevel optimization opportunities may be after a step there may be new opportunities for optimization thus our system performs a sequence of optimization optimization steps until the representation is reached the final representation is to target code we the extensible ir § and first consider optimizations § then § we the treatment of compound expressions § the extensible graph ir we now turn to the level of primitive staged operations using we do not directly produce the second stage program in source form but instead as an extensible intermediate representation ir we refer the reader to our previous work for details on the ir ­ but give a short here the overall structure is that of a of nodes dependency graph in figure which will be the running example for this section we the vector implementation from figure in terms of custom ir nodes the interface is defined in vectors with abstract methods that are implemented in to create ir nodes of type and respectively the framework provides ir base classes via mixed into but not vectors to keep the ir hidden from user code expressions are atomic abstract class case class t extends case class int extends defines whereas is left as an abstract type in base custom composite ir nodes extend they refer to other ir nodes only via symbols there is also a type to define nested blocks not used in figure taking a closer look at reveals that its expected return type is but the result value is of type this conversion is achieved implicitly by implicit def method maintains the correct evaluation order by binding the argument d to a fresh symbol on the conversion def def the counterpart note the argument captures performed statements into a block object additional reflect methods exist to mark ir nodes with various kinds of side effects see for details combining optimizations speculative rewriting many optimizations that are traditionally implemented using an iterative dataflow analysis followed by a transformation pass can also be expressed using various of possibly context dependent rewriting whenever possible we tend to prefer a rewriting variant because rewrite rules are easy to specify separately and do not require programmers to define abstract interpretation lattices rewrites and smart constructors performs forward op while constructing the ir im cse and smart constructors apply pattern rewriting in various kinds of constant propagation this can be seen as adding online pe to the existing offline stage distinction de by rep types in figure can be mixed in with and to simplify zero additions rewriting is integrated with other forward optimizations pattern matches of the form case will look up the available definition of a symbol here is a simple ex program before left and after constant folding middle val x y val x y y x y will later remove the multiplication and the binding for x right vector interface vectors extends base implicit v v def def b low level translation target extends vectors def i def b ir level implementation extends with vectors ir node definitions and constructors case class extends case class extends def def b transformation default case def transformer d match case case case optimizing rewrites can be specified separately extends def case a a case b b case transformer ir low level extends val ir with import ir def d match case case case figure vector implementation with ir and applying rewrites many optimizations are mutually in the presence of loops optimizations need to make optimistic assumptions for the supporting analysis to obtain best results if multiple analyses are run separately each of them effectively makes pessimistic assumptions about the outcome of all others combined analyses avoid the phase ordering problem by solving everything at the same time the challenge of course is to automatically combine analyses and transformations that are implemented independently of one another since the forward optimizations described above are applied at ir construction time where loop information is incomplete previously allowed rewrites and cse only for purely functional operations and not in the presence of imperative loops these pessimistic assumptions together with monolithic compound expressions see section effective combinations of separate rewrites and chambers showed a method of composing separately specified optimizations by interleaving analyses and transformations we use a modified version of their algorithm that works on structured loops instead of cfgs and using dependency information and rewriting instead of explicit data flow lattices to the best of our knowledge we are the first to extend this approach to extensible compilers and a purely rewriting based environment usually rewriting is semantics preserving ie pessimistic the idea is to drop that assumption as a corollary we need to rewrite and be able to rollback to a previous state to get optimistic optimization the algorithm proceeds as follows for each encountered loop apply all possible transforms to the loop body given empty initial assumptions analyze the result of the transformation if any new information is discovered throw away the transformed loop body and the original with updated assumptions repeat until the analysis result has reached a fixpoint and keep the last transformation as result this process can be costly for nested loops but compares to the al of running independent transformations one after another until a global fixpoint is reached here is an example of speculative rewriting showing the un transformed program left the initial optimistic iteration middle and the fixpoint right reached after the second iteration var x var x var x dead var c var c var c while c while true while c if x print print print else x c print print print c c c this algorithm allows us to do all forward data flow analyses and transforms in one uniform combined pass driven by rewriting in the example above during the initial iteration middle separately specified rewrites for variables and conditionals work together to determine that xc is never executed at the end of the loop body we discover the write to c which our initial optimistic assumption c we rewrite the original body again with the new information right this time there is no additional knowledge discovered so the last speculative rewrite becomes the final one speculative rewriting as used here is still a fully static not to be with speculative dynamic optimizations per formed inside virtual machines that may multiple profiling compilation and cycles separate passes transformers previously had a single code generation pass that scheduled the ir graph performing and code motion we generalize this facility to allow arbitrary traversals of the program tree which results from scheduling the ir graph similar to other optimizations and code motion work on highlevel possibly composite ir nodes generic ir traversal and transformation once we have a traversal abstraction transformation out naturally by building a traversal that constructs a new staged program the implementation is shown in figure transformation needs a default case which we call see in figure an ir node will call back to the corresponding smart constructor after applying a transformer to its arguments loops is slightly more complicated than what is shown in figure because of the bound variable case j j implementing custom transformers in our running example we would like to treat linear algebra operations symbolically first with individual ir nodes like and in figure the smart constructor implements a rewrite that simplifies vz to v cse etc will all be performed on these high level nodes after all those optimizations are applied we want to transform our operations to the lowlevel array implementation from figure in a separate pass in figure implements this transformation by back to code traversal val ir expressions import ir def unit foreach def stmt unit foreach transform extends val ir expressions import ir var subst def lookup s in subst def def this def stmt val e subst e e figure traversal and transformer interface namely method in the result of the transform is a staged program fragment just like in figure this greatly simplifies the definition of the transform which would otherwise need to the fill or code using low level ir instead we benefit directly from the staged definition from figure also further rewrites will take place automatically essentially all simplifications are performed after each transform phase thus we guarantee that cse etc have been applied on highlevel operations before they are translated into on which optimizations would be much harder to apply to give a quick example the initial program val v val v val v v v v v will become val v after modulo unfolding of staged worklist transformers and delayed rewriting transformers can be extended with a worklist which is useful if the result may contain terms that need further transformation with a worklist transformer we can register individual rewrites for particular nodes running the transformer applies the rewrites which may register new for the next iteration the process stops if no further work is required empty work list delayed rewriting is a simplified interface that allows specifying together with the operations and the regular immediate rewrites this helps to reduce the needed to define transformations for example here is the from figure as delayed rewrite def i the block is with worklist transformer before the ir node remains a node which allows other smart constructor rewrites to in that expect this pattern compound expressions split and merge since our ir contains structured expressions like loops and conditionals optimizations need to reason about compound statements this is not easy for example a simple algorithm will not be able to remove only pieces of a compound expression our solution is simple yet effective we split many kinds of compound statements assuming that only parts will be needed splitting is implemented just like any other rewrite and thus well with other optimizations see section we find out which parts are needed through the regular algorithm and we the remaining pieces effectful statements a good example of statement splitting is effectful conditionals var a b c var a b c var a c if c if c a if c a a b if c b else c else if c c c from the conditional in the initial program left splitting creates three separate expressions one for each referenced variable mid removes the middle one because variable b is not used and the remaining conditionals are merged back together right of course successful merging requires to keep track of how expres sions have been split an extension of this simple merge facility which attempts to merge expressions that may not have been split before is loop fusion section data structures splitting is also very effective for data struc tures as often only parts of a data structure are used or modified this will be discussed in more detail in section below but here is already a quick example assume c and c are complex numbers val c if test c else c the conditional will be split for each field of a struct internally the above will be represented as val if test else val if test else dead val c dead the computation of the component as well as the struct creation for the result of the conditional are never used and thus they will be removed by data structure optimizations high level data structures are a of modern programming and at the same time stand in the way of compiler optimizations we the main issues compilers have with data structures as used by the two programming object oriented programming treats every data value as an object this is a powerful pattern that makes it easy to extend languages with new functionality in scala eg it is easy to add a complex number class but there is a price to be allocating each complex number as a separate object will not perform well furthermore if we are working with arrays of complex numbers we get much better performance if we use a struct of array representation staging and embedded compilers allow us to abstract away the object abstraction and reason about individual pieces of data that objects are composed of and possibly that data in more efficient ways fp functional programs create of intermediate results this is particularly bad for collection operations compilers can but impure functional languages need sophisticated effects analysis which is hard if there is a lot of abstraction our staged programs are much simpler because abstraction is away we can do better and simpler effects analysis a simple liveness analysis can turn copying into modification a novel loop fusion algorithm data parallel and asymmetric includes and removes many intermediate results section generic struct interface extends abstract class case class extends case class key string extends def def key string struct match case case key splitting array construction case class len extends def v body body match case p v case v body splitting array access def i a match case p i case def a match case len len case figure generic struct interface and transform a generic struct interface product types ie records or are one of the core data structure building blocks it off to have a generic implementation that comes with common optimizations figure defines two ir node types for struct creation and field access the struct creation node takes a hash map that relates static field identifiers with dynamic values and a tag that can hold further information about the data representation the method field tries to look up the desired value directly if the argument is a struct node with this struct abstraction we can implement the data structure splitting and merging example from section by overriding the ifthenelse smart constructor to create a conditional for each field in a struct unions and inheritance the struct abstraction can be extended to sum types and inheritance using a tagged union approach we add a field to each struct that refers to an expression that defines the objects class being a regular struct field it is subject to all common optimizations we extend the complex number example with two complex cartesian extends complex with struct val re double val im double extends complex with struct val r double val double splitting transforms work as before eg conditional expressions are to the fields of the struct but now the result struct will contain the union of the fields found in the two branches inserting null values as appropriate a conditional is created for the field only if the exact class is not known at staging time given straightforward methods cartesian and the expression val a cartesian val b if x a else b produces this generated code val re im r if x null null else null null rr the fields allows virtual dispatch via type tests and type eg to convert any complex number to its cartesian representation def if else val p pr appropriate rewrites ensure that if the argument is known to be a cartesian the conversion is a the type test that the field is only generated if the type cannot be determined statically if the field is never used it will be removed by struct of array and other data format conversions a natural extension of the splitting mechanism is a generic to transform to the mechanism is analogous to that for conditionals we the array constructor that represents expressions of the form i body to create a struct with an array for each component of the body if the body itself is a struct figure note that we tag the result struct with an to keep track of the transformation we also the methods that are used to access array elements and return the length of an array to do the right thing for transformed arrays the data layout is in many cases consider for example calculating complex ie the sign of the components over a vector of complex numbers def if val c else val c to make the test case more interesting we perform the calculation only in one branch of a conditional val vector i cartesian if test else vector all the real parts remain unchanged so the array holding them need not be at all only the parts have to be transformed the total required memory in half uniform array operations like this are also a much better fit for execution the generated intermediate code is val val array holding values val if test i else note how the conditionals for the re and fields have been eliminated since the fields do not change the initial array contains cartesian numbers only if the struct expression will not be referenced in the final code removes the array in the presence of conditionals that produce array elements of different types a possible optimization would be to use a sparse representation for the arrays that make up the result similar to however all the usual sparse vs tradeoffs apply one concern with data representation conversions is what happens if an array is returned from a staged code fragment to the enclosing program in this case the compiler will generate conversion code to return a plain copy of the data loop fusion and deforestation the use of independent and freely composable traversal operations such as is to explicitly coded loops however naive implementations of these operations would be expensive and of intermediate data structures we present a novel loop fusion algorithm for data parallel loops and traversals the core loop abstraction is loops x g i ex fi generator kinds g collect reduce yield statement xs x contexts e loops and conditionals case for all types of generators loops i e x fi loops i e x fi loops i e x fi e x fi vertical case consume collect loops i e x fi i e x loops i e x fi e x vertical case consume bucket collect loops i e x ki fi i j e y x y loops i e x ki fi e x ki figure loop fusion where s is the size of the loop and i the loop variable ing over s a loop can compute multiple results x each of which is associated with a generator g there are three kinds of generators collect which creates a flat data structure reduce which reduces values with the associative operation or which creates a nested data structure generated values by key and applying g to those with matching key loop bodies consist of yield statements that pass values to generators of this loop or an outer loop embedded in some outer context e that might consist of other loops or conditionals note that a yield statement x does not introduce a binding for x but passes a value to the generator identified by x for bucket yield takes pairs this model is expressive enough to represent many common collection operations i x i x vi i if x vi i val w j x wj i x vi vi operation distinct uses a bucket reduction with function rhs which returns the righthand side of a tuple element to return a flat sequence that contains only the rightmost of a duplicate element other operations are by generalizing slightly instead of implementing a operation that returns a sequence of key pairs we can return the keys and values in data structures for a given selector function f that computes a key from a value the equivalent of is i ks vi vs vi this loop ranges over the size of v and produces two result col ks a flat sequence of keys and vs a nested collections of values that belong to the key in ks at the same index the fusion rules are summarized in figure fusion of two loops is only permitted if there are no other dependencies between def val num sum i val sum i figure from to be optimized the loops for example caused by side effects since we are working with a ir dependency information is readily available vertical fusion which a consumer loop into the producer loop is only permitted if the consumer loop does not have any dependencies on its loop variable other than the consumed collection at this index in a data parallel setting where loops are the producer may not be able to compute the exact index of an element in the collection it is building multiple instances of fi are subject to cse and not evaluated twice substituting xi with fi will remove a reference to x if x is not used anywhere else it will also be subject to within fused loop bodies unifying index variable i and substituting references will trigger the usual forward rewriting and simplification thus fusion not only removes intermediate data structures but also provides additional optimization opportunities inside fused loop bodies including fusion of nested loops fixed size array construction can be expressed as loop case x a case x b case x c and concatenation xs ys as loop case i x case i x these patterns with a consumer will duplicate the consumer code into each match case therefore implementations should impose mechanisms to prevent code explosion code generation does not need to emit actual loops for fixed array constructions but can just produce the right sequencing of yield operations case studies we present several case studies to how our compiler architecture and in particular the staging aspect enable advanced optimizations related to data structures all experiments were performed on a precision tn with two processors and gb of ram scala code was run on the oracle java se runtime environment and the bit server vm with default we ran each application ten times to up the jit and report the average of the last runs for each run we the computational portion of the application linear algebra we first consider two examples using an extended version of the staged linear algebra library presented in section in the first example we use staged transformation partial evaluation rewrite rules and fusion together to optimize a logical operation on a sparse matrix into a much more efficient version that only operates on its nonzero values in the second example we show how staged transformations can specialize loops to different computing devices by selecting a dimension to and expressing the loop as a simple while loop in the source language sparse userdefined operators figure shows an from a application this computes a users extends with struct val val val data val val extends with struct val length val data val indices figure data structure definition for sparse matrix and vector preference for other users given the users previous and a n xn similarity matrix the code is written in a way ­ it works whether is or sparse using generic operations our goal is to transform the function into an efficient sparse operation if is sparse and the function argument to returns when the input is a vector row containing only s by expressing this as a transformation rather than during construction the ir node for can take part in other analyses and prior to being to operate on the underlying arrays figure defines new data structures for sparse matrices and vectors in the manner described in section using these definitions we define the transformed version of as follows def f def len use symbolic evaluation to test the userdefined function f against a argument match case if x true case false if transform to operate only on nonzero values new val length val indices val data val else default staging is the critical that allows this transformation to be written and correctly we first use the struct support with inheritance from section to operate uniformly on and sparse matrices and to obtain access to the underlying representation in order to perform the transformation if the argument is a sparse matrix calls which symbolically evaluates the userdefined function f at staging time to discover if it actually has no effect on empty rows in the argument matrix this discovery is in turn enabled by further rewrite rules that implement symbolic execution by optimizing operations for constant values for example we define sum as def end block match case if x case we are able to successfully perform this transformation of because all rewrite rules as the plain scala staged w fusion w fusion sparse parallel threads figure benchmark the shows the normalized execution time of the on a matrix with various optimizations the represents the percentage of sparse rows in the test data speedup numbers are reported at the top of each bar x input matrix k num clusters c distance from each sample to val returns an j val sum i if ci j xi val d if points else points d figure from clustering to be optimized transformed ir is constructed after the transformation the code is further optimized by together the two sum statements in figure but the fused is still only run on nonzero rows figure shows the results of running the with input data of varying as expected the transformation provides greater benefit with increasing because we from a implementation to a one we also implicitly the transformed map function resulting in good speedup over the sequential automatically loop parallelization for heterogeneous processors the final linear algebra example we will consider is the parallelization of a loop in the wellknown clustering algorithm figure shows the which updates the cluster locations in the current iteration to the mean of all the samples currently assigned to that cluster clustering is a statistical algorithm that is amenable to both parallelization and gpu execution however the statement in figure a challenge each of the k cluster locations can be computed in parallel or we can compute the inner sum statement in parallel since k usually on machines with a smaller number of hardware threads it is better to the outer dimension but on parallel machines such as it is much better to the inner sum dimension if we internally represent parallel operators such as and sum as loops a parallelization framework such as can automatically generate parallel andor gpu code for them using the notation from section we can then define a transformation as follows extends self val t new val ir self def loop boolean def collect construct transformed representation var i val size val res while i size i res extends val ir import ir def stm stm match case if match case case case this transformation the function which is internally represented by a loop into an explicit while loop to expose any nested parallel operators although we it here it is easy to add a heuristic to decide statically if known or dynamically if not whether or not to a loop based on its size note that due to staging the transformed representation can be written as simple source code which is easier to understand than direct ir the transformation is only if we are to generate gpu code if we are generating cpu code we the outer loop to our hardware regular expression string and parsers is a popular benchmark in the partial evaluation literature usually specialization produces a set of mutually recursive functions which is also our starting point however this code does not achieve top performance compared to fast automata libraries we show that treating generated functions as data objects and transforming them into a more efficient form yields performance that that of an optimized library we consider multithreaded regular expression that spawn a new conceptual thread to process alternatives in parallel of course these do not actually spawn threads but rather need to be advanced manually by client code thus they are similar to here is an example for the fixed regular expression def true stop in parallel the first argument to guard is a character c defines a singleton character class and w denotes the wildcard we added combinators on top of the core abstractions that can produce from more conventional regular expressions ca ca cb for the example above we can easily add a parser for textual regular expressions on top of these combinators nfa to dfa conversion internally the given uses an api that models nondeterministic finite automata nfa def k boolean xs match case nil nil case e val xs xs w x case e if contains val xs restrict rest knowing k flag e acc s else val xs restrict rest figure nfa exploration type case class x boolean s def x boolean x s def stop nil an nfa state consists of a list of possible transitions each transition may be guarded by a set and it may have a flag to be if the transition is taken it also knows how to compute the following state for simplicity we use set of characters for the guard and a boolean for the flag but of course we could use generic types as well note that the api does not mention where input is obtained from files streams etc we will translate to using staging the automaton class is part of the dfa api the staged api is just a wrapper case class o next i type type def translating an nfa to a dfa is accomplished by creating a dfa state for each encountered nfa configuration def boolean state val c c the framework functions which with the state eg we need to remove ensures termination if the nfa is in fact finite indeed our approach is conceptually similar to generating a dfa using derivatives of regular expressions both approaches rely on identifying approximately equivalent automaton states furthermore computing a symbolic derivative is similar to the nfa by a symbolic character as explained next we use a separate function to explore the nfa space see figure the automaton by a symbolic character to invoke its continuation k with a new automaton ie the possible set of states after the given implementation needs only to treat the wildcard character set denoted by w the other character sets single character or range are treated and it would be straightforward to add more cases without changing this function the algorithm tries to remove as many redundant checks and impossible branches as possible it relies on def def string boolean boolean val x x char val n matched nothing val x x a if n return false var id val x if x var i x val n else while i x val x first case analysis for next state id x id id match case val x matched nothing val x x a val x x char val x if x matched aa else x case val x val x x char matched aa case matched a matched a i val x val x val x x second case analysis for final boolean flag id x b figure generated code for regular expression optimized code on the right the knowing and binary functions on character sets this only works because the guards are values the generated code is shown in figure on the left each function corresponds to one dfa state one straightforward optimization is to signal early if the boolean flag remains the same in all possible next states to do so we change the so that the output can encode two boolean values the original flag and whether we can stop early this does not help for matching but it does for as we can stop as soon as we find an occurrence of transforming closures our final implementation generates a complete from a string input to a boolean output the generated code does not have functions nor automaton states instead we generate a case statement for each function we do this twice in the middle of matching the input the value of the case analysis is the next functional state while for the last character matching its value is the boolean flag output because all the cases are we achieve early by simply returning from the function with the boolean flag output in the of the first case analysis the optimized code is shown in figure on the right figure shows results we compare our naive implementation staged which includes the early optimization to the and libraries on average our naive implementation is more than x faster than and about x slower than in order to speed up our implementation we with various staged transformations our final implementation staged by nearly a factor of and by a factor of for comparison we also tried a completely plain scala version where we directly evaluate the code instead of staging it and it is x slower than · · · summary staged staged plain scala figure benchmark the first graph shows the relative execution time of matching an or long input string of the form ab on the regular expression the second graph summarizes the relative performance over many different inputs and regular expressions are reported on top of each bar val q filter date map case new val key val val val val figure sample query similar to q of the benchmark collection and query operations next we consider the case of executing queries on collections figure shows a simplified version for illustration purposes of query written using the standard scala collections api a straightforward execution of this query as written will be substantially slower than what could be achieved with an optimized lowlevel imperative implementation in particular executing this code in scala will perform a new array allocation for the output of each collection operation furthermore every element of the arrays will be a heap allocated object since both and the anonymous class for the result are implemented as jvm classes these classes however contain only fields no methods and as such we can apply struct optimizations to them during staging since the fields of these can all be represented as jvm primitive types we can eliminate all of the heap allocated objects at runtime by performing to transformations for each array furthermore since as defined by contains fields while query only accesses of those fields the engine eliminates the arrays of the other fields all together once the form is created fusion optimization in addition we eliminate the intermediate array and map data structures produced by each of the collection operations by operations section at two levels at the inner level creating the output struct requires multiple reduction operations fusion combines the reductions for each field into a single loop on the outer level the query groups the collection according to a key function and then maps each group down to a single element we the and map together by using the rule for a operation essentially the algorithm that the output element of each group can be computed directly by reducing the entire input collection into the defined by the key function of the furthermore the vertical rule the filter with the by the addition of each element plain scala staged staged fusion cpu cpu figure performance of query with staging optimizations the shows the normalized execution time for records with reported at the top of each bar to the appropriate group and after fusion the reduction of each element into the appropriate group finally fusion combines the operations for each field array in the form into a single loop therefore the program produces the output collection directly from the input with a single loop and no intermediate arrays sorting the output yields the final result we perform one final optimization during code generation of the fused loop in order to implement operations we internally allocate a generic instance to the however when n operations with the same key function appear in the same loop we do not allocate a for each operation instead we use a single to determine an array index for each key and then use that index to access the n arrays allocated to hold the result of each operation eliminating the overhead of computing the same bucket location multiple times performance results we evaluate these optimizations in figure the execution times are normalized to the plain scala library implementation of the query leftmost bar the middle bar shows the benefits of staging but without applying transformations or fusion here staging is only applying basic optimizations such as common subexpression elimination and code motion in particular the staged version contains the following key improvements higherorder abstractions such as anonymous functions have been eliminated construction of the date object within the filter has been lifted out of the loop and converted into an int pairs of primitive types have been into a single primitive type with twice the bits eg a is encoded as an int and the chains in the inner loop have been fused since we do not perform transformations in this version the struct is generated as a custom jvm class and the unused fields remain in the generated code these optimizations provide approximately x speedup over the library implementation the rightmost bar shows the results of adding to transformations and all of the fusion optimizations described above namely the filter and map operations into a single loop these optimizations provide approximately x speedup over the basic staging optimizations and x speedup over the library implementation in addition this version better when across multiple as it over the size of the input collection rather than over the number of groups ultimately with staging optimizations and parallel code generation we were able to x speedup with over a singlethreaded library implementation heterogeneous targets to conversion greatly improves the ability to generate code for in this example however the data transfer cost any speedup for runs another interesting target besides and gpu devices are big data cluster frameworks such as or we can use the same collections api to target several different frameworks depth depth depth depth plain scala staged staged fusion figure file system to html left represents the template nesting depth and are omitted due to very short running times commercial right the shows normalized running time compared to plain scala speedup numbers are shown at the top of each bar by adding appropriate a recent paper contains more details and demonstrates good string templates string template libraries are in wide use to transform structured data into a flat string representation eg xml html or while the semantics are simple and easy to express in a functional way template engine implementations are often because data copying and intermediate data structures need to be avoided to obtain good performance we present the core of a template engine based on purely functional collection operations list construction concatenation and with loop fusion section taking care of removing intermediate data structures a simple html can be like this def name name a def def when executing this code without fusion optimization each operation traverses the lists produced by the nested template one more time each operation an additional traversal this results in performance that for fixed output size decreases linearly with the level of template nesting the asymptotic worst case complexity is thus quadratic in the output size whereas an imperative implementation would be linear since all function calls are at staging time our fusion transformation section is able to remove the unnecessary traversals and data structures provided that only core collection operations are used to express templates the generated code traverses the input data structure just once strings directly into a single output buffer benchmark results are reported in figure we demonstrate the ability of obtaining asymptotic on a benchmark that generates html representations of a file system structure with growing levels of nesting with fusion enabled grow with the depth of nesting furthermore we implemented templates to the structure of three public web sites yielding of up to x over the plain scala implementation discussion as shown in section our system is able to achieve order of magnitude and in one case even asymptotic on a range of nontrivial high level programs while we are not aware of any other system that achieves comparable results on a similar range of programs these results bear the question whether they could be achieved with similar effort using other means as suggested by one of the reviewers a simpler design would be to perform optimizations dynamically for example one could implement vectors and matrices as a hierarchy of classes with a special class for zero vectors instead of distinguishing zero vectors statically in the ir this is certainly a valid design but it comes with a number of first runtime introduce overhead indirection and virtual calls which also make it harder for the compiler to generate efficient code second while dynamic information may be more precise than static information at some times the possibilities for dynamic optimizations are very limited in other key aspects for example it is not clear how to integrate or any other optimization that requires nonlocal knowledge liveness fusion another question by a was whether staging really simplifies transformation compared to art rewriting systems such as those included in language like or while these systems are we believe they serve a different purpose and that the capability to remove abstraction at all intermediate levels with strong guarantees about the residual code is hard to in the context of program optimization for example the code in figure transforms symbolic linear algebra operations to the same traversals shown in figure with higher order functions and type class instances removed by staging without compiler analysis staging enables us to express a program or model transformer as an interpreter this is an advantage because writing an interpreter in an expressive language is simpler than implementing a transformer even with good support a key aspect is reuse of abstractions of the meta language for example our recent work on javascript generation using cps transform at to generate asynchronous code that is in continuation passing style cps by contrast which is implemented in has to implement the cps transform for each language construct of the object language working with a ir also makes a difference compared to expression trees most importantly it becomes much easier to implement transforms that require dependency information our fusion algorithm section for example does not require loops to be syntactically close or adjacent to be fused def def count staging will inline the and calls and the fusion algorithm will combine all traversals into a single loop any algorithm that only considers dependent traversal expressions will miss the fusion opportunities across the two statements this is commonly the case for pure frontend approaches based eg on c expression templates another key aspect of our design is the different treatment of transforms handled in separate passes one after the other and optimizations combined into joint simplification passes using speculative rewriting we believe this distinction is essential to make optimizations reliable by avoiding phase ordering problems and making sure that highlevel optimizations are applied before take place finally there is the question of programmer effort required to achieve similar results from our perspective the most important aspects are that the optimizations we present can be applied and that they are for all clients of an optimized library we expect that end user programmers will not usually write any transformations on their own but they can add rewrites if they need library developers who wish to add optimizations need to implement functionality corresponding to the code we show eg in figure but they can do so in a gradual way start with code similar to figure add staging figure and if that is not sufficient add further optimizations figure delayed rewriting section can further reduce the guarantees provided by the type system expressions evaluated at staging time and smart constructors no ir node constructed if rewrites match help ensure that optimizations take place in the intended way as a debugging aid transformed code can be at any time related work extensible compilers have been studied for a long time recent examples in the java world are and the glasgow haskell compiler also allows custom rewrite rules there are also elaborate approaches to library specific optimizations inspired by formal methods is a framework for building that derives automated optimizations from semantics annotations on library abstractions is a domainspecific language for implementing optimizations that are amenable to automated correctness reasoning et al present a method of generating compiler optimizations automatically from program examples before and after a transformation is a parallelization framework for developed on top of previously had a notion of a ir some ir nodes could be viewed from several different at the same time say as a parallel loop and as a matrix operation this caused problems because earlier more highlevel views had to be carried along and or fully exploited information was never discarded limiting the choices of the compiler we have used the techniques presented in this work to extend and to implement domainspecific optimizations similar to those presented in the case studies for existing such as was also used as the parallel execution engine for the case studies in this paper compared to this paper presents a general extensible compiler architecture not limited to domain specific languages previous on presented only the pure frontend single pass instantiation the use of types to define binding times in is inspired by earlier work on finally or polymorphic language embedding et al propose languages as libraries in and make use of macros for translation our approach of using staging is similar in spirit earlier work on realistic compilation by program transformation in the context of scheme was presented by and hudak the ability to compile highlevel languages to programming models has been investigated in several contexts et al embedded compilation and used a simple image synthesis as an example languages is a strategy to automatically generate optimized domainspecific libraries in c expression templates are popular to implement limited forms of domain specific optimizations the is restricted to parts of a larger template expression et al shows a library based approach to translating scala programs to code this is largely achieved through java bytecode translation a similar approach is used by to compile highlevel java code to a hardware description language such as our compiler infrastructure implements a set of advanced optimizations in a fashion related work includes program transformations using advanced rewrite rules combining analyses and optimizations as well as techniques for eliminating intermediate results and loop fusion an interesting alternative to speculative rewriting is equality saturation which encodes many ways to express an operation in the ir as an example of using partial evaluation in program transformation and perform closure sion and tail call introduction by applying offline partial evaluating to a suitable interpreter more recently cook et al perform model transformation by partial evaluation of model interpreters partial evaluation corresponds to constant folding and tion whereas our approach allows arbitrary compiler optimizations arbitrary computation at time to remove ab overhead and provides strong guarantees about what be comes part of the residual code type residual vs t static conclusion we have demonstrated a compiler architecture that achieves order of magnitude on highlevel programs by collection operations changing data layout and applying further tions on highlevel objects enabled by intermediate languages with staging and a facility to combine optimizations without phase or problems our system combines several novel pieces with existing techniques which together provide optimization power that is greater than the sum of the parts acknowledgments we thank the popl reviewers for their valuable feedback and suggestions this research was by the european re under grant through oracle order us contract nsf grant the stanford per parallelism lab program supported by oracle intel and additional support from oracle references s v t and m odersky an embedded for high performance big data processing m s o danvy and h k fast partial evaluation of pattern matching in strings acm trans program lang syst ­ j d f p and r a and language for heterogeneous architectures oopsla m a van k and e program transformation with dynamic rewrite rules inf ­ july k j a k h lee t h m odersky and k a heterogeneous parallel framework for domainspecific languages j a derivatives of regular expressions j acm ­ c calcagno w l and x leroy implementing languages using and reflection in j o and c finally partially evaluated staged interpreters for simpler typed languages j program ­ c and k d cooper combining analyses combining optimizations acm trans program lang syst ­ march c and o danvy partial evaluation of pattern matching in strings inf process ­ w r cook b t a and b model transformation by partial evaluation of model interpreters technical report tr ut austin department of computer science d r and d stream fusion from lists to streams to nothing at all in icfp t and g the system modular extensible compiler construction sci comput program ­ c s and o de compiling embedded languages in w editor semantics applications and implementation of program generation volume of lecture notes in computer science pages ­ springer berlin y partial evaluation of computation process an approach to a higherorder and symbolic computation ­ c k and sb fusion for data locality and parallelism d m z l c l and e a domainspecific language for dynamic web applications in oopsla companion c k t and a polymorphic embedding of meta programming system url n d jones c k and p partial evaluation and automatic program generation prenticehall inc upper nj usa s l p jones r g and m m t the nested data parallelism in haskell in s p jones a and t hoare by the rules rewriting as a practical optimisation technique in ghc haskell s j j s w j s smith and t j array design and expression evaluation in ii in l c l and e the language rules for declarative specification of languages and in companion r and p hudak realistic compilation by program transformation in popl k kennedy b a r j c c and j languages a system for automatic generation of domain languages proceedings of the ieee ­ g n t and m odersky javascript as an embedded in ecoop h lee k j a k h t m odersky and k implementing domainspecific languages for heterogeneous parallel computing ieee ­ s d and c chambers composing dataflow analyses and transformations sigplan not ­ january s t d and c chambers automatically proving the correctness of compiler optimizations in pldi a ­ finitestate automata and regular expressions for java a t p and m odersky n m r and a c myers an extensible compiler framework for java in cc n d white and k runtime compilation for in scala s j and a derivatives j program ­ mar d j m q yi and a classification and of abstractions for optimization in preliminary proceedings t lightweight modular staging and embedded compilers abstraction without for highlevel pro gramming phd thesis t and m odersky lightweight modular staging a pragmatic approach to runtime code generation and compiled t and m odersky lightweight modular staging a pragmatic approach to runtime code generation and compiled acm ­ t i and m odersky implementing firstclass polymorphic delimited continuations by a typedirected selective in icfp t a k h lee k j h m odersky and k for performance oriented a and w r cook hybrid partial evaluation oopsla m and p realistic compilation by partial evaluation in pldi a k h lee k j t m wu a r m odersky and k an implicitly parallel domainspecific language for machine learning e and n kobayashi a hybrid approach to online and offline partial evaluation higherorder and symbolic computation ­ w and t and programming with explicit annotations theor comput sci ­ r m z and s equality saturation a new approach to optimization in popl r m and s generating compiler optimizations from proofs in popl p and d partial evaluation for higherorder languages with state technical report url s v r m and m felleisen languages as libraries pldi t l expression templates c inc new york ny t l arrays in in t l and j g siek combining optimizations combining theories technical report university p wadler deforestation transforming programs to eliminate trees theor comput sci ­ p wadler and s how to make adhoc polymorphism less adhoc in popl 