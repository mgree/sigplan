the geometry of types di inria abstract we show that time complexity analysis of higherorder functional programs can be effectively reduced to an simpler although computationally equivalent verification problem namely checking firstorder inequalities for validity this is done by giving an efficient inference algorithm for linear dependent types which given a pcf term produces in output both a linear dependent type and a cost expression for the term together with a set of proof obligations actually the output type judgement is derivable iff all proof obligations are valid this coupled with the already known relative completeness of linear dependent types ensures that no information is lost ie that there are no false positives or moreover the procedure reflects the difficulty of the original problem simple pcf terms give rise to sets of proof obligations which are easy to solve the latter can then be put in a format suitable for automatic or verification by external solvers experimental evaluation has produced results which are briefly presented in the paper categories and subject descriptors f logics and meanings of programs semantics of programming analysis f logics and meanings of programs specifying and verifying and reasoning about programs general terms performance theory verification keywords functional programming higherorder types linear logic resource consumption complexity analysis introduction one of the most crucial properties of programs is the amount of resources like time memory and power they need when executed deriving upper bounds on the resource consumption of programs is crucial in many cases but is in fact an undecidable problem as soon as the underlying programming language is nontrivial if the units of measurement in which resources are measured become concrete and close to the physical ones the problem becomes even more complicated given the many transformation and optimisation layers programs are applied to before being executed a typical example is the one of techniques adopted in realtime systems which do not only need to deal with how many machine instructions a program corresponds to but also with how much time each instruction costs when executed by possibly permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ italy copyright c acm complex architectures including caches pipelining etc a task which is even harder with the current towards architectures a different approach consists in the abstract complexity of programs as an example one can take the number of instructions executed by the program as a measure of its execution time this is of course a less informative metric which however becomes more accurate if the actual time complexity of each instruction is kept low one advantage of this analysis is the independence from the specific hardware platform executing the program at hand the latter only needs to be once a variety of verification techniques have been employed in this context from abstract interpretation to type systems to program logics to interactive theorem proving among the many typebased techniques for complexity analysis a recent proposal consists in going towards systems of linear dependent types as suggested by and the first author in linear dependent type theories a judgement has the form i t where is the type of t and i is its cost an of its time complexity in this paper we show that the problem of checking given a pcf term t and i whether i t holds can be efficiently reduced to the one of checking the truth of a set of proof obligations themselves formulated in the language of a firstorder equational program interestingly simple terms give rise to simple equational programs in other words linear dependent types are not only a sound and relatively complete methodology for inferring time bounds of programs they also allow to reduce complexity analysis to an simpler although computationally equivalent problem which is much better studied and for which a variety of techniques and concrete tools exist the bounds one obtains this way translate to bounds on the number of steps performed by evaluation machines for the calculus which means that the induced metrics are not too abstract after all the type inference algorithm is described in section the scenario then becomes similar to the one in program logics for imperative programs where completeness holds at least for the simplest idioms and weakest preconditions can be generated automatically see eg a benefit of working with functional programs is that type inference the analogue of generating can be done without the need of invariants linear dependent types are simple types annotated with some index terms ie firstorder terms the value of data inside the program type inference produces in output a type derivation a set of inequalities which should be thought of as proof obligations and an equational program e giving meaning to function symbols appearing in index terms see figure a natural thing to do once e and the various proof obligations are available is to try to solve them automatically as an example through smt solvers if automatically checking the inequalities for truth does not a detailed discussion with related work is in section · · · e fx gx x · · · tt e k t e fx h gx y l figure general scheme of the type inference algorithm succeed which must happen in some cases one can find useful information in the type derivation as it tells you precisely which data every symbol corresponds to we elaborate on this issue in section but where does linear dependency come from linear dependent types can be seen as a way to turn geometry of interaction or equivalently games into a type system for the calculus the equational program one obtains as a result of type inference of a term t is nothing but as a description of a token machine for t in presence of linear dependency any term which can possibly be duplicated can receive different although uniform types similarly to what happens in as such this form of dependency is significantly simpler than the one of eg the calculus of inductive constructions linear dependency at a traditionally type systems carry very little information about the value of data manipulated by programs instead focusing on their nature as an example all partial recursive functions from natural numbers to natural numbers can be typed as nat ñ nat in the calculus with natural numbers and higherorder recursion also known as pcf this is not an limit of the typebased analysis of programs however much richer type disciplines have in the last years all of them guarantee stronger properties for typable programs the price being a more complicated type language and computationally more difficult type inference and checking problems as an example types are a way to ensure termination of functional programs based on size information in systems of types a program like t x can be typed as ñ ñ and in general as ñ ñ where i a b in other words the pcf type nat is refined into where i is an expression whose semantics is the set of all natural numbers smaller or equal to i ie the interval r is n the role of size information is to ensure that all functions terminate and this is done by restricting the kind of functions of which one is allowed to form fixpoints types are nonlinear arguments to functions can be freely duplicated moreover the size information is only approximate since the expression labelling base types is only an upper bound on the size of typable values linear dependent types can be seen as a way to precision and linearity into types indeed t receives the following type in d ci dj as bs a b s as one can easily hs is the type of all natural numbers in the interval rk hs n moreover bj is the type of linear functions from to which can be copied by the environment j times the j copies of the function have types obtained by substituting j ´ for b in and this is the key idea behind linear dependency the type is but can be easily turned into ci dj as bs b a b s itself a type of t in the following the singleton interval type ks is denoted simply as notice that linear dependency is not exploited in eg d does not appear free in nor in b s yet precisely captures the functional behaviour of t if d does not appear free in nor in then di can be abbreviated as i linear dependency becomes necessary in presence of higherorder functions consider as another example the term u xy y then else xy u has simple type ñ ñ nat ñ nat one way to turn it into a linear dependent type is the following h kl where j equals when a and j equals i otherwise actually u has type for every i and j provided the two expressions are in the appropriate relation now consider the term v pred px id u the same variable x is applied to the identity id and to the predecessor pred which type should we give to the variable x and to u then if we want to preserve precision the type should reflect both uses of x the right type for u is actually the following c where both i and j evaluate to a if c and to a ´ otherwise if id is replaced by succ in the definition of v then becomes even more complicated the first copy of j is not with a but with either or a linear dependency precisely consists in allowing different copies of a term to receive types which are indexed differently although having the same functional skeleton and to represent all of them in compact form this is in contrast to eg intersection types where the many different ways a function uses its argument could even be structurally different this as we will see in section has important consequences on the kind of completeness results one can hope for if the language in which index terms are written is sufficiently rich then the obtained system is complete in an intensional sense a precise type can be given to every terminating t having type nat ñ nat linear dependency allows to get precise information about the functional behaviour of programs without making the language of types too different from the one of simple types eg one does not need to quantify over index variables as in types the price to pay however is that types and especially higherorder types need to be context aware when you type u as a subterm of v see above you need to know which arguments u will be applied to despite this a compositional type inference procedure can actually be designed and is the main technical contribution of this paper linearity abstract machines and the complexity of evaluation why dependency but linearity are so useful for complexity analysis actually typing a term using linear dependent types requires finding an upper bound to the number of times each value is copied by its environment called its potential in the term v from the example above the variable x is used twice and accordingly one finds c in of higherorder values occurring in a term are crucial parameters for the complexity of evaluating the term by abstract mechanisms the following is an but necessarily informal discussion about why this is the case configurations of abstract machines for the calculus like friedman and and can be thought of as being into two distinct parts · first of all there are entities which are either copied entirely or turned into entities this includes in particular terms in socalled environments each higherorder entity is a subterm of the term the computation started from · there are entities that the machine uses to look for the next redex to be typically these entities are the current term and possibly the stack the essential feature of entities is the fact that they are consumed by the machine the search for the next redex consists in traversing the entities until a redex is found or a entity needs to be turned into a one as an example consider the process of evaluating the pcf term pf pf q then else f pf x q by an appropriate of the see figure initially the whole term is by into it the machine finds a first redex u at that point becomes the obtained closure itself becomes part of the environment and the machine looks into the body of t ending up in an occurrence of f which needs to be replaced by a copy of pf q after an instantiation step a new entity pf q indeed appears note that by an easy combinatorial argument the number of machine steps necessary to reach f is at most proportional to the size of the starting term tu since reaching f requires entities which can only be created through instantiations after a copy of pf q becomes some additional becomes available but not too much x y is after all a subterm of the initial term the careful reader should already have the of this when the time complexity of evaluation we could limit ourselves to counting how many instantiation steps the machine performs as opposed to counting all machine steps we claim on the other hand that the number of instantiation steps equals the sum of of all values appearing in the initial term something that can be easily inferred from the kind of precise linear typing we were about at the beginning of this section up once a dependently linear type has been attributed to a term t the time complexity of evaluating t can be derived for free not only an expression bounding the number of instantiation steps performed by an abstract machine evaluating t can be derived but it is part of the underlying type derivation essentially as a consequence reasoning automatically or not about it can be done following the structure of the program programs and types formally in this section we present some details of d pcf a system of linear dependent types for pcf two versions exist d and d corresponding to callbyname and callbyvalue evaluation of terms respectively the two type systems are different but the underlying idea is basically the same we give here the details of the cbv version which better corresponds to about evaluation but also provide some about the setting terms and terms are given by the usual pcf grammar s t u x n t u xt fix xt t then u else s a value denoted by v w etc is either a primitive integer n or an abstraction xt or a fixpoint fix xt in addition to the usual terms of the calculus there are a fixpoint construction primitive natural numbers with predecessor and successor and conditional branching with a test for zero for instance a simple program computing addition is the following add fix x yz y then z else language of as explained informally in section a type in d pcf consists in an annotation of a pcf type where the annotation consists in some the latter are by a set of index variables v ta b c u and an untyped signature of function symbols denoted by f g h etc we assume contains at least the arithmetic symbols ´ and and we write n for ¨ ¨ ¨ n times are then constructed by the following grammar ij i j k a j k ai a where n is the arity of f in free and bound index variables are defined as usual taking care that all free occurrences of a in a j are bound in variable a by j both in i substitution of an equational program e a meaning in nn n to some symbols of arity n and a valuation mapping index variables to n the semantics i e of an index i is either a natural number or undefined let us describe how we interpret the last two constructions namely bounded sums and forest bounded sums sum of all possible to i describing the meaning of forest on the other hand requires some counts the number of nodes effort informally the in a forest composed using k each node in the forest is uniquely identified by a natural number starting from i and the tree in pre order the index k has the role of describing the number of of each forest node eg the number of children of the node is consider the following forest two trees and consider an index k with a free index variable a such that for n p t u when n p t f pf q then else f pf q u x f ÑÞ x x y x ÑÞ x y y x tu y x t y u y ¨ x u y t y ¨ x x y y y ¨ t y ¨ ° x x y x ÞÑ x y y t y ¨ x pf q then else f pf q y x f y fork x f pf q y ¨ x f y y ¨ fork x f pf q y ¨ x x y x ÞÑ x y y y ¨ fork x f pf q y ¨ ° x add x y py ÑÞ x y x ÞÑ x y fork x f pf q y ¨ ° x y fork x f pf q y ¨ x f pf q y x f y f y ¨ x x y x ÑÞ x y y f y ¨ x f y x y x ÞÑ x y y ¨ ° x y x y x ÑÞ x y y ¨ ° x y figure evaluation of a term in the abstract machine t u and when n p t u that is k describes the number of children of each node then Ï a k since it takes into account the entire forest Ï a k since it takes into account only the leftmost tree Ï a k since it takes into account only the second tree of since it takes into account only the k within the dashed rectangle one may what is the role of forest in the type system actually they play a crucial role in the treatment of recursion where the unfolding of recursive calls produces a tree like structure whose size is just the number of times the recursively defined function will be used globally notice that i e is undefined whenever the equality between i and any natural number cannot be derived from the underlying equational program in particular a forest cardinality may be even if all its subterms are defined has no value because the corresponding as an example i tree consists of an chain and its cardinality is infinite by the way i is the index term describing the structure of the recursive calls induced by the program fix xx semantic judgements a constraint is an inequality on a constraint i j is valid for and e when both i e and j e are defined and i e j e as usual we can derive a notion of equality and strict inequality from a semantic judgement is of the form e i j where is a set of constraints and is the set of free index variables in i and j these semantic judgements are used as axioms in the typing derivations of d pcf and the set of constraints called the index context contains mainly some of bounds for the free index variables such as a k such a judgement is valid when for every valuation Ñ n if all constraints in are valid for e and then so is i j types remember that d pcf is at controlling the complexity of programs the time complexity of the evaluation is thus statically while typing the term at hand the grammar for types distinguishes the subclass of linear types which correspond to terms see section and the one of modal types for terms in d they are defined as follows a b linear types ra is ¨ a js modal types indeed cbv evaluation only values if such a value has an arrow type then it is a function either an abstraction or a fixpoint that can potentially increase the complexity of the whole program if we duplicate it hence we need a bound on the number of times we instantiate it if we want to keep the overall complexity under control this bound call the potential of the value is represented by i in the type ra is ¨ p q also written ai as explained in section js is the type of programs evaluating to a natural number in the closed interval ri js the potential of natural number values is not specified as they can be freely duplicated along cbv evaluation types intuitively the modal type ra is ¨ a is assigned to terms that can be copied i times the kth copy being of type ´ au for those readers who are familiar with linear logic can be thought of as representing the type b ¨ ¨ ¨ b ´ au in the typing rules we are going to define modal types need to be manipulated in an algebraic way for this reason two operations on modal types are required the first one is a binary operation z on modal types suppose that ra is ¨ and that ra js ¨ in other words consists of the first i instances of a ie b ¨ ¨ ¨ b ´ cu while consists of the next j instances of a ie cu b ¨ ¨ ¨ b j ´ cu their sum z is naturally defined as a modal type consisting of the first i j instances of a ie rc i js ¨ a furthermore js z js is just js a bounded sum operator on modal types can be defined by the idea above suppose that rb js ¨ a b da subtyping central to d pcf is the notion of subtyping an inequality relation between linear or modal types can be defined using the e k i e j h e js hs e e e pa q pa j q e a b e j i e ra is ¨ a ra js ¨ b figure subtyping derivation rules of d formal system in fig this relation corresponds to lifting index inequalities to the type level as defined here is a preorder ie a reflexive and transitive relation which allows to with approximations in the typed analysis of programs however in the type inference algorithm we will present in the next section only the symmetric closure of called type equivalence will be used this ensures that the type produced by the algorithm is precise typing a typing judgement is of the form ke t where k is the weight of t that is informally the maximal number of substitutions involved in the cbv evaluation of t including the potential substitutions by t itself in its evaluation context the index context is as in a semantic judgement see section and is a term context assigning a modal type to at least each free variable of t both sums and bounded sums are naturally extended from modal types to contexts with for instance tx y u z tx z u tx z y z u there might be free index variables in and k all of them from typing judgements can be derived from the rules of figure observe that in the typing rule for the abstraction i repre the number of times the value xt can be copied its weight that is the number of substitutions involving xt or one of its sub terms is then i plus for each of these copies the weight of t in the typing rule app on the other hand t is used once as a function without been copied its potential needs to be at least the typ ing rule for the fixpoint is the most complicated one as a first approximation assume that only one copy of fix xt will be used that is k and a does not occur free in b to compute the weight of fix xt we need to know the number of times t will be copied during the evaluation that is the number of nodes in the tree of its recursive calls this tree is described by the index i as explained in section since each occurrence of x in t stands for a recursive call it has h Ï b i nodes at each node b of this tree there is a copy of t in which the occurrence of x will be replaced by the son of b thus have to correspond which ie by b is what the rule now if fix xt is in fact at being copied k times then all the copies of t are represented by a forest of k trees described by i for the sake of simplicity we present here the type system with an explicit subsumption rule the latter allows to relax any bound in the types and the weight thereby some precision in the information provided by the typing judgement however we could alternatively replace this rule by the premises of all the other ones which corresponds to the presentation of the type system given in or in for d restricting subtyping to type equivalence amounts to considering types up to index equality in the type system of figure without the rule this is what we do in the type inference algorithm in section in this case we say that the typing judgements are precise definition a derivable judgement ei t is precise if ej t is derivable e e e i j callbyvalue vs callbyname in d the syntax of terms and of is the same as in d but the language of types differs a b a js ra is ¨ a linear types modal types modal types still represent terms except that now not only values but any argument to functions can be duplicated so modal types only occur in negative position in arrow types in the same way one can find them in the context of any typing judgement x xn n ek t a when a term is typed it is a priori not and its type is linear it is turned into a term when it holds the argument position in an application as a consequence the typing rule app becomes the most one for the weight in d the whole context used to type the argument has to be duplicated whereas in d this duplication of context is in the typing rules for values the readers who are familiar with linear logic could have noted that if we replace modal types by types and we remove all annotations with then d corresponds to the target fragment of the translation from simplytyped calculus to ll and d to the target of the cbv translation in d the weight k of a typing judgement represents the maximal number of substitutions that may occur in the evaluation of t we do not detail the typing rules of d here they can be found in however an important remark is that in d just like in d some semantic judgements can be found in the axioms of a typing derivation and every typing rule is reversible except subsumption the type inference algorithm for d that we present in section can be easily adapted to d abstract machines the evaluation of pcf terms can be simulated through an extension of abstract machine for evaluation or through an extension of felleisen and machine for cbv evaluation both these machines have states in the form of processes that are pairs of a closure ie a term with an environment defining its free variables and a stack representing the evaluation context in the these objects are given by the following grammar closures environment c x t y tx ÑÞ c ¨ ¨ ¨ xk ÑÞ stacks t y ¨ s ¨ p ¨ processes fork xt u y ¨ p c when the environment is empty we may use the notation x t y instead of x t h y for closures the evaluation rules of the are given in figure the fourth evaluation rule is said to be an instantiation step the value of a variable x is replaced by the term x maps to in the underlying environment the which performs cbv evaluation is slightly more complex within closures the value closures are those whose first x e x ei t e e t e i j pa q pa i q x ke t ai e k xt ra is ¨ p q ke t ra s ¨ z ke h he tu u me t ks pj q en u pk z t then u else s q en s e n ns em em t js j s em me t js ´ j ´ s pb q pb h q x ra is ¨ a ej t ra s ¨ b bh e j pa b fix xt q pa ra i ks ¨ b h q e i b bu a where h b i figure typing rules of d component is a value v x v y remember that a value v is of the form n xt or fix xt moreover environments assign only value closures to variables tx ÞÑ v ¨ ¨ ¨ xk ÞÑ the grammar for stacks is the same with one additional construction ¨ that is used to a function lambda abstraction or fixpoint while its argument is computed indeed the latter cannot be substituted for a variable if it is not a value evaluation rules for processes are the same as the ones in figure except that the second and the third ones are replaced by the following v ¨ c ¨ v xt y ¨ x t x ÑÞ v ¨ y v fix xt y ¨ x t x ÞÑ x fix xt y ¨ y ¨ an example of the evaluation of a term by the can be found in figure we say that a term t evaluates to u in an abstract machine when x t y x u y observe that if t is a closed term then u the same when the number of steps is not relevant abstract machines and weight the weight of a typable term was informally presented as the number of instantiation steps in its evaluation abstract machines enable a more precise formulation of this idea fact if t u and ie t a is derivable in d then i e is an upper bound for the number of instantiation steps in the evaluation of t by the if t u and ie t ra s ¨ a is derivable in d then i e is an upper bound for the instantiation steps in the evaluation of t by the this can be shown by extending the notion of weight and of typing judgement to stacks and processes and is the main for proving intensional soundness see section key properties in this section we briefly recall the main properties of d pcf for its as a methodology for complexity analysis we give the results for d but they also hold for d all proofs can be found in the subject reduction property guarantees as usual that typing is correct with respect to term reduction but specifies also that the weight of a term cannot increase along reduction proposition subject reduction for any tu if h ei t is derivable in d and if t Ñ u in cbv then h u is also derivable for some j such that e j i as a consequence the weight does not tell us much about the number of reduction steps a typable term to its normal form socalled intensional soundness on the other hand allows to deduce some sensible information about the time complexity of evaluating a typable pcf program by an abstract machine from its d pcf typing judgement proposition intensional soundness for any term t if ek t js is derivable in d then t evaluates to n in k steps in the with i e n j e and k t ¨ p k e q intensional soundness guarantees that the evaluation of any program typable in d pcf takes at most a number of steps directly proportional to both its syntactic size and its weight a similar theorem holds when t has a functional type if as an example the type of t is then k is parametric on a and pt q ¨ p k e q is an upper bound on the complexity of evaluating t when with any integer a but is d pcf powerful enough to type natural complexity bounded programs actually it is as powerful as pcf itself since any pcf type derivation can be turned into a d pcf one for an expressive enough equational program as by the type inference algorithm section we can make this statement even more precise for terms of base or first order type provided two conditions are satisfied · on the one hand the equational program e needs to be universal meaning that every partial recursive function is representable by some index term this can be guaranteed as an example by the presence of a universal program in e · on the other hand all true statements in the form e i j must be available in the type system for completeness to hold in other words one cannot assume that those judgements are derived in a given recursively enumerable formal system because this would violate incompleteness theorem in fact in d pcf completeness theorems are relative to an oracle for the truth of those assumptions which is precisely what happens in logics x tu y xt y u y ¨ x xt y ¨ x t px ÞÑ cq ¨ y x fix xt y x t px ÞÑ x fix xt ¨ y xx y x t then u else s y xt y fork xu s y ¨ x y fork xt u y ¨ xt y x n y fork xt u y ¨ xu y x y xt y x y xt y xn y x n y xn y x y figure evaluation rules proposition relative completeness if e is universal then for any pcf term t if t m then ek t is derivable in d if for any n p n there exist kn mn such that t n mn then there exist i and j such that a h h ei t rb s ¨ is derivable in d with e mn and e kn for all n p n the careful reader should have that there is indeed a gap between the lower bound provided by completeness and the upper bound provided by soundness this is indeed the reason why our complexity analysis is only meaningful in an asymptotic sense sometimes however programs with the same asymptotic behavior can indeed be distinguished eg when their size is small relative to the constants in their weight in the next section we will see how to make a concrete use of relative completeness indeed we will describe an algorithm that given a pcf term returns a d pcf judgement ek t for this term where e is equational program that is not universal but expressive enough to derive the typing judgement to with the relative part of the result ie the very strong assumption that every true semantic judgement must be available the algorithm also returns a set of side conditions that have to be checked these side conditions are in fact semantic judgements that act as axioms of instances of the subsumption rule in the typing derivation relative type inference given on the one hand soundness and relative completeness of d pcf and on the other undecidability of complexity analysis for pcf programs one may whether looking for a type inference procedure makes sense at all as in the introduction we will not give a type inference algorithm per se but rather reduce type inference to the problem of checking the validity of a set of inequalities modulo an equational program see figure this is the reason why we can only claim type inference to be solvable in a relative sense ie assuming the existence of an oracle for proof obligations why is solving relative type inference useful suppose you have a program t nat ñ nat and you want to prove that it works in a number of steps bounded by a polynomial p n Ñ n eg ¨ x you could of course proceed by building a d pcf type derivation for t by hand or even reason directly on the complexity of t relative type inference simplifies your life it outputs an equational program e a precise type derivation for t whose conclusion is a h h ei t and a set i of inequalities on the same signature as the one of e your original problem then is reduced to verifying i y ti this is an easier problem than the original one first of all it has nothing to do with complexity analysis but is rather a problem about the value of expressions secondly it only deals with firstorder expressions an informal account from the brief discussion in section it should be clear that a compositional type inference procedure for d pcf is nontrivial the type one assigns to a subterm depends on the ways the rest of the program uses the subterm the solution we adopt here consists in allowing the algorithm to return partially unspecified equational programs e as produced in output by t gives meaning to all the symbols in the output type derivation except those occurring in negative position in its conclusion to better understand how the type inference algorithm works let us consider the following term t uv the subterm u can be given type ñ ñ nat ñ nat in pcf while v has type nat ñ nat this means t as a whole has type nat ñ nat and computes the function x ÞÑ ¨ x the type inference algorithm proceeds by giving types to u and to v separately then the two into one suppose we start with v the type inference algorithm refines nat ñ nat into and the equational program av which gives meaning to g in terms of f observe how both f and h are not specified in av because they appear in negative position in intuitively corresponds to the arguments v will be applied to while is the number of times v will be used notice that everything is on a which is something like a global parameter that will later be set as the input to t the function u on the other hand is given type b b b b the newly introduced function symbols are subject to the following equations ¨ b cq b cq b cq b c q b c q b cq again notice that some functions are left unspecified namely l m q and k now a type for uv can be found by just combining the types for u and v following the typing rule for applications first of all the number of times u needs to be copied is set to by the equation then the matching symbols of u and v are defined one in terms of the others q this is the last step of type inference so it is safe to that q and that cq a thus obtaining a fully specified equational program e and the following type for t c as an exercise the reader can verify that the equational program above allows to verify that q a and that a h h e t before on the description of the type inference algorithms some preliminary concepts and ideas need to be introduced and are the topic of this section getting of subsumption the type inference algorithm takes in input a pcf term t and returns a typing judgement j for t together with a set r of socalled side conditions we will show below that j is derivable iff all the side conditions in r are valid moreover in this case j is precise see definition all occurrences of the base type js are in fact of the form and the weight and all h occurring in a subtype ra hs ¨ a are kept as low as possible concretely this means that there is a derivation for j in which the subsumption rule is restricted to the following form e ie t e e i j ej t the three premises on the right down to a set of semantic judgements of the form e ki hi see figure where the are occurring in or or i itself and the his occur in or or are j itself if the equalities ki hi can all be derived from e then the three premises on the right are equivalent to the conjunction on i of the following properties hi e is defined for any Ñ n satisfying see section given e this property called a side condition is denoted by hi Ó actually the type inference algorithm does not verify any semantic or subtyping judgement coming from instances of the subsumption rule instead it turns all index equivalences hi ki into rewriting rules in e and put all side conditions hi Ó in r if every side condition in r is true for e we write e r informally this means that all assumed by the algorithm are indeed valid function symbols types and judgements manipulated by our type inference algorithm have a very shape in particular not every index term is allowed to appear in types and this property will be crucial when showing soundness and completeness of the algorithm itself definition primitive types a type is primitive for when it is on the form or a b with a and b primitive for or ra ¨ a with a r and a primitive for a a type is said to be primitive when it is primitive for some as an example a primitive type for a b is b b informally then a type is primitive when the only allowed index terms are function symbols with the appropriate arity equational programs the equational program our algorithm constructs is in fact a rewriting program every equality corresponds to the partial definition of a function symbol and we may write it j where all free variables of j are in ta if there is no such equation in the rewriting program we say that f is unspecified an equational program e is completely specified if it allows to deduce a precise meaning namely a partial recursive function for each symbol of its underlying signature written e ie none of the symbols in e are unspecified in other words a completely specified equational programs has only one model on the other hand a partially specified equational program ie a program where symbols can possibly be unspecified can have many models because partial recursive functions can be assigned to unspecified function symbols in many different ways all of them consistent with its equations up to now we only with completely specified programs but allowing the possibility to have unspecified symbols is crucial for being able to describe the type inference algorithm in a simple way in the following e and f denote completely specified equational programs while a and b denote rewriting programs that are only partially specified definition model of a rewriting program an interpretation µ of a in e is simply a map from unspecified symbols of a to on the signature e such that if f has arity n then is a term in e with free variables from f ta when such an interpretation is defined we say that e is a model of a and we write µ e ù a notice that such an interpretation can naturally be extended to arbitrary index terms on the signature a and we assume in the following that a rewriting program and its model have disjoint signatures definition validity in a model given µ e ù a we say that a semantic judgement a i j is valid in the model notation µ i j when f i j where f a y e y q f is unspecified in au this definition is naturally extended to side conditions with µ r for f r note that if a is a completely specified rewriting program then any model µ e ù a has an interpretation µ with an empty domain and µ r iff a r still assuming that a and e are disjoint as already mentioned the equational programs handled by our type inference algorithm are not necessarily completely specified function symbols which are not specified are precisely those occurring in negative position in the judgement produced in output this invariant will be very useful and is captured by the following definition definition positive and negative symbols given a primitive type the sets of its positive and negative symbols denoted by and ´ respectively are defined inductively by h ra ¨ p q ´ y ra ¨ p y y ´ then the set of positive resp negative symbols of a judgement ie t is the union of all negative resp positive symbols of the is and all positive resp negative symbols of in t are indicated with symbols like p q given such a p the opposite polarity is p definition specified symbols types and judgments given a set of function symbols s a symbol f is said to be ps when there is a rule j in a such that any function symbol appearing in j is either f itself or in s or a symbol that is ps y remember that when there is no rule j in a the symbol f is unspecified in a a primitive type is said to be pp s when all function symbols in p are ps and all symbols in p are unspecified a judgement ia t is correctly specified when and all types in are primitive for and is p n and all types in are n and all function symbols in i are pn where n is the set of negative symbols of the judgement in other words a judgement is correctly specified if the underlying equational program possibly recursively defines all symbols in positive position depending on those in negative position the structure of the algorithm the type inference algorithm receives in input a pcf term t and returns a d pcf judgement h ke t for it together with a set of side conditions r we will prove that it is correct in the sense that the typing judgement is derivable iff the side conditions hold the algorithm proceeds as follows compute a pcf type derivation for t by structural induction on construct a d pcf derivation for t call it dv and the corresponding set of side conditions r returns r and the conclusion of dv the skeleton or of a modal type resp of a linear type a is obtained by all its and its bounds ra is the skeleton of a d pcf derivation is obtained by replacing each type by its skeleton and all the subsumption rules in pcf the type inference problem is decidable and step raises no difficulty actually one could even assume that the type attributes to t is principal the core of the algorithm is of course step in section we define a recursive algorithm gen that build dv and r by annotating the algorithm gen itself relies on some auxiliary algorithms which will be described in section below all algorithms we will talk about have the ability to generate fresh variables and function symbols strictly speaking then they should take a counter or anything similar as a parameter but we this for the sake of simplicity also we assume the existence of a function p t q that given a set of index variables and a pcf type t returns a modal type primitive for containing only fresh function symbols and such that pr sq t auxiliary algorithms and linear logic the design of systems of linear dependent types such as d and d is strongly inspired by itself a restriction of lin logic actually the best way to present the type inference algorithm consists in first of all introducing four auxiliary algorithms each corresponding to a principle the behaviour of the exponential connectives in linear logic notice that these auxiliary algorithms are the main of both d and d type inference consistently to what we have done so far we will prove and explain them with d in mind all the auxiliary algorithm we will talk about in this section will take a tuple of d types as first argument we assume that all of them have the same skeleton and moreover that all index terms appearing in them are pairwise distinct is the following principle any object say of type a can be made linear of type a that is to say a Ñ a in d pcf being means having a modal type which also contains some quantitative information namely how many times the object can be duplicated at most in d can be simply seen as the principle ra s ¨ a Ñ and is implicitly used in the rules app and fix along the type inference process as a consequence we often need to create fresh instances of in the form of pairs of types being in the correct semantic relation this is indeed possible lemma there is an algorithm der such that given two types primitive for and primitive for a of the same skeleton q a pq pa rq where for every e a if e r then e whenever e where pr sq there is µ e ù a such that µ µ and µ r is pp p and is p p p the algorithm der works by recursion on the pcf type and has thus linear complexity in the proof of lemma see as a consequence proceeds by induction on the structure of contraction another key principle in linear logic is contraction according to which two copies of a object can actually be produced a contraction is used in binary rules like app or if in the form of the operator z this time we need an algorithm ctr which takes three linear types a b and c all of them primitive for pa q and turn them into an equational program and a set of side conditions b cq pi a pq pa rq the parameters i and j are index terms capturing the number of times b and c can be copied a lemma to can indeed be proved about ctr in particular for any e a if e r then e ra i js ¨ a is ¨ z js ¨ eq for some d and e such that a i e d b and a j e e c in linear logic any object having type a can be turned into an object of type a namely an object which is the version of a object is the principle according to which this transformation is possible namely a at the quantitative level this corresponds to splitting a bounded sum into its this is used in the typing rules for functions and fix the auxiliary algorithm corresponding to the principle takes two linear types and builds as usual a rewriting program and a set of side conditions capturing the fact that the first of the two types is the bounded sum of the second pi pa pq pa rq the correctness of can again be proved similarly to what we did in lemma the key statement being that for every e a such that e r the following must hold e rb js ¨ a rb js ¨ c ai ai for some c such that a i b j e c b weakening weakening means that objects can also be erased even when the underlying index is weakening is useful in the rules ax and n once a fresh d type a is produced the only thing we need to do is to produce an equational program a specifying in an arbitrary way the symbols in ap this way preserving the crucial invariants about the equational programs manipulated by the algorithm formally it means that there is an algorithm weak such that a pq a where a is pp h observe how no sets of constraints is produced in output by weak to der ctr and the type inference procedure in this section we will describe the core of our type inference algorithm this consists in a recursive algorithm gen which a pcf type derivation producing in output a d pcf judgement together with an equational program and a set of side conditions in order to correctly create fresh symbols and to format side conditions properly the main recursive function gen also receives a set of index variables and a set of constraints in input thus it has the following signature p i t a rq we will prove that the the output of gen satisfies the following two invariants · has conclusion t pr sq · polarity ia t is correctly specified see tion the algorithm gen proceeds by in an inductive manner it first the types in the conclusion judgement with fresh function symbols to get a d pcf judgement j then a recursive call is performed on the immediate of this way obtaining some d pcf typing judgement ji finally gen generates calling the auxiliary algorithms the equations on function symbols that allow to derive j from the the equations are written in a and the required assumptions of index convergence in r and polarity are the invariants of the algorithm gen in particular the auxiliary algorithms are always called with the appropriate parameters this way enforcing polarity the algorithm computing gen proceeds by case analysis on we give some cases here the other ones are developed in · suppose that is y u yk uk n nat for each i let bi p and ai bi where all the are fresh let be a fresh function symbol then return p n a where the his are fresh symbols and a y y t n u i ¨ · if is on the form t u ñ t u u let p k t ra ¨ p q a rq and p h u a rq let pb s q q a q we then annotate t let p t q and let pc u q q a then we build a context equivalent to z by the property and have the same skeleton so for any y ¨ by in there is some y ¨ cy in possibly after some conversion then let ay q and pay by by there are i i for i such that e ty ¨ z for every e y ay such that e py thus let be fresh symbols and return p t u a rq in output where r r y r y s y u y ry y t y a a y a y b y c y u y ay y y ty ¨ · assume that is x u t t xt u ñ t let a be a fresh index variable and be a fresh func tion symbol and compute p x k t b s q pa q pa qq we build a context to for every y qs ¨ by p let ay q let be a fresh symbol and let pay be qq pa then return p k xt ra ¨ p where q a rq r s y y t e q y a b y y ty ¨ lemma for every and every pcf derivation the output of satisfies and polarity correctness the algorithm we have just finished describing needs to be proved sound and complete with respect to d typing as usual this is not a trivial task moreover linear dependent types have a semantic nature which makes the task of if not proving the desired results even more challenging soundness a type inference procedure is sound when the inferred type can actually be derived by way of the type system at hand as already gen outputs an equational program a which possibly contains unspecified symbols and which as a consequence cannot be exploited in typing moreover the role of the set of proof obligations in r may be at first actually soundness holds for every completely specified e a which makes the proof obligations in r true theorem soundness if is a pcf derivation for t then for any and p i t a rq where ia t is correctly specified and for any e a e r ei t is derivable and precise soundness can be proved by induction on the structure of exploiting auxiliary results like lemma completeness but are we sure that at least one type derivation can be built from the outcome of gen if one such type derivation exists again it is nontrivial to formulate the fact that this is actually the case theorem completeness if t is a precise d judgement derivable by dv then is of the form p i t a rq and there is µ e ù a such that µ r completeness can be proved by an induction on the structure of dv its statement however needs to be appropriately for induction to work again results like lemma greatly help here point states completeness of der and the latter is called by gen many times a direct consequence of soundness and completeness and the remark on definition is the following corollary if a closed term t is typable in pcf with type nat and a derivation then h pi t e rq and t is typable in d iff e r type inference at work the type inference algorithm presented in the previous section has been implemented in ocaml programs types equational programs and side conditions become values of appropriately defined inductive data structures in ocaml while the functional nature of the latter makes the implementation effort easier this section is to discussing the main issues we have along the process which is still the core of our implementation is an ocaml function called taking a closed term t having pcf type t in input returns a typing derivation dv an equational program e and a set of side conditions r the conclusion of dv is a d pcf typing judgement for the input term if t is a firstorder type then the produced judgement is derivable iff all the side conditions in r are valid to do so calls an implementation of gen on t and a context consisting of n unconstrained index variables where n is the arity of t this way obtains a and r as results and then proceeds as follows · if t is nat then a is already completely specified and corollary ensures that we already have what we need · if t has a strictly positive arity then some of the symbols in a are unspecified and appropriate equations for them need to be added to a take for instance a term s of type nat ñ nat returns a r and a typing judgement of the form a h h ak t the source code is available at where j is a positive symbol while f and g are negative thus unspecified in a a can be appropriately completed by adding the equations and a to it this way we are on the behaviour of t when with any natural number represented by a and when the environment needs t only once how about complexity analysis actually we are already there the problem of proving the number of machine reduction steps needed by t to be at most p n Ñ n where p is eg a polynomial becomes the problem of checking e s where e is the appropriate completion of a and s is r y q proposition simplifying equations equational programs obtained in output from contains many equations which are trivial such as n or and as such can be eliminated moreover instances of forest and bounded sums can be greatly simplified as an example be replaced by this allows in particular to fewer and simpler rules thus the next phase a basic simplification procedure has already been implemented and is called by on the output of gen however automatically treating the equational program by an appropriate prover would of course be desirable for this purpose the for to interact with a system sup equational and rewriting logic specification is currently in checking side conditions as already once has produced a pair pa rq the task we started from namely complexity analysis of t is not finished yet checking proof obligations in r is as undecidable as the complexity of t directly since most of the obligations in r are termination statements there is an important difference however statements in r are written in a language the firstorder equational logic which is more amenable to be treated by already existing automatic and tools actually the best method would be to first call as many existing automatic provers as possible on the set of side conditions then the programmer to check those which cannot be proved automatically by way of an interactive theorem prover for this purpose we have implemented an algorithm translating a pair in the form pa rq into a why theory related work complexity analysis of higherorder programs has been the object of many studies we can for example mention the proposals for type systems for the calculus which have been shown to correspond in an extensional sense to eg polynomial time computable functions many of them can be seen as static analysis once a program is assigned a type an upper bound to its time complexity is relatively easy to be the problem with these systems however is that they are usually very weak from an point of view since the class of typable programs is quite restricted compared to the class of all terms working within the resource bounds more powerful static analysis can actually be all of them however are either limited to very specific forms of resource bounds or to a form of higherorder functions or else they do not get of higherorder as the underlying logic consider as an example one of the work in this direction namely system of cost closures the class of programs that can be handled includes the full lazy calculus but the way complexity is about remains higherorder being based on closures and contexts in framework higherorder programs are translated into higherorder equations and the latter are turned into firstorder ones both steps and in particular the second one are not recent works on resource analysis are either limited to firstorder programs or to linear bounds a recent proposal by and allows to reason on the the cost of higherorder functional programs by way of socalled being sure that the actual behaviour of compiled code reflects the annotation the logic in which cost annotations are written however is a higherorder hoare logic none of the proposed systems on the other hand are known to be relatively complete in the sense we use here slot games are maybe the work which is to ours among the many in the literature slot games are simply ordinary games in the sense of game semantics which are however instrumented so as to reflect not only the observable behaviour of higherorder programs but also their performance indeed slot games are fully abstract with respect to an operational theory of improvements due do sands this can be seen as the counterpart of our relative completeness theorem an aspect which has not been investigated much since proposal is whether slot games provides a way to perform actual verification of programs maybe via some form of model checking as we have already mentioned linear dependency can be seen as a way to turn games and strategies into types so one can see the present work also as an attempt to keep programs and strategies closer to each other this way verification another recent work which seems to be quite close to ours is geometry of synthesis in particular when the latter takes the form of type inference conclusions a type inference procedure for d pcf has been introduced which given a pcf term reduces the problem of finding a type derivation for it to the one of solving proof obligations on an equational program itself part of the output truth of the proof obligations correspond to termination of the underlying program any type derivation in d pcf comes equipped with an expression bounding the complexity of evaluating the underlying program proof obligations and the related equational program can be obtained in polynomial time in the size of the input pcf program the main contribution of this paper consists in having shown that linear dependency is not only a very powerful tool for the analysis of higherorder functional programs but is also a way to effectively and efficiently turn a complex problem that of evaluating the time complexity of an higherorder program into a much easier one that of checking a set of proof obligations for truth although as explained in section experimental evaluation shows that proof obligations can potentially be handled by modern tools much remains to be done about the technical aspects of proof obligations into a form which is suitable to automatic or solving actually many different tools could be of help here each of them requiring a specific input format this implies however that the work described in this paper although not providing a methodology has the of allowing to factor a complex problem into a problem namely verification of firstorder inequalities on the natural numbers references abramsky s r p full abstraction for pcf i c ­ rm y and reasoning on cost annotations of functional programs abs de mathematical theory of program correctness prenticehall g b c typebased termination with products in csl lncs vol pp ­ springer r automated higherorder complexity analysis theor comput sci ­ f jc c a why your of provers in first international workshop on intermediate verification languages pp ­ clarke em programming language constructs for which it is impossible to obtain good hoare axiom systems j acm ­ m f s p n j c the system in lncs vol pp ­ cook sa soundness and completeness of an axiom system for program verification siam j on computing ­ u context semantics linear logic and computational complexity in lics pp ­ u m linear dependent types and relative completeness in lics pp ­ u b the geometry of types long version available at u b linear dependent types in a callbyvalue scenario in acm pp ­ v l reversible and optimal theor comput sci ­ e refinement types for specification in pp ­ felleisen m friedman dp control operators the and the calculus tech rep computer science department university dr slot games a quantitative model of computation in acm popl pp ­ dr smith a geometry of synthesis iii resource management through type inference in acm popl pp ­ girard jy a scott p bounded linear logic theor comp sci ­ gulwani s speed symbolic complexity bound analysis in cav pp ­ j k hofmann m multivariate resource analysis in acm popl pp ­ hughes j l a proving the correctness of reactive systems using types in acm popl pp ­ s k hofmann m static determination of quantitative resource usage for higherorder programs in acm popl jl a callbyname lambdacalculus machine higherorder and symbolic computation ­ j odersky m turner dn wadler p callbyname callbyvalue callbyneed and the linear lambda calculus notes theor comput sci ­ plotkin gd as a programming language theor comp sci ­ sands d complexity analysis for a lazy higherorder language in esop lncs vol pp ­ sands d operational theories of improvement in functional languages extended abstract in functional programming pp ­ r j a n s d g c r t f i p j p the worst case execution time problem overview of methods and survey of tools acm trans embed comput syst 