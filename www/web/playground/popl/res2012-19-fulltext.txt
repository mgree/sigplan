a compiler and runtime system for network programming languages christopher princeton university foster cornell university us david walker princeton university abstract networks are a new kind of network architecture in which a controller machine a distributed collection of switches by them to or rules and report traffic statistics the recently formed open whose members include microsoft and others to use this architecture to transform the way that and data center networks are implemented in this paper we define a highlevel declarative language called netcore for expressing policies on netcore is expressive compositional and has a formal semantics to ensure that a of packets are processed efficiently on of on the present new compilation algorithms for netcore and them with a new runtime system that issues rule commands and queries to switches together the compiler and runtime system generate efficient rules whenever possible and the simple manual techniques commonly used to program in addition the algorithms we develop are generic assuming only that the capabilities available on switches satisfy some basic algebraic laws overall this paper a new design for a highlevel network programming language an improved set of compiler algorithms a new runtime system for sdn architectures the first formal semantics and proofs of correctness in this domain and an implementation and evaluation that demonstrates the performance benefits over traditional manual techniques categories and subject descriptors d programming languages language application languages general terms languages design keywords network programming languages domain specific languages the views expressed in this paper are those of the authors and do not reflect the policy or position of the us the department of the the department of or the us copyright association for computing machinery acm that this contribution was or by a or of the us as such the a right to or this article or to allow others to do so for purposes only popl january ­ pa usa copyright © acm introduction a network is a collection of connected devices that traffic from one place to another networks are they connect and on university they send packets between a variety of mobile devices in modern they search requests and orders through data they between networks in san francisco and and they connect the to the in your car naturally these networks have different purposes properties and requirements to service these requirements like and others a variety of devices including which forward packets based on ip addresses switches which forward packets based on mac addresses nat boxes which translate addresses within a network which forbidden or traffic and load which work among servers to name a few while each of these devices behaves differently internally they are all built on top of a data that buffers tags rate limits and collects statistics about packets at high speed more complicated devices like also have a control that run algorithms for tracking the topology of the network and computing through it using statistics from the data and the results computed using the devices specialized algorithms the control or forwarding rules in the data the data is built out of fast hardware capable of forwarding packets at the rate at which they arrive while the control is typically implemented in software however traditional networks appear to be on the of a major on march th microsoft and owners of some of the largest networks in the world the formation of the open foundation the foundations proposal is simple eliminate the control from network devices instead of specific control software into each device the foundation proposes a standard protocol that a separate generalpurpose machine called a controller can use to program and query the data of many devices by moving the control from devices onto machines like will be able to switches and write controller programs to and optimize their networks however they choose networks built on this new architecture which from earlier work on and d are now commonly referred to as networks already several commercial switch support a concrete realization of the protocol required for implementing and researchers have used to develop new algorithms for server data center routing network management finegrained access control traffic monitoring fault of service detection host mobility and many others ­ now the obvious question is why should programming language researchers and the popl community in particular care about these developments the answer is clear some of our most important soon be running an entirely new kind of program using our experience principles tools and algorithms our community has a unique to define the languages these programs will be written in and the infrastructure used to implement them we can have major impact and help make future networks easier to program more secure more reliable and more efficient as a step toward carrying out this we propose a highlevel language called netcore the network core programming language for expressing policies netcore has an intuitive syntax based on familiar settheoretic operations that allows programmers to construct and reason about rich policies in a natural way primitives for packets include bit patterns and arbitrary wildcard patterns it also supports using arbitrary functions to analyze packets and traffic patterns this feature makes it possible to describe complicated dynamic policies such as authentication and load in a natural way using ordinary functional programs unfortunately compiling these rich policies is challenging on the one hand the controller machine has the computational power to evaluate arbitrary policies but the switches do not they can only implement simple kinds of bit matching rules on the other hand a packet to the controller for processing orders of magnitude more than processing it on a switch hence despite the limited computational power of the switches it is critical to find ways for them to perform most packet processing the netcore compiler and runtime system this challenge by analyzing programs and automatically them into two pieces one that runs on the switches and another that runs on the controller moreover this division of does not occur once at compile time it occurs dynamically and repeatedly intuitively when a packet cannot be handled by a switch it is to the controller the controller partially evaluates the packet with respect to the current network policy and dynamically generates new rules that handle said packet as well as others like it the new rules are subsequently sent to the switches so similar packets in the future are handled in the network fast path over time more and more rules are added to the switches and less and less traffic is to the controller we call this iterative strategy reactive specialization our strategy is inspired by the idiom commonly used for sdn applications in which an program manually a rule to handle future traffic every time a packet is to the controller however many programs written manually in this style use inefficient rules rather than wildcard rules because reasoning about the semantics of overlapping quickly becomes very complicated to do by hand hence our strategy improves on past work by providing highlevel abstractions that the need for programmers to deal with the lowlevel details of individual switches synthesizing efficient forwarding rules that exploit the capabilities of modern switches including wildcard rules implemented by memories the process of dynamically unfolding rules on to switches instead of requiring that programmers lowlevel programs manually to summarize the central contribution of this paper is a framework for implementing a canonical highlevel network programming language correctly and efficiently more specifically a controller c network n switch s network n figure example topology · we define the syntax and semantics for netcore section and model the interaction between the netcore runtime system and the network in a process calculus style section this is the first formal analysis of how a controller platform with switches · we develop novel algorithms for compiling network programs and interactions at run time including classifier generation and reactive specialization section · we prove key correctness theorems section establishing simulation relations between our lowlevel distributed implementation strategy and our highlevel netcore semantics we also prove an important theorem showing that our implementation successfully computation from the controller onto switches · we describe a prototype implementation and an evaluation on some simple benchmarks the practical utility of our framework section netcore out of our previous work on another highlevel network programming language has three main pieces an query language for reading network state a language for specifying policies and a functional reactive glue language that processes the results of queries and generates streams of forwarding policies for the network netcore replaces language for expressing forwarding policies with a significantly more powerful language that supports processing packets using arbitrary functions in addition netcore also contains a query language as its predicates can analyze traffic history the main contribution of this paper relative to earlier work on is the design of new algorithms for compiling these rich policies and for the interactions that arise as compiled policies are executed in a network these algorithms handle new policy language and the core elements of old policy language even better in particular the netcore compiler generates efficient switch by using wildcard rules that process more packets on switches instead of simple rules and generating rules ie in advance of when they are needed again to process more packets on switches instead of strictly ie on demand finally netcore has a formal semantics and correctness proofs for its core algorithms whereas had none netcore overview this section presents additional background on and netcore using examples to illustrate the main ideas for we focus on the sdn architecture but we and take with certain details our compiler does not assume the of the current platform overview is based on a architecture in which a controller a collection of switches figure a simple topology with a controller c a single switch s packets may either be processed on switches or on the controller but processing a packet on the controller increases its by several orders of magnitude hence to ensure good performance the controller typically a classifier consisting of a set of rules on each switch each forwarding rule has a pattern that identifies a set of packets an action that specifies how packets matching the pattern should be processed counters that keep track of the number and size of all packets processed using the rule and an integer priority when a packet at a switch it is processed in three steps first the switch selects a rule whose pattern matches the packet if it has no matching rules then it the packet and if it has multiple matching rules then it the one with the priority second the switch updates the counters associated with the rule finally the switch applies the action listed in the rule to the packet in this paper we are concerned with two kinds of actions a forwarding action l lk which the packet to a set of usually one sometimes zero more than one adjacent network locations li where each li may be the name of another switch network or host and a controller action which the packet to the controller for processing netcore a simple static forwarding policy netcore is a declarative language for specifying highlevel policies the netcore compiler and runtime system handle the details of translating these policies to rules and commands to the generated rules on switches the simplest netcore policies are specified using a predicate e that matches some set of packets and a set s of locations to which those packets should be we write these policies e s the simplest predicates match bits in a particular packet header field for example the predicate specifies that the first of the packets source address must be using the standard notation for expressing ip prefix patterns more complex predicates are built by taking the union intersection negation ¬ or difference of simpler predicates analogous settheoretic operations may be used to compose more complex policies from simpler policies as an example consider the following policy switch it states that packets from sources in should be to switch except for packets coming from or going to a destination on port the first challenge in compiling a highlevel language such as netcore to a lowlevel sdn framework such as arises from the relative lack of expressiveness in the switch primitives for instance because switches cannot express the difference of two patterns in a single rule this policy needs to be implemented using three rules in a particular order one that packets from another that all packets going to port and a final rule that all remaining packets from to switch the following classifier implements this policy we write these with the priority rule first patterns are on the left actions are on the right and a the two switch next consider a similar highlevel policy to the first on real switches locations are actually integers corresponding to physical ports on the switch in this paper we model them symbolically switch we can generate a classifier for this policy in the same way switch now suppose that we want to generate a classifier that implements the union of the two policies we cannot combine the in a simple way eg by or interleaving them because the rules interact with each other for example if we were to simply the two lists of rules the rule that packets to port would the forwarding rule for traffic from instead we need to perform a much more complicated translation that produces the following classifier switch switch switch switch switch switch dealing with these often leads sdn programmers to use rules that fully specify every bit in every single header field rules for instance do not use wildcard patterns that match many values for a single header field such as nor do they leave certain header fields completely unconstrained our first implementation of used rules exclusively because such rules were far easier for its runtime system to reason about particularly when it to composing multiple user policies this paper presents new generalpurpose algorithms for synthesizing lowlevel switch that use wildcard rules to the extent possible these new algorithms result in far more efficient system than the one we built in earlier work in original architecture many more packets up being sent to the controller orders of magnitude increase in and many more rules had to be sent to switches the results of our experiments presented in section the magnitude of these differences netcore richer predicates and dynamic policies the policies presented in the previous section were relatively did nothing besides match bits in header fields and forward packets accordingly such static policies can be expressed in simple policy language though they are not be implemented nearly as efficiently as in the netcore system however many applications demand dynamic policies whose forwarding behavior depends on complex functions of traffic history and other information and these richer policies cannot be implemented by simply analyzing bits in header fields as an example suppose we want to build a security application that implements authentication for the topology shown in figure the network n contains a collection of internal hosts n represents the connection to the a is the server that handles authentication for hosts in n and all three elements are connected to each other by the switch s informally we want the network to perform routing and access control according to the following policy forward packets from hosts in n to a from authenticated hosts in n to their intended destination in n and from a and n back to n although not from n to a this policy can be described in netcore as follows ps network ps server a a network where ps a sp any p p p this policy uses an predicate to classify traffic from n as authenticated or an predicate e f has two arguments a filter predicate e over the network traffic history and an almost arbitrary function f the filter predicate generates a controller state which is a collection of traffic statistics represented abstractly as a multiset of pairs the switch being the place where the packet was processed the function f receives the controller state as one its arguments and may analyze it as part of its process in the example above the filter predicate ps selects all traffic coming from the authentication server in this example we will treat an entity sending a packet p as authenticated if the authentication server has ever sent it a packet at any point in the past the function takes three arguments the controller state the switch s that should be handling the packet and the packet p to which the policy applies here the function tests whether the field of the packet p being processed is equal to the of any other packet p in the traffic history and because there is only one switch in this example ignores its s argument in other words it tests whether the authentication server has sent a packet to that sender in the past if it has the predicate is satisfied if not it is not satisfied the function performs these tests using the auxiliary functions any a builtin function that tests whether a boolean function is true of any element of a multiset and a userdefined function that tests whether one packets is equal to another packets this is combined with the other settheoretic operators to implement the overall policy policies that use are easy to write because forwarding decisions can be expressed using arbitrary functional programs these programs can query past traffic history or look up facts they need such as authentication status in a database on the other hand these programs never have to the lowlevel details of generating or runtime system does that tedious work for the programmer of course this expressiveness presents an extreme challenge for the compiler and runtime system while it would be easy to evaluate the results of such policies by sending all packets to the controller doing so would be totally we must find a way to implement the policy while processing the of traffic on switches our implementation strategy for such policies proceeds as follows first we compile the parts of the policy that do not involve as effectively as we can the system generates normal forwarding rules when it can and rules that send packets to the controller otherwise next whenever a packet that cannot be handled by a switch at the controller the runtime system evaluates the packet against the current policy which includes producing a set of forwarding actions there are two possibilities the policy with respect to this packet and similar ones is invariant in other words every subsequent time the system evaluates the policy against this packet it will return the same set of forwarding actions the policy with respect to this packet and similar ones is in other words the set of forwarding actions to be applied to this policy may change in the future in the first case the system can rules on the switch that associate packets similar to the one just processed with the set of forwarding actions just computed because the set of computed actions will never change such rules on switches preserves the semantics of the policy in the second case the system can not rules on the next packet might be differently so the system will have to it on the controller in our example once a host has been authenticated it the function evaluates to true it will continue to do so and is therefore invariant since inferring invariance automatically from an arbitrary program is a difficult problem we currently ask netcore programmers to supply invariance information to the compiler in the form of an auxiliary function in this simple case writing the invariance function is trivial it is true whenever is true sp sp to effectively generate rules even in the presence of predicates the runtime system must be able to determine when returns the same results on one packet as it does on it must be able to calculate the similar packets referred to above observe that an always returns the same results on two different packets if those packets agree on all header fields that the function conversely if the does not examine a particular header field the value of that field does not affect its result hence when generating a policy after evaluating it against a single packet the runtime can substitute for all header fields that the policy does not though it is likely possible to infer the set of any function at least conservatively our current implementation assumes that programmers supply this information explicitly overall these techniques runtime evaluation of policies against particular packets on the controller invariance and specification of header to turn the difficult problem of evaluating policies containing arbitrary functions back in to the simpler problem of compiling static forwarding policies efficiently we call these techniques reactive specialization a core calculus for network programming this section defines the syntax and semantics of netcore a core calculus for highlevel network programming the calculus has two major components predicates which describe sets of packets and policies which specify where to forward those packets figure presents the syntax of these constructs as well as various network values such as and packets notation throughout this paper whenever we define syntax as in the grammar for packets we will use the grammar nonterminal p as a metavariable ranging over the objects being defined the version of the nonterminal p as a metavariable ranging over sets or multisets of such objects and vector notation p for sequences of objects we describe finite sets using the notation x xk and combine sets using operations ¬ and union intersection complement and difference respectively typically we give definitions for intersection and complement and leave union s s and difference s s s as derived forms we also ¬ and use it to booleans its meaning will be clear from context we write multisets using the notation x xn and combine multisets using multiset union m m we write finite maps using the notation x y xn yn and lookup elements of a finite map m using function application network values for simplicity we only model a single kind of network entity to forward to switches s packets p are the basic network values switch s header h switch set s s sn header set h h hn bit b packet p h b hn bn state s p sn pn language x s p wildcard w f × switch × bool predicate e h w switch s e f e e policy e s ¬ figure netcore syntax e x xk h w s p ph matches w switch s s p s s e f s p f s p where s p s p and s p e e e e e ¬ e x s e s x s if x e otherwise x x x ¬ x ¬ x figure netcore semantics values processed by programs which we represent as a finite map from h to b we write ph for the associated with the header h in p we assume all fields have fixed finite length and therefore the set of complete packets is finite the controller state information about packets that arrive at each switch we represent controller state as a multiset of pairs predicates informally predicates select sets of packets that are of interest in some forwarding policy formally a predicate e denotes a set of x a controller state a switch s and a packet p located at s the state component is essential for modeling predicates that depend upon traffic patterns such as the past load on particular links or packets sent and received from various locations figure defines the semantics of predicates we say that a x matches a predicate e when it belongs to the denotation of e we sometimes say that a packet p matches e leaving the state and the switch implicit because they are irrelevant or we also say that a b matches a wildcard w whenever the corresponding bits match for example and both match the wildcard basic predicates have the form h w a packet p matches h w if ph ie the h header of p matches w for example the predicate matches all packets with header field equal to as is in binary another basic predicate switch s matches all packets in any state sent to s more complex predicates are built up from simpler ones using the intersection and complement operators additional building blocks such as true false e e or e e can be implemented as derived forms the most interesting component of the language is the predicate e f the first component of an is a filter predicate e that selects pairs matching e from the current state creating a refined state in other words e acts as a query over the network traffic history the second component f is an almost arbitrary function over and the pair s and p in question the authentication example defined in the previous section used an another example is cond where cond sp cardinality here cardinality is a function that counts the number of elements in a multiset this extracts all web traffic is from the current state the is satisfied if the total number web packets sent is less than or the packet p comes from a particular sender is to make compilation tractable two additional pieces of information are associated with functions f the first piece of information comes from the sets of mentioned in its indexed type × switch × bool such a type restricts f to only examine h of packets in the state and h in the packet for instance the function cond above may be assigned a type h is the empty set as packet counts requires looking at no and h is as cond only the field of its packet argument the second piece of information associated with f comes from its invariance oracle a function f is invariant on s p written invariant s p f if for all we have f s p f s p intuitively a function is invariant on a state when its result does not change no matter what additional information is added to it again as an example the cond function above is invariant on all involving packets from as well as all where cardinality once the total volume of web traffic has the threshold the function always returns true in our implementation the programmer writes invariance by hand as simple haskell functions together the header sets in the types and the invariance oracle allow the compiler to generate effective rules even though the function itself cannot be analyzed however the language of predicates does have one significant limitation it depends upon invariance of predicates there are predicates that are invariant for a long time and hence could have rules on switches for that time but are not invariant we believe our framework can be extended to handle such invariance properties having the compiler rules at the end of a time period or in response to a network event but defer an investigation of this topic to future work synchronous machine t asynchronous machine t t o s p s p t t t s p sp s p t t o s p s p t t t t t t t s p t t s p t sp t t t t t s p s p t t figure reference machines policies policies specify how packets should be through the network basic policies written e s say that packets matching e should be to the switches in s as with predicates we build complex policies by combining simple policies using intersection and negation figure defines the semantics of policies as a function from x to sets of switches s although the policy language is syntactically simple it is expressive in particular are a powerful tool that can be used to express a wide range of behaviors including load finegrained access control and many standard routing policies machines to understand how the network behaves over time we define two abstract machines both machines forward packets according to but they differ in how often the switches traffic statistics with the controller the synchronous machine defines an implementation that at all times has perfect information about the traffic sent over the network of course it would be to implement this machine in a real network because in general it would require sending every packet to the any packet were by a switch there would be a delay between when the packet was and when the controller state was augmented with information about that packet the asynchronous machine defines a more practical implementation like the first machine it is packets according to the policy but it updates its state instead of in with each packet processed hence it makes no guarantees about what it knows about the networks traffic while the synchronous machine can be thought of as the best possible machine the asynchronous machine can be thought of as the worst machine any reasonable implementation will between the two in other words implementations should be policy but users should not expect perfect cost of implementing it would be in practice synchronization with switches typically happens at periodic intervals modulo in the of communication but for simplicity we do not model time explicitly figure defines both reference machines they use the function p which generates a multiset of p t s p s s the state of the synchronous machine includes the netcore policy the state and a multiset t of pending at each step the machine removes a transmission from t processes it using the policy updates the machine state and adds the new generated by the policy to the multiset of pending the state of the asynchronous machine includes the program state and two multisets of t which represents waiting to be processed by the policy and t which represents that have been processed by the policy but have not yet been added to the state the first inference rule for the second machine takes a transmission from t processes it using the policy and places it in t the set of waiting to be incorporated into the second rule takes a transmission from t and adds it to the runtime system in this section we discuss how to implement semantics on a network by giving an operational semantics to the netcore runtime system and the underlying network devices this operational semantics explains the basic interactions between the controller and the switches switch before we can present the runtime system we need a concrete representation of the rules that switches use to process packets a classifier r is a sequence of rules r each containing a pattern z and an action while our highlevel semantics uses sets are represented as sequences to model rule priority within a classifier rules on the left have higher priority than rules on the right the pattern z component of a rule a set of packets and hence is similar to but less general than a predicate in netcore we write p z when packet m matches pattern z we hold patterns abstract to model the variety of different matching capabilities in switches for example switches support prefix pattern matching on source and destination ip addresses ie patterns like but only exact or full unconstrained matching on most other some switches support various other extended patterns such as ranges of the form n n an action is either a set of switches s which packets to each switch in the set or which packets to the controller most switches support other actions such as modifying header fields but for simplicity we only model forwarding given a packet p and a classifier r we match the packet against the classifier by finding the first rule whose pattern matches the packet we write r p z for the matching judgment more formally we define classifier matching as follows note that it selects the priority leftmost matching rule p z p zi p zi z zi i zi i zn n p zi i machine we formalize the operational semantics of the runtime system as a machine in the style of the abstract machine the machines components called are given on the left side of figure for simplicity we assume that packets at a switch may be processed in any order and do not model failures that cause packets to be the c represents the controller machine running the netcore policy in state the s s r z represents switch s with packet classifier r and local switch state z the switch state records the patterns of rules that have been used to match packets but not yet and processed by the controller real switches use integer counters as state for simplicity we represent these counters in unary using a multiset of patterns a transmission t s p represents a packet p en to switch pattern z switch action s rule r z classifier r r rn switch state z z zn m c s s r z t s p h s p machine m m mn observation o s p m o m r p z s p t s s r z t s p sp s s r z z t r p z s s r z t s p s s r z h s p s p s p t specialize s p r c s s r z h s p sp c s p s s r r z t r p z c s s r z z c s p s s r z m o m m m o m m figure the runtime system s finally a help h s p represents a request by switch s to the controller for in processing packet p the operational semantics of the machine is defined by the inference rules on the right side of figure to the notation in this figure we drop the multiset when writing a collection of in other words we write m m instead of m m each operational rule may be labelled with an observation o which records when are processed we use observations in section where we establish equivalences between the machine and the reference machines defined in the last section the rules and model the work done by switches to process packets the former rule is invoked when a packet matches a rule with a action a set of switches to forward to in this case the switch the packet accordingly and records the rule pattern in its state the latter rule is invoked when a packet matches a rule with a controller action in this case the switch generates a help the rule models the work done by the controller to process help the controller the packet using its netcore policy generating new t to be sent into the network and adds the packet to its state in addition the controller uses the netcore compiler to generate new rules to process future similar packets on switches instead of on the controller the compiler is accessed through the call to the specialize function which generates the new rules for the switch in question we hold the definition of this function abstract for now it is defined precisely in the next section the rule models the work done by the controller to transfer information about the packets that matched a particular rule from the switch to the controller state more precisely it chooses a pattern z from a switch state and then uses the lookup judgement r p z to synthesize a packet p that might have matched the corresponding rule in the switch classifier the pair of the packet and the switch are then stored in the controller state as a past transmission that might have occurred the interesting part of this transfer is that the controller stores full packets whereas switches only store sets of patterns and a pattern only specifies part of a its ip address or tag but not the packet itself hence the transfer operation must those parts of the packets that are not specified in the switch pattern and the system as a whole must be correct no matter how the parts of a packet are this places an important constraint on the compiler if the rules and their patterns are not specific enough then although one packet may have matched a rule on a switch a completely different packet may be and passed back to the controller consequently the controller state will not model past network traffic sufficiently accurately and forwarding policies that depend upon past network traffic will not be implemented correctly a second subtle issue with the rule is that the patterns of rules partially overlap and take precedence over patterns from rules hence examining the pattern of a rule in isolation does not provide sufficient information to synthesize a packet that might have matched that rule one must take all of the rules of the classifier and their priority order in to account when synthesizing a packet that may have matched a pattern the rule does this through the use of the full classifier matching judgement finally note that the implementation does not actually all of these practice the switch passes integer counters associated with patterns back to the controller still this nondeterministic rule effectively captures a key correctness criterion for the system the controller program cannot distinguish between any of the packets that might be synthesized by the rule and must be correct no matter which one is of course this is also where the compilers use of header information comes in to play the packets are only different in fields that functions and other predicates do not analyze the netcore algorithms the netcore system performs two distinct tasks · classifier generation given a netcore policy construct a set of one for each switch in the network primitive intermediate form u h w hn wn threevalued boolean b true maybe false pattern intermediate form u z b h policy intermediate form u z s s h i s e i s h w h w o h w true false i s switch s true false if s s if s s i s e f ui zi maybe hi h i where f × switch × bool and i s ei ui zi bi hi i s e e ui uj zi zj bi bj hi ij where i s ei ui zi bi hi and i s e j uj zj bj i s ui zi hi i where i s ei ui zi bi hi i s i s e s i zi zi zi s s hi s hi hi if bi true if bi maybe if bi false where i s ei ui zi bi hi i s ui uj zi zj s s hi ij where i s i ui zi si si hi and i s j uj zj s j s j and s si s j and s si s j i s ¬ ui zi hi i where i s i ui zi si si hi c s r c s i si where i s if si si and hi and consistent i otherwise and i ui zi si si hi and consistent i p j pz ui zi si si hi pu uj zj si si and h p z p z ph ph figure netcore classifier generation · reactive specialization given a packet not handled by the current classifier on a switch generate additional rules that allow the switch to handle future packets with similar header fields without the controller this section presents the key algorithms that implement these tasks parameters the netcore system is parameterized on several structures a lattice of switch patterns and two that map primitive predicates onto and wildcard patterns respectively abstracting some of the lowlevel details of compilation makes it possible to execute netcore policies on many kinds of hardware and even use switches with different capabilities in the same network formally we assume that switch patterns form a bounded lattice a pattern z lower than or equal to another pattern z written z z when z matches a subset of the packets matched by z the element matches every packet and the element matches none notation slightly we write p z to indicate that packet p matches pattern z to ensure that intersections are compiled correctly we require that meets be exact in the sense that p z z if and only if p z and p z the first oracle called the compilation oracle o maps primitives h w into the pattern lattice in many cases the pattern generated by o h w will match the set of packets described by h w exactly but sometimes this is not possible for example switches only support prefix for ip addresses so the best approximation of the pattern is we give the oracle some flexibility in selecting patterns and only require it to satisfy two conditions it must return an overapproximation of the primitive and it must be monotonic in the sense it translates semantically larger primitives to larger patterns formally the requirements on compilation are as follows s p h w implies p o h w and h w h w implies o h w o h w the second oracle called the refinement oracle u takes a primitive h w and a packet p as arguments and produces a pattern h w unlike the compilation oracle which predicates the refinement oracle predicates allowing the compilation infrastructure to generate effective rules for a subset of the pattern of interest while there is generally one best overapproximation there often exist many useful in such cases we by selecting the best underapproximation that matches p for example if we were to refine which cant be compiled exactly on with a packet with source address we would generate the underapproximation yet if we refined the same predicate with a packet with source address we would instead generate classifier generation given a policy the netcore compiler would generate a classifier with the same one that denotes the same function on packets but certain netcore features such as and wildcard patterns when not supported by the underlying hardware cannot be implemented on switches so in general the generated classifier will only approximate the policy and certain packets will have to be processed on the controller the classifier generator works in two phases in the first phase it translates highlevel policies to an intermediate form containing patterns and actions as well as precise semantic information about the policy being compiled in the second phase it builds a classifier by actions to patterns using the semantic information produced in the first phase to determine whether it is safe to forwarding actions to a pattern or whether the special controller action must be used instead the grammars at the top of figure define the syntax for the intermediate forms used in classifier generation the intermediate form for predicates u z b h contains four values an ideal pattern u a switch pattern z a threevalued boolean b and a set of h the ideal pattern u represents the pattern we would generate if the pattern lattice supported arbitrary ideal patterns are represented as a conjunction of header and wildcard pairs we write for the unconstrained ideal an empty conjunction the switch pattern z represents the actual pattern generated by the compiler which is an overapproximation of the ideal pattern in general the threevalued boolean b indicates whether packets matching the predicate should definitely be accepted true rejected false or whether there is information and a answer must be made by the controller maybe to combine threevalued booleans we extend the standard boolean operators as follows maybe false false maybe maybe maybe maybe true maybe maybe the set of h keeps track of the header fields within packets in the controller state that functions may examine this header information is used to ensure the compiler generates sufficiently finegrained switch rules so that when information is from the switch to the controller using the rule discussed in the previous section the information is precise enough to guarantee the correctness of the the intermediate form for policies u z s s h is similar to the form for predicates but instead of a threevalued boolean it records lower and upper bounds s and s on the sets of switches to which a packet might be intuitively a proper forwarding rule can only be generated when we know exactly which switches to forward packets to ie when s and s are equal in other cases the compiler will generate a rule that sends packets to the controller predicate translation the of classifier generation is the function i s e presented in figure which takes a predicate e and switch s as arguments and produces a sequence of intermediate predicates that approximate e on s one of the invariants of the algorithm is that it always generates a complete sequence ie intermediate predicates whose patterns match every packet in addition the algorithm attempts to produce a sequence whose patterns separate packets into two with packets that match the predicate being compiled and another with those that do not however it does not always succeed in doing so for two fundamental reasons the algorithm cannot analyze the decisions made by far as the analysis is concerned are black boxes and certain primitive predicates cannot be expressed precisely using switch patterns the intermediate predicates contain sufficient information for the compiler to reason about the precision of the rules it generates we write i s ei ui zi bi hi to indicate that compiling e returns a sequence of intermediate predicates whose ith element is ui zi bi hi and ui zi bi hi i to denote the sequence of intermediate predicates out of components ui zi bi hi indexed by i the first equation at the top of figure states that the compiler translates primitive predicates h w into two intermediate predicates h w o h w true which contains the switch pattern produced by the compilation oracle and false which by using the pattern ensures that the sequence is complete like these sequences should be interpreted as a series of rules hence the second only things the first does not match the case for switch predicates switch s has two possible outcomes if the switch s whose classifier is being compiled is the same as s then the compiler generates an intermediate form that associates every packet with true otherwise the compiler generates an intermediate form that associates every packet with false intuitively the classifier generated for e f must satisfy three conditions first it should approximate the semantics of f because the behavior of f is unknown at compile time the approximation cannot be exact hence the intermediate predicates generated for the should contain maybe indicating that matching packets should be sent to the controller for processing second it should be structured so that it can identify packets matched by the traffic filter predicate the packets that must be present in the controller state to evaluate f third it should also be sufficiently finegrained to provide information about the set of h mentioned in the type of f which represent the of packets in the state that f hence the compiler recursively generates a sequence of intermediate predicates from e and then iterates through it replacing the threevalued boolean bi with maybe and adding h to the set of hi in each ui bi zi hi to generate intermediate forms for an intersection e e the compiler combines each pair of intermediate predicates generated for e and e the resulting classifier captures intersection in the following sense if a packet matches zi in the first intermediate form and matches zj in the second intermediate form it matches the form with pattern in the result and likewise for ui and uj performing this construction would result in a combinatorial however it is often possible to exploit algebraic properties of patterns to reduce the size of the sequence in the examples below and also section finally the case for predicates iterates through the sequence generated by the compiler for e and the threevalued boolean in each intermediate predicate predicate translation examples to illustrate some of the details of predicate compilation consider the translation of the predicate e e where e is h and e is h and h and h are distinct assume that switch patterns support such as on h the left and right sides of the intersection generate the following intermediate predicates i s e h h true false i s e h h false true note that the negation in e the parts of the intermediate forms designated as it the parts of the sequence that match and do not match the predicate next consider compilation of the intersection and note that we simplify the results slightly using identities such as z z and u u and b true b h h h h true h h false h h false false this classifier can then be simplified further as the last three rules overlap and are associated with the same threevalued boolean h h h h true false now suppose instead that the switch only has limited support for and cannot represent h in this case the compilation note that the compiler does not add the set h mentioned in the type of f to hi this set is used during specialization to determine similar packets oracle provides an overapproximation of the pattern say hence the intermediate predicate for e above would be as follows i s e h true false for another example consider compiling a predicate that includes an such as h f h in this case h similarly to the simple clauses above h h true false if the set of f on the state is h the h f to the following h h maybe h maybe h note that the booleans above have been replaced with maybe indicating that the controller will need to determine whether packets match the predicate however when we the results of compiling h with the results of compiling the we obtain the following h h h h maybe h h h maybe h h h false h false h importantly even though the is ie it has maybe in each intermediate predicate the result is not entirely because b false is false even when b is maybe with predicates can resolve uncertainty likewise as b true is true compiling the union of a clause with an also eliminates uncertainty and although the calculus does not represent unions explicitly its encoding operates as fact we exploit in reactive specialization policy translation the function i s which translates a policy into intermediate form is similar to the translation for predicates figure gives the formal definition of the translation to translate a basic policy e s the compiler first generates a sequence from e and then a pair of actions representing lower and upper bounds for each rule there are three cases if the threevalued boolean bi is true it uses s as both the upper and lower bounds if bi is false it uses as the bounds if bi is maybe it uses as the lower bound and s as the upper bound which represents the range of possible actions the translations of and policies are analogous to the cases for predicates classifier construction the second phase of classifier generation analyzes the intermediate form of the policy and produces a switch classifier the c s function that implements this phase is defined in figure it first uses i s to generate a sequence of intermediate policies and then analyzes each ui si si zi hi to generate a rule there are two possible outcomes for each intermediate policy in the sequence first if the bounds si and si are tight zi is sufficiently fine to collect information about all in hi and we get the same switch bounds si si regardless of whether we match packets using the ideal primitives or the patterns then it is safe for the compiler to throw away the highlevel semantic information bounds and ideal primitives and emit an effective rule zi si otherwise it generates a rule with the controller action the formal conditions needed for this analysis are captured by consistent i and the predicate consistent i is satisfied if looking up an arbitrary packet matching the ith switch pattern yields the same switch bounds as looking it up using the ideal pattern where we extend classifier lookup to sequences of intermediate policies in the obvious way the function the set of constrained fully by z formal properties for a classifier to be sound it must satisfy two properties it must forward packets according to the policy and its rules must encode enough information about the packets that match them to implement the correctness of classifier generation is captured in the following definition and lemma definition classifier soundness a classifier r is sound on switch s with respect to if the following two criteria hold · routing soundness for all s p if r p s then s p s and · collection soundness for all packets p and p if r p z s and r p z s then for all s p s p s p s p s p lemma classifier generation soundness classifier c s is sound on switch s with respect to intuitively routing soundness ensures that the actions computed by looking up rules in the classifier r are consistent with formally the condition states that if looking up a packet p in r on switch s produces a set of switches s then evaluating on containing s and p also produces s note that this condition does not impose any requirements if looking up p in r yields as the packet will be sent to the controller which will evaluate on p directly collection soundness ensures that the rules in r are sufficiently fine so that when the controller collects traffic statistics from switches the rule patterns contain enough information to implement the this is seen in the rule in the machine figure which packets that match the rule being collected collection soundness ensures that packets are correct formally it requires that behave the same on all in which the state has been extended with arbitrary packets p and p matching a given rule z s lemma states that the generated by the netcore compiler are sound reactive specialization the algorithm described in the preceding section generates that can be on switches but as we saw it has some substantial limitations dynamic policies that use cannot be analyzed and even for purely static policies if the switch has support for the classifier needed to implement the policy may be larger than would be practical to generate to deal with these situations we define reactive specialization a powerful generalization of the simple reactive strategy implemented manually by programmers we define reactive specialization using two operations program refinement which expands the policy relative to a new at the controller and pruning which extracts new effective rules from the classifier generated from the expanded policy program refinement when the controller receives a new packet that a switch could not handle it the policy with respect to the packet switch and its current state the idea in program refinement is to augment the program with additional information from this packet that can be used to build a specialized classifier that handles similar packets on the switch in the future figure defines the refinement function the key invariant of this program transformation is that the semantics of the old and new policies are identical however syntactically the new program will typically have a different structure as the transformation uses the packet to unfold primitives and this makes compilation more precise and the program more effective the rules for refining a predicate appear at the top of figure the first rule uses the refinement oracle u to refine basic predicates unlike the compilation oracle which may r x e e r s p h w h w u h w p r x switch s switch s r s p e f e e if invariant x f f x if invariant x f x if ¬ invariant x f where f × switch × bool and x s p and e r x e r x e p h and e p h r x e e r x e r x e r x ¬ r x e r x r x e s r x e s r x r x r x r x ¬ ¬ r x r specialize s p p where r c s r s p figure netcore refinement the predicate the refinement oracle it so that the rest of the compilation infrastructure will be able to generate an effective rule that matches the given packet because the new predicate is the union of the old predicate and an underapproximation the overall semantics is unchanged in some cases especially if the patterns are weak the best underapproximation the refinement oracle can generate is an predicate in many other cases however if the switch supports prefix matching or the refinement oracle will produce a predicate that matches many more packets the second rule refines switch predicates switch s because the switch predicate already reveals the maximum amount of information it cannot be refined further the rule for is the most interesting it uses a similarity predicate that describes the set of packets sent to the same switch that agree on a set of h p h switch s h ph hh we first refine the traffic filter predicate e to add additional structure for traffic collection to ensure that the refined classifier has sufficiently finegrained rules to collect the packets in the controller state examined by f we form the union of the refined traffic filter and the similarity predicate p h restricted to the refined traffic filter next we add additional information about the decision on the packet p to the policy recall that if f is invariant with respect to a x which includes the controller state switch and packet then it will return the same decision on all similar packets in the future in the first case if the is in variant and evaluates to true on the current s p then we refine it by taking the union of the and the similarity predicate p h the second case is similar except that the does not evaluate to true and hence we refine the by the similarity predicate finally in the third case the is not invariant so no sound refinement exists the decision returned by the may change in the future if the controller state changes hence packets must continue being to the controller until the becomes invariant the rules for refining intersection e e and negation predicates and policies are all straightforward pruning in general after a policy has been refined and some of the new rules will be will not process additional packets on the switch we away these useless rules using a function p that removes rules from r that send packets to the controller adding such rules does not improve the efficiency of the switch have nothing to do with the packet p meaning they are irrelevant to specialization with respect to p or overlap with a rule we removed earlier to preserve the semantics of the rules putting it all together we define reactive specialization the function specialize at the bottom of figure by composing refinement recompilation and pruning to generate a specialized classifier from a x and policy formal properties we first establish that specialization and therefore reactive rule generation is sound lemma specialization soundness if r is sound on switch s with respect to and r specialize s p then r r is sound on s with respect to to establish the other properties we need a way of characterizing the packets that go to the controller we define the controller set of a classifier r as follows r p r p the second property we establish is that refinement is monotonic that is if we append reactive rules to a classifier the resulting classifier does not send more packets to the controller that the original one formally lemma specialization monotonicity for all policies and r and r such that r specialize s p we have r r r the final property we establish is that under certain assumptions reactive rules to a classifier results in strictly fewer packets going to the controller to make such a guarantee we need two conditions first the policy must be it must only use features that can be implemented on switches eg on an switch the policy must not match on definition realizable a policy is realizable if for every subterm h w of and p h w we have s p u h w p if and only if p o u h w p realizability states that compiling an underapproximation of a highlevel predicate with respect to a packet matching the predicate yields a rule that exactly corresponds to the predicate second all in the policy must be determinate we formalize this by extending the notion of invariance to full invariance definition fully invariant a policy is fully invariant on if for every subterm of of the form e f and we have invariant s p f for all switches s and packets p for policies satisfying these conditions we can guarantee that the packet used to refine and the policy will never be sent to the controller again lemma specialization progress if is realizable and fully invariant on and r specialize s p then for any classifier r we have p r r correctness properties this section uses the tools developed in the previous section to our two central theoretical results a proof of functional correctness for netcore and a proof of another fundamental theorem which establishes that when are invariant the network eventually reaches a state in which all processing occurs efficiently on its switches functional correctness recall in section we defined two reference machines the synchronous reference machine which at all times knows and has recorded information about every packet processed in the network and the asynchronous reference machine which nondeterministically about packets processed in the network to demonstrate the correctness of the netcore compiler we show that it the space between the asynchronous and synchronous reference machines more formally we prove that the asynchronous reference machine simulates the netcore machine and the machine simulates the synchronous reference machine given a set of switches s and a policy we initialize the machine as follows c s s c s s s the next theorem establishes the relationship between the reference and machines theorem functional correctness given a set of switches s an initial set of t such that t s p t implies s s and a machine m t we have · the asynchronous machine t weakly simulates m · m weakly simulates the synchronous machine t proof sketch we describe the first simulation only the second is similar the simulation relation between the asynchronous machine and the machine satisfies the following each classifier on the machine is sound with respect to there exists an bijection between pending in the asynchronous machine and and help in the machine and there exists an bijection between the processed in the asynchronous machine and the switch states in the machine the initial state satisfies these criteria by classifier generation soundness now consider taking a transition if it a packet the first bijection is preserved by routing correctness if it collects a pattern the second bijection is preserved by collection soundness finally if it generates reactive rules they are sound by specialization soundness the theorem demonstrates that the netcore compiler effectively moves work off of the controller and onto switches even when the program is expressed in terms of patterns the switch cannot implement precisely and functions the compiler cannot analyze formally states that if all of the in the program are invariant then the netcore compiler will eventually rules on switches that handle all future eventually the system can reach a configuration where no additional packets need to be sent to the controller before we can state the theorem precisely we need a few definitions and supporting lemmas first we say that m is derived from policy if t m where is the reflexive transitive closure of the single step judgement ignoring observations second we lift the notion of full invariance to machines m m is fully invariant if the policy is fully invariant with respect to the state we also lift the notion of controller set on to machines m m s p s s r z m and p r the first lemma controller set monotonicity states that the set of packets that require processing on the controller never increases lemma controller set monotonicity if m is derived from and m o m then m m proof follows from specialization monotonicity now we are ready to prove the key lemma needed for the controller set progress lemma states that if the controller program is realizable and has become fully invariant then every time the controller processes a help the controller set becomes strictly smaller in other words every help contains enough information and the compiler is powerful enough to exploit it for the rule to generate useful new classifier rules lemma controller set progress for every realizable policy and fully invariant m derived from if m o m is an instance of then m m proof follows from specialization progress follows from these lemmas as the total number of possible packets is finite the precise statement of says that the runtime system may as opposed to does because the machine may nondeterministically choose to continue forwarding packets using the switches instead of processing the remaining help formally a machine configuration m may if there exists a configuration m such that m m and the rule is not used in any derivation in the operational semantics starting from m with this definition in hand we can state theorem for every realizable policy and fully invariant m derived from we have that m may implementation and evaluation we have implemented a prototype netcore compiler in haskell using the ideas presented in this paper the core algorithms are formulated in terms of abstract type classes eg lattices for switch patterns and this makes it easy to instantiate the compiler for different switch simply has to define instances for a few type classes and provide the appropriate we have built two both switches the first generates wildcard rules the other used for comparison generates the kind of rules used in our earlier work on and most applications optimizations the implementation uses a number of heuristic optimizations to avoid the combinatorial that would result from compiling for example it applies algebraic to remove useless patterns and rules and reduce the size of the intermediate patterns and it needs to manipulate the compilation algorithms identify and remove patterns completely by other patterns and patterns whose switch misses classifier size k k k k k packets k k k k k packets classifier size switch misses k k k k k packets k k k k k packets classifier size switch misses k k k k k packets k k k k k packets figure experimental results full compiler compiler full compiler switch misses classifier size k k k k compiler switch misses k k k classifier size k k k effect is covered by a larger pattern with lower priority but the same actions although these heuristics are simple they go a long way toward ensuring reasonable performance in our experience evaluation to evaluate our implementation we built an instrumented version of the runtime system that collects statistics about the sizes of the generated by the compiler and the amount of traffic handled on switches as opposed to the controller because space for is a limited resource on switches and because the cost of a packet to the controller down its processing by orders of magnitude these metrics quantify some of the most critical performance aspects of the system we compared the performance of the full which makes use of all rules including and which only generates rules also known as rules compilers on the following programs · static policy experiment implements the simple static policy described at the beginning of section this benchmark measures the of compilation strategies based on generating rules · static policy with query experiment packets using the same policy as in but also collects traffic statistics for each host due to this collection this program cannot be directly compiled to a switch least not without expanding all possible hosts thus this benchmark measures the efficiency of reactive specialization · policy experiment packets and collects traffic statistics using the authentication application presented in section this benchmark measures the performance of a more realistic application implemented using to these experiments we generated packets using fs a tool that realistic packet traces from several statistical parameters we ran each experiment on k packets in total for the and benchmarks we generated traffic with active hosts sending packets to an external network for seconds each for the benchmark we generated traffic with hosts a class c network sending traffic to the authentication server and an external network for seconds each the results of the experiments are shown in figure the graphs on the top row show the number of packets that and had to be sent to the controller against the total number of packets processed likewise the graphs on the bottom row show the size of the compiled classifier in terms of number of rules versus total packets the table at the right gives the final results after all k packets were processed in terms of the of packets processed on switches the full compiler the com on nearly all of the benchmarks on the benchmark the full compiler generates a classifier that completely handles the policy so no packets are sent to the controller the line for the full compiler with the the compiler of course a packet to the controller for each distinct generating k rules in total on the benchmark the full compiler generates wildcard rules using reactive specialization that handle all future traffic from each unique host after a packet from it these rules handle many more packets than the rule produced by the compiler on this benchmark it is worth noting that the produced by the full compiler are larger than the ones produced by the compiler especially initially this is due to the fact that the full compiler generates multiple rules in response to a single controller packet to cover a broad space of future similar packets whereas the compiler generates a single for each controller packet one can see that the work done by the full compiler off in terms of the number of packets that must be to the controller moreover over time the size of the classifier approaches that of the full compiler the benchmark demonstrates that the full compiler generates more effective than the compiler even in the presence of functions that it cannot analyze directly note that a large number of packets must be to the controller in any correct they the is not invariant for any host however the full compiler quickly to a classifier that processes all traffic directly on the switch related work building on ideas first proposed in and d was the first concrete system to what is currently known as it provides an interface to and requires that programmers write reactive programs using callbacks and explicit rules there are numerous examples of network applications built on top of using but relatively few that use wildcard rules though load is a nice example of the latter researchers are now developing controller platforms some of them such as designed for java and designed for haskell provide elegant interfaces for new programming languages others such as and improve scalability and fault through parallelization and distribution none of these systems automatically generate reactive protocols or provide formal semantics or correctness guarantees like netcore does both netcore and use highlevel languages to program infrastructure but the end there programs are written in an explicitly distributed style whereas highlevel netcore programs are written as if the program has an view of the entire network the netcore implementation automatically partitions work onto a distributed set of switches and a reactive communication protocol that simulates the semantics of the highlevel language part of the job of the netcore compiler is to generate efficient packet most previous research in this area see taylor for a survey focuses on static compilation the netcore compiler generates in the face of policies with unknown functions and a distributed implementation and compile rich and monitoring programs designed to secure networks and detect down to special hardware and the main difference between netcore and all of these systems is that they are limited to a single device they do not address the issue of how to program complex dynamic policies for a collection of switches and they do not synthesize the distributed communication patterns between the switches and controller active as in the project shares many highlevel goals with but the implementation strategy is entirely different the former uses smart switches to interpret programs in packets while the latter uses switches controlled by a remote host acknowledgments we wish to thank and and the anonymous popl reviewers for discussions about this research and comments on of this paper our work is supported in part by grants n and n and nsf grants and any and are those of the authors and do not necessarily reflect the views of the or nsf references a control platform see g and g the abstract machine in popl pages ­ z a and t ng a system for scalable control technical report tr rice university dec m m j j n n and s network control trans on aug m chen x li r j lin l t and r achieving high performance from compiled network applications while enabling ease of programming in pldi m c e and h towards language support for packet processing in pages ­ s and g an optimizing compiler for rules security systems d et al a of virtual machine mobility in an network aug at acm n foster r m c j a and d walker a network programming language in icfp a g d a myers j g h j and h a clean d approach to network control and management ccr ­ october n t j b m n and s towards an operating system for networks ccr n s m n and r web traffic using aug at acm b s p y p s and n energy in data center networks in l m and j online measurement of large traffic on switches in mar t m n j l m r y h t and s a distributed control platform for production networks in oct b j i and r declarative routing extensible routing with declarative queries in pages ­ n t h g l j s and j turner enabling in networks ccr ­ a a n and r dynamic access control in networks in aug the open foundation mar see http see a system for detecting network in realtime computer networks dec j r b p m and n efficient flow record generation in pages ­ d taylor survey and of packet classification techniques acm comput ­ september a and p hudak functional reactive programming of networks in jan r wang d and j server load in mar k m kobayashi r t m chan n and n research in mobile networks ccr ­ 