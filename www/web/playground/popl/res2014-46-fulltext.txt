toward general of static errors andrew c myers department of computer science cornell university ny abstract we introduce a general way to programmer that are detected by static analyses such as type checking the program analysis is expressed in a constraint language in which result in unsatisfiable constraints given an unsatisfiable system of constraints both satisfiable and unsatisfiable constraints are analyzed to identify the program expressions most likely to be the cause of the of different error is evaluated under the assumption that the programmers code is mostly correct so the simplest are chosen following bayesian principles for analyses that rely on assumptions the also identifies assumptions likely to have been omitted the new error approach has been implemented for two very different program analyses type inference in ocaml and information flow checking in the effectiveness of the approach is evaluated using previously collected programs containing errors the results show that when compared to existing compilers and other tools the general technique identifies the location of programmer errors significantly more accurately categories and subject descriptors d testing and debugging d security and protection information flow f semantics of programming languages program analysis keywords error static program analysis type inference information flow introduction sophisticated type systems and other program analyses enable verification of complex important properties of software advances in type inference dataflow analysis and constraint solving have made these verification methods more practical by reducing both analysis time and annotation burden however the impact on practice is we that a key barrier to adoption of sophisticated analyses is that debugging is difficult when the analysis reports an error when deep nonlocal software properties are being checked the analysis may detect an inconsistency in a part of the program far permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ san diego ca usa copyright c acm from the actual error resulting in a error message determining from this message where the true error lies can require an complete understanding of how the analysis works we are motivated to study this problem based on experience with two programming languages ml whose type inference algorithm sometimes generates complex even error messages and a version of java that statically analyzes the security of information flow within programs but whose error messages also programmers prior work has explored a variety of methods for improving error in each of these languages although these methods are usually specialized to a single language and analysis they still frequently fail to identify the location of programmer in this work we take a more general approach most program analyses including type systems and type inference algorithms can be expressed as systems of constraints over variables in the case of ml type inference variables stand for types constraints are equalities between different type expressions and type inference succeeds when the corresponding system of constraints is satisfiable when constraints are unsatisfiable the question is how to report the failure indicating an error by the programmer the standard practice is to report the failed constraint along with the program point that generated it unfortunately this simple approach often results in error actual error may be far from that program point another approach is to report all expressions that might contribute to the error eg but such reports are often and hard to understand our insight is that when the constraint system is unsatisfiable a more approach should be taken rather than looking at a failed constraint in isolation the structure of the constraint system as a whole should be considered the constraint system defines paths along which information propagates both satisfiable and unsatisfiable paths can help the error an expression involved in many unsatisfiable paths is more likely to be erroneous an expression that lies on many satisfiable paths is more likely correct this approach can be justified on bayesian under the assumption captured as a prior distribution that code is mostly correct in some languages the satisfiability of constraint systems depends on assumptions which we call hypotheses the same general approach can also be used to identify hypotheses likely to be missing a small weak set of hypotheses that makes constraints satisfiable is more likely than a large strong set contributions this paper presents the following contributions a general constraint language that can express a broad range of program analyses we show that it can encode both ml type inference and information flow analysis as well as other analyses including many dataflow analyses section a general algorithm for identifying likely program errors based on the analysis of a constraint system extracted from the pro let move list list let rec loop lst x y dir acc if lst then acc else print string foo in loop lst figure ocaml example line is for a at line public final byte this public void m this this try for int i i catch e figure example line is for a at line gram using a bayesian posterior distribution the algorithm suggests program expressions that are likely errors and offers hypotheses that the programmer is likely to have omitted sections and an evaluation of this new error algorithm on two different sets of programs written in ocaml and as part of this evaluation we use a large set of programs collected from using ocaml to do programming assignments section results do not rely on approach our general approach to errors can be illustrated through examples from two languages ml and ml type inference the power of type inference is that programmers may omit types but when type inference fails the resulting error messages can be consider figure containing simplified ocaml code written by a working on a programming assignment the ocaml compiler reports that the expression at line is a list but is used with type unit however the programmers actual fix shows that the error is the print string expression at line the report arises because currently error methods eg in ocaml sml and haskell unify types according to type constraints or typing rules and report the last expression considered the one on which unification fails however the first failed expression can be far from the actual error since early unification using an erroneous expression may lead type inference down a path of incorrect in our example the inference algorithm ie the types of the four expressions in a particular order built into the compiler one of those expressions is because the inconsistency is detected when unifying its type prior work has to address this problem by either the complete slice of the program relating to a type inference failure or a smaller subset of unsatisfiable constraints unfortunately both variants of this approach can still require considerable manual effort to identify the actual error within the program slice especially when the slice is large label checking error messages are not unique to traditional type inference the analysis of information flow security which checks a different kind of nonlocal code property can also generate messages when security cannot be verified is a language whose static analysis of information flow often generates error messages figure shows a simplified version of code written by a programmer programs are similar to java programs except that they specify security labels in the example omitted labels such as the label of i at line are inferred automatically however label inference works differently from ml type inference algorithms the type checker generates constraints on labels creating a system of inequalities that are then solved iteratively for instance the compiler generates a constraint this for line bounding the label of the argument by that on the formal parameter to write which is this because of type error messages are a product of the iterative process used to solve these constraints the solver uses a process that involves both raising lower bounds and upper bounds on labels to be solved for errors are reported when the lower bound on a label cannot be bounded by its upper bound as with ml early processing of an incorrect constraint may cause the solver to detect an inconsistency later at the wrong location in this example reports that a constraint at line is wrong but the actual programmer is the label at line permits programmers to specify assumptions capturing trust relationships that are expected to hold in the environment in which the program is run a common reason why label checking fails in is that the programmer has these assumptions wrong sharing constraints on ml functor parameters are also assumptions but are simpler and less central to ml programming for instance an assignment from a memory location labeled with a security label to another location with a label might fail to because the crucial assumption is missing that the acts for the that assumption would imply that an information flow from to is secure in this paper we propose a unified way to infer both program expressions likely to be wrong and assumptions likely to be missing overview of the approach as a basis for a general way to errors we define an expressive constraint language that can encode a large class of program analyses including not only ml type inference and label checking but also dataflow analyses constraints in this language assert partial orderings on constraint elements these constraints are then converted into a representation as a directed graph in that graph a node represents a constraint element and a directed edge represents an ordering between the two elements it for example figure is part of the constraint graph generated from the ocaml code of figure each node represents either the type of a program expression or a declared type in the figure nodes are annotated with the line numbers of that expression or declaration each edge represents one constraint generated by an ocaml typing rule for example the leftmost node represents the type of the result of print string which is unit since function loop can return this result the leftmost node is connected by edges to the node representing the result type of loop at line type inference fails if there is at least one unsatisfiable path within the constraint graph indicating a sequence of that generate a contradiction consider for example the three paths p p and p in the figure the end nodes of each path must unit loop ret p a list f ret list p list acc p acc figure part of the constraint graph for the ocaml example represent the same types other such inferred paths exist such as between the node for unit and the node for variable acc but these paths are not shown since a path with at least one variable on an end node is trivially satisfiable we call paths that are not trivially satisfiable such as p p and p the informative paths in this example the paths p and p are unsatisfiable because the types at their are different note that path p corresponds to the expressions in the ocaml code by contrast path p is satisfiable the constraints along unsatisfiable paths form a complete explanation of the error but one that is often too our goal is to be more useful by where along the path the error occurs the key insight is to analyze both satisfiable and unsatisfiable paths in figure the strongest candidate for the real source of the error is the leftmost node of type unit rather than the expression of type list that features in the error report produced by ocaml two general heuristics help us identify unit as the all else equal an explanation for in which programmers have made fewer is more likely this is an application of in this case the minimum explanation is a single expression the unit node which appears on both unsatisfiable paths the unit node appears only on unsatisfiable informative paths but not on the informative satisfiable path p since erroneous nodes are less likely to appear in satisfiable paths the unit node is a better error explanation than any node on path p these two heuristics rely only on graph structure and are to the language and program being the same generic approach can therefore be applied to very different program analyses our tool correctly and precisely points out the actual error in both the ocaml and examples above in addition to identify incorrect expressions the constraint graph also provides enough information to identify assumptions that are likely to be missing constraint language central to our approach is a general core constraint language that can be used to capture a large class of program analyses in this constraint language constraints are inequalities using an ordering that corresponds to a flow of information through a program the constraint language also has constructors and destructors corresponding to computation on that information syntax the syntax of the constraint language is formalized in figure g g g a a c c c i in n i e e e ce e e e e figure syntax of constraints the toplevel goal g to be solved is a conjunction of assertions a each with the form c c where constraint c is the hypothesis that is assumption and constraint c is a conclusion to be satisfied a constraint c either as the hypothesis or as the conclusion of an assertion is a possibly empty conjunction of inequalities i over elements from e based on the ordering we denote an empty conjunction as and abbreviate c as c an element e may be a variable var whose value is to be solved for an application of constructor c con or the ith argument to a constructor application represented by the arity of constructor c is represented as ac constants c are constructors with arity the ordering is treated abstractly but it must define a lattice with the usual join and meet operators which can be used as syntax the bottom and top of the element ordering are and example to model ml type inference we can represent the type as a constructor application bool where int and bool are constants its first projection bool is int consider the expressions acc line and print string line in figure these are branches of an if statement so one assertion is generated to enforce that they have the same type acc unit unit acc section describes in more detail how assertions are generated for ml interpretation of constraints the partial ordering on two applications of the same constructor is determined by the of that constructors arguments for each argument the ordering of the applications is either covariant with respect to that argument denoted by contravariant with respect to that argument or invariant with respect to it more general partial ordering rules on constructors eg a rule cx y cy x can also be handled by our inference algorithm in in a manner similar to the handling of and though with increased complexity with these abstract definitions the validity of constraints can be defined in a natural way a goal g is valid if all assertions it contains are valid an assertion c c is valid if the partial orderings in c are from c using just the lattice properties of the relation and the of the various constructor arguments example let a b c be three distinct constants then a b b c a c is valid by the transitivity of assertion a a b is valid by the definition of join assertion a b is invalid the empty assumption does not the conclusion satisfiability validity as defined so far works for constraints without variables when constraints mention variables they are satisfiable if there exists a valuation of all variables such that the goal after value substitution is valid satisfiability depends on the ground terms t that a variable can map into let t be the greatest fixed point of the following rules · all constants are in t · ct t if ti t and c con notice that ground terms may be infinite this feature is essential for modeling recursive types a valuation var t is a function from variables to ground terms a goal is satisfiable when there exists a valuation such that the goal is valid after substitution using example let var a b c t then a is trivially satisfiable by the valuation a or however a b is unsatisfiable since otherwise b a by the transitivity of yet this ordering on a and b is not expressiveness the constraint language is the interface between various program analyses and our tool to use this tool the program analysis must the compiler or analysis to express a given program analysis as a set of constraints in the constraint language as we now show the constraint language is expressive enough to capture a variety of different program analyses of course the constraint language is not intended to express all program analyses such as analyses that involve arithmetic we leave a larger class of analyses into our framework as future work ml type inference ml type inference maps naturally into constraint solving since typing rules are usually equality constraints on types numerous efforts have been made in this direction eg most of these are similar so we discuss how algorithm t can be into our constraint language extending the approach of and we follow that approach since it supports further our evaluation builds on an implementation of that approach for simplicity we only discuss the subset of ml whose syntax is shown in figure however our implementation does support a much larger set of language features including match expressions and userdefined data types in this language subset expressions can be variables x integers n binary operations functions abstractions fn x e function applications e e or let bindings let x e in e notice that is allowed such as an expression let id fn x x in id the typing rules that generate constraints are shown in figure types t can be type variables to be inferred the integer type int and function types constructed by the typing rules have the form e t c is a typing environment that maps a variable x to a set of types intuitively tracks a set of types with which x must be consistent let be an environment that maps all variables to and x t be a map identical to except for variable x is a pointwise union for all type variables x x x x as before c is a constraint in our language it captures the type equalities that must be true in order to give e the type t note that a type equality t t is just a shorthand for the assertion t t t t most of the typing rules are straightforward to typecheck fn x e we ensure that the type of x is consistent with all in e which is done by requiring x t for all t t the mapping x is since x is bound only in the function definition the rule for is more complicated because of the inferred type of e t may contain free type variables to support we generate a fresh variant of t c where free type variables are replaced by fresh ones for each use of x in e these fresh variants are then required to be equal to the corresponding uses of x creating one variant for each use in the rule for may increase the size of generated constraints and hence make our error algorithm more expensive however we find e x n e e fn x e e e let x e in e t int t t x x x x n int e t c e t c e e int t int t int c c e t c x t fn x e x x t t t x t c e t c e t c e e t t c c e t c e t c x t tn let x e in e x t c c c where t c k tk ck k max n are fresh variants of t c i c ci and ik ik c t t tn tn figure constraint generation for a subset of ml and x are fresh variables in typing rules performance is still reasonable with this approach one way to avoid this limitation is to add constrained types as in we leave that as future work informationflow control in informationflow control systems information is tagged with security labels such as or top secret such security labels naturally form a lattice and the goal of such systems is to ensure that all information flows upward in the lattice to demonstrate the expressiveness of our core constraint language we show that it can express the information flow checking in the language to the best of our knowledge ours is the first general constraint language expressive enough to model the challenging features of label inference and checking statically analyzes the security of information flow within programs all types are annotated with security labels drawn from the label model information flow is checked by the compiler using constraint solving for instance given an assignment x y the compiler generates a constraint ly lx meaning that the label of x must be at least as restrictive as that of y the programmer can omit some security labels and let the compiler generate them for instance when the label of x is not specified assignment x y generates a constraint ly x where x is a label variable to be inferred hence constraints are similar in structure to our general constraint language however some features of are challenging to model label model the basic building block of the is a set of principals representing users and other authority entities principals are structured as a lattice with respect to a relation the proposition a b means a is at least as as b security policies on information are expressed as labels that mention these principals for example the confidentiality label means that the principal permits the principal to learn the labeled information principals can be used to construct integrity labels as well for example consider the following code int x int y x int z if z y the two assignments generate two satisfiable assertions y y the principals and are constants and the covariant constructor p represents confidentiality labels a confidentiality policy can be treated as a covariant constructor on principals integrity policies are dual to confidentiality policies so they can be treated as contravariant constructors on principals the proof can be found in the associated technical report label polymorphism label polymorphism makes it possible to write code that is not to any specific security policy for instance consider a function foo with the signature int b instead of requiring the parameter b to have exactly the label aa the label serves as an upper bound on the label of the actual parameter modeling label polymorphism is straightforward using hypotheses the constraint c b a a is added to the hypotheses of all constraints generated by the method body where the constant c b represents the label of variable b method constraints methods in may contain where clauses explicitly stating constraints assumed to hold true during the execution of the method body the compiler the method body under these assumptions and ensures that the assumptions are true at all method call sites in the constraint language method constraints are modeled as hypotheses dataflow analysis dataflow analysis is used not only to optimize code but also to check for common errors such as uninitialized variables and unreachable code classic instances of dataflow analysis include reaching definitions live variable analysis and constant propagation aiken showed how to formalize dataflow analysis algorithms as the solution of a set of constraints with equalities over the following elements a subclass of the more general set constraints in e a an e e e e where a an are constants is a constraint variable elements represents sets of constants and ¬ are the usual set operators consider live variable analysis let and be the set of program variables that are defined and used in a statement s and let be the statement executed immediately after s two constraints are generated for statement s sin x where sin are constraint variables our constraint language is expressive enough to formalize com dataflow analyses since the constraint language above is nearly a subset of ours set inclusion is a partial order and negation can be eliminated by preprocessing in the common case where the number of constants is finite eg is finite errors and recall that the goal of this work is to the cause of errors therefore we are interested not just in the satisfiability of a set of assertions but also in finding the best explanation for why they are not satisfiable failures can be caused by both incorrect constraints and missing hypotheses incorrect constraints one cause of is the existence of incorrect constraints appearing in the conclusions of assertions constraints are generated from program expressions so the presence of an incorrect constraint means the programmer the wrong expression missing hypotheses a second cause of is the absence of constraints in the hypothesis the absence of necessary hypotheses means the programmer omitted needed assumptions in our approach an explanation for may consist of both incorrect constraints and missing hypotheses to find good we proceed in two steps the system of constraints is first converted into a representation as a constraint graph section this graph is then analyzed using bayesian principles to identify the most likely to be correct section constraint graph the core constraint language has a natural graph representation that enables analyses of the system of constraints in particular the satisfiability of the constraints can be tested via reachability in the graph running example we use the following example throughout this section to illustrate the key ideas behind the constraint graph representation example consider the following set of constraints bool ty ty int we interpret here as the subtyping relation the constructor e represents the function type e e note that the constructor fn is contravariant in its first argument and covariant in its second the identifiers ty ty bool int are distinct constants and are type variables to be inferred the first assertion claims that is a subtype of bool with no hypotheses the third assertion is similar the second assertion says that is a subtype of under the assumption that ty is a subtype of ty to determine whether this goal is satisfiable we construct a constraint graph to infer partial orderings that must hold based on these constraints and the builtin inference rules associated with the relation the constructors used and the operators and constraint graph construction the graph contains a node for each distinct element in the constraint system for each partial ordering e e appearing in assertion conclusions a directed edge exists from e to e representing the legal flow of information we call this edge an edge hypotheses of assertions are recorded on the edges generated by the corresponding conclusions we denote an edge annotated by hypothesis h as for instance the second constraint in our running example ty ty generates an edge ty from node to node additional constructor edges in the constraint graph represent the action of constructors constructor edges connect the constructors arguments to the element representing its result for example ty bool fn fn a constructor edges ty bool ty int fn fn fn fn b full constraint graph ty ty c hypothesis graph figure constraint graph generated from unsatisfiable constraints there would be a constructor edge to the node representing the element bool from each of the nodes for ty and bool as illustrated in figure a constructor edges include the following annotations the constructor name the argument position and the variance of the parameter covariant contravariant or invariant for instance the edge labeled fn the first argument to the constructor application for each constructor edge there is also a dual decomposition edge that the constructor application back to its arguments it is distinguished by an above the constructor name in the graph and has the same variance for example fn to simplify reasoning about the graph edges are also duplicated in the reverse direction with negative variance thus the first assertion in the example bool generates a edge from to bool and a edge in the other direction as illustrated in figure a the constraint graph generated using all three assertions from the example is shown in figure b the dotted arrow formal construction of the constraint graph figure formally presents a function a that translates a set of assertions a an into a constraint graph with annotated edges the graph is represented in the translation as a set of edges defined by the set edge the nodes of the constructed graph are implicitly defined by their connecting edges nodes are drawn from the set node which consists of the legal elements e modulo the least equivalence relation that satisfies the commutativity of the operations and and that is preserved by the productions in figure as shown there are three kinds of edges the edges annotated with hypotheses are generated by the translation rule for ac e e and by the rules for meets and joins constructor edges are generated by the rules enc and which connect a constructor application to its arguments invariant arguments generate edges as though they were both covariant and contravariant so twice as many edges are generated inferring node orderings the constraint graph inferring all relationships that can be proved using the corresponding constraints the idea is to construct a contextfree grammar shown in figure whose productions correspond to inference rules for relationships to perform inference each production is interpreted as a reduction rule that replaces the righthand side with the single edge appearing on the lefthand side for instance the transitivity of is expressed by the first grammar production which derives h from consecutive edges and where p is some variance the inferred edge has hypotheses h and h since the inferred partial ordering is valid only when both h and h hold n node node element e edge n pi n pi n graph edge ag graph graph aa an in ac i in ac ii in ac e e e e ec ec e c enc pi en in pi en ei pi e pi where pi is the variance of argument i to constructor cons ee ec e e i e ei e e ec e ei i e e figure construction of the constraint graph h where c con i ac p and figure contextfree grammar for inference the power of contextfree grammars is needed in order to handle reasoning about constructors in the running example applying transitivity to the constraints yields ty ty int bool then because fn is contravariant in its first ment we derive ty ty similarly we can derive int bool the dotted arrow in figure b to capture this kind of reasoning we use the first two productions in figure in our example of figure b the path from ty to ty has the following edges fn ty fn these edges reduce via the first and then the second production to an edge ty from ty to ty note that the variance is because the first constructor argument is contravariant similarly we can infer another ty edge from int to bool the third grammar production in figure is the dual of the second production ensuring the invariant that each edge has an inverse edge in our example of figure b there is also an edge ty from ty to ty derived from the following edges fn ty fn these edges reduce via the first and then the third production to an edge ty from ty to ty computing all edges according to the contextfree grammar in figure is an instance of reachability which is in the literature and has been used for a number of applications we adapt the dynamic programming algorithm of et al to find shortest paths we call such paths supporting paths since the hypotheses along these paths justify the inferred edges we extend this algorithm to also handle join and meet nodes take join nodes for instance meet is handled the rule e e e e e e e can be used in two directions the direction from left to right is already handled when we construct edges for join elements figure to use the rule in the other direction we use the following procedure when a new edge n is processed for each join element e where n is an argument of the operator we add an edge from e to n if all arguments of the operator have a edge to n checking the satisfiability of edges a edge whether inferred or specified directly in an assertion is added to the graph only when the corresponding ordering is by the constraints along the supporting path hence the constraints along the path must be unsatisfiable if the partial ordering on the end nodes is unsatisfiable when either end node of a edge is a variable or a node where at least one argument of is a variable the edge is trivially satisfiable and hence not informative for error for simplicity we ignore such edges and refer subsequently only to informative edges two informative edges can be inferred in figure b these edges are int bool and ty ty though only the first is shown a edge holds only if all hypotheses on the edge hold too therefore the satisfiability of an edge n is equivalent to the satisfiability of the assertion c n n in our running example the combined hypotheses along both informative edges are ty ty therefore satisfiability of the constraint system reduces to satisfiability of these assertions ty ty int bool ty ty ty ty to check the satisfiability of these assertions we test if the conclusion can be proved from all constraints in the hypothesis recall that a constraint graph the inference of all provable partial ordering given a set of constraints therefore a hypothesis graph is constructed in exactly the same way as the constraint graph to find all provable relations specifically to test if an edge n is satisfiable we construct a hypothesis graph using c as described in section and find all edges as described in section edge n is unsatisfiable if the relationship n cannot be inferred from the hypothesis graph using c for our running example the hypothesis graphs for both informative edges are the same from this graph shown in figure c int bool is not provable the constraints along the supporting path from int to bool form a proof of satisfiable and unsatisfiable paths when the partial ordering on the end nodes of a path is invalid we say that the path is unsatisfiable unsatisfiable paths are helpful because the constraints along the path explain why the inconsistency occurs also useful for error is the set of satisfiable paths paths where there is a valid partial ordering on any two nodes on the path for which a relationship can be inferred any remaining paths are ignored in our error algorithm since by definition they must contain at least one unsatisfiable for brevity we subsequently use the term unsatisfiable path to mean a path that is unsatisfiable ranking the algorithm in section identifies unsatisfiable paths in the constraint graph which correspond to sets of unsatisfiable constraints expressed by our constraint language although the information along unsatisfiable paths already captures why the goal is unsatisfiable all constraints along a path may give more information than the programmer can digest our approach is to use bayesian reasoning to identify programmer errors more precisely a bayesian interpretation the cause of errors can be wrong constraints missing hypotheses or both to keep our method as general as possible we avoid building in domainspecific knowledge about programmers tend to make however the framework does permit adding such knowledge in a straightforward way the entity about which errors are reported can be specific to the language ocaml reports typing errors in expressions whereas reports errors in informationflow constraints to make our approach general we treat entities as an abstract set and assume a mapping from entities to constraints we assume a prior distribution on entities p defining the probability that an entity is wrong similarly we assume a prior distribution p on hypotheses defining the probability that a hypothesis is missing given entities e and hypotheses h we are interested in the probability that e and h are the cause of the error observed in this case the observation o is the satisfiability of informative paths within the program we denote the observation as o o o on where oi sat represents or satisfiability of the corresponding path the observation follows some unknown distribution po we are interested in finding a subset e of entities and a subset h of hypotheses for which the posterior probability p e ho is large meaning that e and h are likely causes of the given observation o in particular a maximum a priori estimate is a pair e h at which the posterior probability takes its maximum value that is at arg p e ho by theorem p e ho is equal to hp oe the factor does not vary in the variables e and h so it can be ignored assuming the prior distributions on and are independent a simplified term can be used oe h pe is the prior knowledge of the probability that a set of entities e is wrong in principle this term might be by learning from a large of programs or using heuristics for simplicity and generality we assume that each entity is equally likely to be wrong we leave the of knowledge to future work we also assume the probability of each entity being the cause is independent hence pe is by pe where p is a constant representing the that a single entity is wrong ph is the prior knowledge of the probability that hypotheses h are missing of course not all hypotheses are equally likely to be wrong for example the hypothesis is too strong to be useful it makes all constraints succeed the likely missing hypothesis is both weak and small our general heuristics for obtaining this term are discussed in section p oe h is the probability of observing the constraint graph given that entities e are wrong and hypotheses h are missing to estimate this factor we assume that the satisfiability of the remaining paths is independent this allows us to write p oe h i p h the term p h is calculated using two heuristics for an unsatisfiable path either something along the path is wrong or adding h to the hypotheses on the path makes the partial ordering on end nodes valid so p oi h is equal to in this case and is otherwise a satisfiable path is with some constant probability p to contain a wrong entity since adding or removing h does not affect a path that is already satisfiable p oi h is not affected by h hence we have p oi h p if path pi contains a constraint generated by some entity in e otherwise p oi h p the first heuristic suggests we only need to consider the entities and hypotheses that explain all unsatisfiable paths otherwise p h for some oi by heuristic we denote this set by g suppose a constant paths are satisfiable and entities e appear on ke of them then based on the simplifying assumptions made we have arg max oe h e h arg max e h arg max ph e h g an intuitive understanding of this is that the cause must explain all unsatisfiable paths the wrong entities are likely to be small e is small and not used often on satisfiable paths since p p by heuristic the missing hypothesis is likely to be weak and small as defined in section which the term ph although this is affected by the values of p and p empirical study suggests that the result is to their values across a broad range see section inferring likely wrong entities the term can be used to calculate the that a subset of entities is the cause however its computation for all possible sets of entities can be therefore we propose an instance of a search based on novel heuristics to calculate optimal solutions in a practical way it seems likely that the precision of our approach could be improved by refining this assumption since the locations in our evaluation usually occur when the programmer makes a similar error multiple times a search is a heuristic search algorithm for finding solution nodes in a graph of search nodes in our instance of the algorithm each search node n represents a set of entities wrong denoted en a solution node is one that explains all unsatisfiable corresponding entities appear in all unsatisfiable paths an edge corresponds to adding a new entity to the current set the key to making a search effective is a good cost function f n the cost function is the sum of two terms gn the cost to reach node n and hn a heuristic function the cost from n to a solution before defining the cost function f n we note that the is equivalent to ce where c log p and c p are both positive constants because p and p hence the cost of reaching n is gn to obtain a good estimate of the remaining is the heuristic function insight is to use the number of entities required to cover the remaining unsatisfiable paths denoted as since c is usually larger than c more specifically hn if otherwise hn c if is covered by one single entity hn c otherwise an important property of the heuristic function is its optimality all and only the most likely wrong subsets of entities are returned the proof is included in the associated technical report the heuristic search algorithm is also efficient in practice on current hardware it takes about seconds when the search space is over more performance details are given in section since the remaining part of our instance of a search is largely standard we leave the details in the technical report the only nonstandard feature is that the search stops when a is found rather than when the first is found since we are interested in all suggestions inferring missing hypotheses another factor in the bayesian interpretation is the that hypotheses assumptions are missing recall that a path from element e to e in a constraint graph is unsatisfiable if the conjunction of hypotheses along the path is to prove the partial ordering e e so we are interested in inferring a set of missing hypotheses that are sufficient to repair unsatisfiable paths in a constraint graph motivating example consider the following assertions bob alice bob bob alice bob alice since the only hypothesis we have is bob meaning is more than bob none of the three constraints in the conclusion holds one trivial solution is to add all invalid conclusions to the hypothesis this approach would add alice bob alice alice to the hypotheses however this naive approach is for two reasons an invalid hypothesis may the program analysis for instance adding an information flow to the hypotheses can violate security the programmer has the task of checking the correctness of every hypothesis a program analysis may combine static and dynamic approaches for instance although most label checking is static some hypotheses are checked dynamically so a large hypothesis may also runtime performance it may also be to select the minimal missing hypothesis but this approach does not work well either a single assumption is always a minimal missing hypothesis for all unsatisfiable paths given any partial order e e can be proved since e e however this assumption is obviously too strong to be useful intuitively we are interested in a solution that is both weakest and minimal in the example above our tool returns a hypothesis with only one constraint alice bob both weakest and minimal we now formalize the minimal weakest missing hypothesis and give an algorithm for finding this missing hypothesis missing hypothesis consider an unsatisfiable path p that supports an edge e n for simplicity we denote the hypothesis of p as hp c and the conclusion cp n n we define a missing hypothesis as follows definition given unsatisfiable paths p p p pn a set of inequalities s is a missing hypothesis for p iff pi p is i intuitively adding all inequalities in the missing hypothesis to the assertions hypotheses removes all unsatisfiable paths in the constraint graph example returning to the example in section it is easy to verify that alice bob is a missing hypothesis that makes all of the assertions valid finding a minimal weakest hypothesis we are not interested in all missing hypotheses instead we want to find one that is both minimal and as weak as possible to simplify the notation we further define the conclusion set of unsatisfiable paths p as the union of all conclusions cp pi p the first insight is that the inferred missing hypothesis should not be too strong definition for a set of unsatisfiable paths p a missing hypothesis s is no weaker than s iff i s p p hp i i i s that is s is no weaker than s if all inequalities in s can be proved from s using at most one existing hypothesis given this definition the first property we show is that every subset of cp that forms a missing hypothesis is weak lemma s cp s is a missing hypothesis no missing hypothesis is strictly weaker than s proof suppose there exists a strictly weaker missing hypothesis s since s is a missing hypothesis i s i all i since s cp i s i s i no weaker than s contradiction for i so s is the lemma above suggests that subsets of cp may be good candidates for a weak missing hypothesis however they are not necessarily minimal for instance the entire set cp is a weak missing hypothesis to remove the redundancy in this weakest hypothesis we observe that some of the conclusions are subsumed by others to be a more general form of missing hypothesis might infer individual hypotheses for each path but it is less feasible to do so more specific we say a conclusion ci subsumes another conclusion cj if ci cj intuitively if ci subsumes cj then adding ci to the hypothesis of pj makes pj satisfiable example return to the example in section the missing hypothesis alice bob is both the weakest and minimal based on lemma and the definition above finding a minimal weakest missing hypothesis in cp is equivalent to finding the minimum subset of cp which subsumes all c cp this gives us the following algorithm algorithm given a set of unsatisfiable paths p p p pn construct the set cp from the unsatisfiable paths for all ci cj in cp add cj to set si the set of conclusions subsumed by ci if ci subsumes cj find the minimum cover m of cp where s s sn and m s return ci si m a force algorithm for finding the minimal weakest missing hypothesis may check all possible hypotheses that is on the order of n the number of all subsets of orderings on elements where n is the total number of elements used in the constraints while the complexity of our algorithm is exponential in the number of unsatisfiable paths in the constraint graph this number is usually small in practice so the computation is still feasible evaluation implementation we implemented our general error tool in java the implementation includes about lines of source code comments and lines as input the tool reads in constraints following the syntax of figure the program analyses to be must be modified to emit those constraints to evaluate our error tool on realworld program analyses we modified the compiler and an extension to the ocaml compiler to generate constraints in our constraint language format is an extension of ocaml that generates the labeled constraints defined in generating constraints in our language format involved only effort changes to the compiler include about loc lines of code above more than loc in the compiler changes to include about loc above the loc of the extension slightly more effort is required for because that compiler did not track the locations of type variables this functionality had to be added to trace constraints back to the corresponding source code case study ocaml error to evaluate the quality of our ranking algorithm we used a of previously collected ocaml programs containing errors collected by et al the data were collected from a course for with at least two years software development experience the data from assignments and in the class each assignment requires to write ­ lines of code from the data we analyzed only type errors which correspond to unsatisfiable constraints errors such as values or too many arguments to a constructor are more easily and are not our focus we also programs using features not supported by and files where the users fix is after these files samples remain analysis analyzing a file and the quality of error report message manually can be inherently we made the following efforts to make our analysis less instead of which error message is more useful we whether the error locations the tools reported were correct to the actual error in the program we use the users changes with larger timestamps as a reference files where the error location is are in our evaluation to ensure the tools return precisely the actual error a returned location is as correct only when it is a subset of the actual error locations one of correctness is that multiple locations can be good suggestions because of for instance consider a simple ocaml program let x true in x even if the programmer later changed true to be some integer the error of the of x and the use of x are still considered to be correct since they bind to the same expression as the fix however the operation and the integer are not since the fix is not related since the ocaml error message reports an expression that appears to have the wrong type to make the reports comparable we use expressions as the program entities on which we run our inference tool reports likely wrong expressions in evaluation recall that our tool can also generate reports of why an expression has a wrong type corresponding to unsatisfiable paths in the constraint graph using such extra information might improve the error message but we do not use that capability in the evaluation another is that our tool inherently reports a small set of program entities expressions in this case with the same quality whereas ocaml reports one error at one time to make the comparison fair we make the following efforts for cases where we report a better result our tools finds the error location that ocaml misses we ensure that all locations returned are correct for other cases we ensure that the of the suggestions are correct moreover the average top rank size is smaller than therefore our evaluation results should not be affected much by the fact that our tool can offer multiple suggestions sensitivity recall that the of entities e being an error is equivalent to the term ce where c log p and c p see section hence the ranking is only affected by the ratio between c and c to test how sensitive our tool is to the choice of p and p we collect two important statistics for a wide range of p and p values the number of programs where the actual error is missing in top rank suggestions among programs the average number of suggestions in the top rank the result is summarized in table we the columns in table such that for any p p decreases exponentially from left to right the last column corresponds to the special case when p the overall quality is best when p p where p p p however the quality of the time in seconds graph building time ranking time constraint graph size of nodes figure performance tions is close for any p and p st p p p the results are not very sensitive to the choice of these parameters if satisfiable paths are ignored p that is c the size is much larger and more errors are missing hence using satisfiable paths is important to quality the quality of the error report is also considerably worse when p is very large relative to p p p this result shows that paths are more important than successful paths but that too importance to the paths eg at p p also the quality of the error report comparison with ocaml and for each file we analyze we consider both the error location reported by ocaml and the of our tool based on the setting p p p we reused the data by the authors of the tool who labeled the correctness of error location report we classify the files into one of the following five categories and summarize the results in figure our approach suggests an error location that matches the programmers fix but the other tools location misses the error our approach reports multiple correct error locations that match the programmers fix but the other tool only reports one of them both approaches find error locations corresponding to the programmers fix both approaches miss the error locations corresponding to the programmers fix our tool misses the error location but the other tool captures it the result shows that reports find about of the error locations but miss the rest reports on error locations are slightly better finding about of the error locations compared with both ocaml and our tool consistently identifies a higher percentage of error locations across all with an average of in about of cases our tool identifies multiple errors in programs according to the data the programmers usually fixed these errors one by one since the ocaml compiler only reports one at a time multiple errors at once may be more helpful limitations of course our tool sometimes misses errors we studied programs where our tool the error location finding that in each case it involved multiple interacting errors in some cases error size p p p p p p p p p p p p p p p table the quality of suggestions with various values of p and p where p p p total a comparison with the ocaml compiler b comparison with total figure results organized by assignment from top to bottom columns represent programs where our tool finds a correct error location that the other tool misses both approaches report the correct error location but our tool reports multiple correct error locations both approaches report the correct error location both approaches miss the error location our tool misses the error location while the other tool identifies one of them for every assignment our tool does the best job of the error the programmer made a similar error multiple times our tool fails to identify such errors because they violate the assumption of error independence as our result suggests this situation is the comparison between the tools is not completely we only collect type errors in the evaluation ocaml is very effective at finding other kinds of errors such as variables or wrong numbers of arguments and not only finds errors but also proposes performance we measured the performance of our tool on a system using a dual core at with g memory results are shown in figure we separate the time spent generating and inferring edges in the graph from that spent computing the results show how the running time of both graph building time and ranking time scale with increasing constraint graph size interestingly graph building including the inference of relationships dominates and is in practice quadratic in the graph size the graph size has less impact on the running time of our ranking algorithm we the reason is that the running time of our ranking algorithm is dominated by the number of unsatisfiable paths which is not strongly related to total graph size considering graph construction time all programs finish in seconds and over are done within seconds ranking is more efficient all programs finish in seconds considering the human cost to identify error locations the performance seems acceptable case study hypothesis inference we also evaluated how helpful our hypothesis inference algorithm is for in our experience with using we have found missing hypotheses to be a common source of errors a of programs was harder to find for than for ocaml we obtained application code developed for other earlier projects using either or a extension these secure tie better worse total number percentage table hypothesis inference result tions are interesting since they deal with realworld security concerns to potential errors programmer would meet while writing the application we randomly removed hypotheses from these programs generating in total files missing ­ hypotheses the frequency of occurrence of each application in these files corresponds roughly to the size of the application for all files generated in this way we classified each file into one of four categories with the results summarized in table the program passed label checking after removing the hypotheses the programmer made assumptions the generated missing hypotheses matched the one we removed the generated missing hypotheses provides an assumption that removes the error but that is weaker than the one we removed in other words an improvement our tool fails to find a better than the one removed the number of redundant assumptions in these applications is considerable we the reason is that the security models in these applications are nontrivial so programmers have difficulty their security assumptions this observation suggests that the ability to automatically infer missing hypotheses could be very useful to programmers all the automatically inferred hypotheses had at least the same quality as manually written ones this preliminary result suggests that our hypothesis inference algorithm is very effective and should be useful to programmers missing hypothesis wrong expression total percentage errors separate combined interactive table case study result separate top rank of both separately computed hypothesis and expression suggestions combined top rank combined result only interactive approach case study combined errors to see how useful our tool is for errors that occur in practice we used a of programs that a collected earlier during the development of the application as errors were reported by the compiler the programmer also clearly marked the nature and true location of the error this application is interesting for our evaluation purposes since it is was developed over the course of six by two it contains both types of errors missing hypotheses and wrong expressions the contains programs one difficulty in working on these programs directly was that files contained many errors this because the code was out earlier by the programmer to better the errors reported by the compiler we that this can be avoided if a better error tool like ours is used for these files we the errors the programmer pointed out in the notes when possible and ignored the rest producing the same also removed result for the remaining programs are shown in table most files contain multiple errors we used the errors recorded in the note as actual errors and an error is counted as being identified only when the actual error is suggested among top rank suggestions the first approach separate measures errors identified if the error type is known or both hypothesis and expression suggestions separately computed are used the result is comparable to the result in sections and where error types are known providing a concise and correct error report when multiple errors interact can be more challenging we evaluated the performance of two approaches providing combined suggestions the combined approach simply the combined suggestions by size despite its simplicity the result is still useful since this approach is automatic the interactive approach missing hypotheses and requires a programmer to mark the correctness of these hypotheses then correct hypotheses are used and wrong entities are suggested to explain the remaining errors we think this is the most promising approach since it involves limited manual effort hypotheses are usually facts of properties to be checked such as is a flow from alice to bob secure we leave a more study of this approach to future work related work program analyses constraints and graph representations modeling program analyses via constraint solving is not a new idea the most related work is on set program analysis and type qualifiers however these constraint languages do not model hypotheses which are important for some program analyses such as information flow program slicing shape analysis and flowinsensitive pointsto analysis are expressible using and reps show the between reachability and a subset of set constraints but only a small set of fact a single appear on the right hand side of a partial order moreover no error approach is proposed for the graphs error for type inference and informationflow control with error reports has led to earlier work on improving the error messages of both mllike languages and efforts on improving messages in mllike languages can be traced to the early work of wand and of johnson and these two pieces of work represent two directions in improving error messages the former traces everything that to the error whereas the latter attempts to infer the most likely cause we only discuss the most related among them but summary provides more details in the first direction several efforts improve the basic idea of wand in various ways despite the of a full explanation to the programmer the reports are usually and hard to follow in the second direction one approach is to alter the order of type unification but since the error location may be used anywhere during the unification procedure any specific order fails in some some prior work builds a type graph from a more limited constraint language and infers error locations based on heuristics mostly for type inference though the weighted heuristic in uses successful type to distinguish types from normal ones information about satisfiable paths is with in our approach to distinguish the constraints that caused errors this is shown to be effective in section a third approach is to generate for errors by searching for similar programs or type substitutions that do typecheck unfortunately we cannot obtain a common to perform direct comparison with some of this prior work it is worth noting that the ranking heuristics used in are there is no obvious way to extend them to information flow for instance we are able to compare directly with the work of et al the results of section suggest that our approach finds error locations more accurately in fact by where searches for are likely to be our approach to be complementary for informationflow control et al propose to generate a trace the informationflow violation although this approach also constructs a from a dependency graph only a subset of the model is handled as in slicing whole paths can yield very error reports recent work by et al informationflow violations in a higherorder polymorphic language but the mechanism is based on heuristics and a more limited constraint language moreover the algorithm in a single unsatisfiable path while our algorithm multiple errors probabilistic inference applying probabilistic inference to program analysis has appeared in earlier work particularly on specification inference our contribution is to apply probabilistic inference to a general class of static analyses allowing errors to be without also related is work on statistical methods for dynamic errors eg these algorithms rely on a different principle statistical do not handle important features for static analysis such as constructors and hypotheses the work of ball et al on errors detected by model checking has exploited a similar insight by using information about traces for both correct execution and for errors to error causes beyond differences in context that work differs in not actually using probabilistic inference each error trace is considered in isolation and transitions are not as causes if they lie on any correct trace missing hypothesis inference the most related work on inferring likely missing hypotheses is the recent work by et al on error using inference this work computes small relevant queries presented to a user that capture exactly the information a program analysis is missing to either or validate the error it does not attempt to identify incorrect constraints with regard to hypothesis inference the algorithm in infers missing hypotheses for a single assertion while our tool finds missing hypotheses that satisfy a set of assertions further the algorithm of infers additional invariants on variables eg x for a constraint variable x while our algorithm also infers missing partial orderings on constructors eg alice bob in section conclusion better tools for programmers the errors detected by program analysis should make them more to use the many powerful program analyses that have been developed the science of programmer errors is still rather primitive but this paper takes a step towards improving the situation our analysis of program constraint graphs offers a general way to identify both incorrect expressions and missing assumptions results on two very different languages ocaml and with little suggest this approach is promising and applicable there are many interesting directions to take this work though we have shown that the technique works well on two very different type systems it would likely be to apply these ideas to other type systems and program analyses and to explore more sophisticated ways to estimate the of different error acknowledgments foster robert and gave many useful comments on this presentation we also thank and for making available the data set they used for their work on kozen for pointing out the similarity between our constraints and set constraints and for a and labeled set of test cases this work was supported by two grants from the of research n and n by grant fa by a grant from the national science foundation and by a grant by the force research laboratory references a aiken introduction to set program analysis science of computer programming ­ a aiken and e l type inclusion constraints and type inference in conf functional programming languages and computer architecture pp ­ o m d j k a and a c myers sharing mobile code with information flow control in proc ieee symp on security and privacy pp ­ may t ball m and s rajamani from to cause errors in counterexample traces in popl pp ­ jan c r and m path problems siam journal on computing ­ s chen and m typing for debugging type errors in popl jan v and c t of illtyped programs technical report university december l m m type assignment in programming languages phd thesis department of computer science university of edinburgh d e a lattice model of secure information flow comm of the acm ­ i t and a aiken automated error using inference in pldi pp ­ j s foster r johnson j and a aiken flowinsensitive type qualifiers acm trans prog lang syst ­ a j b h s and d b bayesian data analysis nd edition c and j b type error slicing in implicitly typed higher order languages science of computer programming p n and b a formal basis for the heuristic determination of minimum cost paths systems science and ieee transactions on ­ b j top quality type error messages phd thesis the sept p hudak s p jones and p wadler report on the programming language haskell sigplan notices may g f johnson and j a a maximum flow approach to isolation in incremental type inference in popl pp ­ d t s and s a effective blame for informationflow violations in intl symp on foundations of software engineering pp ­ t p g back a ng and d from uncertainty to inferring the specification within in pp ­ o lee and k yi proofs about a type inference algorithm acm trans prog lang syst ­ b s m d and c chambers searching for messages in pldi pp ­ b m a x a aiken and m i scalable statistical bug isolation in pldi pp ­ b a v s k rajamani and a specification inference for explicit information flow problems in pldi pp ­ b j on the unification of substitutions in type inference in implementation of functional languages pp ­ b j type errors in functional programs phd thesis laboratory for foundations of computer science the university of edinburgh d and t reps of a class of set constraints and contextfree language reachability theoretical computer science r milner m tofte and r harper the definition of standard ml mit press cambridge ma a c myers and b liskov a model for information flow control in pp ­ a c myers l s s and n java information flow software release july ocaml programming language v j b and f a constraint system for a sml type error technical report university t reps program analysis via graph reachability information and software technology f tip and t b a approach for type errors acm trans on software engineering and methodology ­ m wand finding the source of type errors in popl m wand a simple algorithm and proof for type inference ­ j j and s security type error for higherorder polymorphic languages in acm sigplan workshop on partial evaluation and program manipulation pp ­ d and a c myers toward general of static errors technical report technical report cornell university aug a x b and m statistical debugging simultaneous identification of multiple bugs in pp ­ 