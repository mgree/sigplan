tradeoffs in program analysis stanford university v microsoft research aiken stanford university abstract it is often the case that increasing the precision of a program analysis leads to worse results it is our thesis that this is the result of fundamental limits on the ability to use precise abstract domains as the basis for inferring strong invariants of programs we show that tradeoffs an idea from learning theory can be used to explain why more precise abstractions do not necessarily lead to better results and also provides practical techniques for with such limitations learning theory captures precision using a combinatorial called the vc dimension we compute the vc dimension for different abstractions and report on its as a precision metric for program analyses we evaluate cross validation a technique for addressing tradeoffs on an strength program verification tool called the tool produced using cross validation has significantly better running time finds new and has fewer than the current production version finally we make some for tradeoffs in program analysis categories and subject descriptors d program verification statistical methods f semantics of programming languages program analysis i learning parameter learning keywords program analysis machine learning verification introduction in program analysis it is well understood that abstractions can lead to results however what is not so well understood is that precise abstractions can also produce results in some cases even worse than very abstractions we show that tradeoffs an idea from learning theory can be used to explain how the quality of analysis results changes with precision learning theory precision using a combinatorial called dimension or vc dimension we use vc dimension to analyze the behavior of a number of commonly used program analysis abstractions in addition using cross validation a technique applied in machine learning to address tradeoffs we are able to improve the performance of an strength program verification tool how can better precision affect an analysis note that we are not about cost or question only con permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ san diego ca usa copyright c acm the quality of the results the class of static analysis systems to which our results apply have two distinguishing characteristics first there is some domain of facts eg an abstract domain over which the analysis computes second there is some step in the analysis that takes such facts and attempts to generalize them eg to an invariant of the program many analysis frameworks such as abstract interpretation counterexample abstraction refinement cegar and various other inference techniques have this structure for example abstract interpretation can use widening section and cegar can use interpolants section as a generalization step there are also static analyses that do not have this structure and our results do not apply to them see section using results on tradeoffs we show that increasing the precision of the underlying domain at some point may lead to worse results from the generalization step consider the program in figure which we analyze using different program analyses with increasing precision each program analysis is a basic abstract interpreter over a different abstract domain first consider an interval analysis which infers upper and lower bounds for numeric variables using intervals to analyze the program in figure we obtain the loop invariant i j this result is perhaps the best invariant that one could expect with intervals intervals can only express facts about single variables and hence the invariant has no details on any relationship between i and j the problem is lack of precision the abstract domain is not precise enough to express certain behaviors we increase the precision to which can infer bounds on the sum or difference of pairs of variables we obtain the following loop invariant ij j this result is quite good providing a useful relationship between i and j and good bounds next we use the even more precise domain of polyhedra which can express bounds on arbitrary linear combinations of program variables we obtain the weak invariant i this result is not only worse than it is even worse than intervals increasing the precision of the analysis results in a decrease in the quality of the results let us examine this outcome in some detail the abstract interpreter generates some abstract states and then tries to generalize via joins and widening however because polyhedra are so expressive there are many incomparable polyhedra that describe any finite set of abstract states it is difficult for the generalization step to pick the best one for this particular program from such a large candidate pool for this example we start with i j as the initial abstract state and we show the fixpoint iterations published in first we fit on i j and i j to obtain the polyhedra i j i next we obtain i j i j that i j i j and i j note that there are an unbounded number of polyhedra including the one discovered by int i j while i j ji i i figure example program from that can fit these states the domain of polyhedra is so precise that the generalization step has many valid choices and hence it can pick a hypothesis that these specific abstract states but does not hold in general in the next iteration we obtain i j and finally true using the loop guard and narrowing we terminate with i as the invariant one might be to blame specific choices made in this analysis run for the result but that misses the point that the is general any method that attempts to select the best generalization from a large set of equally but different candidates will run into the same problem on many programs for example other analyses such as meet a similar on this example very abstractly a program has some behaviors and if an analysis is not expressive enough to capture these behaviors then we have and the results are if the analysis engine is too expressive then it can the specific behaviors and fail to generalize currently program analysis designers apply knowledge to avoid and our aim is to give a formal framework to understand these rules of and use the foundations to obtain better tools in learning theory is characterized by and is characterized by variance and by varying the precision one gets a tradeoff low precision leads to high and low variance while high precision leads to low but high variance with an appropriate choice of precision one can balance and variance and obtain good results as an example consider figure is a verification engine in microsoft windows static driver verifier that checks safety properties of windows device drivers in figure increasing values on the indicate increasing precision for details see and section the shows the time taken by on verification tasks a superset of the tasks reported in higher analysis times are of more and quality of results one the tradeoff in figure at low precision the performance of the tool is with increasing precision the decreases and the performance of the tool improves however after a certain point the variance starts increasing and the performance starts to to develop a theory that can explain these empirical observations we need a formal definition of generalization unfortunately even though the term generalization frequently occurs in the program analysis literature defining generalization precisely is difficult and there is no widely accepted definition just as complexity theory works with a model of machines and generates as opposed to quantitative results for comparing the efficiency of algorithms we want a useful theory perhaps working with a model that generates useful feedback about tradeoffs in a recent work applied the definition of generalization given by the probably approximately correct learning framework to prove that a verification algorithm for checking safety properties generalizes according to this definition which we give formally in section an algorithm generalizes if given enough samples of program states as input it is likely to generate predicates that separate almost all the programs safe reachable states from erroneous program states in this paper we explore an alternative use of this framework namely modeling tradeoffs we figure result of running on pairs the best performance is achieved at i do not claim that this definition is the correct definition of generalization however the motivation for our work is largely practical and we show that this existing framework yields some immediately useful results in the future different definitions of generalization might be available and the framework developed here can be instantiated with the alternative definitions to derive other useful conclusions for the benefit of program analysis tools we believe that if program analysis tool designers are explicitly made aware of tradeoffs and have mathematical tools to reason about generalization they can make better design decisions in our framework variance is proportional to vc dimension theorem hence a high vc dimension is of we calculate the vc dimension of several abstractions used in program analysis section including abstractions for numerical programs array manipulating programs and heap manipulating programs we observe that more precise abstractions that is the abstractions with higher variance have higher vc dimension these proofs increase our in the applicability of our framework to program analyses and provide evidence that vc dimension is a useful metric for characterizing the precision of abstractions since our definition of generalization has been from learning theory we can build on wellknown techniques in the machine learning community for addressing tradeoffs cross validation is one of the simplest techniques for this purpose consider a fully automatic analysis tool that has a number of configuration parameters how to set these parameters is usually and the typical case is that such an optimal configuration might not even exist although clearly some configurations are better than others one logical candidate configuration is the one that performs best on a benchmark suite however we demonstrate that this strategy can on the particular benchmark suite and significantly the quality of the tool on new inputs section cross validation avoids and we show that by applying cross validation to the configuration parameters of we are able to significantly improve running time while also reducing the number of and finding new thus can impact program analysis in at least two ways first precise abstract domains can and lead to generalization and hence invariants second tools can their benchmark suite resulting in performance on new analysis tasks we show that these different problems are both instances of tradeoffs and hence the same principles theorem apply to both learning a b c figure example a good fit b c theory addresses tradeoffs in general and can provide techniques to both these problems to summarize our contributions are as follows · we observe that improving precision does not necessarily lead to better results in program analysis we connect this to tradeoffs in learning theory section · we show that vc dimension captures the precision of abstract domains used in program analysis we calculate the vc dimension of a number of abstractions relevant to program analysis including formulas over arrays and separation logic section · we explain several empirical observations in program analysis using tradeoffs for example by incrementally increasing precision one can balance and variance section a strategy that is employed in several existing program analyses section · using tradeoffs we show how to benchmark can result in performance on new inputs using cross validation to guide precision we are able to improve the performance of a fully automatic production quality program verification tool section · we make some specific for tradeoffs in program analysis section we first review concepts from learning theory used in subsequent sections readers already familiar with learning theory may safely skip this section for more details the reader is referred to the by and tradeoffs in tradeoffs have been in machine learning and before introducing the formal definitions it is to look at a machine learning example consider the problem of we are given a set of inputoutput pairs observations x y xn yn and we want to predict the output y for additional but unknown input values x in other words we want to learn a function f such that yi f xi i n the standard way to solve this problem is to consider a template for f a restricted class of functions from which the solution is chosen and then fit the template to these observations figure three different templates to the same set of six observations in figure a we show the best fit line even though we have chosen the best of all possible lines the fit is quite meaning there are large errors even for the observed data points and we do not expect the obtained to be good figure a illustrates lines are too to represent our observations next we fit a quadratic curve figure b and it seems to be a good fit we expect it to produce good figure c shows the fit of a polynomial of degree five a fifth degree polynomial can between the six observations the fit is extremely good for the actual observations but there seems little reason for in the for very large or very small values of x given by this particular choice of function figure c illustrates the nature of the and the for the examples in figure and figure have and differences we observe that both precision and limited precision lead to bad results but there are some obvious differences the program analysis example in figure looks the abstractions are some of the loop behaviors whereas in figure b the quadratic model does not even agree with the observations the difference is due to in machine learning the data is typically whereas programs are precise descriptions therefore the definition of generalization we use and the development in this paper are for the case of course programs can have bugs and these can be thought of as and learning theory has mechanisms for in generalization defining and handling in program analysis is interesting future work learning theory consider an instance space which is the set of all instances suppose each xi is associated with a label xi that belongs to a label set let h be a hypothesis class that is the set of all functions h considered by the learning algorithm the goal of a learning algorithm is to choose an h h such that for each xi is a good estimate of the label xi for the example in section r r and h is a set of polynomials we are given a set s xi yi i m × called the training set and we want to find a hypothesis h h which generalizes over the whole instance space that is for all x hx x is small the notion of instance space is general and can capture program states it can be the collection of points in rn for numerical programs a collection of stack and heap pairs for heap manipulating programs or a valuation of some numerical variables and arrays for array manipulating programs one example of labels can be whether a state is reachable or unreachable and we might want our hypothesis to be a predicate which for each state a label true denoting reachable or false denoting unreachable given some known reachable and unreachable states for program analysis we are interested in predicates as hypothesis classes hence there are only two labels true and false and we limit our discussion to binary labels true false unless stated otherwise given a set of labeled instances called the training set s × one natural method to perform learning is empirical minimization find a hypothesis h h such that the number of labeled instances xi yi s for which yi is by finding a hypothesis that works well on the training set we hope to find a hypothesis which works for the whole instance space however it is not clear whether such an h generalizes next we formally define the notion of generalization that we use learning assumes that the training set s consists of m independent and distributed labeled instances drawn from an arbitrary but fixed distribution d over the instance and label space if we draw a new labeled sample from d then we are interested in the probability that the actual label with the label if x y d then what is pr hx y so first we that is generate a hypothesis using a training set and then we test that is evaluate the performance of the hypothesis on the new samples a learner takes some samples from d as input and with high probability outputs a hypothesis that is approximately one were to draw a new sample from d then with high probability the and the actual label agree since the output of the learner the labels for instances that it has not seen we say that it generalizes definition a learner generalizes if given m samples determined by parameters and the hypothesis space h from a distribution d with probability it outputs a hypothesis h h such that x y d pr hx y now why does this definition make sense suppose the learner wants to an adversary that it can produce hypotheses that generalize if an adversary the training set then she can generate a very bad training set with no information about the structure of the problem for example the adversary can just duplicate a labeled instance an unbounded number of times and can claim to have generated a large or even an unbounded training set or she can generate samples in the training set that are related to some particular behaviors and when testing the generalization properties of the generated hypothesis use completely different behaviors the hypothesis generated using certain behaviors is bound to perform on behaviors that it has no idea about an algorithm that can succeed against such a powerful adversary seems and so it seems reasonable to weaken the adversary first to define generalization the training set should have some guarantee of having a good coverage of behaviors by selecting training inputs randomly we ensure formally that our training set is not generated in testing the adversary might the generated hypothesis by testing on very inputs by testing on samples we also take this power away from the adversary and variance first we formally define the learner finds a hypothesis that the empirical error over a training set s xi yi i n h m m yi i where is the function b if b is true and if b is false in empirical minimization we try to find a hypothesis h that the empirical error h and hope that it generalizes the generalization error h for a hypothesis h is defined as follows h pr hx y the objective of a learning algorithm is to compute a hypothesis with low generalization error by the empirical error we hope to achieve this objective one of the fundamental theorems in machine learning is the following theorem for a hypothesis space h let d v ch defined in section then given m samples empirical minimization with high probability produces h h such that h h o d m and also h h o d m where h is a hypothesis with the minimum generalization error in h this theorem gives us a bound on the generalization error in particular the first part of the theorem bounds the generalization error using the empirical error to generalize well or to have a low generalization error we want the bound to be small this theorem says that we can produce a large generalization error for two reasons the term h the is the generalization error of the best hypothesis in h ie the one that the generalization error if this value is large then the hypothesis class even if we select the best available hypothesis we still have generalization errors the term o d m the variance grows with the vc sion or precision of the hypothesis class if this value is large generalization errors occur from the training data therefore low precision causes generalization error due to and high precision causes generalization error due to variance and this leads to the tradeoff a corollary of this theorem has been used by for an alternative purpose for a specific hypothesis class h using v ch to bound the number of samples required to ensure that the generalization error is below a by trying multiple hypothesis classes in order of increasing precision one can address tradeoffs one starts with an hypothesis class and increases precision until the bounds start in the extreme when we have an empty hypothesis class then the is high and the variance is zero at the other extreme for very expressive classes the can become zero and the variance is high when the size of the hypothesis class increases so that successive hypothesis classes include the previous hypothesis classes then decreases and variance increases by increasing precision we can find a and achieve low bounds on generalization error vc dimension the or vc dimension is a purely combinatorial that measures the of a hypothesis class definition a hypothesis h satisfies a set of labeled instances x iff x x hx x definition given a set x of instances a hypothesis class h x if for any labeling there exists an h h st h satisfies x definition v ch is the cardinality of the largest set that h can in our setting when a hypothesis satisfies a set of labeled instances some of the instances are labeled true and others are labeled false and the hypothesis is a predicate containing all the points labeled true and the points labeled false if a class is able to large sets then it has high precision and it is precisely able to separate instances with different labels the vc dimension is the largest number of points that one can if the vc dimension is too high then we can and does not produce a hypothesis that generalizes in the extreme case if the vc dimension of a hypothesis class is infinite then we cannot bound its generalization error theorem to prove that the vc dimension of a hypothesis class h is at least d we need to show a set of d points in the instance space that h can to prove an upper bound u on vc dimension we need to show that for any possible selection of u points from h cannot the u points since we want an upper bound on generalization error it is generally sufficient to find upper bounds on vc dimension figure one inequality cannot points figure three points in two dimensions using one inequality abstract domains in this section we calculate the vc dimension for several popular abstract domains and show that the vc dimension gives results which match our our goal is not to calculate the vc dimension of every possible abstract domain we consider some simple abstract domains and the techniques we develop are useful but might not be sufficient for computing the vc dimension of other more complicated abstract domains as well the vc dimensions of the numerical domains we consider section section and section follow from standard results in learning theory we also consider predicates over arrays section and separation logic section we are of any previous study of vc dimensions of these domains standard numerical domains our main idea is to compute the vc dimension of different abstract domains in order to capture their precision the formal proofs for the numerical domains discussed in this section are standard material in machine learning the instance space we first consider is rn which can represent the state of numerical programs with no arrays and no data structures arrays and data structures are discussed in sections and single inequality first let us consider the hypothesis class consisting of single linear inequalities we select this class as it is one of the simplest hypothesis classes the following result is known theorem the vc dimension of the set of single inequalities in n dimensions is n consider the dimensional space r this theorem states that the vc dimension of the class of inequalities of the form ax by c is three figure shows a set of three points that can be using this hypothesis class are the points labeled false and are the points labeled true note that we cannot some configurations of three points but that does not matter for vc dimension definition for instance as shown in figure if the points are then there is a labeling that cannot be satisfied by a single inequality there is no inequality that can include the two and the we cannot any configuration of four points using a single inequality and the canonical configurations that cannot be are shown in figure in the first configuration of four points we cannot satisfy the inner point with label false and the other three with label true in the second configuration we cannot satisfy the labeling in which opposite points have the same label and adjacent points have different labels figure one inequality cannot four points to prove theorem observe that an inequality in n dimensions can be upon as f x where f x is a in n dimensions passing through the origin it is easy to see that we can n points with such inequalities consider a set x of n points where the ith point has the coordinate in the ith dimension and otherwise if z x is the set of examples labeled true then the inequality f x where f x has the ith coordinate if xi z and otherwise satisfies x so the vc dimension of an inequality in n dimensions is at least n to get the upper bound we instantiate the following generalized lemma lemma let f be a function class containing functions f r and let a x f x f f then v ca since in n dimensions passing through the origin are generated from n basis vectors we conclude that the dimension of such is n and theorem follows from this simple example of a vc dimension calculation one can observe that computing the vc dimension of a hypothesis class can be a nontrivial task and can require reasoning about the mathematical structures underlying the instance space and the hypothesis class now we proceed to some of the more complicated hypothesis classes that are relevant for program analyses intervals intervals or conjunctions of inequalities of the form c is a abstract domain it is also known for as useful invariants often require relationships between multiple variables theorem the vc dimension of intervals in n dimensions is n proof consider the n points x xn x x xn where xi has as coordinate i and the rest of the zero we can these n points using a construction that is a generalization of figure moreover intervals cannot n points any collection of n points has at most n extreme points an extreme point of a collection of points has the or the lowest coordinate along some dimension among the points in the collection there is at least one point as there are n and n total points since intervals are convex if we consider any set of n points and assign the extreme points the label true and the points the label false then no interval can include all the true and the false points for example if we consider n points consisting of x and the origin then the points in x are extreme points and an interval cannot satisfy the labeling where points in x are labeled true and the origin is labeled false figure four points using intervals in dimensions symmetric have been omitted figure using a polyhedron to satisfy a labeling we now move on to more expressive abstract domains and show that these have higher vc dimension than intervals polyhedra given an arbitrary number of points on a with arbitrary using convex polyhedra can separate the points labeled true from points labeled false an example is shown in figure as a consequence we have the following theorem theorem the vc dimension of polyhedra is infinite using theorem we can conclude that when using polyhedra it is not possible to bound the generalization error in our framework however many useful invariants require general inequalities the fact that the vc dimension of polyhedra is infinite does not prevent us from handling general inequalities and these are addressed in the next section templates if we restrict polyhedra to k inequalities then it turns out that the vc dimension is bounded above by the proof of this fact relies on general composition theorems that relate how the vc dimension when complex hypothesis classes are obtained by composition of more primitive classes here we are composing k inequalities to generate a polyhedra and these theorems are applicable to state these theorems we require additional technical machinery that does not add to the development of the ideas in this paper and therefore we omit them in this case as expected the vc dimension is higher than intervals but seems if one can perform program analysis while keeping the number of inequalities fixed then the generalization error is bounded in our framework and one might be able to invariant inference perform analysis assuming the number of inequalities to be a fixed constant now consider a given boolean combination instead of just conjunctions of inequalities approaches for performing abstract inter with a given fixed number of are known in the case of invariant inference the for finding the invariant is the constructs a template and solves constraints to instantiate template parameters the template can have only conjunctions or a given boolean combination of inequalities indeed the vc dimension of a given boolean combination of k inequalities in n dimensions is also bounded by which is the same as the vc dimension of conjunctions of k inequalities this observation suggests that one does not expect results to significantly due to higher variance when using this more expressive template template constrained matrix domains provide a to vary precision in one can provide linear expressions and abstract interpretation infers lower and upper bounds for them intervals and are special cases of as we increase the number of linear expressions the vc dimension increases in the limit when we have an infinite number of linear expressions becomes polyhedra and the vc dimension is infinite here is an example of a family of abstract domains that can tradeoffs an template choice ensures that we neither nor and therefore leads to better results moreover by calculating the vc dimension one can observe how the precision increases if additional linear expressions are added and thus can help an abstract interpretation designer make systematic decisions for choosing linear expressions by questions such as the expected increase in precision from adding a new linear expression nonlinear arithmetic first we compute an upper bound on the vc dimension of the hypothesis classes composed of polynomial inequalities recall that an upper bound on the vc dimension suffices for bounding the generalization error see theorem we show that the bound depends on the degree of the polynomial suppose we are given that the invariant is composed of quadratic inequalities conceptually we can create a new variable for every up to degree two the quadratic invariant in the old set of variables becomes a linear invariant over the new set of variables the done above carry over just with an increased dimension since the number of increases with the degree under consideration the bound also increases with the increasing polynomial degree in the extreme when the degree is unbounded we cannot bound the vc dimension if there are other sources of similar arguments apply and we can add new variables so that nonlinear invariants are reduced to linear invariants arrays consider an array manipulating program the program state is the valuation of integer variables and potentially unbounded arrays the examples of useful invariants over arrays that occur in the program analysis literature are generally universally quantified predicates involving a small number to of arrays likewise if we consider the predicates of the form i ± ai c as the hypothesis class h and the instance space as the valuation of an array denoted by a then the vc dimension is which is the same as an interval in one variable thus such predicates can from the same problem of as intervals theorem the vc dimension of predicates i ± ai c is proof consider three states and let vi denote the smallest value stored in the array of the ith state among the that need to be satisfied consider the configuration in which the states with the smallest and largest vi are labeled true and the third state is labeled false this labeling cannot be satisfied by this class of predicates also two distinct states each consisting of only a single element in the array are by the predicates of the form x we can extend the above result to a more general statement consider the instance space of points in n dimensions rn that represents the values of n numerical variables also consider the instance space a that represents the values of an array a of unbounded size and n numerical variables a r × rn we define three maps the map m maps each predicate over to a predicate over a mp x x xn jp aj x xn and j x xn we also use m to denote pointwise extension of this map to a set of predicates next f a is a map such that f c c cn is an instance in a where the array a has all elements equal to c and the ith numerical variable of a is assigned ci finally g a is a map where ga c cn assigns the first numerical variable of an arbitrary element of a and the rest of the n variables are assigned values c cn if h is an arbitrary hypothesis class of predicates over then the following result holds theorem v ch v proof to prove v ch v observe that if h can m points p pm using predicates p pm then can f p f pm using mp the proof for the reverse inclusion that is v ch v is similar and uses g and m which exists since m is one to one this result allows us to compute the vc dimension of richer hypothesis classes and boolean combinations of the same for example if we have a numerical variable z and an array a as our program variables then by the results in section and theorem the vc dimension of the class of predicates cz is three and for the class of conjunctions of predicates of the form and z is four separation logic for heap manipulating programs separation logic has as a successful approach we do not review all the details of separation logic here and keep the discussion at an abstract level the predicates or the elements of the hypothesis class are formulas written in separation logic and elements of the instance space or the program states are pairs of a store s and a heap h since we focus on the heap we keep the store fixed our instance space is composed of program states in which the store maps a single variable z to a heap location l in a heuristic algorithm for inference with lists has been described the related fragment shown below has an infinite vc dimension e x x c p emp e e e p q list e an expression is either a variable x or a logical variable x or a constant c eg nil and an assertion says that the heap is either empty or it contains one cons cell and e is the address of the cons cell with contents e and e or the assertion has a separating conjunction pq that the heap into two disjoint parts one where p is true and one where q is true a list denotes a nil terminated linked list the logical variables in the assertions are implicitly existentially quantified theorem vc dimension of the above class of predicates is infinite make figure results of comparing abstract domains published in proof we need to show that given any n we can construct n heaps and them using the predicates of our logic we give a proof sketch the intuition is that we can introduce an unbounded number of logical variables in the heap and lists can encode a boolean choice therefore we can make an unbounded number of boolean choices and unbounded sets suppose n is two the construction below can be generalized to arbitrary n recall the store maps z to l consider two heaps h l l nil l nil nil and h l nil l l nil nil the predicates z x y z x y z x y and z x y the four false false true false false true true true of h h in general by introducing n logical variables we can n heaps obtained using this construction similar to section we can restrict the size of the predicates to bound the vc dimension and hence the generalization error we are of any analysis that restricts the structure of the predicates in separation logic but given the success of invariant inference for numerical programs it seems to be a useful research direction to in the future discussion in this section we consider empirical results from various papers on program analysis and try to interpret these results using tradeoffs we do not claim to be exhaustive the goal here is to show that a variety of useful techniques can be justified using our framework abstract interpretation consider the subset of results figure obtained from the static analyzer that were published in the columns in the table of figure compare the quality of invariants found by polyhedra pk and oct polyhedra pk and intervals box and oct and intervals box respectively in each comparison there are three values the first value is the percentage of invariants for which the results of the first abstract domain are logically stronger than the second the second value is the percentage of invariants for which the results of the second domain are stronger than the first the third is the percentage of incomparable invariants the remaining percentage of invariants are logically identical it can be seen from figure that as precision increases the quality of the inferred invariants gets better this result is expected as richer abstract domains can express invariants that weaker domains cannot we share the observation with the authors that for a percentage of invariants polyhedra perform worse than intervals and other evaluations in not studied here show that the basic forward analysis of can produce better results than the approach of even though the latter can produce more precise intermediate results the authors explain these observation using the of the outcome verified refinement failed did not finish magic new figure results for interpolation with incremental increase of precision published in and magic are based on weakest preconditions and uses interpolants widening operator in abstract interpretation the widening operator is responsible for generalization widening is usually because widening more precise information can lead to worse for example and in this example widening more precise intervals leads to less precise results of the widening operator is related to tradeoffs intuitively if there were a monotonic widening operator then improving the precision of the underlying abstract domain would lead to better consequently there would be no tradeoff the tradeoff seems to be a fundamental limit on generalization and therefore monotonic widening operators for sophisticated abstract domains seem interpolants find simple proofs of of a finite number of spurious counterexample paths and hope that because the proofs are simple the predicates used in the proof will generalize and all possible spurious counterexamples to find good predicates can result in divergence in cegar we the of simple proofs using tradeoffs an interpolant is required to prove the of a spurious counterexample this requirement ensures that the interpolant is not too must be strong enough to prove a potentially useful fact this requirement can also be seen as a means to we are a lower bound on the precision of the language of interpolants to avoid we want the interpolant to be simple the hope is that simple predicates will not to a specific path and hence can avoid high variance the definition of an interpolant ensures simplicity by restricting the variables that can occur in an interpolant which corresponds to the dimension of the instance space and consequently the vc dimension as well however it has been observed that only restricting the variables is not enough to avoid divergence in the language of interpolants or the hypothesis class is restricted to a finite set and this set is expanded if the hypothesis class is finite then in our framework the following result is known theorem if h is obtained from an algorithm h k m is the size of the sample set and is fixed then with probability at least we have h log k m this theorem states that when the language is we have high and as we increase the expressiveness of the language under consideration h increases and so does the variance similar to theorem by increasing the size of the hypothesis class the bounds on the generalization error can be consider the empirical results of jhala and published in and shown in figure magic and are predicate abstraction based tools that use cegar and add predicates during their analysis to perform refinement a choice of refinement predicates can cause divergence these predicates are obtained by spurious counterexample paths if the predicates the paths then they are typically not useful for finding an invariant the last column shows the results obtained from the following strategy for generating predicates use interpolants as refinement predicates and first restrict the interpolants to a finite language l for example l can be the language of predicates that have their numeric constants restricted to either zero or c ± where c is a numerical constant statically occurring in the program the language is incrementally expanded to l l and so on the column just uses the interpolants in the language l we observe that by incrementally increasing the expressiveness of the language of interpolants one can avoid the present in l and obtain better results verifying instead of programs from the benchmark suite of programs incrementally increasing precision in recent years there has been a growing interest in exploring optimal abstractions these papers argue for the most abstraction that is sufficient to prove the desired property of a program this line of work is different from the techniques that aim to compute the least fixed point generally an increase in precision is associated with a decrease in efficiency abstractions are computationally and hence are desirable et al show that quite abstractions can be sufficient to prove most properties of interest et al have an abstraction refinement algorithm that tries abstractions of increasing cost stratified analysis is another technique that incrementally increases precision abstract interpretation is performed in a stratified fashion running successive analyses of increasing precision by incrementally increasing the number of program variables under consideration the later analyses use the results of the previous analyses and heuristics based on dataflow dependencies by performing the stratified analysis the authors obtain the same or better invariants than classical or alternative widening schemes for all the cases of their study all of these including discussed in section can be seen as examples of addressing tradeoffs by starting with low precision and moving to high precision it is also possible to achieve the same effect by starting from high precision and moving towards lower precision eg as done in in the next section we explore whether the techniques for tradeoffs in machine learning can also benefit program analysis tools cross validation in this section we describe our experience with applying techniques for addressing tradeoffs to program analysis tools in particular we discuss the application of cross validation to a verification engine in windows machine learning tools generally have a number of precision and finding a configuration of these that does not the training set is recognized as a problem one widely used solution is cross validation while providing formal guarantees for cross validation algorithms is a topic of current research cross validation has been found to be extremely useful and is standard practice in machine learning we study the simplest cross validation algorithm here evaluation of more sophisticated variants such as cross validation is left as future work cross validation partitions the training set into training data and test data next multiple learning algorithms called are on the training data and tested on the test data the learner that generates the hypothesis performing best on the test data is selected for example for the example of section we would consider a subset of the observations as training data and the rest as test data say a split now consider different learning algorithms where each algorithm a polynomial of a different degree on the training data we find the best fit line quadratic etc for the training data and pick the degree which corresponds to the hypothesis that performs the best on our test data we observe that the learner obtained from cross validation generates a hypothesis using training data that generalizes to test data if we a learner on the full training set without performing cross validation then we might in figure the curve that corresponds to the fifth degree polynomial curve best the observations among linear quadratic and fifth degree polynomials and thus performs the best on the observed data during cross validation the fifth degree polynomial on the training data and shows large from the observations in the test data thus cross validation can serve as a guide which fifth degree polynomials for this example and prevents here is a full description of cross validation randomly split the training set s into and for each learner mi it on to get a hypothesis hi training can be just over pick the learner mk corresponding to hk with smallest error on mk on s to obtain the output hypothesis h note that once we have selected the learner mk that generalizes best to the test data the final hypothesis is computed by training mk on all of the available data we remark that cross validation is not a and if used can itself start to is an strength tool for checking windows device driver properties has a benchmark suite of pairs and in the current production version its precision have been to perform the best on these benchmarks as discussed above this process can lead to in fact we show that simply by using cross validation we can significantly improve performance in our setting the instances are input programs that analyzes and the hypothesis class consists of all the versions of that can be created by different choices of configuration parameters we are given some benchmarks and these constitute our training set the labels for these benchmarks correct or are known the goal of the learner is to find a hypothesis which is a tool that generalizes well that is even for new programs it has not seen we want it to assign the correct label for the learner is simply which selects the best parameter configuration for on the training set we consider a sequence of tools where i the number of test steps is one of the most important parameters in see for details the actual details of how works are what is here is that a higher i corresponds to increased precision in figure the timing results are shown where each has been on the full benchmark set the parameters other than i have been to obtain the best results we observe the expected tradeoff curve as i is increasing the precision is increasing and the results first improve with increasing precision and then they from figure the best value of i for all pairs is note that uses the total runtime as a performance metric hence we also use total runtime for comparing performance of different configurations during cross validation however more general performance metrics are certainly possible we observe that the authors of have discovered a good configuration of the tool which we call the old old is on the figure result of running on test pairs with i tool time min table performance of and on new pairs the timeout was set to full set of pairs and is the current production version now we consider the alternative performing cross validation on we split the pairs into training data consisting of pairs and test data consisting of pairs we each finding values for the parameters other than i so that it performs as well as possible on the training data next we pick the that performs best on the test data shown in figure from figure this best value occurs for i next we on all pairs to obtain the new when verification is performed on new verification tasks in this case a new set of pairs we observe that the new with i shows better generalization properties than the old with i even though the new performs worse on the pairs the old was the best configuration it actually performs better on the new verification tasks as shown in table we conclude that old was to the benchmarks here we have been able to reduce the run time by more than find new and decrease the number of simply by using cross validation to set configuration parameters here are some additional details about table the new and the old find in common the new out on one which the old could find also there are that are common between the old and the new such a result is expected as there is no best configuration using cross validation we are trying to find a configuration that avoids and is expected to work well in most cases to summarize we validate that has been to its benchmark suite and we remove this by cross validation to obtain a better tool we believe other existing tools could be similarly improved with the straightforward application of cross validation and limitations in this section we consider the implications of tradeoffs in the design of automatic program analyses we also discuss some limitations of our approach and make a number of specific tradeoffs can help in the selection of an abstract domain one should avoid abstract domains with infinite vc dimension otherwise we cannot bound the generalization error in our framework theorem and we expect it to be large in practice one should use domains with finite vc dimension while ensuring the domain is rich enough to express the invariants of interest to avoid large generalization error due to one natural way to achieve this goal is to start with an domain and increase precision by expanding the hypothesis class section we now discuss how tradeoffs apply to fully automatic verification most practical tools have benchmarks and some parameters to take for instance the tool which has more than a parameters the abstract domains to enable widening strategies and so on the best choice for the parameters is not usually clear if one aims for a fully automatic tool like then before the tool one generally performs the parameters are to give best results on a benchmark suite but if tradeoffs exists then this strategy is one can to the benchmarks and the performance of the tool on new inputs section a common practice in the program analysis community is to to a benchmark suite and then report results given the results in this paper it is clear that this approach provides no protection against and is therefore weak as an extreme example one can build a tool for any benchmark suite by simply creating a lookup table with the correct answer for each benchmark this tool works in the reported experimental results and fails completely in practice a more realistic scenario is that a tool her tool gives results on some program p in her benchmark suite she makes a change and as a result the tool works better on p but has the tool really improved in general or has the tool designer to p it might be that the tools users were better off without the change a tool that performs on some of its benchmarks is not a bad thing if the alternative is the goal of course should be to build tools that work on previously examples not just on the benchmark suite an experimental methodology that inputs into a training set that can influence the design of the tool and a separate test set that cannot the design of the tool is one way to validate that tools generalize beyond the benchmarks used to design them some researchers may be doing this already but we are of this point explicitly in the literature note that this might not be directly applicable to tools that are not designed to be fully automatic for example relies on user interaction to select the appropriate parameters for the program under analysis despite our general of static analysis tools to benchmarks there are situations in which that may be the right thing to do when the goal is to analyze a specific program eg the verification of sel rather than to build a tool that works for any program it might make sense to to the one important benchmark the situation is also different for analysis algorithms that can be proven to give an optimal or best result such as certain classes of type inference dataflow analysis abstract interpretation over finite lattices and the analysis of and programs here the setting is sufficiently tractable that the generalization strategy is provably the best possible or generalization is not needed at all and how well the algorithm works on a single full benchmark suite is a reasonable practice finally if one had some guarantee that a benchmark suite was representative of all possible inputs there would be no need for a separate test this case improving performance on the training set guarantees improvement in general however it seems difficult to obtain or prove that a benchmark suite is representative even for a particular domain the drivers we consider as the training set in section are a superset of the benchmarks of and from our results we can conclude that they are not representative the on software verification was introduced as a common platform to compare tools however the current structure of the does not avoid the benchmarks and the expected results of the are public and the benchmarks used to compare tools are a subset of the released benchmarks this design addresses a real concern which is that because the semantics of c is and the benchmarks are c programs all benchmarks in advance helps ensure that the and participants agree on the intended meaning of the programs our to of to evaluate software tools is that they should always evaluate the tools on some new programs as a check on these programs must be chosen carefully to ensure that any possible variations in interpretation are irrelevant to the verification task conclusion because of tradeoffs increasing the precision of a program analysis can lead to a decrease in the quality of results we have adapted the learning framework to explain tradeoffs in program analysis and used vc dimension as a measure of the precision of abstract domains we have computed the vc dimension for some popular abstractions for numerical array manipulating and heap manipulating programs and we observe that more precise abstractions have higher vc dimension we have also shown that standard techniques for addressing tradeoffs such as incrementally increasing precision and using cross validation in parameters are applicable to program analysis tools acknowledgments we thank gupta rajamani and the anonymous reviewers for their constructive comments this work was supported by nsf grant this material is also based on research by the force research laboratory under agreement number fa the us is to and for purposes any copyright notation references g m and f discovering invariants via simple component analysis j comput ­ s and a a survey of procedures for model selection statistics ­ r p m hill e and e precise widening operators for convex polyhedra sci comput program ­ d second on software verification summary of in tacas pages ­ d t a henzinger and g program analysis with dynamic precision in pages ­ c m pattern recognition and machine learning information science and statistics springerverlag new york inc isbn n k l and a rybalchenko on solving universally quantified horn clauses in sas pages ­ a a d and m k and the dimension j acm ­ n h s a h d s and h learning of general geometric concepts j acm ­ c calcagno d p w ohearn and h yang compositional shape analysis by means of in popl pages ­ g c and n l c on in model selection and subsequent selection in performance evaluation journal of machine learning research ­ s e m clarke a and o predicate abstraction with minimum predicates in pages ­ e m clarke o s y lu and h counterexampleguided abstraction refinement in cav pages ­ e m clarke d n and k predicate abstraction of programs using sat formal methods in system design ­ m s and h linear invariant generation using nonlinear constraint solving in cav pages ­ p cousot and r cousot static determination of dynamic properties of programs in pages ­ p cousot and r cousot abstract interpretation a unified lattice model for static analysis of programs by construction or approximation of fixpoints in popl pages ­ p cousot and r cousot comparing the galois connection and approaches to abstract interpretation in pages ­ p cousot and n automatic discovery of linear among variables of a program in popl pages ­ p cousot r cousot j l a and x why does scale up formal methods in system design ­ p a few useful things to know about machine learning acm ­ t and h precise fixpoint computation through strategy iteration in esop pages ­ t and h precise relational invariants through strategy iteration in csl pages ­ s e and r networks and the computation ­ p a v s k rajamani and s compositional program analysis the power of alternation in popl pages ­ b s and s gulwani a numerical abstract domain based on expression abstraction and max operator with application in timing analysis in cav pages ­ s gulwani s and r program analysis as constraint solving in pldi pages ­ a gupta r majumdar and a rybalchenko from tests to proofs in tacas pages ­ j d and m a path sensitive static notes theor comput sci ­ j d and m representations for abstract interpretation combined analysis algorithms and experimental evaluation in sas pages ­ t a henzinger r jhala r majumdar and k l abstractions from proofs in popl pages ­ r jhala and k l a practical and complete approach to predicate refinement in tacas pages ­ m and d algorithmic stability and bounds for computation ­ m j and u v an introduction to computational learning theory mit press cambridge ma usa isbn g j k g d p d k r m t sewell h and s sel formal verification of an kernel acm ­ g m and b the an p o and m learning minimal abstractions in popl pages ­ k l an theorem prover theoretical computer science ­ a the abstract domain higherorder and symbolic computation ­ d and l using bounded model checking to focus fixpoint iterations in sas pages ­ d and j l stratified static analysis based on variable dependencies notes theor comput sci ­ a y ng preventing of data in pages ­ a v and s k rajamani an empirical study of optimizations in in pages ­ j c reynolds separation logic a logic for shared mutable data structures in lics pages ­ s h b and z manna scalable analysis of linear systems using mathematical programming in pages ­ s f i and a gupta static analysis in disjunctive numerical domains in sas pages ­ r s gupta b a aiken and a v verification as learning geometric concepts in sas pages ­ l g a theory of the acm ­ x m and h yang finding abstractions in parametric dataflow analysis in pldi pages ­ 