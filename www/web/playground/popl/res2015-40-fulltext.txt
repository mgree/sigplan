weighted automata in compositional reasoning about concurrent probabilistic systems he wang national laboratory for information science and technology school of software university key laboratory for information system security of state key laboratory of computer science institute of software of sciences abstract we propose the first sound and complete compositional verification technique for probabilistic safety properties on concurrent systems where each component is an markov decision process different from previous works weighted assumptions are introduced to completeness of our framework since weighted assumptions can be implicitly represented by binary decision diagrams we give an learning algorithm for to infer weighted assumptions experimental results suggest promising for our compositional technique categories and subject descriptors d verification model checking general terms theory verification keywords compositional verification probabilistic model checking algorithmic learning introduction probabilistic programs are widely in various systems for problems requiring substantial computation resources their solutions can be too costly for practice purposes for many such problems probabilistic algorithms may better expected worstcase running time than the worstcase running time of any classical algorithm probabilistic methods hence become a technique to solve hard problems in practice indeed the ieee standard has employed probabilistic methods to avoid the transmission in networks similar to classical systems probabilistic systems are not always free of errors in order to ensure their correctness verification techniques have been developed to analyze probabilistic systems just like classical systems a probabilistic system may consist of several concurrent components the number of system states also permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than the authors must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ copyright is held by the publication to acm acm increases exponentially in the number of concurrent components addressing the state explosion problem is crucial to probabilistic verification techniques for classical systems compositional verification aims to the state explosion problem by divide and suppose a classical system m m composed of two concurrent components m m and p an intended property about the system consider the reasoning proof rule for classical systems m a a m p m m p the notation m a means that a simulates all behaviors of m informally the rule says that to show the composed system satisfying p it suffices to find a classical assumption a such that a simulates m and a composed with m satisfies p as well a useful assumption needs to be small at least smaller than m and able to establish the intended property finding useful assumptions in reasoning appears to require although heuristics have been proposed to construct such assumptions automatically they are not always applicable have to provide assumptions manually such tasks are very time and can be extremely difficult to carry out on large systems interestingly the problem of finding useful classical assumptions can be solved by active machine learning in active machine learning a learning algorithm infers a representation of an unknown target by making queries to a the framework thus a mechanical to answer such queries together with a learning algorithm the framework is able to find assumptions automatically for classical systems the l learning algorithm for regular languages suffices to infer classical finite automata as classical assumptions other techniques have also been developed to find useful assumptions for compositional verification of classical systems from the classical framework one that two are essential to finding probabilistic assumptions first a sound and invertible reasoning proof rule for probabilistic systems is needed a sound proof rule allows us to analyze by finding probabilistic assumptions an invertible proof rule additionally guarantees the existence of such probabilistic assumptions when probabilistic systems satisfy intended properties second a learning algorithm for probabilistic assumptions is also needed with a carefully designed mechanical probabilistic assumptions can then be inferred by the framework for probabilistic systems finding a sound and invertible reasoning proof rule does not appear to be a problem indeed the classical proof rule can be extended to probabilistic systems via probabilistic simulation learning probabilistic assumptions however is more difficult to the best of our knowledge an active learning algorithm for probabilistic systems is yet to be found in fact it is undecidable to infer labeled probabilistic transition systems under a version of learning model learning algorithms for general probabilistic systems may not exist after all given the absence of learning algorithms for probabilistic systems some authors propose restricted proof rules with only classical assumptions since classical assumptions can be represented by classical finite automata the l algorithm is employed to infer such assumptions in restricted probabilistic reasoning proof rules yet classical assumptions are of expressing general probabilistic behaviors such restricted proof rules are not invertible subsequently existing probabilistic reasoning frameworks are sound but incomplete we propose a sound and complete reasoning framework for verifying probabilistic safety properties on markov decision processes let m and m be and pp a probabilistic safety property our most idea is to consider weighted assumptions in our new reasoning proof rule m e a a m pp m m pp where a is a weighted automaton intuitively m e a means that every transition of a has a weight not less than the probability of the corresponding transition in m compared to the proof rules in ours but does not restrict the expressive power of assumptions more precisely we consider weighted automata whose weights are between and as weighted assumptions since transition functions of weighted automata can be probability distributions the class of weighted automata subsumes our reasoning proof rule is trivially invertible in order to find weighted assumptions in our framework we also need a learning algorithm for such assumptions although active learning algorithms for probabilistic systems are still unknown weighted assumptions on the other hand are due to the relaxation on transition functions our second is to adopt a wellknown representation that enables a simple learning algorithm for weighted assumptions observe that weighted automata can be implicitly represented by binary decision diagrams we hence develop an learning algorithm for and it to infer implicitly represented weighted assumptions with the two a mechanical is designed to guide our learning algorithm to find weighted assumptions for probabilistic safety properties we successfully develop a sound and complete reasoning framework by the problem of learning probabilistic systems in addition to completeness and weighted assumptions can also be very efficient note that assumptions are not unique if a probabilistic assumption establishes a probabilistic property a slightly different weighted but not necessarily probabilistic assumption most likely will establish the property as well since there are more useful weighted assumptions our new framework can be more effective in finding one of them additionally inferring weighted assumptions implicitly allows us to better integrate the framework with symbolic probabilistic model checking indeed experimental results from realistic test cases such as ieee and standards are promising start i si si start done si si done figure compositional verification can the state explosion problem even for probabilistic programs our technical contributions are summarized as follows · we propose the first sound and invertible reasoning proof rule with weighted assumptions for probabilistic safety properties on · we give an mtbdd learning algorithm under active learning model it uses a polynomial number of queries in the sizes of target and variable sets · with our new proof rule and learning algorithm we give the first sound and complete reasoning framework for probabilistic safety properties on · we compare our new technique with the monolithic probabilistic model checker experimental results suggest promising for our compositional technique this paper is organized as follows in section we illustrate our compositional verification technique by a small example in section of probabilistic systems and probabilistic model checking are provided section presents our sound and invertible reasoning proof rule the mtbdd learning algorithm is described in section our reasoning framework is given in section section reports the experimental results on parameterized test cases finally section concludes this paper a motivating example consider the probabilistic system node node composed of two i in figure the process has four states the initial state si the ready state si the state si and the failed state si initially both node and node begin at their respective initial states s and s the system may start up all nodes by the start action or choose one node to start by either the start or start action the two processes node and node on shared actions when the system starts up all nodes by the start action node to its ready state s with probability or to its state s with the probability simultaneously node to its ready and states s and s with probabilities and respectively note that the sum of probabilities on each action is each action hence gives a probabilistic distribution over states for actions only the process moves other processes hence node to its state s while node remains in its initial state s when the system chooses to start up node with the action start similarly when the process is at its ready state si it to its state si with probability or to its failed state si with probability on the action observe that the probability of a transition is not shown when it is hence from si to si with probability on the action in the system node node the system state ss is the failed state the system is designed so that the probability of reaching the failed state is no more than formally the intended property is assumption a learning algorithm counterexample node e a and a node no yes done figure overview start start go done s start start go done s start start go done s start start go done s figure weighted assumption a specified by the probabilistic computation tree logic formula where failed stands for f ss and f is the in the future temporal operator we would like to check whether the system satisfies the probabilistic property by compositional verification compositional reasoning with the proof rule to show node node it suffices to find a weighted assumption a that · node e a equivalently a performs every transition of node with no less probability and · a node equivalently the system a node satisfies the probabilistic property clearly one could choose a to be node if the system satisfies the intended probabilistic property but then the premise a node is precisely the conclusion node node would not benefit from compositional verification by choosing node as a weighted assumption overview we follow the framework to infer a weighted assumption satisfying the two conditions in the last subsection in the framework a learning algorithm is to infer weighted assumptions with the help of a mechanical the learning algorithm presents assumptions to the the checks if a weighted assumption the premises in the reasoning proof rule if not the mechanical will help the learning algorithm refine assumptions by counterexamples figure gives an overview of the framework on a weighted assumption a the checks node e a and invokes a model checker to verify a node if both premises are we are done otherwise the provides a counterexample to the learning algorithm the learning algorithm then modifies the weighted assumption a accordingly we illustrate the framework with concrete examples a assumption consider a weighted assumption a in figure on the actions start start go and done the assumption a can from a state to any state similar to the weight of a transition is not shown when it is for instance a from the state s to the state sj on the action go with weight for every j in comparison the process node moves from the state s to the states s s s s on the action go with probabilities respectively figure observe that a is not an since the sum of weights from the state s on the action go is ss start ss go ss go ss figure witness to a node ss start ss go ss go ss figure corresponding path in node node on receiving the weighted assumption a the mechanical whether the assumption a both premises in our probabilistic compositional verification proof rule it first checks if the assumption a performs every transition of node with a weight not less than the probability in node this is clearly the case consider for instance the transitions from s to s s s s on the action go the weights associated with these transitions of a are all equal to they are not less than the probabilities associated with the corresponding transitions of node respectively the premise node e a is the mechanical then checks the other premise by model checking model checking technically a probabilistic model checker does not take weighted assumptions as inputs since a is a weighted assumption a node need not be an a probabilistic model checker can not verify whether a node directly we need to lift the probabilistic model checking algorithm to weighted assumptions after model checking we find that the property does not hold on a node a witness to a node is shown in figure the witness has only one path from the initial state ss to the failed state ss its weight is × × is not satisfied on a node witness checking since a node the mechanical concludes that the weighted assumption a does not establish the intended probabilistic property on the other hand the mechanical cannot conclude that the system node node does not satisfy the property either since a has larger weights than node a weighted witness to a node is not necessarily a witness to node node before the weighted assumption a the mechanical checks if the witness to a node is spurious or not recall that the weighted assumption a contains all transitions in node the witness to a node therefore corresponds to a path in node node figure also recall that the weight associated with a transition in the weighted assumption start s s start go done s s done figure weighted assumption a a is not less than the probability of the corresponding transition in node the probability of the corresponding path in node node can be much smaller than the weight of the witness to a node indeed the corresponding path in node node has probability × × it does satisfy the intended probabilistic property the witness to a node is hence spurious the mechanical then should help the learning algorithm the weighted assumption by sending a counterexample selecting counterexamples in order to remove the spurious witness in figure from the weighted assumption a the mechanical selects a transition in the weighted assumption a which most to the spurious witness in figure the transitions s start s and s go s in the weighted assumption a contribute to the spurious witness the mechanical can send either of the transitions as a counterexample to the learning algorithm here let us say the mechanical sends the transition s go s as the counterexample the learning algorithm will then update the weight of the selected transition in revised weighted assumptions learning assumption after receiving a counterexample the learning algorithm will another weighted assumption suppose the learning algorithm the weighted assumption a figure for any transition its weight in a is no less than the probability of the corresponding transition in node for example the weighted assumption a from the state s to the states s s s s with weights respectively on the action go the corresponding transitions in node have probabilities respectively we have node e a a node moreover holds by model checking according to our compositional verification proof rule the mechanical concludes that the system node node satisfies the intended probabilistic property note again that a is a not a probabilistic assumption although a and node have the same number of states in the explicit representation their implicit mtbdd representations are different a has nodes and terminals node has nodes with terminals in the implicit representation compositional verification replaces the component node with a slightly smaller weighted assumption a in fact node is the only probabilistic assumption that can establish the probabilistic property if only probabilistic assumptions were considered reasoning would not be effective in this example weighted assumptions gives our framework more useful assumptions in compositional verification weighted automata and markov decision processes given a finite set s a weighted function on s is a mapping s q a weighted function on s is denoted as a vector of length s a probability distribution on s is a function d s q that ss ds a point distribution s on s s is a probability distribution where st if t s and st otherwise denote the set of weighted functions and probability distributions on s by s and ds respectively clearly ds s definition a weighted automaton wa is a tuple m s act t where s is a finite set of states s is an initial state act is a finite alphabet of actions and t s × act s is a weighted transition function a finite path in m is a nonempty finite sequence s s n sn where s i act and si i si is a transition with t si for all i n we denote by i si the i th state and n its length the weight wt of a finite path is t s s × t s s × · · · × t sn denote the set of all finite paths in m let be a set of finite paths is prefix containment free if for every is not a proper prefix of when is prefix containment free the weight wt of is wt definition a weighted automaton wa m s act t is a wa where t s t for every s t s and act a wa is nondeterministic there may be multiple transitions between two states on different actions adversaries are used to resolve nondeterministic choices in was let s denote a nonempty sequence of states in s and acts the set act t s t for some t an deterministic adversary is a function s act such that ss sn more general notion of adversaries involving exists but deterministic ones are sufficient for our problem a wa m under an adversary is therefore deterministic let m denote the set of adversaries of m we write m for the wa whose transitions are by the adversary m definition a markov decision process m s act t is a wa where t s ds or t s is the constant zero weighted function for every s s and act since the weighted functions returned by weighted transition functions of are probability distributions the weight with each transition in is referred to as probability let si be a finite set and i si for i define s s s × s observe that s × s and if s and s then s s definition let mi si ti be wa for i the parallel composition of m and m written m m is a wa m m s × s act act t where ts s t s s s ts ts ts if act if act if act act observe that parallel composition of two was yields a wa and parallel composition of two yields an example the process node in figure is an with s s s s s act node start start go done and s if s s start others are defined similarly on the other hand a in figure is a wa since its weighted transition function does not return probability distributions probabilistic model checking for fix a finite set ap of atomic propositions we focus on probabilistic safety properties specified by probabilistic computation tree logic pctl in the form of pp with p and true a ¬ u where a is an atomic proposition a state formula a path formula and u the until temporal operator for example the probability of an error occurrence is at most is specified as u err where err is a state formula indicating the occurrence of an error pctl and the safety fragment in general allow nested operators in this paper we consider a fragment known as conditional reachability probability and leave the extension to general pctl safety property as our future work given m s act t and pp with u we define wt where i i ¬ observe that is prefix containment free and prob is the probability of reaching states along states under the adversary let max m denote the maximal prob ability that is satisfied at s over all adversaries we say that s satisfies pp written m s pp if p m satisfies pp written m pp if m pp we write pm s and as ps and ax respectively if m is clear let u the probability can be by an iterative algorithm the computation starts from the states satisfying and iterates backward to compute the max probability of reaching these states from the states satisfying more precisely define max if s if s s if s s i t s t × ts otherwise then i the computation iterates until a weighted witness to m pp is a pair c where m is an adversary with p and c is a set of finite paths in m such that for all c for all proper prefix of and p observe that the set c is prefix containment free hence is welldefined we obtain the of m written m c by removing all transitions not appearing in any path of c from m example consider the weighted witness c shown in figure where ss start ss ss go ss ss ss go and c ss start ss go ss go ss the weight of c is × × model checking for was consider our proof rule where a is a weighted assumption the parallel composition a m usually yields a wa but not an the probabilistic model checking algorithm in preceding section needs be adapted to was given a state s of a wa m and a probabilistic safety property pp we say s satisfies pp written m s pp if the weight p where is defined similarly as for here is referred to as weights rather than probability note we again omit the subscript m when it is clear we say that m satisfies pp written m pp if m pp an iterative algorithm similar to the one for is used to compute for was a difference is that the iterative computation may not converge in a wa to avoid divergent computation define min max t s t × ts if s s i effectively the value of when the latter let ax i we have and ax when note that the properties we are interested in are of the form pp with p when p such properties are trivially satisfied for p we have p if p ax an reasoning proof rule reasoning proof rules for probabilistic systems are proposed in those proof rules replace a probabilistic component in a composition with a classical assumption since classical assumptions can not characterize all probabilistic behaviors of the replaced component such rules are not invertible we propose an reasoning proof rule that replaces a probabilistic component with a weighted automaton we begin with the weighted extension of the classical simulation relation definition let m s act t and m s act t be was we say m is embedded in m written m e m if s s act act and t s t t s t for every s t s and act lemma let m m n be was and pp a probabilistic safety property m e m implies m n e m n m e m and m pp imply m pp proof let m s act t m s act t and n sn tn be was by m e m we have s s and act act hence m n and m n have the same state space initial state and alphabet since t s t t s t and tn p q t s t × tn p q t s t × tn p q for every s t s p q sn and act hence tm n s p t q tm n s p t q for every act by definition since t s t t s t p for m pp thus p m pp as well lemma shows that the operator e is compositional and preserves probabilistic safety properties hence theorem let mi si ti be for i and pp a probabilistic safety property then the following proof rule is both sound and invertible m e a a m pp m m pp where a sa ta is a wa proof soundness of the proof rule follows from lemma by m e a m m e a m lemma since a m pp we have m m pp lemma the proof rule is also invertible when the conclusion holds m itself is a weighted assumption the two premises are trivially note here we need p to conclude p learning weighted automata we adopt the framework to generate an assumption a in the reasoning proof rule theorem to apply our new proof rule a weighted assumption is needed one could employ learning algorithms that infer explicit quantitative models like multiplicity automata those learning algorithms require complex and accurate matrix operations they hence induce substantial computation and implementation overheads to avoid such overheads we adopt a different representation to enable a simple and efficient learning technique more precisely we use to represent weighted assumptions implicitly to infer implicitly represented weighted assumptions we then develop an mtbdd learning algorithm under learning model binary decision diagrams let b denote the boolean domain fix a finite ordered set of boolean variables x x x xn a valuation v v vn of x assigns the boolean value vi to the boolean variable xi let µ and be valuations of x and y respectively with x y the concatenation of µ and is the valuation µ of xy such that if z x and z if z y for y x the restriction y of on y is a valuation of y that yy y for y y let f x bn q be a function over x we write f for the function value of f under the valuation let fx and fx be functions over x fx fx denotes f f for every valuation of x a binary decision diagram mtbdd over x is a rooted directed acyclic graph representing a function f x bn q an mtbdd has two types of nodes a nonterminal node is labeled with a variable xi it has two outgoing edges with labels and a terminal node is labeled with a rational number the representation supports binary operations for instance the mtbdd of the sum of two functions is computed by traversing the of the two functions given a valuation of x f can be obtained by traversing the mtbdd of f x starting from the root one follows edges by values of the boolean variables labeling the nodes when a terminal node is reached its label is the value f since a function f x and its mtbdd are equivalent f x also denotes the mtbdd of the function f x by the notation it is straightforward to represent a wa by let m s act t be a wa without loss of generality we assume s n and act m we use x x x xn x x x xn to encode states and next states in s and z z z to encode actions in act let be valuations of x x and a valuation of z the action valuation of x is valid if it maps at most one variable to at most one action can be taken a valuation of x or x encodes a state s a valid action valuation encodes an action act define lm if otherwise fm t if is valid otherwise then the mtbdd encoding of m is x lm x z fm z x x we will represent a wa by its mtbdd encoding from now on example consider the process node in figure where the states are si for i and the alphabet of actions is act start start go done we use x s s to encode the set of states and z start start go done to encode the alphabet of actions the mtbdd of z x x is shown in figure in the figure the terminal node by start start go done s s s s figure mtbdd of the number and its incoming edges are not shown edges are labeled by and dotted edges are labeled by hence for instance the process node from s to s on the action go with probability by the path note that valuations of z need be valid thus only the valuations and of z yield nonzero values of using theorem is as follows corollary let mi xi xi z z xi xi be for i and pp a probabilistic safety property then m e a a m pp m m pp where a x lm x z x x is a wa the l learning algorithm for regular languages we adapt the l algorithm to infer an mtbdd representing a weighted assumption l is a learning algorithm for regular languages assume a target regular language only known to a the l algorithm infers a minimal deterministic finite automaton the target regular language by the following queries to the · a membership query if a string belongs to the target language and · an equivalence query if a finite automaton the target language if not the has to provide the learning algorithm a string as a counterexample the l algorithm uses membership queries to construct the transition function of a deterministic finite automaton when it constructs a deterministic finite automaton consistent with previous membership queries l an equivalence query to check if the automaton does recognize the target regular language if so the algorithm has learned the target language correctly otherwise the counterexample is used to improve the finite automaton it can be shown that the l algorithm always infers the minimal deterministic finite automaton any target regular language within a polynomial number of queries an mtbdd learning algorithm since any wa can be represented by we develop an mtbdd learning algorithm to infer weighted assumptions let f x be an unknown target mtbdd we assume a to answer the following types of queries · on a membership query mq with a valuation of x the answers f · on an equivalence query with a conjecture mtbdd gx the answers yes if f g otherwise she returns a valuation of x with f g as a counterexample observe that a valuation of x can be represented by a binary string of length x to illustrate how our mtbdd learning algorithm works consider an unknown mtbdd f x with exactly values and r since there are finitely many binary strings of length x the language r of binary strings representing valuations of x that evaluate f to r is regular the l learning algorithm for regular languages hence can be used to infer a finite automaton the language r the learning algorithm applies the theorem for regular languages it constructs the transition function of the minimal deterministic finite automaton for any unknown regular language by membership and equivalence queries about the unknown target since the minimal deterministic finite automaton for r is structurally similar to the mtbdd f with two terminal nodes the l algorithm can be modified to infer with two terminal nodes generally an unknown mtbdd f x has k values r r rk it evaluates to a value ri on a valuation of x moreover the language ri of binary strings representing valuations of x that evaluate f to ri is regular for every i k consider generalized deterministic finite automata with k acceptance types on any binary string the computation of a generalized deterministic finite automaton ends in a state of an acceptance type formally define a l over an alphabet to be a partition l l lk of that is and li lj when i j a finite automaton d q q f consists of a finite state set q an alphabet a transition function q × q an initial state q q and acceptance types f f f fk where form a partition of q define q q and q aw q a w where a and w for a string w we say d accepts w with type i if q w fi let w d accepts w with type i a hence accepts a ld i k it is almost straightforward to show a generalized theorem for theorem the following statements are equivalent a l l l lk is accepted by a define the relation r over such that if and only if for every z xz yz li for some i r is of finite index in order to learn general we modify the l algorithm to generate consider binary strings of length x representing valuations of x since an mtbdd evaluates a valuation to a value the values of an mtbdd partition x with x an mtbdd in fact gives a partition of in other words an mtbdd defines a by theorem the modified l algorithm infers a minimal that accepts the defined by an unknown mtbdd it remains to derive an mtbdd learning algorithm from the modified l algorithm for two minor problems need to be addressed in the design of our mtbdd learning algorithm first the modified l algorithm makes membership queries on binary strings of arbitrary lengths the for learning only answers membership queries on valuations over fixed variables second the modified l algorithm presents a as a conjecture in an equivalence query the mtbdd however accepts as to solve these problems we apply the techniques in when the modified l algorithm a membership query on a binary string our mtbdd learning algorithm checks if the string has length x if not the mtbdd learning algorithm returns to denote the weight otherwise the mtbdd learning algorithm the corresponding valuation of x to the and returns the answer to the modified l algorithm when the modified l algorithm gives a in an equivalence query the mtbdd learning algorithm transforms the automaton into an mtbdd it basically turns the initial state into a root each state at distance less than n into a nonterminal node labeled with variable xi and each state at distance n into a terminal node theorem let f x be a target mtbdd the mtbdd learning algorithm outputs f in polynomial time using of f log x membership queries and at most f equivalence queries proof the modified l algorithm outputs the minimal f using of f log m membership queries and at most f equivalence queries where m is the length of the longest counterexample every membership or equivalence query of the modified l algorithm induces at most one query in the mtbdd learning algorithm when the modified l algorithm makes an equivalence query with a the mtbdd learning algorithm transforms it into an mtbdd of the same size in polynomial time whenever a counterexample is obtained from the mtbdd the mtbdd learning algorithm the corresponding binary string of length x to the modified l algorithm hence the learning algorithm infers the mtbdd f with of f log x membership and f equivalence queries the verification framework with our new reasoning proof rule section and learning algorithm for section we can now describe our sound and complete learning framework let mi xi xi z z xi xi be i and pp a probabilistic safety property to verify if mm pp holds we aim to generate a wa a x lm x z x x to the premises m e a and am pp to find such a weighted assumption a we use the mtbdd learning algorithm to infer an mtbdd x x as the weighted transition function recall that the mtbdd learning algorithm relies on a to answer queries about the target mtbdd we therefore will design a mechanical to answer queries from the learning algorithm figure let be a valuation encoding an action and valuations encoding states the mechanical consists of the membership query resolution algorithm and the equivalence query resolution algorithm the membership query resolution algorithm answers a membership query mq by the weight associated with the transition from to on action in a weighted assumption the premises of the reasoning proof rule similarly the equivalence query resolution algorithm answers an equivalence query by checking whether the mtbdd fa represents the weighted transition function of a weighted assumption the equivalence query resolution algorithm should return a counterexample when fa does not represent a suitable weighted transition function recall that m itself is trivially a weighted assumption our simply uses the weighted transition function fm of m as the target in the worst case our framework will find the weighted assumption m and hence completeness in practice it often finds useful weighted assumptions before m is inferred mq fm mtbdd learning algorithm eq fa mechanical fa false m e a true am pp false c true pp true false mm pp mm pp is by c figure learning framework for compositional verification membership queries our membership query resolution algorithm targets the weighted transition function of m clearly m itself and hence can be used as a weighted assumption on the membership query mq the mechanical simply returns fm input mq output a rational number answer mq with fm algorithm equivalence queries on an equivalence query the mechanical is given an mtbdd x x consider the wa a x lm x z x x we need to verify if both premises of the reasoning proof rule in corollary hold the equivalence query resolution algorithm first checks if m e a if not there are valuations and with fm fa the equivalence query resolution algorithm returns as a counterexample to if m e a the equivalence query resolution algorithm continues to check whether a m pp holds by model checking if a m pp holds the mtbdd learning algorithm has inferred a weighted assumption that establishes m m pp by the reasoning proof rule in corollary otherwise the equivalence query resolution algorithm obtains a weighted witness c to a m pp from model checking it then checks if the weighted witness is spurious recall that m m and a m have the same state set and action alphabet due to m m e a m the m mc is welldefined if m mc pp the weighted witness c is spurious the algorithm then analyzes the spurious weighted witness c and returns a valuation as the counterexample otherwise the algorithm concludes m mc pp with the weighted witness c algorithm example consider the weighted witness in figure the node is shown in figure there is but one path in node this path ends in ss and hence satisfies failed its weight however is thus node the weighted witness in figure is spurious input output yes a counterexample to a x lm x z x x if fa fm then answer with the counterexample receive a new equivalence query call if am pp then answer with yes return mm pp else let c be a weighted witness to am pp if m mc pp then select a transition µ µ with the maximal contribution from a mc answer eq fa with x µ x receive a new equivalence query call else return mm pp with c end end algorithm selecting counterexamples given a spurious weighted witness c the mechanical selects a transition from c as a counterexample to the mtbdd learning algorithm the counterexample is intended to remove the spurious weighted witness c from weighted assumptions let c be a spurious weighted witness with a mc pp and m mc pp recall that m mc and a mc have the same state set initial state and alphabet the only differences between m mc and a mc are the weights associated with transitions in order to remove the spurious weighted witness c we would like to select transitions which m mc from a mc most significantly more precisely for any transition t in c let at and t be the sets of paths in respectively a mc and m mc which contain transition t define t to be the contribution of transition t in the spurious weighted witness c the mechanical simply selects a transition t with the maximal contribution the weight of the selected transition in a will be revised to the probability of the corresponding transition in m its contribution will be in following note that contributions of transitions are computed using for efficiency details are omitted due to space limit observe moreover that selecting one transition may not eliminate the spurious weighted witness since a spurious weighted witness contains several transitions the weight of the witness may not be reduced sufficiently after a few transitions subsequently the same spurious weighted witness may be by model checking the premises with a revised weighted assumption in order to reduce the number of model checking invocations we reuse the same spurious weighted witness to compute counterexamples more precisely our implementation checks if the current spurious weighted witness is eliminated from revised weighted assumptions if not the mechanical selects another transition from the spurious weighted witness to further refine the revised weighted assumptions since a spurious weighted witness is used to several weighted assumptions the number of model checking invocations is reduced correctness the correctness of our reasoning framework for probabilistic systems follows from theorem we establish the soundness completeness and termination of the new framework in the remainder of this section theorem soundness let mi xi xi z z xi xi be for i pp a probabilistic safety property and x x an mtbdd · if returns mm pp then m m pp holds · if returns mm pp with c then c is a weighted witness to mm pp proof when our framework reports m m pp in algorithm a weighted assumption a x lm x z x x such that m e a and a m pp has been inferred by the soundness of the reasoning proof rule theorem m m pp on the other hand suppose our framework reports m m pp the weighted witness c to a m pp has been verified to be a witness to m m pp theorem completeness let mi xi xi z z xi xi be for i and pp a probabilistic safety property · if mm pp then returns mm pp for some mtbdd x x · if mm pp then returns mm pp with a weighted witness c proof in our framework the mtbdd learning algorithm targets the weighted transition function of m it will infer fm z x x eventually theorem if m m pp the learning algorithm always infers a weighted assumption a in the worst case a is m such that m e a and a m pp hence returns m m pp otherwise the learning algorithm always infers a weighted assumption a in the worst case a is m such that m m pp is by c returns m m pp theorem termination let mi xi xi z z xi xi be for i and pp a probabilistic safety property our framework reports m m pp or m m pp within a polynomial number of queries in fm z x x and z x x proof in our framework the mtbdd learning algorithm targets the weighted transition function of m it will infer the target mtbdd fm z x x using on n log m membership queries and at most n equivalence queries where n fm z x x and m z x x theorem at this point the weighted assumption a is m the mechanical reports either m m pp or m m pp experiments we have implemented a prototype of our compositional verification technique on top of it accepts an specified in the modeling language and a probabilistic safety property the mtbdd learning algorithm is implemented by modifying the l algorithm in library with package the membership query resolution algorithm algorithm and the embedded checking algorithm algorithm are implemented using probabilistic model checking in the equivalence query resolution algorithm algorithm is performed by using the mtbdd engine we generate counterexamples by the techniques in all experiments were run on a virtual machine with cpu and gb ram our compositional approach is evaluated on several parameterized examples all examples are derived from the for each model we check a probabilistic safety property all models and properties are briefly described below · consensus models a randomized algorithm which allows n processes in a distributed network to reach a consensus by accessing a global shared counter parameterized by k the specification describes the probability that eventually all processes make a decision but some two processes do not agree on the same value is at most p · models the local area networks specified in the ieee standard of a network cannot to their own transmission each has a counter with the maximal value of b to minimize the of transmission the time bounded version of this model is considered the specification describes the probability that either counter the number k within some time is at most p · models the tree identify protocol of the ieee high performance serial among nodes connected in a network a root node needs to be to act as the manager of the in the network the time bound for message transmission is parameterized by the implementation version of this model is considered the specification describes the probability that a root node is eventually before some time passes is at most p · models a randomized solution to the philosophers problem n philosophers around a circular table philosophers share a resource a can if he obtains the resources from both sides the specification describes the probability that philosophers do not obtain their shared resource simultaneously is at most our tool selects one process of the model as m and the composition of other processes as m selecting the composition of multiple processes as m can be done by solving the decomposition problem here we employ a simple heuristic choose the process with the minimal interface alphabet the interface alphabet of a process is the set of shared actions for example the model consists of four processes and with interface alphabets send send f f time f send time f send and time respectively we choose as m by our heuristic the first experiment compares the performance of our compositional approach compositional with the monolithic probabilistic model checking in monolithic experimental results are listed in table for each model and a corresponding probabilistic safety property pp we compute the property of by and report it in the column note that the property pp holds on the model if and only if p the satisfiability of the property pp on the model is reported in the result column for each test case we show the model size size and run time time the model size counts the number of mtbdd nodes for the weighted transition function of the composed model the run time includes the time spent on all stages including model construction model checking witness analysis and assumption learning for monolithic approach both with the mtbdd engine and with the hybrid engine are tested their run times are listed in tm and th columns respectively for compositional approach the number of calls call is also reported the last column reduction shows the reduction of model size and time of our compositional approach to all time is in seconds and the symbol ­ indicates either timeout or gb the results are very in of cases the compositional verifier significantly moreover a reduction of in time is achieved in cases and a reduction of in model sizes is in cases our compositional approach benefits the verification by avoiding the construction of the whole model in the size reduction column of table our compositional approach succeeds in learning an assumption a such that the size of a m is much smaller than that of m m in most cases only for the two smallest cases in the consensus example our compositional approach performs worse one possible reason is that the sizes of the models are so small that compositional verification is redundant also observe that performs much better then in the consensus example techniques monolithic or not may not be the best choice for this example hybrid and and ours techniques can also be compared in table the results depend on the examples similar were also reported in for all cases in the consensus example performs much better than both techniques for all cases in the example the of and are similar however in the more realistic and examples runs out of memory quickly when the model size becomes large the second experiment evaluates the impact of the probability bound p on the effectiveness of compositional verification this experiment is performed on examples with different probability bounds the results on the example with b k are in figure a when p is above or nearly above the performance of our approach goes down quickly the reason is that the property becomes satisfied when p more equivalence queries are then required to infer a proper assumption to prove both premises of the reasoning rule on the other hand if the probability bound p is less than a weighted assumption suffices to verify the property by composed model we mean m m for monolithic checker and a m for our checker can be observed on the consensus example with n k figure b the result from the example with figure c is quite different observe that the actual probability of the examples is its properties are trivially for any p thus there is no edge in figure c as in other figures in the example the probability is and hence the properties are always satisfied for any p similar to we do not observe any edge and hence skip the figure of the example compositional verification however always the monolithic algorithm regardless of satisfiability of properties in both examples related works the most relevant works to ours are in their proof rules assumptions are classical deterministic finite automata the l algorithm has been applied to infer classical assumptions in as discussed above classical assumptions cannot express general probabilistic behaviors such techniques are sound but incomplete we adopt weighted automata as assumptions to have a sound and invertible proof rule our technique is both sound and complete a sound and invertible reasoning proof rule for probabilistic io systems is given in the framework however only works for fully probabilistic discrete time markov chains and may not terminate our technique in contrast applies to markov decision processes and always terminates undecidability of inferring labeled probabilistic transition systems under active learning model is shown in a necessarily restricted learning algorithm for such probabilistic systems is also proposed in the same paper in addition to learning different concepts the restricted algorithm does not membership queries whereas ours does an alternative direction for generating probabilistic assumptions is to use abstraction refinement techniques in probabilistic assumptions are conservative abstractions of system components they are iteratively refined by counterexamples however relies on partitioning the explicit state space to construct assumptions we are not aware of any symbolic implementation of techniques for classical or probabilistic systems various learning algorithms have been proposed for probabilistic systems these learning algorithms adopt passive learning model they are not applicable to the reasoning framework in learning algorithms for binary decision diagrams were proposed in in an algorithm was developed the work in used a classification learning algorithm for regular languages both algorithms inferred deterministic finite automata and transformed them into decision diagrams conclusion we proposed a sound and complete reasoning technique for probabilistic safety properties on instead of probabilistic assumptions we infer weighted assumptions for compositional verification using an mtbdd learning algorithm our technique generates implicit representations of weighted assumptions experimental results show that the reasoning technique the monolithic probabilistic model checking in most of the test cases our technique can be applied to sequential probabilistic systems let m be an and pp a probabilistic safety property one generates a wa a such that m e a and a pp by our technique it is however not when m is composed of concurrent since the construction of the composition can be very expensive the computation should table experimental results monolithic vs compositional example consensus p p p p param result false false false false false false false false false true false true true false false false false false true true true true true th ­ ­ ­ ­ ­ ­ ­ ­ monolithic tm size compositional call t size reduction time size monolithic compositional monolithic compositional p a mtbdd size run time s monolithic compositional monolithic compositional p b mtbdd size run time s monolithic compositional monolithic compositional p c figure of p on the performance of approaches red for compositional blue for monolithic run time s mtbdd size be deferred after concurrent components are simplified reasoning presented in this paper is certainly currently our implementation receives a finite set of paths as weighted witnesses to m pp generally weighted witnesses to m p p where are represented as graphs with strongly connected components we plan to generalize transition contributions to select counterexamples from spurious weighted witnesses with strongly connected components we moreover would like to extend our learning framework to verifying richer properties such as general probabilistic safety or liveness properties acknowledgments he corresponding author and are supported by the national plan cb the nsf of the national key technology rd program the and development of project of and the university scientific research program wang corresponding author is supported by the of science and technology of e my corresponding author is supported by the natural science foundation of under grant no the international program for research references ieee standard for a serial ieee pages ­ oct ieee standard for information and information exchange between systems local and area requirements part lan access control mac and physical layer specifications ieee of ieee pages ­ march d learning regular sets from queries and counterexamples information and computation ­ j and m fast randomized consensus using shared memory journal of algorithms ­ c and jp principles of model checking mit press c e m clarke v m and m symbolic model checking for probabilistic processes in icalp volume of lncs pages ­ springer a f n h e and s learning functions represented as multiplicity automata journal of acm ­ may a and l de model checking of and nondeterministic systems in volume of lncs pages ­ springer b jp c m d and d r the automata learning framework in cav volume of lncs pages ­ springer y chen h m t k and b learning markov models for stationary system behaviors in formal methods volume of lncs pages ­ springer yf chen e m clarke a yk and by wang automated reasoning through implicit learning in cav volume of lncs pages ­ springer e clarke o s y lu and h counterexampleguided abstraction refinement in cav volume of lncs pages ­ springer j m d and c s learning assumptions for compositional verification in tacas volume of lncs pages ­ springer j m g s and l a clarke up is hard to do an evaluation of automated reasoning acm transactions on software engineering and methodology l de m g d and r symbolic model checking of probabilistic processes using and the representation in tacas volume of lncs pages ­ springer l m and d compositional verification of probabilistic systems using learning in pages ­ ieee l t m and d compositional verification for synchronous probabilistic systems in volume of lncs pages ­ springerverlag m p c and jy yang binary decision diagrams an efficient data structure for matrix representation formal methods in system design ­ r and d learning ordered binary decision diagrams in volume of lncs pages ­ springer m d and c s refining interface alphabets for compositional verification in tacas volume of lncs pages ­ springer m c s and d automated reasoning by abstraction refinement in cav volume of lncs pages ­ springer a gupta k l and z fu automated assumption generation for compositional verification in cav volume of lncs pages ­ springer t jp and d counterexample generation in probabilistic model checking ieee transactions on software engineering ­ h and b a logic for reasoning about time and reliability formal aspects of computing ­ f he by wang l and l symbolic reasoning through bdd learning in pages ­ acm a m g and d a tool for automatic verification of probabilistic systems in tacas volume of lncs pages ­ springer jp l and l probably safe or live in proceedings of the joint of the annual conference on computer science logic csl and the annual symposium on logic in computer science lics pages ­ acm s and e m clarke a parallel algorithm for constructing binary decision diagrams in pages ­ ieee a c s and e m clarke abstraction refinement for probabilistic systems in cav volume of lncs pages ­ springer a c s and e m clarke learning probabilistic systems from tree samples in lics pages ­ ieee m g and d probabilistic symbolic model checking with a hybrid approach international journal on software tools for technology transfer ­ m g d and h verification for probabilistic systems in tacas volume of lncs pages ­ springer d and m o on the advantage of free choice a symmetric and fully distributed solution to the philosophers problem extended abstract in popl pages ­ acm h y chen m t d k g and b learning probabilistic automata for model checking in pages ­ ieee h y chen m t d k g and b learning markov decision processes for model checking r and p randomized algorithms cambridge university press a an efficient query learning algorithm for ordered binary decision diagrams information and computation ­ d a implementation of symbolic model checking for probabilistic systems phd thesis university of r and n probabilistic simulations for probabilistic processes in concur volume of lncs pages ­ springer learning probabilistic automata and markov chains via queries machine learning ­ r n e b and jp minimal critical for markov models in tacas volume of lncs pages ­ springer r n a e jp and b highlevel counterexamples for probabilistic automata in pages ­ ieee h f he w n x and m data based decomposition for reasoning in pages ­ ieee 