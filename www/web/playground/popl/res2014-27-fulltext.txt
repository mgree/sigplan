tabular a probabilistic programming language andrew d gordon microsoft research and university of edinburgh microsoft research microsoft research microsoft research university john microsoft research abstract we propose a new kind of probabilistic programming language for machine learning we write programs simply by annotating existing relational schemas with probabilistic model expressions we describe a detailed design of our language tabular complete with formal semantics and type system a rich series of examples illustrates the expressiveness of tabular we report an implementation and show evidence of the of our notation relative to current best practice finally we describe and verify a transformation of tabular schemas so as to predict missing values in a concrete database the ability to query for missing values provides a uniform interface to a wide variety of tasks including classification clustering and ranking categories and subject descriptors d programming languages language application languages i artificial intelligence learning keywords bayesian reasoning machine learning pattern probabilistic programming relational data introduction the core idea of this paper is to write probabilistic models by annotating relational schemas we illustrate this idea on a database for recording outcomes of a game without players name string matches player player bool in this concrete schema we have a players table with column name and a matches table with columns player player and permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page for components of this work owned by others than acm must be abstracting with credit is permitted to copy otherwise or republish to post on servers or to redistribute to lists requires prior specific permission andor a fee request permissions from popl january ­ san diego ca usa copyright c acm player as well as scalar types such as bool or string a column may have a type such as which means the column holds integer keys to the players table for simplicity we assume that every table has a primary key id a common case in practice we also assume that in a table with n rows the keys are integers normalized to lie in the range n thus we omit the primary key column from schemas to illustrate some of the key ideas of tabular we consider the model et al which is at to make of players of roughly equal as in online in this model each player has an underlying numeric players in a match are copies of their and each match is by the player with the greater performance players name matches player player string real input latent gaussian real real bool input input latent latent output although its starting point is the underlying concrete schema a tabular schema may contain additional latent columns which contain random variables to help model concrete data in our example the players table has a latent column containing a numeric for each player while the matches table has latent columns and containing the of the two players in the match so that a schema defines a probability distribution over database instances we annotate columns with probabilistic model expressions which define distributions over entries in the column model expressions allow to be made for the values of associated columns our example shows three sorts of annotated column a concrete column marked as an output has a model expression that values of the column for example the column is an output its model expression indicates the is the player with the greater performance the model expression can be applied to predict a future match outcome based on from training data a concrete column marked as an input is used to condition the probabilistic model but has no model expression and cannot be by the model for example the player column in the matches table is an input it is used to characterize a match but is not considered to be finally a column marked as latent is an auxiliary column not present in the concrete database whose model expression forms part of the model and can be for example the column has a model expression indicating each entry is drawn from a gaussian distribution with mean and precision a tabular program the columns of the concrete database into input and output columns and determines a probabilistic model that the output columns given the input columns if all the cells in a concrete column have values we say the column is observed but otherwise when there are missing values we say it is observable we consider two forms of inference in both forms input columns are observed in we assume that output columns are have data for each cell in the the task is to predict the latent columns towards the end of the paper in section we also consider where output columns are observable and the task is to predict the missing values in output columns given a table of players and a table listing the outcomes of matches between those players infers a numeric for each player used for consider the following tables of players and matches players id name alice bob matches id player player false false initially assigns the same prior to each player given data showing that player has been by player who in turn has been by player infers posterior distributions the likely ranking player player player the problem for tabular is to determine the probability distribution over latent databases for a given schema given a concrete database in theory the latent database is a joint distribution over all latent columns of the database in a practical implementation we consider only the projections of each of the variables in the latent database in particular for the schema on the concrete database above the representation of the distribution over latent databases consists of the following tables id gaussian gaussian gaussian id gaussian gaussian gaussian gaussian the distribution over the latent database can be stored in the same relational store as the original concrete database with the concrete tables while tabular is specific to the domain of specifying probabilistic models for relational data users are free to whatever programming or query notation is appropriate to the data into relational form and to the results of inference in this mode we use tables with missing values in observed columns as queries for example the following amounts to a query how likely it is that player would player to help decide on placing a matches id player player the result of such a query might be the following indicating there is an player will player id a for probabilistic modelling in designing tabular we have in mind data the large class of end users who wish to model and learn from their data who have some knowledge of probability distributions and database schemas but who are not necessarily programmers tabular supports the following for modelling data start with the schema such as the players and matches tables add latent columns and write probabilistic models for latent and observed columns have a prior are copies of the player with the performance a learn latent columns and table parameters from complete data we learn players from a of match outcomes b or predict missing values from data we predict a future match outcome based on a row pp there is more to the whole cycle of learning from as and preprocessing data and and interpreting the above addresses a crucial component models as factor graphs factor graphs are a standard class of probabilistic graphical models of data with many applications and friedman having modelled data with a factor graph one can apply a range of inference algorithms to infer properties of the data or make the model was originally expressed as a factor graph such as the one below in typical and notation the circular nodes of the graph represent random variables and the black are factors relating random variables the large enclosing boxes labelled players i and matches m are known as and indicate that the subgraphs are to be replicated the two dotted boxes are known as and and indicate choices by an incoming edge the nodes for some random variables are to indicate they are observed while variables are latent together with exact factor annotations factor graphs represent joint probability distributions like many notations factor graphs become as models become complex instead we turn to probabilistic programming languages where models are code random variables are program variables factors are primitive operations are loops and are conditionals or switches bugs et al is the most popular example and there is much current interest witness the in this paper we create models with a direct interpretation as factor graphs by writing schema annotations in a highlevel probabilistic language in the design of tabular by using the relational modelling of the data encoded in the concrete schema we write models because each table description implicitly defines a loop a over its rows moreover we save our user the of writing code to transfer data and results between language and database the main conceptual in tabular are annotations on a relational schema so as to construct a graphical model with input output and latent columns a grammar of model expressions to the models for latent and output columns with the semantics of tables and schemas given as models from the models for individual columns infer latent columns from the concrete database given input columns and output columns infer missing values in output columns given input columns and output columns technical contributions and evaluations we present the detailed syntax and type system of tabular and semantics by translation to a core probabilistic calculus fun theorem translation preserves typing asserts that the semantics respects the tabular type system theorem asserts that a certain factor graph expressed in fun correctly implements we describe an implementation of tabular using based on our semantics to test tabular in practice we a series of models for data first performed using directly et al with essentially the same results theorem a transformation on tabular schemas that implements in terms of an extended version of this paper with additional examples and appears as a technical report gordon et al b fun and the pattern fun probabilistic programming for factor graphs we use a version of the core calculus fun et al with arrays of deterministic size but without a operation observe within expressions this version of fun can be seen as a firstorder subset of the stochastic lambdacalculus ramsey and it is also to and fun expressions have a semantics in the probability monad but also have a direct interpretation using factor graphs we have scalar types bool int and real record types that are constructed from field typings and array types let string int and vector real and matrix vector let c range over the field names s range over constants of base type and let t mean that constant s has type t types and values scalars records arrays t v s bool int real tu s rt t scalar type type rt c t rt field typings v s c v cn vn v vn expressions of fun e e f xs if e then f else f r ec e en ef for x e f let x e in f ge en de en r c e r expression variable constant ifthenelse record literal projection array literal lookup scope of index x is f let scope of x is f primitive g with arity n distribution d with arity n field bindings we write fv for the set of variables occurring free in a phrase of syntax such as an expression e and identify syntax up to consistent renaming of bound variables we sometimes use tuples e en and tuple types t · · · tn below they stand for the corresponding records and record types with numeric field names n we write fst e for e and snd e for e the empty record represents a void or unit value we write c t cn tn for a concrete record type and thus for the empty record type c e cn en for a concrete record term and use the syntax ci and ci to index the components of a record type or term when ordering or c and c where c is a set of field names when ordering is irrelevant field typings and field bindings are just association lists we sometimes use rt rt to denote the concatenation of field typings rt and rt and r r for the concatenation of field bindings we implicitly identify record types up to reordering of field typings we assume a collection of total deterministic functions g including arithmetic and logical operators we also assume families d of standard probability distributions including for example the following a gaussian takes a precision parameter precision the standard follows from the identity precision distributions d x t xn tn t real bool gaussian mean real precision real real beta a real b real real shape real scale real real length int alpha real vector discrete vector int range int int models we explain the semantics of tabular by translating to bayesian models encoded using fun expressions we consider a bayesian model to be a probabilistic function from some input to some output that is by a parameter itself generated from a deterministic our semantics is compositional the model of a whole schema is from models of tables which themselves are composed from models of rows from models of individual cells this formulation follows gordon et al a with two refinements first when we apply a model to data the model output is that is each output is a pair consisting of an observed component like a game outcome in plus an latent component like a performance in second the is passed to the sampling distribution w x as well as to the parameter distribution for convenient model building notation for bayesian models w x eh default eh deterministic ew distribution over parameter given h distribution over output given h w and x and parameters both determine the distribution of outputs given an input the difference is that we specify our knowledge of parameters but not using the prior distribution so that our uncertainty about parameters but not is reduced by on data for example here is a model for linear that is the task of a straight line to data points this example illustrates the informal notation for fun expressions used in section for instance we write a to mean that random variable a is distributed according to we write x e to indicate that x is the value of deterministic expression e linear of informal notation for fun w x the record the record a a b b where a and b the pair y z where z wa x and y in our formal semantics for tabular we use a compact notation p eh h w for a model our example is written in compact notation as follows a in let b in a a b b h w z wa x in let y in y z we use variable x for the input y for the observed output z for the latent output w for the parameter and h for the databases as fun values we view a database as a record t b tn bn holding relational tables b bn named t tn a table b is an array r rm of rows where each row is a record ri c v cn vn where c cn are the columns of the table and v vn are the items in the column for that row we view a table as an array so that a primary key is simply an index into the array and omit primary keys from rows the column annotations in a tabular schema partition a whole database into a pair d dx dy where dx is the input database with the input columns of each table and dy is the observed database with the observed columns of each table for each table the numbers of rows in the input and observed databases must match the latent database dz is a database with just the latent columns of the schema and the database parameter vw is a record holding parameters for each table the purpose of is to predict the database parameter and latent database from the input and observed databases distributions induced by a model in later sections we define the semantics of a tabular schema as a model p in general a model p defines several probability distributions · prior pw h is w · full sampling py z h w x is y z w x · sampling distribution py h w x is py z h w · predictive distribution py x h is py h w training data for a model consists of a pair d dx dy where dy is the observed output given input dx in our case dx is the input database and dy is the observed database on such data d dx dy we obtain posterior distributions · posterior pw d h dx h · posterior latent pz d h pw d h the term dx h is known as the evidence for the model used later in our comparison of different models on the same given d dx dy the semantics of is to compute the posterior pw d h on the database parameter and the posterior latent distribution pz d h on the latent database tabular by example tabular and the generative process for tables a schema s is an ordered list of tables named t tn each of which has a table descriptor t that is itself an ordered list of typed columns named c cn the key concept of tabular is to place an annotation a on each column so as to define a probabilistic model for the relational schema we present first a core version of tabular where the model expressions m on columns are simply fun expressions e tabular schemas tables and annotations s t a s t ts t c a t t a input m e to be completed database schema table descriptor annotation parameter input output latent model expression the types t on concrete columns are typically scalars but our semantics allows these types to be arbitrary the tabular syntax for types and expressions slightly extends fun syntax with features to find the sizes of tables and to dereference keys additional types and expressions of tabular fun t e t · · · type e · · · e expression the expression returns the number of rows in table t the expression e returns the item in column c of the row in table t by the integer e in the common case when e is a column ck annotated with type we write as a shorthand for ck values of type are integers as keys to the table t for simplicity our type system treats each type as a for int generative process for tables a table descriptor t is a function from the concrete table holding the input and output columns to the predictive table which additionally holds the latent columns the descriptor defines a generative process to produce the and parameters of the table and the output and latent columns of the table by a loop over the rows of the table in step outside the loop over the data we process the annotations in turn to define the and parameters ignoring the input output and latent annotations · c defines c as the deterministic expression e · c samples c from probabilistic expression e in step a loop over each row of the concrete table we process the annotations in turn to sample independently each row of the predictive table with items for each of the input output and latent columns · c input copies c from the input row · c samples c from probabilistic expression e · c samples c from probabilistic expression e in step inside the data loop we ignore the and parameter annotations although expressions may depend on the variables defined in step outside the loop a schema s describes a generative process to produce the and parameters of each table and the predictive table for each concrete table tables and columns are in sequence although the variables bound in step cannot refer to variables bound later in step later on we formalize the generative processes for tables and schemas using our model notation step corresponds to the and prior parts while step corresponds to the gen part example this standard model is used to generate random bits with a probability distribution that is itself random it is a key of models alpha beta int int real bool param output in step of the generative process we define both alpha and beta as and sample from the distribution beta the uniform distribution on the unit interval in step we generate each row of the table by sampling the variable from the distribution on bool which returns true with probability overall we sample the shared parameter whereas we sample each output independently for each row a concrete database for this schema is simply one table with a single column containing booleans inference computes the distribution of the parameter distributions with in bayesian theory the beta distribution over the parameter of the distribution is a particular case of a prior it is convenient for efficient inference to choose a prior that is to a sampling distribution hence we define primitive models for various standard sampling distributions and library of primitive models p p eh h w primitive model h h w µ h h h w w n h h w these models are defined as primitives built from closed fun expressions the model is exactly equivalent to our previous example the of a determines whether the probability vector of length n drawn from the symmetric uniformly distributed towards sparse vectors or vectors notice that gaussian is a distribution d that can occur within an expression e while is a primitive model that may occur as a model expression m in the full syntax of tabular tabular we add primitive and indexed model expressions to enable the expression of complex models the syntax of model expressions m m model expression e simple pc e cn en primitive with indexed the semantics of a model expression m for a column c is a model p whose output explains how to generate the entry for c in each row of a table the model p has a restricted form p h w with no and where h ey and x hence in our notations below we omit the bound variables h and x a simple model e produces its output by running e model for simple model expression e prior the empty record the empty record y where y e a primitive model pc ec cc acts like the library model p except that when c fc cc and c c c is set to ec if c c and otherwise to the default fc model for pc ec cc prior the empty record ec cc c fc ec cc c fc w an indexed model creates its parameter to be an array of instances of the parameter of m and produces its output like m but using the parameter instance indexed by model for where p is the model for m prior the empty record w where wi for i y where i generative process for tables in full tabular in the full language the model expression for a column c has both a parameter and an output we use the variable c for the parameter and the variable c for the output in step the generative process we process the annotations in turn to define the and parameters · c defines c as the deterministic expression e · c samples c from and samples c from where p models m · c input is ignored · c samples c from where p models m · c samples c from where p models m in step a loop over each row of the input table we process the annotations in turn to define each row of the predictive table · c is ignored · c is ignored · c input copies c from the input row · c samples c from where p models m · c samples c from where p models m the generative process for the core language is a special case where the variables are empty records as before the variables defined in step are static variables defined once per table whereas the variables defined in step are defined for each row of the table the variables help define the semantics of tabular but are not directly available to tabular programs examples of models and queries a model is a probabilistic choice between two or more other models we begin with several of model of two our first model makes use of the library models and z bool latent g real latent g real latent y real output if z then g else g in step of the generative process we sample parameters z containing the from the prior of and parameters g g each containing a mean µ and precision from the prior of the empty lists in and indicate that we use the default built into the models that is and µ in step we generate each row of the table by sampling z from the distribution g and g from the distributions g and g and finally defining the output y to be g or g depending on z given a concrete database for this schema a column y of random numbers that is expected to be into two clusters around the means of the two inference the posterior distributions of the parameters z g and g and also in the latent columns the inferred distribution of each z indicates how likely each y is to have been drawn from each of the clusters of an array of to generalize to a we first decide on a number n of components clusters in this case we set n to randomly select a cluster we use the library model which has an integer n and outputs natural numbers less than n the default value of n is to define a model with n components we the default as a model is to a that outputs or n int z int latent y real output n the indexed model n denotes a model whose parameter is an array of n parameter records containing mean µ and precision fields for the underlying model the output of the indexed model is obtained by first the parameter record at index z and then getting an output from the model with those parameters the parameter of column z is a probability vector of length n an array of nonnegative real numbers that sum to indicating the of each output value the parameter for the y column is an array of n parameter records for the underlying model the observed output of each row is determined by first sampling the cluster z from the discrete distribution and then sampling from n with n we recover our previous of two schema our final model is a tabular version of the factor graph in figure of and where it was automatically generated from a relational schema user z name age z title year u m score int string bool int int string int int int latent input output output latent input output output input input output the model for the score column illustrates a of notations regarding indexed models first a model me f e f is short for me fe f second we write me as short for me n when we know that e is output by each row in the user table belongs to one of four clusters indexed by the latent variable z which has a model for each cluster there is a corresponding distribution over and age similarly each row in the table is modelled by a indexed by z with and year attributes finally each row in the table has links to a user u and to a m and also a score attribute that is modelled by a discrete distribution indexed by the clusters of the user and the corresponding to a stochastic block model and and we illustrate direct use of with reference to and also a programming style where we introduce new query tables purely for the purpose of queries first as illustrated in section given tables of players and matches inference computes distributions for the latent column these can be used to do or to display in it also infers distributions for the and columns which may indicate whether a player was on form or not on the of a particular match second suppose we wish to on the outcomes of matches between members p and q of the players table we add a fresh query table which has the same schema as matches except that is latent instead of being an observed output we place one row in this new table with p for player and q for player and inference computes distributions for the three latent columns including a for indicating the of a by placing multiple rows in the table we can predict the outcomes of multiple matches player player real real bool input input latent latent latent third consider an online situation where there is a large table of players and a relatively small number of players qi to begin fresh online games we may wish to select one of the qi to play against a new player p to do so we add the sim query table below and fill it with rows p qi for each i the latent column similar holds true if the two players are close in less than units apart inference this column with distributions which can be used to select a close in to p both the means and of the of players enter into the probability of being similar thus making use of the full probabilistic formulation sim player player similar bool input input latent formal semantics of tabular semantics of fun review we here recall the semantics of fun without observations et al we write e t to mean that in type environment x t xn tn xi distinct expression e has type t let mean that e contains no occurrence of d the typing rules for fun are standard for a firstorder functional language some examples follow below selected typing rules of fun expressions e t fun random d x t · · · xn tn u ei ti for i n de en u fun ei t for i n e en t fun iter x int f t e int for x e f t fun index e t f int ef t the interpretation of a type t is the set vt of closed values of type t real numbers integers records and so on using the standard topology a function f t u is measurable if f a vt is measurable for all measurable a vu all continuous functions are measurable a finite measure µ over t is a function from subsets of vt to the nonnegative real numbers that is additive that is if a a are pairwise disjoint the finite measure µ is called a probability measure if if µ is a probability measure on t and f t u is measurable we let f µ f a in this context f is called a random variable the semantics of a closed fun expression e is a probability measure pe over its return type it is defined via a semantics of open fun expressions ramsey and in the ity monad we write pe for the probability measure corresponding to a closed expression e if e t then pe is a probability measure on vt if e t · · · tn and for i m we have vi ui and fi and x t xn tn fi ui we write pe x xn f v · · · fm vm for a version of the conditional probability distribution of pe given f v vm where f x xn f fm semantics of models a model is associated with four types a type h a parameter type w an input type x and an output type y model types and typing of models q p q q xy type of model model eh h h h ew w eh h w xy h wx x ey y in a model y is a pair type where the second component holds the latent variables of the model given a model the standard distributions are obtained as follows proposition given a model p eh h w such that p xy z the following fun expressions denote the standard distributions · prior let h eh in ew · full sampling where h vh w vw x vx let h vh in let w vw in let x vx in · sampling where h vh w vw x vx let h vh in let w vw in let x vx in fst · joint posterior where x vx y pe w yz fst yz where e let h eh in let w ew in let x vx in w · posterior where p is the joint posterior and · posterior latent snd where p is the joint posterior typing and translation of tabular when typing schemas we use binding times to track the availability of variables let b be the set h w xyz of binding times ordered such that h w xyz here h stands for the deterministic phase w stands for the nondeterministic parameter phase and xyz stands for the generative phase of the computation we use metavariables and pc to range over b informally variables declared at one time may only be used in expressions typed at or above that time the current time pc is maintained as an additional index of the tabular typing judgments binding times are also used to prevent the mention of nondeterministic parameters in expressions used as necessarily deterministic and generative data in the construction of either or parameters when translating to fun binding times ensure that the target program is and deterministic where needed tabular levels and typing environments pc h w xyz x t t rt binding time environment empty variable typing predictive row type for t environments declare variables with their binding time and type and tables with their predictive row types judgments of the tabular type system t pc e t pc m w t tq sq environment is wellformed in type t is wellformed in at binding time pc expr e has type t in at pc model m has w returns t in table t has type q in schema s has type q formation rules for environments env empty env var env table t x dom rt t dom x t t rt formation rules for types t type scalar type array t s t type record c c c tc the translation of a tabular schema to a model is performed by four judgments though defined the relations are partial functions on untyped terms and total functions on welltyped tabular terms judgments of the translation e f m ew we tp sp tabular expression e translates to fun expr f model m translates to ew we marked up table t translates to model p marked up schema s translates to p lemma if s p and s p then p p theorem translation preserves typing if s q then there exists p such that s p and p q expressions the main when translating schemas is to support keys we use the notation e within fun expressions to stand for the column c of the row in table t indexed by key e in particular when constructing the model for a table t j we may dereference a key of type to a previous table ti with i j for instance in the schema there is a reference from t matches to t players to translate such keys we that for each table ti there is a global variable named ti that holds the predictive table for ti that is the join of the input xi the output yi and the latent zi for each i hence an expression e means for example player to typing rules for tabular expressions pc e t tabular var x t pc x t pc tabular pc t int t dom pc int tabular deref pc e int xyz pc pc e tc t d td dc cc rule tabular var allows a reference to x only if x is declared with a binding time l pc where pc is the current binding time translation rules for tabular expressions e f trans var trans trans deref e f xx t e the remaining rules are simple translations model expressions typing rules for model expressions pc m w t model simple pc e t pc e t model p r h w p c y c c c h ec hc pc pc wy model indexed pc m w t pc int pc w t h int primitive models must have void input we allow to only replace a part c of their c the upper bound of an indexed model has binding time h since it must be deterministic and the same for all rows of the table translation rules for model expressions m p trans simple w e f e wf trans w p c h w ec ec c c ec if c c then ec else fc eh c pc let h eh in ew h eh in let x in ey trans indexed w m ew for ew w in ey a simple model has no prior the prior of an indexed model is an array of independent samples of the prior of the underlying model in the output we use the prior value at index tables the typing and translation rules for tables are defined inductively and determine the semantics for the shared shared parameter and a pair of output and latent columns for a single row of the table typing rules for tables t q table empty table a h e h c h h t xy z c a ht c h xy z table param a w m ww c w w t h rw xy z c dom c a w t h c w c w rw xy z table input a input c xyz x t z c a xt c x z table output a xyz m wy c xyz y t h rw xy z c a y t h c w rw x c y ry z table latent a xyz m w z c xyz z t h rw xy rz c a zt h c w rw xy c z rz rule table ensures that e is deterministic and closed and declares c at binding time h so it can be referenced at all binding times rule table param ensures that m is checked at level w not pc so that its generative expression has no data dependencies and is safe to use at the parameter level rule table input extends the context with c declared at xyz rule table output extends the context with c declared at xyz and records the types of parameter c and output c by extending the parameter and output record types of the table rule table latent is symmetric to table output but instead extends the latent record type the translation rules for tables make use of auxiliary ranged over by l these denote a spine of fun ending in a hole and are defined inductively as follows core fun let contexts l l let x e in l let context hole let binding the operation l e the hole of a l with a body e producing a fun expression e e let x e in l e let x e in l e translation rules for tables t p trans empty h h w x trans c h w x e eh t rh h w xe c e c eh rh c hc in ew h w c hc in e trans param h ec c c c h w x m ew t eh h w xe c param m eh c ew in let c ec in c c c rw h w c wc in e trans input t eh h w xe c h w x c input eh h w c xc in e trans output h h w x c m ew t eh h w ez c output m eh c ew in c rw h w c let wc wc in ec in loc c ry ez trans latent h h w x c m ew t eh h w rz c latent m eh c ew in c rw h w c let wc wc in ec in c c rz rule trans merely extends the record of the remaining table and c as the projection ht in the prior and gen of the model rule trans param extends table ts prior with two fields for the prior and gen of m and parameter c as the projection wc in the gen of the row rule trans input just binds c as the projection xc of input row x in the gen of the table but does not export c since it is neither output nor latent rule trans output just defines c as the gen of its model whose parameter wc is obtained from wc c is exported in the output record of the row rule trans latent is symmetric to trans output but instead extends the latent record for example here is a schema for linear real real a real param b real param x real input z real latent ax b y real output the row semantics of this table is as follows for readability we inline some variable definitions since this table only uses simple model expressions the fields for the parameters of model expressions all contain the empty record modulo these redundant fields we recover the model from section model for a row of the table w x a a a b z y let z wa xx in let y in schemas typing rules for schemas s q schema empty schema table t t h s rh rw rx ry rz h t int rh w t w rw x t rx y t ry z t rz t ts h w x y z rule schema table uses the model type of the table to extend the context with a declaration of the tables size t at level h t is used in the translation of as well as the predictive row type of t this is the union of its input output and latent fields the tables default of type h are applied in the translation of t and do not appear in the type of the schema the rule extends the components of the schemas model type with additional fields for the table size the parameters of the table as a nested record the inputs of the table a nested array of records and the pair of output and latent table records extended with fields for the output and latent arrays of records for t translation rules for schemas s p trans empty h h w x trans table t eh ht ew ht wt ry rz rx c c s rh h w sz et let ht eh in let wt wt in for i t let xi in lt rx ry rz ey for i t c ez for i t c h ht eh in t h w x t t t ts t rh t let ht eh in ew in let t ht in lwt t rw h w t ht in let t et in ey ry t ez rz rule trans table takes the model for the parameters and a single row of t and constructs a model that once from the prior of t then ts output distribution across an array of size t the intermediate array et contains the predictive table for t merging the input output and latent of t as single records expressions ey and ez are used to the array of merged records into separate arrays of output and latent the rule extends ss record with a default binding for t with arbitrary value table sizes must be consistently before inference translation examples to illustrate our schema translation and our treatment of keys here is the translation of rewritten a little for readability first the two row models for the two tables followed by the model of the whole schema model for a row of table players p w x let gaussian in model for a row of table matches p w x let in let in let in model for the schema players matches players matches w x let players for i let gaussian in let matches for i let player in let player in let in let in let in player player player player players for i matches for i players for i matches for i a reference learner for we conclude with a learner api a programming interface for the api allows a user to a split into input and observed databases to perform queries we a database and a schema into a learner l d s where d dx dy and dx is the input database and dy is the observed database we assume the types of d and s match as discussed in the next section to pick out the sizes of tables in a database we let t b tn bn t b tn bn we support the following functional api · let ls be the empty learner that is s plus a pair of databases with the right table names but no table rows · let dx dy be l dx dx dy dy s where is concatenation of arrays in records and l dx dy s · let be the posterior distribution pw d h induced by p where l d s p models s and h dx · let be the posterior latent distribution pz d h induced by p where l d s p models s and h dx compared to the reference learner of gordon et al a this new api can learn latent outputs since it works on models our current implementation uses fun to compute approximate forms of the posterior distributions on the database parameter and latent database and them to the relational store the api allows an incremental implementation where the abstract state l is represented by a distribution over the parameters and latent variables computed after each call to our current implementation does not support this optimization maintains the whole d and does inference from when necessary the incremental formulation of our learner is consistent with the algebraic classifier formulation of which reductions in computational complexity for and enable efficient online and parallel training algorithms based on the monoidal or group structure of such now that we have schema typing and a semantics of schemas as models we can perform inference as follows a learner l dx dy s is if s xy z and dx x and dy y and for all tables ti doms we have in particular the empty learner is not since it contains empty tables we can now implement a latent column query theorem if l dx dy s is there is a closed fun expression such that if µ w yz fst yz dy then and snd proof assume that s eh h w and let expression let h dx in let w ew in let x dx in w by proposition µ as above yields the distributions outline of practical implementation our implementation builds on the pattern of gordon et al a in which models are represented as records of f representing typed fun expressions our initial tabular implementation generates such models this target two advantages the fragments are compact yet statically checked for type correctness the resulting terms are easily to produce efficient sampling code for clarity the semantics in section splits compilation into typechecking followed by untyped translation to create we need to fs type checker that our dynamically constructed are composed in a statically safe manner the most direct way to do so is to the separate typing and translation judgments as single elaboration judgments that typechecking with translation the f of this idea is a triple of polymorphic functions that represent the typing contexts as a pair of nested tuples contexts are extended as required by using polymorphic recursion in recursive calls to elaboration the output of elaboration is a value of existential type containing both the target type and the target translation of the source term since type variables have accurate runtime representations in net we can directly compare the types of generated subexpressions as needed avoiding the need to maintain separate type representations participants ability questions answer difficulty answer responses advantage know guess response response real int real real int real bool int int int latent latent latent latent input output input input latent latent latent latent input output gaussian gaussian if know then else guess figure the model in tabular and notation the model is implemented in annotations to the three main tables participants questions and responses tables and provide a mechanism for missing data case study intelligence testing tabular has been designed to make the paradigm of machine learning usable for who are not machine learning we describe a case study of data analysis using tabular based on a from intelligence testing our case study relies on models first published by et al and data provided by the cambridge centre based on testing material by we use a of responses to a standard intelligence test called standard matrices the test consists of questions each a matrix of shapes with one element missing and possible answers exactly one of which is correct the sample consists of who filled for its standardization in the in the factor graph for the full model is shown in figure responses and true answers may or may not be observed figure also the full model in tabular each participant is characterized by a latent ability each question is characterized by a true answer a difficulty and a parameter responses depend on and under the model an advantage variable is calculated as the difference between ability of participant and difficulty of question the boolean variable know which represents whether the participant knows the answer or not is modelled as a over advantage with as the parameter db returns its first argument and is a to the underlying inference algorithm to apply a factor for better convergence guess represents a random guess from a uniform distribution over all possible responses the participants response is taken to be the question answer if know is true and guess otherwise the model relies on two sources of observed data correct answers to the questions and responses provided by a subset of correct answers can be provided through the table a subset of given responses can be provided through the table note that there are simplified versions of the full model in which a only the ability is modelled a model or b the and the questions difficulties are modelled da model the model is run once to answer two types of queries given a subset of the true answers and a subset of given responses i infer the missing correct answers to questions and ii infer the missing responses of figure shows how the tabular implementation differs from the implementation on a sample run where of re and of true answers are the data contained participants questions training questions responses and training responses the inference results of and tabular based implementations are very similar they differ slightly because of differences in the way our compiler translated the tabular formulation into code from the direct implementation by an however the code including the necessary data transformation code is much longer than the and readable tabular code that was added to the existing data schema to describe the same model high compilation times are not due to the tabular to fun translation which takes less than one second for each model but to a in the fun compiler fun all data before compiling a convenient but unnecessary measure avoided by to demonstrate that the compile times can be reduced we a second compiler tabular ii that translates tabular programs directly to on the case study tabular ii improves compile times by two orders of magnitude and inference time by up to one order of magnitude yielding performance that is more with figure inference of latent columns requires that all output columns contain a valid value at each row however many real contain missing values infers the posterior probability of missing values in output columns on observed values actually present in the database in a query each attribute is either known or missing we use to denote missing values database d v v r c v cn vn r r rn d t r tn rn missing or known value row table database let a learner dx dy s be a learner where dx is a normal value and dy is a database such a learner can be as defined in section where we let t for any t and the result of inference on a learner is the joint posterior distribution for all the entries in dy in addition to the latent columns and the parameters of each table for a formal model a a a da da da language tabular tabular ii tabular tabular ii tabular tabular ii loc data loc model loc inference loc total compile seconds infer seconds model log evidence log prob test responses log prob test answers figure comparison of tabular and implementations of different variants of the model for machine configuration precision t cpu e with gb ram windows and net definition we need to compute the observations of dy that is the entries in dy present in the database and their values observations of a query oe · oe true oe v e v oe ci oe oe ti in vi in in ri if dy s is a learner and s eh h w then the prior distribution of l is given by pe where e let h dx in let w ew in let x dx in w and the joint posterior is the conditional probability distribution pe w yz example of is an experimental embedding of probabilistic inference in a http given a probabilistic model for the whole can fill in the missing values of empty cells and also detect cells whose values are far from what is by the model an can be considered as a learner where each column is an output but may have missing values and there is an additional latent column for each row the tabular schema below corresponds to the generalized gaussian model produced by on a table we here consider only columns other data types such as booleans and integers can also be encoded as vectors of real numbers with appropriate invertible link functions gg v vector latent x real output v x real output v x real output v the library model is to but outputs vectors from a multivariate gaussian distribution with gaussian and the query is a table gg containing the data with empty cells replaced by such as the following gg id x x x here yx yx · · · yx yx translating to queries can be by translating them to a latent column query and performing inference on the latter the key idea is to create a new table for each output column of the original table that contains just the known values in that column in the translation of the table each output column is simply turned into a latent column for example the gg model translates to the following tables gg v vector x real x real x real latent latent latent latent v v v x r input v real output rx x r input v real output rx x r input v real output rx above the query tables x x and x each contain a value column v and a reference column r which denotes the row from which the value since the gg table contains no input columns all the data is in the query tables x id r v x r v x r v formal translation we fix a learner dx dy s where s t j tj jm and each table tj c ji a ji let oj i j a ji o we let and a a otherwise we then translate the schema s as follows r input int v jc ji if i oj tj c ji a ji s t j tj t ji jm to translate the database we first translate the observations in dy r k ji v ji the translations of the original tables have no observed values j finally we can combine these tables into a new database dx dy dx t j ji jm dy t j t ji jm lemma if dx dy s is a learner then dx dy s as defined above is a learner to answer the query using the results of inference for the translated learner we need to go from an inferred distribution for the translated schema s to a distribution for the original schema s this is done by the function i defined below iw z t j wt j jm t j c ji zt jm t j c ji zt jm we can now show that the translation is correct it reduces to theorem let l dx dy s be a learner let l dx dy s as defined above and let µ be the semantics of the latent column query on l as given in theorem then is a version of the joint posterior conditional distribution pe w yz of l proof sketch the compilation merely adds deterministic data and copies of random variables which are then ignored by i as an optimization an implementation might only translate observed columns where some data is missing in the current database into new tables in the example above there are no missing values in column x in the database so it can remain observed in gg and no new table needs to be created for its contents recall the schema of section given existing tables of users and suppose we wish to to user i that they are likely to rate with five to do so we first modify the annotation on the column of the table adding a uniform prior distribution u m score int input output output we then add a single row u i m score to the existing data in the table denoting that user i has an unknown with this query is then translated to a corresponding latent column query in the manner defined above inference returns a discrete distribution over ids for the missing value finally high probability ids can be selected for to the user in a variation of this query we can weight the results by how many people have seen that is each to this end we add between rows a shared frequency prior by instead using the model for the column and then proceed as above related work probabilistic programming languages there is by now a number of probabilistic programming languages that differ in their target expressive power performance and bugs bayesian inference using sampling et al is a simple language for specifying probabilistic models that allows for inference using sampling it is widely used in the bayesian community but so far does not scale to large microsoft et al achieves better through support of deterministic approximate inference algorithms such as expectation propagation and message passing church et al is a relatively new probabilistic programming language based on lisp which allows for recursion and enables bayesian models through memoization furthermore there are languages like and which incorporate concepts as well et al is an imperative framework for constructing graphical models in the form of factor graphs used mostly for information extraction all these languages follow the traditional paradigm of separating the code from the data schema and hence make it necessary to the data schema within the language and to import the data from a database on the other hand tabular is focused on learning from relational data and does not directly address some of the application areas of probabilistic programming such as as inverse et al et al or decision making for security et al probabilistic databases probabilistic databases represent a line of research in which the database community is concerned with the question of how to handle knowledge in relational databases see for example et al typically the assumption is made that each tuple is only in the database with a given probability and that the presence of different tuples are independent events the resulting probabilistic database can be interpreted in terms of the possible worlds semantics it is further assumed that the probability values associated with each tuple are provided by the data collector for example from knowledge about errors or from probabilistic models outside the probabilistic database the main technical difficulty is to evaluate queries against probabilistic databases because despite the independence assumption on the presence of tuples complex queries involving logical and operators can lead to difficult inference problems this is also the main difference to the tabular approach whereas probabilistic databases work with concrete probabilities tabular works with database schemas containing simple tuples possibly with missing values and allows building probabilistic models based on that data in contrast to probabilistic database systems tabular is thus compatible with the of existing relational statistical relational learning statistical relational learning operates in domains that exhibit both uncertainty and relational structure see and for an overview several contributions focus on combining probability and firstorder logic such as bayesian logic et al which allows reasoning about unknown objects or wang et al which the world of probabilistic databases and statistical relational learning tabular is more closely related to work that makes direct use of data in a relational database schema such as et al et al and and tabular is based on directed graphical models distinguishing it from markov logic and tabular was also inspired by a concept called van which the sql query language with statements that construct a factor graph aligned with a given database schema in summary tabular can be viewed as a language that enables the construction of statistical relational models directly from a schema but goes beyond prior work in this field in that it allows the introduction of latent variables and models continuous as well as discrete variables tabular was directly inspired by the question of finding a textual notation for the factor graphs generated by and which constructs a hierarchical graphical model in et al from an arbitrary relational schema et al is a related model which handles single tables with mixed types real integer bool with tabular these types of model can be implemented in a few lines of code and we the automatic synthesis of a tabular program that best models a given relational similar to the work of et al on matrix conclusions we propose probabilistic programming as a new principle of programming language design the idea is to design a probabilistic modelling language by starting with a database schema and it with notations for describing random variables their probability distributions and how they relate to data matching the schema and what is to be inferred acknowledgments about this work with john and john were made many contributions to the fun system on which this work depends and on a draft we would like to thank john and from the cambridge centre as well as for providing the iq for research purposes references y t t and j how to a test without knowing the answers a bayesian graphical model for adaptive and testing in proc s j a d gordon and c v deriving probability density functions from probabilistic functional programs in proc tacas volume of lncs pages ­ springer c m machine learning transactions of the society a mathematical physical and engineering sciences j a d gordon m j and j van measure transformer semantics for bayesian machine learning in proc esop volume of lncs pages ­ springer available at n n c and d probabilistic databases in the acm ­ p and m markov logic a unifying framework for statistical relational learning in proc pages ­ l and b editors introduction to statistical relational learning the mit press l n friedman d a and b probabilistic relational models in and w r a thomas and d j a language and program for complex bayesian modelling the ­ m a categorical approach to probability theory in b editor categorical aspects of topology and analysis volume of lecture notes in mathematics pages ­ springer n v k d m k and j b church a language for generative models in proc pages ­ press a d gordon m j g t a s rajamani and c a pattern for bayesian reasoning in proc popl pages ­ acm press a a d gordon t n c j and j tabular a probabilistic programming language technical report microsoft research b r r w t and j b exploiting compositionality to explore a large space of model structures in proc pages ­ press p analytic database for a new kind of user the data in proc pages ­ acm d c and d probabilistic models and models in and r t and t a bayesian system in proc pages ­ mit press m algebraic a generic approach to fast online training and parallel training in proc o and c embedded probabilistic programming in proc volume of lncs pages ­ springer d and n friedman probabilistic graphical models the mit press v k t d y n and j b approximate bayesian image interpretation using generative probabilistic programs to appear in proc available at http p s m and m dynamic enforcement of security policies in proc pages ­ ieee computer a k and s probabilistic programming via defined factor graphs in proc pages ­ associates b b s j russell d d l and a probabilistic models with unknown objects in proc probabilistic logical and relational learning a further synthesis t and j m in proc pages ­ mit press t j j and d microsoft research cambridge j and d relational dependency networks journal of machine learning research ­ k and t a b and prediction for stochastic j assoc ­ a the design and implementation of a generalpurpose probabilistic language in and a an objectoriented probabilistic programming language technical report n ramsey and a stochastic lambda calculus and monads of probability distributions in proc popl pages ­ acm p c v m gordon and j b learning systems of categories in proc pages ­ science society s and t compiling relational database into probabilistic graphical models abs j van query language post available at may d z wang e m and j m large data with probabilistic graphical models proc ­ aug d n d a and j m nonstandard interpretations of probabilistic programs for efficient inference in proc pages ­ 