automatic design of data processing systems project mac massachusetts institute of technology cambridge mass abstract an automated data processing system designer is described attention is on the io such systems and the development of optimizing design heuristics the design of data organization and data accessing · procedures for data processing on large files of data is a common and recurrent activity in modern data processing applications a considerable amount of understanding and in the this area has been developed and it is time to begin and this process it should be possible to develop a system where the user has merely to specify the characteristics of his input output and intermediate data objects and their and the system will automatically determine the data and accessing procedures that are optimal for his application the optimizer for i an automatic programming system prototype at project mac provides an example of how such automation can be accomplished this paper describes the theory and algorithms behind the optimizer that is currently operational at project mac the first reported efforts in the area of formalizing data processing system design are those of he a data processing a collection of computations that operate on data files in this model a file is a set of records each with a single key and one or more data items each computation makes all of the records of its input files to produce records of its output files a volume is associated with each pair where the computation reads or writes that file this is defined to be the volume of the file in bytes by the number of passes made by the computation over that file the design objective is the minimization of total system volume volume reduction is accomplished by merging computations so that a file can be read once for all rather than separately for each and by merging files thus eliminating key redundancy and making the file smaller than the sum of the sizes of its component files et al have analyzed this formulation and developed an algorithm for that generates all feasible by implicit enumeration and corresponding system configurations to find the one that volume because the computations generally involve very simple the of the processing cost for such systems is usually due to io thus an optimization strategy that focuses mainly on io reduction is appropriate but implicit in the model are two simplifying assumptions that weaken it as a design optimization tool for practical systems i that costs depend directly on total volume as defined above and that all files have a single key also ignored is the cost due to core but in by storage io activity this is a secondorder effect in most operating user is primarily by the io event the transfer of a single block to or from primary storage rather than by total volume furthermore the number of io events necessary when a computation processes a file depends on not just the total volume of the file but also i the number of records in a block the number of records used by the computation and the access method eg sequential random the limitation of one key per record is completely artificial and at variance with practical files frequently have more than one key in their records cf this means that they can be sorted in more than one way by the same token a computation that a file can process its records in more than one order by their keys this the possibility of conflict either between two computations that process the records of a file in two different key orders or between two files with different sort orderings that are processed by the same computation the manner in which such conflicts are resolved eg a file using random instead of sequential affect the total io cost of the system the of sort order conflicts also the evaluation of io due to file and computation merging the i optimizer takes all of these design considerations into account the i model i is an automatic programming system for generating oriented miss such systems involve a sequence of runs or job steps that are to be performed at specified times they are assumed to involve significant io activity due to processing of records from large of data as control and or record keeping of this type central to the design of such systems is the notions ol the data set a data set is a collection of similar data that are to be processed in a similar way an example is the set of all levels in a in the domain of i a data set is assumed to consist of fixed format records eg one for the level of each item associated with each record are date and keys the key values of a record uniquely distinguish it eg the data set can be by item since there is only one level record per item and so can be used to select it thus a data set is essentially the same as a relation and its keys are what calls primary keys the application of an operation to the members of a data set or sets is a the order of application of the operation by a computation is assumed to be to the user in fact he may think of them as being performed in parallel however every computation does in fact inputs according to a particular ordering chosen by l on their keys computations typically match data items from different data sets by their keys and operate on the matching items to produce a corresponding output data item for example an order computation matches the total orders for a given item against the level of that item and determines the amount of that item to be computations may also group the members of a data set by common keys and operate on each group to produce a single corresponding output following our example suppose that item orders can come from several sources so that both the item and the source of an order are needed as keys to distinguish it to form the total of all orders for each item a system must group the orders by item and sum over the order amounts in each group in this context then data is the process of constructing computations and data sets and determining the best and for them in the domain of t only three types of file and three types of accessing methods are considered the are consecutive organized solely on the basis of their successive physical location in the file for i records in a sequentially organized data set are required to be completely ordered index sequential records are organized in such a way that by reference to indices associated with the data set it is possible to quickly any record and read or update them additions or do not require a rewrite of the file each data set sort ordering determined by he optimizer associated with it and usually the records are nearly ordered in this way records are stored in a random manner but there is a mapping function from the a record to its physical address the are sequential for and sequential data sets records can be read and the physical order in which they appear in the data file for an index sequential data set the operating systems sequential access software records in the sort order the data set random records are referenced read updated added or deleted through the values of keys supplied by the computation this method is applicable only for accessing data sets or for index sequential core table an input file is initially read into core in its an output file after being core is sequentially copied onto the device where it will when finished with records are accessed in core sequentially if their sort order with the processing order of the computation otherwise by binary search directly organized data hashing the optimization criterion optimization as viewed by this part of i is simply cost minimization further because the miss are assumed to be i i o this is with access minimization an access is defined as the reading or writing of a single storage block which corresponds to a single operating system io event in for a particular data set a block consists of a fixed number of records example a simple example of an in the domain of i is the store chain and system shown in fig i boxes indicate computations and arrows represent data flows this system contains computations that update the levels file to reflect received from find the total amount of each item ordered by all stores f i l l the orders determine the total reductions in the levels file accordingly check for the of orders and make orders to the proper this example illustrates the important factors in data processing design consider the for each item that is ordered by a is determined it could be implemented by considering each possible combination of values for the keys item determining whether the conditions are suitable for producing a record viz that the item in question has been ordered by the store under consideration and if so applying the associated operation to generate such a record this requires a test for every possible combination and each test involves a to one or more of the input data sets a given not all of the stores will order and most that do will order only a fraction of the possible items such an i o w m oi l m m c c o m m l o g d t o n u e would involve an large number of in contrast since it can be determined by analyzing the fill that the operation will be applicable only when there is a record in computation could be designed to consider only those store and item combinations for which there are records in in this case the computation is said to be by said to be its data set note that not every input to a computation is a suitable driver for the update only used it might not contain a record for each item causing be incomplete another factor in optimization is illustrated by the update if the records of organized and sorted in the same order as those of the driver can be used in the order in which it is stored this means that blocking can be used to minimize accesses if there are b records per block and n records in received the number of will be no greater than nb · however if the two inputs are not sorted in the the computation will have to search usually requiring more than one access for each record of it is needed alternatively be given organization and accessed randomly but the blocking advantage would be lost these considerations of data sets consecutive data set organization compatible sort orderings and blocking will figure importantly in the design process the optimizer in i the optimizing designer of data set organization and data the is given a relational description of the basic data and the relations among them fig is an example of such a description for the his of fig its job is to i design the particular their a contents information contained b organization direct index sequential c storage device d orderings by key values design each job step of the a which computations it includes b its c its data sets d the order by key values in which it processes the records of its input data sets determine whether sorts are where they should be performed determine the the job steps all design decisions are made in an effort to minimize the total number of accesses that must be performed in the execution of the his in preparation for this design process an analysis of the his description is performed in order to determine properties relevant to making good decisions among these properties are the candidates for data sets for each computation and the average and maximum sizes of each data set the latter may require some of the user during the design user may be to give further information that can be used to determine the sizes of newly designed data sets access minimization there are three the i uses in designing miss so as to minimize accessing i making use of data set blocking data sets and computations blocking since an defined as a reading or writing of a block of records be reduced if blocking factors greater than one are used and if processing and data set are in such a way that the records of a block can be used at the where core table not possible ie when the whole file will not fit in available core this means that a data set must and therefore have consecutive or index and it must be sorted in an order that is the same as processing orders of the computations accessing it when core table access is possible for a data set the number of just the number of blocks in the data set regardless of whether the data sets sort order matches the processing order of accessing computations each block get it into or out of core because blocking is the most effective way of reducing accesses the optimizer tries to make it possible for the sort orders of data are too big for core table match the processing orders of their in as many cases as possible however conflicts among sort order can and will arise sometimes it is to introduce computations so that a data set can have two or more differently sorted versions data computations the relational description of the his identifies the data entities and operations the straightforward implementation of such a description is to make one data set for each data entity and one computation for each operation this can be considered as the to be manipulated by the designer the designer aggregate data sets and computations in order to reduce accesses the of data sets data set in which there is one record for each set of records in the original data sets that match by keys for example the aggregate of the data sets in a and b is shown in fig c nil ni in n ni ini n i a dept b c figure data set where a record is represented by a tuple whose last element is the key value and whose first the data items data set is when two or more data sets are read or written by the same computation accesses can be saved if the shared data sets are and processing is so single record of the aggregate can be accessed where more than one record from each of the data sets would have to be accessed without two other ways of reducing the number of illustrated in a and b represent data sets boxes represent computations it may be to combine computations that read the same data set so that they can all access a record at the same time if their for the shared data set agree each record to be be read once for all rather than once for each computation this is called fig a refers to combining two computations when the output of one is used as the input to the other and their processing orders agree fig b in that case the second computation does not have to read the output records of the can simply be the one computation to the other as needed the of computations is more than a simple merging looping redundancy is eliminated so that the composite result is more efficient than its general design of the optimization the access techniques given above require that the key order of processing agree in a special way with the organization of the data being processed this is where the fundamental difficulty in optimization lies a data sets organization and the accessing method of a computation using it cannot be determined independently of each other or of other data set and computation the organization of a data set limits the ways in which it can be practically accessed by a computation ind conversely the accessing method of a computation restricts the of a data set that it accesses furthermore a data set is typically more than one computation with possibly conflicting for its organization and a computation accesses more than one data set with conflicting for accessing methods finally data set organization constraints tend to propagate through computations because it is most efficient for a computation to write its outputs in the same key order in which it reads its inputs since that is the order in which the output records will be generated so optimization of the type we are considering is a problem in global the straightforward solution of evaluating the cost of every possible combination of sort order device organization and for data computations in every possible configuration to determine the least expensive is out by the involved even with mathematical and special purpose it would be slow to make optimization tractable a heuristic approach g figure a of computations © figure b vertical of computations must be taken first different kinds of of data sets which objects to aggregate must be possible further must be introduced where it is not strictly possible for the sake of additional simplicity such forced does not mean though that decisions that are in fact coupled are treated as if they were independent the decisions are still made with a certain of their effects on other decisions finally as a first order approximation the optimizer does what is reasonable locally and then somewhat for global the optimization algorithm o u r optimization algorithm consists of the following steps development of maximal potential for reducing accesses through blocking for the initial configuration computations where in the current context a g g r e g a t i n g data sets where in the current context iteration over steps and until no further is suggested determination of data sets determination of device and organization for each data set and of access method for each computation determination of optimal blocking factors step i up the initial configuration to take advantage of blocking the determination of mutually sort orders for computations and data sets that will allow maximal advantage to be taken of blocking which requires matching sort orders and sequential accessing where core table access is not possible must be considered first in the optimization process and other optimization techniques are of little value if they force sorting or other methods of accessing that can require orders of magnitude more io events than sequential accessing as explained above it is necessary for the sort order of a data set that is not core table accessible to be made the same as the processing order of a computation that in order to reduce that computation by blocking but there are user constraints on data set sort orders and inherent of computations for constraints on their sort orders that must be taken into account too as a result of these each computation and data set will have a sort order constraint restricting the possible sort orders it may be given a data set l key may have the that its records be sorted on first then the optimizer could decide to sort them first by key i and then by key under key or vice versa the factors that affect are i user constraints the user may specify that inputs or outputs have a particular associated uniqueness a particular computation presents the same to all of the data sets that it reads or writes and a particular data set presents the same to all of the computations that read or write it parallelism a computation outputs records in the same key order in which its inputs are read computation the optimizer that the input data set to a computation be sorted first by the keys that distinguish the groups it operates on that way it can be designed to process one group at a time otherwise groups must be processed in parallel requiring costly accesses if the core available for is sequential access preference to take advantage of blocking t w o for which there is a common possible sort order are called if a data set and a computation accessing it have consistent both can be assigned the same one that to each of their original thus potential for sequential blocking in general it will not be possible to satisfy all and constraints simultaneously conflicts arise preventing the sequential accessing of every data set at best therefore the optimizer can only try to find the what it aims for is the of the total volume of data accessed sequentially even here it must to make to that is it tries to come as close as possible to this maximum without an amount of effort its method is to follow the implications of the initial constraints and throughout the network of computations until conflicts arise and then try to resolve those conflicts as as possible it tries to associate with each computation and data set a restrictive that is consistent with its own preference if any and with all of the constraints on and for the of all of the immediately adjacent objects in the net for data sets this is the set of all for computations this is the set of all inputs and outputs as this is possible it has what it wants otherwise it has discovered a conflict of interest which it attempts to resolve in such a way that the greatest volume possible of records involved can be we have found that this simple assignment algorithm produces good if not optimal results in the systems we have tested this occurs because typically i there are very few conflicts and those that do occur are generally simple and local when an assignment of restrictive has been determined for all of the objects in the system heuristic of computations and data sets can be performed it is not to aggregate computations and data sets if this will increase the io cost by sequential accessing that may have been possible before so the of two data sets or computations will be permitted only if their are consistent on the other hand restrictive constraints would prevent that would otherwise be possible thus the on finding restrictive step computation computations are considered for if they are have consistent and access a common data set this data set may be either a common input or a common intermediate data is the output of one and and input to the others vertical in order to preserve existing sequential accessing the of the aggregate must be consistent with each of the of the computations further to avoid otherwise possible the assigned to the aggregate is the restrictive mutual restriction of the of the components nevertheless it will not be for the of the aggregate to be more constrained than the of the computations that into it this means that the decision to make a particular may prevent other by of inconsistency that were possible for example computations a b and c may be pairwise but the of all three may be impossible that is if say a and b are the of the result may be inconsistent with that of c even though the of a is consistent with that of c and the of b is consistent with that of c finding an exact by considering all possible combinations of all possible is again by the heuristic approximation is made that what is optimal locally is good for the system and again we have observed and conjecture that the simplicity of typical systems is such that this is a approach the optimizer considers candidates two at a time as the order of treatment is significant the policy is locally to consider in order of the number of events they are expected to save furthermore classes of are performed in the following order vertical of computations reading common system inputs s other computation vertical are considered to have a higher priority than the variety because the former may result in the entire elimination of data sets that is the data sets involved will neither have to be read nor written that eliminate reads of system inputs are over other because such inputs often on such storage as tape and which are relatively and costly to access repeatedly step data set data sets are considered for if they have consistent are not system inputs or outputs and are both outputs of a common computation and inputs to another common computation as with computations care must be taken so that the of the aggregate is consistent with each of the of the data sets in order to preserve whatever sequential accessing may exist as the introduction of arbitrary constraints can prevent otherwise possible here too the assigned to the aggregate is the restrictive mutual restriction of the of the components but again the of the aggregate may be more constrained than the of the data sets that into it so a particular may prevent others and the order in which they are performed is important the optimizer considers those computations with more than one output in order of the total volume in records that they process for each computation its largest output data set is considered for with as many of the others in order of their size as possible the criteria for being that be consistent and that the total cost of reading the aggregate by all computations for which it is an input is lower than the total cost without step iteration computation and data set are repeated until no further is possible this iteration is necessary because the of data sets may make further pairwise computation feasible and similarly the of computations may make additional data set feasible step data set determination choosing data sets for each computation is straightforward about of the computations in the initial before system configuration are found to have a only a single candidate further decreases the number of cases where there are multiple candidates for each of the few remaining computations for which a candidate must be chosen of the total average accesses necessary for each are made possibly requiring the user to supply information about data set sizes and the accessing candidate is chosen step devices organization a and determination some assignments of device and organization for data sets will already have been made by the user typically he has specified the devices and for the system inputs and outputs reports generally have device and so must be organized additionally data sets are constrained to be accessed sequentially by the computations they and so their are either consecutive or index sequential within this context the remaining assignments are made by cases the optimizer considers one data set at a time and binds its device and organization and the accessing methods of each accessing computation before considering the next if a data set has no conflicts with its it is given consecutive organization sequential unless otherwise specified by the user device disk if it is a system input that is only partially sorted core table used if its size permits otherwise a sorting computation is inserted if a data set has conflicts with its and it is core table size all by core table if a data set has a conflict and is too big for core table access there are three alternatives i give it consecutive organization have the compatible computations access it sequentially and insert sorting computations to produce versions that can be accessed by the rest of the computations give it index sequential organization have the compatible computations access it sequentially and the others randomly s give it direct organization and have all access it randomly each alternative assignment is sent to the job cost and the one that cost is chosen step determination of blocking factors choice of blocking factors can be to the very last step in the optimization process the cost of accessing as a function of the block size initially as block size increases but then it a minimum and begins to rise as core become significant however for the scheme we have studied the minimum usually occurs either beyond the operating systems block size limit or so close to it that the core does not play a significant role in cost so the optimal assignment of blocking factors by total system cost as a function of the blocking factor is determined subject to the constraints that the blocking factor be less than the operating system upper limit eg no block can be larger than a single disk track and that the total buffer space at any time cannot the available core experience while this process will not find the true it produces a good and usually solution for real and problems the reason for this is that from our experience so far in typical systems global effects are weak step determination does a good job of the consequences of these global effects down to a local level and after that what is reasonable to do in a fairly local context is almost always good if not best for the overall system design the approach described in this paper is within the spirit of state of the art design the goal is to get a system that works and works well designers do not amounts of time and energy to get as close to an as possible unless timing andor cost constraints are critical although our optimizer lacks the and special case knowledge of a system designer we believe that the application of sound though not perfect optimization rules will more than for this even with the few relatively simple systems that have been on so far the optimizer has proposed designs which were not at all obvious but which under careful proved to be quite good references a relational model of data for large shared data comm acm june early j relational level data structures for programming languages comp dept u of california berkeley m w and i an overview of a definition system acm sigplan april w methodology for optimization in automatic programming systems unpublished bs thesis project m a c mit o internal memo status of i project mac mit g internal memo the new question project mac 