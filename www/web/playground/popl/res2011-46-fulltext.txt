the essence of compiling with traces palsberg computer science department university of california usa abstract the technique of compilation was introduced by et al and was further developed by et al it currently success in javascript engine a jit compiler runtime profiling to optimize paths while enabling the optimized code to out to the original code when the path has been this optimization strategy differs from those of other jit compilers and the question of which trace optimizations are sound in this paper we present a framework for reasoning about the soundness of trace optimizations and we show that some traditional optimization techniques are sound when used in a trace compiler while others are unsound the converse is also true some trace optimizations are sound when used in a traditional compiler while others are unsound so traditional and trace optimizations form incomparable sets our setting is an imperative calculus for which tracing is explicitly out in the semantics we define optimization soundness via a notion of bisimulation and we show that sound optimizations lead to confluence and of stores categories and subject descriptors d program verification correctness proofs formal methods d processors compilers f semantics of programming languages operational semantics general terms languages theory keywords compilation compiler correctness bisimulation introduction with the of web the web browser has become a platform that rich interactive applications the technology central to this transformation of the web browser is javascript dynamic nature has since then become a performance the performance of dynamic languages is much worse than statically typed languages and javascript is no exception moreover traditional jit compilation techniques designed for static typed languages are for javascript the work of et al was adapted as a novel jit compilation technique called trace compilation ­ a jit compiler uses runtime profiling to approximate the hot permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page to copy otherwise to republish to post on servers or to redistribute to lists requires prior specific permission andor a fee popl january ­ austin texas usa copyright c acm paths loops in the program and only those paths the executed bits of code are interpreted the idea is quite intuitive if there is a repeatedly executed section of the code that section should be top priority for compiling to native code for example a web application might take many rows of data and transform them into a format this loop would be where the program the of its time a jit detects that this loop is a hot execution path and it to native code tracing jit compilers are amenable to javascript and their greatest success in javascript engine it is available in versions and greater and metrics report that approximately people in the world are using the tracing jit tracing jit compilers differ greatly in technique from many other jit techniques it the following question which trace optimizations are sound we the essence of trace compilation to a simple imperative calculus with an operational semantics this allows us to formally investigate notions of correctness of jit compilers and the properties that trace optimizations must satisfy to be sound we present a soundness criterion for trace optimizations and we prove a determinism theorem whether one traces or not the final store will be the same our framework is modular in two ways first an optimization designer needs only prove that a given optimization satisfies our correctness criterion the determinism theorem then follows second the composition of two sound optimizations is itself sound we the first kind of modularity to easily prove soundness of the folding of free loop variables and dead branch elimination proving optimizations unsound is equally simple we show that dead store elimination is unsound with an counterexample readers can easily proceed like we did to prove additional trace optimizations sound our proof of the determinism theorem has the following steps first we prove that an recorded trace of the loop is correct or bisimilar to the original loop we then prove that the original program with the new trace in place of the old loop is bisimilar to the original program this then sets the stage for sound optimizations sound optimizations are those that do not this behavioral correctness guarantee had from bisimilarity finally we put the pieces together and prove confluence and of stores via a lemma and a strip lemma our framework shows surprisingly that traditional optimizations and tracing jit optimizations do not stand in a subset relation in either direction in one direction it is clear the exact average of usage from january until july of versions and is that traditional optimizations are not subsumed by trace optimizations informally trace optimizations are not to be correct for all possible executions and contexts they are to be correct only for a particular execution and a particular context for instance in the paper we prove folding of variables that are not assigned to be sound for tracing but unsound in general in a traditional setting in the other direction trace optimizations are also not subsumed by traditional optimizations the reason for this is more subtle the domain of trace optimizations is restricted to only the trace the code surrounding the trace is to the optimizer traditional optimizations on the other hand are to both the prefix and the suffix of the trace in that their domain is the entire procedure in other words those optimizations can prove properties on entire procedures while trace optimizations cannot for one optimizations know that their local variables are dead after the function exits trace optimizations cannot make the same assumption about their local variables after the trace exits in the paper we show dead store elimination to be unsound as a trace optimization to expand upon the of the two sets of optimizations it is to out the differences between our work and recent works in compiler correctness the program equivalence condition lemma found in et al basically states classical bisimilarity as the condition under which to the correctness of optimizations an optimized program must be bisimilar to the original and their respective final stores must contain the same values for all variables in their framework optimizations are formulated as rewrite rules with side conditions expressed in temporal logic despite this difference we can make the following comparisons we write m n for to mean that the program m is bisimilar to n when their initial stores are suppose there is an optimization function o their correctness criterion describes traditional optimizations and is extensional for all p p op for all stores our correctness criterion describes tracing optimizations and is intensional suppose the program p into two components w the traced loop and k the rest of the program the criterion is then for all w k w k ow k for note how our optimization function takes a state in addition to a program to produce an optimized program guaranteed to be correct when the initial store is fixed to be that store and the rest of the program is fixed to be k this captures that trace optimizations may not be straightforwardly used in a classical setting the correctness criterion in chambers et al raises yet more differences between classical optimization soundness and trace optimization soundness at the of their formulation of soundness is contextual equivalence however note that we have said that trace optimization correctness is to a particular computation suffix this necessarily trace optimization correctness from being contextual equivalence classical optimizations about the behavior of a program to substitute optimized portions for the original portions before execution trace optimizations on the other hand know exactly the behavior for which they need to optimize and substitute optimized portions during execution this also that the input to trace optimizations is we do not know a priori what loops are hot the input to classical optimization on the other hand is fixed in chambers et al and et al this input is the entire program as mentioned before the domains of the two kinds of optimizations are simply different the remainder of this paper is organized as follows in section we introduce our language its operational semantics and discuss certain properties of compiling with traces in section we show that the operational semantics of our language is correct up to weak bisimulation and relate the results to confluence in section we explore various optimizations both provably correct and provably incorrect into the framework in section we discuss related work section concludes compiling with traces what is essential to the trace compilation technique what features must our calculus contain at its core it is a method of compiling loops we must therefore have loops more specifically it is a technique of recording loop bodies at runtime and optimizing them the second essential part must thus be the ability to record execution what is optimized then is not the text of the loop but a runtime execution path through the loop in other words we are optimizing some fixed execution so when the execution diverges from the recorded path there must exist a mechanism to return us to the original program the third and final component is this mechanism in the literature of trace compilation these are called we modeling the many other features of the technique that exist in implementations for instance we shall not model the heuristics in how one actually detects a hot path of execution we will simply build in the ability to record a path of execution nondeterministically this nondeterminism will be realized via overlapping reduction rules nor shall we model implementation details such as trace trees and their interactions we aim to keep the calculus minimal yet highlevel even human consumption the language and its execution we first present the syntax and smallstep operational semantics for a simple imperative language with two of the three essential loops and the ability to out modeled by continuations the syntax of our language inspired by the calculus presented in is shown in figure concretely traces are a subset of normal statements they are meant to be sections of code with so there are only assignments and we use x to range over variables i to range over nonnegative integers to range over stores and l m n k s p to range over statements throughout the rest of the paper the transition rules are shown in figure assume is the real addition operator on integers we use a labeled transition system where labels correspond to store updates we assume the reader is familiar with such systems as they are used in the literature of concurrency the only observable transitions are store updates which are labeled by the store all other transitions are labeled and are the subscript b denotes transition rules the subscript a denotes a strict subset of the transition rules that will be used in the proofs the subscript t denotes tracing transition rules the rules in figure are common to both the rules do the usual things is the rule that applies continuations in the it says to the current with the continuation s recording traces we extend the execution with the ability to record traces the set of rules is a proper subset of the tracing rules ie bt the abstract syntax is the same between the two languages the additional transition rules are shown in figure starting traces we start a trace at the beginning of a while loop for technical reasons for the proof of correctness we record when we have already at least one iteration of the loop also note that trace puts the reduction rules in recording mode which is represented syntactically as tuples the components are e n x b x x w while b do s s c s c skip x e w if b then s b to s expressions boolean expressions loops statements commands t ct t ct skip x e b to s traces recorded commands figure syntax of the simple imperative language and traces e n if e n x if e x true b false true false if b is x x if b is x x if b is x x if b is x x xi store updates actions x e k xe k where xe skip k k if b then s k k if b false if b then s k s k if b true while b do s k if b then s while b do s k b to s k k if b false b to s k tb s if b true figure shared transition rules assign seq while ¬b x if b is x x if b is x if b then s while b do s k t while b do s k s while b do s k if b true trace kw t x e k t xe kw t x e k kw t skip k t kw t skip k where xe kw t if b then s k t kw t b to s k k if b false kw t if b then s k t kw t ¬b to k s k if b true kw t while b do s k t kw t skip if b then s while b do s k if kw while b do s k kw t while b do s k t b do t k if kw while b do s k kw t k t k if k t k kw k abort figure tracing transition rules in order the store the point of the trace the trace thus far and the current program being reduced recording traces the recording rules record one command at a time and it to the end of the trace concatenation is simple the trace itself is a section of code so we pieces of code that jump back to code when the condition we traced no longer holds when we record conditionals to ease the task of proving correctness a skip to the trace while unrolling the loop its side condition is to ensure that we are recording an inner loop inside the current loop we are tracing and that we have not come full and finished tracing the work for up a trace is done in whose side condition is mutually exclusive with that of ending wellbehaved traces we end the trace and it back into the program using when we finish tracing the body of the loop we know we have finished when we come back to reducing the same loop that started the trace we compile the loop that was traced into the same language the actual optimization is to the semantics we assume that there is a sound optimization function o statement × store statement what soundness entails here will be made precise when we investigate correctness informally soundness means that the output of the o function does the same thing as the original code as far as observable behavior store updates goes ending traces we are not guaranteed to finish tracing the body of the loop that loop body might never terminate consider the following example assume s never changes b to a b while a do s while b do s if we start tracing the outer loop once we start executing the inner loop we will never finish the outer loop body and thus never finish tracing implementations of trace compilation then must use heuristics to end the trace if it is on for too long in our semantics we model this by introducing another nondeterministic rule that stops the trace abort this rule shares the same premises with all record rules where is a wildcard note that there are no axioms for recording this means is that instead of the semantics getting stuck when trying to trace a trace we abort the trace that is we do not model higherorder tracing also note that aborts side condition is mutually exclusive with which is intuitively the good situation of a successful trace in this way the rule models the semantics of out of tracing mode for all bad situations example trace recording to help illustrate the tracing rules and to build some concrete intuition consider the following example example input x while x do y note that this is a simplification in our model in actual tracing the compiled code is in machine language the rule is modeled as presented instead of the alternative of kw t k t k if kw k for a proof of correctness while y do y z b a there are two loops the inner loop only iterates once the variable a is computed at some earlier point in the program we give a of tracing the outer loop we build up the trace in with our through of the reduction rules each that the record rules append to the trace is displayed one by one to start line of the input is matched by assign so we reduce by assign line is a while loop which we reduce by while while converts the loop into an if statement testing the condition x this is indeed true by how we the store in line so we can reduce by or trace in the interest of tracing we reduce by trace the trace built thus far is empty or only entered recording mode but we actually recorded any commands yet line in the input is an assignment which is matched by the assignment itself onto the trace y example trace line in the input is the inner loop and we will now see how the tracing rules deal with recording loops the loop itself will first reduce to an if via which a skip to the trace in reducing the resulting if we are testing the condition y it is true so we reduce using the result is that we append a as a to the trace the computation that would have been executed had the condition been false gets up as a continuation and gets put into the body of the shown in the listing skip y to z b a while x do y while y do y z b a now that we have the for entering into the inner loop we trace the body of the inner loop as code line in the input is another assignment which we record using y after the body of the inner loop we attempt to reduce the next iteration of that loop again the loop will first reduce to an if by this a skip unlike the last time however the condition y is now false so we instead reduce using we append another as before but the continuation is different since the condition was false in the actual execution we need to include the statement that would have been executed if the condition were true after that statement we package the rest of the iteration of the outer loop and append it skip y to y while y do fv statement variables fv s x x is free in s f expression statement command × store × v statement n if e n f e v x if e x x v e if e x x v f s v f c v s if s f c v f s v if s c s x f e v if c x e c otherwise o statement × store × v statement os while b do f s fv s if s while b do s s otherwise figure variable folding o y z b a while x do y while y do y z b a finally we apply twice to lines ­ and append the assignments to the trace z b a having successfully traced an iteration of the loop we now reduce by to the trace back into the program using the identity as the o function the continuations for the as ki the final traced loop is as follows abbreviated and traced example loop while x do y skip y to k y skip y to k z b a example o we have seen the output of tracing but we obviously want to do more than that we want to optimize consider optimization shown in figure that folds away variables that we never assign to inside a traced loop first we define a function that the free in the sense of variables of a statement it is assumed to be defined in the usual way next we define a function f that does the actual optimization v is the set of free variables finally the o function is just a wrapper around f that and passes in the free variables if we apply it to our running example where a we fold the assignment to b on line of the abbreviated example b the main benefit of runtime optimization is that we can be more than with optimization here we presented a simple conservative folding of free loop variables the idea is that free variables in the loop be treated as constants and until we break out of the loop we cannot be so with a static version of this kind of folding as we can only do so if we know that the variables we want to fold are constants for the of program execution here however we only need to know that the variables value does not change until the loop is finished correctness what does it mean for a trace to be correct first correctness of the traced code is behavioral trace has to do the same thing as the original code to prove confluence of the program text such as in pfenning is as there are no guarantees in trace compilation of the traced code back to the same text as the original program in a reactive for instance we might many inner loops and those compiled inner loops might execute forever waiting for user input but even in those cases of infinite execution we still want to reason about correctness the need for infinite executions suggests the tool of bisimilarity second correctness of the traced code is intensional correctness unlike compiler correctness we cannot say that an optimized trace is observationally equivalent or has the same sequence of observable reductions to the original loop in the traditional extensional sense specifically an optimized trace need not be observationally equivalent to the original loop under all stores consider the following version of our little example x while x do b a a reasonable optimization if a as we have seen would be to replace the loop body with b but this code is in our simple model the trace is effectively discarded after the loop exits there is no way to a traced loop once it exits this is not the case in practice where constructs such as methods allow compiled traces to be called multiple times in those cases the tracing jit has to add in more guards and to guard the values we omit this complexity most definitely not observationally equivalent to the original the original has a free variable a and the optimized code b does not are also problematic how do we ensure that we jump back to the right place in the original code we retain the familiar notion of observational equivalence but it over stores and computation namely a trace is correct if it is observationally equivalent to the original loop for the store that the original loop is currently reducing under and for the rest of the program that the original loop would have reduced under to formalize these we model correctness using bisimulations over stores to a particular suffix will be made formal in the definition of o soundness in section bisimulation techniques see popular use in process calculi there is also existing work in the analysis and correctness proofs of program transformation the definitions here are built upon but slightly different from the standard notions found in the concurrency literature as they are defined over a store observational equivalence also becomes formally defined as the notion of bisimilarity let the set of labels be defined as follows act is a store update definition if r act then r is the sequence all occurrences of are removed definition if r · · · n act we write m r m to mean m · · · · · · n · m that is there may be any number of transitions between the observable sequences in this particular system the primary observable entity is the store itself so the intuitive meaning of a program becomes the sequence of store updates it performs we only concern ourselves with closed pairs in this paper where the definition of closed is as follows definition for a store and a program m we say m is closed if for all variables that appear in m x is defined for the rest of the paper when we say for any store or for all stores we mean for all stores that form closed pairs with the programs under consideration definition bisimulation a bisimulation for two reduction relations x y is a relation r such that r m n implies whenever m x m then for some n n y n and r m n whenever n y n then for some m m x m and r m n in the above definition we notation and let m m n and n range over both statements and triples of statements that is since it does not add to the discussion to distinguish between tuples and tuples in the definition we use a single metavariable to range over both the traditional notion of bisimilarity is a special case of this one two programs are bisimilar in the traditional sense if they are bisimilar for all stores definition m is said to be bisimilar to n under reduction relations x y for a store written m xy n for if r m n for some bisimulation r on x y in other words xy r r is a bisimulation for x y lemma bisimilarity is an equivalence relation before stating the main lemma we note that all nondeterministic rules in our system step to the same store we prove this later in lemma for simplicity in stating the main lemma we simply say that the two branching stores are always the same we are now ready to state the main lemma in the literature lemmas are usually single the trace calculus however has a modal with tuples as one mode and tuples as the other mode as such our calculus has six up to symmetry lemma lemma all of the following hold for ­ kw t p is wellformed a notion we will upon in section if r p t m and r p t n then m t t n for p rr m t t n if r p t kw t m and r p t kw t n then kw t m t t kw t n for p rr kw t m t t kw t n if r p t m and r p t kw t n then m t t kw t n for p rr m t t kw t n if r kw t p t m and r kw t p t n then m t t n for kw t p rr m t t n if r kw t p t kw t m and r kw t p t kw t n then kw t m t t kw t n for kw t p rr kw t m t t kw t n if r kw t p t m and r kw t p t kw t n then m t t kw t n for kw t p rr m t t kw t n this lemma says that should execution branch into two branches both branches will do the same thing at least observationally we aim to use the main lemma to arrive at a more familiar place confluence of stores namely corollary the rest of this section is organized as follows section introduces the idea of wellformedness for the tuples or the tracing rules section introduces the correctness criterion of the trace section proves the main lemma section the relationship between confluence and our bisimulation result many proofs in this section are omitted for brevity the reader may find them in the full version of the paper at wellformedness of tuples when the calculus to a trace it steps to a configuration in the shape of a tuple the four components are in order the store the point in the original code when we started tracing the trace so far and the statement currently being reduced not all tuples are created equal however as not all tuples are wellformed intuitively wellformedness is something like an incremental version of correctness only wellformed tuples eventually become fully correct traces thus we want it to be an invariant of the computation wellformedness is a tight and relationship between the original loop the trace thus far and the current informally we need the trace thus far to be a recording of all the steps that the original loop took just before it reached the current before we formally define wellformedness we formalize what it means to be a trace thus far definition partial trace relation a partial trace relation is a relation t such that t t l implies that whenever t b t then for some l l b l and if t by t l otherwise t t l the are as follows t is the trace and l is the original code recall that both t and l are just statements the formalization is a variation on the standard simulation relation and captures the two properties that a partially constructed trace intuitively satisfies first models back to the original code so we expect the descendants to be exactly equal second the partial trace is partial so it can terminate before the original code does that the rest has not yet been traced definition we call t a partial trace to l for a store written t l for if t t l for some partial trace relation t in other words t t is a partial trace relation for the definition of wellformedness and subsequent lemmas we will be working with the reduction relation a which we have not used yet as well as a notion of being stuck recall that the reduction relation a b definition for a reduction relation x we say a configuration m is not iff m or m x m for some m definition wellformedness a tuple kw t m is wellformed iff all the following hold kw while b do l k for all either t r a and l kw r a m or for some t t ra t and t is but not for all t l kw for this property formalizes the invariant we wish computation to preserve first kw must be the loop where we started tracing second the trace t must do one of two things it must either go far enough by terminating in the same reduction sequence that body of the original loop to reduce to the current statement m or it must eventually step to some descendant that can only reduce by third t must be a partial trace to the original loop for all stores we now prove that the reduction relation t preserves this invariant we will do this in two steps first we prove that the tracing rules rules that step from a tuple to another wellformedness lemma let p kw t m if p is wellformed and p t p such that p is a tuple then p is also wellformed proof the first conjunct holds trivially because no rules change kw and we proceed to prove the next two conjuncts together by case analysis on the structure of the reduction relation second we prove that whenever we a we step from a tuple to a resulting tuple is wellformed lemma if m t p where p is a tuple then p is wellformed proof by inversion the only rule that results in a tuple is trace let w while b do s we have if b then s w k t w k s w k the first two conjuncts are clearly satisfied it remains to prove conjunct that s w k for all it suffices to exhibit a partial trace relation t such that for all t s w k since t we exhibit the empty relation as one such t lemmas and lead us to a more general lemma about the transitive reflexive closure of the t reduction relation this lemma is not used in the rest of the section but does clearly that wellformedness is a property preserved by computation in our calculus lemma if m r t p where p is a tuple then p is wellformed correctness of the trace recall that the intuition for wellformedness is that it is an incremental correctness with it we can now build up a bisimulation relation lemma lemma for some t let w while b do l and w while b do t if for some k t l w k for all and holds of t l w k then w k bb w k for all for all either t r a and l w k r a w k the trace is in fact strongly bisimilar to the original code since we are simply recording some execution path it be surprising that the resulting trace is exactly equivalent to the original path in the interest of less mechanism and since weak bisimilarity subsumes strong bisimilarity we will directly prove weak bisimilarity or for some t t ra t and t is but not this lemma is the correctness property we want to express of traces in w is the original loop and w is the new loop with the trace in the t l w k for all and conditions are what hold of t l w k per wellformedness at the point of the lemma says once we come full and the recorded trace into the original program that trace is actually equivalent to the original loop the reader should bear in mind that this is an almost extensional equivalence a stronger notion than intensional equivalence the insight here is since we have proven a more restrictive property than we need we can relax it for optimization we need to make the relation larger extensional bisimilarity to the intensional version proof by of a bisimulation relation r under the relations b b such that r w k w k for all proof of the lemma with the tool of bisimilarity under our we can now make precise the notion of the soundness of o this definition will be crucial for the proof of the lemma definition an o function is sound iff for any w w k such that w k bb w k for all stores ow k bb w k for proof of the lemma if r and r are the same then we are done as m n or kw t n kw t n it is straightforward to verify that r r for and which are deterministic so we only need to prove and we can rewrite these more precisely below the nondeterministic rules are and trace we have m n by inversion further by lemma kw t n is wellformed if r p t m and r p t kw t n then m t t kw t n for p trace m t t kw t n the nondeterministic rules are record and abort we have kw kw and m n by inversion similarly by lemma kw t n is wellformed if r w t p t m and r kw t p t kw t n then m t t kw t n for kw t p abort record m t t kw t n to show that t t holds for both it suffices to exhibit a bisimulation relation r under the reductions t t such that r m kw t n and r m kw t n hold we claim the following relation is a bisimulation for any m n u kv kw t r m n m kv u n kw t m n m bb n for m bb n for and kv u n is wellformed m bb n for and kw t m is wellformed the rest is omitted for brevity from bisimulation to confluence this section aims to be the interface between bisimulation and confluence as correctness is often studied in terms of and confluence we here to prove something to confluence of stores to show the adequacy of our operational semantics traditionally confluence theorems are proven from the bottom up using a lemma that lemma to build a strip lemma and finally using the strip lemma to construct confluence bisimilarity however allows us to skip the iteration of the lemma indeed there is no to an lemma here we instead use bisimilarity to directly obtain a strip lemma nevertheless the techniques and diagrams in this section are strongly by the clear and readable approach of pfenning first we prove the assumption needed for all cases of the lemma that nondeterministic branching always branches to configurations with the same store lemma if m t m and m t m then and proof straightforward case analysis lemma strip lemma if r m t m and r m t m then for some n n m t n and m t n such that n t t n for m r r m m s s n t t n proof by case analysis on the structure of r and the definition of t t now we can prove the property for stores on the reduction which we call store confluence theorem store confluence if r m t m and r m t m then for some n n m t n and m t n such that n t t n for m r r m m s s n t t n proof by induction on the structure of r finally theorem implies the familiar notion of store for terminating programs corollary if m t and m t then sound and unsound optimizations we have achieved our project of proving the essence of trace compilation correct yet at the same time that result is largely interesting due to its modularity with respect to the o function in this section we show the example o from section to be sound and explore which kinds of o functions are sound and which are not sound soundness of variable folding let f fv and o be the ones presented in figure for brevity we assume that fv s is defined in the usual way and correctly generates the set of free variables for s that is for some store fv s the set of variables which s never writes to in during reduction lemma b is deterministic proof b has no points of nondeterminism lemma o is sound proof assuming we have for some w w k such that w k bb w k for all stores we want to show that ow k bb w k for our technique will be showing that ow k bb w k and then obtaining the desired result via transitivity of bb we proceed by case analysis on the o function case s while b do s we want to show that while b do f s fv s k bb while b do s k it suffices to exhibit a bisimulation relation r such that r while b do f s fv s k while b do s k let s f s fv s we claim the following relation is a bisimulation relation for any m note that and the loops are fixed from the assumption r m m while b do s k while b do s k if b then s while b do s k if b then s while b do s k f n fv s while b do s k n while b do s k x x for all free variables in s we proceed by case analysis on the reduction step the left side and right side are the same by lemma b is deterministic both sides step using the same rule producing the same descendants but then they are in r by construction m r m m r m the left side is while b do s k and the right side is while b do s k both sides reduce by way of while their descendants are in r by construction while b do s k r while b do s k while while if b then s · · · k r if b then s · · · k the left side is if b then s while b do s k and the right side is if b then s while b do s k suppose the left side reduce by then both sides step to k which is already in r by way of the first if b then s · · · k r if b then s · · · k k r k the left side is if b then s while b do s k and the right side is if b then s while b do s k suppose the left side reduce by we can then fill out the diagram as follows the descendants are in r by way of the fourth as s f s fv s if b then s · · · k r if b then s · · · k s · · · k r s · · · k the left side is f n fv s while b do s k and the right side is n while b do s k the left side steps by way of assign by inversion n c n and f n fv s f c fv s f n fv s further c x e for brevity let n f n fv s by case analysis on e we have two in the case where e n we have an identity in the case where e x x v e x we know that x is defined from assumption that the left side steps at all this means the left side step looks like the following the call to f is abbreviated due to space c n · · · k b xx n · · · k where xx by inversion then we see that the right side starting with c also steps by assign by the definition of we have the following reduction for the right side c n · · · k b x x n · · · k where x x for these two descendants to be in r we need x x we know this to hold for all free variables in s as their guarantees them to be never written to during ss reduction we assumed that fv correctly generates the set of free variables for a statement it is easy to see that c is a descendant of s thus x fv s and x x holds since fv is correct x is not free in s therefore x x y y for all y free in s this finally gives us r x x n n which holds by way of the fourth for the diagram below let v x f n fv s · · · k r n · · · k assign assign xv f n fv s · · · k r xv n · · · k all other cases where s is something other than a while loop are identities the proofs for the are symmetric we have shown ow k bb w k for by transitivity we have the desired result of ow k bb w k the most interesting part of the proof is that in every we on the right side to be able to the left sides move exactly in a single step this is a stronger property than required by the bisimulation definition which says only visible moves need to be soundness of dead branch elimination what kinds of optimizations only visible moves one can imagine that during tracing we may generate many spurious suppose we extend our variable folding example to also eliminate dead or the modifications needed for f are shown in figure for example considered the following example trace with a dead clearly x is free in the body of the traced loop and the boolean expression x is always going to be false example trace with dead while x do x to k z the above example into the extended o function will output the following example trace with dead optimized away while x do z such an optimization does not generate code that exactly the original this fails to hold if we dead conditionals as the original code would still need to take a step to evaluate the conditional to false before it to show that this new optimization is still bisimilar let us extend lemma with the proof sketch of a new and its converse new and its converse for lemma the left side is f n fv s while b do s k and the right side is n while b do s k the left side takes some step let n c n and f n fv s f n fv s we are concerned with the case when c b to k f b fv s false all other cases for c are identities for brevity let n f n fv s the left side step looks like the following for some n the call to f is abbreviated due to space n · · · k b n · · · k by inversion we know that b false since we assumed that fv correctly generates the set of free variables for s and c is a so b false by inversion then we see that the right side starting with c steps by c n · · · k b n · · · k it remains to show that n can take a step to match the left side step that f n fv s took we again decompose n into its first command and continuation we iteratively apply the same reasoning we just until the first command is not b to k f b fv s false for these other cases f acts as an identity for the first command and as congruence for the continuation so clearly it will take the same step in the diagram below let mean or more times f n fv s · · · k r n · · · k n · · · k f n fv s · · · k r n · · · k true false f b v true false undef if b x x v x if b x x v x if b x x v x if b x x v x otherwise f c v if c b to s f b v false figure variable folding extended with dead branch elimination f c v if c x e x has no use sites in the trace figure variable folding extended with dead branch and dead store elimination the converse is considerably simpler we have the case where the right side steps by by the same reasoning above concerning free variables we see that the left side would have had its optimized away into thus we can complete the diagram by using id f n fv s · · · k r n · · · k id f n fv s · · · k r n · · · k all other cases are still identities of dead store elimination finally we want to explore what kinds of optimizations are simply unsafe in the tracing framework put formally we want to ask what kind of optimizations do not produce bisimilar code with our existing o function suppose we were to extend it with dead store elimination that is suppose variables that we assign to but have no use sites inside the trace body are simply this is shown informally in figure for example consider the following example trace with a dead assignment the variable z is assigned but never used example trace with dead assignment while x do z the above example into the extended o function will output the following example trace with dead assignment optimized away while x do intuitively this is unsafe because even though z is dead inside the trace there very well may be use sites of z after the trace this intuition is reflected formally taking our example above we need to show that z takes a step that can be by for some by inversion z can step only by assign z b z needs to be able to match this move but b z s for any s in fact it does not step at all in this fashion this optimization does not output bisimilar code and is not safe for use inside the tracing framework soundness of composition one property that correct optimizations in our framework is that the composition of two correct optimizations also yield a correct optimization we give the following two lemmas to demonstrate this property lemma let i statement × store statement be the identity function on its first argument i is sound proof trivial by the definition of lemma let f g statement × store statement be two sound optimizations let their composition be defined as f g s f gs f g is sound proof we want to show that for any w w k such that w k bb w k for all stores f gw k bb w k for by soundness of g on w w k we know that gw k bb w k by soundness of f on w gw k we then know that f gw k bb w k but f gw k f gw so we are done related work the work carried out in this paper depends on both and concurrency techniques though the of both are large there is a of relevant papers that explore purely operational compiler correctness of jit compilers from as a highlevel as ours nevertheless we have taken as well as comparisons with several works relevant is work on parallel compiler correctness we believe to be though also bisimulations to prove of a different than our own his approach is the combination of syntaxdirected denotational semantics and essentially his picture is also closer to the traditional picture of compiler correctness that is the compilation process preserves denotation up to bisimulation than ours as his compiler is an compiler the of work is that he recognized that induces bisimilarity in the conclusion he admits that almost all the required reasoning is done in the and as such he can reuse work already done in sequential compiler correctness unlike work our of correctness is purely syntaxdirected the translation itself if the jit tracing can be seen as such becomes a process since we have to it out in the operational semantics this is what makes the nontrivial our notion of informally becomes something to but this is much less powerful than as it does not directly imply bisimilarity we also do not have the of to bear the of the calculus machinery so our technique here while still using bisimulations is at once more basic and less elegant there is also a of literature exploring using bisimulation to show program equivalence by way of contextual equivalence in pierce et al et al and wand et al ­ though the topics of their specific differ they all concern themselves with using bisimulation as a more tractable proof technique to prove contextual equivalence without having to universally quantify over all contexts their setting is modeled within the calculus pierce et al for instance aims to prove sound and complete with respect to contextual equivalence for a modified calculus with recursive types wand aims to improve the proof technique and reasons about a calculus extended with explicit stores these works are basic into the nature of the proof technique ours is an application of the technique to prove equivalence of a dynamically transformed program we also at by an entirely different motivation that of proving determinism of a jit compiler that performs the dynamic transform we are not met with the difficulty of universally contexts in fact we fix our correctness to hold for one context only method of creating formally correct jit compilers for x is at the much lower level of abstraction machine language they use hoare logic and so still retain a of the denotational we are much from the than they are conclusions and future work we have demonstrated a paradigm for highlevel purely operational correctness of the tracing jit compilation technique via bisimulation and confluence unlike traditional compiler correctness where the translation process from the source language to the target language is an opaque function trace requires the be out explicitly we overcome this difficulty by using bisimulations though we to maintain continuity with existing purely operational correctness approaches by returning to confluence we hope that the theoretical framework we have provided will prove useful in reasoning about trace compilers at a high level we hope that we have up a of possible future research in the foundational differences traditional and trace optimizations though a different problem we also feel applying the trace compilation technique to an applicative setting namely the calculus will be a it is also interesting to further explore o and the question of just what exactly is observable in computation we also hope to look at deriving tools from the techniques described here in the future acknowledgements we thank michael and the javascript team for discussions about the tion of trace compilers we also thank lee and the anonymous reviewers for draft reading and helpful discussions references and a transparent dynamic optimization system in pldi pages ­ acm michael and a jit compiler for in oopsla michael and michael compilation in execution environments without interpreters in w smith michael and michael tracing for web trace compilation for the next generation web applications in pages ­ proof of translation in natural semantics in lics pages ­ flanagan and matthias felleisen the semantics of future and its use in program optimization in popl pages ­ acm efficient bytecode verification and compilation in a virtual machine phd thesis michael david david r kaplan hoare w smith michael and michael type specialization for dynamic languages in pldi pages ­ acm a j michael a and robert n a programming approach to springerverlag and mitchell wand small bisimulations for reasoning about higherorder imperative programs in popl pages ­ acm david d jones van and proving correctness of compiler optimizations by temporal logic in popl pages ­ acm and craig chambers automatically proving the correctness of compiler optimizations in pldi pages ­ acm metrics usage robin milner communication and concurrency prentice hall o verified compiler on x in popl pages ­ acm frank pfenning a proof of the churchrosser theorem and its representation in a logical framework journal of automated reasoning and b a complete coinductive syntactic theory of sequential control and state in popl pages ­ acm and benjamin c pierce a bisimulation for dynamic sealing in popl pages ­ acm and benjamin c pierce a bisimulation for type abstraction and recursion in popl pages ­ acm mitchell wand compiler correctness for parallel languages in pages ­ mitchell wand and william d set constraints for destructive array update optimization journal of functional ­ mitchell wand and constraint systems for useless variable elimination in popl pages ­ acm 