# Questions for Professor Greenberg.

How are we tokenizing the documents? By words? What punctuation, apostrophes (I guess those are interpreted as one word), and hyphenated terms (I did see something in the parse.py about replacing hyphens)?

Should I use stemming or lemmatization? Have we experimented with the performance / effects of both yet? I saw a `use_wordnet` flag in the parse.py file but wasn't sure if we ever set that to `False`.

