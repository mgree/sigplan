{"article_publication_date": "10-19-2012", "fulltext": "\n Reusing Debugging Knowledge via Trace-based Bug Search Zhongxian Gu Earl T. Barr Drew Schleck Zhendong \nSu Department of Computer Science, University of California, Davis {zgu,etbarr,dtschleck,su}@ucdavis.edu \nAbstract Some bugs, among the millions that exist, are similar to each other. One bug-.xing tactic is \nto search for similar bugs that have been reported and resolved in the past. A .x for a similar bug can \nhelp a developer understand a bug, or even directly .x it. Studying bugs with similar symptoms, programmers \nmay determine how to detect or resolve them. To speed debugging, we advocate the systematic capture and \nreuse of debugging knowledge, much of which is currently wasted. The core challenge here is how to search \nfor similar bugs. To tackle this problem, we exploit semantic bug information in the form of execution \ntraces, which precisely capture bug semantics. This paper introduces novel tool and language support \nfor semantically querying and analyzing bugs. We describe OSCILLOSCOPE, an Eclipse plugin, that uses \na bug trace to exhaustively search its database for similar bugs and return their bug reports. OSCILLOSCOPE \ndisplays the traces of the bugs it returns against the trace of the tar\u00adget bug, so a developer can visually \nexamine the quality of the matches. OSCILLOSCOPE rests on our bug query lan\u00adguage (BQL), a .exible query \nlanguage over traces. To realize OSCILLOSCOPE, we developed an open infrastructure that consists of a \ntrace collection engine, BQL, a Hadoop-based query engine for BQL, a trace-indexed bug database, as well \nas a web-based frontend. OSCILLOSCOPE records and up\u00adloads bug traces to its infrastructure; it does \nso automatically when a JUnit test fails. We evaluated OSCILLOSCOPE on bugs collected from popular open-source \nprojects. We show that OSCILLOSCOPE accurately and ef.ciently .nds similar bugs, some of which could \nhave been immediately used to .x open bugs. Categories and Subject Descriptors D.2.2 [Software Engi\u00adneering]: \nDesign Tools and Techniques; D.2.5 [Software Engineering]: Testing and Debugging; D.3.1 [Programming \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n12, October 19 26, 2012, Tucson, Arizona, USA. Copyright &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. \n. . $10.00 Languages]: Formal De.nitions and Theory General Terms Design, Languages, Reliability Keywords \nOSCILLOSCOPE, reusing debugging knowledge 1. Introduction Millions of bugs have existed. Many of these \nbugs are similar to each other. When a programmer encounters a bug, it is likely that a similar bug has \nbeen .xed in the past. A .x for a similar bug can help him understand his bug, or even directly .x his \nbug. Studying bugs with similar causes, programmers may determine how to detect or resolve them. This \nis why programmers often search for similar, previously resolved, bugs. Indeed, even .nding similar bugs \nthat have not been resolved can speed debugging. We theorize that, in spite of the bewildering array \nof applications and problems, limitations of the human mind imply that a limited number of sources of \nerror underlie bugs [17]. In the limit, as the number of bugs in a bug database approaches all bugs, \nan ever larger proportion of the bugs will be similar to another bug in the database. We therefore hypothesize \nthat, against the backdrop of all the bugs programmers have written, unique bugs are rare. Debugging \nunites detective work, clear thinking, and trial and error. If captured, the knowledge acquired when \ndebugging one bug can speed the debugging of similar bugs. However, this knowledge is wasted and cannot \nbe reused if we cannot search it. The challenge is to ef.ciently discover similar bugs. To answer this \nchallenge, this paper employs traces to precisely capture the semantics of a bug. Informally, an execution \ntrace is the sequence of operations a program performs in response to input. Traces capture an abstraction \nof a program s input/output behavior. A bug can be viewed as behavior that violates a program s intended \nbehavior. Often, these violations leave a footprint, a manifestation of anomalous behavior (Engler et \nal. [5]) in a program s stack or execution trace. This paper introduces novel tool and language support \nto help a programmer accurately and ef.ciently identify similar bugs. To this end, we developed OSCILLOSCOPE, \nan Eclipse plugin, for .nding similar bugs and its supporting infras\u00adtructure. At the heart of this infrastructure \nis our bug query language (BQL), a .exible query language that can express a wide variety of queries \nover traces. The OSCILLOSCOPE in\u00adfrastructure consists of 1) a trace collector, 2) a trace-indexed bug \ndatabase, 3) BQL, 4) a query engine for BQL, and 5) web and Eclipse user interfaces. OSCILLOSCOPE is \nopen and includes the necessary tool support to facilitate developer involvement and contribution. The \nOSCILLOSCOPE database contains and supports queries over both stack and execution traces. Stack traces \nare less precise but cheaper to harvest than execution traces. We quantify this precision trade-off in \nSection 4.4. When avail\u00ad able, stack traces can be very effective, especially when they capture the program \npoint at which a bug occurred [3, 28]. Indeed, a common practice when bug-.xing is to paste the error \ninto a search engine, like Google. Usually, the error gen\u00aderates an exception stack trace. Anecdotally, \nthis tactic works surprisingly well, especially with the errors that novices make when learning an API. \nOSCILLOSCOPE generalizes and automates this practice, making systematic use of both execution and stack \ntraces. To validate our hypothesis that unique bugs are rare, we collected 877 bugs from the Mozilla \nRhino and Apache Commons projects. We gave each of these bugs to OSCILLO-SCOPE to search for similar \nbugs. We manually veri.ed each candidate pair of similar bugs that OSCILLOSCOPE reported. If we were \nunable to determine, within 10 minutes, that a pair of bugs was similar, i.e. knowing one bug a programmer \ncould easily .x the other, we conservatively deemed them dissim\u00adilar. Using this procedure, we found \nsimilar bugs comprise a substantial portion of these bugs, even against our initial database: 273 877 \n 31%. OSCILLOSCOPE .nds duplicate bug reports as a special case of its search for similar bugs; while \nduplicates comprised 74 of the 273 similar bugs, however, the majority are nontrivially similar. These \nbugs are .eld bugs, not caught during development or testing, and therefore less likely to be similar, \na fact that strengthens our hypothesis. When querying unresolved bugs against resolved bugs in the Rhino \nproject, OSCILLOSCOPE matches similar bugs effec\u00adtively, using information retrieval metrics we precisely \nde.ne in Section 4.1. Of the similar bugs OSCILLOSCOPE returns, 48 of the could have been immediately \nused to .x open bugs. Finding bug reports similar to an open, unresolved bug promises tremendous practical \nimpact: it could reuse the knowledge of the community to speed debugging. Linus Law states given enough \neyeballs, all bugs are shallow. An effective solution to the bug similarity problem will help developers \nexploit this precept by allowing them to reuse the eyes and minds behind past bug .xes. OSCILLOSCOPE \nhas been designed and developed to this end. We make the following main contributions: We articulate \nand elaborate the vision that most bugs are similar to bugs that have already been solved and take the \n.rst steps toward a practical tool built on traces that validates and shows the promise of this vision; \n We present OSCILLOSCOPE a tool that uses traces to  1 DynaBean myBean = new LazyDynaBean(); 2 myBean.set(\"myDynaKey\", \nnull); 3 Object o = myBean.get(\"myDynaKey\"); 4 if ( o == null ) 5 System.out.println( 6 \"Expected result.\" \n7 ); 8 else 9 System.out.println( 10 \"What actually prints.\" 11 ); Figure 1: When a key is explicitly \nbound to null, LazyDynaBean does not return null. .nd similar bugs and reuse their debugging knowledge \nto speed debugging; We have developed an open infrastructure for OSCILLO-SCOPE, available at http://bql.cs.ucdavis.edu, \ncom\u00ad prising trace collection, a trace-indexed bug database, the bug query language BQL, a Hadoop-based \nquery engine, and web-based and Eclipse plugin user interfaces; and  We demonstrate the utility and \npracticality of our approach via the collection and study of bugs from the Apache Commons and Mozilla \nRhino projects.  2. Illustrating Example Programmers must often work with unfamiliar APIs, some\u00adtimes \nunder the pressure of a deadline. When this happens, a programmer can misuse the API and trigger cryptic \nerrors. The following section describes a real example. Java programmers use getter and setter methods \nto interact with Java beans. To handle dynamic beans whose .eld names may not be statically known, Java \nprovides Reflection and Introspection APIs. Because these APIs are hard to un\u00adderstand and use, the Apache \nBeanUtils project provides wrappers for them. BeanUtils allows a programmer to in\u00adstantiate a LazyDynaBean \nto set and get value lazily without statically knowing the property name of a Java bean, as on line 2 \nof Figure 1. When an inexperienced programmer used LazyDynaBean in his project, he found, to his surprise, \nthat, even though he had explicitly set a property to null, when he later retrieved the value from the \nproperty, it was not null. Figure 1 shows sample code that exhibits this bug: executing it prints What \nactually prints. , not Expected result. . Since this behavior was quite surprising to him, the pro\u00adgrammer \n.led bug BeanUtils-342 on March 21, 2009. Five months later, a developer replied, stating that the observed \nbehavior is the intended behavior. In Figure 2, LazyDynaBean s get method consults the internal map values \non line 9. If the result is null, the get method .rst calls the method createOtherProperty, which by \ndefault calls createProperty to instantiate and return an empty ob\u00adject. In the parameter list of createOtherProperty, \nget calls getDynaProperty, which returns Object.class on a name Figure 2: LazyDynaBean.get(String name) \nfrom revision r295107, Wed Oct 5 20:35:31 2005. 1 public Object get(String name) { 2 if (name == null) \n{ 3 throw new IllegalArgumentException( 4 \"No property name specified\" 5 ); 6 } 7 8 // Value found 9 \nObject value = values.get(name); 10 if (value != null) { 11 return value; 12 } 13 14 // Property doesn \nt exist 15 value = createProperty( 16 name, 17 dynaClass.getDynaProperty( 18 name 19 ).getType() 20 ); \n21 22 if (value != null) { 23 set(name, value); 24 } 25 26 return value; 27 }  Figure 3: OSCILLOSCOPE \nreturns bug reports similar to BeanUtils-342. Figure 4: Snapshot of the bug report for BeanUtils-24. \nbound to null. He did, however, suggest a workaround: sub\u00adclass LazyDynaBean and override its createOtherProperty \nmethod to return null when passed Object.class as its type parameter. This in turn would cause LazyDynaBean.get() \nto return null at line 26, the desired behavior. How could OSCILLOSCOPE have helped the programmer solve \nthis problem? Assuming the OSCILLOSCOPE database had been populated with traces from the BeanUtils project, \na programmer would use OSCILLOSCOPE to look for bugs whose traces are similar to the trace for her bug, \nthen return their bug reports. Then she would examine those bug reports to look for clues to help her \nunderstand and .x her bug. Ideally, she would .nd a .x that she could adapt to her bug. Bug BeanUtils-342 \nis the actual bug whose essential behavior Figure 1 depicts. To use OSCILLOSCOPE to search for bugs similar \nto BeanUtils-342, a developer can .rst issue a prede.ned query. When a developer does not yet know much \nabout their current bug, a prede.ned query that we have found to be particulary effective is the suf.x \nquery . This query deems two bugs to be similar when the suf.xes of their traces can be rewritten to \nbe the same; its effectiveness is due to the fact that many bugs terminate a program soon after they \noccur. When a developer speci.es the suf.x length and edit distance and issues the suf.x query to search \nfor bugs similar to BeanUtils-342, OSCILLOSCOPE returns the bug reports in Figure 3. The .rst entry is \nBeanUtils-24, where the get method of LazyDynaBean did not return null even when the property was explicitly \nset to null. OSCILLOSCOPE executes the suf.x query by comput\u00ading the edit distance of the suf.x of BeanUtils-342 \ns trace against the suf.x of each trace in its database. Here is the tail of the method call traces of \nBeanUtils-342 and BeanUtils-24, the closest bug OSCILLOSCOPE found: BeanUtils-342 BeanUtils-24 ... ... \nLazyDynaBean set BasicDynaClass setProperties LazyDynaBean isDynaProperty DynaProperty getName LazyDynaClass \nisDynaProperty DynaProperty getName LazyDynaClass getDynaProperty LazyDynaBean isDynaProperty DynaProperty \ngetType LazyDynaClass isDynaProperty LazyDynaBean createProperty LazyDynaClass getDynaProperty LazyDynaBean \ncreateOtherProperty LazyDynaBean get LazyDynaBean set LazyDynaBean createProperty DynaProperty getType \nLazyDynaBean set LazyDynaBean class$ DynaProperty getType Each method call in these two traces is an \nevent; informally, OSCILLOSCOPE looks to match events, in order, across the two traces. Here, it matches \nthe two calls to isDynaProperty followed by getDynaProperty, then the calls to get and set. Intuitively, \nthe distance between these two traces is the num\u00adber of method calls one would have to change to make \nthe traces identical. Figure 4 is the snapshot of the bug report of BeanUtils-24. The same developer \nwho replied to BeanUtils-342 had also replied to BeanUtils-24 four years earlier. From his .x to BeanUtils-24, \nthe .x for BeanUtils-342 is immediate. With the help of OSCILLOSCOPE, the programmer could have solved \nthe bug in minutes, instead of possibly waiting .ve months for the answer. This example shows how OSCILLO-SCOPE \ncan help a programmer .nd and reuse the knowledge embodied in a bug report to .x an open bug. 3. Design \nand Realization of OSCILLOSCOPE This section introduces the key components of OSCILLO-SCOPE: its user-level \nsupport for trace-based search for sim\u00adilar bugs, its bug query language, and core technical issues we \novercame to implement it. 3.1 User-Level Support To support trace-based search for similar bugs, OSCILLO-SCOPE \nmust harvest traces, allow users to de.ne bug similar\u00adity either by selecting prede.ned queries or by \nwriting cus\u00adtom queries, process those queries to search a trace-indexed database of bug reports, display \nthe results, and present a user interface that makes this functionality easy to use. Figure 5 de\u00ad picts \nthe architecture of OSCILLOSCOPE that supports these tasks. Eclipse Plugin Most developers rely on an \nintegrated de\u00advelopment environment (IDE); to integrate OSCILLOSCOPE smoothly into the typical developer \ns tool chain and work.ow, especially to complement the traditional debugging process, we built OSCILLOSCOPE \nas an Eclipse plugin. OSCILLO-SCOPE also supports a web-based user interface, described in a tool demo \n[9]. OSCILLOSCOPE automates the instrumentation of buggy programs and the uploading of the resulting \ntraces. When a developer who is using OSCILLOSCOPE encounters a bug and wants to .nd the bug reports \nsimilar to her current bug, she tells OSCILLOSCOPE to instrument the buggy code, then re-triggers the \nbug. OSCILLOSCOPE automatically uploads the resulting trace to its trace-indexed database of bug reports. \nPrede.ned Queries OSCILLOSCOPE is equipped with prede.ned queries; in practice, users need only select \na query and specify that query s parameters, such as a regular expression over method names or an edit \ndistance bound, i.e. a measure of the cost of writing one trace into another which Section 3.2.2 describes \nin detail. Since bugs often cause a program to exit quickly, we have found that the suf.x query, (introduced \nin Section 2) which compares short suf.xes of traces with a modest edit distance bound, to be quite effective. \nSection 4.1 describes how we discovered and validated this suf.x query. The bulk of OSCILLOSCOPE s prede.ned \nqueries, like the suf.x query, have simple, natural semantics. Once the buggy trace has been uploaded, \nthe developer can allow OSCILLOSCOPE to automatically use the last selected query to search for similar \nbugs and return their bug reports, or select a query herself. OSCILLOSCOPE s query engine, which is based \non Hadoop, performs the search. When a JUnit test fails, these steps occur automatically in the background: \nOSCILLOSCOPE instruments and reruns the test, uploads the resulting test to the database, then, by default, \nissues a prede.ned suf.x query to search for similar bugs and returns the results. This feature is especially \nvaluable as it targets bugs that occur during development and are more likely to be similar to other \nbugs than .eld bugs which have evaded regression testing, inspection and analysis to escape into deployment. \nTo speed response time, OSCILLOSCOPE returns partial results as soon as they are available. Users refresh \nto see newer results. OSCILLOSCOPE can also visually compare traces to help developers explore and understand \nthe differences and similarities between two traces. To .nd the bug report with the .x to the bug in \nour illustrating example in Section 2, the developer would direct OSCILLOSCOPE to instrument the buggy \ncode, re-triggered the bug, then issued the default suf.x query. Behind the scenes, OSCILLOSCOPE would \nharvest and upload the trace, then execute the query and display the resolved bug report with the relevant \n.x. Trace Granularity Using the OSCILLOSCOPE, program\u00admers can trace code at statement or method granularity, \nas show in Figure 6. Statement-granular traces allow OSCIL-LOSCOPE to support the search for local bugs, \nbugs whose behavior is contained within a single method, and facilitates matching bugs across different \nprojects that run on the same platform and therefore use the same instruction set archi\u00adtecture. OSCILLOSCOPE \nsupports coarser granularity traces, such as replacing a contiguous sequence of method call on a single \nclass with that class name, by post-processing. Chop Points Instrumentation is expensive, so we pro\u00advide \nOSCILLOSCOPE provides a chop operator that allows programmers to set a chop point, a program point at \nwhich tracing starts or ends. A programmer can also use chop points to change the granularity of tracing \nfrom method calls to statements. By restricting tracing to occur within a pair of chop points, chopping \nenables the scalability of .ne-grained, i.e. statement-granular, tracing. When debugging with OS-CILLOSCOPE, \na developer will resort to setting chop points to collect a partial trace for their current bug when \ncollect\u00ading a complete trace is infeasible. Knowing where to place chop points involves guesswork; its \npayoff is the ability to query the OSCILLOSCOPE database. Thus, the OSCILLO-SCOPE database contains partial \ntraces and traces that may contain events at different levels of granularity. In a world in which OSCILLOSCOPE \nhas taken hold, devel\u00adopers will routinely upload traces when they encounter a bug. To this end, we have \nsetup an open bug database, available at our website, and welcome developers to contribute both the buggy \ntraces and the knowledge they acquired .xing the bugs that caused them. Our tool is not just for students \nand open source developers. We have made the entire OSCILLOSCOPE framework privately deployable, so that \ncompanies can use OSCILLOSCOPE internally without having to share their bug database and worry about \nleaking con.dential information. We have posited that, when two bugs may share a root cause, this fact \nmanifests itself as a similarity in their traces. Prior to this region of similarity, of course, the \ntwo traces might be arbitrarily different. Internally, OSCILLOSCOPE relies on BQL, its bug query language, \nand the insight and ingenuity of query writers to write BQL queries that isolate the essence of a bug; \nthese queries de.ne bug similarity  Figure 5: Debugging with the OSCILLOSCOPE Framework. Figure 6: \nSelecting execution trace granularity in the OSCIL-LOSCOPE Eclipse plugin. and drive OSCILLOSCOPE s search \nfor similar bugs. Thus, OSCILLOSCOPE rests on BQL, which we describe next.  3.2 BQL: A Bug Query Language \nA bug is behavior that violates a user s requirements. The core dif.culty of de.ning a bug more precisely \nis that different users, even running the same application, may have different requirements that change \nover time. In short, one user s bug can be another s feature. To tackle this problem, we have embraced \nexpressivity as the central design principle of BQL; it allows writing of queries that embody different \nde.nitions of buggy behavior. To support such queries, our database must contain program behaviors. Three \nways to capture behavior are execution traces, stack traces, and vector summarizations of execution traces. \nAn execution trace is a sequence of events that a program emits during execution and precisely captures \nthe behavior of a program on an input. Collecting execution traces is expensive and they tend to be long, \nmostly containing events irrelevant to a bug. Stack traces encode execution events keeping only the current \npath from program entry to where a bug occurs. Finally, one can summarize an execution trace into a vector \nwhose components are a .xed permutation of a program s method calls. For example, the summarization of \ntrace ABA into a vector whose components are ordered alphabetically is (2,1), which discards the order \nof events. In Section 4.4 we quantify the loss of precision and recall these approaches entail. Therefore, \nwe de.ne bug similarity and support queries for bug data, such as a .x, in terms of execution traces. \n 3.2.1 Terminology T denotes the set of all traces that a set of programs can generate. Each trace in \nT captures an execution of a program (We discuss our data model in more detail in Section 3.2.3). B is \nthe set of all bugs. The bug b . B includes the af.icted program, those inputs that trigger a bug and \nthe traces they induce, the reporter, dates, severity, developer commentary, and, ideally, the bug s \n.x. The function t : B . 2T returns the set of traces that trigger b. The set of all unresolved bugs \nU and the set of all resolved bugs R partition B. We formalize the ideal bug similarity in the oracle \nf. For b0,b1 . B, T if b0 is similar to b1 wrt f similar(b0, b1)=(1) F otherwise which we use to de.ne, \nfor b . B, [b] = {x . B | similar(b,x)}, (2) the set of all bugs that are, in fact, similar to b. For \nb0,b1 . B, and the query processing engine Q, T if b0 is similar to b1 wrt Q match(b0,b1)=(3) F otherwise \n which we use to de.ne [b]= {x . B | match(b,x)}, (4) the set of all bugs that the query for b returns. \nIdeally, we would like [b] =[b], but for debugging it suf\u00ad.ces that we are 1) sound: match(bi,bj) . similar(bi,bj), \nand 2) relatively complete: similar(bi,bj) ..bk . [bj] - {bi} match(bi,bk), for any bi = bj. In Section \n4, we demon\u00ad strate the extent to which we use traces to achieve these goals. Both [b] and [b] are re.exive: \n.b . B, b . [b] . b . [b], which means that {b}. [b]n[b] 0. We are often interested = /in bugs similar \nto b other than b itself, so we also de.ne Ub =[b] nU -{b} unresolved bugs that match b (5) Rb =[b] n \nR -{b} resolved bugs that match b. (6)  (query) ::= SELECT (bug)+[ FROM (db)+] WHERE (cond) [DISTANCE \n(distance)] (db) ::= X | ALL (cond) ::= (cond) &#38;&#38; (cond)|(cond) || (cond)| ( (cond) ) | INTERSECT?((bug), \n(pat)[, d][,n]) | JACCARD?((bug), (pat),t[, d]) | SUBSET?((bug),(pat)[,d]) (bug) ::= Traces | [ len ](bug)|(bug)[ \nlen ] | PROJ((bug),S) (pat) ::= s |(bug)|(pat) | (pat)|(pat)* | ( (pat) ) Figure 7: The syntax of BQL: \nX is a project; for the bug b, Traces = t(b); s is an event; and S is a set of events. 3.2.2 The syntax \nof BQL Figure 7 de.nes the syntax of BQL, which is modeled after the standard query language SQL. The \nquery SELECT b FROM ALL WHERE INTERSECT?(b, \"getKeySet\") returns all bugs whose traces have a nonempty \nintersection with the set of all traces that call the getKeySet method. The clause FROM Project1, Project2 \nrestricts a query to bugs in Project1 or Project2. The terminal ALL removes this restriction, and is \nthe default when the FROM clause is omitted. Predicates In addition to the standard Boolean operators, \nBQL provides SUBSET?, INTERSECT?, and JACCARD? predi\u00adcates to allow a programmer to match a bug with \nthose bugs whose traces match the pattern, when the target bug s traces are a subset of, have a nonempty \nintersection with, or overlap with those traces. For example, SELECT b FROM ALL WHERE SUBSET?(b,b527) \nreturns those bugs whose traces are a subset of b527 s traces. Traces may differ in numerous ways irrelevant \nto the semantics of a bug. For example, two traces may have taken different paths to a buggy program \npoint or events with the same semantics may have different names. Concrete execution traces can therefore \nobscure semantic similarity, both cross-project and even within project. As a .rst step toward combating \nthe false negatives this can cause, BQL allows two traces to differ in Levenshtein edit distance within \na bound. The Levenshtein distance of two strings is the minimum number of substitutions, deletions and \ninsertions of a single character needed to rewrite one string into another. The Levenshtein distance \nof 011 and 00 is 2 (011 . 01 . 00). The application of edit distance to a bug s traces generates a larger \nset of strings. Thus, BQL adds the distance param\u00adeter d to its set predicates to bound the edit distance \nused to produce traces during a similarity search. Edit distance relaxes matching and can introduce false \npositives. To com\u00adbat this source of imprecision, the INTERSECT? operator also takes n, an optional constraint \nthat speci.es the minimum number of a bug s traces that must be rewritten into one of the target bug \ns traces. For example, assume b527 contains multiple traces that trigger an assertion failure and a program\u00admer \nwants to search for other bugs with multiple traces. The program could use the predicate INTERSECT?(b, \n50, b527, 3), which forms the set of pairs of traces t(b) \u00d7 t(b527 ) and is true if the members of at \nleast 3 of these pairs can be rewritten into one another using 50 edits. Operators When we know enough \nabout the problem domain or salient features of our bug, we may wish to restrict where traces match. \nThe terminal behavior of a buggy program, embodied in the suf.x of its execution trace, often captures \na bug s essential features. Or we may wish to consider only those traces in which the application initialized \nin a certain fashion and restrict attention to pre.xes. Thus, BQL provides pre.x and suf.x operators. \nThese operators use array bracket notation and return the speci.ed length pre.x or suf.x. A programmer \nmay wish to project only a subset of events in a trace. For example, when searching for bugs similar \nto b527, a developer may want to drop methods in the log package to reduce noise. To accomplish this \ntask, he writes SELECT bug FROM ALL WHERE SUBSET?( PROJ(bug, \"read,write,close\"), b527, 10 ) where \"read,write,close\" \nnames the only methods in the trace in which we are interested. Patterns The last line of Figure 7 de.nes \nthe BQL s pattern matching syntax. Here, the terminals are either (through the (bug) the rule), t(bug), \nthe set of traces that trigger a bug, or s . Sx, a symbol (i.e. event) in a trace. Patterns that mix \nsymbols from different event alphabets can succinctly express subsets of traces. For instance, a query \nwriter may wish to .nd bugs that traverse the class ca on the way to the method m1 in cb and then execute \nsome method in the class cc. The pattern cam1cc achieves this. As a concrete example, consider a developer \nwho wishes to .nd all traces that invoke methods in the class PropertyConfiguration (abbreviated PC in \nthe query below) before invoking the method escapeJava in StringEscapeUtils; this generates the query \nSELECT bug FROM Configuration WHERE INTERSECT?(bug, \"PC escapeJava\").  3.2.3 Semantics BQL rests on \na hierarchy of disjoint alphabets of execution events, shown in Figure 8. At the lowest level, execution \nevents are instructions. An instruction symbol encodes an opcode and possibly its operands; a method \nsymbol encodes a method s name and possibly its signature and parameters. Sequences of instructions de.ne \nstatements in the source language; sequences of statements de.ne basic blocks whose sequences de.ne a \npath through a method. Thus, a project  Figure 8: Hierarchy of trace event alphabets. [SELECT (b1,\u00b7\u00b7\u00b7 \n,bn) FROM (db1, \u00b7\u00b7\u00b7 ,dbm) WHERE cond]  = {(b1,\u00b7\u00b7\u00b7 , bn) | (b1,\u00b7\u00b7\u00b7 ,bn) . [cond] l n [dbi]} i.[1,m] \n[SELECT (b1,\u00b7\u00b7\u00b7 ,bn) FROM (db1, \u00b7\u00b7\u00b7 ,dbm) WHERE cond DISTANCE dist]  = {(b1,\u00b7\u00b7\u00b7 , bn) | (b1,\u00b7\u00b7\u00b7 ,bn) \n. [cond][dist] n [dbi]}i.[1,m] X if db = X . T [db] = T if db = ALL [dist] .{h, l,u} [cond1 &#38;&#38; \ncond2] = .d .[cond1]d . [cond2]d [cond1 || cond2] = .d .[cond1]d . [cond2]d [( cond )] = [cond] = .d \n.[cond]d Figure 9: Semantics of queries and Boolean operators. * [p] = L (p) . T [p ] = [p]* [p | p] \n= [p] . [p][( p )] = [p] Figure 10: Semantics of the pattern operators. de.nes a sequence of languages \nof traces de.ned over each alphabet in its hierarchy. Formally, a project de.nes a disjoint sequence \nof alphabets Si, i . Nwhere S1 is the instruction alphabet. Let L (P@S) denote the language of traces \nthe project P generates over the event alphabet S. Then, for the project P and its alphabets, each symbol \nin a higher level language de.nes a language in P s lower-level languages: .s . Si+1, L (s) . L (P@Si). \nTraces can mix symbols from alphabets of different abstrac\u00adtion levels, so long as there exists an instruction-level \ntransla\u00adtion of the trace t such that t . L (P@S1). The semantics of BQL is mostly standard. Figure 9 \nstraightforwardly de.nes queries and the standard Boolean operators. It de.nes [dist] as a distance function \nand uses lambda notation to propagate its binding to the BQL s set predicates. The semantics of patterns \nin Figure 10 are stan\u00ad [INTERSECT?(b, p)] = [b] n [p] = 0/ [INTERSECT?(b, p, d)] = [b] nd [p] = 0/ [INTERSECT?(b, \np, n)] = |[b] n [p]|= n [INTERSECT?(b, p,d, n)] = |[b] nd [p]|= n [JACCARD?(b, p,t)] = |[b] n [p]|= t \n|[b] . [p]| [JACCARD?(b, p,t, d)] = |[b] nd [p]|= t |[b] . [p]| [SUBSET?(b, p)] = [b] . [p] [SUBSET?(b, \np, d)] = .x . [b],.y . [p] d (x,y) = d Figure 11: Semantics of the intersect, Jaccard and subset predicates. \n[Traces] . 2T [PROJ(b,S)] = [b] S [[ len ]b] = {a |.\u00df a\u00df . [b] .|a| = len} [b[ len ]] = {\u00df |.a a\u00df . \n[b] .|\u00df | = len} Figure 12: Semantics of the trace operators. dard; the L , used to de.ne pattern semantics, \nis the classic language operator. Set Predicates and Edit Distance In Figure 9, [b] = [Traces] . 2T and \nthe edit distance function [dist] has signature S*\u00d7 S*. N. The set of allowed edit distance functions \nis {h,l,u}. In this set, h denotes Hamming, l denotes Levenshtein (the default) and u denotes a user\u00adspeci.ed \nedit distance function. BQL allows query writers to specify a distance function to give them control \nover the abstraction and cost of a query. For instance, a query writer may try a query using Hamming \ndistance. If the results are meager, he can retry the same query with Levenshtein distance, which matches \nmore divergent traces. For d .{h,l,u}, the function nd :2S*\u00d7 2S*\u00d7N. 2S* is X ndY = {z | z . X,.y . Y \nd(y,z) = d .z . Y,.x . X d(x,z) = d} (7) and constructs the set of all strings in X or Y that are within \nthe speci.ed edit distance of an element of the other set. We use nd to de.ne JACCARD? and INTERSECT? \nin Figure 11. For JACCARD?, the Jaccard similarity must meet or exceed t . [0,1]; for INTERSECT?, the \ncardinality of the set formed by nd must meet or exceed n . N. A user-de.ned distance function u may \nbe written in any language so long as it matches the required signature. One could de.ne distance metric \nthat reduces a pair of method traces to sets of methods then measure the distance of those sets in terms \nof the bags of words extracted from the method names, identi.ers or comments. Alternatively, one could \nde.ne a Jaccard measure over the sets of methods or classes induced by two traces, scaling the result \ninto N. Trace Operators To specify the length of pre.xes and suf.xes in Figure 12, we use len . N. In \nthe de.nition of PROJ, proji is the projection map from set theory and S . Sx, i.e. S is a subset of \nsymbols from one of the event abstraction alphabets. BQL s concrete syntax supports regular expressions \nas syntactic sugar for specifying S.  3.3 Implementation Four modules comprise OSCILLOSCOPE: a bytecode \ninstru\u00admentation module, a trace-indexed database, a query pro\u00adcessing engine, and two user interfaces. \nThe instrumentation module inserts recording statements into bytecode. The in\u00adstrumentation module is \nbuilt on the ASM Java bytecode manipulation and analysis framework. For ease of smooth interaction with \nexisting work.ows, our database has two forms: a standalone database built on Hadoop s .le system and \nan trace-based index to URLs that point into an existing Bugzilla database. The OSCILLOSCOPE plug-in \nfor Eclipse supports graphically comparing traces. A challenge we faced, and partially overcame, is that \nof allowing the comparison of traces visually regardless of their length. An interesting challenge that \nremains is to allow a user to write, and re.ne, queries visually by clicking on and selecting portions \nof a displayed trace. The web-based UI is AJAX-based and used Google s GWT. Internally, traces are strings \nwith the syntax (Trace) ::= (Event)|(Event)(Trace) (Event) ::= (Method)|(Instruction) (Method) ::= M \nFQClassName MethodName Signature | S FQClassName MethodName Signature (Instruction) ::= I OPCODE (Operands) \n(Operands) ::= OPERAND | OPERAND (Operands) An example method event follows M org/apache/commons/beanutils/LazyDynaClass \n\\ getDynaProperty (Ljava/lang/String;) \\ Lorg/apache/commons/beanutils/DynaProperty;. Here, M denotes \nan instance method (while S in the syntax denotes a static method). The fully quali.ed class name follows \nit, then the method name, and .nally the method signature. To capture method events, we inject logging \ninto each method s entry. For statements, we inject logging into basic blocks. To produce coarser-grained \ntraces, we post\u00adprocess method-level traces to replace contiguous blocks of methods in a single class \nor package with the name of the class or package. Query Engine The overhead of query processing lies \nin two places: retrieving traces and comparing them against the target trace. For trace comparison, we \nimplemented an optimized Levenshtein distance algorithm [10]. To scale to large databases (containing \nmillions of traces), OSCILLO-SCOPE s query engine is built on top of Apache Hadoop, a framework that \nallows for the distributed processing of large data sets across clusters of computers. The essence of \nHadoop is MapReduce, inspired by the map and reduce func\u00adtions commonly used in functional programming. \nIt enables the processing of highly distributable problems across huge datasets (petabytes of data) using \na large number of com\u00adputers. The map step divides an application s input into smaller sub-problems and \ndistributes them across clusters and reduce step collects the answers to all the sub-problems and combines \nthem to form the output. OSCILLOSCOPE s query processing is an ideal case for MapReduce, since it compares \nall traces against the target trace. This comparison is embarrassingly parallelizable: it can be divided \ninto sub-problems of comparing each trace in isolation against the target trace. Each mapper processes \na single comparison and the reduce step collects those bug identi.ers bound to traces within the edit \ndistances bound to form the .nal result. Section 4 discusses the stress testing we performed for OSCILLOSCOPE \nagainst millions of traces. 3.4 Extending OSCILLOSCOPE with New Queries OSCILLOSCOPE depends on experts \nto customize its pre\u00adde.ned queries for a particular project. These experts will use BQL and its operators \nto write queries that extract trace subsequences that capture the essence of a bug. Learning BQL itself \nshould not be much of a hindrance to these ex\u00adperts, due to its syntax similarity to SQL and its reliance \non familiar regular expressions. To further ease the task of writing queries, OSCILLOSCOPE visualizes \nthe difference of two traces returned by a search and supports iterative query re.nement by allowing \nthe query writer to edit the history of queries he issued. To write effective queries, an expert will, \nof course, need to know her problem domain and relevant bug features; she will have to form hypotheses \nand, at times, resort to trial and error. The payoff for a query writer, and especially for an organization \nusing OSCILLOSCOPE, is that, once written, queries can be used over and over again to .nd and .x recurring \nbugs. Across different versions of a project, even though methods may change names as a project evolves, \nOSCILLOSCOPE can still .nd similar bugs if their signature in a trace is suf.ciently localized and enough \nsignposts, such as method names, remain unchanged so that edit distance can overcome the distance created \nby those that have changed. Single Regex Queries Configuration-323 occurred when DefaultConfigurationBuilder \nmisinterpreted property val\u00adues as lists while parsing con.guration .les. The reporter speculated that \nthe invocation of ConfigurationUtils.copy() during internal processing was the cause. To search for bugs \nin the Configuration project that invoke the copy() method in the ConfigurationUtils class, the reporter \ncould have issued SELECT bug FROM Configuration WHERE SUBSET?(bug, \"ConfigurationUtils.copy()\").  The \nresult set contains 272 and 283, in addition to 323. Developers acknowledged the problem and provided \na workaround. Thus, the bug 323 can be solved identifying and studying 272 and 283. These bugs predate \n323. Here, OSCILLOSCOPE found usefully similar bugs using a query based on a simple regular expression \nthat matched a single method call. Lang-421 is another example of a bug for which a simple query parameterized \non a regular expression over method names would have sped its resolution. In this bug, the method escapeJava() \nin the StringEscapeUtils class incorrectly escaped the / character, a valid character in Java. In Apache \nCommons, the Configuration project depends on Lang. To .nd out similar bugs in the Configuration project, \nwe set a = StringEscapeUtils.escapeJava and issue the query SELECT b FROM ALL WHERE INTERSECT?( PROJ(b, \norg/apache/commons/lang/*), a ) to search for all bugs in the database that match pat. This query returns \nfour bugs. It returns Lang-421, the bug that motivated our search. The second bug is Configuration-408, \nwhere forward slashes were escaped when a URL was saved as a property name. Studying the description, \nwe con.rmed that StringEscapeUtils.escapeJava() caused the problem. The third bug, Configuration-272, \nconcerns incorrectly escaping the , character; the class StringEscapeUtils still exhibits this problem. \nManually examining the last bug, Lang-473, con.rms that it duplicates Lang-421. It is our experience \nwith bugs like these that led us to add the simple regex query to our suite of prede.ned queries. 3.4.1 \nLimitations Overcoming instrumentation overhead is an ongoing chal\u00adlenge for OSCILLOSCOPE. For example, \nstress-testing Find-Bugs revealed .ve-fold slowdown. Our .rst, and most impor\u00adtant, countermeasure is \nour chop operator, described above in Section 3.1. In our experience, statement-granular tracing would \nbe infeasible without it. Longer term, we plan to em\u00adploy Larus technique to judiciously place chop points \n[18]. Another direction for reducing overhead is to use sampling, then trace reconstruction, as in Cooperative \nDebugging [19]; the effectiveness of this approach depends, of course, on OSCILLOSCOPE garnering enough \nparticipation to reliably acquire enough samples to actually reconstruct traces. Cur\u00adrently, OSCILLOSCOPE \nhandles only sequential programs. To handle concurrent programs, we will need to add thread identi.ers \nto traces and explore the use of vector distance on interleaved traces. 4. Evaluation This evaluation \nshows that OSCILLOSCOPE does .nd similar bugs and, in so doing, .nds a generally useful class of suf.x\u00adbased \nqueries. It measures how OSCILLOSCOPE scales and demonstrates the accuracy of basing search on execution \ntraces. To evaluate OSCILLOSCOPE, we collected method-level traces and studied bugs reported against \nthe Apache Commons (2005 2010): comprising 624153 LOC and 379163 lines of comment, and Rhino (2001 2010): \ncomprising 205775 LOC and 34741 lines of comment projects. We chose these projects because of their popularity. \nIn most cases, reporters failed to provide a test case to reproduce the bug. Even with a test case, recompiling \nan old version and reproducing the bug was a manual task that consumed 5 minutes on average. This explains \nwhy OSCILLOSCOPE s trace database contains 656 of the 2390 bugs reported against Apache Commons and 221 \nof the 942 bugs reported against Rhino. For each bug, we recorded related information from the bug tracking \nsystem such as source code, the .x (if available), and developer comments. Our database currently contains \n877 traces (one trace per bug) and its size is 43.1 MB. The minimum, maximum, mean, and variance of the \ntrace lengths in the database are 2, 50012, 5431.1, and6.68 \u00d7 107. Experimental Procedure In general, \nwe do not know [b], those bugs that are actually similar to each other (Equation 2). We manually approximated \n[b] from [b], an OSCILLOSCOPE result set, in two ways. First, we checked whether two bugs shared a common \nerror-triggering point, the program point at which a bug .rst causes the program to violate its speci.cation. \nWe studied every candidate pair of bugs that OSCILLOSCOPE reported to be similar for at most 10 minutes. \nFor example, we deemed Rhino-217951 and 217965 to be similar because a Number.toFixed() failure triggered \neach bug. Second, we recorded as similar any bugs identi.ed as such by a project s developers. For example, \na Rhino developer commented in Rhino-443590 s report that it looks like Rhino-359651. Given our limited \ntime and knowledge of the projects, we often could not determine whether two bugs are, in fact, similar. \nWhen we could not determine similarity, we conservatively deemed the bugs dissimilar. This procedure \ndiscovers false positives, not false negatives. To account for false negative, we introduce the relative \nrecall measure in Section 4.1 next. We discuss our methodology s construct validity in Section 4.5. Using \nthis experimental procedure, we found that similar bugs comprise a substantial portion of bugs we have \ncollected to date: 273 877 31%. 74 of the 273 similar bugs are identical, caused by duplicate bug reports; \nthe majority, however, are nontrivially similar. Since we conjecture that the number of ways that humans \nintroduce errors is .nite and our database contains .eld bugs, we expect the proportion of similar bugs \nto increase as our database grows. 4.1 Can OSCILLOSCOPE Find Similar Bugs? To show that OSCILLOSCOPE \naccurately .nds similar bugs and to validate the utility of a default query distributed with OSCILLOSCOPE, \nwe investigate the precision and recall of suf.x queries issued against the 221 bug traces we harvested \nfrom the Rhino project. We queried OSCILLOSCOPE with 48 unresolved Rhino bugs. We found similar bugs \nfor 14 of these In our context, we have bugs, under our experimental procedure. When computing the measures \nbelow, we used this result as our oracle for [b]. precision = |[b] n [b]| recall = |[b] n [b]|. (10) \n |[b]| When we do not deeply understand a bug, matching trace |[b]| suf.xes is a natural way to search \nfor bugs, since many bugs To restrict precision to R, we de.ne cause termination. This insight underlies \nthe suf.x query we .rst introduced in Section 2. For suf.xes of length len, the suf.x query is SELECT \nbug FROM Rhino WHERE SUBSET?(tbug[len], bug[len], distance). Two parameters control suf.x comparison: \nlength and edit distance. We conducted two experiments to show how these parameters impact the search \nfor similar bugs. In the .rst experiment, we .x the suf.x length at 50 and increase the allowed Levenshtein \ndistance. In the second experiment, we .x the Levenshtein distance to 10 and vary the suf.x length. Table \n1 depicts the results of the .rst experiment, Table 2 those of the second. We report the data in Table \n2 in descending suf.x length to align the data in both tables in order of increasing match leniency. \nIn both experiments, we are interested in the number of queries for unresolved bugs that match a resolved \nbug, as these might help a developer .x the bug. Recall that R is the set of resolved bugs (Section 3) \nand Rb (Equation 6) is the set of resolved bugs similar to b. In the second column, we report how many \nunresolved bugs match any resolved bugs. For this purpose, we de.ne UR = {b . U | Rb = 0/}, then, in \ncolumn three, we report the percentage of unresolved bugs that match at least one resolved bug. To show \nthat our result sets are accurate, we compute their average size across all unresolved bugs as 1 if [b] \nn X = 0/ respr(X,b)=|([b]n[b])nX| (11) otherwise. |[b]nX| then, in column four, we report the average \nrespr 1 . respr(R,b) (12) |U| b.U which we manually compute via our experimental procedure. The majority, \n34 48, of the unresolved bugs in our Rhino data set are unique. These unique bugs dominate the average \nrespr at lower edit distance and longer suf.x length. When distance increases from 10 to 20, and suf.x \nlength drops from 50 to 25, the average respr drops dramatically. The reason is that Rhino, a JavaScript \ninterpreter, has a standard exit routine. When a program ends without runtime exceptions, Rhino calls \nfunctions to free resources. Most the Rhino traces end with this sequence which accounts for OSCILLOSCOPE \ns FPs and decreases the average respr. In general, both Table 1 and Table 2 show how the greater abstraction \ncomes at the cost of precision. Our experimental procedure is manual and may overstate recall because \nwe do not accurately account for FNs; ac\u00adcurately accounting for FNs would require examining all traces. \nRather than directly report recall (Equation 10), we over-approximate it with relative recall: relrecall(b)= \n.. . .|Rb| =|Rb|. (8) |U| 1 if b = {b} 1 0 (13) if [ b] n [b] -{b} = /0 otherwise, In the fourth column, \nwe report this average, then because R might grow to be very large, we report the average cardinality \nof |Rb| as a percentage of |R|. This is the average number of resolved bugs a developer would have to \nexamine for clues he might use to solve an unresolved bug. It is a proxy for developer effort. In this \nexperiment, even the two most lenient matches Levenshtein distance 30 and 25 length suf.xes do not \nburden a developer with a large number of potentially similar bugs. For Rhino, the maximum number of \nbugs returned by OSCILLOSCOPE for a bug is six. The effort to analyze each result set is further mitigated \nby the fact that OSCILLOSCOPE returns ranked result sets, in order of edit distance. Next, we report \nthe precision and recall of OSCILLO-SCOPE over R, the resolved bugs. When TP denotes true positives, \nFP denotes false positives, and FN denotes false which measures how often OSCILLOSCOPE returns useful \nresults. Relative recall scores one whenever a bug 1) is unique or 2) has at least one truly similar \nbug. Because .[b], b . [b] n [b], we need to test these two cases separately to distinguish between returning \nonly b when b is, in fact, unique in the database from returning b, but failing to return other, similar \nbugs when they exist. In contrast to precision, relative recall does not penalize the result set for \nFPs; in contrast to recall, it does not penalize the result set for FNs. In practice, relative recall \ncan be manually veri.ed, since we only need to .nd a counter-example in [b] to falsify the .rst condition \nand checking the second condition is restricted by |[b]|, whose maximum across our data set is 11. As \nwith precision, we restrict relative recall .. . 1 if [b] = {b} if ([b] n [b] -{b}) n X = / otherwise. \n negatives, recall that relrecall(X, b)= 1 0 TP TP precision = recall = . (9) (14) TP + FP TP + FN  \nDistance |UR| |UR||U| |Rb| |Rb||R| Average Respr(R,b) Average Relrec(R,b) Average F-score(R,b) 0 3 6.3% \n0.08 0.05% 0.98 0.75 0.85 10 12 25.0% 0.67 0.39% 0.93 0.94 0.93 20 18 37.5% 1.29 0.75% 0.82 0.96 0.88 \n30 31 64.6% 2.29 1.32% 0.56 0.98 0.71 Table 1: Measures of the utility of our approach as a function \nof increasing Levenshtein distance and .xed suf.x length 50, .b . U; UR is the subset of the unresolved \nbugs U for which OSCILLOSCOPE .nds a similar resolved bug; |UR| is the percentage |U|of unresolved bugs \nthat are similar to a resolved bug; |Rb| is the average number of resolved bugs returned for each unresolved \nbug b; since R will vary greatly in size, we report |Rb| , the size of each result set as a percentage \nof the resolved bugs; the |R| meaning of the measures Respr, Relrec and restricted F-score are de.ned \nin the text below. Suf.x Length |UR| |UR||U| |Rb| |Rb||R| Average Respr(R,b) Average Relrec(R,b) Average \nF-score(R, b) 200 2 4.2% 0.08 0.05% 0.97 0.73 0.83 100 5 10.4% 0.19 0.11% 0.97 0.79 0.87 50 12 25.0% \n0.67 0.39% 0.93 0.94 0.93 25 24 50.0% 1.83 1.06% 0.68 0.96 0.80 Table 2: Measures of the utility of \nour approach as a function of decreasing suf.x length and .xed distance threshold 10, .b . U; for a detailed \ndiscussion of the meaning of the .rst four columns, please refer to the caption of Table 1; the remaining \ncolumns are de.ned in the text below. and, in column .ve, we report its average, 1 . relrecall(R,b). \n(15) |U| b.U By de.nition two paths are different. At edit distance zero, a bug can only match itself, \npossibly returning duplicate bug reports. Many paths differ only in which branch they took in a conditional \nand thus OSCILLOSCOPE can match them even with a small edit distance budget, which accounts for the large \nrise in relative recall moving from an edit distance budget of 0 to 10. Minor differences accumulate \nwhen longer suf.xes of two traces are compared. The loss of irrelevant detail accounts for the large \nrise in relative recall moving from suf.x length 100 to 50. The F-score is the harmonic mean of precision \nand recall. Here we de.ne it using restricted precision and relative recall: 2 \u00b7 relrecall(X,b) \u00b7 respr(X, \nb) . (16) relrecall(X,b)+ respr(X,b) Column six in both tables reports this measure. Unique bugs dominate \nthe .rst Levenshtein measurement. At distance zero, OSCILLOSCOPE returned .ve bugs in total. Among the \n34 unique bugs, OSCILLOSCOPE found a similar bug for only one of them. At suf.xes of length 200, OSCILLOSCOPE \nreturned two bugs in total and both are true positives. The distance zero and suf.x length 200 queries \nare more precise, but also more susceptible to FNs, because they return so few bugs. Correctly identi.ed, \nunique bugs increase relative recall. With the increase of distance or decrease in suf.x length, OSCILLOSCOPE \n.nds more potentially similar bugs, at the cost of FPs. At distance 10 and suf.xes of length 50, OSCILLOSCOPE \nfound similar bugs for 11 of the non\u00adunique bugs, and returned only one FP for a unique bug. These tables \ndemonstrate the utility of OSCILLOSCOPE. Its query results often contain resolved bugs that may help \nsolve the open, target bug. They are small and precise and therefore unlikely to waste a developer s \ntime. The second most strict measurement, edit distance 10 in Table 1 and suf.x length 50 in Table 2 \nis the sweet spot where OSCILLOSCOPE is suf.ciently permissive to capture interesting bugs while not \nintroducing too many FP. The six FPs have similar test cases, but their errors are triggered at different \nprogram points. For instance, Rhino-567484 and 352346 have similar test cases that con\u00adstruct and print \nan XML tree. A problem with XML construction triggers 567484, while an error in XML.toXMLString() trig\u00adgers \n352346. Two of FN occurred because the distance thresh\u00adold was too low. At distance 20, OSCILLOSCOPE \nmatches these two bugs without introducing FPs. Logging calls, us\u00ading VMBridge, separate these two bugs \nfrom their peers and required the additional 10 edits to elide. We failed to .nd similar bugs for 496540 \nand 496540 because the calls made in Number.toFixed() changed extensively enough to disrupt trace similarity. \nTo reduce our FP and FN rates, we plan to investigate object-sensitive dynamic slicing and automatic \na-renaming which, given trace alignment, renames symbols, here method names, in order of their appearance. \nTo handle FNs like those caused by the interleaving of new calls such as logging, we intend to evaluate \ntrace embedding, i.e. de\u00adtermining whether one trace a subsequence of another, as an additional distance \nmetric.  4.2 How Useful are the Results? In this section, we show how OSCILLOSCOPE can help a programmer \n.x a bug by identifying bugs similar to that bug. To do so, we issue a suf.x query for each bug. The \nquery Suf.x-query SELECT bug FROM ALL WHERE SUBSET?(tbug[50], bug[50], 30) returns a distance-ranked \nresult set for tbug. We used trace suf.xes of length 50 because OSCILLOSCOPE performed best at this length \n(Section 4.1). We then manually examined the target bug and the bug in the result set with the smallest \ndis\u00adtance from the target bug, using our experimental procedure. As a degenerate case of bug similarity, \nOSCILLOSCOPE effectively .nds duplicate bug reports and helps keep a bug database clean. OSCILLOSCOPE \nreported 33 clusters of du\u00adplicate bug reports, 28 of which the project developers had themselves classi.ed \nas duplicates. We manually examined the remaining .ve and con.rmed that they are in fact dupli\u00adcates \nthat had been missed. We have reported these newly\u00addiscovered duplicates to the project maintainers. \nSo far, one of them, (JXPATH-128, 143), has been con.rmed and closed by the project maintainers. We have \nyet to receive con\u00ad.rmations on the other four: Configuration-30 and 256, Collections-100 and 128, BeanUtils-145 \nand 151, and Rhino-559012 and 573410. Next, we discuss a selection of interesting, similar bugs returned \nby Suf.x-query. OSCILLOSCOPE is particularly effective at .nding API misuse bugs. From the BeanUtils \nproject, Suf.x-query identi.ed as similar BeanUtils-42, 117, 332, 341, and 372, which all incorrectly \ntry to use BeanUtils in conjunction with non-public Java beans. Upon seeing a .x for one of these, even \na novice could .x the others. Configuration-94 and 222 describe bugs that happen when a new .le is created \nwithout checking for an existing .le. Configuration-94 occurs in AbstractFileConfiguration.save() and \n222 in the PropertiesConfiguration constructor. Their .xes share the same idea: check whether a .le exists \nbefore creating it. Lang-118 and 131 violate preconditions of StringEscapeUtils.unescapeHtml(). Lang-118 \nfound that StringEscapeUtils does not handle hex entities (pre.xed with 0x ), while 131 concerns empty \nentities. Both .xes check the input: seeing one, a developer would know to write the other. Lang-59 (resolved) \nand Lang-654 (unresolved) de\u00adscribe the problem that DataUtils.truncate does not work with daylight savings \ntime. Studying Lang-564, we found its cause was a partial (i.e. bad) .x of Lang-59. Developers of Lang \ncon.rmed this .nding. BeanUtils-115 is the problem that properties from a DynaBean are not copied to \na standard Java bean. In BeanUtils-119, NoSuchMethodException is thrown when setting value to a property \nwith name \"aRa\". The root cause for both bugs is the passing of a parameter that violates Java bean naming \nconventions: when the second character of a getter/setter method is uppercase, the .rst must also be. \nAgain, from a .x for either, the .x for the other is immediate.  In addition to identifying bug pairs \nsuch as those we just discussed, OSCILLOSCOPE ef.ciently clusters unresolved bugs. In Rhino, OSCILLOSCOPE \nclustered seven unresolved bugs 444935, 443590, 359651, 389278, 543663, 448443, and 369860. These bugs \nall describe problems with regular expressions. The project developers themselves deemed some of these \nbugs similar. Problems in processing XML trees causes Rhino-567484, 566181 and 566186. When processing \nstrings, Rhino throws a StackOverflowException in both Rhino-432254 and 567114. Although we found interesting, \nsimilar bugs comparing only suf.x traces, we also learned some of the de.ciencies of this strategy. When \na bug produces erroneous output and does not throw an exception, the location of the error is usually \nnot at the end of the execution trace. Suf.x queries produce FPs on such bugs, especially when they share \na pre.x be\u00adcause their test cases are similar. The bug Configuration-6, improper handling of empty values, \nand Configuration-75, the incorrect deletion of XML subelements, formed one such FP. Suf.x comparison \ncan miss similar bugs when a bug oc\u00adcurs in different contexts. The incorrect implementation of ConfigurationUtils.copy() \ncaused Configuration-272, 283 and 323. These three bugs differ only in the location of their call to \nthis buggy method. The suf.x operator alone failed to detect these bugs as similar, because the context \nof each call is quite different. Database Size Time to completion (s) (million) 1, 4GB 4CPU 1, 6GB 8CPU \n2, 10GB 12CPU 3, 14GB 16CPU 4, 16GB 20CPU 5, 144GB 38CPU 1 38.2 28.6 23.1 21.1 18.9 14.2 2 72.3 41.3 \n32.8 24.2 22.5 18.3 3 104.7 53.8 48.4 34.6 28.6 20.2 4 140.7 70.1 58.4 49.4 35.2 25.7 5 171.3 81.2 64.5 \n53.5 47.9 28.0 Table 3: Measures of OSCILLOSCOPE s query processing time against different sizes of \ndatabases with different Hadoop cluster settings; The Hadoop cluster setting x, yGB zCPU denotes a cluster \nwith x nodes, a total of y GB of memory, and z CPUs. 4.3 Scalability Our central hypothesis, that unique \nbugs are rare against the backdrop of all bugs, means that OSCILLOSCOPE will become ever more useful \nas its database grows, since it will become ever more likely to .nd bugs similar to a target bug. This \nraises performance concern: how much time will it take to process a query against millions of traces. \nTo gain insight into how the size of bug database and Hadoop cluster settings impact the query time, \nwe conducted a two-dimension stress testing. In the .rst dimension, we .x the database size and increase \nHadoop cluster resources. In the second dimension, we .x the Hadoop cluster and vary the database size. \nWe replicated our 877 traces to create a database with millions of traces. We used the Suf.x-query as \nthe candidate query in the experiment. For each setting, we issued Suf.x-query .ve times and computed \nthe average time to completion. Table 3 depicts the results. The .rst column lists the database size, \nwhich ranges from one to .ve million traces. The rest of the columns show the time to process the query \nfor each each cluster con.guration. The Hadoop cluster setting is expressed as the following: x, yGB \nzCPU denotes a cluster with x nodes, a total of y GB of memory, and z CPUs. The settings we used were \nCluster 1 1, 4GB 4CPU Cluster 2 1, 6GB 8CPU Cluster 3 2, 10GB 12CPU Cluster 4 3, 14GB 16CPU Cluster 5 \n4, 16GB 20CPU Cluster 6 5, 144GB 38CPU Figure 13 shows that the query processing times grow linearly \nwith the increase of database size for all the settings, the expected result which con.rms that all the \nnodes in the cluster were used. Figure 13 also shows that query times drop more dramatically for large \ndatabase sizes than for small ones. This is because the performance gain gradually overcomes the network \nlatency as the workload grows. Figure 13 clearly demonstrates that OSCILLOSCOPE s query processing will \ntake less time as a function of the nodes in its Hadoop cluster.  4.4 Execution Trace Search Accuracy \nExecution traces accurately capture program behavior, but are expensive to harvest and store. To show \nthis cost is worth paying, we compare the execution trace search accuracy against the accuracy of vector \nand stack trace searches. Vector Researchers have proposed a vector space model (VSM) to measure the \nsimilarity of execution traces [29]. To compare the VSM metric against OSCILLOSCOPE s use of edit distance \nover execution traces, we repeated the Rhino experiment in Section 4.1 using VSM. The VSM approach transforms \nexecution traces to vectors, then computes their distance1. In the experiment, we tuned two parameters, \ntrace length and distance threshold of the VSM algorithm to op\u00adtimize the respr and relrecall results. \nWe varied the trace lengths using suf.xes of length 25, 50, 100, 200, and un\u00adbounded. For the distance \nthreshold, we tried [0.5,1.0] in increments of 0.1. Suf.x length 50 and distance threshold  0.9 was \nthe sweet spot of the VSM metric where it achieved 0.85 respr and 0.90 relrecall, for an F-score of 0.87. \nAt suf.x of length 50 and edit distance 10, OSCILLOSCOPE outper\u00adforms VSM, with 0.93 respr and 0.94 relrecall \nfor and 0.93 F-score. We hypothesize the reason is that the VSM model does not consider the temporal \norder of method invocations. The search for similar bugs requires two operations, search and insertion. \nVSM requires the recomputation of the vector for every trace in the database whenever an uploaded trace \nintroduces a new method, but it can amortize this insertion cost across searches, which are relatively \nef.cient because they can make use of vector instructions. In contrast, insertion is free for OSCILLOSCOPE, \nbut search operations require edit distance computations. Are Execution Traces Necessary? The OSCILLOSCOPE \ndatabase contains both stack and execution traces. Execution traces are more precise than stack traces, \nbut more expensive to harvest. Stack traces are cheaper to collect, but not always available and less \nprecise. For example, a stack trace is usually not available when a program runs to completion but produces \nincorrect output; they are less precise because they may not capture the program point at which a bug \noccurred or when they do, they may capture too little of its calling context. Here, we quantify this \ndifference in the precision to shed light on when the effort to harvest execution traces might be justi.ed. \n1Users of OSCILLOSCOPE, who wish to use VSM s distance function, can implement it as a custom distance \nfunction. 70 Rhino bugs in our database have stack traces. 33/70 bugs are similar to at least one other \nbug, under our exper\u00adimental procedure. In the experiment, we issued OSCILLO-SCOPE queries to search \nfor these similar bug pairs, one restricted to stack traces and the other to execution traces. The stack \ntrace query was SELECT b1, b2 FROM Rhino WHERE SUBSET?( PROJ(b1, stack), PROJ(b2, stack) ). In the query, \nPROJ(b1, stack) extracts the stack trace of a bug. We ranked each result set in ascending order by distance. \nWe deemed the closest 50 to match under OSCILLOSCOPE, since manually examining the 50 pairs was tractable \nand each of the 70 bugs appears in at least one of these 50 pairs in both result sets. We examined each \nof these 50 bug pairs to judge whether it is a TP or an FP. Of the remaining pairs, which we deemed non-matches, \nwe examined all the FN. The stack trace result set contains 26 TP, 24 FP (the 50 that OSCILLOSCOPE matched), \nand 6 FN (from among the remaining pairs). The fact that most stack traces are similar accounted for \nnearly half the FPs. The largest distance over all 50 pairs is only 5, compared with 35 for the execution \ntraces. We examined the stack traces of the 6 FNs. The distance be\u00adtween Rhino-319614 and 370231 is large \nbecause the bugs be\u00adlong to different versions of Rhino between which the buggy method was heavily reformulated. \nBoth Rhino-432254 and 567114 throw stack over.ow exceptions whose traces differ greatly in length. Because \nRhino-238699 s stack trace does not contain the error-triggering point, Compile.compile(), it does not \nmatch 299539. The other three FN have distances 8, 14, and 14, so FPs crowd them out of the top 50 pairs. \nThe execution trace result set contains 41 TP, 9 FP, and 1 FN. False positives appear when the edit distance \nexceeds 20. The only FN is the pair Rhino-319614 and 370231, which is also a FN in the stack trace result \nset and for the same reason: the relevant methods were extensively rewritten. The stack trace result \nset matched similar bugs with 0.52 respr, 0.81 relrecall (See Section 4.1) and 0.63 F-score; the execution \ntrace result set achieved 0.82 respr, 0.98 relrecall, and 0.89 F-score. These results make clear that \nthe cost of instrumenting and running executable to collect execution traces can pay off, especially \nas a fallback strategy when queries against stack traces are inconclusive.  4.5 Threats to Validity \nWe face two threats to the external validity of our results. First, we evaluated OSCILLOSCOPE against \nbugs collected from the Rhino and Apache Commons projects, which might not be representative. Second, \nmost of the bug reports we gathered from these projects lack bug-triggering test cases. Without test \ncases, we were unable to produce traces for almost 70% of the bugs in these projects. Our evaluation \ntherefore rests on the remaining 30%. It may be that those bugs whose reports contain a test case are \nnot representative. These two threats combine to shrink the number of traces over which OSCILLOSCOPE \noperates. Nonetheless, our results are promising. We have conjec\u00adtured that unique bugs are rare, in \nthe limit, as a trace database contains all bug traces. The fact that we have already found useful examples \nof similar bugs in a small population lends support to our conjecture. Not only is the population small, \nbut it contains only .eld bugs. In particular, it lacks predeploy\u00adment bugs, which are resolved during \ninitial development. We contend that these bugs are more likely to manifest common misunderstandings \nand be more similar than .eld defects. Our evaluation is also subject to two threats to its con\u00adstruct \nvalidity. First, one cannot know [b] in general. We described how we manually approximated [b] in our \nexperi\u00admental procedure above. Project developers identi.ed 43% of these bugs as similar to another bug. \nFor the remaining 57%, as with any manual process involving non-experts, our assessments may have been \nincorrect. Second, our database currently contains method, not instruction, granular traces. To the extent \nto which a method-level trace fails to capture bug semantics, our measurements are inaccurate. Our evaluation \ndata is available at http://bql.cs.ucdavis.edu. 5. Related Work This section opens with a discussion \nof work that, like OSCILLOSCOPE, leverages programming knowledge. Often, one acquires this knowledge \nto automate debugging, which we discuss next. We close by discussing work on ef.ciently analyzing traces. \nReuse of Programmer Knowledge Leveraging past so\u00adlutions to recurrent problems promises to greatly improve \nprogrammer productivity. Most solutions are not recorded. However, the volume of those that are recorded \nis vast and usually stored as unstructured data on diverse systems, in\u00adcluding bug tracking systems, \nSCM commit messages, and mailing lists. The challenge here is how to support and pro\u00adcess queries on \nthis large and unwieldy set of data sources. Each project discussed below principally differs from each \nother, and OSCILLOSCOPE, in their approach to this problem. To attack the polluted, semi-structured bug \ndata problem, DebugAdvisor judiciously adds structure, such as construct\u00ading bags of terms from documents, \nand they de.ne a new sort of query, a fat query, that unites structured and unstructured query data [1]. \nIn contrast, OSCILLOSCOPE uses execution traces to query bug data. Thus, our approach promises fewer \nfalse positives, but may miss relevant bugs, while DebugAd\u00advisor will return larger result sets that \nmay require human processing. Indeed, Ashok et al. note there is much room for improvement in the quality \nof retrieved results. When compilation fails, HelpMeOut saves the error mes\u00adsage and a snapshot of the \nsource [11]. When a subsequent compilation succeeds, HelpMeOut saves the difference be\u00adtween the failing \nand succeeding versions. Users can enter compiler errors into HelpMeOut to search for .xes. In con\u00adtrast, \nOSCILLOSCOPE handles all bug types and compares execution traces, not error messages. Dimmunix, proposed \nby Jula et al., prevents the recurrence of deadlock [15]. When a deadlock occurs, Dimmunix snapshots \nthe method invocation sequence along with thread scheduling decisions to form a deadlock signature. \nDimmunix monitors program state. If a program s state is similar to a deadlock signature, Dimmunix either \nrolls back execution or refuses the lock request. Dimmunix targets deadlock bugs, while OSCILLOSCOPE \nsearches for similar bugs across execution traces of all types of bugs. Code peers are methods or classes \nthat play similar roles, provide similar functionality, or interact in similar ways. A recurring .x is \nrepeatedly applied, with slight modi.cations, to several code fragments or revisions. Proposed by Ngueyn \net al., FixWizard syntactically .nds code peers in source code, then identi.es recurring .xes to recommend \nthe application of these .xes to overlooked peers [23]. OSCILLOSCOPE uses traces, which precisely capture \nbug semantics, to search for similar bugs in a database of existing bugs. Automated Debugging Detecting \nduplicate bug reports is a subproblem of .nding similar bugs. Runeson et al. use natural language processing \n(NLP) to detect duplicates [27]. Comments in bug reports are often a noisy source of bug se\u00admantics, \nbut could complement OSCILLOSCOPE s execution trace-based approach. In a recent work, Wang et al. augment \nthe NLP analysis of bug reports with execution informa\u00adtion [29]. They record whether or not a method \nis invoked during an execution into a vector. We compare Wang et al. s approach to ours in Section 4.4. \nBug localization and trace explanation aim to .nd the root cause of a single bug or identify the likely \nbuggy code for manual inspection [2, 4, 7, 8, 12 14, 16, 20, 21, 30 32]. OSCILLOSCOPE helps developers \n.x a bug by retrieving similar bugs and how they were resolved. OSCILLOSCOPE and bug localization complement \neach other. A root cause discovered by bug localization may lead to the formulation of precise OSCILLOSCOPE \nqueries for similar, resolved bugs; storing only trace subsequence identi.ed by a root cause can also \nsave space in the database. Ef.cient Tracing and Analysis Researchers have proposed query languages over \ntraces for monitoring or verifying program properties [25, 26]. Goldsmith et al. propose the Program \nTrace Query Language (PTQL) for specifying and checking a program s runtime behaviors [6]. A PTQL query \ninstruments and runs a program to check whether or not a speci.ed property is satis.ed at runtime. Martin \net al. s Program Query Language (PQL) de.nes queries that operate over event sequences of a program to \nspecify and capture its design rules [22]. PQL supports both static and dynamic analyses to check whether \na program behaves as speci.ed. Olender and Osterweil propose a parameterized .ow analysis for properties \nover event sequences expressed in Cecil, a constraint language based on quanti.ed regular expressions \n(QREs) [24]. These three query languages are designed for analyzing a single, property-speci.c trace; \nOSCILLOSCOPE collects raw, un.ltered traces and tackles the bug similarity problem in the form of trace \nsimilarity. 6. Conclusion and Future Work Debugging is hard work. Programmers pose and reject hy\u00adpotheses \nwhile seeking a bug s root cause. Eventually, they write a .x. To reuse this knowledge about a bug, we \nmust accurately .nd semantically similar bugs. We have proposed comparing execution traces to this end. \nWe have de.ned and built OSCILLOSCOPE, a tool for searching for similar bugs, and an open infrastructure \n trace collection, a .exible query language BQL, a query engine based on Hadoop, database, and both a \nweb-based and plugin user interface to sup\u00adport it. BQL allows a user to 1) de.ne bug similarity and \n2) use that de.nition to search for similar bugs. We evalu\u00adated OSCILLOSCOPE on a collection of bugs \nfrom popular open-source projects. Our results show that OSCILLOSCOPE accurately retrieves relevant bugs: \nWhen querying unresolved bugs against resolved bugs in the Rhino project it achieves 93% respr, 94% relrecall \nfor an F-score of 0.93 (Section 4.1). OSCILLOSCOPE s database will grow from our activities and the contributions \nof others. We will use it to study questions such as Can we quantify the precision-scale trade-off of \nvarying trace granularity? , What proportion of bugs can only be captured at statement granularity? , \nand Which parts of the system do few buggy traces traverse? We also plan to add the traces of correct \nexecutions to OSCILLOSCOPE s database, which currently contains only buggy executions and compare them. \nWe intend to explore adding call context to produce execution trees; because OSCILLOSCOPE is a general \nand .exible framework, adding this support requires only modifying instrumentation and de.ning a distance \nmeasure over execution trees. In short, we intend to enhance and further experiment with our framework \nto gain additional insight into What makes bugs similar? . OSCILLOSCOPE is an open project. We invite \nreaders to use OSCILLOSCOPE and help us make it more general. Please refer to our website http://bql.cs.ucdavis.edu \nfor tutorials and demonstrations. Acknowledgments This research was supported in part by NSF (grants \n0917392 and 1117603) and the US Air Force (grant FA9550-07-1\u00ad0532). The information presented here does \nnot necessarily re.ect the position or the policy of the government and no of.cial endorsement should \nbe inferred. References [1] B. Ashok, J. Joy, H. Liang, S. Rajamani, G. Srinivasa, and V. Vangala. DebugAdvisor: \nA recommender system for de\u00adbugging. In Proceedings of the 17th Joint Meeting of the European Software \nEngineering Conference and the ACM SIG\u00ad  SOFT International Symposium on the Foundations of Software \nEngineering, 2009. [2] I. Beer, S. Ben-David, H. Chockler, A. Orni, and R. Tre.er. Explaining counterexamples \nusing causality. In Proceedings of the International Conference on Computer Aided Veri.cation, 2009. \n[3] N. Bettenburg, S. Just, A. Schr\u00f6ter, C. Weiss, R. Premraj, and T. Zimmermann. What makes a good bug \nreport? In Proceed\u00adings of the 16th ACM SIGSOFT International Symposium on the Foundations of Software \nEngineering, 2008. [4] P. Dhoolia, S. Mani, V. S. Sinha, and S. Sinha. Debugging model-transformation \nfailures using dynamic tainting. In Proceedings of the 24th European Conference on Object-Oriented Programming, \n2010. [5] D. R. Engler, D. Y. Chen, and A. Chou. Bugs as inconsistent behavior: A general approach to \ninferring errors in systems code. In Proceedings of the ACM Symposium on Operating Systems Principles, \n2001. [6] S. Goldsmith, R. O Callahan, and A. Aiken. Relational queries over program traces. In Proceedings \nof the 20th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, \n2005. [7] A. Groce, S. Chaki, D. Kroening, and O. Strichman. Error explanation with distance metrics. \nInternational Journal on Software Tools for Technology Transfer, 8(3):229 247, 2006. [8] A. Groce and \nW. Visser. What went wrong: Explaining counterexamples. In SPIN Workshop on Model Checking of Software, \n2003. [9] Z. Gu, E. Barr, and Z. Su. BQL: Capturing and reusing de\u00adbugging knowledge. In Proceedings \nof the 33rd International Conference on Software Engineering (Demo Track), 2011. [10] D. Gus.eld. Algorithms \non Strings, Trees, and Sequences: Computer Science and Computational Biology. Cambridge University Press, \n1997. [11] B. Hartmann, D. MacDougall, J. Brandt, and S. R. Klemmer. What would other programmers do: \nSuggesting solutions to error messages. In Proceedings of the 28th International Conference on Human \nFactors in Computing Systems, 2010. [12] K. Hoffman, P. Eugster, and S. Jagannathan. Semantics-aware \ntrace analysis. In Proceedings of the 2009 ACM SIGPLAN Conference on Programming Language Design and \nImplemen\u00adtation, 2009. [13] E. W. Host and B. M. Ostvold. Debugging method names. In Proceedings of the \n23rd European Conference on Object-Oriented Programming, 2009. [14] J. A. Jones, M. J. Harrold, and J. \nStasko. Visualization of test information to assist fault localization. In Proceedings of the 24th International \nConference on Software Engineering, 2002. [15] H. Jula, D. Tralamazza, C. Zam.r, and G. Candea. Deadlock \nimmunity: Enabling systems to defend against deadlocks. In Proceedings of the 8th USENIX Conference on \nOperating Systems Design and Implementation, 2008. [16] S. Kim, K. Pan, and E. J. Whitehead, Jr. Memories \nof bug .xes. In Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software \nEngineering, 2006. [17] A. Ko and B. Myers. A framework and methodology for studying the causes of software \nerrors in programming systems. Journal of Visual Languages &#38; Computing, 16(1-2):41 84, 2005. [18] \nJ. R. Larus. Whole program paths. In Proceedings of the 1999 ACM SIGPLAN Conference on Programming Language \nDesign and Implementation, 1999. [19] B. Liblit. Cooperative Bug Isolation. Springer-Verlag, 2007. [20] \nB. Liblit, A. Aiken, A. Zheng, and M. Jordan. Bug isolation via remote program sampling. In Proceedings \nof the 2003 ACM SIGPLAN Conference on Programming Language Design and Implementation, 2003. [21] B. Liblit, \nM. Naik, A. Zheng, A. Aiken, and M. Jordan. Scalable statistical bug isolation. In Proceedings of the \n2005 ACM SIGPLAN Conference on Programming Language Design and Implementation, 2005. [22] M. Martin, \nB. Livshits, and M. S. Lam. Finding application er\u00adrors and security .aws using PQL: A program query \nlanguage. In Proceedings of the ACM Conference on Object-Oriented Programming, Systems, Languages, and \nApplications, 2005. [23] T. T. Nguyen, H. A. Nguyen, N. H. Pham, J. Al-Kofahi, and T. N. Nguyen. Recurring \nbug .xes in object-oriented programs. In Proceedings of the 32nd International Conference on Software \nEngineering, 2010. [24] K. Olender and L. Osterweil. Cecil: A sequencing constraint language for automatic \nstatic analysis generation. IEEE Transactions on Software Engineering, 16(3):268 280, 1990. [25] G. Pothier \nand E. Tanter. Summarized trace indexing and query\u00ading for scalable back-in-time debugging. In Proceedings \nof the 25th European Conference on Object-Oriented Programming, 2011. [26] G. Pothier, E. Tanter, and \nJ. Piquer. Scalable omniscient debug\u00adging. In Proceedings of the 22nd ACM SIGPLAN Conference on Object-Oriented \nProgramming, Systems, Languages, and Applications, 2007. [27] P. Runeson, M. Alexandersson, and O. Nyholm. \nDetection of duplicate defect reports using natural language processing. In Proceedings of the 29th International \nConference on Software Engineering, 2007. [28] A. Schr\u00f6ter, N. Bettenburg, and R. Premraj. Do stack traces \nhelp developers .x bugs? In Proceedings of the 7th IEEE Working Conference on Mining Software Repositories, \n2010. [29] X. Wang, L. Zhang, T. Xie, J. Anvik, and J. Sun. An approach to detecting duplicate bug reports \nusing natural language and execution information. In Proceedings of the 30th International Conference \non Software Engineering, 2008. [30] W. Weimer, T. Nguyen, C. L. Goues, and S. Forrest. Au\u00adtomatically \n.nding patches using genetic programming. In Proceedings of the 31st International Conference on Software \nEngineering, 2009. [31] A. Zeller. Yesterday, my program worked. Today, it does not. Why? In Proceedings \nof the 7th ACM SIGSOFT International Symposium on the Foundations of Software Engineering, 1999. [32] \nX. Zhang, N. Gupta, and R. Gupta. Locating faults through automated predicate switching. In Proceedings \nof the 28th International Conference on Software Engineering, 2006.  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Some bugs, among the millions that exist, are similar to each other. One bug-fixing tactic is to search for similar bugs that have been reported and resolved in the past. A fix for a similar bug can help a developer understand a bug, or even directly fix it. Studying bugs with similar symptoms, programmers may determine how to detect or resolve them. To speed debugging, we advocate the systematic capture and reuse of debugging knowledge, much of which is currently wasted. The core challenge here is how to search for similar bugs. To tackle this problem, we exploit semantic bug information in the form of execution traces, which precisely capture bug semantics. This paper introduces novel tool and language support for semantically querying and analyzing bugs. We describe OSCILLOSCOPE, an Eclipse plugin, that uses a bug trace to exhaustively search its database for similar bugs and return their bug reports. OSCILLOSCOPE displays the traces of the bugs it returns against the trace of the target bug, so a developer can visually examine the quality of the matches. OSCILLOSCOPE rests on our bug query language (BQL), a flexible query language over traces. To realize OSCILLOSCOPE, we developed an open infrastructure that consists of a trace collection engine, BQL, a Hadoop-based query engine for BQL, a trace-indexed bug database, as well as a web-based frontend. OSCILLOSCOPE records and uploads bug traces to its infrastructure; it does so automatically when a JUnit test fails. We evaluated OSCILLOSCOPE on bugs collected from popular open-source projects. We show that OSCILLOSCOPE accurately and efficiently finds similar bugs, some of which could have been immediately used to fix open bugs.</p>", "authors": [{"name": "Zhongxian Gu", "author_profile_id": "81464657435", "affiliation": "University of California, Davis, Davis, CA, USA", "person_id": "P3856210", "email_address": "zgu@ucdavis.edu", "orcid_id": ""}, {"name": "Earl T. Barr", "author_profile_id": "81300234801", "affiliation": "University of California, Davis, Davis, CA, USA", "person_id": "P3856211", "email_address": "etbarr@ucdavis.edu", "orcid_id": ""}, {"name": "Drew Schleck", "author_profile_id": "81548952656", "affiliation": "University of California, Davis, Davis, CA, USA", "person_id": "P3856212", "email_address": "dtschleck@ucdavis.edu", "orcid_id": ""}, {"name": "Zhendong Su", "author_profile_id": "81100108298", "affiliation": "University of California, Davis, Davis, CA, USA", "person_id": "P3856213", "email_address": "su@ucdavis.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384684", "year": "2012", "article_id": "2384684", "conference": "OOPSLA", "title": "Reusing debugging knowledge via trace-based bug search", "url": "http://dl.acm.org/citation.cfm?id=2384684"}