{"article_publication_date": "10-19-2012", "fulltext": "\n Reducing the Barriers to Writing Veri.ed Speci.cations Todd W. Schiller Michael D. Ernst University \nof Washington {tws,mernst}@cs.washington.edu Abstract Formally verifying a program requires signi.cant \nskill not only because of complex interactions between program sub\u00adcomponents, but also because of de.ciencies \nin current ver\u00adi.cation interfaces. These skill barriers make veri.cation economically unattractive by \npreventing the use of less\u00adskilled (less-expensive) workers and distributed work.ows (i.e., crowdsourcing). \nThis paper presents VeriWeb, a web-based IDE for veri.\u00adcation that decomposes the task of writing veri.able \nspec\u00adi.cations into manageable subproblems. To overcome the information loss caused by task decomposition, \nand to re\u00adduce the skill required to verify a program, VeriWeb incor\u00adporates several innovative user \ninterface features: drag and drop condition construction, concrete counterexamples, and speci.cation \ninlining. To evaluate VeriWeb, we performed three experiments. First, we show that VeriWeb lowers the \ntime and monetary cost of veri.cation by performing a comparative study of VeriWeb and a traditional \ntool using 14 paid subjects con\u00adtracted hourly from Exhedra Solution s vWorker online mar\u00adketplace. Second, \nwe demonstrate the dearth and insuf.\u00adciency of current ad-hoc labor marketplaces for veri.cation by recruiting \nworkers from Amazon s Mechanical Turk to perform veri.cation with VeriWeb. Finally, we characterize the \nminimal communication overhead incurred when Veri-Web is used collaboratively by observing two pairs \nof de\u00advelopers each use the tool simultaneously to verify a single program. Categories and Subject Descriptors \nD.2.4 [Software En\u00adgineering]: Software/Program Veri.cation Keywords program veri.cation, human factors, \ncrowd\u00adsourcing Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, USA. Copyright c . 2012 ACM 978-1-4503-1561-6/12/10. \n. . $10.00 1. Introduction Ideally, in a well-modularized program, it would be possi\u00adble to perform software \nengineering tasks on each module locally. Nonetheless, some software engineering tasks re\u00adquire, or bene.t \nfrom, a global view. For example, it is dif.\u00adcult to decompose the task of writing method contracts (for\u00admal \nspeci.cations) into independent subproblems because a change to one method contract may require changes \nto others as a result of interdependencies between program elements a method s contract may depend on \nthe contracts for other methods it calls. Likewise, the task of writing an application or object speci.cation \nwould likely bene.t from considering multiple object and methods speci.cations simultaneously. Attempting \nto decompose tasks like these results in infor\u00admation (context) loss, the severity of which depends on \nthe quality of the program s design and documentation; incom\u00adplete information leads to confusion, mistakes, \nand wasted or duplicated effort. For writing and verifying formal speci.ca\u00adtions, the dif.culty is exacerbated \nby the complexity and in\u00adaccessibility of current veri.cation interfaces. Coupled with a high cost of \nlabor (the average developer in the United States earns $90,170 per year [35]), the result is that the \ntime and money costs of veri.cation outweigh the bene.ts for the general software development community. \nTools and techniques that enable less-skilled workers to perform veri.cation would make formally establishing \ncor\u00adrectness more economically attractive. To the extent that such tools decompose the problem, the tools \nwould further improve the economics of veri.cation by enabling parallel and distributed work.ows (e.g., \ncrowdsourcing to workers in a global marketplace). VeriWeb Veri.cation Interface This paper presents \nVeri-Web, a tool for creating veri.able Java Modeling Language (JML) [6, 22] speci.cations for existing \nprograms by lever\u00adaging the crowd, a (potentially distributed) set of workers solving problems via web \nclient. Figure 1 shows a partial example of a method contract, expressed in JML. VeriWeb decomposes the \nlarger task of writing a veri.\u00adable program speci.cation into smaller subtasks: creating one method s \npreconditions and postconditions. To compen\u00adsate for the effects of decomposition and to lower the tool \ns skill requirement, VeriWeb includes several novel interface  class Queue {int / * @spec public * / \ncurrentSize ; / * @invariant currentSize >= 0; * / ... / *@ requires x != null ; @ ensures currentSize \n== \\ old ( currentSize ) + 1; @ exsures ( RuntimeException ) . . . * / public void enqueue(Object x) \nthrows RuntimeException { ... }} Figure 1. An example partial Java Modeling Language (JML) contract \nfor the enqueue method of a queue. Re\u00adquires clauses state properties that must hold when the method \nis called. Ensures clauses state properties that must hold when the method exits normally. Exsures clauses \nstate properties that must hold when the method throws the declared exception. Invariants state properties \nthat must hold whenever an object is visible, e.g., after a constructor or a public method call. A tool \nsuch as ESC/-Java2 [9] can verify that the program meets the speci.cation, and that no unexpected exceptions \n(e.g., NullPointerExcep\u00adtion) will be thrown. features: drag and drop contract construction, concrete \ncoun\u00adterexamples, contract inlining, and context clues. VeriWeb additionally includes contract suggestions \ninferred from dy\u00adnamic traces. VeriWeb can be viewed as an IDE for veri.\u00adcation, where the combined effect \nof the features is greater than the sum of the individual parts. By enabling the use of less-skilled \nlabor for verifying an existing program, VeriWeb admits the following work.ow for skilled feature developers: \n1. The skilled developer writes a program or feature. 2. The skilled developer (optionally) writes a \npartial JML speci.cation of the program or feature. 3. The skilled developer submits the program and \npartial speci.cation to VeriWeb, which utilizes the crowd to complete a veri.able JML speci.cation. \n Extended Static Checking Internally, VeriWeb uses ESC/-Java2 [9] to perform Extended Static Checking \n[4, 7, 13, 14, 19, 24] of the program against the generated speci.cation. Extended Static Checking is \na veri.cation approach that can be viewed as a front-end or user interface to an automated theorem prover \n[11, 12, 27]. The user writes a speci.cation consisting of method preconditions, method postconditions, \nand object invariants. (Figure 1 shows a partial example of a method contract, expressed in the Java \nModeling Language (JML) [6, 22].) The extended static checker converts both the speci.cation and the \nprogram code into logical formu\u00adlas, passes the combined formulas to the theorem prover, and reports \nto the user whether or not the program satis.es the speci.cation. If not (that is, the veri.cation attempt \nfailed), the user revises the speci.cations or the code using the feed\u00adback from the checker, and then \ntries again. One example veri.cation task is to prove that a program will never throw an unexpected null \npointer exception, re\u00adgardless of input. If a public method unconditionally deref\u00aderences a parameter, \nthen the property is true only if the method is never called with null as an argument. The user can express \nthis requirement by writing a method precon\u00addition that the parameter cannot be null. ESC/Java2 would \nthen be able to prove that the method cannot throw an unex\u00adpected null pointer exception. However, the \naddition of the precondition introduces the requirement that the correspond\u00ading argument at each call \nsite is non-null. In the end, ESC/-Java2 veri.es that all the user-written contracts are mutually consistent, \nthat the user-written contracts are consistent with the code, and that there are no null pointer exceptions. \nNote that, while this is partial veri.cation, and not veri.cation of full functional correctness, any \nrequisite functional proper\u00adties (e.g., data value properties) are veri.ed en passant. VeriWeb Use Case \nVeriWeb is targeted at the veri.cation of domain-independent and expert-speci.ed properties in existing \nclient code. VeriWeb is not targeted at the discovery of new application-speci.c properties, veri.cation \nof full functional correctness, the veri.cation of library code, or the development of new programs. \nVeriWeb focuses on client code rather than library code. We have observed that library code use is cross-cutting, \nwhereas client code is more lightly coupled. Furthermore, the speci.cations of library code are complicated, \nin order to support general use consider the C++ Standard Template Library. By contrast, the speci.cations \nfor client code are relatively simpler, and the speci.cations required to show the absence of unexpected \nexceptions in client code are even simpler. Therefore, there is ample opportunity to reduce the cost \nof partial formal veri.cation by focusing on client code. Since VeriWeb operates by having users solve \nmethod subtasks, the tool is unlikely to uncover complex object in\u00advariants and global application properties. \nVeriWeb is better\u00adsuited to verifying language properties (e.g., that the pro\u00adgram will never deference \na null pointer) and localized ap\u00adplication properties. This use case is aligned with the under\u00adlying \ntool ESC/Java2 [9], as well as similar techniques such as Microsoft s Code Contracts [8]. Evaluation \nTo measure the time and monetary cost of ver\u00adi.cation, we performed a comparative study in which work\u00aders \nfrom Exhedra Solutions s vWorker [37] labor market\u00adplace performed veri.cation with ESC/Java2, either \nthrough its Eclipse interface or through VeriWeb. To understand the potential for using ad-hoc crowdsourced \nlabor for veri.ca\u00adtion, we recruited workers from Amazon s Mechanical Turk to use VeriWeb. To understand \nthe overhead incurred when performing veri.cation collaboratively with VeriWeb, we observed undergraduate \nstudents using the tool to collabo\u00adratively verify a small program.  Figure 2. VeriWeb client interface \nshowing a write preconditions subproblem. Top: information bar linking to instructions, FAQ, and messages \nabout the problem (Section 2.2.4). Left: the drag and drop interface for writing conditions (Section \n2.1.3). Right Top: ESC/Java2 warning locations are underlined in red in the source code view, and the \nwarnings are shown at the top right. Lower right: source code view. Javadoc is available for code highlighted \nin blue. To view the documentation and warnings, the user simply hovers their mouse over the underlined \ncode. The user has toggled the inline speci.cations (Section 2.2.3) for method isEmpty. Contributions \nThis paper makes 3 primary contributions: 1. We present novel veri.cation interface features to address \nthe challenges of decomposition and a distributed work\u00adforce. VeriWeb, a browser-based tool for program \nveri.\u00adcation, incorporates these features. 2. We quantitatively characterize the time and monetary costs \nof veri.cation. 3. We identify and describe challenges and threats to valid\u00adity that are unique to studying \nveri.cation in a global and collaborative setting.  The paper proceeds as follows. Section 2 describes \nVeri\u00adWeb s design principles and implementation. Section 3 poses three primary research questions. Section \n4 quanti.es the monetary cost of veri.cation when contracting semi\u00adskilled labor on an hourly basis. \nSection 5 explores the use of ad-hoc labor from Amazon s Mechanical Turk for per\u00adforming veri.cation. \nSection 6 characterizes the overhead incurred when performing collaborative veri.cation with VeriWeb. \nSection 7 discusses the results. Finally, Section 8 presents related work, and Section 9 concludes. 2. \nVeriWeb We created VeriWeb, a tool for verifying Java programs. Internally, VeriWeb uses the ESC/Java2 \nveri.cation tool.1 1 Other veri.cation tools exist, such as Microsoft s Code Contracts [8]. We originally \nwanted to use Code Contracts for this research, but ESC/Java2 can statically reason about array properties \nthat Code Contracts cannot. VeriWeb decomposes the task of veri.cation into subprob\u00adlems that users solve \nin a web interface (Figure 2). A live demo is available at the VeriWeb project page http: //www.cs.washington.edu/homes/tws/veriweb/. \nWe designed VeriWeb around two major principles: active guid\u00adance (Section 2.1) and explanations in context \n(Section 2.2). 2.1 Active Guidance Active guidance encouraging users to reason in a certain way, or \nrestricting their set of actions is aimed at aid\u00ading reasoning and preventing time-wasting mistakes. \nVeri-Web guides users between subproblems by choosing the next subproblem for users (Section 2.1.1) and \nwithin a subprob\u00adlem by making suggestions (Section 2.1.2) and preventing syntax errors (Section 2.1.3). \n 2.1.1 Guided Decomposition VeriWeb guides the user through the veri.cation task by asking the user to \nsolve 4 types of subproblems: 1. Select method preconditions from a list 2. Write method preconditions \n 3. Write method postconditions (that are true when the method exits normally) 4. Write method exceptional \npostconditions  ESC/Java2 has its own disadvantages: it only fully supports the Java 1.4 speci.cation \n(Java 5 was released in 2004), and it can be unsound.  VeriWeb offers the selection and writing of preconditions \nas separate problems not just because this focuses the user on the most relevant task at any moment, \nbut also because they require different modes of reasoning. Selecting precon\u00additions from a list of possibilities \nsuggested by VeriWeb re\u00adquires forward reasoning, to determine whether an error will occur. Writing preconditions \nis a more dif.cult task that re\u00adquires backward reasoning from an error to determine the weakest precondition \nthat will prevent it. Active Subproblems A subproblem is active if it requires work from a user. There \nare three reasons that a subproblem may be active: 1. The subproblem is the cause of an ESC/Java2 warning. \nIf there is a warning within a method body, the precondition set is considered to be the cause. 2. A \nuser has identi.ed the subproblem as being the cause of an error elsewhere. For example, an unveri.able \npostcon\u00addition might be caused by a too-weak precondition or by a too-weak postcondition on a callee \nmethod. Likewise for ESC/Java2 warnings occurring within the method body. 3. Every postcondition problem \nis initially marked as active. Users can often quickly write a few obvious postcondi\u00adtions (e.g., about \nthe return value for a boolean method), and we found that doing so can substantially speed the veri.cation \nprocess.  As long as any subproblems are active, the user is presented with an active subproblem to \nsolve. When no subproblem is active, the program has been veri.ed. Subproblem dependencies VeriWeb computes \ndependen\u00adcies among subproblems and stores these as a directed de\u00adpendence graph that exhibits the same \nhigh-level structure as the call graph. Subproblem P1 depends on subproblem P2 if a change in the solution \nto P2 may invalidate the solution to P1. This property is independent of whether subproblem P1 has actually \nbeen solved yet. Whenever a user changes the solution to a subproblem, VeriWeb activates some or all \nof the subproblems that de\u00adpend on it (Figure 3): If a precondition is strengthened, VeriWeb activates \nthe precondition subproblems for the method s callers.  If a precondition is weakened, VeriWeb activates \nthe postcondition subproblems for the method.  If a postcondition is weakened, VeriWeb naively activates \nall problems for the method s callers.  In each of these situations, VeriWeb re-checks each subprob\u00adlem \nand only activates subproblems for which an error oc\u00adcurs. If no error occurs, then no additional work \nis required and VeriWeb does not require the user to re-visit the sub\u00adproblem. Figure 3. Intra-and intermethod \ndepends-on graph for a method B that calls a method A. When a contract clause is weakened, subproblems \nwith weaken edges leading to the contract are activated. For example, if the ensures clause for method \nA was weakened to \\result > 4, both the precon\u00addition and postcondition problems for method B would be \nactivated. When a contract clause is strengthened, subprob\u00adlems with a strengthen edge leading to the \ncontract are activated. For example, if a new precondition x< 3is added to method A, the precondition \nsubproblems for method B would be activated to ensure the call to method A uses a valid x value. Note \nthat these basic rules capture more complex rela\u00adtionships between properties via composition. For example, \naccording to the second activation rule, if a method s precon\u00additions are weakened, VeriWeb will activate \nthe postcondi\u00adtion problem for that method. If the change causes the post\u00adcondition set to be weakened, \nthen VeriWeb will activate all the problems for the method s callers according to the third activation \nrule. The leaves of the active subproblem graph those nodes that are active and have no active children \n are the set of subproblems that can be productively assigned to workers. In fact, VeriWeb can assign \nthese subproblems to be solved in parallel by different workers. When assigning a subproblem, VeriWeb \ngives preference to users who have already worked on a subproblem, and to users who have marked that \na speci.cation was incorrect. Collaborative Use To support multiple users working si\u00admultaneously, VeriWeb \ncurrently just naively performs syn\u00adchronization on an entire project. Each user keeps a local view of \nthe master project speci.cation; the local view is updated whenever the user requests a new subproblem. \nWhen a user submits a solution to a subproblem P1, Veri-Web checks whether the solution invalidates the \nassumptions of any other subproblem P2 currently being worked on. Veri-Web lets the other worker continue \nworking on P2. However, when the worker submits P2, VeriWeb records the clauses in the solution for future \nuse, but does not modify the master speci.cation. Recursion The current implementation of VeriWeb only \nsupports single-method recursion. It is straightforward to extend VeriWeb to mutually recursive methods: \nmake a tree of the (possibly degenerate) strongly connected components (SCCs), and arbitrarily choose \none method from each SCC that is a leaf. Once the user solves this subproblem, the SCC is either broken \nor is smaller.  Object Invariants Object invariants require non-modular reasoning about all public methods \nin a class, which is at odds with VeriWeb s decomposition into subproblems. Veri-Web does not display \nobject invariants nor give the user the opportunity to write object invariants directly. VeriWeb does, \nhowever, compute and utilize object in\u00advariants by lifting conditions that appear as preconditions and \npostconditions for all the public methods for the type. The tool expedites object invariant discovery \nin the follow\u00ading ways: Any precondition that refers to object state is automat\u00adically checked as a \n(potential) postcondition for the method. (Section 2.1.2 discusses how VeriWeb suggests and checks potential \ncontracts.)  Any method invariant a clause in both a method s pre-and postconditions is automatically \nsuggested in precondition selection problems for other public methods in the class, and is automatically \nchecked as a (potential) postcondition for other public methods.  When a clause has been established \nas a method invariant for at least half of the non-pure public methods in the class (those methods annotated \nas not mutating state), the interface prompts the user to indicate whether or not the clause is an object \ninvariant; if the user agrees, the condition is added as a precondition to the other methods and subproblems \nare activated as previously described.  This scheme for handling object invariants is far from per\u00adfect; \nnon-trivial object invariants should by written directly by the feature developer.  2.1.2 Contract Suggestions \nWriting contracts from scratch is dif.cult, especially for users unfamiliar with the speci.cation language. \nVeriWeb generates, and presents to users, a suggested set of possible clauses. Users can often complete \ntheir task just by selecting clauses; in other cases, users can select some clauses and write others. \nContract suggestions convey two kinds of bene.ts. First, it is much faster, and requires less creativity, \nto select clauses rather than writing them from scratch. Second, the inferred clauses serve as a model \nwhen writing new contracts: they can both illustrate the syntax in which clauses are written and, even \nwhen slightly incorrect, can inspire users to write similar clauses. We have observed all of these bene.ts. \nVeriWeb makes suggestions for both pre-and postcondi\u00adtions; however, the list of subproblem types (Section \n2.1.1) includes select preconditions but not select postcondi\u00adtions because VeriWeb automatically performs \nall postcon-Figure 4. Subfragment highlighting, shown here in the drag and drop interface, helps users \nread and understand clauses.  dition selection. When postcondition problems are pre\u00adsented to the user, \nVeriWeb queries ESC/Java2 regarding each suggested postcondition, displays them all, and indi\u00adcates which \nones are veri.able and which ones are not veri\u00ad.able; the user may hide the unveri.able ones. There are \nmultiple ways to generate suggested clauses. The current VeriWeb implementation uses Daikon [16, 17] \nto dynamically infer likely clauses from execution traces. Because the suggested clauses generalize observed \nexecu\u00adtions, they may not be true; even if true, they may be be\u00adyond ESC/Java2 s ability to verify them. \nHowever, Nimmer and Ernst [28] found that (1) users annotating programs are not encumbered by false inferred \nclauses, and (2) for two of their three subject programs, including inferred clauses had a statistically \nsigni.cant positive correlation with successful veri.cation.  2.1.3 Drag and Drop Contracts We observed \nthat many users wasted time attempting to write invalid clauses clauses that were either syntacti\u00adcally \nor semantically incorrect. For example, some users at\u00adtempted to write facts about local variables in \na method s preconditions. To guide users in writing expressions, we de\u00adveloped a drag and drop interface \nto allow users to construct clauses from a pool of starter fragments. Invalid fragments, such as the \n\\result and \\old fragments when writing pre\u00adconditions, are not available. Fragments have holes where \nother fragments can be inserted or removed. Contracts dis\u00adplayed in documentation can be added to the \ndrag and drop interface by clicking on a button next to the contract. Large contracts and fragments, \nespecially those involving complex constructs such as universal quanti.cation, are dif\u00ad.cult for humans \nto read. VeriWeb s subfragment highlight\u00ading (shown in Figure 4) enables users to better understand the \nsubexpression groups. Additionally, the highlighting in\u00addicates the subfragment that will be removed \nwhen the user clicks and drags. Subexpression highlighting is also enabled for contracts displayed in \nother parts of the interface.  2.2 Explanations in Context Program veri.cation tools tend to be inscrutable. \nAfter a ver\u00adi.cation failure, it can be dif.cult for users to understand the tool s internal state or \nreasoning steps, and to know what changes would permit a speci.cation to be veri.ed. Users are often \nfrustrated as they try to form and maintain their own mental model of what the tool knows and/or can \nprove. VeriWeb offers clues to make this work easier or to elim\u00adinate it entirely. VeriWeb explains the \nrelationship of ele\u00adments within a subproblem with concrete counterexamples (Section 2.2.1) and tool \ntips (Section 2.2.2), and between Figure 5. VeriWeb clauses are executed over a dynamic  trace. If \nthe clause is falsi.ed by the trace, the parameter and .eld values before and after the call are displayed \nin an expandable tree grid. subproblems with contract inlining (Section 2.2.3) and in\u00adtermethod dependency \ninformation (Section 2.2.4). 2.2.1 Concrete Counterexamples A typical program veri.cation tool indicates \nthat a given clause is either provable or unprovable. Given an unprov\u00adable clause, a user does not know \nwhether the contract is false, the clause is true but beyond the reasoning abilities of the veri.cation \ntool, or the clause would be provable if other parts of the speci.cation were improved. We observed users \nspending signi.cant amounts of time fruitlessly at\u00adtempting to prove false clauses. To eliminate this \nwasted effort, VeriWeb presents the user with concrete counterex\u00adamples for clauses that are demonstrably \nfalse with respect to a set of real executions (e.g., from running the test-suite). VeriWeb does not \ncurrently generate concrete executions or tests; these are provided by the feature developer. VeriWeb \ndisplays the counterexample information in two ways. First, when the user hovers the mouse pointer over \na subexpression of the clause, a tooltip shows the value of that subexpression. Second, the user can \nexplore all the parameter and return values via an expandable tree grid (see Figure 5). Expanding the \ngrid shows the .elds of each object. Our run-time checking currently has two limitations: First, no testing \nis done of conditions in exceptional postcon\u00additions (because our trace generator does not handle excep\u00adtional \nexits). Second, expressions involving universal quan\u00adti.cation can be tested only if the quanti.ed-over \nvariable has explicit lower and upper bounds. 2.2.2 Task-speci.c Tooltips Contextual help messages are \nweaved into the interface. For example, in the drag and drop interface, hovering the mouse over a hole \nin a \\forall expression shows what type of expression should be placed there (e.g., a predicate to bound \nthe quanti.ed variable). Other examples of contextual help messages include tips shown between problems \n(while the next problem is load\u00ading), FAQ links displayed above and below interface ele\u00adments, and additional \ntooltips. Figure 6. Contract inlining: contracts for a method are aligned with the method in the source \ncode. Users can toggle the display of unproven postconditions. Inlining the specs for multiple methods \ncan help to identify information gaps. 2.2.3 Contract Inlining When verifying a program, information \nabout the absence of knowledge (e.g., Why doesn t the tool have enough in\u00adformation to prove this postcondition? \n) can be as important as information about what the tool does know. To help users locate information \ngaps between method calls, VeriWeb offers contract inlining in the source view. Contract inlining displays \na method s contracts around a method call in the source code: the preconditions are above, the postconditions \nare below, and everything is horizontally aligned with the method call. An example is shown in Figure \n6. The inlined contracts currently provide two pieces of additional infor\u00admation: (1) the set of preconditions \nthat are not met at the call site and (2) a user-toggleable list of unproven (potential) postconditions \nfor the callee method. Initially, no contracts are inlined; the user can display as many or as few as \ndesired. Contract inlining even works for source lines with multiple method calls: the inlined contracts \nare displayed from outside to inside in the order that the calls appear on the line (i.e., horizontally \naligned with the corresponding call). Contracts can be verbose and might clutter the display, but we \nhypothesize that users only need to inline contracts for 2 3 methods at a time (to visualize the information \ngaps). Therefore, our approach should scale even to methods that make many method calls.  2.2.4 Intermethod \nDependency Information Postcondition Dependencies We observed that it is dif.\u00adcult for new users to form \nand maintain a mental model about what the veri.er knows after a method call. One ram\u00adi.cation of this \nis that the users do not realize that warnings in a method may be caused by de.cient postconditions for \nthe method s callees. Contract inlining (Section 2.2.3) ad\u00addresses this problem. To further address it, \nwhen VeriWeb displays veri.er warnings, it also lists the methods that are called before the warning \nand indicates that the user may need to re.ne the postconditions of those methods. While data .ow analyses \ncould be utilized to make this informa\u00adtion more precise, pilot tests showed that users still found these \nmessages helpful.  Information Transfer Because methods depend on one another, some information must \ntransfer between subprob\u00adlems. In VeriWeb, when a user assigns blame (e.g., marking that a callee s postconditions \nare too weak), the user is asked to explain the problem either in English or pseudo-code. Relevant messages \nfrom other subproblems are displayed with the current subproblem. The user completing the newly created \ntask can mark whether or not the comment was help\u00adful. If the user indicates that the comment was not \nhelpful, the user is prompted to explain why they did not .nd the comment helpful. In the future, we \nplan to use this feature to automate the handling of payments when deploying Veri-Web on an ad-hoc marketplace \nsuch as Mechanical Turk [1]. 3. Research Questions To validate the VeriWeb tool, as well as the general \npoten\u00adtial for crowdsourced program veri.cation, we posed three research questions: RQ 1. What is the \ncost (time and money) of program veri.\u00adcation? To answer this question, we performed a comparative study \nof VeriWeb and the ESC/Java2 plugin for Eclipse (Sec\u00adtion 4). The subjects were 14 programmers recruited \nfrom Exhedra s vWorker labor marketplace. We found that the VeriWeb workers took both less time and money \non average than the Eclipse workers for the subject program. Addition\u00adally, we found that worker progress \nwith VeriWeb was more consistent than with Eclipse, which was characterized by work toward incorrect \nsolutions and oscillations around lo\u00adcal optima. RQ 2. Can ad-hoc labor be used to crowdsource program \nveri.cation? To explore this question, we recruited workers from Ama\u00adzon s Mechanical Turk at varying \npay levels to complete VeriWeb subproblems (Section 5). We found the labor pool to be very shallow; additionally, \nthe expected pay was not competitive with that of hourly contracted workers (c.f. Sec\u00adtion 4). The preliminary \nresults suggest that current ad-hoc labor marketplaces are not well-suited for veri.cation. RQ 3. How \ndoes decomposition and communication over\u00adhead affect the performance of collaborative veri.cation? To \ncharacterize the overhead, we observed two pairs of com\u00adputer science undergraduates each using VeriWeb \nto col\u00adlaboratively verify a small program (Section 6). We found that, for each pair, VeriWeb generally \nisolated one partici\u00adpant from the other for one pair, neither participant ob\u00adserved any communication \nfrom the other. Additionally, the observations highlighted that while collaboration can speed veri.cation \nby enabling users to address a root cause in par\u00adallel, a single poor-performing user can signi.cantly \nderail veri.cation. Pilot Studies During the development of VeriWeb, we per\u00adformed pilot studies similar \nto the .rst study above. These studies were performed with both hourly contracted workers and more than \n40 computer science undergraduate students. 4. The Cost of Program Veri.cation Creating a cost-and time-effective \ntool requires solving an optimization problem [31]. Though the problem involves many complicated factors, \nan approximation consists of only two complementary questions: RQ 1.1. Given a .xed amount of money, \nhow much veri\u00ad.cation can you buy? RQ 1.2. Given a .xed amount of time, how much veri.\u00adcation can you \nbuy? In this section, we begin to quantitatively answer both of these questions by contracting workers \non Exhedra Solu\u00adtions s vWorker [37] marketplace to verify a small project StackAr, which we consider \nto be of moderate dif.culty based on prior work [28]. vWorker Labor Marketplace The vWorker [37] market\u00adplace \nboasts a global workforce of over 320,000 registered workers, over 150,000 registered employers, and \n1,500 2,500 projects in open bidding at any given time. Employ\u00aders typically post small to medium business \nprojects ($50 $1000), including design and programming jobs, for work\u00aders to bid on in an open auction \n(invite-only auctions are also possible). The employer then selects a worker(s) to work on the project. \nFor its role as a matchmaker and arbiter, vWorker takes a 7.5% 15% cut from worker pay. Several sites \noffer services similar to vWorker. We chose vWorker due to the .rst author s positive experience with \nthe service in the past. 4.1 Experimental Design We posted a project with the headline Write Java program \nspeci.cations on vWorker. The project posting had users place hourly bids for up to 6 hours of work and \nstated that we would accept multiple bids. We placed no restrictions on the workers. (vWorker lets you \naccept bids only from workers in developed countries, or mandate that the worker use a webcam so that \nyou can monitor their work.) Subject Program We used the StackAr program [39], an implementation of an \narray-based stack, from a previous study of program veri.cation [28]. The program consists of a data \ntype (library class) paired with a test program that throws run-time exceptions if certain correctness \nproperties do not hold. For example, the client checks that a call to isEmpty() returns true for a new \nstack. The StackAr class has 8 methods, consisting of 49 NCNB lines of code. The client class consists \nof 79 NCNB lines of code. Using ESC/Java2, 23 JML annotations are necessary to show that the data type \nand client suffer no unexpected run-time exceptions.  We added clauses and annotations asserting the \ntypes of the objects and arrays so that we did not have to teach the participants the type syntax of \nJML (these clauses were trivial, and were automatically inferred by Daikon). The Daikon-inferred clauses, \nwhich were offered to both VeriWeb and Eclipse users (see below), came from Nimmer s client code, which \nused the library in realistic ways [28]. By contrast, VeriWeb s counterexamples came from the included \nADT clients, which are (rather impover\u00adished) example uses. As a result, the VeriWeb counterexam\u00adples \ndid not add any additional information beyond what the Eclipse users could learn by inspecting the included \nADT clients. Treatments We used the ESC/Java2 plugin for Eclipse 3.52 as the standard user interface. \nVeriWeb suggests possible clauses inferred by Daikon, so to level the playing .eld, we inserted those \nsame the inferred speci.cations in the subject programs used by Eclipse. Both interfaces were hosted \non a group of Rackspace Cloud server instances (Eclipse was served via Windows terminal services). Progress \nin Eclipse was logged by recording each text edit, and recording when the user invoked ESC/Java2 to check \nthe project. Participants We received 26 of.cial bids ranging from $6/hour to $110/hour (Mean: $21.8/hour; \nMedian: $14.5). For the experiment, we accepted the 18 bids in the $6/hour $22/hour range and split the \nparticipants into treatment groups to produce roughly equivalent rate distributions. Complete bid data, \nincluding worker country, can be found on the project website.3 Worker Skill We expected that assigning \nthe groups based on requested pay would result in groups with similar pro\u00adgramming skill distributions. \nWe asked each worker both their programming and Java experience.4 Figure 7 shows that workers bids do \nnot correlate with experience, plac\u00ading into doubt our initial assumption that requested pay is a viable \nproxy for skill. Instructions Each worker was given a web link to a de\u00adscription of JML [6, 22] contract \nsyntax and instructions for how to run their respective tool. Workers were instructed to spend approximately \n1 hour on a warm-up tutorial. Upon completing the tutorial, they were given a quiz about JML speci.cations \nand their tool to ensure comprehension. For each worker s initial quiz attempt, we provided a list of \nques\u00adtions answered incorrectly, references to the sections con\u00adtaining the answers to the questions, \nand had the workers correct their answers before continuing with the main task. Aside from one worker, \nall of the workers answered at least one question incorrectly on their initial quiz attempt. 9 out of \n2 http://secure.ucd.ie/products/opensource/ESCJava2/ 3 http://www.cs.washington.edu/homes/tws/veriweb/ \n4 We asked for this information after the experiment. Asking before might have given workers a motivation \nto lie, in order to obtain a higher wage.  Figure 7. Requested hourly pay (bid) versus programming experience \nreported upon completion of the project. Work\u00aders bids were not an accurate proxy for programming expe\u00adrience. \nthe 14 workers .nishing the project reported spending longer than an hour on the warmup. A deadline of \none week was given for the project; though this deadline was not enforced, it may have contributed to \nthe 4 workers who did not com\u00adplete the project. Edit Distance Performance Metric The number of warn\u00adings \nreported by ESC/Java2 is not an accurate measure of progress, as one incorrect annotation can mask other \nwarn\u00adings. Therefore, we instead use edit distance, the minimal number of additions and removals of top-level \nJML annota\u00adtions that yields a known veri.able solution. This is a lower bound on the amount of work \nrequired to complete the spec\u00adi.cation task. (Actually, we use a modi.ed version of the metric that accounts \nfor the differences between the tools. See the Appendix.) A veri.cation problem has many possible (legal) \nsolu\u00adtions. The target solution set included solutions from [28], our pilot studies, and the solutions \ndiscovered during the study themselves. Whenever a solution included an object invariant, we added another \nsolution in which the object in\u00advariant was explicitly listed as method conditions. The edit distance \nis the distance from a worker s current version of the program, to the nearest of any of these legal \nsolutions. To ef.ciently calculate distance to veri.able speci.ca\u00adtions, we wrote a tool to normalize \nthe speci.cations and perform a textual comparison. The normalization rewrites constant subexpressions \nand inequalities, uses De Morgan s law to split conjunctions (e.g., P ==> Q &#38;&#38;Ris split into \nP ==> Qand P ==> R), etc.  4.2 Results 8 VeriWeb workers and 6 Eclipse workers completed the project; \nthe other workers (3 Eclipse workers, and 1 VeriWeb worker) would not comment on why they dropped out \nof the project. Figure 8 shows the distance to the nearest veri.able Figure 8. Top: progress as a function \nof time. Bottom: progress as a function of money spent. Data series (line) coloring is consistent between \nthe graphs, e.g., the darkest lines in both Eclipse graphs both correspond to the same worker.  solution \nfor the workers as both a function of time and money spent. The graphs illustrate three salient features. \nFirst, on average, the VeriWeb workers completed the project faster and for less money than the workers \nusing Eclipse. However, the relatively small number of workers in the study prevented the rejection of \nboth the null-hypothesis that the completion time distributions are the same and the null-hypothesis \nthat the money spent distributions are the same (the one-tailed Mann-Whitney U-test p-values are 0.0985 \nand 0.0694, respectively). If the Eclipse worker with 14 years of programming experience an American \nwho only charged $9/hr is excluded from the sample, the dif\u00adferences are both signi.cant (one-tailed \np-values of 0.0202 and 0.0116, respectively). The variance in completion time is smaller for VeriWeb. \nThis is promising because one main goal of VeriWeb is to commoditize veri.cation, and low vari\u00adance (predictability) \nand commoditization go hand in hand. However, the null-hypothesis that the variances are equal cannot \nbe rejected when using the non-parametric Brown-Forsythe Levene-type test (p-value is 0.0725). Second, \nthe VeriWeb work.ow is characterized by con\u00adtinual progress with intermittent back-tracking. By contrast, \nthe Eclipse work.ow is characterized both by work toward incorrect solutions and by oscillations around \nlocal minima. As with the reduced variation in completion time, this is en\u00adcouraging with respect to \ncommoditization. Third, while the total cost of veri.cation is linearly corre\u00adlated with total time for \nEclipse (r =0.946), the correlation is lower for VeriWeb (r =0.694) indicating that different workers \nshould be chosen whether you are optimizing for time or money. 4.2.1 Effects of Skill Figure 9 shows \nthe time taken to complete the task given the workers hourly rates and reported programming experience. \nAs previously noted, the rate is likely not a good proxy for skill, as it was not correlated with worker \nexperience. Multiple factors may confound the relationship between bids and skill. Variations in the \ncost of living and exchange rates be\u00adtween countries, and within countries, may cause the as\u00adsumption \nto be violated if the geography of the work\u00adforce is diverse. We adjusted the bids based on an OECD report \non purchasing power parity [30], but this still did not account for the differences (due to intranational \nbid variation).  Figure 9. Left: time to complete the task as a function of hourly wage. Bubble size \nindicates the worker s relative programming experience. Right: time to complete the task as a function \nof the worker s self-reported programming experience. Bubble size indicates the worker s relative hourly \nwage. There is no relationship between wage and productivity for either tool this is likely explained \nby the absence of a relationship between requested hourly wage and programmer experience (see Figure \n7). vWorker employs a worker rating system. Workers with higher ratings may demand a premium; skilled \nworkers with a short work history may proffer a low bid in order to establish a good rating.  4.2.2 \nFeature Usage The 14 successful workers took a short survey about their tool s speci.c features. Suggested \nConditions Contrary to what we found in pilot studies, the VeriWeb users only found the suggested condi\u00adtions \nmoderately helpful (mean of 3 5/8 on a 5-point Likert item), due the to inclusion of incorrect or irrelevant \ncondi\u00adtions. One user also felt that some of the conditions took too much effort to understand. The Eclipse \nworkers found the suggested conditions more helpful (mean of 4 1/3 on a 5-point Likert item). Drag and \nDrop Interface Three of the users preferred the drag and drop interface, four users preferred the text \ninter\u00adface, and one user used both depending on the complexity of the condition (preferring it for complex \nconditions). As expected, the drag and drop interface was preferred for not having to worry about incorrect \nsyntax; text entry was pre\u00adferred for speed. Contract Inlining All of the VeriWeb users reported using \nthe toggle-able contract inlining to solve the task (however two users responses indicate that they misunderstood \nthe feature to which the question was referring). Users reported using the feature to (1) remember conditions \nfor methods that had already been visited, (2) identify missing postcon\u00additions, (3) view multiple method \nspeci.cations simultane\u00adously, and (4) understand unexpected exceptions. Counterexamples VeriWeb generated \nconcrete counterex\u00adamples for 4 of the 24 inferred clauses provided to the workers. While VeriWeb also \nprevented users from writing additional clauses that violated the trace, Eclipse did not the successful \nEclipse workers introduced an additional 26, 13, 9, 9, 3, and 1 falsi.able clauses when working on StackAr. \nThe distribution of these clauses lifetimes were highly skewed with a mean of 34 minutes, and a median \nof just 2 minutes. Unsurprisingly, the Eclipse user who intro\u00adduced 26 falsi.able clauses took the longest \nto complete the task. For 2 of the users, the number of falsi.able precon\u00additions met or exceeded the \nnumber of falsi.able postcon\u00additions introduced. Falsi.able preconditions are especially harmful because \nusers propagate the false preconditions to the method s callers. VeriWeb provided counterexamples to \neach of the work\u00aders, however only four of the eight workers reported be\u00ading aided by counterexamples. \nOne user reported using the feedback to identify an off-by-one error. Another user re\u00adported using the \nfeedback to identify a missing condition for a callee.  4.3 Discussion Feature evaluation Directly measuring \nthe effect of each VeriWeb feature on performance would have required an in\u00adtractably large number of \nsubjects. The survey responses and quantitative results indicate that each of the features con\u00adtributed \nto the usability of VeriWeb. Feature usage metrics suggested that certain features, especially concrete \ncoun\u00adterexamples, are good .rst candidates for individual evalu\u00adation. Threats to Validity Due to the \nsmall study size and single subject program, the performance characteristics observed may not generalize. \nIn particular, the following caveats ap\u00adply:  VeriWeb s speed is bounded by ESC/Java2 s speed, and \nESC/Java2 does not scale.  The subject program is array-based, whereas much Java code is collection-based. \nWe opted to not use a subject program with collections because the ESC/Java2 speci.\u00adcations for collections \nare fragile (often resulting in un\u00adsoundness) and signi.cantly slow the checker.  More general threats \nto validity are discussed in Section 7.3. 5. Veri.cation with Ad-hoc Labor In this section, we explore \nthe (lack of) ad-hoc veri.cation labor supply on Amazon s Mechanical Turk marketplace [1] for performing \nVeriWeb subproblems. 5.1 Mechanical Turk Labor Marketplace Amazon s Mechanical Turk [1] is an online \nmarketplace where employers can post human intelligence tasks (HITs) to be solved by a global ad-hoc \nworkforce. HITs are typi\u00adcally small tasks that are easy for a human to perform but dif.cult to automate. \nCommon tasks include image labeling, preference surveys, and audio transcription. 5.2 Experimental Design \nFor our experiment, we created a batch of 50 HITs with the title Answer questions about Java methods \nand description Describe what must be true when a method is called and after it runs. Each HIT required \nthe worker to complete at least three VeriWeb subproblems (guaranteeing that each worker solved at least \none precondition problem); workers could complete additional subproblems within the HIT for additional \npay. Subject Program The subject program was the array\u00adbased stack, StackAr, previously described in \nSection 4. To provide a consistent VeriWeb experience for each worker, each worker worked on their own \ncopy of the project. Work\u00aders did not work collaboratively with other users. The sub\u00adproblems presented \nto a worker followed the methodology described in Section 2.1.1. Variable Pay Mechanical Turk permits \nrequesters to em\u00adbed external websites within a HIT, informing the website whether the HIT is in preview \nmode, or has been accepted by the worker. We used this information to allow the workers to try using \nVeriWeb before accepting the HIT. Typical HITs pay a .xed rate, however, the service offers the ability \nto pay a bonus for good work. Combined with the preview feature, we use this functionality to randomly \npay a different rate to each worker (between $0.15 and $0.35 a subproblem). We set the static pay rate \nof the HIT to $0.00 and appended the phrase BONUS PER QUESTION to the HIT title and description. The \npreview screen for the HIT informed each worker the amount that they would be paid for solving each subproblem. \nBrowser sessions were used to ensure that each worker only saw a single price, even when previewing the \nHIT multiple times. The VeriWeb interface was modi.ed to display the amount of money the worker had earned \nso far in the top information bar. Learning Curve VeriWeb s steep learning curve relative to other tasks \non Mechanical Turk is a major obstacle. We opted to not provide a formal tutorial, instead relying on \nVeriWeb s contextual help (see Section 2.2.2); additionally, the .rst subproblem was chosen to be trivially \neasy a se\u00adlect requires problem consisting of a single choice suf.cient to eliminate all warnings to \nencourage acceptance.  5.3 Results and Discussion Ideally, the results of the experiment would enable \nthe cre\u00adation of a labor supply curve: the number of subproblems solved vs. pay (per subproblem). However, \nworker response was unenthusiastic fewer than 10 workers accepted the HIT over the course of 3 days. \nWorkers required at least $0.25 per subproblem to complete any subproblems. No Me\u00adchanical Turk worker \ncompleted more than 5 subproblems, indicating that the rate would have to be further increased to account \nfor problem dif.culty. We elected not to perform the study again with higher rates, as the effective \nhourly rate would not have been competitive with the hourly rate for workers contracted via vWorker (see \nSection 4). Threats to Validity In addition to a possible lack of gen\u00aderalizability due to the use of \nthe single subject program StackAr, the following three factors likely affected the ob\u00adserved result: \n The absence of a .xed price for the HIT reduces views of the HIT by workers who search for HITs by payment \nrange.  The HIT is unique and unusual. Workers are unlikely to invest in learning to perform a HIT if \nthey perceive the opportunity to perform similar tasks in the future is low. Conversely, the novelty \nof the HIT may attract some workers that would not normally work on the HIT.  Labor supply may vary \nthroughout the week. However, allowing a HIT to run for a whole week would be sub\u00adoptimal as the HIT \nages, its position in the latest HIT list lowers, decreasing the chance that it will be seen by a worker. \n Despite these threats to validity, it is clear that VeriWeb (or the presentation of VeriWeb) is insuf.cient \nto address the ease-of-use and pro.tability demands of the Amazon Mechanical Turk labor pool. 6. Overhead \nof Collaborative Veri.cation Since VeriWeb decomposes the task of writing a veri.able speci.cation into \nsubtasks, multiple workers can work on the task simultaneously. For programs with limited coupling (modulo \nlibrary code), workers can largely work in parallel. To characterize the overhead incurred when workers \nwork on interdependent subtasks, we observed .ve computer science undergraduate students using VeriWeb \nto individually and collaboratively verify a queue data type and client class.  6.1 Experimental Design \nParticipants The study participants were computer science undergraduate students at the University of \nWashington with 3 5 years programming experience. Use of undergraduates was intended to emulate the use \nof semi-skilled workers. Treatments We observed three treatments. For each treat\u00adment, the participants \n.rst (individually) performed a tuto\u00adrial, and then veri.ed the subject program. Treatment 1: a single \nuser performed the tutorial, took and received feedback on a comprehension quiz, and then veri.ed the \nsubject program.  Treatment 2: two users each individually performed the tutorial, and then collaboratively \nveri.ed the subject pro\u00adgram without the assistance of dynamic counterexam\u00adples.  Treatment 3: two users \neach individually performed the tutorial, individually took and received feedback on a comprehension \nquiz, and then collaboratively veri.ed the subject program.  Treatment 2 was performed before the other \ntreatments. The lack of dynamic counterexamples for Treatment 2 was due to a tool setup error. The quiz \nfrom the study in Section 4 was added in light of the Treatment 2 results (see Section 6.2) we wanted \nto separate the effects of learning and tool comprehension from the effects of collaboration. Subject \nProgram We used the QueueAr program [39], an implementation of an array-based queue, which was also used \nin a previous study of program veri.cation [28]. Like StackAr in the comparative study (Section 4), the \nprogram consists of a data type (library class) paired with a test pro\u00adgram that throws a run-time exception \nif certain correct\u00adness properties do not hold. Figure 10 shows the dependency graph for the subject \nprogram; we selected this subject pro\u00adgram because it exhibits a small amount of parallelism suit\u00adable \nfor the number of workers in the study.  6.2 Results Completion Time The individual participant (Treatment \n1) with 2.5 years of programming experience completed the task in 52 minutes. For reference, Nimmer and \nErnst had previously reported that none of their 41 participants could complete the task within a 60 \nminute period (participants were stopped after an hour) [28]. In both Treatment 2 and Treatment 3, the \nparticipants worked in parallel on subproblems containing warnings caused by a single root cause a missing \nexceptional post\u00adcondition for the enqueue method. For both treatments, these subproblems were the most \ndif.cult for the partici\u00adpants to solve. The pair in Treatment 2 completed the task in 54 minutes (wall \nclock time). 20 of the 54 minutes was spent assessing the missing exceptional postcondition: one participant \nspent 20 minutes studying the checkConstructor method; the other spent 10 minutes on the checkIsEmpty \nmethod. As both participants had stalled, we asked the participants to explain their reasoning out loud. \nBased on this information, we reminded the participants how to mark a callee s excep\u00adtional postconditions \nas insuf.cient. Both participants were then able to continue. The pair in Treatment 3 had not completed \nthe task af\u00adter 55 minutes (wall clock time), but could not continue as a user input caused the tool \nto crash. One participant correctly added an exceptional postcondition to enqueue, but this was insuf.cient \nas the isFull method was missing a postcondi\u00adtion about its return value. The other participant then \nbegan needlessly strengthening method preconditions in an attempt to avoid the exception, ultimately \ntriggering a combination that caused the tool to crash. Messages During the course of the task, neither \nparticipant in Treatment 2 worked on a subproblem that had associated messages from the other participant. \nIn Treatment 3, one of the participants worked on a problem with a message from the other participant \nthat consisted of two syntactically correct JML clause suggestions. Since the tool does not attempt to \nparse messages, the participant saw the message as just an extra step to adding the missing conditions. \n The lack of direct communication was a result of Veri\u00adWeb s rules for assigning subproblems, which \nprefers work\u00aders with knowledge of the subproblem (see Section 2.1.1). Additionally, for Treatment 2, \nthe participants did not intro\u00adduce incorrect speci.cations that required multiple levels of backtracking. \n 6.3 Discussion The results highlight a major bene.t of collaborative veri.\u00adcation with VeriWeb: workers \ncan simultaneously work on the same underlying issue. If one worker gets stuck, another worker might \nbe able to identify and solve the issue by ap\u00adproaching it from a different perspective. The Treatment \n3 results also expose a major drawback of collaborative veri.\u00adcation with VeriWeb: a single worker can \nderail the veri.ca\u00adtion process by solving subproblems incorrectly. For the subject program, VeriWeb \ns problem preference system performed well for presenting a consistent work.ow to the individual users. \nTherefore, future research on the ef.cacy of the message system must still be performed, as the opportunity \nfor interaction increases as the number of workers and size of the call graph increases. Experimental \nDesign Having the participants work side by side conveyed the bene.t of a single observer being able \nto observe the workers simultaneously. Unfortunately, the setup precludes having the participants narrate \ntheir thought process. Additional studies should have multiple observers observing the participants in \nisolation to enable narration. Threats to Validity In addition to the threats described in Section 4.3, \nthe following threats to validity exist: The results may not scale to a larger number of workers working \nsimultaneously due to increased interaction.  All of the participants were native English speakers, \nand the documentation and variable names were in English. Workers with limited ability to read / write \na common language are likely to incur greater overhead when com\u00admunicating with other workers.  The \nparticipants had all completed the same core com\u00adputer science curriculum. In practice, workers may not \nhave shared computer science training, or formal training at all.  Overall, we believe the bene.t of \nbeing able to simulta\u00adneously observe the participants in person outweighed the threats to validity caused \nby our participant selection. 7. Discussion 7.1 Crowdsourced Veri.cation in Practice Our primary study \n(Section 4) explored the hourly pay model for veri.cation in a global marketplace. Such a model lies \nin the middle of the spectrum between ad-hoc labor marketplaces like Mechanical Turk (Section 5) where \nwork engagements are .eeting, and contract labor that is typical of more traditional approaches such \nas outsourcing. It remains to be seen under what conditions the different positions on the spectrum are \noptimal for program veri.cation and other software engineering tasks. In any case, veri.cation and speci.cation \nexperience is scarce. Therefore, for the time being, employers that opt to use crowdsourcing must decide \nwhether to pay a premium for workers with formal methods experience or to train new users to use the \ntechnology, as we did in the vWorker study. The most cost-effective approach is most likely to meet the \nworkers half-way: building tool support to decrease the skill demands of the task, and, at the same time, \noffering targeted training to set the workers up for success. 7.2 Validity of Speci.cations VeriWeb \ngenerates a speci.cation that is suf.cient to prove the lack of runtime exceptions; the correct or intended \nspeci.cation is unknowable without input and validation from the feature designers. However, there are \nthree ways that VeriWeb can be led to the intended speci.cation: 1. Informal documentation (Javadoc) \n 2. Checks in the implementation (e.g., checking of precon\u00additions and throwing an IllegalArgumentException) \n 3. The feature designer can write JML speci.cations for high-level properties  Approaches #1 and #2 \ncan be used by VeriWeb with no extra effort from the feature designer. Approaches #2 and #3 induce veri.cation \ncriteria that lead VeriWeb to .nd the correct speci.cation, while Approach #1 depends on the workers \ntaking cues from the documentation.  7.3 Threats to Experimental Validity In addition to the experiment-speci.c \nthreats enumerated in Sections 4, 5, and 6, the following threats to experimental validity apply. Learning \nEffect Workers may perform better because they acquire more experience (a learning effect ). For the \nstud\u00adies in Section 4 and Section 6, we employed a mandatory warmup and quiz to mitigate the learning \neffect. Program Correctness All of our subject programs are correct in the sense that ESC/Java2 can verify \nlack of run-time exceptions without modifying the source code. The performance and mindset of workers \nlikely depends on (1) whether the program is correct, and (2) whether they believe that the program is \ncorrect. We did not tell the vWorker and Mechanical Turk workers that the programs were correct or incorrect. \nThe vWorker workers might have inferred this from the task description. It is possible that (accurate) \nbelief that a program is correct might qualitatively change the way that the workers would work.  Future \nresearch should investigate work.ows for veri.ca\u00adtion when software modi.cations are necessary, such \nas for .xing bugs or improving design. An intuitive work.ow for dealing with software bugs would be to \nescalate areas that cannot be veri.ed to software developers (possibly escalat\u00ading to more-skilled program \nveri.ers .rst). Program Complexity Real programs often contain poor design: complex control .ow, long \nmethods, undesirable dependencies/coupling, etc. By contrast, the StackAr and QueueAr programs used in \nthe experiments are very sim\u00adple, and lack the use of common features, such as collec\u00adtions. Complexity \nand poor design make all software en\u00adgineering tasks, including writing contracts, more dif.cult. Well-written, \nsimpler code will cost less to work with. Speci.cation Complexity General-purpose libraries, such as \nthe Java JDK, have extremely complicated speci.cations. In our experience, the JML contracts for client \ncode are signi.cantly less complicated. Just as it is not desirable to have low-skilled workers design \nlibraries, it is not desirable to have low-skilled workers write speci.cations for libraries. Additionally, \nin practice, JML speci.cation writers sim\u00adplify contracts by splitting them into cases. VeriWeb cur\u00adrently \ndoes not support cases, so users must use implication, which becomes unwieldy. One way to support cases \nin Veri-Web would be introduce a new subproblem type that asks the user to write (or select) the cases \nfor a method contract. 8. Related Work In this section, we survey related work in program veri.ca\u00adtion \nand crowdsourcing. Interfaces for Traditional Veri.cation Tools The tradi\u00adtional program veri.cation \nliterature does not focus on user interface design, but sometimes contains it as a component. For example, \nHoudini, a static contract inference tool for ESC/Java, produces static HTML reports that contain hy\u00adperlinks \nto locations in the source code [18]. Flanagan and Leino reported that when working with Houdini, they \nre\u00adpeatedly asked questions such as Why didn t Houdini infer this precondition? They note that anecdotal \nevidence shows that the presentation of the refuted annotations and the cor\u00adresponding causes are the \nmost important aspect of the user interface. Unfortunately, the authors did not enumerate any of their \nother questions. Other work has focused on explicit de.ciencies found in tools. For example, Kiniry [21] \naugmented the output of ESC/Java with warnings when the analysis may be either unsound or incomplete. \nPex for Fun (http://www.pexforfun.com/) is a web\u00adbased game designed to help students practice program\u00adming \n[34]. Levels come in two forms: puzzles and coding duels. Puzzles ask the user a question about a method, \nor group of methods, e.g., for what input values does the pro\u00adgram throw an exception? In a coding duel, \nthe user must write a method that behaves the same as a secret implemen\u00adtation. The Pex tool is used \nto generate relevant input values; problems can also include Code Contracts to guide Pex [3]. Usable \nProgram Veri.cation Microsoft Code Contracts [8] provides language support for integrating contracts \ninto pro\u00adgrams in the .NET languages, as opposed to using a separate speci.cation language such as JML. \nThe productized ver\u00adsion of the tool can statically verify some properties; other speci.cations are checked \nat run time. Leino s Dafny lan\u00adguage and veri.er also aims to provide guarantees for imper\u00adative languages \nbeyond that of extended static checking [23]. It provides support for a richer set of properties, translating \nDafny code to the Boogie veri.cation language [2] to pro\u00adduce the necessary conditions for a SAT solver. \nThere is a growing body of work on inferring contracts, using static analysis, dynamic analysis, and \na combination of both. Independent of this work, Yi Wei et al. utilized traces to invalidate quanti.ed \nexpressions generated by generalizing contracts [38]. There is also a push to depart from the de facto \ndelin\u00adeation of program, contracts, and veri.er these new ap\u00adproaches often adopt a pay-as-you-go mentality. \nFor ex\u00adample, pluggable type checking techniques [15, 29] allow users to incrementally extend the standard \ntype system to check richer properties such as nullness, interning, and in\u00adformation tainting. In [32], \nSheard et al. outline how the same philosophy can be applied in dependently typed lan\u00adguages that provide \nlanguage-based veri.cation. They argue that cognitive burden is reduced because properties .t natu\u00adrally \ninto the language. Crowdsourcing Software Engineering Despite the grow\u00ading base of crowdsourcing literature, \nthere has been little academic exploration of crowdsourcing in software engi\u00adneering. The only formal \ncrowdsourcing research of which we are aware focused on end-user programming [33]. The commercial sector \nappears to have taken greater interest. For example, uTest is a start-up that uses crowdsourcing to pro\u00advide \non-demand software testing services [36]. Ad-hoc Labor Markets The creation of Mechanical Turk in 2005 \nspurred crowdsourcing research by providing easy access to a global workforce for ad-hoc tasks. As the \n.eld matures, research is transitioning from one-off proof of con\u00adcept projects to crowdsourcing frameworks \n[25] and formal modeling of work.ows [10]. As the understanding of dynamics of the labor market develops, \ntools that combine ad-hoc labor to achieve a larger goal are possible. For example, Soylent, a word processor \nbacked by MTurk, introduces a Find-Fix-Verify pattern for managing worker variance in tasks [5]. In the \nFind phase, users identify parts of the document that need more work. In the Fix phase, workers propose \nrevisions to the parts identi.ed during the Find phase. Finally, in the Verify phase, workers determine \nwhich suggestions are the best.  Economics of Ad-Hoc Crowdsourcing Horton and Chilton [20] construct \na formal model of labor supply based on reservation wage the wage that causes the bene.t of per\u00adforming \na task to exceed the bene.ts of performing other tasks. They characterize this wage with two experiments: \n(1) increasing task dif.culty while keeping a workers wage constant and (2) decreasing the wage while \nkeeping the dif.\u00adculty constant. The task they used, clicking between two ver\u00adtical rectangles, requires \nlittle skill. The lack of skill require\u00adment is commonplace among the economic crowdsourcing literature. \nMason and Watts [26] explore the rationality of the Mechanical Turk workplace, that is to what extent \nthe workspace satis.es traditional economic models in which pay determine economic quality and quantity. \nTo vary the price paid to workers, they set a base rate for the HIT and use Mechanical Turk s preview \nfunctionality to inform workers of an additional bonus. They .nd that pay has a positive re\u00adlationship \nwith the quantity of work performed on a task, but does not have a measurable effect on the quality. \nFur\u00adthermore, they provide evidence that enjoyment, intrinsic motivation, and other factors play a signi.cant \nrole in the marketplace. 9. Conclusion We have presented VeriWeb, a web-based tool for program veri.cation \nthat incorporates novel interface features to ac\u00adcommodate task decomposition and reduce the level of \nskill required to write veri.able speci.cations for programs. It is designed around the principles of \nactive guidance and expla\u00adnations in context, helping its users to do the right thing and to understand \nwhy it is the right thing. To evaluate VeriWeb, we performed three experiments with two small subject \nprograms. While we found evidence that current ad-hoc crowdsourcing marketplaces cannot sup\u00adport VeriWeb, \nwe found that VeriWeb lowered the monetary and time cost of veri.cation when using contract workers. \nAdditionally, we observed that VeriWeb can be used collab\u00adoratively with minimal communication overhead. \nOur vision is a world in which software veri.cation is more economically feasible, and therefore is performed \nmore often. A path to this vision is crowdsourcing: making veri.cation tasks accessible to a broad range \nof developers, even those with relatively little training. Our work is one small step along this path. \nAcknowledgments We thank Colin Gordon, Brian Burg, and Sai Zhang for their feedback on VeriWeb s user \ninterface. Additionally, we thank all of the students and workers who participated in pilot studies and \nprovided invaluable feedback, and the anonymous reviewers for their thoughtful suggestions. This material \nis based upon work supported by a Na\u00adtional Science Foundation Graduate Research Fellowship under Grant \nNo. DGE-0718124 and DARPA under Grant No. FA8750-11-2-0221. References [1] Amazon Mechanical Turk, Apr. \n2012. https://www. mturk.com/. [2] M. Barnett, B.-Y. E. Chang, R. DeLine, B. J. 002, and K. R. M. Leino. \nBoogie: A modular reusable veri.er for object-oriented programs. In FMCO, pages 364 387, 2005. [3] M. \nBarnett, M. Fahndrich, P. de Halleux, F. Logozzo, and N. Tillmann. Exploiting the synergy between automated-test\u00adgeneration \nand programming-by-contract. In 31st Interna\u00adtional Conference on Software Engineering, pages 401 402, \nMay 2009. [4] M. Barnett, K. R. M. Leino, and W. Schulte. The Spec# programming system: An overview. \nIn CASSIS, pages 49 69, Mar. 2004. [5] M. S. Bernstein, G. Little, R. C. Miller, B. Hartmann, M. S. Ackerman, \nD. R. Karger, D. Crowell, and K. Panovich. Soy\u00adlent: a word processor with a crowd inside. In Proceedings \nof the 23nd annual ACM symposium on User interface soft\u00adware and technology, UIST 10, pages 313 322, \nNew York, NY, USA, 2010. ACM. [6] L. Burdy, Y. Cheon, D. Cok, M. D. Ernst, J. Kiniry, G. T. Leavens, \nK. R. M. Leino, and E. Poll. An overview of JML tools and applications. STTT, 7(3):212 232, June 2005. \n[7] B. V. Chess. Improving computer security using Extended Static Checking. In IEEE Symposium on Security \nand Privacy, pages 160 173, Berkeley, California, May 12 15, 2002. [8] Code Contracts user manual. http:// \ndownload.microsoft.com/download/C/2/7/ C2715F76-F56C-4D37-9231-EF%8076B7EC13/userdoc. pdf, Sept. 2010. \n[9] D. R. Cok and J. R. Kiniry. ESC/Java2: Uniting ESC/Java and JML. In Construction and Analysis of \nSafe, Secure, and Interoperable Smart Devices, CASSIS 2004, Revised Selected Papers, volume 3362 of LNCS, \npages 108 128, Marseille, France, Mar. 10 13, 2004. [10] P. Dai, Mausam, and D. S. Weld. Decision-theoretic \ncontrol of crowd-sourced work.ows. In AAAI Conference on Arti.cial Intelligence, pages 1168 1174, 2010. \n[11] L. De Moura and N. Bj\u00f8rner. Z3: An ef.cient SMT solver. In TACAS, pages 337 340, Apr. 2008. [12] \nD. Detlefs, G. Nelson, and J. B. Saxe. Simplify: A theorem prover for program checking. Technical Report \nHPL-2003\u00ad148, HP Labs, Palo Alto, CA, July 23, 2003.  [13] D. L. Detlefs. An overview of the Extended \nStatic Checking system. In Proceedings of the First Workshop on Formal Methods in Software Practice, \npages 1 9, Jan. 1996. [14] D. L. Detlefs, K. R. M. Leino, G. Nelson, and J. B. Saxe. Extended static \nchecking. SRC Research Report 159, Compaq Systems Research Center, Dec. 18, 1998. [15] W. Dietl, S. Dietzel, \nM. D. Ernst, K. Mus\u00b8lu, and T. Schiller. Building and using pluggable type-checkers. In ICSE, pages 681 \n690, May 2011. [16] M. D. Ernst, J. Cockrell, W. G. Griswold, and D. Notkin. Dynamically discovering \nlikely program invariants to support program evolution. IEEE TSE, 27(2):99 123, Feb. 2001. [17] M. D. \nErnst, J. H. Perkins, P. J. Guo, S. McCamant, C. Pacheco, M. S. Tschantz, and C. Xiao. The Daikon sys\u00adtem \nfor dynamic detection of likely invariants. Sci. Comput. Programming, 69(1 3):35 45, Dec. 2007. [18] \nC. Flanagan and K. R. M. Leino. Houdini, an annotation assistant for ESC/Java. In Formal Methods Europe, \npages 500 517, Mar. 2001. [19] C. Flanagan, K. R. M. Leino, M. Lillibridge, G. Nelson, J. B. Saxe, and \nR. Stata. Extended static checking for Java. In PLDI, pages 234 245, June 2002. [20] J. J. Horton and \nL. B. Chilton. The labor economics of paid crowdsourcing. In Proceedings of the 11th ACM Conference on \nElectronic Commerce, EC 10, pages 209 218, 2010. [21] J. R. Kiniry, A. E. Morkan, and B. Denby. Soundness \nand completeness warnings in ESC/Java2. In Proceedings of the Fifth International Workshop on Speci.cation \nand Veri.cation of Component Based Systems (SAVCBS), 2006. [22] G. T. Leavens, A. L. Baker, and C. Ruby. \nPreliminary design of JML: A behavioral interface speci.cation language for Java. ACM Softw. Eng. Notes, \n31(3), Mar. 2006. [23] K. Leino. Dafny: An automatic program veri.er for functional correctness. In E. \nClarke and A. Voronkov, editors, Logic for Programming, Arti.cial Intelligence, and Reasoning, volume \n6355 of Lecture Notes in Computer Science, pages 348 370. Springer Berlin / Heidelberg, 2010. [24] K. \nR. M. Leino and G. Nelson. An extended static checker for Modula-3. In Compiler Construction 98, pages \n302 305, Apr. 1998. [25] G. Little, L. B. Chilton, M. Goldman, and R. C. Miller. Turkit: tools for iterative \ntasks on mechanical turk. In Proceed\u00adings of the ACM SIGKDD Workshop on Human Computation, HCOMP 09. \nACM, 2009. [26] W. Mason and D. J. Watts. Financial incentives and the per\u00adformance of crowds . In Proceedings \nof the ACM SIGKDD Workshop on Human Computation, HCOMP 09, pages 77 85, New York, NY, USA, 2009. ACM. \n[27] G. Nelson. Techniques for Program Veri.cation. PhD thesis, Stanford University, Palo Alto, CA, 1980. \nAlso published as Xerox Palo Alto Research Center Research Report CSL-81\u00ad 10. [28] J. W. Nimmer and M. \nD. Ernst. Invariant inference for static checking: An empirical evaluation. In FSE, pages 11 20, Nov. \n2002. [29] M. M. Papi, M. Ali, T. L. Correa Jr., J. H. Perkins, and M. D. Ernst. Practical pluggable \ntypes for Java. In ISSTA, pages 201 212, July 2008. [30] Purchasing power parities for GDP and related \nindicators. http://stats.oecd.org/Index.aspx?DataSetCode= PPPGDP, Apr. 2012. [31] T. W. Schiller and \nM. D. Ernst. Rethinking the economics of software engineering. In FoSER, pages 325 330, Nov. 2010. [32] \nT. Sheard, A. Stump, and S. Weirich. Language-based veri.\u00adcation will change the world. In Proceedings \nof the FSE/SDP workshop on Future of software engineering research, FoSER 10, pages 343 348, New York, \nNY, USA, 2010. ACM. [33] K. T. Stolee and S. Elbaum. Exploring the use of crowdsourc\u00ading to support empirical \nstudies in software engineering. In Proceedings of the 2010 ACM-IEEE International Symposium on Empirical \nSoftware Engineering and Measurement, ESEM 10, pages 35:1 35:4, New York, NY, USA, 2010. ACM. [34] N. \nTillmann, J. de Halleux, and T. Xie. Pex for fun: Engineer\u00ading an automated testing tool for serious \ngames in computer science. Technical Report MSR-TR-2011-41, Microsoft Re\u00adsearch, Redmond, WA, March 2011. \n[35] U.S. Bureau of Labor Statistics. Computer software engi\u00adneers, applications, May 2010. http://www.bls.gov/oes/ \ncurrent/oes151031.htm. [36] uTest: Software testing, Apr. 2012. http://www.utest. com. [37] vWorker.com: \nMore capable, accountable and affordable. guaranteed., Apr. 2012. http://www.vworker.com. [38] Y. Wei, \nC. A. Furia, N. Kazmin, and B. Meyer. Inferring better contracts. In Proceedings of 2011 International \nConference on Software Engineering (ICSE 2011), 2011. [39] M. A. Weiss. Data Structures and Algorithm \nAnalysis in Java. Addison Wesley Longman, 1999. Appendix: Edit Distance Adjustment The edit distance \nmetric assumes that each contract is either present or not present. However, VeriWeb introduces a third \nstate for a contract, because VeriWeb remembers contracts that are not currently in use but may be tried \nor presented to the user in the future. This state is not relevant to the Eclipse computation. We adapt \nthe edit distance metric as shown in Table 1 on the following page. Any user-written clause that is not \nin the target set counts toward the edit distance, as does any clause that is in the target set but not \nin the candidate set either as a user-written or VeriWeb-suggested clause. In general, a VeriWeb-suggested \nclause is treated like a user-written one. The exception is that a correct postcondition that has not \nyet been proved by VeriWeb is not counted against the user. Such a clause typically exists only because \nthe user has not proceeded to the appropriate postcondition subproblem; when the user does, the postcondition \nwill be automatically added by VeriWeb without any human intervention.  A clause can be proven but wrong \neither because the proof depends on other wrong clauses, or because the target speci.cation does not \ninclude that clause (another veri.able speci.cation, however, might include the clause). Sensitivity \nto Inferred Invariants The distance metric is sensitive to the inferred set of invariants. Let I be the \nin\u00adferred speci.cation with preconditions IR and postcondi\u00adtions IE. Let X be the nearest veri.able speci.cation \nwith preconditions XR and postconditions XE. The distance for the Eclipse user is |X \\ I| +|I \\ X| , \nthe number of conditions that were not inferred by Daikon plus the number of conditions that were incorrectly \ninferred by Daikon. For a possibly different nearest veri.able solu\u00adtion X, the distance for the VeriWeb \nuser is |XR| +|XE \\ IE| , the number of preconditions in the solution plus the number of postconditions \nthat inference failed to detect. The differ\u00adence in distance is given by: (|XR \\ IR| +|XE \\ IE| +|I \\ \nX|)- (|XR| +|XE \\ IE|) =(|XR \\ IR| +|IR \\ XR| +|IE \\ XE|)-|XR| =(|I \\ X| +|IR \\ XR|)- (|IR \\ XR| +|IR \nn XR|) = |I \\ X|-|IR n XR| , the number of incorrectly inferred conditions less the num\u00adber of correctly \ninferred preconditions. Invariant source Eclipse VeriWeb Object invariants User-written WRONG n/a Generalized \nn/a WRONG Missing RIGHT RIGHT Preconditions User-written WRONG WRONG User-selected n/a WRONG Not user-selected \nn/a RIGHT Missing RIGHT RIGHT Postconditions User-written WRONG WRONG Proven n/a WRONG Unproven n/a (none) \nMissing RIGHT RIGHT Table 1. Which clauses count toward the edit distance metric, when comparing a candidate \nspeci.cation (set of clauses) with a target speci.cation. When considering a con\u00adtract clause in the \ncandidate set, WRONG means to count any clause that is not in the goal set, and RIGHT means to count \nany clause that is in the goal set. The Eclipse column indicates that the edit distance for a precondition \nis the sum of the number of wrong user\u00adwritten clauses, plus the number of missing (but necessary, or \nRIGHT ) clauses. The VeriWeb column indicates that the edit distance is the sum of the number of user-written \nand user-selected wrong clauses, plus the number of non-user\u00adselected (but necessary) clauses, plus the \nnumber of missing (but necessary) clauses.     \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Formally verifying a program requires significant skill not only because of complex interactions between program subcomponents, but also because of deficiencies in current verification interfaces. These skill barriers make verification economically unattractive by preventing the use of less-skilled (less-expensive) workers and distributed workflows (i.e., crowdsourcing). This paper presents VeriWeb, a web-based IDE for verification that decomposes the task of writing verifiable specifications into manageable subproblems. To overcome the information loss caused by task decomposition, and to reduce the skill required to verify a program, VeriWeb incorporates several innovative user interface features: drag and drop condition construction, concrete counterexamples, and specification inlining.</p> <p>To evaluate VeriWeb, we performed three experiments. First, we show that VeriWeb lowers the time and monetary cost of verification by performing a comparative study of VeriWeb and a traditional tool using 14 paid subjects contracted hourly from Exhedra Solution's vWorker online marketplace. Second, we demonstrate the dearth and insufficiency of current ad-hoc labor marketplaces for verification by recruiting workers from Amazon's Mechanical Turk to perform verification with VeriWeb. Finally, we characterize the minimal communication overhead incurred when VeriWeb is used collaboratively by observing two pairs of developers each use the tool simultaneously to verify a single program.</p>", "authors": [{"name": "Todd W. Schiller", "author_profile_id": "81392618862", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P3856040", "email_address": "tws@cs.washington.edu", "orcid_id": ""}, {"name": "Michael D. Ernst", "author_profile_id": "81100204056", "affiliation": "University of Washingon, Seattle, WA, USA", "person_id": "P3856041", "email_address": "mernst@cs.washington.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384624", "year": "2012", "article_id": "2384624", "conference": "OOPSLA", "title": "Reducing the barriers to writing verified specifications", "url": "http://dl.acm.org/citation.cfm?id=2384624"}