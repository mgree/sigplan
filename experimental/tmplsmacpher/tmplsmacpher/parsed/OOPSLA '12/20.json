{"article_publication_date": "10-19-2012", "fulltext": "\n Automatically Enhancing Locality for Tree Traversals with Traversal Splicing Youngjoon Jo and Milind \nKulkarni School of Electrical and Computer Engineering Purdue University {yjo,milind}@purdue.edu Abstract \nGenerally applicable techniques for improving temporal lo\u00adcality in irregular programs, which operate \nover pointer\u00adbased data structures such as trees and graphs, are scarce. Fo\u00adcusing on a subset of irregular \nprograms, namely, tree traver\u00adsal algorithms like Barnes-Hut and nearest neighbor, previ\u00adous work has \nproposed point blocking, a technique analo\u00adgous to loop tiling in regular programs, to improve locality. \nHowever point blocking is highly dependent on point sort\u00ading, a technique to reorder points so that consecutive \npoints will have similar traversals. Performing this a priori sort re\u00adquires an understanding of the \nsemantics of the algorithm and hence highly application speci.c techniques. In this work, we propose \ntraversal splicing, a new, gen\u00aderal, automatic locality optimization for irregular tree traver\u00adsal codes, \nthat is less sensitive to point order, and hence can deliver substantially better performance, even in \nthe absence of semantic information. For six benchmark algo\u00adrithms, we show that traversal splicing can \ndeliver single\u00adthread speedups of up to 9.147 (geometric mean: 3.095) over baseline implementations, \nand up to 4.752 (geomet\u00adric mean: 2.079) over point-blocked implementations. Fur\u00adther, we show that in \nmany cases, automatically applying traversal splicing to a baseline implementation yields per\u00adformance \nthat is better than carefully hand-optimized imple\u00admentations. Categories and Subject Descriptors D.3.4 \n[Processors]: [compilers,optimization] General Terms Languages, Algorithms, Performance Keywords locality \ntransformations, temporal locality, cache, irregular programs, tree traversals Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, \nUSA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 1. Introduction Achieving high \nperformance in many applications requires achieving good locality of reference. While there has been \nmuch work on automatic techniques for improving local\u00adity in regular programs, which operate over dense \nmatrices and arrays [20], there has been comparatively little work on general techniques for improving \nlocality in irregular programs, which operate over pointer-based data structures such as trees and graphs. \nThere has been various work to enhance the spatial locality of pointer based structures, ei\u00adther at allocation \ntime or through garbage collection, some of which are automatic [6 8, 22, 38, 40]. On the other hand, \nattempts at enhancing temporal locality has focused on ad hoc techniques that leverage application semantics \n[1, 2, 24, 28, 30, 32, 33, 36], or work well for sparse-matrix style al\u00adgorithms [9, 27, 37], but not \nthe tree-and graph-based al\u00adgorithms that proliferate in domains like data mining and graphics. In this \npaper, we focus on general transformation techniques to improve temporal locality in irregular pro\u00adgrams. \nIf these techniques can be integrated into compilers, then we can offer ef.cient, locality-optimized \nimplementa\u00adtions of irregular algorithms to programmers without requir\u00ading that they carefully hand-optimize \ntheir applications. One important class of irregular applications is traversal codes, applications that \nperform repeated traversals of irreg\u00adular data structures. Examples include well-known scienti.c algorithms \nsuch as Barnes-Hut [3], data mining algorithms such as point correlation and nearest neighbor [16], and \ngraphics algorithms like ray tracing with bounding volume hierarchies [39]. These algorithms feature \nrepeated traver\u00adsals of highly irregular trees, with unpredictable application\u00adand input-dependent traversal \nsizes, shapes and orders. Ex\u00adploiting locality in these algorithms is critical because their performance \nis dominated by memory-access time, and care\u00adless accesses to irregular data structures are likely to \nresult in cache misses. Any technique that can turn a substantial por\u00adtion of those misses into hits \nhas the potential to dramatically improve performance. Prior work focusing on temporal locality in particular \nap\u00adplications has proposed changing the order in which points (i.e. the entities that traverse the tree) \nare processed [2, 24, 28, 36]. By sorting the points such that points processed consecutively have similar \ntraversals, locality can be en\u00adhanced (as discussed in Section 2). Unfortunately, perform\u00ading point sorting \nrequires analyzing the points prior to the traversals to rearrange them effectively. Determining an ef.\u00adcient \nway of performing this a priori sort requires an un\u00adderstanding of the semantics of the algorithm and \nhence highly application-speci.c techniques. Indeed, for some al\u00adgorithms, the traversals are so complex \nthat it is unclear how to do an a priori sort of the points to maximize traversal overlap, even when \narmed with semantic knowledge.  In prior work we discussed an abstract model of tree traversal codes \nthat analogizes them to doubly-nested loops as seen in regular algorithms like vector outer product [19]. \nThe outer loop is a loop of points that must traverse the tree, while the inner loop is a loop over the \nnodes that make up the traversal, irrespective of their position in the tree. Using this model, we proposed \na transformation called point blocking, which essentially tiles the point loop: rather than perform\u00ading \na single point s entire traversal before moving on to the next point, a group of points are placed into \na block, and the block traverses the tree, with each point in the block interact\u00ading with the necessary \nportions of the tree. Point blocking can deliver substantial locality and performance improve\u00adments for \nlarge traversals. Unfortunately, point blocking s performance is sensitive to the order in which points \nare pro\u00adcessed; achieving the best possible performance from point blocking requires that the point sorting \noptimization be per\u00adformed. As a result, point blocking is not a truly automatic, application-agnostic \ntechnique, depending instead on point sorting. Given the dif.culty of reasoning about the behavior or \nlocality of irregular algorithms, it is unlikely that optimiza\u00adtions such as point sorting will be effectively \napplied to most implementations of tree traversal algorithms. In fact, since point sorting is only useful \nas a locality enhancement tech\u00adnique, it may not be applied at all by a non-locality-aware programmer. \nAs a result, point blocking s effectiveness is muted in the typical case: when applied to na\u00a8ive code \nthat does not consider locality effects. Because our goal is to develop automatic compiler transformations \nto improve the locality of these algorithms, we need a transformation that does not assume any semantics-based, \napplication-speci.c intervention by the programmer. Our approach: traversal splicing In this paper, we \npropose a new optimization, traversal splic\u00ading, introduced in Section 3. Much as point blocking tiles \nthe point loop, traversal splicing tiles the traversal loop: each point s traversal is divided into a \nnumber of partial traver\u00adsals, and we perform a partial traversal for all points before moving on to \nthe next partial traversal for any point. There are two key advantages to traversal splicing. First, \nthe performance of splicing is largely independent of the order of points, decoupling its behavior from \napplication\u00adspeci.c sorting transformations. Second, the order in which partial traversals are performed \ncan be changed during exe\u00adcution. In particular, as points traverse the tree, we can use their traversal \nhistory as a predictor of their remaining traver\u00adsal patterns, and group similar points together. In \nessence, traversal splicing sorts the points on the .y but without any application-speci.c optimization. \nSection 4 discusses how to mitigate the overheads of traversal splicing by exploiting general structural \nproperties of traversal algorithms. Section 5 describes an automatic, source-to-source transformation \nframework that can apply traversal splicing to any application that performs repeated, recursive traversals \nof a tree, and presents a tuning frame\u00adwork that automatically selects the appropriate optimization parameters \nfor traversal splicing. We evaluate our traversal splicing framework on six benchmark algorithms, and \nshow that traversal splicing de\u00adlivers (single-thread) speedups of up to 9.147 (geometric mean: 3.095) \nwhen compared to straightforward imple\u00admentations of these algorithms, and up to 4.752 (geomet\u00adric mean: \n2.079) when compared to point-blocked versions. Furthermore, for each benchmark we compare a traversal\u00adspliced \nimplementation to a manually transformed version with both point sorting and point blocking applied. \nWe .nd that across our benchmarks, automatically applying traver\u00adsal splicing to na\u00a8ive implementations \nis competitive with hand-optimized implementations. Contributions The contributions of this paper are: \n The development of traversal splicing, a new, general transformation for tree traversal codes which \ncan effec\u00adtively transform applications in the absence of semantics\u00adbased optimizations.  The implementation \nof a transformation and tuning framework that can automatically transform tree traversal algorithms to \napply traversal splicing.  Experimental evidence that traversal splicing can not only effectively improve \nthe performance of tree traver\u00adsal codes, it can provide results competitive with hand\u00adtransformations \nthat carefully leverage application se\u00admantics.  2. Background 2.1 Tree traversal algorithms and locality \nThe pattern of repeated tree traversals is a recurring theme, appearing in algorithms such as Barnes-Hut \n[3], nearest neighbor [16], iterative closest point [17] and many ray trac\u00ading algorithms [39], among \nothers. We adopt some unifying terminology when discussing these algorithms: points are the entities \nthat traverse the tree (they may be astral bodies in Barnes-Hut, rays in ray tracing, etc.), while nodes \nare the  1 Set<Point> points = / * points */ 2 Node root = buildTree(points); 3 foreach ( Point p : \npoints ) { 4 recurse(p, root); 5 } 6 7 void recurse(Point p, Node n) { 8 if (!canCorrelate(p, n.boundingBox)) \n{ 9 return  10 } else if (n. isLeaf ()) { 11 p.updateCorrelation(n.getPoint()); 12 } else { 13 recurse(p, \nn.leftChild); 14 recurse(p, n.rightChild); 15 } 16 } (a) Pseudocode of point correlation 1 Set<Point> \npoints = / * points */ 2 foreach ( Point p : points ) { 3 foreach (Node n : p.oracleNodes()) { 4 visit(p, \nn); 5 } 6 } (b) Point correlation as doubly nested loop Figure 1. Point correlation individual elements \nof the tree data structures that are be\u00ading traversed. Our de.nition of a tree traversal algorithm is \nthus: an algorithm where each of a set of points recursively traverses a tree of nodes. Note that the \ntraversals in these algorithms are recursive, and hence depth-.rst. To explain the behavior of tree traversal \nalgorithms, we will use point correlation (PC) as an example. The two-point correlation can be calculated \nfor a set of points by determin\u00ading, for each point, p, the number of other points in the set that fall \nwithin a certain radius, r of p. PC is an important al\u00adgorithm in many disciplines, such as bioinformatics \nand data mining [16]. The na\u00a8ive approach to PC would be to compare each point to every other point in \nthe data set, an O(n2) process. To accelerate the procedure, the standard approach is to build a spatial \nstructure over the points called a kd-tree [4]. This structure is built top-down: a root node is created \nwith a bounding box that encompasses all the points. Then a split\u00adplane is computed that partitions the \npoints in the bounding box into two equal pieces, creating two children nodes for the root, each with \ntheir own bounding box. This process is repeated until the leaf nodes contain single points. Now PC can \nbe performed by a recursive traversal of the kd\u00adtree. Each point p starts at the root and only traverses \na child if the bounding box of that child can contain points within r of p. Thus, large portions of the \ntree need not be traversed, reducing the overall run time. The pseudocode for this algorithm is given \nin Figure 1(a). While all tree traversal algorithms we consider have the same basic structure, they each \ntraverse their trees accord\u00ading to different criteria and use trees with different struc\u00adtures (oct-trees \nfor Barnes-Hut, kd-trees for nearest neigh\u00adbor, bounding volume hierarchies for ray tracing), and dy\u00adnamically \nallocate their trees according to input data. Hence, it appears at .rst glance that there may not be \nany uni\u00ad (a) Sample tree for point correlation (running example) (b) Iteration space (c) Iteration \nspace, post-sorting Figure 2. Sample tree and iteration spaces   fying principles governing their \nlocality. However, in prior work, we suggested eliding the traversal pattern, and indeed the tree structure \nitself, and instead considering each point s traversal as if the set of nodes it must visit were provided \nby some oracle [19]. This abstraction yields a consistent view of each algorithm as a simple doubly-nested \nloop, as shown in Figure 1(b) (the visit method abstracts whatever compu\u00adtations a point performs when \nit visits a node of the tree). As we shall see, this simple abstraction allows us to reason ef\u00adfectively \nabout not only the locality behavior of a tree traver\u00adsal algorithm, but also the effects of transformations \napplied to the algorithm. Figure 2(a) shows a sample kd-tree for PC, with nodes numbered in heap order, \nand .gure 2(b) shows an iteration\u00adspace diagram for one set of traversals; this will serve as a running \nexample throughout the paper. Each circle in the iteration space diagram represents one dynamic instance \nof the loop body. The vertical axis shows the outer loop over the points, while the horizontal axis shows \nwhich nodes are visited by each point. Note that each point does not visit each node. We can use reuse \ndistance [25] to analyze the locality behavior of the algorithm. We note that each point enjoys good \nlocality, while each tree node has relatively poor locality: concentrating on tree node 4 @, we see that \nbetween point A s .rst access to 4@, @ and C s reuse of 4we must visit all 14 other nodes of the tree \nbefore we return to 4 @. In general, the reuse distance for most nodes will be proportional to the size \nof the tree.  2.2 Point sorting One popular approach to improving the locality of tree\u00adtraversal codes \nis to sort points so that points with similar  1 Set<Block<Point >> blocks = / * points */ 2 foreach \n( Block<Point> b : blocks ) { 3 foreach (Node n : b.oracleNodes()) { 4 foreach (Point p : b.validPointsAt(n) \n{ 5 visit(p, n); 6 } 7 } 8 } (a) Abstract pseudocode for point blocking (c) Iteration order for point \nblocking  Figure 3. Point blocking Bench Runtime CPI L2 miss rate mark (seconds) Unsort Sort Unsort \nSort Unsort Sort BH 102.7 27.9 2.44 1.23 0.429 0.021 PC 448.0 216.9 1.70 0.85 0.385 0.043 NN 383.8 190.2 \n2.00 1.13 0.486 0.227 kNN 529.3 147.8 2.43 0.91 0.481 0.226 BT 687.2 245.2 2.12 0.81 0.431 0.184 RT 155.5 \n102.5 1.31 1.10 0.275 0.163 Table 1. Ef.cacy of sorting with point blocking traversals are executed \nclose together, as in Figure 2(c), which shows the same traversals as Figure 2(b), but in a different \norder. Because points have different traversals, this sorting can reduce reuse distance; when point A \nand point C are executed consecutively, the reuse distance of tree node 4 @ drops to 8, and, in general, \nthe reuse distance for a node will be proportional to the size of a traversal. Unfortunately, the right \nexecution order for points is application-speci.c and can require signi.cant programmer effort to divine. \nProposals that rely on point sorting adopt approaches such as using the tree-order of points [36] or \nspace-.lling curves [2]. Choosing the appropriate order for a given algorithm requires deep knowledge \nof the algorithm s behavior; this is especially problematic for algorithms such as nearest neighbor, \nwhere different points traverse the tree in different orders. In Section 3, we introduce a new local\u00adity \noptimization that performs this sorting on-the-.y and does not require any application-speci.c knowledge \nto im\u00adplement.  2.3 Point blocking Though sorting is a useful optimization that reduces the reuse distance \nfor tree nodes to be on the order of traversal size, it loses its effectiveness when inputs, and hence \ntraver\u00adsal sizes, get too large. Prior work introduced point blocking as a method for improving locality \nfor tree traversal codes when traversal sizes attenuate the bene.ts of sorting [19]. The essence of point \nblocking is to tile the point loop, yielding the abstract pseudocode shown in Figure 3(a). The arrows \nin Figure 3(b) show the new iteration order when ap\u00adplying point blocking to the sorted traversals of \nFigure 2(c), with block size 3. Note that the tree nodes enjoy improved locality: they will incur misses \nonce per block, instead of once per point. Further, the reuse distance of a point is on the order of \nthe block size, so as long as blocks are properly sized, points will suffer only cold misses. We note, \nhowever, that point blocking s effectiveness re\u00adlies on point sorting as a preprocessing pass. A more \nprecise characterization of the locality behavior of a point blocked code is that a tree node suffers \none miss per block that visits it. If the points are sorted, then points that visit a particular node \nare likely to be collected into a relatively small num\u00adber of blocks, and hence the node will suffer \nfew misses. If the points are unsorted, each block will have to visit more tree nodes (as its points \ntraversals will have less overlap and hence cover more ground). Thus, each tree node is visited by more \nblocks, and will suffer more misses. Consider node 4 @from our running example. In the sorted version \nof the point\u00adblocked code (Figure 3(c)), all the points that visit 4 @ are in the same block, and 4 @ \nonly suffers a single miss. However, if we were to apply point blocking to the original order of points \nfrom Figure 2(b), we note that two blocks would have to visit 4 @, resulting in two misses. Table 1 shows \nthe runtimes, CPI and L2 miss rates of several benchmarks with point blocking, with and without sorting1. \nWhile point blocking can often improve perfor\u00admance for both scenarios with and without sorting, the \ngap between sorted point blocking and unsorted point blocking remains signi.cant. Hence we would like \nto combine point blocking with point sorting to achieve the best performance. Unfortunately, as discussed \nbefore, performing sorting re\u00adquires application-speci.c knowledge, and a general trans\u00adformation cannot \nrely on having sorted inputs. In the next section, we introduce a transformation whose performance is \nless dependent on the order in which points are processed, obviating the need for application-speci.c \nsorting. 3. Traversal Splicing This section introduces traversal splicing, a novel transfor\u00admation for \ntree traversal algorithms that addresses the short\u00adcomings of the techniques discussed in Section 2. \nIn par\u00adticular, it does not rely on any application-speci.c seman\u00adtic knowledge (e.g., how to sort points) \nto work effectively. In other words, traversal splicing can deliver good results even for simple, baseline \nimplementations, without relying on programmer intervention to enhance locality. 3.1 What is traversal \nsplicing? The most intuitive way to visualize traversal splicing is that, rather than tiling the point \nloop, as in point-blocking, it tiles the traversal loop, yielding the abstract pseudocode of Fig\u00adure \n4(a). Thus, rather than picking a block of points and fol\u00adlowing their traversals through the entire \ntree, traversal splic\u00ad 1 The results in Table 1 are obtained via PAPI [10], with the random inputs on \nthe Opteron II machine, described in more detail in Section 6.1.2.  1 Set<Point> points = / * points \n*/ 2 Set<Set<Node>> traversals = / * tree */ 3 foreach ( Set<Node> t : traversals ) { 4 foreach ( Point \np : points ) { 5 foreach (Node n : p.oracleNodesWithin(t) { 6 visit(p, n); 7 } 8 } 9 } (a) Abstract \npseudocode for traversal splicing (b) Iteration order for traversal splicing Figure 4. Traversal splicing \n  ing takes a single point and executes a partial traversal of the tree. It then takes the next point \nand executes a partial traversal and so on. Once each point has executed a partial traversal, the .rst \npoint s traversal picks up from where it left off. This process can be extended by dividing each traversal \nup into several partial traversals, whose executions are inter\u00adleaved. In essence, each point s traversal \nis chopped up into pieces, and the pieces are rearranged and stitched together in a different order; \nhence, traversal splicing. The nodes at which the traversals are paused are called splice nodes. Figure \n4(b) shows the effects that traversal splicing has on the iteration space from Figure 2(b), with the \niterations that visit the splice nodes ( @4, 567 @, @ and @) .lled in2. Note that the partial traversals \nare executed in lock-step, and if a point does not encounter a splice node (consider point B, which does \nnot visit node 45 @ or @), it resumes once all other points have arrived at the next node it should visit. \nWe can use the iteration space diagram to reason about the locality effects of traversal splicing. We \nnote that each node in the tree has good locality. The reuse distance of a tree node is bounded by the \ndistance between splice nodes. As long as the splice nodes are not too far apart (i.e., each set of nodes \nin line 3 of Figure 4(a) .ts in cache), we get only cold misses on the tree nodes. The points, however, \nwill miss once per partial traversal (when a point pauses at a splice node, it will not be re-accessed \nuntil every other point has completed a partial traversal). Interestingly, the order of points is irrelevant \nto this locality analysis. Hence, traversal splicing is agnostic to whether or not the points are sorted. \nDynamic sorting Note that the above analysis assumes that splice nodes are not too far apart. Unfortunately, \nthe structure and size of the tree can be extremely irregular, lead\u00ading to sets of nodes (line 3 of Figure \n4(a)) that may exceed cache. However, the partial traversals of points (line 5 of 2 Note that this assumes \nthat every point in the algorithm performs its depth\u00ad.rst traversal in the same order; we discuss the \nimplications of algorithms where points can visit nodes in different orders later. Figure 4(a)) may still \n.t in cache. In this situation, the or\u00adder of points once again matters: points with similar partial \ntraversals between splice nodes can enjoy good locality if they are processed consecutively. But how \ncan this reorder\u00ading be done automatically? To perform this scheduling, we exploit a key insight about \ntree traversal algorithms. Two points that reach the same splice node of a tree have had similar traversals \nup to this point (points with substantially dissimilar traversals will have been truncated prior to arriving \nat the splice node). Furthermore, their traversals similarity implies that the con\u00adtinuations of the \ntraversals are likely to be similar as well. We can thus use the order in which points reach splice nodes \nas a proxy for the similarities of their traversals. Hence, we will reorder the points as they arrive \nat splice nodes. As an example, consider applying this strategy to the traversals of Figure 4(b). We \nwill process the points in their original order until splice node 4 @. Because points B and D did not \nreach node @, they will be reordered with respect 4to points A, C and E. The order in which the points \nwill be processed for the partial traversals between 45 @ and @ is (A, C, E, B, D). Note that this new \norder is precisely the order in which the points would have been executed had they been sorted a priori \n(see Figure 2(c)). As the traversals continue, the points arrive at 5 @ in their current order, so no \nreordering is done. Next, the points will continue on to 6Note that the reuse distance for @ is @. 6improved: \npoints B and D are now processed consecutively. At 6 @, the points will be reordered again, to (E, B, \nD, A, C), and so on. This continuous reordering has the effect of sorting the points as they traverse \nthe tree, so that points with similar traversals will wind up near each other in the processing order. \nCombining traversal splicing and point blocking Finally, if we consider the inner two loops of Figure \n4(a), we note that it looks like the abstract version of the original traversal code. If the partial \ntraversals become too large, it is clear that applying point blocking to the inner two loops (which, \nrecall, are now operating over dynamically sorted points) can further enhance locality. Section 5.4 discusses \nhow applying point blocking to partial traversals allows our transformed code to tolerate larger partial \ntraversals, and hence reduces its sensitivity to splice node placement. Section 6 quanti.es the performance \nof point blocking and traversal splicing individually, and the combination of the two.  3.2 More complex \ntraversals Matters are more complicated when traversals of points do not take the same path through the \ntree. In PC, each point traverses the tree in the same order, aside from truncations; there is a single \nglobal traversal order, and each point s traversal is a .ltered subset of that order. Figure 5 shows \nthe pseudocode for nearest neighbor (NN), in which the traversal order is not .xed. For example, at a \ngiven node, some points may visit the node s left child before its right, while other points visit its \nright child before its left.  This scenario is both a more complex challenge for traversal splicing \nand a more promising opportunity. In ap\u00adplications like PC, each point visits the splice nodes in the \nsame order, though truncation may prevent it from reaching a particular splice node. In NN, points may \nvisit splice nodes in different orders, even disregarding the effects of trunca\u00adtion. Because the traversals \nhave the potential to diverge substantially, leaving the points unsorted can yield very poor performance. \nFurther, the exact path of each traversal may be highly input dependent, making a priori sorting dif.cult. \nIn such a scenario, each point follows its prescribed traversal until it reaches its .rst splice node, \neven if different points reach different splice nodes. Splicing, with reorder\u00ading, occurs as before. \nThen points continue on to the second splice node, and so on. Hence, each point s traversal is still \ndivided into partial traversals, and the partial traversals are still executed in lockstep. More precisely, \nthe computation is divided into n phases, one per splice node (and hence one per partial traversal). \nIn phase i, each point p executes the partial traversal starting at the (i - 1)th splice node and ending \nwith the ith splice node in that point s particular traversal. After each phase, the points at each splice \nnode are sorted according to their traversal history as before. Note that this phasing approach is merely \na generalization of the splicing procedure for ap\u00adplications like PC. In PC, because each point follows \nthe same path through the tree, every point s ith phase ends at the same splice node. The only complication \nis when a point is truncated before reaching a splice node, skipping one or more splice nodes. In this \ncase, the truncated point conceptually still participates in that splice node s phase, but without performing \nany work; the point s traversal will resume when the phases for any skipped splice nodes are over. Section \n5 describes this phas\u00ading algorithm, and how it can be scheduled ef.ciently, in more detail. 1 Set<Point> \npoints = / * points */ 2 Node root = buildTree(points); 3 foreach ( Point p : points ) { 4 recurse(p, \nroot); 5 } 6 7 void recurse(Point p, Node n) { 8 if (!canBeCloser(p, n.boundingBox)) { 9 return ;  10 \n} else if (n. isLeaf ()) { 11 p.updateClosest(n.getPoint()); 12 } else { 13 double split = p.value(n.splitType); \n14 if ( split <= n.splitValue) { 15 recurse(p, n.leftChild); 16 recurse(p, n.rightChild); 17 } else { \n18 recurse(p, n.rightChild); 19 recurse(p, n.leftChild); 20 } 21 } 22 } Figure 5. Pseudocode of nearest \nneighbor  3.3 Correctness Traversal splicing performs a comprehensive restructuring of a program s accesses \nto a tree. Care must be taken, there\u00adfore, to ensure that the restructuring does not violate any de\u00adpendences. \nTo explain the criteria governing the correctness of traversal splicing, we appeal to the iteration space \ndia\u00adgram, and draw an analogy between traversal splicing and loop tiling. Traversal splicing is the equivalent \nof tiling the traversal loop in the doubly-nested loop formulation of tree traversal algorithms shown \nin Figure 1(b), and the cor\u00adrectness criteria for loop tiling (that the loop be fully per\u00admutable) still \napply. In particular, the order in which a point visits nodes in its traversal is unchanged, as is the \norder in which a given tree node is visited by different points. Thus, traversal splicing can be performed \nin the presence of intra\u00adtraversal dependences (e.g., if some data associated with the point is updated \nat each leaf node of the point s traversal) or dependences that cross traversals but stay within the \nsame node (e.g., if a counter at each node is updated whenever a point visits the node). When reordering \nis performed during splicing, the cor\u00adrectness criteria change. Each point s traversal still occurs in \nthe prescribed order, and hence intra-traversal dependences are preserved. However, because the order \nin which partial traversals happen can be shuf.ed around, inter-traversal de\u00adpendences are no longer \nguaranteed to be preserved. Nev\u00adertheless, loop parallelizability is a suf.cient condition: if the outer \npoint loop of the original traversal algorithm can be parallelized, traversal splicing is legal. This \nsame condi\u00adtion is necessary for any application-speci.c, ad hoc point sorting optimization to be legal. \n 3.4 Splice node placement In principle, splice nodes can be located at any point in the tree. Indeed, \ndifferent points can use different splice nodes. However, there are certain principles that govern the \nselection of splice nodes. 1. If different points exhibit different traversal orders, the phasing algorithm \noutlined above requires that each point that will encounter a splice node in phase i encounter that phase \ns splice node at the same time. This can easily be accomplished by placing all splice nodes at a uniform \ndepth from the root node . Section 4.4 discusses how this criterion can be relaxed. 2. The deeper in \nthe tree splice nodes are placed, the more splice nodes, and hence partial traversals, there are. Plac\u00ading \nsplice nodes too deep can result in high overhead (due to the extra bookkeeping necessary to keep track \nof par\u00adtial traversals) and poor locality (as points incur a miss once per partial traversal). 3. Conversely, \nif the points are too high in the tree, then too many points will reach the same splice nodes, reducing \nthe ef.cacy of the reordering optimization. Further, the portions of the traversal below the splice nodes \nwill be   (a) Traversal with explicit and implicit splice nodes Figure 6. Top and bottom phases large, \npotentially resulting in poor locality, though this effect can be mitigated by applying point blocking, \nas discussed in Section 3.1 Good splice node placement requires striking a balance between placing the \nnodes too shallow in the tree for reorder\u00ading to be useful and placing them too deep to take advantage \nof reordering. Section 5.4 describes our approach to splice node selection. 4. Optimizations Traversal \nsplicing as described in Section 3 comes at a cost. It requires that traversals be paused and resumed \nat splice nodes. In principle this would require maintaining the full stack for each point s traversal, \nallowing it to be paused at a splice node and its continuation resumed in the next phase. Rather than \nstoring a point s stack in some ancillary data structure, we record the information in the tree itself. \nWhen a point is at some node n in its traversal, each level of its stack can be stored in the appropriate \nancestor of n. Hence, at each node in the tree, we will store, per point, any information needed by the \npoint at that level in its recursion. This includes any local variables of the recursive method, as well \nas a program counter, recording where in the recursive method the point was when it descended from the \nnode. This information needs to be tracked for each point, and can consume space proportional to the \ndepth of any traversal. Because all points are in .ight simultaneously, the amount of extra space required \nper point can lead to prohibitive overheads for traversal splicing. This section discusses optimizations \nthat take advantage of structural characteristics of the target tree traversal algo\u00adrithm that allow \ntraversal splicing to be implemented with far less space overhead. In the following presentation, D refers \nto the depth of the splice nodes (recall from Section 3.4 that we place all splice nodes at a uniform \ndepth from the root). 4.1 Implicit splice nodes To avoid storing stack information at every node along \na point s traversal, we note that partial traversals that start after a splice node (e.g., the partial \ntraversals starting after node 5@ in Figure 4(b)) visit the entire subtree of the splice node before \nreturning higher in the tree. We introduce the concept of implicit splice nodes: nodes that act as splice \nnodes for the purposes of pausing and restarting traversals, but are not explicitly marked by the transformation. \nThe last node visited by any partial traversal in the subtree below an explicit splice node is an implicit \nsplice node. Figure 6(a) shows the resulting decomposition of a traversal over the tree in Figure 2(a). \nThe explicit splice nodes are shaded in gray, while the implicit splice nodes are shaded with diagonal \nlines. We can categorize the partial traversals as top traversals that traverse the top portions of the \ntree and end at an explicit splice node, and bottom traversals that traverse the lower portions of the \ntree and end at an implicit splice node. Notably, the bottom traversals are equivalent to normal recursive \ntraversals over the subtrees rooted at the explicit splice nodes. Therefore, we need only track information \nfor traversal splicing for top phases. Because any top phase is no larger than a single path from the \nroot to a splice node, the maximum amount of space required to track each point is now O(D).  4.2 Reducing \nstack storage With the addition of implicit splice nodes, a point s stack only needs to be explicitly \ntracked in top phases. We next aim to reduce the amount of state that needs to be saved dur\u00ading the top \nphases. We note that tail recursion optimization is commonly performed for recursive methods: if the \nlast oper\u00adation by a recursive method is a recursive call, then the stack does not need to be saved upon \nmaking the call. While the recursive methods in tree traversals tend not to be purely tail recursive \n(as there are recursive calls for each child node), we can consider pseudo-tail recursive methods. A \npseudo\u00adtail recursive method is a recursive method for traversing the tree where any recursive call within \nthe method is imme\u00addiately followed either by another recursive call or a method exit. Because no local \nvariables are used between recursive calls, we need not track any information other than a pro\u00adgram counter \nfor each point at each level in the recursion. To track a point s program counter ef.ciently, we group \neach straight-line series of calls in a pseudo-tail recursive method into a call set. Each call set has \na .xed sequence of recursive calls (i.e., a .xed order of visiting a node s children), so a point s behavior \nat this node is completely determined by which call set it uses, and the call set it uses is computed \nbefore the .rst recursive call. The nearest neighbor (NN) code of Figure 5 shows an example of a pseudo-tail \nrecursive method with multiple call sets. Here the two possible call sets (traversal orders) are {leftChild, \nrightChild} (lines 15 &#38; 16), and {rightChild, leftChild} (lines 18 &#38; 19), and which call set \na point will take is decided before any recursive calls are made. Thus, at each level, a point need only \ntrack which call set it used, and where in the call set it was, to fully reconstruct its stack. The phased \nnature of the traversal splicing algorithm means that every point s location in their respective call \nsets will be aligned; at any given time, every point will be executing the .rst recursive call of their \ncall set, or every point will be executing the second recursive call of their call set, etc. Hence, at \neach depth we can track a global phase number that maintains where in its call set each point is. A point \nonly needs to maintain which call set it is using at a given level.  4.3 Inferred order We can further \nreduce the amount of storage required dur\u00ading traversal splicing by noting that in many algorithms, the \ncall sets of the recursive method follow a particular pattern. Speci.cally, each call set makes the same \nnumber of recur\u00adsive calls, and in a given phase of the algorithm, each call set is operating on a different \nchild. That is, there is no i such that the ith call of two call sets are performed on the same child. \nHence, at a particular level, if we know which phase the algorithm is in (what i is) and we know which \nchild node the point traversed during the phase, we can infer which call set the point used at this level, \nand so no longer need to record it. NN is an example of an algorithm from which order can be inferred. \nThe two call sets each have two calls, and in the .rst phase, the points in the .rst call set visit leftChild, \nand the points in the second call set visit rightChild. In the second phase, we know that any point that \nvisited leftChild must now visit rightChild, and vice versa. Note further that because the data structure \nbeing tra\u00adversed is a tree, knowing where a point is in the tree at any given time during execution uniquely \ndetermines the path from the root to that point. Hence at a given level we need not store which child \nthe point visited, either. This elimi\u00adnates the need to track any information per point aside from a \nglobal phase number per level (shared across all points) and the current tree node the point is at. This \nreduces storage needs to O(1) per point. A special case of inferred order is when there is a single call \nset in a recursive method as in PC (Figure 1(a)).  4.4 Splice node elision Given our policy of placing \nsplice nodes at a .xed depth, cer\u00adtain top phases, which begin just above the splice depth, will necessarily \nbe very short. For example, the partial traversal starting at node 5 @ is only one node long! To avoid \nthe over\u00adhead of performing splicing when it is unlikely to be effec\u00adtive, we elide splice nodes for \nshort phases. That is, if a par\u00adtial traversal is likely to be short, we combine it with the fol\u00adlowing \npartial traversal. In practice, for top phases that begin a short distance above the splice depth D, \nwe do not perform splicing and instead immediately begin a bottom phase, as shown in Figure 6(b). This \ncan be both good and bad for locality. It is good as we incur fewer misses on each point (as we have \nfewer phases), but the combined bottom phase becomes larger, potentially outstepping cache. When splice \nnode elision is combined with point blocking, the latter ef\u00adfect is mitigated. Section 5.4 discusses \nour strategy for splice node elision. 1 Set<Point> points = /* points */ 2 Node root = buildTree ( points \n); 3 mapTree(root); 4 allocBuffers (); 5 foreach (Phase p : unrollRecursion ()) { 6 topPhase(p.depth, \np.phase); 7 bottomPhase ( ) ; 8 } Figure 7. High level view of traversal splicing 5. Implementation \nThis section discusses how to realize the abstract concept of traversal splicing in actual code, how \nthe on the .y sorting is implemented, and how splicing can be applied and tuned automatically. The splice \ndepth is denoted D. 5.1 Splicing and sorting Figure 7 shows a high level view of how traversal splicing \nis implemented. Recall that traversal splicing chops up a point s traversal into many partial traversals \nthat are split at splice nodes, both explicit and implicit (Section 4.1). Bottom phases start at the \nchildren of the explicit splice nodes (nodes at depth D+1), but top phases start immediately after bottom \nphases complete. To .nd the nodes at which phases must begin, we map the top of the tree up to (and including) \nthe children of splice nodes (line 3). Each of these nodes can be the start of a new top or bottom phase. \nFor each of these nodes we allocate buffers into which points can be saved, so they can be resumed by \ntop phases. We also allocate a buffer which can save D +1 call set ids for each point (line 4). The partial \ntraversals are executed as pairs of top and bottom phases as described in Figure 6(a). Because a top \nphase can start at any node up to the splice depth, there are 2\u00d7pD phases (where p is the maximum number \nof recursive calls within a call set, and 2 is for top and bottom each); unrollRecursion (described in \nAppendix A) identi.es each phase. Each top phase (line 6) executes a single path through the tree to \nan explicit splice node, saving points and call set ids into the buffers, so that traversals can be resumed \nby later phases. Each bottom phase (line 7) executes all paths of the subtree rooted at an explicit splice \nnode, without saving points and call set ids. The last node visited in the bottom phase is by de.nition \nan implicit splice node. The .rst top phase starts every point at the root of the tree. Subsequent top \nphases and bottom phases gather points saved into buffers from previous top phases, and resume them at \nappropriate nodes based on the call set id. Bottom phases resume only points paused at explicit splice \nnodes, whereas top phases also resume points saved above splice nodes due to truncation. Hence top phases \ngather points from multiple nodes, and dynamic sorting (described in Sec\u00adtion 3.1) arises naturally as \npoints are reordered based on the node they were saved at. There is no dynamic sorting at bot\u00adtom phases, \nas they gather points from a single node. More details of the implementation of both splicing and dynamic \nsorting are in Appendix A.  (a) Barnes-Hut (BH) (b) Point Correlation (PC) (c) Nearest Neighbor (NN) \nFigure 8. Runtime with varying block sizes and splice depths 5.2 Local variables and intermediary methods \nBecause traversal splicing entails chopping up the recursion, it also requires that all methods surrounding \nthe recursive method be split into prologues and epilogues. All prologues are executed before the .rst \ntop phase, and all epilogues are executed after the last bottom phase. This requires that any local variables \nwhich are de.ned in the prologue(s) and used in the epilogue(s) be saved in additional space allocated \nper point. Further, any local variables within the recursive method passed as an argument to subsequent \nrecursive calls must also be saved for all points at all depths. This can con\u00adtribute signi.cantly to \nheap usage as shown in Section 6.2.  5.3 Automatic transformation We developed a transformation framework, \ncalled TreeSplicer, written as a series of passes in the JastAdd framework [11]. TreeSplicer can automatically \napply traversal splicing, point blocking [19], or a combination of both. It identi.es if splic\u00ading can \nbe applied by examining if the recursive method is pseudo-tail recursive. TreeSplicer does not apply \nsplicing to non-pseudo-tail-recursive codes. TreeSplicer determines which optimization level to use by \nproducing a matrix of all call sets in the recursive method. If there are no con.ict\u00ading traversals in \nany phase, the node inferred optimization can be used. We apply further optimizations when there is a \nsingle call set. Recall that splicing is only performed on parallelizable traversals of trees. We currently \nrely on annotation to ensure that the recursive structure being traversed is a tree, and that there are \nno inter-point dependencies; determining these properties automatically is beyond the scope of this paper. \nA more detailed discussion of TreeSplicer is deferred to Appendix B. We have made the source code of \nTreeSplicer public at https://sites.google.com/site/treesplicer.  5.4 Tuning the transformation Critical \nto the performance of splicing is selecting a good splice depth, D. The parameters are dependent on both \nma\u00adchine (e.g., L1, L2 cache size) and algorithmic characteris\u00adtics (e.g., average traversal size). In \nour point blocking work, we proposed an autotuning technique for selecting block size where a small portion \nof the input points are used to con\u00adstruct blocks of varying sizes, and the best performing block size \nis chosen as the transformation parameter [19]. A simi\u00adlar approach is infeasible for splicing, because \nwe would like to apply splicing over the full set of points. In principle, we would like to choose the \nsplice depth, D, such that each partial traversal completely .ts in cache. In practice, because tree \ntopology is unpredictable, and the size of partial traversals is input and algorithm dependent, .nding \na suitable D is dif.cult. Recall that in Section 3.1, we observed that point block\u00ading can be applied \non top of traversal splicing to mitigate the effects of large partial traversals. Figure 8 shows the \nrun\u00adtime of three benchmarks with varying block sizes and splice depths (on the Opteron system described \nin Section 6.1.2). The .ve lines compare splicing only, to splicing with block\u00ading applied to the bottom \nphases3 with block size 32 1024. We note that point blocking reduces traversal splicing s sen\u00adsitivity \nto D for BH and PC, which have large bottom phases, and does not harm the performance of NN, which has \nsmall bottom phases. Serendipitously, this means that by combin\u00ading splicing and blocking, we need not \nbe as careful about choosing D. Empirical results suggest that a depth at half of the average reach (the \ndepth of the nodes where a point s recursion is stopped) is a good splice depth. We use the autotuning \ntechnique of prior work [19] to choose the block size, and record the average reach of all points in \nthe test blocks. If we are not applying blocking, we use 10 randomly selected points to compute the reach. \nWe set D as half of the average reach. We also determined empirically that splice node elision (see Section \n4.4) should be performed if a top phase begins fewer than D/2 levels above D. As we limit the autotuning \nto less than 1% of the points, it consumes less than 1% of traversal time. The parameters chosen by the \nautotuner for our experiments are presented in Appendix C.  5.5 Discussion Some algorithms dynamically \ngenerate new points. An ex\u00adample would be mirror rays in ray tracing, which we do not currently consider. \nOne possible solution to handle this is to create an outer loop over the point set, and defer dynami\u00adcally \ngenerated points to a subsequent outer-loop iteration. Very large number of points can require prohibitive \namounts of memory, as the memory overhead of splicing increases proportionally to the number of points. \nFor very large num\u00adber of points it would be necessary to split the points into smaller groups, and apply \nsplicing on a group at a time. We currently do not implement these features. 3 We only apply blocking \nto bottom phases because top phases take a single path through the tree, and will be at most D nodes \nlong.  6. Evaluation This section presents our experimental evaluation. We start with evaluation methodology, \nthen explore the memory overheads of splicing. Next we discuss experimental results for serial runs of \neach of our benchmarks, and show that splicing is often competitive with manual optimizations that exploit \nsemantic knowledge. Finally we discuss parallel re\u00adsults and demonstrate that traversal splicing scales \nto many threads. 6.1 Evaluation methodology To demonstrate the ef.cacy of TreeSplicer, we evaluate it \non six tree traversal algorithms, from various domains ranging from scienti.c applications to data-mining \nand graphics4. We evaluate .ve versions of each benchmark. Base: the baseline described for each benchmark \nbelow.  Block: automatic point blocking as described in prior work [19].  Splice: automatic traversal \nsplicing as described in Sec\u00adtion 5.  Block+Splice: automatic traversal splicing combined with automatic \npoint blocking for bottom phases.  Block+SpliceElision: the splice node elision optimiza\u00adtion is added \nto Block+Splice.  Note that our baseline benchmarks are true baselines: no application-speci.c a priori \nsorting is performed. The benchmarks were written in Java and executed on the HotSpot VM 1.7 with 12GB \nheap. Each con.guration was run 8 times in a single VM invocation, the .rst run and the min/max runs \ndropped, and the mean of 5 runs was recorded. We enforced a coef.cient of variation5 of 0.05 by extending \nthe number of runs until steady state if necessary, and this yields errors of at most \u00b16.21% of the mean \nwith 95% con\u00ad.dence [13]. 6.1.1 Benchmarks Barnes-Hut (BH) is a scienti.c kernel for performing n\u00adbody \nsimulation [3]. All n bodies are placed into an oct-tree. Each body traverses the tree to compute the \nforce(s) act\u00ading upon it. In the terminology of our optimization classes from Section 4, BH is an inferred \norder algorithm with a sin\u00adgle call set. We use the implementation from the Lonestar benchmark suite \n[21] with a single iteration, and two inputs. Random is one million randomly generated bodies. Plum\u00admer \nis the class C input from the Lonestar suite, with one million bodies generated from a Plummer model. \nPoint Correlation (PC) is described in detail in Section 2. PC is an inferred order algorithm with a \nsingle call set. 4 We included a benchmark, Lightcuts, in prior work to demonstrate the robustness of \nthe transformation framework [19]. In that work, we were unable to run the benchmark with large enough \ninputs for locality transfor\u00admations to be useful and so do not evaluate its performance in this work. \n5 CoV is the sample standard deviation divided by the mean. We use three inputs. Random has one million \nrandomly generated points in 3 dimensions. Covtype is real data on forest cover type with 580,000 points \nin a 54-dimensional space [12]. We reduced the dimensionality to 7 via random projections [5], and took \n200,000 points in random order. Mnist is real data on handwritten digits with 8,100,000 points in a 784 \ndimensional space [23]. We again reduced the dimensionality to 7, and took 200,000 points. Nearest Neighbor \n(NN) is described in Section 3.2. This implementation saves a bounding box of all points within each \nnode of the kd-tree, and splits nodes until there are four points or fewer in the leaf nodes. NN is an \ninferred order algorithm, but has two call sets. We use three inputs with separate training and test \nsets. For each point in the test set, we .nd the nearest neighbor in the training set. Random has a training \nset and test set of one million points each, randomly generated in a 7-dimensional space. We also used \nthe Covtype and Mnist inputs described above, with a training set and test set of 200, 000 points each. \nk-Nearest Neighbor (kNN) is an optimized k-nearest neighbors benchmark, using a different kd-tree variant \nthan NN. This implementation does not save a bounding box per node. Instead it uses the difference between \nthe split plane and the query point s corresponding dimension value as a termination condition. This \ncondition is less aggressive in pruning nodes than the bounding box, but requires less com\u00adputation. \nThis implementation also saves the median point in non-leaf nodes, providing further speedup by reducing \nthe number of nodes traversed. kNN is an inferred order algo\u00adrithm with two call sets. We use the same \nthree inputs as for NN, with k =5. Ball Tree (BT) is a nearest neighbor benchmark using a ball tree [31]. \nWe use the implementation from the WEKA data mining software [18]. For ef.ciency reasons, our base\u00adline \nuses an optimized distance computation that is tailored to our inputs. However, we did not alter the \ndata structure or the actual traversal code; TreeSplicer was applied to the out of the box traversal \nalgorithm. The distance computation kernel we modi.ed is not touched by our transformations. BT is an \ninferred order algorithm with two call sets. We use the same three input sets as for NN, with Random \nhaving 600, 000 instead of one million points. Ray Tracing (RT) can be accelerated with tree-structured \nbounding volume hierarchies (BVHs) that accelerate ray\u00adobject intersection tests. Our benchmark is extracted \nfrom the BVH-based ray tracer of Walter et al. [39]. Ray tracing is the most general benchmark we tackle, \nas it is not an inferred order algorithm and has four call sets, and hence we must track each call set \nexplicitly6. However, it is pseudo-tail recursive. The input is a randomly generated scene with four \nmillion triangles. We rendered a single frame with 512\u00d7512 eye rays and 32 lights (hence, 32 shadow rays \nper eye ray). 6 The non-inferred order with four call sets is due to the implementation we use. It is \nnot a fundamental limitation of the BVH algorithm.  Benchmark Lines of code Transform time (ms) Input \nTraversal time % Traversal time (s) Heap usage Base (MB) SpliceElision (% increase) -O0 -O1 -O2 Barnes-Hut \n466 1045 Random 99.1 123.6 210 127.6 46.7 18.6 Plummer 85.3 222.8 210 90.5 31.4 18.1 Point Correlation \n396 1037 Random 99.6 895.2 188 132.4 21.3 2.1 Covtype 99.0 776.3 59 96.6 18.6 8.5 Mnist 94.4 147.4 56 \n107.1 21.4 5.4 Nearest Neighbor 450 1004 Random 98.7 380.1 390 47.2 2.6 Covtype 95.8 359.0 72 70.8 9.7 \nMnist 96.6 476.8 74 60.8 2.7 k-Nearest Neighbor 378 1114 Random 99.8 818.2 478 72.8 59.4 Covtype 85.1 \n74.2 92 78.3 52.2 Mnist 92.0 185.8 92 72.8 50.0 Ball Tree 6199 2060 Random 99.1 1102.0 316 51.3 39.2 \nCovtype 93.5 235.8 84 90.5 76.2 Mnist 97.4 737.7 84 92.9 76.2 Ray Tracing 3988 1960 Random 42.9 166.4 \n781 159.9 Table 2. Transform time, traversal time and heap usage 6.1.2 Platforms We evaluate our benchmarks \non three systems with different cache con.gurations. The Opteron system runs Linux 2.6.24 and contains \ntwo dual-core AMD Opteron 2222 chips. Each chip has 128K L1 data cache per core and 1M L2 cache per core. \n The Niagara system runs SunOS 5.10 and contains two 8-core UltraSPARC T2 chips. Each chip has 8K L1 \ndata cache per core and 4M shared L2 cache.  The Opteron II system runs Linux 2.6.32 and contains four \ntwelve-core AMD Opteron 6176 chips. Each chip has 64K L1 data cache per core, 512K L2 cache per core, \nand two 6M shared L3 caches.  6.1.3 Traversal times For each of our benchmarks, we measure the amount \nof time spent in traversals, as this is the portion of code our transfor\u00admations change. RT has two traversal \nphases, the .rst where initial rays are cast, and the second where shadow rays are traced. Because the \ninitial rays in the .rst phase are sorted inherently, we measure the second phase to demonstrate the \nef.cacy traversal splicing on unsorted points7. Column 5 of Table 2 shows the percentage of total time \nspent in traversals for each baseline benchmark/input on the Opteron II system. It can be seen that the \ntraversal time is the dominant compo\u00adnent of the total runtime. The exception in RT is because the BVH \ntree takes considerable time to build. We note that for real applications the same BVH will be used for \nmultiple frames, making the traversal time dominant. Hereafter we will discuss the performance only of \nthe traversal portions of the benchmarks. 6.1.4 Transformation times Columns 2 3 of Table 2 shows the \nlines of code and trans\u00adformation times for our benchmarks8. Our transformations are very quick, amounting \nto less than two seconds. 7 The second phase accounts for 92.3% of the overall traversal time. 8 Transformation \ntimes are on an Intel Core 2 Duo with blocking, splicing, and all optimizations applied.  6.2 Optimizations \nand heap usage Traversal splicing requires additional space to save state of paused points. Columns 7 \n10 of Table 2 shows the heap usage of the baseline algorithms, and the percentage increase of the transformed, \nspliced implementations (with splice node elision) at three optimization levels: -O0 is the default spliced \nimplementation for pseudo-tail recursive codes (Section 4.2). -O1 is an optimized spliced implementation \nwhich ex\u00adploits inferred order (Section 4.3). -O2 is an optimized spliced implementation for codes with \na single call set (Section A.3). The percentage increase is left blank when the optimiza\u00adtion level is \nnot applicable to the algorithm. The memory overheads of splicing comes from extra state used to save \nfour types of data: (i) paused points; (ii) call set ids for non\u00adnode-inferred-order algorithms; (iii) \nlocal variables in inter\u00admediary methods; and (iv) local variables within the recur\u00adsive method. Type \n(i) is proportional to P (the number of points) and is always incurred for traversal splicing. -O2 re\u00adduces \nthe overhead of (i) by having a point buffer per level instead of a point buffer per node for -O1. (ii) \nis proportional to P \u00d7 D and is incurred for -O0. (iii) is proportional to P and is incurred for BH, \nBT and RT. (iv) is proportional to P \u00d7 D and is incurred for kNN. Hence we .nd that PC and NN at -O2 \nwhich incur only (i) have the smallest heap in\u00adcreases, and RT which incurs (i), (ii) and (iii) has the \nlargest heap increase. Note that the % increase depends on the size of points and local variables compared \nto the entire applica\u00adtion footprint, so a direct comparison across benchmarks is dif.cult. Not applying \nsplice node elision increases the heap over\u00adhead by 10.4%, 29.5% and 2.8% respectively for -O0, -O1 and \n-O2, over the heap size for splicing with splice node eli\u00adsion, as there are more phases and hence more \nstate to save.  6.3 Serial results Figure 9 show speedups of Block, Splice, Block+ Splice and Block+ \nSpliceElision over Base for each benchmark/in\u00adput pair, for the three systems. The geometric mean of \nthe  (a) Opteron (b) Niagara (c) Opteron II  Figure 9. Serial improvements: Speedup over Base \n(a) CPI (Cycle per instruction) Figure 10. Performance counters  speedups across all benchmark/input \npairs on the Opteron are 1.787, 3.508, 3.857 and 3.992 for Block, Splice, Block+ Splice and Block+ SpliceElision \nrespectively. The geomet\u00adric mean of the speedups are 1.284, 2.034, 2.210 and 2.358 on the Niagara, and \n1.440, 3.147, 3.022 and 3.151 on the Opteron II. Combined across all systems, the mean speedups are 1.489, \n2.821, 2.953 and 3.095. Figures 10 and 11 shows performance counter results on the Opteron II collected \nwith PAPI [10]. The geometric mean of the L2 miss rate is 0.608 for Base. Applying point block\u00ading in \nBlock reduces this to 0.419. Applying traversal splic\u00ading in Splice independently brings the mean L2 \nmiss rate to 0.206 and combining both point blocking and traversal splicing in Block+Splice reduces this \nto 0.163 which is less than a third of the baseline L2 miss rate. This enhanced lo\u00adcality is re.ected \nin the CPI, which on average is 3.673 for Base, but is reduced to 1.813, 1.164 and 0.982 for Block, Splice \nand Block+Splice respectively. Further adding splice node elision increases the L2 miss rate slightly \nto 0.182 due to larger partial traversals, but decreases CPI to 0.961 likely due to L3 cache on the Opteron \nII. The instruction increase of of Block, Splice, Block+ Splice and Block+ SpliceElision over Base is \n1.396, 1.006, 1.249, 1.214 each. Point blocking often has substantial in\u00adstruction overhead, because \nhighly divergent points result in sparse blocks. The instruction overhead of applying splicing is almost \nnegligible. Combining point blocking with traver\u00adsal splicing reduces the instruction overhead signi.cantly, \nas the dynamic sorting makes the blocks more dense. Adding splice node elision reduces the instruction \noverhead further. We see that point blocking is often an effective opti\u00admization, even without an a priori \nsorting pass. However, traversal splicing consistently outperforms blocking, often by signi.cant amounts, \ndue to its ability to reorder points on the .y. The improvements are particularly marked for kNN where \nwe see speedups up to 9.147 for kNN-Mnist on the Opteron and 6.192 for kNN-Random on the Opteron II. \nOn the Opteron II, this is because the L2 miss rate decreases 5\u00adfold from 0.688 to 0.139, which lowers \nthe CPI 6-fold from 6.13 to 0.99 for kNN-Random. In many cases the bottom phases still outstep cache \neven after traversal splicing. Fur\u00adther applying point blocking to the bottom phases can attain 13% more \nimprovement over the baseline, and adding splice node elision squeezes out 14% more. The inferred order \noptimizations described in Section 4.3 (-O0, -O1 and -O2 in Section 6.2) have insigni.cant impact on \nruntime, within error bounds of the con.dence interval.  6.4 Comparison to manual optimization (sorting) \nIn the previous section, we showed that splicing can attain substantial improvements over both the baseline \nand point blocking with unsorted points. But how far are we from the performance of applications that \nhave been hand-optimized, including with application-speci.c sorting? To explore this question, we compare \nthe Block+SpliceElision version of each benchmark to a new version: Block+Sort, a hand\u00adoptimized implementation \nthat applies point blocking (but not splicing) to manually-sorted points. In essence, this new version \nmanually applies the best-available prior approaches for enhancing locality. Sorting is done in tree \norder [36] for all benchmarks other than RT. For RT, the unsorted baseline uses the default ordering, \nwhere shadow rays are processed in the order they are produced (i.e., in the order of the initial eye \nrays), while the sorted version groups shadow rays according to their light source. Sorting time is included \nfor NN, kNN and BT where sorting has additional overhead because the points are different from the entities \nused to build the tree, amounting to between 0.3% 5.8% of the traversal time.  (a) Barnes-Hut (b) Point \nCorrelation (c) Nearest Neighbor (d) kNN (e) Ball Tree (f) Ray Tracing (a) Barnes-Hut (b) Point Correlation \n(c) Nearest Neighbor (d) kNN (e) Ball Tree (f) Ray Tracing Figure 14. Speedup of transformed versions \non n threads over Base on n threads on Opteron II (a) Scalability on Niagara (b) Scalability on Opteron \nII Figure 15. Scalability of Base Figure 12 shows the performance of Block+SpliceElision normalized to \nBlock+Sort. The ef.cacy of manual, a priori sorting compared to traversal splicing varies with benchmark \nand system. Manual sorting is particularly effective for BH, whereas it is particularly ineffective for \nkNN. The geometric mean of the normalized runtimes is 0.951, 0.986 and 0.961 for the Opteron, Niagara \nand Opteron II systems respec\u00adtively. We see that across our 15 benchmark/input pairs, our fully automatic \ntransformation is competitive with manual, application-speci.c optimization. While thus far we have pitched \ntraversal splicing as an au\u00adtomatic optimization in the absence of semantic information, it remains effective \neven when combined with manual sort\u00ading. The speedups of the sorted counterparts, Block+Sort, Splice+Sort, \nBlock+Splice+Sort and Block+SpliceElision+ Sort over Base+Sort across all benchmarks and systems are \n1.796, 1.650, 1.752 and 1.968 respectively. When the points are sorted, point blocking is generally better \nthan traversal splicing, but a combination of blocking, splicing, and splice node elision can attain \neven more improvements. The over\u00adall improvements are less than for unsorted points, because unsorted \npoints have worse locality to begin with, and hence more room for improvement. We present the full graphs \nfor sorted improvements in Figure 16, in the Appendix.  6.5 Parallel improvements TreeSplicer takes \nsequential code as input, and outputs se\u00adquential code9. To test our benchmarks on multicores, we manually \nparallelized each benchmark, with Java threads. Base and Block were parallelized by processing multiple \npoints or blocks in parallel, and load balancing was ap\u00adplied with work stealing. Splice, Block+ Splice \nand Block+ SpliceElision are not amenable to load balancing as splic\u00ading requires having a large number \nof points up front, to exploit dynamic sorting among them. Splice, Block+ Splice and Block+ SpliceElision \nwere parallelized by statically and uniformly distributing the points among threads, with each thread \napplying splicing to its portion of points. We also measured Base and Block without load balancing and \nfound that load imbalance degrades performance for large num\u00adber of threads. Hence the different parallelization \nstrategy for splicing detracts from the improvements of splicing. We present results up to 64 threads \nfor the Niagara, at which point we are employing 4-way multithreading, and up to 48 threads on the Opteron \nII. In the interests of brevity, we present results for the random inputs for all benchmarks. We .rst \npresent the scalability of the benchmark baselines in Figures 15(a) and 15(b), to show that the baselines \ndo in\u00addeed scale. Figures 13 and 14 show parallel improvements of Block, Splice, Block+ Splice and Block+ \nSpliceElision over Base for each benchmark. While our techniques can de\u00adliver substantial improvements \neven in a parallel setting, the improvements generally decline as the number of threads in\u00adcrease. This \nis due to four factors. First, autotuning is cur\u00adrently done sequentially and limits speedup according \nto Amdahl s law. While we have limited the autotuning over\u00adhead to 1% for the serial case, parallelization \ncan increase its overhead to up to 30% for 64 threads. Second, splic\u00ad 9 TreeSplicer could automatically \ntransform parallel code and apply the appropriate parallelization strategy, if a particular parallelization \nscheme were integrated into it. We currently do not implement this.  ing s dependence on exploring large \nnumbers of points to exploit reordering means that its relative improvement drops as scale increases, \nas each thread processes fewer points. This effect should be mitigated for larger inputs, where each \nthread will receive more points. Third, the static distribution of work leads to signi.cant load imbalance \nat large num\u00adbers of threads. Finally, hardware multithreading hides miss latency and attenuates the \nimportance of locality optimiza\u00adtions. The .rst issue affects all autotuned versions, while the second \nand third affect only the spliced versions. The last issue affects all versions on the Niagara at 32 \n64 threads. 7. Related Work Much of the work on optimizing locality in irregular algo\u00adrithms has focused \non scheduling computation so that tasks likely to access similar data are scheduled in close succes\u00adsion \nto exploit temporal locality. This has been the strat\u00adegy of choice for optimizing sparse-matrix algorithms \n[9, 27, 37], where most approaches use an inspector-executor approach to scheduling. The structure of \nthe computational tasks is found in an inspection phase, which rearranges them to improve locality. The \nrearranged schedule is then exe\u00adcuted. Inspector-executor approaches are less useful for tree traversal \ncodes, as the inspection phase requires performing the traversals, incurring all the misses we hope to \navoid. Scheduling approaches for tree traversals (the sorting optimizations we discuss in Section 2.1) \nhave instead used semantic knowledge to schedule the points without perform\u00ading the traversals, often \nwith space .lling curves [2, 28]. Singh et al. order the points in Barnes-Hut according to their position \nin the oct-tree [36].Mansson et al. propose various sorting optimizations for re.ected rays in ray trac\u00ading \n[24]. In fact sorting for ray tracing is dif.cult and im\u00adportant enough that Moon et al. .rst construct \nand traverse a simpli.ed scene, and sort rays based on hit points of the simpli.ed scene [28]. There \nhave been a few application-speci.c, manual ap\u00adproaches similar to traversal splicing, targeting locality \nfor ray tracing. Pharr et al. partition the scene into a uniform grid of voxels [32]. As rays traverse \na scene, they pause at the boundaries of voxels and are resumed later. By processing rays on a per-voxel \nbasis, locality is improved. They sched\u00adule voxels based on a cost-bene.t heuristic which consid\u00aders \nthe amount of geometry already cached and the number of the rays paused at the voxel. Navr\u00b4atil et al. \npause traver\u00adsals at queue points (akin to splice nodes) in the tree [30]. Queue points are placed so \nthat its subtree .ts entirely in L2 cache. Navr\u00b4 atil s thesis focuses on dynamic schedul\u00ading of rays \nto enhance locality for memory ef.ciency and scalability across thousands of distributed processors [29]. \nAila and Karras extend Navr\u00b4 atil et al. s work to hierarchi\u00adcal queue points and massive parallelism \n[1]. They use dy\u00adnamic programming to place treelets (subtrees at which rays are paused) with balanced \nsurface area, hence minimizing treelet transitions per ray, and discuss considerations for a dedicated \nGPU architecture which supports ef.cient paus\u00ading and resuming of rays. These approaches are able to \nmake smarter decisions on splice node placement and point scheduling (e.g., next voxel to process) with \nsemantic knowl\u00adedge of the algorithm. We believe TreeSplicer could poten\u00adtially make such smart decisions \nautomatically without se\u00admantic knowledge, through a dynamic analysis of traversal patterns at runtime. \nPingali et al. propose computation reordering, whereby an individual computation can be paused during \nits exe\u00adcution and coalesced with other computations that are ac\u00adcessing the same part of the data structure \n[33]. However, computation reordering is more a set of principles for op\u00adtimization than an optimization \nitself: correctly applying reordering requires manually transforming algorithms in application-speci.c \nways. Traversal splicing can be seen as a special, disciplined case of computation reordering, apply\u00ading \nto tree traversals, that can be implemented automatically and ef.ciently. Most prior compiler efforts \ntargeting irregular programs have focused either on analysis, like shape analysis [15, 35], or parallelization \n[14, 26]. Rinard and Diniz prove that the traversal loop is parallelizable through commutativity analy\u00adsis, \nwhereas traversal splicing requires that the point loop be parallelizable [34]. These approaches are \ncomplementary to ours. Our automatic transformation framework can bene.t from analyses to identify tree-shaped \ndata structures or par\u00adallelizable loops over irregular data structures, allowing us to infer properties \nwe currently identify through annotation. A large number of prior studies have investigated improv\u00ading \nspatial locality in irregular algorithms. Truong et al. pro\u00adpose ialloc to facilitate programmer driven \n.eld reorganiza\u00adtion and instance interleaving, where frequently used hot .elds of many instances are \ninterleaved, so that cold .elds do not occupy the same cache line as hot .elds [38]. Chill\u00adimbi et al. \nautomate this process for Java programs with a pro.ling and program transformation tool, and automati\u00adcally \ngenerate .eld reordering recommendations for C pro\u00adgrams [6]. Separate work by Chillimbi et al. propose \ncc\u00admorph to reorganize memory layout of trees and ccmalloc to collocate objects based on programmer provided \nhints [7], and perform relayout during garbage collection [8]. Lattner and Adve use a context-sensitive \npointer analysis to seg\u00adregate distinct instances of heap allocations into separate memory pools, and \nthereby enhance locality at the macro\u00adscopic level of entire data structures [22]. Wang et al. also aggregate \naf.nitive heap objects into dedicated memory re\u00adgions, but do so dynamically without access to the source \ncode [40]. Because these approaches focus on data layout, they do not target temporal locality, as the \ntransformations presented in this paper do. We expect our transformations to be complementary to these \nspatial-locality-enhancing ap\u00adproaches.  8. Conclusions We presented traversal splicing, a comprehensive, \ngeneral, automatic transformation that applies to tree traversal codes such as Barnes-Hut and nearest \nneighbor. Unlike previously proposed transformations, such as point sorting or point blocking, the effectiveness \nof traversal splicing is not depen\u00addent on .rst performing any application-speci.c, semantics\u00adaware hand \ntransformation. Furthermore, traversal splicing exploits the insight that during a point s traversal \nof a tree, its remaining traversal structure can be predicted from its past behavior. Hence, we can splice \ntogether traversals from many points, using this predictive property to group points with similar traversals \ntogether. We showed that when presented with baseline algorithms, our automated traversal splicing transformations \noutper\u00adformed, often substantially, previously presented transfor\u00admations for traversal codes. In fact, \nwe showed that applying traversal splicing to traversal algorithms, even in the absence of any hand optimization, \ncan yield results competitive with manually-transformed, locality-enhanced implementations. We note that \nwhile we only apply traversal splicing to tree traversal algorithms, there is nothing, in principle, \npre\u00adventing a similar optimization from being applied to any ir\u00adregular traversal algorithm. Splice nodes \ncan be viewed as boundary nodes in a tree, and traversals that reach bound\u00adary nodes are paused until \na later date. One could place simi\u00adlar boundary nodes in a graph by performing graph partition\u00ading. A \ngraph traversal (e.g. a graph search) would then oper\u00adate within a single partition, and, when it reached \na partition boundary, would be paused, only to be resumed later when other traversals accessing the same \npartition were found. While the scheduling complexity of applying a splicing-like algorithm to general \ngraphs is substantially higher than for trees, there is nevertheless a possibility of deploying an even \nmore general version of traversal splicing. This is a promis\u00ading area for future work. Acknowledgments \nThis work was supported in part by an NSF CAREER Award (CCF-1150013) and grants from the Purdue Research \nFoun\u00addation and Intel. The authors would like to thank Michael Goldfarb, Hyokun Yun and the anonymous \nreferees for their useful comments. We are grateful to Bruce Walter for pro\u00adviding the Ray Tracing benchmark \ncode, and Vijay Pai and his students for providing support for machines on which our tests were conducted. \nReferences [1] T. Aila and T. Karras. Architecture considerations for trac\u00ading incoherent rays. In Proceedings \nof the Conference on High Performance Graphics, HPG 10, pages 113 122, Aire\u00adla-Ville, Switzerland, Switzerland, \n2010. Eurographics Asso\u00adciation. [2] M. Amor, F. Arg\u00a8uello, J. L\u00b4opez, O. G. Plata, and E. L. Zapata. \nA data parallel formulation of the barnes-hut method for n -body simulations. In Proceedings of the 5th \nInternational Workshop on Applied Parallel Computing, New Paradigms for HPC in Industry and Academia, \npages 342 349, 2001. [3] J. Barnes and P. Hut. A hierarchical o(nlogn) force\u00adcalculation algorithm. Nature, \n324(4):446 449, December 1986. [4] J. L. Bentley. Multidimensional binary search trees used for associative \nsearching. Commun. ACM, 18:509 517, Septem\u00adber 1975. [5] E. Bingham and H. Mannila. Random projection \nin dimen\u00adsionality reduction: applications to image and text data. In Proceedings of the seventh ACM \nSIGKDD international con\u00adference on Knowledge discovery and data mining, KDD 01, pages 245 250, New York, \nNY, USA, 2001. ACM. [6] T. M. Chilimbi, B. Davidson, and J. R. Larus. Cache\u00adconscious structure de.nition. \nIn Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implementation, \npages 13 24, 1999. [7] T. M. Chilimbi, M. D. Hill, and J. R. Larus. Cache-conscious structure layout. \nIn Proceedings of the ACM SIGPLAN 1999 conference on Programming language design and implemen\u00adtation, \npages 1 12, 1999. [8] T. M. Chilimbi and J. R. Larus. Using generational garbage collection to implement \ncache-conscious data placement. In Proceedings of the 1st international symposium on Memory management, \npages 37 48, 1998. [9] C. Ding and K. Kennedy. Improving cache performance in dynamic applications through \ndata and computation reorgani\u00adzation at run time. In Proceedings of the ACM SIGPLAN 1999 conference on \nProgramming language design and implemen\u00adtation, pages 229 241, 1999. [10] J. Dongarra, K. London, S. \nMoore, P. Mucci, and D. Terpstra. Using papi for hardware performance monitoring on linux systems. In \nIn Conference on Linux Clusters: The HPC Revolution, Linux Clusters Institute, 2001. [11] T. Ekman and \nG. Hedin. The jastadd extensible java compiler. In Proceedings of the 22nd annual ACM SIGPLAN conference \non Object-oriented programming systems and applications, pages 1 18, 2007. [12] A. Frank and A. Asuncion. \nUCI machine learning repository, 2010. [13] A. Georges, D. Buytaert, and L. Eeckhout. Statistically rig\u00adorous \njava performance evaluation. In Proceedings of the 22nd annual ACM SIGPLAN conference on Object-oriented \nprogramming systems and applications, OOPSLA 07, pages 57 76, New York, NY, USA, 2007. ACM. [14] R. Ghiya, \nL. Hendren, and Y. Zhu. Detecting parallelism in c programs with recursive data structures. IEEE Transactions \non Parallel and Distributed Systems, 1:35 47, 1998. [15] R. Ghiya and L. J. Hendren. Is it a tree, a \ndag, or a cyclic graph? a shape analysis for heap-directed pointers in c. In POPL 96: Proceedings of \nthe 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 1 15, 1996. [16] \nA. G. Gray and A. W. Moore. N-Body Problems in Statistical Learning. In T. K. Leen, T. G. Dietterich, \nand V. Tresp, editors,  Advances in Neural Information Processing Systems (NIPS) 13 (Dec 2000), 2001. \n[17] M. Greenspan and M. Yurick. Approximate kd-tree search for ef.cient ICP. In Fourth International \nConference on 3-D Digital Imaging and Modeling, pages 442 448, 2003. [18] M. Hall, E. Frank, G. Holmes, \nB. Pfahringer, P. Reutemann, and I. H. Witten. The weka data mining software: an update. SIGKDD Explor. \nNewsl., 11(1):10 18, Nov. 2009. [19] Y. Jo and M. Kulkarni. Enhancing locality for recursive traversals \nof recursive structures. In Proceedings of the 2011 ACM international conference on Object oriented program\u00adming \nsystems languages and applications, pages 463 482, 2011. [20] K. Kennedy and J. Allen, editors. Optimizing \ncompilers for modren architectures:a dependence-based approach. 2001. [21] M. Kulkarni, M. Burtscher, \nK. Pingali, and C. Cascaval. Lon\u00adestar: A suite of parallel irregular programs. In 2009 IEEE International \nSymposium on Performance Analysis of Systems and Software (ISPASS), pages 65 76, April 2009. [22] C. \nLattner and V. Adve. Automatic pool allocation: improv\u00ading performance by controlling data structure \nlayout in the heap. In Proceedings of the 2005 ACM SIGPLAN conference on Programming language design \nand implementation, pages 129 142, 2005. [23] G. Loosli, S. Canu, and L. Bottou. Training invariant support \nvector machines using selective sampling, 2005. [24] E. Mansson, J. Munkberg, and T. Akenine-Moller. \nDeep co\u00adherent ray tracing. In Proceedings of the 2007 IEEE Sympo\u00adsium on Interactive Ray Tracing, pages \n79 85, 2007. [25] R. L. Mattson, J. Gecsei, D. R. Slutz, and I. L. Traiger. Evalu\u00adation Techniques for \nStorage Hierarchies. IBM Systems Jour\u00adnal, 9(2):78 117, 1970. [26] L. A. Meyerovich, T. Mytkowicz, and \nW. Schulte. Data parallel programming for irregular tree computations. In 3rd USENIX workshop on hot \ntopics in parallelism, 2011. [27] N. Mitchell, L. Carter, and J. Ferrante. Localizing non-af.ne array \nreferences. In Proceedings of the 1999 International Conference on Parallel Architectures and Compilation \nTech\u00adniques, pages 192 , 1999. [28] B. Moon, Y. Byun, T.-J. Kim, P. Claudio, H.-S. Kim, Y.- J. Ban, S. \nW. Nam, and S.-E. Yoon. Cache-oblivious ray reordering. ACM Trans. Graph., 29(3):28:1 28:10, July 2010. \n[29] P. A. Navratil. Memory-ef.cient, scalable ray tracing. PhD thesis, 2010. [30] P. A. Navratil, D. \nS. Fussell, C. Lin, and W. R. Mark. Dynamic ray scheduling to improve ray coherence and bandwidth uti\u00adlization. \nIn Proceedings of the 2007 IEEE Symposium on Interactive Ray Tracing, RT 07, pages 95 104, Washington, \nDC, USA, 2007. IEEE Computer Society. [31] S. M. Omohundro. Five balltree construction algorithms. Technical \nreport, 1989. [32] M. Pharr, C. Kolb, R. Gershbein, and P. Hanrahan. Rendering complex scenes with memory-coherent \nray tracing. In Pro\u00adceedings of the 24th annual conference on Computer graphics and interactive techniques, \npages 101 108, 1997. [33] V. K. Pingali, S. A. McKee, W. C. Hseih, and J. B. Carter. Computation regrouping: \nrestructuring programs for temporal data cache locality. In Proceedings of the 16th international conference \non Supercomputing, pages 252 261, 2002. [34] M. Rinard and P. C. Diniz. Commutativity analysis: a new \nanalysis technique for parallelizing compilers. ACM Trans. Program. Lang. Syst., 19(6):942 991, 1997. \n[35] M. Sagiv, T. Reps, and R. Wilhelm. Parametric shape anal\u00adysis via 3-valued logic. ACM Transactions \non Programming Languages and Systems, 24(3), May 2002. [36] J. P. Singh, C. Holt, T. Totsuka, A. Gupta, \nand J. Hennessy. Load balancing and data locality in adaptive hierarchical n\u00adbody methods: Barnes-hut, \nfast multipole, and radiosity. J. Parallel Distrib. Comput., 27(2):118 141, 1995. [37] M. M. Strout, \nL. Carter, and J. Ferrante. Rescheduling for locality in sparse matrix computations. In Proceedings of \nthe International Conference on Computational Sciences-Part I, pages 137 148, 2001. [38] D. N. Truong, \nF. Bodin, and A. Seznec. Improving cache be\u00adhavior of dynamically allocated data structures. In Proceed\u00adings \nof the 1998 International Conference on Parallel Archi\u00adtectures and Compilation Techniques, pages 322 \n, 1998. [39] B. Walter, K. Bala, M. Kulkarni, and K. Pingali. Fast ag\u00adglomerative clustering for rendering. \nIn IEEE Symposium on Interactive Ray Tracing (RT), pages 81 86, August 2008. [40] Z. Wang, C. Wu, and \nP.-C. Yew. On improving heap memory layout by dynamic pool allocation. In Proceedings of the 8th annual \nIEEE/ACM international symposium on Code gener\u00adation and optimization, CGO 10, pages 92 100, New York, \nNY, USA, 2010. ACM. A. Detailed splicing implementation This section presents a more in-depth description \nof the implementation of traversal splicing. We use the nearest neighbor (NN) algorithm of Figure 5 as \na running example, and the combinations of Figures 17, 18 and the recurse method of Figure 5 comprises \nthe full spliced implementa\u00adtion.    A.1 Setup and recursion unrolling In the actual implementation, \nthe high level view of Figure 7 is moved to lines 1-8 of Figure 17. We map the top of the tree and save \nthe nodes into an array based on a heap ordering of the tree as in Figure 2(a). Non-existent nodes are \nmarked null in the array. Point buffers into which points can be saved are allocated for all non-null \nnodes. Both the tree map and point buffers are indexed from 1 (the root) to CD+2 - 1 (the rightmost leaf), \nwhere C is maxChildren, and D is the splice depth. Point buffers need to accommodate an arbitrary number \nof points and are implemented as ArrayLists. The call set id buffer is a two dimensional array of size \nP \u00d7 D (P is the number of points). The partial traversals of all points are executed by pairs of top \nand bottom phases. While bottom phases are equiva\u00adlent to normal recursive traversals, top phases execute \na path through the tree down to the splice depth, and needs to save additional state so that traversals \ncan be resumed. Figure 18 shows a transformed recursive method, recurseSplice, to realize top phases \nfor the pseudocode for the nearest neighbor code of Figure 5. recurseSplice performs  (a) Opteron \n(b) Opteron (c) Opteron II  only the .rst traversal of each call set, later traversals in the call \nsets will be directly called (on the appropriate child) in subsequent phases. Points that reach the splice \ndepth are saved into the node at which they should be resumed (not the node at which the point is paused) \n(lines 14 and 21), and points that are truncated beforehand are saved into the node at which they are \ntruncated (lines 3 and 7). The call set id is saved per point per depth (lines 16 and 23). Because the \ntransformed code performs only the .rst traversal of a call set, subsequent partial traversals of the \ntree above the splice nodes are performed by later top phases. Top phase traversals are paused at explicit \nsplice nodes, and the traversal is resumed by bottom phases that continue the traversal to the implicit \nsplice node. The order of phases can be determined by partially unrolling the recursive traversal (unrollRecursion \nin lines 19-29 in Figure 17), which basically expands the remaining top phases and appends a bottom phase \nafter each top phase. The .rst top phase is started by calling recurseSplice for all points (line 6 of \nFigure 17. The depth of the nodes at which to resume and the phase number are passed as arguments to \nthe top phase. For a bottom phase, the depth argument will always be D +1, and all phase numbers are \niterated upon. For our running example (Figures 4 and 6(a), D =2, root is depth 0), the order of phases \nis T0-0 (top phase at depth 0, traversal phase 0), B (bottom phase), T2-1, B, T1-1, B, T2-1, B (as in \nFigure 6(a)). The .rst top phase always performs traversal phase 0 (the .rst traversal), and because \nour example has two phases per call set, all other top phases start with the second traversal in the \nset.  A.2 Dynamic sorting In our implementation the dynamic sorting comes naturally, as points are reordered \nduring top phases based on the node they were saved at. At each top phase, we gather all points which \nshould execute in this phase (i.e., all points that have not been truncated above the level of the top \nphase), and resume them at the appropriate node based on the point s call set id (lines 37-50 of Figure \n17). For example at T2-1, we will gather points that have been paused or truncated at nodes 4@, and resume \nat nodes @ 7 @ 154@. At T1-1 we will gather points at nodes 2@, and resume at nodes 2 @ 15@ and 3 @. \nMore speci.cally for our particular example of NN, at T2-1, we will gather points at nodes 489 @, @, \n@ and resume at node 5 @, because these points have visited {leftChild}  1 Set<Point> points = / * points \n*/ 2 Node root = buildTree(points); 3 mapTree(root); 4 allocBuffers (); 5 foreach ( Point p : points \n) { 6 recurseSplice(p, root , 0); 7 } 8 unrollRecursion(0, 0, 0, 0); 9  10 void allocBuffers () { 11 \nnumLeafNodes = power(maxChildren , D + 1); 12 numNodes = numLeafNodes * maxChildren ; 13 for ( int i=1; \ni < numNodes ; i ++) { 14 if ( treeMap [ i ] != null ) allocPointBuffers ( i ); 15 } 16 allocCallSetIdBuffer \n(); 17 } 18 19 void unrollRecursion ( int d1 , int p1 , int d2 , int p2 ) { 20 if ( d2 < D) { 21 for \n( int i=0; i < maxPhases ; i ++) { 22 if (i == 0) unrollRecursion(d1, p1, d2 + 1, i); 23 else unrollRecursion(d2 \n+ 1, i, d2 + 1, i); 24 } 25 } else { 26 if (d1 != 0) topPhase (d1 , p1 ); 27 bottomPhase ( ) ; 28 } 29 \n} 30 31 void topPhase ( int depth , int phase ) { 32 int start = power(maxChildren , depth ); 33 int \nend = start * maxChildren ; 34 for ( int i = start; i < numNodes ; i ++) { 35 swapPointBuffersAndClearDest(i); \n36 } 37 for ( int i = start; i < end; i++) { 38 int substart = i ; 39 int subend = i +1; 40 while ( substart \n< numNodes ) { 41 for ( int j = substart ; j < subend ; j ++) { 42 foreach ( Point p : getPointsAtNodeIndex \n( j )) { 43 Node n = nextNode(p.getCallSet(depth), phase); 44 recurseSplice(p, n, depth); 45 } 46 } 47 \nsubstart *= maxChildren ; 48 subend *= maxChildren ; 49 } 50 } 51 } 52 53 void bottomPhase () { 54 for \n( int i = numLeafNodes ; i < numNodes ; i ++) { 55 foreach ( Point p : getPointsAtNodeIndex ( i )) { \n56 for ( int j=0; j < maxPhases ; j ++) { 57 Node n = nextNode(p.getCallSet(D), j); 58 recurse(p, n); \n59 } 60 } 61 } 62 } Figure 17. Pseudocode of recursion unrolling and should now visit {rightChild}. The \npoints which resume at 5 @ are better sorted now because they are in order of truncation at 489 @, reach \n@ .rst, and reach @ .rst. Similarly, points at nodes @ resume at node @, points at nodes 5@, 11 @, 104@6, \n@12, 137 @ resume at node @, and so on. To separate the points saved in the previous phase which need \nto be processed, and the points being saved in this phase, we use a toggle mechanism with two point buffers \nper node. At the start of a top phase, all point buffers below the depth argument are swapped, so the \nprevious destination 1 void recurseSplice(Point p, Node n, int depth ) { 2 if (!canBeCloser(p, n.boundingBox)) \n{ 3 n.savePoint(p); 4 return ; 5 } else if (n. isLeaf ()) { 6 p.updateClosest(n.getPoint()); 7 n.savePoint(p); \n8 } else { 9 double split = p.value(n.splitType); 10 if ( split <= n.splitValue) { 11 if ( depth < \nD) { 12 recurseSplice(p, n.leftChild , depth + 1); 13 } else { 14 n.leftChild.savePoint(p); 15 } 16 \np.saveCallSet(depth, 0); // call set 0 17 } else { 18 if ( depth < D) { 19 recurseSplice(p, n.rightChild \n, depth + 1); 20 } else { 21 n.rightChild.savePoint(p); 22 } 23 p.saveCallSet(depth, 1); // call set \n1 24 } 25 } 26 } Figure 18. Transformed recursive method for top phases buffer is the source buffer and \nvice versa (lines 34-36). Then the new destination buffer is cleared, and points will be saved to it \nduring this phase. This is only done for top phases, as bottom phases do not save state. For each bottom \nphase, all points that were paused at the explicit splice nodes (and saved into the children of explicit \nsplice nodes, nodes 815 @ @) will traverse the appropriate sub\u00adtrees below the splice nodes according \nto their call set id (lines 54-61). There is no sorting at bottom phases as points are gathered from \na single node.  A.3 Optimizations For inferred order algorithms, as in nearest neighbor, the call set \nid is unnecessary, and the next node can be determined completely by the previous node the point was \nat, and the phase number. Hence i can replace p.getCallSet() (lines 43 and 57 in Figure 17), and loop \ninvariant statements can be hoisted out of the loop. For single call set algorithms, recursion unrolling \ncan be simpli.ed even further. We need not scan through multiple subtrees as only a single path from \nthe root will have saved points at a time. Hence unrollRecursion can pass a nodeIndex to both the top \nand bottom phases to indicate where to look for points. Furthermore, we need only one point buffer per \nlevel, instead of one point buffer per node. Splice node elision can be realized by executing a merged \nbottom phase at line 23 of Figure 17, if the depth is a .xed distance from D. The merged bottom phase \nuses the code for topPhase but doesn t swap point buffers and calls recurse instead of recurseSplice. \nB. Automatic transformation Automatically transforming traversal codes to apply splic\u00ading .rst requires \nrecognizing the traversal structure. While  Benchmark Input Block Sizes Splice Depths Opteron Niagara \nOpteron II Opteron Niagara Opteron II Block Block +Splice Block Block +Splice Block Block +Splice Splice \nBlock +Splice Splice Block +Splice Splice Block +Splice BH Random 512 512 512 512 512 512 2 2 2 2 2 2 \nPlummer 512 512 512 512 512 512 3 3 3 3 3 3 PC Random 512 512 512 512 512 512 8 8 8 8 8 8.6 Covtype 128 \n128 128 128 128 128 7 7 7 7 7 7 Mnist 128 128 128 128 128 128 7 7 7 7 7 7 NN Random 512 512 512 512 512 \n512 7 7 7 7 7 7 Covtype 128 128 128 128 128 128 7 7 7 7 7 7 Mnist 128 128 128 128 128 128 7 7 7 7 7 7 \nkNN Random 512 512 170.67 512 512 512 8 8 8 8 8 8 Covtype 89.6 128 0 128 51.2 128 7 7 7 7 7 7 Mnist 128 \n128 42.67 128 128 128 7 7 7 7 7 7 BT Random 512 512 512 512 512 512 6 6 6 6 6 6 Covtype 128 128 128 128 \n128 128 6 6 6 6 6 6 Mnist 128 128 128 128 128 128 6 6 6 6 6 6 RT Random 1024 1024 1024 1024 1024 1024 \n8 8 8 8 8 8 Table 3. Autotuned parameters recognizing a traversal code structure can be expedited with \nprogrammer annotations, many traversal codes have a com\u00admon algorithmic structure that does not require \nannotations to recognize. In particular, many traversal codes are writ\u00adten by recursive function calls \non recursive data structures. TreeSplicer identi.es code which performs repeated recur\u00adsive traversals \nof a recursive structure as a candidate for traversal splicing. A recursive method, m, can be recognized \nby .nding a call to itself within a method s body10. A recur\u00adsive structure can be recognized by .nding \na class, c, with at least one .eld f of the same class (or superclass). TreeSplicer then determines whether \nthe recursive method performs a recursive traversal of any identi.ed recursive structures. This might \nhappen in one of two ways: (i) if m takes an object o of class c as an argument, and passes o.f as an \nargument to the recursive call; or (ii) if m is a member method of c and it performs the recursive call \nby invoking f.m() (in other words, the data structure node is the implicit this argument). TreeSplicer \nuses a call graph analysis to determine that the recursive method is called (either directly, or through \na chain of calls) from a loop in the application. If the loop is annotated as parallel and the recursive \nstructure is annotated as a tree, TreeSplicer transforms the code as described in Sections 5.3 and A. \nFigure 17 is a template customized based on the optimiza\u00adtion levels which is plugged in to the transformed \ncode. Fig\u00adure 18 is constructed by making a duplicate of the original recursive method, stripping all \ncalls other than the .rst call of each call set, and adding code to save points and call set ids. Intermediary \nmethods are split around the call path to the recursive method, into prologues and epilogues. Code is \nadded to save any local state which persists from prologue to epilogue, and local variables within the \nrecursive method which is passed as an argument to recursive calls. Autotun\u00ading code is added before \nany splicing setup is done (before line 3 of Figure 17), as the setup requires the splice depth, 10 A \nmore sophisticated approach is to look for cycles in a call graph; the simple approach here suf.ces for \nour benchmarks. which is determined by the autotuner. Points consumed for autotuning are skipped in the \npoint loop (lines 5-7). We have made the source code of TreeSplicer public at https://sites.google.com/site/treesplicer. \nC. Autotuned parameters Table 3 shows the autotuned parameters averaged across the recorded serial runs \non the three systems. The maximum block size is limited to 0.1% of the total points, so that each block \nsize can be tested 5 times, and the autotuning over\u00adhead limited to 1% of the points. The selected block \nsizes are often capped at the maximum (512 for random inputs with one million points, and 128 for real \ninputs with 200, 000 points, 1024 for RT with 223 points), because the points are unsorted, and the blocks \nbecome sparse quickly.For kNN the divergence is worse enough that sometimes the autotuner de\u00adcides not \nto do point blocking. The splice depths are deter\u00admined by the average reach of points, and is machine \ninde\u00adpendent. Slight differences are due to the random sampling of points for autotuning. BH has a smaller \ndepth because it is an octtree. For other benchmarks, random inputs have larger depths than real inputs, \nbecause there are more points, and the tree is larger.  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Generally applicable techniques for improving temporal locality in irregular programs, which operate over pointer-based data structures such as trees and graphs, are scarce. Focusing on a subset of irregular programs, namely, tree traversal algorithms like Barnes-Hut and nearest neighbor, previous work has proposed point blocking, a technique analogous to loop tiling in regular programs, to improve locality. However point blocking is highly dependent on point sorting, a technique to reorder points so that consecutive points will have similar traversals. Performing this a priori sort requires an understanding of the semantics of the algorithm and hence highly application specific techniques. In this work, we propose traversal splicing, a new, general, automatic locality optimization for irregular tree traversal codes, that is less sensitive to point order, and hence can deliver substantially better performance, even in the absence of semantic information. For six benchmark algorithms, we show that traversal splicing can deliver single-thread speedups of up to 9.147 (geometric mean: 3.095) over baseline implementations, and up to 4.752 (geometric mean: 2.079) over point-blocked implementations. Further, we show that in many cases, automatically applying traversal splicing to a baseline implementation yields performance that is better than carefully hand-optimized implementations.</p>", "authors": [{"name": "Youngjoon Jo", "author_profile_id": "81464662508", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856097", "email_address": "yjo@purdue.edu", "orcid_id": ""}, {"name": "Milind Kulkarni", "author_profile_id": "81331496893", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856098", "email_address": "milind@purdue.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384643", "year": "2012", "article_id": "2384643", "conference": "OOPSLA", "title": "Automatically enhancing locality for tree traversals with traversal splicing", "url": "http://dl.acm.org/citation.cfm?id=2384643"}