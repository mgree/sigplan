{"article_publication_date": "10-19-2012", "fulltext": "\n Safe Compiler-driven Transaction Checkpointing and Recovery Jaswanth Sreeram Santosh Pande Intel Labs \nGeorgia Institute of Technology jaswanth.sreeram@intel.com santosh@cc.gatech.edu Abstract Several studies \nhave shown that a large fraction of the work performed inside memory transactions in representative pro\u00adgrams \nis wasted due to the transaction experiencing a con\u00ad.ict and aborting. Aborts inside long running transactions \nare especially in.uential to performance and the simplicity of the TM programming model (relative to \nusing .negrained locking) in synchronizing large critical sections means that large transactions are \ncommon and this exacerbates the prob\u00adlem of wasted work. In this paper we present a practical transaction \ncheckpoint and recovery scheme in which trans\u00adactions that experience a con.ict can restore their state \n(in\u00adcluding the local context in which they were executing) to some dynamic program point before this \naccess and begin execution from that point. This state saving and restoration is implemented by checkpoint \noperations that are generated by a compiler into the transactions body and are also optimized to reduce \nthe amount of state that is saved and restored. We also describe a runtime system that manages these \ncheck\u00adpointed states and orchestrates the restoration of the right checkpointed state for a con.ict on \na particular transactional access. Moreover the synthesis of these save &#38; restore op\u00aderations, their \noptimization and invocation at runtime are completely transparent to the programmer. We have imple\u00admented \nthe checkpoint generation and optimization scheme in the LLVM compiler and runtime support for the TL2 \nSTM system. Our experiments indicate that for many parallel pro\u00adgrams using such checkpoint recovery \nschemes can result in upto several orders of magnitude reduction in number of aborts and signi.cant execution \ntime speedups relative to plain transactional programs for the same number of threads. Categories and \nSubject Descriptors D.1.3 [Programming Techniques]: Concurrent Programming, Parallel programming\u00adsoftware \ntransactional memory systems Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 Keywords software transactional memory, checkpointing, continuations \n1. Introduction A critical section in a generic Software Transactional Mem\u00adory (STM) system which could \npotentially access the same shared data as other concurrent threads continues to execute until it detects \nreal con.icts (either at the time of an ac\u00adcess or later). A con.ict occurs for example when a concur\u00adrent \nthread has written to a variable that the critical section read. When this occurs the results and intermediate \nvalues computed so far in the critical section are rendered invalid and are therefore discarded. In other \nwords when some (ab\u00adstract) inputs to the critical section are perturbed it aborts the current computation, \ndiscards the outputs and restarts the computation. The amount of work wasted by aborted trans\u00adactions \nis substantial -studies have reported that between 25-95% of work performed in transactions in representative \nprograms is wasted due to aborts [20]. In eager validation transaction systems con.icts are checked for \nevery access inside the transaction. While this means large overheads are incurred, doomed transactions \ncan be detected early and less work is wasted. In a lazy system con.icts are checked at commit time and \nsome TM systems use a hybrid approach for different types of con.icts. Regardless of the validation scheme, \ncon.icts discovered in most optimistically concur\u00adrent critical sections cannot be resolved without at \nleast one abort. For large long running critical sections or for those which have high levels of contention \nfor shared data, this fact means that a large amount of work executed specula\u00adtively is unavoidably wasted \nwhen the critical section tries to commit. Techniques such as transaction check-pointing [11] , open \nand closed nesting and abstract nested trans\u00adactions [9] have been studied which propose to lower the \noverhead of aborts by only partially undoing the effects of a transaction in the case of an con.ict. \nOther systems such as DASTM [10] automatically forward values between a pair of con.icting transactions \nso that both may commit. Several proposals have introduced methods for multiversion recon\u00adciliation in \nmobile databases to reintegrate (often con.ict\u00ading) updates to global data from multiple clients while \npre\u00adserving serializability[6]. In this paper we propose a prac\u00adtical mechanism for solving con.icts \nin which a transaction which experiences a con.ict on an access attempts to recover from the con.ict \nby correcting its state including its read\u00ad/write sets on-the-.y. This recovery is achieved by essen\u00adtially \nrestoring the transactions state to a dynamic program point before this access and beginning the transactions \nexe\u00adcution from that point. To do this the transaction must have encountered that program point during \nits execution so far and its state must have been saved at that point. Through a series of such checkpoint \nsaves and restores, the transaction can roll forward through con.icts and ultimately commit successfully. \nThe transactions state may be saved at several program points and is maintained as a totally ordered \nchain. When addressing a con.ict for a particular access the TM runtime chooses a particular checkpointed \nstate and restores the transactions state to it. This state saving and restoration is performed by checkpoint \noperations that are automatically generated into a transactions body by a compiler pass which also optimizes \nthe amount of state that is saved to minimize the runtime overheads performing this save and restore. \nSyn\u00adthesizing these checkpoints does not require reasoning about properties such as commutativity or \nabstract inverses and does not fundamentally change transactional semantics and properties such as opacity \nand isolation are preserved. More\u00adover the synthesis, optimization and runtime invocation of checkpoints \nare completely transparent to the programmer.  Our experiments indicate that for several parallel trans\u00adactional \nprograms using such checkpoint operations can re\u00adsult in several orders of magnitude reduction in number \nof aborts and execution time speedups of up to 4X relative to their plain transactional counterparts \nwith the same number of threads. The major contributions of our work and the organization of the rest \nof this paper are as follows: We describe the semantics and execution model of nested handlers for transactions \nin a generic TM system (Section 2)  We present a compiler pass for generating checkpoints at compile \ntime and discuss several optimizations to reduce the overhead of state-saving (Section 3).  We describe \nthe runtime support for maintaining check\u00adpoints and for orchestrating their restore (Section 4)  We \ndiscuss the safety and correctness of using checkpoint operations to recover from con.icts (Section 5) \n We present experimental evidence that shows that by automatically enabling transactions to roll-forward \nin this manner, many access-time and commit-time con\u00ad.icts can be solved without aborting and consequently \nperformance can be improved for a variety of workloads (Section 6)  2. Checkpointing and Recovery When \na long-running transaction experiences a con.ict it is forced to abort thereby discarding all the work \nit has done so far and restart. Previous studies [2, 21] have observed that for many representative programs, \nbetween 25-95% of the work done by transactions is wasted due to aborts. One way to re\u00adduce this wasted \nwork is to enable a con.icting transaction to take a recovery action that enables the transaction to \nmake forward progress. This recovery action could for example correct the read, write, read-and-write \nsets of the transac\u00adtion or add/remove elements from them and ultimately help the transaction roll-forward \nand commit. Requiring the pro\u00adgrammer to specify such a recovery action is impractical as it would defeat \nthe programmability advantages that memory transactions provide. For long transactions containing deep \ncall chains, describing these recovery actions would be cum\u00adbersome and require deep familiarity with \nthe program. On the other hand, automatically synthesizing a recovery action is also challenging for \nseveral reasons. In order to repair the transactions state, this synthesized recovery action would at \na high-level, have to be aware of what portion of the transac\u00adtions state needs to be repaired and the \nspeci.c values with which to repair the transactions read/write sets. This is dif\u00ad.cult as it requires \nnot only the compiler to infer complex program-level semantics but also requires maintaining a dy\u00adnamic \nprogram dependence graph (PDG) at run-time to de\u00adcide which portion of the transactions state needs to \nbe aug\u00admented and/or modi.ed to recover from the con.ict (see [1] and references therein). Additionally \nthe speci.c recovery action needed may be different depending on whether there was an execution time \ncon-.ict (during transaction execu\u00adtion) or because of a con.ict at validation time (during an commit \nattempt by the transaction). Our approach to this problem is rooted in the observa\u00adtion that the transaction \nitself is a recovery action for every con.ict that can occur in it. Speci.cally, for a con.ict on any \naccess in a dynamic instance of a transaction, if the transactions state can be restored to a valid state \nat some dynamic program point just before the access, then the portion of the transaction after this \npoint is a valid re\u00adcovery action for that con.ict. Indeed an abort can simply be thought of as a checkpoint \nin which the program point at which the state is saved is at the very beginning of the trans\u00adaction. \nOur solution to this problem consists of a compiler pass that analyzes a transaction, generates checkpointing \nop\u00aderations at the appropriate points and applies optimizations that reduce the overheads of maintaining \nand invoking these checkpoints and a runtime system that orchestrates the sav\u00ading and restoration of \nall the checkpoints saved by a trans\u00adaction. A generic transaction checkpoint that saves the state of \na transaction after it has executed some set of statements (S1) and before it has executed another set \nof statements (S2) is as shown in Figure 1.  atomic { <...txn stmts (S1)...> CheckpointSave(); <...txn \nstmts (S2)...> } Figure 1: A transaction checkpoint 2.1 Execution Model An informal model of the execution \nof a checkpoint opera\u00adtion (such as the one in Figure 1 is as follows (implementa\u00adtion level details \nare discussed in later sections): 1. Checkpoint Save: When a transaction encounters a checkpoint save \noperation during its execution it saves its state and adds it to the transactions totally ordered set \nof saved checkpoints. The precise de.nition of the state of the transaction is explained in more detail \nlater. 2. Checkpoint Restore: If a con.ict is detected for an ac\u00adcess to memory address Addr, the transaction \nrestores the state of the transaction to some checkpoint that was saved before this access to Addr and \nif no such check\u00adpoint exists the transaction simply aborts. After a suc\u00adcessful checkpoint restore the \ntransaction is in a consis\u00adtent and valid state. That is: (a) It has not observed any uncommitted state \nfrom other transactions and (b) Its read-set RTp , write-set WTp and read-and-write set   RWTp are \nvalid and coherent After a checkpoint has been restored, the transaction begins to execute from the instruction \nfollowing the Checkpoint Save above and with the same state that was captured then. 3. Accesses: After \na checkpoint has been restored, the trans\u00adaction continues to execute from the instruction follow\u00ading \nthe checkpoint save step above. The control-.ow paths and the set of transactional and non-transactional \naccesses that occur from that point on may be different from the previous execution -the transaction \ncan access memory locations it has already accessed before or it can access new memory locations. These \naccesses are vali\u00addated (during the access itself, at commit-time or both depending on the TM model) \njust like the other accesses made in the transaction. 4. Opacity: When invoked from inside transactions \nthat satisfy the Opacity property [13], checkpoint handlers also satisfy this property. 5. Isolation: \nAfter a checkpoint restore, the transaction only observes consistent state, i.e., it is guaranteed to \nnot see any updates that have not been committed by a live con\u00adcurrent transaction. The transaction opacity, \nisolation and  coherence properties are discussed in more detail in Sec\u00adtion 5. 6. Completion: When \nthe transaction s body completes its execution after possibly several checkpoint saves and re\u00adstores, \nit attempts to commit as normal and its entire read\u00ad/write sets are validated. If this validation is \nsuccessful (and in lock-based TMs, if the transaction is also able to acquire locks on all memory locations \nin its read-and\u00adwrite and write-only sets), the transaction can commit. Over the course of its execution \na transaction may save multiple checkpoints. The set of checkpoints saved by a transaction have a strict \ntotal ordering -namely the order in which they were saved. This ordering is used on a con\u00ad.ict to decide \nwhich checkpoint to restore to, as restoring to checkpoint that was saved after this con.icting access \noc\u00adcurred would not eliminate the con.ict. As discussed later, the checkpoint restoration mechanism attempts \nto restore the latest checkpoint that occurred before the con.icting access. 3. Generating Checkpoint \nOperations In order to generate the checkpoint save and restore opera\u00adtions at compile-time and to invoke \nthem at execution time, the principle questions that we need to answer are: (a) where should the compiler \ninsert checkpoints for a given transac\u00adtion (b) how can a runtime capture and restore the complete state \nof a transaction ef.ciently (c) how often should check\u00adpoints be captured (d) how should the various \ncheckpoints for a single instance of a transaction be validated and man\u00adaged. First we consider the problem \nof saving a transaction s state. The set of memory locations read, written and read\u00adand-written by a \ntransaction T just before a dynamic pro\u00adgram point p are denoted by RTp , WTp and RWTp respec\u00adtively \n(we refer to these sets together as read/write sets). The state of the local variables (both transactional \nand non\u00adtransactional), heap, program stack and registers are denoted by LTp , HTp , STKTp and REGTp \nrespectively. We refer to the tuple STp : <RTp , WTp , RWTp , LTp , HTp ,STKTp , REGTp > as the state \nor execution context of a live transaction T just before program point p (the subscripts Tp or p may \nbe dropped when not necessary). When a checkpoint is restored due to a con.ict, it begins execution in \nexactly the same context in which it was saved. Such a scheme requires saving a transaction s state at \nsome arbitrary point in its execution and restoring it at some other instant during its lifetime. Implementation \nof such save/re\u00adstores is easy to achieve in languages with support for .rst class continuations but \nis challenging for languages that do not have such a support. Here we present a form of contin\u00aduations \n(for the C/C++ languages) that transactions use to save and restore state during a checkpoint operation. \n  Figure 2: Saving and restoring the state of the stack on a con.ict 3.1 Persistent First-Class Continuations \nFor a dynamic program point p in a transaction T, we de.ne a persistent continuation that encapsulates \na transaction s complete state STp as de.ned above, immediately before p. By persistent we mean that \nthe continuation continues to exist after p and also after the program stack frame at p ceases to be \nlive (for example, if the function containing p returns to its caller). Each of RTp , WTp and RWTp can \nbe saved into this continuation -if we assume that each of them are maintained as ordered lists, then \ntheir states can be captured simply as the position of the last inserted element in each of them. In \naddition, this continuation also captures the transaction heap memory allocations and deallocations and \nlike the read/write sets above, these are restored when the continuation is activated on a con.ict. In \naddition to the read/write sets, normal transactions also maintain a write\u00adset for local variables since \nthey have to be restored when a transaction aborts and restarts. This local variable write\u00adset is also \ncaptured in the continuation. This continuation is also used to record the program stack starting at \nthe frame containing the start of T to the frame at the top of the stack at p. Thus the states of the \nlocal variables LTp -the state of local variables in the current stack frame at p and in all other live \nstack frames underneath, are also recorded. A checkpoint H is said to be registered with its transaction \nT when execution of T encounters the CheckpointSave() call. Then a continuation is created on a transaction-private \nregion of the heap for the checkpoint H that encapsulates STp . Figure 2 shows a transaction saving the \nstate of the stack as part of a continuation while in function f4(). Later, while executing f7(), the \ntransaction accesses a transactional variable that at commit time is found to have a con.ict. At this \npoint the continuation saved in f4() is restored and execution is resumed from that point. The compiler \npass for inserting checkpoints into a trans\u00adaction s body is shown in Figure 3(a). This .gure also shows \nthe IR output in Figure 3(c) for the list search function shown in Figure 3(b). The compiler pass processes \ncallers before callees and the call graph is processed in depth-.rst order. The pass starts with the \nfunction body containing the trans\u00adaction s boundary (the start and end instructions). The pass begins \nby inserting a special marker instruction at the begin\u00adning of the transaction. This marker instruction \nessentially stack allocates (alloca) a transaction-local marker vari\u00adable seen in lines 5-6 in Figure \n3(c). When a continuation is saved, the state of the program stack STKTp at point p in that transaction \nis saved relative to the state of the program stack at this marker variable. At runtime, when a checkpoint \nis saved at dynamic program point p when the stack pointer register contains esp, the stack is saved \nby copying the por\u00adtion of the stack between esp and the marker variable into the continuation. So all \nthe stack frames that are live at p are recorded. The pass then inserts a call to the checkpoint save \noperation CheckpointSave() before each transac\u00adtional load operation it encounters (line 7 in Figure \n3(a) and line 28 in Figure 3(c)). In typical transactions, transactional loads to shared values are the \nmost frequently occurring transactional operations and are also the transactional oper\u00adations which have \nthe highest likelihood of experiencing a con.ict. So it makes sense to insert checkpoint operations before \ntransactional loads. Other systems such as the one in [16] instead insert checkpoints before speci.c \nstore opera\u00adtions. This makes .ne-grained control of checkpointing less feasible (since stores are relatively \nless frequent than loads) and also means that these system will not directly help the performance of \ntransactions that are either read-only or are read-intensive. On the other hand, while associating check\u00adpoints \nat speci.c chosen loads gives us better coverage of the transaction, saving a checkpoint at every dynamic \ntransac\u00adtional load is obviously prohibitively expensive in practice. In our system, these checkpoint \noperations inserted before every load are treated only as potential program points to save a checkpoint. \nAt runtime a transaction decides whether a checkpoint is actually saved by evaluating a few simple heuristics. \nThis and other techniques to reduce state saving overheads are described below.  3.2 Reducing State \nSaving Overheads Saving the local and shared read/write sets, heap alloc/deal\u00adlocs and registers at a \npoint in a transaction takes a constant amount of space and time and as a result is relatively inex\u00adpensive. \nSaving a potentially unbounded program stack how\u00adever, is not and the amount of state that is to be saved \non a checkpoint save operation can be signi.cant especially if this save is deep in a call chain (as \nin the case of the checkpoint save operation in function f7() in Figure 2). Moreover trans\u00adactional loads \nare quite frequent and since we augment every load with a potential checkpoint save operation, reducing \nthe amount of state saved on each checkpoint and reducing the frequency of checkpointing itself are critical \nto the perfor\u00admance. Our implementation of the compiler pass outlined in  1 CheckpointTxnRegion(Function \n*F, Inst *start, Inst *end, Inst *marker) { 3 if(marker == NULL) { Inst * marker = InsertMarkerAt(start); \n5 txnStackDepth = start->stackdepth(); } 7 foreach Transactional Load Inst i2(start, end) 9 state_opts \n= i->stackdepth() -txnStackDepth; InsertCheckpointBefore(i,marker, state_opts); 11 foreach Transactional \nCall Site c2(start, end) 13 { callee = c->targetFunction; 15 if (!ProcessedCallTargets.add(callee)) { \nCheckpointTxnRegion(callee, c->start, c->end, 17 marker); } 19 } -----------------------(a) -----------------------\u00ad \n21 atomic { int big_array[100]; 23 list_find(key, list); } 25 // list->head is read-only 27 node_t *list_find \n(int key, node_t node_t *x = list->head; 29 for(;x;) { if(x->key == key) 31 break; x = tm_read(&#38;(x->next)); \n33 } return x; 35 } *list) {  -----------------------(b) -----------------------\u00ad 1 { call @tm_start(..) \n3 ; marker could have been alloced here %big_array = alloca i64*100 5 %m = alloca i64 store %m, %txn->marker, \n7 call list_find(%txn, %key, %head) call @tm_end(..) 9 } define node_t* @list_find(Thread_* %txn, 11 \ni64 %key, node_t* %head) { 13 entry: %x = alloca node_t*; 15 %1 = load %head store %1, %x; 17 br label \n%bb2 bb2: 19 %12 = load %x; %13 = icmp ne %12,null 21 br %13, %bb, %bb3 bb: 23 %4 = load %x %5 = load \n%x->key 25 %6 = icmp eq %7, %8 br %6, %bb3, %bb1 27 bb1: %9 = call @CheckpointSave(%txn) 29 %7 = call \n@tm_read(%txn, x->next) store %7, %x 31 br %bb2 bb3: 33 %8 = load %x ret %8 35 } ----------------------\u00ad(c) \n------------------------  Figure 3: (a) Overview of compiler pass to checkpoint transactional regions \n(b) routines for atomic list search (c) simpli.ed IR generated by the compiler pass in (a) for the code \nin (b) Figure 3(a) performs a few state-saving optimizations that are not illustrated in this .gure but \nwhich merit discussion. The stack allocation of the marker variable is typically done just before the \ntransaction s start (Figure 3(a) line 4). That is, during a checkpoint save, everything on the pro\u00adgram \nstack from the current stack register to the last allo\u00adcated stack variable is saved by the checkpoint. \nIn the .rst optimization the compiler attempts to eliminate saving the regions of the stack that are \nnot written to in the transaction. For example the stack allocation of the array big array in the Figure \n3(c) is not written to in the transaction but may be referenced later in that function. If the marker \nvariable were allocated normally just after the transaction s start, ev\u00adery checkpoint save operation \nwould also save the state of this big array. Instead, the pass attempts to lower the po\u00adsition of marker \non the stack such that it is allocated after this array -in line 5 instead of line 3 in Figure 3(c). \nBefore the pass inserts a checkpoint in line 7 in Figure 3(a) it checks if that particular access occurs \nin the same stack frame as the transaction s start and end. If so then the portion of the stack frame \nthat is to be saved and restored is signi.cantly reduced (modi.cations to the stack allocated local variables \nare tracked by the transaction itself and so need not be saved here). Additionally it then checks if \nany of the local variables in the transaction s enclosing scope can be written to in the transaction. \nIf it can be guaranteed that they are not then the contents of the stack need not be saved at all. This \noptimization is especially bene.cial for small transactions that do not access any stack state (such \nas trans\u00adactions that atomically increment a shared global counter). Runtime Heuristics The compiler \npass inserts a checkpoint save operation before every transactional load, at runtime these calls to the \ncheckpoint save operation evaluate a set of heuristics to decide if a checkpoint is to be saved before \nthe dynamic load about to be executed. 1. Age of the transaction: One heuristic we use is the num\u00adber \nof dynamic transactional loads/stores that the trans\u00adaction has executed so far. This metric is often \na good indicator of the amount of work that the transaction has performed so far, since we do not want \nvery short running transactions to execute potentially costly checkpoint save operations. Therefore a \ntransaction will only save state at a checkpoint operation if the number of dynamic loads/\u00adstores so \nfar is greater than some threshold nldst. 2. Time elapsed since last checkpoint: The second heuris\u00adtic \ncontrols the frequency of saving checkpoints by checking if the current checkpoint save operation is \n   Figure 4: A transaction-private, circular buffer with k entries for saving and retrieving ordered \ncheckpoints atleast nfreq number of loads since the last one. A value of nfreq =1 would mean that a checkpoint \nsave would be performed for every dynamic transactional load or store. 3. Total number of active checkpoints: \nThe third heuristic checks if the total number of active saved checkpoints for a transaction is less \nthan some threshold nsaved. This is to reduce the cost of picking a checkpoint to restore during a con.ict \nand also to control the memory footprint for transactions that save a large amount of state on each checkpoint. \n 4. Average abort rate of the transaction: In low-contention scenarios where a transaction aborts rarely, \nthe bene.t of saving and restoring checkpoints is low. On the other hand, for a transaction that is experiencing \na very high abort rate especially after it has completed a signi.cant amount of work, saving and restoring \ncheckpoints can help reduce the amount of work it rolls back. This heuris\u00adtic compares the number of \naborts a transaction has expe\u00adrienced so far to a threshold and decides whether to save a checkpoint \nat an upcoming load or not.  All four of the thresholds described above are .xed on a per-transaction \nbasis at compile-time in our implementa\u00adtion. However making these thresholds tunable by the trans\u00adaction \nitself may be useful in some cases. For example, if a transaction is experiencing a high rate of aborts \ndue to high contention-levels, then it may accelerate its own rate of checkpointing so as to avoid these \naborts. For some pro\u00adgrams the programmer may be able to provide better esti\u00admates for these thresholds \nthan the compiler or runtime but this adversely impacts the programmability advantages of memory transactions. \nIn all our experiments in Section 6 the values for these parameters were chosen arbitrarily to be (1,256,32,1). \nFor the counter program, the frequency threshold of 256 would result in a checkpoint never be\u00ading saved. \nThus for just this program, we use the values (1,1,32,1) as the parameters for the heuristics, simply \nto force checkpoint-saving. 4. Runtime Support Checkpoint Chaining When a transaction experiences a con.ict \nit attempts to .nd the latest checkpoint that was saved before the access that caused the con.ict. To \ndo this, each transaction maintains a private timestamp which is sim\u00adply a monotonically increasing counter \nthat is incremented every time the transaction makes a transactional load or a store (note that this \ntimestamp is distinct from the transac\u00adtion s clock which is used to validate accesses in STMs that use \nglobal clocks). When a checkpoint is saved, this check\u00adpoint is tagged with the transaction s timestamp \nat that in\u00adstant and added to an ordered list of saved checkpoints. On a transactional access, the item \nbeing added to the transac\u00adtion s read/write sets is also tagged with the transaction s timestamp at \nthe time of the access. This allows the runtime to ef.ciently .nd the latest checkpoint that occurred \nbefore a particular con.icting access -it simply iterates over the ordered list of checkpoints and .nds \nthe one with the high\u00adest recorded timestamp that is also lower than the timestamp than the read/write \nset element is tagged with. The runtime chooses this checkpoint to restore to since it represents the \nlast known valid state of the transaction as far as this partic\u00adular access is concerned. The transaction \nthen validates all the read/write set elements that are tagged with a timestamp lower than this element \nand if successful, restores the check\u00adpoint. This validation step is to ensure that when the trans\u00adaction \nis restored to this saved checkpoint, its read/write sets at that point are valid and coherent. One way \nof storing these timestamps is in a circular\u00adbuffer with k-entries as shown in Figure 4. When a transac\u00adtion \nsaves a new checkpoint, it is inserted into this buffer into the slot pointed to by put and put is advanced \nto the next slot (in a predetermined direction, clockwise in this case). So at any instant this buffer \nholds the totally-ordered last k saved checkpoints. On con.ict to an access with times\u00adtamp t , the transaction \nstarts at put and iterates in the op\u00adposite direction (counter-clockwise in this example) to .nd a checkpoint \nwith a timestamp t<t'. If it .nds such a check\u00adpoint, we are guaranteed that there is no other checkpoint \nwith a timestamp t'' such that t<t'' <t'. When the check\u00adpoint with timestamp t' is returned, all the \nother checkpoints with timestamp higher than t' are invalidated since they were saved in a program state \nthat is after t'.   4.1 TM Model The discussion of checkpointing semantics and their execu\u00adtion model \nso far is independent of the speci.c TM model. Here we describe the support needed in the TM itself for \nreg\u00adistering and invoking checkpoints and so we focus on certain types of TM systems for this discussion. \nAt a high-level, the TM model we consider is that of a lock-based, write\u00adback, software TM that guarantees \nopacity, uses commit\u00adtime locking and performs validation at both encounter time (during an access) as \nwell as at commit time. This describes a large variety of systems including TL2 [23], TinySTM [4] and \nDSTM [5] among others. A thread begins executing a transaction T by calling tm start(). In this step \nall of T s data structures such as read/write sets, .lters etc., are al\u00adlocated and/or initialized. The \nglobal clock is also sampled and the timestamp is stored as T s start time. This clock is simply a monotonically \nincreasing global counter and the start time is used in the con.ict detection stage for deter\u00admining \nwhether a variable accessed during execution of T was concurrently updated by another concurrent thread. \nThe body of T the tm read(), tm write() and related calls for performing speculative accesses to shared \ndata. When .nished, T attempts to commit by calling tm end(). This marks the start of the validation \n(also referred to as con.ict detection) phase which we describe in more detail below. Validation and \nRestoring Checkpoints: In the .rst step in T attempts to validate RTp and validate and acquire a lock \non each element in its RWTp and WTp sets. The outline of this step for RWTp is shown in Algorithm 1. \nFor each element e in RWTp its current version number is compared to T s start time. If the former is \ngreater, then e was updated by another transaction i.e., e is invalid and T is aborted. If not, it checks \nwhether e is currently locked by another concurrent transaction. If it is then the latter will most likely \ncommit sometime in the future and update e thereby rendering T s copy invalid. Thus in this case too \nit aborts immediately. If e was both valid and not locked then T attempts to acquire a lock on it and \naborts if it is not able to. This process is repeated for every element in its read-write and write sets. \nIn the next step the read set for T is validated. This is similar to the above except no locks are acquired \n-for an element in the read set, if it is not currently locked and its version number is lower than T \ns start time then the element is considered valid. If all the elements of the read/write sets have been \nfound to be valid and all the locks are successfully acquired, then T is considered to have been validated \nand it moves into the write-back stage. In this stage, the values computed by T and produced into its \nlocal write buffer are .nally committed to main memory. After this, the transaction has .nished committing \nand releases all the locks it acquired in the validation step above. A checkpoint is invoked when the \nvalidation of its parent transaction encounters a con.ict. A high-level outline of the commit-time con.ict \ndetection stage for variables that are read-and-written is shown in Algorithm 1. Lines 6 -16 are related \nto the corrective con.ict resolution while the rest of the algorithm describes the standard detection \nand resolu\u00adtion scheme in our lock based optimistic concurrency con\u00adtrol system. The outer for-loop (which \nis also part of normal con.ict detection) iterates over the elements in read/write set and validates \nand locks them. If validation (isValid()) and lock acquisition (getLock()) for a particular ele\u00adment \nare both successful, that element is marked as valid (markValidated() in line 4). If either of these \nsteps fails for an element then the transaction attempts to .nd the latest checkpoint that was saved \nafter that particular ac\u00adcess (chooseCheckpoint() in line 6). If no such check\u00adpoint can be found, then \nthe transaction aborts. Otherwise, it validates the portion of its read-set upto the con.icting element \n(validateReadSetUntil()) in line 7). This prevents the transaction from restoring to a state that is \nin\u00advalid (speci.cally, a state in which its read-set has been in\u00advalidated). It then drops all the locks \nit has acquired so far (DropLocks() in line 9), samples the global clock and .nally restores the checkpoint \nthat was found (line 11). After restoring a checkpoint the transaction may modify its newly restored \nread/write sets in two ways. It may extend the read/write sets by calling tm read() or tm write(). That \nis, new elements are created and added to the respective tails of its read/write sets. Therefore these \nnew elements are in turn validated as the outer for-loop in Algorithm 1 reaches them when the transaction \nattempts to commit again. Sec\u00adondly the transaction may modify the values cached in the elements in read-write \nor write-only sets by writing to mem\u00adory locations it wrote to before the checkpoint restore. This does \nnot affect whether an element is or will be successfully validated. It also does not invalidate an already \nvalidated el\u00adement since the transaction would have acquired a lock for that element before it began \nexecuting. Validating the trans\u00adaction and invoking checkpoints for con.icts to read-only and write-only \n(or write-and-read) elements proceeds in a similar way except no locks are acquired for memory loca\u00adtions \nthat are read-only. Similarly the encounter-time valida\u00adtion algorithm for read/write transactional accesses \nis similar to the one above except that no locks are acquired.  Algorithm 1 Con.ict Detection for RW \nset 1: // To validate locations that are read and then written: 2: for all e . T.RWSET do 3: if isValid(e) \n&#38;&#38; getLock(e) then 4: markValidated(e) 5: continue 6: else if (c=T.ChooseCheckpoint(T,e)) then \n7: if ValidateReadSetUntil(T,e) &#38;&#38; T.retries<MAX RESTORES then 8: T.retries++ 9: DropLocks(T) \n10: readGClock(T, e) 11: RestoreHandler(T, c) 12: else 13: return TABORT 14: end if 15: else 16: return \nTABORT 17: end if 18: end for 19: T.HoldsLocks = true Multiple Con.icts: During a transaction s execution \nor vali\u00addation, multiple locations that it has accessed may have been invalidated. In practice this is \nquite common and the vali\u00addation/restoration scheme presented here handles this case seamlessly. Even \nthough multiple read/write set elements have been invalidated a transaction only detects con.icts one \nat a time. When a con.ict for a particular access has been detected, before the appropriate checkpoint \nis restored the transaction attempts to validate its read/write set as it ex\u00adisted when the checkpoint \nwas saved. If there were (not yet detected) con.icts to locations accessed before this partic\u00adular access, \nthen this validation step will fail and the trans\u00adaction simply aborts. In the second case, if there \nwere (not yet detected) con.icts to locations accessed after this partic\u00adular access, then these con.icts \ncan be safely ignored since the checkpoint restore would restore the transaction to an instant when accesses \nto these locations did not yet occur. After the checkpoint is restored, these same locations may be once \nagain accessed and they will be validated as they would be in a normal transaction. 5. Safety A TL2-like \nTM has the following properties: 1. Memory locations are added to the R, W, RW sets in the order in which \nthey were .rst accessed. For elements in each of these sets we de.ne an order ej -ei if ej appears before \nei in the set. 2. A transaction never reads inconsistent state.  3. Transactional reads or writes to \nthe same memory loca\u00adtion are not collapsed. Informally, T can commit successfully if the following se\u00adquence \nof checks are successful i) R is coherent and ii) RW &#38; W are coherent and locks can be acquired on \nall their elements and iii) R is still coherent Consider step (ii) during commit-time validation for \nT. According to the algorithm above, T aborts if lock acquisi\u00adtion failed for some word ei . RW or if \nthe version number changed since it was read i.e., it is no longer coherent. Con\u00adsider the latter case. \nWhen this con.ict is detected, startT < versionei and versionei = globalclock (0) where startT is T s \nstart time, versionei is the version of ei last written and globalclock is the current value of the global \nclock. Since the con.ict detection validates elements in order, this means .ej-ei . RW: ej is valid (1) \nBefore a checkpoint is restored the R is validated until ei. Therefore .ek-e . R: ek is valid (2) After \nthe checkpoint is restored, the last elements in RW and R in these newly restored sets are the ones immediately \nbefore ei in those sets before the checkpoint was restored. And therefore from (1) and (2), the newly \nrestored R and RW sets are coherent and valid and therefore the transaction T isin a consistent and valid \nstate. Moreover, since its read/write sets are valid at that point, the transaction can safely read the \nglobal clock and move its own startT forward to start ' where T start ' = globalclock (3) T Restoring \nthe transaction can eliminate the con.ict on ei as follows. After the transaction restore, lets say the \ntransaction accesses the memory location corresponding to ei again. From (3) and the second part of (0), \nversionei = start ' (4) T So this new access to the memory location corresponding to ei is guaranteed \nto see a valid version of ei and this access is guaranteed to not result in an encounter-time con.ict. \nAfter a checkpoint restore for ei, the transaction may have performed speculative loads or stores on \nnew memory locations. These new accesses are simply appended to the list of yet-to-be-validated accesses \n(just as would happen in a normal speculative access in T) and are locked and validated much like ei \n-when the transaction ultimately attempts to commit, each of the read, read-write and write-sets are \nre\u00advalidated in their entirety.  In our TM model (which corresponds to a TL2 like STM), transactional \nwrites to private (local) heaps locations are logged in a manner similar to transactional writes to shared \nheap locations. That is, the transaction maintains a separate local write buffer that logs the values \nbeing written. These values written are committed in order when the transaction commits successfully. \nSo the entire series of values being written to a transaction-private memory location are logged and \ntherefore a checkpoint restore can restore these values to any point in the transaction s execution. \nThe checkpoint and restore mechanisms handle these local read/write sets the same way they handle RTp \n, RWTp and WTp . However unlike the read/write sets, the transaction-local heap accesses need not be \nvalidated and no locks need be acquired on them. 5.1 Opacity When speci.ed inside transactions that satisfy \nthe Opacity property [13], checkpoint operations also satisfy this prop\u00aderty. Informally this means: \n Atomicity: All operations performed within a commit\u00adted transaction before and after all checkpoint \nrestores appear as if they happened at some indivisible point dur\u00ading an instant between the start of \nthe transaction and its commit.  Aborted State: The effects of an operation performed inside an aborted \ntransaction before or after a checkpoint operation are never visible to any other transaction.  Consistency: \nA transaction always observes a consistent state of the system, before and after all checkpoint re\u00adstores. \n  5.2 Isolation: A transaction before or after a checkpoint restore only ob\u00adserves consistent state, \ni.e., it is guaranteed to not see any updates that have not been committed by a live concur\u00adrent transaction. \nAlso, inserting checkpoint operations into a transaction at compile-time does not require knowledge of \neither (a) other concurrently executing transactions or (b) how the other transactions may have modi.ed \nvariables that caused the con.ict (which invoked this checkpoint) or (c) how many other transactions \ncommitted between the start of this transaction and the invocation of the checkpoint. However, even though \ncheckpoint handlers are semantically transparent, using them results in a different global ordering of \ntransactions than when they are not used and also permits a different subset of all con.ict-serializable \nschedules. 6. Experimental Evaluation We implemented the compiler pass for generating check\u00adpoint operations \nand optimizing them in the LLVM [8] com\u00adpiler (v2.4) and the runtime support for checkpoints in the TL2 \nTM system [23]. In this section we analyze the per\u00adformance impact of applying these corrective checkpoint \nre\u00adstores through experiments on parallel transactional work\u00adloads in the STAMP suite [2]. The list program \nis a li\u00adbrary component of STAMP that is used extensively in many workloads in the suite. The counter \nprogram implements a simple shared counter updated concurrently by several threads, a commonly occurring \nparallel programming arti\u00adfact. We used an unmodi.ed TL2 STM [2] as our base\u00adline optimistic concurrency \ncontrol system. Both the un\u00admodi.ed TL2 baseline and our checkpointing TL2 STMs use write-buffering, \nlazy-validation and commit-time lock\u00ading. All workloads were compiled using LLVM and gcc\u00ad 4.3.3 for .nal \ncode generation, with the default optimization .ags for each workload. We ran all experiments in Linux \non a machine with dual Intel Xeon X5500 4-core processors in which each was core clocked at 2.93GHz and \neach core also had hyperthreading enabled (for a total of 16 contexts). To reduce interference due to \nscheduling each thread was bound to a speci.c processor core uniformly. All the work\u00adloads were executed \nwith the standard reference inputs if de\u00ad.ned (else the inputs are described in the discussion below). \nThe baseline versions of the programs use normal optimistic concurrency control in transactions using \nan unmodi.ed TL2 STM and hence do not save checkpoints or restore them on con.icts. All timing measurements \nwere the average of 5 runs. The plots in Figure 5 show the speedups obtained using checkpoints -we use \nthe metric speedup to refer to the ratio of the execution time for the baseline case (with unmodi\u00ad.ed \nTL2) to that of the execution time using our compiler and runtime scheme for the same number of threads. \nWe experimented with several values for the set of parameters (nldst ,nfreq, nsaved, naborts) for the \nheuristics for reduc\u00ading state saving overheads but due to space limitations we report results for the \nset of values (1,256,32,1) except for the counter program for which we used (1,1,32,1) since a frequency \nthreshold of 256 would result in a checkpoint never being saved. 6.1 counter The counter program implements \na simple shared counter that is incremented by concurrent threads. This is a com\u00admonly occurring parallel \nprogramming construct in many parallel programs. The program has a single transaction that simply performs \na read, increment and write to the counter. The checkpoint save for this transaction does not have to \nsave any stack state. When the transaction validates its read\u00adand-write access, it acquires a lock on \nthe address of the counter and after a restore, it simply executes the entire transaction body while \nretaining the lock and then validates successfully and commits. This corrective action reduces the abort \nrate quite signi.cantly as is seen in Figure 7. The ex\u00adecution time speedup due to this ranges from 1.4X \nto over 4X. Although the amount of work done in each transaction is small, the amount of contention for \nthis program is very high. We noticed that for 16 threads even though the num\u00adber of aborts are reduced \n(meaning many of the checkpoint restores are successful), the overhead of executing them out\u00adweighs the \nbene.ts for this level of contention. There is very little state saved on a checkpoint as shown by the \ndata in Ta\u00adble 1. Moreover, almost every con.ict can trigger a restore in this program leading to a high \nnumber of average checkpoint restores per successful commit as shown in Figure 6.  6.2 list The list \nprogram implements a single linked list without duplicate key values. This program (or rather the linked \nlist library used by this program) is used extensively in the other STAMP benchmarks. The program creates \nand initializes an initial list and launches several threads which perform concurrent operations on this \nlist. An operation can be one of insert, find or remove with a speci.ed key to insert, .nd or remove \nwith each of them corresponding to 20%, 60% and 20% respectively of the total number of operations performed \non the list. Each of these operations is implemented as a transaction. Given a key k to insert the insert \nroutine iterates over the list and .nds the right position to insert this key into. Then the actual modi.cation \nof next pointers takes place as in standard list insertion. Similarly the remove routine iterates over \nthe list to .nd the element to remove. The insert and remove routines also increment and decrement the \nsize of the list. Since all three operations involve traversing through the list, most of the time spent \nin transactions in this program is spent in iterating through a list looking for a key (similar to the \ncode shown in Figure 3(b)). As each new element is encountered during iteration, an optimistic load is \nperformed on its next .eld. If there is a con.ict on this .eld then after the checkpoint is restored \nthe new next pointer is loaded (using tm read) and the search is resumed. The reduction in aborts due \nto this corrective action is signi.cant and the improvement in execution time ranges from 1.4X to 3.2X \n(Figure 5). The speedup is limited by the overheads of validating state before restoring a checkpoint \n-during the corrective action many newly committed pointers may be encountered which will be added to \nthe read/write sets and which will have to be validated. Moreover, if a con.ict occurs on reads to these \nnewly committed pointers a checkpoint may be restored again. Therefore there may be several checkpoint \nrestores for each successful commit. This is supported by the high number of restores per successful \ncommit shown in Figure 6. 6.3 kmeans The kmeans program implements a transactional version of the popular \nKmeans algorithm using optimistic concurrency control [2]. This workload contains a total of three critical \nsections implemented inside transactions. The .rst two add a value to a shared scalar variable. The checkpoint \noperations for these transactions are similar to the one discussed in the example of incrementing a shared \ncounter. Most of the time spent in transactions is in a third transaction in the work() Table 1: All \nnumbers are for 4 threads. Column (A) is the percentage of checkpoint restores that ultimately resulted \nin a commit of a transaction that would have otherwise aborted. Column (B) is the average size in bytes \nof the state saved by a checkpoint operation. Column (C) is the average call stack depth of a checkpoint \nsave operation, relative to the transaction s own stack frame Program Column (A) Column (B) Column (C) \ncounter 84.46 8 0 list 71.48 78 1 kmeans 82.18 16 0 ssca2 4.77 16 0 genome 66.18 64 2 sssp 8.02 92 3 \nvacation 73.23 64 3 intruder 1.61 178 6 labyrinth 54.05 112 2 bayes 13.8 198 2 function. This transaction \nbegins inside an outer loop and contains a loop within itself which updates elements in an array of numbers. \nMost of the con.icts suffered by a trans\u00adaction are due to accesses to shared values inside this in\u00adner \nloop. The average cost of each con.ict is high too -a con.ict on an access inside this loop means that \nthe updates made to the array so far are discarded and the transaction restarts updating the array from \nscratch. With a checkpoint the transaction instead restores state to the point just before the con.ict \ntherefore reducing wasted work. Additionally, since the transactional accesses are in the same stack \nframe as the transaction s start, very little state is actually saved (Table 1) since checkpointing the \nread/write sets takes a con\u00adstant amount of time and space, irrespective of their size. The reduction \nin abort rate for kmeans is shown in Figure 7. Note that the Y-axis in the .gure uses a log-scale. The \nabort rate is reduced by several orders of magnitude in some cases when using checkpoints. Figure 5 also \nshows that there is also a signi.cant reduction in running time -up to 1.58X in the case of 8 threads. \n 6.4 ssca2 Most of the critical sections in ssca2 are small and perform simple operations such as increments \nor adding scalar values to shared variables. However most of the time spent in this program is spent \nin one particular critical section which is inside a 2-deep loop nest. Corrective handling of con.icts \ncan therefore be very bene.cial here. However the transac\u00adtion also contains control .ow that is predicated \nupon shared variables. The future transactional accesses that will be per\u00adformed depend strongly on the \nresults of the accesses already performed. Hence for this transaction, when the checkpoint is invoked \nit rebuilds most of the transaction s state (accord\u00adFigure 5: Speedup in execution time over a parallel \nTL2 baseline version of the program running with the same number of threads (each bar shows the ratio \nbn/cn where bn is the wall clock execution time of the plain TL2 version of the program and cn is the \nexecution time of the checkpointed version).   Figure 7: Aborts in baseline and checkpointed versions \nof the programs (plots for intruder, vacation, labyrinth and list omitted for brevity) ing to the new \ncontrol .ow paths). Moreover, although the size of the state saved is quite small as seen in Table I, \nthe short sizes of the transactions and their high frequency means that checkpointing and restoring them \nresults in high overheads (the overhead of performing a checkpoint save is high relative to the amount \nof work done in each transaction instance). This is re.ected in the experimental results we ob\u00adtained \nwhich are shown in Figure 7 -the number of aborts is reduced signi.cantly but as can be seen in Figure \n5 the maximum speedup in execution time is about 1.18X.  6.5 genome The genome benchmark implements \na gene sequencing program that reconstructs the gene sequence from segments of a larger gene. There are \nseveral transactions for which checkpoints are generated -two of them together account for a signi.cant \nfraction of the total time spent in transactions. These transactions perform operations on a shared table \ndata structure which is in turn backed by a concurrent linked list. Therefore the checkpoints for these \ntransactions are similar to the checkpoints for the optimistic concurrent list opera\u00adtions discussed \nin the list program above. The speedup for this program due to this corrective con.ict resolution ranges \nfrom 1.14X to 1.59X.  6.6 SSSP The SSSP workload consists of a parallel transactional im\u00adplementation \nof Dijkstra s shortest path algorithm. The pro\u00adgram consists of multiple threads which execute a number \nof steps in each of which they perform several updates and queries on a dense graph. A query operation \nspeci.es a ver\u00adtex for which the shortest path from the source is returned while an update changes the \nlength of an edge. The graph being manipulated contains 300 vertices and is densely con\u00adFigure 6: Average \nnumber of checkpoint restores successful commit  nected. Each query involves an O(n2) amount of computa\u00adtion \nand a checkpoint is quite effective in amortizing this cost over updates. The level of connectivity in \nthe graph plays a signi.cant role in the amount of state that has to be saved and restored in the checkpoint. \nThe speedups for this pro\u00adgram range from about 0.87X to 2.42X. For sparse graphs we expect the performance \nimprovements to be higher since a change to an edge weight will result in fewer number of successor vertices \nbeing examined.  6.7 bayes The bayes program implements an algorithm for learn\u00ading Bayesian networks \nfrom observed data. The speedups for bayes were signi.cant -almost 2X with 4 threads. This program contains \nseveral transactions of varying sizes rang\u00ading from short transactions incrementing counters to long \nrunning transactions that query shared lists. Most of the contention and aborts came from a long transaction \nin the TMfindBestInsertTask() function which is read\u00adonly and iterates several shared linked lists while \nother trans\u00adactions are modifying them. As in the case of the list pro\u00adgram, checkpoints are effective \nin avoiding wasted work in this program by restoring the state of instances of this read\u00adonly transaction \nto an earlier point in their execution rather than aborting and starting from scratch. 6.8 Vacation \nThe vacation program from STAMP implements a travel reservation system powered by a non-distributed database. \nThe database consists of several table which are imple\u00admented as Red-Black trees. The reduction in aborts \nwas not as dramatic for most con.gurations as shown in Figure 7. The best speedup over the baseline version \nwas noted for the case of two threads -approximately 1.54X shown in the plot in Figure 5. Checkpoints \nare most effective in improving execution time for programs in which the save point occurs after some \nsigni.cant work has been done in the transaction (work which would be discarded in the case of an abort \nbut which is salvaged if a checkpoint is restored instead). In this program the highly contended accesses \noccur fairly early on in the transactions and therefore less work is discarded due to aborts. 6.9 Intruder \nThe intruder program implements a signature based net\u00adwork intrusion detection system. The targets of \ncontention for this program are several queue, list and tree datastruc\u00adtures that are used in the network \npacket capture, reassem\u00adbly and detection phases. In [21] the authors observed that a push operation \nonto a shared queue operation was the main source of contention in this program. Additionally this push \noperation occurs towards the end of a long run\u00adning transaction which means that quite a bit of work \nis wasted due to con.icts on this operation. With checkpoints the transaction simply restores its state \nto an earlier point in its execution therefore signi.cantly reducing wasted work. However the average \nstack depth of a checkpoint save is 7 (Table I) meaning the amount of state saved is also high. Inspite \nof this checkpoints improved execution time signi.\u00adcantly -nearly twice as fast as baseline TL2 with \n4 threads. 6.10 labyrinth The labyrinth in STAMP implements Lee s algorithm for .nding the shortest \ndistance between two given points on a grid. All the transactions contain a small high-contention critical \nregion that checks the status of a shared .ag. If this .ag is set, the transaction forces itself to restart \nwithout even attempting to commit. Therefore saving a checkpoint for this access would not be useful \nsince when the transaction attempts to commit it has typically already validated itself. However there \nare other accesses that are served well by checkpointing and this program shows moderate speedups of \nupto 1.75X (or about 42%) over the TL2 baseline.  6.11 Note on overheads The additional memory overhead \nof checkpointing is shown in Table 1, Column(B). Note that this only includes the amount of stack state \nsaved. It does not include the memory required for checkpointing registers, heap state and transac\u00adtional \nread/write sets as these are also saved by the baseline TM. One exception is storing the intermediate \nstates of read\u00ad/write sets and heap alloc/de-allocs. The overhead for saving these is constant in space \nfor a checkpoint (roughly equiva\u00adlent to saving 4 pointers) and is excluded from the Table. The magnitude \nof performance improvement from using checkpoints depends on the cumulative cost of state saving and \nrestoration, relative to the cost (including wasted work) of a complete abort. We found that the cumulative \namount of state saved strongly correlated to the speedups. While trans\u00adaction internal state such as \nread/write sets and speculative heap alloc/deallocs were quite ef.cient to checkpoint, the Figure 8: \nOverhead of checkpoint saving in an execu\u00adtion of list with very high-contention -60%/20%/20% find/insert/remove \nand a small key range. Each of the lines shows speedup over single-threaded TL2 for a spe\u00adci.c value \nof n freq, the frequency of checkpointing as de\u00adscribed in Section 3.2  cost of saving stack frames \nwas especially in.uential on per\u00adformance. Therefore transactions with accesses occurring in the same \nstack frame without any local variables being mod\u00adi.ed, performed best. Additionally our technique is \nbetter suited to long running transactions that would lose a sub\u00adstantial amount of work on an abort. \nThe frequency of saving checkpoints has an interesting in.uence on running time. If this frequency is \ntoo high, the state saving overheads domi\u00adnate and performance can be poor. However if this frequency \nis too low, a checkpoint restore may restore state to a point very early in the transaction therefore \nminimizing the reduc\u00adtion in wasted work. This suggests that there may be a pro\u00adgram speci.c (and input \ndata set speci.c) sweet spot for this frequency -a question that we intend to explore in future work. \nThe plot in Figure 8 shows the overheads of saving check\u00adpoints for a high-contention list that is used \nwith a very small key range. The overheads are all quite small with the higher frequency of saving checkpoints \nresulting in slightly higher overheads (this plot does not include the overhead of .nding and restoring \na checkpoint, only that of saving one). The small amount of state to be saved per checkpoint is the principle \nfactor in these low overheads. The Figure 5 shows that for all the programs the overhead of saving checkpoints \nin a single-threaded execution is not signi.cant. This is because of the contention heuristic described \nin Section 3.2. This heuristic throttles the rate of checkpoint saving when the average abort ratio is \nlow. Since in a single\u00adthreaded case the abort ratio is zero, effectively no check\u00adpoints are saved. \nFigure 8 therefore shows that even the most frequent checkpointing considered here does not result in \nsigni.cantly worse parallel speedup than the baseline TL2 case. 7. Related Work There is a substantial \namount of literature on contention management for transactions and con.ict resolution in par\u00adticular. \nThe studies in [7][5][23] propose various resolution schemes which decide which of a pair of con.icting \ntransac\u00adtions is allowed to commit. However none of these allow for both transactions in the con.ict \nto successfully commit. In [12] the authors propose a TM model that in theory allows two con.icting transactions \nto commit provided the online opacity-permissiveness property is preserved. The DASTM system in [10] \nis a dependence-aware STM in which data is forwarded between two transactions that have a dependence \nso that both of them can commit safely. Abstract Nested Transactions [9] allow a programmer to specify \noperations that are likely to be involved in benign con.icts and which can be re-executed. In [11] the \nauthors propose annotating boosted transactions with checkpoints which allows them to partially abort. \nTo our knowledge this work was the .rst to propose the notion of transaction checkpointing and this work \nremains the closest to the work presented in this paper. These checkpoints were de.ned in the context \nof boosted objects with commutative methods and storing and saving state including the program stack \nand active frames was done manually. In contrast, checkpoints in our work can be placed at arbitrary \npoints in the transaction without needing com\u00admutativity of operations and their generation and execution \nis completely transparent to the programmer. In [20] the au\u00adthors describe an HTM protocol and system \nthat supports intermediate state checkpointing. However, this system does not appear to perform complete \ncheckpoints -speci.cally, the state of the stack is not saved -this is critical since the checkpoint \nmay have been saved in a stack frame that has since returned (and therefore the checkpoint cannot be \nre\u00adstored if the stack is not saved). This is a common occurrence in most of the programs we studied. \nIn TMs with open nesting [14] physical serializability is traded off for abstract serializability. With \nopen nesting two transactions may con.ict at the memory level but both may be permitted to execute if \nthe abstract state of shared data is consistent with some serial execution. The RetCon [15] hardware \nmechanism tracks symbolic dependences between shared values and uses it to repair transactions. The Twilight \nSTM system [18] augments transactions with special irrevo\u00adcable code that repairs the transactions when \ninconsistencies are detected before transaction commit. The Galois model in [3] model and transactional \nboosting in [16] rely on commu\u00adtativity properties of methods and both allow for eliminat\u00ading structural \ncon.icts. Methodologies for developing self\u00adadjusting programs -programs that are able to automatically \nand ef.ciently respond to changes in their inputs, have been studied for a few decades now. A number \nof algorithms in domains such as graph problems and geometry have been shown to have ef.cient incremental \nalgorithms An exhaus\u00adtive survey of prior work in this area is in [17]. Such pro\u00adgrams may be speci.ed \nin a special language or framework such as [1], [22] [19] that provide runtimes for recording de\u00adpendences \nand other information that are used to direct the re-execution. Finally there is a signi.cant amount \nof work on reconciling con.icting updates in mobile and distributed database systems [6] which is closely \nrelated to the present work.  8. Conclusions In this work we presented a compiler-driven con.ict recov\u00adery \nscheme using which a transaction that has been inval\u00adidated due to one or more con.icts can attempt to \nrecover from them with the help of checkpoints that restore the trans\u00adaction s state to a previous intermediate \npoint in its exe\u00adcution and execute from that point. We described compiler optimizations to reduce the \namount of state saved by these checkpoints and runtime support for .nding and restoring a checkpoint. \nOur experimental evaluation shows that using such checkpoints reduced the number of aborts by several \norders of magnitude for some programs and speedups of up to 4X in execution time on a real machine, relative \nto trans\u00adactional programs that did not use them. One interesting av\u00adenue for future work is a cost model \nof transaction execution that can be used at runtime to decide whether a particular program location \nis cost-effective for saving a checkpoint -a host of factors from the depth of the call stack at that \npoint, to the amount of work done so far in the transaction, need to be evaluated to guarantee that a \nsave/restore will bene.t perfor\u00admance. Compiler analyses especially points-to analyses can be very useful \nin reducing the amount of state (especially, thread-local stack state) that is saved and restored. Acknowledgments \nThe authors gratefully acknowledge the support of NSF grants CCF-1018544 and CCF-0916962. References \n[1] Acar U.A., Hammer M.A., Chen. Y, CEAL: A C-Based Lan\u00adguage for Self-Adjusting Computation . In Proceedings \nof the International Conference on Programming language design and implementation (PLDI) 2009. [2] Minh \nC.C., Chung J., Kozyrakis K., Olukotun K., STAMP: Stanford Transactional Applications for Multi-Processing \n. In IISWC 2008 35-46 [3] Kulkarni M., Pingali K., Walter B., Ramanarayanan G., Bala K., Chew L.P., Optimistic \nparallelism requires abstractions . In the International Conference on Programming Language and Design \n2007 [4] Felber P., Fetzer C., and Riegel T., Dynamic Performance Tun\u00ading of Word-Based Software Transactional \nMemory . In Pro\u00ad ceedings of the 13th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming \n(PPoPP) 2008 [5] Herlihy M., Luchangco V., Moir M., and Scherer W.N., Soft\u00adware transactional memory \nfor dynamic-sized data structures . In Proceedings of the twenty-second annual symposium on Prin\u00adciples \nof distributed computing (PODC 03) [6] Phatak, S.H. and Badrinath, B.R. Multiversion Reconciliation for \nMobile Databases in Proceedings of the 15th international Conference on Data Engineering 1999. [7] W. \nN. Scherer III and M. L. Scott, Advanced Contention Management for Dynamic Software Transactional Memory \nin Symp. on Principles of Distributed Computing, Las Vegas, NV, 2005 [8] The LLVM Compiler Infrastructure, \nwww.llvm.org [9] T. Harris and S. Stipic Abstract Nested Transactions , in 2nd Workshop on Transactional \nComputing (TRANSACT 07). [10] Ramadan, H. E., Roy, I., Herlihy, M., Witchel, E, Commit\u00adting Con.icting \nTransactions in an STM , International Sym\u00adposium on the Principles and Practice of Parallel Programming \n(PPOPP) 2009. [11] M. Herlihy and E. Koskinen Checkpoints and continuations instead of nested transactions \n, in the 3rd Workshop on Transac\u00adtional Computing [12] Dmitri Perelman and Idit Keidar, On Avoiding Spare \nAborts in Transactional Memory , in Proc. 21st Symposium on Paral\u00adlelism in Algorithms and Architectures \n(SPAA) 2009. [13] Guerraoui R., and Kapalka M., On the correctness of transac\u00adtional memory. In Principles \nand Practice of Parallel Program\u00adming (PoPP) 2008. [14] Moss, J. E. and Hosking, A. L. 2006. Nested transactional \nmemory: model and architecture sketches . In Science of Com\u00adputer Programming 63, 2 (Dec. 2006), 186-201 \n[15] Blundell C., Raghavan A., Martin M.K., RETCON: transac\u00adtional repair without replay . In Proceedings \nof the 37th annual international symposium on Computer architecture (ISCA 10) [16] Herlihy, M. and Koskinen, \nE., Transactional boosting: a methodology for highly-concurrent transactional objects . In Symposium \non Principles and Practice of Parallel Programming 2008. [17] Ramalingam G. and Reps T., A Categorized \nBibliography on Incremental Computation. In Proceedings of the 20th An\u00adnual ACM Symposium on Principles \nof Programming Languages pages 502510, 1993. [18] Bieniusa A., Middelkoop A., Thiemann P., Actions in \nthe Twilight: Concurrent irrevocable transactions and Inconsistency repair . Technical Report 257, Insitut \nfr Informatik, Universitt Freiburg, 2010 [19] Pugh, W. and Teitelbaum, T., Incremental computation via \nfunction caching . In Proceedings of the 16th ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages 1989. [20] Waliullah M.M. and Stenstrom P., Intermediate Checkpoint\u00ading with Con.icting Access \nPrediction in Transactional Memory Systems . In Proceedings of the 22nd IEEE International Paral\u00adlel \nand Distributed Processing Symposium 2008.  [21] Zyulkyarov F., Stipic S., Harris T., Unsal O.S., Cristal \nA., Hur I., and Valero M. Discovering and understanding performance bottlenecks in transactional applications \nPACT 2010 [22] Yellin, D. M., Strom, R. E., INC: a language for incremental computations ACM Transactions \non Programming Languages and Systems 13, 2 (Apr. 1991), 211-236. [23] Dice D., Shalev O., Shavit N., \nTransactional Locking II . In proceedings of the 20th International Symposium on Distributed Computing \n(DISC), Stockholm, Sweeden, Sept. 2006.   \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Several studies have shown that a large fraction of the work performed inside memory transactions in representative programs is wasted due to the transaction experiencing a conflict and aborting. Aborts inside long running transactions are especially influential to performance and the simplicity of the TM programming model (relative to using finegrained locking) in synchronizing large critical sections means that large transactions are common and this exacerbates the problem of wasted work. In this paper we present a practical transaction checkpoint and recovery scheme in which transactions that experience a conflict can restore their state (including the local context in which they were executing) to some dynamic program point before this access and begin execution from that point. This state saving and restoration is implemented by checkpoint operations that are generated by a compiler into the transaction's body and are also optimized to reduce the amount of state that is saved and restored. We also describe a runtime system that manages these checkpointed states and orchestrates the restoration of the right checkpointed state for a conflict on a particular transactional access. Moreover the synthesis of these save and restore operations, their optimization and invocation at runtime are completely transparent to the programmer. We have implemented the checkpoint generation and optimization scheme in the LLVM compiler and runtime support for the TL2 STM system. Our experiments indicate that for many parallel programs using such checkpoint recovery schemes can result in upto several orders of magnitude reduction in number of aborts and significant execution time speedups relative to plain transactional programs for the same number of threads.</p>", "authors": [{"name": "Jaswanth Sreeram", "author_profile_id": "81338490999", "affiliation": "Intel Labs, Santa Clara, CA, USA", "person_id": "P3856034", "email_address": "jaswanth.sreeram@intel.com", "orcid_id": ""}, {"name": "Santosh Pande", "author_profile_id": "81409594751", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P3856035", "email_address": "santosh@cc.gatech.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384620", "year": "2012", "article_id": "2384620", "conference": "OOPSLA", "title": "Safe compiler-driven transaction checkpointing and recovery", "url": "http://dl.acm.org/citation.cfm?id=2384620"}