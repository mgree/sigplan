{"article_publication_date": "10-19-2012", "fulltext": "\n IFRit: Interference-Free Regions for Dynamic Data-Race Detection * Laura Ef.nger-Dean Brandon Lucia \nHans-J. Boehm Luis Ceze Dan Grossman HP Labs University of Washington hans.boehm@hp.com {e.nger,blucia0a,luisceze,djg}@cs.washington.edu \nAbstract We propose a new algorithm for dynamic data-race detec\u00adtion. Our algorithm reports no false \npositives and runs on arbitrary C and C++ code. Unlike previous algorithms, we do not have to instrument \nevery memory access or track a full happens-before relation. Our data-race detector, which we call IFRit, \nis based on a run-time abstraction called an interference-free region (IFR). An IFR is an interval of \none thread s execution during which any write to a speci.c variable by a different thread is a data race. \nWe insert instrumentation at compile time to monitor active IFRs at run-time. If the runtime observes \noverlapping IFRs for con.icting accesses to the same variable in two dif\u00adferent threads, it reports a \nrace. The static analysis aggre\u00adgates information for multiple accesses to the same variable, avoiding \nthe expense of having to instrument every memory access in the program. We directly compare IFRit to \nFastTrack [10] and Thread-Sanitizer [25], two state-of-the-art fully-precise data-race detectors. We \nshow that IFRit imposes a fraction of the overhead of these detectors. We show that for the PARSEC benchmarks, \nand several real-world applications, IFRit .nds many of the races detected by a fully-precise detector. \nWe also demonstrate that sampling can further reduce IFRit s performance overhead without completely \nforfeiting preci\u00adsion. * This material is based upon work supported by the National Science Foundation \nGraduate Research Fellowship under Grant DGE-0718124, the National Science Foundation under Grants CCF-1064497 \nand CCF\u00ad0702226, the University of Washington Royalty Research Fund, and gifts from Google. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October \n19 26, 2012, Tucson, Arizona, USA. Copyright c . 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 Categories \nand Subject Descriptors D.1.3 [Programming Techniques]: Concurrent Programming parallel program\u00adming; \nD.2.5 [Software Engineering]: Testing and Debug\u00adging monitors, testing tools General Terms Languages, \nAlgorithms, Reliability Keywords data-race detection, concurrency, interference\u00adfree regions 1. Introduction \n 1.1 Motivation Concurrent programming has become the norm, as mul\u00adtiprocessor computers require programmers \nto use paral\u00adlelism to achieve high performance. In addition, many im\u00adportant application domains (e.g., \nmobile computing, dis\u00adtributed sensing) are inherently concurrent. Unfortunately, concurrent programming \nis dif.cult, and concurrent pro\u00adgramming errors are common. A data race is a pathological concurrent \nprogram behav\u00adior that is often the result of a programming error. A data race occurs when multiple threads \naccess the same mem\u00adory location, at least one access is a write, and the accesses are not ordered by \nsynchronization. Determining the mean\u00ading of a program s execution when a data race has occurred is deeply \nproblematic. In C and C++, programs that per\u00admit data races have unde.ned semantics [2] an execution \ncan literally do anything. Many managed languages like Java have memory models that provide somewhat \nstronger guar\u00adantees about what a program with data races is allowed to do. Memory models that provide \nsuch strong guarantees and also permit ef.cient language implementations are complex and subtle. As a \nresult, data races should almost always be avoided [2, 26]. Programmers must write programs carefully \nto ensure data races are prohibited in all executions. Unfortunately, code with potential data races \nis easily overlooked and even thorough testing may fail to uncover data races that only rarely affect \nprogram behavior. The dif.culty of dealing with data races necessitates tools to detect them.  Over \nthe last .fteen years, many data-race detectors have been developed, exploring the design space along \nfamiliar axes of static vs. dynamic detection and performance vs. precision. A typical dynamic data-race \ndetector observes an execution and reports if data races occur on (just) that execution. Ideally such \na detector would run fast, report all data races, and not give any false reports, i.e., reported data \nraces that did not occur. In practice, fully precise data-race detectors run programs orders of magnitude \nslower [10, 21] than uninstrumented execution, so it is typically useful to sacri.ce precision in principled \nways while still detecting many data races.  1.2 Our Approach This paper describes IFRit, a dynamic \ndata-race detector that never reports false data races but may miss data races.1 Prior work with this \nstrategy has relied on sampling: removing in\u00adstrumentation from some memory accesses. Our work can also \nleverage sampling, but more fundamentally, it separates the instrumentation from the memory access and \ncan coa\u00adlesce the instrumentation for many accesses to the same vari\u00adable. For example, consider:2 lock(m); \nfor(int i = 0; i < 1000; i++) { ... *x =...; ... } unlock(m); Assuming the loop contains no synchronization, \nthere is no reason to instrument each access to x. Instead, to detect data races on x, it suf.ces to \ninstrument the region between the lock and unlock as writing to x, which dynamically requires instrumentation \nonly before and after the loop. Moreover, we go beyond simple synchronization-free re\u00adgions by incorporating \nour recent work on interference-free regions [7], as explained in more detail in Section 2. Con\u00adsider \nthis example, where again we assume any code not shown is known to be synchronization-free: *x = ...; \nwhile(...) { *x = ...; lock(m); ... unlock(m); } *x = ...; 1 Ifrits are spirit-beings from Arabian mythology \nthat, like data races, are known for being mischievous and elusive. 2 Here, we assume that x is a local \nvariable or register, and the location pointed to by x is a shared variable. Henceforth when we say accessing \nx or writing to x, we are referring to the location pointed to by x. Here again it suf.ces to instrument \nthat x is written to some\u00adwhere in this code region, which can be done once before and once after the \nloop. It is surprising that doing so cannot lead to reporting data races that are not true data races, \nsince the code above does have synchronization. But any concur\u00adrent access to x would have to race with \none of the accesses to x in the code above. To place instrumentation in sound places while improving \nperformance, we use static analysis. For a given variable, we can conservatively identify interference-free \nregions, hence\u00adforth IFRs, which for the purposes of data-race detection are regions in which any concurrent \naccess to the variable is in\u00addeed a data race. For the second example above, the key in\u00adsight is that \nthe IFRs induced by the accesses to x overlap such that every code point falls in at least one IFR for \nx.  1.3 Results Our work is the .rst to use the notion of IFRs for data-race detection. We have implemented \nour technique in the LLVM compiler framework [14] and used it to detect data races in mature real-world \nsoftware. The implementation requires a novel static analysis for soundly identifying IFRs and a dy\u00adnamic \nanalysis for .nding races using the static instrumen\u00adtation. We directly compare our system with two \nstate-of\u00adthe-art systems and show our performance is considerably better orders of magnitude better in \nseveral cases. We show that the races our system detects include nearly all the races reported by the \nprecise detectors. We show that by combin\u00ading our approach with sampling we can reduce our over\u00adheads \nenough that our technique could be used in deployed systems or integrated into a build environment. We \nalso com\u00adpare our results with published results for other imprecise data-race detectors. We have developed \na formal model of IFRs for data-race detection and use it prove correctness: Any data race reported by \nour approach is a true data race. The remainder of this paper is organized as follows. Sec\u00adtion 2 discusses \nthe details of the IFR abstraction and ex\u00adplains our new technique for applying IFRs to data-race de\u00adtection. \nSection 3 presents the static analysis used by IFRit to insert instrumentation calls. Section 4 discusses \nthe run\u00adtime implementation. Section 5 formalizes IFRs and proves that our technique is sound. Section \n6 gives empirical data il\u00adlustrating IFRit s low overhead and high precision. Section 7 discusses prior \nwork related to IFRit in more detail, and Sec\u00adtion 8 concludes and lays out possible future work. 2. \nInterference-Free Regions This section introduces interference-free regions and applies them to dynamic \ndata-race detection. 2.1 Background An interference-free region, or IFR, is a region of a single thread \ns execution trace surrounding an access to a shared variable [7]. The region extends from the .rst acquire \naction  Figure 1. Interference-free region for an access. prior to the access, to the .rst release \naction following the access, noninclusive. By an acquire action, we mean an ac\u00adtion that is synchronized \nwith earlier actions in the trace; for example, a lock acquire is an acquire action because earlier releases \nof the lock synchronize with the acquire. Similarly, a release action synchronizes with later actions \nin the trace. These synchronizes-with relationships, com\u00adbined with the ordering on actions of each thread \ninduced by program order, determine the happens-before order, a par\u00adtial order over actions in the execution \n[13]. Other acquire actions include thread joins and reads of Java volatile vari\u00adables or C11 atomic \nvariables. Other release actions include thread create actions and writes to Java volatile variables \nor C11 atomic variables. Note that IFRs are purely dynamic constructs, and there\u00adfore, for each memory \naccess in an execution trace, there is exactly one IFR for that access, and all operations by the same \nthread either do or do not fall into the IFR for that ac\u00adcess. For example, Figure 1 shows the IFR for \nan access to shared variable x. The .gure shows only the actions for a single trace. The IFR extends \nfrom the lock at line A to the unlock at line B. Note that the IFR is always at least as large as the \naccess s synchronization-free region, which extends from the most recent synchronization action to the \nnext synchronization action. We can characterize the IFR as the union of three regions: the synchronization-free \nregion surrounding the access, the acquire-free region before the synchronization-free region, and the \nrelease-free region after the synchronization-free region. We distinguish two types of IFRs: regions \nsurrounding a read of a shared variable, and regions surrounding a write. We will call these read IFRs \nand write IFRs, respectively. Interference-free regions are so called because while the thread is executing \nthe interference-free region, no other thread can write to the shared variable accessed by the IFR s \naccess without inducing a data race. A data race is a pair of accesses to the same variable by different \nthreads, where at least one access is a write and the accesses are not ordered by synchronization. Therefore, \nif an execution has no data races, the variable (e.g., x in Figure 1) is interference-free for the duration \nof the execution of the region. Prior work [7] used IFRs for compiler optimization. Compilers typically \nperform optimizations only within syn\u00adchronization-free regions. Since the compiler may assume data-race-freedom \nfor C and C++ programs [11, 12], IFRs extend the scope in which the compiler can reason sequen\u00adtially \nabout a variable s value. Our work takes the fundamen\u00adtal idea of interference-free regions and applies \nit to another purpose: dynamic data-race detection.  2.2 IFRs for Data-Race Detection We say that two \nIFRs overlap if parts of their executions happen simultaneously: that is, the .rst IFR to start must \nend before the second IFR begins.3 The novel insight of this work, then, is this: If the IFRs for two \naccesses to the same variable in different threads overlap, and at least one is a write IFR, then the \naccesses form a data race. For example, consider the execution shown in Figure 2. Two threads access \nvariable x, and one of the accesses is a write. We have highlighted the corresponding (overlapping) IFRs \nfor these accesses: a write IFR for the write to x in Thread 1, and a read IFR for the read of x in Thread \n2. The .gure shows a number of possible happens-before edges in the execution. Note that there is no \nway to trace a path between the two accesses to x using the happens\u00adbefore edges. Therefore, the two \naccesses are not ordered by happens-before, and form a data race. We propose a dynamic data-race detection \nscheme based upon this insight about overlapping IFRs: 1. First, a compiler analysis identi.es code points \nthat fall within IFRs for accesses. 2. Based on this analysis, we statically insert calls to our run-time \nto start and stop dynamic monitors for different memory locations. 3. During execution, we report overlapping \nmonitored re\u00adgions for con.icting, concurrent accesses.  If two IFRs for the same variable do not overlap, \nthe two accesses may or may not form a data race. Figure 3(a) shows a case in which the accesses are \nordered by synchronization on a mutex m1. However, it is possible that the IFRs for two racy accesses \nwill not overlap; in Figure 3(b), the two threads use different mutexes to protect variable x, but the \ncritical sections do not happen to overlap, so we will not catch the race. Such cases represent false \nnegatives in our detector. Our detector is therefore sound and incomplete: no false positives, but some \nfalse negatives. Although our algorithm has false negatives, it does have the nice quality that we might \ninformally call pseudo-com\u00adpleteness : if we run a program with our detector on enough different executions, \nwe will eventually catch any data race 3 Our implementation inserts instrumentation that serializes the \nbeginnings and ends of IFRs for the same variable. See Section 4.3 for more details.   Figure 3. Non-overlapping \nIFRs. in the program. This property follows from a standard the\u00adorem about happens-before memory models \n(e.g., Theorem 8.2 in [5]): if an execution of a program has a data race (i.e., two con.icting accesses \nunordered by happens-before), then there exists a sequentially consistent execution in which the two \naccesses execute consecutively.4 If the racing accesses occur consecutively, their IFRs will overlap, \nso there exists an execution of the program for which our algorithm would catch the race. This distinguishes \nus from heuristic-based al\u00adgorithms [24], which only look for certain classes of data races (e.g., races \ncaused by inconsistent locking). Because there are typically many variable accesses dur\u00ading an execution, \nand therefore many interference-free re\u00adgions, we use several techniques to reduce the overhead of our \ndynamic detector. First, our static analysis merges the IFRs for accesses to the same variable whose \nIFRs over\u00adlap, allowing us to insert a single instrumentation call for many actual IFRs in the execution. \nSecond, if IFRs for two or more variables start and stop at the same point, we handle all of the variables \nwith a single instrumentation call. Third, 4 An execution is sequentially consistent if all threads see \nthe same global order of actions, the actions of each thread are in program order, and every read sees \nthe most recent write to the same location. we can sample IFRs to reduce the burden on the run-time. \nSince any two overlapping IFRs for con.icting, concurrent accesses represent a data race, sampling does \nnot compro\u00admise our soundness guarantee. The detector is usable with\u00adout sampling, but even limited use \nof sampling (say, moni\u00adtoring 50% of the time) yields appealingly low performance overheads (see Section \n6).  2.3 Synchronization-Free Regions A possible variant on this data-race detection scheme would be \nto use the same kind of instrumentation, but monitor for overlapping synchronization-free regions (SFRs) \ninstead of overlapping interference-free regions. We believe that a scheme using interference-free regions \nis superior for two reasons. First, as shown in Figure 1, the interference-free re\u00adgion for an access \nalways subsumes the synchronization-free region for an access, so monitoring interference-free regions \nwill .nd more bugs. Second, the larger size of interference\u00adfree regions directly implies a smaller number \nof instrumen\u00adtation calls (for example, the detector does not need to stop monitoring variables at acquire \ncalls), so the performance overhead of a synchronization-free region detector would likely be higher. \nPrior work on con.ict exceptions by several of the authors of this paper uses synchronization-free regions \nto imple\u00adment a lightweight hardware concurrency exception model [16]. The model ignores data races in \nnon-overlapping SFRs, much as IFRit s algorithm ignores data races in non\u00adoverlapping IFRs. The paper \nproves formally that exception\u00adfree executions (i.e., executions with no data races in over\u00adlapping SFRs) \nare guaranteed to have sequentially consis\u00adtent behavior. Moreover, SFRs execute atomically in the absence \nof exceptions. If IFRit s static inference of IFRs were ideal, IFRit too would guarantee sequential consistency \nfor exception-free executions and atomicity of SFRs, since IFRs are always strictly larger than SFRs \nand therefore IFRit would report strictly more races than the con.ict-exceptions work. Of course, IFRit \ns static analysis is necessarily con\u00adservative, so the SFR surrounding an access may not be fully covered \nby the dynamic monitors.5 Therefore IFRit has weaker guarantees than the con.ict-exceptions work. IFRit \ns advantage is that it does not need to instrument every memory access and it does not require specialized \nhardware. 3. Static Analysis This section presents our static analysis to insert instrumen\u00adtation for \nthe run-time. We start by explaining the types of instrumentation calls implemented by our analysis (Section \n3.1) and giving a simple correctness criterion for the analy\u00adsis (Section 3.2). Then we present the algorithm \nin two steps. First, we describe a simpli.ed algorithm for inserting instru\u00admentation calls (Section \n3.3). Second, we present the re.ned algorithm actually used in our implementation (Section 3.4). 5 For \nan example of why this might happen, see Section 3.6.  The simpli.ed version is a useful starting point, \nand the re\u00ad.ned algorithm derives directly from the ideas discussed in Section 3.3. Section 3.5 details \nthe data.ow analysis we use to implement the algorithm, and Section 3.6 discusses a lim\u00aditation of our \nprototype implementation. Throughout this section we will refer to variables ; vari\u00adables here refer \nto any memory location at run-time, includ\u00ading array elements, global variables, and so on. In our imple\u00admentation, \nvariables are SSA names within a compiler pass, so we are guaranteed that the variable always points \nto the same memory location at run-time. Because LLVM automat\u00adically converts non-address-taken local \nvariables to registers, our analysis does not process local variables if their address is not taken. \n3.1 Instrumentation The static analysis inserts calls to the run-time to start and stop monitors for \ndifferent variables.6 A strong monitor for variable x indicates that the thread is currently in an IFR \nfor a write to x. A weak monitor for variable x indicates that the thread is in an IFR for either a read \nor write to x. If monitors for the same variable are active in two different threads at the same time, \nand at least one of the monitors is strong, then there must be overlapping IFRs for con.icting, concurrent \naccesses, so the run-time reports a data race. Initially, we consider a simple instrumentation scheme \nin which we start and stop monitors for a single variable via three instrumentation calls: start_strong_monitor(void \n*x); start_weak_monitor(void *x); stop_monitor(void *x); start strong monitor and start weak monitor \nhave no effect if a monitor of the correct type is already active for the argument. stop monitor has \nno effect if no monitor is active. For example, a simple critical section could be instru\u00admented as follows: \nlock(m); start_strong_monitor(x); ... *x = ...; ... stop_monitor(x); unlock(m); A downside of using \nstatic instrumentation is that the monitored region may not cover the entire dynamic IFR of an access; \nfor instance, in Figure 4(a), our analysis does not insert the instrumentation to start the monitor until \nafter the if statement.7 On the plus side, since monitors are tied to 6 Our use of the term monitor has \nnothing to do with the synchronization construct of the same name. 7 Placing a call to start strong monitor \nbefore the if statement would violate the .rst correctness criterion in Section 3.2.  (b) Monitored \nregions may combine IFRs for several ac\u00adcesses to the same variable. Figure 4. Two examples of IFRs \nvs. monitors in execution. variables, not accesses, we can use a single monitor to cover the IFRs for \nmany accesses to the same variable. In Figure 4(b), the program may read x one or more times, but we \nonly need to call start weak monitor once.  3.2 Correctness Ideally, we would insert calls to start \nstrong monitor, start weak monitor and stop monitor such that a mon\u00aditor would be active if and only \nif the corresponding point in the program s execution fell in an IFR for an access to the monitor s variable. \nIn practice, we cannot statically deter\u00admine the boundaries of every IFR, so we monitor a subset of the \npossible operations that fall into one or more IFRs in the execution. Crucially, we must not start a \nmonitor unless an IFR for an access to the monitor s variable is active, and we must stop the monitor \nif no such IFRs are active. Formally, we meet the following two correctness conditions: 1. Consider any \nexecution trace from a call to start strong monitor(x) to stop monitor(x), with no in\u00adtervening calls \nto stop monitor(x). Each operation in the trace must fall within an IFR for a write of x. 2. Consider \nany execution trace from a call to start weak monitor(x) to stop monitor(x), with no intervening calls \nto stop monitor(x). Each operation in the trace must fall within an IFR for a read or write of x.  \n 3.3 Simpli.ed Algorithm This section presents a simple intraprocedural algorithm for inserting instrumentation \ncalls. First, for each program point p, we .nd two sets of vari\u00adables: 1. W[p]: the set of variables \nthat must be written on any path through the current function s control-.ow graph from p to the next \nacquire call (or the end of the function). 2. RW[p]: the set of variables that must be read or written \non any path through the current function s control-.ow graph from p to the next acquire call (or the \nend of the function).  At each program point p, W[p] represents the set of vari\u00adables for which it is \nsound to start a strong monitor us\u00ading start strong monitor(x). The reason: If the variable will be written \nbefore the next acquire on every path from p, then all executions of p must be in an IFR for a write \nto the variable. Similarly, RW[p] represents the set of variables for which it is sound to start a weak \nmonitor using start weak monitor. Although it is sound to insert start * monitor calls at any program \npoint, we try to minimize the number of calls by adding instrumentation in only three places: (1) after \nacquire calls; (2) after unknown function calls; and (3) at the beginning of basic blocks. As long as \nwe insert all possible start * monitor calls at these three types of program points, inserting calls \nanywhere else in the program is redundant: every other program point is dominated either by an earlier \ncall in the same basic block, or by the beginning of the basic block. When inserting calls to stop monitor, \nthe problem changes from a must-analysis which variables must be ac\u00adcessed after this program point to \na may-analysis: which monitors may have started before this program point? For each program point p, \nwe need A[p], the set of variables for which there exists a path from an access to the variable to p, \nwith no intervening release calls. IFRs always end at release calls, so we insert calls to stop monitor \njust before release calls, as well as before unknown function calls (since the call may perform a release) \nand at the end of each function (to avoid interprocedural reasoning). At each of these loca\u00adtions, if \nwe insert a call to stop monitor for every variable in A[p], we will have satis.ed the correctness conditions \nin Section 3.2. However, inserting calls for every variable in A[p] is too conservative. We must take \ncare not to stop monitors too early. For example, we should stop the monitor for x in Figure 5(a) at \nthe end of the IFR for the second access to x, not the .rst. Therefore, at release calls, we insert stop \nmonitor calls only for variables in A[p] - RW[p] (i.e., variables for which this program point does not \nfall in an IFR). For variables in A[p] nW[p] (i.e., variables for which this program point falls in a \nwrite IFR), we do not need to  (a) Stopping a monitor too early. (b) Downgrading a monitor from strong \nto weak. Figure 5. Stopping and downgrading monitors. insert any instrumentation. For variables in A[p]n(RW[p]- \nW[p]) (i.e., variables for which this program point falls in an IFR, but not necessarily a write IFR), \ninstead of stopping the monitor, we downgrade it from strong to weak. This requires a fourth instrumentation \nfunction: downgrade_monitor(void *x); For example, in Figure 5(b) the strong monitor induced by the write \nto x is downgraded at the end of the critical section for m1. In summary, we can insert instrumentation \ncalls as fol\u00adlows to meet the correctness criteria of Section 3.2: 1. At acquire calls, unknown function \ncalls, and the be\u00adginning of each basic block, we insert a call to start strong monitor for each variable \nx in W[p]. 2. At acquire calls, unknown function calls, and the begin\u00adning of each basic block, we insert \na call to start weak monitor for each variable x in RW[p] -W[p]. 3. At release calls, we insert a call \nto stop monitor for variables in A[p] - RW[p]. 4. At release calls, we insert a call to downgrade monitor \nfor variables in A[p] n (RW[p] -W[p]). 5. At unknown function calls and the end of the function, we \ninsert calls to stop monitor for all variables in A[p].   3.4 Re.ned Algorithm In our actual implementation, \ninstead of starting each mon\u00aditor separately, we merge the start strong monitor and start weak monitor \ncalls for different variables into a sin\u00adgle call with a varargs argument: start_monitors(int num_weak, \nint num_strong, ...); The .rst num weak arguments to the call after the two integers are the weak monitors \nto start (i.e., RW[p]-W[p]), and the next num strong arguments are the strong monitors to start (W[p]). \nOther than this change, which is useful because it allows the run-time to start several monitors at once, \nthe algorithm for starting monitors is basically as presented in Section 3.3. One difference is that \nwe have a second helper analysis to identify redundant start moni\u00adtors calls; many calls are not necessary \nbecause they are dominated by a previous call to start monitors.  In contrast, our approach to stopping \nmonitors differs sig\u00adni.cantly from Section 3.3. Instead of stopping each indi\u00advidual monitor separately, \nwe default to stopping all active monitors, except for a set of monitors which are permitted to continue \nthrough the call. stop_all_monitors_except(int num_weak, int num_strong, ...); The num weak arguments \ncorrespond to calls to downgrade monitor, since only weak monitors for these variables are permitted \nto continue through the call. As with start mon\u00aditors, the sets of variables for which we do not stop \nstrong and weak monitors are W[p] and RW[p] -W[p], respec\u00adtively. In other words, we stop all monitors \nexcept those whose variables have active IFRs at that program point. This inverted interface is an improvement \nover the sim\u00adpli.ed algorithm because now we do not need to add in\u00adstrumentation before unknown function \ncalls or at the end of a function. As long as we instrument every release call in the program, every \nmonitor will be stopped at the .rst release call it encounters dynamically, unless the call is stat\u00adically \nknown to fall into an IFR for that variable. This means our detector has the surprising and useful quality \nthat even though our compiler analysis is strictly intraprocedural, a dy\u00adnamic monitor can start in one \nfunction and end in another. The other function might be the function s caller, a callee of the function, \nor even another function called long after the function returns. This introduces a small soundness issue, \nsince release calls in uninstrumented libraries, or indirect calls to primi\u00adtive release functions, will \nnot stop monitors. However, we have found that programs typically do not rely on synchro\u00adnization in \nlibrary code to protect shared data in the main program, so missing these release calls is a relatively \nminor problem. This problem is not fundamental to our algorithm; it would be possible to dynamically \nintercept release calls. In practice, we encountered only one case in our benchmarks where a program \nused function pointers for synchronization calls.  3.5 Data-Flow Analysis Our compiler analysis is an \nintraprocedural backwards data\u00ad.ow analysis. Working from the end of each function to the beginning, \nwe identify variables that must be accessed on every path from a given program point to the next acquire \ncall: W and RW. (The re.ned analysis does not use A.) The initial values (at the end of the function) \nare W[pend]= RW[pend]= {}. The sets propagate through statements as Statement Statement type form W[p] \nRW[p] Load p : r = *x; W[p .] RW[p .] .{x} Store p : *x = r; W[p .] .{x} RW[p .] .{x} Acquire p : lock(m); \n{} {} Release p : unlock(m); W[p .] RW[p .] Call p : f(...); {} {} Other p : ... ; W[p RW[p Figure 6. \nSummary of our backwards data-.ow analysis to insert instrumentation calls. p. is the program point after \nthe statement at point p. shown in Figure 6. At load and store operations we update the W and RW sets. \nThe sets get killed at acquire calls and unknown function calls. At control-.ow merge points, we take \nthe intersection of the incoming sets. We implemented this analysis in the LLVM compiler framework [14]. \n 3.6 Short-Scope Monitors In the previous section, we discussed inserting calls to start monitors at \nthe beginning of basic blocks or af\u00adter unknown function calls. However, in some cases, instru\u00admenting \nat these locations is not possible, because one or more variables for which a monitor is being started \nare not in scope. For example, consider the following loop: int array[10]; // global variable ... int \ni = 0; int *x; do { x = &#38;array[i]; *x = i; i++; } while (i < 10); Without analyzing the compiled \nversion of this program, we can infer that there will be at least 10 different IFRs per execution: one \nfor each value of x. Therefore that we cannot simply insert a single start monitors call for x before \nthe loop. When translated to SSA form, x s de.nition is inside the loop: int *array; // global variable \nentry: array = ...; goto loop; loop: int i_1 = PHI [0, entry] [i_2, loop]; int *x = array + i_1; store \ni_1 into x; inti_2 = i_1+1;  if (i_2 < 10) goto loop else goto done; done: return; Our analysis will \ndiscover that the monitor for x should start at the beginning of the entry block; however, x is not in \nscope at the beginning of the entry block. Practically, the earliest we can start the monitor for x is \nafter x is initialized: ... int *x = array + i_2; start_strong_monitor(x); store i_2 into x; ... Placing \nthe call within the body of the loop has the effect of starting an IFR for each element in the array, \nwhich is what we expected from examining the source code. Our instrumentation pass therefore works as \nfollows: for every monitor start whose variable is not in scope, we insert a special call (either to \nstart weak monitor or to start strong monitor) that starts a single monitor right after the variable \ns de.nition. We call such monitors short-scope monitors, because the scope of the variable being monitored \nlimits the duration of the monitor. We have found that there tend to be many short-scope monitor starts \nin program exe\u00adcutions, since typically such calls cover exactly one memory load or store. Since handling \nall of these calls can be very expensive, our dynamic analysis can start monitors for only a subset of \nthese calls in order to recover performance; this will be discussed in more detail in Section 4.4. 4. \nDynamic Analysis There are two parts to IFRit s dynamic analysis. First, IFRit tracks which threads have \nactive monitors for which memory locations. Second, IFRit detects races by identifying con\u00ad.icting monitors \nfor the same location in different threads. 4.1 Dynamic Monitors The static analysis informs the dynamic \nanalysis of program points where monitors should start and stop. At run-time, IFRit maintains a data \nstructure called the Active Monitors Table (AMT). The AMT maps each memory location to a set of monitor \nrecords for that location. There is one monitor record for each thread executing a monitor for a particular \nmemory location. A monitor record stores the program counter where the monitor began, the thread ID of \nthe thread executing the monitor, and whether the monitor is weak or strong. Each thread also maintains \ntwo thread\u00adlocal sets of memory locations representing active weak and strong monitors. Following the \nkey insight of the FastTrack algorithm [10], the AMT holds at most one strong monitor per memory location \nat a time. In the data-race-free case, there is no need to store more than one monitor, since writes \nto a memory location are totally ordered. If more than one thread starts a strong monitor for a given \nlocation concurrently, the tool will report a data race. This optimization might result in fewer data-race \ndetections, but only for executions where at least one data race is reported. When a thread reaches a \ncall to start monitors, it looks up each argument in the AMT, adds a monitor record to the table s entry \nfor each argument (unless one is already active), and updates its local sets. When a thread encounters \na call to stop all monitors except, it iterates through its local sets, removing monitor records from \nthe AMT for all memory locations in the local sets except those listed as arguments. 4.2 Detecting Data \nRaces IFRit detects data races using the information stored in the AMT. When a thread reaches a call \nto start monitors, it performs a race check on every memory location passed to start monitors (except \nthose for which monitors are already active) before updating the AMT. To perform the race check, the \nthread looks at the set of monitor records associated with each memory location. If the thread performing \nthe race check is starting a strong monitor, and another thread already has an active monitor (weak or \nstrong) for the location, IFRit concludes there is a data race. If the thread performing the check is \nstarting a weak monitor, IFRit concludes there is a data race only if another thread has an active strong \nmonitor for that location. When a thread detects a data race, it reports its current program counter, \nand the program counter stored in the monitor record that the thread found in the AMT.  4.3 Implementation \nWe implemented IFRit s dynamic analysis from scratch in a run-time library. The library s API exposes \nthe start mon\u00aditors, start strong monitor, start weak monitor and stop all monitors except functions. \nThe runtime implements the AMT as two arrays of 2n hash tables, where n is a small positive integer that \nis, 2n pairs of hash tables, where each pair includes one hash table for strong moni\u00adtor records and \none hash table for weak monitor records.8 Monitor records are assigned to the appropriate hash table \nin the array by masking off n bits in the monitor s associated memory location. We found that partitioning \nthe AMT in this way was extremely valuable for regaining parallelism, as compared to earlier designs \nin our development process that used just two hash tables for all monitor records. Each pair of hash \ntables in the AMT is synchronized using a mutex lock. In addition to preventing the hash tables from \nbeing corrupted by concurrent accesses, this simple synchronization scheme also has the effect of serializing \nmonitor starts for each location. The threads sets of monitors are implemented as two thread-local hash \ntables, one for active strong monitors and 8 The results presented in Section 6 use n =5.  one for active \nweak monitors. Because this information is stored locally, many calls to the runtime do not need to do \nany synchronization they simply check to see whether the monitor is already active (in the case of start \nmonitors or its variants) or whether there are any active monitors that need to be stopped (in the case \nof stop all monitors ex\u00adcept).  4.4 Performance Considerations IFRit has a strong correctness guarantee: \neven if not all monitors are started, we will report no false positives, as long as monitors are stopped \nat the appropriate time (or earlier). Therefore, we can ignore some calls to start monitors without compromising \nsoundness. We leverage this in two ways: limiting short-scope monitors, and sampling. First, as discussed \nin Section 3.6, so-called short-scope monitors are numerous enough to be a burden on the run\u00adtime. A \ncommon case is that a thread will be iterating through a large array, which requires starting a new mon\u00aditor \non every iteration. The idea of our static instrumenta\u00adtion is to use a few calls to represent many accesses, \nso these small-scope calls are problematic. Therefore we have an optional mode for our detector that \nstarts only a subset of these monitors. Speci.cally, we allow a maximum of k dynamic monitors per static \ncall site. This optimization is intended to exploit the observation that if one iteration of the loop \nis racy, it is likely that the rest will be racy as well. We have found that this optimization provides \nconsiderably better performance while catching almost as many races as the fully-monitored mode. We did \n.nd one race which was missed by this optimization: a loop in one of the PARSEC benchmarks (streamcluster) \nwas not racy for its .rst 512 iterations, but was racy for the rest. Second, we implemented sampling. \nOur runtime executes a sampling period for a window of execution every second. During a sampling period, \nthe runtime executes all calls to start monitors and its variants. For instance, with a sam\u00adpling rate \nof 1%, IFRit monitors the execution for one one\u00adhundredth of a second every second. When the period ends, \nwe ignore calls to start monitors and its variants. We chose this sampling technique because we suspect \nmonitor\u00ading many memory locations simultaneously .nds more bugs than sparsely sampling monitors at all \ntimes. Sampling is ef\u00adfective: at a sampling rate of 50%, overheads went down by an order of magnitude. \nWe also implemented an optimization for programs that have long single-threaded phases: if there is only \none thread running, we ignore calls to start monitors. This affects neither soundness nor completeness: \nonce the thread .nishes its work, it must call pthread create to start a new thread. pthread create is \na release operation. Therefore any mon\u00aditors collected during the single-threaded phase would be Figure \n7. Even though neither access happens during the other access s IFR, we can detect the race in this case \nbe\u00adcause the accesses IFRs overlap. stopped before the pthread create call anyway, so there is no point \nto starting these monitors.9 5. Formalism and Correctness This section proves that the central idea of \nour detector is correct: if two interference-free regions for con.icting, concurrent accesses overlap, \nthen the accesses must form a data race. The proof is based on a proof in prior work, which showed that \nother threads could not write to a variable during an IFR for that variable without inducing a data race \n[7]. The property we prove here is stronger, because the racing access may not happen during the other \naccess s IFR (see Figure 7 for an example). We use similar notation to prior work to improve clarity. \nTo simplify the presentation, we use a version of the C++11/C11 memory model that has been abstracted \nand simpli.ed in unessential ways [11, 12]. An execution of a program is a quadruple (A, =sb,<sw, =hb). \nA is a set of actions, where each action a is a triple of a thread ID t, a kind of action k, and a unique \nID u: a =(t, k, u). We do not specify which kinds of actions are used in the execution, but we assume \nthere are reads and writes of variables, as well as some form of synchronization. The sequenced-before \nrelation =sb totally orders unique IDs with the same thread ID. The synchronizes-with relation <sw is \na strict partial order over unique IDs, which orders synchro\u00adnization actions: u1 <sw u2 implies that \nu1 is the UID for a release action, and u2 is the UID for an acquire action. The happens-before relation \n=hb is the re.exive transitive closure of the union of =sb and <sw: =hb=(=sb . <sw)* . Our goal is to \nprove that two overlapping IFRs for con\u00ad.icting, concurrent accesses to the same variable always im\u00ad \n9 The pthread create call might allow some monitors to continue through it, but we do not think this \nis a concern. Ignoring these monitors does not affect soundness, and it would be very easy to special-case \ncalls to stop all monitors except so that the monitors would be started before the pthread create call. \n ply that the accesses form a data race. This is stated in the following theorem: Theorem 1. Consider \ntwo IFRs I1 and I2 for actions (t1, k1,u1) and (t2,k2,u2). Assume that t1 .t2 and that k1 = and k2 are \neither read(x) or write(x), and at least one is a write. Then if I1 and I2 overlap, (t1,k1,u1) and (t2,k2,u2) \nform a data race. First, we de.ne interference-free regions and data races with respect to our formal \nmodel. beginaccess De.nition 1 (IFR). An IFR is a triple I =(u,u, uend) where the following conditions \nhold: access) . 1. There exist t, kaccess, and x such that (t, kaccess,u= read(x) or kaccess A and either \nkaccess = write(x). begin access end 2. u<sb u<sb u. begin access 3. For all u such that u<sb u =sb u, \nu s associ\u00adated kind is not an acquire synchronization. access end 4. For all u such that u=sb u =sb \nu, u s associ\u00adated kind is not a release synchronization. De.nition 2 (Data race). Two actions (t1,k1,u1) \nand (t2, k2,u2) . A form a data race if: 1. t1 . = t2; 2. k1 and k2 are either reads or writes of the \nsame variable, and at least one is a write; 3. and the two actions are not ordered by happens-before: \nu1 .=hb u2 and u2 .=hb u1.  Suppose we have two IFRs I1 and I2 in a given execution. I1 and I2 do not \noverlap if either I1 ends before I2 begins, or I2 ends before I1 begins. Therefore, we say that two IFRs \noverlap if neither of these conditions holds: begin De.nition 3 (Overlapping IFRs). Two IFRs I1 =(u, \n1 access end begin access end u,u) and I2 =(u,u,u) overlap if 11 222 end begin end begin u.=hb uand \nu.=hb u. 12 21 In order to prove our main theorem, we .rst prove a sup\u00adporting lemma about the structure \nof happens-before edges. Effectively, we need to show that in order for there to be a happens-before \nedge between two actions in different threads, there must be a release synchronization action in the \n.rst thread that is sequenced after the .rst action, and an acquire synchronization action in the second \nthread that is sequenced before the second action. Lemma 1. Let (t1,k1,u1), (t2,k2,u2) . A such that \nt1 . = t2 and u1 =hb u2. Then there exist u3 and u4 such that u1 =sb u3 =hb u4 =sb u2, u3 s associated \nkind is a release synchronization, and u4 s associated action is an acquire synchronization. The proof \nof Lemma 1 is given in Appendix A . Lemma 1 leads directly to the proof of Theorem 1. begin access end \nbegin Proof. Let I1 =(u,u,u) and I2 =(u, 111 2 access end u,u). Assume that the two accesses do not form \na 22 access access access data race; i.e. that either u1 =hb u2 or u2 =hb access u. Proceed by cases: \n1 access access 1. uu. By Lemma 1, this happens-before 1 =hb 2 edge must go through a release action \nin Thread t1, and an acquire action in Thread t2. Formally, there exist u3 access access and u4 such \nthat u1 =sb u3 =hb u4 =sb u, u3 is a release synchronization, and u4 is an acquire synchronization. By \nDe.nition 1, it must be the case that the release and acquire actions do not fall in I1 and end begin \nI2, respectively: u1 =sb u3 and u4 =sb u2 . By end transitivity of happens-before, we have that u=hb \n1 begin u, contradicting our assumption that the two IFRs 2 overlap. access access 2. u=hb u. This case \nis symmetric to the .rst. 21 We have therefore proved that our algorithm produces no false positives. \n6. Evaluation There are four main goals to our evaluation of IFRit: (1) We highlight IFRit s low runtime \noverheads and character\u00adize the impact of sampling on IFRit s overheads; (2) We demonstrate that IFRit \neffectively detects data races in sev\u00aderal mature applications and assess the impact of sampling on IFRit \ns race-detection capability; (3) We qualitatively an\u00adalyze the output of IFRit by examining several discovered \nraces; and (4) Throughout our evaluation, we provide a head\u00adto-head comparison with ThreadSanitizer, \na state-of-practice happens-before data-race detection tool with widespread commercial adoption [25] \nand FastTrack, a state-of-the-art happens-before data-race detection tool [10, 21]. This section focuses \non comparison with precise data\u00adrace detectors; in Section 7 we discus other imprecise ap\u00adproaches, such \nas detectors that implement sophisticated sampling techniques [6, 18] or detectors that use hardware \nwatchpoints [9]. 6.1 Experimental Setup To benchmark IFRit, we used the PARSEC-2.1 benchmark suite [3] \nand a set of real applications. PARSEC is comprised of a set of programs representative of emerging concurrent \napplications, such as data mining, vision, and video encod\u00ading. We ran the PARSEC benchmarks with their \n8 threaded pthreads con.guration on the simsmall input set. We ex\u00adcluded three of the 13 benchmarks: \none, freqmine, used OpenMP for synchronization, and our runtime currently su\u00adports only pthreads; a second, \nvips, used GLib for synchro\u00adnization, which our runtime uses for hash tables and there\u00adfore cannot be \ninstrumented; a third, facesim, crashed during our tests due to memory requirements. To evaluate IFRit \nfurther, we used unmodi.ed versions of MySQL, Apache, and PBZip2. MySQL is an industrial\u00adstrength database \nserver. We used MySQL-5.5.15, running Figure 8. Overhead of IFRit compared to uninstrumented code for \nthe PARSEC benchmarks and a suite of real applications. Average and geometric mean are over the .rst \neight PARSEC benchmarks.  with its default con.guration. To benchmark MySQL, we used the sysbench OLTP \nbenchmark running under its de\u00adfault con.guration. Apache is a webserver. We used ver\u00adsion httpd-2.0.48 \nwith its worker thread con.guration. We ran tests using ApacheBench, issuing 10000 requests from 8 request \nthreads. PBZip2 is a parallel .le compres\u00adsion/decompression tool. We used PBZip2-0.9.1, running with \n8 threads. To benchmark PBZip2, we decompressed a 150MB text .le. We compiled applications using LLVM \nand our instru\u00admenting compiler pass. For our baseline, we compiled ap\u00adplications using LLVM, but without \nour instrumenting pass. The PARSEC benchmarks were run on a dual 4-core 2.27 GHx Intel Xeon E5562 with \n10GB of RAM. The real appli\u00adcations were run on a 4-core 2.8 GHz Intel Xeon E5462 with 16GB of RAM. In \nour evaluation we directly compared IFRit to Thread\u00adSanitizer s Valgrind implementation [25] and an implemen\u00adtation \nof FastTrack for C/C++ using DynamoRio [21]. We ran experiments with ThreadSanitizer on our machines. \nThe authors of [21] provided us with data from their exper\u00adiments with their FastTrack implementation; \nthey used a quad-socket, 8-core 2.0 GHz Intel Xeon X7550 for their ex\u00adperiments.  6.2 Overheads PARSEC \nFigure 8 shows the overheads imposed by IFRit on the PARSEC benchmarks compared to FastTrack and ThreadSanitizer. \nFastTrack data was available for only the .rst eight PARSEC benchmarks, so we have listed the av\u00aderage \nand geometric mean for those programs only. The ge\u00adometric mean de-emphasizes the effect of outliers. \nWe ran each PARSEC program three times for each sampling rate and used the mean of the three execution \ntimes. In addition to the fully instrumented IFRit data, we show the overheads for a variant of IFRit \nwhere short-scope monitors are limited to a maximum of ten monitors per static call site at a time. IFRit \ns overheads are low. On all but three PARSEC benchmarks, the fully instrumented version of IFRit out\u00adperforms \nThreadSanitizer. For four of the eight benchmarks for which we have FastTrack numbers, IFRit outperforms \nFastTrack. The geometric mean of IFRIT s slowdown across the entire PARSEC suite is 46.3x, compared to \n147.4x for ThreadSanitizer and 57.3x for FastTrack. If we enable the Figure 9. Effect of sampling on \nIFRit s performance overhead. Average and geometric mean are over the ten PARSEC benchmarks.  short-scope \noptimization, which limits the number of moni\u00adtors per static call site, IFRit performs better than both \nFast-Track and ThreadSanitizer on every PARSEC benchmark, with an overall geometric mean of 12.2x. Several \nof the benchmarks (blackscholes, raytrace, x264, canneal) have especially low overheads. The main reason \nfor these low overheads is that the structure of the parallelism in these programs amortizes the analysis \ncost imposed when monitors start and end. Blackscholes has fork-join structure, so it does little sharing \nand few monitor starts and ends, relative to the amount of computation being performed by the program. \nBodytrack, swaptions, .uidanimate, streamcluster, dedup and ferret saw higher overheads. In these cases, \nmore fre\u00adquent starting and stopping of monitors leads to a larger frac\u00adtion of the execution time spent \nrunning IFRit s instrumenta\u00adtion code. These applications also tend to have many short\u00adscope monitors. \nFor example, in streamcluster threads re\u00adpeatedly iterate over arrays of points, coordinated via barrier \nsynchronization calls. This results in many short-scope mon\u00aditors, since most memory accesses happen \nwithin tight loops, as well as many calls to stop all monitors except, since there are so many calls \nto pthread_barrier_wait. Note that streamcluster s performance greatly improves when we cap the number \nof short-scope monitors. Overall, these data show that IFRit s overheads are com\u00adparable to prior race \ndetection tools [10, 21, 25]. In most of our benchmarks, monitors are started and stopped in\u00adfrequently \nenough that the cost of our instrumentation is amortized by program execution. In these cases, IFRit \ns low overhead results from not having to instrument every mem\u00adory access. For benchmarks with a large \nnumber of short\u00adscope monitors, selectively omitting some monitors on a per-call-site basis is extremely \neffective in recovering perfor\u00admance without sacri.cing much coverage (we discuss cov\u00aderage more in Section \n6.3). Real applications Figure 8 also shows overheads for the real applications compared to uninstrumented \nexecution. For these applications, we ran the benchmarking code only once per con.guration.  IFRit s \noverhead running on real applications is similar to the overheads we saw for PARSEC. Our best case is \nPBZip, with overheads around 4x. Like dedup and blackscholes, PBZip has low overhead because of its parallelism \nstrategy. In PBZip workers share data with a main thread through a worklist, but synchronize infrequently. \nThe infrequent start\u00ading and stopping of monitors that results leads to PBZip s low overhead. Our experiments \nwith PBZip show how avoid\u00ading per-memory-operation overhead saves performance. In FastTrack and ThreadSanitizer, \neach data access is instru\u00admented to check or update a vector clock. In contrast, IFRit only needs to \nupdate its state at the beginning of large re\u00adgions during which many data accesses are performed. IFRit \ns worst case full application is MySQL, which incurs a 66X overhead. While higher than the overheads \nin Apache and PBZip, IFRit s overhead is far lower than ThreadSanitizer s overhead of around 160X. When \nwe apply our short-scope monitor optimization MySQL s overhead is reduced to 59X. The difference indi\u00adcates \nthat short-scope monitors contribute to MySQL s over\u00adhead. Both PBZip and Apache saw little bene.t from \nthe short-scope monitor optimization, suggesting their perfor\u00admance is not limited by starting and stopping \nshort-scope monitors. 6.2.1 Impact of Sampling on Performance Figure 9 gives the overheads for IFRit \nwith sampling en\u00adabled for 1%, 10% and 50% of the execution time. We give the average and geometric mean \nfor all 10 PARSEC bench\u00admarks. Sampling is very effective at reducing IFRit s over\u00adheads for PARSEC, \nwith a geometric mean of 4.2x, 2.6x, and 2.0x slowdown for 50%, 10% and 1% sampling, respec\u00adtively. Sampling \nalso helps a great deal for the some of the real applications. MySQL runs much faster under sampling \n(15-30 times faster), but under sampling, no data races are detected (see Table 1). Apache, on the other \nhand, runs with nearly no overhead under sampling, and still detects many data races half of the races \nreported without sampling are reported with a 50% sampling rate, and 30% of the races reported without \nsampling are still reported with a 1% sam\u00adpling rate. PBZip also enjoys nearly no overhead with 50% sampling \nand still detects all the races reported by IFRit without sampling.  6.3 Race-Detection Coverage Table \n1 lists the number of unique races reported by our tool for the benchmarks. We found races in all three \nreal applica\u00adtions and in four of the 13 PARSEC benchmarks. To assess the coverage of IFRit, we directly \ncompare to the coverage of ThreadSanitizer. We discuss the races found by IFRit and ThreadSanitizer in \nSection 6.4. The data show that in each of the PARSEC programs that had any races reported, Thread-Sanitizer \ndetects some races that IFRit did not detect. The programs with the biggest difference in coverage are \nferret and x264. In x264, ThreadSanitizer found 72 races while IFRit Races 1% 10% 50% 10/PC Full TS bodytrack \n111 55 10 x264 2 33 72 streamcluster 112 23 24 ferret 38 Apache 6810 1919 21 PBZip 2 22 2 MySQL 1111 \n14 Table 1. Number of unique races found by IFRit in various con.gurations. TS shows races reported by \nThreadSanitizer. Omitted benchmarks had no detected races. IFRit found only 3. In ferret, IFRit missed \nall of the races ThreadSanitizer reported. As we shall discuss in Section 6.4, many of these races are \nrelated to memory accesses in code not instrumented by IFRit. Note that missing these races is a limitation \nof our prototype, not a fundamental limitation of our IFR-based approach. In contrast, in the real application \nbenchmarks we used, IFRit s coverage is nearly identical to ThreadSanitizer s cov\u00aderage. IFRit and ThreadSanitizer \ndetect the same races as PBZip. IFRit misses two races in Apache, and three races in MySQL. 6.3.1 Impact \nof Short-Scope Monitor Optimizaton on Coverage When we limit the number of short-scope monitors per code \npoint, IFRit s coverage is identical in all cases except stream\u00adcluster. Streamcluster executes a loop \nthat starts short-scope monitors. The memory accesses in the .rst 512 iterations of the loop are not \nracy, but the remaining accesses are racy. The accesses occur at the same code point, so we miss these \nraces with this optimization enabled. Looking back to Figure 8, the data show that the reduction in overhead \nresulting from this optimization is very large. The data in Table 1 show that the degradation of coverage \nis almost negligible. Together these results demonstrate that limiting the number of short-scope monitor \ns per code point is bene.cial.  6.3.2 Impact of Sampling on Coverage Sampling reduces IFRit s coverage, \nbut even with sampling IFRit detects many data-races. Sampling 50% of the execu\u00adtion, IFRit detects some \nraces in all programs in which it de\u00adtected races without sampling, except MySQL. Using even sparser \nsampling further reduces IFRit s coverage. However, even with a sample rate of 1% IFRit still detects \nraces in streamcluster, bodytrack, and Apache. The data in Figure 9 show that sampling reduces over\u00adheads \nconsiderably the geometric mean overhead at 1% sampling rate is about 2X, and only slightly higher at \n10% sampling rate. The data in Table 1 show that IFRit is still useful for .nding data-races when sampling \nis active. To\u00adgether, these results show that sampling is one way to trade off precision for increased \nperformance.   6.4 Analysis of Detected Races In order to track down these reported data races, we \ncom\u00adpiled and ran a second version of each racy benchmark with debugging information and less aggressive \noptimization. Our tool prints out the program counter for the start mon\u00aditors call for each side of the \ndata race, as well as a stack trace for the call that triggered the report. The static analysis also \nprints a list of instrumentation calls and their associated accesses. 6.4.1 Races in PARSEC Most of the \nPARSEC benchmarks had no races reported by either IFRit or ThreadSanitizer. (We did not have access to \nthe DynamoRIO FastTrack race reports, but the paper men\u00adtions a race in canneal which neither IFRit nor \nThreadSan\u00aditizer reported.) Both tools found races in bodytrack, x264, and streamcluster. ThreadSanitizer \nalso found races in ferret which were not detected by IFRit. Bodytrack IFRit found .ve data races in \nbodytrack, four of which were caused by the same bug involving the misuse of condition variables. The \nlast race was caused by threads reading a structure that had not been fully initialized. ThreadSanitizer \nreported 10 unique races, including the two problems identi.ed by IFRit. ThreadSanitizer also found a \nrace involving an unprotected counter which was not reported in IFRit. However, that race did show up \nin IFRit during runs run with a larger input (simmedium), and running IFRit on the simmedium input was \nfaster than run\u00adning ThreadSanitizer on the simsmall input. Fixing these three root causes resolved all \nof the race reports from both ThreadSanitizer and IFRit. X264 IFRit reported three data races in x264, \none of which was con.rmed by ThreadSanitizer. ThreadSanitizer reported 72 races, most of them within \nmemcpy in libc, which was not instrumented by IFRit s static analysis and therefore was not monitored \nfor races. Streamcluster IFRit reported three data races in stream\u00adcluster. Two of the races were on \nlocal variables declared static. static local variables are scoped to their func\u00adtion or block, but correspond \nto a single global object, so threads executing the function simultaneously can race on the variable. \nThe third race was caused by a missing barrier call. It appears that the pthreads code was improperly \ntrans\u00adlated from code that used Intel s TBB (Threading Building Blocks) Library.10 ThreadSanitizer reported \n23 unique races in streamclus\u00adter, including the three reported by IFRit. We determined that the remaining \nraces reported by ThreadSanitizer boiled down to two root causes. First, a function passed its argu\u00adments \nby value rather than by reference; since pass-by-value arguments are not listed as loads in the LLVM \nIR, IFRit did 10 http://threadingbuildingblocks.org/ not instrument those memory accesses. The second \nrace was on a pointer being freed, which ThreadSanitizer counts as a write and IFRit does not. Ferret \nThreadSanitizer found 43 races in ferret that were not reported by IFRit. Two races, one on a shared \ncounter and a second on a shared boolean .ag, were not detected by IFRit because the racy monitors in \nIFRit were of very short duration, and never happened to overlap. The remaining races were in libc, which \nwas not instrumented by IFRit s static analysis and therefore was not monitored. 6.4.2 Races in Real \nApplications MySQL IFRit reported 11 races in MySQL. Three races in MySQL were the result of unsynchronized \naccesses to termination .ags written in the main program thread and read in a signal handling thread \nduring server shutdown. Two reported races involved lock meta-data in MySQL s wrapper for pthread locks. \nThe remaining races are on unsynchronized .ags and a linked list implementation in debugging code. These \nraces are unsurprising. Debugging code is often disabled in pro\u00adduction, so it may be less thoroughly \ntested than other code. IFRit and ThreadSanitizer had comparable coverage for MySQL. ThreadSanitizer \nreported 14 races in MySQL, in\u00adcluding 9 of the 11 races that IFRit reported. ThreadSani\u00adtizer did not \nreport two races IFRit reported and IFRit did not report four races that ThreadSanitizer reported. Apache \nIFRit reported 19 different races in Apache. Seven were caused by a well-known bug in Apache s logging \ncode that can lead to garbled log output [15, 17, 27]. Five more were caused by races that nearby comments \nindicated were known or intentional. Intentional or not, these races should be reported because even \nbenign races can result in in\u00adcorrect behavior [4]. The other races were all on improperly synchronized \n.ags. IFRit has nearly the same race detection coverage as ThreadSanitizer. IFRit detected all the races \nreported by ThreadSanitizer except for two. ThreadSanitizer did not re\u00adport one of the two .ag races \nthat IFRit detected. PBZip IFRit reported two races in PBZip. One of the races involves unsynchronized \naccesses to a .ag variable signaling a termination condition to worker threads. The other race involves \nconcurrent accesses to .elds of an output buffer structure. One thread .lls the buffer and writes the \n.elds. Concurrently, the thread that empties the buffer reads the .elds without synchronizing. The races \nreported by IFRit were the same races reported by ThreadSanitizer.  6.5 Discussion: IFRit vs. Other \nDetectors Throughout this evaluation, we have compared IFRit directly to FastTrack and ThreadSanitizer. \nLike these precise detec\u00adtors, IFRit is sound, so for all three there are no false positive races reported. \nFastTrack and ThreadSanitizer are also com\u00adplete, meaning they detect all races in an execution. IFRit \nis not complete, but the data show that IFRit exploits a critical tradeoff of completeness for performance. \n Figure 8 shows that IFRit s overhead is much lower than FastTrack and ThreadSanitizer. For the PARSEC \nprograms, IFRit consistently outperformed the other techniques with our short-scope monitor optimization \nenabled. Comparing IFRit s overhead on our real application benchmarks to the overhead of ThreadSanitizer, \nIFRit is the clear winner with overheads far less than ThreadSanitizer. IFRit s performance advantage \nis a key distinction from prior techniques. Table 1 shows that IFRit detects most of the races de\u00adtected \nby FastTrack and ThreadSanitizer in the application code of the programs we evaluated. While we provide \nno completeness guarantee, our data show that IFRit is a pow\u00aderful tool for detecting data-races. Together \nour performance and coverage results illustrate that IFRit recovers a large amount of performance by \ntrading off what we empirically found to be a small margin of com\u00adpleteness. We consider this tradeoff \npro.table, as the reduc\u00adtion in overhead makes data-race detection cheap enough for practical frequent \nuse. FastTrack and ThreadSanitizer pay a very high performance cost to provide completeness guaran\u00adtees. \nTheir overhead may be a barrier to their frequent use by developers in practice. Another important distinction \nbetween IFRit and both prior detectors is IFRit s ability to sample program execu\u00adtion. Sampling gives \ndevelopers a knob to turn that scales back the overhead of race detection. FastTrack and Thread-Sanitizer \ndo not provide such a knob. IFRit s overheads with sampling enabled are sometimes less than 2X. Furthermore, \nas we describe in Section 6.3 IFRit still detects many races detected by the precise detectors with sampling \nenabled. IFRit s soundness guarantees, combined with such low over\u00adheads make it practical to integrate \nrace detection with a development framework like continuous testing [23]. Fast\u00adTrack s and ThreadSanitizer \ns overheads are likely to be too high for continuous use. In some cases (e.g., Apache, PBZip, canneal, \nblackscholes) IFRit s overheads are low enough that they would be tolerable for use in deployed systems. \n7. Related Work A variety of tools have been developed to help .nd data races. Static race detection \ntools [1, 8, 20] analyze program code, and attempt to prove the absence of data races in all program \nexecutions. Static techniques are useful in that they can statically prove a program is data-race-free, \nbut they also must be conservative because they lack information that is available only during program \nexecution. We will focus on dynamic techniques. Dynamic race detectors mostly fall into two categories: \nhappens-before detectors [6, 10, 18, 21, 25] and lockset de\u00adtectors [24]. Lockset detectors like Eraser \n[24] track the locks held at each access and report a race if accesses to a lo\u00adcation are not consistently \nprotected by the same lock. These techniques are based on a heuristic that every shared vari\u00adable will \nbe consistently protected by the same lock which may lead to false positives. Although it is possible \nto reduce false positives by introducing more heuristics (e.g., read\u00adonly data), any false positives \nrepresent a waste of the devel\u00adoper s time. This problem with false positives also applies to hybrid \ntechniques such as MultiRace [22], RaceTrack [28], and ThreadSanitizer s hybrid mode [25]. Happens-before \ndetectors work by tracking the order of synchronization actions in order to determine if con.icting accesses \nare or are not ordered by happens-before. Typically, these algorithms use vector clocks [19], a data \nstructure that tracks the relative timing between different threads of execution in a process. Such race \ndetectors report a data race if two accesses to the same shared state occur are not ordered by the happens-before \nrelation. ThreadSanitizer s non-hybrid mode (which we used for comparison to IFRit in Section 6) is a \nstandard happens-before detector that uses valgrind to instrument binaries. The current state-of-the-art \nimplementation of vector clocks, FastTrack [10], achieves an average 8.5x slowdown and is fully precise \n i.e., it produces no false positives and reports at least one race if the execution contained any races. \nFastTrack achieves this relatively low overhead by looking only for shortest races i.e., if access A \nraces with later ac\u00adcesses B and C, only the race with access B will be reported. Practically, this means \nthat the algorithm only has to track the most recent writer for each shared variable. In its current \nform, IFRit performs comparably to FastTrack when either sampling enabled or the short-scope optimization \nare en\u00adabled. This is signi.cant since FastTrack is implemented in a managed language (Java), while IFRit \nruns on unmanaged code (C/C++). As dicussed in Section 6, a recent paper reimplemented FastTrack for \nx86 binaries using the DynamoRio instrumen\u00adtation platform [21]. As expected, FastTrack is still more \nef.cient than a standard happens-before detector (Thread-Sanitizer), but its overheads are much more \nnoticeable than the Java versions: a geometric mean of around 50x for a set of 10 PARSEC benchmarks. \nThe authors of that paper re\u00adduce the overheads by about 50% using Aikido, a custom hypervisor that uses \npage faults to quickly detect con.icts. We compared IFRit to the non-Aikido version of FastTrack, since \nIFRit does not require a custom hypervisor. IFRit, even without any sampling enabled, outperforms FastTrack, \nwith a geometric mean of 36.3x on eight of the ten benchmarks used for FastTrack (FastTrack s geometric \nmean for those eight was 57.4x). In turn, FastTrack detects more data races than IFRit, since FastTrack \nis a fully-precise algorithm. Several other tools have been developed that use sampling to reduce the \noverhead of fully-precise vector-clock detec\u00adtors. Pacer uses FastTrack during sampling periods, and \nalso does a small amount of work during non-sampled periods to ensure proportionality: the number of \nraces detected should scale linearly with the size of the sampling period [6]. Un\u00adlike Pacer, IFRit does \nnot do any work during non-sampled periods (except to check a boolean .ag), so we miss races where only \none of the monitor starts is sampled. However, the relatively smaller number of instrumentation points \nin IFRit means that we can afford to sample for longer peri\u00adods, which mitigates Pacer s concern about \nproportionality. Our overheads at 10% sampling are comparable to Pacer s at 10%, even though we are running \non C/C++ code instead of Java.  LiteRace [18] also uses sampling to improve the perfor\u00admance of vector \nclocks. They use dynamic pro.ling to iden\u00adtify cold functions, which they hypothesize are more likely \nto contain unnoticed data races. This adaptive sampling is a technique we could adapt to IFR-based data \nrace detection. LiteRace achieves low overheads via adaptive sampling and also by using logging to postpone \nrace checks until after ex\u00adecution. Like us, LiteRace runs on unmanaged C/C++ code, although they instrument \nbinaries rather than source code. IFRit has higher overheads than LiteRace, but we perform race checks \nat runtime instead of of.ine. IFRit s overheads with sampling are comparable to those for LiteRace with \nthread-local adaptive sampling. DataCollider [9] is a heuristic detector that tries to catch data races \nin OS kernels red-handed : it freezes one thread before a memory access, and sets a hardware watchpoint \nto trap writes to the memory location in other threads. This is similar to IFRit in that both try to \nidentify accesses that hap\u00adpen at roughly the same time. IFRit differs from DataCol\u00adlider in that we \ndo not require hardware watchpoints, so we can monitor many variables simultaneously. 8. Conclusion &#38; \nFuture Work We have presented IFRit, a new dynamic data-race detection algorithm for arbitrary C and \nC++ programs. IFRit improves on prior work by coalescing the instrumentation for multiple accesses to \nthe same variable, reducing runtime overhead. We require no specialized hardware and detect races with \nno false positives. IFRit is a natural approach to dynamic data-race detec\u00adtion without the overhead \nof tracking a full happens-before relation. Our prototype implementation of this algorithm in\u00addicates \nthat we can detect races in real programs without in\u00adducing too much overhead. A possible future improvement \nwould be to improve our strategy for short-scope monitors. Our current scheme sim\u00adply limits the number \nof short-scope monitors per static call site; a more adaptive strategy (say, starting a monitor with \nprobability inversely proportional to the number of active monitors at that call site) would be more \nthorough. We plan to extend our static analysis to be interprocedu\u00adral. Currently, we treat function \ncalls conservatively, some\u00adtimes starting monitors later than necessary. Interprocedural analysis allows \nus to propagate information through func\u00adtion calls, increasing the lengths of monitored regions, and \n.nding data races. We also plan to implement IFRit for Java programs. Acknowledgments Thanks to the anonymous \nreviewers and to Mark Oskin for their valuable comments on earlier drafts of this paper. References [1] \nM. Abadi, C. Flanagan, and S. N. Freund. Types for safe locking: Static race detection for Java. ACM \nTransactions on Programming Languages and Systems (TOPLAS), 28(2):207 255, March 2006. [2] S. V. Adve \nand H.-J. Boehm. Memory models: A case for rethinking parallel languages and hardware. Communications \nof the ACM (CACM), 53(8):90 101, August 2010. [3] C. Bienia. Benchmarking Modern Multiprocessors. PhD \nthesis, Princeton University, January 2011. [4] H.-J. Boehm. How to miscompile programs with benign data \nraces. In USENIX Workshop on Hot Topics in Parallelism (HotPar), 2011. [5] H.-J. Boehm and S. Adve. Foundations \nof the C++ con\u00adcurrency memory model. In ACM SIGPLAN Conference on Programming Language Design and Implementation \n(PLDI), 2008. [6] M. D. Bond, K. E. Coons, and K. S. McKinley. Pacer: pro\u00adportional detection of data \nraces. In ACM SIGPLAN Confer\u00adence on Programming Language Design and Implementation (PLDI), 2010. [7] \nL. Ef.nger-Dean, H.-J. Boehm, P. Joisha, and D. Chakrabarti. Extended sequential reasoning for data-race-free \nprograms. In ACM SIGPLAN Workshop on Memory Systems Performance and Correctness (MSPC), 2011. [8] D. \nEngler and K. Ashcroft. RacerX: effective, static detection of race conditions and deadlocks. In ACM \nSymposium on Operating Systems Principles (SOSP), 2003. [9] J. Erickson, M. Musuvathi, S. Burckhardt, \nand K. Olynyk. Ef\u00adfective data-race detection for the kernel. In USENIX Sym\u00adposium on Operating Systems \nDesign and Implementation (OSDI), 2010. [10] C. Flanagan and S. N. Freund. FastTrack: ef.cient and precise \ndynamic race detection. In ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), \n2009. [11] ISO JTC1/SC22/WG14. ISO/IEC 9899:2011, Informa\u00adtion technology Programming languages C. \nhttp: //www.iso.org/iso/iso_catalogue/catalogue_ tc/catalogue_detail.htm?csnumber=57853. Draft available \nat http://www.open-std.org/jtc1/sc22/ wg14/www/docs/n1570.pdf. [12] ISO JTC1/SC22/WG21. ISO/IEC 14882:2011, \nInformation technology Programming languages C++. http: //www.iso.org/iso/iso_catalogue/catalogue_ \n tc/catalogue_detail.htm?csnumber=50372. Draft available at http://www.open-std.org/jtc1/sc22/ wg21/docs/papers/2012/n3337.pdf. \n[13] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communications of \nthe ACM (CACM), 21 (7):558 565, July 1978. [14] C. Lattner and V. Adve. LLVM: A compilation framework \nfor lifelong program analysis &#38; transformation. In International Symposium on Code Generation and \nOptimization (CGO), 2004. [15] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from mis\u00adtakes: a comprehensive \nstudy on real world concurrency bug characteristics. In International Conference on Architectural Support \nfor Programming Languages and Operating Systems (ASPLOS), 2008. [16] B. Lucia, L. Ceze, K. Strauss, S. \nQadeer, and H.-J. Boehm. Con.ict exceptions: Simplifying concurrent language seman\u00adtics with precise \nhardware exceptions for data-races. In ACM IEEE International Symposium on Computer Architec\u00adture (ISCA), \n2010. [17] B. Lucia, B. P. Wood, and L. Ceze. Isolating and understand\u00ading concurrency errors using reconstructed \nexecution frag\u00adments. In ACM SIGPLAN Conference on Programming Lan\u00adguage Design and Implementation (PLDI), \n2011. [18] D. Marino, M. Musuvathi, and S. Narayanasamy. LiteR\u00adace: effective sampling for lightweight \ndata-race detection. In ACM SIGPLAN Conference on Programming Language De\u00adsign and Implementation (PLDI), \n2009. [19] F. Mattern. Virtual time and global states of distributed sys\u00adtems. In International Workshop \non Parallel and Distributed Algorithms and Applications (PDAA), 1988. [20] M. Naik, A. Aiken, and J. \nWhaley. Effective static race detec\u00adtion for Java. In ACM SIGPLAN Conference on Programming Language \nDesign and Implementation (PLDI), 2006. [21] M. Olszewski, Q. Zhao, D. Koh, J. Ansel, and S. Ama\u00adrasinghe. \nAikido: accelerating shared data dynamic analy\u00adses. In International Conference on Architectural Support \nfor Programming Languages and Operating Systems (ASPLOS), 2012. [22] E. Pozniansky and A. Schuster. MultiRace: \nEf.cient on\u00adthe-.y data race detection in multithreaded C++ programs. Concurrency and Computation: Practice \nand Experience, 19 (3):327 340, March 2007. [23] D. Saff and M. D. Ernst. Reducing wasted development \ntime via continuous testing. In IEEE International Symposium on Software Reliability Engineering (ISSRE), \n2003. [24] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. An\u00adderson. Eraser: a dynamic data \nrace detector for multithreaded programs. ACM Transactions on Computer Systems (TOCS), 15(4):391 411, \nNovember 1997. [25] K. Serebryany and T. Iskhodzhanov. ThreadSanitizer data race detection in practice. \nIn Workshop on Binary Instrumen\u00adtation and Applications, 2009. [26] J. .c\u00b4ik and D. Aspinall. On validity \nof program transforma\u00adSev.tions in the Java memory model. In European Conference on Object-Oriented Programming \n(ECOOP), 2008. [27] J. Yu and S. Narayanasamy. A case for an interleaving con\u00adstrained shared-memory \nmulti-processor. In ACM IEEE Inter\u00adnational Symposium on Computer Architecture (ISCA), 2009. [28] Y. \nYu, T. Rodeheffer, and W. Chen. RaceTrack: ef.cient detection of data race conditions via adaptive tracking. \nIn ACM Symposium on Operating Systems Principles (SOSP), 2005. A. Proof of Lemma 1 Lemma 1. Let (t1,k1,u1), \n(t2,k2,u2) . A such that t1 . = t2 and u1 =hb u2. Then there exist u3 and u4 such that u1 =sb u3 =hb \nu4 =sb u2, u3 s associated kind is a release synchronization, and u4 s associated action is an acquire \nsynchronization. Proof. Proof by induction on the derivation of u1 =hb u2. As t1 . = t2, u1 .=sb u2. \n If u1 <sw u2. By the de.nition of <sw, u1 is a release synchronization and u2 is an acquire synchronization. \nLet u3 = u1 and u4 = u2. By re.exivity of =sb, we have that u1 =sb u3 =hb u4 =sb u2.  If u1 =hb u5 =hb \nu2, let t5 be the thread ID for u5. Either t5 = t1, t5 ..  = t2, or t5 = t1 and t5 = t2. t5 = t1. Then \nt5 . = t2, so by the inductive hypothesis there exist u6 and u7 such that u5 =sb u6 =hb u7 =sb u4, u6 \nis a release synchronization, and u7 is an acquire synchronization. Let u3 = u6 and u4 = u7. As t5 = \nt1 and u1 =hb u5, it must be that u1 =sb u5, and by transitivity of =sb, u1 =sb u6. Therefore u1 =sb \nu6 =hb u7 =sb u2. t5 = t2. Then t5 =.t1, so by the inductive hypothesis there exist u6 and u7 such that \nu1 =sb u6 =hb u7 =sb u5, u6 is a release synchronization, and u7 is an acquire synchronization. Let u3 \n= u6 and u4 = u7. As t5 = t2 and u5 =hb u2, it must be that u5 =sb u2, and by transitivity of =sb, u7 \n=sb u2. Therefore u1 =sb u6 =hb u7 =sb u2. t5 .t1 and t5 .t2. We apply the inductive hy\u00ad == pothesis \ntwice. First, there exist u6 and u7 such that u1 =sb u6 =hb u7 =sb u5 and u6 is a release syn\u00adchronization. \nSecond, there exist u8 and u9 such that u5 =sb u8 =hb u9 =sb u2 and u9 is an acquire syn\u00adchronization. \nLet u3 = u6 and u4 = u9. By transi\u00adtivity of =hb, we have that u1 =sb u6 =hb u9 =sb u4.    \n\t\t\t", "proc_id": "2384616", "abstract": "<p>We propose a new algorithm for dynamic data-race detection. Our algorithm reports no false positives and runs on arbitrary C and C++ code. Unlike previous algorithms, we do not have to instrument every memory access or track a full happens-before relation. Our data-race detector, which we call IFRit, is based on a run-time abstraction called an interference-free region (IFR). An IFR is an interval of one thread's execution during which any write to a specific variable by a different thread is a data race. We insert instrumentation at compile time to monitor active IFRs at run-time. If the runtime observes overlapping IFRs for conflicting accesses to the same variable in two different threads, it reports a race. The static analysis aggregates information for multiple accesses to the same variable, avoiding the expense of having to instrument every memory access in the program.</p> <p>We directly compare IFRit to FastTrack and ThreadSanitizer, two state-of-the-art fully-precise data-race detectors. We show that IFRit imposes a fraction of the overhead of these detectors. We show that for the PARSEC benchmarks, and several real-world applications, IFRit finds many of the races detected by a fully-precise detector. We also demonstrate that sampling can further reduce IFRit's performance overhead without completely forfeiting precision.</p>", "authors": [{"name": "Laura Effinger-Dean", "author_profile_id": "81372593342", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P3856114", "email_address": "effinger@cs.washington.edu", "orcid_id": ""}, {"name": "Brandon Lucia", "author_profile_id": "81384609259", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P3856115", "email_address": "blucia0a@cs.washington.edu", "orcid_id": ""}, {"name": "Luis Ceze", "author_profile_id": "81100112680", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P3856116", "email_address": "luisceze@cs.washington.edu", "orcid_id": ""}, {"name": "Dan Grossman", "author_profile_id": "81405594870", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P3856117", "email_address": "djg@cs.washington.edu", "orcid_id": ""}, {"name": "Hans-J. Boehm", "author_profile_id": "81423595101", "affiliation": "HP Labs, Palo Alto, CA, USA", "person_id": "P3856118", "email_address": "hans.boehm@hp.com", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384650", "year": "2012", "article_id": "2384650", "conference": "OOPSLA", "title": "IFRit: interference-free regions for dynamic data-race detection", "url": "http://dl.acm.org/citation.cfm?id=2384650"}