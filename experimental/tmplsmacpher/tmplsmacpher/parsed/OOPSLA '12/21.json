{"article_publication_date": "10-19-2012", "fulltext": "\n Elixir: A System for Synthesizing Concurrent Graph Programs * Dimitrios Prountzos Roman Manevich Keshav \nPingali TheUniversityofTexasat Austin,Texas,USA dprountz@cs.utexas.edu,roman@ices.utexas.edu,pingali@cs.utexas.edu \nAbstract Algorithms in new application areas like machine learning and network analysis use irregular \ndata structures such as graphs, trees and sets. Writing ef.cient parallel code in these problem domains \nis very challenging because it requires the programmer to make many choices: a given problem can usually \nbe solved by several algorithms, each algorithm may have many implementations, and the best choice of \nalgorithm and implementation can depend not only on the characteristicsof the parallel platformbut also \non properties of the input data such as the structure of the graph. One solution is to permit the application \nprogrammer to experiment with different algorithms and implementations without writing every variant \nfrom scratch. Auto-tuning to .nd the best variant is a more ambitious solution. These solutions require \na system for automatically producing ef\u00ad.cient parallel implementations from high-level speci.ca\u00adtions. \nElixir, the system described in this paper, is the .rst step towards this ambitious goal. Application \nprogrammers write speci.cations that consist of an operator, which de\u00adscribes the computations to be \nperformed, and a schedule for performing these computations. Elixir uses sophisticated inference techniques \nto produce ef.cient parallel code from such speci.cations. We used Elixir to automatically generate many \nparal\u00adlel implementations for three irregular problems: breadth\u00ad.rst search, single source shortest path, \nand betweenness\u00adcentrality computation. Our experiments show that the best generatedvariants canbe competitive \nwith handwritten code for these problems from other research groups; for some in\u00adputs, theyeven outperform \nthe handwritten versions. * Thisworkwas supportedbyNSF grants 0833162, 1062335,and 1111766 as well as \ngrants from the IBM, Intel and Qualcomm Corporations. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012,Tucson, Arizona, USA. Copyright \nc &#38;#169; 2012ACM 978-1-4503-1561-6/12/10... $10.00 Categories and Subject Descriptors D.1.3[Programming \nTechniques]: Concurrent Programming Parallel Program\u00adming; I.2.2[Arti.cial Intelligence]:Automatic Programming \nProgram synthesis General Terms Algorithms, Languages, Performance,Ver\u00adi.cation Keywords Synthesis, Compiler \nOptimization, Concurrency, Parallelism, Amorphous Data-parallelism, Irregular Pro\u00adgrams, OptimisticParallelization. \n1. Introduction New problem domains such as machine learning and so\u00adcial network analysis aregiving rise \nto applications with ir\u00adregular data structures like graphs, trees, and sets. Writing portable parallel \nprograms for such applications is challeng\u00ading for many reasons. The .rst reasonis that programmers usuallyhaveachoice \nof many algorithms for solving a given problem: even a relatively simple problem likethe single-source \nshortest path (SSSP) problem in a directed graph can be solved using Dijkstra s algorithm[11], the Bellman-Ford \nalgorithm[11], the label-correcting algorithm[22], and delta-stepping[22], among others. These algorithms \nare described in more detail in Sec. 2,but what is important here is to note that which algorithmis best \ndepends on manycomplexfactors: There are complicated trade-offs between parallelism and work-ef.ciency \nin these algorithms; for example, Dijk\u00adstra s algorithmisverywork-ef.cientbutithas relatively little \nparallelism, whereas the Bellman-Ford and label\u00adcorrecting algorithms canexhibitalotof parallelismbut \nmay be less work-ef.cient. Therefore, the best algorith\u00admic choice may depend on the number of cores \nthat are available to solve the problem.  The amount of parallelism in irregular graph algorithms is \nusually dependent also on the structure of the input graph. For example, regardless of which algorithm \nis used, there is little parallelism in the SSSP problem if the graph is a long chain (more generally, \nif the diameter of the graph is large); conversely, for graphs that have a small diameter such as those \nthat arise in social network applications, there may be a lot of parallelism that can be exploited by \nthe Bellman-Ford and label-correcting   algorithms. Therefore, the best algorithmic choice may depend \non the size and structure of the input graph. The best algorithmic choice may depend on the core architecture. \nIf SIMD-style execution is supported ef\u00ad.ciently by the cores as is the case with GPUs, the Bellman-Ford \nalgorithm may be preferable to the label\u00adcorrecting or delta-stepping algorithms. Conversely, for MIMD-styleexecution, \nlabel-correcting or delta-stepping may be preferable. Another reason for the dif.culty of parallel programming \nof irregular applications is that even for a given algorithm, there are usually a large number of implementation \nchoices that mustbe madeby the performance programmer. Eachof the SSSP algorithms listed above has a \nhost of implemen\u00adtations; for example, label corrections in the label correct\u00ading algorithm can be scheduled \nin FIFO, LIFO, and other orders, and the scheduling policy can make a big differ\u00adence in the overall \nperformance of the algorithm, as we showexperimentallyin Sec. 6. Similarly, delta-stepping has a parameter \nthat can be tuned to increase parallelism at the cost of performing extra computation. As in other parallel \nprograms, synchronization can be implemented using spin\u00adlocks, abstract locks, or CAS operations. These \nchoices can affect performance substantially, but even expert program\u00admers cannot always make the right \nchoices. 1.1 Synthesis of Irregular Programs One promising approach for addressing these problems is \nprogram synthesis. Instead of writing programs in a high\u00adlevel language like C++ or Java, the programmer \nwrites a higher level speci.cation of what needs to be computed, leaving it to an automatic system to \nsynthesize ef.cient par\u00adallel code for a particular platform from that speci.cation. This approach has \nbeen used successfully in domains like signal processing where mathematics can be used as a spec\u00adi.cation \nlanguage[29]. However, irregular graph problems do not have mathematical structure that can be exploited \nto generate implementation variants. In this paper, we describe a system called Elixir that synthesizes \nparallel programs for shared-memory multicore processors, starting from irregular algorithm speci.cations \nbased on the operator formulation of algorithms [25]. The operator formulation is a data-centric description \nof algo\u00adrithms in which algorithms are expressed in terms of their action on data structures rather than \nin terms of program\u00adcentric constructs like loops. There are threekeyconcepts: active elements, operator, \nand ordering. Active elements are sites in the graph where there is computationtobe done.Forexample,in \nSSSP algorithms, eachnodehasalabelthatisthelengthofthe shortestknown path from the source to that node; \nif the label of a node is updated, it becomes an active node since its immediate neighbors must be examined \nto see if their labels can be updated as well. The operator is a description of the computation that \nis done at an active element. Applying the operator to an active element creates an activity. In general, \nan operator reads and writes graph elements in some small region containing the active element. These \nelements are said to constitute the neighborhood of this activity. The ordering speci.es constraints \non the processing of active elements. In unordered algorithms, it is semantically correct to process \nactive elements in anyorder, although dif\u00adferent orders may have different work-ef.cency and paral\u00adlelism. \nA parallel implementation may process active ele\u00adments in parallel provided the neighborhoods do not \nover\u00adlap. The non-overlapping criterion can be relaxed by using commutativity conditions, but we do not \nconsider these in this paper. The pre.ow-push algorithm for max.ow com\u00adputation, Boruvka s minimal spanning \ntree algorithm, and Delaunay mesh re.nement are examples of unordered algo\u00adrithms. In ordered algorithms \non the other hand, there may be application-speci.c constraints on the orderin which ac\u00adtive elements \nare processed. Discrete-event simulation is an example: any node with an incoming message is an active \nelement, and messages must be processed in time order. The speci.cation language described in this paper \nper\u00admits application programmers to specify (i) the operator,and (ii) the schedule for processing active \nelements; Elixir takes care of the rest of the process of generating parallel imple\u00admentations. Elixir \naddresses the following major challenges. How do we design a language that permits operators and scheduling \npoliciestobe de.ned conciselybyapplication programmers?  The execution of an activity can create new \nactive ele\u00adments in general. How can newly created active elements be discovered incrementally without \nhaving to re-scan the entire graph?  How should synchronization be introduced to make ac\u00adtivities atomic? \n Currently, there are two main restrictions in the speci\u00ad.cation language. First, Elixir supports only \noperators for which neighborhoods contain a .xed number of nodes and edges. Second, Elixir does not support \nmutations on the graph structure, so algorithms such as Delaunay mesh re\u00ad.nement cannot currentlybeexpressedin \nElixir.We believe Elixir can be extended to handle such algorithms, but we leave this for future work. \nThe rest of this paper is organized as follows. Sec. 2 presents the key ideas and challenges, using a \nnumber of algorithms for the SSSP problem. Sec. 3 formally presents the Elixir graph programming language \nand its semantics. Sec.4 describes our synthesis techniques. Sec. 5 describes our auto-tuning procedure \nfor automatically exploring im\u00adplementations. Sec. 6describes our empiricalevaluationof Elixir. Sec. \n7 discusses related work and Sec. 8 concludes the paper. 2. Overview In this section, we present the \nmain ideas behind Elixir, us\u00ading the SSSP problem as a running example. In Sec. 2.1 we discuss the issues \nthat arise when SSSP algorithms are writ\u00adteninaconventional programming language.InSec. 2.2,we describe \nhow operators can be speci.ed in Elixir indepen\u00addently of the schedule. and how a large number of different \nscheduling policies can be speci.ed abstractly by the pro\u00adgrammer. In particular, we show how the Dijsktra, \nBellman-Ford, label-correcting and delta-stepping algorithms can be speci.ed in Elixir just by changing \nthe scheduling speci.\u00adcation. Finally, in Sec. 2.3 we sketch how Elixir addresses the two most important \nsynthesis challenges: how to synthe\u00adsize ef.cient implementations, and how to insert synchro\u00adnization. \n 2.1 SSSP Algorithms and the Relaxation Operator Given a weighted graph and a node called the source, \nthe SSSP problem is to compute the distance of the shortest path from the source node to every other \nnode (we assume the absenceof negative weightcycles).As mentionedin Sec. 1, there are many sequential \nand parallel algorithms for solv\u00ading the SSSP problem such as Dijkstra s algorithm[11], the Bellman-Ford \nalgorithm[11], the label-correcting algo\u00adrithm[22], and delta-stepping[22]. In all algorithms, each node \na has an integer attribute a.dist that holds the length of the shortest known path to that node from \nthe source. This attribute is initialized to 8 for all nodes other than the source where it is set to \n0,andisthenloweredtoits .nalvalueusing iterative edge relaxation:if a.dist is lowered and (i) there is \nan edge (a, b) of length wab, and (ii) b.dist >a.dist + wab, the value of b.dist is lowered to a.dist \n+ wab. However, the order in which edge relaxations are performed can be differ\u00adent for different algorithms, \nas we discuss next. In Fig. 1we present pseudocode for the sequential label\u00adcorrecting and the Bellman-Ford \nSSSP algorithms with the edge relaxation highlighted in grey. Although both algo\u00adrithms use relaxations, \nthey may perform them in different orders and different numbers of times. We call this order the schedule \nfor the relaxations. The label-correcting algo\u00adrithm maintains a worklist of edges for relaxation. Initially, \nall edges connected to the source are placed on thisworklist. At each step, an edge (a, b) is removed \nfrom theworklist and relaxed; if there are several edges on the worklist, the edge to be relaxed is chosen \nheuristically. If the value of b.dist is lowered, all edges connected to b are placed on the worklist \nfor relaxation. The algorithm terminates when the worklist is empty. The Bellman-Ford algorithm performs \nedge relax\u00adations in rounds. In each round, all the graph edges are re\u00adlaxedin some order.Atotalof |V \n|-1 rounds are performed, where |V | is the number of graph nodes. Although both al\u00adgorithms arebuilt \nusing the same basic ingredient, as Fig. 1 shows, it is not easy to change from one to another. This \nis because the code for the relaxation operator is intertwined Label Correcting BellmanFord INITIALIZATION: \nINITIALIZATION: for each node a in V{ for each node a in V{if (a ==Src) a. dist = 0; if (a ==Src) a. \ndist = 0; else a. dist = 8; else a. dist = 8; }} RELAXATION: RELAXATION: Wl = new worklist(); for i = \n1.. |V |- 1 { // Initialize worklist . for each e=(a,b, w) { for each e = (Src, , ) { if (a.dist+ w \n< b.dist){ Wl.add(e); }b.dist= a.dist+w; while (!Wl.empty()) { } (a,b,w)= Wl.get(); } if (a.dist+ w < \nb.dist){ b.dist= a.dist+w; for each e = (b, c, w ) Wl.add(e); } }  Figure 1: Pseudocode for label-correcting \nand Bellman-Ford SSSP algorithms. intimately with the code for maintaining worklists, which is an artifact \nof how the relaxations are scheduled by a partic\u00adular algorithm.Inaconcurrent setting, the code for synchro\u00adnization \nmakes the programs even more complicated. 2.2 SSSP in Elixir Fig.2shows several SSSP algorithms written \nin Elixir. The major components of the speci.cation are the following. 2.2.1 Operator Speci.cation Lines \n1 2 de.ne the graph. Nodes and edges are represented abstractlybyrelations thathavecertain attributes. \nEach node has a unique ID attribute(node) and an integer attribute dist; during the execution of the \nalgorithm, the dist at\u00adtributeofa nodekeeps trackofthe shortestknownpathto that node from the source. \nEdges have a source node, a des\u00adtination node, and an integer attribute wt,which is the length of that \nedge. Line 4 de.nes the source node. SSSP algo\u00adrithms use two operators, one called initDist to initialize \nthe dist attribute of all nodes (lines 6 7), and another called relaxEdge to perform edge relaxations \n(lines 9 13). Operators are describedbyrewrite rulesin whichthe left\u00adhand side is a predicated subgraph \npattern, called the redex pattern, and the right-hand side is an update. A predicated subgraph pattern \nhas two parts, a shape constraint and a value constraint. A subgraph G. of the graph is said to satisfy \nthe shape constraint of an operator if thereisa bijection betweenthe nodesintheshape constraint and the \nnodes in G. that preserves the edge relation. The shape constraint in the initDist operator is satis.ed \nby every node in the graph, while the one in the relaxEdge operator is satis.ed by every pair of nodes \nconnected by an edge. A value constraint .lters out some of the subgraphs that satisfy the shape constraint \nby imposing restrictions on the values of attributes; in the case of the relaxEdge operator, the conjunction \nof the shape and value constraints restricts attention to pairs of nodes (a, b) which have an edge from \na to b, and whose dist attributes satisfy the constraint a.dist + wab <b.dist. A subgraph that satis.es \nboth the shape and value constraints of an operator is said to match the predicated sub-graph pattern \nof that operator, and will be referred to as a redex of that operator; it is a special case of the general \nnotion of neighborhoods in the operator formulation[25]. The right-hand side of a rewrite rule speci.es \nupdates to someofthevalue attributesofnodesandedgesinasubgraph matching the predicated subgraph pattern \non the left-hand side of that rule. In this paper, we restrict attention to local computation algorithms[25]that \nare not allowed to morph the graph structure by adding or removing nodes and edges. Elixir allows disjunctive \noperators of the form op1 or ... or opk where all operators opi share a common same shape con\u00adstraint. \nThe Betweenness Centrality program shown in App. A and discussed in Sec. 6 uses disjunctive operators \nwith2disjuncts. Statements de.ne how operators are applied to the graph. Alooping statement has one the \nforms foreach op , for i=low..high op ,or iterate op where op is an opera\u00adtor.A foreach statement.ndsall \nmatchesofthegivenoper\u00adator and applies the operator once to each matched subgraph in some unspeci.ed \norder. Line 15 de.nes the initialization statement to be the application of initDist once to each node. \nA for statement applies an operator once for each value of i between low and high.An iterate statement \napplies an operator ad in.nitum by repeatedly .nding some redex and applying the operator to it. The \nstatement termi\u00adnates when no redexes exist. Line 16 expresses the essence of the SSSP computation as \nthe repeated application of the relaxEdge operator (for now, ignore the text >> sched ). It is the responsibility \nof the user to guarantee that iterate arrivestoa.xed-point aftera .nite numberof stepsbyspec\u00adifying meaningful \nvalue constraints. Finally, line 17 de.nes the entire computation to be the initialization followed by \nthe distances computation. Elixir programs can be executed sequentially by repeat\u00adedly searching the \ngraph until a redex is found, and then ap\u00adplying the operator there. Three optimizations are needed to \nmake this baseline, non-deterministic interpreter, ef.cient. 1. Even in a sequential implementation, \nthe order in which redexesareexecutedcanbe importantforwork-ef.ciency and locality. The best order may \nproblem-dependent, so it is necessary to give the application programmer con\u00adtrol over scheduling. Sec. \n2.2.2 gives an overview of the scheduling constructs in Elixir. 2.Toavoid scanningthe graph repeatedlyto \n.nd redexes,it is desirable to maintain a worklist of potential redexes in the graph. The application \nof an operator may enable and 1 Graph [nodes(node : Node, dist : int) 2 edges(src : Node, dst : Node, \nwt : int) ] 3 4 source : Node 5 6 initDist =[ nodes(node a, dist d) ] . 7 [ d = if (a == source) 0 else \n8] 8 9 relaxEdge =[ nodes(node a, dist ad) 10 nodes(node b, dist bd) 11 edges(src a, dst b, wt w) 12 \nad+ w < bd] . 13 [ bd = ad+ w] 14 15 init = foreach initDist 16 sssp = iterate relaxEdge \u00bb sched 17 main \n= init; sssp Algorithm Schedule speci.cation Dijkstra sched = metric ad \u00bb group b Label-correcting sched \n= group b\u00bb approx metric ad \u00bb unroll 2 DELTA: unsigned int .-stepping-style sched = metric (ad+w)/ DELTA \nBellman-Ford NUM NODES: unsigned int // override sssp sssp = for i=1..( NUM NODES -1) step step = foreach \nrelaxEdge Figure 2: Elixir programs for SSSP algorithms. disable redexes, so the worklist needs to be \nupdated in\u00adcrementally whenever an operator is applied to the graph. The worklist can be allowed to contain \na superset of the set of actual redexes in the graph, provided an item is tested when it is taken off \nthe worklist for execution. Sec. 2.3.1 gives a high-level description of how Elixir maintains worklists. \n 3. In a parallel implementation, each activity should appear tohaveexecuted atomically. Therefore, Elixir \nmust insert appropriate synchronization. Sec. 2.3.2 describes some of the main issues in doing this. \n 2.2.2 Scheduling Constructs Elixir provides a compositional language for specifying commonly used scheduling \nstrategies declaratively and au\u00adtomatically synthesizes ef.cient implementations of them. We use Dijkstra-style \nSSSP computation to present the keyideas of our language. This algorithm maintains nodes in a priority \nqueue, ordered by the distance attribute of the nodes. In each iteration, a node of minimal priority \nis re\u00admoved from the priority queue, and relaxations are per\u00adformed on all outgoing edges of this node. \nThis is described by the composition of two basic scheduling policies: 1. Given a choice between relaxing \nedge e1 =(a1,b1) and edge e2 =(a2,b2) where a1.dist <a2.dist,give e1 a higher priority for execution. \nIn Elixir, this is expressed by the speci.cation metric ad. 2.To improve spatial and temporal locality, \nit is desirable to co-schedule active edges that have the same source node a, in preference to interleaving \nthe execution of edges of the same priority from different nodes. In Elixir this is expressed by the \nspeci.cation group b, which groups together relaxEdge applications on all edges outgoing from a to a \nsuccessor b. This can be viewed as a re.nement of the metric ad speci.cation, and the compositionof these \npoliciesisexpressedas metric ad >> group b. These two policies exemplify two general scheduling schemes: \ndynamic and static scheduling. Scheduling strate\u00adgies that bind the scheduling of redexes at runtime \nare called dynamic scheduling strategies, since theydetermine the pri\u00adorityofa redex usingvaluesknownonlyat \nruntime.Typ\u00adically, they are implemented via a dynamic worklist data\u00adstructure that prioritizes its contents \nbased on the speci.c policy. In contrast, static scheduling strategies, such as grouping, bind scheduling \ndecisions at compile-time and are re.ected in the structure of the source code that implements composite \noperators out of combinations of basic ones. One of the contributions of this paper is the combination \nof static and dynamic scheduling strategies in a single system. The main scheduling strategies supported \nby Elixir are the fol\u00adlowing. metric e The arithmetic expression e over the variables of the redex pattern \nis the priority function. In practice, many algorithms use priorities heuristically so they can tolerate \nsome amount of priority inversion in scheduling. Exploiting thisfactcanleadtomoreef.cient implementations,soElixir \nsupports a variant called approx metric e. group V speci.es that every redex pattern node v . V should \nbe matched in all possible ways. unroll k Some implementations of SSSP perform two\u00adlevel relaxations: \nwhen an edge (a,b) is relaxed, the out\u00adgoing edges of b are co-scheduled for relaxation if they are active, \nsince this improves spatial and temporal locality. This can be viewed as a form of loop unrolling. Elixir \nsupports k-level unrolling, where k is under the control of the appli\u00adcation programmer. (op1 or op2) \n. fuse speci.es that instances of op1,op2 working on the same redex should create a new composite operator \napplying the operators in sequence (in left-to-right order of appearance). Fusing improves locality and \namor\u00adtizes the cost of acquiring and releasing locks necessary to guarantee atomic operator execution. \nThe group, unroll, and fuse operations de.ne static scheduling strategies. We use the language of Nguyen \net al. [23]to de.ne dynamic scheduling policies that combine metric with LIFO and FIFO policies, and \nuse implementa\u00adtionsof theseworklists from the Galois framework[2]. Fig. 2 shows the use of Elixir scheduling \nconstructs to de.ne a number of SSSP implementations. The label\u00adcorrectingvariant[22]is an unordered \nalgorithm, which, on each step, starts from a node and performs relaxations on all incident edges, up \nto two hops away. The delta-stepping variant[22]operates on single edges and usesa. parameter to partition \nredexes into equivalence classes. This heuristic achieveswork-ef.ciencyby processing nodesin orderof \nin\u00adcreasing distance from the source, while also exposing par\u00adallelism by allowing redexes in the same \nequivalence class tobe processedin parallel. Finally Bellman-Ford[11]works ina SIMD styleby performinga \nseriesof roundsin whichit processes all edges in the graph.  2.3 Synthesis Challenges We .nish this \nsection with a brief description of the main challenges that Elixir addresses. First, we discuss how \nElixir optimizes worklist maintenance and second how it synchro\u00adnizes code to ensure atomic operator \nexecution. 2.3.1 Synthesizing Work-ef.cient Implementations To avoid scanning the graph repeatedly for \nredexes, it is necessary to maintain a worklist of redexes, and update this worklist incrementally when \na redex is executed since this might enable or disable other redexes. To understand the issues, consider \nthe label-correcting implementation in Fig.1,which iteratesoverall outgoingedgesofb and inserts them \ninto the worklist. Since the worklist can be allowed to contain a superset of the set of the redexes \n(as long as items are checked when they are taken from the worklist), another correctbutlessef.cient \nsolutionisto insertalledges incident to either a or b into the worklist. However, the programmer manually \nreasoned that the only place where new useful work can be performed is at the outgoing edges of b, since \nonly b.dist is updated. Additionally, the programmer could experiment with different heuristics to improve \nef.ciency. For example, before inserting an edge (b, c) into the worklist, the programmer could eagerly \ncheck whether b.dist + wbc = c.dist. In a general setting with disjunctive operators, differ\u00adent disjuncts \nmay become enabled on different parts of the graph after an operator application. Manually reason\u00ading \nabout where to apply such incremental algorithmic steps can be daunting. Elixir frees the programmer \nfrom this task. In Fig. 2 there is no code dealing with that aspect of the computation; Elixir automatically \nsynthesizes the worklist updates and also allows the programmer to easily experi\u00adment with heuristics \nlike the above without having to write much code. Another meansof achievingwork-ef.ciencyisbyusinga good \npriority function to schedule operator applications. In certain implementations of algorithms such as \nbetweenness centrality and breadth .rst search, the algorithm transitions through different priority \nlevels in a very structured manner. Elixir can automatically identify such cases and synthesize customized \ndynamic schedulers that are optimized for the particular iteration patterns. attid Graph attributes \nacid Action identi.ers opid Operation identi.ers var Operator variables and global variables ctype C++ \ntype program ::= graphDef global+ opDef+ actionDef+ graphDef ::= Graph [nodes(attDef+ edges(attDef+)] \nattDef global ::= ::= attid :ctype | attid : set[ctype] var : ctype opDef opExp tuple boolExp arithExp \nsetExp assign att ::= ::= ::= ::= |::= |::= ::= ::= opid = opExp [ tuple* (boolExp) ] . [ assign* ] nodes(att*)| \nedges(att*) !boolExp | boolExp &#38;boolExp | arithExp < arithExp arithExp == arithExp | var in setExp \nnumber | var | arithExp + arithExp | arithExp -arithExp if (boolExp) arithExp else arithExp empty |{var}| \nsetExp + setExp | setExp -setExp var = arithExp | var = setExp | var = boolExp attid var  data attributes \nassociated with its nodes and edges. Next, a program de.nes global variables that actions may access \nfor reading. Global variables may be accessed for reading and writing outside of actions by the larger \napplication. The graph program then de.nes operators and actions. Operators de.ne unit transformations \nthat may be applied to a given subgraph. They are used as building blocks in statements that apply operators \niteratively. An important limitation of operatorsisthattheymayonlyupdatedata attributes,butnot morph \nthe graph (add or remove nodes and edges). Actions compose statements and name them. Theycompile to C++ \nfunctions that take a single graph reference argument. 3.1 Graphs and Patterns actionDef ::= acid = stmt \nstmt ::= iterate schedExp | foreach schedExp | for var = arithExp .. arithExp stmt | acid | invariant? \nstmt invariant? | stmt; stmt schedExp ::= ordered | unordered unordered ::= disjuncts ordered ::= opsExp \nfuseTerm? groupTerm? metricTerm disjuncts ::= disjunctExp | disjunctExp or disjuncts disjunctExp ::= \nstatExp dynSched opsExp ::= opid | opid or opsExp statExp ::= opsExp fuseTerm? groupTerm? unrollTerm? \ndynSched ::= approxMetricTerm? timeTerm? fuseTerm ::= >> fuse * groupTerm ::= >> group var unrollTerm \n::= >> unroll number metricTerm ::= >> metric arithExp approxMetricTerm ::= >> approx metric arithExp \ntimeTerm ::= >> LIFO | >> FIFO Figure 3: Elixir language grammar (EBNF). The notation e? means that e \nis optional.  2.3.2 Synchronizing Operator Execution To guarantee correctness in the context of concurrent \nexecu\u00adtion, the programmer must make sure that operators execute atomically. Although it is not hard \nto insert synchronization code into the basic SSSP relaxation step, the problem be\u00adcomes more complex \nonce scheduling strategies like unroll and group are used since the resulting super-operator code can \nbe quite complex. There are also manysynchronization strategies that could be used such as abstract locks, \nconcrete locks, and lock-free constructs like CAS instructions, and the trade-offs between them are not \nalways clear even to ex\u00adpert programmers. Elixir frees the programmer from having to worry about these \nissues because it automatically introduces appropriate .ne grained locking. This allows the programmer \nto focus on the creative parts of problem solving and still get the performance bene.ts of parallelism. \n3. The Elixir Graph Programming Language In this section, we formalize our language whose grammar is \nshown in Fig. 3. Technically, a graph program de.nes graph transformations, or actions, that may be used \nwithin an application. It .rst de.nes a graph type by listing the Let Attr denotea .nite setof attributes. \nAn attribute denotes a subtype of one of the following types: the set of numeric values Num (integers \nand reals), graph nodes Node and sets of graph nodes P(Node). Let Val def = Num . Node . P(Node) stand \nfor the union of those types. De.nition 3.1 (Graph). 1 A graph G =(V G,EG , AttG) is a triple where V \nG . Node the graph nodes, EG . V G \u00d7 V G are the graph edges, and AttG :((Attr \u00d7 V G) . Val) . ((Attr \n\u00d7 V G \u00d7 V G) . Val) associates values with nodes and edges. We denote the set of all graphs by Graph. \nDe.nition 3.2 (Pattern). A pattern P =(V P ,EP , AttP ) is a connected graph over variables. Speci.cally, \nV P . Var are the pattern nodes, EP . V P \u00d7 V P are the pattern edges, and AttP :(Attr \u00d7 V P ) . Var \n. (Attr \u00d7 V P \u00d7 V P ) . Var associates a distinct variable (not in V P ) with each node and edge. We \ncall the latter set of variables attribute variables. We refer to (V P ,EP ) as the shape of the pattern. \nIn the sequel, when no confusion is likely, we may drop superscripts denoting the association between \na com\u00adponent and its containing compound type instance, e.g., G =(V, E). De.nition 3.3 (Matching). Let \nG be a graph and P be a pattern. We say that \u00b5 : V P . V G is a matching (of P in G), written (G, \u00b5) \n|= P , if it is one-to-one, and for every edge (x, y) . EP there exists an edge (\u00b5(x),\u00b5(y)) . EG . We \ndenote the set of all matchings by Match : Var . Node. Weextendamatching\u00b5 : V P . V G toevaluate attribute \nvariables \u00b5 :(Graph \u00d7 Var) . Val as follows.Forevery attribute a, pattern nodes y, z . V P , and attribute \nvariable x, we de.ne: \u00b5(G, x)= AttG(a, \u00b5(y)) if AttP (a, y)= x, \u00b5(G, x)= AttG(a, \u00b5(y),\u00b5(z)) if AttP (a, \ny, z)= x. Lastly, we let \u00b5 extend to evaluate expressions over the variablesofa patternby structural \ninductionoverthe natural de.nitionsof the sub-expression types de.nedin Fig. 3. 1Our formalization naturally \nextends to graphs with several node and edge relations,but for simplicityof the presentation wehave just \noneof each. 3.2 Operators In the sequel, we will denote syntactic expressions by refer\u00adringto their \nrespective non-terminalsinFig. 3. De.nition 3.4 (Operator). An operator is a triple denoted by op =[Rop, \nGdop] . [Updop] where Rop is called the redex pattern;Gdop is a Boolean-valued expression over the variables \nof the redex pattern, called the guard; and Updop : V R . Expr contains an assignment per attribute variable \nin the redex pattern, in terms of the variables of the redex pattern (for brevity, we omit identity assignments). \nWe now de.ne the semantics of an operator as a function that transforms a graph for a given matching \n[[\u00b7]] : opExp . (Graph \u00d7 Match) . Graph. Let op =[R, Gd] . [Upd] be an operator and let \u00b5 : V R . V G \nbe a matching (of R in G).We say that \u00b5 satis.es the shape constraint of op if (G, \u00b5) |= R. We say that \n\u00b5 satis.es the value constraint of op (and shape constraint), written (G, \u00b5) |= R, Gd,if \u00b5(G, Gd)= True. \nIn such a case, \u00b5 induces the subgraph D denotedby \u00b5(R), which we call a redex and de.ne by: def V D \n= {\u00b5(x) | x . V R} , ED def = EG n (V D \u00d7 V D) , def AttD = AttG|(Att\u00d7V D).(Att\u00d7V D\u00d7V D) . We de.ne an \noperator application by G. =(V G,EG , Att.), (G, \u00b5) |= R, Gd; [[op]](G, \u00b5)= G, else where D = \u00b5(R) and \nthe node and edge attributes in D are updated using the expressions in Upd, respectively: . . v . V D,v \n= \u00b5(xv) \u00b5(G, Upd(y)), Att.(a, v)= and AttR(a, xv)= y; . Att(a, v) else. . . (u, v) . ED , . . \u00b5(G, Upd(y)),u \n= \u00b5(xu),v = \u00b5(xv)Att.(a, u, v)= . and AttR(a, xu,xv)= y; . . Att(a, u, v) else. The remainder of this \nsection de.nes the semantics of statements. iterate and foreach statements have two dis\u00adtinct .avors: \nunordered iteration and ordered iteration.We de.ne themin that order.Wedo not de.ne for statements as \ntheir semantics is quite standard in all imperative languages.  3.3 Semantics of Unordered Statements \nUnordered statements have the form iterate unordExp or foreach unordExp where unordExp has the form opsExp \n>> statExp >> dynSched . The expression opsExp is either a single operator op or a disjunction of operators \nop1 or ... or opk having the same shape(Rop1 = ... = Ropk ). Intuitively, a disjunction repre\u00adsents alternative \ngraph transformations.We de.ne the short\u00adhand opi..j = opi or ... or opj . The expression statExp, called \na static schedule, is a pos\u00adsibly empty sequence of static scheduling terms, which may include fuse, \ngroup, and unroll.If opsExp is a disjunc\u00adtion thenit mustbe followedbya fuse term. Anexpression of the \nform opsExp>>statExp de.nes a composite operator by grouping together operator applications in a statically\u00adde.ned \n(i.e., determined at compile-time)way.We refer to such an expression as a static operator. The expression \ndynSched, called a dynamic schedule,is a possibly empty sequence of dynamic scheduling terms, which may \ninclude approx metric, LIFO, and FIFO.A dynamic schedule determines the order by which static op\u00aderators \nare selected for execution by associating a dynamic priority with each redex. To simplify the exposition, \nin this paper we present the semantics under the simplifying assumption that statExp is empty.Forthefull \ntechnical treatment,the readeris referred to the accompanying technical report[27]. 3.3.1 Preliminaries \nDe.nition 3.5 (ActiveElement). An active element, denoted by elem(op,\u00b5), pairs an operator op with a \nmatching \u00b5 . Match. It stands for the potential application of op to (G, \u00b5). We denote the set of all \nactive elements by A. We de.ne the new set of redexes for an operator and for a disjunction of operators, \nrespectively by def = Rop RDX[[op]]G = {\u00b5 . Match | (G, \u00b5) |, Gdop} , = RDX[[op1]]G . ... . RDX[[opk]]G. \n RDX[[op1..k]]G def We de.ne the set of redexes of an operatorop. createdby an application of an operator \nop by let G. =[[op]](G, \u00b5) def DELTA[[op, op.]] (G, \u00b5)= in RDX[[op.]]G. \\ RDX[[op.]]G. We lift the operation \nto disjunctions: def DELTA[[op]] (G, \u00b5)= DELTA[[op, opi]] (G, \u00b5) . a, opc..dac=i=d  3.3.2 De.ning \nDynamic Schedulers Let iterate exp be a statement and let op1..k be the op\u00aderators belonging to exp.An \niterate statement executes by repeatedly .nding a redex for an operator and applying that operator to \nthe redex. An execution of iterate yields a (possibly in.nite) sequence G = G0,...,Gk,... where Gi+1 \n=[[op]](Gi,\u00b5i). We nowde.nea scalar prioritypr(t, e, Gi) and partial or\u00adder =t forascheduling term t \n.{metric, approx metric, LIFO, FIFO}, an active element e, and a graph Gi: def pr(metric a, e, Gi)= \n\u00b5i(Gi,a) def pr(approx metric a, e, Gi)= fuzz(\u00b5i(Gi,a)) def pr(LIFO,e,Gi)= pr(FIFO,e,Gi)= i ' ' p =metric \na p . p = p ' ' p =approx metric a p. p = p ' ' p =LIFO p . p = p ' ' p =FIFO p . p = p where fuzz(x) \nis some approximation of x. For an expression d = t1>> ... >>tk . dynSched,we def de.ne pr(d, e, Gi)= \n(pr(t1,e,Gi),..., pr(tk,e,Gi)) and p =d pby the lexicographic quasi order =lex, which is also de.ned \nfor vectors of different lengths. A prioritized active element elem(op,\u00b5,p) is an active element associated \nwith a priority. We denote the set of all prioritized active elements by AP . For two prioritized active \nelements v = (op,\u00b5v,pv) and w =(op,\u00b5w,pw), we de.ne v = w if vwpv =lex pw def We de.ne the type of prioritized \nworklists by WP = AP* . We say that a prioritized worklist . = e1 \u00b7 ... \u00b7 ek .WP is ordered according \nto a dynamic scheduling expression d . dynSched,if ei =d ej implies i = j for every 1 = i = j = k. We \nwrite PRIORITY[[d]] . = .. if .. is a permutation of . preserving the quasi order =d.We de.ne the following \nworklist operations for a dynamic scheduling expression d: def EMPTY = E, def POP . =(head(.), tail(.)) \n, def MERGE[[d]] (., d)= PRIORITY[[d]] (. \u00b7 d) , def INIT[[d]] G = MERGE[[d]] (E, RDX[[op1..k]]G) where \nhead(.) and tail(.) refer to the .rst element and (possibly empty) suf.x of a sequence, respectively. \n 3.3.3 Iteratively Executing Operators def Wede.ne the set of program states asS= Graph.Graph\u00d7 Wl.We \nwill write G + Wl to denote a graph-worklist pair. The meaning of statements is given in terms of a transition \nrelation with two forms of transitions: 1. (S, s) =. s., means that the statement S transforms the state \ns into s. and .nishes executing; 2. (S, s) =.(S.,s.), means that the statement S trans\u00adform the state \ns into s. to which the remaining statement S. should be applied.  The de.nition of =. isgivenbythe rulesinFig. \n4.The semantics induced by the transition relation yields (possibly in.nite) sequences of states s1,...,sk,.... \nA correct par\u00adallel implementation gives the illusion that each transition occurs atomically, even though \nthe executions of different transitions may interleave.  3.4 Semantics of Ordered Statements Ordered \nstatements have the form iterate opsExp >> statExp >> metric exp . iterateinit starts executing iterate \ne by initializing a worklist with the set of redexes found in G (iterate exp,G) =.(iterate exp,G + Wl) \nif Wl = INIT[[exp]] G iteratestep executes an operator opi in exp (iterate exp,G + Wl) =.(iterate exp,G' \n+ Wl'') if (elem(opi,\u00b5,p), Wl')= POP Wl G' =[ opi]](G, \u00b5) .= DELTA[[opi, op1..k]] (G, \u00b5) Wl'' = MERGE[[exp]] \n(Wl', .) iteratedone returns the graph when no more operators can be scheduled (iterate exp,G + EMPTY) \n=. G foreachinit, foreachdone same rules as for iterate foreachstep executes an operator opi in exp (foreach \nexp,G + Wl) =.(foreach exp,G' + Wl') if (elem(opi,\u00b5,p), Wl')= POP Wl G' =[ opi]](G, \u00b5) Figure 4: An operational \nsemantics for Elixir statements. The static scheduling expression statExp is the same as in the unordered \ncase, except that we do not allow unroll. The expression opsExp is either a single operator op or a disjunction \nof operators op1..k having the same shape. If opsExp is a disjunction then it is followed by a fuse term. \nPrioritized active elements are dynamically partitioned into equivalence classes Ci based on the value \nof exp. The execution then proceeds as follows:We startby processing active elements from the equivalence \nclass C0, which has the lowest priority. Applying an operator to active elements from Ci can produce \nnew active elements at other priority levels, e.g., Cj . Once the work at priority level i is done we \nstart processingworkatthenextlevel.We will restrict our attention to the class of algorithms where the \npriority of new active elements is greater than or equal to the priority of existing active elements(i \n= j). Under this restriction, we are guaranteed to never miss work as we process successive priority \nlevels. The execution terminates when all work (at the highest priority level) is done. All the algorithms \nthat we studiedbelongtothis class.Theaboveexecution strategyad\u00admits a straightforward and ef.cient parallelization \nstrategy: associate with each Ci abucket Bi and process in parallel all work inbucket Bi before moving \nto Bi+1. This implements the so-called level-by-level parallel execution strategy. 3.5 Using Strong \nGuards for Fixed-Point Detection Our language allows de.ning iterate actions that do not terminate for \nall inputs. It is the responsibility of the pro\u00adgrammer to avoid de.ning such actions. When an action \ndoes terminateforagiven input,itisthe responsibilityofthe compiler to ensure that the emitted code detects \nthe .xed\u00adpoint and stops. Let \u00b5 be a matching and D = \u00b5(G) be the matched subgraph. Further, let G. =[[op]](G, \n\u00b5). One way to check whether an operator application leads to a .xed-point is to check whether an operator \nhas had an any effect, i.e., ? Att.D = AttD. This requires comparing the result of the op\u00aderator application \nto a backup copyof D, created prior to its application.However, this approachis ratherexpensive.We opt \nfor a more ef.cient alternative by placing a requirement on the guards of operators, as explained next. \nDe.nition 3.6 (Strong Guard). We say that an operator op has a strong guard if for every matching \u00b5, \napplying the operator disables the guard. That is, if G. =[[op]](G, \u00b5) = Gdop then (G.,\u00b5) |. A strong \nguard allows to check (G, \u00b5) |= Gdop, which involves just reading the attributes of D and evaluating \na Boolean expression. Further, strong guards help us improve the precision of our incremental worklist \nmaintenance by supplying more information to the automatic reasoning procedure, as ex\u00adplained in Sec. \n4.3. Our compiler checks for strong guards at compile-time and signals an error to the programmer oth\u00aderwise \n(see details in Sec. 4.3). In our experience, strong guardsdo not limitexpressiveness.Foref.ciency, operators \nare usually written to act on a graph region in a single step, which leads to disabling their guard. \n4. Synthesis In this section, we explain how to emit code to implement Elixir statements.We use the notation \nCode(e) for the code fragment implementing the mathematical expression e in a high-level imperative language. \nThis section is organized as follows. First, we discuss our assumptions regarding the implementation \nlanguage. Sec. 4.1 describes the synthesis of operator-related proce\u00addures. Sec. 4.2 describes the synthesis \nof the E XPAND op\u00aderation, whichisusedto synthesizeRDXandasabuilding block in synthesizing DELTA. Sec. \n4.3 describes the synthe\u00adsis of DELTA via automatic reasoning. Sec. 4.4 puts together the elements needed \nto synthesize unordered statements. Fi\u00adnally, Sec. 4.5 describes the synthesis of ordered statements. \nImplementation Language and Notational Conventions We assume a Scala-like language containing standard \ncon\u00adstructs for sequencing, conditions, looping, and evaluation of arithmetic and Boolean expressions \nsuch as the ones used in Elixir. Operations on sets are realized via methods on set data structures. \nWe assume that the language allows static typingby the notation v : t, meaning thatvariable v has type \nt.To promote succinctness,variablesdo not require declara\u00adtion and come into scope upon initialization.We \nwrite vi..j to denote the sequence of variables vi,...,vj. Record types are written as record[f1..k], \nmeaning that an instance r of the record allows accessingthevaluesof .elds f1..k, written as r[fi]. We \nuse static loops (loops preceded by the static keyword) to concisely denote loops over a statically-known \nrange, which the compiler unrolls, instantiating the induction variablesin the loop body as needed.We \nsimilarly use static conditions. In de.ning procedures, we will use the notation f[statArgs](dynArgs) \nto mean that f is specialized for the statically-given arguments statArgs and accepts at runtime the \ndynamic arguments dynArgs.We note that, since we as\u00adsume a single graph instance, we will usually not \nexplicitly include it in the generated code. Graph Data Structure. We assume the availability of a graph \ndata structure supporting methods for reading and up\u00addating attributes, and scanning the outgoing edges \nand in\u00adcoming edges of a given node. The code statements corre\u00adsponding to these methods are as follows. \nLet vn and vm be variables referencing the graph nodes n and m, respectively. Let a be a node attribute \nand b be an edge attribute. Let da and db be variables of the appropriate types for attributes a and \nb, respectively, having the values d and d., respectively. da = get(a, vn) assigns AttG(a, n) to da \nand db = get(a, vn,vm) assigns AttG(b, n, m) to db.  set(a, vn,da) updates the value of the attribute \na on the node n to d:Att. = Att(a, n) . d, and set(b, vn,vm,db) updates the value of the attribute b \non the edge (n, m) to d.:Att. = Att(b, n, m) . d. .  edge(vn,vm) checks whether (n, m) . EG .  succs(vn) \nand preds(vn) return (iterators to) the sets of nodes {s | (n, s) . EG} and {p | (p, n) . EG}, respectively. \n nodes returns (an iterator to) the set of graph nodes V G .  In addition, we require that the graph \ndata structure be linearizable2. 4.1 Synthesizing Atomic Operator Application Let op =[nt1..k, et1..m, \nbexp] . [nUpd1..k, eUpd1..m] be an operator consisting of the following elements, for i =1..k and j =1..m:(i) \nnode attributesnti = nodes(node ni,ai vi); (ii) edge attributes etj = edges(src sj, dst dj ,bj wj );(iii) \na guardexpression bexp = opGd;(iv) node updatesnUpdi = vi ;and (v) edge updateseUpdj . nExpi= wj. eExpj \n. We note that in referring to pattern nodes the naming of variables ni, sj , dj , etc. are insigni.cant \nin themselves,but rather stand for different ways of indexing the actual set of variable names.Forexample \nn1 and s2 may both stand fora variable a . Fig.5shows the codes we emit, as procedure de.nitions, for \n(a) evaluating an operator, (b) for checking a shape con\u00adstraint, and (c) for checking a value constraint. \nThe procedure apply uses synchronization to ensure atomicity. The procedure .rst reads the nodes from \nthe matching variable mu into local variables. It then copies the variables to another set of variables \nused for locking. We assume a total order over all nodes, implemented by 2In practice, our graph implementation \nis optimized for non-morphing actions.Werelyonthelocks acquiredbythe synthesizedcodeto correctly synchronize \nconcurrent accesses to graph attributes. def apply[op](mu : record[n1..k]) = static for i =1..k {ni \n= mu[ni]}if checkShape[op](mu) static for i =1..k {lk i = ni} sort(lock less, lk 1..k) static for i =1..k \n{lock(lk i)} static for i =1..k {vi = get(ai,ni)} static for j =1..m {wj = get(bj ,sj ,dj )} if Code(bexp) \n static for i =1..k {set(ai,ni, Code(nExpi))}static for j =1..m {set(bj ,sj ,dj , Code(eExpj ))}static \nfor i =1..k {unlock(lk i)} (a) Code([[op]]). def checkShape[op](mu : record[n1..k]) : bool = static for \ni =1..k {ni = mu[ni]}// Now sj and dj correspond to \u00b5(sj ) and \u00b5(dj ). static for i =1..k static for \nj =1..k if ni = nj // Check if \u00b5 is one-to-one. return false static for j =1..m if \u00acedge(sj ,dj ) // \nCheck for missing edges. return false return true = Rop). (b) Code((G, \u00b5) | def checkGuard[op](mu : \nrecord[n1..k]) : bool = static for i =1..k {ni = mu[ni]}static for i =1..k {vi = get(ai,ni)}static for \nj =1..m {wj = get(bj ,sj ,dj )}return Code(bexp) (c) Code(\u00b5(G, Gdop)). Figure 5: Operator-related procedures. \nthe procedure lock less, which we use to ensure absence of deadlocks. The statement sort(lock less, lk1..k) \nsorts the lock variables, i.e., swaps their values as needed, using the sort procedure. Next, the procedure \nacquires the locks in as\u00adcending order (we use spin locks), thus avoiding deadlocks. Then, the procedure \nreads the node and edge attributes from the graph and evaluates the guard. If the guard holds the update \nexpressions are evaluated and used to update the at\u00adtributes in the graph. Finally, the locks are released. \nSince operators do not morph the graph checkShape does not require anysynchronization. The procedure \ncheckGuard is synchronized using the same strategy as apply. Fig.6shows the code we emit for Code([[relaxEdge]]). \n 4.2 Synthesizing EXPAND We now de.ne an operation EXPAND, which is used for implementing the group \nstatic scheduling term (detailed in the technical report), RDX, and DELTA. Let R be a pattern and v1..m \n. V R and vm+1..k = V R \\ v1..m be two complementing subsets of its nodes such that v1..m inducesa connected \nsubgraphof R.We de.ne the set of matchings V R . G identifying with \u00b5 on the node variables v by  def \napply[relaxEdge](mu: record[a,b]) = a = mu[a]; b = mu[b]; if (a !=b. edge(a, b)) // Inline checkShape[relaxEdge](mu). \nlk1=a;lk 2=b; if lock less(lk 2,lk 1) // inline sort  swap(lk 1, lk 2);  lock(lk 1); lock (lk 2); \n ad = get(dist , a); bd =get(dist , b); w= get(wt, a, b); if ad + w < bd // Inline checkGuard[relaxEdge](mu). \nset (dist,b, ad+w); unlock(lk 1); unlock (lk 2);  Figure 6: Code([[relaxEdge]]). We now explain how \nto synthesize a procedure that accepts a matching \u00b5 . W . V G, where W is any superset of v1..m, and \ncomputes all matchings \u00b5. . V R . V G such that \u00b5(vi)= \u00b5.(vi) for i =1..m. We can bind the variablesvm+1..k \nto graph nodes in dif\u00adferent orders, but it is more ef.cient to choose an order that enables scanning \nthe edges incident to nodes that are already bound. The alternative way requires scanning the entire \nset of graph nodes for each unbound pattern node and checking whether it is a neighbor of some bound \nnode, which is too inef.cient. We represent each ef.cient bind\u00ading order by a permutation of vm+1..k, \nwhich we denote by um+1..k, and by an auxiliary sequence T (R, vm+1..k)= (um+1,wm+1, dirm+1),..., (uk,wk, \ndirk) where each tu\u00adple de.nes the connection between an unbound node uj and a previously bound node \nwj and the direction of the edge between them forward for false and reverse otherwise. More formally, \nfor every j =(m +1)..k we have that wj . v1..j and if dirj = false then (uj ,wj ) . ER and otherwise \n(wj ,uj ) . ER . T is computed as a spanning tree. The procedure expand, shown in Fig. 7, .rst updates \n\u00b5. for 1..m and then uses T (R, vm+1..k) to bind nodes vm+1..k.Eachnodeisboundtoall possiblevaluesbyaloop \nusing the procedure expandEdge, which handles one tuple in (uj,wj , dirj). The loops are nested to enumerate \nover all combinations of bindings. We note that a matching computed by the enumeration does not necessarily \nsatisfy the shape constraints of R as some of the pattern nodes may be bound to the same graph node and \nnot all edges in R may be present between the corresponding pairs of bound nodes. It is possible to .lter \nout matchings that do not satisfy the shape constraint or guard via checkShape and checkGuard, respectively. \nWeuseexpand to de.ne Code(RDX[[op]](G, \u00b5)) in Fig. 8.  4.3 Synthesizing DELTA via Automatic Reasoning \nWe now explain how to automatically obtain an overap\u00adproximation of D ELTA[[op, op.]] (G, \u00b5) for anytwo \noperators op =[R, Gd] . [Upd] and op. =[R. , Gd.] . [Upd.] and a def EXPAND[[op,v]](G, \u00b5)= {\u00b5. . V R \n. V G | \u00b5|v = \u00b5.|v} . matching \u00b5, and how to emit the corresponding code. def expand[op,v1..m,T : record[nm+1..k]] \n(mu : record[n1..k], f : record[n1..k] . Unit) = mu ' = record[n1..k] // Holds the expanded matching. \nstatic for i =1..m {mu ' [vi]= mu[vi]}expandEdge[m +1, expandEdge[m +2, ... expandEdge[k, f(mu ' )] ...] \n// Inner function. def expandEdge[i, code]= [ui,wi, diri]= T [i] // ui is unbound and wi is bound. static \nif diri = true for s . succs(mu[wi]) mu ' [ui]= s code // inline code else for p . preds(mu[wi]) mu \n' [ui]= p code // inline code Figure 7: Code for applying a function f to each matching in EXPAND[[op,v1..m]](G, \n\u00b5). def redexes[op](f : record[n1..k] . Unit) = for v . nodes mu = record[n1] expand[op,n1,T (R, n2..k)](mu,f \n ' ) def f ' (mu : record[n1..k]) = if checkShape[op](mu) . checkGuard[op](mu) f(mu) Figure 8: Code for \ncomputing RDX[[op]](G, \u00b5) and applying a function f to each redex. The de.nition of DELTA given in Sec. \n3 is global in the sense that it requires searching for redexes in the entire graph, which is too inef.cient. \nWe observe that we can rede.ne D ELTA by localizing it to a subgraph affected by the application of the \noperator op, as we explain next. For the rest of this subsection, we will associate a match\u00ading \u00b5 with \nthe corresponding redex pattern R using the no\u00adtational convention \u00b5R. Let \u00b5R and \u00b5R1 be two matchings \ncorresponding to the operators above.We say that \u00b5R and \u00b5R1 overlap, written \u00b5R A \u00b5R1 , if the matched \nsubgraphs overlap: \u00b5R(V R) n \u00b5R1 (V R1 )= \u00d8. Then, the following equality holds: DELTA[[op, op.]] (G, \n\u00b5R)= let G. =[[op]](G, \u00b5R) in {\u00b5R1 | \u00b5R1 A \u00b5R, (G, \u00b5R1 ) |= R. , Gd. , (G.,\u00b5R1 ) |= R. , Gd. . We note \nthat any overapproximation of DELTA can be used in correctly computing the operational semantics of an \niterate statement. However, tighter approximations lead to reductionin uselesswork.We proceedbydeveloping \nan overapproximation of the local de.nition of DELTA. Given a matching \u00b5R, the set of overlapping matchings \n\u00b5R1 can be classi.ed into statically-de.ned equivalence classes, de.ned as follows. If \u00b5R1 A \u00b5R then \nthe overlap between \u00b5R(V R) and \u00b5R1 (V R1 ) induces a partial function . : V R -V R1 de.ned as .(x)= \nx. if \u00b5R(x)= \u00b5R1 (x.). We call the function . the in.uence function of R and R. and denote the domain \nand range of . by .dom and .range, respectively.Two matchings \u00b51 and \u00b52 are equivalent if R1 R1 theyinduce \nthe same in.uence function ..We can compute the equivalence class [.] of an in.uence function . by [.]= \nEXPAND[[op.,.range]](G, \u00b5R) . Let infs(op, op.)= .1..k denote the in.uence functions for the redex patterns \nR and R.. We de.ne the function . V R1 shift : Match \u00d7 (V R ) . Match, which accepts a matching \u00b5R and \nan in.uence function . and returns the part of a matching \u00b5R1 restricted to .range: shift(\u00b5R,.) def . \n= {(.(x),\u00b5R(x)) | x . .dom} The .rst overapproximation we obtain is def DELTA1[[op, op.]] (G, \u00b5R)= EXPAND[[op.,.range]](G, \nshift(\u00b5R,.)) . ..infs(op,op1) Astraightforward way to obtain a tighter approximation is to .lter out \nmatchings not satisfying the shape and value constraints of op. . We say that an in.uence function . \nis useless if for all graphs G and all matchings \u00b5R1 the following holds: for Rop, Gdop G. =[[op]](G, \n\u00b5R) either (G, \u00b5R1 ) |= 11 , mean\u00ading that an active element elem(op.,\u00b5R1 ) has already been 1 , Gdop \n= Rop scheduled, or (G.,\u00b5R1 ) |1 , meaning that the ap\u00adplication of op to (G, \u00b5R) does not modify the \ngraph in a way that makes \u00b5R1 (G.) a redex of op . Otherwise we say that . is useful.We denote the set \nof useful in.uence func\u00adtions by useInfs(op, op.).We can obtain a tighter approxi\u00admation DELTA2[[op, \nop.]] (G, \u00b5R) via useful in.uence func\u00adtions: def DELTA2[[op, op.]] (G, \u00b5R)= EXPAND[[op.,.range]](G, \nshift(\u00b5R,.)) . ..useInfs(op,op1) We use automated reasoning to .nd the set of useful in.uence functions. \nIn.uence Patterns. For every in.uence function .,we de.ne an in.uence pattern and construct it as follows. \n1. Start with the redex pattern Rop and a copy R. of Rop1 where all variables have been renamed to fresh \nnames. 2. Identify a node variable x . V R with a node variable .(x) and rename node attribute variables \nin R. to the variables used in the corresponding nodes ofR. Similarly rename edges attributes for identi.ed \nedges.  Example 4.1 (In.uencePatterns). Fig. 9 shows the six in\u00ad.uence patterns for operator relaxEdge \n(for now, ignore the text below the graphs). Here Rop consists of the nodes a and b (and the connecting \nedge) and R. consists of the nodes c and d (and the connecting edge). We display identi.ed nodes by listing \nboth variables inside the node ellipse. Intuitively, the patterns determine that candidate redexes are \none of the following types: a successor edge of b,a successor edge of a, a predecessor edge of a, a predecessor \nedge of b,anedgefrom b to a, and the edge from a to b itself. Query Programs. To detect useless in.uence \nfunctions, we generate a straight-line program over the variables of the corresponding in.uence pattern, \nhaving the form: 1 assume (Gd) 2 assume !(Gd )// comment out if identity pattern 3 update(Upd) 4 assert \n!(Gd ) Intuitively, the program constructs the following veri.ca\u00adtion condition: (1) if the guard of \nR, Gd, holds; and (2) the guard of R. , Gd., does not hold; and (3) the update Upd as\u00adsignnewvalues; \nthen(4)theguard Gd. does not hold for the updated values. Proving the assertion means that the corre\u00adsponding \nin.uence function is useless. The case of op = op. and the identity in.uence function is special. The \ncompiler needs to check whether the guard is strong, and otherwise emit an error message. This is done \nby constructing a query program where the second assume statement is removed. Wepass these programstoaprogramveri.cation \ntool(we use Boogie[6]and Z3[12]) asking it to prove the last as\u00adsertion. This amounts to checking satis.ability \nof a propo\u00adsitional formula over the theories corresponding to the at\u00adtributes types in our language \n integer arithmetic and set theory. When the veri.er is able to prove the assertion, we remove the corresponding \nin.uence function. If the veri.er cannot prove the assertion or a timeout is reached, we con\u00adservatively \nconsider the function as useful. Example 4.2 (Query Programs). Fig. 9 shows the query pro\u00adgrams generated \nby the compiler for each in.uence pattern. Out of the six in.uence patterns, the veri.er is able to rule \nout all except (a) and (e), which together represent the edges outgoing from the destination node, with \nthe special case where an outgoing edge links back to the source node. Also, the veri.er is able to prove \nthat the guard is strong for (f). This results with the tightest approximation of DELTA. We note that \nif the user speci.es positive edges weights (weight : unsigned int) then case (e) is discovered to be \nspurious. Fig. 10 shows the code we emit for DELTA2.Werepresent in.uence functions by appropriate records. \n 4.3.1 Optimizations Our compiler applies a few useful optimizations, which are not shown in the procedures \nabove. def delta2[op, op ' ,.1..q](mu : record[n1..k], f : record[n1..k] . Unit) = static for i =1..q \n//For each useful in.uence function. mu ' = record[n1..k] // Assume len(.i)= |.idom|. for j =1..len(.i) \n// Initialize mu' for .idom. mu ' [.(nj )] = mu[nj ] expand[op ' ,.irange,T (R ' ,V R1 \\ .irange)](mu \n' ,f ' ) def f ' (mu : record[n1..k]) = if checkShape[op ' ](mu) . checkGuard[op ' ](mu) f(mu)  Figure \n10: Code for computing DELTA2[[op, op.]] (G, \u00b5) and applying a function f to each matching. Reducing \nOverheads in checkShape. The procedure ex\u00adpand uses the auxiliary data structure T to compute poten\u00adtial \nmatchings. In doing so it already checks a portion of the shape constraint the edges of the redex pattern \nthat are included in T . The compiler omits checking these edges. Often, T includes all of the pattern \nedges; in such cases we specialize checkShape to only check the one-to-one condi\u00adtion. Reusing Potential \nMatchings. In cases when two opera\u00adtors have the same shape, we apply expand and reuse the matchings \nfound for both of them.  4.4 Synthesizing Unordered Statements Weimplement the operational semantics \nde.ned in Sec.3.3.3 by utilizing the Galois system runtime, which enables us to: (i) automatically construct \na concurrent worklist from a dynamic scheduling expression, and (ii) process the el\u00adements in the worklist \nin parallel by a given function. We use the latter capability by passing the code we syn\u00adthesize for \noperator application followed by the code for DELTA2[[op, op.]] (G, \u00b5), which inserts the found elements \nto the worklist for further processing. 4.5 Synthesizing Ordered Statements Certain algorithms, such \nas[5, 19], have additional proper\u00adties that enable optimizations over the baseline ordered par\u00adallelization \nscheme discussed in Sec. 3.4. For example, in the case of Breadth-First-Search(BFS), one can show that \nwhen processing work at priority level i, all new work is at priority level i+1. This allows us to optimize \nthe implemen\u00adtation to contain only twobuckets: Bc that holds work items at the current priority level, \nand Bn that holds work items at the next priority level. Hence, we can avoid the overheads associated \nwith the generic scheme, which supports an un\u00adbounded numberofbuckets. Additionally, since Bc is effec\u00adtively \nread-only when operating on work at level i,wecan exploit this to synthesize ef.cient load-balancing \nschemes when distributing the work contained in Bi to the worker threads. Currently Elixir uses these \ntwo insights to synthe\u00ad  itives(LOMP). Alternatively, it can use a bulk-synchronous worklist(BS)provided \nby the Galois library. Grouping: In the case of the SSSP relaxEdge operator, we can group either on a,or \nb, creating a push-based or a pull-based version of the algorithm. Additionally, Elixir uses groupingto \ndeterminethetypeofworklist items.For example,worklist items for SSSP can be edges (a,b),but if the group \nb directive is used, it is more economical to use node a as the worklist item. In our benchmarks, we \nconsider using either edges or nodes as worklist items, since this is the choice made in all practical \nimplementations. Unroll Factor: Unrolling produces a composite operator. This operator explores the subgraph \nin a depth-.st order. Shape/Value constraint checks (VC/SC): We consider the following class of heuristics \nto optimize the worklist manip\u00adulation. After the execution of an operator op, the algorithm may need \nto insert into the worklist a number of matchings \u00b5, which constitute the delta of op. Before inserting \neach such \u00b5, we can check whether the shape constraint (SC) and/or the value constraint (VC) is satis.ed \nby \u00b5, and if it is not, avoid inserting it, thus reducing overhead. Eliding such checks at this point \nis always safe, with the potential cost of populating the worklist with useless work. In practice, there \nare many more choices such as the order of checking constraints and whether these constraints are checked \ncompletely or partially. In certain cases, eliding check ci may be more ef.cient since performing ci \nmay require holding locks longer. Elixir allows the user to specify which SC/VC checks should be performed \nand provides three default, useful polices: ALL for doing all checks, NONE for doing no checks, and LOCAL \nfor doing only checks that can be performed by using graph elements already accessed bythe currentlyexecuting \noperator.Thelastoneis especially useful in the context of parallel execution. In cases where both VC \nand SC are applied, we always check them in the order SC, VC. 6. Empirical Evaluation Toevaluatetheeffectivenessof \nElixir,we perform studieson three problems: single-source shortest path(SSSP), breadth\u00ad.rst-search(BFS), \nand betweenness centrality(BC).We use Elixir to automatically enumerate and synthesize a number of program \nvariants for each problem, and compare the per\u00adformance of these programs to the performance of existing \nhand-tuned implementations. In the SSSP comparison, we use a hand-parallelized code from the Lonestar \nbenchmark suite[18].In theBFS comparison, we useahand-parallelized code from Leiserson and Schardl[19], \nand for BC, we use a hand-parallelized code from Bader and Madduri[5]. In all cases, our synthesized \nsolutions perform competitively, and in some cases, theyoutperform the hand-optimized im\u00adplementations. \nMore importantly, these solutions were pro\u00adduced throughasimple enumeration-basedexploration strat\u00adegyofthedesign \nspace,anddonotrelyonexpertknowledge from the user s part to guide the search. Elixir produces both serial \nand parallel C++ implementa\u00adtions.Itusesgraphdata structuresfromtheGaloislibrarybut all synchronization \nis done by the code generated by Elixir, so the synchronization codebuilt into Galois graphs is not used. \nThe Galois graph classes use a standard graph API, and it is straightforward to use a different graph \nclass if this is desirable. Implementations of standard collections such as sets and vectors are taken \nfrom the C++ standard library. In our experiments, we use the following input graph classes: Road networks: \nThese are real-world, road network graphs of the USA from the DIMACS shortest paths chal\u00adlenge[1].We \nuse the full USA network(USA-net)with 24M nodes and 58M edges, the Western USA network (USA-W)with 6M \nnodes and 15M edges,and the Florida network (FLA) with 1M nodes and 2.7M edges. Scale-free graphs: These \nare scale-free graphs that were generated using the tools provided by the SSCA v2.2 benchmark[3]. The \ngenerator is based on the Recur\u00adsive MATrix (R-MAT) scale-free graph generation al\u00adgorithm[10]. The sizeof \nthe graphsis controlledbya SCALE parameter;agraph contains N =2SCALE nodes, M =8 \u00d7 N edges, with each \nedge having strictly pos\u00aditive integer weight with maximum value C =2SCALE . For our experiments we removed \nmulti-edges from the generated graphs.We denotea graphof SCALE = X as rmatX . Random graphs: These graphs \ncontain N =2k nodes and M =4 \u00d7 N edges. There are N - 1 edges connect\u00ading nodes in a circle to guarantee \nthe existence of a con\u00adnected component and all the other edges are chosen ran\u00addomly, following a uniform \ndistribution, to connect pairs of nodes.We denotea graph with k = X as randX . We ran our experiments \non an Intel Xeon machine run\u00adning Ubuntu Linux 10.04.1LTS 64-bit. It contains four 6\u00adcore 2.00 GHz Intel \nXeon E7540 (Nehalem) processors. The CPUsshare128GBofmain memory.Each corehasa32KB L1cacheandauni.ed256KBL2cache.Each \nprocessorhas an18MBL3cachethatissharedamongthe cores.For SSSP and BC the compiler usedwasGCC 4.4.3.For \nBFS, the com\u00adpiler used was Intel C++ 12.1.0. All reported running times are the minimum of .ve runs. \nThe chunk sizes in all our ex\u00adperiments are .xed to 1024 for CF and 16 for CL. One aspect of our implementation \nthat we have not opti\u00admized yet is the initialization of the worklist, before the exe\u00adcutionofa parallelloop.Our \ncurrent implementationsimply iterates over the graph, checks the operator guards and pop\u00adulatestheworklist \nappropriatelywhenaguardis satis.ed.In most algorithms, the optimal worklist initialization is much simpler.Forexample, \nin SSSP we just have to initialize the worklistwiththe sourcenode(whenwehave nodesaswork\u00adlist items).Astraightforwardwayto \nsynthesize this codeis Dimension Value Ranges Group {a, b, NONE}Worklist {CF, OBM, LGEN} UnrollFactor \n{0, 1, 2, 10, 20, 30}VC check {ALL,NONE}SC check {ALL,NONE} Table 2: Dimensions explored by our synthetic \nSSSP vari\u00adants. Variant GR WL UF VC SC fPr v50 b OBM 2 .. ad/. v62 b OBM 2 \u00d7 . ad/. v63 b OBM 10 \u00d7 . \nad/. dsv7 b LGEN 0 .. ad/. Table3: Chosenvalues and priority functions(fPr)for best performing SSSP variants(.denotes \nALL, \u00d7 denotes NONE). to ask the user for a predicate that characterizes the state before each parallel \nloop.For SSSP, this predicate would as\u00adsert that the distance of the source is zero and the distance \nof all other nodesis in.nity.With this assertion, we can use our delta inference infrastructure to synthesize \nthe optimal worklist initialization code. This feature is not currently im\u00adplemented, so the running \ntimes that we report (both for our programs and programs that we compare against) exclude this part and \ninclude only the parallel loop execution time. 6.1 Single-Source Shortest Path Wesynthesizeboth orderedand \nunorderedversionsofSSSP. InTab. 2, we present the rangeofexploredvaluesin each dimension for the synthetic \nSSSP variants. In Tab. 3,we present the combinations that lead to the three best perform\u00ading asynchronous \nSSSP variants(v50,v62,v63)and the best performing delta-steppingvariant(dsv7). In Fig. 12a and Fig. 12b \nwe compare their running times with that of an asynchronous, hand-optimized Lonestar implementation on \ntheFLAandUSA-Wroadnetworks.Weobservethatinboth cases the synthesized versions outperform the hand-tuned \nimplementation, with the leveled version also having com\u00adpetitive performance. All algorithms are parallelized \nusing the Galois infras\u00adtructure, theyuse the sameworklist con.guration, with .= 16384, and the same \ngraph data-structure implementation. The value of . was chosen through enumeration and gives thebest \nperformanceforallvariants.The Lonestarversionis a hand-tuned lock-free implementation, loosely based \non the classic delta-stepping formulation[22].It maintainsawork\u00adlist of pairs [v, dv * ],wherev isanode \nand dv * is an approxi\u00admation to the shortest path distance of v (following the orig\u00adinal delta-stepping \nimplementation). The Lonestar version does not implement any of our static scheduling transfor\u00admations. \nAll synthetic variants perform .ne grained locking to guarantee atomic execution of operators, checking \nof the VC andevaluationofthe priority function.Forthe synthetic delta-steppingvariant dsv7 Elixir uses \nLGEN since newwork after the application of an operator can be distributed in var\u00adious (lower) priority \nlevels. An operator in dsv7 works over a source node a and its incident edges (a, b), which belong to \nthe same priority level. In Fig. 12c we present the runtime distribution of all syn\u00adthetic SSSP variants \non the FLA network. Here we summa\u00adrize a couple of interesting observations from studying the runtime \ndistributions in more detail. By examining the ten variants with the worst running times, we observed \nthat they all use a CF (chunked FIFO) worklist policy and are either operating on a single edge or the \nimmediate neighbors of a node (through grouping), whereas the ten best performing variants all use OBM. \nThis is not surprising, since by using OBM there are fewer updates to node distances and the algo\u00adrithm \nconvergesfaster.To get the best performance though, we must combine OBM with the static scheduling transforma\u00adtions. \nInterestingly, combining the use of CF with grouping and aggressive unrolling(byafactorof20) producesavari\u00adant \nthat performs only two to three timesworse than the best performing variant on both input graphs. 6.2 \nBreadth-First Search We experiment with both ordered and unordered versions of the BFS.InTab. 4andTab. \n5, we present the rangeofex\u00adploredvalues for the synthetic BFS variants and the combina\u00adtions thatgivethe \nbest performance, respectively.InFig. 13, we present a runtime comparison between the three best\u00adperforming \nBFS variants (both asynchronous and leveled), and two highly optimized, handwritten, lock-free parallel \nBFS implementations. The .rst handwritten implementation is from the Lonestar benchmark suite and is \nparallelized using the Galois system. The second is an implementation from Leiserson and Schardl[19], \nand is parallelized using Cilk++.Weexperiment with three different graph types.For the rmat20 and rand23 \ngraphs, the synthetic variants per\u00adform competitively with the other algorithmsFor the USA\u00adnet graph, \nthey outperform the hand-written implementa\u00adtions at high thread counts (for 20 and 24 threads). To understand \nthese results, we should consider the struc\u00adture of the input graphs and the nature of the algorithms. \nLeveled BFS algorithms try to balance exposing parallelism and being work-ef.cient by working on one \nlevel at a time. If the amount of available work per level is small, then they do not exploit the available \nparallel resources effectively. Asynchronous BFS algorithms try to overcome this problem by being more \noptimistic.Toexpose more parallelism, they speculatively work across levels. By appropriately picking \nthe priority function, and ef.ciently engineering the algo\u00adrithm, the goal is reduce the amount of mis-speculation \nin\u00adtroducedbyeagerlyworkingon multiplelevels.Focusingon the graph structure, we observe that scale-free \ngraphsexhibit the small-world phenomenon; most nodes are not neighbors of one another,but most nodes \ncanbe reached fromevery (a) FLA runtimes (b) USA-W runtimes (c) FLA runtime distribution 3500 1400 \n  3000 2500 1200 1000 Time (ms) Time (ms) 2000 800 1500 600 1000 500 400 200 Threads Variant other by \na small number of hops . This means that the di\u00adameter of the graph and the number of levels is small \n(12 for rmat20). The random graphs that we consider also have a small diameter (17 for rand23). On the \nother hand, the road networks, naturally, have a much larger diameter (6261 for USA-net). The smaller \nthe diameter of the graph the larger the number of nodes per level, and therefore the larger the amount \nof available work to be processed in parallel. Our experimental results support the above intuitions. \nFor low diameter graphs we see that the best performing synthetic variants are, mostly,leveled algorithms(v17,v18, \nv19).For USA-net which has a large diameter, the per-level paral\u00adlelism is small, which makes the synthetic \nasynchronous algorithms(v11, v12, v14)more ef.cient than others. In fact, at higher thread counts (above \n20) they manage to, marginally, outperform even the highly tuned hand-written implementations.For all \nthreevariants we use .=8. This effectively, merges a small number of levels together and al\u00adlows for \na small amount of speculation, which allows the al\u00adgorithms to mine more parallelism. Notice that, similarly \nto SSSP, all three asynchronous variants combine some static scheduling (small unrollfactor plus grouping) \nwitha good dynamic scheduling policyto achieve the best performance. The main take-away message from \nthese experiments is that no one algorithm is best suited for all inputs, especially in the domain of \nirregular graph algorithms. This validates our original assertion that a single solution for an irregular \nproblem may not be adequate, so it is desirable to have a system that can synthesize competitive solutions \ntailored to the characteristics of the particular input. For level-by-level algorithms, there is also \na spectrum of interesting choices for the worklist implementation. Elixir can deduce that BFS under the \nmetric ad scheduling policy can have only two simultaneously active priority levels, as we discussed \nin Sec. 4.5. Therefore, it can use a customized Dimension Value Ranges Group {b, NONE}Worklist {OBM, \nLOMP, BS} UnrollFactor {0, 1, 2}VC check {ALL,NONE}SC check {ALL,NONE} Table4: Dimensionsexploredby \nour syntheticBFS variants. Variant GR WL UF VC SC fPr v11 b OBM 1 .. ad/. v12 b OBM 2 .. ad/. v14 b OBM \n1 . \u00d7 ad/. v16 b OBM 0 . \u00d7 ad/. v17 b BS 0 .. ad v18 b LOMP 0 .. ad v19 b BS 0 . \u00d7 ad Table 5: Chosen \nvalues and priority functions forBFS vari\u00adants.We chose .=8.(.denotes ALL, \u00d7 denotes NONE.) worklist \nin which a bucket Bk holds work for the current level andabucket Bk+1 holds work for the next. Hence, \nwe can avoid the overheads associated with LGEN, which sup\u00adports an unbounded numberofbuckets. BS is \na worklist that canbeusedtoexploitthisinsight. Additionally,sincenonew work is added to Bk while working \non level k, threads can scan the bucket in read-only mode, further reducing over\u00adheads. Elixir exploits \nboth insights by synthesizing a cus\u00adtom worklist LOMP using OpenMP primitives. LOMP is pa\u00adrameterized \nby an OpenMP scheduling directive to explore load-balancing policies for the threads querying Bk (in \nour experiments we used the STATIC policy). 6000 (a) rmat20 (b) rand23 (c) USA-net 6000 Time (ms) \n  800 600 5000 5000 4000 4000 Time (ms) 3000 400 3000 2000 2000 200 1000 1000 124 8 16 20 24 124 8 \n162024 124 8 162024 Threads Threads Threads Forward Phase Backward Phase Dimension Ranges Ranges Group \n{a, b, NONE}{a} Worklist {LOMP, BS}{CF} UnrollFactor {0}{0} VC check {ALL,NONE}{LOCAL} SC check {ALL,NONE}{ALL,NONE} \nTable 6: Dimensions explored by the forward and backward phase in our synthetic BC variants.  6.3 Betweenness \nCentrality The betweenness centrality(BC)of a node is a metric that captures the importance of individual \nnodes in the overall network structure. Informally, it is de.ned as follows. Let G =(V, E) be a graph \nand let s, t be a .xed pair of graph nodes. The betweenness score of a node u is the percent\u00adage of shortest \npaths between s and t that include u. The betweenness centrality of u is the sum of its betweenness scores \nfor all possible pairs of s and t in the graph. The most well known algorithm for computing BC is Brandes \nalgo\u00adrithm[7]. In short, Brandes algorithm considers each node s in a graph as a source node and computes \nthe contribution due to s tothe betweennessvalueofeveryothernode u in the graphasfollows:Ina.rstphase,it \nstartsfrom s andexplores the graph forwardbuildingaDAGwith all the shortest path predecessors of each \nnode. In a second phase it traverses the graph backwards and computes the contribution to the be\u00adtweenness \nof each node. These two steps are performed for all possible sources s inthegraph.For spaceef.ciency,prac\u00adtical \napproaches to parallelize BC (e.g.[5]) focus on process\u00adingasingle source node s atatime, and parallelize \nthe above two phases for each such s. Additionally, since it is compu\u00adtationally expensive to consider \nall graph nodes as possible source nodes,theyconsideronlyasubsetof source nodes(in practice this provides \na good approximation of betweenness values for real-world graphs[4]). Variant GR WL UF VC SC fPr v1 \nNONE BS 0 (.,L) (.,.) ad v14 b LOMP 0 (.,L) (.,.) ad v15 b BS 0 (.,L) (\u00d7,\u00d7) ad v16 b LOMP 0 (.,L) (\u00d7,\u00d7) \nad v24 b LOMP 0 (\u00d7,L) (\u00d7,\u00d7) ad Table7: Chosenvaluesand priority functionsforBC variants (.denotes ALL, \n\u00d7 denotes NONE, L denotes LOCAL).For the backward phase there is a .xed range of values for most parameters \n(seeTab. 6). In the SC column the pair (F, B) denotes that F is used in the forward phase and B in the \nbackward phase. fPr is the priority function of the forward phase. InTab. 6andTab. 7,we presentthe rangeofexploredval\u00adues \nfor the synthetic BC variants and the combinations that give the best performance, respectively.We synthesized \nso\u00adlutions that perform a leveled parallelization of the forward phase and an asynchronous parallelization \nof the backward phase. In Fig. 14 we present a runtime comparison between the three best performing BC \nvariants, and a hand-written, OpenMP parallel BC implementation by Bader and Mad\u00adduri[5], whichis publiclyavailableinthe \nSSCA benchmark suite[3]. All algorithms perform the computation outlined above for the same .ve source \nnodes in the graph, i.e. they execute the forward and backward phases .ve times. The re\u00adported running \ntimes are the sum of the individual running times of all parallel loops. We observe that in the case \nof theUSA-W road network our synthesized versions manage to outperform the hand\u00adwritten code, while in \nthe case of rmat20 graph the hand\u00adwritten implementation outperforms our synthesized ver\u00adsions.Webelievethisismainlyduetothefollowing \nreason. During the forward phase, both the hand-written and synthe\u00adsized versions build a shortest path \nDAG by recording for each node u, a set p(u) of shortest path predecessors of u. (a) rmat20 30000 25000 \n20000 15000 10000 5000 Time (ms) Time (ms) Threads (b) USA-W 50000 40000 30000 20000 10000 Threads The \nset p(u) therefore contains a subset of the immediate neighbors of u. In the second phase of the algorithm, \nthe hand-writtenversionwalks theDAGbackward to update the valuesof each node appropriately.For each nodeu,it \niterates over the contents of p(u) and updates each w . p(u) appro\u00adpriately. Our synthetic codes instead \nexamine all incoming edges to u and use p(u) to dynamically identify the appropri\u00adate subset of neighbors \nand prune out all other in-neighbors. In the case of rmat graphs, we expect that the in-degree of authority \nnodes to be large, while in the road network case the maximum in-degreeis much smaller.Weexpect there\u00adfore \nour iteration pattern to be a bottleneck in the .rst class of graphs.Astraightforwardwayto handle this \nproblemis to add support in our language for multiple edge types in the graph.Byhavingexplicit predecessor \nedgesin the graph in\u00adstead of considering p(u) as yet another attribute of u, our delta inference algorithm \nwill be able to infer the more opti\u00admized iteration pattern.Weplantoaddthis supportin future extensions \nof our work. 7. Related Work We discuss related work in program synthesis, term and graph rewriting, \nand .nite-differencing. Synthesis Systems. The SPIRAL system uses recursive mathematical formulas to \ngenerate divide-and-conquer im\u00adplementations of linear transforms[29].Divide-and-conquer is used in the \nPochoir compiler[34], which generates code for .nite-difference computations, given a .nite-difference \nstencil, and in the synthesis of dynamic programming al\u00adgorithms[28]. This approach cannot be used for \nsynthe\u00adsizing high-performance implementations of graph algo\u00adrithms since most graph algorithms cannot \nbe expressed using mathematical identities; furthermore, the divide-and\u00adconquer pattern is not useful \nbecause the divide step requires graph partitioning, which usually takes longer than solving the problem \nitself. Green-Marl[16]is an orchestration lan\u00adguage for graph analysis. Basic routines like BFS and DFS \nare assumedtobe primitives writtenbyexpert programmers, and the language permits the composition of such \ntraversals. Elixir gives programmers a .ner level of control and pro\u00advidesa richersetof scheduling policies;infact,BFSis \none of the applications presented in this paper for which Elixir can automatically generate multiple \nparallel variants, com\u00adpetitive with handwritten third-party code. There is also a greater degree of \nautomation in Elixir since the system can explore large numbers of scheduling policies automatically. \nGreen-Marl provides support for nested parallelism, which Elixir currently does not support. In[23]Nguyenet \nal. de\u00adscribesa synthesis procedure forbuilding high performance worklists. Elixir uses theirworklists \nfor dynamic scheduling, and adds static scheduling and synthesis from a high-level speci.cation of operators. \nAnother line of work focuses on synthesis from logic speci.cations[17,33].The user writesa logical formulaand \na system synthesizes a program from that. These speci.ca\u00adtions are at a much higher level of abstraction \nthan in Elixir. Another line of work that focuses on concurrencyis Sketch\u00ading[32]andParaglide[35].There,thegoalisto \nstartfroma (possibly partial) sequential implementation of an algorithm and infer synchronization to \ncreate a correct concurrent im\u00adplementation. Automation is used to prune out a large part of the state \nspace of possible solutions or to verify the cor\u00adrectnessof each solution[36]. Term and Graph Rewriting. \nTerm and graph rewriting[30] are well-established research areas. Systems such as Gr\u00adGen[14],PROGRES[31]and \nGraph Programming (GP)[26] are using graph rewriting techniques for problem solving. The goals however \nare different than ours, since in that set\u00adting the goal is to .nd a schedule of actions that leads to \na correct solution. If a schedule does not lead to a solution, it fails and techniques such as backtracking \nare employed to continue the search. In our case, every schedule is a solu\u00adtion and we are interested \nin schedules that generate ef.cient solutions. Additionally, none of these systems is focused on concurrencyandthe \noptimizationof concurrencyoverheads. Graph rewriting systems try to perform ef.cient incre\u00admental graph \npattern matching using techniques such as Rete networks[8,15].Ina similar spirit, systems that are based \non data.ow constraints are trying to ef.ciently perform in\u00adcremental computations using runtime techniques[13]. \nUn\u00adlike Elixir, none of these approaches focuses on parallel ex\u00adecution. In addition, Elixir tries to \nsynthesize ef.cient incre\u00admental computations using compile-time techniques to infer high quality deltas. \n Finite-differencing. Finite differencing[24]has been used to automatically derive ef.cient data structures \nand algo\u00adrithms from high level speci.cations[9, 21]. This work is not focused on parallelism. Differencing \ncan be used to come up with incremental versionsof .xpoint computations[9]. Techniques based on differencing \nrely on a set of rules, which are most often supplied manually, to incrementally compute complicated \nexpressions. Elixir automatically in\u00adfersasoundsetofrulesforourproblemdomain, tailoredfor a given program, \nusing an SMT solver. 8. Conclusion and Future Work In this paper we present Elixir, a system that is \nthe .rst step towards synthesizing high performance, parallel implemen\u00adtations of graph algorithms. Elixir \nstarts from a high-level speci.cation with two main components: (i) a set of oper\u00adators that describe \nhow to solve a particular problem, and (ii)a speci.cationofhowto schedule these operatorstopro\u00adduce an \nef.cient solution. Elixir synthesizes ef.cient paral\u00adlel implementations with guaranteed absence of concurrency \nbugs, such as data-races and deadlocks. Using Elixir, we au\u00adtomatically enumerated and synthesized a \nlarge number of solutions for interesting graph problems and showed that our solutions perform competitively \nagainst highly tuned hand\u00adparallelized implementations. This shows the potential of our solution for \nimproving the practice of parallel program\u00adming in the complex .eld of irregular graph algorithms. As \nmentioned in the introduction, there are two main re\u00adstrictions in the supported speci.cations. First, \nElixir sup\u00adports only operators for which neighborhoods containa.xed number of nodes and edges. Second, \nElixir does not support mutations on the graph structure.We believe Elixir can be extended to handle \nsuch algorithms, but we leave this for future work. Another interesting open question is how to integrate \nElixir speci.cations within the context of a larger project that mixes other code fragments withabasic \ngraph algorithm code. Currently Elixir allows the user to insert inside an op\u00aderator fragmentsof uninterpretedC++code.Thisway,appli\u00adcation \nspeci.c logic can be embedded into the algorithmic kernel easily, under the assumption that the uninterpreted \ncode fragment does notaffect the behaviorof the graphker\u00adnel. Assessing the effectiveness of the above \nsolution and checking that consistencyis preserved by transitions to the uninterpreted mode is the subject \nof future work. Acknowledgments We would like to thank the anonymous referees and Noam Rinetzky for \ntheir helpful comments, and to Xin Sui for manyuseful discussions. References [1] 9th DIMACS Implementation \nChallenge. http://www.dis. uniroma1.it/~challenge9/download.shtml, 2009. [2] Galois system. http://iss.ices.utexas.edu/?p= \nprojects/galois, 2011. [3] D. Bader., J. Gilbert, J.Kepner, and K. Madduri. Hpcs scal\u00adable synthetic \ncompact applications graph analysis (SSCA2) benchmark v2.2, 2007. http://www.graphanalysis.org/ benchmark/. \n[4] D. A. Bader, S. Kintali, K. Madduri, and M. Mihail. Approx\u00adimating betweenness centrality. In WAW, \n2007. [5] D. A. Bader and K. Madduri. Parallel algorithms for evaluat\u00ading centrality indices in real-world \nnetworks. In ICPP, 2006. [6] M. Barnett, B. E. Chang, R. DeLine, B. Jacobs, and K. Rus\u00adtanM. Leino. Boogie:Amodular \nreusableveri.er for object\u00adoriented programs. In FMCO, 2005. [7] U. Brandes. A faster algorithm for betweenness \ncentrality. Journal of Mathematical Sociology, 25:163 177, 2001. [8] H. Bunke,T. Glauser, andT.Tran. \nAn ef.cient implementa\u00adtion of graph grammars based on the RETE matching algo\u00adrithm. In Graph Grammars \nand Their Application to Com\u00adputer Science, 1991. [9]J.CaiandR.Paige. Programderivationby.xedpoint com\u00adputation. \nSci. Comput. Program., 11(3), 1989. [10] D. Chakrabarti,Y. Zhan, andC.Faloutsos. R-MAT:A recur\u00adsive model \nfor graph mining. In In SIAM Data Mining, 2004. [11] T. Cormen, C. Leiserson, R. Rivest, and C. Stein, \neditors. Introduction to Algorithms. MIT Press, 2001. [12] L. De Moura and N. Bj\u00f8rner. Z3: an ef.cient \nSMT solver. In TACAS, 2008. [13] C. Demetrescu, I. Finocchi, and A. Ribichini. Reactive im\u00adperative programming \nwith data.owconstraints. In OOPSLA, 2011. [14] R. Gei, G. Batz, D. Grund, S. Hack, and A. Szalkowski. \nGr\u00adgen:Afast spo-based graph rewriting tool. In Graph Trans\u00adformations, 2006. [15] A. H. Ghamarian, A. \nJalali, and A. Rensink. Incremental pattern matching in graph-based state space exploration. In GraBaTs, \n2010. [16] S. Hong, H. Cha., E. Sedlar, and K. Olukotun. Green-marl: a DSL for easy and ef.cient graph \nanalysis. In ASPLOS, 2012. [17] S. Itzhaky, S. Gulwani, N. Immerman, and M. Sagiv. A simple inductive \nsynthesis methodology and its applications. In OOPSLA, 2010. [18] M.Kulkarni,M. Burtscher,C. Cascaval, \nandK. Pingali. Lon\u00adestar:Asuiteof parallel irregular programs.In ISPASS, 2009. [19]C.E. LeisersonandT.B. \nSchardl. Awork-ef.cient parallel breadth-.rst search algorithm (or how to cope with the non\u00addeterminism \nof reducers). In SPAA, 2010. [20] A. Lenharth, D. Nguyen, and K. Pingali. Priority queues are notgood \nconcurrent priority schedulers.TechnicalReportTR\u00ad11-39, UT Austin, Nov 2011. [21] Y. A. Liu and S. D. \nStoller. From datalog rules to ef.cient programs with time and space guarantees. ACM TOPLAS, 31(6), 2009. \n[22] U.Meyer andP. Sanders. .-stepping:Aparallelizable short\u00adest path algorithm. J. Algorithms, 49(1):114 \n152, 2003. [23] D. Nguyen and K. Pingali. Synthesizing concurrent sched\u00adulers for irregular algorithms. \nIn ASPLOS, 2011. [24] R. Paige and S. Koenig. Finite differencing of computable expressions. ACM TOPLAS, \n4(3):402 454, 1982. [25] K. Pingali, D. Nguyen, M. Kulkarni, M. Burtscher, M. A. Hassaan, R. Kaleem, \nT. H. Lee, A. Lenharth, R. Manevich, M. M\u00b4endez-Lojo, D. Prountzos, and X. Sui. The TAO of parallelism \nin algorithms. In PLDI, 2011. [26] D. Plump. The graph programming language GP. In CAI, 2009. [27] D. \nPrountzos, R. Manevich, and K. Pingali. Elixir:Asystem for synthesizing concurrent graph programs.Technical \nReport TR-12-16, UT Austin, Aug 2012. [28] Y. Pu, R. Bodik, and S. Srivastava. Synthesis of .rst-order \ndynamic programming algorithms. In OOPSLA, 2011. [29] M.P\u00a8uschel,J.M.F. Moura,J. Johnson,D.Padua,M.Veloso, \nB. Singer, J. Xiong, F. Franchetti, A. Gacic, Y. Voronenko, K. Chen, R. W. Johnson, and N. Rizzolo. SPIRAL: \nCode generation for DSP transforms. Proceedings of the IEEE, 2005. [30] G. Rozenberg, editor. Handbook \nof graph grammars and computing by graph transformation: Volume I. Foundations. World Scienti.c Publishing \nCo., Inc., 1997. [31] A. Sch\u00a8urr, A. J.Winter, and A.Z\u00a8undorf. Handbook of graph grammars and computing \nby graph transformation. chapter The PROGRES approach: language and environment.World Scienti.c Publishing \nCo., Inc., 1999. [32] A. Solar-Lezama, C.G. Jones, and R. Bodik. Sketching con\u00adcurrent data structures. \nIn PLDI, 2008. [33] S. Srivastava,S. Gulwani, andJ.Foster. From programveri.\u00adcation to program synthesis. \nIn POPL, 2010. [34]Y.Tang,R.A. Chowdhury,B.C.Kuszmaul, C.K. Luk, and C. E. Leiserson. The Pochoir stencil \ncompiler. In SPAA,2011. [35] M.Vechev and E.Yahav. Deriving linearizable .ne-grained concurrent objects. \nIn PLDI, 2008. [36] M.Vechev, E.Yahav, and G.Yorsh. Abstraction-guided syn\u00adthesis of synchronization. \nIn POPL, 2010. A. Betweenness Centrality Fig. 15 shows an Elixir program for solving the betweenness\u00adcentrality \nproblem. The Elixir language allows a programmer to specify in\u00advariants (in .rst-order logic) and use \nthem to annotate ac\u00adtions, as shown in lines 44 and 45. (We avoided discussing annotations until this \npoint to simplify the presentation of 1 Graph[ nodes(node : Node dist : int 2 sigma : double delta : \ndouble 3 nsuccs : int preds : set[Node], 4 bc: double bcapprox : double) 5 edges(src : Nodedst : Node \ndist : int ) 6] 7 8 source : Node 9 10 // Shortest path rule . 11 SP =[ nodes(node a, dist ad) 12 nodes(node \nb, dist bd, sigma sigb, preds pb, nsuccs nsb) 13 edges(src a, dst b) 14 (bd > ad+ 1) ] . 15 [bd =ad +1 \n16 sigb =0 17 nsb =0 18 ] 19 20 // Record predecessor rule . 21 RP =[ nodes(node a, dist ad, sigma sa, \nnsuccs nsa) 22 nodes(node b, dist bd, sigma sb, preds pb) 23 edges(src a, dst b, dist ed) 24 (bd ==ad+1)&#38;(ed \n!= ad)] . 25 [sb =sb+sa 26 pb =pb+ a 27 nsa = nsa+1 28 ed =ad 29 ] 30 31 // UpdateBC rule. 32 updBC = \n[nodes(node a, nsuccs nsa, delta dela, sigma sa) 33 nodes(node b, nsuccs nsb, preds pb, bc bbc, 34 bcapprox \nbbca, delta delb, sigma sb) 35 edges( src a, dst b) 36 (nsb==0&#38;a in pb) ] . 37 [ nsa =nsa -1 38 dela \n= dela+ sa / sb * (1 + delb) 39 bbc = bbc -bbca+ delb 40 bbca = delb 41 pb=pb-a 42 ] 43 44 backwardInv \n: .a : Node,b : Node :: a . preds(b)=.\u00ac(b . preds(a)) 45 Forward = iterate (SP or RP)\u00bb metric ad \u00bb fuse \n\u00bb group b 46 Backward = iterate {backwardInv} updBC \u00bb group a 47 main = Forward; Backward Figure 15: \nElixir program for betweenness centrality. statements.) Our compiler adds these invariants as con\u00adstraints \nto the query programs to further optimize the in\u00adference of useful in.uence patterns. Additionally, we \ncould use these invariants to optimize the redexes procedure in order to avoid scanning the entire graph \nto .nd redexes. In all of our benchmarks this optimization would reduce the search for redexes to just \nthose including the source node.    \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Algorithms in new application areas like machine learning and network analysis use \"irregular\" data structures such as graphs, trees and sets. Writing efficient parallel code in these problem domains is very challenging because it requires the programmer to make many choices: a given problem can usually be solved by several algorithms, each algorithm may have many implementations, and the best choice of algorithm and implementation can depend not only on the characteristics of the parallel platform but also on properties of the input data such as the structure of the graph. One solution is to permit the application programmer to experiment with different algorithms and implementations without writing every variant from scratch. Auto-tuning to find the best variant is a more ambitious solution. These solutions require a system for automatically producing efficient parallel implementations from high-level specifications. Elixir, the system described in this paper, is the first step towards this ambitious goal. Application programmers write specifications that consist of an <i>operator</i>, which describes the computations to be performed, and a <i>schedule</i> for performing these computations. Elixir uses sophisticated inference techniques to produce efficient parallel code from such specifications.</p> <p>We used Elixir to automatically generate many parallel implementations for three irregular problems: breadth-first search, single source shortest path, and betweenness-centrality computation. Our experiments show that the best generated variants can be competitive with handwritten code for these problems from other research groups; for some inputs, they even outperform the handwritten versions.</p>", "authors": [{"name": "Dimitrios Prountzos", "author_profile_id": "81388601660", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P3856099", "email_address": "dprountz@cs.utexas.edu", "orcid_id": ""}, {"name": "Roman Manevich", "author_profile_id": "81100232411", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P3856100", "email_address": "roman@ices.utexas.edu", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "The University of Texas at Austin, Austin, TX, USA", "person_id": "P3856101", "email_address": "pingali@cs.utexas.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384644", "year": "2012", "article_id": "2384644", "conference": "OOPSLA", "title": "Elixir: a system for synthesizing concurrent graph programs", "url": "http://dl.acm.org/citation.cfm?id=2384644"}