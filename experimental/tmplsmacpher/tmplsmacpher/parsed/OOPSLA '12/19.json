{"article_publication_date": "10-19-2012", "fulltext": "\n A Black-box Approach to Understanding Concurrency in DaCapo Tomas Kalibera Matthew Mole Richard Jones \nJanVitek University ofKent, Canterbury Purdue University Abstract Increasing levels of hardware parallelism \nare one of the main challenges for programmers and implementers of managed runtimes. Any concurrency \nor scalability improvements must be evaluated experimentally. However, application benchmarks available \ntoday may not re.ect the highly con\u00adcurrent applications we anticipate in the future. They may also behave \nin ways that VM developers do not expect. We provide a set of platform independent concurrency-related \nmetrics and an in-depth observational study of current state of the art benchmarks, discovering how concurrent \nthey re\u00adally are, how they scale the work and how they synchronise and communicate via shared memory. \nCategories and Subject Descriptors D.3.3[Programming Languages]: Language Constructs and Features Concur\u00adrent \nProgramming Structures Keywords Benchmarks, DaCapo, concurrency, scalability 1. Introduction Inthefaceof \ntechnologicalandphysical limitationsprevent\u00ading further clock speed increases, hardware designers have \nturned to providing processors with increasing numbers of cores Intel has 48-core processors, Tilera \n64-core pro\u00adcessors and Azul ships 54-core \u00d7 16 processor systems. All these systems provide shared memory \nand varying degrees of coherency. To keep delivering ever more powerful appli\u00adcations, programmers must \nturn their attention to making good use of those cores. This means not only parallelising their algorithms, \nbut also avoiding timing accidents due to non-local memory accesses or cache coherencytraf.c. Man\u00adaged \nlanguage runtimes, or virtual machines (VM), lift some of thisburden. High-level concurrencylibraries \nand runtime Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n12 October 19 26, 2012,Tuscon, Arizona, USA. Copyright c &#38;#169;2012ACM 978-1-4503-1561-6/12/10... \n$10.00 services, such as just-in-time compilation and garbage col\u00adlection (GC), make it much easier to \nwrite code that runs ef.ciently on a variety of platforms. To achieve this, VMs are becoming increasingly \naware of architectural issues that affect scalability. Garbage collectors may be NUMA-aware when allocating \nmemory, take advantage of multiple cores to speed up memory reclamation andkeep thread-local objects \ntogether to reduce cache coherencytraf.c. If researchers are to develop VMs that meet the chal\u00adlenges \nset by highly parallel hardware architectures, they need to know what multi-threaded programs really \ndo. VM development is often motivated by the performance of a given system on some suite of well known \napplications. Such benchmarks in.uence development. Theymay accel\u00aderate, retard or misdirect energy and \ninnovation [6]. The questions for research on multi-and many-core hardware are how new designs and implementations \nscale with increas\u00ading numbers of cores. Modern architectures have complex performance models. When these \nare coupled with dynamic code generation and memory management techniques that move data on the .y, it \nbecomes very challenging to predict how a particular program will perform and how it will scale. We argue \nthat a necessary starting point is for researchers to understand how the benchmarks they use stress the \nvarious components of a platform (e.g. shared memory access, cache coherency, synchronisation, GC). We \naim to provide insights on the suitability of bench\u00admarks, particularly those written in Java, for scalability \nstud\u00adies on parallel hardware. We distinguish the terms concur\u00adrency and parallelism. Concurrency is \na software engineer\u00ading tool to model real-world systems that has long been used as a programming model, \neven on sequential hardware [23]. Concurrent programs can offer better responsiveness [7] by leveraging \nthe scheduler. Concurrency is also a mechanism for achieving improved throughput on parallel hardware.We \nare interested in the latter. In Java, threads typically commu\u00adnicate via shared memory. The impact on \nthroughput and scalability of communication through shared memory is im\u00adportant for both application \nprogrammers and VM designers. Communication may lead to contention, both for software\u00adlevel constructs \nsuch as locks, and hardware artefacts such as cache-lines. VM implementors work hard to reduce the costs \nincurredby communication,butin orderto designnew optimisations they need benchmark suites that provide \nrep\u00adresentative models of applications in the wild , and insights into how these benchmarks use shared \nmemory.  An underlying principle of this study is platform inde\u00adpendence: to explore how benchmarks \nmay behave on any hardware platform and any language runtime rather than on just those currently at hand. \nWe use a combination of observational techniques based on program instrumenta\u00adtion and on measurements \nof platform independent metrics. We measure, for instance, the frequency at which different threads access \na shared object (application behaviour) rather than measuring hardware performance counters (the con\u00adsequences \nof an application s behaviour given a particular scheduling on a particular platform). We focus on Java-level \nmemory operations that are spec\u00adi.ed in the source code. These include reads and writes to .elds, monitor \nacquisitions/releases and object alloca\u00adtions. Memory operations may stress concurrency mecha\u00adnisms in \ntwo ways. First, objects may be accessed by multi\u00adple threads. Second, these may access different objects \nthat happen to collide in the memory hierarchy, a property known as false sharing. Both can be the source \nof non-local mem\u00adory access and/or cache coherencytraf.c.False sharing may have a signi.cant impact on \nperformance [18] but, since we are interested in the platform independent behaviour of applications, \nwe do not focus on it. The stress induced by shared access may be trivial unless an object is used by \nmul\u00adtiple threads repeatedly within some time window. The cost of such regular accesses depends on the \nnumber of changes of ownership of the object. This can happen through mod\u00adi.cations, typically writes \nand monitor acquisitions. We in\u00advestigate the extent to which more than one thread accesses a shared \nobject and patterns of these accesses. Scalability is a core metric for evaluating the performance of \nparallel systems.To measure scalability, onewould runa parallel benchmark several times, on a varying \nnumber of cores. One expects a scalable benchmark to divide the work equally between the available cores, \ngiving a nearly linear speed-up. We investigate whether benchmarks that are used to study parallel systems \nare indeed scalable. We ask how manythreads in these benchmarks take part in concurrency\u00adstressing memory \noperations. We offer a detailed observational study of the DaCapo Java workloads [6, 20]. Our goal is \nto provide platform\u00adindependent insights into (a) how concurrent these work\u00adloads really are, (b) how \nto characterize the patterns of concurrent behaviour exhibited by the individual programs, (c) the extent \nto which they can be used for testing the scala\u00adbility of VMs, and (d) the stress theyput on memory and \npar\u00adticularly shared memory. The source code of our implemen\u00adtations is at www.cs.kent.ac.uk/projects/gc/dacapo. \n2. Related Work Dufour et al. proposed a range of dynamic metrics to char\u00adacterise Java programs in terms \nof memory use, concur\u00adrency, and synchronisation [12]. They measured concur\u00adrency by thread density, \nthe maximum number of threads ever runnable at the same time, and by the amount of code executed while \na given number of threads are runnable. We believe that code is not necessarily the most revealing ob\u00adservable \nof a concurrent system as, for example, a loop that only operates over local variables does not really \npose any interesting challenges to a virtual machine and can be run in parallel with pretty much anything. \nAn arguably better met\u00adric is to study memory operations on data that are actually (or potentially) shared. \nFurthermore, we posit that only the threads that do a substantial amount of memory access are relevant. \nConsider the case of a benchmark with a large num\u00adber of threads that poll on network connections. These \nare always runnablebutdo nearly no actualwork.A betterway to get an upper bound on contention might be \nto measure the number of monitor hand-offs between threads. Dufour et al. also argue that metrics should \nbe robust (a small change in behaviour leads to a small change in value), discriminating (a large change \nleads to a large change) and platform independent.We set out to follow their recommen\u00addations, while \nnoting that robustness and discrimination are somewhat hard to de.ne and some degree of platform de\u00adpendence \nis unavoidable (at least due to scheduling). Gidra et al. measured the scalability of certain runtime \ncomponents, such as the various garbage collection algo\u00adrithms, in the HotSpot Java VM [14]. Theyfound \nthat the cu\u00admulative stop-the-world pause times increase with the num\u00adber of threads as does the total \ntime spent in GC. They claim that most object scanning and copying is done by a GC thread running on \na remote node. Although these results suggest an alarming lack of scalability, they need to be con\u00ad.rmed \nwith the NUMA-aware version of the HotSpot GC which was not used in the study. Chen et al. [8] provide \nan observational scalability analysis of HotSpot. They explain some scalability issues by stalls at the \nhardware level (cache misses, DTLB misses, pipeline misses and cache-to-cache transfers) and measure \nthe bene.ts of thread-local allocation buffers and a HotSpot heuristic for determining a good size of \nthe young generation. They .nd the bene.ts to be appli\u00adcation dependent,but notveryVM dependent. In terms \nof program instrumentation, we followed in the footsteps of Binder et al. [4]. Their approach differs \nin that theystatically instrument core classes that are loaded before an agent can take control, which \nallows them to add .elds to these classes.We use an agent instead, both to re-instrument classes already \nloaded and to instrument new classes as they load.  1 4 8 163264 Processors (Driver Threads) (a) 64-core \n4-node AMD; HotSpot 1.7. 1 4 8 163264 (b) AzulVega3, 864 processors; AzulVM 1.6. 1 4 8 16 32 64 128 \n256 512 1024 (c) AzulVega3, 864 processors; AzulVM 1.6. 0.2 0.51 2 5102040 1024 0.5 1 2 5 10 20 4064 \n0.5 1 2 5 10 20 4064 3. DaCapo Blackburn et al. [6] introduced the DaCapo benchmarks in 2006 and provided \nperformance measurements and work\u00adload characteristics, such as object size distributions, allo\u00adcation \nrates and live sizes. The original1 suite consists of the following benchmarks: antlr6 generates a parser \nand lex\u00adical analyzer, bloat6 performs a number of optimisations and analyses on Java bytecode .les, \nchart6 plots line graphs and renders PDF, eclipse6 runs tests for the Eclipse IDE, fop6 generates a PDF \n.le, hsqldb6 executes a number of transactions, jython6 interprets thepybench Python bench\u00admark, luindex6 \nindexes a set of documents, lusearch6 searches for keywords, pmd6 analyzes a set of Java classes and \nxalan6 transforms XML documents into HTML. The DaCapo 2009 suite updated a few of the original bench\u00admarks, \neclipse9, fop9, jython9, luindex9, lusearch9, pmd9 and xalan9, as well as introducing new applications: \navrora9 is a simulation program, batik9 produces SVG images, h29 executes a number of transactions, sunflow9 \nrenders a set of images using ray tracing, tomcat9 runs a set of queries against a web server, tradebeans9 \nand trade\u00adsoap9 run the daytrader benchmark. Multi-threading is not widely used in DaCapo 2006, with \nonly three programs using more than one thread(lu\u00adsearch6, hsqldb6 and xalan6). DaCapo 2009 supports \nscaling of seven benchmarks to an arbitrary number of driver threads with identical copies of the code \non subproblems. By default, the suites use the number of logical processors in the system. This does \nnot mean, however, that the benchmarks scale well. There are two ways to evaluate scalability vary either \nthe number of physical cores available to the whole system (operating systems allow this), or the number \nof driver threads. The former has been used [8] but the lat\u00adter is more convenient [14]. The implied \nassumption in both cases is that the amount of work done remains the same. When run on a 64-core AMD \nsystem, sunflow9 keeps improving as cores are added. Figure 1(a) shows that only sunflow9 and tomcat9 \nare able to get more than 10\u00d7speed up. tradesoap9 barely gets more than a 3\u00d7 improvement. tradebeans9 \nand h29 actually report degraded throughput with higher thread counts. For comparison, Figure 1(b) re\u00adports \nthe result on an Azul system. Here, scalability is not as good. Only three benchmarks get a speedup better \nthan 5\u00d7(sunflow9, xalan9, lusearch9). tomcat9 scales better on the AMD. Absolute throughput numbers are \nalso much higher on AMD. Azul numbers for more than 64 threads are shown in Figure 1(c). Other than sunflow9 \nwhich sees small improvements up to 256 threads, performance is ei\u00adther stable or decreases with more \ncores(h29 is striking). Of the seven 09 benchmarks that do not support driver-thread scaling, avrora9, \neclipse9, pmd9 and luindex9 are multi\u00adthreaded. 1We distinguish versions of DaCapo by a 6 (2006) or 9 \n(2009) subscript. Figure 1: Speedup v. number of driver threads, Dacapo 09.  period  Metric Density \n/ any Periodic density / any Density / shared Periodic density / shared Density / spot-shared Periodic \ndensity / spot-shared Value 3 { 1 { 2 { 1 {   }{ 1 { 0 { }{ } } } }{  } } }{ } reset Figure 2: \nConcurrency metrics. Each solid symbol in the timeline is an operation performed by a thread on some \nobject. Different symbols denote operations by different threads. We count the number of threads that \nperform operations (open symbols) in some intervals. The value of a metric is the median of the cardinalities \nof these sets of operations performed. 4. Metrics for Concurrency We devised new metrics to characterize \nhow concurrent ap\u00adplications use shared memory. Communication patterns be\u00adtween threads are key to understanding \nconcurrent multi\u00adthreaded applications. In Java, this is typically achieved through shared memory and \nsynchronization.2 Thus our measurements focus on memory locations that are shared by multiple threads \nand on the choice of mechanisms for synchronization between threads. 4.1 Characterising Concurrency \nOccasional communication adds little overhead so we fo\u00adcus on those threads that contribute signi.cantly \nto the work done by a benchmark in some interval, e.g. the entire exe\u00adcution or, of more interest, some \nshorter interval. If we are interested in which phases of execution threads are active, the metric might \nbe to count how many threads contribute signi.cantly (95% of the operations) in each 100ms period and \nthen compute the median. Objects may be shared only transiently and otherwise accessed by only one thread \nat a time. As widely spaced operations on an object by different threads are unlikely to be interesting, \nwe might rather con\u00adsider only operations by different threads in some small spot interval. To capture \nthis, a metric might reset the status of spot-shared objects to local , or unshared, at regular inter\u00advals \n(say, every 10ms), and then count only sharing within these intervals. We start with an example, computing \nsome of our metrics for the trace in Figure 2, which shows some operations, say 12 writes, performed \non a single object by three threads (de\u00adnoted by , and _). We motivate our choice of metrics as follows. \n2Synchronization mechanisms include synchronized statements, bar\u00adriers with wait /notify, volatile variables, \natomic classes from the java.util.concurrent.atomic package and other high level abstrac\u00adtions such futures \nor concurrent queues.  Density / any counts all the different threads that con\u00adtribute signi.cantly \nto each operation on an object through\u00adout the execution. In Figure 2, three threads contribute. However, \nas we discussed earlier, we want to exclude operations that are not particularly interesting for con\u00adcurrency. \n Density / shared We are interested in the impact of com\u00admunication between threads. This metric also \ncomputes thread density, but only for objects that have become shared. In Figure 2, the object is not \nshared initially so we count only the operations performed by the and _threads.  Periodic density / \nany This metric captures how threads maybe activeindifferent phases.Forexample,an initial\u00adisation phase \nmay construct an object which is then used in later phases.To capture this, wedivideexecution into periods, \ncompute the thread density in each one and re\u00adport the median. In the example, we see that 1, 1 and 2 \nthreads act on the object in each period respectively; the periodic density is 1.  Periodic density \n/ shared A single thread might only initialise an object but then hand it on to another thread for use. \nThis pattern is unlikely to stress concurrency mechanisms indeed, the lifetimes of the two threads may \nnot even overlap. This metric tends to exclude such behaviour by computing the periodic thread density \nfor objects that have become shared. In our example, there are0,1and2operationson shared objectsin each \nperiod respectively, so the median is 1.  Density / spot-shared We are particularly interested in the \npressure placed upon concurrencymechanisms by ob\u00adjects whose ownership changes rapidly. Examples might \ninclude contended locks, volatiles and other objects used for synchronization. Spot-sharing resets objects \nshared status regularly. This metric measures the thread density   of recently (i.e. spot-shared) objects \nonly. For example, in Figure 2, only the thread discovers a recently shared object. Periodic density \n/ spot-shared Finally, we are interested in the concurrency pressure in different phases of the program. \nThus, this metric computes the period density of spot-shared objects.  4.2 Traces Some metrics require \nprecise de.nition to understand what behaviours we capture. In the de.nitions below, we assume the presence \nof a full trace of the program, although our im\u00adplementation is mostly on-the-.y. Let a log Lbe a sequence \nof time-stamped operations: allocations, reads, writes and monitor enters. t L ={a o : thread t allocated \nobject o at time t} t t .{r o : thread t read from object o at time t} t t .{w o : thread t wrote to \nobject o at time t} t t .{e o : thread t entered monitor of object o at time t} t We assume that no \ntwo operations can happen at the same time. We use the meta-variable a to range over operations. Filter \nL.T selects operations from a time interval T: L.T={at o . L : t . T} t To select only operations of \nspeci.c type, e.g. reads, we write L.r . To select operations of more than one type from L, say reads \nand writes, we write L.rw . We write L* for a set {L1,L2,...,Ln}. Filter on L* extends to its inner-sets: \nL*.T = {L1.T,L2.T,...,Ln.T}. 4.3 Operations We can describe numbers of individual operations and their \nratios interesting as metrics of workload intensity and of the balance of operations such as: total \nnumber of reads |L.r | ratio of reads and writes |L.r |/|L.w | We further split the counts into operations \non arrays, object instance .elds and static .elds.We also compute the rate of the different operations. \nOperations are counted in instru\u00admented benchmarks, where overheads are large, but rates are computed \nby dividing them by the execution time of the uninstrumented benchmark. Execution times obviously de\u00adpend \non the clock speed ofthe hardware used.To minimise this dependency, we normalise rates. That is, rather \nthan re\u00adporting the absolute rate x of an operation in a benchmark, we sometimes give the normalised \nrate (x - \u00b5)/s where \u00b5 is the mean across all benchmarks and s2 the variance. This transformsthe distributionto \none with mean0andvari\u00adance 1. 4.4 Aging Object behaviouris wellknowntovaryby age.Wewantto understand \nthe interaction between object age and concur\u00adrency related attributes such as sharing, the age at which \nan object is accessed, and so on.We measure age in the number of timer ticks (with a period of 10ms), \nwhich we adjust to an approximate time in uninstrumented execution as follows: timeu approx.time = ticks \n\u00d7 period \u00d7 timei where timeu and timei are the uninstrumented and instru\u00admented execution times of the \nbenchmark respectively.  4.5 Shared Memory Accesses Shared memory accesses are one of thekey observables \nfor this work. We describe them using the following functions. Ashared access is an operation for which \nthe object accessed was previously allocated, written to or synchronized on by another thread. t ' ShrR(L) \n={rto . L : .att '' o . L.[0,t) . t .t} = t ' ShrW (L) ={wto . L : .att '' o . L.[0,t) . t .= t} t ' \nShrE(L) ={ett '' o . L.[0,t) . t .t} o . L : .at = A write or monitor entry is said to be alternating \nif the immediately preceding access (excluding reads) was by a different thread. o . L : .at ' t AltW \n(L)={w t ' o . L.[0,t),wea . t ' t .t ..att '' '' o . L.(t ' ,t),wea} t AltE(L)={eto . L : .att '' o \n. L.[0,t)wea . .at '' ' t .t .. t '' o . L.(t ' ,t),wea = } We de.ne additional functions to select \nshared operations on objects that have been accessed by a different thread re\u00adcently. For this we use \ntime windows of a constant length, o. In practice we use periods of 10ms of instrumented exe\u00adcution. \nAs the instrumentation overhead is large, this corre\u00adsponds to a much shorter interval of uninstrumented \nexecu\u00adtion. t ' SpotShrR(L)={rto . L:.att '' o . L.[o.t/o.,t] .t .= t} t ' SpotShrW (L)={wto . L:.att \n'' o . L.[o.t/o.,t] .t .= t} t ' SpotShrE(L)={wto . L:.att '' o . L.[o.t/o.,t] .t .= t} We de.ne spot-shared \noperations based on what has al\u00adready happened in their time-window, L[o.t/o.,t], which allows ef.cient \nimplementation with periodic timer. A sys\u00adtem recording a full trace could de.ne spot-sharing based on \nwhat happened up to o (seconds) before the operation of interest, L.[t-o,t]. The de.nitions allow metrics \nfor numbers of shared op\u00aderations of each type and for their ratios. Examples include (more in our results \nsection):  total number of shared reads |ShrR(L)| total number of spot-shared writes |SpotShrW (L)| \nratio of shared reads and writes |ShrR(L)|/|ShrW (L)| Before attempting to optimise a system, it is \noften im\u00adportant to understand how contributions to a metric are dis\u00adtributed. Commonly,afew elements \ndominate.We de.ne the function covers that tests whether a set of threads T covers a signi.cant fraction \n.of operations in a given log L: covers(T,L)=|L.T |=.|L| The covset function returns the smallest subset \nof threads that cover a signi.cant number of operations in L: covset(L)= min{. . 2T : covers(.,L)} This \nfunction gives us the most active threads in the set, whichis usefulin understanding the program.Forexample, \nwe might report the smallest age for which . = 95% of all accesses are to objects younger than that age. \nWe de.ne the thread density of a set of operations as the size of the covering set: density1(L)=|covset(L)|.We \nfurther de.ne the density of multiple sets of operations, so that we can cover each individual type of \noperation (i.e. reads, writes). The generalized function is:   density(L * )= covset(L) L.L* Note \nthat density is trivially not equivalent to the density of the union of the operations density1(.L*). \nThis alternative de.nition is possible, but our de.nition has the advantages that it does not give preference \nto anykind of operation (any element of L*), and it is easier to implement. We base the following concurrencymetrics \non density: thread density with any operations: density({L.r ,L.w ,L.e,L.a})  thread density with shared \noperations:  density({ShrR(L),ShrW (L),ShrE(L)}) thread density with alternating modi.cations: density({AltW \n(L),AltE(L)}) thread density with spot-shared operations: density({SpotShrR(L),SpotShrW (L),SpotShrE(L)}) \nShared activity may be concentrated in only a short inter\u00adval of a program s execution. The level of \ntrue concurrency will be lower than the density if the activities of some of the threads do not overlap. \nTo better capture this, we consider thread activity in each of a number of small intervals, or pe\u00adriods \n we use periods of 100ms of instrumented execution. Formally, we de.ne periodic density to focus on time \nwin\u00addows of a constant length d> 0. We summarise densities using medians, () pdensity(L * )= median density(L \n* .[k\u00b7d,(k+1)\u00b7d)) k.N Based on pdensity, we de.ne the following metrics: periodic thread density with \nany operations: pdensity({L.r ,L.w ,L.e,L.e})  periodic thread density with shared operations:  pdensity({ShrR(L),ShrW \n(L),ShrE(L)}) periodic thread density with alternating modi.cations: pdensity({AltW (L),AltE (L)}) periodic \nthread density with spot-shared operations: pdensity({SpotShrR(L),SpotShrW (L),SpotShrE(L)}) Any value \nreturned is an upper-bound on the true concur\u00adrency, because our window can always be so large that it \nregards sequential activity as concurrent. d and o should be as small as possible, but d must be much \nlarger than o for spot-metrics to be stable.  4.6 Concurrency Patterns of Shared Accesses We explore \na number of common concurrency patterns fur\u00adther. One such pattern is CREW: concurrent read, exclusive \nwrite. Another is unique ownership, where only one thread owns an object at a time and a thread only \naccesses ob\u00adjects thatitowns.A particularexample mightbe thata pro\u00adducer thread creates an object before \npassing it to consumer threads. A special case is a stationary object, which is cre\u00adatedby one thread \nand then readby otherthreadsbut never written again. An object o is stationary in trace L at time t if \nit is never written after being read, including if it has never been written at all. An object o is single-writer \nin trace Lat time t if it has been written to by at most one thread before t: readonly(o,t,L)=|L.[0,t],w \n|=0 writeonly(o,t,L)=|L.[0,t],r |=0 stationary(o,t,L)= writeonly(o,t,L) . ()() tt max wt '' o . L.[0,t]< \nmin r ' o . L.[0,t] t ' t ' t '' ' t ' singlewriter(o,t,L)=|{t : .wt ' o . L.[0,t]}|= 1 A thread t is \nthe owner of an object o at time t if no other thread has accessed the object since t s last access. \nAccess a to an object o by a thread t is same-owner when t owns o: owner(t,o,t,L)=.att ' o . L.[0,t]: \n () '' t <t ..att '' ' o . L.[t ' ,t] : t = t sameowner(ato,L)= owner(t,o,t,L) t Based on these de.nitions, \nwe say that a shared read is read-only if it is to a read-only object, stationary if it is toa stationary \nobject, single-writer ifitistoa single-writer object and same-owner if sameowner holds. Similar def\u00adinitions \napply to writes. We now de.ne .ve disjoint shar\u00ading patterns that cover all shared accesses, and we use \nthese to characterise individual benchmarks and individual access types (all reads and writes, reads \nand writes separately, stat\u00adics, arrays, etc): S1, shared read-only/write-only accesses: read-only reads \nand write-only writes S2, additional shared stationary accesses: stationary accesses that are not S1 \n S3, other shared, single-writer accesses: single-writer accesses that are not S1 or S2  S4, other \nshared, same owner accesses: same-owner accesses that are not S1, S2 or S3  S5, all remaining shared \naccesses:  all shared accessed not S1-S4 Note that read-only reads are also stationary. While not a \nproperty of the de.nitions, in DaCapo stationary reads are also single-writer. Hence, S1+S2 includes \nall stationary reads and S1+S2+S3 includes all single-writer reads. S2 is an empty set for writes because \nall stationary writes are also write-only by de.nition. Hence S1+S2 includes all station\u00adary accesses \neven for writes. Not all write-only writes are single-writer. For example, when an array is used to collect \nresults from multiple threads, the collection will be formed by write-only writes. To obtain the disjoint \ngroups S1-S5, in our implementa\u00adtion we count shared accesses that are read-only/write-only, stationary, \nsingle-writer, both single-writer and stationary, same-owner but neither stationary nor single-writer, \nand all shared accesses (of anypattern).  4.7 Shared-Used and Shared-Reachable It is often interesting \nfor implementers to know which ob\u00adjects were actually accessed by multiple threads as well as which objects \ncould possibly have been accessed by multi\u00adple threads. For this purpose we will de.ne the notions of \nshared-used for an object that is used by multiple threads and shared-reachable for an object that could \nbe accessed by multiple threads based on reachability. 5. Measurement Methodology We measure behaviour \nthrough instrumenting Java programs and the VM with two independent infrastructures, one that depends \nsolely on bytecode instrumentation and the other inside JikesRVM. 5.1 Bytecode Instrumentation Java \ns dynamic instrumentation feature allows instrumenting the bytecode of any class that the VM loads. The \nlogic of our probes is implemented in Java and runs in the same VM, but we isolate its direct impacts \nfrom our measurements.We customised btrace [2] for our needs. Design. We insert probes at instructions \nof interest, such as .eld accesses, calls or synchronisation points. A probe might update an object-related \nmetadata structure (e.g. to mark the object as shared ) stored in a hash table. A probe might also increment \na thread-related thread-local counter. We designed our own hash table for the metadata as we require \nfast look-up, safety in the presence of concurrent access and realistic accounting. Eachbucket(a collision \nset) has a separate self-organising single-linked queue for each thread. When looking for metadata of \na given object, a probe locates the correct bucket and then searches the queue of the current thread. \nIt will .nd an entry there for any thread\u00adlocal object and also for anyshared object that the thread \nhas accessed before. Each entry contains a weak reference to the object it represents and a regular reference \nto metadata for that object. Periodically, entries of dead objects are detected, processed and removed \nfrom the queues. Whenever an entry is found in the local queue it is moved to the front (self-organisation) \nfor performance rea\u00adsons. If the entry is not found in the local queue, queues of other threads are searched \n(again, the search order is self\u00adorganising, .rst searching queues of threads where entries had been \nfound recently). Once found, the entry is copied to the local queue forfaster access next time. The only \nsyn\u00adchronization needed is between a thread moving an entry to the head of the local queue and a remote \nthread scanning the queue. In the rare case of contention, remote threads spin but local threads give \nup moving an entry to the front of the queue. A dedicated timer thread samples thread-based counter values \nand prints them to a .le every 100ms (instru\u00admented time). The timer thread is paused during the (stop\u00adthe-world) \nGC, excluding it from our measurements. Obstacles. To circumvent the 64KB Java method size limit, we \noptimised btrace to generate fewer instructions. We disabled instrumentation for static initialisers \nfor a few constant arrays in batik and fop. We disabled the byte\u00adcode veri.er in order to insert probes \nafter the new bytecode but before the constructor s <init> method. We modi.ed btrace to instrument libraries \nbefore running the application to increase precision of tracing, and so that its own activity canbe isolated.We \nalsoextendedittokeepa cacheof .eld modi.ers known at instrumentation time and to pass class references \nto probes, so that our probes can always deter\u00admine if an access is to a volatile .eld. Some applications \nare sensitive to timeouts, so we increased the initialisation deadlines in the DaCapo harness and patched \nsome JDK 7 classes to ignore timeouts. We .xed bugs in tomcat9 and btrace.  Platform and Benchmarks. \nWe used DaCapo 2006-10-MR2 [6] and 9.12-bach with their largest inputs. Trace data is collected on one \niteration of the benchmark ex\u00adcluding start-up. Timings are recorded on uninstrumented runs where we \nstrived to get at least 5 repetitions and 10 iterations; unless speci.ed otherwise timings are the mean \nof these runs. We ran HotSpot 1.7.0 01 for Linux/x86 64, btrace [2] 1.2.1 and ASM [1] 3.3 on a 4.8GHz \nIntel Core i7, with 4 hardware threads (hyper-threading disabled) and 16GB of RAM. For instrumented runs, \nwe allowed the VM to use 14GB of RAM. Some experiments were run on an AzulVega3with 864 processors with \nthe Azul VM 1.6. Limitations. Our instrumentation may prevent some opti\u00admisations. Some sampling erroris \ninevitable,but samplingis unlikely to be very regular, so errors should be random and easy to spot when \nwe look at multiple results. Our instru\u00admentation may in.uence thread interleaving and hence some of \nthe sharing and concurrencycharacteristics we measure.  5.2 Virtual Machine Instrumentation Although \nour bytecode instrumentation can measure which objects are accessed by more than one thread, it cannot \nde\u00adtect the number of objects reachable from a thread. However, components of the GC do precisely this. \nThe extensible de\u00adsignof JikesRVM[3] makesit ideal for suchexperiments. Design. We customise the compiler \nto insert probes after memory relevant operations.We ensure that probes are nei\u00adther instrumented nor \ncall any instrumented code.We apply the probes only to application and not to VM code. We add metadata \npertaining to threads to the VM, and that pertaining to objects into their header.We map memory operations \ninto reads and writes and count them per object.For each object we keep two bitmaps, one of all threads \nthat ever accessed it and one of all threads from which it was ever reachable. To identify reachable \nobjects, we run mock per-thread traces through the heap, recording the set of objects each thread can \nreach at that time. Obstacles. WeextendedJikesRVMto instrument accesses to primitive .elds. Probes for \nstatic .elds require special care: they need class-based metadata which we keep in the VM s internal \nrepresentation of the classes, but this is only available when the containing class is loaded. Filtering \nout VM-speci.c activity is complicated by the VM allocating its objects in the same heap. We adopt the \nmethodology of Jones&#38;Ryder [16] and use JikesRVM s baseline compiler. We count only operations by \napplication threads, including the .naliser and reference handler threads; we exclude all other VM threads. \nWe force a special GC at shut down to count anyremaining live objects. Platform and Benchmarks. We modi.ed \na development version of JikesRVM (21/7/11), and ran all the benchmarks that JikesRVM could run ona dual \nquad-core 2.27GHz Intel Xeon machine with 12GB RAM. We let the benchmarks scale the workload to use all \navailable processors. 6. Characterising the DaCapo Workload The DaCapo suites use of concurrencyis complex. \nIn some cases, it is used primarily as a design tool; in other cases, the goal is performance on parallel \nhardware. The DaCapo harness allows setting the number of threads that drives the workload, but this \ndoes not fully determine how many threadsdoa substantial amountofwork concurrently.Work\u00adloads often spawn \nthreads of their own, either directly or in\u00addirectly through libraries. Some threads live for the entire \nexecution of the benchmark, some only for one iteration and some only for short-term tasks within iterations. \nSome are active throughout whole execution of the benchmark, some only throughout one iteration (e.g. \navrora9)or some phase of it (e.g. h29), and sometimes tens or hundreds of threads are created each for \na single short-term task (e.g. eclipse9). Moreover, the number of threads spawned may depend on the hardware \n tomcat9 spawns poller threads that service network connections depending on the number of logical processors \navailable. On the other hand, even benchmarks that do not spawn any new threads can generate work for \nthe VM s reference handler and .naliser threads. Our goal is provide a black-box view of these applications \nand thus give developers an understanding of the benchmarks use of concurrency. 6.1 Operations We summarise \nthe operations performed by the DaCapo benchmarks in Figure 3. The .gure compares statistically normalised \nrates across benchmarks (higher means that a benchmark performed relatively more operations of that par\u00adticular \nkind than other benchmarks).For instance, the graph shows that the lusearch9 benchmark allocates fewer \nbut larger objects than average; it acquires fewer monitors but makes more memory accesses. xalan9 enters \nby far the most locks per second of anybenchmark (and nearly4 stan\u00addard deviations more than the mean \nover all benchmarks). We examine memory operations known to be of interest to GC design. Some of the \nbenchmarks in the suite are quite memory intensive. lusearch9 hasbyfar the largest alloca\u00adtion rate (4GB/s),3 \nfollowed by xalan6 (1.7GB/s) and sun\u00adflow9 (1.4GB/s). The median allocation rate is 424MB/s. The highest \nrate of memory accesses (reads+writes) is for sunflow9 (5.9G/s), followed by bloat6 (3.3G/s) and xal\u00ad \n31KB of memory is 1024 bytes,but 1K objects is 1000 objects.  4 3 2 1 0 -1  Reads and Writes Bytes \nAllocated Objects Allocated Monitors Entered  antlr6avrora9batik9bloat6chart6eclipse6eclipse9fop6fop9h29hsqldb6jython6jython9luindex6luindex9lusearch6lusearch9pmd6pmd9sunflow9tomcat9tradebeans9tradesoap9xalan6xalan9 \nFigure 3: Operations per second normalised to have zero mean and unit variance. Intuitively, this shows \nhow each benchmark differsfromtheDaCapoaverageinaway consistent acrossdifferent measuressuchasbytes allocatedor \nmonitors entered.For example, we see that lusearch9 allocates more space thanaveragebut entersfewer monitors. \nan6 (3.1G/s). The lowest access read+write rate is by fop6 (95M/s) and the median is 806M/s. All benchmarks \ndo more reads than writes. The geomean read/write ratio is 3, highest with sunflow9 (11\u00d7), h29 (11\u00d7)and \nhsqldb6 (8\u00d7), and lowest with jython6 (2\u00d7). The distribution is right-skewed (to higher values). Signif\u00adicantly \nmore reads than writes is a well known property of most programs, e.g. exploited by replicating and genera\u00adtional \nGCs. The read/write ratio is higher for scalars than for arrays. The ratio of scalar/array memory accesses \ndif\u00adfers greatly across benchmarks. It is highest with avrora9 (13\u00d7), followed by sunflow9 (7\u00d7)andbloat6 \n(6\u00d7). There are more array accesses than scalar ones for only three benchmarks, batik9 (scalar/array \nratio 0.65\u00d7), jython6 (0.78\u00d7)andtomcat9 (0.94\u00d7). Most accesses are non-static. The non-static/static \nratio ranges from 7\u00d7 (jython9)to as much as 480\u00d7 (sunflow9), with geomean 40\u00d7 and median 38\u00d7. Statics \nalso have a very large read/write ratio (3.7K\u00d7 geomean, 20\u00d7 minimum and 436K maximum). All metric distributions \nwere heavily right-skewed to\u00adwards higher values. This has a serious consequence for per\u00adformance evaluation. \nSummaries over these benchmarks are unlikely to be robust and omitting a benchmark for any rea\u00adson, good \nor bad, might in.uence results. Furthermore, re\u00adsults are very sensitive to errors when measuring the \nout\u00adlying benchmarks. They are also sensitive to errors in these benchmarks (e.g. the very large allocation \nrate in lusearch9 isduetoabuginthe lucene library [24]). It is therefore essential to interpret benchmark \nperfor\u00admance results individually.A good strategy for optimisation might be to pick one benchmark with \na high overhead to attack and then look at ways to make it faster without dras\u00adtically slowing others, \ne.g. by optimising reads and writes in sunflow9, locking in xalan6 or GC in lusearch9.  6.2 Age Recall \nthat our metric is the smallest age for which 95% of all accesses are to younger objects. For all but \n2 of the benchmarks, this age for reads is equal to or larger than that for writes. The geomean age over \nall benchmarks is about 8\u00d7 larger for reads than for writes. Thus writes clearly hap\u00adpen early in an \nobject s life: this agrees with other work [5]. We express this by saying that reads are older than writes \n. Young accesses do not dominate. In manybenchmarks, even very old (several seconds) objects are often \naccessed. In par\u00adticular the age of reads is often proportional to execution time.  AnyOpsAlt.ModifsSpot-sharedShared \nAnyOpsAlt.ModifsSpot-sharedShared #ofThreads Density Periodic density avrora9 25 22 25 25 22 22 22 22 \n30 xalan6 8 8 8 8 8 8 8 8 12 tomcat9 9 15 12 14 9 8 7 8 26 tradesoap9 13 44 32 28 7 13 12 11 267 h29 \n5 4 4 5 4 4 4 4 9 tradebeans9 4 4 4 4 4 4 4 4 221 xalan9 4 4 4 4 4 4 4 4 8 pmd9 5 5 5 5 4 4 3 3 8 hsqldb6 \n201 385 382 393 1 34 15 15 405 lusearch6 63 59 66 64 44 0 57 59 68 lusearch9 4 5 5 5 4 0 4 4 8 sunflow9 \n9 9 9 9 4 0 4 4 13 antlr6 1 1 1 2 1 0 0 1 4 batik9 3 3 5 6 1 0 0 1 11 bloat6 1 1 1 2 1 0 0 0 4 chart6 \n1 2 1 3 1 0 0 0 6 eclipse6 2 6 5 5 1 0 0 1 53 eclipse9 28 197 191 72 1 1 0 1 341 fop6 1 1 1 2 1 0 0 0 \n4 fop9 1 1 1 2 1 0 0 1 4 jython6 1 1 1 2 1 0 0 0 4 jython9 1 1 2 2 1 0 0 1 4 luindex6 1 2 2 2 1 0 0 1 \n4 luindex9 2 3 3 3 1 0 0 1 5 pmd6 1 1 1 2 1 0 0 1 4 Table 1:Levels of concurrency. Numbers of threads \nthat contribute signi.cantly over all execution or periodically (within any 100ms interval).Work is any \nread, write or monitor enter (Any, Spot-shared, Shared), or alternating writes and monitor enters (Alt.Modifs). \n 2 3 42 3 42 3 42 3 4 0.6 0.8 11.2 1.5 2 0.6 0.8 11.2 1.5 2  0.6 0.8 11.2 1.5 2 . Shared Reads Shared \nWrites Shared Entries . Spot-shared Reads Spot-shared Writes Spot-shared Entries Alternating Writes \nAlternating Entries  2 3 42 3 42 3 4 Figure 4: Increase in shared operations as driver threads are increased \nfrom 2 to 4.  6.3 Levels of Concurrency Table 1 gives the number of threads that contribute signi.\u00adcantly \nto DaCapo workloads on a 4-core machine.4 It tells us how parallel and multi-threaded the benchmarks \nreally are, not simply the number of threads that theyspawn.5 DaCapo 09 was designed to expose richer \nbehaviour and concurrency on large working sets [13]. However, our measurements show that only 9 of 14 \nDaCapo 09 bench\u00admarks (and 2 of 11 DaCapo 06) are meaningfully parallel, i.e. have a periodic density/anyops \ngreater than 1. Only these benchmarks can be expected to scale. Note a large value for periodic density/any \nops is an indication of parallel slack\u00adness [22]. eclipse9 and hsqldb6 are heavilymulti-threaded (large \ndensity/anyops)but not parallel. Multi-threaded non-parallel benchmarks, as expected, do not pose a signi.cant \nchallenge to shared memory (periodic density with alt modifs, spot\u00adshared, and shared ops is1 or 0), \nand hence are not suitable for evaluating concurrency-related performance optimisa\u00adtions. Indeed, the \nlack of any challenge to shared memory is good for scalability of parallel benchmarks. sunflow9 and lusearch9 \nhave scarcely anyalternating modi.cations; sunflow9 scales best and lusearch9 second/third best of the \nDaCapo 09 benchmarks). Note that few alternating mod\u00adi.cations implies few contended monitor enters. \nThe number of threads created (last column in Table 1, which includes the .naliser and the reference \nhandler threads) is a poor approximation of concurrency the number of threads doing a meaningful amount \nof work (density/any ops) is much smaller, and even smaller is the number of threads doing so concurrently \n(periodic density/anyops). 6.4 Scaling Workloads We explore how the DaCapo 09 benchmarks with signi.cant \nlevels of concurrency (h29, lusearch9, sunflow9, tom\u00adcat9, tradebeans9, tradesoap9, xalan9) behave as \nwe drive them with1,2,3 or4 threads.Ifdevelopers areto use a benchmark for scalability experiments, theyneed \nsome in\u00adtuition for the impact of adding cores/threads. For instance, they may expect a scalable benchmark \neither to divide the same amount of work among more threads (better perfor\u00admance) or to continue to give \nthe same quantum of work to each thread (more work done). Above all, theyneed a bench\u00admark to behave \npredictably. Unshared operations behave as expected: their numbers are preserved as the number of driver \nthreads increases. Un\u00adfortunately, none of the benchmarks scales its shared opera\u00adtions in a predictable \nand consistent way. lusearch9, xal\u00adan9 and tradesoap9 seem to perform the same number of shared operations \nfor 2, 3 and 4 threads, but fewer for 1 thread. This suggests that the degree of sharing in these pro\u00ad \n4Note that the metrics allow density to be greater with alt. modifs than anyops . 5Threading in DaCapo \n9-12, dacapobench.org grams is determined directly or indirectly by the number of driver threads. On \nthe other hand, the level of sharing in the remaining benchmarks(h29, sunflow9, tomcat9, trade\u00adbeans9)seems \nto be .xed. Figure 4 compares the change in numbers of operations with 2, 3 and 4 driver threads. The \n.gure shows shared and spot-shared reads, writes and monitor enters, as well as alternating writes and \nmonitor enters. None of the bench\u00admarks scale the numbers of either spot-sharing operations or alternating \nmodi.cations ina predictablefashion: often, as the number of one operation increases the number of an\u00adother \ndecreases. As the number of driver threads increases from 2 to 4, the number of alternating reads in \nxalan9 in\u00adcreases by over 1.5\u00d7. xalan9 is memory intensive so this suggests that memory is likely to \nbecome bottleneck. The shared operations stay about the same with all benchmarks, which tallies with \nthe intuition that worker threads make the same number of accesses if the problem size remains con\u00adstant. \ntomcat9 is the best scaling benchmark. Observe that its spot-shared and alternating operations do not \nincrease with the number of driver threads: both of these kinds of operation are inhibitors to scalability. \nIn contrast, the num\u00adber of alternating writes increases in h29, the worst scaling benchmark. Alternating \nentries also appear to increase with workers in sunflow9,but do not hinder scaling because they are so \ninfrequent (Table 3). These results show that one cannot easily evaluate how the bene.t of a VM optimisation \nchanges with the number of processors, at least not without checking the changes in concurrency stress. \n 6.5 Shared Memory Accesses Most DaCapo 06 benchmarks have a single user thread, so it is no surprise \nthat in 10 out of 25 benchmarks less than 0.5% of reads and writes are shared . In each of the remaining \n15 benchmarks, the fraction of shared reads tends to be much higher than the fraction of shared writes \n(Table 2). Shared accesses are therefore even more heavily dominated by reads (28\u00d7 by geomean) than are \nthread-local accesses (3\u00d7). Ar\u00adray accesses are more shared than scalar accesses in most of these benchmarks \n(10 out of the remaining 15). Statics are particularly highly shared: in 12 of these benchmarks, at least \n95% of their accesses are shared.6 Sharing is also bi\u00adased towards statics: although only a geomean of \nabout 2% of all accesses are static, 22% of all shared accesses are to statics. Counting only spot-sharing \n(repeated, recent sharing) reduces the fraction of shared accesses by varying degrees. Itfallsby the \nmostin avrora9 (from 89% to only 6%), fol\u00adlowed by h29 (49% to 6%) and tradesoap9 (36% to 15%). Only \n4 benchmarks have more than 10% of accesses spot\u00adshared: sunflow9 (49%), tradebeans9 (35%), trade\u00ad 6We \ncount sharing of statics at class granularity, so two threads accessing different static .elds of the \nsame class would count as sharing.  Reads and Writes Reads WritesArrays Scalars Statics Reads and Writes \nReadsWritesArraysScalars Statics Shared /All Spot-Shared /All avrora9 89 90 83 95 88 99 6 7 1 6 5 97 \nsunflow9 53 58 0 75 50 100 49 53 0 74 45 97 h29 49 53 5 54 42 99 6 7 1 5 4 27 tradebeans9 48 57 6 37 \n45 100 35 41 4 26 29 94 tradesoap9 36 45 8 19 43 100 15 20 2 10 14 81 tomcat9 20 23 13 17 18 100 8 11 \n3 5 8 78 xalan6 19 21 6 19 12 100 14 16 3 11 8 89 eclipse9 17 20 6 15 14 68 1 1 0 1 1 1 eclipse6 13 17 \n0 18 2 97 0 0 0 0 0 0 hsqldb6 13 14 13 9 13 70 6 7 1 6 4 61 xalan9 13 14 6 12 10 100 7 8 2 6 5 78 lusearch6 \n8 10 5 11 6 100 2 3 0 3 0 97 pmd9 6 8 0 6 4 99 2 3 0 2 1 50 jython9 3 4 0 0 0 20 0 0 0 0 0 0 lusearch9 \n3 4 0 2 0 100 3 3 0 2 0 96 Table 2: Percentage of accesses that were shared or spot-shared, across different \naccess types, scalars and arrays and statics. The .gure shows only scalable benchmarks that have at least \n0.5% reads and writes shared or spot-shared, using4 driver threads. soap9 (15%) and xalan6 (14%); with \nany sharing only 4 had less than 10%. Reads continue to be more likely than writes to be spot-shared \n, and 20% of all spot-shared ac\u00adcesses are still to statics. The fractions of alternating modi.cations \nare very low, the highest being tradebeans9 (1.2% of all writes), fol\u00adlowed by h29 (0.7%) and hsqldb6 \n(0.6%). Only 7 bench\u00admarkshaveover 0.2%(tradebeans9, h29, hsqldb6, tom\u00adcat9, tradesoap9, xalan9 and xalan6). \nIn contrast to the measures above, scalars are more shared than arrays in nearly all benchmarks. Static \nalternating writes vary substan\u00adtially between benchmarks, from 73% for tradebeans9 to 0.2% for h29. \nThey account for 5% of all alternating writes, despite the rarity of static writes. In Figure 5 we further \ndiscriminate shared accesses by sharing pattern, for every benchmark with at least 0.5% of reads and \nwrites shared. The topmost graph shows all reads and writes, with the bold black line indicating the \npercentage of shared reads and writes. Sharing may take several forms, each presenting different stress \nto the VM. The coloured bars summarise these patterns by increasing level of challenge. The lowest bar \n(S1) shows the percentage of shared accesses to objects that were only read or only written; the second \nbar (S2) adds stationary objects (to which there may have been a sequence of writes, then a read but \nno further writes7); and the third bar (S3) adds any objects written by only one thread yet not included \nin the previous bars. The fourth bar (S4) adds any accesses to objects that were last accessed by the \nsame thread, so ownership8 does not change. The last bar (S5) accounts for the remaining, and most challenging, \nshared accesses; most intriguingly there are scarcely any. From the other graphs in the same .gure, we \nsee that this kind of write sharing is present in some benchmarks (chie.y tomcat9 and pmd9, and for statics \ntradebeans9),but becomes negligible when reads are included. We can also see that almost all shared accesses \nby sunflow9, the best scaling benchmark from Figure 1, fall into the .rst three categories. Nearly all \nits shared reads are single-writer; its shared writes are to write-only arraysbut account for little \nsharing. Arrays and statics are very likely to be stationary. Many instance .eld reads are single-writer, \nif not stationary, but writes to single-writer objects are less common for scalars than arrays. The incidence \nof read-only statics is at .rst sur\u00adprising. However, the VM and DaCapo harness load and ini\u00adtialise \nseveral hundred classes before our Java agent can in\u00adstrument them. Many of these statics will be used \nas con\u00adstants and thus appear as read-only (S1) whereas a better classi.cation might be stationary (S2). \nApart from trade\u00adbeans9 statics and lusearch9 instance .eld reads, sharing that changes ownership but \nis neither stationary nor single writer (S5) is largely concentrated on arrays. In summary, we see that \nthe workloads include a rela\u00adtively small number of shared memory accesses, and only to memory shared \nthrough simple patterns. 7Hence,no stationary write appearsinthe .gures. 8We treatany accessas possessionofownership. \n 100 80 60 40 20 0 100 80 60 40 20 0 100 80 60 40 20 0 100 80 60 40 20 0 100 80 60 40 20 0  tradesoap9 \n Instance Field Reads Instance Field Writes 100 80 60 40 20 0 Array Reads Array Writes 100 80 60 40 \n20 0 Static Reads Static Writes 100 80 60 40 20 0 tradesoap9tomcat9tomcat9xalan6xalan6  avrora9 avrora9 \n sunflow9 sunflow9  tradebeans9 tradebeans9 h29 h29 eclipse9eclipse9hsqldb6hsqldb6eclipse6eclipse6xalan9xalan9 \nlusearch6 lusearch6 pmd9 pmd9 lusearch9 lusearch9 jython9 jython9 tradesoap9tradesoap9tomcat9tomcat9xalan6xalan6eclipse9eclipse9 \n avrora9 avrora9 sunflow9 sunflow9 tradebeans9 tradebeans9 h29 h29 hsqldb6hsqldb6eclipse6eclipse6xalan9xalan9 \nlusearch6 lusearch6 pmd9 pmd9 lusearch9 lusearch9 jython9 jython9  All Reads and Writes S1: Shared Read/Write \nOnly S2: Additional Shared Stationary S3: Other Shared, Single-Writer S4: Other Shared, Same Owner S5: \nAll Remaining Shared  All Reads All Writes 100 80 60 40 20 0  Figure 5: Sharing patterns for different \nkinds of accesses. The black line denotes accesses of each kind shared in any pattern (numbers fromTable2).The \ncoloured barsshowthe percentageof shared access that conformtoa particular pattern, suchas read-only \nor write-only (S1), stationarybut not read-only (S2), single-writerbut not stationary (S3) and so on. \n D  B sunflow9 C tradebeans9 D F E H A G tradesoap9 xalan6 tomcat9 xalan9 avrora9 hsqldb6 D 0 5 1015 \n0 5 1015 Object Age (seconds of uninstrumented execution) Object Age (seconds of uninstrumented execution) \nFigure 6: The percentage of accesses up to a given age to objects that are shared or spot-shared. Only \nbenchmarks with at least5%of accesses spot-shared are shown. Benchmarks are orderedin thekeysby spot-sharing. \nUnusually, many objectsin avrora9 are shared at an early agebut spot-sharing soon decreases.  6.6 Shared \nAccesses By Age It is interesting for GC design to ask whether sharing is re\u00adlated to age.We record the \nage and locality of accesses, for both any and spot sharing. Figure6 shows the results for benchmarks \nwith at least 5% of accesses spot-shared. We observe that the older an object accessed, the higher is \nthe chance that it is shared. Young accesses are likely to be to local objects. This is somewhat intuitive, \nas we would expect that objects need some time to become shared,but this trend continues even for very \nold objects. This trend is exhibited by all these benchmarks except for avrora9, where the rate of increase \nin the fraction of accesses to spot-shared objects drops sharply after about 10ms, although the any sharing \nratekeeps increasing rapidly. This suggests that in avrora9 single threads often make access to objects \nthat were earlier shared. However, for the most part, young objects are very unlikely to be shared, suggesting \nopportunities for discrimi\u00adnating their handling. 6.7 Lock Operations Synchronization operations are \ndetailed inTable 3. The .rst column shows the rate of monitor enters (acquisitions of a monitor). Some \nsingle-threaded benchmarks have high rates (e.g. jython6 13M/s), whereas the best-scaling multi\u00adthreaded \nbenchmark(sunflow9)makes only 1K enters per second. The next three columns show the percentage of lock \nenters on objects that are shared, spot-shared or prone to al\u00adternating modi.cations (which includes \nlockingby alternat\u00ading threads). Single-threaded benchmarks may show shared enters because of threads \nused by the VM or libraries, such as the .nalizer, reference handler,AWT and Java2D threads. The next \nthree columns (5-7) show the maximum age of most objects locked, i.e. where 95% of locks were to objects \nof that age or younger. The age is given in seconds of unin\u00adstrumented execution (shown in the last column). \nIn many but not all benchmarks, this age is large or even close to the total execution time, suggesting \nthat long-lived objects are subject to locking throughout their lifetime. Unsurprisingly, the number \nof locks acquired on spot-shared objects seems to have a substantial effect on scalability. The benchmarks \nthat scale best on AMD, sunflow9 and tomcat9, use fewer monitor enters, especially fewer spot-shared \nenters, than the worst scaling ones, h29 and tradebeans9 (Figure 1(a)).  6.8 Volatile Accesses Volatile \n.elds serve as a means of communication among threads, but the benchmarks make comparatively few ac\u00adcesses \nto volatiles (5% in jython9; 1% in antlr6, tom\u00adcat9, hsqldb6, fop9, and jython9; less in others). How\u00adever, \nthe rates of volatile accesses are still reasonably high (Figure 7). The highest rate is with jython9 \n(26M/s), the median is 664K/s and the lowest rate is with sunflow9, the most scalable benchmark (4.6K/s \nonly). Many static writes are volatile: in 12 of the 25 bench\u00admarks, more than half are to volatile .elds. \nLess than a few percent of other types of accesses (static reads, non-static reads, non-static writes) \nare to volatiles. Because static .elds are (potentially) accessible to all threads, synchronisation is \nnecessary and volatile provides a convenient mecha\u00adTable 3: Monitor enter operations (locks). The table \nshows the rate of enters, what percentage of these were shared, spot-shared and alternating, and the \nmaximum ages of most (minimum age for which 95% of locks were to objects of that age or younger). E.g. \nsunflow9 only acquires about 1,000 monitors per second, 46% of which protected shared objects; its shared \nobjects tend to be older than its unshared ones. Although pmd6 is multi-threaded and has a high rate \nof entries, nearly all are to local objects.  Mon.Enters Shared Spot-shared Alternating AllEnters Shared \nSpot-shared NativeExec. Rate [K/s] % of Enters Max Age of Most [s] Time [s] antlr6 8912 0 0 0 0.0 1.7 \n1.7 1.8 avrora9 972 66 65 57 6.9 7.1 7.0 7.4 batik9 538 1 0 0 0.2 2.8 1.2 3.4 bloat6 7699 0 0 0 0.0 0.7 \n0.9 11.0 chart6 6986 0 0 0 0.0 4.0 4.2 4.8 eclipse6 5918 3 0 0 0.0 11.6 11.7 26.1 eclipse9 1517 27 4 \n1 22.7 28.6 15.6 39.8 fop6 782 1 0 0 0.2 0.6 0.6 1.0 fop9 1534 0 0 0 0.9 0.9 0.8 1.4 h29 4991 92 19 1 \n7.2 7.2 7.3 20.4 hsqldb6 2498 12 7 3 0.8 1.6 1.5 3.2 jython6 12680 0 0 0 11.6 0.9 1.3 13.9 jython9 6575 \n14 0 0 12.4 16.3 1.5 16.9 luindex6 923 1 0 0 1.6 2.3 2.2 2.3 luindex9 264 3 1 0 0.8 1.1 1.1 1.2 lusearch6 \n4759 37 0 0 2.3 2.5 0.1 3.0 lusearch9 2514 0 0 0 0.1 2.3 2.1 2.5 pmd6 23813 0 0 0 0.0 4.5 4.8 5.5 pmd9 \n599 62 27 7 2.1 2.1 2.2 3.3 sunflow9 1 46 15 16 3.4 4.1 4.0 4.7 tomcat9 2635 16 6 3 7.1 9.9 9.5 10.9 \ntradebeans9 7941 99 44 6 8.5 8.5 8.4 8.9 tradesoap9 11006 23 9 3 11.4 15.2 15.0 17.5 xalan6 59421 22 \n15 3 5.1 6.3 6.2 6.8 xalan9 31954 11 5 2 3.4 6.1 6.0 6.6 nism, often used for e.g. lazy initialisation \nby a single thread. Static .elds are commonly used for constants. The evidence from the statistics for \nstatic reads and writes supports this hypothesis (Figure 5): we see that the overwhelming ma\u00adjority of \nstatic reads are to (at worst) single-writer objects and scarcely any static writes change ownership \n(except for tradebeans9). As volatiles are used for similar tasks to locks, we com\u00adpare the number of \nvolatile accesses and monitor enters. Dif\u00adferent benchmarks favour different mechanisms. Although on \n(geometric)average, enteris used6\u00d7 (median4\u00d7)more frequently than volatile access, some benchmarks use \nmore volatile accesses than enters jython9 (4\u00d7), sunflow9 (4\u00d7), hsqldb6 (1.2\u00d7) and some use about the \nsame (pmd9, tomcat9, fop9). Volatile access seems as important as locking for optimisation. We investigated \nwhich volatile .elds are accessed most often, i.e. account for 95% of volatile accesses in a bench\u00admark.Typically, \nthere werefewand these were usedbyonly a few call sites. Over all the DaCapo benchmarks, just 102 .elds \nfrom 59 classes and 35 packages cover 95% of the volatile accesses of each benchmark. Most of these classes \nare from the Java I/O, references, re.ection, concurrent (in\u00adcluding atomics), and collection class libraries \n(Table 4). Some are from benchmark-speci.c libraries. 10 out of 25 benchmarks have larger read/write \nratios for volatiles than for non-volatiles (the extreme is hsqldb6: 339\u00d7 vs. 10\u00d7).  6.9 Wait and Notify \nInterestingly, the wait and notify methods are rarely used. Of all the benchmarks, 15 make fewer than \n100 of these calls (less than 13 per second) rates are shown in Figure 7. Six benchmarks issue fewer \nthan 100 calls per second(trade\u00adsoap9, eclipse6, hsqldb6, lusearch6, xalan6 and h29). The remaining ones \nare lusearch9 (158/s), xalan9 (361/s), eclipse9 (17K/s) and avrora9 (61K/s). wait is invoked on only \n29 classes in the whole suite. The hottest class is Table 4: Classes with volatile .elds accessed most \nin the DaCapo benchmarks. Standard Java libraries are in the upper part, benchmark libraries in the lower \npart.  Package (# of classes) Comment, Selected Classes java.io (6) *Stream, RandomAccessFile java.lang \n(6) Class, re.ect.Constructor, re.ect.Method, ref.ReferenceQueue, System, Thread java.math (1) BigDecimal \njava.net (1) URI java.nio (2) I/O, locale java.util.concurrent.atomic (4) Atomic Boolean, Integer, Long, \nReference java.util.concurrent.locks (2) AbstractQueueSynchronizer java.util.concurrent (4) Concurrent \nHashMap, LinkedQueue java.util.zip (3) ZipFile, *InputStream java.util (7) HashMap, HashTable, AbstractMap, \nlogging.Logger, regex.Pattern, locale . . . sun (7) Streams, sockets, fonts, . . . org.apache (6) Benchmark \nlibraries (batik, catalina, fop, lucene, tomcat). I/O, properties, . . . org.eclipse.core.internal (3) \nOrderedLock, ResourceInfo, ElementTree org.h2 (3) Database, Session, MemoryUtils org.hsqldb (2) jdbcStatement, \nSession org.osgi (1) ServiceTracker org.python.core (1) PyUnicode RippleSynchronizer in avrora9, which \nis the target of about 6M calls; it implements global simulation time for a parallel discrete event simulator. \nSimulator threads use it to wait for data from other threads that are notkeeping up (and notify the synchroniser \nof their progress). The next hottest class is ReadManager in eclipse9 (about 33K calls), which synchronises \nworker threads that read .les in parallel. Apart from avrora9 and eclipse9, the number of these calls \nis small enough that performance should not be an issue.  6.10 Concurrent APIs The java.util.concurrent \nAPIs are widely used in jython9 (over 175M total or 7.4M/s); .ve other bench\u00admarks have between 100 349K \ncalls/s(h29, tradebeans9, hsqldb6, tomcat9); another .ve do 10 16K/s(luindex6, antlr6, eclipse9, pmd9, \nxalan6). The rates are shown in Figure 7. The remaining benchmarks issue fewer than 10K calls per second. \navrora9 and sunflow9 have the lowest number of calls (less than 200) of all benchmarks. Only jython9, \npmd9, tomcat9, tradebeans9 and tradesoap9 call the concurrency APIs directly; other benchmarks do so \nonly through other Java libraries. Overall, the suites used atomics (integer, boolean, long, reference, \nreference .eld updater), concurrent hash maps and linked queues, copy-on\u00adwrite sets and array lists, \nlinked blocking queues, re-entrant read-write locks, semaphores, thread pools and future tasks. In summary, \nwhat can we learn from DaCapo s usage of different concurrency mechanisms, such as monitor en\u00adters, volatile \naccesses, concurrent API and wait/notify calls? Several benchmarks are signi.cant outliers. jython9 makes \nheavy use of all these mechanisms except wait/notify (par\u00adticularly concurrent hash maps, which in turn \nuse volatiles), even though it is essentially single-threaded (Table 1). The behaviour of some benchmarks \nhas changed signi.cantly be\u00adtween releases. xalan9 hasbyfar the highest monitor enter rate of all benchmarks \nand also has a very high volatile ac\u00adcess rate; xalan6 also has very high rates but lower than xalan9. \npmd6 uses locks heavily but does not use volatiles that much. jython6 behaves very differently to jython9, \nwitha muchlower rateof concurrent calls andvolatiles,but more monitor enters. avrora9 makes much use \nof wait/no\u00adtify calls (and hence monitors), but has a very low rate of concurrent calls and quite a low \nrate of volatile accesses. sunflow9 stands out in that it has the lowest enters rate and makes limited \nuse of the other mechanisms compared to other benchmarks; it is the most scalable benchmark (Fig\u00adure \n1(a)) in the suite.  6.11 Shared-Used and Shared-Reachable Objects VMs have an interest in which objects \nare, or could poten\u00adtially be, accessed by more than one thread. There are a num\u00adber of advantages to \nsegregating objects into thread-local heaplets that can be collected in isolation, without stopping other \nthreads. This approach accords well with transactional workloads where there is almost no trace left \nafter a transac\u00adtion commits. It makes it easier tokeep thread-local objects on the local node in a NUMA \nsystem, helping to address the problems of the allocation wall [26] or the memory wall .9 It can also \nreduce coherencytraf.c due tofalse sharing. Static or dynamic analyses (or a combination of both) can \nbe used to identify shared objects, trading precision for cost of analysis and object management. The \nbene.ts depend on precision. An escape analysis may determine an object to be shared if it is potentially \naccessible by more than 9www.azulsystems.com/presentations/ qconsf-state-of-the-art-in-java-gc  Monitor \nEntries  Volatile Accesses Concurrent API Calls Wait/Notify Calls 1e+06 1e+04 1e+02 1e+00    antlr6avrora9batik9bloat6chart6eclipse6eclipse9fop6fop9h29hsqldb6jython6jython9luindex6luindex9lusearch6lusearch9pmd6pmd9sunflow9tomcat9tradebeans9tradesoap9xalan6xalan9 \nFigure 7: Rates of monitor entries, volatile accesses, concurrent API calls and wait+notify calls (operations \nper second). one thread [21]. Alternatively, in languages like ML, only objects annotated as mutable \nneed be considered shared as immutable objects can be freely copied [9, 10]. Static analyses overestimate \nthe number of objects actually shared. Most are conservative in the face of (or ignore) dynamic class \nloading; Jones &#38; King [15] use an optimistic, and hence more precise, analysis that falls back at \nrun-time to more conservative assumptions should a dynamically loaded class invalidate parts of a prior \nanalysis. Runtime analysis delivers more precise results but re\u00adquires read or write barriers to detect \nescape [17]. The most conservative approach is to have a write barrier mark as shared the entire transitive \nclosure of an object at the point it becomes reachable from another shared object [11] we call these \nobjects shared-reachable. Precise techniques identify as shared-used only those objects that are actually \naccessed by more than one thread, but permit references from shared to local objects. Here, a read barrier \nis needed to detect sharing. Whilst read barriers are generally considered expensive [25], Haskell combines \nthe barrier cheaply with its closure entry mechanism [19]. We compared the incidence of shared-used and \nshared\u00adreachable objectsin JikesRVM. Note thateven benchmarks with a single user thread also employ a \n.naliser thread and so will report a non-zero number of objects that are shared\u00adreachable, e.g. from \nstatics. Shared-use was detected by our probes on the .y. To compute shared-reachability, we ran a GC \nafter every 10MB of allocation to give a lower bound of the set of shared-reachable objects. We measured \nthe proportion of all objects allocated by a benchmark that became shared-used or shared-reachable (Figure \n8), by number and by volume. Reachability is an even poorer approximation than we expected of how many \nobjects will be accessed by more than one thread. Although only a negligible fraction of space is actually \nshared in these benchmarks (7% in avrora9, but below 1% in others, and we know that in avrora9 about \n70% of shared accesses are made without ownership changes, see Figure 5), the shared\u00adreachablevolumeis \nlarge:over 10%in7of14 benchmarks, over 20% in 4 and as much as 61% in hsqldb6. In 11 of the 14 benchmarks, \nthe fraction of shared-reachable objects is larger by volume than by number, so we can speculate that \nshared-reachable objects tend to be the bigger ones. Surprisingly, this is not true for shared-used objects:5of \nthe 14 benchmarks have a higher fraction of shared-used objects by number than by volume. We also compared \nthe fraction of reads and writes to ob\u00adjects that were shared-used or shared-reachable (Figure 9). There \nare many more shared-reachable objects than are ac\u00adtually shared; the differenceis highest with xalan9 \n(19 per\u00adcentage points for reads and17 for writes).We tracked which objects were reachable from static \n.elds at every GC. hsql\u00addb6 has the largest number and volume of shared-reachable objects. It turns out \nthat 97% of objects that were shared\u00adreachable were also reachable from at least one static .eld at some \npoint. Only .ve types accounted for a signi.cant fraction (more than 95%) of these objects.  6.12 Unused \nObjects A noticeable number of write-only objects are initialised by the benchmarks but never read nor \nsynchronised, some read-only objects are never initialised nor synchronised, some locked-only objects \nare used only for synchronisa\u00adtion a common metaphor and some objects are nei\u00adther read, written nor \nsynchronised. Our tool only captures accesses from Java, but we manually veri.ed selected allo\u00adcation \nsites in the benchmarks and found that surprisingly manyof the objects are not used by native code, either. \nFigure 10 shows a particularly large proportion of write\u00adonly objects in jython6, jython9 and chart6. \nIn jython , write-only objects are mostly backing arraysof stringbuffers allocated eagerly by the lexical \nanalyser but not used later. Thesebuffers are created froma single-character string,but the class libraries \npre-allocate a 17-char backing array for each. In chart6, the vast majority of write-only objects are \nbacking arrays for lines of text input: string comparisons on two lines ignore the arrays if their lengths \ndiffer. chart6 also creates a signi.cant number of noti.cation objects, but fre\u00adquently there are no \nlisteners to be noti.ed so these objects are never read. pmd6 pre-initialises many objects to repre-Figure \n9: Percentage of reads from and writes to objects that were ever used by multiple threads (shared-used) \nor objects that were ever potentially reachable from multiple threads (shared-reachable). Accesses to \nShared Objects [%] 25 20 15 10 5 0 antlr6 avrora9 bloat6 eclipse6 fop6 hsqldb6 jython6 luindex6 luindex9 \n Figure 8: Percentage of all objects ever used by (shared-used) or reachable from multiple threads (shared-reachable). \n30  % Shared-Used Reads Objects Used and Reachable by Multiple Threads [%] antlr6 avrora9 bloat6 eclipse6 \nfop6 60 50 40 30 20 10 0 hsqldb6  jython6 luindex6 luindex9lusearch6 lusearch9 pmd6 xalan6  % Shared-Reachable \nReads % Shared-Used Writes % Shared-Reachable Writes lusearch6 lusearch9 pmd6 xalan6 % Shared-Used \n(Number) % Shared-Reachable (Number) % Shared-Used in (Size) % Shared-Reachable (Size) xalan9 xalan9 \n 60 50 40 30 20 10 0     Unused Objects Write-only Objects Read-only Objects Locked-only Objects \n antlr6avrora9batik9bloat6chart6eclipse6eclipse9fop6fop9h29hsqldb6jython6jython9luindex6luindex9lusearch6lusearch9pmd6pmd9sunflow9tomcat9tradebeans9tradesoap9xalan6xalan9 \nFigure10:Volumeofobjectsthatareonlyread,onlywritten,only synchronizedorunusedbyJava.Whilesomeoftheseobjects \nare read or writtenby native code,it turns out that many, particularly write-only, objects are not readby \nnative code either.In chart6 write-only objects are mostly strings representing lines of input that are \nskipped during processing. In jython6 they are mostly compiler objects eagerly allocated and initialisedbut \nnot needed later. sent the contexts of XPath predicates,but these are often not needed and hence write-only. \nIn general, write-only objects arise through eager allocation and/or preparation of data that is not \nalways needed for later computation. Objects are often read-only because only their default value is \nneeded. chart6 also creates string pre.xes as part of a decimal number formatter but often no characters \nare added so the pre.x s backing array is never written. Eager allocation also leads to objects not being \nused subsequently. For example, the python compiler in jython pre-allocates hashmaps for metadata that \nare not always needed. Manybig integers representing zero produce unused single-element int arrays. In \njython, the DaCapo results-validation reads a long text .le, which includes many empty lines leading \nto the creation of manyzero-length byte arrays. We also found instances of objects that were read-only \nor unused in Java, but were written or read from the native code.For instance, I/Obuffers used when readinga \n.le ap\u00adpeared read-only . Buffers used for class loading appeared unused as they were only read by the \nVM s native code. 7. Conclusion We provide a number of platform independent metrics of concurrency, shared \nmemory use and scalability of Java ap\u00adplications. Based on tracing, these metrics allow black-box studies \nof large applications, giving the developers an under\u00adstanding of what applications really do. Our approach \ntakes into account application libraries and standard Java libraries, but abstracts from anyhardware \nplatform or VM implemen\u00adtation. We believe that it will help application developers gain insights into \ntheir programs by understanding scalabil\u00adity bottlenecks independent of the platform used. Similarly \nVM developers need con.dence that the benchmarks they use to measure new optimisations do stress the \noptimisation they are trying to test. For this they need not only good ap\u00adplication benchmarksbut also \nto understand their behaviour. For example, to test locking optimisations, they need appli\u00adcations which \nuse locksina non-trivial manner.We provide metrics to examine how the programs use locking and other \nconcurrencymechanisms such as volatile accesses, synchro\u00adnisation through atomic memory operations, or \nfrequent ac\u00adcesses to shared memory by different threads. We implement our metrics in two tools, one \nwhich uses bytecode instrumentation and one which modi.es a Java VM. We provide an observational study \nof what the Da\u00adCapo 09 and DaCapo 06 benchmarks really do in terms of concurrencyand shared memory use.We \nbelieve our results will help developers to choose the right benchmarks for their purposes. We measure \nhow many threads in these bench\u00admarks actually contribute signi.cantly to the work of the benchmark. \nWhile manybenchmarks are multi-threaded, we .nd that their threads do not communicate much or commu\u00adnicate \nonly in limited ways. Moreover, communication often does not change predictably as we vary the number \nof cores deployed. Although limited synchronisation is good for scal\u00adability, these limitationshavetobekeptin \nmind when bench\u00admarks are used as test applications by VM implementors at\u00adtempting to optimise the concurrencysupport \ntheyprovide.  We hope that our .ndings for large Java applications will inform VM development, particularly \nmemory manage\u00adment and concurrent GCs for many-core systems with shared memory. The DaCapo benchmarks \ndo not scale beyond 20 or so cores on the platforms we have looked at and some even degrade. There are \nmore reads than writes, and many more static reads than static writes. Reads tend to be to older ob\u00adjects \nthan writes. Young objects do not dominate memory accesses. Shared accesses are dominated by reads. Array \nac\u00adcesses tend to be more shared than scalar ones, but scalars tend to have more frequent changes of \nownership. Static ac\u00adcesses are massively shared.Volatile accesses are quite rare compared to all accesses, \nbut only somewhat less frequent than locks. Nearly half of all static accesses tend however to be volatile. \nThe chance that an access is shared increases with age. There are many accesses to objects reachable \nfrom multiple threads, but few are accessed by multiple threads: reachabilityisa grossover-estimateof \nsharing.Anon-trivial volume of memory is reachable from statics, which imme\u00addiately contributes to this \nover-estimate in nearly single\u00adthreaded workloads. Furthermore, a non-trivial volume of memory is never \nused. Finally, we thank Doug Lea, Eliot Moss, Andreas Sewe, Pavel Parizek, Thomas Shilling and the anonymous \nre\u00adviewers for their thoughtful comments and suggestions.We are grateful for the support of the EPSRC \nthrough grant EP/H026975/1 and NSF 1048398, 1019518, 1019607. References [1] ASM project. http://asm.ow2.org, \n2011. [2] BTrace. http://kenai.com/projects/btrace, 2011. [3] B. Alpern, C.R. Attanasio et al. Implementing \nJalape no in Java. In Object-Oriented Programming, Systems, Languages and Applications (OOPSLA), 1999. \n[4] W. Binder, J. Hulaas and P. Moret. Reengineering standard Java runtime systems through dynamic bytecode \ninstrumen\u00adtation. In Source Code Analysis and Manipulation (SCAM), 2007. [5] S.M. Blackburn and K.S. \nMcKinley. Ulterior reference count\u00ading: Fast garbage collection without a long wait. In Object-Oriented \nProgramming, Systems, Languages and Applica\u00adtions (OOPSLA), 2003. [6] S.M. Blackburn, R. Garner et al. \nThe DaCapo benchmarks: Java benchmarking development and analysis. In Object-Oriented Programming, Systems, \nLanguages and Applica\u00adtions (OOPSLA), 2006. [7] A. Burns and A.J. WellingS. Real-Time Systems and Pro\u00adgramming \nLanguages: ADA 95, Real-Time Java and Real-Time POSIX. Addison-Wesley, 3rd edition, 2001. [8] K.-Y. Chen, \nJ.M. Chang, and T.-W. Hou. Multithreading in Java: Performance and scalability on multicore systems. \nIEEE Transactions on Computers, 60(11), 2011. [9] D. Doligez and G. Gonthier. Portable, unobtrusive garbage \ncollection for multiprocessor systems. In Symposium on Prin\u00adciples of Programming Languages (POPL), 1994. \n[10] D. Doligez and X. Leroy. A concurrent generationalgarbage collector for a multi-threaded implementation \nof ML. In Sym\u00adposium on Principles of Programming Languages (POPL), 1993. [11] T. Domani, E.K. Kolodner \net al. Thread-local heaps for Java. In International Symposium on Memory Management (ISMM), 2002. [12] \nB. Dufour, K. Driesen et al. Dynamic metrics for Java. In Object-Oriented Programming, Systems, Languages \nand Ap\u00adplications (OOPSLA), 2003. [13] H. Esmaeilzadeh,T. Cao et al. Looking back on the language and \nhardware revolutions: Measured power, performance and scaling. In Architectural Support for Programming \nLanguages and Operating Systems (ASPLOS), 2011. [14]L.Gidra,G. Thomasetal. Assessingthe scalabilityofgarbage \ncollectors on many cores. In Programming Languages and Operating Systems (PLOS), 2011. [15] R.E. Jones \nand A.C. King. A fast analysis for thread-local garbage collection with dynamic class loading. In Source \nCode Analysis and Manipulation (SCAM), 2005. [16] R.E. Jones and C. Ryder. A study of Java object demograph\u00adics. \nIn International Symposium on Memory Management (ISMM), 2008. [17] R.E. Jones, A.L. Hosking, and J.E.B. \nMoss. The Garbage Collection Handbook: The Art ofAutomatic Memory Manage\u00adment. Chapman&#38; Hall, 2011. \n[18] T. Liu and E.D. Berger. Sheriff: precise detection and auto\u00admatic mitigation of false sharing. In \nObject Oriented Pro\u00adgramming Systems, Languages and Applications (OOPSLA), 2011. [19] S. Marlow and S.L. \nPeyton Jones. Multicoregarbage collec\u00adtion with local heaps. In International Symposium on Memory Management \n(ISMM), 2011. [20] K. Shiv, K. Chow et al. SPECjvm2008 performance charac\u00adterization. In SPEC Benchmark \nWorkshop on Computer Per\u00adformance Evaluation and Benchmarking, 2009. [21] B. Steensgaard. Thread-speci.c \nheaps for multi-threaded pro\u00adgrams. In International Symposium on Memory Management (ISMM), 2000. [22] \nL.Valiant.Abridging model for parallel computation. CACM, 33(8):103 111, 1990. [23] P.H. Welch and J.B. \nPedersen. Santa Claus: Formal analysis of a process-oriented solution. ACMTrans. Comput. Syst., 32 (4), \n2010. [24] X. Yang, S.M. Blackburn et al. Why nothing matters: the impact of zeroing. In Object Oriented \nProgramming Systems Languages and applications (OOPSLA), 2011. [25] X. Yang, S.M. Blackburn et al. Barriers \nreconsidered, friendlier still! In International Symposium on Memory Man\u00adagement (ISMM), 2012. [26] Y. \nZhao, J. Shi et al. Allocation wall: A limiting factor of Java applications on emerging multi-core platforms. \nIn Object-Oriented Programming, Systems, Languages and Ap\u00adplications (OOPSLA), 2009.    \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Increasing levels of hardware parallelism are one of the main challenges for programmers and implementers of managed runtimes. Any concurrency or scalability improvements must be evaluated experimentally. However, application benchmarks available today may not reflect the highly concurrent applications we anticipate in the future. They may also behave in ways that VM developers do not expect. We provide a set of platform independent concurrency related metrics and an in-depth observational study of current state of the art benchmarks, discovering how concurrent they really are, how they scale the work and how they synchronise and communicate via shared memory.</p>", "authors": [{"name": "Tomas Kalibera", "author_profile_id": "81758861257", "affiliation": "University of Kent, Canterbury, United Kingdom", "person_id": "P3856093", "email_address": "t.kalibera@kent.ac.uk", "orcid_id": ""}, {"name": "Matthew Mole", "author_profile_id": "81548893856", "affiliation": "University of Kent, Canterbury, United Kingdom", "person_id": "P3856094", "email_address": "mrm30@kent.ac.uk", "orcid_id": ""}, {"name": "Richard Jones", "author_profile_id": "81100555993", "affiliation": "University of Kent, Canterbury, United Kingdom", "person_id": "P3856095", "email_address": "R.E.Jones@kent.ac.uk", "orcid_id": ""}, {"name": "Jan Vitek", "author_profile_id": "81100018102", "affiliation": "Purdue University, West Lafayette, USA", "person_id": "P3856096", "email_address": "jv@cs.purdue.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384641", "year": "2012", "article_id": "2384641", "conference": "OOPSLA", "title": "A black-box approach to understanding concurrency in DaCapo", "url": "http://dl.acm.org/citation.cfm?id=2384641"}