{"article_publication_date": "10-19-2012", "fulltext": "\n White Box Sampling in Uncertain Data Processing Enabled by Program Analysis TaoBao,YunhuiZheng,XiangyuZhang \nDepartmentofComputerScience,PurdueUniversity {tbao,zheng16,xyzhang}@cs.purdue.edu Abstract Sampling \nis a very important and low-cost approach to un\u00adcertaindataprocessing,in whichoutputvariationscausedby \ninputerrors are sampled.Traditional methodstend to treat a programas ablackbox.Inthispaper,weshowthatthrough \nprogram analysis, we can exposetheinternals ofsample ex\u00adecutionssothattheprocesscanbecomemoreselective \nand focused. In particular, we develop a sampling runtime that can selectively sample in input error \nbounds to expose dis\u00adcontinuity in output functions. It identi.es all the program factorsthatcanpotentiallyleadtodiscontinuityandhashthe \nvalues of such factors during execution in a cost-effective way. The hash values are used to guide the \nsampling pro\u00adcess.Our results showthatthetechniqueis veryeffectivefor real-worldprograms.It can achievetheprecision \nof ahigh sampling rate with the cost of alower sampling rate. Categories and Subject Descriptors F.3.2 \n[Logics and Meanings of Programs]: Semantics of Programming Lan\u00adguages; D.2.5[SoftwareEngineering]:TestingandDebug\u00adging \nGeneral Terms Experimentation,Algorithms,Veri.cation Keywords continuity, discrete factors, monte carlo, \npro\u00adgram analysis, robustness, uncertainty 1. Introduction Uncertainty poses a prominent challenge to \ndata process\u00ading programs. Traditional data processingprograms handle scalar data values. However in \nthe presence of uncertainty originated from instrument errors and measurement preci\u00adsionlimitations,inputdatahave \nerrorbounds.Itis veryim\u00adportant to reason about if a program would behave differ\u00adently withinput errors. \nPermission to make digital or hard copies of all or part of this work for personal or classroomuseisgranted \nwithoutfeeprovided that copiesarenot madeordistributed forpro.tor commercial advantage andthat copiesbearthis \nnotice andthefull citation onthe .rstpage.Tocopy otherwise,torepublish,topostonservers ortoredistribute \ntolists, requiresprior speci.cpermission and/or afee. OOPSLA 12, October19 26,2012,Tucson,Arizona,USA. \nCopyright c &#38;#169; 2012ACM978-1-4503-1561-6/12/10. . .$10.00 Longterm rainfallpredictionis oftenrealizedbysoftware \noperating onSeaSurfaceTemperature(SST)data[24].Due to the dif.culty and cost of deploying sensors, SST \ncon\u00adtains a lot of interpolated data, which are uncertain. Such uncertainty canlead todifferentprediction \nresults.In[26], it was shown that aprogram used toprocessdatafrom abi\u00adology experimentcannotproperly \nmodel the uncertainty in aparameterprovidedbyhuman scientistsbased on their ex\u00adperience such that a protein \nwas mistakenly classi.ed as a cancer indicator. Such mistakes could be highly costly be\u00adcause follow-up \nwet-bench experiments are usually carried out based on the data processing outcome. In bioinformat\u00adics, \none of the most widely used sources for protein data isUniprot[9],in whichproteinsareannotated withfunc\u00adtions.The \nannotations may comefrom real experiments(ac\u00adcurate) or computationbased onprotein similarity(uncer\u00adtain). \nSoftware operating on these data has to be aware of theuncertaintyissue[16].Softwarefacilitating.nancialde\u00ad \ncision makingis often requiredto model uncertainty[18]. Traditionally,uncertaintyanalysisis conductedonthe \nun\u00adderlying mathematical models[25].However, moderndata processing uses more complex models and relies \non com\u00adputers and programs, rendering mathematical analysis dif\u00ad.cult. Recognizing the importance of \nuncertain data pro\u00adcessing, recently, researchers have proposed database tech\u00adniquesto store,maintainandquery \nuncertaindata[15,21]. However, more sophisticated data processing is often per\u00adformedoutside adatabasebyprograms \nwritteninhighlevel languages.Addressing uncertaintyfrom theprogram analy\u00adsisperspectivebecomes natural.Continuity \nanalysis[6]is a static analysis that proves a program always produces con\u00adtinuous output given a set \nof uncertain inputs. However, in uncertaindataprocessing,manypropertiesofinterestaredy\u00adnamic. For instance, \nwhether output is reliable in the pres\u00adence of uncertaintyisdependent on the concreteinput error bounds.Itdemands \nanalyzingprogram executioninstead of theprogram.Moreimportantly, while static continuity anal\u00adysishasbeen \nshowntobe effective on simpleprograms such as sorting algorithms, real world programs are a lot more \ncomplex,involving complexcontrol.ows,high orderfunc\u00adtions, array andpointer manipulations.Automaticderivative \ncomputation[3]uses compilerstoinstrumentaprogramso thatoutputderivativescanbe automaticallycomputed.How\u00adever, \nthese techniques cannotdirectly reason about changes withininput errorbounds;they alsohavedif.cultiesinhan\u00addling \ncertainlanguagefeatures,such ascontrol .owrelated statements.  MonteCarlo(MC) methodsprovidea simple \nand effec\u00adtive means of studying uncertainty[5,15,23]. They ran\u00addomly selectinput valuesfromprede.neddistributions \nand aggregatethe computedoutputstoyieldstatisticalinsightsin the output space.While continuousfunctionsare \nrelatively easier tobe approximatedbyMC methods, asdataprocess\u00ading is realized by complex programs, outputs \nare often no longer continuousfunctions ofthe uncertaininputs.Discon\u00adtinuity poses signi.cant challenges. \nSome of the problems are illustrated in Fig 1. These .gures show how the out\u00adputchanges accordingto the \nvariation of an uncertaininput. Points representsamples.The .rstproblemin(a)isthatfrom the samples,it \nis hard to determineif the outputis continu\u00adous(curve A)ordiscontinuous(B).The secondproblemin (b)is \nthat even thoughweknow that the curveisdiscontin\u00aduous,it isyet dif.cult todetermine where the discontinuity \noccurs. In this case, it could occur at any locations such as the three shown in the .gure, resulting \nin curves aab, a\u00dfb, and a.b, respectively.Thethirdproblemin(c)is moreprob\u00adlematic asthefour samples appeartofollow \na simple mathe\u00admaticalfunction(a straightline through a and b)butindeed thereis adiscontinuoussegmentinbetweena \nand b.Inmany cases, these segments are so small that they arepronetobe\u00ading omitted by random sampling. \nIn fact, we observe that a tinydiscontinuous segment along a simplelinearfunction wasthe root causefor \nmisclassifying anirrelevantprotein as a cancerbiomarkerin our experiment. Traditionalblack-boxMCapproaches \ncanbe made adap\u00adtive by comparing the output values of two samples to de\u00adtermineif additional samples \nare needed,thatis, additional samples willbeacquiredinbetweentwo samplesiftheir cor\u00adrespondingoutputsdiffersubstantially.However,suchmeth\u00adods \nare sub-optimal. They have dif.culties in handling the case in Fig. 1 (c). Also, unnecessary samples \nmay be ac\u00adquiredfor a continuousfunctionifits slopeislarge. In thispaper, wedevelop a white-box MC method,pow\u00adered \nby program analysis. The technique aims to guide the sampling process through a lightweight dynamic analysis \nso that output discontinuity for given uncertain inputs can be disclosed with a small number of samples. \nDiscontinu\u00adouspointsbreakan outputcurve(i.ethe curverepresent\u00adinghow output varies according to the uncertaininput)into \na set of continuous segments, which can be easily approxi\u00admated with atraditionalMCprocess.Our observationisthat \nfor many data processing tasks implemented by programs, discontinuityis mainly causedbylanguage artifacts \nsuch as conditional statements and type casting, instead of the in\u00adtorMC sample execution,particularly, \nthe value changes of these artifacts, andusethe monitoredvaluestodirecttheMC process to selectively collect \nmore samples where disconti\u00adnuityislikely tohappen. At a high level, the technique works as follows. \nDuring a sample execution, the technique generates a hash value that aggregates the execution of the \nlanguage artifacts that could potentially lead to discontinuity. If two sample runs have the same hash \nvalue, implying the same control .ow and identical discrete coef.cients, the output functions in the \ntwo runs have the same mathematical form, suggesting continuity in the range delimited by the two samples. \nIf thehash valuesdiffer,indicatingdiscontinuity, an additional sample is taken in between the two original \nsamples. The process continuestoinspectthetwo sub-rangesdividedby the new sample, until all sub-ranges \nbecome continuous or the discontinuous points are suf.ciently narrowed down. Our contributions arehighlightedasfollows. \n Weformallyde.netheproblem andidentifythepossible sources of discontinuity in a program, called discrete \nfactors.  Wepropose a noveldynamic analysisthathashesthe val\u00adues ofdiscretefactorsduring an execution.Thehash \nval\u00adues canbeleveragedbyMC algorithmsto achieve selec\u00adtive sampling.We alsopropose a more sophisticated \nver\u00adsion of the analysis that hashes only the discrete factors relevant to the selected output.It allows \nus to avoid con\u00adsidering changesinirrelevantdiscretefactorstoprevent unnecessary samples.  Weproposethebasic \nselectiveMCalgorithm andits two extensions. One extension takes two sampling intervals as con.guration, \nand aimsto achievetheprecision of the small interval with the cost of the large one. The other extensionaimstoachieveoptimal \nsamplinggivena .xed budget (i.e. the number of samples allowed). We also studythe safety of the algorithms. \n We observe that in real world programs, there are small code regions in which control .ow differences \ndo not cause discontinuity. Such differences are mostly inten\u00adtional for purposes such as optimization. \nWe develop a pro.lertoidentifysuchcode regionsandprovetheir con\u00adtinuities.Our algorithms canthus avoidhashingthese \nre\u00adgions.  We evaluateourtechniqueonasetofSPEC2000.oating\u00adpoint programs and a biology data processing \nprogram. The results show that the proposed white-box sampling techniquecanidentifydiscontinuity effectively \nand ef.\u00adciently.  2. Discontinuity andDiscreteFactors Given an executionthatisderivedfrom a concreteinput, \nwe assumepart of theinputis uncertain.Our ultimategoalis to trinsic mathematical functions. Hence, the \nidea is to moni\u00ad  Output  Input (a) Continuous? (b) Where is the discontiuity? (c) Missing segments? \nFigure1. Three sampleproblems causedbydiscontinuity. understandhowtheprogramoutputchangeswithintheinput \nerrorbound. We use x1, x2, ... todenote multiple uncertaininputs.An exampleforsuchuncertaininputisa realnumberintheinput \narray receivedfrom a sensor.We only consider real number inputs unless stated otherwise. Program execution \nis hence denotedas afunction overthe uncertaininput P(x1, ...xn). In thispaper, we aim to develop a white-boxMC \nmethod that can quickly and effectively determine the shape of the output function, especially the discontinuity \nof the function, as continuous portions can be easily approximated by a regularMCprocess. We .rstpreciselyde.nethetermcontinuity.Tosimplify \nthediscussion,we assumethereis only oneuncertaininput in thede.nition, even though our technique supports \nmulti\u00adple uncertaininputs. De.nition1. (Continuity)P(x) is saidtobe continuous at a pointx = cifthefollowingholds:For \na value e > 0,however small,there exists some value d > 0suchthatfor anyx within the error bound and \nsatisfying c -d < x < c + d, we have P(c) -e < P(x) < P(c)+ e. P(x) is discontinuous at x = c if the \nabove condition is not satis.ed.P(x) is saidtobe continuous ifitis continuous throughoutthe error bound \nof x. Intuitively,if P is continuousregarding the uncertainin\u00adputx,then any small changetothe value of \nx can only cause a small changeto the output value P(x). Discrete Factors. Our goal is to identify the \ndiscontinu\u00adity, which cannotbe easily exposedby sampling the output. Some oftheproblems areillustratedbyFig.1anddiscussed \ninSection1.Ourobservationisthattheinternalsof a sam\u00adple executionprovidealotofhintstothediscontinuityofthe \noutputfunction.Wede.ne the term discrete factor to repre\u00adsent suchprogram artifacts. De.nition2. A discretefactoris \nan operation thathas real values as operands andproduces adiscrete value as result. In most cases,discretefactors \nare the root causefor out\u00adputdiscontinuity.We assume uncertaininputvalues are con\u00adtinuousin their errorbound.Toinducediscontinuity \non out\u00adput, these continuous inputs have to go through some dis\u00adcretefactors and resultindifferentdiscrete \nvalues.Thebasic idea of our technique is hence to monitor the execution of discrete factors to detect \ndiscontinuity and guide the sam\u00adpling process. Next, we discuss the most common discrete factors that \nwe have observed over a set of real world pro\u00adgrams. Type cast.Acontinuous.oating-pointvaluecanbe casted \nto a discrete type, such as integer, leading to the disconti\u00adnuity in the .nal output. Besides explicit \ncasting, implicit casting may also be automaticallyperformedby a compiler when necessary, such as whendiscrete \noperations(e.g. mod) areappliedto .oating-point values. Discrete mathematicallibraryfunctions.Dataprocessing \nprograms usually make heavy use of third-party mathemat\u00adicallibrary functions.Some of thesefunctions \narediscrete, such as SIGN(v), which returns1if v ispositive,-1if neg\u00adative,and0 otherwise.They may eventuallylead \ntodisrup\u00adtions alongthe output curve. Control .ow. Modernprogramminglanguages allow de\u00adveloperstomanipulatecontrol.owthrough \nconstructssuch as if-then-else statements andloops.These constructs arethekey elementsthat allowdataprocessingtogobeyond \nthe traditional pure mathematical modeling. However, they substantially increase the dif.culty of uncertainty \nanalysis by introducing discontinuity. In particular, if a value com\u00adputed from uncertain inputs is used \nin a predicate and the predicate guards the following computation leading to the output, there is a good \nchance discontinuity is introduced. Thereasonisthatthebranch outcomemay varydepending on the uncertain \nvalues, leading to different mathematical forms of the output. In our experience, control .ow is the \ndominantdiscretefactor. Consider thefollowingexample. 1. x=...; //the uncertain input  2. if (x>=2.0) \n 3. f=x+3.0;  4. else  5. f=3.0-x;  The predicate at line 2 makes x = 2.0 a discontinuous point. \nOn its left (x < 2.0), the curve takes on the shape of f(x)= 3.0-x; on the right including the point \nx = 2.0, f(x)= x+ 3.0. Besides discrete factors, discontinuity may also arise from the intrinsic mathematical \nmodel. For example, f = 1 isdiscontinuousat x = -1.0; f = tan(x) isdiscontin\u00ad x+1.0 = -p p uous at x \n2, 2, ....  We observethatin real worldprograms,such operations are often guarded by predicates or the \ninput domains are speci.ed in such a way that the discontinuous points are excluded. For the above f \n= 1 example, a predicate x+1.0 is often used to guard against x = -1.0 to avoid runtime exceptions.As \naresult,themathematicaldiscontinuity also manifestsitself asacontrol.owdiscontinuity. There are also \notherprogramminglanguage artifacts that do nothave correspondenceinthe mathematicalworld, such as arrays, \npointers, and bit operations. However, these arti\u00adfacts themselves cannot initiate discontinuitysuch \nthat they are not considered as discrete factors. For example, if the computation of an output involves \nan array element A[i] which is affected by the uncertain input. Assume the in.u\u00adence is through the array \nindex i, whichis of the(discrete) integertype.There mustbe aprecedingdiscretefactor, such as an explicit \norimplicit type castbecause the uncertainin\u00adput is of .oating-point type. Therefore, as long as we track \nthe value change of thatfactor, we capture thediscontinuity propagatedthroughthe arrayelement. Given \nthe de.nition of discrete factors, we have the fol\u00adlowing theorem that serves as the foundation of our \ntech\u00adnique.Givenanexecution,wedenotethe output as a math\u00adematical function of the uncertain input o(x). \nHere o(x) is limitedtobe an elementaryfunctionandittakes realnumber inputs only.One can consider thefunctionis \nconstructedby setting the uncertain input as a symbolic variable and then performing symbolic execution \nconcurrently with the con\u00adcrete execution. Theorem 1. Given two sample executions, if all of their discrete \nfactorsproduce the same discrete values, they must have the same outputfunction o(x). Intuitively, if \nthe two executions have the same discrete values, they must have taken the same program paths and all \nthepointers and arrayindices mustbeidenticalto ensure the samedatadependences.As a result,the symbolic \noutput functions mustbeidentical.Theformalproofis elided. Infeasibility of Using Symbolic Analysis. Note \nthat one might formulate the challenge as a dynamic test generation problem thatgenerates uncertaininputvalues \nto explorethe differentvalues of thediscretefactors.For example, explor\u00ading all the possible program \npaths within the input error bound may be able to expose the discontinuity caused by control .ow.However,wefound \nthatthisisimpracticalfor real world data processing programs for the following two reasons.(1)Path conditionfunctions \nare often ofhigh order. We found that 10 out of 11 SPEC CFP 2000 programs we have studiedhavehigh order(=2)path \nconditions 1. More importantly, they are mostly in a complex form, involving functions suchas square \nroot, sin/cos, andfraction.These pathconditionsgobeyondthe capability of existing solvers. 1We acquire \nsuchnumbersthroughpro.ling, without conducting any math\u00adematical reduction. (2)The entailedsymbolic executionistoo \nexpensivefor our target scenario. The reasoning is as follows: if a technique causes X times slowdown, \none may chooseto collect X MC samplesinstead of using the technique. 3. AnIllustrativeExample We use \nan example to illustrate the technique. Fig. 2 (a) showstheprogram.Variable x is uncertain;its valueis \nwithin an error bound [a,b] around the original value 1.5. The output function o(x) may take different \nforms, depending on the value of x. If x < 1.0(line 3), o(x)= 1(line 4). If x =1.0,dependingon the comparison \nt(x) > 0.3(line6),it may take the form o(x)= 0.3(line 7) or o(x)= 0.75(line 9). Function t(x) is of a \nhigh order, rendering techniques relying on constraint solving in-applicable. The curves for t(x) and \no(x) aredepictedinFig.2(c).Ourtechniqueaimsto leverageprogramanalysistoguidecollecting asmall set of \nsamplesthatdisclosethe shape of the outputfunction o(x), particularlyitsdiscontinuity. Our technique \n.rst identi.es all the discrete factors in the program. They are places that operate on real values and \nproduce discrete values, and thus the root causes of thediscontinuity.In thisprogram,lines3 and6 arediscrete \nfactors as they operate on real values and produce boolean outputs, andthe type cast atline2is also adiscretefactor. \nDuringa sample execution, wegenerate ahash valuethat is the aggregation of the values of all the discrete \nfactors encountered.Two sample runshaving the samehash value suggests(likely) continuity.Notethatthe \nstates of thetwo executions are still largely different despite the identical hashvalue.Forexample,the \n.oatingpoint computationthat is data dependent on the uncertain input may very likely havedifferentvalues.If \nthehash valuesdiffer, an additional sampleis takeninbetweenthe two original samples and the techniquecontinuestoinspectthetwo \nsub-rangesdividedby the new sample, until a thresholdis reached. Assume we start with two samples a and \nb (step(1)in Fig.2(b)).Readers can refertoFig.2(c) forthe samples and their corresponding outputs. The \norder of sampling is denotedbythe alphabetical order of the samples.Line3has differentbranch outcomesin \nthe two sample runs, resulting in different hashes. An additional sample c is then taken at the mid-value \nof a and b,dividing the regionintotwo sub\u00adregions[a,c]and[c,b].Thetechnique .rstconsiders[a, c] (step(2))anddividesit \nwith an additional sample d.Subre\u00adgion[a, d]isfurtherdividedbye (step(3)).Thehashvalues of a and e areidentical.Theprocessceasesto \ncollectmore samples in[a, e].Instead,it collects morein[e, d]until a small sampling interval threshold \nis reached at B @, disclos\u00ading the discontinuouspoint at x = 1.0.Notethatthe ranges with the samehash \nvalue(andthus no samples neededin between),such as[a, e],denotethe savingsbroughtbyour technique.Auniform \nsamplingscheme withthethresholdas  (a) program x=sample(1.5); 1 y=(int) x; 2 if (x<1.0) 3 o=1; 4 else \n5 if (t(x)>0.3) 6 o=0.3; 7 else 8 o=0.75; 9 z=1+y; 10 (b) sampling steps (1) [a, b] H(a)=0 T H(b)=2 F \nT (2) [a, c] [c, b] H(a)=0 T H(c)=1 F T H(c)=1 F T H(b)=2 F T (3) [a, d] (5) [d, c] H(a)=0 T H(d)=1 \nF T  H(d)=1 F T H(c)=1 F T [a, e] (4) [e, d] ... (6) [d, g] H(a)=0 T H(e)=0 T H(e)=0 T H(d)=1 F T \nH(d)=1 F T H(g)=1 F F [e, f] [f, d] ... H(e)=0 T H(f)=1 F T H(f)=1 F T H(d)=1 F T (c) curves and samples \n1.0 0.75 0.3 1.0 2.0 x Figure2. Anillustrative example.Thedotsin(c) represent the samples.Thehighlighted \nstatementsin(a) representdiscrete factors.A nodein(b) representsasampling regionwith thehash valuesof \nthelowerand upperbounds.The numbersdenote the steps. H(a) denotes thehash value of sample a and operator \n\u00b7 denoteshash aggregation(e.g. 0\u00b7 T denotes thehash of y= 0 andthepredicate atline3havingthe true value). \nsmall astheinterval of[e, f](atpoint B@)requires alot more samples. PracticalChallenges. In order to \nmake the technique work for real world programs, we need to further overcome the followingchallenges. \n Discrete factors in two sample runs may behave differ\u00adently. However, such differences may not be relevant \nto the output variable, andhence they should not cause ad\u00additional samples.For exampleinFig.2(a),thediscrete \nfactor at line 2 has nothing to do with o(x) and should be excluded from hashing. Similarly, if z is \nthe output variableinstead of o,thecontrol.owdifferencesinlines 3-9 shouldbe excluded.Wedevelop a slice-basedhash\u00ading \nalgorithm thathashes only thediscretefactorsin the dynamic slice of the output variable on the .y.  \nIn reality,developersmay writeprogramsin such a way thatcontrol.ow variationsdo notleadtodiscontinuity.It \nwouldleadto redundantsamplesif notproperlyhandled. We observe that such effects are usuallypresentin \nsmall code regions. We develop a pro.ler to identify regions from the whole code base and prove that \nthey must be continuous.Thus, we can avoidhashingthe control.ows in these regions.  It may not be safe \nto skip sampling when two hashes are identical. Consider the example in Fig. 2. The two hashes of d and \nc areidentical(point C  @ in Fig. 2(b)). If additional samples are nottakeninbetween(e.g. g), we will \nmissthetwodiscontinuouspointsinthat subre\u00adgion.Therefore,we need to study the safety of omitting sampling. \nThefollowingsectionsdescribe the techniqueindetails and our study of the abovepracticalproblems. 4. BasicHashingSemantics \nIn this section, we discuss the basic hashing semantics that hashesthe values ofall thediscretefactors \nencountereddur\u00adingprogram execution.Recall that two identicalhashes(of two respective sample runs) indicate \nthat the mathematical forms ofthe two outputfunctions areidentical(Theorem1), assuming a perfect hashing \nscheme. The discussion is lim\u00adited to one uncertain input for simplicity, while our tech\u00adnique supports \nmultiple uncertaininputs. Program P ::= s l Stmt s ::= s1; s2 | skip | x := e | while x .l 0 dos | if \nx .l 0 then s1 else s2 | exit Expr e ::= x | v | sample(r1, r2)l | e1 binopl e2 | discrete( f, e) | x \n.l 0 Value v ::= n | r | b Var x, Function f .Identifier n .Zr .Real l . Label b.Boolean Figure3. Language \nLanguage. To facilitate formal discussion, we introduce a kernel language. The syntax is presented in \nFig. 3. Note that relational operations are normalized to x . 0, with . denotinga relational operator.The \nreasonis that we need to study the real values(nottheboolean values) of relational expressions. Such \nvalues are explicitly denoted by x after normalization.Forinstance,aconditional statement if y< 1.0... \nisnormalizedto x := y-1.0; ifx < 0 .Itallows us to reason aboutthe value of y-1.0. We support three kinds \nof values: integers, real val\u00adues, and boolean values. Real values could be uncertain. A sample(r1, r2) \nexpression represents a sample within the error bound of [r1,r2]. We explicitly model discrete func\u00adtions \nas discrete( f, e). The expression denotes the discrete value generated by applying function f to a real \nvalue e. Type casts and the sign(x)method are examples of such discretefunctions. Operational Semantics. \nThe semantics is presented in e' Fig.4.The expressionruleshavetheformof s : e . ., e -. Given the store \ns, an expression e evaluates to a hash ' value . and a new expression e . The variable expression x, \nsample expression sample(r1, r2), and the binary opera\u00adtion v1 binop v2 are not discrete factors so that \ntheir eval\u00ad  E ::= E;s | [\u00b7]s | x :=[\u00b7]e | if [\u00b7]e then s1 else s2 | [\u00b7]e binop e | v binop [\u00b7]e | discrete( \nf, [\u00b7]e) | [\u00b7]e .. 0 DEFINITION: Store s : Var .Value Hash . . Z getSample(l, r1, r2) : sample a value \nat l within errorbound [r1, r2] e ' EXPRESSION RULES s : e . ., e - e e s : x .., s(x) - -s : sample(r1, \nr2)l .., getSample(l, r1, r2) e e s : v .l 0 -. lT, T if v . 0 s : v .l 0 . lF, F if -\u00ac(v . 0) e s : \nv1 binop v2 .., v3 where v3 = v1 binop v2 - e s : discrete( f, r) -n where n = . n, f(r) s ' . s ' STATEMENT \nRULES s : s -, s s : x :=l v s-. s[x . v], skip s : skip;s s-. s, s s : if T then s1 else s2 s-. s, s1 \ns : if F then s1 else s2 s-. s, s2 s s : while e dos . s, if s else skip -e then s;while e do ' GLOBAL \nRULES s, ., s . s ' , . ' , s 5. SamplingAlgorithms In this section, wediscuss a number ofsampling algorithms \nand their safety. The .rst one is a greedy but unsafe algo\u00adrithm that aggressively avoids collecting \nunnecessary sam\u00adples.The secondoneis animprovedalgorithmthatprovides certainguarantee.Thethird oneworkswith \na .xed sampling budget(i.e. the number ofsamplesispre-de.ned).All these algorithms assume single uncertaininputfor \nsimplicity.Our technique supports multiple uncertaininputs. Algorithm1 GreedyAlgorithm. Input: apair \nof samplepoints .1 and .2. sampleDriver (.1,.2) 1: .1 := P(.1) 2: .2 := P(.2) 3: return .1\u00b7 sampleInside(.1, \n.1, .2, .2) \u00b7 .2 e s ' ' - - s : e . .e, e s : s . s ' , s s, ., E[e]e . s, . . .e, E[e ' ]e s, ., E[s]s \n. s ' , ., E[s ' ]s [H-EXPR] [H-STMT] Figure4. HashingSemantics uation generates a void hash value, denoted \nas .. Observe that we don t hash uncertain input values despite the fact that they are the origin of \nexecutiondifferences.In contrast, the hash for a relational expression v .l 0is a uniqueinte\u00adger representingthelabelof \nthe expression l andthebranch outcome. Intuitively, if these relational operations are used in conditional \nstatements or loops, the hash values capture the execution control .ow. The hash for a discrete function \nis thegenerateddiscrete value. Statement rules are standard.Theglobal rulesare of the ' form s, ., s \n. s ' , . ' , s ,in which s is the store and . the globalhash.Rule[H-EXPR]speci.es an evaluation step \nre\u00adgardingexpression e.It aggregates thehash value .e gener\u00adated by the expression evaluation to the \nglobal hash .. Op\u00aderator . denotesthehash operation.In ourimplementation, we use addition asthehash operation \n2.Forthe voidhash ., wehave . . .= .. Rule[H-STMT] speci.es one stepin evaluating a state\u00adment. According \nto the hashing semantics, we essentially ag\u00adgregate the branch outcomes of all the predicate instances \nencounteredduring execution and the values of all thedis\u00adcretefunctions,including those embeddedin an \nexpression. Itfeatureslowruntimeoverheadasit only entailsadditions at selected places. This is critical \nfor practicality of white\u00adbox sampling because if the technique were heavy-weight, one could simply collect \nmany random samples. 2Addition is not a perfect hash scheme. However, our experience shows that such \na simple schemeis suf.cient. Input:two samples andtheirhashes; Output: a sequence of samplepointsin(.1, \n.2); De.nition: t denotesthe terminationthreshold; sampleInside (.1, .1, .2, .2) 4: if .1 = .2 .|.1-.2|< \nt 5: return nil 6: else { 7: .m := (.1+ .2)/2 8: .m := P(.m) 9: return sampleInside(.1, .1, .m, .m) \u00b7 \n.m \u00b7 10: sampleInside(.m, .m, .2, .2) 11: } 5.1 GreedyAlgorithm Algorithm1describes agreedyalgorithm.It \ntakestwo sam\u00adples asinputandgenerates a sequence of samples,including the two inputs. Ideally, the samples \nsuf.ciently expose dis\u00adcontinuity. Function sampleDriver(.1, .2) represents the overall process. It .rst \nexecutes the two input samples to produce two hash values. It then calls function sampleInside(). The \nfunctionreturnsthe neededsamplesinsidethe range(.1,.2), excluding the two samples themselves. The .nal \noutput is the resulting sequencefrom sampleInside() prepended with .1 and appended with .2. InfunctionsampleInside(),the \nalgorithmtestsifthe two provided hashes are the same. If so, it aggressively ceases to collect more samples \nin between the two given samples (line4).Anotherterminationconditionisthat eventhetwo hashes are different, \nif the distance of the two provided samples is less than a pre-de.ned threshold t (line 4), no more samples \nare collected.Otherwise,it computes another sample representing the mid value of the range(line7), and \nthen recursively calls sampleInside()forthetwo subregions. The resulting subsequences are concatenated \nwith the mid\u00adsample(lines9 and10).  An example canbefoundinFig.2(c).Giventheinitial samples a and b,part \nofthe sampling sequenceis a\u00b7 e\u00b7 f \u00b7 d\u00b7 c\u00b7 .... Despiteits simplicity, thegreedyalgorithmis not safe.It \nmisses thediscontinuityin[d,c]. Figure 5. Design space.Ais a uniformMC samplingpro\u00adcess with asmallinterval,B \naprocesswith alargeinterval, andC ourtwo-threshold algorithm.  5.2 Two-ThresholdAlgorithmwithGuarantee \nGiventwo sample runs withthe samehashvalues, assuming a perfect hashing scheme, it is unfortunately intractable \nto determineifitis safeto avoidsamplinginbetween.No mat\u00adter how small the range delimited by the two \nsamples is, it is alwayspossibletohavea conditional statementpredicat\u00adingon an expressionthatis not monotonicinthe \nregion(e.g. line6inFig.2)such that even though thepredicatehas the samebranch outcome atthetwo samplepoints(e.g. \nd and c in Fig. 2), it may have a different branch outcome some\u00adwhereinbetween(e.g. ginFig.2), renderinga \nsamplingde\u00ad cision based on the hash values at the boundaries unsound. Notethatthe mathematicalform ofthe \nexpressionislikelya high orderfunctioninpractice such that reasoning aboutits monotonicityanalyticallyisinfeasible.Next, \nwedescribe an algorithmthatprovides certainguarantee while retainingthe advantage of white-box sampling. \nDe.nition3. We sayadiscontinuouspointd(i.e. a valuein the range of the uncertaininput that causesdiscontinuityin \nthe output)isdetectedif and only if thereexisttwo samples .i and .i+1 in sequence such that: (1)d.[.i, \n.i+1]; (2)thereis not anotherdiscontinuouspointd '.[.i, .i+1]; (3)theirhashes .(.i) = .(.i+1). Intuitively, \na discontinuous point is detected if its pres\u00adenceis capturedby two samples withdifferenthash values \nand there are no otherdiscontinuouspointsinbetweenthe two samples. Theorem 2. A regular MC process that \nperforms uniform sampling with an interval t guarantees to detect any dis\u00adcontinous point that has a \ndistance = t to its neighboring discontinouspoints. Proof:Let d be such adiscontinuouspoint and dp, ds \nbeits immediate preceding and succeeding discontinuous points, respectively.There mustbetwo neighboringsamples \n.p and .s suchthat .p .[dp, d]and .s .[d, ds].Assumingaperfect hashing scheme and all mathematical library \nfunctions are continuous, .(.p)= .(.s), d isdetected.. AuniformMC with a smallinterval ts has strongguaran\u00adteebut \nalso ahigh cost, asdenotedbypointAin thedesign spaceinFig.5.In contrast, a uniformMC with alargeinter\u00adval \ntl has weakguaranteebutalow cost(pointB). WedevelopaMC algorithmthat can achievethebene.ts of both A \nand B (i.e. point C in Fig. 5). The algorithm takes two sampling intervals tl and ts, denoting the large \nand smallintervals, respectively.Ithas alow cost close toB and the practical precision close to A, that \nis, it can detect a set of discontinuous points close to A in practice. In the theoretically worst case, \nit provides the same guarantee as B. The idea is to continue taking additional samples in be\u00adtween two \nsamples thathavethe samehash value,if thedis\u00adtancebetweenthetwosamplesislargerthantl (i.e.thelarger intervalprovided).It \nis describedin Algorithm2. Ithas the samedriverasthegreedy algorithm.The onlydifferenceis atline4in sampleInside \n(), whichis the termination condi\u00adtion of sampling. Algorithm2 Two-ThresholdAlgorithm. De.nition: tl \ndenotes the termination threshold for re\u00adgions withthe samehash; ts denotes the termination threshold \nfor re\u00adgions withdifferenthashes sampleInside (.1, .1, .2, .2) 4: if(.1 = .2.|.1-.2|< tl) . (.1= .2.|.1-.2|< \nts) 5: return nil 6: else { 7: /*thesameaslines7-10inAlgo.1*/ 8: } In the exampleinFig.2, with tl = 0.5and \nts = 0.15, we get the sequence ofsamples asdescribedin(b),in which an additional sample g is takenin[d,c]such \nthat alldiscontin\u00aduouspoints aredetected.  5.3 SafetyInPractice While in the worst case, the two-threshold \nalgorithm with thresholds tl and ts can only provide the same guarantee as a standard uniform sampling \nalgorithm with the large threshold tl, in practice, we observe that the algorithm is almost as precise \nas a standard algorithm with the small threshold ts (see Section 8). In this section, we discuss the \nreasonsbehindthis. Intuitively, we observe that the majority of the discrete factors are monotonic. Recall \nthat a discrete factor d turns a real valueto adiscrete value.Wedenotethe real value of a discrete factor \nas a function dr(x) of the uncertain input x. Assume through sampling, we observe that two sample runshave \nthe samehash.They musthave the samediscrete  z //x in [1.0,3.0] 1. y=x-2.0;  2. if (y>=0)  3. f=x+3.0; \n 4. else  5. f=3.0-x;  6. z=f-5.0;  7. if (z>0)  8. o=...;  1.0 0 -3.0 -4.0 (a) (b) x Figure \n6. Function z(x) of the discrete factor at line 7 has two live ranges: one corresponds to y>=0 being \ntrue and the otherbeingfalse.They representtwofunctions z(x)= x-2.0and z(x)= -x-2.0. valuesfor allthediscretefactors,includingthat \nof d.If dr(x) is monotonic withinthe rangedelimitedbythetwo samples, it is easy to infer that the discrete \nvalue of d for each input value withinthe range mustbe the same 3. For example,if the value of z in thepredicate \nof a condi\u00adtional statement if z . 0... is monotonicinthe range[.1, .2]anditproducesthetruevalueinbothsampleruns,itmust \nconsistentlyproduce the same true valuefor any samplesin between.The aggregatedeffect of all such monotonicpredi\u00adcatesisthatthesamecontrol.owmustbetakenforall \nsam\u00adplesinside the range; similarly, all thediscrete co-ef.cients in the outputfunction mustalsohave \nthe same value, ensur\u00ading the same mathematicalform of the outputfunctionand also the continuity within \nthe range. Formally,the realfunction of adiscretefactor dr(x) does not need to be monotonic within the \nentire error bound of x to ensure safety. Instead, we introduce the notion of live range of adiscretefactor, \nwhichis essentially a sub-rangein the errorboundofx.Adiscretefactor mayhave multiplelive ranges.Aslong \nas alldiscretefactors are monotonicin each of their live ranges, not necessarily the entire error bound, \nsafetyis ensured. De.nition 4. A sub-range [.1, .2] of the uncertain input error boundis a live range \nof adiscretefactord if and only if for any .t .[.1, .2], all the discrete factors encountered duringprogramexecutionP(.t) \nbefored musthavethe same discrete value. Alive rangehas thefollowingproperty. Property 1. The real function \ndr(x) of a discrete factor d musthave the same mathematicalforminitslive range. Thepropertycanbe easilyderivedfrom \nthede.nition of live range. Intuitively, within the entireinputerrorbound,there may beinputsthatinducedifferentcontrol.owpathsleadingto \na 3An implicit assumption is that discrete operations themselves are always monotonic, whichistrueinpractice.Forinstances,typecasts \nand compar\u00adisons are monotonic operations. discretefactoranddifferentdiscrete co-ef.cientscomputed along \nthesepaths,hence, the realfunction of thefactor may be ofdifferentforms.Alive range represents aninput \nsub\u00adrangethathas the sameform. Example.Consider the exampleinFig.6(a).There are two discretefactors: \nthe comparisons atlines2and7.Forline2, thereis only oneliverange,whichistheentireerrorbound [1.0,3.0].Therealfunctionhasonlyoneformy(x)= \nx-2.0. Forline7,there aretwolive ranges,[1.0,2.0] and(2.0,3.0] as theprecedingdiscretefactor atline2hasdifferent \nvalues for the two ranges. The factor hence has two mathematical forms as showninFig.6(b).. Withthe abovede.nitions, \nwehave a suf.cient condition for safety. Theorem 3. If the real functions of all discrete factors are \nmonotonic in their live ranges, it is safe to skip samplings within two samples with the samehash value. \nThetheorem canbeprovedbycontradiction.Theproofis elidedforbrevity.The theorem essentiallydemandsthat \nthe individualmathematicalforms of adiscretefactorbe mono\u00adtonic.It substantiallylowers the requirement \non monotonic\u00adity to ensure safety: a discrete factor does not need to be monotonic in the whole sampling \nrange, but rather its in\u00addividual live ranges. In practice, live ranges could be very small. For examplein \nFig.6, although z(x) is not monotonicin the whole sampling region[1.0,3.0],itis monotonicinthe two live \nranges. According to Theorem 3, even the greedy algorithmis safefor thisprogram. Althoughwehavereducedthe \nsafetyproblemtotheprob\u00adlem ofdeterminingmonotonicity ofdiscretefactorsin their live ranges, solvingthe \nmonotonicityproblem analyticallyis still very challenginggiventhemathematical complexity of the real \nfunctions of discrete factors and the resource con\u00adstraint imposed by the design goal of competing with \nran\u00addom sampling. We instead perform empirical study to ob\u00adserve the monotoniciy in practice. We make \nthe following observations.Empirical support canbefoundinSection8. 1. The majority (94-100%) of the discrete \nfactors are in\u00addeed monotonic in their live ranges. Live ranges could be very small(<10% of the entire \nsampling region for complexprograms).This suggeststhat we mayhave very few safety violationsinpractice.We \nmake thefollowing speculation. Data processing algorithms are developed by humans and humans are usually \nnot good at reason\u00adingand controlling.uctuatingfunctions.Therefore,they use many conditional statements \ntodivide such complex functionsinto monotonicregionsthattheycan easilyrea\u00adson about.Weplanto conductstudy \nof codepatternsin thefuturetodeepen our understanding. 2. Somediscretefactorshave non-monotonicrealfunctions \n(0-6%), but most ofthem always produce the same dis\u00adcrete value and thusdon tlead to any false negativesin \n  sampling.The majority of the cases are causedby apat\u00adtern similartothefollowing:the non-monotonicfunction \nis the addition of a slowly changing function with large valuesand a .uctuatingfunctionwith very small \nvalues. Hence,despitethe non-monotonicity,afterdiscretization, the same valueis alwaysyielded. 3.There \nare some very rare cases(2intotalin our study) that are both non-monotonic and yielding different dis\u00adcrete \nvalues. They are the real threats to safety. How\u00adever, the corresponding program was written in such \na way that there exist correlated predicates that guard the non-monotonicity. Our observations are limited \nto the benchmarks and the experimental setup. However, even in the worst scenario thattherealfunctionsofdiscretefactorsare \nnotmonotonic in their live ranges and they yield different values after discretization, the two-threshold \nalgorithm is still able to provide certainlevelofguarantee. Ensuringthe safetyof avoiding samplinginbetween \ntwo runs with the same hash represents only one aspect of the overall soundness. The termination threshold \nts also indi\u00adcates that we may notdetect segments that are smaller than ts.It maybefeasibletodesign a \nmore sophisticatedtermina\u00adtion condition: theprocess terminates only when thediffer\u00adencebetweenthe sequencesofdiscretefactorsoftwo \nsample runsis minimal,forinstance,theircontrol .owpathsdiffer by only onepredicate.The challengeliesinbalancing \ncom\u00adplexity and the entailed overhead as we are competing with the very cheap uniform sampling.We willleaveit \nto ourfu\u00adture work.  5.4 FixedBudgetAlgorithm We have also developed a best-effort algorithm that tries \nto performoptimal samplinggivena .xedbudget(i.e.a .xed number of samples allowed). The algorithm starts \nin the same way as the greedy algorithm but employs a priority queue to select the next sample point. \nWhen enqueued the subregions areprioritizedbased on:(1)iftheyhavedifferent hash values; (2) their sizes. \nAt each step it dequeues one subregion to further sample in between from the priority queue, untilthebudgetis \nused up. 6. HashingSlices In the algorithmsdiscussedin theprevious section, aglobal hashvalueis computedfor \nan execution.However,this may be too restrict in practice. In some cases, even though two executionshavedifferenthashvalues,thedifferencemaynot \nbe relevantto the output. In this section, we discuss a more sophisticated hashing algorithm that hashes \nonly the discrete factors relevant to the output. It maintains a hash value for each variable on the \n.y, denoting the set of discrete factors that have been directly/indirectly used to compute the current \nvalue of the variable(i.e.discretefactorsinitsdynamic slice [1]).When Algorithm3 Fixed-budgetAlgorithm. \nInput: apair of samplepoints .1 and .2 and abudget B. sampleDriver (.1,.2, B) 1: .1 := P(.1) 2: .2 := \nP(.2) 3: Q.enqueue(< .1, .1, .2, .2 >) 4: return .1\u00b7 .2\u00b7 sampleInside(Q, B) Input: apriorityqueue Qand \nabudget B; Output: a sequence of samplepointsin(.1, .2); De.nition: t denotesthe terminationthreshold; \nsampleInside (Q,B) 5: < .1, .1, .2, .2 >:= Q.dequeue() 6: if .1 = .2 .|.1-.2|< t . B.overbudget() 7: \nreturn nil 8: else { 9: .m := (.1+ .2)/2 10: .m := P(.m) 11: Q.enqueue(< .1, .1, .m, .m >) 12: Q.enqueue(< \n.m, .m, .2, .2 >) 13: return .m \u00b7 sampleInside(Q, B) 14: } we determine whether a mid-sample is needed, \nwe com\u00adpare the hash values associated with the output variable. It is worthmentioningthat althoughconceptuallywe \narehash\u00ading the discrete factors in output dynamic slices, the com\u00adputationispeformedonthe .ywithout \nexplicitly computing anydynamic slices. The semantic rules arepresentedinFig.7.We introduce ahash store \nG that maps a variabletoitshash value.A stack S is used to propagate hash values through control depen\u00addences.Eachstack \nentryisthehash value of apredicate.We extend the syntax of expression in Fig. 3 to include a pair consisting \nof a value anditshash, whichisthehash aggrega\u00adtion ofalldiscretefactor valuesinthe slice ofthe value.This \nspecial type of expressions is not part of the source code. Theyonly occurduring evaluation. e' The expression \nruleshave theform of s, G : e . e -.in which G is the hash store. For a variable expression x, if its \nhash value is void, it evaluates to a regular value; if not, it evaluatestoits value and the correspondinghash. \nA sampling expression evaluates to a sample value ac\u00adquiredfrom outside anditslabell as thehash.Notethatitis \nthe onlyplace thatinitiates a non-voidhash.Itis analogous to theintroduction of a taintin the taint analysis.In \nthe sub\u00adsequent execution, a valueis related to the uncertaininputif ithas a non-voidhash. For a relationaloperation,ifthe \nlhs valueis apair, mean\u00ading that it is relevant to the sample input, the evaluation re\u00adsult is a pair \nconsisting of the comparison outcome and the aggregation of the lhs hash and the operation s hash. If \nthe  DEFINITION: HashStore G : Var .Hash CDStack S ::= . Expr e ::= ... |(v, .) e ' EXPRESSION RULES \ns, G : e . e - ee s, G : x . s(x) s, G : - -if G(x)= . x .(s(x), G(x)) if G(x)= . e s, G : sample(r1, \nr2)l - .(getSample(l, r1, r2),l) e s, G : (v, .).l 0 -v . 0 .(T, . .lT) if e s, G : v .l 0 -v . 0 . T \nif e s, G : (v1, .1)binop (v2, .2)-= v1 binop .(v3, .1 . .2) where v3 v2 e s, G : (v1, .1)binop v2 -= \nv1 binop v2 .(v3, .1) where v3 e s, G : v1 binop v2 . v3 where v3 = v1 binop -v2 e s, G : discrete( f, \n(r, .)) - .(f(r), . . f(r)) e s, G : discrete( f, r) -f(r) . s '' . s ' , G ' STATEMENT RULES s, G, S \n: s -, S , s s s, G, S : x :=l (v, .). s[x . v], G[x . . . last(S )], S , -skip s, G, S : x :=v . s[x \n. v], G[x . last(S )], S , l -sskip if last(S )= . s, G, S : x :=v . s[x . v], G, S , l -sskip if last(S \n)= . s s, G, S : if (T, .)then s1 else s2 . s, G, S \u00b7 (last(S ) . .), s1;endif - s s, G, S : ifT then \ns1 else s2 . s, G, S , s1 - s s, G, S \u00b7 .t : endif . s, G, S , -skip s '' . s ' , G ' GLOBAL RULES s, \nG, S , s -, S , s es '' s, G : e . e s, G, S : s -, s -. s ' , G ' , S ' s, G, S , E[e]e . s, G, S , \nE[e ' ]e s, G, S , E[s]s . s ' , G ' , S ' , E[s ' ]s [S-EXPR] [S-STMT] Figure7. SliceHashingSemantics \nlhs valueis a singleton,the evaluationproducesthe single\u00adton comparison outcome.Notethat we omitthe rulesforthe \nfalse casesforbrevity. For abinaryoperation,if either valueis apair,the result\u00adingvalueis also apair,includingthe \naggregatedhash value. For a discrete function application, if the parameter is a pair,thegenerateddiscrete \nvalueis aggregatedto thehash. The statementruleshavetheform of s'' . s ' , G ' s, G, S : s -, S , s ,in \nwhich S is the stackto allow hash computation through control dependence. For an as\u00adsignment statement,if \nthe rhs valueis apair,theevaluation updatesboththe store andthehash store.Thehash ofthe lhs variableis \nthe aggregation of the rhs expressionhash . and thehash ofits controldependence, whichis thelast entryin \nS .If the rhs valueis asingletonanditscontroldependence hashis not void,thehash storedisupdated with \nthecontrol dependencehash.It means that althoughthe assigned value is not computedfrom the sampleinput, \nthe execution of the assignmentisguardedby apredicate relevant to the sample input. Fora conditional \nstatement,if theevaluationof therela\u00adtional operationyields apair, the stackis appended with the aggregationofthepredicate \ns own controldependencehash, i.e. the last entry of S , and the relational expression hash ..Sincethe \naggregatedhashbecomesthe newlast entry in S , future evaluations occur inside the branch will use it \nas trace val hash S 1. x=sample(1.5) 2.2 1 . 2.y=(int) x 2 1. 2 . 3.if x-1.0<0 F3F . 1 .\u00b7 (3F . 1) 6. \nif t(x)-0.3>0T 6T . 1. (3F . 1) .\u00b7 (3F . 1) \u00b7 (6T . 1. 3F . 1) 7. o=0.3 0.3 6T . 1. 3F . 1 .\u00b7 (3F . \n1) \u00b7 (6T . 1. 3F . 1)  endif .\u00b7 (3F . 1) endif . 10. z=1+y 3 1. 2 . Table1. Evaluation oftheprograminFig.2(a)with \nsample 2.20. their controldependencehash, re.ectingthat evaluationsin\u00adside a branch of a conditional \nare control dependent on the predicate of the conditional. The evaluation also appends a special statement \nendif to the end of thebranch.Evaluation of an endif statement leads to the removal of the last entry \nin S , meaning evaluations beyond the end of a branch are no longer controldependent on thepredicate.Note \nthat the LIFO nature of the stack captures the nesting effect of the controldependence. If the evaluation \nof the relational operationyields a sin\u00adgleton, there is no need to append a new entry to the stack or \nthe endif statement to the end of thebranch,denotingthe irrelevance of thepredicate.Note that any statements \nevalu\u00adatedinside thebranch nonethelesshave their controldepen\u00addencehashinheritedfromthe currentlast entry \nof S . Example. Table 1 presents an example evaluation of the program in Fig. 2(a) with a sample . = \n2.20. Observe that at line 1, the hash is 1, the label of the statement. At line 2, the hash is the aggregation \nof x s hash and the generated discrete value2.Atline3, thehashis the aggregation of x s hash and the \nbranch outcome 3F; it is also appended to S . The entryis removedfrom S at the second endif.Atline6, \nthehashisthe aggregationof x s hash, the branch outcome 6T, andthe controldependencehash.Atline7,thehash \nof o is inherited through controldependence.Atline8, the hash of z isinheritedfromy. Safety.Wehavethefollowingsuf.cientconditionfor \nsafety. Theorem4(Safety-Slice). Given twoinputsamples .1 and .2, assume the slice hashes of the output \nvariable are iden\u00adtical in the two runs. For a discrete factor that generates a discrete valuefrom a \nrealvalue,letthe real valuebedenoted as afunctiondr(x) over the uncertaininput x.If (1)alldr(x) are monotonicin \ntheirlive ranges; (2) all memory addresses remain unchanged within the range, then the outputfunction \nmustbe continuousin[.1, .2]. Condition(1) requires alldiscretefactorsin the execu\u00adtion, not only in \nthe slice, to be monotonic. Consider the example in Fig. 8(I), function f is non-monotonic. In the two \ninitial sample runs with x = 2.0 and x = 4.0, the false branchofline3is taken.As a result, statement4is \nnot exe\u00adcuted.Thedynamicslice of o at 5 containslines 1 and2 in  (I) (II) Figure8. Examplesfor safetyissues. \nboth runs, withoutincludingline4.As a result, alldiscrete factorsinthe slices are monotonic.However,we \ncan easily seethatitis unsafeto skip sampling.Condition(1)precludes such cases. Condition(2)is topreclude \narrayindices orpointersdif\u00adferences(note that ourdiscussionheregoesbeyondthelan\u00adguageinFig.3).Theycouldcause \noutputdiscontinuity.Con\u00ad sider the exampleinFig.8(II).Assume the twoinitial sam\u00adples to be x = 0.4 and \nx = 0.6. The slice of o at 6 contains onlyline2inboth runs.The twodiscretefactors: thepredi\u00adcate atline3 \nandthe cast atline4 areboth monotonic.How\u00adever, the output function is not continuous, as its value is \nde.ned atline5 when x = 0.5.Condition(2) excludes such casesbyensuringthat memoryaddressesdo notchange \nwith differentsamples. Theproofis omitted.Intuitively,since allpredicatesare monotonic and all memory \naddresses do not change, there cannotbe any newdependencesinthe output slicefor any sample in between. \nAccording to our experience, the two conditions mostly hold in practice. Note it is currently not affordabletoanalytically \nvalidatethemonthe .y. Deciding Global Hashing or Slice Hashing. Slice based hashingis more expensiveduetothe \nmorecomplexinstru\u00admentation, althoughit can avoid redundantsamples caused byirrelevantdiscretefactors.Hence,itwouldonlybebene.\u00adcial \nwhen there are enoughirrelevantdiscretefactors todis\u00adcountitshigherinstrumentationcost.We usethefollowing \nmethodtopredicttheapplicabilityofslicingbasedapproach. We runtheprogramtwice with the same sampleinput,one \nwithglobalhashing and the otherwith slicehashing.If the number of thediscretefactorsin theglobalhash \nand thatin the slice hash differ substantially, we will proceed with the slicebased approach. 7. IdentifyingContinuousCores \nA basic assumption of our technique is that if two sample runs produce different hash values, there must \nbe disconti\u00adnuitybetweenthetwo samples.However,it may notbethe case in practice. Developers can write \nprograms in such a way that the output function is continuous even though the control.owvaries.Inthissection,wediscusshowtodetect \nprogramregionsthathave suchcharacteristicsandprovethat they are continuous despite control .ow differences. \nThen thehashingalgorithms can avoid collectingpredicatehashes inside these regions. 1 x := sample(3.0); \n1 x := sample(0.5); {y= f(c)} 2 y := 0.0; 2 A[5] := ...; 1 for... 1 o = A[0] /*f(2) = f(4) = 1, f(3) \n= -1*/ 3 if x >= 0.5 2 if x < c 2 for i:= 1to c 3 if f(x) < 0 4 i:=(int) x\u00d710.0; 3 o:=f(x); 3 if o < \nA[i] 4 y:=y+ 1.0; 5 A[i] :=...; 4 else 4 o:=A[i]; 5 o:=x-y; 6 o:=A[5]; 5 o:=y; (I) (II) Figure 9. Real \ncontinuous core examples from 178.galgel. Variable o denotes the output; c denotes a compiler time constant; \nf(x) is a continuous function. The .rstlinein(I) representstheprecondition. Considerthe exampleinFig.9(I).Itis \na codingpattern used a few times in 178.galgel. The output function is f(x) when x < c. It becomes f(c) \nwhen x = c and remains that value for x > c. The developer hoists the computation of f(c) from the else \nbranch to outside of the loop for bet\u00adterperformance.Observe the outputfunctionis continuous. However,iftheinitial \ntwo sample runs arefor x = c-1and x = c+ 1,theyhavedifferentcontrol.ow. We call such code regions continuous \ncores, which are formallyde.nedasfollows. De.nition 5. A continuous core is a conditional statement s(including \nits branches) that is modeled as o = s(I) with input I the set of variables used in s and de.ned outside, \noutput o the variable computedby s and usedlaterby other statements, and s(I) is a continuous function \nin the domain ofI,despitecontrol .owvariations. The de.nition also covers loop statements, which are \na special case of conditional statement. Wedevelop apro.ling techniquetodetect candidatesof continuous \ncores. Basically, the pro.ler .rst detects predi\u00adcatesthat may evaluatetodifferentbranch outcomesin their \nlive ranges.For eachpredicatelive rangein which thepred\u00adicateevaluatestoboth trueandfalse, atitsimmediatepost\u00addominator(i.e. \nthejointpoint of the twobranches), thepro\u00ad.ler inspects the values of all the variables that are de.ned \ninside the conditional and use by others outside the condi\u00adtional within thelive range,to checkif they \nappear continu\u00adous.If so, they are continuous core candidates.We elide the details ofthepro.lerforbrevity. \nGiven a continuous core candidate, we then prove the continuity in the presence of control .ow variation. \nThe techniquein[6] triestoprovestaticallythat aprogramis continuous regarding a given set of variables. \nWe adapt the technique tohandle the conditional statementsidenti.edby ourpro.ler.Thekeyideaisto .rstprovethetwobranchesof \ntheconditional statementtobe continuous,and thenensure thatthetwo continuousfunctionshaveidenticaloutputvalue \nat the boundary input value at which the branch outcome changes.Intuitively,it meansthatthetwobranchfunctions \nyield outputs in.nitely close to the same value as the input getsin.nitelyclose to theboundary value. \nConsider the exampleinFig.9(I).The statementsin the true and false branches are both continuous on their \nown.  And observethat atthebounarypoint x = c, bothbranches yieldthe same output value, ensuring continuity. \nOther Patterns. There are afew othercodingpatternsthat give rise to continuous cores.Fig.9(II)shows another \nvery common core in 178.galgel. It returns the maximum value of an array. Observe that the program is \ncontinuous, i.e.the outputchangescontinuouslywiththe uncertaininput. For example, assume an arrayA[0-2]= \n{1.0, 2.0, 3.0}, and A[1] is uncertainandit varieswithinrange[2.0, 4.0].When A[1] changes from 2.0 to \n4.0. The output function over the uncertaininput o1(A[1]) = 3.0 when A[1] changesfrom2.0 to3.0and then \no2(A[1]) = A[1] when A[1] changesfrom3.0 to 4.0. Observe that o1 and o2 have the same value at the boundaryA[1]= \n3.0,hencetheytogetherdenote a continous function. Toprovethepatternis continuous.We completelyunroll \nthe loop. Each unrolled iteration has the following form, with oi the outputde.ned at the ithiteration. \n3 ifoi-1 < A[i] 4 oi:=A[i]; else oi:=oi-1; Observethatthefunctionsfrombothbranchesare contin\u00aduousbythemselves,andtheyhavethesame \nvalue oi = A[i] at the boundary oi-1 = A[i].Wehaveencountered afew more corepatterns.They canbeproved \nsimilarly. 8. EmpiricalEvaluation Our system consists of several components. A modi.ed compiler, to instrument \nprograms to compute the hash val\u00adues,isbuilt ontop of gcc.The samplingdriveris writtenin Python. Our \nsystem supports both C/C++ and Fortran. It is publicallyavailable4. Our experiments are performed on \nan Intel i7 2.70GHz machine with4GBRAMinstalled.We useSPECCFP2000 andonebiochemicaldataprocessingprogram(deisotope) \nas the benchmark set. Three programs from SPEC CFP 2000 are excluded. 189.lucas is a program that identi\u00ad.es \nprime numbers and hence uncertainty analysis is not applicable. 177.mesa and 179.art are excluded as \nthey takediscreteinputs.Wehavetotally 12programs(3C and 9Fortran).We randomly selecttheuncertaininputs.Foran \nuncertain input value v, we assume its error bound to be [50%*v,150%*v]. Table 2 shows the basic characteristics \nof the programs. Itincludesthelines ofcode,the order ofpredicatefunctions (regardingthe uncertain input), \nthe non-polynomial mathe\u00admaticalprimitivesinvolvedin thesefunctions and the num\u00adber of(continuous)segmentsinthe \noutputcurve.We acquire thepredicatefunction orders andthe non-polynomialprimi\u00adtives throughpro.ling. \nAs we can seefromthetable, a number ofprogramshave highorderpredicatefunctions.Note that, ourpro.ler \nworks 4http://www.cs.purdue.edu/homes/tbao/smartMC.tar.gz program LOC pred. order non-poly funcs in pred. \n# of segments 168.wupwise 5K -2 ~2 cos, log, sqrt 1 171.swim 466 0 1 172.mgrid 463 -1 ~2 abs, sqrt 1 \n173.applu 4K -1 ~17 abs, sqrt 1 178.galgel 27K 2 abs, sqrt 100+ 183.equake 2K -1 ~3 sin, cos, sqrt 6 \n187.facerec 3K 2 abs, sin, sqrt 5 188.ammp 14K 2 sin, cos, sqrt 1 191.fma3d 60K -3 ~6 sin, abs, sqrt \n1 200.sixtrack 47K -1 ~2 sin, abs, sqrt 1 301.apsi 7K -1 ~14 sin, abs, sqrt 2 -4 deisotope 2K 2 4 Table2. \nProgram characteristics. bycomputingtheorderofthelhsvaluefromtheordersofthe rhs values.Hence,ithas to \napproximatein some situations. Given h(x)=(1/ f(x))(g(x)) in which f(x) is afunctionof order 1 and g(x) \nof order 2, we conservatively assume the result h(x) will have the order of 1. Moreover, most of the \nprogramshavenon-polynomialprimitivefunctionsinvolved, such astrigonometricfunctions or square root.This \nsupports our earlier discussion about the dif.culty of applying sym\u00adbolic techniquesto analyzingthepathconditions. \nAlso observethat7 out of the12benchmarks are contin\u00aduous.However,itdoes not mean our techniqueis not \nuseful for them. Black-box MC approaches will have dif.culty in determiningif there are smalldiscontinoussegments \nalong the output curve(case(c)inFig.1). 8.1 MonotonicityofDiscreteFactors Inthe .rstexperiment,we studythe \nmonotonicityofdiscrete factors.We check thechangesof themonotonicityforeach liverangeof everydiscretefactor.We \ncollect1000 samples for eachprogramusing a regularMC algorithmto conduct our study.Table3 shows the results.Column2 \ncontains the numberofdiscretefactorsin eachprogram.Some ofthepro\u00adgramshave continuous cores so that their \nnumbers are after excludingthepredicatesinthecores.The mono. column shows the percentage of the discrete \nfactors that are mono\u00adtonic in their live ranges. The \u00ac mono. . fixed-val column presents the percentage \nof the discrete factors that are not monotonicbut alwaysyieldthe same value afterdis\u00adcretization.The \n\u00ac mono. .diff-val columnpresents the number of those that are neither monotonic noryielding thesamediscretevalue.Thesearethe \ncasesthat couldlead to safety issues. The last column shows the length of live ranges as thepercentage \nof the entire errorbound. Thediscussion ofthe results canbefoundinSection5.3. The results imply that \nit is highly unlikely for our algo\u00adrithms to miss samples when they cease to collect sam\u00adplesdue toidenticalhash \nvalues.There are only twopoten\u00adtiallyharmfuldiscretefactorsbeingobserved(onein each of 183.equake and \n187.facerec),bothpredicates.How\u00ad  program # d\u00adfactor mono. \u00ac mono. . .xed-val \u00ac mono. . diff-val avg. \nlive range 168.wupwise 171.swim 172.mgrid 173.applu 178.galgel 183.equake 187.facerec 188.ammp 191.fma3d \n200.sixtrack 301.apsi deisotope 0* 0* 0 52 3219K* 23043K 1891K 8070K 1521 40364K 301333K 121K ---100% \n98.68% 99.97% 94.46% 100% 100% 100% 100% 99.99% ---0 1.32% 0.03% 5.54% 0 0 0 0 0.01% ---0 0 1 1 0 0 0 \n0 0 ---100% 4.37% 9.96% 7.42% 100% 100% 100% 2.94% 29.25% *The numbers are afterprecluding continuous \ncores. Table3. Monotonicityofdiscretefactors. ever, they did not cause any problem because there exists \nanotherpredicate(after theproblematicpredicate) thathap\u00adpenedtohavedifferentbranch outcomes at theboundary \nof the non-monotonic live range of the problematic predicate functions,entailingdifferenthashes and thusmoresamples \ninbetween. 8.2 RuntimeOverheadand theGreedySampling Algorithm The second experimentisto evaluatethe runtime \noverhead of the two hashing semantics. The results are shown in Ta\u00adble4.Columns3-5present the resultsfor \nthebasic(global) hashing scheme.Observe thatitis highly ef.cient(an aver\u00adage of2.67% overheadfor each \nsample run). Columns6-8present the resultsfor the slice-basedhash\u00ading scheme. Observe that it is more \nexpensive, with an av\u00aderage overhead of 231%. This is due to its more heavy\u00adweight instrumentation. Another \nreason is that we haven t triedhard to optimize ourimplementationyet.Observe that it makes differences \non three programs: 187.facerec, 301.apsi and deisotope.For deisotope,it reduces thenumberofsamplesfrom55to20,while \nstillpreciselyex\u00adposes all thediscontinuouspoints.For 187.facerec and 301.apsi,the reductions are muchlarger(from117to12 \nandfrom167to24 respectively) and easilypay offthe extra overhead. Thereasonfornotobservingmorebene.cialcasesforthe \nslice-based approach is that most of the benchmarks have very cohesive coding structure. They tend to \nhave a very small number of outputs, and most intermediate computa\u00adtiondirectly/indirectlycontributestothese \noutputs.We spec\u00adulate for larger scale scienti.c programs, when more func\u00adtionalities areintegratedinto \naprogram, we willhave abet\u00adter chancetoobservethebene.t.Notethatherewepresent the results of the two \nhashing schemes only for evaluation and comparisonpurpose.Asdiscussed earlier(inSection6), we have an \neasy method to predict which hashing scheme shouldbe used andthe useris supposedtojust apply one approach. \n It is also worth mentioning that the overheads are not affectedby the sampling algorithms. We observe \nthat the greedy algorithm is very effective formostprograms.Forthosethat are continuousinthe en\u00adtire \ninput error bound, the algorithm was able to identify that the hashes of the initial two sample runs \nare identical, it then stops collecting more samples right away. Except 178.galgel, the discontinuous \npoints of all programs are precisely detected by both the basic and slice hashing schemes(i.e. each continouspointisdelimitedby \ntwo sam\u00adples with different hashes). This is not surprising because according tothe study of the monotonicity \nandTheorem3, we almost never miss discontinuous points when we stop collecting more samplesduetoidenticalhash \nvalues.  8.3 EffectivenessoftheSamplingAlgorithms In the third experiment, we study the effectiveness \nof the threeproposed sampling algorithms.We .rstcollect10000 uniformsamples using a regularMC to acquirea \nverypre\u00adcise output curve, called the ideal curve. Then we measure the error of the curvesgeneratedby \nthedifferent algorithms regardingtheideal curve as showninFig.10.Inthis experi\u00ad ment, wefocus on theprogramsthathavediscontinuity. \n program native basic slice time overhead # of samples time overhead # of samples 168.wupwise 2.05 2.14 \n4% 2 6.56 220% 2 171.swim 0.15 0.15 1% 2 0.36 142% 2 172.mgrid 3.31 3.32 0% 2 11.13 236% 2 173.applu \n0.06 0.07 6% 2 0.23 271% 2 178.galgel 0.69 0.70 10% 416 3.34 384% 416 183.equake 0.20 0.21 6% 66 0.36 \n81% 66 187.facerec 1.23 1.25 2% 117 4.12 235% 12 188.ammp 2.72 2.73 0% 2 3.51 29% 2 191.fma3d 0.02 0.02 \n0% 2 0.06 200% 2 200.sixtrack 2.22 2.27 2% 2 12.60 468% 2 301.apsi 1.75 1.77 1% 167 9.02 415% 24 deisotope \n0.02 0.02 0% 55 0.04 90% 20 AVERAGE 2.67% 231% Table4. Ef.ciencyof thebasic(global)hashing and the slicebasedhashing.The \nresults arebased on thegreedy algorithm Greedy 2-Threshold Fix-Budget program |.| err |.| err |.| err \n178.galgel 416 0.58 462 0.58 250 0.76 183.equake 66 0.11 101 0.07 50 0.35 187.facerec 71 0.50 102 0.54 \n50 0.45 301.apsi 24 0.80 76 0.26 40 0.40 deisotope 55 0.12 93 0.52 45 0.13 AVERAGE 0.42 0.39 0.42 Table5. \nEffectiveness ofthe three samplingalgorithms. |.| is the number of samples; err denotes the ratiobetween \nour error and the error of the same number of uniform samples. Table 5 shows the number of samples needed \nfor each algorithm and the corresponding relative errors. A relative erroristhe error of the curveacquiredby \nour approachdi\u00advidedby the error of the curvegeneratedby the same num\u00adber of uniform samples. For example, \nthe relative error of 183.equake for the greedy algorithm is 0.11 means that the errorofthecurvewiththe66greedysamplesis \nonly11% of the error of the curve with 66 uniform samples. For the two-thresholdalgorithm,thetwothresholds \nare tl = 2%and ts = 0.1%ofthe errorbound,equivalentto collecting50 and 1000 samples,respectively.Forthe \n.xedbudgetalgorithm, becausethe curvesofdifferentprogramsareverydifferent, we use roughly half the number \nof the two-threshold sam\u00adples as thebudget. From the results, we observe the following. All three algorithms \nare effectively producing more precise curves compared to thosegeneratedby uniform sampling.In some cases, \nthe error of our approach is only 7% of the error of uniform sampling. The relative error of 178.galgel \nap\u00adpears not so impressive as the others. But we will see from our later case study that the regular \nMC misses many dis\u00adcontinuous points while we don t. It is not re.ected in the relative errorbecausethe \nmissing segments are so smallthat their contributions to the error are also small. The relative error \nof 301.apsi is larger than others because the con\u00adtinuous segments are curvy and our algorithms avoid \ncol\u00adlecting samples inside continuous segments. Note that our algorithms anyway capture all thediscontinuouspoints. \nThe greedy algorithm requires less number of samples comparedtothe two-threshold algorithm andthe relative \ner\u00adrors of these two approaches are comparable. Sometimes, the greedy algorithm has a smaller number \nof samples but larger relative error (183.equake) because it does not collect samples in continuous segments \nsuch that its curve hasaworse .t.Sometimes,thetwo-thresholdalgorithmhas a larger relative error even \nthough it collects more sam\u00adples(deisotope). The reason is that the curve is simple enough such that \neven uniform sampling has low absolute errors. To better understand the bene.t of our algorithms, we \ngradually increase the number of uniform samples from a number smaller than the samples of our methods(say50) \nto 10000 anddepictthe changeof samplingprecision(i.e.the error between the approximate curve and the \nideal curve) with respecttothe numberof samples.Wethenprojectthe results from our algorithms to such \n.gures. The results are shown in Fig. 11. In the .gures, the y axis represents the precision with0denotingthehighestprecision(with10000 \nsamples) and1 thelowestprecision(with50 samples).We can clearly see our approaches can achieve theprecision \nof a high sampling rate with the cost of a low sampling rate. For examples, in 178.galgel, with around \n450 samples (bothgreedyand2-threshold), we can achieve theprecision of1600 uniform samples.For 183.equake, \nwith60 sam\u00adples(greedy),we can achievetheprecision of600 uniform samples.  8.4 EffectofDifferentUncertainInputs \nSofar, ouruncertaininputsare randomlyselected.Inthis ex\u00adperiment, we studythe effectof selectingdifferent \nuncertain inputs and observeif our results stillhold.We observethat for most of ourprograms,inputs are \nuniform,e.g.they are  00.2 1.2 250 500 750 1000 1250 1500 1750 2000 0 500 1000 1500 2000 0 100 200 \n300 400 500 600 700 800 900 1.1 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 Budget Greedy / 2-threshold MC 0 0.2 \n1.2 1 0.8 0.6 0.4 Budget 2-threshold Greedy MC 0.2 0.4 0.6 0.8 1 MC Greedy 2-threshold Budget  (a) \n178.galgel (b) 183.equake (c) 187.facerec 0 1.2 25 100 300 500 700 900 50 100 150 200 250 300 350 400 \n450 500 1.2 1 0.8 0.6 0.4 0.2 budget 2-threshold Greedy MC  0 0.2 0.4 0.6 0.8 1 MC Greedy 2-threshold \nBudget (d) 301.apsi (e) deisotope Figure11. The comparisonsbetween our algorithms anduniformMC. elements \nof an array.Picking adifferent uncertaininputhas little effect on the sampling results. For each program, \nwe randomly selected afewinputs andthe results were more or less the same.The onlyexceptionis 301.apsi.Itsinputis \nnot an array,but rather a set ofparametersthathavedifferent meanings.Sincethere are39differentinputsin \n301.apsi, we randomlypick three of them,the samplesgeneratedby ourgreedyalgorithmsare24,167,76, andthe \ncorresponding relative errors are0.80,0.49,0.26,respectively.Observethat ourmethodsare consistentlybetterthanMCapproaches.For \nthe majority of the cases, the bene.ts are substantial.There is one case that the relative erroris close \nto1indicatingour approach is not that better than regular MC. The reason is thattheoutputis a simplefunctionof \nthatparticularuncer\u00adtain input such that even a regular MC provides good ap\u00adproximation.  8.5 CaseStudies \nIn thelastexperiment, wepresent our experience with afew cases. Program178.galgel is aninterestingcase,withwhich \nour algorithmsgenerate over400 samples.Fig12 showsthe output functions computed by different approaches. \nLet us .rstfocus onthe actualcurvethatisgeneratedby10000uni\u00adformsamplesand supposed tobethe oracle.Thecurvehas \nmanysmall missing segments,but otherwise appears contin\u00aduous. Observe in the zoom-in view, at around \n120%, those overlappingdots areessentially a sequenceof tiny continu\u00adous segments separatedby missing \nsegments.Byinspecting the code, we observethattheprogramis notstableforthein\u00adputsfallingin missingsegments.Itfailsto \nconvergeandpro\u00adduces no result. Our algorithms are able to closely approx\u00adimatethe real curve.We also \ncollect500 and1000 random samples in uniform distribution for comparison. From the zoom-in view, one \ncan observe that a number of points are missingfrom thelower two curves.However, thisis not the worst \nscenario.When using the traditionalMC,people may tend to begin with a small number of samples. Due to \nthe fact that the curve has very large continuous segments and the missing segments are often muchsmaller,itis \nverylikely that the missing segments are completely missed,leading to wrong conclusions. These results \nclearly show the bene.t of white-box sampling. Observe from Table 4 that it only requires416 samples(greedy) \nandhas only10% overhead (basic). Another example comesfrom theLC-MS(LiquidChro\u00admatography MassSpectrometry) \nprocess[27], whichis an effectivetechnique usedin real-world cancerbiomarkerdis\u00adcovery. A biomarker is \na protein which undergoes changes in concentrationindiseased samples.Todetectbiomarkers, proteinsfrom \ncancerpatients andnormalpeople arelabeled differently anddigestedinto smallerpiecescalled peptides. After \nthe LC-MS process, each peptide would ideally lead to two peaks, or a doublet. One of them corresponds \nto the normalpeptide markedwith alightlabel andthe other corre\u00adspondsto the cancer sample marked with \naheavylabel.The intensityratio ofthedoubletindicatesthe relative concentra\u00adtion ofproteinsfrom which \nthepeptides weregenerated.  deisotope is a program carries out the data process\u00ading in LC-MS process. \nIt takes raw data from serum then producesthe matchingdoublets withtheirintensities. However, this program \nis highly sensitive to data un\u00adcertainty. A tiny variation in the input may lead to differ\u00adentdoubletsbeinggenerated.Sample \noutputsare shownin Fig. 13(a). The x-axis represents the variation of an input providedby thescientist \naccording totheir experience(and thus uncertain)from50% to150% ofits original value, and they-axis shows \nthe computedintensity of outputtedpeaks. We canobservethattheintensity of thepeakschangessub\u00adstantially, \nleading to the potential change of the biomarker. Oritmay evendisappear,meaningadifferentset ofdoublets \nisgenerated. 1200 1000 800 600 400 200 0 50.0% 60.0% 70.0% 80.0% 90.0% 100.0% 110.0% 120.0% 130.0% 140.0% \n150.0% (a) Output variation overtheinput changes. deisotope Actual White-Box MC Trad. MC (30 samples) \n100.0% 105.0% 110.0% 115.0% 120.0% 125.0% 130.0% 135.0% 140.0% 145.0% 150.0% (b) Different curves plotted \nby white-box and traditional MC methods, with the actual curve shown on the top. Figure13. Case studyof \ndeisotope. Removing false positives caused by uncertainty is very critical since the results determine \nthe subsequent research typically involving signi.cant effort and expense in wet\u00adbench experiments. Sampling \nprovides a reasonably low\u00adcost methodtoinspect the effect of uncertainty. Withoutloss ofgenerality,we \nselect one of thepeaksin the outputted doublets for a close study. Fig. 13(b) shows the change of its \nintensity, by varying the uncertain input from 100% to 150% of its original value. Observe with 20 samples, \nourtechniqueis abletoprecisely model the curve while atraditionalMC with30 samples cannot.Observethat \non the left, the traditional MC approach misses the small segment.This correspondsto a missing doubletthat \ncauses anirrelevantproteinbeing misclassi.ed asthebiomarker. 9. RelatedWork Uncertaintyanalysis andMCmethod. \nSampling-based,or MonteCarlo approachesto uncertaintyandsensitivityanaly\u00adsis are widely used[13,17].Severaltechniquesareproposed \nto improve the ef.ciency of MC methods by parallelizing MC trials [2,4].In[22], an execution coalescing \ntechnique wasproposedtopack multipleMC trialsin a single run, us\u00adingvectors.Thesetechniquesdo not aim \natguidingthe sam\u00adplingprocess to expose criticalpoints usinga small number of samples.Moreimportantly,theydon \ntfocus on analyzing program artifacts to achievethegoal. Other approaches to uncertainty analysis include \nmodel checking[12], automateddifferentiation[3], and controlled perturbation[14].Theyareeithertooheavyweightorhaving \ndif.cultyhandlingheterogenousdata(certain anduncertain) andprogramartifactssuch ascontrol.ow. MC methods \nare also used in identifying critical input and code regions[5], anddetecting bugsin numericalpro\u00ad grams[23].Our \nworkis complementaryto these techniques byreducingthe number of neededMC trials. Static analysis for \nprogram continuity and robustness. Thetechniquein[6]shares a similar scenario of analyzing continuity \nfor programs. It uses static analysis to soundly reasonaboutcontinuity,byprovingwhetheragivenprogram \nencodesa continuousfunction.Ontopof[6],[7]furtherana\u00ad lyzes andquanti.esthe robustness ofaprogramtoinput \nun\u00adcertainty.Thesetechniquesare static.In contrast, our work is dynamic. For many real-world programs, \ncontinuity and robustness cannotbe staticallyproved astheydepend onthe concreteinputs andinput errors.These \ntechniques and ours are synergetic.Infact, we adapttheirtechniquetoprovecon\u00adtinuity of continuous coresidenti.edby \nourdynamic analy\u00adsis. Others. Taint analysis [8, 20], information .ow track\u00ad ing[19], orlineagetracing[26] \naredynamic analysisthat track orevenquantifyinformation .owduringprogramex\u00adecution. They can be used \nto correlate inputs and outputs. However, uncertain data processing requires direct reason\u00ading abouthowinput \nchanges lead to output changes, which demands reasoningacross multiple executions. White-boxfuzzing[10,11]is \nan effectivetechniquefor dynamictestgeneration.Itanalyzesprogramexecutionsalso in a white-boxfashion,butbasedon \nsymbolic execution and constraint solving techniques.In theory,it canbe applied to explorethedifferentvalues \nofthediscretefactorsfor agiven program.However,duetothe complexityofhigh orderpath conditions and thehuge \noverhead,itisimpracticalfor real world uncertaindataprocessing. 10. Conclusion Wedevelopa whitebox sampling \ntechniquethat allows sci\u00adentists to selectively and ef.ciently sample discontinuous pointsin outputfunctions,giveninputerrorbounds.Itworks \nby analyzingprogramexecution.Inparticular,it ef.ciently hashesthe values ofcertainprogram artifacts calleddiscrete \nfactors during sample execution that are the root causes of the discontinuity in output. It then compares \nthe hashes of multiple runs todetermineif additional samples are needed. We propose two hashing schemes \nand three sampling al\u00adgorithms with different tradeoffs in precision and cost. We also carefully studytheir \nsafety.Forprogramsin which con\u00adtrol.owdifferences(acrossmultiplesampleruns)areinten\u00adtional and hence \ndo not affect continuity, we use a pro.ler to identify such code regions and statically prove that they \nare continuous so that we don t need to hash their runtime values.Our results show that the techniqueis \nvery effective for real-worldprograms. Acknowledgments This researchis supported,inpart,by theNationalScience \nFoundation(NSF) undergrants0845870 and0916874.Any opinions, .ndings, and conclusions or recommendationsin \nthis paper are those of the authors and do not necessarily re.ect the views ofNSF. References [1] H. \nAgrawal and J. R. Horgan. Dynamic program slicing. In PLDI 90. [2] V. N. Alexandrov, I. T. Dimov, A. \nKaraivanova, and C. J. K. Tan. Parallelmonte carlo algorithmsforinformation retrieval. Math.Comput.Simul.,62(3-6),2003. \n[3] J. Barhen and D. B. Reister. Uncertainty analysis based on sensitivities generated using automatic \ndifferentiation. In ICCSA,2003. [4] I. Beichl, Y. A. Teng, and J. L. Blue. Parallel monte carlo simulation \nof mbegrowth. In IPPS,1995. [5] M.Carbin andM.C.Rinard. Automaticallyidentifying criti\u00adcalinput regions \nand codein applications. In ISSTA,2010. [6] S. Chaudhuri, S. Gulwani, and R. Lublinerman. Continuity \nanalysis ofprograms. In POPL,2010. [7] S.Chaudhuri,S.Gulwani,R.Lublinerman, andS.Navidpour. Provingprograms \nrobust. InESEC/FSE,2011. [8] J.Clause,W.Li, andA.Orso. Dytan: agenericdynamic taint analysisframework \nIn ISSTA,2007. [9] U.Consortium. Theuniversalproteinresource(uniprot) in 2010. NucleicAcidsRes,38(Database \nissue),Jan2010. [10] P. Godefroid, A. Kiezun, and M. Y. Levin. Grammar-based WhiteboxFuzzing. In PLDI,2008. \n[11] P.Godefroid,M.Y.Levin, andD.Molnar. AutomatedWhite\u00adboxFuzzTesting. InNDSS,2008. [12] M. P.E. Heimdahl, \nY. Choi, and M. W. Whalen. Deviation AnalysisThroughModelChecking. In ASE,2002. [13] J. C. Helton, J. \nD. Johnson, C. J. Sallaberry, and C. B. Stor\u00adlie. Survey of sampling-based methods for uncertainty and \nsensitivity analysis. ReliabilityEng.&#38;Sys.Safety,91(10-11), 2006. [14] Y.C.Ho,M.A.Eyler, and T.T.Chien. \nAgradient technique for general buffer storage design in a production line. Inter\u00adnational Journal ofProduction \nResearch,1979. [15] R.Jampani,F.Xu,M.Wu,L.L.Perez,C.Jermaine, andP.J. Haas. Mcdb: a monte carlo approach \nto managing uncertain data. In SIGMOD,2008. [16] P. D. Karp. What we do not know about sequence analysis \nand sequencedatabases. Bioinformatics,14(9),1998. [17] M.D.McKay,R.J.Beckman, andW.J.Conover. Acompari\u00adson \nof three methodsfor selecting values ofinput variablesin the analysis of output from a computer code. \nTechnometrics, 42(1),2000. [18] M. G. Morgan and M. Henrion. Uncertainty: A Guide to Dealing with Uncertainty \nin Quantitative Risk and Policy Analysis. CambridgeUniversityPress,1992. [19] S.McCamantandM.Ernst.QuantitativeInformationFlow \nas NetworkFlowCapacity. In PLDI,2007. [20] J. Newsome and D. Song. Dynamic Taint Analysis for Au\u00adtomaticDetection,Analysis, \nandSignatureGeneration ofEx\u00adploits onCommoditySoftware. InNDSS,2005. [21] S. Singh, C. May.eld, R. Shah, \nS. Prabhakar, S. E. Hambr\u00adusch, J. Neville, and R. Cheng. Database support for proba\u00adbilistic attributes \nand tuples. In ICDE,2008. [22] W.N.Sumner,T.Bao,X.Zhang, and S.Prabhakar. Coalesc\u00ading executions forfast \nuncertainty analysis. In ICSE,2011. [23] E. Tang, E. Barr, X. Li, and Z. Su. Perturbing numerical calculations \nfor statistical analysis of .oating-point program (in)stability. InISSTA,2010. [24] S. Tripathi and R. \nS. Govindaraju. Engaging uncertainty in hydrologic data sets using principal component analysis: Banpca \nalgorithm. Water Resour.Res.,44(10),Oct2008. [25] B. A. Worley. Deterministic uncertainty analysis. Technical \nReport ORNL-6428, Oak Ridge National Lab. TN (USA), 1987. [26] M. Zhang, X. Zhang, X. Zhang, and S. Prabhakar. \nTracing lineagebeyond relational operators. In VLDB,2007. [27] X. Zhang, W. Hines, J. Adamec, J. M. Asara, \nS. Naylor, and F.E.Regnier. An automated methodfor the analysis of stable isotope labeling data inproteomics. \nJournal of the American Society forMassSpectrometry,16(7):1181 1191, July2005. [28] X. Zhang, S. Tallam, \nN. Gupta, and R. Gupta. Towards locating execution omission errors. In PLDI,SanDiego,CA, 2007.   \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Sampling is a very important and low-cost approach to uncertain data processing, in which output variations caused by input errors are sampled. Traditional methods tend to treat a program as a blackbox. In this paper, we show that through program analysis, we can expose the internals of sample executions so that the process can become more selective and focused. In particular, we develop a sampling runtime that can selectively sample in input error bounds to expose discontinuity in output functions. It identifies all the program factors that can potentially lead to discontinuity and hash the values of such factors during execution in a cost-effective way. The hash values are used to guide the sampling process. Our results show that the technique is very effective for real-world programs. It can achieve the precision of a high sampling rate with the cost of a lower sampling rate.</p>", "authors": [{"name": "Tao Bao", "author_profile_id": "81331488648", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856204", "email_address": "tbao@cs.purdue.edu", "orcid_id": ""}, {"name": "Yunhui Zheng", "author_profile_id": "81464648069", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856205", "email_address": "zheng16@cs.purdue.edu", "orcid_id": ""}, {"name": "Xiangyu Zhang", "author_profile_id": "81384614270", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856206", "email_address": "xyzhang@cs.purdue.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384681", "year": "2012", "article_id": "2384681", "conference": "OOPSLA", "title": "White box sampling in uncertain data processing enabled by program analysis", "url": "http://dl.acm.org/citation.cfm?id=2384681"}