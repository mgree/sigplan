{"article_publication_date": "10-19-2012", "fulltext": "\n Execution Privatization for Scheduler- Oblivious Concurrent Programs Jeff Huang Charles Zhang Department \nof Computer Science and Engineering The Hong Kong University of Science and Technology {smhuang, charlesz}@cse.ust.hk \n Abstract Making multithreaded execution less non-deterministic is a promising solution to address the \ndif.culty of concur\u00adrent programming plagued by the non-deterministic thread scheduling. In fact, a vast \ncategory of concurrent programs are scheduler-oblivious: their execution is deterministic, re\u00adgardless \nof the scheduling behavior. We present and formally prove a fundamental observa\u00adtion of the privatizability \nproperty for scheduler-oblivious programs, that paves the theoretical foundation for privatiz\u00ading shared \ndata accesses on a path segment. With privatiza\u00adtion, the non-deterministic thread interleavings on the \npri\u00advatized accesses are isolated and as the consequence many concurrency problems are alleviated. We \nfurther present a path and context sensitive privatization algorithm that safely privatizes the program \nwithout introducing any additional program behavior. Our evaluation results show that the pri\u00advatization \nopportunity pervasively exists in real world large complex concurrent systems. Through privatization, \nseveral real concurrency bugs are .xed and notable performance im\u00adprovements are also achieved on benchmarks. \nCategories and Subject Descriptors D.2.5 [Software En\u00adgineering]: Testing and Debugging Debugging aids; \nTrac\u00ading; Diagnostics Keywords Privatization, Scheduler-oblivious, Concurrency 1. Introduction Despite \ndecades of multicore practice, developing good quality concurrent software remains notoriously dif.cult \ndue to the non-deterministic and astronomical thread interleav- Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, \nUSA. Copyright c @ 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 ings. In principle, concurrent programs \nare free to exhibit the non-deterministic behavior allowed by the scheduler, and it is the responsibility \nof the programmers to prevent the non-determinism from impairing the program correctness (using synchronizations, \nfor example). In practice, how\u00adever, a vast category of real world concurrent programs are deterministic-by-default \nor, more generally, scheduler\u00adoblivious that, given the same input, they are always ex\u00adpected to produce \nthe same output. As noted by Bocchino Jr. et al. [9, 10], almost all scienti.c computing, encryp\u00adtion/decryption, \nsorting, compiler and program analysis, and processor simulation algorithms are expected to exhibit the \nscheduler-oblivious behavior. Scheduler-oblivious concurrent programs are much eas\u00adier to reason about, \nbecause their execution is determinis\u00adtic w.r.t. the program state transition: given the same initial \nstate, it always reaches the same .nal state, regardless of the thread scheduling (assuming a random \nbut fair sched\u00aduler) [10, 14]. Nevertheless, facing the astronomical thread interleavings, it is still \nchallenging to write correct and ef\u00ad.cient scheduler-oblivious programs. Although a signi.cant research \neffort has been invested in language design [7, 10], compiler [5], runtime environment [12, 13], operating \nsys\u00adtem [4, 6], and hardware [14, 15] to .nd practical solutions, all these approaches essentially limit \nthe execution paral\u00adlelism and incur performance penalty. How to ef.ciently support the deterministic \nexecution of scheduler-oblivious programs remains an open problem. In this paper, we identify a fundamental \nproperty we call privatizability of scheduler-oblivious programs. This prop\u00aderty enables us to develop \nan execution privatization tech\u00adnique that makes scheduler-oblivious programs more deter\u00administic without \ncompromising the parallelism and, addi\u00adtionally, may improve the program performance. The priva\u00adtizability \nproperty is closely related to but slightly different from the classical con.ict and view serializability \nproperty [8, 41, 44]. It concerns about the view consistency over a subset of shared data access scenarios: \nread-after-write and read-after-read, which account for a half of the four basic scenarios1. Under a \ncertain condition, the program can be soundly privatized to an equivalent program in which the two accesses \nare always executed sequentially.  More speci.cally, consider a path segment, p, in a scheduler\u00adoblivious \nprogram, with no blocking statement (i.e., thread synchronization), and with two successive accesses \nto the same data, where the .rst access is a read or write, and the second is a read. Suppose in a correct \nexecution of the pro\u00adgram (given a certain input), these two accesses are executed sequentially without \ninterleaving by a third write. The pri\u00advatizability property says that the second read, which is a shared \ndata access in the program, can actually be changed to a local access, which always returns the local \nvalue stored by the .rst access. Let us call the shared data accesses such as the second read privatizable \naccesses, the operation of changing a privatizable shared access to be local privati\u00adzation, and the \nmodi.ed program privatized program. The soundness of the privatizability property is easy to follow. \nSince p contains no blocking statement, with no control of the thread scheduling behavior, the execution \nof p could al\u00adways continue without waiting for other threads and nor does it block the execution of \nother threads. In other words, for any input, there always exists a schedule in the original pro\u00adgram \nsuch that the two accesses read or write the same value, making it reach the same .nal state as that \nof the privatized program. And because the original program is scheduler\u00adoblivious, for all schedules, \nit will reach the same .nal state. Hence, both the privatized program and the original program will always \nreach the same .nal state given the same input. We formally prove a theorem of the privatizability property \nin Section 2.2. While guaranteeing the program state equivalence, priva\u00adtization brings a nice bene.t \nto the program: it isolates the effect of the thread interleaving on the privatized accesses without \nadding any synchronization. The privatized program will no longer experience any non-determinism caused \nby the potentially erroneous interleaving on the privatized ac\u00adcesses and, at the same time, no performance \nis lost. More\u00adover, as the original heap accesses become stack accesses after privatization, the program \nperformance can also be im\u00adproved. In return, many concurrency problems caused by the non-deterministic \nthread interleavings, e.g., concurrent pro\u00adgram testing and debugging, can be greatly alleviated for \nscheduler-oblivious programs. We discuss the applications in more detail in Section 7.1. Taking advantage \nof this observation, we propose an au\u00adtomatic privatization technique for scheduler-oblivious pro\u00adgrams. \nAn important condition for applying privatization is that the observed execution of the path segment \np in the pri\u00advatizability property (in which the two accesses are executed without interleaving) should \nbe correct. Otherwise, if it is buggy, every execution of p would be wrong after privatiza\u00adtion. To bias \nour results to correct executions, our technique 1 The rest two are write-after-read and write-after-write. \n.rst conducts a dynamic analysis on a set of common cor\u00adrect executions to .nd privatizable accesses. \nIn this way, we guarantee that the privatization is only performed when the privatizable accesses can \nbe correctly privatized. The privatization may be applied either at runtime or of\u00ad.ine. The key technical \nchallenge is how to guarantee the privatization correctness, that it does not introduce addi\u00adtional behavior \nbeyond what could be exhibited by the orig\u00adinal program. We present an of.ine program transformation \napproach including a path and context sensitive privatization algorithm that guarantees, for all scheduler-oblivious \npro\u00adgrams with complicated control .ows and contexts, it does not introduce any new program behavior \ncompared to the original. We have implemented our technique for Java and eval\u00aduated it on a set of popular \nmultithreaded benchmarks as well as .ve real world large complex concurrent systems, including Apache \nDerby, Tomcat, Jetty, OpenJMS and Jig\u00adsaw. Our experimental results show that: (1) The privatiza\u00adtion \nopportunities pervasively exist in concurrent programs. We found a total of 5,119 privatizable accesses \nin the .ve large real systems. The overall percentage of privatizable ac\u00adcesses (the number of privatizable \nversus the total number of shared data access locations) ranges from 14.7% to 30.7% in the privatized \nexecutions. (2) Our technique is effective in repairing two typical classes of concurrency bugs. In our \nstudy of nine real world concurrency bugs, our privatization technique is able to .x seven of them. (3) \nWith our tech\u00adnique to automatically privatize the original heap accesses, we are also able to improve \nthe performance of the evaluated benchmarks by 4.3%-17.9%. We highlight our contributions as follows: \n1. We present a fundamental observation of the privati\u00adzability property that enables privatizating shared \ndata ac\u00adcesses in scheduler-oblivious programs, which helps sup\u00adporting their deterministic execution \nwithout compromising the parallelism. 2. We present a novel path and context sensitive execution privatization \ntechnique that safely privatizes the program without introducing any extra program behavior. 3. We evaluate \nour technique on a set of large complex Java programs and the results show that several real bugs are \n.xed without incurring any performance penalty, and no\u00adtable performance improvement is achieved on benchmarks. \n The remainder of this paper is organized as follows: Sec\u00adtion 2 presents and formally proves the privatizability \nprop\u00aderty; Section 3 presents an overview of execution privati\u00adzation; Section 4 presents the technical \ndetails of our tech\u00adnique; Section 5 presents our implementation and Section 6 reports our experimental \nresults; Section 7 further discusses the application scope of privatization; Section 8 discusses related \nwork and Section 9 concludes this paper.  2. Privatizability of Scheduler-Oblivious Programs The cornerstone \nof our work is the fundamental privatizabil\u00adity property of scheduler-oblivious programs. In this section, \nwe present and formally prove a theorem of this property. The theorem forms the foundation of privatization, \nwhich re\u00adduces the non-deterministic in.uence of the thread interleav\u00adings for scheduler-oblivious programs, \nbene.ting many con\u00adcurrent program testing and debugging tasks. Before show\u00ading the theorem in Section \n2.2, we .rst describe a general concurrent program execution model in Section 2.1. 2.1 Concurrent Program \nExecution Modeling To make our approach more general, we de.ne a model in a similar style to the work \nof Flanagan and Freund [16]. A concurrent program in our language consists of a set of concurrently executing \nthreads T = {t1,t2, ...} that com\u00admunicate through a global store s. The global store consists of a set \nof variables S = {s1,s2, ...} that are shared among threads. Each thread has also its own local store \np, consisting of the local variables and the program counter to the thread. We use s[s] to denote the \nvalue of the shared variable s on the global store. Each thread executes by performing a se\u00adquence of \nactions on the global store or the thread s own local store. Let a refer to an action and var(a) the \nvariable accessed by a.If var(a) is a shared variable, we call a a global action, otherwise it is a local \naction. Note that for any global action, it operates on only one variable on the global store. This is \nalso true for synchronization actions, though they are only enabled when certain pre-conditions are met. \nFor local actions, the number of accessed variables on the local store is not important in our modeling. \nThe program execution is modeled as a sequence of tran\u00adsitions de.ned over the program state S=(s, .), \nwhere s is the global store and . is a mapping from thread identi\u00ad.ers ti to the local store pi of each \nthread. Since the pro\u00adgram counter is included in the local store, each thread is deterministic and the \nnext action of ti is determined by ti s current local store pi. Let ak be the kth action in the global \norder of the program execution and Sk-1 be the program state just before ak is performed (S0 is the initial \nstate), the state transition sequence is: a1a2a3 S0 -. S1 -. S2 -. ... (1) Given a concurrent system \ndescribed above, the execution semantics of performing a are de.ned as follows: a var(a) ./S G(a)= ti \npi -. pm i (2) a (s, .) -. (s, .[ti := pm]) i aa var(a)= s . S G(a)= ti pi -. pms[s] -. sm[s] i (3) a \n(s, .) -. (s[s := sm[s]], .[ti := pm]) i Local action When a local action is performed by a thread, only \nthe local store of that thread is changed to a new state determined by its current state. The global \nstore and the local stores of the other threads remain the same. Global action When a global action is \nperformed by a thread ti on the shared variable s, only s and pi are changed to new states. The states \nof all the other shared variables on the global store as well as the local stores of all the other threads \nremain the same. Each action corresponds to a program statement, and one statement may be executed multiple \ntimes in an execution. To make the execution model general to different program\u00adming languages, we consider \nthe following types of state\u00adments that correspond to global actions: READ/WRITE -a thread reads the \nvalue of a shared variable in the global store into its local store/assigns some value to a shared variable \nin the global store;  LOCK/UNLOCK -a thread acquires/releases a lock;  SIGNAL/WAIT -a thread sets the \nvalue of a conditional variable to 1/waits for a conditional variable to become 1 and resets it back \nto 0 after it becomes 1.  FORK/JOIN -a thread forks a new thread/waits for the termination of another \nthread;  YIELD -a thread yields execution to another thread.  Memory model A memory consistency model \nde.nes what value a READ action will return. For example, the simplest but the most strict model, sequential \nconsistency (SCMM) [24], requires that a READ always returns the value written by the most recent WRITE \non the same mem\u00adory address. Various relaxed memory models [2, 28, 29] have been developed to admit additional \noptimizations by imposing fewer constraints on the value returned from READ operations. For simplicity, \nwe consider SCMM in this paper. Nevertheless, our approach should also be gen\u00aderal to relaxed memory \nmodels such as TSO and PSO [35]. For JMM [28] which is highly intricate [40], it is still dif.\u00adcult to \nreason about the soundness of our approach. Thread scheduling and interleaving Under SCMM, in any execution, \nthere exists a global order among all the ac\u00adtions, and a READ action always return the value written \nby the most recent WRITE on the same variable in this global order. We call this global order a schedule, \ndenoted by .. . is non-deterministic, it may be different in different exe\u00adcutions. A thread interleaving \noccurs when two successive actions from a thread in . are interleaved by some action from a different \nthread. Thread interleaving is a natural re\u00adsult of the execution parallelism. However, due to the non\u00addeterminism, \nthread interleaving is also the root cause of many concurrency problems, e.g., atomicity violation [41] \nand atomic-set serializability violations [38]. Scheduler-oblivious A vast category of concurrent pro\u00adgrams \nare scheduler-oblivious. A scheduler-oblivious pro\u00adgram requires that, given the same input, it always \nreturns the same output, regardless of the behavior of the underlying  thread scheduler. More speci.cally, \nin our modeling, given the same initial state S0, for any schedule ., the compu\u00adtation of a scheduler-oblivious \nprogram always reaches the same .nal state SN : ... (S0,.) -. SN (4) Blocking statement A blocking statement \nis a statement that, when executed, may enforce a thread interleaving or in\u00adtroduce an execution ordering \nbetween threads. In our mod\u00adeling, thread synchronizations (LOCK/UNLOCK/SIGNAL/ WAIT/FORK/JOIN/YIELD) \nare blocking statements, and READ/WRITE are non-blocking. LOCK/UNLOCK state\u00adments are blocking because \nthey may wait if the lock is unavailable. For SIGNAL/FORK statements, they introduce an execution ordering \nbetween statements from different threads. For WAIT statement, it always blocks .rst, and then waits \nuntil another thread sets some conditions to be true. For JOIN statements, it must waits until the termination \nof another thread. And for YIELD statement, it always yields the execution to another thread.  2.2 A \nTheorem of Privatizability In a scheduler-oblivious program P , consider a path seg\u00adment, p, with two \nsuccessive global actions, ai and aj,to the same shared variable, s, on the global store, where ai is \na READ or a WRITE, and aj is a READ. Let program P m be a privatized version of P on p, in which the \nglobal action aj in P is changed to be a local action, aj' ,in P m, such that aj' stores the value read \nor written by ai into a local variable in the thread s local store. All the other actions in P and P \nm are the same. Consider the executions of P and P m given the same in\u00adput. Let vi denote the value read \nor written by ai, and v(j,j') the values returned by a(j,j'). Clearly, vi = vj' always holds, because \nin P m , aj' always reads the same value as that by ai. However, in P , vi may not be necessarily equal \nto vj , be\u00adcause ai and aj may be interleaved by a third WRITE to s from a different thread that changes \nthe value of s.Nev\u00adertheless, we have the following theorem of privatizability property on the equivalence \nbetween P and P m: THEOREM 2.1. If p contains no blocking statement, P is equivalent to P m such that, \ngiven the same initial state, they always reach the same .nal state. Proof Let us consider an execution \nwhere p is only executed once with an arbituary schedule .. The proof is similar if p is executed multiple \ntimes. According to Rule (1), the state transitions of P and P m are illustrated as follows: aiaj-1 aj \n(S0,.) . ... -. Si . ... ---. Sj-1 -. Sj ... . SN aiaj-1 aj' (S0,.) . ... -. Si . ... ---. Sj-1 --. Sj' \n... . SN' Since the only difference between P and P m is on aj and aj' ,toprove SN' =SN , it is suf.cient \nto show Sj' = Sj . Recall that aj only reads the value of s on the global store sj-1 and stores it to \nthe thread s local store, say m. According to Rule (2), for the global store, we have sj' = sj-1 = sj. \nAccording to Rule (3), for the local store, the ' only difference between .jand .j is the value of m.In \n' .j,itis vj' , and in .j,itis vj . Because vj' = vi in P m , ' if we can show vi = vj , then we must \nhave .j=.j and hence SN' =SN can be proved. Now let us consider the schedule .. If there is no WRITE \naction to s between ai and aj, then clearly, we have vi = vj, and hence SN' =SN . Suppose such a schedule \nexists and let us call it .no. So, we have shown P m is equivalent to P at least for the schedule .no. \nOn the other hand, if there is a WRITE action by another thread between ai and aj in ., then we may have \nvi vj . Nevertheless, recall = Rule (4), by the de.nition of scheduler-oblivious, we have ... (S0,.) \n-. SN for any schedule. That is, even if . = .no that makes vi vj , . should drive the program to the \nsame = .nal state as that by .no. Therefore, SN' =SN always holds as long as .no exists. We now prove \nthe existence of .no for any initial program state by contradiction. Suppose .no does not exist, it means \nthat for all schedules, ai and aj must be interleaved by a third WRITE. For scheduler-oblivious programs, \nthen there must exist a blocking statement in p. The reason is that the block\u00ading behavior is the only \nway to enforce a thread interleaving or introduce an execution ordering between threads under a fair \nbut non-deterministic thread scheduler. Without a block\u00ading action in p, a thread may always continue \nexecuting to the end of p, if not preempted by the scheduler. Since we as\u00adsume p does not contain a blocking \nstatement, .no must exist under the assumption. Therefore, Theorem 2.1 is proved. Theorem 2.1 paves the \ntheoretical foundation for priva\u00adtizing scheduler-oblivious concurrent programs. Since P m is equivalent \nto P , we can soundly privatize P to P m accord\u00ading to our purpose. With privatization, the non-deterministic \nthread interleavings on the privatized data accesses (such as aj) are isolated and, more importantly, \nthe program per\u00adformance is not impaired but rather improved as the origi\u00adnal heap accesses become stack \naccesses after privatization. We are now ready to present our privatization technique for scheduler-oblivious \nprograms. 3. Overview The key concept of our work is the privatization of scheduler\u00adoblivious programs. \nEssentially, privatization changes the shared variable accesses in the original program to local ones \nin the privatized program, under the condition that the behavior of original program is not changed. \nWe have shown the condition and the soundness of privatization in Theorem 2.1. In this section, we .rst \nuse two motivating examples to illustrate the idea and the bene.ts of privatization on concur\u00adrency bug \n.xing and on the program performance. We then  1 public class TableDescriptor {2 FormatableBitSet referencedColumnMap;3 \npublic String getObjectName(){4 if (referencedColumnMap == null) { public void 5  16 setReferencedColumnMap( \n){ 6} 17 referencedColumnMap = null 7 else } 18 8{ 9 for (int i = 0; i <...; i++) 10 { 11 referencedColumnMap.isSet( \n) 12 }} 13 14} } 15 public class TableDescriptor {2 FormatableBitSet referencedColumnMap; 3 public String \ngetObjectName_privatized(){ FormatableBitSet referencedColumnMap_local =4 referencedColumnMap; if (referencedColumnMap_local== \nnull) 1 5 { 6  7 } 8 else 9 { 10 for (int i = 0; i <...; i++) 11 { referencedColumnMap_local.isSet( \n) 12 } 13 } 14 } 15} Figure 1. Top: a real bug #2861 in Apache Derby. The program crashes with NullPointerException \nwhen a thread references the shared data structure referencedColumnMap at line 11 after another thread \nsets it to null in the method setReferencedColumnMap. Bottom: the getObjectName method after privatization. \npresent the challenges for guaranteeing the privatization cor\u00adrectness, which we address in detail in \nthe next section. 3.1 Motivating Examples Bug .xing The code snippet in Figure 1 (top) shows a real crash \nbug in the Apache Derby database. When a thread calls the getObjectName method on a shared TableDescriptor, \nit .rst checks whether the .eld referencedColumnMap is null or not (line 4). If referencedColumnMap is \nnot null, the thread will enter a loop and dereference it (line 11). There is a potential interleaving \nbetween the two accesses to referencedColumnMap. When it happens, another thread may set referencedColumnMap \nto null (line 17) between line 4 and line 11, causing the .rst thread to throw a NPE at line 11. Worse, \ndue to the non-determinism of this interleav\u00ading, this bug is dif.cult to reproduce and to .x. As reported \nin the bug repository2, it took as long as a year before this bug was .nally .xed by the developer. To \n.x this bug, essentially, the effect of this erroneous interleaving on the program state must be eliminated. \nOne 2 https://issues.apache.org/jira/browse/DERBY-2861 volatile num = 100,000,000; while(true){ while(true){ \nsynchronized(lock) synchronized(lock) { { num_local = num-1;num--; num = num_local; if(num ==0)if(num_local \n==0) System.exit(0); System.exit(0); } } } } Figure 2. The benchmark contains 8 threads simultaneously \ndecreasing the shared variable num. The privatized version (right) is 17.9% faster than the original \nversion (left). option is to add synchronizations (e.g., locks) to completely prohibit this interleaving, \nwhich however, also limits the degree of parallelism. After a closer look at this program, we can see \nthat there is an intriguing characteristic with respect to the dereference to referencedColumnMap at \nline 11 that we can leverage to eliminate the erroneous in\u00adterleaving without using synchronization. \nSpeci.cally, in correct executions of this code segment, the dereference to referencedColumnMap should \nalways dereference the same value as the preceding access to referencedColumnMap by the same thread at \nline 4. This indicates that this shared data access is privatizable: we can privatize it to dereference \na thread local variable referencedColumnMap local that stores the value of the access to referencedColumnMap \nat line 4 by the same thread, as shown in Figure 1 (bottom). In this way, the dereference to referencedColumnMap \nwill always dereference a non-null variable, regardless of the thread interleaving. The bug is .xed without \nadding any synchronization. Performance improvement To assess the effect of pri\u00advatization on the program \nperformance, we design a micro\u00adbenchmark (Figure 2) to conduct controlled experiments for quantifying \nthe runtime characteristics of the privatization effect. The benchmark consists of concurrent threads \nthat repeatedly decrease a shared counter (a volatile integer) in a loop until its value reaches 0. The \ncounter decreasing and the termination checking operations are enclosed in a synchro\u00adnized block to ensure \nthe counting correctness. We control the number of threads and the initial value of the counter to measure \nthe program execution time. Figure 2 (left part) shows the original micro-benchmark. Since the second \nread to the counter always returns the same value as that of its preceding write access, the second read \ncan actually be privatized to return the value of a local vari\u00adable that stores the value of the write \naccess. The priva\u00adtized version is shown in Figure 2 (right part). In our experi\u00adments on a 8-core machine \nwith 8 threads and with the initial value of the counter set to 100,000,000, the privatized ver\u00adsion \n(40.2s) is 17.9% faster than the original version (49.0s).  foo_local = foo; public class StringBuffer \n{ if (foo_local == null){ private int count; public synchronized public synchronized foo = new Foo(); \nna\u00efve StringBuffer append(StringBuffer sb) { StringBuffer delete(int start, int end) { int len = end \n-start; } foo_local.m(); 5678 1234 9 int len = sb.length(); sb.getChars( , len, ); if (foo == null){ \n count -= len;} foo = new Foo(); foo_local = foo; 3 } if (foo_local == null){  foo.m(); } path-sensitive \nfoo.m();  10 foo = new Foo(); public synchronized public synchronized void getChars( ) { int length(){ \n 4 14 } else foo_local.m(); Figure 3. Privatization must be path-sensitive   3.2 Privatization Challenges \n On the surface, privatization seems an easy problem. For in\u00adstance, for the bug .xing example in Figure \n1, we may sim\u00adply replace the shared read to referenceColumnMap at line 11 to a local read to referenceColumnMap \nlocal which stores the same value read by the access to referenceColumnMap at line 4. However, in practice, \nwe have to address the fol\u00adlowing touch challenges: Path-sensitivity The privatization must be path-sensitive. \nA privatizable access is de.ned speci.c to a certain path segment. It might not be privatizable on a \ndifferent path. To understand this problem, let us consider a simple program in Figure 3 (left part). \nThe program .rst checks whether a shared variable foo is null or not at line 1.If foo is null, it is \nassigned to a new Foo object at line 2. Then the program invokes the method m on foo at line 4. Suppose \nthat, in our collected execution traces of the program, we only observed the path through lines (1 . \n4), which is possible as foo might always be not null initially. We would .nd that the second read to \nfoo at line 4 is privatizable (because it always return the same value as the .rst read to foo at line \n1). However, if we naively privatize the second read in the same way as what we do for the Derby bug \n#2861 in Figure 1, the resulting program (shown in the right-top of Figure 3) would be incorrect, because \nif foo is initially null, the invocation of m would then dereference a null variable. The correct privatization \nshould consider the path containing the second read and the .rst read, and perform the privatization \nspeci.c to this path, as shown in the right-bottom of Figure 3. Context-sensitivity Besides the path-sensitivity, \nthe pri\u00advatization should also be context-sensitive. Shared data ac\u00adcesses in different calling contexts \nmay access different val\u00adues, either written by the same thread or possibly by a dif\u00adferent thread. Therefore, \nan access that is privatizable in one calling context might not be privatizable in another. This problem \nis illustrated by the StringBu.er bug in Fig\u00adure 4. The two accesses to count at line 11 in the method \ngetChars and at line 15 in the method length, respec\u00adtively, are invoked within the context of the append \nmethod, which is inter-procedural and spans several method calls and control branches. The access to \ncount in the method 15 return count; 11 if (srcEnd > count)) { throw new StringIndexOutOfBoundsException( \n} 12 13 ); }}} Figure 4. An atomicity violation in the append method of java.lang.StringBuffer class. \nThe program throws StringIndexOutOfBoundsException when a thread at line 11 references the stale length \nof sb changed by another thread at line 8. while (shared) { local = shared while (local){ } } Figure \n5. Privatization must preserve progressiveness getChars at line 11 is a privatizable, because in correct \nex\u00adecutions, it always reads the same value as the read access to count at line 15 in the method length. \nHowever, this repeated read is only privatizable within the calling context append. It might not be privatizable \nfor all calling contexts in the program. For instance, it is possible that the getChars method is called \nfrom an external method in which count is written by a remote thread and then directly accessed in getChars. \nTherefore, we have to consider the calling con\u00adtext speci.c to the privatizable access. Progressiveness \nPrivatization changes an originally shared variable access into a local one by modifying it to return \nthe local value stored by a preceding access (to the same shared data). If the shared data is changed \n(by another thread) be\u00adtween the privatized access and its preceding access, the modi.ed access will \nnot see the change. This is problematic when the change is expected by the program. Because we have observed \nin the correct execution that the change does not happen (the privatized access returns the same value \nas its preceding access), the change should not be expected on the observed path segment. However, there \nis an impor\u00adtant progressiveness property we must preserve: the pro\u00adgram must be able to continue execution \nafter privatization. For example, if there is a blocking operation somewhere between the privatized access \nand its preceding access, the program may block forever until the shared data is changed. Also, when \nthe privatized access is inside a loop and the value of the access is related to the loop condition, \nafter privatization, the program may never escape from the loop. Figure 5 illustrates this problem. The \nprogram imple\u00adments a simple barrier function with which the thread cannot progress until the .ag shared \nis set to false by another thread. Suppose the initial value of shared is true. If we naively privatize \nthe access to shared tobe local , the resulting program may never exit from the while loop. In the original \nprogram, however, this situation only happens if the other thread is never scheduled to change the value \nof shared , which is different from the semantics of the original program. Therefore, we must also consider \nthe pro\u00adgressiveness for the privatization correctness.  4. Execution Privatization To address these \nchallenges, we have developed a path and context sensitive privatization algorithm, to make sure the \nprivatization only applies to the correct execution paths we have observed and to guarantee the privatization \ndoes not introduce extra behavior to the program. Our technique consists of two phases: dynamic trace \nanal\u00adysis and the code privatization. The dynamic trace analysis phase presents the privatizable accesses \nto the privatization phase, which then performs the path-sensitive and context\u00adsensitive privatization \non the program source or bytecode. In this section, we present our technique in detail. We also show \nthe privatization correctness in Section 4.4. 4.1 Preliminaries We .rst de.ne a few basic concepts. We \nwill use these concepts to describe our technique in the rest of this section. DEFINITION 1. A basic \nblock (BB) contains a sequence of program statements with only one entry point and one exit point. This \nde.nition refers to the standard notion of basic block in the control .ow graph (CFG). In our method, \nwe give each BB in the program a unique ID. DEFINITION 2. A shared data access point (SAP) is a statement \nin some BB that reads or writes shared data be\u00adtween threads at runtime. Each SAP has a unique location \nin the program with the access type .{WRITE,READ}. For example, the simple pro\u00adgram in Figure 6 has three \nSAPs, at lines 1, 2, 4, respectively, and their access types are READ, WRITE, READ, respectively. As \nSAP is a static instruction of shared data accesses, a SAP may be executed multiple times at runtime. \nDifferent execution instances may access different shared memory locations, because of the possible pointer \naliases. In our method, we also distinguish different execution instances of a SAP at runtime. DEFINITION \n3. A trace captures a multi-threaded program execution as a sequence of events d = (ei). We consider \nthe following four types of events: SAPE (t,s,m): a thread t executes a SAP s accessing a shared memory \nlocation m. D-SAP1 1 if (foo == null){2 foo = new Foo();3 } D-SAP2 4 foo .m(); P-SAP1,2 Figure 6. D-SAP \nand P-SAP are path-sensitive BBI (t,b): a thread t enters a BB b.  BBO (t,b): a thread t exits from \na BB b.  BLOCK (t): a thread t executes a blocking statement.  DEFINITION 4. A privatizable SAP (P-SAP)isa \nREAD SAPE in the trace that returns the value read or written by its preceding SAPE by the same thread, \nand without a BLOCK by the same thread in between,. This preceding SAPE is called the dependent SAP (D-SAP) \nof the P-SAP. DEFINITION 5. A privatizable path (P-Path) is a path seg\u00adment containing a P-SAP in the \ntrace by the same thread. The P-Path starts from the BB containing the D-SAP and ends at the BB containing \nthe P-SAP. P-Path is represented by the sequence of BBs executed be\u00adtween the P-SAP and the corresponding \nD-SAP by the same thread. P-SAP and D-SAP are path-sensitive. For example, in Figure 6, there are two \npairs (D-SAP1, P-SAP1) and (D-SAP2, P-SAP2) following the P-Paths through lines (1 . 4) and (2 . 4), \nrespectively. DEFINITION 6. The calling context of a P-SAP or a D-SAP is the sequence of active methods \nand the method call sites on the stack, when the P-SAP or the D-SAP is executed. The calling context \nde.ned here is similar to the standard de.nition [11, 36]. We used it to determine whether to per\u00adform \nthe privatization on the P-SAP or not (Section 4.3.1). Note that the calling context can be computed \nef.ciently by analyzing the BBI and BBO events in the trace, without any extra information at runtime. \n  4.2 Dynamic Trace Analysis The goal of our dynamic trace analysis is to .nd all the P-SAPs manifested \nin the observed correct executions. Each reported P-SAP is also associated with the P-Path, which is \nused by the second phase to perform the privatization. Algorithm 1 shows our dynamic trace analysis algorithm. \nOur algorithm to extract the D-SAP and P-SAP is similar to the work of AVIO [26] and CTrigger [31], that \nD-SAP is related to the P-instruction and P-SAP is related to the I-instruction. Differently, P-SAP in \nour work is limited to READ access only, and our algorithm also needs to make sure that there is no blocking \noperation between the D-SAP and the P-SAP by the same thread. Moreover, what we take as input is a set \nof correct execution traces. Sharing the same essence with [42], our work does not require the availability \nof erroneous executions to eliminate the erroneous thread interleavings.  Algorithm 1 Dynamic Trace \nAnalysis (d) 1: Input: d -a trace 2: Let M denote all shared memory locations in d 3: dm . the sequence \nof SAPEs in d that access a shared memory location m 4: dm . the sequence of SAPEs in dm that are performed \nt by a thread t 5: for each m . M do 6: for each READ SAPE s . dm do 7: sdef . the most recent WRITE \nSAPE in dm 8: t . the thread of s 9: sm . the most recent SAPE in dm before s t 10: if sm is a WRITE \nthen mm 11: s. s def 12: else m 13: sdef . the most recent WRITE SAPE in dm t m before s 14: end if m \n15: if sdef == sthen def 16: p-path p . the sequence of BBs by t in d from sm to s 17: if p does not \ncontain a BLOCK statement then 18: report p as privatizable 19: end if 20: end if 21: end for 22: end \nfor To .nd a P-SAP, our algorithm iterates through the se\u00adquence of SAPEs on each shared memory location \nby each thread. For each READ SAPE, s, that accesses the shared memory location, m, by a thread, t, we \n.rst .nd the most recent SAPE on m before s that is performed by t, say it is sm. To determine whether \ns is privatizable or not, we com\u00ad m pare the most recent WRITE SAPEs on m that is before s(including \nsm) and the most recent WRITE SAPEs on m that is before s. If they are the same, we continue to check \nwhether the path p from sm to s by t in the trace contains a BLOCK operation or not. If not, we report \ns as a P-SAP and p the corresponding P-Path. The same procedure is applied for all threads and all shared \nmemory locations in each trace. Finally, we obtain a set of P-SAPs computed from all the traces. For \na set of traces, the results of P-SAPs are merged. Two P-SAPs are considered equivalent if their P-Paths \nare identical.  4.3 Path and Context Sensitive Privatization The execution privatization is essentially \na program transfor\u00admation process that takes the P-SAPs reported in the trace P-SAP trace D-SAP P-Path \np = bi-bi+1-bi+2 -bj  P-SAP trace D-SAP P-Path p = b i-b i+1-b i+2 -b j Figure 7. Conceptual view of \nexecution privatization. The privatization is tailored to the P-Path. analysis phase and produces a privatized \nversion of the pro\u00adgram in which the P-SAPs are all privatized. We iterate through the list of P-SAPs \nand perform the privatization for each of them. For each P-SAP, the privatization is tailored to the \nasso\u00adciated P-Path, as illustrated in Figure 7. Conceptually, we clone the P-Path for each P-SAP and \nattach it to the pro\u00adgram. Most part of the cloned P-Path is the same as the orig\u00adinal, with the main \ndifference that the P-SAP is privatized to access a thread local variable which contains the value ac\u00adcessed \nby the D-SAP. More formally, consider a P-Path p =bi-bi+1-bi+2-...-bj where the D-SAP and P-SAP are in \nm the BBs bi and bj, respectively. We clone p to be p=bm i \u00adbm i+1-bm -...-bm , where bi m =bi (D-SAP \n. D-SAPm), bm i+2ji+1 =bi+1,...,bm j-1 =bj-1, and bm j =bj (P-SAP . P-SAPm). D-SAPm and P-SAPm are determined \nby the privatization rules. Moreover, to ensure the soundness, the P-Path clone must guarantee that pm \nis executed in the privatized program iff p is executed in the original program. Furthermore, recall \nthat we must also consider the pro\u00adgressiveness before performing any naive privatization. The key to \nprogressiveness is that any shared data access inside a loop should be able to see the change to the \nshared data; otherwise, the program may never progress out of the loop. To address this problem, after \nprivatizing all the P-SAPs, we perform an additional inter-procedural loop analysis to de\u00adcide that whether \nany privatized P-SAP is inside a loop or not. If it is, we ensure that not all P-SAPs inside the loop \nare privatized. In this way, because at least one P-SAP still accesses the shared data, any change to \nthe shared data is guaranteed to be visible to all the P-SAPs. In the rest of this section, we .rst show \nthe privatization rules in detail. Then we present our path and context sensi\u00adtive P-Path cloning algorithm. \n 4.3.1 Privatization Rules Figure 8 shows the privatization rules of D-SAP and P-SAP. The P-SAP is a \nREAD access to some shared variable s. Our privatization replaces it to read a local variable s local \nin\u00adstead. The value of s local is obtained from the privatiza\u00adtion of the D-SAP. According to the different \naccess types  D-SAP D-SAP WRITE s WRITE s_local s = s_local READ s s_local = s READ s_local P-SAP P-SAP \nREAD s READ s_local Figure 8. Privatization rules of D-SAP and P-SAP D-SAP int local1 = getData(); int \ngetData(){ 1 3 return shared; 2 int local2 = getData(); }  P-SAP Figure 9. The P-SAP and the D-SAP \nare at the same pro\u00adgram location (line 3). Nevertheless, because their calling contexts are different \n(line 1 and line 2, respectively), they are still privatizable. of the D-SAP, the treatments are slightly \ndifferent. If the D-SAP is a WRITE access, we .rst change it to store the written value into a local \nvariable s local and then insert a new statement s=s local after it, that stores the value in s local \nto s. If the D-SAP is a READ access, we .rst in\u00adsert a new statement that stores the value of s into \ns local and then change the D-SAP to read s local instead of s. Clearly, in this way, when the P-SAP \nis executed, instead of reading the original shared variable s, it will read the local variable s local \nwhich stores the value of s. Privatization scope Note that our privatization is appli\u00adcable to the whole \nprogram and is general to all calling con\u00adtexts in the trace. It is not limited to a single method or \na single module. The P-Path may span multiple modules and contain multiple method calls. Also, the P-SAP \nand D-SAP may be at the same program location, as long as their calling contexts (De.nition 6) in the \nP-Path are different. Figure 9 shows an example. We use the sequence of BBI events and their call sites \nto represent the calling context. Starting from the beginning of the trace to the P-SAP (D-SAP), every \nBBI event by the same thread is added to the calling context, and when there is a BBO event, the corresponding \nBBI event is deleted from the context. Privatization transitivity An interesting property of the privatization \nis the transitivity. The D-SAP of a P-SAP itself might also be a P-SAP, which has its own D-SAP. This \nforms a loop of D-SAPs and P-SAPs if every D-SAP is a P-SAP in the loop, or a chain when there exists \na D-SAP which is not a P-SAP. When it forms a chain, let us call the only D-SAP the ancestor. The ancestor \ngives us a nice property that its local value can be directly used by all the other P-SAPs in the chain. \nThis property makes the reuse of the local variable possible, freeing us from creating a new local variable \nfor each P-SAP. Progressiveness guarantee However, we must be careful when the P-SAPs and their D-SAPs \nform a loop. As noted in Section 3.2, we must make sure the privatization does not break the progressiveness \nof the original program. If any of the P-SAPs and their D-SAPs form a loop, after the privatization, \nall the P-SAPs in the loop are privatized and the change to the shared data would not be seen by the \nprivatized P-SAPs. When the shared data is related to the loop condition, the program may be inside the \nloop forever. The key to addressing this problem is to break the loop, ensuring that at least one P-SAP \ninside the loop should be able to see the change to the shared data. We resolve this problem by performing \na whole program loop analysis after privatizing all the P-SAPs. For each privatized P-SAP, we check whether \nit is inside a loop of P-SAPs or not. If it is, we simply unprivatize one of the P-SAPs. In this way, \nat least one shared data access is not privatized and can see the change to the shared data. Hence, all \nP-SAPs are able to see the change. We repeat this process until all loops break. Therefore, the progressiveness \nof the original program is preserved. Variable visibility An additional problem we need to address is \nthe visibility of the local variable s local when the D-SAP and the P-SAP are within different methods. \nBecause s local is only visible in the method in which it is declared, the P-SAP cannot read it from \na different method. For such inter-procedural cases, we declare s local as a thread local static variable. \nThe variable is a static .eld of a singleton class added to the program, and it is unique for each P-SAP. \nIn this way, the P-SAP is able to read s local directly. Algorithm 2 P-Path Clone (p) 1: Input: p = bibi+1 \n...bj -the DP-Path 2: for k . i+1 to j do 3: if bk is an entry BB to a new method m then 4: clone m to \nmprivatized 5: update the call site in bk-1 to mprivatized 6: else 7: if bk has more than one predecessor \nin the CFG then 8: clone the subgraph rooted by bk to that by bm k 9: update the edge from bk-1 to bm \nk 10: end if 11: end if 12: end for  4.3.2 Path and Context Sensitive P-Path Clone Because there might \nbe complicated control .ows and pos\u00adsibly in.nite number of paths in the program, the main chal\u00adlenge \nof the P-Path clone is to ensure that only the P-Path is cloned but not any other path. That is, for \nall the other paths in the program except the P-Path, they remain unchanged in the privatized program. \nTo achieve this, our algorithm care\u00adfully clones the P-Path by taking care of every BB and the context \nin the P-Path. Algorithm 2 shows our P-Path clone algorithm. It traverses each BB in the P-Path from \nbi to bj , which contain the D-SAP and the P-SAP, respectively. For each BB, it .rst checks whether the \nBB is an entry block to a new method or not. If yes, it means that the path has an inter-procedural transition, \nand we hence clone the new method and also update the corresponding invocation site in the preceding \nBB. Otherwise, the BB goes through an intra\u00adprocedural cloning process. In the intra-procedural phase, \nour algorithm checks whether the BB has multiple prede\u00adcessors in the CFG or not. If yes, it means that \nthere are other paths different from the P-Path that pass through this BB. So we clone the subgraph rooted \nby this BB in the CFG and update the edge from the preceding BB to it correspond\u00adingly. This procedure \nis repeated for every BB until all the BBs in the P-Path are processed. Finally, the whole P-Path is \ncloned and all the BB transitions on the P-Path are correctly updated.   Examples Figure 10 and Figure \n11 illustrate the privati\u00adzation of the intra-procedural and inter-procedural cases, re\u00adspectively. In \nthe intra-procedural case, the P-Path is cloned and the D-SAP and the P-SAP are updated to D-SAP and \nP-SAP respectively in the cloned P-Path, and all the other paths remain the same. For the inter-procedural \ncase, in ad\u00addition to the intra-procedural treatments, we also have to handle the method transitions. \nIn the example, suppose the P-Path spans the methods m1 and m2, inside which the D-SAP and the P-SAP \nare accessed, respectively. In the pri\u00advatized version, m1 and m2 are cloned to be m1 privatize and m2 \nprivatize, respectively, and their invocation sites in the paths are also updated correspondingly.  \n 4.4 Privatization Correctness An important property guaranteed by our approach is that, for any scheduler-oblivious \nprogram, the privatization is safe: it does not introduce additional behavior beyond what could be exhibited \nby the original program. In this section, we prove the follow theorem: THEOREM 4.1. Our execution privatization \nis safe for all scheduler-oblivious programs. Proof The key requirement of an scheduler-oblivious pro\u00adgram \nis that the program computation is regardless of the underlying thread scheduling. Given the same input \nand the same execution environment, even if the scheduling is dif\u00adferent, it always returns the same \noutput. Since our privati\u00adzation algorithm is tailored to the P-Path, which is a part (a segment) of \nan observed correct execution, it is suf.cient to prove the privatization correctness of the P-Path. \nRemember that in the P-Path, the D-SAP and P-SAP are two consecutive accesses to the same shared data. \nSince our privatization only changes the P-SAP to read the same value as that read or written by the \nD-SAP, and the P-Path does not contain a blocking statement, it satis.es the conditions of privatization \nin Theorem 2.1. By the theorem of the privatizability property, for any input, the privatized program \nis guaranteed to reach the same .nal state as that by the original program. The privatization correctness \nis proved. 5. Implementation We have implemented and evaluated our technique for Java. Figure 12 shows \nthe architecture of our execution privatiza\u00adtion system. It contains four main components: the instru\u00admentor, \nthe recorder, the analyzer, and the privatizer. The instrumentor is a Soot3 bytecode transformation phase \nthat prepares a program for use with our execu\u00adtion privatization system. It instruments the shared vari\u00ad \n3 http://www.sable.mcgill.ca/soot/  able accesses, blocking statements, and the basic block en\u00adtrances/exits, \nwhich are recorded for all threads in a global order by the recorder at runtime. In Java, we consider \nOb\u00adject.wait(), Thread.join(), Thread.yield(), and the boundaries of synchronized blocks and methods \nas blocking statements. We chose Soot as our instrumentation framework for its compatibility with the \nnewest JDK 1.7 and easy-to-analyze intermediate representation (Jimple IR). However, our ap\u00adproach is \ngeneral and should apply beyond Java bytecode. The recorder is similar to existing systems that determin\u00adistically \nrecord executions [13, 20]. Our current recorder is implemented as a separate Java library invoked from \nthe instrumented program. When a program runs, the recorder saves the runtime traces into the database. \nEach event in the trace is either a shared variable access, a blocking opera\u00adtion, or a basic block entrance/exit \n(BBI/BBO), containing the thread ID, the shared memory location at runtime or the ba\u00adsic block ID, the \naccess type (READ/WRITE/BLOCK/BBI/BBO), and the program location of the event. The recorder does not \nrecord program input data, because our analysis does not need this information. The analyzer is a stand-alone \nprogram that reads the run\u00adtime traces from the database and computes the P-SAPs for each program. To \ncompute them, the analyzer .rst extracts a total order of SAPs per each shared memory location, for each \nthread, from the execution trace. It then extracts the P-SAPs using the ordered SAPs. To .nd the P-SAPs, \nthe an\u00adalyzer analyzes each pair of two consecutive SAPs by the same thread for each shared data. If \nthe latter SAP reads the value written by the preceding SAP or they both read the value written by the \nsame write, then the latter SAP is a P-SAP, and the corresponding P-Path is reported. The privatizer \nis the key component of our system. It is implemented as a whole program transformation phase in Soot. \nTaking the P-SAPs and the program source (or the program bytecode with the program location information) \nas the input, the privatizer privatizes the P-SAPs along their associated P-Paths in the recorded executions. \nThe core of the privatization is to change the P-SAP, which originally is a shared data read access, \nto a local access that instead reads the value returned by its corresponding D-SAP. To ensure the privatization \ncorrectness, the privatizer clones the P-Path and inserts it into the program according to Algorithm \n4.3.1 and following the rules in Section 4.3.1. 6. Experiments Our evaluation aims at answering the following \ntwo research questions: RQ1. Usefulness -What is the impact of privatization? How useful it is? How does \nit affect program maintenance? RQ2. Effectiveness -How much privatization opportunity is there in real \nworld concurrent systems? To evaluate the usefulness, we use nine real concurrency bugs to assess the \nbug .xing capability of the privatization, and three popular multithreaded benchmarks as well as a micro-benchmark \nto understand the performance improve\u00adment brought by the privatization. To evaluate the effective\u00adness, \nwe apply our system on .ve large complex real world concurrent server programs to see how many privatizable \nac\u00adcesses there are in these systems. We also report the program size increase after privatization, which \nmay affect the pro\u00adgram maintenance. All experiments were conducted on two 8-core 3.00GHz Intel Xeon \nmachines with 16GB memory and Linux 2.6.22 and JDK1.7. 6.1 Concurrency Bug Fixing By isolating the potential \nerroneous preemptive interleav\u00adings, execution privatization has the effect of .xing concur\u00adrency bugs. \nThe salient feature of privatization is that, un\u00adlike the general concurrency bug .xing techniques [23, \n42] that often incur non-ignorable program slowdown, privati\u00adzation does not result in any additional \nruntime overhead. Moreover, because the .xing strategy of privatization does not introduce any extra \nsynchronization into the program, it is completely free from deadlock, which has to be resolved using \nsophisticated dynamic techniques by the state of art approaches [23, 42]. We have applied our system \nto nine real world crash bugs, one from the StringBu.er library in JDK-1.4.2, four from Derby, and four \nfrom Netty. Table 1 shows a summary of these bugs. Most of these bugs are hard to .x by the developers. \nSome of them even lasted for as long as a year before they were .xed, such as Derby #1573 and Derby #2861. \nOur experiments show that, among the nine bugs, the privatization is able to .x seven of them (as shown \nin Column 5 of Table 1). We conclude that the privatization is applicable to .x\u00ading two classes of concurrency \nbugs: p(WRITE)-r(WRITE)-c(READ) and p(READ)-r(WRITE)-c(READ), belonging to two of the .ve types of all \nthe atomicity violations [31]. In these two types of bugs, the c access is privatizable. For scheduler\u00adoblivious \nprograms, it is expected that privatization is the correct or even the most proper way to .x these two \ntypes of bugs. For instance, three of the seven .xed bugs (Derby #1573, Derby #2861 and Jetty #425) were \nindeed .xed by the developers using the source code level privatization. A typical scenario where the \nprivatization applies but may not .x the bug is illustrated in Figure 13. Both the bugs Derby #4018 and \nJetty #408 that our privatization fails to .x belong to this pattern. The two accesses to list should \nalways return the same data, not only the list ref\u00aderence, but also the whole list itself. Privatization \nmakes the list reference private, but not for the whole content of the list. Hence, the list content \ncan still be changed by other threads. To .x this bug, a synchronization mechanism  Bug ID Application \nExisting .x Fix time (days) Fixed by privatization? StringBuffer JDK 1.4.2 Documented thread unsafe - \nYES Derby1573 Derby-10.2.1.6 privatization 365 YES Derby2861 Derby-10.3.2.1 privatization 365 YES Derby3260 \nDerby-10.3.1.4 synchronization 46 YES Derby4018 Derby-10.4.2 synchronization 168 NO Jetty-284 Jetty-6.1.2 \nsynchronization 1 YES Jetty-1269 Jetty-6.1.8 code structure change 33 YES Jetty-425 Jetty-6.1.3 privatization \n268 YES Jetty-418 Jetty-5.x synchronization 19 NO Table 1. Results of real concurrency bug .xing for(int \ni=0;i<list.size();i++) {// May throw IndexOutofBoundsException // if another thread modifies the // content \nof the list list.get(i); } Figure 13. Privatization may not repair this bug Program Input Time-original \nTime-privatized Microbench 100M/8 threads 49.0s 40.2s(17.9%) RayTracer C/100 threads 5.6s 4.9s(12.2%) \nMotercarlo C/100 threads 9.2s 8.8s(4.3%) Moldyn C/100 threads 11.5s 10.7s(6.7%) Figure 14. Frequent shared \narray accesses in RayTracer  Table 2. Performance improvement by privatization is needed to protect \nthe list content from being modi.ed when the list is iterated.  6.2 Performance Improvement An additional \nadvantage of the execution privatization is that, by privatizing the shared heap accesses to be local \nstack accesses, it can help improve the program performance. To understand the range of this performance \nimprovement ef\u00adfect, we have designed a micro-benchmark (presented in Section 3.1, Figure 2). To further \nevaluate the performance impact, we also apply our technique to three popular multi\u00adthreaded benchmarks, \nincluding RayTrace, Montercarlo, and Moldyn. In all these benchmarks, we start 100 threads with the input \nsize C. Table 2 shows the performance results. All data are av\u00aderaged over 10 runs. With privatization, \nall these subjects have nontrivial performance improvement. For our micro\u00adbenchmark, the performance \nimprovement is as large as 17.9%. For the other benchmarks, the performance improve\u00adment ranges from \n4.3% to 12.2%. In fact, all these bench\u00admarks have small number of privatizable locations. The rea\u00adson \nfor the notable performance improvement is that these privatizable locations are hot access points during \nthe ex\u00adecution. Most of them are volatile and are frequently exe\u00adcuted in loops. After privatization, \nas they all become local accesses, it is expected that the program performance could volatile boolean[] \nIsDone; public void DoBarrier(int myid) { boolean donevalue = !IsDone[myid]; while( ){ for( ){while(IsDone[ \n] != donevalue){ }} }IsDone[myid] = donevalue; while(IsDone[0] != donevalue) { }} be improved signi.cantly. \nFigure 14 shows such a typical case in the RayTracer benchmark. Direct accesses to .eld array variables \nare frequently used by programmers, how\u00adever, the .eld array variables are mostly read-only after the \ninitialization. Clearly, it is easy to write code in this way, but it is not a good practice for the \nprogram performance. We also experimented our approach with the real world large systems. However, because \nthe performance effect of the privatizable accesses in the large systems are relatively small (compared \nwith the other instructions), we did not observe signi.cant performance boost on them.  6.3 Pervasive \nPrivatization Opportunities To evaluate the effectiveness of execution privatization, we have applied \nour system to a set of real world applications, including .ve large complex server systems: Apache Derby, \nTomcat, Jetty, OpenJMS and Jigsaw. To maximize the us\u00adage of the privatized executions, we .rst collect \ntypical good executions with different program inputs in the test suite under random schedules. For each \nprogram, we collect the traces of 100 good runs with 10 different inputs and 10 ran\u00addom schedules for \neach input. Table 3 reports the privatization statistics. In these real world systems, we found a total \nof 5,119 privatizable ac\u00adcesses, which accounts for 23.6% of the total (21,733) shared data accesses \nin them (for accesses with the same program location, we only count once). The overall percent\u00ad  Application \nSize(bytes) Size-privatize Increase Jetty-6.1.x 1,678,586 1,712,820 34,234(2.03)% OpenJMS-0.7.7 3,563,274 \n3,833,938 270,664(7.60%) Tomcat-6.0.33 7,434,520 7,791,321 356,801(4.80%) Jigsaw-2.2.6 8,665,258 8,900,182 \n234,924(2.71%) Derby-10.2-4 23,600,432 24,059,525 459,093(1.95%) Table 4. Bytecode size increase after \nprivatization age for each program ranges from 14.7% to 30.7%. The result clearly demonstrates that there \nexist pervasive priva\u00adtization opportunities in real world large complex concur\u00adrent systems. More importantly, \nour result strongly supports the effectiveness in applying execution privatization for real world applications. \nThrough manual inspection of the large amount of priva\u00adtizable accesses, we have also identi.ed several \ntypical rea\u00adsons for the pervasive privatization opportunity: Shared variable name reusing To access \nthe same data at different program locations, for the sake of programming easiness, a common practice \nby programmers is to reuse the same identi.er to directly access the data. For exam\u00adple, the privatizable \naccess at line 11 in the Derby bug ex\u00adample (Figure 1) is manifested as a reusing of the identi.er referencedColumnMap, \nwhich is also used by the read ac\u00adcess at line 4. In fact, all cases of privatizable accesses are manifested \nwith the variable name reuse. Programmers tend to reason in a modularized way that they frequently use \nthe same variable to access the same shared data, without caring about the thread interleaving effect. \nUnexpected sharing Programmers are often unaware of concurrency when writing the code. Since they do \nnot ex\u00adpect the sharing among multiple threads, they feel safe that in a sequential environment the compiler \nwould automat\u00adically help with the privatization. Unfortunately, in multi\u00adthreaded circumstances, it \nis in general very hard for stan\u00addard compilers to do such optimization across threads. This often happens \nwhen a sequential library code is used in a multithreaded program, which is unintended by the library \ndeveloper. For example, we found quite a few privatizable accesses in the logging library log4j, which \nis used by both Tomcat and OpenJMS. Complicated control .ow and context Another typical reason we .nd \nthrough our study is that the privatizable accesses may span over complicated control .ows or calling \ncontexts, which is dif.cult for the programmers to reason about. For example, in the StringBu.er bug \nin Figure 4, the two accesses to the shared data count at lines 11 and 15 span several method calls and \ncontrol branches. Facing the large number of calling contexts and control .ows, it is usually dif.cult \nfor the programmers to reason about the privatizable accesses.  6.4 Program Maintenance With many bene.ts \nof the privatization, a direct cost is that it may affect the program maintenance. As our technique uses \nbasic block clone to perform the privatization, it increases the size of the program as the cloned basic \nblocks are in\u00adserted into the privatized program. Intuitively, our privati\u00adzation might face the problem \nof cloning too much when there is a long P-Path on the CFG between the D-SAP and the P-SAP. Nevertheless, \nthis problem seldom happens. In our case, that would mean there is no intermediate access to the same \nshared data on the P-Path, which we can easily promote the P-SAP to be in the same block as the D-SAP, \nwithout incurring any data or control .ow change. Table 4 reports the bytecode size increase by privatization \nin the real world large systems. The overall size increase ranges from 1.95% in Derby to 7.60% in OpenJMS, \nwhich is relatively small. In our studied systems, for most of the privatizable accesses, the D-SAP and \nthe P-SAP are within the same procedure (see Table 1, Column 5-6) and their basic blocks are often next \nto each other. For these cases, because we do not need to clone the entire method but rather the intra-procedural \nP-Path, the space increase is often much smaller compared to the inter-procedural cases. On the other \nhand, since many .eld variable accesses be\u00adcome local ones through privatization, the number of .eld \nvariable accesses in the original program are reduced. We advocate that our technique is also good for \nprogram main\u00adtenance in some aspects. For example, when refactoring a .eld name, there are fewer places \nto change in the program. To understand a program fault related to a .eld reference, the size of the \ncause-effect chain to the privatized .eld accesses is also reduced. 7. Discussions Besides concurrency \nbug .xing, execution privatization has a wide range of applications in concurrent program testing and \ndebugging. We discuss a few of the applications in this section. We also brie.y discuss some caveats \nrelated to the application scope of the privatization. 7.1 Concurrent Program Testing and Debugging Record/replay \nThe record and replay technique [19, 20, 30] aims at fully reenacting an earlier program execution. For \nconcurrent programs, it is one of the most important techniques for program understanding and debugging. \nIn general, record/replay requires capturing and enforcing the thread interleavings at runtime, which \noften incurs signif\u00adicant program slowdown that limits its applicability at the production site. With \nprivatization, the portion of thread in\u00adterleavings on the privatized accesses no longer exist. Con\u00adsequently, \nthe overhead incurred by capturing this portion of interleavings is completely eliminated, hence, dramatically \nimproving the performance of record/replay.  Application LOC #Shared accesses #Privatizable accesses \n#Intra-procedural #Inter-procedural Jetty-6.1.x 49,746 1,362 219(16.1%) 175(79.9%) 44(20.1%) OpenJMS-0.7.7 \n154,563 6,934 2,126(30.7%) 1,997(93.9%) 129(6.1%) Tomcat-6.0.33 339,405 8,543 1,260(14.7%) 1,173(93.1%) \n87(6.9%) Jigsaw-2.2.6 381,348 1,699 510(30.0%) 347(68.0%) 163(32.0%) Derby-10.2-4 665,733 3,195 968(30.3%) \n840(86.8%) 128(13.2%) Table 3. Statistics of the privatization results Deterministic multithreading \nThe key insight of deter\u00administic multithreading (DMT) is that a small set of sched\u00adules is often enough \nfor good performance. By limiting the program to exercise a small well tested set of schedules, DMT explores \na good tradeoff between the program per\u00adformance and the reliability. To achieve this goal, existing \ntechniques employ either static type systems [7, 10] or run\u00adtime support [5, 6, 14]. With execution privatization, \nDMT techniques can ignore the schedule enforcement on the pri\u00advatized accesses. Ultimately, for the set \nof executions that follow the same path as the privatized execution, the perfor\u00admance would also be signi.cantly \nimproved. Concurrency bug understanding Recent research [21, 22] has shown that concurrent program execution \ntraces of\u00adten contain many thread context switches that perplex the bug reasoning processes. A simpli.ed \ntrace with fewer con\u00adtext switches will greatly help reducing the debugging effort by reducing the number \nof places in the trace where we need to look for the cause of the bug. With execution privatization, \nfuture executions of the privatized program would contain less thread interleavings. The bug reasoning \nprocess based on the privatized execution trace would also be simpli.ed. 7.2 Privatization Scope Although \nexecution privatization brings in many advantages, we note that it has also the limited application scope: \nBug repair An important note on concurrency bug re\u00adpair is that privatization is not general to .xing \nall con\u00adcurrency bugs but only the two classes of atomicity viola\u00adtion bugs where the privatization .ts \nin (Section 6.1). As pointed out by Attiya et al. [3], expensive synchronizations cannot be eliminated \nfor the operations of read-after-write (RAW) to different shared variables and atomic write-after\u00adread \n(AWAR) to the same shared variable. For concurrency bugs such as order violations that miss the happens-before \nrelation across different threads, privatization is also non\u00adapplicable and adding synchronization is \nnecessary to bridge the happens-before dependence. Hence, privatization should not be considered as a \nreplacement to synchronization, but is rather complementary to it. On the other hand, privatiz\u00adable accesses \nare not all necessarily (though they are often) related to concurrency bugs. Lock removing Another caveat \nis that privatization does not eliminate or change the original synchronization opera\u00adtions in the program. \nAlthough it looks plausible that some lock/unlock operations in the original program can be re\u00admoved \nafter the shared accesses inside it are privatized, we note that it is in general dangerous to do it \nas it might change the program semantics. Take the program in Figure 15 as an example. The empty lock/unlock \noperation at lines 5/6 cannot be removed, because together with the fork opera\u00adtion at line 2 they form \na happens-before relation between thread t1 and thread t2. A data race on accessing the shared variable \nx would occur if the empty lock/unlock operations are eliminated. Another reason is that on the perspective \nof the memory model, besides regulating the thread accesses to the shared data, synchronizations have \nthe effect of cache clearance. Removing synchronizations eliminates this effect, which breaks the semantics \nfor programs that rely on cache effect to achieve certain behaviors. 8. Related Work To our best knowledge, \nwe present the .rst known execution privatization technique for scheduler-oblivious programs in this \nwork. In this section, we discuss several approaches in different directions that are related to our \nwork. Compiler optimization Variable privatization [17, 25, 37] is a traditional compiler optimization \ntechnique that im\u00adproves performance by privatizing the variables whose up\u00addated values do not reach \na later thread. The compiler needs not generate target store instructions for the update of the privatized \nvariables because they will not cause read-after\u00adwrite data dependence across threads. Different from \nvari\u00adable privatization, our technique privatizes local reads (read\u00adafter-write) and repeated reads (read-after-read) \ntailored to the runtime path segments, which may exhibit more privati\u00adzation opportunity. Transactional \nmemory Transactional memory systems [18, 34] simpli.es parallel programming by providing database transaction \nguarantees for regions of code. Similar to the privatization effect, shared data reads in a transaction \nnat- Thread t1 Thread t2 1 lock l lock l 5 2 fork t2 unlock l 6 3 write x; read x; 7  4 unlock l Figure \n15. The lock/unlock operations at line 5/6 can not be removed, though there is no code to execute between \nthem.  urally access the local copy of the shared data. Different from execution privatization, transaction \nmemory systems may need to roll back once a con.ict happens after com\u00adpleting an entire transaction. \nTo improve the performance, several privatization techniques are also proposed for trans\u00adactional memory \noptimization. For instance, Wu et al. [43] develop runtime techniques to reduce the overhead of unnec\u00adessary \nread and write barriers for operations that are known to be con.ict-free. Adl-Tabatabai et al. [1] integrate \nthe JIT compiler to remove redundant logging and locking for read accesses of STM operations. Atomicity \nviolation .xing A recent advance by Jin et al. [23] proposes an automated technique that .xed six out \nof eight real atomicity violation bugs, using sophisticated static analysis combined with dynamic monitoring \nto resolve deadlocks. Weeratunge et al. [42] also present a lock based approach to effectively suppress \nconcurrency errors by en\u00adforcing the atomicity property observed from good execu\u00adtions. Synchronization \nis a more general way to .xing con\u00adcurrency bugs, nevertheless, a drawback of using synchro\u00adnizations \nis that it may incur large runtime overhead. Alter\u00adnatively, privatization targets at .xing a subset \nof atomicity violations, without introducing any additional overhead. Runtime surviving concurrency bugs \nA line of active research [13, 27, 32, 33, 39, 45] has also been proposed for detecting and surviving \nconcurrency bugs at runtime. ISOLATOR [32] makes the execution of a buggy program more robust by isolating \nthe well-behaved threads from ill\u00adbehaved ones. ToleRace [33] detects and tolerates asymmet\u00adric races \nin lock-based programs through replication. Atom-Aid [27] proposes a hardware architecture to reduce \nthe pos\u00adsibility of atomicity violations. Yu and Narayanasamy [45] uses hardware transaction to constrain \nthe program s execu\u00adtion from untested interleavings. More recently, Veeraragha\u00advan et al. propose a \nsystem called Frost [39] that survives data races by running multiple replicas with complementary schedules. \nCui et al. develop PEREGRINE [13] that gener\u00adalizes the reusable schedule to more inputs by computing \nthe path constraints. 9. Conclusion We have presented a fundamental observation of the pri\u00advatizability \nproperty that enables sound privatization of scheduler-oblivious programs. We have also presented a path \nand context sensitive privatization algorithm that auto\u00admatically and safely privatizes shared data accesses \nwithout introducing any extra program behavior. Our experiments demonstrate that many concurrency problems \nare alleviated through privatization. We believe our approach is a valuable contribution to concurrent \nsoftware development. Acknowledgement We thank Wei Le for insightful discussions about early drafts of \nthis work. We gratefully acknowledge the anonymous reviewers for their constructive comments. This research \nis supported by RGC GRF grants 622208 and 622909. References [1] Ali-Reza Adl-Tabatabai, Brian T. Lewis, \nVijay Menon, Brian R. Murphy, Bratin Saha, and Tatiana Shpeisman. Com\u00adpiler and runtime support for ef.cient \nsoftware transactional memory. In PLDI, 2006. [2] Sarita V. Adve and Mark D. Hill. Weak ordering a new \nde.nition. SIGARCH Comput. Archit. News, 1990. [3] Hagit Attiya, Rachid Guerraoui, Danny Hendler, Petr \nKuznetsov, Maged M. Michael, and Martin Vechev. Laws of order: expensive synchronization in concurrent \nalgorithms cannot be eliminated. In POPL, 2011. [4] Amittai Aviram, Shu-Chun Weng, Sen Hu, and Bryan \nFord. Ef.cient system-enforced deterministic parallelism. In OSDI, 2010. [5] Tom Bergan, Owen Anderson, \nJoseph Devietti, Luis Ceze, and Dan Grossman. Coredet: a compiler and runtime system for deterministic \nmultithreaded execution. In ASPLOS, 2010. [6] Tom Bergan, Nicholas Hunt, Luis Ceze, and Steven D. Grib\u00adble. \nDeterministic process groups in dos. In OSDI, 2010. [7] Emery D. Berger, Ting Yang, Tongping Liu, and \nGene No\u00advark. Grace: safe multithreaded programming for c/c++. In OOPSLA, 2009. [8] Philip A. Bernstein \nand Nathan Goodman. Concurrency con\u00adtrol in distributed database systems. ACM Comput. Surv., 1981. [9] \nRobert L. Bocchino, Jr., Vikram S. Adve, Sarita V. Adve, and Marc Snir. Parallel programming must be \ndeterministic by default. In HotPar, 2009. [10] Robert L. Bocchino, Jr., Vikram S. Adve, Danny Dig, Sarita \nV. Adve, Stephen Heumann, Rakesh Komuravelli, Jeffrey Over\u00adbey, Patrick Simmons, Hyojin Sung, and Mohsen \nVakilian. A type and effect system for deterministic parallel java. In OOP-SLA, 2009. [11] Michael D. \nBond, Graham Z. Baker, and Samuel Z. Guyer. Breadcrumbs: ef.cient context sensitivity for dynamic bug \ndetection analyses. In PLDI, 2010. [12] Jacob Burnim and Koushik Sen. Asserting and checking de\u00adterminism \nfor multithreaded programs. In ESEC/FSE, 2009. [13] Heming Cui, Jingyue Wu, John Gallagher, Huayang Guo, \nand Junfeng Yang. Ef.cient deterministic multithreading through schedule relaxation. In SOSP, 2011. [14] \nJoseph Devietti, Brandon Lucia, Luis Ceze, and Mark Oskin. Dmp: deterministic shared memory multi-processing. \nIn AS-PLOS, 2009. [15] Joseph Devietti, Jacob Nelson, Tom Bergan, Luis Ceze, and Dan Grossman. Rcdc: \na relaxed consistency deterministic computer. In ASPLOS, 2011. [16] Cormac Flanagan and Stephen N Freund. \nAtomizer: a dy\u00adnamic atomicity checker for multithreaded programs. In POPL, 2004. [17] Manish Gupta. \nOn privatization of variables for data-parallel execution. In IPPS, 1997.  [18] Maurice Herlihy and \nJ. Eliot B. Moss. Transactional memory: Architectural support for lock-free data structures. In ISCA, \n1993. [19] Derek R. Hower and Mark D. Hill. Rerun: Exploiting episodes for lightweight memory race recording. \nIn ISCA, 2008. [20] Jeff Huang, Peng Liu, and Charles Zhang. LEAP: Lightweight deterministic multi-processor \nreplay of concurrent Java pro\u00adgrams. In FSE, 2010. [21] Jeff Huang and Charles Zhang. An ef.cient static \ntrace sim\u00adpli.cation technique for debugging concurrent programs. In SAS, 2011. [22] Nicholas Jalbert \nand Koushik Sen. A trace simpli.cation technique for effective debugging of concurrent programs. In FSE, \n2010. [23] Guoliang Jin, Linhai Song, Wei Zhang, Shan Lu, and Ben Liblit. Automated atomicity-violation \n.xing. In PLDI, 2011. [24] L. Lamport. How to make a multiprocessor computer that cor\u00adrectly executes \nmultiprocess programs. IEEE Trans. Comput., 1979. [25] Zhiyuan Li, Jenn-Yuan Tsai, Xin Wang, Pen-Chung \nYew, and Bess Zheng. Compiler techniques for concurrent multithread\u00ading with hardware speculation support. \nIn LCPC, 1997. [26] Shan Lu, Joseph Tucek, Feng Qin, and Yuanyuan Zhou. Avio: detecting atomicity violations \nvia access interleaving invari\u00adants. In ASPLOS, 2006. [27] Brandon Lucia, Joseph Devietti, Karin Strauss, \nand Luis Ceze. Atom-aid: Detecting and surviving atomicity violations. In ISCA, 2008. [28] Jeremy Manson, \nWilliam Pugh, and Sarita V. Adve. The java memory model. In POPL, 2005. [29] Dan Marino, Abhayendra Singh, \nTodd Millstein, Madan Musuvathi, and Satish Narayanasamy. Drfx: A simple and ef\u00ad.cient memory model for \nconcurrent programming languages. In PLDI, 2009. [30] Pablo Montesinos, Luis Ceze, and Josep Torrellas. \nDelorean: Recording and deterministically replaying shared-memory multi-processor execution ef.ciently. \nIn ISCA, 2008. [31] Soyeon Park, Shan Lu, and Yuanyuan Zhou. Ctrigger: ex\u00adposing atomicity violation \nbugs from their hiding places. In ASPLOS, 2009. [32] Sriram Rajamani, G. Ramalingam, Venkatesh Prasad \nRan\u00adganath, and Kapil Vaswani. Isolator: dynamically ensuring isolation in comcurrent programs. In ASPLOS, \n2009. [33] Paruj Ratanaworabhan, Martin Burtscher, Darko Kirovski, Benjamin Zorn, Rahul Nagpal, and Karthik \nPattabiraman. De\u00adtecting and tolerating asymmetric races. In PPoPP, 2009. [34] Nir Shavit and Dan Touitou. \nSoftware transactional memory. In PODC, 1995. [35] CORPORATE SPARC International, Inc. The SPARC archi\u00adtecture \nmanual (version 9). 1994. [36] William N. Sumner, Yunhui Zheng, Dasarath Weeratunge, and Xiangyu Zhang. \nPrecise calling context encoding. In ICSE, 2010. [37] Jenn-Yuan Tsai, Zhenzhen Jiang, and Pen-Chung Yew. \nPro\u00adgram optimization for concurrent multithreaded architectures. In LCPC, 1998. [38] Mandana Vaziri, \nFrank Tip, and Julian Dolby. Associating synchronization constraints with data in an object-oriented \nlanguage. In POPL, 2006. [39] Kaushik Veeraraghavan, Peter M. Chen, Jason Flinn, and Satish Narayanasamy. \nDetecting and surviving data races us\u00ading complementary schedules. In SOSP, 2011. [40] Jaroslav .c\u00b4ik \nand David Aspinall. On validity of program Sev.transformations in the java memory model. In ECOOP, 2008. \n[41] Liqiang Wang and Scott D. Stoller. Runtime analysis of atomicity for multithreaded programs. TSE, \n2006. [42] Dasarath Weeratunge, Xiangyu Zhang, and Suresh Ja\u00adganathan. Accentuating the positive: Atomicity \ninference and enforcement using correct executions. In OOPSLA, 2011. [43] Peng Wu, Maged M. Michael, \nChristoph von Praun, Takuya Nakaike, Rajesh Bordawekar, Harold W. Cain, Calin Cas\u00adcaval, Siddhartha Chatterjee, \nStefanie Chiras, Rui Hou, Mark Mergen, Xiaowei Shen, Michael F. Spear, Hua Yong Wang, and Kun Wang. Compiler \nand runtime techniques for soft\u00adware transactional memory optimization. Concurr. Comput. : Pract. Exper., \n21, 2009. [44] Min Xu, Rastislav Bod\u00b4ik, and Mark D. Hill. A serializability violation detector for shared-memory \nserver programs. In PLDI, 2005. [45] Jie Yu and Satish Narayanasamy. Tolerating concurrency bugs using \ntransactions as lifeguards. In MICRO, 2010.    \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Making multithreaded execution less non-deterministic is a promising solution to address the difficulty of concurrent programming plagued by the non-deterministic thread scheduling. In fact, a vast category of concurrent programs are scheduler-oblivious: their execution is deterministic, regardless of the scheduling behavior. We present and formally prove a fundamental observation of the privatizability property for scheduler-oblivious programs, that paves the theoretical foundation for privatizing shared data accesses on a path segment. With privatization, the non-deterministic thread interleavings on the privatized accesses are isolated and as the consequence many concurrency problems are alleviated. We further present a path and context sensitive privatization algorithm that safely privatizes the program without introducing any additional program behavior. Our evaluation results show that the privatization opportunity pervasively exists in real world large complex concurrent systems. Through privatization, several real concurrency bugs are fixed and notable performance improvements are also achieved on benchmarks.</p>", "authors": [{"name": "Jeff Huang", "author_profile_id": "81472647687", "affiliation": "The Hong Kong University of Science and Technology, Hong Kong, China", "person_id": "P3856170", "email_address": "smhuang@cse.ust.hk", "orcid_id": ""}, {"name": "Charles Zhang", "author_profile_id": "81435599989", "affiliation": "The Hong Kong University of Science and Technology, Hong Kong, China", "person_id": "P3856171", "email_address": "charlesz@cse.ust.hk", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384670", "year": "2012", "article_id": "2384670", "conference": "OOPSLA", "title": "Execution privatization for scheduler-oblivious concurrent programs", "url": "http://dl.acm.org/citation.cfm?id=2384670"}