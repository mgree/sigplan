{"article_publication_date": "10-19-2012", "fulltext": "\n On the Bene.ts and Pitfalls of Extending a Statically Typed Language JIT Compiler for Dynamic Scripting \nLanguages Jose Castanos David Edelsohn Kazuaki Ishizaki Priya Nagpurkar Toshio Nakatani Takeshi Ogasawara \nPeng Wu IBM Thomas J. Watson Research Center IBM Research -Tokyo {castanos,edelsohn,pnagpurkar,pengwu}@us.ibm.com \n{nakatani,takeshi}@jp.ibm.com kiszk@acm.org Abstract Whenever the need to compile a new dynamically \ntyped language arises, an appealing option is to repurpose an ex\u00adisting statically typed language Just-In-Time \n(JIT) compiler (repurposed JIT compiler). Existing repurposed JIT compil\u00aders (RJIT compilers), however, \nhave not yet delivered the hoped-for performance boosts. The performance of JVM languages, for instance, \noften lags behind standard inter\u00adpreter implementations. Even more customized solutions that extend the \ninternals of a JIT compiler for the target lan\u00adguage compete poorly with those designed speci.cally for \ndynamically typed languages. Our own Fiorano JIT compiler is an example of this problem. As a state-of-the-art, \nRJIT compiler for Python, the Fiorano JIT compiler outperforms two other RJIT compilers (Unladen Swallow \nand Jython), but still shows a noticeable performance gap compared to PyPy, today s best performing Python \nJIT compiler. In this paper, we discuss techniques that have proved effective in the Fiorano JIT compiler \nas well as limitations of our current implementation. More importantly, this work offers the .rst in-depth \nlook at bene.ts and limitations of the repurposed JIT compiler approach. We believe the most common pitfall \nof existing RJIT compilers is not focusing suf.ciently on specialization, an abundant optimization opportunity \nunique to dynamically typed languages. Unfortunately, the lack of specialization cannot be overcome by \napplying traditional optimizations. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 Categories and Subject Descriptors D.3.4 [Programming Languages]: \nProcessors Compilers; Optimization; Incre\u00admental Compilers; Interpreters General Terms Languages Keywords \nScripting Languages; Python 1. Introduction Dynamically typed languages are becoming increasingly popular \ndue to productivity improvements enabled by rapid prototyping and incremental deployment cycles. While \npro\u00adgrammers rely on the .exibility of dynamic typing, higher\u00adlevel data structures, and meta-programming \nto continuously improve their applications and unfold new features, the same features that appeal to \nprogrammers directly impact perfor\u00admance and make code optimization very challenging. The initial implementation \nof a dynamically typed script\u00ading language is typically a simple interpreter where applica\u00adtions can \nrun one to three orders of magnitude slower than equivalent implementations in C or Java, especially \nfor small kernels. The past decade has seen signi.cant advances in dy\u00adnamically typed language Just-In-Time \n(JIT) compilers es\u00adpecially for Javascript [3, 12, 15, 24], but also for other lan\u00adguages such as Python \n[5, 7, 13, 17]. 1.1 The Repurposed JIT Compiler Phenomenon There are two types of dynamically typed language \nJIT com\u00adpilers: one type is based on an existing JIT compiler de\u00adsigned for statically typed languages \nthat has been repur\u00adposed (an RJIT compiler) and the other type is designed from scratch for the target \nlanguage. Examples of RJIT com\u00adpilers include translation to C++, such as HipHop [4] (PHP); translations \nto Java, such as JRuby [6], Jython [7], Rhino [9] (JavaScript); translation to CLR, such as IronPython \n[5]; re\u00adpurposes of the LLVM JIT compiler [31], such as Unladen Swallow [13] (Python) and Rubinius [10] \n(Ruby); and repur\u00adposes of the IBM production-level Java JIT compiler, such as P9 [36] (PHP) and ours \n[29] (Python). The main appeal of the repurposing approach is the re\u00adduction of development and maintenance \ncosts by leverag\u00ading a mature infrastructure that includes a well-de.ned IL, a rich set of optimizations, \nsupport for multiple platforms, and other standard services. However, the performance of the RJIT compiler \napproach has not been satisfactory. To\u00adday, the largest performance gains are seen on JIT compilers for \ndynamically typed language that use radically different approaches from traditional statically typed \nlanguage com\u00adpilation. The Shootout benchmarks [11], a popular benchmark for evaluating the performance \nof various programming lan\u00adguages and implementations, show that the performance of RJIT compilers is \nmuch lower than when a JIT compiler is designed speci.cally for the dynamically typed language. For example, \nPyPy [17], the fastest Python JIT compiler, is a tracing JIT compiler and a custom virtual machine (VM). \nOther RJIT compiler implementations include Un\u00adladen Swallow (an LLVM-based JIT compiler for CPython), \nFiorano (a RJIT compiler for CPython based on the IBM Java JIT compiler [35]), and Jython (a Java JIT \ncompiler operating on Java bytecodes translated from Python pro\u00adgrams). Among these RJIT compilers, Jython \nyields little performance improvement over CPython, and though Un\u00adladen Swallow and the Fiorano JIT compiler \nare noticeably faster than CPython, they do not generate the kind of dra\u00admatic improvements of PyPy. \n 1.2 Effective Optimizations for RJIT compilers In this paper, we present a detailed analysis of RJIT \ncompiler performance based on our experience in building the Fiorano JIT compiler. We focus on maximizing \nthe effectiveness of a JIT compiler that uses a statically typed IR when applied to a dynamically typed \nlanguage. We identify three important acceleration techniques for RJIT compilers. Dynamically typed languages \npresent unique compiler optimization challenges and many optimization techniques have been created to \novercome them [20, 27]. After early optimizations transform the dynamically typed scripting lan\u00adguage \nidioms into sequences more familiar to existing JIT compilers, the full strength of a standard JIT compiler \nopti\u00admization engine can address the remaining gaps. Chang et al. reported on the relative importance \nof these early op\u00adtimizations [22]. To make the RJIT compiler effective, the system must .rst apply optimizations \ntargeted speci.cally at dynamically typed language. 1.2.1 Specialization, Specialization, Specialization \nA key insight is the observation that, when moving from statically typed to dynamically typed languages, \nthere is an important shift in the optimization opportunities. For a dynamically typed language, the \nmain overhead comes from generic implementations of the primitive oper\u00adations and generic data representations \nas a result of sup\u00adporting the rich semantics. For example, in CPython, all of the data is in objects \nand a generic add operation can in\u00advolve tens of basic blocks with complex control-.ows and object allocations. \nThe main optimization opportunities in such languages come from specialization, which is a form of strength \nreduction on generic implementations. Not only are there many specialization opportunities (since most \ncommon Python bytecodes can be specialized), but the payoff can be huge. Sometimes, the specialized code \ncan be orders of mag\u00adnitude faster than the original generic implementations. This means the RJIT compiler \nshould be primed for ef\u00adfective specialization. Dynamically typed languages intro\u00adduce greater optimization \nchallenges and bene.t consider\u00adably from high-level optimizations based on the language semantics. Failing \nto specialize with suf.cient coverage and suf.cient accuracy is the most common reasons an RJIT compiler \nis ineffective. 1.2.2 Less Is More One surprising observation is that the less a RJIT com\u00adpiler depends \non existing optimizations for specialization, the more effective the RJIT compiler is, and the more it \nben\u00ade.ts from its other existing optimizations. This has to do with the type of generic implementations \nthat the specialization optimizations have to deal with. The generic operations implemented in a dynamically \ntyped lan\u00adguage runtime often involve library calls, heap side-effects, and complex control-.ows, which \nare very hard to analyze. Therefore, specializations based on a traditional data-.ow framework are often \nunreliable due to the fundamental dif\u00ad.culty in accurately computing global side-effects. As a re\u00adsult, \nan effective RJIT compiler relies more on techniques that are not based on data-.ow framework, such as \nfeedback\u00addriven specialization. 1.2.3 Guard-based Specialization Another intriguing aspect of the less \nis more phenomenon is that not only is specialization a strength reduction opti\u00admization itself, it is \nalso a catalyst to enable effective use of existing optimizations in a RJIT compiler. This is because \nspecialized codes, unlike their generic counterparts, often are clean of data-.ow inhibitors such as \ncalls, heap accesses, and control-.ow join. To fully maximize the bene.t of existing optimizations, we \nadvocate guard-based specialization (see Section 3.3.1) that creates a specialized code guarded by a \nruntime con\u00addition. A code specialized by guard does not include the un-specialied path in the compiled \ncode and if the guard fails, the compiled code will bail out to the interpreter. This is in contrast \nto traditional versioning-based specialization that versions the code into a fast (specialized) version \nand a slow (generic) version. While both approaches achieve a similar degree of strength reduction, guard-based \nspecializa\u00adtion is much more amenable to data-.ow analysis because it removes slow-path code, which often \nincludes library calls and memory side-effects, from the scope of the optimizer.  1.3 Common Pitfalls \nof the RJIT Compiler Approach In spite of the effectiveness of their acceleration techniques, RJIT compilers \ndo not achieve the same level of perfor\u00admance as custom designed scripting language JIT compilers. Is \nthe RJIT compiler approach fundamentally not viable for achieving maximum performance from dynamically \ntyped languages? While we cannot offer a de.nitive answer, our experience building an RJIT compiler and \nevaluating several alternatives sheds some lights on the limitations and com\u00admon pitfalls of the RJIT \ncompiler approach. One key lesson we learned is that dynamically typed lan\u00adguage runtimes play a critical \nrole in the effectiveness of RJIT compilers. The design of the Fiorano JIT compiler in\u00adcluded an early \ndecision to reuse the CPython runtime with as few changes as possible.1 However, most generic dynam\u00adically \ntyped language runtimes are not designed for effec\u00adtive compilation. For example, the execution trace \nof a typi\u00adcal Python bytecode implemented by the Jython runtime of\u00adten consists of between 150 to 300 \nJava bytecodes, where one-third of the bytecodes perform heap operations, method invocations, and branches, \nand we found similar execution characteristics in CPython. Therefore, the .rst common pit\u00adfall of the \nRJIT compiler approach is an overreliance on the JIT compiler alone to improve the performance, while \nmuch less efforts are devoted to improving the runtime to reduce path length or to facilitate compilation. \nFor fat runtimes like Jython or CPython, long instruction paths are the most dominant source of overhead. \nTo reduce the amount of computation bloat, the longest instruction paths should be greatly shortened \nto approach the ef.ciency of statically typed languages. The second common pitfall of the RJIT compiler \napproach is an overreliance on traditional redundancy elimination optimizations, such as commoning or \ndead code elimination, to reduce the path lengths in the fat runtime. As shown in Jython, these optimizations \nare not very effective. This is because fat runtimes impose two ma\u00adjor hurdles to data-.ow analysis: \n(1) limited analysis scope because long call-chains cannot be inlined, and (2) limited ability to remove \nredundant heap computations because the heap analysis must be conservative. In contrast, specializa\u00adtion \nbased on language semantics proves to be more effective because it often requires no or only local data-.ow \ninforma\u00adtion.  1.4 Contribution and Organization This is the .rst work of its kind that offers an in-depth \nlook at the approach of extending a JIT compiler designed for statically typed languages to optimize \ndynamically typed languages. The paper makes the following contributions: 1 This is primarily for compatibility \nreasons because there is a large Python code base that directly interacts with the internal data structures \nof CPython via extension modules. We offer fresh insights about why the performance of RJIT compilers \noften lags behind that of dynamically typed language JIT compilers designed from scratch and we identify \nseveral common pitfalls that contribute to this effect.  We present design principles for effective \nRJIT compil\u00aders and recommend several techniques: early, feedback\u00addirected, guard-based specialization; \npartial IL extension; and semantic inlining.  We give details about the design and implementation of \nFiorano, our own RJIT compiler for Python, discuss its strengths and weakness, and quantify the bene.ts \nof different optimization strategies.  The rest of the paper is organized as follows: Section 2 gives \nan overview of Python. Section 3 discusses techniques to effectively map dynamically typed language semantics \nto the intermediate representation of an existing JIT compiler. In Section 4 we present our Fiorano JIT \ncompiler implemen\u00adtation. A detailed evaluation of the Fiorano JIT compiler is presented in Section 5, \nfollowed by an analysis of four differ\u00adent Python compilers and other dynamically typed language compilers \nin Section 6. We conclude in Section 7. 2. Python Language and Implementation Python is a general-purpose, \nhigh-level, object-oriented pro\u00adgramming language that encourages productivity and sup\u00adports several \ndynamic features such as dynamic typing and dynamic objects [26]. 2.1 CPython Semantics The default de \nfacto standard implementation of Python is referred to as CPython [8]. CPython is written in C and com\u00adpiles \nPython programs into bytecodes which are then exe\u00adcuted in a stack-based virtual machine. Other implementa\u00adtions \nof Python exist, such as Jython (mapping Python to the JVM) [7], IronPython (mapping Python to CLR/.Net) \n[5], Cython (translating to C) [16], and PyPy (Tracing JIT com\u00adpiler ) [17]. Figure 1 shows an example \nof CPython byte\u00adcodes. 2.1.1 Data and Object Model Every data in Python is an object, including the primitive \ndata types such as integer. Object .elds are called attributes and are stored as name-value pairs in \ndictionaries associated with the object, the class, or its parent classes. Python objects are dynamic \nas they can change their classes, and attributes can be modi.ed, deleted, or added to an existing object. \nThis means there are no .xed object structure at compile time. 2.1.2 Local Variables and Constants Local \nvariables are accessed through the LOAD FAST or STORE FAST bytecodes, and constants are accessed via \nLOAD CONST. As shown in Figure 1, all three of these byte\u00adFigure 1. CPython bytecodes corresponding to \nthe loop in Figure 3.a. 15 LOAD_FAST 2 (x) 18 LOAD_FAST 0 (size) 21 COMPARE_OP 0 (<) 24 JUMP_IF_FALSE \n24 (to 51) 27 POP_TOP 28 LOAD_FAST 3 (res) 31 LOAD_CONST 2 (1) 34 BINARY_ADD 35 STORE_FAST 3 (res) 38 \nLOAD_FAST 2 (x) 41 LOAD_CONST 2 (1) 44 BINARY_ADD 45 STORE_FAST 2 (x) 48 JUMP_ABSOLUTE 15 case BINARY_ADD: \nw = POP(); v = POP(); x = PyNumber_Add(v, w); Py_DECREF(v); Py_DECREF(w); PUSH(x); if (x == NULL) throwException(); \nbreak; Figure 2. CPython bytecode handler for BINARY ADD. codes encode an integer value that is used \nto directly index the local or constant arrays stored in the PyFrameObject.  2.1.3 Name Resolution Object \n.elds (known as attributes) and global variables (in\u00adcluding method names) are referenced by name and \nrep\u00adresented using the bytecodes LOAD ATTR(GLOBAL) or STORE ATTR(GLOBAL). The name resolution in CPython \nis done each time these bytecodes are executed. In a typ\u00adical Python application, name resolution occurs, \non aver\u00adage, every 5 to 10 bytecodes [32]. Name resolution itself is also quite expensive and involves \nextensive error checking, pointer indirections, invocations of runtime helpers,and hash table lookups. \n 2.1.4 Generic Operations Arithmetic and compare operations, such as + , are type generic. Implementations \nof these operations are much heav\u00adier than their type-speci.c counterparts. They often involve complex \ncontrol-.ows that dispatch to speci.c sequences of instructions linked to the operand types. Both the \noperands and the results of generic operations are objects. Figure 2 shows the CPython handler for a \ntypical generic operation, BINARY ADD. Most Python bytecode handlers use a similar code pattern that \ninvolves a call to the run\u00adtime that implements the actual computation of the byte\u00adcode, some reference \ncount handling via Py INCREF() and Py DECREF(), and error checking that may throw ex\u00adceptions.  2.1.5 \nMethod Invocation For Python, a function invocation allocates and initializes the PyFrameObject, which \nincludes a PC, the operand stack, and local variables, and passes the arguments of Python. Method invocation \nis very common in Python and occurs, on average, every 7 bytecodes in a typical Python applica\u00adtion [32]. \n 2.2 Characteristics of Dynamically Typed Language Runtime Most standard and generic implementation \nof dynamically typed languages suffer from high overhead. Not only does a generic implementation incur \nsigni.cant runtime overhead, it also reduces the effectiveness of traditional optimizers. 2.2.1 Performance \nand Overhead Unlike statically typed languages, the interpretation over\u00adhead for dynamically typed languages \ncontributes only a small fraction of the performance gap [32]. Instead, the over\u00adhead comes mostly from \nthe generic implementation of a rich and highly dynamic semantics. Key features are: generic typing is \nuniversal and requires type dispatch inside each op\u00aderation; a monolithic object representation is used \nfor all of the data types including simple ones such as integers; most operations can throw exceptions; \nand most named accesses are resolved dynamically and repeatedly at runtime. This is in steep contrast \nto statically typed languages such as Java where there is often a straightforward mapping between Java \nprimitives and machine instructions. 2.2.2 A Case Study of Jython Jython is another implementation of \nPython in which a Python program is .rst translated into Java bytecode and then run as a normal Java \napplication, which is optimized by a standard Java JIT compiler. The Shootout benchmarks demonstrate \nthat Java JIT compilers cannot greatly improve the performance of Jython, which sometimes is slower than \nCPython. In the design space of RJIT compilers, Jython falls at one end of the spectrum, where minimal \ncustomization for the target scripting language is incorporated into the JIT com\u00adpiler. We use Jython \nas an example to illustrate the chal\u00adlenges of optimizing a generic dynamically typed language runtime \nand the limitations of traditional compilers when op\u00adtimizing such runtimes. Consider the Python loops \nshown in Figure 3. All three loops compute the same value, but use different Python constructs. While \na C implementation of the same computation involves only a few machine instruc\u00adtions per iteration, the \nnumber of operations performed by Jython is signi.cantly higher. As shown in Table 1, each it\u00aderation \nof the loops in Figure 3 executes between 500 and 1100 Java bytecodes, where heap accesses, object alloca\u00ad \n def calc1(self,res,size): def calc3(self,res,size): def foo(self): x = 0 x = 0 return 1 while x < size: \nwhile x < size: res += 1 res += self.a def calc2(self,res,size): n += 1 x += 1 x = 0 return res return \nres while x < size: res += self.foo() x += 1 return res (a) localvar-loop (b) getattr-loop (c) call-loop \n Figure 3. Simple Python Loops. # Java bytecode path length per Python loop iteration path length per \nPython bytecode (a) localvar-loop (b) getattr-loop (c) call-loop LOAD LOCAL ADD LOAD ATTR COMPARE CALL \nheap-read 47 80 131 3 5 29 17 53 heap-write 11 11 31 0 2 4 2 16 heap-alloc 2 2 5 0 1 1 0 2 branch 46 \n70 101 2 8 19 18 34 invoke (JNI) 70(2) 92(2) 115(4) 0 (0) 17 (0) 23 (0) 26 (2) 23 (2) return 70 92 115 \n0 17 23 26 23 arithmetic 18 56 67 0 5 38 8 11 local/const 268 427 583 6 60 152 96 154 Total 534 832 1152 \n12 115 289 191 313 Table 1. Number of Java bytecode executed by Jython for one iteration of the Python \nloops shown in Figure 3 and for one Python bytecode, where heap-read (heap-write) includes get(put).eld/get(put)static \nbytecode, and heap-alloc includes new/anew bytecode. tions, branches, and invocations account for nearly \none third of the instruction mix. Table 1 also shows the instruction path length to exe\u00adcute one Python \nbytecode in Jython. In most cases, a single Python bytecode involves between 160 and 300 Java byte\u00adcodes, \nspans more than 20 method invocations, and performs many heap-related operations. Similar path lengths \nare also observed in the CPython implementation.2 This path length information quanti.es the fundamental \noverhead of a generic dynamically typed language runtime. There are many excessively long paths due to \nthe implemen\u00adtations of the language primitives. The keys to using a con\u00adventional compiler to optimize \nsuch runtimes are: (1) mas\u00adsive (partial) inlining to see through deep chains of method invocations; \n(2) accurate heap analysis to eliminate redun\u00addant heap accesses and allocations; and (3) massive redun\u00addancy \nelimination to shorten the instruction path lengths. 3. Designing a Dynamically Typed Language JIT Compiler \nUsing a Static IL This section describes design considerations when repurpos\u00ading a type-speci.c optimization \nframework for dynamically typed languages. We .rst discuss the implications of using an existing, type-speci.c \nintermediate language for Python (Section 3.1), then present our approach to address the se\u00ad 2 A typical \nthree-step execution of CALL FUNCTION, LOAD ATTR, and BINARY ADD yields between 30 and 60 basic blocks \neach in the CPython implementation. mantic gap between the Intermediate Language (IL) and the Python \nprimitives (Section 3.2), and .nally explain how to maximize the effectiveness of the existing optimization \nen\u00adgine (Section 3.3). 3.1 Implication of Using a Static Intermediate Language The IL is the foundation \nof any compiler optimization frame\u00adwork. We refer to the IL used for a statically typed language as a \nstatic IL. A static IL is by de.nition type speci.c, which means that each primitive operation has a \nunique type signa\u00adture for its operands and result. In addition, the names (sym\u00adbols) referenced by a \nstatic IL are usually resolved statically or semi-statically3. In contrast, dynamically typed languages \noften use type-generic bytecodes and require dynamic name resolution. Given the difference between these \ntwo semantics, build\u00ading a dynamically typed language JIT compiler based on an existing static IL has \nseveral implications: 1. Translating dynamically typed language bytecodes to a static IL can have a signi.cant \noptimization effect. This is because the generic implementations are the major source of overhead in \ndynamically typed languages, and thus the mapping to a static IL can be a powerful special\u00adization optimization \nin itself. In contrast, IL translation 3 For semi-static name resolution, the symbols are resolved into \nconcrete values (e.g., offsets) once at runtime and will remain resolved unless certain events such as \ndynamic class loading occur) in a typical statically typed language compiler is seldom viewed as an \noptimization. 2. To perform effective specialization during the translation process, we need to collect \ninformation and sometimes perform analysis outside the static-IL based optimization framework. For example, \nthe lowering of type-generic bytecodes to a static IL requires type information, which can be collected \nvia a type pro.ler (on the interpreter) or from a type inference engine analyzing the bytecode of a dynamically \ntyped language. 3. The design point of the original static-IL optimizer may differ from that of a dynamically \ntyped language JIT compiler. For instance, what is deemed as a high-level optimization in a JIT compiler \nfor statically typed lan\u00adguages (such as escape analysis) may be a basic opti\u00admization (such as allocation \nremoval) in a dynamically typed language JIT compiler.  The rest of the section discusses how to map \nrich, dynam\u00adically typed language semantics to a static IL to maximize the optimization effects during \ntranslation and when reusing existing optimizations.  3.2 Feedback-directed Runtime Specialization While \na generic implementation of rich, dynamic semantics is the major source of overhead in dynamically typed \nlan\u00adguages, operations at each speci.c program counter (PC) of\u00adten exhibit a strongly biased behavior, \nsuch as the types for an arithmetic operation, the location of a resolved reference, or the target of \nan invocation. This provides fertile ground for specialization, which can be viewed as a form of strength \nreduction of generic operations and generic data representa\u00adtions. Such specialization typically includes \nspecialization of operation types, name resolution, and invocation targets. We used runtime feedback \n(pro.ling) as the primary means to decide on when and what to specialize in our framework. Simplicity \nis the main appeal of runtime feed\u00adback since it requires little program analysis and is a pure runtime \ntechnique. This allows feedback-directed special\u00adization to happen as early as possible, thus maximizing \nthe optimization effect of the translation step. In such a design, the accuracy and coverage of runtime \nfeedback pro.les be\u00adcome .rst-order constraints on the effectiveness of the JIT compiler. Such heavy \nreliance on pro.ling is not used in statically typed language JIT compilers, but it is a typical characteristic \nof JIT compilers for dynamically typed lan\u00adguages. While other approaches rely on program analysis to \nde\u00adduce specialization targets such as types [1, 39], they often require an IL that closely matches the \ntarget language se\u00admantics for the analysis engine. Therefore, analysis-based specialization is not appropriate \nfor a framework like ours that is based on an existing static IL. if (x->type == PyFloat) { // fast path \nt = PyFloat_fromFloat(-x); } else { // slow path t = PyNumber_Negate(x); } (a) versioning-based guard(x->type \n== PyFloat) // fast path t = PyFloat_fromFloat(-x); (b) guard-based Figure 4. Pseudo code of two approaches \nfor specializa\u00adtions. 3.3 Effective Lowering from Dynamically typed Language Semantics to Static IL \nGiven the semantic difference between Python bytecode and a typical static IL, the translation to a static \nIL, when done ineffectively, can result in a naive translator that re\u00adplaces every single Python bytecode, \nsuch as BINARY ADD, with the corresponding CPython runtime routine, such as PyNumberAdd(), in the resulting \nIL. Not only does lit\u00adtle specialization occur during the mapping, but the IL after translation is .lled \nwith runtime calls that are hard for any traditional optimization framework to analyze. In contrast, \nan effective IL translation will be an impor\u00adtant optimization in itself if it takes a generic implementa\u00adtion, \nspecializes it, and makes it speci.c. The rest of this section discusses general techniques for effective \nmapping to a static IL. 3.3.1 Guard-based Specialization Figure 4 shows two approaches to specializing \na generic op\u00aderation. The .rst example is versioning-based, and includes both a specialized implementation \n(a fast path) and a generic implementation (a slow path). The second example is guard\u00adbased, but includes \nonly the fast path. A guard is a condi\u00adtional form of control-.ow in which, when a guard test fails, \nthe execution bails out to the interpreter and does not re\u00adturn to the current compilation scope. While \nboth achieve the same degree of strength reduction for the generic op\u00aderation, the impact on subsequent \ndata-.ow analysis dif\u00adfers. Guard-based specialization does not introduce any join\u00adnode in the control-.ow \ngraph after specialization, while versioning-based specialization does, which is the key dis\u00adtinction \nbetween these two forms of specialization. The elimination of control-.ow join via guard-based spe\u00adcialization \nis a key mechanism to enable effective data-.ow in a specialized program. In essence, guard-based specializa\u00adtion \nprunes a generic control-.ow graph, where each generic operation in the control-.ow graph had many outgoing \nedges for different type combinations, based on a given set of spe\u00adcialization conditions.  3.3.2 (Partial) \nIL Extension Because the IL is the foundation of any optimization frame\u00adwork, any IL extension often \nimplies extension to optimiza\u00adtions operating on the IL. When reusing an existing opti\u00admization framework, \nhowever, only a few IL extension may be accommodated. One use of IL extension is to encapsulate common \ncode patterns into the IL to avoid disrupting the analysis frame\u00adwork. For instance, most Python bytecode \nhandlers, as shown in Figure 2, involve error checking and the handling of reference counting, both of \nwhich involve control-.ow and calls to runtime helpers such as throwException.A straightforward translation \nof a Python bytecode handler to a static IL would create several basic blocks per bytecode. Instead, \nwe can introduce IL instructions to represent refer\u00adence counting and error handling, which are later \nexpanded into the actual code sequences after most optimizations has been completed. Another scenario \nfor IL extension is to extend the seman\u00adtics of an existing IL to express a target language primitive \nwith different semantics. We call such an extension a partial IL extension because the IL is only used \nin selected opti\u00admizations where the semantic differences do not matter, but has to be expanded into \nthe original IL instructions with the additional IL instructions to perform Python-speci.c func\u00adtions. \n 3.3.3 Semantic Inlining of Runtime Helpers The presence of unknown library calls (often in binary form \nand on the heap) is a major inhibitor to effective data-.ow analysis. Semantic inlining [37] is a technique \nthat allows the semantics of standard runtime helpers, which in our context are CPython runtime helpers, \nto be encoded directly into the compiler. Semantic inlining is commonly used in modern JIT compilers \nfor important Java standard class, such as java/lang/String. In a JIT compiler for dynamically typed \nlanguages, the simplest form of semantic inlining speci.es the memory side-effects of the CPython runtime \nhelpers. More advanced forms of semantic inlining can include optimizations on recognized runtime helpers, \nsuch as folding a call sequence (i.e. the sequence PyInt asInt(PyInt fromInt(x)) that retrieves the value \nof a newly created boxed value from x). 4. Fiorano JIT Compiler Implementation This section describes \nour Fiorano JIT compiler for CPython to illustrate the steps we took to repurpose a mature Java JIT compiler \nas a dynamically typed language JIT compiler. Details of the design and implementation of the Fiorano \nJIT compiler can be found in [29]. 4.1 Overview The Fiorano JIT compiler was developed on top of the \nTes\u00adtarossa JIT compiler [35], a mature compilation infrastruc\u00adture that supports statically typed languages, \nsuch as Java, and multiple platforms, such as POWER and x86. The in\u00adfrastructure consists of three customizable \ncomponents: IL generator, IL optimizator, and code generator. To support Python, we .rst added a new \nIL generator that translates Python bytecode into the infrastructure s type\u00adspeci.c IL. We then perform \nPython-speci.c, guard-based specialization to convert as much as possible of the type\u00adgeneric Python \nbytecode into the type-speci.c IL, such as integer and .oat. After that, we can reuse most of the stan\u00addard \noptimizations in the Testarossa JIT compiler with some changes to support the minimal IL extensions we \nmade to accommodate the Python semantics. We also added a few Python-speci.c optimizations inside the \noptimization engine of the Testarossa JIT compiler. Our JIT compiler supports a variety of optimization \nlevels, which trade optimization complexity against speed. The Fiorano JIT compiler is attached to the \nCPython interpreter as a shared library. JIT compilation is triggered if the execution count of a method \nexceeds a prede.ned hotness threshold. There is also a major component that adds pro.ling and runtime \nfeedback between CPython and the Fiorano JIT compiler. We extended the existing interpreter pro.ling \nmechanism to collect Python-speci.c pro.les. 4.2 Runtime Pro.le and Feedback Our JIT compiler has a \nruntime pro.ler and offers the API to the interpreter. Through the API, the interpreter can send the \npro.ler types of the objects used when the interpreter executes bytecode. The API calls were inserted \nin one-third of the Python opcodes that the JIT compiler can optimize by using runtime type information. \nWe could have done sampling for these API calls, but we always send the data to the pro.ler in our evaluation \nsystem, because we believe that the overhead of the API calls is negligible after compiling most of the \nfunctions during a suf.ciently long warm-up time. Up to .ve types are collected for each bytecode address \nbefore compilation. The pro.ler updates the frequencies of the types for each bytecode with the received \ndata, which are used later by the JIT compiler to optimize the code. We modi.ed the original source code \nof CPython to insert the API calls. The JIT compiler can know how frequently each bytecode was executed \nand which types occurred most frequently from the pro.led data. The JIT compiler can .nd the data saved \nby the pro.ler by specifying the bytecode address. If there is no pro.led data for a given bytecode, \nthen the JIT compiler does not optimize it because it will be executed rarely. If the JIT compiler obtains \na distribution for the data, such as 100% use of the type A or 50% for type A and 50% for type B, then \nthe JIT compiler can use the data distribution for optimizations. For most of the optimizations, the \nJIT compiler can specialize the code with the type if the pro.le shows that a type is used for 100% of \nthe executions of the target bytecode.  4.3 IL Generation and Early Optimizations This subsection describes \nmapping Python bytecodes to the IL and the Python-speci.c optimizations we apply during IL generation. \n4.3.1 Stack Frame Design for JITed code While the interpreter allocates a PyFrameObject at each method \ninvocation, we choose to allocate a stack frame on the system stack for the JITed code. Using different \nframe designs for the interpreter and JITed codes is a common practice in modern JIT compilers since \nit is much easier for a compiler to optimize stack-allocated variables com\u00adpared to heap-allocated variables. \nWhen a JITed method is invoked, arguments for the method are copied-in from a CPython frame to a JIT \ncompiler s frame. Each local vari\u00adable is assigned to a stack-allocated variable. The height of the operand \nstack is also assigned to a stack-allocated vari\u00adable. When a JITed method invokes a method or yields, \nlive variables are copied-out from the stack frame of the JITed code to the CPython frame. Using such \na stack frame design of the JITed code, accesses to Python local variables can be mapped directly to \nour IL representing local accesses.  4.3.2 Mapping Python Bytecodes to the IL During the IL generation, \na Python bytecode, by default, is translated into a call to a corresponding CPython runtime helper. For \nexample, when processing the UNARY NEGATE bytecode, we generate a call to PyNumber Negate(). Such a translation \n(referred to as the naive translation) is straightforward and preserves the semantics of the byte\u00adcode \nas implemented in CPython. However, the resulting IL would look like subroutine-threaded code with many \nrun\u00adtime calls. If later optimizations cannot specialize such run\u00adtime calls to a faster sequence of \nexpanded instructions, then the bene.ts of applying traditional optimizations are limited. To address \nthe limitation of the naive translation, we added a new opcode, guard, and re-mapped the semantics of \na few existing IL instructions to Python semantics. Note that a full extension of the IL to Python semantics \nwould defeat the purpose of reusing an existing JIT compiler. This is because any IL extension requires \nsome degree of modi\u00ad.cation to the existing optimizations that operate on the IL, and therefore a full \nextension would require fundamental and pervasive changes to the existing JIT compiler. The new guard \nopcode is a control-.ow IL instruction with a condition operand and a target basic block (BB) num\u00adber. \nHere are the semantics of the guard opcode: execu\u00adtion falls through if the condition is true,o therwise, \nthe ex\u00adecution bails out to the interpreter through the target BB. Note that, upon the guard failure, \nthe execution does not re\u00adturn to the current compilation scope. When bailing out to th interpreter, \nthe target BB restores the states of the inter\u00adpreter frame PyFrameObject to be consistent with the stack \nframe of the JITed code at the point of the guard fail\u00adure, which includes local variables, the operand \nstack, block structures for a for loop and try, and the interpreter PC. We extended the existing iaload \nopcode, which origi\u00adnally represented an indirect load with an offset to a refer\u00adence, to represent the \ncommon semantics of LOAD ATTR. The generic implementation of LOAD ATTR includes the handling of many \ncorner cases and can cause side-effects. For example, LOAD ATTR may have heap side-effects when calling \ngetattr , allowing a user to de.ne actions. When our JIT compiler determines that LOAD ATTR only performs \na simple load from the heap, it maps LOAD ATTR to the iaload opcode. Such mapping allows optimizations \nsuch as common sub-expression elimination and partial re\u00addundancy elimination to apply to the translated \nIL. However, LOAD ATTR may execute unexpected operations through operator overloading at runtime that \nwere not apparent at compilation time. Therefore, our JIT compiler also inserts a guard opcode before \nthe iaload opcode. Note that when the targeted data-.ow optimizations are completed, the iaload is converted \nback to the actual implementa\u00adtion of LOAD ATTR. A similar extension is also used for the iastore opcode \nfor STORE ATTR. We also extended the existing isinstance opcode that checks whether an object is an instance \nof a class in Python. We map Python s built-in function isinstance to the corresponding IL in\u00adstruction, \nso that redundant isinstance operations may be removed by an existing optimization in the JIT com\u00adpiler \n[29]. After adding the IL extensions, most Python bytecodes would still be mapped to a call to a Python \nruntime helper. To minimize the impact of calls with unknown side-effects on the data-.ow analysis, we \nexpose the important data-.ow properties of such helpers to the compiler. Such properties include: (1) \nwhether or not a runtime helper has side-effects (for general data-.ow); (2) whether it can be eliminated \nwhen the result is not used (for redundancy elimination); and (3) the type signature of a helper (for \ntype-.ow analysis). For instance, PyFloat fromFloat is known to the compiler as an operation with side-effect \nthat returns a PyFloat ob\u00adject, but it is also know that the operation can be eliminated if the return \nobject is not used. 4.3.3 Type Specialization In dynamically typed languages, a generic operation such \nas PyNumber Negate is very slow because there is over\u00adhead to determine the actual action for each object \nbased on its type. If the type of the given object is dominated by the particular type for a generic \noperation, then our JIT com\u00adpiler inserts a guard opcode for the dominant type. If the type guard can \nbe inserted, then our JIT compiler can apply type specialization, which replaces a runtime helper for \nthe o = PyNumber_Negate(x) (a) original IL guard(x->type == PyFloat) f = -(x->float_value) ... (b) \nspecialization with unboxing guard(x->type == PyFloat) f = -(x->float_value) t = PyFloat_fromFloat(f) \n... (c) type specialization Figure 5. Guard-based optimizations. generic operation with a faster implementation \nfor the partic\u00adular type, as shown in Figure 5 (b). After the type specializa\u00adtion, the generated IL \nsequence will consist of primitive type operations and an object allocation for the primitive type if \nthe result of an operation produces a primitive type, such as int or .oat.  4.3.4 Specialization for \nName Resolution We apply two Python-speci.c optimizations in the IL gener\u00adation phase. The .rst is specialization \nfor the value loaded by LOAD GLOBAL. Previous work [26] observed that the value of a global variable, \nwhich often refers to a method resolved by name, rarely changes after the initialization of an appli\u00adcation. \nBased on this observation, our JIT compiler looks up the value of the global variable in a dictionary \nat compila\u00adtion time and puts that value into the IL instructions as a constant [13]. In addition, our \nJIT compiler installs watch\u00aders in the dictionary so that when a variable is updated, the watcher invalidates \nthe compiled code and the method is re\u00adcompiled without applying this optimization upon the next invocation. \nThe second optimization recognizes common Python built-in functions. Since the built-in function names \nare searched for by the LOAD GLOBAL bytecode, this optimiza\u00adtion is closely linked with the previous \none. If that optimiza\u00adtion has identi.ed a value as a constant and it includes a function pointer for \na built-in function, then our JIT com\u00adpiler handles it as that built-in function [13]. 4.3.5 Control-.ow \nRepresentation of Exceptions Another consideration during the IL generation is how to represent exceptions \nin the control-.ow graph. Unlike Java where only reference bytecodes may throw exceptions, any instruction \nexcept for the stack-manipulating Python byte\u00adcodes may throw exceptions, such as out-of-memory excep\u00adtions, \nTo avoid injecting too many control-.ow into the gen\u00aderated IL, we use a factored control-.ow graph (CFG) \n[23] to handle these frequent exception checks in our IL. The fac\u00adtored CFG maximizes the size of a basic \nblock for which existing optimizations can effectively be applied.  4.4 Late Python-speci.c Optimizations \nThis section describes Python-speci.c optimizations that are applied after IL generation. Some are added \nas new opti\u00admizations to the optimization pipeline of a Java JIT com\u00adpiler. Others are extended from \nexisting optimizations in the JIT compiler. 4.4.1 Type Propagation In Python, all data is represented \nas objects. In our JIT com\u00adpiler, an object is represented as a struct in a heap, with typical .elds \nin the object, such as reference count, type, and value. Each .eld of a Python object is accessed by \nan indirect memory access using a base address with an offset. During the type propagation optimization \npass, the type .eld is handled to propagate the known type infor\u00admation such as integer, .oat, or list. \nThe computed type in\u00adformation can be used to eliminate redundant guard and to support additional type \nspecialization. 4.4.2 Allocation Removal and Unboxing Optimization As described in Section 4.3.3, early \ntype specialization may result in a IL sequence that performs a type-speci.c opera\u00adtion, produces a primitive \nvalue, and then boxes the value into the corresponding Python object. Eliminating unnec\u00adessary boxing \nof primitive values is the most importance optimization for this type of specialization and is done by \na data-.ow optimization called the unboxing optimization. The purpose of unboxing optimization is to \navoid redundant allocation of primitive CPython objects. In particular, unboxing is used to assign a \nscalar value in the object to a stack-allocated variable so the corresponding object allocation can be \neliminated. If the object is used later at return or in code that bails out to the interpreter, then \nour JIT compiler inserts an object allocation using the scalar value. This approach makes our unboxing \nwidely applicable than the previous approach [18]. Our JIT compiler uses unboxing as shown in Figure \n5 (c). 4.4.3 Late Reference Count Injection For reference counting, our JIT compiler does not generate \ncode for maintaining the reference counting for an object, which means that the IL ignores reference \ncounting during most of the optimization phases. Instead, we inject late-stage code for handling reference \ncounting using the algorithm in [30]. This late expansion avoids fragmentation of the basic blocks, which \nmakes the code more data-.ow friendly. 4.4.4 Semantic Inlining of Runtime Helpers This technique inlines \nthe fast-path implementation of the LOAD ATTR and STORE ATTR bytecodes directly into the IL representation \nwithout calling slow runtime helpers. This is a form of specialization from the generic implementations \nof these opcodes. Benchmark Description django use the Django template system to build a 150x150 cell \nHTML table .oat arti.cial, .oating point heavy benchmark nbody the n-body shootout benchmark nqueens \nsmall solver for the 8-queens problem pystone Dhrystone written in Python richards the classic Richards \nbenchmark rietveld macrobenchmark for Django using the ri\u00adetveld code review application slowpickle serializaing \nPython objects using the python pickle module slowspit.re use the Spit.re template system to build a \n1000x1000 cell HTML table slowunpickle deserializing Python objects using the python pickle module spambayes \nrun a canned mailbox through the Spam\u00adbayes ham/spam classi.er Table 2. Description of the Unladen-Swallow \nbenchmarks  4.4.5 Specialization of isinstance and hasattr Built-ins The isinstance built-in checks \nwhether an object is an instance of a class. The hasattr built-in checks whether an attribute name exists \nin an object. The generic form of hasattr has a cost similar to that of LOAD ATTR. Because both built-ins \ndeal with the type metadata of a Python object and return a true or false result, one can specialize \nthe results of these built-ins if the type of the input object is known (either via type propagation \nor as deduced from its type guards) [29]. 5. Evaluation of the Fiorano JIT Compiler We next examine the \neffectiveness of our JIT compiler. We .rst discuss the overall performance of our JIT compiler at different \noptimization levels. We then analyze the ef.cacy of different specialization techniques. In the next \nsection we compare our approach against several other approaches and discuss their relative advantages \nand shortcomings. 5.1 Methodology We performed our experiments on a 3.8-GHz Intel i7 2600k processor \nwith 8GB RAM, running Fedora Core 15 Linux. We used CPython version 2.6.4 [2] as our baseline inter\u00adpreter, \nand eleven benchmarks from the Unladen Swallow benchmark suite [14]. Table 2 summarizes the benchmarks \nused. All eleven benchmarks are single-thread python pro\u00adgrams, ranging from simple microbenchmarks like \nfloat and pystone to benchmarks based on real-world Web ap\u00adplications like django and rietveld. For the \nJIT com\u00adpiler performance data (for both our and other approaches), we report the post-warmup, steady-state \nperformance, since we are targeting long-running Web applications.  5.2 Performance at Different Optimization \nLevels Figure 6 shows the performance for different optimization levels of our JIT compiler compared \nto the standard CPython interpreter. Each higher optimization level performs addi\u00adtional optimizations \nin addition to lower level optimizations. Note that, the Fiorano JIT compiler does not yet perform in\u00adlining \nof user-level Python methods. The noOpt-level disables almost all local and global op\u00adtimizations in \nthe IL optimization phase of the origi\u00adnal JIT compiler. In essence, only the IL generation and code \ngeneration components of the original JIT com\u00adpiler are exercised with no Python-level specialization. \nInterpreter-level pro.ling is also always enabled at all op\u00adtimization levels. The cold-level enables \nbasic-block-level (local) optimiza\u00adtion, such as common sub-expression elimination, value propagation, \nand dead-store elimination, as well as opti\u00admizations speci.c to Python, including local type prop\u00adagation, \ntype specialization, semantic inlining of run\u00adtime helpers, and reference counting optimizations. This \nlevel performs most optimizations for dynamically typed languages during the IL translation (Section \n4.3). The isinstance and hasattr (Section 4.4) specializa\u00adtions are also enabled since these optimizations \ndo not rely on data-.ow analysis and are inexpensive to perform. The warm-level enables standard data-.ow \noptimizations across basic blocks (global) as well as Python-speci.c optimizations such as unboxing optimization \nand global type propagation. The hot-level enables more expensive global optimizations such as partial \nredundancy elimination. As shown in Figure 6, the Fiorano JIT compiler achieves an average speedup of \n2 over CPython. The noOpt-level achieves a 1.2x performance improvement over the CPython interpreter. \nFor typed languages, a basic (naive) compiler that directly translates Python bytecodes into calls to \nthe Python runtime produces code that looks like subroutine threaded code. The gain from such compilation \nis limited since, according to an earlier study [32], the interpreter dis\u00adpatch overhead accounts for \nless than 5% of the CPython execution time. The biggest gains are observed when switching to the cold \nlevel. Most of these gains at the cold-level come from basic Python speci.c optimizations that do not \nrely on the IL-level optimizations in the original JIT compiler. For in\u00adstance, most of the improvement \nin django comes from effective specialization of the isinstance and hasattr built-ins. Another observation \nis that the cold-and warm-level op\u00adtimizations are a lot more effective on small kernels, such as float \nand nbody. At the cold level, the bene.ts of type specialization and the semantic lining of runtime helpers \nfor  Figure 6. Performance by optimization level LOAD ATTR are easily observed in float, where we dou\u00adble \nthe performance going from noOpt to cold. At the warm\u00adlevel, the JIT compiler removes object allocations \nand heap accesses from the critical path of hot loops via unboxing of the integer and .oat objects. As \na result, we achieved signif\u00adicant speedups of 5 and 4 for float and nbody, respec\u00adtively. In contrast, \nthe gains from higher-level optimizations, ei\u00adther traditional or Python-speci.c, are not as signi.cant \non benchmarks with mostly non-hotspots, such as django, rietveld, slowspitfire, and spambayes. We ob\u00adserve \nalmost no performance gain when moving from the warm to hot level of optimization. We believe this is \ndue to several limitations of our current implementation, such as the lack of Python-level method inlining \nand PC-speci.c pro.ling that is context-insensitive, both of which are more relevant to larger workloads. \nAlso, our reliance on a data\u00ad.ow framework to perform common specializations such as name resolution \nspecialization (e.g., for LOAD ATTR) and the unboxing optimization may become unreliable for larger workloads, \nsince these optimizations often require accurate heap analysis to con.rm the legality of the transformations. \n 5.3 Effectiveness of Specialization We instrumented the JIT compiler and the CPython inter\u00adpreter to \ntrack the numbers and types for the Python byte\u00adcodes executed for a complete run of all of the benchmarks. \nThese results are shown in Figure 7. The different data sets in this .gure are:  Bytecodes executed \nby the interpreter, which is further divided into Interpreted, meaning those interpreted be\u00adfore the \nmethod is compiled; and Interpreted-guard\u00adfailure, meaning those interpreted due to a guard failure in \na compiled method, which occurs when a guard or watchpoint fails in the JITted code, forcing a branch \nto a side exit routine that returns execution to the interpreter for the remainder of that function. \n Bytecodes executed in the JITted code, which are subse\u00adquently divided into:  Compiled-unspecializable, \nwhich are simple byte\u00adcodes such as those that manipulate the interpreter stack (i.e. POP TOP), load \na constant (LOAD CONST) or access a local variable (LOAD FAST). The com\u00adpiler will remove these operations \nduring IL genera\u00adtion or by using standard optimizations. This group in\u00adcludes control .ow bytecodes \n(i.e. JUMP IF FALSE) which are translated to CFG edges by the JIT com\u00adpiler. Unspecializable also includes \nbytecodes for complex data structures that we currently do not spe\u00adcialize such as BUILD LIST.  Benchmark \nCOMPARE/BINARY CALLS ATTRIBUTE ACCESS Total %Specialized Success Failure Total %Specialized Success Failure \nTotal %Specialized Success Failure django 46M 31.43 0.04 113M 2.51 0.02 87M 61.14 0.08 .oat 72M 99.78 \n0.00 36M 42.76 0.00 138M 99.78 0.00 nbody 359M 95.20 0.00 10K 0.00 0.00 8K 0.00 0.00 nqueens 103M 99.91 \n0.00 15M 0.00 0.00 8K 0.00 0.00 pystone 169M 80.61 0.00 57M 0.00 0.00 78M 0.00 0.00 richards 38M 79.27 \n0.00 26M 0.00 0.00 102M 25.02 3.97 rietveld 50M 56.74 0.11 125M 3.73 0.00 149M 64.10 5.21 slowpickle \n50M 74.63 0.00 116M 10.05 0.00 95M 27.22 0.00 slowspit.re 52M 97.95 0.00 156M 0.00 0.00 49K 0.88 0.00 \nslowunpickle 82M 80.17 0.00 88M 3.25 0.00 45M 1.55 0.00 spambayes 158M 75.76 0.00 117M 15.29 0.00 150M \n42.07 0.17 Figure 7. Effectiveness of Specialization Specializable bytecodes, which are Python bytecodes \nthat are subject to specialization by the compiler. These bytecodes are Compiled-unspecialized because \nwe do not have enough runtime information at com\u00adpile type to decide on a specialization strategy. They \nare Compiled-specialization-succeeded if we success\u00adfully specialize the bytecodes (and the guards do \nnot fail) or Compiled-specialization-failed if the guards for specialization fails. All of the benchmarks \nexecute the majority of their byte\u00adcodes in the JITted code, where the warmup phase (except for slowpickle) \nis less than 10% of the total amount of bytecode. In all cases, there are very few guard failures, al\u00adthough \nin django this results in about 20% of bytecodes being executed in the interpreter. We narrowed this \nspeci.c case down to a particular call site for a builtin function (len) for PyListObject types: although \nfor 90% of the cases in this speci.c call site the specialization is correct, the re\u00admaining 10% of failures \nhave a major impact on the all of the other bytecode. Benchmarks that rely on data structures that we \nspecial\u00adize effective such as float or nbody result in around 40% of successful specializations, but \nfor most of the other more complex benchmarks the successful specialization rate is less than 20%. These \nbenchmarks also show that we are missing approximately 20% of the potentially specializable bytecode, \neither because we lack suf.cient runtime informa\u00adtion or because we currently do not specialize those \nobject types. The table in Figure 7 gives further insight into the cov\u00aderage and failures observed for \nthree sets of specializ\u00adable bytecodes: unary, binary, or compare operations (i.e. BINARY ADD), function \ncalls (i.e. CALL FUNCTION) and attribute access (i.e. LOAD ATTR). The total column rep\u00adresents the total \nnumber of bytecode instructions executed for each group and for each benchmark, while the next two columns \nshow the success and failure rates of specializa\u00adtion. For each benchmark we show up to three of the \nmost relevant types (from a total of approximately 100 types in CPython). From this table, it is clear \nthat our JIT compiler can successfully specialize most compare and binary opera\u00adtions for most benchmarks, \nas well as the attribute access for several of the benchmarks. But our success rate is still low for \napproximately half of the attribute access benchmarks and for most of the function call bytecodes.  \n5.4 Allocation Removal Another way to evaluate the effectiveness of our environ\u00adment is to analyze the \nnumber of CPython objects that we were able to remove during optimization. Table 3 shows the total number \nof PyObjects initialized by the interpreter and the percentage reduction we observed in our JIT com\u00adpiler \nat the hot optimization level for a complete run. Note that at the noOpt level both the interpreter and \nthe JIT com\u00adpiler allocate the same number of Python objects. The table also lists, for each benchmark, \nthe top two object types by percent reduction along with the contribution of each object type to the \ntotal reduction. Not surprisingly, the Python objects that we specialize and box and unbox effectively \nare easily removed by the compiler from simple benchmarks like float and nbody where standard optimization \ntechniques like value propaga\u00adtion work well. These are the benchmarks where we obtain some of our biggest \nspeedups. For the float benchmark, we are able to remove 56.25% of all the PyFloatObjects and 45.48% \nof the total objects. Also the JIT compiler spe\u00adcializes a generic mechanism for calling init object \ninitializers. This specialization removed PyMethod objects (shown as instancemethod) by 33.33% for float. \nThese specializations signi.cantly reduce the complexity of the JITted code, but the improvement remains \nlimited to 5 times or lower. In web applications like the django and rietveld benchmarks the specialization \nfor the isinstance builtin call resulted in the removal of PyCFunction objects (builtin functions or \nmethods). We removed approximately 68.38% of PyCFunction objects in the django bench\u00admark. Also specialization \nfor name resolution removed PyStringOjbects (str) by keeping the resolved val\u00adues of IMPORT NAME bytecode. \nFor django, we removed 90.60% of all the PyStringObjects and 50.02% of the total objects.  5.5 Method \nInlining Method inlining is a well-know and important optimization in a optimizing compiler. This embeds \na callee body into a caller at method invocation call sites, and expands the compilation scope. In addition, \nmethod inlining can reduce the overhead of complicated argument passing in Python such as is used for \nkeyword arguments. In Section 5.2, we mentioned that our JIT compiler does not currently support inlining \nof user-level python functions. In order to evaluate the potential of expanding the compila\u00adtion scope, \nwe manually embedded a callee body into a caller at method invocation call sites for float and richards. \nWe observed 1.05x and 1.81x performance improvements for float and richards compared to the original \nver\u00adsions, respectively. This improvement is fairly constant be\u00adtween optimization levels, and since \nthe gains in hot are not relatively higher than noOpt, this indicates that we are not automatically taking \nadvantage of larger scopes. Note that this is an idealized upper-bound on what can currently be achieved \nwith the current implementation (since manually inlining we modi.ed the original program). We are currently \nimplementing method inlining. Ideally, when a compiler applies method inlining, it is better for per\u00adformance \nto allocate only one incorporated frame instead of allocating and deallocating the callee frames. In \nsome cases, a callee frame is referenced at runtime. To correctly support the Python runtime, these two \ncases must be supported: re.ection: Dynamically typed languages support more powerful re.ection features \nsuch as the inspection of live objects including a PyFrameObject. To inspect a PyFrameObject with sys. \ngetframe() requires preparing the complete status of PyFrameObject. If the method inlining allocates \nonly one object, the run\u00adtime needs to reconstruct the PyFrameObject when sys. getframe() is executed \n[17], which is a rela\u00adtively complicated implementation.  bailing out to the interpreter: When the execution \nbails out to the interpreter, the execution environment should restore the state of the interpreter frame \nPyFrameobject to be consistent with the stack frame for the JIT com\u00adpiler, which includes local variables, \nthe operand stack, and the interpreter PC. If the method inlining allo\u00adcates only one object, the runtime \nneeds to reconstruct a PyFrameobject that was implemented in another language [28], which is again a \nrelatively complicated implementation.  6. Understanding Other Dynamically Typed Language JIT Compilers \nThis section evaluates several other Python JIT compil\u00aders against our system, and contrasts other approaches \nto improve the performance of dynamically typed languages against the RJIT compiler approach. 6.1 Evaluation \nof Other Python JIT Compilers We evaluated our Fiorano JIT compiler against three other Python JIT compilers: \nJython 2.5.24 [7], Unladen Swal\u00adlow [13], and PyPy 1.8 [17], two of which are RJIT compil\u00aders except \nfor PyPy. Figure 8 shows the performance of the four JIT compilers relative to CPython on the Unladen \nSwal\u00adlow benchmarks. At one end of the spectrum is PyPy, which is by far the best performing Python JIT \ncompiler and which can sometimes signi.cantly outperform CPython. Jython is at the other end of spectrum, \nwith a performance that is al\u00adways below that of CPython. While the Fiorano JIT compiler 4 Jython 2.5.2 \ndoes not take advantage of the new InvokeDynamic and Method Handles in Java 7. Benchmark Objects allocated \nper run (interpreter) %Reduction (total) % Reduction (top two object types) Object type %Reduction %Contribution \nto total django 1,667,456 50.02 builtin function or method str 68.38 90.60 70.32 24.28 .oat 499,745 45.48 \n.oat instancemethod 56.25 33.33 90.00 10.00 nbody 5,460,067 42.49 .oat int 42.59 100.00 99.15 0.85 nqueens \n692,209 0.0027 int 4.76 100.00 pystone 349,255 0.00 richards 540,501 0.00 rietveld 928,134 15.10 builtin \nfunction or method instancemethod 28.83 11.09 62.72 16.90 slowpickle 246,801 1.46 traceback str 100.00 \n1.46 66.67 33.33 slowspit.re 2,733,040 0.00 slowunpickle 12,800 3.23 traceback 50.0 100.0 spambayes 559,302 \n1.89 .oat builtin function or method 16.28 0.10 98.12 1.85 Table 3. Allocation Removal Figure 8. Speedup \nover CPython on the Unladen Swallow benchmark suites. Figure 9. Speedup over CPython on common Python \nprimitives measured in pybench.  consistently outperforms the Unladen Swallow, it can only come close \nto PyPy on small benchmarks such as float and nbody. It is important to highlight that in this .gure \nJython executes on top of the same Java VM that was the base of our Fiorano JIT compiler and uses the \nsame standard compiler optimizations. The weakness of the RJIT compilers is evident when comparing the \nperformance of common Python operations measured by pybench. The pybench benchmark consists of kernels \nto evaluate common Python idioms wrapped in tight loops. While the benchmark is designed to measure the \nperformance of interpreters rather than compilers, low or negative performance improvements for common \nPython primitives reveal weaknesses in a compiler. As shown in Fig\u00adure 9, Jython is unable to optimize \nany category of Python primitives. While the Fiorano JIT compiler performs well on arithmetic operations, \nit falls short on call, lookup (of attribute or global value), and the handling of non-numeric data (i.e., \nstring, unicode, dictionary, list, and tuple). In both broad categories of call and lookup oper\u00adations, \nwe do quite well on the invocation and lookup of the built-in functions, but not on other types of calls \nand lookups. In contrast, a new JIT compiler based on a new object model (like Jython or PyPy) complicates \nthe compatibil\u00adity with existing CPython modules and extensions. These modules (like NumPy or cPickle) \ntypically interact with CPython s internal representation. Therefore, new JIT com\u00adpilers either require \nrewriting these modules to avoid using CPython objects, or conversions between their own internal objects \nand CPython s objects at the module boundaries. 6.1.1 PyPy As shown in Figure 8, PyPy achieves by far \nthe most gains and consistently outperforms the other JIT compilers except for slowspitfire.5 PyPy is \na meta-tracing JIT compiler for a custom Python VM written in RPython. It is similar to Jython in two \nways: (1) the entire runtime is readily avail\u00adable to the JIT compiler; (2) it relies heavily on data-.ow \noptimization to remove redundant computation in generic implementations. So why is PyPy relatively more \neffective than Jython? The effectiveness of PyPy comes from two characteristics. PyPy s custom runtime \nenables more effective specializa\u00adtion, such as its object model implementation that promotes specialization \nof runtime constants [19] and the use of hid\u00adden classes [21] to maximize successful specialization for \nlookup operations. The second characteristics is that PyPy s optimizing JIT compiler is designed to overcome \nthe two major inhibitors of effective data-.ow on generic language runtime, limitation on the analysis \nscope and on the heap redundancy elimination. Using a trace selection algorithm customized for the Python \nruntime, PyPy is able to form compilation scopes that encompass all runtime code executed in a Python-level \nloop. Such customization includes annotating the runtime to 5 The modest improvement of slowspitfire \nis because the benchmark produces an object allocation pattern for which PyPy s garbage collector is \nnot optimal. direct the tracer to unroll loops or to avoid tracing through methods with too many side-exits. \nTo enable heap redundancy elimination, PyPy takes ad\u00advantage of the lack of any split or inner-join in \na trace to per\u00adform a very aggressive abstract interpretation on each trace. Such abstract interpretation \nyields accurate must-points-to and must-aliasing information to allow aggressive heap re\u00addundancy removal \n[18].  6.2 JVM Languages and InvokeDynamic A typical approach to RJIT compilers is to convert a dynam\u00adically \ntyped language program to a Java application and let the Java JIT compiler optimize the program. This \nis typical for JVM languages. Jython [7] and JRuby [6] fall into this category. The Iron languages like \nIronPython [5] and map scripting languages for the .Net CLR framework form a sim\u00adilar group. The performance \nhopes of JVM-based dynam\u00adically typed languages are largely unful.lled. All of them suffer from similar \noptimization dif.culties. In Java 7, InvokeDynamic was introduced to improve the performance of method \ninvocation in dynamically typed languages. As shown in [34], InvokeDynamic has not produced any dramatic \nperformance improvements in Jython or JRuby. Fundamentally, no conventional Java JIT compiler has shown \nan ability to dramatically reduce the redundant computation in JVM language runtimes that are laden with \nheap operations and invocations. 6.3 Trace-vs. Method-based Dynamically Typed Language JIT Compilers \nWhile PyPy is the best performing Python JIT compiler, there is no clear evidence to prove that trace-based \ncompila\u00adtion is more effective for dynamically typed languages than method-based compilation. In particular, \nin the domain of JavaScript, where the competition for performance is quite .erce, all of the major leading \ncommercial JIT compilers are method-based. Some have even replaced trace-based ones [15, 24]. Indeed, \nwhether or not a JIT compiler is trace-based is not the main factor determining its effectiveness. For \nexam\u00adple, the published results for two trace-based Java JIT com\u00adpilers [25, 38] show only accelerations \n(less than 30%) for pybench using Jython over using a standard method-based Java JIT compiler.  6.4 \nDiscussion Almost all of the dynamically typed language JIT compil\u00aders share the common traits of a custom \nlanguage runtime to assist the JIT compiler with specialization and a carefully tuned, custom design \nof the object layout for ef.ciency. For example, LuaJIT [33] is a highly-tuned re-implementation of the \nLua language, with both an interpreter and a JIT com\u00adpiler. The interpreter is a customized, hand-written, \ndirect threaded, architecture-speci.c design that trades increased complexity for increased speed. The \ninterpreter preserves considerable semantic and contextual information in its in\u00adternal representation, \nincluding about type inference and loop analysis. The internal representation also stores un\u00adboxed constants \ndirectly in the IL and predictively narrows the values used as induction variables and index expressions \nto integers. Dynamic language interpreters have taken two basic approaches. One can implement a fairly \nsimple, easy-to\u00adunderstand interpreter, parsing the program into fat, high\u00adlevel bytecodes or abstract \nsyntax trees that retain much of the dynamism while deferring the semantic details to the runtime support \nfunctions. Alternatively, there are intricate, complicated parsers with powerful analyses to uncover \nmore of the program semantics and express them in a richer IL representation. Developing JIT compilers \nfor these dynamically typed languages must choose between the same, basic top-down or bottom-up approaches. \nThe safety and correctness of op\u00adtimization transformations depend on semantic knowledge and understanding \nof the program by the compiler. This in\u00adformation needs to be transmitted to the compiler or the compiler \nmust infer the information from somewhere. In the case of dynamically typed languages, this depends on \nwhere the information is represented and stored. Dynamic programming languages have a rich, expressive \nsyntax with many high-level constructs. This allows the pro\u00adgrams to capture lots of semantic information \nand knowl\u00adedge. The compiler can discover these semantics from the parser and early program analysis, \nannotating a rich IL with knowledge gleaned from the program and the original con\u00adtext. Alternatively, \nthe IL can remain abstract and opaque with semantic knowledge in the runtime, from which the JIT compiler \nmust extract the information. 7. Conclusion In the realm of dynamically typed language JIT compilers, \nwhy is the reuse of JIT compilers, much less effective than designing from scratch and using non-traditional \noptimiza\u00adtions? Why can t we extend JIT compilers designed for stat\u00adically typed languages? In this paper, \nwe offered our insights into the reasons based on our own experience of building a RJIT compiler for \nPython and through evaluations of other Python JIT com\u00adpilers. We identi.ed common pitfalls of RJIT compilers: \n(1) not focusing suf.ciently on the right optimization opportuni\u00adties such as specialization; and (2) \nnot .nding the right place to tackle the specialization problems and frequently relying too much on existing \noptimizations. Our point, however, is not to argue against the repurposing of JIT compilers, but to de.ne \nguiding principles and to promote techniques to con\u00adstruct an effective RJIT compiler. The problem boils \ndown to how to design an effective dynamically typed language JIT compiler based on an op\u00adtimization \nengine designed for statically typed languages, where there is a clear shift of optimization opportunities \nwhen moving from statically to dynamically typed lan\u00adguages. Dynamically typed languages present new \nchal\u00adlenges and require new optimization strategies not present in previous-generation JIT compilers. \nTo effectively reuse a JIT compiler, we need to prepend more optimizations tar\u00adgeted at dynamically typed \nlanguage features and simplify the representation to a form more easily consumed by an RJIT compiler. \nWe offer three guiding principles:  Effective specialization should be the top design priority of any \nRJIT compiler. Many RJIT compilers fall into the common trap of overreliance on existing optimizations \nfor specialization.  The importance of specialization explains the bene.ts of some common best practices \nin non-RJIT compilers and VMs, such as the use of hidden classes [21] to help specialize dynamic name \nresolution. These best practices should be bene.cial to and adopted by RJIT compilers.  Traditional \ndata-.ow optimizations do improve perfor\u00admance. There are general techniques to maximize the bene.ts \nof traditional optimizations, such as early, guard\u00adbased specialization, semantic inlining, and IL exten\u00adsions. \n References [1] JavaScript. http://www.ecma.ch/ecma1/STAND/ ECMA-262.HTM. [2] CPython. http://www.python.org/. \n[3] Google V8 JavaScript engine. URL http://code. google.com/p/v8/. [4] HipHop project. https://github.com/facebook/ \nhiphop-php/wiki/. [5] IronPython. http://ironpython.codeplex.com/. [6] JRuby. http://jruby.org/. [7] \nJython. http://www.jython.org/. [8] Python language. http://www.python.org. [9] Rhino. http://www.mozilla.org/rhino/. \n[10] Rubinius. http://rubini.us/. [11] Shootout: the computer language benchmarks. http:// shootout.alioth.debian.org/. \n[12] SquirrelFish extreme JavaScript engine, 2008. http://webkit.org/blog/189/ announcing-squirrelfish/. \n[13] Unladen-Swallow project, . http://code.google. com/p/unladen-swallow/wiki/. [14] Unladen-Swallow \nbenchmarks, . http://code.google. com/p/unladen-swallow/wiki/Benchmarks. [15] M. Bebenita, F. Brandner, \nM. Fahndrich, F. Logozzo, W. Schulte, N. Tillmann, and H. Venter. SPUR: a trace-based JIT compiler for \nCIL. In Proceedings of ACM conference on Object-Oriented Programming, Systems, Languages, and Applications \n(OOPSLA), pages 708 725, 2010. [16] S. Behnel, R. Bradshaw, and D. S. Seljebotn. Cython. http://cython.org/. \n[17] C. F. Bolz, A. Cuni, M. Fijalkowski, and A. Rigo. Trac\u00ading the meta-level: PyPy s tracing JIT compiler. \nIn Proceedings of the 4th workshop on the Implementation, Compilation, Optimization of Object-Oriented \nLanguages and Programming Systems (ICOOOLPS), pages 18 25, 2009. [18] C. F. Bolz, A. Cuni, M. Fijalkowski, \nM. Leuschel, S. Pedroni, and A. Rigo. Allocation removal by partial evaluation in a tracing JIT. In Proceedings \nof the 20th ACM workshop on Partial Evaluation and Program Manipulation (PEPM), pages 43 52, 2011. [19] \nC. F. Bolz, A. Cuni, M. Fijalkowski, M. Leuschel, S. Pe\u00addroni, and A. Rigo. Runtime feedback in a meta-tracing \nJIT for ef.cient dynamic languages. In Proceedings of the 6th workshop on Implementation, Compilation, \nOptimization of Object-Oriented Languages, Programs and Systems (ICOOOLPS), 2011. [20] C. Chambers and \nD. Ungar. Making pure object-oriented languages practical. In Proceedings of ACM conference on Object-Oriented \nProgramming, Systems, Languages, and Applications (OOPSLA), volume 26, pages 1 15, 1991. [21] C. Chambers, \nD. Ungar, and E. Lee. An ef.cient implemen\u00adtation of self a dynamically-typed object-oriented language \nbased on prototypes. In Proceedings of ACM conference on Object-Oriented Programming, Systems, Languages, \nand Applications (OOPSLA), pages 49 70, 1989. [22] M. Chang, B. Mathiske, E. Smith, A. Chaudhuri, A. \nGal, M. Bebenita, C. Wimmer, and M. Franz. The impact of optional type information on JIT compilation \nof dynamically typed languages. In Proceedings of the 7th symposium on dynamic languages, pages 13 24, \n2011. [23] J.-D. Choi, D. Grove, M. Hind, and V. Sarkar. Ef.cient and precise modeling of exceptions \nfor the analysis of Java programs. In Proceedings of ACM workshop on program analysis for software tools \nand engineering, pages 21 31, 1999. [24] A. Gal, B. Eich, M. Shaver, D. Anderson, D. Mandelin, M. R. \nHaghighat, B. Kaplan, G. Hoare, B. Zbarsky, J. Oren\u00addorff, J. Ruderman, E. W. Smith, R. Reitmaier, M. \nBebenita, M. Chang, and M. Franz. Trace-based Just-In-Time type specialization for dynamic languages. \nIn Proceedings of ACM conference on Programming Language Design and Implementation (PLDI), pages 465 \n478, 2009. [25] C. H\u00a8aubl and H. M\u00a8ossenb\u00a8ock. Trace-based compilation for the Java HotSpot virtual \nmachine. In Proceedings of ACM conference on the Principles and Practice of Programming in Java (PPPJ), \npages 129 138, 2011. [26] A. Holkner and J. Harland. Evaluating the dynamic be\u00adhaviour of Python applications. \nIn Proceedings of the 33rd Australasian conference on computer science, pages 19 28, 2009. [27] U. H\u00a8olzle \nand D. Ungar. A third generation Self imple\u00admentation: Reconciling responsiveness with performance. \nIn Proceedings of ACM conference on Object-Oriented Programming, Systems, Languages, and Applications \n(OOPSLA), Oct. 1994. [28] U. H\u00a8olzle, C. Chambers, and D. Ungar. Debugging opti\u00admized code with dynamic \ndeoptimization. In Proceedings of ACM conference on Programming Language Design and Implementation (PLDI), \nJune 1992. [29] K. Ishizaki, T. Ogasawara, J. Castanos, P. Nagpurkar, D. Edel\u00adsohn, and T. Nakatani. \nAdding dynamically-typed lan\u00adguage support to a statically-typed language compiler: perfor\u00admance evaluation, \nanalysis, and tradeoffs. In Proceedings of conference on Virtual Execution Environments (VEE), pages \n169 180, 2012. [30] P. G. Joisha. A principled approach to nondeferred reference\u00adcounting garbage collection. \nIn Proceedings of conference on Virtual Execution Environments (VEE), pages 131 140, 2008. [31] C. Lattner \nand V. Adve. LLVM: A compilation frame\u00adwork for lifelong program analysis &#38; transformation. In Proceedings \nof international symposium on Code Generation and Pptimization (CGO), Mar 2004. [32] N. Mostafa, C. Krintz, \nC. Cascaval, D. Edelsohn, P. Nag\u00adpurkar, and P. Wu. Understanding the potential of interpreter\u00adbased \noptimizations for Python. Technical Report 2010-14, UCSB, Jan 2010. [33] M. Pall. LuaJIT. http://luajit.org/. \n[34] J. Siek, S. Bharadwaj, and J. Baker. JVM summit: invokedynamic and Jython, 2011. http://wiki.jvmlangsummit.com/images/8/8d/ \nIndy_and_Jython-Shashank_Bharadwaj.pdf. [35] V. Sundaresan, D. Maier, P. Ramarao, and M. Stoodley. Ex\u00adperiences \nwith multi-threading and dynamic class loading in a Java Just-In-Time compiler. In Proceedings of international \nsymposium on Code Generation and Pptimization (CGO), pages 87 97, 2006. [36] M. Tatsubori, A. Tozawa, \nT. Suzumura, S. Trent, and T. On\u00adodera. Evaluation of a Just-In-Time compiler retro.tted for PHP. In \nProceedings of conference on Virtual Execution Environments (VEE), pages 121 132, 2010. [37] P. Wu, S. \nMidkiff, J. Moreira, and M. Gupta. Ef.cient support for complex numbers in Java. In Proceedings of the \nACM conference on Java grande, pages 109 118, 1999. [38] P. Wu, H. Hayashizaki, H. Inoue, and T. Nakatani. \nReducing trace selection footprint for large-scale java applications with\u00adout performance loss. In Proceedings \nof ACM conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), pages \n789 804, 2011. [39] H. Zhao. HipHop compiler for PHP: Transforming PHP into C++. http://www.stanford.edu/class/ee380/ \nAbstracts/100505.html.      \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Whenever the need to compile a new dynamically typed language arises, an appealing option is to repurpose an existing statically typed language Just-In-Time (JIT) compiler (repurposed JIT compiler). Existing repurposed JIT compilers (RJIT compilers), however, have not yet delivered the hoped-for performance boosts. The performance of JVM languages, for instance, often lags behind standard interpreter implementations. Even more customized solutions that extend the internals of a JIT compiler for the target language compete poorly with those designed specifically for dynamically typed languages. Our own Fiorano JIT compiler is an example of this problem. As a state-of-the-art, RJIT compiler for Python, the Fiorano JIT compiler outperforms two other RJIT compilers (Unladen Swallow and Jython), but still shows a noticeable performance gap compared to PyPy, today's best performing Python JIT compiler. In this paper, we discuss techniques that have proved effective in the Fiorano JIT compiler as well as limitations of our current implementation. More importantly, this work offers the first in-depth look at benefits and limitations of the repurposed JIT compiler approach. We believe the most common pitfall of existing RJIT compilers is not focusing sufficiently on specialization, an abundant optimization opportunity unique to dynamically typed languages. Unfortunately, the lack of specialization cannot be overcome by applying traditional optimizations.</p>", "authors": [{"name": "Jose Castanos", "author_profile_id": "81100254095", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P3856058", "email_address": "castanos@us.ibm.com", "orcid_id": ""}, {"name": "David Edelsohn", "author_profile_id": "81100466085", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P3856059", "email_address": "edelsohn@us.ibm.com", "orcid_id": ""}, {"name": "Kazuaki Ishizaki", "author_profile_id": "81100389738", "affiliation": "IBM Research - Tokyo, Tokyo, Japan", "person_id": "P3856060", "email_address": "kiszk@acm.org", "orcid_id": ""}, {"name": "Priya Nagpurkar", "author_profile_id": "81100165666", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P3856061", "email_address": "pnagpurkar@us.ibm.com", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM Research - Tokyo, Tokyo, Japan", "person_id": "P3856062", "email_address": "nakatani@jp.ibm.com", "orcid_id": ""}, {"name": "Takeshi Ogasawara", "author_profile_id": "81100012455", "affiliation": "IBM Research - Tokyo, Tokyo, Japan", "person_id": "P3856063", "email_address": "takeshi@jp.ibm.com", "orcid_id": ""}, {"name": "Peng Wu", "author_profile_id": "81408599112", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P3856064", "email_address": "pengwu@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384631", "year": "2012", "article_id": "2384631", "conference": "OOPSLA", "title": "On the benefits and pitfalls of extending a statically typed language JIT compiler for dynamic scripting languages", "url": "http://dl.acm.org/citation.cfm?id=2384631"}