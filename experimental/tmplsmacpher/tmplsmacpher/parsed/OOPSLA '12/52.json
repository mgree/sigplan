{"article_publication_date": "10-19-2012", "fulltext": "\n Detecting Problematic Message Sequences and Frequencies in Distributed Systems Charles Lucas Sebastian \nElbaum Department of Computer Science and Engineering University of Nebraska Lincoln Lincoln, NE, U.S.A. \n {clucas,elbaum}@cse.unl.edu Abstract Testing the components of a distributed system is challenging as \nit requires consideration of not just the state of a component, but also the sequence of messages it \nmay receive from the rest of the system or the environment. Such messages may vary in type and content, \nand more particularly, in the frequency at which they are generated. All of these factors, in the right \ncombination, may lead to faulty be\u00adhavior. In this paper we present an approach to address these chal\u00adlenges \nby systematically analyzing a component in a distributed system to identify speci.c message sequences \nand frequencies at which a failure can occur. At the core of the analysis is the genera\u00adtion of a test \ndriver that de.nes the space of message sequences to be generated, the exploration of that space through \nthe use of dy\u00adnamic symbolic execution, and the timing and analysis of the gen\u00aderated tests to identify \nproblematic frequencies. We implemented our approach in the context of the popular Robotic Operating \nSys\u00adtem and investigated its application to three systems of increasing complexity. Categories and Subject \nDescriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation; D.2.5 [Software Engineer\u00ading]: \nTesting and Debugging General Terms Veri.cation, Reliability Keywords Distributed Systems, Test Case \nGeneration, Message Frequencies 1. Introduction Distributed systems consist of nodes which operate autonomously \nbut coordinate their actions towards a common goal. Such coordi\u00adnation relies on communication through \nmessage passing, and for systems requiring less coupling among its nodes, it is often asyn\u00adchronous. \nValidating the nodes of such distributed systems requires the exploration of three distinct inputs spaces. \nFirst, it requires explor\u00ading the state of the node under test. Second, it requires exploring the potential \nsequences of messages of different types that the node under test may receive from different sources. \nThird, it requires ex- Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. \n. . $10.00 David S. Rosenblum School of Computing National University of Singapore Republic of Singapore \n david@comp.nus.edu.sg ploring the frequencies at which such messages are generated since they may cause \nlost messages that affect system behavior. As we shall see, existing test case generation techniques \ncan assist with the .rst action to explore and set the node state. Com\u00adplemented with a sequence message \nmodel, existing techniques can assist with the second action of exploring message sequences. Ex\u00adploring \nthe space of message frequencies, however, is not supported by existing techniques. We lack a de.nition \nof what constitutes the space of interesting frequencies and a model that includes the unique attributes \nof such asynchronous distributed systems such as the use of message queues. In this work we present an \napproach to address all the require\u00adments to validate a component in a distributed system that uses asynchronous \nmessage passing. In particular, our approach focuses on the popular publish/subscribe paradigm that further \ndecouples senders and receivers, making them unaware of the overall system topology. Our approach is \nable to generate message sequences to validate a component, and more uniquely, is able to estimate mes\u00adsage \nfrequencies that may be problematic for a component under test. With such frequencies at hand, our approach \ncan pinpoint situ\u00adations where failures due to lost messages may occur and situations where the messages \ncould be sent at a higher rate with no increased likelihood of failure. Given a target node, our approach \noperates in .ve phases. First, it analyzes the messages that the target component can receive to build \na symbolic sequence of those messages. Second, it uses dy\u00adnamic symbolic execution to generate tests \nfor the component un\u00adder test utilizing the built symbolic sequence as a driver. Third, it runs and times \nthe generated tests to estimate a safe frequency bound (no messages would be lost at lower frequencies) \nand an unsafe bound (messages would be lost eventually at frequencies higher than the bound). Fourth, \nit derives further tests by system\u00adatically dropping a message. Fifth, it reports the speci.c sequences \nleading to a failure, sequences that with speci.c frequencies may cause a message to be dropped, and \nmessages that could be sent at a higher frequency without the risk of a failure. We have implemented \nan instance of the approach for the Robotic Operating System [20] which relies heavily on the pub\u00adlish/subscribe \nparadigm. In this work we provide three case studies showing the potential of the approach to address \nthe challenges aforementioned across systems of different complexity. The studies illustrate how the \napproach can be used to detect failures associated with lost messages and characterize the message frequencies \nthat a node should be able to manage. The rest of the paper is organized as follows. Section 2 details \na motivating example and an overview of our approach. Section 3 introduces some concepts and technologies \nused throughout the paper. Section 4 describes our approach and Section 5 details its Figure 1. iRobot \nsystem with .ve components that pass messages through queued communication channels.  implementation \nfor ROS. Section 6 gives our preliminary assess\u00adment. Section 7 surveys related work and Section 8 summarizes \nour effort and future research directions. 2. Motivating Example The diagram shown in Figure 1 represents \na simpli.ed version of an iRobot Roomba[14]. Its purpose is to explore and map an unknown room. The robot \nhas a set of motorized wheels for movement, a forward-facing proximity sensor for obstacle detection, \nand an odometer. The software system has .ve components, two for driv\u00ading sensors and actuators, one \nfor creating a map, one for collision avoidance, and one to control movement. The collision avoidance \ncomponent calculates the time to impact with an obstacle based on the proximity sensor reading and the \nintended velocity (magnitude and angle) reported by the controller. As long as the robot is not near \nan obstacle, it records the current location on the map. The components communicate by passing messages \non queued channels provided by a middleware. For simplicity we assume that the queues are of length one, \nwhich allows the receiving component to hold the next message while the current message is processed. \nThe controller component sends its intended velocity to the colli\u00adsion avoidance component at regular \ntime intervals. The collision avoidance component instructs the robot to slow down if the robot is within \ntwo time steps of hitting an obstacle, and to stop if it is within one time step of a collision. The \ninstruction to slow down is necessary because the robot cannot stop in one time step due to inertia. \nWhen stopped, the controller rotates the robot until the col\u00adlision avoidance component returns that \nthe system is safe given the desired velocity. Listing 1 presents an excerpt of the collision avoidance \ncompo\u00adnent of that system. There is a variable to track the state of the robot (line 2) and a variable \nrepresenting the proximity to an obstacle (line 3), and two callback methods that are invoked when messages \nof type speed and proximity are published by other nodes in the system. The proximity callback (line \n5) simply updates the local proximity variable. The velocity callback (line 8) updates the state of the \nrobot based on the proximity and velocity values. If time to impact is less than two time steps and the \nrobot is not in a working state, then the state will be switched to stopped (line 17). However, at less \nthan two time steps, if the robot was not slowed then a col\u00adlision will occur (line 19). If the time \nto impact is 2 then the robot will slow down (line 25) and the mapping task reduced. Otherwise, 1 // \nCollision Avoidance Component 2 State state ; // {working , slowed , stopped , crashed } 3 int proximity; \n4 5 proximityCallback(int proximityP) { 6 proximity = proximityP ; 7 8 velocityCallback(int speed) { \n9  10 if(speed == 0) 11 time to impact = 9999; 12 else 13 time to impact = proximity / speed ; 14 15 \nif(time to impact < 2) { 16 if(state != working) 17 state = stopped; 18 else { 19 state = crashed; 20 \nassert(false); // Failure 21 } 22 } 23 else 24 if(time to impact < 3) { 25 state = slowed; 26 reduceMapping \n( ) ; 27 } 28 else { 29 state = working; 30 activateMapping (); 31 } 32 33 publish(time to impact); 34 \n} Listing 1. Excerpt of collision avoidance component. if there is no obstacle in proximity (the time \nto impact is great than 2), then the robot continues normally, performing its mapping. The component \npublishes a time to impact message (line 33). This scenario illustrates one type of the subtle faults \nmentioned in Section 1. The fault only manifests when a velocity message that would move the system state \nto slowed is lost to the collision avoid\u00adance component because its queue is full. This scenario results \nin the controller not adjusting the robot speed which leads to a crash. This fault is not evident if \nall messages are received and processed in the correct order. However, if a velocity or proximity message \nis not received the robot may not be able to avoid colliding with the wall. Exposing such faults requires \n1) setting the components in the right state, 2) exploring the potential sequences of messages, and 3) \ndropping a key message in the communication among those processes. 2.1 Problem Characterization To illustrate \nthe program and message sequence space our approach will be exploring, we characterize the velocityCallback \nmethod of the collision avoidance component. Since there are multiple paths through the method s code, \nthe timing may vary depending on the input. Table 1 lists the paths through the code and the timing data \nof each. Paths 1 and 5 result in the robot being in the working state and take the longest at 280 milliseconds. \nIf the time step of the robot is longer than these slowest paths, there will not be any messages dropped, \nbecause the collision avoidance component can process messages at least as fast as they are produced \nby the proximity sensor component or the controller component. Paths 2 and 3, when the robot will stop \nor crash, are the quickest at 80 milliseconds. Table 1. Enumeration of feasible paths through the collision \navoidance code excerpt and their durations in milliseconds. Paths depend on speed, state and time to \nimpact (tti) (computed based on speed and proximity) Path # execution time Path Path Conditions Outcome \n1 2 3 4 5 280 80 80 180 280 T,F,F F,T,T F,T,F F,F,T F,F,F speed = 0 speed = 0 . tti < 2.!working speed \n= 0 . tti < 2 . working speed = 0 . 2 = tti < 3 speed = 0 . tti = 3 working . activateMapping() stopped \ncrashed slowed . reduceMapping() working . activateMapping() Sequence m1 m2 m3 m4 m5 m6 1 p=3 v=1 p=2 \n*v=1 p=1 v=1 2 p=3 v=0 p=2 *v=1 p=1 v=1 3 p=4 v=1 p=4 *v=2 p=4 v=4 4 p=4 v=1 *v=2 v=4 p=1 p=1 5 p=4 v=1 \n*v=2 p=1 p=1 v=1  Figure 2. Likelihood of failures occurring at various frequencies for the Roomba example. \nBelow 3.6 Hz no messages will be dropped because all paths complete faster than messages arrive. Above \n12.5 Hz, many messages will be dropped as the execution of their corresponding paths are slower. The \narea between will be .lled by our approach. If the time step is shorter than this, messages will be continually \ndropped, which could easily lead to the illustrated failure. If the time step is between those 80 and \n280 milliseconds, the likelihood of a failure occurring varies based on the content of the incoming messages. \nFigure 2 illustrates this variability. When velocity messages are sent at a frequency greater than 1 \n= 12.5 Hz, any path could 0.08s cause an error, so the system is deemed unsafe. If the frequency is less \nthan 1 =3.6 Hz, any path can be traversed before a new 0.28s message arrives, so no messages will be \ndropped. The area between these two frequencies is initially uncertain, as it depends on the content \nof the messages received, the order in which messages are received, the size of the message queue, and \nthe policy on dropping messages. Our approach aims to explore and characterize these areas.  2.2 Sketch \nof Approach Our approach explores the component behavior in the presence of sequences of messages while \nsearching for frequencies at which a message in a sequence would be dropped leading to a failure. This \nsearch simulates all interleavings of a sequence of messages coming into a component. For each interleaving, \nour approach tests whether the dropping of a message will cause a failure. For the sequences with a dropped \nmessage that caused failure, the approach will .nd a minimum frequency at which the messages need to \narrive in order to cause that failure. Table 2. Examples of message sequences which will cause a fail\u00adure \nif a message is dropped. A p message signi.es a proximity be\u00ading received. A v message signi.es a velocity \nbeing received. An asterisk indicates the message that causes the failure if dropped. For instance, for \nRoomba, when simulating sequences consist\u00ading of three proximity messages and three velocity messages \nour approach can identify situations where a velocity message will be dropped, causing a failure to occur. \nThis will occur if a message leading to a slowed state is lost. Table 2 shows some sequences of this \ntype which will expose this fault. The sequences vary in the order of the messages and the content. What \nstays the same is that the robot will be working after the .rst velocity message, slowed after the second, \nand stopped after the third. In each of these sit\u00aduations, the failure occurs if the second velocity \nmessage, the one that will cause for the state to be slowed, is dropped. For this to occur, three velocity \nmessages need to be received by the collision avoidance component in the time it takes for it to process \nthe .rst message. This leads to a necessary frequency of 3 = 10.7 Hz. 0.28s The discovery of these sequences \nis automated by our technique. 3. Background We have implemented our approach for systems built on top \nof the publish/subscribe paradigm, and we make use of dynamic symbolic execution. This section provides \nthe background in those two areas. 3.1 Publish/Subscribe Publish/subscribe is a popular paradigm for \ndistributed computing [9]. The primary objective of these systems is to decouple produc\u00aders and consumers \nby introducing a middleware system which han\u00addles message passing between publishers and subscribers. \nA pub\u00adlisher is a producer of content and a subscriber is a recipient of content. Publishers will send \nmessages to the middleware, which will then forward those messages to interested subscribers. In some \ncases, a publisher will advertise the types of content it will produce when it registers with the middleware \nsystem. The main advantages provided by a publish/subscribe middle\u00adware system are referential decoupling, \n.ow decoupling, and time decoupling [9]. Referential decoupling refers to the fact that the two processes \ndo not need to know about each other, they only need to know the type of information that they will produce \nor desire to consume. Flow decoupling refers to the fact that the two processes will not block each other \nwhen information is passed. Time decou\u00adpling refers to the fact that there is no built-in requirement \nfor when a message will be passed and received. A message can be sent at any time and can be received \nat any time.  Figure 3. Generic publish/subscribe system These decoupling advantages come at the cost \nof the system testability. This is largely due to the abstraction of the behavior of the communication. \nWithout a thorough understanding of how data travels through the system, it is dif.cult to write tests \nthat span the gap between a publisher and a subscriber [1]. In its most general sense a publish/subscribe \nsystem does not make any guarantees on the order at which messages will arrive. As such, any testing \napproach will need to check the orderings of the messages as well as their content. The possibility of \nmessages being dropped compounds this as certain sequences of messages may or may not cause drops. Additionally \nthe messages may arrive at times unrelated to when they were sent, meaning messages spaced out in a safe \nway may still arrive at once, causing messages to be dropped. In order to mitigate these problems, some \npublish/subscribe sys\u00adtems will hold themselves to certain quality of service attributes. As a means \nof classi.cation, Baresi et al. identi.ed a series of such at\u00adtributes [1] such as message reliability, \nmessage ordering, .ltering, and real-time guarantees. Meeting some of these properties restricts the \ndecoupling of the general system to allow for predictable be\u00adhavior. In addition and of particular relevance \nto message queues, there are properties such as subscription propagation delay (time between subscribing \nand .rst receiving messages), repliable mes\u00adsages (whether or not subscriber sends acknowledgement upon \nre\u00adceipt of a message), and queue drop policy (deciding which mes\u00adsage to drop if there is no room in \nthe queue). In de.ning our approach, we will restrict the publish/subscribe middleware with some of these \nproperties.  3.2 Message Passing in Publish/Subscribe Systems Conceptually a connection between a publisher \nand a subscriber will be supported by three queues. The queues store messages to be consumed later, and \nmay take different forms, such as .rst-in .rst out or a priority queue. This allows for messages to be \nheld if the subscriber is busy when they arrive. Figure 3 illustrates the structure of a general system. \nWhen a message is published it is .rst placed into an outgoing queue, Qo. After a delay, do, the message \nis transmitted from Qo to a dispatch queue, Qd, managed by the core infrastructure of the middleware. \nThis core could, in reality, be a series of separate systems which pass the message along, each with \nanother dispatch queue. The core pulls this message out of Qd, after a delay, dd, and transfers it to \nthe incoming queues, Qi, of any subscribers inter\u00adested in that message. For example, in Figure 3, the \nmessages on the velocity pub topic published by the controller component (Qov) reach the collision avoidance \ncomponent through its queue Qiv. If the subscriber is not currently processing a message, after a delay \nof di, it will be passed to a handler or callback which will process the message. Each of the queues \nhas an associated length, l(Q). When a message is sent to a full queue, either the incoming message or \na message from the queue must be dropped. If we assume that a message, m, takes dm to be processed by \nits associated handler, the total time from a message being published to being fully processed is dsum \n= do +dd +di +dm. Repeating message m with a delay of less than dsum will cause messages to back up in \nthe queue, which can lead to a dropped message. We can associate a frequency with this delay as .m = \n1 . dsum  3.3 Dynamic Symbolic Execution To explore the effects of various message sequences in a compo\u00adnent, \nits handling methods need to be analyzed. Sequences could be randomly generated, but there is no guarantee \nthat all distinct behaviors will be discovered. Instead we seek to discover all possi\u00adble paths through \na target component that may be exercised by all potential sequences of messages of a given size. This \nexploration is done through a process called dynamic symbolic execution (DSE) [13]. DSE works by executing \na program given a starting concrete in\u00adput and recording the path it takes. The path is represented through \na path condition that consists of all of the predicates encountered along that path. When the run is \n.nished, DSE will examine the path condition up to the last predicate before the end of the pro\u00adgram. \nFrom here it will attempt to solve the path condition as a set of constraints including the negation \nof that last predicate. For ex\u00adample, given slowed is false and speed is zero, the code listed in Figure \n1 with follow path 1 from Table 1. As a next step, DSE may try to switch the predicate (time to impact \n< 3) to true. This is not possible, since speed will need to be something other than zero. In\u00adstead it \nwill try the next predicate up, which will fail the same. The third predicate up (state != working) can \nbe changed yielding path 2. The .nal output from DSE is a set of inputs which will traverse each path \nthrough the method. These paths can each be timed and all of the relevant dm s found. 4. Approach We \nwould like to identify message sequences and speci.c frequen\u00adcies that may be problematic for a system \nunder test, both to iden\u00adtify situations where failures may occur and situations where the messages could \nbe sent at a higher rate with no increased likelihood of a failure. Failures are de.ned as those which \ncrash a program or violate an assertion. The analysis takes as input the source code of a component and \nthe number of messages to be considered, builds a driver that re.ects the space of message sequences \nof a certain length, and uses dynamic symbolic execution to generate the sequences of messages to traverse \nthat space exhaustively. For each of these sequences the approach calculates a frequency at which a failure \ncan occur due a dropped message and, if the frequency of incoming message is .xed, it determines if the \nfrequency is too conservative. Figure 4 outlines the .ow of the analysis. The program under test is .rst \ntranslated to be processed by DSE and instrumented to incorporate a symbolic message sequence to act \nas the testing driver. Next, DSE is run and the resulting tests are analyzed to produce the timing characterization, \nto .nd the problematic frequencies, and to create two test suites. One of the test suites reveals faults \neven if no messages are dropped, and the other applies the problematic frequencies to .nd faults from \ndropped messages. 4.1 Initial Assumptions To simplify the formulation of our technique we will start \nby mak\u00ading some assumptions about the publish/subscribe system. First, we will assume Total Ordering, \nmeaning that messages will always be Figure 4. Automated exploration of a component through message \ninterleavings. received in the same order. This may not be the same order they were sent, but it will \nbe consistent between runs. This ensures that tests generated for a component will operate the same when \nrun on that component, rather than the messages being received in a dif\u00adferent order. Second, we will \nassume a Hard Real-Time Guarantee by assuming that di, do, and dd are each zero, meaning that mes\u00adsages \nare instantly available to subscribers after being published. We will also be treating the system as \nif it had the Head Drop pol\u00adicy, meaning the oldest message is dropped if the queue is full, and treat \nmessages as reliably arriving at their destination queue.  4.2 Exploring Message Interleavings At the \ncore of our approach is the encoding of the space of message sequences relevant to the component under \ntest. The analysis starts by identifying the callbacks the component uses to process incom\u00ading messages. \nGiven a target message sequence size provided by the tester, the analysis then produces a test driver \nthat, if traversed exhaustively, would result in the exploration of all the relevant in\u00adterleavings of \nmessages to expose the behavior of the component under test. More formally, given the set of callbacks \nin the component under test C = {c1,c2, ..., cn}, where each ci . C has a ci.type corresponding to the \nmessage subscription topic handled by the callback, and given a sequence size s, our approach will generate \na sequence of messages SymSeq =(m1,m2, ..., ms) | .mj . SymSeq.ci . C, mj .type = ci.type This representation \nconstrains the size of the message sequence and the type of messages allowed, but it leaves the message \norder and their content unconstrained in order for it to be explored. The driver implementation structure, \nwhich will be later exempli.ed in detail, consists of a loop that iterates from 1 to s, and a switch \nstatement that resides within the loop with c cases, one for each callback. The type of messages and \ntheir values are declared symbolically and explored through dynamic symbolic execution. This step is \nreferred to as the DSE instrumentation in Figure 4. The result is a set of inputs (message sequences) \nthat exercise all the potential paths in the component given the bound s. Call this set S. Also recorded \nin the generation of these inputs is the result of whether or not a failure was detected. Since the inputs \nthat cause a failure do not depend on whether messages are dropped, they are placed into set Sf and are \ndisregarded in the next phase of the analysis. This leaves a set of passing inputs, Sp. From this set \nthe technique determines the effect of dropped messages. For each of these inputs, the technique creates \na new set, Sdi, of inputs by dropping each message of the original sequence once. The union of all sets \nSdi is referred to as Sd. Finally each item in Sd is tested. Those that pass are placed into a set SPd \nand those that fail are placed into a set SFd. For the items in SFd, the dropping of a message was the \ncause of the failure. To reason about when messages are dropped from the sequences in Sd, the processing \ntime of each message is recorded. These sets and their relations are summarized in Figure 4. The described \nsets and timing pro.les are analyzed to generate the outputs of the approach. 4.3 Frequency Analysis \nThere are three outputs of the approach based on the data gener\u00adated. The .rst of these is the set of \ninitially failing message se\u00adquences, Sf . This set of sequences reveals failures if no messages are \ndropped. The second output is the maximum frequency allow\u00adable to ensure that no messages are dropped. \nThis is done by ex\u00adamining the timing pro.les of the sequences in SPd and SFd. By identifying the maximum \nduration of each callback, our approach .nds the minimum frequency at which a message could be dropped. \nAs long as the input frequency of messages is below that frequency, the queues will never .ll, and no \nmessages will be dropped. The third output is generated from set SFd and the timing pro.les of its member \nsequences. The total elapsed time, te, is the sum of time spent processing by each callback method called \nup to the point a message is dropped. The frequency of incoming QT +i messages required to cause this \ndrop to occur is . = Hz, te where QT is the length of the queue for the channel the dropped message arrives \non, and i is the index of the dropped message. Since message i is dropped, the previous i - 1 messages \nhave been processed in te. By receiving the message i plus QT additional messages, the queue will be \nfull when the last message is received, causing message i to be dropped. For each item f of SFd, a test \nis generated as a pair (s, .), where s is the sequence in Sp from which f was created, and . is the frequency \nnecessary to cause the desired message in s to drop. If there are no predicates that depend on the timing \nof the code itself, this set is output as the .nal test suite. In the case that the code does depend \non its own timing, such as with a timeout, the tests need to be run on the system under test to ensure \nthat the prescribed frequency doesn t cause the execution to follow an unintended path. Those that do \nnot exhibit the intended failure on the system under test are removed from the .nal test suite. 4.4 \nAssumption Relaxation While continuing to allow our system to function we can relax some of the assumptions \nwe made in Section 4.1. In particular, if we instead assume a Soft Real Time Guarantee (.xing an average \ndo, di, and dd, rather than a maximum), we can still expect messages to arrive at the same average frequency \nthey are published. We can also drop the Total Ordering constraint in favor of a Pair\u00adwise FIFO (messages \nbetween any given pair of components are .xed in order), as we are testing various interleavings of messages. \nRelaxing these assumptions may make failures non-deterministic in whether they appear on a given test \nor not, but the approach will still reveal them. 5. Implementation 5.3 Transforming ROS Code We have \nimplemented our approach in the context of ROS [20], Robotic Operating System. For the ROS code to be \nanalyzed by KLEE [4], the DSE framework under which we have implemented our system, we must transform \nthe code, to facilitate symbolic execution. Once this transformation is complete, the driver code with \nthe symbolic variables and sequence order is placed into the target node. After KLEE has been run on \nthis code, the generated tests are run on a separate modi.ed version of the code where the execution \nof callbacks is timed and messages are dropped. 5.1 ROS Robotic systems lend themselves to a publish/subscribe \narchitec\u00adture, as the individual components are naturally distributed, they may consume and produce multiple \nmessages, and may have inde\u00adpendent update rates. ROS is one of the frameworks available sup\u00adporting \nthis paradigm for robotic system development, providing the middleware to support message passing as \nwell as an extensive set of libraries to handle common tasks, and support for system deployment. ROS \nhas an active open source community and it pro\u00advides implementations of many full systems made for robots \nsuch as the iRobot Roomba [14] and the PR2 Robot [11] by Willow Garage, which we study in this work. \nROS structure differs slightly from the basic model presented in Section 3. In particular, ROS does not \nhave a dispatch queue. In its place a connection is created between a publisher and subscriber. The core \nof ROS handles the creation of these connections and therefore keeps an index of the current subscriptions \nand advertised publications. There is still a dd, but it is simply the transport time along the created \ncommunication channel. Table 3 describes ROS in terms of the dimensions listed in Section 3. These parameters \nfall within those required by our relaxed assumptions in Section 4.4. An application in ROS is organized \ninto a set of nodes. Each node can have several publishers and subscribers. Each publisher and subscriber \nwill be created with an associated message type and a topic name. For example, the velocity topic in \nthe system de.ned in Section 2 passes messages with a speed attribute. The topics are how ROS connects \na publisher and a subscriber together. A topic can have multiple publishers and subscribers. Subscribers \nare registered with a callback method which receives the message and processes it upon arrival. The length \nof queues are set manually and no guarantees are placed on the timing of messages being passed. If a \nnode has multiple subscribers, ROS gives the option of having the callbacks in separate threads or in \none thread, even though their incoming queues are separate.  5.2 KLEE KLEE is a Dynamic Symbolic Execution \nengine built on top of the compiler infrastructure LLVM [16]. LLVM, standing for Low-Level Virtual Machine, \nis a compiler infrastructure that allows the compilation of languages like C, C++, Objective-C, and Fortran \ninto assembly language operations. KLEE is run on this assembly code. By default, and in this work, KLEE \nuses a depth-limited, depth .rst search to identify paths through the input program. It .rst runs the \nprogram with an arbitrary input, recording encountered predi\u00adcates, then backtracks from the program \nexit to the last predicate before this and also above the depth limit. It then generates and optimizes \na set of constraints to negate that last predicate and runs it through a constraint solver, STP [10]. \nThe solution to this set of constraints is an input which follows the previously explored path down to \nthis last predicate, then follows the opposite branch. The search continues until all paths are covered. \nFor each path that is covered, KLEE outputs a set of values of the symbolic variables to follow that \npath. There are some limitations to using KLEE. In particular for our context, KLEE s solver does not \ncurrently support .oating point operations. Additionally, external methods not compiled using LLVM are \nnot analyzable by KLEE. These two limitations re\u00adquire the transformation of the input ROS code into \na form on which KLEE can be executed. The rules in Table 4 summarize the transformations we performed. \nCurrently these transformations are performed manually but systematically with the support of various \nscripts, and we plan to automate them in the future. As we are only interested in the subscription callbacks \nand their helper methods, we .rst discard any additional ROS speci.c struc\u00adtures which aid only in message \ntransport and topic registration. This includes the actual declarations of publishers and subscribers. \nThe message types are simpli.ed into basic objects with members for each piece of data they carry. This \ninvolves creating headers for these new messages to replace the ROS-generated ones. We also replace any \nwarning or error macros from ROS with assertions that KLEE can understand and target. Any remaining ROS \nmethod calls are treated as other external method calls. To handle such method calls, we revert to the \ncode as it was prior to processing with KLEE, and on that unprocessed code we record timing data of each \ncall and then replace the calls with mocking calls designed to keep the same average duration as the \ntimed calls. Floating point operations are dealt with by including a .xed point library[17]. Floating \npoint variables and constants are con\u00adverted to their .xed point counterparts and mathematical opera\u00adtions \nare rewritten using .xed point operations. This replacement adds complexity to the mathematical formulations \nand reduces pre\u00adcision, but allows KLEE to analyze this code. 5.4 Generating Sequences and Dropping \nMessages Following the process outlined in Section 4, the main method of the node is replaced by a driver \nfor KLEE. This driver creates symbolic variables and implements the driver in Listing 2 to handle various \ninterleavings of messages. For each message, it checks a symbolic variable to determine which callback \nto invoke. With this in place KLEE is invoked and a set of inputs (messages that match the callback topics) \ncorresponding to paths through the code is generated. Additionally, KLEE outputs whether an error was \nthrown given a particular input. After this, a second, separate version of the transformed code is created. \nThis second version is instrumented to drop messages and to time callback methods. The dropping of a \nmessage is handled by simply inserting a check before a callback is called. For each run of this code, \none message is dropped, so this check determines if a particular message is the one singled out to be \ndropped. If dropped, the callback is simply not executed. The remaining messages each have their callbacks \ntimed, and the cumulative value of these tim\u00adings up to the message being dropped is recorded as te. \nSince the set of symbolic variables, message contents and or\u00adderings, is the same as in the sequence \ngeneration phase, KLEE allows us to run the tests created in the context of the .rst version of the code \non the second version. Each test that did not throw an error in the sequence generation phase is run \nthrough this second version of the code to test the dropping of each message. Those executions that do \nthrow an error represent failures due to dropped messages. As described in detail in Section 6.4, we \nrecord average timings over multiple executions to mitigate the effects of any noise introduced by the \ninstrumentation. 5.5 Frequency Analysis and Output Presentation Once the exploration is complete, the \nthree outputs described in Section 4 are generated. The set of originally failing KLEE tests Parameter \nValue Description Message Reliability Present Noti.cations are guaranteed to eventually arrive at interested \nsubscribers. Message Ordering Pair-wise FIFO Noti.cations are delivered to a given subscriber in FIFO \norder with respect to publish operations from the same publisher. Filtering Precise Noti.cations are \nonly delivered for subscribed events. Real-Time Guarantees Soft RT On average, noti.cations are delivered \nin T time units after being published. Propagation Delay Absent Subscriptions are immediately active \nand deliver event noti.cations. Repliable Messages Absent Subscriptions set up to convey replies travel \nindependently of the original noti.cation. Message Priorities Absent All noti.cations are treated the \nsame way. Queue Drop Policy Head Drop Given queues of length L, messages exceeding this threshold cause \nthe oldest message to be dropped. Table 3. ROS quality of service parameters. Pattern Class Pattern \nInstance Transformation Includes Include: #include .* E Declarations PSDeclarations: ros:: ( NodeHandle \n| Subscriber | Publisher |...) .* Advertise: publisherID = nodeHandleID .advertise( .* ) Subscribe: subscriberID \n= nodeHandleID .subscribe( .* ) Publish: publisherID .publish( .* ) E E E E Callbacks/external methods \nCallback: callbackID (const msgType ConstPtr&#38; paramID ) ExternalMethod: (objectID . )?methodID ( \n(arg( , arg)*)? ) callbackID ( msgType paramID ) Mock ROS macros RosAssert: ( ROS WARN | ROS ERROR ) \n( .* ) RosDebug: ROS DEBUG( .* ) assert(false) E Types FloatT: double | .oat | ... FloatC: [0-9]+ . \n[0-9]+ .xedpt (.xedpt) FloatC Op . .xedpt Op FloatExpr: Expr Op (FloatC|FloatVar) Table 4. Transformation \nrules for modifying ROS code into code analyzable by KLEE. 1 // Inputs : 2 // n: number of callbacks \n3 // s: number of messages in sequence 4 // callback i (m): callback method for topic i 5 // m i[s]: \nsymbolic messages for topic i 6 ... 7 int idx [n] = { 0 ,0..0 } ; 8 int choose[s] = {symbolic };  9 \n... 10for(j = 1 to s){11 if(choose[j] == 0){12 callback 0(m 0[idx[0]); 13 idx [0]++; 14 }15 else if(choose[j] \n== 1){16 callback 1(m 1[idx[1]); 17 idx [1]++; 18 } 19 ... 20 else if(choose[j] == n){ 21 callback n(m \nn[idx[n]); 22 idx [n]++; 23 } 24 }  25 ...  Listing 2. Driver generated and explored to reveal interleavings \nof s messages on n topics. is kept and is available for further investigation. The maximum allowable \nfrequency and the set of generated tests are presented in a graph similar to Figure 2 in Section 2. The \nset of generated tests themselves are saved, and through the use of tools included with the distribution \nof ROS, can be instantiated and sent to the original system in ROS with the calculated frequency. Artifact \n# Subscriptions Assertions LOC CA 2 3 50 turtlesim 2 1 150 PR2 head 3 4 175 PR2 grip 3 6 250 Table 5. \nArtifacts 6. Assessment To evaluate our implementation, we studied its application to the four ROS artifacts \nfrom three systems characterized in Table 5. For each artifact, we counted the number of tests generated \nunder each stage of the approach, the number of tests that violated assertions, and for the test cases \nwhere dropped messages cause failures, we provide a characterization frequencies ranges that contribute \nto those failures. When dropped messages do not cause a failure, we assess whether the messages frequency \nis overly conservative. As a means of comparison, we implemented a random test case generation approach. \nThis approach will randomly select a message type (among the ones subscribed by the node under test), \nand then randomly select a value to match that type assuming a uniform distribution, repeating the process \nto create sequences of the same length as in our approach. For each artifact, we generated 100 megabytes \nof random tests comprising thousands of test sequences. To assess the second phase of our approach, a \nmessage is chosen at random and dropped if the original test passes. We report the results of this comparison \nin Table 6 and discuss these results in detail below. In the case of the PR2 controllers, we do not bother \nto compare the results against random testing, since neither technique found new failures due to dropped \nmessages. For each program, we generated sequences of eight messages, which we determined to be the number \nof messages needed for the Artifact/Technique S Sp Sf SPd SFd CA Our 9.1 5.1 3.9 3.6 1.6 CA Rand 678 \n13.2 676 11.9 1.6 turtlesim Our 14.4 12.8 1.6 9.3 3.5 turtlesim Rand 487 487 0.0 0 0 PR2 head Our 18.9 \n2.5 16.5 2.5 0 PR2 grip Our 24.7 2.7 22.0 2.7 0 Table 6. Summary of the generation of tests. Numbers \nlisted are in thousands of sequences. approach to run at least 24 hours on the smallest of the programs. \nThe bound of 24 hours was chosen arbitrarily as a bound that a practitioner might reasonably be expected \nto choose. 6.1 Collision Avoidance from iRobot This system, described in Section 2 and listed as CA in \nTable 6, does not use any .oating point arithmetic. As such, the only transformations performed on the \ncode are the removal of the ROS constructs and the simpli.cation of the message data structures. It subscribes \nto two types of messages, proximity to an obstacle, and desired velocity. The .rst two rows of Table \n6 summarize the sets of sequences generated through the process. After 24 hours, the approach gener\u00adated \n9068 tests. Of these tests, 3923 (43%) violated an assertion. Of the passing 5145 tests, 1566 (18%) violated \nan assertion when a message was dropped. Based on this information, the ap\u00adproach computed the message \nfrequencies necessary to cause such failures, which ranged between 4.29 Hz and 7.14 Hz. Figure 5(a) displays \nthe failure likelihood due to lost messages across the fre\u00adquencies depicted on the x-axis. Visible on \nthe graph are the model maximum frequency below which no messages will be dropped (3.6Hz) and the minimum \nfrequency which guarantees that even\u00adtually a message will be dropped (12.5Hz). Between those, the fre\u00adquencies \nof the tests which fail due to dropped messages are plot\u00adted. For this artifact, the 1566 tests in SFd \nonly reveal 14 distinct frequencies at which failures are found. Note that at 7.2Hz num\u00adber of tests \nfailing due to dropped messages is maximized, which is why the lines .attens out before reaching the \n12.5Hz frequency. Over 678,000 random tests, making up 100MB of test data, were generated to compare \nagainst the tests generated by our ap\u00adproach. This generation took just over one hour to complete. Of \nthese tests, 13292 (2.0%) passed with no message dropped. The 676,000 that fail simply trigger an assertion \nthat checks for prop\u00aderly formatted messages, which illustrates one of the weaknesses of random test \ncase generation. There were 1596 (0.02%) tests which failed when a message was dropped. Of those, 1470 \nrevealed the same failure, resulting from a trivial scenario consisting of two ve\u00adlocity messages arriving \nalmost simultaneously causing the .rst ve\u00adlocity message to be dropped, but requiring a frequency of \nover 100kHz, which is extremely unlikely to occur. Most interestingly, however, is that the frequencies \nassociated with the rest of the ran\u00addom tests were not smaller than 5.4Hz (1.1Hz higher than our ap\u00adproach), \nwhich indicates that random missed some problematic se\u00adquences that occur at low and relatively common \nfrequencies.  6.2 Turtlesim The second system used for evaluation is turtlesim. Turtlesim is a basic \nsimulator used as a tutorial for newcomers to ROS. It simulates a robot moving in a 2D space, and draws \nthis simulated robot moving around a virtual space. The node for the robot has two callbacks, one for \nsetting the velocity of the robot, and one for providing the dimensions of the space the robot is in \nand a time step for the robot to move.  ious frequencies for the Collision Avoidance and Turtlesim exam\u00adples. \nCollision Avoidance failures were detected between 4.3 and 7.2 Hz. Turtlesim failures were detected between \n2300 and 8100 Hz, and around 100 kHz. As summarized in rows 3 and 4 of Table 6, our implementation produced \n14436 tests in the span of 24 hours. Only 1640 (11%) failed on an assertion, leaving 12796 passing tests. \nOf those passing tests, 3521 (25%) failed when a message was dropped. Figure 5(b) shows the results derived \nfrom those failed tests. Below 1136 Hz, no messages can be dropped, and the frequency would have to be \nabove 100k Hz in order to guarantee that messages would eventually be dropped. However, our approach \nfound failures in sequences of 8 or less messages with a frequency as low as 2300 Hz. For the random \ntest suite of 480000 tests, only three failed on an assertion, and of the passing tests, none failed \nafter a message was dropped. The poor performance of the randomly generated suite is due to two factors. \nFirst, the velocity of the simulated robot and the dimensions of the space it resides in are passed in \nmessages, which results in a large potential variety of messages that could arrive. Second, to make the \nassertion fail, the message values of velocity and the wall location must be coupled to put the robot \nnear the wall, which is a rare combination. The chances of random test generation to produce the values \nnecessary to violate that assertion in such a large space are remote. On the other hand, dynamic symbolic \nexecution approaches thrive in these situations.  6.3 PR2 Controllers The .nal assessment was done on \na pair of controllers for the PR2 Robot [11]. The controllers in question are for the head and the grippers. \nThe head controller directs actuators which pan and tilt the head of the robot. The gripper controller \nopens and closes a gripper and also enforces a maximum force to exert with the gripper if it is obstructed. \nBoth of these controllers operate in the same fashion. They have three subscriptions, one for a goal \nstate which starts the controller, one to cancel the current trajectory, and one which receives the current \nstate and checks the progress towards the goal state. The third of these messages is set to arrive at \na .xed rate of 100 Hz. For the head controller, our approach generated 18937 tests, and for the gripper \ncontroller, it generated 24718 tests. Of these gener\u00adated tests, 16485 (87%) failed for the head controller, \nand 22041 (89%) failed for the gripper controller. All of these failures, how\u00adever, corresponded to the \nincorrect formation of the input messages. Dropping messages did not reveal new failures. Upon further \nin\u00advestigation we found the explanation: the assertions in these con\u00adtrollers only serve to validate \nthe input messages, and do not em\u00adploy the state set up by previous messages. Since dropping messages \ndoes not cause a failure, the ap\u00adproach reverts to a more strict criterion that no messages should be \ndropped. Utilizing the maximum timings for the callbacks, the tool reports that the head controller s \nupdate goal callback can op\u00aderate at up to 12000Hz with no messages dropped, the cancel call\u00adback can \noperate at over 100 kHz with no messages dropped, and the update callback can also run at over 50 kHz \nwith no messages dropped. For the gripper controller, all three callbacks can be run at over 100 kHz \nwithout messages being dropped. This means that for both controllers, the 100 Hz rate at which the update \nis being called could be increased greatly without causing messages to be dropped. Increasing this rate \ncould lead to more precise movement and quicker reactions to unexpected events.  6.4 Discussion The \nthree studies showed the viability of the approach and provide different insights about the artifacts \nthat illustrate the potential of the approach. For the .rst artifact, the systematic exploration of the \nspace of messages performed by our approach reveals slightly more than random in that the lower frequency \ntests that lead to a failure are identi.ed, but the overall performance gains are questionable given \nthe cost of our approach. For the second artifact the gains are more evident as the space de.ned by the \nmessages and the an\u00adalyzed nodes is complex enough that our approach provides failing sequences with \ndropped messages while random tests cannot. Last, the study of the PR2 components shows a different usage \nof the information our approach collects to identify overly conservative message frequency settings. \nIt is apparent that the nature of the faults and of the assertions as\u00adsociated with their detection affects \nthe ability of both our approach and random testing. If a fault and the assertion that can reveal it \nhas a very limited range of inputs for which it can be identi.ed and violated, random testing will struggle. \nOur approach, however, is perfectly suited to this sort of setting. If the fault is common to many message \nsequences, using a technique such as ours adds un\u00adnecessary overhead. Clearly approaches that combine \nboth (e.g., DART [13]) would seem bene.cial. Similarly, the performance of the procedures to explore \nmessage frequencies and their effects on dropping messages may have been underestimated as the nodes \nbe\u00adhaviors were sometimes affected but this was not detected by the available assertions before the tests \nwere completed. Although effective in this preliminary assessment, the approach is neither precise nor \ncomplete. The frequency .ndings can be sus\u00adceptible to the noise introduced by the instrumentation to \ncollect timing data. We mitigate this issue by running the generated tests on the uninstrumented code; \nstill, tests that cause a failure due to dropped messages and are close to the safe frequency bound may \nreport false positives, and similarly with false negatives. The ap\u00adproach is also bound by the standard \nlimitations of approaches that utilize solvers to generate inputs based on collected path condi\u00adtions. \nIn our setting of robotic systems, for example, the inability of the solver to handle .oating point arithmetic \nlimits the number of systems that can be analyzed without resorting to .xed point arithmetic, which may \nobscure some of the program paths. Last, the approach targets the one node under test without taking \ninto consideration its context. As such, it may report false positives. For example, the frequency needed \nto cause a message to be dropped may not be possible given the nodes that create those messages. The \nsame can be said for the content of the messages. As mentioned, we apply our approach only on individual \nROS nodes. The nodes we studied are of a size similar to the nodes of other systems present in ROS and \ndescribed in the literature, so the scalability of our approach at the node level is not a concern. But \nit remains to be seen how scalable the approach might be if we were to try to generalize it to analyze \nmessage exchanges between multiple nodes. Related to the issue of scalability is the length of the message \nsequences used. As described earlier, we used sequences of eight messages to ensure that the approach \nwould run at least 24 hours on all programs. In general, it would be useful to study a range of sequence \nlengths in order to obtain a better understanding of the cost-effectiveness of the approach. 7. Related \nWork Much of the work in testing distributed systems focuses on sup\u00adporting a distributed test architecture, \nwhere an individual tester is placed at each I/O port of the implementation under test. Two key aspects \nof this line of work are the problems of controllability, forc\u00ading inputs to be received in the correct \norder, and observability, de\u00adtermining which input caused a particular output [15]. Of particular interest \nare the conditions necessary to resolve these problems con\u00adsidering whether the testers are coordinated \nor not [15], or whether they are synchronized through messages [6, 7]. The centralized ap\u00adproach given \nin this paper can be treated as a special case of the coordinated method presented in [15]. Another related \nline of work focuses on model-based test gen\u00aderation where the models attempt to capture some of the \ncritical as\u00adpects of the distributed system. For example, timed automata mod\u00adels have been used to generate \ninputs and then check whether the system under test conforms to the timing speci.cations [5]. Simi\u00adlarly, \ndiscrete-event simulation models of distributed systems have been used as way to de.ne test adequacy \ncriterion for distributed systems suites [22]. The MoDist [32] model checker veri.es dis\u00adtributed systems \nby capturing all actions in the distributed sys\u00adtem under test and exploring those actions to reveal \nfailures. More speci.cally in the context of publish/subscribe systems, Michlmayr et al. [18, 19] created \na framework to facilitate the generation of test oracles based on linear temporal logic properties imposed \non the system. Baresi et al. [2, 3] also developed a framework for ver\u00adifying speci.c properties of publish/subscribe \nsystems through the introduction of richer primitives into model checkers to bring them closer to the \nnecessities of these systems. Closer to out work, Sasnauskas et al. also applies dynamic sym\u00adbolic execution \nto distributed systems focusing on eliminating re\u00addundant states that arise in the representation of \ndistributed exe\u00adcutions [24]. In contrast, the focus of our work is to test individ\u00adual nodes of distributed \nsystems in the presence of the kinds of communication anomalies that arise in such systems. In other \nwork they present KleeNet, a debugging environment for detecting fail\u00adures that arise due to phenomena \nthat arise nondeterministically in wireless sensor networks, such as node failures, lost messages, duplicated \nmessages and corrupted messages [23]. In contrast, in our work we implicitly trust the reliability of \nthe communication medium and instead identify failures that arise due to the sequenc\u00ading and timing of \nmessages and the need to drop messages when message queues are full. There are several efforts oriented \ntowards aiding debugging of distributing systems through continuous monitoring. These tech\u00adniques and \ntools aim to facilitate the logging and comparison of behavior and consumed resources against speci.cations \nor previ\u00adous behavior, and the replay of previously monitored execution [12, 21, 29]. Concurrency validation \nand veri.cation is an area closely re\u00adlated to that of distributed testing, as a distributed system can \nbe considered a concurrent system. Testing these systems requires the exploration of both the paths to \nthe system and their potential in\u00adterleavings due to concurrently running processes [30]. Tools like \ndBug [28], Eraser[25], and RacerX[8], for example, are built to identify concurrency faults in systems \nby examining the interleav\u00adings of concurrent events. The dif.culty of .nding such faults par\u00adallels \nthe dif.culty of .nding faults under message passing, as not only the input data matters, but also the \norder of occurrence. As mentioned earlier, the order of occurrence can also be sys\u00adtematically explored \nwith a stateless model checker, which searches over the space of interleavings of concurrency events \n[31]. Since the reported faults may not be realizable in practice, signi.cant ef\u00adfort is invested in \ndiscriminating the real ones, often through some form of dynamic analysis [26]. Also, specialized coverage \nmetrics for the test suites of concurrent systems have been developed using the notion of coverage saturation \nsince accounting the total num\u00adbers of paths and schedules is impractical [27]. We note that in order \nfor concurrency failures to occur, however, there must be shared memory accessed in a problematic way. \nIn a message passing framework, such as a publish/subscribe one, this type of failure does not occur \nbetween components passing messages. Instead the communicated data is copied and transported by the middleware \nwith no shared memory between the publisher and the subscriber. 8. Conclusion and Future Work We introduced \nan approach for testing distributed systems with re\u00adspect to the arrival sequence and frequency of messages. \nUsing dynamic symbolic execution in conjunction with a driver to sim\u00adulate various message orderings, \nour approach generates message sequences to test a node in a distributed system. By analyzing and timing \nthese sequences, our approach creates tests which reveal faults due to message sequences and dropped \nmessages, and char\u00adacterizes the callbacks in a node to determine what frequencies the node could tolerate \nwithout risking such faults. We implemented the approach in the context of ROS, and evaluated it on three \nsys\u00adtems. On two of the systems it produced a large number of test cases which exhibit failures due to \ndropped messages. On the third system, no such failures are found, but the de.ned message input frequency \nis found to be overly conservative. We compared the ap\u00adproach against randomly generated tests, and identi.ed \nfactors and settings where the proposed approach would be particularly advan\u00adtageous. Next, we would \nlike to expand the scope of our approach to multiple connected nodes. In addition to ensuring generating \nmore realistic messages along internal connections between nodes, such expansion would also provide more \ninsight into the frequencies at which other nodes will actually generate messages. Considering multiple \nnodes, however, not only increases the state space DSE needs to explore, but also introduces the potential \nfor loops between nodes, greatly increasing the overall complexity. To expand the applicability to other \npublish/subscribe middleware systems, we also plan to relax some of the assumptions we set. In particular, \nprioritized messages add an extra layer of complexity to both the message sequence generation and to \nthe handling of a .lled queue. Similarly, allowing for messages to be lost during communication would \nallow the same failures targeted in this work to occur at low frequencies, but these occurrences could \nnot be controlled. Finally, we are in the process to complete the automation of the transformation phases \nof this approach to make the process more easily applicable by the whole ROS community. Acknowledgments \nThis material is based in part upon work supported by AFOSR\u00ad9550-09-1-0687 and EOARD-FA8655-10-1-3007. \nAny opinions, .ndings, and conclusions or recommendations expressed in this material are those of the \nauthor(s) and do not necessarily re.ect the views of AFOSR or EOARD. References [1] L. Baresi, C. Ghezzi, \nand L. Mottola. Towards .ne-grained automated veri.cation of publish-subscribe architectures. In Formal \nTechniques for Networked and Distributed Systems, pages 131 135. Springer- Verlag, 2006. [2] L. Baresi, \nC. Ghezzi, and L. Mottola. On accurate automatic veri.\u00adcation of publish-subscribe architectures. In \nthe International Confer\u00adence on Software Engineering, pages 199 208, 2007. [3] L. Baresi, C. Ghezzi, \nand L. Mottola. Loupe: Verifying publish\u00adsubscribe architectures with a magnifying lens. IEEE Transactions \non Software Engineering, 37(2):228 246, March-April 2011. [4] C. Cadar, D. Dunbar, and D. Engler. KLEE: \nunassisted and automatic generation of high-coverage tests for complex systems programs. In the USENIX \nConference on Operating Systems Design and Implemen\u00adtation, pages 209 224, 2008. [5] R. Cardell-Oliver. \nConformance test experiments for distributed real\u00adtime systems. In the ACM SIGSOFT international Symposium \non Software testing and Analysis, pages 159 163, 2002. [6] J. Chen, R. Hierons, and H. Ural. Conditions \nfor resolving observ\u00adability problems in distributed testing. In Formal Techniques for Net\u00adworked and \nDistributed Systems FORTE 2004, volume 3235, pages 229 242. Springer Berlin / Heidelberg, 2004. [7] \nJ. Chen, R. M. Hierons, and H. Ural. Overcoming observability problems in distributed test architectures. \nInf. Process. Lett., 98(5): 177 182, June 2006. [8] D. Engler and K. Ashcraft. RacerX: effective, static \ndetection of race conditions and deadlocks. In the Symposium on Operating systems principles, pages 237 \n252, 2003. [9] P. T. Eugster, P. A. Felber, R. Guerraoui, and A. M. Kermarrec. The many faces of publish/subscribe. \nACM Computing Surveys, 35:114 131, 2003.  [10] V. Ganesh and D. L. Dill. A decision procedure for bit-vectors \nand arrays. In the Computer Aided Veri.cation Conference, pages 524 536, 2007. [11] W. Garage, 2012. \nURL http://www.willowgarage.com/pages/pr2. [12] D. Geels, G. Altekar, P. Maniatis, T. Roscoe, and I. \nStoica. Friday: Global comprehension for distributed replay. In the Symposium on Networked Systems Design \nand Implementation, page 21. [13] P. Godefroid, N. Klarlund, and K. Sen. DART: directed automated random \ntesting. In the ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 213 223, \n2005. [14] IRobot, 2012. URL http://www.irobot.com/company. [15] A. Khoumsi. A temporal approach for \ntesting distributed systems. IEEE Transactions on Software Engineering, 28:1085 1103, Novem\u00adber 2002. \n [16] C. Lattner and V. Adve. LLVM: A Compilation Framework for Lifelong Program Analysis &#38; Transformation. \nIn the International Symposium on Code Generation and Optimization, March 2004. [17] G. T. Leavens. The \njava modeling language(jml). URL http://sourceforge.net/apps/wordpress/fixedptc/. [18] A. Michlmayr, \nP. Fenkam, and S. Dustdar. Architecting a testing framework for publish/subscribe applications. In the \nInternational Computer Software and Applications Conference, pages 467 474, 2006. [19] A. Michlmayr, \nP. Fenkam, and S. Dustdar. Speci.cation-based unit testing of publish/subscribe applications. In the \nIEEE International ConferenceWorkshops on Distributed Computing Systems, pages 34 , 2006. [20] M. Quigley, \nK. Conley, B. P. Gerkey, J. Faust, T. Foote, J. Leibs, R. Wheeler, and A. Y. Ng. Ros: an open-source \nrobot operating system. In International Conference on Robotics and Automation Workshop on Open Source \nSoftware, 2009. [21] P. Reynolds, C. Killian, J. L. Wiener, J. C. Mogul, M. A. Shah, and A. Vahdat. Pip: \ndetecting the unexpected in distributed systems. In the Conference on Networked Systems Design &#38; \nImplementation, pages 115 128, 2006. [22] M. J. Rutherford, A. Carzaniga, and A. L. Wolf. Simulation-based \ntest adequacy criteria for distributed systems. In the ACM SIGSOFT inter\u00adnational Symposiumon Foundations \nof Software Engineering, pages 231 241, 2006. [23] R. Sasnauskas, O. Landsiedel, M. H. Alizai, C. Weise, \nS. Kowalewski, and K. Wehrle. Kleenet: discovering insidious interaction bugs in wireless sensor networks \nbefore deployment. In International Confer\u00adence on Information Processing in Sensor Networks, pages 186 \n196, 2010. [24] R. Sasnauskas, O. S. Dustmann, B. L. Kaminski, K. Wehrle, C. Weise, and S. Kowalewski. \nScalable symbolic execution of distributed sys\u00ad tems. In International Conference on Distributed Computing \nSystems, pages 333 342, 2011. [25] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. Anderson. \nEraser: a dynamic data race detector for multithreaded programs. ACM Trans. Comput. Syst., 15(4):391 \n411, Nov. 1997. [26] K. Sen. Race directed random testing of concurrent programs. In the Conference on \nProgramming Language Design and Implementation, pages 11 21, 2008. [27] E. Sherman, M. B. Dwyer, and \nS. Elbaum. Saturation-based testing of concurrent programs. In the European Software Engineering confer\u00adence \nand the ACM SIGSOFT Symposium on the Foundations of Soft\u00adware Engineering, pages 53 62, 2009. [28] J. \nSimsa, R. Bryant, and G. Gibson. dBug: systematic evaluation of distributed systems. In the international \nconference on Systems software veri.cation, pages 3 3, 2010. [29] A. Singh, P. Maniatis, T. Roscoe, and \nP. Druschel. Using queries for distributed monitoring and forensics. In the ACM SIGOPS/EuroSys European \nConference on Computer Systems 2006, pages 389 402, 2006. [30] R. Taylor, D. Levine, and C. Kelly. Structural \ntesting of concurrent programs. IEEE Transactions on Software Engineering, 18:206 215, 1992. [31] C. \nWang, M. Said, and A. Gupta. Coverage guided systematic concur\u00adrency testing. In the International Conference \non Software Engineer\u00ading, pages 221 230, 2011. [32] J. Yang, T. Chen, M. Wu, Z. Xu, X. Liu, H. Lin, M. \nYang, F. Long, L. Zhang, and L. Zhou. MODIST: transparent model checking of unmodi.ed distributed systems. \nIn the USENIX Symposium on Net\u00adworked Systems Design and Implementation, pages 213 228, 2009.    \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Testing the components of a distributed system is challenging as it requires consideration of not just the state of a component, but also the sequence of messages it may receive from the rest of the system or the environment. Such messages may vary in type and content, and more particularly, in the frequency at which they are generated. All of these factors, in the right combination, may lead to faulty behavior. In this paper we present an approach to address these challenges by systematically analyzing a component in a distributed system to identify specific message sequences and frequencies at which a failure can occur. At the core of the analysis is the generation of a test driver that defines the space of message sequences to be generated, the exploration of that space through the use of dynamic symbolic execution, and the timing and analysis of the generated tests to identify problematic frequencies. We implemented our approach in the context of the popular Robotic Operating System and investigated its application to three systems of increasing complexity.</p>", "authors": [{"name": "Charles Lucas", "author_profile_id": "81548896756", "affiliation": "University of Nebraska - Lincoln, Lincoln, NE, USA", "person_id": "P3856207", "email_address": "clucas@cse.unl.edu", "orcid_id": ""}, {"name": "Sebastian Elbaum", "author_profile_id": "81100007035", "affiliation": "University of Nebraska - Lincoln, Lincoln, NE, USA", "person_id": "P3856208", "email_address": "elbaum@cse.unl.edu", "orcid_id": ""}, {"name": "David S. Rosenblum", "author_profile_id": "81100493122", "affiliation": "National University of Singapore, Singapore, Singapore", "person_id": "P3856209", "email_address": "david@comp.nus.edu.sg", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384683", "year": "2012", "article_id": "2384683", "conference": "OOPSLA", "title": "Detecting problematic message sequences and frequencies in distributed systems", "url": "http://dl.acm.org/citation.cfm?id=2384683"}