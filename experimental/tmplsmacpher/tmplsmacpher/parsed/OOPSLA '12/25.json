{"article_publication_date": "10-19-2012", "fulltext": "\n LEAN: Simplifying Concurrency Bug Reproduction via Replay-supported Execution Reduction Jeff Huang Charles \nZhang Department of Computer Science and Engineering The Hong Kong University of Science and Technology \n{smhuang, charlesz}@cse.ust.hk Abstract Debugging concurrent programs is known to be dif.cult due to \nscheduling non-determinism. The technique of multipro\u00adcessor deterministic replay substantially assists \ndebugging by making the program execution reproducible. However, facing the huge replay traces and long \nreplay time, the de\u00adbugging task remains stunningly challenging for long run\u00adning executions. We present \na new technique, LEAN, on top of replay, that signi.cantly reduces the complexity of the replay trace \nand the length of the replay time without losing the determin\u00adism in reproducing concurrency bugs. The \ncornerstone of our work is a redundancy criterion that characterizes the re\u00addundant computation in a \nbuggy trace. Based on the redun\u00addancy criterion, we have developed two novel techniques to automatically \nidentify and remove redundant threads and in\u00adstructions in the bug reproduction execution. Our evaluation \nresults with several real world concur\u00adrency bugs in large complex server programs demonstrate that LEAN \nis able to reduce the size, the number of threads, and the number of thread context switches of the replay \ntrace by orders of magnitude, and accordingly greatly shorten the replay time. Categories and Subject \nDescriptors D.2.5 [Software En\u00adgineering]: Testing and Debugging Debugging aids; Trac\u00ading; Diagnostics \nKeywords Concurrecy Defect, Execution Reduction, Re\u00adplay Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, \nUSA. Copyright c @ 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 1. Introduction 1.1 Motivation Bug \nreproduction is of critical importance for software de\u00adbugging. Yet, reproducing concurrency bugs is \nknown to be dif.cult due to scheduling non-determinism. The technique of multiprocessor deterministic \nreplay (MDR) is able to fully reproduce previous executions, attracting a signi.cant re\u00adsearch attention \nfor debugging concurrent programs in the multicore era [1, 5, 8, 9, 13, 17, 18, 22, 30]. As MDR aims \nat faithfully reenacting an earlier execution, the core focus throughout the decades of research has \nbeen on reducing the runtime recording overhead while preserving the determin\u00adism. Several recent work \n[8, 9, 17, 30] demonstrates that the future of low overhead MDR is positive, via special hard\u00adware design \n[8, 17] or even clever software-level approaches [9, 30]. However, we argue that MDR alone is not often \nsuf.cient for debugging. Even with a zero-recording-overhead MDR support, the debugging task can remain \nstunningly challeng\u00ading for concurrent programs. We identify two main reasons. First, most real world \nconcurrent applications are large and complex. For any non-trivial real execution, the execution trace \ncould be huge and complicated, containing millions (or even billions) of critical events [27] and hundreds \nof thou\u00adsands of thread context switches [10, 11]. Facing the ocean of shared memory dependencies and \nthread interleavings, it is very hard for programmers to locate the bug by inspect\u00ading the huge amount \nof trace information, never to mention the space needs for storing the trace. Moreover, the perfor\u00admance \nof replay is often slow and hard to predict. As replay typically requires enforcing the scheduling behavior, \nit is of\u00adten signi.cantly slower (5x-39000x [1, 22]) than the native execution. For long running executions, \nthe replaying phase may never end within a bounded time budget. It is very frus\u00adtrating for programmers \nto wait but without knowing when the bug will be reproduced. To make MDR more practical for supporting \nconcurrent program debugging, we advocate the simpli.cation of the re\u00adplay execution and the speeding-up \nof the replaying process,  for j =1:M expected = account.get()+i account.increment(i) T0 Ti A: assert \naccount.get()==expected expected=account.get()-i account.decrease(i) B: assert account.get()==expected \nFigure 1. A typical test case for stressing testing an account function. A signi.cant amount of computation \nin a buggy execution of this program may be redundant. so that programmers can locate and understand \nconcurrency bugs more effectively using a simpli.ed reproducible buggy execution. To achieve this goal, \nwe propose LEAN, a con\u00adcurrency bug reproduction technique on top of MDR, that signi.cantly reduces the \ncomplexity (size, threads, and con\u00adtext switches) of the replay trace and shortens the replay time without \nlosing the determinism.  1.2 Key Observation Our key observation is that most computations in the buggy \nexecution are often redundant for reproducing the concur\u00adrency bug. As shown by Vaziri, Tip and Dolby \n[29], most concurrency bugs are exhibited by only two threads and one or two shared variables. The rest \nof the threads and shared variable accesses, if not pre-requisite to understand the bug, are redundant \nand can be removed from the execution. This observation is also empirically con.rmed by a comprehen\u00adsive \nstudy by Lu et al. [15] on real world concurrency bugs showing that the manifestation of more than 96% \nof the ex\u00adamined concurrency bugs involves no more than two threads, 66% of the non-deadlock concurrency \nbugs involve only one variable, and 97% of the deadlock concurrency bugs in\u00advolve at most two resources. \nThis observation also re.ects the common wisdom demonstrated by years of industrial experience (IBM ConTest \n[6], Stress testing [19] and Mi\u00adcrosoft Chess [20]) that most concurrency bugs in practice are triggered \nby a few threads and a small number of context switches. For example, the stress testing for exposing \ncon\u00adcurrency bugs typically forks as many threads as possible to repeatedly execute the same code. However, \nwith correct in\u00adterleaving, a few threads and repetitions are often suf.cient to trigger the bug. To \nfurther elucidate this observation, consider a simple, but common, test case for stress testing an account \nfunc\u00adtion in Figure 1. The parent thread T0 forks a number of (N) children threads Ti (i =1, 2,...,N), \neach of which re\u00adpeatedly validates two account functions a number of (M) times: increasing and decreasing \nthe account by a certain amount (i). There are three assertions (A, B, C) in the pro\u00adgram for the checking \nof correctness. When an assertion is violated, in the worst case, the buggy execution trace con\u00adtains \nM threads (excluding T0) and M \u00d7 N iterations of in\u00adcreasing/decreasing operations on the account. However, \nin the best case, only two threads and two iterations are able to reproduce the bug. For instance, the \naccount increasing func\u00adtion maybe non-atomic, and an erroneous interleaving hap\u00adpened between the 5th \nand 10th iterations of threads T(2,3), causing assertion A to be violated. To reproduce the error, the \n5th and 10th iterations of threads T(2,3) (plus the erro\u00adneous interleaving) are suf.cient. The rest \nof the computa\u00adtion is redundant and can be eliminated from the execution without affecting the bug reproduction. \n 1.3 Contributions We propose a redundancy criterion to characterize the re\u00addundant computation in a \nbuggy trace. The criterion ensures that, after removing a redundant computation, the resultant execution \nis able to reproduce the same concurrency bug, which preserves the debugging information but easier to \nrea\u00adson about as the amount of computation is greatly reduced. Based on the criterion, LEAN simpli.es \nthe buggy execu\u00adtion by iteratively identifying and removing the redundant computation from the original \nexecution trace (skipping the computation by controlling the execution) and, at the same time, enforcing \nthe same schedule between threads in the re\u00adduced execution as that in the original buggy execution. \nThe .nal result produced by LEAN is a simpli.ed execution with all the redundant computation removed. \nThe key challenge we address is how to effectively iden\u00adtify the redundant computation. We further categorize \nthe re\u00addundant computation into two dimensions: the whole-thread redundancy and the partial-thread redundancy. \nThe whole\u00adthread redundancy characterizes redundant threads of which the entire computation is redundant. \nFor example, all the other threads except T(0,2,3) in our illustrating example are redundant threads \nand all their computation can be removed. The partial-thread redundancy characterizes the more .ne\u00adgrained \nredundant instructions as part of each individual thread. For example, all the other iterations (except \nthe 5th and 10th) of threads T(2,3) in our illustrating example belong to the partial-thread redundancy. \nWe develop two effective techniques based on delta\u00addebugging [38] to identify the whole-thread redundancy \nand the partial-thread redundancy, respectively. To reduce the search space of delta-debugging, we utilize \nthe parent\u00adchildren relationship between threads to iteratively identify the whole-thread redundancy \nusing the dynamic thread hi\u00aderarchy graph. For the partial-thread redundancy, as it is ineffective to \nenumerate every combination of the instruc\u00adtions for each thread, we combine an adapted multithreaded \nprogram slicing technique [26] and a repetition analysis to remove irrelevant instructions and to identify \nthe redun\u00addant iterations of computation. To further improve the effec\u00adtiveness, we also provide an easy-to-use \nrepetition analysis framework that allows the programmers to annotate repeti\u00ad  tive code segments of \nwhich some execution iterations are 2.1 Concurrent Program Execution Modeling potentially redundant. \nAll those redundant iterations are then automatically validated and .ltered out by our technique. Note \nthat the redundancy criterion is black-box in na\u00adture. It does not rely on any data or control dependency \ninformation of the program, and is completely based on the bug reproduction property. This allows us \nto explore more simpli.cation opportunities than those white-box ap\u00adproaches such as program slicing \n[7, 12, 21] that rely on the program/system dependence graphs for removing the state\u00adments that are irrelevant \nto the fault. We have implemented LEAN on top of our replay sys\u00adtem LEAP [9] for Java programs. Our evaluation \nresults on a set of real concurrency bugs in popular multithreaded benchmarks as well as several large \ncomplex concurrent systems demonstrate that LEAN is able to signi.cantly re\u00adduce the complexity of the \nreproducible buggy execution and shorten the replay time without losing the determinism. LEAN produces \na simpli.ed execution typically within 20 iterations that deterministically reproduces the concurrency \nbug. LEAN is able to reduce the size of the replay trace by as large as 324x, the number of threads and \nthread context switches by 99.3% and 99.6%, and shorten the replay time by more than 300x. We believe \nLEAN is able to signi.cantly improve the debugging effectiveness of MDR for reproduc\u00ading concurrency \nbugs and to reduce the debugging effort for concurrent programs. We highlight our contributions as follows: \n1. We present a trace redundancy criterion and charac\u00adterize two dimensions of redundancy for concurrency \nbug reproduction with the MDR support. 2. We present a technique and the design of a tool proto\u00adtype \nto automatically simplify the buggy trace by removing the two dimensions of trace redundancy. 3. We \nevaluate our technique on a set of real concurrency bugs in several large complex Java programs and the \nresults demonstrate the effectiveness of our technique.  The remainder of this paper is organized as \nfollows: Sec\u00adtion 2 presents the fundamentals of our trace redundancy cri\u00adterion; Section 3 presents \nour technique; Section 4 presents our implementation and Section 5 presents a case study of simplifying \nthe reproduction of a real concurrency bug; Sec\u00adtion 6 reports our experimental results; Section 7 discusses \nrelated work and Section 8 concludes this paper.  2. Fundamentals The cornerstone of this work is a \nredundancy criterion for characterizing the redundant computation in a buggy multi\u00adthreaded execution \nw.r.t. to a concurrency bug. We .rst in\u00adtroduce the common program modeling used by DMR tech\u00adniques to \nsupport concurrency bug reproduction and then present our redundancy criterion based on the model. Debugging \nis often based on a reproducible buggy execu\u00adtion trace. A trace captures a concurrent program execution \nas a sequence of events d = (ei) that contains all the critical computation (i.e., thread synchronizations \nand the read/write accesses to the shared variables), in their execution order. During execution, these \nevents are monitored by MDR tech\u00adniques to support the deterministic replay. More formally, an event \ne can be of the following forms [24]: FORK(s,tp,tc) denotes a thread tp starts a thread tc while executing \nthe statement s;  JOIN(s,tp,tc) denotes a thread tp waits for the termina\u00adtion of a thread tc while \nexecuting the statement s;  READ(s,v,t) denotes that thread t reads a shared variable v while executing \nthe statement s;  WRITE(s,v,t) denotes that thread t writes to a shared variable v while executing the \nstatement s;  ACQ(s,l,t) denotes the acquisition of a lock l by thread t while executing the statement \ns;  REL(s,l,t) denotes the releasing of a lock l by thread t while executing the statement s;  SND(s,g,t) \ndenotes the sending of a message with unique ID g by thread t while executing the statement s;  RCV(s,g,t) \ndenotes the reception of a message with unique ID g by thread t while executing statement s.  As threads \nexecute concurrently, events by different threads may interleave. A preemptive interleaving occurs if \ntwo successive events from the same thread in the trace are interleaved by events from other threads, \nbut they could have been executed continuously without the interleaving. Preemptive interleaving is non-deterministic, \nbecause it de\u00adpends on the behavior of the thread scheduler and the timing variations between threads \n[22]. And because of the non\u00addeterminism, preemptive interleavings are the root cause of many concurrency \nbugs. The schedule of an execution is the projection of the trace on the thread ID, which captures all \nthe preemptive inter\u00adleavings in the trace. If a schedule contains no preemptive interleaving, we say \nit is sequential and, otherwise, non\u00adsequential. Since the schedule contains the execution order information \nof all the critical computation across threads, it can be used to deterministically reproduce the program \nexe\u00adcution [2]. Starting with an initial state S0 and, following a schedule ., the program can reach \na .nal state Sf . We say . exhibits a bug if Sf satis.es a predicate, say f, that denotes the bug. The \nbug predicate is de.ned as follows: DEFINITION 1. (Bug predicate) A bug predicate, f, char\u00adacterizes \nthe exhibition of a bug in the program execution over the .nal program state. The bug is exhibited in \nthe exe\u00adcution iff f(Sf ) is evaluated to be true.  Following different schedules, however, Sf may be \ndif\u00adferent and may or may not satisfy f. We call the bug a se\u00adquential bug if any sequential schedule \nis able to exhibit it, and a concurrency bug if only a non-sequential schedule can exhibit it. To reproduce \na concurrency bug, essentially, a MDR technique captures or computes the schedule . in the buggy run \nand then enforces the same . in all replay runs to exhibit the bug. 2.2 A Model of Trace Redundancy From \na high level view, LEAN simpli.es the concurrency bug reproduction by controlling the program execution \nto skip instructions in the program that are redundant to re\u00adproducing the bug. Generally speaking, an \ninstruction (or a group of instructions) as a part of the program execution cannot be arbitrarily skipped, \nas it may result in two pos\u00adsible negative consequences: the program malfunctions, or the bug disappears. \nThe program might malfunction if the skipped instruction is an indispensable part of the program logic, \nwhile the bug might disappear if the skipped instruc\u00adtion is involved in the buggy interleavings that \nare related to the bug. Either consequence will make the reduced execu\u00adtion not useful for debugging. \nWe propose a redundancy criterion for the concurrency bug reproduction that ensures none of the two consequences \nwill happen if a redundant instruction is skipped. The ba\u00adsic idea is that, after removing the redundancy, \nthe reduced execution is still suf.cient for understanding the bug, i.e., the same bug is reproduced. \nA subtle problem in de.ning the criterion is that, in practice, we may not have such a bug predicate \nf as that de.ned in De.nition 1. In practice, we of\u00adten use assertions or rely on runtime exceptions \nto determine whether a bug is exhibited or not. However, the assertions or exceptions may be insuf.cient \nto distinguish between the be\u00adhavior of the bug manifestation and the behavior of program malfunction, \nin which case the program is no longer work\u00ading properly as expected due to the removal of a necessary \ninstruction. For example, the assertion that characterizes the bug in the original execution may always \nbe violated after removing a certain instruction. Although the reduced execu\u00adtion manifests the violation \nof the assertion, it is not useful for debugging because the assertion is not able to character\u00adize the \nsame bug as that in the original execution. We tackle this issue from the perspective of thread inter\u00adleavings. \nFor a concurrency bug, essentially, it is some non\u00addeterministic buggy interleavings that cause the bug \n(assum\u00ading the input is deterministic). For debugging, programmers want to understand how the bug occurs \nwith these buggy interleavings. If the program executes sequentially and be\u00adhaves correctly, the bug \nshould not manifest. On the other hand, if the program malfunctions after removing an instruc\u00adtion, either \nthe program cannot proceed to execute the buggy statement or the bug predicate f is always satis.ed regard\u00adless \nof the buggy interleavings. Therefore, we de.ne the re\u00addundancy criterion as follows: DEFINITION 2. (Trace \nredundancy criterion) Consider a trace d that exhibits a concurrency bug (d drives the program to a state \nsatisfying the bug predicate f) and a subset E of the events in d. Let d\\E denote the trace d with the \nevents in E removed. We de.ne E is redundant if after removing the events in E, the following two conditions \nare satis.ed: I. d\\E can still drive the program to a state that satis.es f;  II. any sequential schedule \nof the reduced execution does not satisfy f. We assume f characterizes a concurrency bug. The sound\u00adness \nof this criterion is easy to follow. First, Condition I and Condition II together ensure that the reproduced \nbug is a concurrency bug, because f is satis.ed under the original buggy schedule (excluding the events \nin E), but not a se\u00adquential schedule. Second, consider condition II, since f is evaluated but not satis.ed \n(i.e., the bug does not manifest) under a sequential schedule1, the program does not malfunc\u00adtion after \nremoving the events in E. Otherwise, either f is not evaluated or f is always satis.ed. Hence, the same \ncon\u00adcurrency bug is reproduced under Conditions I and II. It is worth noting that the trace redundancy \nis not de.ned over a single event but a subset of events in the trace, which correspond to a group of \ninstructions in the program execu\u00adtion. The reason is that redundant instructions are not inde\u00adpendent \nbut may be closely related to each other. A group of instructions may be redundant but any single instruction \nof them may not. For example, suppose an erroneous inter\u00adleaving between the 5th and 10th iterations \nof threads T(2,3) manifests the bug in Figure 1. The whole computation of thread T1 is redundant, but \nany single instruction of T1 alone is not. Without any dependence information between the in\u00adstructions, \nremoving the trace redundancy is essentially a combinatorial optimization problem, which is exponential \nto the number of instructions in the original buggy execution. To facilitate more effective simpli.cation, \nwe further characterize the redundancy into two dimensions: whole-thread redundancy -all computation \nof a certain thread is redundant;  partial-thread redundancy -redundant instructions as part of each \nindividual thread.  This categorization utilizes the thread identity relation\u00adship between the computations. \nIn practice, threads as sep\u00adarate control .ows in the program are more likely to be in\u00addependent to each \nother than the individual instruction. We can skip all the computation of the redundant thread. Com\u00adpared \nto the whole-thread redundancy, the partial-thread re\u00ad 1 Note that we do not need to check all sequential \nschedules but checking any one of them is suf.cient to validate whether the concurrency bug is still \na concurrency bug or not.  dundancy examines the more .ne-grained instructions lo\u00adcal to each individual \nthread. If an instruction by a certain thread is redundant, we can skip it during the execution of that \nthread. In our illustrating example, all the other threads except T(0,2,3) are redundant, which belong \nto the whole\u00adthread redundancy, and most of the repetitions of threads T(2,3) are redundant, which belong \nto the partial-thread re\u00addundancy.  3. Automatic Redundance Removing We propose two techniques to remove \nthe trace redun\u00addancy for simplifying the concurrency bug reproduction. The .rst technique effectively \nvalidates and removes the whole-thread redundancy by adapting delta-debugging [38] using the thread hierarchy \ninformation. Sharing the essence of delta-debugging, our technique produces a 1-minimal set of threads \n[38] that are not redundant in the buggy execu\u00adtion. The second technique targets at effectively removing \nthe partial-thread redundancy related to irrelevant instruc\u00adtions and repetitions. It combines a dynamic \nmultithreaded slicing technique and a static repetition analysis to improve the simpli.cation ef.ciency, \nas well as a simple annota\u00adtion framework that integrates programmers hints to further reduce the search \nspace. The entire simpli.cation process is deterministic. There is no interleaving non-determinism during \nsimpli.cation as we control all the thread scheduling (including the non-preemptive ones) during the \nreplay. 3.1 Removing Whole-Thread Redundancy Our general idea to identify and to remove the whole\u00adthread \nredundancy follows the approach of hierarchical delta-debugging [16, 38]. We use a bisection method to \npick candidate threads and test whether they can be removed from the execution or not. More speci.cally, \nwe control the pro\u00adgram to disable the selected candidate threads and validate the reduced execution \nfor the two conditions de.ned in our redundancy criterion in Section 2.2. Our technique for re\u00admoving \nthe whole-thread redundancy is fully automatic. It does not require any user intervention. There are \ntwo main challenges we address in our ap\u00adproach. First, threads may not be arbitrarily removed. For example, \nif a parent thread is removed, all its descendants are disabled. Second, after removing a redundant thread, \nwe must compute the schedule of the remaining threads (in or\u00adder to deterministically replay the reduced \nexecution). Our approach contains two core treatments to address these two problems. First, we extract \na dynamic thread hierarchy graph of the original buggy execution (TH-Tree) and perform the delta-debugging \nbased on the TH-Tree, to make sure that if a parent thread is disabled, all its descendent threads are \ndis\u00adabled. Figure 2 shows an example of the TH-Tree. For exam\u00adple, if T1 and T3 are selected, all their \ndescendants (shown in the gray boxes in Figure 2) are also selected. Second, we compute the schedule \nfor the remaining threads by project-Figure 2. An example of dynamic thead hierarchy graph (TH-Tree). \nWhen T1,3 are selected, all T1,3 and their descen\u00addents (gray color) are disabled. ing the trace on \nthe thread ID without the IDs of the selected candidate threads and their descendants. The schedule is \nen\u00adforced in the validation run to test whether the bug can still be reproduced or not. In this way, \nthe thread schedule of the remaining execution is the same as that in the original buggy execution, which \npreserves the erroneous interleavings. Algorithm 1 summarizes our algorithm. Given the orig\u00adinal buggy \ntrace, it produces a simpli.ed trace (execution) containing only the 1-minimal set of threads that is \nable to reproduce the bug. The 1-minimal property means that, all remaining threads are necessary such \nthat removing any one of them would cause the reduced execution to fail to repro\u00adduce the bug. Our algorithm \nstarts by iterating on the height of the TH-Tree. In each iteration, we always pick the candi\u00addate threads \nwith the same height. Starting from the threads with height 1 (the main thread is of height 0), we .rst \nse\u00adlect the candidate threads (thread sets) to be validated for the redundancy. If a parent thread is \nselected, its descendants are all disabled. We then process the selected threads using a delta-debugging \nalgorithm, as shown in Figure 3. Each in\u00advocation of delta-debugging computes the 1-minimal set of threads \n(in the input threads denoted by cx) that are nec\u00adessary to reproduce the bug. The set cx in the ddmin \nal\u00adgorithm corresponds to the selected threads. The validate procedure (Algorithm 2) corresponds to the \ntest function in delta-debugging. It tests whether the two conditions in the redundancy criterion are \nboth satis.ed or not after disabling the selected threads: (1) the bug is reproduced with the com\u00adputed \nschedule of the remaining threads; (2) the bug is not reproduced with a sequential schedule. If both \nconditions are true, it means that the selected threads are redundant and they are removed from the execution. \nThis process is re\u00adpeated for all levels of threads in the TH-Tree, until no new thread can be removed. \nUnique thread identi.cation In our algorithm, an ad\u00additional problem we need to address is how to consistently \nidentify threads across runs, as the validation run requires matching the selected threads. We take a \nsimilar approach as that in jRapture [25] to identify threads and their chil\u00addren. The key observation \nis that each thread should create  Let validate and .. be given such that validate(..)=X(fail). The \nalgorithm computes ... is 1-minimal....and,X=)..., validate(.. . ...such that,2)...(2ddmin=)..ddmin(= \n ..   ..G........ . ..g. ..... . .........  ....G........ . ..g. ..... .... .... .. . .... ........ \n........ .. .... ......... . .. .. . .. .... . .. .. G ......... ... .. / ... ...are pairwise disjoint, \nand .., all.... . .. . ..= ...,..-...= .. where Figure 3. The delta-debugging algorithm. The function \nvalidate return true if the two conditions in the redundance criterion are both satis.ed. For conciseness, \nthe input trace is ignored in the ddmin algorithm. Algorithm 1 RemoveWholeThreadRedundancy(d) 1: Input: \nd -the original trace (ei)2: Output: d' -the simpli.ed trace with all redundant The thread identi.er \nin Figure 2 illustrates this identi.cation strategy. Note that the identi.cation is performed only once \non the initial TH-Tree and remains consistent for all the sim\u00ad threads removed pli.cation runs. 3: TH \nTree .ExtractThreadHierarchyGraph(d) 4: height . the height of(TH Tree) 3.2 Removing Partial-Thread Redundancy \n5: for level . 1: height do 6: thread set . get threads(TH Tree,level) 7: minimal threads .DeltaDebugging(d,thread \nset) 8: redundant threads . (thread set \\ minimal threads) and their descendents 9: remove redundant \nthreads from TH Tree 10: remove all events by redundant threads in d 11: end for 12: return d Algorithm \n2 Validate(d,disabled threads) 1: Input: d -a trace (ei)2: Input: disabled threads -a set of disabled \nthreads 3: d' . remove all events by disabled threads in d 4: . .get schedule(d') 5: .seq .get sequentialschedule(d') \n6: if IsBugReproduced(d',.) then To identify the partial-thread redundancy, we may directly apply delta-debugging \non the granularity of the individual instructions. However, this naive approach is ineffective be\u00adcause \nenumerating and validating every combination of the instructions for each individual thread could be \nvery expen\u00adsive. To improve the ef.ciency, our technique combines the multithreaded dynamic slicing with \na repetition analysis to identify the redundant computation local to each individual thread. The dynamic \nslicing tracks the data and control de\u00adpendencies between instructions in the execution trace and removes \nthose instructions that are irrelevant to the bug. The repetition analysis is a heuristic that targets \nat removing the redundancy related to repetitions. To further improve the effectiveness of repetition \nanalysis, LEAN also provides a simple framework that allows programmers annotating the repetitive code \nsegments, which signi.cantly reduces the search space of trace simpli.cation. We next describe them in \ndetail. 7: if IsBugNotReproduced(d',.seq) then 8: return true 3.2.1 Multithreaded dynamic slicing 9: \nend if 10: end if 11: return false its children threads in the same order, though there may not exist \na consistent global order among all threads. We there\u00adfore create a consistent identi.cation for all \nthreads based on the parent-children order relationship. More speci.cally, starting from the main thread \n(T0), each thread maintains a thread-local counter for recording the number of children it has forked \nso far. And everytime a new thread is forked, it is identi.ed with its parent thread ID associated with \nthe counter value. For instance, suppose a thread ti forks its jth child thread, this child thread will \nbe identi.ed as ti:j . The dynamic dependence graph (DDG) is the classical model for slicing single-threaded \nexecutions, which cap\u00adtures the dynamically exercised Read-After-Write (RAW) and control dependencies. \nEach node in the DDG represents an execution instance of a statement (an instruction) while the edges \nrepresent the dependences. For multithreaded exe\u00adcution, Tallam et al. [26] proposes a dynamic slicing \nmodel\u00ading for data race detection. Their model extends the DDG to consider the additional data dependencies \non shared variable accesses. Our slicing model for concurrency bug reproduction is similar to but more \nstrict than the model by Tallam et al. [26]. To guarantee the deterministic bug reproduction, in ad\u00addition \nto the shared variable read/write dependencies, we also need to consider the dependencies on synchronization \n operations. Speci.cally, given the buggy execution, we con-3.2.2 Repetition analysis struct a multithreaded \ndependence graph (MDG) that con\u00adsists of the DDG for each individual threads as well as the following \ndependence relations between the instructions ai and aj by different threads: Synchronization dependencies \nREL.ACQ: ai is the REL operation that releases the lock acquired by the ACQ operation aj; FORK.START: \nai is the FORK operation that forks the thread whose START operation (a dummy instruc\u00adtion introduced \nas the .rst operation of a thread) is aj; EXIT.JOIN: ai is the EXIT operation of a thread (a dummy instruction \nintroduced as the last operation of a thread) that the JOIN operation aj joins; SND.RCV: ai is the SND \noperation that sends the message received by the RCV operation aj; Shared variable dependencies -ai and \naj are consecutive on the same shared variable: WRITE.READ: ai is a WRITE and aj is a READ; READ.WRITE: \nai is a READ and aj is a WRITE; WRITE.WRITE: both ai and aj are WRITE ac\u00ad cesses. Note that the WRITE.WRITE \ndependency must be in\u00adcluded in the MDG, to ensure the correctness of MDR [10]. Otherwise, a read in \nthe replaying phase may return the value written by a different write from that in the original buggy \nexecution, which may cause the failure of MDR. Algorithm 3 shows our dynamic slicing algorithm for removing \nthe partial-thread redundancy. We .rst construct the MDG that includes both the DDG for each thread in \nthe execution and the synchronization and shared variable dependencies. Starting from the buggy instruction \nwhich violates the bug predicate, we perform a backward analysis that keeps only the instructions with \na direct or a transitive dependency relation to the buggy instruction. All the other instructions are \nmarked to be irrelevant to reproducing the bug and are skipped in the simpli.ed execution. Algorithm \n3 DynamicMultithreadedSlicing(d,af ) 1: Input: d -the full execution trace after removing all redundant \nthreads 2: Input: af -the buggy instruction 3: Output: d ' -the simpli.ed trace 4: mdg . ConstructMultithreadedDependencyGraph(d) \n5: mdg ' . ReverseEdge(mdg) 6: relevant instructions . DepthFirstSearch(af ) on mdg ' 7: d ' . remove \nthe instructions from d that are not in relevant instructions 8: return d ' Redundancy is often caused \nby repetitions. Speci.cally, we observe that a large portion of redundant computation by each individual \nthread is rooted by the repetitive code blocks (RCBs) that contain repeated operations in loops. The \noper\u00adations inside a RCB are expected to execute a few iterations upon the loop condition with no break \noperation. The loop variable is often a primitive data (e.g., integers) that used as a counter for counting \nthe number of iterations so far. We propose a static repetition analysis to identify RCBs in the program. \nThe RCBs are used as a pool of potential redun\u00addant computation that we may simplify. Each execution \niter\u00adation of a RCB is considered as potentially redundant. After validating the redundancy of an iteration \nusing our redun\u00addancy criterion, we can remove all computation of this iter\u00adation from the execution. \nOur repetition analysis is based on a simple intra-procedural loop analysis. For each loop, we consider \ntwo conditions to mark it as a potential RCB. First, the loop condition contains only primitive or concrete \ndata and the loop variable is only incremented or decremented once in each iteration. Second, there is \nno break operation inside the loop (exceptions are allowed). Despite the simplicity, our experiments \nshow that this analysis is effective and ef.cient for identifying redun\u00addant computation caused by RCBs. \nAlgorithm 4 RemoveRepetitionRedundancy(p,d) 1: Input: p -the program 2: Input: d -the trace after slicing \n3: Output: d ' -the .nal simpli.ed trace 4: statements . GetRepetitiveCodeBlocks(p) 5: threads . get \nthreads(d) 6: for t in threads do 7: for s in statements do 8: all iterations . get iterations(d,t,s) \n 9: minimal iterations .DeltaDebugging(d,all iterations) 10: remove (all iterations\\minimal iterations)in \nd 11: end for 12: end for 13: return d Algorithm 4 shows our algorithm for removing the partial\u00adthread \nredundancy caused by repetitions. This algorithm is applied after slicing the buggy trace. We .rst identify \nthe RCB that contains potential redundant computation. We then perform delta-debugging on each iteration \nof the RCB for each thread, to validate the redundancy of the computation corresponding to the iteration. \nA framework for repetition analysis LEAN also pro\u00advides an option for the programmers to annotate RCBs, \nwhich can help signi.cantly improve the effectiveness of our automatic repetition analysis. Our general \nobservation is that programmers often have the knowledge of whether some   Ti for j =1:M { @rcb-begin \nexpected = account.get()+i account.increment(i) A: assert account.get()==expected expected=account.get()-i \naccount.decrease(i) B: assert account.get()==expected @rcb-end } Figure 4. Some iterations of the code \nblock demarcated by @rcb-begin and @rcb-end are speci.ed as potentially redundant. Program Simplified \nBuggy trace Buggy trace  Figure 5. An overview of LEAN code blocks is repetitive or not (in particular, \nin writing the test drivers). This piece of information is in fact easy for the programmers to specify \n(e.g., using simple annotations), but very dif.cult to identify by any automatic approach because of \nthe absence of the repetition criterion. More importantly, without any further intervention, we can help \nprogrammers automatically validate whether some executions of the RCBs are redundant or not, and eliminate \nthem from the buggy ex\u00adecution if they are redundant. Our framework is easy to use. Programmers simply \nmark the beginning and the end of the RCB by @rcb-begin and @rcb-end, respectively. For example, programmers \nmay mark the RCB for thread Ti in a way as shown in Figure 4. We then perform delta-debugging on each \niteration of the code, and .lter out most redundant iterations. Also, this framework is .exible. New \nannotations may be added after each round of simpli.cation, when programmers get more information about \nthe bug from the intermediate simpli.ed execution.  4. Implementation To evaluate our technique, we \nhave implemented LEAN on top of LEAP [9], our MDR framework for applications writ\u00adten in Java2. Figure \n5 shows an overview of LEAN.Given the target concurrent program and the buggy execution trace, LEAN .rst \nremoves the whole-thread redundancy from the 2 Our technique is general to concurrent programs written \nin any program\u00adming language. trace using our adapted hierarchical delta-debugging algo\u00adrithm (Algorithm \n1). It then further simpli.es the resultant execution by removing the partial-thread redundancy using \nour dynamic multithreaded slicing algorithm (Algorithm 3) and the repetition analysis (Algorithm 4). \nThe .nal output produced by LEAN is a simpli.ed buggy execution in which all the redundant computation \nis skipped in the replayed ex\u00adecution. For the delta-debugging, we faithfully implemented the algorithm \ndescribed in Figure 3. Our slicing implementation is based on the Indus framework [23], which we adapt \nfor dynamic multithreaded execution traces. In addition to the data dependencies across threads, slicing \nalso takes care of all the data and control dependencies internal to each individual thread in the execution. \nFor supporting MDR, LEAN collects in the trace the following types of events in a global order: read/write \nac\u00adcesses to shared variables, lock acquisition/release, thread fork/join, and wait/notify events. To \nsupport recording long running programs which produce large traces, LEAN does not put the entire trace \nin the main memory but saves it to a database. To disable an instruction, we instrument the program to \ninsert control statements before the statement (Jimple state\u00adment in Soot3) which corresponds to the \ninstruction. For example, to disable a thread, we insert control instrumen\u00adtation before Thread.start() \nand Thread.join() to make sure that the disabled thread is not executed and joined by any other thread. \nWe distinguish the dynamic thread by assign\u00ading a unique ID to each thread instance (explained in Section \n3.1). For the partial-thread redundancy, we also maintain a thread local counter for each annotated RCBs, \nto denote the iteration instance of each thread in executing the RCB. To control the thread schedule, \nwe reuse the application\u00adlevel scheduler of LEAP. The thread IDs of all the events in the trace form \na global schedule. After disabling a thread, we simply remove the thread ID from the global schedule. \nTo enforce a sequential schedule, we control the execution of a thread until it terminates or cannot \ncontinue execution (i.e., waiting for a lock or joining for the termination of another thread) and then \nrandomly pick an enabled thread to pro\u00adceed. For removing the partial-thread redundancy, we also associate \neach event in the trace with its corresponding state\u00adment in the program. User annotated RCBs are interpreted \nas special statement blocks. To generate the remaining sched\u00adule after disabling a certain iteration \nof a RCB, we .rst re\u00admove the corresponding events in the trace according to the RCB and the per-iteration \ninformation, and then compute the schedule by performing a projection of the remaining trace on the thread \nID. 3 http://www.sable.mcgill.ca/soot/  TableDescriptor { getObjectName(){ if (referencedColumnMap == \nnull){ } else{    for (int i = 0; i <...; i++){ referencedColumnMap.isSet( ) } } } } 1 TestEmbeddedMultiThreading \n{ 2 3 4setReferencedColumnMap( ){ 5 referencedColumnMap = null; 6} 7 8 9 10 11 12 13 Figure 6. A real \nconcurrency bug #2861 in Derby. The 14 15 thread interleaving following the solid arrow on the shared \n16 data referencedColumnMap crashed the program with 17 18 NullPointerException. 19 20 21 22 5. A Case \nStudy 23 24 In this section, we present a case study of a real concurrency 25 bug reproduction in Apache \nDerby DBMS4. We illustrate 26 27 how LEAN simpli.es the bug reproduction w.r.t. the whole- 28 29 } thread \nredundancy and the partial-thread redundancy in a detailed view. 31 32 33 5.1 Description of Derby Bug \n#2861 34 35 Figure 6 shows the concurrency bug #2861 we study in the 36 Apache bug database5. The bug \nis concerned with a thread 37 38 safety issue in the org.apache.derby.iapi.sql.dictionary 39 40 .TableDescriptor \nclass. The shared data referencedColumnMap 41 is checked for null at the top of the getObjectName 42 \n43 method and later dereferenced if it is not null. Due to an erro\u00ad 44 neous interleaving, another thread \ncan set referencedColumnMap 45 to null in the setObjectName method and causes the pro\u00adgram to crash by \nthrowing a NullPointerException. Fig\u00adure 7 shows a driver program (also documented in the bug database) \nfor triggering the bug. Ignore all the gray areas for the moment; these are statements inserted by LEAN. \nThe driver program starts N threads each creating (lines 41-45) and then dropping (lines 48-51) a separate \nview against the same source view, repeated M times. Because of the non\u00addeterminism, the bug is very \ndif.cult to manifest with a small N and M. In our experiment with N=2 and M=2 on an eight-core Linux \nmachine, we did not observe a single run of failure after 1000 runs. With a larger number of threads \nand repetitions, the probability of triggering the bug is increased. When we set N=10 and M=10, we were \nable to trigger the bug in three out of 1000 runs. With the help of a MDR system such as LEAP,weare able \nto deterministically reproduce the bug once it mani\u00adfests. The problem is that the bug reproduction run \nis too complicated, with too many threads (11) and thread context switches (6,439). The size of the execution \ntrace (which con\u00adtains the critical events only) is as large as 94.1M, and it took LEAP as long as 466 \nseconds to reproduce the bug. Given 4 http://db.apache.org/derby/ 5 https://issues.apache.org/jira/browse/DERBY-2861 \nmain(String args[]){ int numThreads = Integer.parseInt(args[0]); int numIterations = Integer.parseInt(args[1]); \n//register the embedded driver and create the test database EmbeddedDriver driver = new EmbeddedDriver(); \nconn = DriverManager.getConnection(\"jdbc:derby:DERBY2861\"); stmt = conn.createStatement(); sql = \"CREATE \nVIEW viewSource AS SELECT col1, col2 FROM schemamain.SOURCETABLE stmt.execute(sql); stmt.close(); //create \ntest threads Thread[] threads = new Thread[numThreads]; for (i = 0; i < numThreads; i++) threads[i] \n= new Thread(new ViewCreatorDropper( \"schema1.VIEW\" + i, \"viewSource\", \"*\", numIterations)); //start \ntest threads for (int i = 0; i < numThreads; i++)  if(shouldStartThread(threads[i])) threads[i].start(); \nthreads[i].start(); //wait for threads to terminate for (int i = 0; i < numThreads; i++)  if(shouldJoinThread(threads[i])) \nthreads[i].join(); threads[i].join(); } 30 ViewCreatorDropper implements Runnable { 46 47 48 49 50 51 \n52 53 54 55 56 } ViewCreatorDropper(String viewName, String sourceName, String columns, int iterations) \n{ m_viewName = viewName; m_sourceName = sourceName; m_columns = columns; m_iterations = iterations; } \nrun( ){ for (i = 0; i < m_iterations; i++) { @rcb-begin if(shouldExecuteIteration(i)) { //create view \n stmt = conn.createStatement(); sql = \" \"CREATE VIEW \" + m_viewName + \" AS SELECT \" + m_columns + \" \nFROM \" + m_sourceName ; stmt.execute(sql); stmt.close(); //drop view stmt = conn.createStatement(); sql \n= \" \" \"DROP VIEW \" + m_viewName ; stmt.execute(sql); stmt.close(); @rcb-end } } Figure 7. A real world \ntest driver for triggering the concur\u00adrency bug in Figure 6. The statements inserted by LEAN to simplify \nthe execution are shown in the gray areas. such a bug reproduction run, it is still challenging for the \nprogrammers to understand the bug by inspecting the trace. 5.2 How LEAN Simpli.es the Bug Reproduction \nLEAN simpli.es the reproduction of this bug by removing the redundant computation in the reproducible \nbuggy execu\u00adtion. Although there are ten testing threads each of which re\u00adpeats ten times in the buggy \nexecution, we can observe that, in the best case, two testing threads each with one iteration is suf.cient \nto trigger the bug. All the other eight threads and nine iterations are redundant and can be removed \nfrom the bug reproduction run.  Taking the original buggy execution as the input, LEAN .rst identi.es \nand removes the redundant threads in the execution using Algorithm 1. Figure 8 illustrates the sim\u00adpli.cation \nprocess. Because the dynamic thread hierarchy graph in the buggy execution contains only one level of \nthreads, the entire simpli.cation process invokes the delta\u00addebugging procedure only once, which directly \napplies on threads T(1,2,...,10). To skip a thread, LEAN controls the exe\u00adcution of the program by inserting \na condition checking be\u00adfore Thread.start() and Thread.join() (as shown in the gray areas at lines 23 \nand 27 in Figure 7). A thread is not started or joined if it is removed. After four rounds of simpli.cation, \nthreads T(2,3) remain in the reduced execution and all the other threads are removed. This process took \n1,841 seconds in our experiment. After removing the redundant threads, 75.1M(79.8%) of the events in \nthe original buggy trace were removed and the size of the remaining trace was reduced to 19M. After removing \nthe whole-thread redundancy, LEAN then further processes the reduced buggy execution to remove the partial-thread \nredundancy. It .rst performs the dynamic slicing to remove irrelevant instructions using Algorithm 3. \nAs slicing tracks all the dynamic data dependencies across threads as well as all the intra-thread data \nand control de\u00adpendencies in the remaining buggy execution, it took LEAN 553 seconds to .nish the slicing \nprocess in our experiment, and an additional 6.2M(6.6%) of the events were removed from the trace. Similar \nto the control of threads, we simply insert control statements before the irrelevant instructions to \nskip their executions. LEAN then continues to simplify the reduced buggy ex\u00adecution by removing the redundant \nrepetitions using Algo\u00adrithm 4. Our automatic repetition analysis successfully iden\u00adti.ed the RCB at \nlines 42-53 in the test thread, as demarcated by @rcb-begin and @rcb-end at lines 41 and 54 in Figure \n7. To control the execution of a certain iteration i of the RCB, we insert a control statement before \nthe RCB with i as the input parameter (as shown in the gray area at line 40), deter\u00admining whether the \nith iteration is enabled or not. Figure 9 illustrates the simpli.cation process for LEAN to remove the \nredundant execution iterations of the RCB of threads T(2,3). After ten rounds of simpli.cation, the 7th \niteration of T2 and the 4th iteration of T3 remain and all the other iterations are removed. This process \ntook around 200 seconds in our ex\u00adperiment. An additional 11.6M (12.3%) of the events were removed and \nthe size of the .nal buggy trace was reduced to around 2.01M. In total, it took LEAN 2,593 seconds to \nsimplify the original buggy execution. The .nal simpli.ed execution was able to reproduce the same bug \nand was signi.cantly simpler than the original buggy execution. The simpli.ed trace size was reduced \nby 47x (from 94.1M to 2.01M), containing only 3 threads (T(0,2,3)) and 433 thread context switches, and \nits replay time by LEAN was shortened by 46x (from Figure 9. Illustration of delta-debugging for removing \nthe redundant repetitions for the remaining threads T(2,3). Iij denotes the jth iteration of thread Ti \nwhere i=2,3 and j=1,2,. . . ,10. After ten rounds of simpli.cation, the 7th it\u00aderation of T2 and the \n4th iteration of T3 remain and all the other iterations are removed.  446 to 10.2 seconds). Moreover, \nall the instrumentations and the thread scheduler in LEAN are transparent to the programmers, such that \nthe debugging task can be performed on the simpli.ed buggy execution in a normal debugging environment. \n  6. Experiments The goal of our technique is to improve the effectiveness of the MDR support for debugging \nconcurrent programs, via removing the redundancy from the reproducible buggy trace. Accordingly, our \nevaluation aims at answering the following two research questions: RQ1. Effectiveness -Is LEAN effective \nin simplifying real buggy traces? How much reduction of the replay time and the trace complexity (i.e., \nsize, threads, and con\u00adtext switches) can our approach achieve? RQ2. Ef.ciency -How ef.cient is LEAN \nfor identifying and removing the trace redundancy? All experiments were conducted on two eight-core 3.00GHz \nIntel Xeon machines with 16GB memory and Linux 2.6.22 and JDK1.7.  Program Original Trace Simpli.ed \nTrace Size #Thread #CS Replay Time Size #Thread #CS Replay Time BuggyPro 460K 34 1,003 1.27s 13.2K(.97.1%) \n4(.88.2%) 28(.97.2%) 39ms(.97%) Tsp 44.1M 5 9,190 280s 22.1M(.49.9%) 3(.40.0%) 4,588(.50.0%) 115s(.58.9%) \nArrayList 1.72M 451 2,381 6.5s 6.4K(.99.6%) 3(.99.3%) 10(.99.6%) 20ms(.99.7%) LinkedList 2.20M 451 2,564 \n7.2s 6.8K(.99.7%) 3(.99.3%) 10(.99.6%) 22ms(.99.7%) OpenJMS 128.9M 36 7,287 606s 1.82M(.98.5%) 7(.80.5%) \n415(.94.3%) 16.3s(.97.3%) Tomcat 38.2M 13 3,543 206s 1.26M(.96.7%) 4(.69.2%) 111(.96.9%) 3.3s(.98.4%) \nJigsaw 20.1M 11 2,322 154s 416K(.98.0%) 3(.72.7%) 64(.97.2%) 2.4s(.98.4%) Derby 94.1M 11 6,439 466s 2.01M(.97.8%) \n3(.72.7%) 433(.92.5%) 10.2s(.97.6%) Table 2. Experimental results -RQ1: Effectiveness  Program SLOC \nInput/#Threads#Iterations BuggyPro 348 race exception/33/- Tsp 709 map4/4/- ArrayList 5,979 not-atomic \nbug/450/- LinkedList 5,866 not-atomic bug/450/- OpenJMS-0.7.7 262,842 order violation bug/20/10 Tomcat-5.5 \n339,405 bug#37458/10/10 Jigsaw-2.2.6 381,348 NPE bug/10/10 Derby-10.3.2.1 665,733 bug#2861/10/10 Table \n1. Evaluation benchmarks 6.1 Benchmarks We quantify our technique using a set of widely used third\u00adparty \nconcurrency benchmarks with known bugs. We con\u00ad.gure the program inputs to generate buggy traces of dif\u00adferent \nsizes and complexity. To understand the performance of our technique on real applications in practice, \nwe also in\u00adclude several large concurrent server systems in our bench\u00admarks. The .rst column in Table \n1 shows the benchmarks used in our experiments. BuggyPro is a multithreaded bench\u00admark from the IBM ConTest \nbenchmark suite [6], ArrayList and LinkedList are open libraries from Suns JDK 1.4.2, and Tsp is a parallel \nbranch and bound algorithm for the travel\u00adling salesman problem from ETH [31]; The remaining four benchmarks \nare real server systems. OpenJMS-0.7.7 is an enterprise message-oriented middleware server, Tomcat-5.5 \nis a widely used JSP and Servlet Container, Jigsaw-2.2.6 is W3C s leading-edge web server platform, and \nDerby\u00ad 10.3.2.1 is a widely used open source Java RDBMS from Apache. Column 2 (SLOC) reports the source \nlines of code of our evaluation benchmarks. The sizes range from a few hundred lines to over 600K lines \nof code. Column 3 (In\u00adput/#Threads#Iterations) reports the input data (the bug, the number of threads, \nand the iterations, if available) con.gured in the recorded execution of the benchmark.  6.2 RQ1: Effectiveness \nThe goal of our .rst research question is to evaluate how ef\u00adfective our technique is for simplifying \nthe buggy execution traces of real concurrent programs. To generate the data nec\u00adessary for investigating \nthis question, we proceed as follows. For each benchmark, we .rst run it multiple times with ran\u00addom \nthread schedule until the bug manifests and use LEAN to collect the corresponding buggy trace of each \nrun. For each trace, we then apply our technique to produce a simpli\u00ad.ed trace with the redundancy removed. \nDuring the simpli.\u00adcation process, we .rst remove the whole-thread redundancy and then the partial-thread \nredundancy (consists of both slic\u00ading and the repetition analysis). The whole process is fully automatic \nwith no user intervention. We measure the per\u00adcentage of trace size reduction with respect to the two \ndimen\u00adsions of redundancy. We also quantify the .nal simpli.cation results in terms of the reductions \nof the trace size, the num\u00adber of threads and the number of thread context switches, as well as the replay \nspeedups. To demonstrate the simpli.ca\u00adtion effectiveness of our approach, we also compared LEAN with \nan execution reduction technique ER [27] that uses the dependence graph for the simpli.cation. Table \n2 reports our .nal simpli.cation results. Columns 2-5 (Size, #Thread, #CS, Replay Time) report the size \nof the original trace, the number of threads, the number of thread context switches (including both non-preemptive \nand preemptive ones) in the original trace, and the replay time of the original trace, respectively, \nwhile Columns 6-9 report the corresponding statistics of the simpli.ed trace. As the table shows, the \nsize of the original trace ranges from 460KB (BuggyPro) to more than 128MB (OpenJMS) on disk, which take \nfrom 1.27 seconds to more than 10 minutes to replay to reproduce the bug. The original trace is also \nof signi.cant complexity w.r.t. the number of threads and the number of context switches, ranging from \n5 threads in Tsp to 451 threads in ArrayList and LinkedList, and from 1,003 context switches in BuggyPro \nto 9,190 context switches in Tsp. LEAN was able to greatly reduce the trace complexity for all the concurrency \nbugs in our experiments. The trace size is reduced by 49.9% (2x) in Tsp to as large as 99.7% (324x) in \nLinkedList, the number of threads is reduced by 40% to 99.3%, and the number of context switches is reduced \nby 50% to 99.6%. Moreover, the replay time is also greatly shortened after simpli.cation, ranging from \n58.9% (2.4x) in Tsp to 99.7% (327x) in LinkedList. In the four large server Table 3. Decomposed effectiveness \non trace size reduction  Program Whole Redundancy Partial Redundancy Slicing Repetition BuggyPro 445K(96.9%) \n1.8K(0.2%) - Tsp 21.7M(49.2%) 0.4M(0.7%) - ArrayList 1.71M(99.6%) - - LinkedList 2.19(99.7%) - - OpenJMS \n100.8M(78.2%) 7.3M(5.7%) 20.0M(15.5%) Tomcat 23.6M(61.9%) 4.2M(11.0%) 9.1M(24.0%) Jigsaw 16.0M(79.4%) \n0.91M(4.5%) 2.7M(13.4%) Derby 75.1M(79.8%) 6.2M(6.6%) 11.6M(12.3%) applications, the replay time is \nconsistently shortened by around 98% (64x). Table 3 reports the simpli.cation effectiveness w.r.t. each \nof the three components in terms of the trace size reduc\u00adtion. Column 2 reports the percentages of the \nwhole-thread redundancy reduced by the hierarchical delta-debugging (HDD), while Columns 3-4 report that \nof the partial-thread redundancy, reduced by the slicing and the repetition anal\u00adysis, respectively. \nIn the small benchmarks, the percentage of whole thread redundancy ranges from 49.2% to 99.7%. LEAN did \nnot identify much partial thread redundancy in these small benchmarks. Slicing removes only 0.2% and \n0.7% redundancy, respectively, in BuggyPro and Tsp.For the real server programs, the percentage of whole-thread \nredundancy ranges from 61.9% to 79.8%. For the partial\u00adthread redundancy, slicing and repetition analysis \nare both more effective than that for the small benchmarks. Slicing removes 4.5% to 11% redundant computation \nin the four large server programs, while the percentage of redundancy removed by the repetition analysis \nranges from 12.3% to 15.5%. We note that the amount of redundancy in the buggy traces is closely related \nto the number of threads and the number of repetitions con.gured as input to the program. With more redundancy \nin the buggy trace, LEAN would have better simpli.cation ratio. Nevertheless, we believe our result is \nrepresentative as our experimental setup re.ects the typical concurrency testing scenarios in the development \ncy\u00adcle (such as the effective random testing in the IBM ConTest tool [6] and the stress testing in Chess \n[20]). Comparing with the ER [27] The execution reduction (ER) technique proposed by Tallam et al. [27] \nalso aims at reducing the trace size, for supporting the tracing of long running multithreaded programs. \nER works by tracking a dy\u00adnamic dependence graph of the execution events. The events are grouped into \nregions and threads such that the size of the dependence graph can be reduced. By analyzing the depen\u00addence \ngraph, ER removes the regions of events or threads that are irrelevant to the fault. As ER relies on \nthe dynamic dependence graph, it cannot remove the redundant compu\u00adtation that has data/control dependencies \nto the fault. While LEAN relies on the redundancy criterion and the dynamic Program ER LEAN BuggyPro \n2.1% 97.1% Tsp 0.0% 49.9% ArrayList 2.9% 99.6% LinkedList 3.0% 99.7% OpenJMS 10.2% 98.5% Tomcat 6.9% \n96.7% Jigsaw 4.6% 98.0% Derby 2.5% 97.8% Table 5. Comparison between LEAN and ER veri.cation, it is \nable to explore more simpli.cation oppor\u00adtunities. We compared the simpli.cation effectiveness on the \ntrace size reduction between LEAN and ER. Table 5 shows the result. For our evaluation benchmarks, LEAN \nis much more effective than ER. ER does not .nd many irrelevant events (the percentage of simpli.cation \nranges from 0.0%-10.2%), because almost all threads have data dependencies between each other on shared \nvariables, while LEAN can effectively remove the redundant threads and the repetitive computation through \nthe hierarchical delta-debugging and our repetition analysis. 6.3 RQ2: Ef.ciency The goal of our second \nresearch question is to assess if our approach is ef.cient in simplifying the buggy trace. Since LEAN \nworks in a black-box style (applying delta-debugging except for the dynamic slicing part) to iteratively \nsimplify the trace, it may take a long time (many rounds) to produce the .nal simpli.cation. As in each \nround it requires two re\u00adplay runs to validate the redundancy (for the two redundancy conditions in our \ncriterion), the ef.ciency of LEAN is an im\u00adportant concern for the usefulness in practice. Hence, during \nthe trace simpli.cation, we also record the number of delta\u00addebugging rounds (for dealing with both the \nwhole-thread redundancy and the partial-thread redundancy) and measure the time needed for each of the \nthree components of LEAN to produce the .nal simpli.ed trace. As we use the repetition analysis to identify \nthe RCBs, we also report the statistics of the repetition analysis result to assess its usefulness in \nim\u00adproving the simpli.cation effectiveness of LEAN. Table 4 shows the experimental results for our research \nquestion RQ2. Columns 2-3 and 5-6 report the number of simpli.cation rounds (including the failed runs) \nand the time taking LEAN to remove the whole-thread redundancy and the redundant repetitions, respectively, \nfrom the original trace (the same trace as that in Table 2). Generally, the num\u00adber of rounds is dependent \non the amount of redundancy, while the simpli.cation time is dependent on the amount of redundancy as \nwell as the length of the original trace. For the small benchmarks, LEAN took 2 to 18 rounds for validating \nthe whole-thread redundancy, which took 8 to 99 seconds of the execution time. For the large systems, \nsince their traces  Program HDD Slicing Repetition RCB #Rounds Time Time #Rounds Time All Real BuggyPro \n6 8s 155ms - - 4 0 Tsp 2 199s 12s - - 3 0 ArrayList 18 55s 2s - - - - LinkedList 18 58s 2s - - - - OpenJMS \n13 4,265s 330s 11 152s 1 1 Tomcat 5 1,082s 308s 12 55s 1 1 Jigsaw 4 630s 210s 10 37s 1 1 Derby 4 1,841s \n553s 10 200s 1 1 Table 4. Experimental results -RQ2: Ef.ciency are much larger, LEAN took 4 to 13 rounds \nand 630 to 4,265 seconds to remove the whole-thread redundancy, and 10 to 12 rounds and 37 to 200 seconds \nto remove the redundant repetitions. Column 4 reports the time needed for slicing the trace (including \nboth the construction time of the mul\u00adtithreaded dependence graph (MDG) and the analysis time for slicing \nthe MDG). Because slicing considers all the in\u00adstructions in the buggy execution, it is more expensive \nfor large server programs (which have longer and more complex traces) than that for the small benchmarks. \nThe slicing time for the four large server programs in our experiments ranges from 210 to 553 seconds. \nSummary Compared to the original replay time, the sim\u00adpli.cation time is typically 4x-8x longer (except \nTsp, which is in fact shorter). However, considering the signi.cant trace simpli.cation ratio, we believe \nthe time cost is acceptable (even for the large systems). Moreover, as the simpli.ca\u00adtion task is fully \nautomatic (transparent to programmers) and can be easily parallelized, programmers do not need to worry \nabout the simpli.cation procedure. For very long running executions, programmers may also choose to set \na time bound for the simpli.cation. When the simpli.cation does not .nish within the time bound, programmers \ncan still have the partially simpli.ed trace (sharing the spirit of delta\u00addebugging). On the aspect of \nrepetition analysis, Columns 7-8 report the total number of identi.ed RCBs and the number of real RCBs \namong them in each benchmark. For the small bench\u00admarks, our analysis identi.ed 4 RCBs in BuggyPro and \n3 in Tsp, but none of them are truly redundant. Our analysis does not report any RCB in LinkedList and \nArrayList.For the large systems, our analysis successfully identi.ed all the RCBs in the test drivers. \nIn testing real concurrent systems, there is often a large number of repetitions (in order to in\u00adcrease \nthe bug .nding possibility). We note that the repeti\u00adtion analysis plays an important role in effectively \nreducing this kind of partial-thread redundancy, though (as our result suggests) the precision of our \nrepetition analysis is not opti\u00admized.  7. Related Work Concurrency bug reproduction has attracted \nextensive re\u00adsearch efforts in the multicore era. However, there is little research that targets at simplifying \nthe concurrency bug re\u00adproduction. We discuss several key related work in this sec\u00adtion. Thread interleaving \nsimpli.cation Chio and Zeller [3] .rst proposed a delta-debugging technique to isolate failure\u00adinducing \nthread schedules in concurrent programs. The pro\u00adposed technique is useful for identifying the context \nswitch points that might cause the bug. Jalbert and Sen [11] .rst proposed a dynamic trace simpli.cation \ntechnique that uses replay to reduce the thread context switches in the buggy trace. Our SimTrace [10] \nalgorithm further improves the ef\u00ad.ciency of trace simpli.cation by statically exploring the trace dependence \ngraph. In general, these techniques do not reduce the redundant computation in the replayed execution \nor the trace. Program slicing Program slicing [28, 34, 39] has been widely used for debugging, that determines \nwhich parts of a program are relevant to a given program point of inter\u00adest (i.e., the buggy statement). \nA number of ef.cient ap\u00adproaches [7, 12, 21] are also proposed for slicing concurrent programs. Essentially, \nslicing relies on the program/system dependence graphs (PDG/SDG) for the simpli.cation, and it cannot \nsimplify beyond the data and control dependencies in the program or the trace. Differently, our notion \nof redun\u00addancy is based on the bug reproduction property, which is not limited by the PDG/SDG, allowing \nus to explore more sim\u00adpli.cation opportunities. For instance, a computation may still be redundant even \nif it has data or control dependency to the buggy state, as long as without it the same bug can be reproduced. \nWeeratunge et al. [33] also propose a novel dual slicing approach to locate and explain the root cause \nof a concurrency failure by comparing the failing and correct schedules. Different from dual slicing, \nour approach aims at producing a simpli.ed buggy execution, without the need of a correct schedule. Multiprocess \ndeterministic replay MDR has witnessed signi.cant progress in recent years. The state of art software\u00adonly \napproaches, DoublePlay [30] supports low overhead full-program replay by of.oading the recording processes \nto extra cores, and our MDR system LEAP [9] enables the lightweight recording using a new type of local \norder to track the shared memory dependency. Many hardware approaches are also proposed to greatly reduce \nthe recording overhead with special hardware design. Rerun [8] exploits episodic memory race recording \nto achieve ef.cient logging (around 4B per 1000 instructions), while DeLorean [17] promises much smaller \nlog sizes and higher replay speeds by inves\u00adtigating the total sequence of chunk commits.  Of.ine search \nTo further reduce the overhead to make concurrency bug reproduction applicable for the produc\u00adtion use, \nresearchers have also explored the idea of of.ine search with only partial runtime information. PRES \n[22] presents a feedback-based approach to use an intelligent replayer during diagnosis time to explore \nthe unrecorded non-deterministic space. ODR [1] and Respec [13] develop lightweight online solutions \nand rely on of.ine search to achieve output-determinism (which is suf.cient for bug re\u00adproduction). ESD \n[37] further reduces the runtime tracing overhead by symbolically exploring the complete thread scheduling \ndecisions via execution synthesis. Weeratunge et al. [32] present an approach to generate a failure inducing \nschedule by comparing the core dumps at of.ine (utilizing the execution indexing technique [36]). Replay \nlog reduction and checkpointing Lee et al. [14] develop a novel log reduction technique that selectively \nrecords extra information and utilizes it to achieve reduc\u00adtion. A key ingredient of their work is the \nunit-based loop analysis (similar to our repetition analysis) that reduces re\u00addundant loop iterations \nbased on unit annotation for loops. Another direction to simplify bug reproduction is through checkpointing \n[4, 35, 40]. By memoizating the program state that is near the buggy execution point, these techniques \ncan help signi.cantly reduce the replay time. A limitation of checkpointing for debugging is that it \nonly reproduces a par\u00adtial causal chain to the bug. In cases of bugs that contaminate program state from \nthe beginning of the execution, program\u00admers may need the full execution history to locate the root cause \nof bug.  8. Conclusion Debugging concurrent programs has been a long-standing challenging problem. We \nhave presented a novel technique LEAN to simplify the concurrency bug reproduction by re\u00admoving the redundant \ncomputation from the buggy trace with the replay-supported execution reduction. Our experi\u00admental results \nshow that LEAN is able to signi.cantly re\u00adduce the complexity of the reproducible buggy execution and \nshorten the replay time. With LEAN, we believe the ef\u00adfectiveness of debugging concurrent programs can \nbe greatly improved. Acknowledgement We thank the anonymous reviewers for their constructive comments. \nThis research is supported by RGC GRF grants 622208 and 622909. References [1] Gautam Altekar and Ion \nStoica. ODR: output deterministic replay for multicore debugging. In SOSP, 2009. [2] Jong-Deok Choi and \nHarini Srinivasan. Deterministic replay of java multithreaded applications. In SPDT, 1998. [3] Jong-Deok \nChoi and Andreas Zeller. Isolating failure\u00adinducing thread schedules. In ISSTA, 2002. [4] William R. \nDieter and James E. Lumpp Jr. A user-level checkpointing library for posix threads programs. In FTCS, \n1999. [5] George W. Dunlap, Dominic G. Lucchetti, Michael A. Fetter\u00adman, and Peter M. Chen. Execution \nreplay of multiprocessor virtual machines. In VEE, 2008. [6] Eitan Farchi, Yarden Nir, and Shmuel Ur. \nConcurrent bug patterns and how to test them. IPDPS, 2003. [7] Dennis Giffhorn and Christian Hammer. \nPrecise slicing of concurrent programs. Automated Software Engg., 2009. [8] Derek R. Hower and Mark D. \nHill. Rerun: Exploiting episodes for lightweight memory race recording. In ISCA, 2008. [9] Jeff Huang, \nPeng Liu, and Charles Zhang. LEAP: Lightweight deterministic multi-processor replay of concurrent Java \npro\u00adgrams. In FSE, 2010. [10] Jeff Huang and Charles Zhang. An ef.cient static trace sim\u00adpli.cation technique \nfor debugging concurrent programs. In SAS, 2011. [11] Nicholas Jalbert and Koushik Sen. A trace simpli.cation \ntechnique for effective debugging of concurrent programs. In FSE, 2010. [12] Jens Krinke. Context-sensitive \nslicing of concurrent pro\u00adgrams. In ESEC/FSE, 2003. [13] Dongyoon Lee, Benjamin Wester, Kaushik Veeraraghavan, \nSatish Narayanasamy, Peter M. Chen, and Jason Flinn. Re\u00adspec: ef.cient online multiprocessor replayvia \nspeculation and external determinism. In ASPLOS, 2010. [14] Kyu Hyung Lee, Yunhui Zheng, Nick Sumner, \nand Xiangyu Zhang. Toward generating reducible replay logs. In PLDI, 2011. [15] Shan Lu, Soyeon Park, \nEunsoo Seo, and Yuanyuan Zhou. Learning from mistakes: a comprehensive study on real world concurrency \nbug characteristics. ASPLOS, 2008. [16] Ghassan Misherghi and Zhendong Su. Hdd: hierarchical delta debugging. \nIn ICSE, 2006. [17] Pablo Montesinos, Luis Ceze, and Josep Torrellas. Delorean: Recording and deterministically \nreplaying shared-memory multi-processor execution ef.ciently. In ISCA, 2008. [18] Pablo Montesinos, Matthew \nHicks, Samuel T. King, and Josep Torrellas. Capo: a software-hardware interface for practical deterministic \nmulti-processor replay. In ASPLOS, 2009.  [19] Madan Musuvathi and Shaz Qadeer. Chess: systematic stress \ntesting of concurrent software. In Proceedings of the 16th international conference on Logic-based program \nsynthesis and transformation, 2007. [20] Madanlal Musuvathi, Shaz Qadeer, Thomas Ball, G\u00b4erard Basler, \nPiramanayagam A. Nainar, and Iulian Neamtiu. Find\u00ading and reproducing heisenbugs in concurrent programs. \nIn OSDI, 2008. [21] Mangala Gowri Nanda and S. Ramesh. Interprocedural slicing of multithreaded programs \nwith applications to java. ACM Trans. Program. Lang. Syst., 2006. [22] Soyeon Park, Yuanyuan Zhou, Weiwei \nXiong, Zuoning Yin, Rini Kaushik, Kyu H. Lee, and Shan Lu. PRES: probabilis\u00adtic replay with execution \nsketching on multi-processors. In SOSP, 2009. [23] Venkatesh Prasad Ranganath and John Hatcliff. Slicing \ncon\u00adcurrent java programs using indus and kaveri. Int. J. Softw. Tools Technol. Transf., 2007. [24] Koushik \nSen. Race directed random testing of concurrent programs. In PLDI, 2008. [25] John Steven, Pravir Ch, \nBob Fleck, and Andy Podgurski. jrapture: A capture/replay tool for observation-based testing. In ISSTA, \n2000. [26] Sriraman Tallam, Chen Tian, and Rajiv Gupta. Dynamic slicing of multithreaded programs for \nrace detection. In ICSM, pages 97 106, 2008. [27] Sriraman Tallam, Chen Tian, Rajiv Gupta, and Xiangyu \nZhang. Enabling tracing of long-running multithreaded pro\u00adgrams via dynamic execution reduction. In ISSTA, \n2007. [28] Frank Tip. A survey of program slicing techniques. Journal of Programming Languages, 1995. \n[29] Mandana Vaziri, Frank Tip, and Julian Dolby. Associating synchronization constraints with data in \nan object-oriented language. In POPL, 2006. [30] Kaushik Veeraraghavan, Dongyoon Lee, Benjamin Wester, \nJessica Ouyang, Peter M. Chen, Jason Flinn, and Satish Narayanasamy. Doubleplay: parallelizing sequential \nlogging and replay. In ASPLOS, 2011. [31] Christoph von Praun and Thomas R. Gross. Object race detection. \nIn OOPSLA, 2001. [32] Dasarath Weeratunge, Xiangyu Zhang, and Suresh Jagan\u00adnathan. Analyzing multicore \ndumps to facilitate concurrency bug reproduction. In ASPLOS, 2010. [33] Dasarath Weeratunge, Xiangyu \nZhang, William N. Sumner, and Suresh Jagannathan. Analyzing concurrency bugs using dual slicing. In ISSTA, \n2010. [34] Mark Weiser. Program slicing. In TSE, 1984. [35] K. Whisnant, Z. Kalbarczyk, and R. K. Iyer. \nMicro\u00adcheckpointing: Checkpointing for multithreaded applications. In IOLTW, 2000. [36] Bin Xin, William \nN. Sumner, and Xiangyu Zhang. Ef.cient program execution indexing. In PLDI, 2008. [37] Cristian Zam.r \nand George Candea. Execution synthesis: a technique for automated software debugging. In EuroSys, 2010. \n[38] Andreas Zeller and Ralf Hildebrandt. Simplifying and isolat\u00ading failure-inducing input. TSE, 2002. \n[39] Xiangyu Zhang and Rajiv Gupta. Cost effective dynamic program slicing. In PLDI, 2004. [40] Lukasz \nZiarek and Suresh Jagannathan. Lightweight check\u00adpointing for concurrent ml. J. Funct. Program., 2010. \n  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Debugging concurrent programs is known to be difficult due to scheduling non-determinism. The technique of multiprocessor deterministic replay substantially assists debugging by making the program execution reproducible. However, facing the huge replay traces and long replay time, the debugging task remains stunningly challenging for long running executions. We present a new technique, LEAN, on top of replay, that significantly reduces the complexity of the replay trace and the length of the replay time without losing the determinism in reproducing concurrency bugs. The cornerstone of our work is a redundancy criterion that characterizes the redundant computation in a buggy trace. Based on the redundancy criterion, we have developed two novel techniques to automatically identify and remove redundant threads and instructions in the bug reproduction execution. Our evaluation results with several real world concurrency bugs in large complex server programs demonstrate that LEAN is able to reduce the size, the number of threads, and the number of thread context switches of the replay trace by orders of magnitude, and accordingly greatly shorten the replay time.</p>", "authors": [{"name": "Jeff Huang", "author_profile_id": "81472647687", "affiliation": "The Hong Kong University of Science and Technology, Hong Kong, China", "person_id": "P3856112", "email_address": "smhuang@cse.ust.hk", "orcid_id": ""}, {"name": "Charles Zhang", "author_profile_id": "81435599989", "affiliation": "The Hong Kong University of Science and Technology, Hong Kong, China", "person_id": "P3856113", "email_address": "charlesz@cse.ust.hk", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384649", "year": "2012", "article_id": "2384649", "conference": "OOPSLA", "title": "LEAN: simplifying concurrency bug reproduction via replay-supported execution reduction", "url": "http://dl.acm.org/citation.cfm?id=2384649"}