{"article_publication_date": "10-19-2012", "fulltext": "\n Exploring Multi-Threaded Java Application Performance on Multicore Hardware Jennifer B. Sartor and \nLieven Eeckhout Ghent University, Belgium Abstract While there have been many studies of how to schedule \nap\u00adplications to take advantage of increasing numbers of cores in modern-day multicore processors, few \nhave focused on multi-threaded managed language applications which are prevalent from the embedded to \nthe server domain. Managed languages complicate performance studies because they have additional virtual \nmachine threads that collect garbage and dynamically compile, closely interacting with applica\u00adtion threads. \nFurther complexity is introduced as modern multicore machines have multiple sockets and dynamic fre\u00adquency \nscaling options, broadening opportunities to reduce both power and running time. In this paper, we explore \nthe performance of Java applica\u00adtions, studying how best to map application and virtual ma\u00adchine (JVM) \nthreads to a multicore, multi-socket environ\u00adment. We explore both the cost of separating JVM threads \nfrom application threads, and the opportunity to speed up or slow down the clock frequency of isolated \nthreads. We per\u00adform experiments with the multi-threaded DaCapo bench\u00admarks and pseudojbb2005 running \non the Jikes Research Virtual Machine, on a dual-socket, 8-core Intel Nehalem machine to reveal several \nnovel, and sometimes counter\u00adintuitive, .ndings. We believe these insights are a .rst but important step \ntowards understanding and optimizing man\u00adaged language performance on modern hardware. Categories and \nSubject Descriptors D3.4 [Programming Languages]: Processors Memory management (garbage collection); \nOptimization; Run-time environments General Terms Performance, Measurement Keywords Performance Analysis, \nMulticore, Managed Lan\u00adguages, Java Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 1. Introduction Multicore systems are here to stay. Since \nthe .rst multicore processor was released over a decade ago, today s proces\u00adsors implement up to a dozen \ncores, and each generation is expected to have an increasing number of cores because of Moore s law [19]. \nMulticore processors are abundant across all segments of the computer business, from hand-held de\u00advices \nsuch as smartphones and tablets, up to high-end sys\u00adtems. Today s servers even host multiple sockets, \neffectively creating systems with multiple tens of cores. Given the ubiquity of multi-threaded managed \nlanguage applications [22], now running on modern multicore hard\u00adware, their performance is critical \nto understand. While mul\u00adticore performance has been extensively studied for multi\u00adthreaded applications \nwritten in traditional programming languages, understanding the performance of applications written in \nmanaged languages such as Java has received lit\u00adtle attention. Part of the reason is the complexity of \nthese managed workloads. Java virtual machine (JVM) service threads such as garbage collection, pro.ling, \nand compila\u00adtion threads, interact with application threads in many com\u00adplex, non-deterministic ways. \nNot only can these threads stop the application, e.g., in order to collect a full heap, they also interact \nwith the application at the microarchitectural level, sharing hardware resources, such as cores, cache \nand off-chip bandwidth. Looking forward, optimizing performance while taking into account power and energy \nwill be even more impor\u00adtant. Handheld devices have to minimize total energy con\u00adsumption with given \nperformance goals, e.g., soft real-time deadlines. For high-end servers, the goal is typically to op\u00adtimize \nperformance while not exceeding a given power bud\u00adget. Several studies point out that with the end of \nDennard scaling [7] slowed supply voltage scaling it will no longer be possible to power on the entire \nprocessor all the time, a problem referred to as dark silicon [9]. Intel s Turbo Boost technology [15], \nwhich enables increasing clock fre\u00adquency for a limited number of cores and for a short duration of time, \ncould be viewed as a form of dark silicon. Understanding and optimizing managed application per\u00adformance \non multicore processors and systems is therefore a complicated endeavor. Some of the fundamental ques\u00adtions \nthat arise are: How many application and JVM service threads yield optimum performance? Should one initiate \nas many application and garbage collection threads as there are cores on the chip? If so, does this hold \ntrue for multi-socket systems as well? Further, how should one distribute the ap\u00adplication, garbage collection \nand compiler threads across the various cores and sockets in the system? In particular, in case of a \nmulti-socket system, should one schedule JVM ser\u00advice threads on the same socket as the application threads, \nand what is the performance penalty, if any, from of.oading JVM service threads to another socket? If \noptimizing per\u00adformance, but taking power into consideration, should one speed up all threads or just \nthe application threads? And how much does performance degrade by slowing down JVM ser\u00advice threads? \n In this paper, we provide insight into Java multi-threaded application performance on multicore, multi-socket \nhard\u00adware. We perform a comprehensive set of experiments us\u00ading a collection of multi-threaded DaCapo \nbenchmarks and pseudojbb2005 on a dual-socket 8-core Intel Nehalem sys\u00adtem with a current version of \nthe Jikes Research Virtual Ma\u00adchine. We vary the number of cores and sockets, the num\u00adber of application \nand garbage collection threads, clock fre\u00adquency, thread-to-core mapping and pinning, and heap size. \nWe are the .rst to extensively explore the space of multi\u00adthreaded Java applications on multicore multi-socket \nsys\u00adtems, and we reveal a number of new, and sometimes sur\u00adprising, conclusions: 1. Java application \nrunning time bene.ts signi.cantly from increasing core frequency. 2. When isolating JVM collector threads \nonto a separate socket from application threads, there is a cost: less than 20% of performance. However, \nisolating the compila\u00adtion thread is usually performance neutral, but for some benchmarks actually improves \noverall performance. 3. If power-constrained, lowering the frequency of JVM service threads costs a \nfraction of performance, but 3-5 times less than when scaling application threads. 4. Many benchmarks \nachieve good performance when all threads run on a single socket at the highest frequency, and the second \nsocket is kept idle. However, all but one benchmark have optimal performance when isolating the compilation \nthread and lowering its frequency. 5. For our benchmarks, the best performance running on one socket \nis usually obtained by keeping the number of application threads equal to the number of collection threads, \nalso equal to the number of cores. With two sock\u00adets, it is better to pair application and collector \nthreads together (unless there is a lot of inter-thread communi\u00adcation), and almost all benchmarks bene.t \nfrom increas\u00ading the number of application threads to be equal to the number of cores while setting the \nnumber of collection threads to half of that.  6. Pinning application and collector threads to cores \ndoes not always improve performance, and some benchmarks bene.t from letting threads migrate. 7. During \nstartup time, the cost of isolating JVM service threads to another socket is less than at steady-state \ntime, while lowering their frequency still deteriorates perfor\u00admance several times less than that of \nlowering the appli\u00adcation threads.  Analyzing Java applications on modern multicore, multi\u00adsocket hardware \nreveals that it is dif.cult to follow a set of rules that will lead to optimal performance for all applica\u00adtions. \nHowever, our results reveal insights that will assist in identifying the right number and scheduling \nof applica\u00adtion and JVM service threads that will preserve performance while saving the critical resource \nof power in future systems. 2. Motivation As managed languages have become ubiquitous from the embedded \nto the server market and in between, it is im\u00adportant to analyze their performance on multicore hard\u00adware. \nBecause Java applications run on top of a virtual runtime environment, which includes its own management \nthreads and introduces non-determinism for applications, choosing the correct parameters and con.gurations \nto bal\u00adance performance and energy is complex. Modern machines also introduce extra runtime parameters \nto study, offering multiple sockets and frequency (and voltage) scaling op\u00adtions. Researchers have been \nstudying the performance and power consumption for managed languages as part of prior work [4 6, 10, \n11, 13], demonstrating that it is an impor\u00adtant problem. However, studies in prior work were limited \nto single-socket systems, single-threaded Java applications, and/or isolated JVM service performance, \nnot end-to-end performance. (See Section 5 for a more detailed description of prior work.) In this work \nwe expand the exploration space consider\u00adably. We consider multi-threaded applications written in a managed \nprogramming language, Java. We focus on multi\u00adcore, multi-socket hardware. We also aim at understanding \nhow design choices in terms of thread-mapping, frequency scaling, thread-pairing, pinning, and more affect \nend-to-end application performance. More speci.cally, there are a num\u00adber of fundamental questions related \nto Java application per\u00adformance on multicore multi-socket systems that merit fur\u00adther investigation. \nNumber of application and JVM service threads. First, we would like to explore the number of application \nand col\u00adlection threads given a particular number of cores. Default methodology sets the number of application \nand collection threads equal to the number of cores; however, this does not necessarily take into account \nthe sharing of hardware re\u00adsources between these and other JVM threads, nor does it take into account \nmemory access rami.cations of commu\u00adnicating across sockets. Although current practice is to have thread-local \nallocation, collection threads are not tied to any particular application thread or core (in our Java \nvirtual ma\u00adchine), and can touch many areas of memory and incur inter\u00adthread synchronization in order \nto reclaim space. Other JVM threads such as those that perform dynamic compilation and on-stack replacement, \nalso interact with application threads and share hardware resources. It is unclear whether hardware resource \nsharing between JVM and application threads actu\u00adally helps or hurts performance, because of either better \ndata locality or resource contention.  Thread-to-core/socket mapping. Inherent in the choice of the \nnumber of threads is where to place these threads on a multicore multi-socket system. Current systems \nlargely leave thread scheduling up to the operating system which can preempt and context switch threads \nwhen necessary. Intuition suggests we should use all cores and maximize parallelism. Further, benchmarks \nwith large working sets could bene.t from the aggregate last-level cache capacity across sockets. However, \nas previously mentioned, moving data between sockets increases inter-thread communication, leading to \nmore memory accesses and coherence traf.c and potentially higher synchronization costs. Frequency scaling \nand power implications. Because mod\u00adern machines include the ability to dynamically scale the core frequency, \nanother axis to explore is the rami.cations of scaling on total application performance. Because power \nis a .rst-order concern, and will become even more con\u00adstrained in future systems, it might be worth \npaying the price of somewhat reduced performance for power savings. Hence, we need to analyze both the \ncost of moving threads to another core or socket and then additionally, the cost of lowering the clock \nspeed. Because JVM service threads do not run constantly, we surmise that they should be amenable to \nrunning at scaled-down frequencies without hurting appli\u00adcation performance drastically. However, both \ncompilation and collection can be on the application critical path if they need to stop the workload \nto perform on-stack replacement or collect garbage when the heap is full. Garbage collection threads \nhave to trace all live heap pointers to identify dead data to reclaim, and could thus suffer from higher \nmemory communication if moved to a separate socket. Analyzing this challenging experimental space will \nshed light on the rami.cations of the many con.guration and scheduling decisions on overall goals, whether \ntime or power. We explore the performance of Java workloads, while keeping power in mind, along these \nmany orthogonal axes on modern multicore, multi-socket hardware to provide new, sometimes surprising, \n.ndings and insights. 3. Experimental Setup Before presenting the key results obtained from this study, \nwe describe our experimental setup of running Java work\u00adloads on multicore, multi-socket hardware. Figure \n1. Diagram of our two-socket, eight-core Intel Ne\u00adhalem experimental machine, with memory hierarchy. \n3.1 Hardware Platform The hardware platform considered in this study is an Intel Nehalem based system, \nmore speci.cally, an IBM x3650 M2 with two Intel Xeon X5570 processors that are 45-nm West\u00admere chips \nwith 4 cores each, see Figure 1. The two sockets are connected to each other through QuickPath Interconnect \nTechnology; and feature a 1333 MHz front-side bus. Each core has a private L1 with 32 KB for data and \n32 KB for instructions. The uni.ed 256 KB L2 cache is private, and the 8 MB L3 cache is shared across \nall four cores on each socket. The machine has a total main memory capacity of 14 GB. We can use dynamic \nfrequency scaling (DFS) on the Nehalem to vary core frequencies between 1.596 GHz and 3.059 GHz. However, \non this machine, it is only possi\u00adble to change the frequency at a socket-level. Therefore, all four \ncores on each socket run at the same frequency, and we are unable to evaluate more than two different \nfrequencies simultaneously. For all of our experiments, we set the fre\u00adquency only to the lowest and \nhighest extremes to test the limits of performance. We turn hyperthreading off in all of our experiments. \n 3.2 Benchmarks and JVM Methodology We perform experiments with the Jikes Research Virtual Machine (RVM), \nhaving obtained the source from the Mer\u00adcurial repository in December, 20111. We updated from a stable \nrelease of Jikes because of a revamping of their ex\u00adperimental methodology code. We modi.ed Jikes slightly \nto identify and control JVM service thread placement for the purpose of frequency scaling. By default, \nwe pin applica\u00adtion and garbage collection threads to a particular core, and other JVM service threads \nare placed on the socket with ap\u00adplication threads, but not pinned to a speci.c core. We also perform \nexperiments without pinning application and JVM threads for comparison, and these results will be discussed \nin Section 4.5. In addition to threads that perform garbage col\u00ad 1 changeset 10414: 5c59ac91ff06  Benchmark \nSuite avrora DaCapo Bach 50 54 lusearch DaCapo Bach 34 8152 lusearch-.x DaCapo Bach 34 1071 pmd DaCapo \nBach 49 385 sun.ow DaCapo Bach 54 1832 xalan DaCapo Bach 54 1104 pjbb2005 SPECjbb2005 200 1930 Table \n1. Benchmark characteristics. lection and compilation, other JVM service threads include a thread to \nperform .nalization and do timing, many threads to coordinate on-stack-replacement (called organizer \nthreads) of dynamically recompiled method code, and a main thread that is the .rst to execute the application \ns main method. Jikes does not perform compilation in parallel, so there is only one compilation thread. \nWe perform experiments on the multi-threaded Java ap\u00adplications in the DaCapo benchmark suite [4] version \n9.12\u00adbach that successfully run on our version of Jikes RVM. These include avrora, lusearch, pmd, sun.ow, \nand xalan. We also perform experiments on pseudojbb2005 [3, 4], a vari\u00adant of SPECjbb2005 with a .xed \nnumber of warehouses that allows for measuring the execution time. These bench\u00admarks are real-world open-source \napplications. Table 1 de\u00adtails which suite our benchmarks came from, their mini\u00admum heap size in MB and \ntheir total allocation in MB [25]. We control the number of application and collection threads with command-line \n.ags (which are by default both set to four). Our experiments use the best-performing, default gen\u00aderational \nImmix heap con.guration [2]. Although there are garbage collection threads running in parallel, the collector \nis stop-the-world meaning that the application is stopped during both nursery and whole-heap collections. \nWe per\u00adformed all experiments at 1.5, 2, and 3 times the minimum heap size for each benchmark. For conciseness, \nwe present some graphs only with 2 times the minimum heap size. We perform 10 invocations of each benchmark, \nand results are presented as the average of these 10 runs, along with 90% con.dence intervals. For steady-state \nexecution, which is the default reported throughout results, we measure the 15th it\u00aderation. Although \nthere is non-determinism inherent in us\u00ading the just-in-time adaptive compilation system that detects \nhot methods and dynamically re-compiles them to higher optimization levels, measuring the 15th iteration \ngives time for the benchmark to reach more steady-state behavior. For comparison, we present two separate \nsections of results at startup time, which are gathered during the .rst iteration of each benchmark. \nThe compilation and on-stack-replacement threads are most active during startup time, and thus we in\u00adclude \nresults for complete evaluation. For targeted further analysis, we perform experiments with a version \nof the lusearch benchmark updated with a bug .x. The original lusearch derives from the 2.4.1 stable \nrelease of Apache Lucene, and has a very high allocation rate. This version included a bug that when \n.xed, only con\u00additionally allocates a large data structure and cuts allocation by a factor of eight [25]. \nFor additional experiments, we use a version updated with revision r803664 of Lucene that con\u00adtains the \nbug .x [20], and we call this lusearch-.x . We include results in all graphs for lusearch-.x ; however, \nwe still include the original lusearch because it is the of.cial benchmark in the current release of \nthe DaCapo suite.  3.3 Methodological Design Recent hardware trends point to a need for power savings \nand capping while delivering high levels of performance. Al\u00adready, recent research [6] has explored the \neffect of scaling the frequency of service threads on power and energy, ex\u00adplicitly targeting a heterogeneous \nmulticore processor with big and small cores. While this is an important step to inform future hardware \ndesign, it is also important to focus on evalu\u00adating how to obtain the best performance from current \nmulti\u00adsocket systems. Therefore, we measure end-to-end perfor\u00admance for multi-threaded workloads, and \nstudy the effect of both isolating and scaling JVM and application threads. Because we are unable to \nperform per-core DFS in our hardware setup, we employ a two-step process. We .rst mi\u00adgrate certain threads \nto a separate socket and evaluate the cost of isolating threads on running time. Subsequently, we quantify \nand report relative performance differences when comparing frequency settings after isolating threads. \nBy do\u00ading so, we separate the effect of scaling frequency from iso\u00adlating threads onto another socket, \nand hence provide insight into how scaling affects end-to-end performance in multi\u00adcore systems. The \npercentage of time spent in JVM threads is less than in the application threads. For this reason, we \nexpect scal\u00ading down the frequency of JVM threads to have a smaller impact on end-to-end performance \nthan scaling application threads. However, there is close interaction between applica\u00adtion and JVM threads \nwhich makes this an interest space to explore. In particular, because we use a stop-the-world col\u00adlector, \napplication threads are stopped during garbage col\u00adlection, hence slowing down collection only by a small \nfrac\u00adtion may have signi.cant impact on overall application per\u00adformance. Likewise, just-in-time compilation \nthreads gener\u00adate optimized code during runtime, and slowing these down may cause the application to \nrun unoptimized code for a longer period of time, which may also have signi.cant im\u00adpact on overall performance. \nFurthermore, the interaction between JVM and application threads because of updating code and managing \ndata leads to unpredictable interference in the memory subsystem. The interactions between appli\u00adcation \nand JVM threads are diverse and complex, and hence, we perform detailed experiments to evaluate the impact \nof both pairing and scaling the frequency of application and JVM threads.  4. Results We now present \nour experimental results running multi\u00adthreaded Java applications on modern multicore, multi\u00adsocket hardware, \nin which we vary the number of cores and sockets, the number of application and garbage collection threads, \nclock frequency, thread-to-core mapping and pin\u00adning, and heap size. Because of the complexity of the \nspace, we break up the analysis in a number of comprehensive steps. We .rst discuss the general effect \nfrequency scaling has on application execution time. We then analyze the cost of isolating some JVM threads \nto a separate socket. After isolating JVM threads, we analyze the cost of scaling the frequency of isolated \nJVM service threads from the high\u00adest to the lowest value on end-to-end performance. Because we see a \nsigni.cant cost from isolating garbage collection threads, we then perform experiments that pair application \nand collection threads, but put some pairs on each socket. Finally, we analyze the effect that thread \npinning has on per\u00adformance, and the effect of varying both the number of ap\u00adplication and collector \nthreads while running on one socket. 4.1 The Effect of Scaling Frequency Although a detailed power study \nis beyond the scope of this paper, we .rst wanted to explore the effect core fre\u00adquency has on application \nperformance. Figure 2 presents the speedup as a percentage of execution time of boost\u00ading core frequency \nfrom the lowest to the highest, when all threads, including four collector threads, run on only one socket. \nResults are for all six benchmarks for our range of three heap sizes. Finding 1. Java workloads bene.t \nsigni.cantly from scaling up the clock frequency. Doubling clock frequency leads to between 27% and 50% \nperformance improvement. Results are not sensitive to heap size. We see that doubling the clock speed \ndoes not lead to doubling the performance improvement, which would be 100%. Our benchmarks fall short \nof perfect scaling, most likely because of inter-thread synchronization and memory intensity. However, \nas we will see in all of our results, core frequency is one of the most signi.cant factors in determin\u00ading, \nand improving, application time. Below, we will show that scaling down JVM thread frequency does not \naffect per\u00adformance as drastically as for application threads.  4.2 The Cost of Isolation We now analyze \nthe performance penalty inherent to iso\u00adlating JVM service threads to another socket. We investi\u00adgate \nthis cost for four collector threads, for the compilation thread, for all JVM service threads except \ncollector threads, Figure 2. Percent execution time improvement when boost\u00ading frequency from lowest \nto highest on one socket. and then all JVM threads together. The motivation for this experiment is twofold. \nFirst, in order to investigate the fea\u00adsibility of scaling down the frequency of only JVM service threads, \nwe must .rst isolate threads onto a separate socket, because we are only able to scale frequency at the \nsocket\u00adlevel. Second, we want to study how isolating JVM service threads to another socket hurts (through \nreduced data local\u00adity) or helps performance (by getting off the application s critical path). Isolating \ngarbage collection threads. Figures 3 through 6 show the cost of isolating some JVM threads to a second \nsocket, as compared with the execution times when run\u00adning all threads on one socket. The graphs present \nsteady\u00adstate performance differences for our three heap sizes run\u00adning at both the highest and lowest \ncore frequencies. While some benchmarks performance varies with heap size, we see larger performance \ndifferences between high and low core frequencies. Finding 2. Isolating garbage collection threads to \na sep\u00adarate socket leads to a small performance degradation (no more than 17%) for most benchmarks because \nof increased latency between sockets; however, one benchmark substan\u00adtially bene.ts (up to 66%) from \nincreased cache capacity. Figure 3 shows that all but one application suffer from isolating four collection \nthreads, due to more data commu\u00adnication between sockets. For all but lusearch, the degrada\u00adtion is less \nthan 5% for the lowest frequency, and less than 17% for the highest. Lusearch is an outlier that particularly \nsuffers from both increasing the number of collector threads (as shown in Figure 24) and from isolating \nthose threads to another socket, with over 40% degradation in performance.  Figure 3. Percent execution \ntime improvement of isolating four collector threads to a second socket. Figure 4. Percent execution \ntime improvement of isolating the compiler thread to a second socket. Figure 3 shows that when lusearch \ndoes not suffer from huge amounts of allocation, lusearch-.x s cost of moving collec\u00adtor threads to another \nsocket lowers to be in line with other benchmark trends. Interestingly, avrora bene.ts from isolating \nthe collector threads to another socket. Performance improves by 36% and 66% at the lowest and highest \nfrequencies, respectively. However, it should be noted that the con.dence intervals for avrora are large, \nand thus performance greatly varies from run to run. For avrora, running all application and collector \nthreads on one socket makes the threads contend more for the cache, and avrora bene.ts from the increased \ncache capacity of two sockets. Analyzing hardware perfor\u00admance counters revealed fewer L3 misses when \nthe collector threads were isolated. We will see later that avrora is particu\u00adlarly sensitive to application-thread \nto core mapping because application threads do not have uniform behavior, and share data (see analysis \nin Sections 4.4 and 4.5). Figure 5. Percent execution time improvement of isolating all JVM non-collector \nthreads to a second socket. Isolating the JIT compilation thread. Figure 4 shows the effect of isolating \njust the compilation thread, with four collector and four application threads that are pinned to the \n.rst socket. Finding 3. Isolating the compilation thread to a sepa\u00adrate socket leads to a either a performance \nboost, or is performance-neutral. Only avrora s performance at high frequencies suffers because of increased \nlatency between sockets. Four benchmarks have very little change to performance when the compiler thread \nis isolated to a separate socket dur\u00ading steady-state. Lusearch and pjbb2005 see a performance win, especially \nat the higher frequency, by isolating the com\u00adpiler away from other application and collector threads. \nOnly avrora sees a performance hit, up to 100% with the highest frequency (although con.dence intervals \nare large). It is pos\u00adsible that when the application is sped up, it is more sensi\u00adtive to the compiler \nbeing separated from other JVM threads such as those that perform on-stack-replacement. When an\u00adalyzing \nstartup time, indeed avrora is the only benchmark for which performance degrades when isolating the compiler \nor on-stack-replacement threads (see further analysis in this section regarding Figures 8 and 9). Isolating \nall JVM service threads. Figure 5 shows the cost of isolating all JVM service threads except for the \nfour collection threads (or nonGC) which remain on the .rst socket with application threads. Figure 6 \nthen shows the impact to performance when all JVM threads are isolated onto another socket. Finding 4. \nIsolating all JVM service threads to a separate socket leads to larger performance degradation for a \nfew benchmarks (only one suffers more than 22% degradation, and only at the highest frequency), while \nothers are only slightly negatively affected by of.oading computation and memory to another socket. \n  Figure 6. Percent execution time improvement of isolating all JVM threads to a second socket. Interestingly, \nwhile avrora bene.ted from collector thread isolation, performance severely degrades when isolating all \nnonGC threads. At the higher frequency, performance degra\u00addation goes down to 107%! Comparing this to \nisolating all JVM service threads, we see that avrora still suffers, but less, with a maximum of only \n88% degradation. Avrora is more sensitive to frequency changes in combination with isola\u00adtion than other \nbenchmarks, and is more sensitive to nonGC threads being isolated, probably because of memory interac\u00adtion. \nThe bene.t avrora obtained from of.oading the collec\u00adtor memory activity to another socket is shown in \nthe differ\u00adence between the nonGC and JVM isolation graphs. Besides avrora, other benchmarks show smaller \ndegra\u00addations to performance when isolating nonGC, or all JVM service threads. For nonGC thread isolation, \npmd has up to 12% performance loss, but other benchmarks either slightly improve or slightly degrade \nperformance. When isolating all JVM service threads, lusearch can suffer over 40%, but the .xed version \nof lusearch lowers this cost to less than 20%. In general, isolating all JVM threads seems to have a \nperfor\u00admance impact that is the sum of isolating collection threads and isolating nonGC threads (Figures \n3 and 5), which is overall slightly degraded. Pmd can suffer as much as 22% when isolating JVM threads, \nwhich we investigate further by isolating certain JVM service threads. We isolate only the main thread, \nwhich calls the application s main method, and then only the organizer thread, which performs on-stack \nreplacement. Results are shown in Figure 7 on the left-most bars, and we see that isolating each of these \ntwo JVM threads makes pmd s performance suffer, more so at the higher fre\u00adquency. Overall, we see that \nbenchmarks respond differently to the isolation of particular JVM service threads. While avrora bene.ts \nfrom separating collection threads, all other bench\u00admarks suffer up to 17%. However, other benchmarks \nbene.t slightly or are neutral to isolating the compilation thread. All benchmarks but avrora do not \nsuffer much from isolat-Figure 7. Percent execution time improvement of isolating and scaling the frequency \nof certain JVM threads for pmd and pjbb2005, at 1.5 times the minimum heap size. Figure 8. Percent execution \ntime improvement of isolating the compiler thread to a second socket at startup time. ing nonGC threads. \nWhile the cost to isolate JVM service threads is not insigni.cant, it is not unreasonable if the need \nfor power savings is paramount. Isolation during startup. Although application perfor\u00admance matters most \nduring steady-state execution, the com\u00adpilation thread in particular is most active during JVM startup \ntime. Thus, for completeness, we analyze the cost of isolating the compiler thread also at startup time. \nFinding 5. During startup time, isolating only the compi\u00adlation thread has little impact on performance, \nbut isolat\u00ading all nonGC threads actually improves performance for all benchmarks but avrora. In general, \nthe performance of isolating JVM threads to another socket is better, with some performance improvements, \nat startup time than at steady\u00adstate time. Figure 8 presents the cost of isolating the compiler thread \nat startup time (relative to a baseline run also at startup time), during the .rst iteration of a benchmark \nwhen the compiler Figure 9. Percent execution time improvement of isolat\u00ading the organizer (on-stack \nreplacement) thread to a second socket at startup time.   is most active. We see all benchmarks but \navrora have neg\u00adligible impact on performance when the compiler thread is placed onto another socket. \nAvrora, which has high varia\u00adtion between different runs, sometimes sees degradation of performance and \nsometimes improvement. Although during steady-state, in Figure 4, lusearch and pjbb2005, see an im\u00adprovement \nin running time due to separating the compila\u00adtion thread, at startup, there is not a big impact on perfor\u00admance. \nThere is more communication and memory sharing between the JVM and application threads during actual \ncom\u00adpilation activity that does not exist during steady-state, and thus dampens the bene.t of using extra \nCPU and memory resources during startup time. However, with avrora during startup, we do not see the \nlarge hit to performance at high frequencies that we did during steady-state, therefore con\u00adcluding that \navrora is less sensitive to frequency changes at startup time. In Figures 10 and 11, we redo the experiments \nisolating nonGC and JVM threads at startup time. At startup time, isolating all JVM threads except the \ncollector threads leads to performance improvements for all but avrora, including up to 16% for pjbb2005. \nThis contrasts with the neutral or slightly negative (especially for avrora and pmd) results we saw at \nsteady-state time. Apparently during startup, the nonGC threads bene.t from the extra resources of another \nsocket, and do not suffer from extra communication be\u00adtween threads via memory. We see similar trends \nfor all JVM threads, which, at startup time, suffer less from isolation than at steady-state time, even \nleading to performance improve\u00adments for xalan and pjbb2005. Because there is a noticable difference \nbetween the startup performance of isolating just the compiler thread, and all nonGC JVM threads, we \nperform experiments also isolating only the organizer threads that perform on-stack\u00adreplacement. Figure \n9 shows that the organizer threads play the main part in the performance of nonGC threads. When organizer \nthreads are isolated during startup time, all bench\u00admarks but avrora see a performance bene.t, following \nvery similar trends to Figure 10, with pjbb2005 improving only slightly less. Although organizer threads \ninteract with appli\u00adcation memory, they bene.t from being isolated to another socket with extra resources. \nIn fact, the organizer threads have a larger impact on performance than the singular com\u00adpilation thread \nitself. The difference between both avrora and pjbb2005 s performance when isolating only the organizer \nthread and isolating all nonGC threads is due to the main thread (see pjbb2005 in Figure 7), and other \nJVM service thread interaction.  4.3 Analyzing Frequency Scaling After exploring the cost of isolating \nJVM threads, we now analyze the cost of scaling each socket s clock frequency from the highest to the \nlowest in regards to execution time. In Figure 12 to 15, we compare against a baseline of separat\u00ading \nsome JVM threads to a second socket, with both sockets at the highest frequency. The left groupings of \nbars compare a con.guration lowering only the isolated JVM threads fre\u00adquency, while the right groupings \ncompare lowering only the application and non-isolated threads frequency. We present all results at all \nthree heap sizes for all benchmarks, but note that there is little heap-size variation.  Finding 6. \nOn average, lowering the frequency of collector threads does degrade performance (usually less than 20%), \nbut degrades about .ve times less than lowering application thread frequency. Figure 12 compares scaling \ndown the frequency of four collector threads in the left three bar groupings versus ap\u00adplication threads \non the right. Although collector threads can be on the application critical path because they force the \napplication to pause during collection, collector threads do not run all the time, and thus they are \namenable to be\u00ading scaled down for more power-conscience environments. Maximally, avrora performance \ndegrades 69% for scaling collector threads (with large con.dence intervals), with the next highest benchmark \ndegradation at 20%. After scaling application threads, we see avrora s performance can de\u00adgrade up to \n315%! Finding 7. Lowering the core frequency for the isolated compiler thread affects performance very \nlittle, while appli\u00adcation performance suffers greatly. Figure 13 shows that lowering the core frequency \nfor only the compiler thread does not affect steady-state performance on average. In Figure 16, we show \nthat at startup time, the compilation thread is also unaffected by lowering frequency. In comparison, \nbenchmark application threads can degrade by as much as 100% when we scale down frequency at steady-state \ntime. Interestingly, avrora is the only applica- Figure 13. Percent execution time improvement when low\u00adering \nfrequency from highest to lowest, either compiler thread, or application (plus the rest) socket. tion \nthat does not see a performance degradation when the application and other threads are scaled down together. \nFinding 8. If worrying about a power budget, scaling down the frequency of JVM threads, while costing \nsome perfor\u00admance (usually less than 20%), has a much more reasonable effect on overall execution time \nas compared to scaling ap\u00adplication threads, which can take twice the running time. Looking at the cost \nof scaling all but collector threads in Figure 14, and scaling all JVM service threads in Fig\u00adure 15, \nwe see that sun.ow s application threads suffer the most, more than 90%, from running at a lower clock \nspeed, while JVM threads performance degrades less than 30% for all benchmarks. It is interesting to \nnote the change in per\u00adformance for scaling pjbb2005 s application threads down, between the con.guration \nwhere the compilation thread is isolated (above 80% in Figure 13), and when nonGC threads are isolated \n(less than 20%). We see similar, but less drastic, trends for pmd. Both benchmarks see more of a performance \nhit when isolated nonGC threads frequency is scaled down on the left in Figure 14. The difference in \nperformance is due to the JVM main service thread, as analyzed with perfor\u00admance counters, which is grouped \nwith the application when the compiler is isolated and with nonGC threads when they are isolated. Figure \n7 shows that the cost of lowering the fre\u00adquency of the isolated main JVM thread is quite signi.cant \nfor pmd and pjbb2005.  On average, scaling JVM threads degrades performance by around 11% in comparison \nwith 50% for application threads. As we see in Figure 15, lusearch-.x, pmd, sun\u00ad.ow, and xalan suffer \nthe most from lowering application frequency, which are the same benchmarks that bene.tted the most from \nboosting frequency in Figure 2. On the other hand, lusearch, pmd, and pjbb2005 suffer the most for scal\u00ading \ndown JVM threads. Avrora, while having large varia\u00adtion in results, does not suffer as much degradation \nfor either the application or the JVM threads when their frequency is scaled down. Frequency scaling \nduring startup. Here, we study the ef\u00adfects of frequency scaling during startup time, and juxtapose that \nwith the previous results for steady-state performance. Finding 9. Scaling the frequency of JVM threads \nat startup time follows similar trends as scaling at steady-state time. During startup, lowering the \nfrequency of JVM threads de\u00adgrades performance three times less than when scaling down application thread \nfrequency. Figure 16 shows the change in execution time when we scale down the frequency, from highest \nto lowest, of the iso\u00adlated compilation thread versus all other threads scaled down on the other socket \nat startup time. This graph is very simi\u00adlar with Figure 13, showing that although avrora experiences \nFigure 16. Percent execution time improvement when low\u00adering frequency from highest to lowest, either \ncompiler thread, or application (plus the rest) socket, at startup time. performance variation, all \nother benchmarks are unaffected by scaling compilation thread frequency, and suffer heavily (around 80% \nor more) when the other socket s frequency is scaled down. When scaling the frequency of nonGC and all \nJVM threads, Figures 17 and 18 show that startup time has very similar effects as during steady-state \nexecution. When plac\u00ading the application and collector threads at a lower fre\u00adquency, lusearch and pjbb2005 \nsee a slightly worse degrada\u00adtion for startup time as compared with steady-state time in Figures 14 and \nFigure 15. These benchmarks might be more sensitive to changes at startup time because of higher levels \nof dynamic compilation. Avrora alone seems to be largely unaffected by frequency scaling at startup time. \nOverall, lowering the frequency of all JVM threads at startup time degrades performance on average by \n16%, and with appli\u00adcation threads 47%. While these degradations are slightly less than steady-state \ntime s degradations of 10% and 50%, respectively, we observe the same trends.   Overall best performance. \nFigure 4.3 compares overall execution times of all benchmarks as we move from lower to higher frequencies \n(left to right) and explore isolating various JVM threads, and scaling the frequency of either the isolated \nor application and other threads. These results present runs for two times the minimum heap size. Whereas \nprevious graphs gave a percentage improvement in execution time over a baseline run, we present here \nrunning times normalized to the minimum time over this set of experiments (so lower is better) in order \nto analyze trends. Finding 10. When power-constrained in a multi-socket en\u00advironment, it is better to \neither keep application and JVM service threads on one socket, and power down the other socket(s), or \nto isolate the compilation thread onto the sec\u00adond socket and lower its frequency. Figure 4.3 s left \ngrouping shows performance when all sockets are run at the low frequency. Moving right to the second \ngrouping, we boost the frequency of the application and non-isolated threads. Performance generally improves \n(closer to one on the graph), but not always for avrora. Al\u00admost universally, going from the second to \nthe third group\u00ading, now keeping the .rst socket at the lowest frequency and boosting only the isolated \nJVM threads on the second socket, running time increases. Avrora shows similar trends, but unlike other \nbenchmarks, achieves the lowest perfor\u00admance when collection threads are separated from applica\u00adtion \nthreads and application threads are boosted. We sur\u00admise avrora has a lot of inter-application communication \nand collection threads interfere with cache and bandwidth resources. Finally, the last grouping shows \noverall improve\u00adments when we boost the frequency of both sockets. For all benchmarks but avrora, the \nfastest run times come from ei\u00adther the con.guration where the compilation thread is iso\u00adlated and the \nother socket is boosted (HiApp-LoComp), or the con.guration also with the compilation thread isolated, \nbut both sockets boosted (IsolateComp-Hi). For benchmarks except for avrora, lusearch, and pjbb2005, \nthe con.guration with all threads running together on one socket at the highest frequency (1Socket-Hi) \nperforms almost optimally as well.  4.4 Pairing Application and Collector Threads Because our benchmarks \nhave up to 17% performance degradation from isolating collector threads to another socket, here we explore \nthe effect of splitting work between sockets without separating all application from collection threads. \nIn this section, we pair an application and a collection thread and place half of the pairs on each socket. \nFigure 20 presents results for running two application and two collection threads on each socket, with \nall application and collection threads pinned to cores. The graph shows both running all threads on one \nsocket, and isolating collection threads to a second socket versus this paired-and-divided con.guration. \nWe present results at high and low frequencies for two times the minimum heap size. Finding 11. Other \nthan the anomalous avrora benchmark which likely has high levels of inter-thread communication, applications \nbene.t more from pairing collector threads to\u00adgether with application threads while running on multiple \nsockets, with performance comparable to running on only one socket.  Figure 20. Percent execution time \nimprovement going from all threads on one socket and from isolating four collector threads on a second \nsocket to pairing application and collec\u00adtor threads and placing half each socket, at twice the mini\u00admum \nheap size. In comparison with all threads on one socket (left bars), lusearch, lusearch-.x, pmd, sun.ow, \nand xalan have almost the same running time when pairing and dividing applica\u00adtion and collection threads. \nAt the highest frequency, sun\u00ad.ow degrades performance by 9% because of more com\u00admunication through memory, \nwhile pjbb2005 has improved performance by 15% by using twice the last-level cache as with one socket. \nUnfortunately, although avrora bene.t\u00adted from separating collector threads, dividing application threads \ncosts up to 220% of performance. Upon further in\u00advestigation, unhalted cycle and L3 miss performance \ncoun\u00adters on the Nehalem reveal that avrora s application threads have non-uniform behavior. Some run \nmany more cycles and incur more last-level cache misses than others. It is also pos\u00adsible avrora s application \nthreads have signi.cant data shar\u00ading, because the application threads suffer many more L3 misses when \ndivided between sockets. In comparison with isolating all collection threads from application threads \n(right bars), benchmarks mostly see pos\u00aditive impacts to performance when pairing and dividing ap\u00adplication \nand collection threads. Again, avrora suffers heav\u00adily from dividing up application threads this time \nby max\u00adimally 460%, albeit with large con.dence intervals. Other benchmarks either improve (lusearch \nby 29% and pjbb2005 by 17%) or maintain performance by preserving some local\u00adity between application \nand collector threads. We explore increasing the number of application and col\u00adlection threads using \nthe same paired-and-divided method\u00adology. Using four application and four collector threads as a baseline, \nFigure 21 presents performance improvements for two times the heap size. We .rst increase the number \nof application threads to eight while keeping four collector threads, and then increase both to eight \nthreads. Finding 12. When running on two sockets, surprisingly, it is not always recommended to set the \nnumber of threads equal to the number of cores. All but one benchmark bene.t from setting the number \nof application thread to the number of cores, but only two of our benchmarks bene.t from boosting the \nnumber of collection threads above four. The graph shows that almost all benchmarks improve performance \nby having as many application threads as cores (eight), but few bene.t from increasing to eight collec\u00adtion \nthreads. Speci.cally, avrora and sun.ow bene.t from using more application threads, and are less sensitive \nto the number of collector threads, improving performance by 95% and 34-40%, respectively. Other benchmarks \nde\u00adgrade performance when going from four to eight collector threads. Lusearch s performance degrades \nsigni.cantly (up to 295%), but with the allocation bug-.x, lusearch-.x does not experience the large \ndegradation when going to eight collection threads. Pseudojbb2005 alone has signi.cant per\u00adformance degradation \nwith more application threads, around 100%. This degradation could be due to not enough work to keep \napplication threads busy, and increases in thread\u00adsynchronization time (particularly with the main thread). \nIn Section 4.6 we analyze the effect of varying the number of application and collection threads on one \nsocket, but the optimal con.guration highly depends on the benchmark.  4.5 The Effect of Pinning Because \nall previous experiments were performed while pin\u00adning application and collection threads to cores, this \nsection explores the effects of removing some thread-pinning on per\u00adformance. The experiments presented \nin this section have all threads placed on one socket and are for two times the min\u00adimum heap size. Figure \n22 compares pinning only the col\u00adlector thread, only the application threads, and pinning no threads \nagainst a baseline of pinning all application and col\u00adlector threads.  Finding 13. Most benchmarks \nare neutral to application and collector thread pinning, but a few bene.t from thread migration. Apart \nfrom avrora, Figure 22 shows that other bench\u00admarks are mostly insensitive to pinning, with small exeuc\u00adtion \ntime improvements from not pinning the collector (PinApp). Lusearch-.x and xalan perform slightly worse \nwhen not pinning the application (PinGC). Pjbb2005 alone clearly bene.ts from thread movement, and more \nat the higher frequency (38%) It is possible that, like avrora, pjbb2005 has more sharing between application \nthreads, and because collection threads are not assigned work based on a particular application thread, \nthey, too, bene.t from moving to take advantage of sharing at higher levels of the cache. Avrora is again \nan anomaly, having signi.cant bene.t, up to 89%, from not pinning application threads. We have surmised \nthat avrora application threads could have a lot of data sharing, and have discussed that they have non\u00aduniform \nbehavior. We performed an experiment to test if avrora s large performance discrepancy between pinning \nand not pinning threads has to do with using only one of the two memory controllers on the socket. We \nre-executed the baseline experiment, pinning all application and collection threads on one socket, but \nthis time starting pinning at core one instead of core zero (still subsequently using round\u00adrobin scheduling). \nWith this small change, as compared with starting at core zero, avrora s execution time improves by 37 \nand 46% at the low and high frequencies, respectively. This surprising result reveals that avrora is \nparticularly sensitive to how application threads are mapped to cores because perturbations radically \nchange memory subsystem behavior. Other benchmarks did not suffer from the same artifact. Therefore, \navrora is a particularly anomalous benchmark Figure 23. Percent execution time improvement when vary\u00ading \nthe number of application and collector threads, on one socket at twice the minimum heap size. that prefers \ngrouping application threads on the same socket and letting them migrate between cores, and separating \nonly collection threads to another socket.  4.6 Changing the Number of Collector and Application Threads \nFinally, we explore changing the number of application and collector threads, limiting experiments to \none socket. Our baseline experiment is always with four application and four collection threads. We .rst \nanalyze the effect of decreasing the number of collector threads while keeping the applica\u00adtion threads \nat four. Then, keeping the same number of ap\u00adplication and collector threads, we vary the number between \nthree and .ve. And .nally, we compare increasing the num\u00adber of application threads from four to eight \nwhile holding the collector threads at four. All results are presented in Fig\u00adure 23 at two times the \nminimum heap size. Finding 14. When running on only one socket, the per\u00adformance sweet-spot is setting \nthe number of application threads and the number of collection threads equal to the number of cores. \nFigure 23 shows that almost all benchmarks suffer slightly or remain neutral from lowering the number \nof collection threads from four to one, with four application threads. Avrora in particular has degraded \nperformance. Only luse\u00adarch bene.ts, but this is due to its excessive allocation as lusearch-.x is performance-neutral. \nBecause parallel col\u00adlector threads steal work from a list of pointers to identify live data and are \nnot accessing only core-local data, lusearch could be a pathological case where the collector threads \nare doing more coordination than useful work. We investi\u00adgated the performance for lusearch and lusearch-.x \ngoing from one to four collector threads in Figure 24, with times normalized to lusearch with one collector \nthread (at each re\u00adspective frequency), and lower numbers representing better times. Although lusearch \nbene.ts for up to three collector threads, performance degrades signi.cantly with four col\u00adlector threads \nespecially at a higher frequency. The same graph shows that lusearch-.x does not suffer in the same way \nand obtains better overall performance.  Figure 23 also compares running with three and .ve ap\u00adplication \nand collector threads. Except for pjbb2005, all benchmarks degrade in performance when increasing or \nde\u00adcreasing threads from the 4-4 con.guration. Pseudojbb2005 surprisingly performs 85% better with only \nthree application and collector threads, probably because these are suf.cient to perform the computational \nwork for the input set, and more threads just increase inter-thread communication. The right grouping \nin Figure 23 increases the number of appli\u00adcation threads to eight, while keeping the collector threads \nat four. Because this experiment is performed on one socket, all benchmarks suffer because many threads \nare contending for limited resources. In general, setting the number of col\u00adlector threads equal to application \nthreads, and equal to the number of cores, seems to obtain best performance for our multi-threaded benchmarks \nrunning on one socket. 5. Related Work We .rst discuss previous work that tried to understand the performance \nof managed language applications, and their rami.cations on power. We then discuss research that is related \nto dynamic voltage frequency scaling. 5.1 Understanding JVM Services Performance and Power Hu and John \n[13] perform a simulation-based study and eval\u00aduate how processor core characteristics, such as issue \nqueue size, reorder buffer size and cache size, affect JIT com\u00adpiler and garbage collection performance. \nThey conclude that JVM services yield different performance and power characteristics compared to the \nJava application itself. Esmaeilzadeh et al. [10] evaluate the performance and power consumption across \n.ve generations of microproces\u00adsors using benchmarks implemented in both native and man\u00adaged programming \nlanguages. The authors considered end\u00adto-end Java workload performance, like our work, but as\u00adsumed a \nsingle socket where we use multi-socket systems. Further, we explore how isolating and slowing/speeding \nup JVM service threads affects end-to-end performance. Cao et al. [6] study how a Java application can \npoten\u00adtially bene.t from hardware heterogeneity. They tease apart the interpreter, JIT compiler and garbage \ncollector, conclud\u00ading that JVM services consume on average 20% of total en\u00adergy, ranging from 10% to \n55% across the set of applica\u00adtions considered in the study. They further study how clock frequency, \ncache size, hardware parallelism and gross mi\u00adcroarchitecture design options (in-order versus out-of-order \nprocessor cores) affect the performance achieved per unit of energy for each of the JVM services. Through \nthis analy\u00adsis, they advocate for heterogeneous multicore hardware, in which JVM services are run on \ncustomized simple cores and Java application threads run on high-performance cores. There are at least \ntwo key differences between this prior work [6] and ours. First, our experimental setup considers multi-socket \nsystems, not individual processors. Second, we focus on end-to-end Java workload performance whereas \nCao et al. consider the Java application and the various JVM services in isolation. These key differences \nenable us to evaluate how scaling down frequency for particular JVM services affects overall Java workload \nperformance. This is done by separating out the JVM service of interest to an\u00adother socket and scaling \nits frequency. A number of con\u00adclusions that we obtain are in line with Cao et al. In par\u00adticular, we \ncon.rm that the Java application itself bene.ts signi.cantly from increasing clock frequency, and garbage \ncollection bene.ts much less. We also con.rm a slight im\u00adprovement to application performance if the \ncompiler is iso\u00adlated, regardless of whether the isolated compiler runs at the highest or lowest frequency. \nHowever, we also obtain a number of conclusions that are quite different from Cao et al. Whereas Cao \net al. conclude that high clock frequency is energy-ef.cient for the JIT compiler, we .nd that it has \nlimited impact on overall end-to-end performance. Also, al\u00adthough reducing clock frequency for the garbage \ncollector may be energy-ef.cient according to Cao et al., we .nd that it negatively affects end-to-end \nbenchmark performance.  5.2 DVFS Dynamic Voltage and Frequency Scaling (DVFS) is a widely used power \nreduction technique: DVFS lowers supply volt\u00adage and clock frequency to reduce both dynamic and static \npower consumption. DVFS is being used in commercial pro\u00adcessors across the entire computing range: from \nthe embed\u00added and mobile market up to the server market. Extensive research has been done towards how \nto take advantage of DVFS and reduce overall energy consumption while meet\u00ading speci.c performance targets, \nor improving performance while not exceeding a given power budget, either through the operating system \n[17], managed runtime system [23], com\u00adpiler [12, 24] or architecture [14, 16, 21].  Intel s TurboBoost \ntechnology [15] provides the ability to increase clock frequency for a short duration of time if the \nprocessor is operating below its power, current and temper\u00adature speci.cation limits. The frequency at \nwhich the pro\u00adcessor can operate during a TurboBoost period depends on the number of active cores, i.e., \nthe maximum frequency is higher when fewer cores are active. TurboBoost thus has the potential to improve \nperformance by increasing clock fre\u00adquency during single-threaded execution phases of a multi\u00adthreaded \nworkload, or when few programs are active in case of a multi-program workload environment. DVFS is typically \napplied across the entire chip, i.e., in case of a multicore processor, all cores are scaled up or down. \nFor example, our experimental setup using the Intel Nehalem machine only allows for setting the clock \nfrequency at the socket-level. However, recent work has focused on per-core scaling, see for example \n[8, 16, 18]. Per-core DVFS would be a useful extension to the work presented in this paper, but we leave \nit for future work. Barroso and H\u00a8 olzle [1] coined the term energy-proportional computing to refer to \nthe property that a computer system should consume energy proportional to the amount of work that it \nperforms. Ideally, a computer system should not con\u00adsume energy when idling, and should only consume \nenergy proportional to its utilization level. Our .ndings suggest that many benchmarks obtain very good \nperformance when all threads run on one socket, and hence also advocating mini\u00admizing idle power. 6. \nConclusions This paper is one of the .rst to explore the many axes of ex\u00adperimental setup of multi-threaded \nJava applications running on multicore, multi-socket hardware, drawing novel conclu\u00adsions that will help \noptimize running time while minimiz\u00ading power. While varying the number of threads, we have shown on \na single socket, the number of application and col\u00adlector threads should be equal to the number of cores. \nOn two sockets, most benchmarks bene.t from pairing appli\u00adcation and collection threads, but achieve \nbest performance with fewer collector threads than application threads. While we found a cost, usually \nless than 20%, to of.oading JVM collection threads to another socket, we found that then low\u00adering the \ncore frequency of that socket provided reasonable performance degradations in comparison with lowering \nthe clock speed of application threads, which is very detrimental to performance. However, isolating \nthe compilation thread is performance-neutral or improves performance; and isolating threads at startup-time \nis less detrimental, also sometimes improving benchmark performance, than during steady-state time. Many \nbenchmarks achieve good performance when keeping all threads on one socket, leaving the second socket \nidle to also save power. However, overall execution time is lowest for all but one benchmark when the \ncompilation thread is isolated, and for the other benchmark when the collector thread is isolated (both \nregardless of the isolated thread s frequency). These interesting insights will help bal\u00adance time and \npower goals of both managed language work\u00adloads and hardware resources. This paper is a .rst, but important, \nstep towards suggest\u00ading enhancements to both operating systems and hardware to facilitate balancing \nperformance and energy on multi\u00adsocket systems. It would be useful for the operating system to work with \nthe virtual machine to identify JVM threads separately from application threads and offer more .ne\u00adgrained \ncontrol over their movement and mapping to cores. If pinning threads is discovered to be detrimental \nto perfor\u00admance based on pro.ling or hardware performance counters, threads could be dynamically moved. \nSimilarly, if hardware provides support for monitoring and scaling power dynam\u00adically, on a per-core \nbasis for .ner time quanta, we could scale individual JVM threads for compilation and on-stack\u00adreplacement, \nfor example, just at startup time. We could potentially also avoid the cost of isolating collection threads \nif hardware could scale one core only during collection, and not during application running time. When \nthe cost of iso\u00adlation cannot be avoided, cache optimizations for prefetch\u00ading or otherwise avoiding \ninter-socket traf.c could improve memory system behavior. Our extensive analysis of end\u00adto-end performance \nof multi-threaded managed applications running on multicore chips offers insights into how to design \nand optimize machines from application to virtual machine to operating system to memory system, down \nto hardware. Acknowledgements We thank the anonymous reviewers for their valuable feed\u00adback. We also \nthank Wim Heirman for his help and tech\u00adnical expertise on thread pinning and frequency scaling on the \nIntel Nehalem machine. The research leading to these results has received funding from the European Research \nCouncil under the European Community s Seventh Frame\u00adwork Programme (FP7/2007-2013) / ERC Grant agreement \nno. 259295. References [1] L. A. Barroso and U. H\u00a8olzle. The case for energy-proportional systems. IEEE \nComputer, 40:33 37, Dec. 2007. [2] S. M. Blackburn and K. S. McKinley. Immix: A mark\u00adregion garbage collector \nwith space ef.ciency, fast collection, and mutator locality. In Programming Language Design and Implementation \n(PLDI), pages 22 32, Tuscon, AZ, June 2008. [3] S. M. Blackburn, M. Hirzel, R. Garner, and D. Stefanovic.\u00b4pjbb2005: \nThe pseudojbb benchmark. URL http: //users.cecs.anu.edu.au/ steveb/research/ research-infrastructure/pjbb2005. \n [4] S. M. Blackburn, R. Garner, C. Hoffman, A. M. Khan, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, \nD. Framp\u00adton, S. Z. Guyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, \nT. VanDrunen, D. von Dincklage, and B. Wiedermann. The DaCapo bench\u00admarks: Java benchmarking development \nand analysis. In ACM SIGPLAN Conference on Object-Oriented Programing, Sys\u00adtems, Languages, and Applications \n(OOPSLA), pages 169 190, Oct. 2006. [5] S. M. Blackburn, K. S. McKinley, R. Garner, C. Hoffman, A. M. \nKhan, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. Hosking, M. Jump, H. \nLee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, D. von Dincklage, and B. Wiedermann. \nWake up and smell the cof\u00adfee: Evaluation methodology for the 21st century. Communi\u00adcations of the ACM, \n51(8):83 89, Aug. 2008. [6] T. Cao, S. M. Blackburn, T. Gao, and K. S. McKinley. The yin and yang of \npower and performance for asymmetric hardware and managed software. In The 39th International Symposium \non Computer Architecture (ISCA), pages 225 236, June 2012. [7] R. H. Dennard, F. H. Gaensslen, V. L. \nRideout, E. Bassous, and A. R. LeBlanc. Design of ion-implanted mosfet s with very small physical dimensions. \nIEEE Journal of Solid-State Circuits, Oct 1974. [8] J. Dorsey, S. Searles, M. Ciraula, S. Johnson, N. \nBujanos, D. Wu, M. Braganza, S. Meyers, E. Fang, and R. Kumar. An integrated quad-core Opteron processor. \nIn Proceedings of the International Solid State Circuits Conference (ISSCC), pages 102 103, Feb. 2007. \n[9] H. Esmaeilzadeh, E. R. Blem, R. S. Amant, K. Sankaralingam, and D. Burger. Dark silicon and the end \nof multicore scaling. In 38th International Symposium on Computer Architecture (ISCA), pages 365 376, \nJune 2011. [10] H. Esmaeilzadeh, T. Cao, Y. Xi, S. M. Blackburn, and K. S. McKinley. Looking back on \nthe language and hardware rev\u00adolutions: Measured power, performance, and scaling. In Pro\u00adceedings of \nthe 16th International Conference on Architec\u00adtural Support for Programming Languages and Operating Systems \n(ASPLOS), pages 319 332, June 2011. [11] A. Georges, D. Buytaert, and L. Eeckhout. Statistically rig\u00adorous \nJava performance evaluation. In Proceedings of the Annual ACM SIGPLAN Conference on Object-Oriented Pro\u00adgramming, \nLanguages, Applications and Systems (OOPSLA), pages 57 76, Oct. 2007. [12] C.-H. Hsu and U. Kremer. The \ndesign, implementation, and evaluation of a compiler algorithm for CPU energy reduction. In Proceedings \nof the International Symposium on Program\u00adming Language Design and Implementation (PLDI), pages 38 48, \nJune 2003. [13] S. Hu and L. K. John. Impact of virtual execution environ\u00adments on processor energy consumption \nand hardware adap\u00adtation. In International Conference on Virtual Execution En\u00advironments (VEE), pages \n100 110, June 2006. [14] C. J. Hughes, J. Srinivasan, and S. V. Adve. Saving energy with architectural \nand frequency adaptations for multimedia applications. In Proceedings of the 34th Annual International \nSymposium on Microarchitecture (MICRO), pages 250 261, Dec. 2001. [15] Intel Coorporation. Intel turbo \nboost technology in Intel core microarchitecture (Nehalem) based processors, Nov 2008. [16] C. Isci, \nA. Buyuktosunoglu, C.-Y. Cher, P. Bose, and M. Martonosi. An analysis of ef.cient multi-core global power \nmanagement policies: Maximizing performance for a given power budget. In Proceedings of the International \nSym\u00adposium on Microarchitecture (MICRO), pages 347 358, Dec. 2006. [17] C. Isci, G. Contreras, and M. \nMartonosi. Live, runtime phase monitoring and prediction on real systems and application to dynamic power \nmanagement. In Proceedings of the Interna\u00adtional Symposium on Microarchitecture (MICRO), pages 359 370, \nDec. 2006. [18] W. Kim, M. S. Gupta, G.-Y. Wei, and D. Brooks. System level analysis of fast, per-core \nDVFS using on-chip switching regulators. In Proceedings of the International Symposium on High-Performance \nComputer Architecture (HPCA), pages 123 134, Feb. 2008. [19] G. E. Moore. Readings in computer architecture. \nchapter Cramming more components onto integrated circuits, pages 56 59. Morgan Kaufmann Publishers Inc., \nSan Francisco, CA, USA, 2000. [20] Y. Seeley. JIRA issue LUCENE-1800: QueryParser should use reusable \ntoken streams, 2009. URL https://issues. apache.org/jira/browse/LUCENE-1800. [21] G. Semeraro, D. H. \nAlbonesi, S. G. Dropsho, G. Magklis, S. Dwarkadas, and M. L. Scott. Dynamic frequency and volt\u00adage control \nfor a multiple clock domain microarchitecture. In Proceedings of the International Symposium on Microarchi\u00adtecture \n(MICRO), pages 356 367, Nov. 2002. [22] TIOBE Software. TIOBE programming community index, 2011. http://tiobe.com/tpci.html. \n[23] Q. Wu, V. J. Reddi, Y. Wu, J. Lee, D. Connors, D. Brooks, M. Martonosi, and D. W. Clark. A dynamic \ncompilation framework for controlling microprocessor energy and perfor\u00admance. In Proceedings of the International \nSymposium on Microarchitecture (MICRO), pages 271 282, Nov. 2005. [24] F. Xie, M. Martonosi, and S. Malik. \nCompile-time dynamic voltage scaling settings: Opportunities and limits. In Proceed\u00adings of the International \nSymposium on Programming Lan\u00adguage Design and Implementation (PLDI), pages 49 62, June 2003. [25] X. \nYang, S. Blackburn, D. Frampton, J. Sartor, and K. McKin\u00adley. Why nothing matters: The impact of zeroing. \nIn Proceed\u00adings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages \nand Applications (OOPSLA), pages 307 324, Oct 2011.   \n\t\t\t", "proc_id": "2384616", "abstract": "<p>While there have been many studies of how to schedule applications to take advantage of increasing numbers of cores in modern-day multicore processors, few have focused on multi-threaded managed language applications which are prevalent from the embedded to the server domain. Managed languages complicate performance studies because they have additional virtual machine threads that collect garbage and dynamically compile, closely interacting with application threads. Further complexity is introduced as modern multicore machines have multiple sockets and dynamic frequency scaling options, broadening opportunities to reduce both power and running time.</p> <p>In this paper, we explore the performance of Java applications, studying how best to map application and virtual machine (JVM) threads to a multicore, multi-socket environment. We explore both the cost of separating JVM threads from application threads, and the opportunity to speed up or slow down the clock frequency of isolated threads. We perform experiments with the multi-threaded DaCapo benchmarks and pseudojbb2005 running on the Jikes Research Virtual Machine, on a dual-socket, 8-core Intel Nehalem machine to reveal several novel, and sometimes counter-intuitive, findings. We believe these insights are a first but important step towards understanding and optimizing managed language performance on modern hardware.</p>", "authors": [{"name": "Jennfer B. Sartor", "author_profile_id": "81100262404", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "P3856084", "email_address": "jennifer.sartor@elis.ugent.be", "orcid_id": ""}, {"name": "Lieven Eeckhout", "author_profile_id": "81330490198", "affiliation": "Ghent University, Ghent, Belgium", "person_id": "P3856085", "email_address": "Lieven.Eeckhout@elis.ugent.be", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384638", "year": "2012", "article_id": "2384638", "conference": "OOPSLA", "title": "Exploring multi-threaded Java application performance on multicore hardware", "url": "http://dl.acm.org/citation.cfm?id=2384638"}