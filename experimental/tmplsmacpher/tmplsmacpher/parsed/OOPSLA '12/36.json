{"article_publication_date": "10-19-2012", "fulltext": "\n AUTOMAN: A Platform for Integrating Human-Based and Digital Computation Daniel W. Barowy Charlie Curtsinger \nEmery D. Berger Andrew McGregor Department of Computer Science University of Massachusetts, Amherst Amherst, \nMA 01003 {dbarowy,charlie,emery,mcgregor}@cs.umass.edu Abstract Humans can perform many tasks with ease \nthat remain dif.\u00adcult or impossible for computers. Crowdsourcing platforms like Amazon s Mechanical Turk \nmake it possible to harness human-based computational power at an unprecedented scale. However, their \nutility as a general-purpose computational platform remains limited. The lack of complete automation \nmakes it dif.cult to orchestrate complex or interrelated tasks. Scheduling more human workers to reduce \nlatency costs real money, and jobs must be monitored and rescheduled when workers fail to complete their \ntasks. Furthermore, it is often dif.cult to predict the length of time and payment that should be budgeted \nfor a given task. Finally, the results of human\u00adbased computations are not necessarily reliable, both \nbecause human skills and accuracy vary widely, and because workers have a .nancial incentive to minimize \ntheir effort. This paper introduces AUTOMAN, the .rst fully automatic crowdprogramming system. AUTOMAN \nintegrates human\u00adbased computations into a standard programming language as ordinary function calls, \nwhich can be intermixed freely with traditional functions. This abstraction lets AUTOMAN programmers \nfocus on their programming logic. An AUTO-MAN program speci.es a con.dence level for the overall computation \nand a budget. The AUTOMAN runtime system then transparently manages all details necessary for schedul\u00ading, \npricing, and quality control. AUTOMAN automatically schedules human tasks for each computation until \nit achieves the desired con.dence level; monitors, reprices, and restarts human tasks as necessary; and \nmaximizes parallelism across human workers while staying under budget. Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, \nUSA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 Categories and Subject Descriptors \nH.1.2 [Information Systems]: Human information processing; D.3.2 [Language Classi.cations]: Specialized \napplication languages; G.3 [Probability and Statistics]: Probabilistic algorithms (includ\u00ading Monte Carlo) \nGeneral Terms Languages, Algorithms, Human Factors Keywords Crowdsourcing, Programming Languages, Qual\u00adity \nControl 1. Introduction Humans perform many tasks with ease that remain dif.cult or impossible for computers. \nFor example, humans are far better than computers at performing tasks like vision, mo\u00adtion planning, \nand natural language understanding [23, 27]. Most researchers expect these AI-complete tasks to remain \nbeyond the reach of computers for the foreseeable future [28]. Recent systems streamline the process \nof hiring humans to perform computational tasks. The most prominent exam\u00adple is Amazon s Mechanical Turk, \na general-purpose crowd\u00adsourcing platform that acts as an intermediary between labor requesters and workers \n[2, 16]. Domain-speci.c commercial services based on Mechanical Turk include CastingWords and ClariTrans, \nwhich perform accurate audio transcription, and Tagasaurus and TagCow, which perform image classi.cation. \nHowever, harnessing human-based computation in general and at scale faces the following challenges: \nDetermination of pay and time for tasks. Employers must decide in advance the time allotted to a task \nand the payment for successful completion. It is both dif.cult and important to choose these correctly \nsince workers will not accept jobs with a too-short deadline or too little pay.  Scheduling complexities. \nEmployers must manage the trade-off between latency (humans are relatively slow) and cost (more workers \nmeans more money). Because workers may fail to complete their tasks in the allotted time, jobs need to \nbe tracked and reposted as necessary.   Low quality responses. Human-based computations al\u00adways need \nto be checked: worker skills and accuracy vary widely, and they have a .nancial incentive to minimize \ntheir effort. Manual checking does not scale, and simple majority voting is insuf.cient since workers \nmight agree by random chance. Contributions This paper introduces AUTOMAN, a programming system that \nintegrates human-based and digital computation. AUTO-MAN addresses the challenges of harnessing human-based \ncomputation at scale: Transparent integration of human and digital computation. AUTOMAN incorporates \nhuman-based computation as func\u00adtion calls in a standard programming language. The AUTO-MAN runtime system \ntransparently manages scheduling, bud\u00adgeting, and quality control. Automatic scheduling and budgeting. \nThe AUTOMAN runtime system schedules tasks to maximize parallelism across human workers while staying \nunder budget. AUTO-MAN tracks job progress and reschedules and reprices failed tasks as necessary. Automatic \nquality control. The AUTOMAN runtime sys\u00adtem performs automatic quality control management. AUTO-MAN \nautomatically creates enough human tasks for each computation to achieve the con.dence level speci.ed \nby the programmer. For example, given a desired con.dence level of 95% and a function with .ve possible \nanswers, AUTOMAN initially schedules at least three tasks (human workers). Because the chances of all \nthree agreeing due to random chance is under 5%, a unanimous response would be considered acceptable. \nIf all three workers do not agree, AUTOMAN will schedule another three tasks, at which point 5 out of \n6 must agree to achieve a 95% con.dence level (Section 5). 2. Background: Crowdsourcing Platforms Since \ncrowdsourcing is a novel application domain for pro\u00adgramming language research, we start by summarizing \nthe necessary background on crowdsourcing platforms. Our dis\u00adcussion in this section focuses on Amazon \ns Mechanical Turk, but other existing crowdsourcing platforms are similar. Mechanical Turk acts as an \nintermediary between em\u00adployers (known as requesters) and employees (workers, or colloquially, turkers) \nfor short-term assignments. Human Intelligence Tasks (HITs). In Mechanical Turk parlance, individual \ntasks are known as HITs, which stands for human intelligence tasks. HITs include a short description, \nthe amount the job pays, and other details. Most HITs on Mechanical Turk are for relatively simple tasks, \nsuch as does this image match this product? Compensation is generally low, since employers expect that \nwork can be completed on a time scale ranging from seconds to minutes. Pay for HITs range from a single \npenny to several dollars. Each HIT is represented as a question form, composed of any number of questions, \nand associated metadata such as a title, description, and search keywords. Questions can be one of two \ntypes: a free text question, where workers can provide a free-form textual response, or a multiple-choice \nquestion, where workers make one or more selections from a list of possible options. AUTOMAN currently \nonly supports multiple choice and restricted free-text questions. Requesters: Posting HITs. Mechanical \nTurk allows HITs to be posted manually, but also exposes a web service API that allows basic details \nof HITs to be managed programmati\u00adcally [2], including posting HITs, collecting completed work, and paying \nworkers. Using this API, it is straightforward to post similar tasks to Mechanical Turk en masse. HITs \nsharing similar qualities can be grouped into HIT groups. A requester may also instruct Mechanical Turk \nto paral\u00adlelize a particular HIT by indicating whether each HIT should be assigned to more than one worker. \nBy increasing the num\u00adber of assignments, Mechanical Turk allows additional work\u00aders to accept work for \nthe same HIT, and the system ensures that the parallel workers are unique (i.e., that a single worker \ncannot complete the same HIT more than once). Workers: Performing Work. Mechanical Turk workers can choose \nany of the available tasks on the system for which they are quali.ed (see below): as of this writing, \nthere are approximately 275,000 HITs posted. When workers choose to perform a particular HIT, they accept \nan assignment, which grants them a time-limited reservation for that particular piece of work; that is, \nno other worker may accept it. HIT Expiration. HITs have two timeout parameters: the amount of time that \na particular HIT should remain in the listings, known as the lifetime of a HIT, and the amount of time \nthat a worker has to complete an assignment once it is accepted, known as the duration of an assignment. \nIf after accepting an assignment for a HIT, a worker exceeds the assignment s duration without submitting \ncompleted work, the reservation is cancelled, and the work is returned to the pool of available assignments. \nIf a HIT reaches the end of its lifetime without its assignments having been completed, the HIT expires \nand is removed from the job board. Requesters: Accepting or Rejecting Work. Once a worker completes and \nsubmits an assignment, the requester is noti\u00ad.ed. The requester then can accept or reject the completed \nassignment. Acceptance of an assignment indicates that the completed work is satisfactory, and the worker \nis then au\u00adtomatically paid for his or her efforts. Rejection withholds payment, and the requester, if \nso inclined, may provide a tex\u00adtual justi.cation for the rejection. AUTOMAN automatically manages acceptance \nand rejection; see Section 3.2. Worker Quality Control. A key challenge in automating work in Mechanical \nTurk is attracting and retaining good workers, or at least discouraging bad workers from partici-1 pating. \nHowever, Mechanical Turk provides no way for re\u00ad  2 questers to seek out speci.c workers. 43 Instead, \nMechanical Turk provides a quali.cation mecha\u00ad 5 nism to limit which workers may perform a particular \nHIT. 76 A common quali.cation is that workers must have an over\u00ad 8 all assignment-acceptance rate of \n90%. However, given the 109 wide variation in tasks on Mechanical Turk, overall worker 11 12 accuracy \nis of limited utility. 13 For example, the fact that a worker who is skilled in and 14 favors audio transcription \ntasks may have a high accuracy 15 16 17 rating, but there is no reason to believe that this worker can \nalso perform Chinese-to-English language translation 18 19 20 21 tasks. Worse, workers who cherry-pick \neasy tasks and thus have a high accuracy rating actually may be less quali.ed 22 than a worker who routinely \nperforms dif.cult work that is occasionally rejected. 3. Overview AUTOMAN is a domain-speci.c language \nembedded in Scala [25]. AUTOMAN s goal is to abstract away the de\u00adtails of crowdsourcing so that human \ncomputation can be as easy to invoke as a conventional function. 3.1 Using AUTOMAN Figure 1 presents \nan example (toy) AUTOMAN program. The program computes which of a set of cartoon characters does not \nbelong in the group. Notice that the programmer does not specify details about the chosen crowdsourcing \nbackend (Mechanical Turk) except for account credentials. Crucially, all details of crowdsourcing are \nhidden from the AUTOMAN programmer. The AUTOMAN runtime manages interfacing with the crowdsourcing platform, \nschedules and determines budgets (both cost and time), and automatically ensures the desired con.dence \nlevel of the .nal result. Initializing AUTOMAN. After importing the AUTOMAN and Mechanical Turk adapter \nlibraries, the .rst thing an AUTOMAN programmer does is to declare a con.guration for the desired crowdsourcing \nplatform. This con.guration is then bound to an AUTOMAN runtime object, which instanti\u00adates any platform-speci.c \nobjects. Specifying AUTOMAN functions. Functions in AUTO-MAN consist of declarative descriptions of questions \nthat the workers must answer; they may include text or images, as well as a range of question types, \nwhich we describe below. Con.dence level. An AUTOMAN programmer can option\u00adally specify the degree of \ncon.dence they want to have in their computation, on a per-function basis. AUTOMAN s de\u00adfault con.dence \nis 95% (0.95), but this can be overridden as needed. The meaning and derivation of con.dence is dis\u00adcussed \nin Section 5. Metadata and question text. Each question requires a title and description, used by the \ncrowdsourcing platform s user import edu.umass.cs.automan.adapters.MTurk._ object SimpleProgram extends \nApp { val a = MTurkAdapter { mt => mt.access _key_ id = \"XXXX\" mt.secret_access_key = \"XXXX\" } def which_ \none() = a.RadioButtonQuestion { q => q.budget = 8.00 q.text = \"Which one of these does not belong?\" q.options \n= List( a.Option( oscar , \"Oscar the Grouch\"), a.Option( kermit , \"Kermit the Frog\"), a.Option( spongebob \n, \"Spongebob Squarepants\"), a.Option( cookie , \"Cookie Monster\"), a.Option( count , \"The Count\") ) } \n println(\"The answer is \" + which _one()()) } Figure 1. A complete AUTOMAN program. This program computes, \nby invoking humans, which cartoon character does not belong in a given set. The AUTOMAN programmer speci.es \nonly credentials for Mechanical Turk, an overall budget, and the question itself; the AUTOMAN runtime \nmanages all other details of execution (scheduling, budgeting, and quality control). interface. These \n.elds map to Mechanical Turk s .elds of the same name. A question also includes a textual representation \nof the question, together with a map between symbolic constants and strings for possible answers. Question \nvariants. AUTOMAN supports multiple-choice questions, including questions where only one answer is cor\u00adrect \n( radio-button questions), or where any number of an\u00adswers may be correct ( checkbox questions), as well \nas restricted-text entry forms. Section 5 describes how AUTO-MAN s quality control algorithm handles \nthese different types of questions. Invoking a function. An AUTOMAN programmer can in\u00advoke a function \nas if it were any ordinary (digital) func\u00adtion. Here, the programmer calls the just-de.ned function which_one() \nwith no input. The function returns a Scala future object representing the answer, which can be passed \nto other Questions in an AUTOMAN program before the hu\u00adman computation is complete. AUTOMAN functions \nexecute in the background in parallel as soon as they are invoked. The program does not block until it \nreferences the function output, and only then if the human computation is not yet .nished.  3.2 AUTOMAN \nExecution Figure 2 depicts an actual trace of the execution of the pro\u00ad gram from Figure 1, obtained \nby executing it with Amazon s Mechanical Turk. This example demonstrates that ensuring valid results \neven for simple programs can be complicated. Starting Tasks. At startup, AUTOMAN examines the form of \nthe question .eld de.ned for the task and determines that, in order to achieve a 95% con.dence level \nfor a question with .ve possible choices, at minimum, it needs three different workers to unanimously \nagree on the answer (see Section 5). AUTOMAN then spawns three tasks on the crowdsourcing backend, Mechanical \nTurk. To eliminate bias caused by the position of choices, AUTOMAN randomly shuf.es the order of choices \nin each task.  AUTOMAN s default strategy is optimistic. For many tasks, human workers are likely to \nagree unanimously. Whenever this is true, AUTOMAN saves money by spending the least amount required to \nachieve the desired statistical con.dence. However, AUTOMAN also allows users to choose a more aggressive \nstrategy that trades a risk of increased cost for reduced latency; see Section 4.3. Quality Control. \nAt time 1:50, worker 1 accepts the task and submits Spongebob Squarepants as the answer. Forty seconds \nlater, worker 2 accepts the task and submits the same answer. However, twenty seconds later, worker 3 \naccepts the task and submits Kermit . In this case, AUTOMAN s optimism did not pay off, since worker \n3 does not agree with worker 1 and 2. Because this result is inconclusive, AUTOMAN schedules three additional \ntasks. At this point, AUTOMAN recomputes the minimal number of agreements, which turns out to be .ve \nout of six. Sec\u00adtion 5.2 presents a full derivation of the closed-form formulas that AUTOMAN uses to \ncompute these values. Memoization of Results. During program execution, AUTO-MAN saves the intermediate \nstate of all human functions, namely scheduler objects, to a database. This means that if a program is \ninterrupted, AUTOMAN s scheduler is able to resume the program precisely where it left off. Memoiza\u00adtion \nis automatic and transparent to the programmer. This abstraction serves two purposes. First, the AUTOMAN \nfunc\u00adtion may be called an arbitrary number of times, resulting in substantial time savings. Second, \nif the program s execu\u00adtion is interrupted in any way, the programmer may resume the program without \nlosing their investment in human labor. Programmers who do not want to use memoized results need only \ndelete the memo database at startup. Initial Time Estimates. When de.ning a task, the program\u00admer can \nspecify the task duration , the time available to work once the task has been accepted. On Mechanical \nTurk, this number serves as an indication to the worker of the dif\u00ad.culty of the task. In AUTOMAN, this \n.gure is set to 30 seconds by default. A second time parameter, the lifetime of a task, indicates how \nlong the crowdsourcing backend should keep the task around without any response from workers. In AUTOMAN, \nthe default task lifetime is set to 100\u00d7 the duration. At 51 minutes into the computation, task 6 exceeds \nits lifetime and is cancelled. Since AUTOMAN does not yet have a statistically valid result, it reschedules \nthe task, this time Figure 2. A schematic of an actual execution of the example program in Figure 1, \nwith time advancing from top to bottom. AUTOMAN .rst spawns 3 tasks, which (for .ve choices) suf.ce to \nreach a 95% con.dence level if all workers agree. Since the 3 workers do not agree, AUTOMAN schedules \nthree additional tasks (5 of 6 must now agree). When task 6 times out, AUTOMAN spawns a new task and \ndoubles both the budget and time allotted. Once task 7 completes, AUTOMAN returns the result. extending \nboth the task timeout and lifetime by a factor of two (see Section 4.1). Rebudgeting: Time and Pay. AUTOMAN \ndoes not require the programmer to specify exactly how much each worker should be paid. AUTOMAN currently \nuses the timeout param\u00adeter and calculates the base cost for the task using the US Federal minimum wage \n($7.25/hr). For 30 seconds of work, the payment is $0.06 US Dollars. AUTOMAN automatically manages task \nrewards and du\u00adrations by doubling both whenever a task s lifetime expires, which maintains the same \nminimum wage (see Section 4.1). In the absence of an automatic mechanism, programmers would be required \nto determine the fair wage of the task mar\u00adketplace manually. Given the subjectivity of a fair wage, \nknowing the appropriate wage a priori is dif.cult or impossi-1 ble in most cases.  2 3 Automatic Task \nAcceptance and Rejection. AUTOMAN 45 does not accept or reject work until the quality control al\u00ad 6 gorithm \nhas chosen the correct answer. Payment is deferred 87 until the computation completes. If the programmer \ns account 9 10 runs out of funds before the program is complete, AUTOMAN 11 returns the answer with the \nhighest con.dence. The program\u00ad 12 13 mer may then choose to either add more money to their ac-14 count \nand resume the computation, using stored results from 15 16 the memoization database, or to accept the \nlower-quality an-17 swer and pay all of the workers. 18 19 One hour and 9 minutes into the computation, \na worker 20 submits a sixth answer, Spongebob Squarepants . AUTO\u00ad 21 22 MAN again examines whether the \nanswers agree and it .nds 23 that 5 out of the 6 answers agree. AUTOMAN can now reject 24 25 the null \nhypothesis that 5 workers agreed by choosing the 26 same answer randomly with 95% con.dence. The runtime \n27 28 system then returns the answer to the program, and the user s 29 regular program resumes. 30 31 \nAUTOMAN then informs the crowdsourcing backend to 32 pay the .ve workers whose answers agreed. Four workers \n33 34 are paid $0.06, and one is paid $0.12. The one worker whose 35 answer did not agree was not paid. \nFor Mechanical Turk, 36 which supports rejection noti.cations, AUTOMAN informs 37 38 39 40 workers who \nprovided incorrect responses that their work was not accepted, and includes the correct response and \n41 con.dence as justi.cation. 42 43 The fact that AUTOMAN does not pay for incorrect work 44 reduces \nits cost, especially as the number of workers increases. For example, if 12 responses have been received \nfor a ques\u00adtion with 5 choices, only 7 must agree to achieve a 95% con.dence level. As the number of \nworkers increases, the proportion required for agreement drops further, making re\u00adjecting incorrect work \neven more desirable. 4. Scheduling Algorithm Figure 3 presents pseudo-code for AUTOMAN s main sched\u00aduler \nloop, which comprises the algorithms that the AUTO-MAN runtime uses to manage task posting, reward and \ntime\u00adout calculation, and quality control. 4.1 Calculating Timeout and Reward AUTOMAN s overriding goal \nis to recruit workers quickly and at low cost in order to keep the cost of a computation within the programmer \ns budget. AUTOMAN posts tasks in rounds, which have a .xed timeout during which tasks must be completed. \nWhen AUTOMAN fails to recruit workers in a round, there are two possible causes: workers were not willing \nto complete the task for the given reward, or the time allotted was not suf.cient. AUTOMAN does not distinguish \nbetween these cases. Instead, the reward for a task and the time allotted are both increased by a constant \nfactor k every wage = DEFAULT_WAGE value _of_time = DEFAULT_VALUE_OF_TIME duration = DEFAULT_ DURATION \nreward = wage * duration budget = DEFAULT_BUDGET cost = $0 .00 tasks = [] answers = load_saved_answers \n() timed _out = false confident = false while not confident: if timed_ out: duration *= 2 reward *= 2 \ntimed_out = false if tasks.where(state == RUNNING).size == 0: most _votes = answers.group_ by(answer).max \nrequired = 0 while min_ votes(choices , most _votes + required) > most_ votes + required: required += \n1 if required == 0: confident = true else can_afford = floor((budget -cost) / reward) if can _afford \n< required: throw OVER_BUDGET ideal = floor(value _of_time / wage) to_ run = max(required, min(can _afford, \nideal)) cost += to _run * reward tasks . appendAll ( spawn_ tasks ( to_run )) else : num _timed_out = \ntasks.where(state == TIMEOUT).size if num_timed_out > 0: timed_ out = true cost -= num_timed _out * reward \n foreach t in tasks.where(state == ANSWERED): answers . append (t. answer ) save_ answer (t. answer ) \nreturn answers.group_by(answer).argmax Figure 3. Pseudo-code for AUTOMAN s scheduling loop, which handles \nposting and re-posting jobs, budgeting, and quality control; Section 5.2 includes a derivation of the \nformulas for the quality control thresholds. time a task goes unanswered. k must be chosen carefully \nto ensure the following two properties: 1. The reward for a task should quickly reach a worker s minimum \nacceptable compensation (Rmin), e.g., in a logarithmic number of steps. 2. The reward should not grow \nso quickly that it would give workers an incentive to wait for a larger reward, rather than work immediately. \n Workers do not know the probability that a task will re\u00admain unanswered until the next round. If the \nworker assumes even odds that a task will survive the round, a growth rate of k =2 is optimal: it will \nreach Rmin faster than any lower value of k, and workers never have an incentive to wait. Sec\u00adtion 4.4 \npresents a detailed analysis. Lines 13-16 in Figure 3 increase the reward and duration for tasks that \nhave timed out.  In AUTOMAN, reward and time are speci.ed in terms of the worker s wage ($7.25/hour \nfor all the experiments in this paper). Doubling both reward and time ensures that AUTOMAN will never \nexceed the minimum time and reward by more than a factor of two. The doubling strategy may appear to \nrun the risk that a worker will game the computation into paying a large sum of money for an otherwise \nsimple task. However, once the wage reaches an acceptable level for some proportion of the worker marketplace, \nthose workers will accept the task. Forcing AUTOMAN to continue doubling to a very high wage would require \ncollusion between workers on a scale that we believe is infeasible, especially when the underlying crowdsourcing \nsystem provides strong guarantees that worker identities are independent.  4.2 Scheduling the Right \nNumber of Tasks AUTOMAN s default strategy for spawning tasks is optimistic: it creates the smallest \nnumber of tasks required to reach the desired con.dence level if the results are unanimous. Line 19 in \nFigure 3 determines the number of votes for the most popular answer so far. Lines 20-23 iteratively compute \nthe minimum number of additional votes required to reach con.dence. If no additional votes are required, \ncon.dence has been reached and AUTOMAN can return the most popular answer (line 44). Using the current \nreward, AUTOMAN computes the maxi\u00admum number of tasks that can be posted with the remaining budget (line \n28). If the budget is insuf.cient, AUTOMAN will terminate the computation, leaving all tasks in an unveri.ed \nstate (lines 29-30). The computation can be resumed with an increased budget or abandoned. Mechanical \nTurk will au\u00adtomatically pay all workers if responses are not accepted or rejected after 30 days. 4.3 \nTrading Off Latency and Money AUTOMAN also allows programmers to provide a time-value for the computation, \nwhich tells AUTOMAN to post more than the minimum number of tasks. AUTOMAN always schedules at least \nthe minimum number of tasks required to achieve con.dence in every round. If the programmer speci.es \na time value, AUTOMAN will schedule the following number of tasks (lines 31-32): time value maxmin required, \nmin wageOnce AUTOMAN receives enough answers to reach the speci.ed con.dence, it cancels any outstanding \ntasks. In the worst case, all posted tasks will be answered before AUTOMAN can cancel them, which will \ncost no more than time value \u00b7 task timeout. This strategy runs the risk of paying substantially more \nfor a computation, but can yield dramatic reductions in latency. We re-ran the example program given \nin Figure 1 with a time\u00ad value set to $50, 7\u00d7 larger than the current U.S. minimum wage. In two separate \nruns, the computation completed in 68 and 168 seconds; we also ran the .rst computation with the default \ntime-value (minimum wage), and those computations took between 1 and 3 hours to complete.  4.4 Derivation \nof Optimal Reward Growth Rate When workers encounter a task with a posted reward of R, they may choose \nto accept the task, or wait for the reward to grow. Let pa be the probability that the task will still \nbe available after one round of waiting. We make the assumption that, if the task is still available \nafter i - 1 rounds, then the probability that the task is available in the ith round is at most pa. Hence, \nif the player s strategy is to wait i rounds and then complete the task, i E[reward] = pkiR, a i since \nwith probability at most pthe reward will be kiR and a otherwise the task will no longer be available. \nNote that the expected reward is maximized with i =0 if k = 1/pa. Therefore, k =1/pa is the highest value \nof k that does not incentivize waiting and will reach Rmin faster than any lower value of k. Workers \ncannot know the true value of pa. In the absence of any information, 1/2 will be used as an estimator \nfor pa and this leads to AUTOMAN s default value of k =2. However, it is possible to estimate pa. Every \ntime a worker accepts or waits for a task, we can treat this as an independent Bernoulli trial with the \nparameter pa. The maximum likelihood estimator for pa equals p a = argmax x t(1 - x)n-t x.[0,1] where \nt is the number of times a task has been accepted amongst the n times it has been offered so far. Solving \nthis gives p a = t/n. The dif.culty of accurately estimating pa using ad hoc quality control is a strong \nargument for automatic budgeting. Implementing this estimation is a planned future enhance\u00adment for AUTOMAN. \n5. Quality Control Algorithm AUTOMAN s quality control algorithm is based on collecting enough consensus \nfor a given question to rule out the possi\u00adbility, with a desired level of con.dence, that the results \nare due to random chance. Section 5.3 justi.es this approach. Initially, AUTOMAN spawns enough tasks \nto meet the de\u00adsired con.dence level if all workers who complete the tasks agree on the same answer. \nFigure 5 depicts the initial con.\u00ad dence level function. Computing this value is straightforward: if \nk is the number of choices, and n is the number of tasks, the con.dence level reached is 1 - k(1/k)n. \nAUTOMAN computes the lowest value of n where the desired con.dence level is reached.  Question Variants. \nFor ordinary multiple choice questions where only one choice is possible ( radio-button questions), k \nis exactly the number of possible answers. However, hu\u00admans are capable of answering a richer variety \nof question types. Each of these additional question types requires its own probability analysis. Checkbox \nQuestions. For multiple choice questions with c choices and any or all may be chosen ( checkbox questions), \nk is much larger: k =2c . For these questions, k is so high that a very small number of workers are required \nto reject the null hypothesis (ran\u00addom choice). However, it is reasonably likely that two lazy workers \nwill simply select no answers, and AUTOMAN will erroneously accept that answer is correct. To compensate \nfor this possibility, AUTOMAN treats checkbox questions specially. The AUTOMAN programmer must specify \nnot only the question text, but also an inverted question. For instance, if the question is Select which \nof these are true , the inverted question should read, Select which of these are NOT true. AUTOMAN then \nensures that half of the HITs are spawned with the positive question, and half with the inverted question. \nThis strategy makes it less likely that lazy workers will inadvertently agree. Restricted Free-Text Questions \nEntering a small amount of text into a text input is essentially equivalent to a sequence of radio-button \nquestions, where the possible radio-button options are the valid range of textual inputs for the given \nquestion. This form of input is cumbersome for human workers. By providing a small text .eld with a pattern \nrepresenting valid inputs, AUTOMAN satis.es its requirement for analysis while making data entry easier. \n The input speci.cation in AUTOMAN resembles COBOL s picture clauses. For example, a telephone number \nrecognition application would use the pattern 09999999999. A matches an alphabetic character, B matches \nan optional alphabetic character, X matches an alphanumeric character, Y matches an optional alphanumeric \ncharacter, 9 matches a numeric character, and 0 matches an optional numeric character. All other characters \nonly match themselves (e.g., - matches - ). This particular pattern will match a string with nine or \nten numeric characters. Assuming a uniform probability for each possibility, cal\u00adculating the number \nof matching 7-character numeric strings is straightforward: k = 107. Again, k is often very large, so \na small number of HITs suf.ce to achieve high con.dence in the result. As with checkbox questions, AUTOMAN \nmust treat free\u00adtext questions specially to cope with lazy workers. Here a lazy worker might simply hit \nthe Submit button, submitting the empty string and biasing the answer. To avoid this problem, AUTOMAN \nonly accepts the empty string if it is explicitly allowed as a legal input. In this case, AUTOMAN requires \nthat workers enter a special value, N/A. (a) A radio-button question. k = c.  l (c) A free-text question. \nk =i=0 pi where l is the length of the pattern and pi is the number of distinct characters matched by \nthe picture clause at position i. Figure 4. Question types handled by AUTOMAN [15]. We are currently \ninvestigating the use of a subset of regular expressions as input speci.cations. Regular expressions \nare far more powerful than COBOL picture clauses, but also more dif.cult to analyze. The number of matching \nstrings must be bounded, otherwise the probability calculation is not possible. This limitation rules \nout the use of the *, +, and {x,} (where x is an integer) operators in patterns. Alternation must also \nbe handled carefully as the pattern a?a?b matches only three strings, b, ab, and aab. A na\u00a8ive counting \nimplementation would count ab twice. 5.1 Overview of the Quality Control Algorithm In order to return \nresults within the user-de.ned con.dence interval, AUTOMAN iteratively calculates two threshold functions \nthat tell it whether it has enough con.dence to terminate, and if not, how many additional workers it \nmust recruit. Formally, the quality control algorithm depends on two functions, t and \u00a3, and associated \nparameters a, \u00df, and p *. The t(n, a) and \u00a3(p *,\u00df) functions are de.ned such that: t(m, a) is the threshold \nnumber of agreeing votes. If the workers vote randomly (i.e., each answer is chosen with equal probability), \nthen the probability that an answer meets the threshold of t(n, a) when n votes are cast is at most a. \na will be determined based on the con.dence parameter chosen by the programmer (a = 1 -con.dence).  \n\u00a3(p *,\u00df) is the minimum number of additional workers to recruit for the next step. If there is a popular \noption such that the probability a worker chooses it is p and p>p * (and all other options are equally \nlikely), then if AUTO-MAN receives votes from n = \u00a3(p *,\u00df) workers some answer will meet the threshold \nt(n, a) with probability at least 1 - \u00df.  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Humans can perform many tasks with ease that remain difficult or impossible for computers. Crowdsourcing platforms like Amazon's Mechanical Turk make it possible to harness human-based computational power at an unprecedented scale. However, their utility as a general-purpose computational platform remains limited. The lack of complete automation makes it difficult to orchestrate complex or interrelated tasks. Scheduling more human workers to reduce latency costs real money, and jobs must be monitored and rescheduled when workers fail to complete their tasks. Furthermore, it is often difficult to predict the length of time and payment that should be budgeted for a given task. Finally, the results of human-based computations are not necessarily reliable, both because human skills and accuracy vary widely, and because workers have a financial incentive to minimize their effort.</p> <p>This paper introduces AutoMan, the first fully automatic crowdprogramming system. AutoMan integrates human-based computations into a standard programming language as ordinary function calls, which can be intermixed freely with traditional functions. This abstraction lets AutoMan programmers focus on their programming logic. An AutoMan program specifies a confidence level for the overall computation and a budget. The AutoMan runtime system then transparently manages all details necessary for scheduling, pricing, and quality control. AutoMan automatically schedules human tasks for each computation until it achieves the desired confidence level; monitors, reprices, and restarts human tasks as necessary; and maximizes parallelism across human workers while staying under budget.</p>", "authors": [{"name": "Daniel W. Barowy", "author_profile_id": "81548608456", "affiliation": "University of Massachusetts, Amherst, MA, USA", "person_id": "P3856151", "email_address": "dbarowy@cs.umass.edu", "orcid_id": ""}, {"name": "Charlie Curtsinger", "author_profile_id": "81488673489", "affiliation": "University of Massachusetts, Amherst, MA, USA", "person_id": "P3856152", "email_address": "charlie@cs.umass.edu", "orcid_id": ""}, {"name": "Emery D. Berger", "author_profile_id": "81100228645", "affiliation": "University of Massachusetts, Amherst, MA, USA", "person_id": "P3856153", "email_address": "emery@cs.umass.edu", "orcid_id": ""}, {"name": "Andrew McGregor", "author_profile_id": "81100422581", "affiliation": "University of Massachusetts, Amherst, MA, USA", "person_id": "P3856154", "email_address": "mcgregor@cs.umass.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384663", "year": "2012", "article_id": "2384663", "conference": "OOPSLA", "title": "AutoMan: a platform for integrating human-based and digital computation", "url": "http://dl.acm.org/citation.cfm?id=2384663"}