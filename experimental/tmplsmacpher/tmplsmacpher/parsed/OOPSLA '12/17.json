{"article_publication_date": "10-19-2012", "fulltext": "\n Work-Stealing Without The Baggage * Vivek Kumar , Daniel Frampton \u00a7, Stephen M. Blackburn , David Grove \n, Olivier Tardieu Australian National University \u00a7Microsoft IBM T.J. Watson Research Abstract Work-stealing \nis a promising approach for effectively ex\u00adploiting software parallelism on parallel hardware. A pro\u00adgrammer \nwho uses work-stealing explicitly identi.es poten\u00adtial parallelism and the runtime then schedules work, \nkeep\u00ading otherwise idle hardware busy while relieving overloaded hardware of its burden. Prior work has \ndemonstrated that work-stealing is very effective in practice. However, work\u00adstealing comes with a substantial \noverhead: as much as 2\u00d7 to 12\u00d7 slowdown over orthodox sequential code. In this paper we identify the \nkey sources of overhead in work-stealing schedulers and present two signi.cant re\u00ad.nements to their implementation. \nWe evaluate our work\u00adstealing designs using a range of benchmarks, four dif\u00adferent work-stealing implementations, \nincluding the popu\u00adlar fork-join framework, and a range of architectures. On these benchmarks, compared \nto orthodox sequential Java, our fastest design has an overhead of just 15%. By contrast, fork-join has \na 2.3\u00d7 overhead and the previous implemen\u00adtation of the system we use has an overhead of 4.1\u00d7. These \nresults and our insight into the sources of overhead for work\u00adstealing implementations give further hope \nto an already promising technique for exploiting increasingly available hardware parallelism. Categories \nand Subject Descriptors D1.3 [Software]: Con\u00adcurrent Programming Parallel programming; D3.3 [Program\u00adming \nLanguages]: Language Constructs and Features Concurrent programming structures; D3.4 [Programming Languages]: \nPro\u00adcessors Code generation; Compilers; Optimization; Run-time en\u00advironments. General Terms Design, \nLanguages, Performance. Keywords Scheduling, Task Parallelism, Work-Stealing, X10, Managed Languages. \n* This work is supported by IBM and ARC LP0989872. Any opinions, .ndings and conclusions expressed herein \nare the authors and do not necessarily re.ect those of the sponsors. Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, \nUSA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 1. Introduction Today and in \nthe foreseeable future, performance will be delivered principally in terms of increased hardware paral\u00adlelism. \nThis fact is an apparently unavoidable consequence of wire delay and the breakdown of Dennard scaling, \nwhich together have put a stop to hardware delivering ever faster sequential performance. Unfortunately, \nsoftware parallelism is often dif.cult to identify and expose, which means it is often hard to realize \nthe performance potential of modern processors. Work-stealing [3, 9, 12, 18] is a framework for allowing \nprogrammers to explicitly expose potential paral\u00adlelism. A work-stealing scheduler within the underlying \nlan\u00adguage runtime schedules work exposed by the programmer, exploiting idle processors and unburdening \nthose that are overloaded. Work-stealing schedulers are used in program\u00adming languages, such as Cilk \n[9] and X10 [3], and in applica\u00ad tion frameworks, such as the Java fork/join framework [12] and Intel \nThreading Building Blocks [18]. Although the speci.c details vary among the various im\u00adplementations \nof work-stealing schedulers, they all incur some form of sequential overhead as a necessary side ef\u00adfect \nof enabling dynamic task parallelism. If these over\u00adheads are signi.cant, then programmers are forced \nto care\u00adfully tune their applications to expose the right amount of potential parallelism for maximum \nperformance on the targeted hardware. Failure to expose enough parallelism re\u00adsults in under-utilization \nof the cores; exposing too much parallelism results in increased overheads and lower over\u00adall performance. \nOver-tuning of task size can lead to brit\u00adtle applications that fail to perform well across a range of \ncurrent hardware systems and may fail to properly exploit future hardware. Therefore, techniques that \nsigni.cantly re\u00adduce the sequential overheads of work-stealing schedulers would simplify the programmer \ns task by mostly eliminat\u00ading the need to tune task size. In this paper, we analyze the source of the \nsequential overheads in the X10 work-stealing implementation. We then identify two substantially more \nef.cient designs. The key to our approach is exploiting runtime mechanisms al\u00adready available within \nmanaged runtimes, namely: a) dy\u00adnamic compilation, b) speculative optimization via code\u00adpatching and \non-stack-replacement, and c) support for ex\u00adception delivery. We implement our ideas in the Jikes RVM \ninfrastructure and empirically assess them using both a language-based work stealing scheduler, X10, \nand a library\u00adbased framework, Java fork/join. We evaluate our implemen\u00adtation on a range of x86 processors \nand use the OpenJDK JVM to validate some of our important results. Our new de\u00adsigns reduce the sequential \noverhead of X10 s work-stealing implementation to just 15%, down from a factor of 4.1\u00d7. Our implementation \nalso performs substantially better than the alternative fork-join framework on our benchmark set. The \nprincipal contributions of this paper are as follows: a) a detailed study of the sources of overhead \nin an existing work-stealing implementation; b) two new designs for work\u00adstealing that leverage mechanisms \nthat exist within modern JVMs; c) an evaluation using a set of benchmarks ported to run in native Java, \nthe Java fork-join framework, and X10; and d) performance results that show that we can almost completely \nremove the sequential overhead from a work\u00adstealing implementation. These contributions should give further \nimpetus to work-stealing as a model for effectively utilizing increasingly available hardware parallelism. \n2. Background This section provides a brief overview of work-stealing be\u00adfore discussing the fundamental \noperations required to im\u00adplement work-stealing. The section also describes two basic approaches for \nexpressing work-stealing: the finish-async model (as used by X10), and fork-join. Abstractly, work-stealing \nis a simple concept. Worker threads maintain a local set of tasks and when local work runs out, they \nbecome a thief and seek out a victim thread from which to steal work. The elements of a work-stealing \nruntime are often character\u00adized in terms of the following aspects of the execution of a task-parallel \nprogram: Fork A fork describes the creation of new, potentially par\u00adallel, work by a worker thread. The \nruntime makes new work items available to other worker threads. Steal A steal occurs when a thief takes \nwork from a victim. The runtime provides the thief with the execution context of the stolen work, including \nthe execution entry point and suf\u00ad.cient program state for execution to proceed. The runtime updates \nthe victim to ensure work is never executed twice. Join A join is a point in execution where a worker \nwaits for completion of a task. The runtime implements the syn\u00adchronization semantics and ensures that \nthe state of program re.ects the contribution of all the workers. 2.1 An Implementation-Oriented Overview \nThe focus of this is paper is on identifying and reducing the sequential overheads of work-stealing, \nso we now turn to those issues that are pertinent to implementing work\u00adstealing. It may help to think \nof the implementation of work\u00ad (a) Execution graph for fib(4) 1 def fib(n:Int):Int { 2 if (n < 2) return \nn; 3 4 val a:Int; 5 val b:Int; 6 7 finish { 8 async a = fib(n-1); 9 b = fib(n-2); 10 } 11 12 return \na + b; 13 } (b) Coded in X10. 1 Integer compute() { 2 if (n < 2) return n; 3 4 Fib f1 = new Fib(n -1); \n5 Fib f2 = new Fib(n -2); 6 7 f1.fork(); 8 int a = f2.compute(); 9 int b = f1.join(); 10 11 return a \n+ b; 12 } (c) Coded in Fork-Join. Figure 1. Using work-stealing to implement Fibonacci. stealing in \nterms of the following basic phases, each of which require special support from the runtime or library: \n1. Initiation. (Allow tasks to be created and stolen atomi\u00adcally). 2. State management. (Provide suf.cient \ncontext for the thief to be able to execute stolen execution, and the ability to return results). 3. \nTermination. (Join tasks and ensure correct termination).  We now explain each of these and what they \nrequire of the runtime, .rst generally, then concretely in terms of X10 and Java Fork-Join implementations. \nTo help illustrate work-stealing, we use a running ex\u00adample of the recursive calculation of Fibonacci \nnumbers. Figure 1 shows X10 and Fork-Join code for computing Fi\u00ad bonacci numbers, along with a graph \nof the recursive calls made when executing fib(4). Calls to the non-recursive base case (n<2) are shown \nas rectangles.  (b) Deque Figure 2. State during evaluation of fib(4). Execution is at the .rst fib(0) \ntask. Dashed arrows indicate work to be done. Dotted boxes indicate completed work.  2.2 Initiation \nInitiation is concerned with ensuring that: 1) tasks are avail\u00adable to be stolen whenever appropriate, \nand 2) each task is only executed once. An idle thread may make itself useful by stealing work, so becoming \na thief. This begins with the thief identifying a victim from which to steal. For example, the thief \nmay randomly select a potential victim and if they appear to have work available, attempt a steal. Tasks \nare typically managed by each worker using a double ended queue (deque), one deque per worker, as il\u00adlustrated \nin Figure 2. Each worker pushes tasks onto the head of its deque using an unsynchronized store operation. \nBoth the worker and any potential thieves then use atomic compare-and-swap (CAS) instructions to remove \ntasks from the worker s deque, with the worker acquiring from the head (newest), and thieves attempting \nto acquire from the tail (oldest). Tasks are thus made available and only stolen once, with the deque \ndiscipline minimizing contention and increasing the probability that long-running tasks are stolen. \n 2.3 State Management When a task is stolen, the thief must: 1) acquire all state re\u00adquired to execute \nthat task, and 2) provide an entrypoint to begin execution of the task, and 3) be able to return or com\u00adbine \nreturn state with other tasks. Work-stealing implemen\u00ad (b) Thief Figure 3. State for victim and thief \nafter stealing the con\u00adtinuation of fib(4). tations typically meet requirements 1) and 3) through the \nuse of state objects that capture the required information about the task, and provide a location for \ndata to be stored and shared across multiple tasks. Requirement 2) is handled dif\u00adferently depending \non the execution model, and is discussed in more detail below for speci.c systems. 2.4 Termination In \ngeneral, the continued execution of a worker is depen\u00addent on completion of some set of tasks, each of \nwhich may be executed locally, or by a thief. Such dependencies are made explicit by the programmer and \nmust be respected by the implementation of the work-stealing runtime. The work\u00adstealing runtime must: \n1) handle the general case where ex\u00adecution waits, dependent on completion of stolen tasks exe\u00adcuting \nin parallel. However, it is also critical for the sched\u00aduler to: 2) ef.ciently handle the common case \nwhere no tasks in a particular context are stolen, and therefore are all exe\u00adcuted in sequence by a single \nworker. Furthermore, work\u00adstealing schedulers also aim to: 3) maintain a particular level of parallelism. \nTo ensure that this occurs, when a worker is waiting on completion of a stolen task, instead of suspending \nthe worker, the scheduler may attempt to have that worker .nd and execute another task.  Figure 4. Sequential \noverheads for work-stealing runtimes on Jikes RVM. 2.5 Work-Stealing in X10 X10 is a strongly-typed, \nimperative, class-based, object\u00adoriented programming language. X10 includes speci.c fea\u00adtures to support \nparallel and distributed programming. A computation in X10 consists of one or more asynchronous activities \n(light-weight tasks). A new activity is created by the statement async S. To synchronize activities, \nX10 pro\u00advides the statement finish S. Control will not return from within a .nish until all activities \nspawned within the scope of the .nish have terminated. X10 restricts the use of a local mutable variable \ninside async statements. A mutable variable (var) can only be assigned to or read from within the async \nit was declared in. To mitigate this restriction, X10 permits the asynchronous initialization of .nal \nvariables (val). A .nal variable may be initialized in a child async of the declaring async. A de.nite \nassignment analysis guarantees statically that only one such initialization will be executed on any code \npath, so there will never be two con.icting writes to the same variable. X10 s work-stealing scheduler \nis implemented as the combination of an X10-source-to-X10-source program trans\u00adformation and a runtime \nlibrary. The program transformation synthesizes code artifacts (continuation methods and frame classes) \nrequired by the runtime scheduler. X10 meets the key work-stealing requirements as follows: Initiation. \nX10 s work-stealing workers use deques as de\u00adscribed above. Like Cilk, X10 adopts a work-.rst schedul\u00ading \npolicy: when a worker encounters an async statement, it pushes the continuation of the current task to \nits deque and proceeds with the execution of the async body. For instance, a worker running fib(4) .rst \nexecutes fib(3) (Figure 1(b) line 8), making the fib(2) work item (line 9) available for others to steal. \nWhen done with async fib(3), the worker attempts to pop the head deque item and if non-null, will execute \nthe continuation (fib(2), line 9). State management. Each thread s stack is private, so in or\u00adder to \npermit multiple workers to concurrently access and update the program state, the X10 compiler encapsulates \nsharable state into frame objects. Consequently, methods are rewritten to operate on .elds of frame objects \ninstead of local variables. Frame objects are linked together into trees that shadow the tree structure \nof the task graph. In other words, Figure 1(a) represents the tree of frame objects assembled during \nthe execution of fib(4). When X10 is compiled to Java, frame objects are created on the heap to ensure \nthat they are accessible to both the worker and potential thieves. In the C++ implementation however, \nan optimization is per\u00adformed that sees frame objects stack-allocated by default, and only lazily migrated \nto the heap when a steal occurs [19]. The X10 compiler analyzes the source code and indexes all of the \npoints immediately after async statements ( reentry points). It then generates a second copy of the source \ncode in which methods take a pc (program counter) as an extra argu\u00adment. The control .ow of the generated \nmethods is altered so as to permit starting execution at the speci.ed pc. Termination. If a worker proceeds \nfrom the beginning of a .nish block to its end without detecting a steal, then that worker has itself \ncompleted every task in the .nish context and may return. Termination is more complex when a steal occurs. \nWhen a thief steals a work item within the scope of a finish, the scheduler begins maintaining an atomic \ncount of the active tasks within that finish body. When a worker completes a task, or execution reaches \nthe end of the finish body, the count is atomically reduced and checked. If the count is non-zero, the \nworker gives up and searches for other work to process. When the count is zero, then the .nish is complete \nand worker starts executing the continuation of the .nish statement.  2.6 Work-Stealing in Java Fork-Join \nThe general design of Fork-Join framework is a variant of the work-stealing framework devised by Cilk \nand is explained in detail in [12]. Here we brie.y discuss its key components. The Fibonacci code for \nFork-Join is shown Figure 1(c). Initiation. The Fork-Join library includes both help-.rst and work-.rst \nimplementations. To allow fib(n-1) to be stolen, the user explicitly heap-allocates a Fib object (Fig\u00adure \n1(c), line 4) and calls fork() on this object (line 7). Like X10, every worker thread maintains a deque. \nfork pushes a task to the deque, making it available to be stolen. State Management. In Fork-Join, tasks \nare represented as task objects. These objects include: methods for scheduling and synchronizing with \nthe task, any state associated with the task, and an explicit entrypoint for executing the task. Termination. \nWhen a worker thread encounters a join operation, it processes other tasks, if available, until the subject \nof the join has been completed (either by the worker or by a thief). When a worker thread has no work \nand fails to steal any from others, it backs off (via yield, sleep, and/or priority adjustment) and tries \nagain later unless all workers are known to be similarly idle, in which case they all block until another \ntask is invoked from the top-level. 3. Motivating Analysis As we noted earlier, although work-stealing \nis a very promis\u00ading mechanism for exploiting software parallelism, it can bring with it formidable overheads \nto the simple sequen\u00adtial case. These overheads make the task of the program\u00admer challenging because \nthey must use work-stealing judi\u00adciously so as to extract maximum parallelism without incur\u00adring crippling \nsequential overheads. To shed light on this and further motivate our designs, we now: 1) identify and \nmea\u00adsure the sequential overheads imposed by existing work\u00adstealing runtimes, and 2) measure the dynamic \nsteal ratio across a range of parallel benchmarks, showing that un\u00adstolen tasks are by far the common \ncase. We use eight well\u00adknown benchmarks expressed in X10, Fork-Join and sequen\u00adtial Java. We ported \nthe code where necessary and have made the code publicly available (section 6). We discuss the details \nof our methodology in Section 6. 3.1 Sequential Overheads In order to measure sequential overheads, \nwe take work\u00adstealing runtimes and execute each of them with a single worker. No stealing can occur in \nthis case, so the runtime support for stealing is entirely redundant to this set of exper\u00adiments. This \narti.cial situation allows us to selectively leave out aspects of the runtime support, providing an opportunity \nto analyze the overheads due to work-stealing in more de\u00adtail. As a baseline we use a straightforward \n(sequential) Java implementation of each benchmark. We have identi.ed three major sources of sequential \nover\u00adhead in existing work-stealing runtimes. Two are closely re\u00adlated to the overheads identi.ed in \nthe previous sections, namely initiation and state management. The .nal overheads relate to code restructuring \nand other changes necessary to support work-stealing. Initiation. The deque is an obvious source of \noverhead for the victim, which must use synchronized operations to perform work (even when nothing is \nstolen). This overhead may be a signi.cant problem for programs with .ne-grained concurrency. To measure \nthis overhead, we took the X10 and Fork-Join work-stealing runtimes and measured sin\u00adgle worker performance \nwith all deque operations removed. (Recall that the deque manages pending work, so the strictly sequential \ncase of a single worker, it is entirely redundant.) These results are shown as X10WS (No Deque) and Fork-Join \n(No Deque) in Figure 4. These results show that the deque accounts for just over 30% and 50% of all sequential \noverheads for X10WS (Default) and Fork-Join respectively. State Management. As discussed in Section 2, \nwork\u00adstealing runtimes typically allocate state objects to allow sharing and movement of execution state \nbetween tasks. In pure Java, these objects must be heap allocated, leading to signi.cant overheads. In \naddition to the direct cost of al\u00adlocation and garbage collection, these objects may also be chained \ntogether, and may limit compiler optimizations. Fig\u00adure 4 shows the overhead of allocating these state \nobjects in the X10 Java work-stealing runtime by removing their al\u00adlocation in a system that already \nhad the deques removed. In this case all values are passed on the stack, and no copy\u00ading was required \nbecause only a single worker exists. This is shown as X10WS (No Context, No Deque) in Figure 4. We did \nnot need to perform a similar experiment for Fork-Join as it would reduce to the sequential Java case \n(and thus would show zero overhead). We can see that the allocation of these state objects is a very \nsigni.cant cost, averaging just under half of the total overhead. Code Restructuring. In order to support \nthe stealing of tasks, the runtime must generate entrypoints with which the thief can resume execution. \nThis is typically performed by splitting up methods for the different finish and asyncs. This code restructuring \naccounts for part of the performance gap between X10WS (No Context, No Deque) and sequential Java. In \neffect, this overhead includes all overheads due to X10-to-Java compilation, of which only a subset would \nbe necessary to support work-stealing. 3.2 Steal Ratio Work-stealing algorithms aim to ensure that suf.cient \ntasks are created to keep all processors busy. In practice, how\u00adever, much of this potential parallelism \nis not realized, due to other activities or a reduced availability of parallelism. Con\u00adsequently it may \nbe the case that most tasks are consumed locally. Clearly the number of stolen tasks is bounded by the \ntotal number of tasks, but the fraction of tasks actually stolen (the steal ratio) is an important component \nin determining  Figure 5. Steals to task ratio if, and how, cost should be traded off between all tasks \nand stolen tasks. We performed a study to understand the steal ratios across a range of benchmarks. We \ninstrumented each of the work\u00adstealing runtimes to measure both the total number of tasks produced (executed \nasync blocks in X10, and fork() calls in Fork-Join) as well as total number of tasks stolen. We show \nthe measured ratio in Figure 5. It is clear from the .gure that stealing is generally uncom\u00admon and in \nmany cases extremely rare. A single steal may move substantial work (consider divide-and-conquer algo\u00adrithms). \nBecause of this, relatively few steals may address a load imbalance. Although both LU and Heat workloads \nhave steal ratios that approach one in ten, for many of the bench\u00admarks the ratio is around one in a \nmillion. This result shows that steals are generally uncommon and suggest that eagerly preparing for \na steal is likely to be an inef.cient strategy. 4. Approach As discussed in Section 2, for work-stealing \nto function correctly we must be able to 1) identify a task to be stolen, 2) provide suf.cient context \nto allow a thief to execute a stolen task, and 3) reintegrate state due to computation performed by a \nthief back into the victim s execution context. Each of these operations is only required for tasks that \nare actually stolen, and as we saw in 3.2, steal ratios for many programs are close to zero. The performance \nof unstolen tasks is therefore critical to overall performance. In this work, we try to push the limits \nas to what aspects of the above functions can be deferred until a steal occurs, moving them off the critical \npath of unstolen tasks. Our particular approach is to leverage advanced facilities that exist within \nthe implementation of a modern managed runtime. 4.1 Scalability Concerns A simple measure of the success \nof a parallelization con\u00adstruct is scalability. Of course one way to improve scala\u00adbility is to enhance \nthe parallel case at the expense of the base sequential case. In practice, this is what happens with \nexisting work-stealing frameworks, which involve substan\u00adtial overheads in the sequential case. By corollary, \nour ap\u00adproach of moving overhead off the common sequential case (to be absorbed at steal time by the \nthief) will reduce the apparent scalability. In our evaluation we express scalability for all systems \nas speedup relative to the sequential Java base case, sidestepping this pitfall by focusing instead on \noverall performance. Our argument is that scalability is a means to improved overall performance, not \nan end in and of itself. The question then becomes, is it possible to build a system that aggressively \ndefers steal-related work, and what is the actual cost tradeoff of doing so. 4.2 Techniques Our approach \nis based on several basic techniques, each de\u00adscribed in more detail in the context of the implementations \ndiscussed in Section 5. 1. We use the victim s execution stack as an implicit deque. 2. We modify the \nruntime to extract execution state directly from the victim s stack and registers. 3. We dynamically \nswitch the victim to slow versions of code to reduce coordination overhead.  We are unaware of any \nprevious work that uses either of the .rst two approaches, and due to the support of a managed runtime, \nwe are able to perform the third more aggressively than prior work, and with reduced overhead in the \ncommon case. 5. Implementation We have implemented and evaluated two work-stealing run\u00adtimes, X10WS \n(OffStack): a modi.cation of the default X10 work-stealing runtime for JVMs, and X10 (Try-Catch): a simple \nruntime implementation on plain Java. Both imple\u00admentations support the X10 finish-async model of execu\u00adtion. \nOur plain Java X10 (Try-Catch) runtime is targeted by the X10 compiler. This section describes each of \nour work stealing runtime implementations in terms of the work-stealing requirements we enumerated in \nSection 2: initiation, state management, and termination. 5.1 Runtime Supported X10WS (OffStack) 5.1.1 \nInitiation One of the key insights behind the X10WS (OffStack) de\u00adsign is that we can avoid maintaining \nan explicit deque by using existing runtime mechanisms to extract the informa\u00adtion from the worker s \ncall stack. This approach eliminates the signi.cant overhead of managing an explicit deque, but requires \nalternative mechanisms to synchronize the victim and thief, and to manage the set of stealable tasks. \nStack as an implicit deque. In our system the deque is im\u00adplicitly stored within the worker s call stack. \nThe X10 com\u00adpiler transforms each X10 async body into a separate Java method (as it does normally). We \nattach @IsAsync annota\u00adtions to these methods, and @HasContinuation annotations to all methods that call \nasync (and thus have continuations). A stealable continuation is identi.ed by a caller callee pair with \na caller marked @HasContinuation and a callee marked @IsAsync. The head of the deque corresponds to the \ntop of the worker s stack. Each worker maintains a stealFrame pointer, which points to the tail of the \ndeque and is managed as described below. The body of the deque is the set of all stealable frames between \nthe top of the stack and the frame marked by stealFrame. Any worker thread with a non-null stealFrame \n.eld is a potential victim. Victim Thief handshake. Once a thief .nds a potential victim, it uses the \nruntime s yieldpoint mechanism to force the victim to yield the victim is stopped while the steal is \nperformed. The yieldpoint mechanism is used extensively within the runtime to support key features, including \nexact garbage collection, biased locking, and adaptive optimiza\u00adtion. Reusing this mechanism allows us \nto add the hooks to stop the victim without any additional overhead. Note that between the point at which \na thief attempts a steal, and the point the victim thief handshake begins, it is possible that a task \nmay no longer be available to steal. We measured the frequency of such failed steal attempts in our evaluation \nat around 5-10%. Maintaining stealFrame. Recall that stealFrame is the pointer to the tail of the implicit \ndeque. Workers and thieves maintain stealFrame cooperatively. When a worker starts executing a task, \nstealFrame is null, signifying that there is nothing available to steal. When a worker wants to add a \ntask to its (implicit) deque, it .rst checks stealFrame. If stealFrame is null, then the implicit deque \nis empty so stealFrame is updated to point to the new task, which is now the tail of the (implicit) deque. \nIf stealFrame is already set, then the new task is not the tail so stealFrame is left un\u00admodi.ed. stealFrame \nmust also be updated as tasks are re\u00admoved. A worker must clear stealFrame when it consumes the tail. \nA thief updates stealFrame during a steal to either point to the next stealable continuation, or null \nif no other stealable continuation exists. Ensuring atomicity. A worker detects that a continua\u00adtion \nit expected to return to has been stolen by checking if stealFrame has been set to null. In this case \nthere is no work left locally, and the worker becomes a thief and searches for other work to execute. \n 5.1.2 State Management When a task is stolen, the thief must take with it suf.cient state to run the \ntask within the thief s own context. This includes all local variables used by the stolen task. In our \nrunning example, it is just the parameter n, which is used on line 9 of Figure 1(b). In the X10WS (OffStack) \nsystem, we perform lazy state extraction, extracting the state from the victim thread only when a steal \noccurs. Extracting state off the stack. We extract state from the victim stack into the heap so that \nthe thief may access the state while the victim continues to execute. Because the vic\u00adtim is stopped \nduring a steal, we are trivially able to dupli\u00adcate its complete execution state down to the steal point, \nin\u00adcluding the stack and registers. At this point the victim may resume execution. The thief then extracts \nthe state out of the duplicate stack for each stolen continuation. It does this by unwinding the duplicate \nstack whilst a copyingStates .ag is set, which causes re.ectively generated code to be exe\u00adcuted for \neach frame. The re.ective code captures local state for the frame and interns it in a linked list of \nheap objects, one object per frame. At this point all necessary victim state has been captured and the \nthief may commence execution. The principal difference between our approach and the default X10 mechanism \nis that we perform state extraction lazily, only when an effective steal occurs. Compared to the X10 \nJava backend, our lazy approach avoids a large amount of heap allocation. The X10 C++ backend also has \nan approach which delays the allocation, but not the use of state objects, by initially allocating them \non the stack and only lazily moving them to the heap when a steal occurs [19]. In the common case our \napproach avoids state extraction and allocation altogether. Executing stolen tasks. Once state has been \nextracted, the thief executes against its heap-interned duplicate stack. The thief executes specially \ngenerated slow versions of the continuations, which access the heap rather than the call stack for local \nvariables. This is essentially identical to the default X10 implementation.  5.1.3 Termination Control \nmust only return from a finish context when all tasks within the context have terminated. To support \nthis, each thread has a singly linked list with a node for each finish context that the thread is executing. \nEach dynamic finish context has a unique node shared by all threads running in that context. These nodes \nform a tree structure, with a root node for the finish context that represents the entire program. In \nX10WS (OffStack), four important pieces of information are saved at each .nish node: A linked list of \nstolen states.  Frame pointers that identify where it was stolen from the victim, and where it is now \nrunning in the thief.  A synchronized count of the number of active workers (initially 2 for the thief \nand victim).  An object for storing partial results.  To ensure termination, when each thread leaves \nthe finish context they decrement the synchronized count. The thread that drops the count to zero is \nresponsible for execut\u00ading the continuation of the finish context. The nodes are also used as the point \nfor communicating any data that is required to be made available after the finish.  5.2 X10 (Try-Catch) \nJava implementation Our X10 (Try-Catch) implementation is more radical. Our principal goal was to understand \njust how far we could re\u00adduce sequential overheads. To do this we started with plain Java and built a \nbasic work-stealing framework upon it. We have modi.ed the X10 compiler to compile to X10 (Try-Catch). \nThus X10 (Try-Catch) represents a new backend for work-stealing. Because we express X10 (Try-Catch) directly \nin plain Java code, it is straightforward to make direct com\u00adparisons with Java Fork-Join and sequential \nJava. 5.2.1 Leveraging Exception Handling Support In Java, the programmer may wrap sections of code \nin try blocks, and provide catch blocks to handle particular types of runtime exceptions. When an exception \nis thrown within the context of a try block for which there is a catch block that matches the exception \ns type, control is transferred to the start of the catch block. Exceptions propagate up the call stack \nuntil a matching catch block is found, or if no matching block is found the thread is terminated. To \nsupport exception handling, the runtime must maintain a table with entries that map the instruction address \nrange of a try block to the instruction address for the catch block, annotated by the type of exception \nthat can be handled. Exception handling is designed to allow for the graceful handling of errors. Because \nexceptions are important and po\u00adtentially expensive, JVM implementers have invested heav\u00adily in optimizing \nthe mechanisms. Our insight is to leverage these optimized mechanisms to ef.ciently implement the pe\u00ad \n1 int fib(n) { 2 if (n < 2) return n; 3 int a,b; 4 try { 5 try { 6 WS.setFlag(); 7 a = fib(n-1); 8 WS.join(); \n9 } catch (WS.JoinFirst j) { 10 j.addFinishData(0, a); 11 WS.completeStolen(); 12 } catch (WS.Continuation \nc) { 13 // entry point for thief 14 } 15 b = fib(n-2); 16 WS.finish(); 17 } catch (WS.FinishFirst f) \n{ 18 f.addFinishData(1, b); 19 WS.completeFinish(); 20 } catch (WS.Finish f) { 21 for(WS.FinishData fd: \nf.data) { 22 if (f.key == 0) a = fd.value; 23 if (f.key == 1) b = fd.value; 24 } 25 } 26 return a + b; \n27 } Figure 6. Pseudocode for the transformation of fib(n) into X10 (Try-Catch). culiar control .ow \nrequirements of work-stealing. We can avoid much of the expense generally associated with excep\u00adtion \nhandling as we never generate a user-level stack trace; we do not require this trace for work-stealing \n(we only need the VM-level stack walk). Our X10 (Try-Catch) system annotates async and finish blocks \nby wrapping them with try/catch blocks with special work-stealing exceptions as shown in Figure 6. We \ncan then leverage the exception handling support within the runtime and runtime compilers to generate \nexception table entries to support work stealing. These allow the X10 (Try-Catch) runtime to walk the \nstack and identify all async and finish contexts within which a thread is executing. The role of each \nexception type, and how the information is used in the runtime are described in more detail over the \nfollowing sections. 5.2.2 Initiation As in the X10WS (OffStack) implementation, X10 (Try-Catch) avoids \nmaintaining an explicit deque. The key differ\u00adence between the implementations is that in X10 (Try-Catch) \nwe use marker try/catch blocks, not method annotations, to communicate the current deque state to the \nwork-stealing runtime. Instead of identifying a continuation by a pair of methods using a frame pointer, \nwe use a combination of the frame pointer and the offset of a speci.c catch block. We use the same thief-initiated \nhandshake for synchronization between the victim and the thief. Stack as an implicit deque. X10 (Try-Catch) \nmaintains a steal .ag for each worker thread that indicates that its deque may have work available to \nsteal. The steal .ag is set as the .rst action within an async (see line 6 in Figure 6). The steal .ag \nis cleared when the worker or a thief determines that there is no more work to steal. As with X10WS (OffS\u00adtack), \nthe head of the task deque corresponds to the top of the call stack. The list of continuations (from \nnewest to oldest) is established by walking the set of catch blocks that wrap the current execution state. \nWe walk this list by running a modi.ed version of the runtime s exception delivery code, searching for \ncatch blocks that handle WS.Continuation exceptions. As we .nd entries, we simulate advancing into the \ncatch block and repeat the search for exception han\u00addlers again, .nding successively older continuations. \nEach worker has a stealToken that acts as a tail for the deque. The stealToken indicates the point at \nwhich the continua\u00adtion of that worker has been stolen. Any continuations dis\u00adcovered after that point \ndo not belong to that worker, and must therefore not be stolen from it. Ensuring atomicity. Atomicity \nis guaranteed through the use of the stealToken, which acts as a roadblock for the worker and thieves \nto prevent either running or stealing con\u00adtinuations past that point. We saw above how the stealToken \nis used during the steal operation. To ensure that the vic\u00adtim does not run the continuation again, each \nasync ends with a call to WS.join(). This call is responsible for check\u00ading whether the continuation \nhas been stolen. This requires checking whether the frame pointer and catch block off\u00adset for the stealToken \nmatch the innermost continuation for that async, which is discovered using modi.ed exception de\u00adlivery \ncode. When a steal is performed, the thief must also steal the stealToken from the victim thread, and \nplace a new stealToken on the victim to prevent it from executing the stolen continuation.  5.2.3 State \nManagement In X10 (Try-Catch), state is acquired by the thief through duplicating the entire execution \nstate of the victim thread, in\u00adcluding the stack and register state. Unlike in X10WS (Off-Stack), the \nstate is not extracted to the heap, but is used directly. No additional resume method is required for \nthe entrypoint; execution can be transferred to the appropriate continuation point by delivering a WS.Continuation \nexcep\u00adtion. The exception delivery code must be slightly different, because the exception must be delivered \nto the correct han\u00addler (it is not always the innermost exception handler for WS.Continuation that is \ncorrect). Merging local variable state. While providing the correct state to start the continuation is \nmade easy, X10 (Try-Catch) does not have a resume mode to fall back on where local vari\u00adable state is \nall stored on the heap. This complicates merg\u00ading the results of each of the tasks because the system \nmust merge the local variables held by multiple copies of the same frame. In the Fibonacci example, the \nvalue of a is set within the async, while the value of b is set in the continuation. After the .nish, \nboth a and b must be set to ensure that the correct value is returned. At the end of each async or finish \nblock there is a call to a runtime support method (WS.join() or WS.finish()). When these methods are \ncalled, two con\u00additions are checked: 1) a steal has occurred within the ap\u00adpropriate .nish block, and \n2) the programmer has de.ned a catch block (WS.Join or WS.FinishFirst respectively) to save results. \nIf both conditions are met, control is re\u00adturned to the catch block by throwing an exception, allow\u00ading \nthe code to provide local variable values with calls to addFinishData(key, value). Each key represents \na local variable: in our example key 0 maps to a and 1 maps to b. The last task to .nish executing within \nthe .nish can then access all of these provided values, ensuring that all results are set correctly. \n 5.2.4 Termination Termination is handled in a similar way to X10WS (OffS\u00adtack). A node is lazily created \nfor each finish context in which a task is stolen. This node maintains an atomic count of the number \nof active tasks in the finish context, and provides a location for local variable state to be passed \nbe\u00adtween threads, as described in the previous section. When a thread decrements the atomic count to \nzero, it becomes responsible for running the continuation of the finish con\u00adtext. The X10 (Try-Catch) \nruntime will deliver a WS.Finish exception at the appropriate point, allowing the thread to ex\u00adtract \nlocal variable state and continue out from the finish. This may also update the thread s stealToken, \nif the last thread to .nish execution was not the thread running the end of the finish body. When this \noccurs, that thread runs the body of any WS.FinishFirst handler to communicate lo\u00adcal variables, deposits \nits stealToken in the .nish node, and searches for other work to complete. 5.2.5 Optimizing Runtime \nSupport Calls Within the X10 (Try-Catch) runtime, there are many calls to various WS methods. In the \ncommon case where no steal has occurred, only the call to WS.setFlag() needs to be ex\u00adecuted. The call \nto WS.join() is only required if the con\u00adtinuation for the enclosing async has been stolen. Similarly, \nthe call to WS.finish() is only required if at least one steal has occurred within that finish context. \nTo avoid sequen\u00adtial overhead in the common case, we generate special fast versions of methods with these \ncalls, where compiled code for the calls to these methods are overwritten by NOP instruc\u00adtions. This \nmakes it simple for us to transfer execution be\u00adtween these methods as required, without requiring any \nad\u00additional exception handling tables for the fast version of the code. Both fast and slow versions of \nthe code always make calls to fast versions. We also force calls to WS.setFlag() to be inlined by the \noptimizing compiler, which on our primary Intel platform reduces to a simple store instruction. Excluding \nindirect changes in compilation due to the presence of the try/catch blocks, the only sequential over\u00adhead \nin X10 (Try-Catch) is the execution of WS.setFlag(), and some additional NOP instructions. 6. Methodology \n 6.1 Benchmarks Because the primary goal of our work is to reduce the se\u00adquential overheads of work-stealing, \nwe have intentionally selected benchmarks with fairly .ne-grained task struc\u00adtures. As the parallel tasks \nbecome coarser, the overheads of work-stealing become less signi.cant for overall perfor\u00admance and the \nperformance of all the approaches to work\u00adstealing tend to converge. We have used a collection of eight \nbenchmarks, which are brie.y described below (they are available at http://cs.anu.edu.au/ vivek/ ws-oopsla-2012/). \nFor each case we ported the bench\u00ad mark to native Java (for the sequential case), Java Fork-Join, X10 \n(using both the try-catch and default targets), and our plain Java try-catch system. For six of the eight \nbenchmarks we manually generated the code to target the X10WS (OffS\u00adtack) runtime (we did not implement \nautomatic codegen sup\u00adport for X10WS (OffStack)). Having implementations of a common set of benchmarks \nallowed us to perform apples-to\u00adapples comparisons of the different work-stealing systems. Unless speci.ed \nbelow, each benchmark is run without any granularity parameter. The six benchmarks we have for all systems \nare: Fibonacci A simple recursive computation of Fibonacci numbers. This benchmark is a commonly used \nmicro\u00adbenchmark for task scheduling overhead, as the problem is embarrassingly parallel and the amount \nof computation done within each task is trivial. For our experiments we computed the 40th Fibonacci number. \nIntegrate Recursively calculate area under a curve for the 3 polynomial function x+ x in the range 0 \n<= x<= 10000. This benchmark is similar in spirit to Fibonacci, but each task contains an order of magnitude \nmore work. Jacobi Iterative mesh relaxation with barriers: 100 steps of nearest neighbor averaging on \n1024 \u00d7 1024 matri\u00adces of doubles (based on an algorithm taken from Fork-Join [12]). Matmul Matrix multiplication \nof 1024 \u00d7 1024 matrices of doubles (based on an algorithm from Habanero Java [2]). NQueens The classic \nN-queens problem where 12 queens are placed on a 12 \u00d7 12 board such that no piece could move to take \nanother in a single move (based on an algorithm from Barcelona OpenMP Tasks Suite [6]). Quicksort A recursive \nalgorithm to quicksort a 100 million element array. It is very sensitive to memory bandwidth due to the \nconsequences of having to move data between processors, and thus to aggregate memory bandwidth of the \nsystem as a whole. The .nal two benchmarks are signi.cantly more compli\u00adcated and we did not perform \nthe manual code translation required for X10WS (OffStack): LU Decomposition Decomposition of 1024\u00d71024 \nmatri\u00adces of doubles (based on algorithm from Cilk-5.4.6 [16]). Block size of 16 is used to control the \ngranularity. Heat Di.usion Heat diffusion simulation across a mesh of size 4096 \u00d7 1024 (based on algorithm \nfrom Cilk\u00ad5.4.6). Leaf column size of 10 is the granularity parame\u00adter. Timestep used is 200. 6.2 Hardware \nPlatform All experiments were run on a machine with two Intel Xeon E7530 Nehalem processors. Each processor \nhas six cores running at 1.87 GHz sharing a 12 MB L3 cache. The ma\u00adchine is con.gured with 16 GB of memory. \n 6.3 Software Platform We modi.ed both X10 and Jikes RVM, starting with the base versions described below. \nJikes RVM Version 3.1.2. We used the production build. X10 (Try-Catch) and X10WS (Default) Based on X10 \n2.2.2.1, svn revision 23688. Fork-Join Version 1.7.0. X10WS (OffStack) Based on X10 2.1.2, svn revision \n20276. Cilk++ Intel s Cilk++ SDK preview (build 8503) [1]. We compile our benchmarks with optimization \nlevel -O2 and used the Miser memory manager to avoid the lock con\u00adtention and false sharing associated \nwith C/C++ runtime memory management functions [15]. OpenJDK 64-Bit Server VM (build 20.0-b11, mixed \nmode). 6.4 Measurements For each benchmark con.guration combination, we ran six invocations, with three \niterations per invocation, where each iteration performed the kernel of the benchmark .ve times. We report \nthe mean of the .nal iteration, along with a 95% con.dence interval based on a Student t-test. For each \nin\u00advocation of the benchmark, the total number of garbage col\u00adlector threads is kept the same as application \nthreads. A heap size of 921 MB is used across all systems. Other than this, all VMs used in our experiments \npreserve their default settings. Many of the benchmarks make extensive use of arrays. While the Fork-Join \nand sequential versions of the bench\u00admarks use Java arrays directly, the X10 compiler is not cur\u00adrently \nable to optimize X10 array operations directly into Java array operations, but does so through a wrapper \nwith get/set routines. To understand the signi.cance of this over\u00adhead, we also measure a system that \nwe call JavaWS (Try-Catch), which uses try-catch work-stealing but operates di\u00adrectly on Java arrays \nwithout X10. 7. Results We start by measuring the sequential overhead of each of the systems before \nevaluating overall performance, includ\u00ading speedup. We then examine the effect of the different approaches \non memory management overheads. We .nish by measuring steal ratios and failed steal attempts for each \nbenchmark using the modi.ed systems. 7.1 Sequential Overhead Our primary focus for this paper is the \nreduction of sequen\u00adtial overheads as a means of improving overall throughput. Using the same methodology \nas in Section 3.1, we restrict the work-stealing runtimes so that they only use a single worker thread \nand then compare their performance to the purely sequential version of the program. Figure 7 shows the \nsequential overhead of the original X10WS (Default), Fork-Join, our two optimized implemen\u00adtations, and \nthe JavaWS (Try-Catch) system that uses regular Java arrays. Sequential overheads for X10WS (Default) \nand Fork-Join are as high as 18\u00d7 and 8.4\u00d7 respectively (both for the Fibonacci benchmark). On average \nX10WS (OffStack) eliminates more than half of the sequential overheads of X10WS (Default) and performs \nslightly better than Fork-Join. The X10 (Try-Catch) implementation has consistently low sequential overheads \nacross all benchmarks, including for Heat Di.usion and LU Decomposition, where the se\u00adquential overhead \nis already quite low on all the systems.  7.2 Work Stealing Performance Figure 8 shows the speedup relative \nto sequential Java for each of the benchmarks and runtimes on our 12 core ma\u00adchine. Note that we did \nnot measure LU Decomposition and Heat Di.usion for X10WS (OffStack). This is because we do not have automatic \ncodegen support for X10WS (Off-Stack). These results clearly illustrate that the sequential overheads \nof work-stealing are the dominant factor in over\u00adall program performance. The results for the JavaWS \n(Try-Catch) runtime are extremely promising. Even in extreme examples of .ne-grained concurrency like \nFibonacci and In\u00adtegrate it is able to outperform the sequential version of the program at 2 cores and \ndeliver a signi.cant speedup at 12 cores (7\u00d7 and 8.5\u00d7 respectively). Despite exhibiting excel\u00adlent scalability, \nneither X10WS (Default) or Fork-Join are able to overcome their larger sequential overheads and show \nsigni.cant performance improvements over the sequential code for Fibonacci or Integrate even when using \nall 12 cores. The differences between the runtimes are less dra\u00admatic on the other .ve benchmarks, but \nthe overall trend holds. All four runtimes show reasonable levels of scala\u00adbility, but the lower sequential \noverheads of JavaWS (Try-Catch) and X10WS (OffStack) result in better overall per\u00adformance. In Figure \n8 LU Decomposition and Heat Di.usion show unusual behavior. In LU Decomposition Fork-Join outper\u00adforms \nX10 (Try-Catch), and in Heat Di.usion all of the systems perform nearly identically. Figure 5(a) shows \nthat LU Decomposition and Heat Di.usion are two benchmarks with high steal ratios (8% and 13% respectively \nat 12 cores). From Figure 2.4 we can also see that the LU Decomposition and Heat Di.usion benchmarks \nhave almost zero sequential overhead, indicating that the total number of stealable tasks is low. This \nis not a situation where our approach will deliver signi.cant gains, but we can see that JavaWS (Try-Catch) \nstill performs nearly the same as all other systems. To increase the con.dence in our results, we also \ncom\u00adpared overall performance with Cilk++, a C++ implementa\u00adtion of work stealing, and Fork-Join running \non OpenJDK. Figure 9 shows the result of this experiment. In most cases, the running time for JavaWS \n(Try-Catch) is very compet\u00aditive, particularly as the number of threads is increased. A notable exception \nis LU Decomposition where the JavaWS (Try-Catch) implementation is signi.cantly slower. The Jikes RVM \nresults in Figure 8(g) show that this slowdown affects all Jikes RVM con.gurations, not just JavaWS (Try-Catch), \nsuggesting pathology in the underlying VM which is independent of our work stealing implementations. \nWe are looking into this. 7.3 Memory Management Overheads A signi.cant source of performance improvement \nis due to the fact that X10WS (OffStack) dramatically reduces the number of heap-allocated frame objects, \nand X10 (Try-Catch) removes them altogether. Figure 10 shows the frac\u00ad tion of time spent performing \ngarbage collection for each of the systems measured. As expected, X10WS (OffStack) and X10 (Try-Catch) \nhave signi.cantly lower memory manage\u00adment overheads than the other work-stealing runtimes. There is \nstill measurable time spent in garbage collection for the NQueens and LU Decomposition benchmarks, but \nthis is the case even for the sequential versions of these benchmarks. Across all programs the garbage \ncollection fraction is less than 10%. Note that the garbage collection fraction does not include the \npotentially signi.cant cost of object allocation during application execution. To ensure our performance \nimprovements were not due to poor collector performance in Jikes RVM, we also measured the Java based \nsystems on OpenJDK, and saw that the collection time fraction was similar, and we know from previous \nwork [21] that the allo\u00ad cation performance of Jikes RVM is highly competitive. 7.4 Steal Ratios To \nensure that our modi.cations did not dramatically affect behavior, we also measured the steal ratios \nfor our optimized systems. The results in Figure 11(c) for X10WS (OffStack) do not differ signi.cantly \nto those for the original system in Figure 5(a). The steal ratio for X10 (Try-Catch) is between the steal \nratios for the other two systems and is shown in  * We have no implementation of LU Decomposition or \nHeat Di.usion for X10WS (OffStack). Figure 7. Overhead when running with a single thread (relative to \nsequential Java). Figure 11(a). We also measured the frequency at which steal attempts failed (due to \neither another thief or the victim winning the race to start that continuation). Figure 11(b) and Figure \n11(d) show the failed steal attempts for X10 (Try-Catch) and X10WS (OffStack) respectively. In general, \nthe fraction of steal failures is less than 10%, only rising above this .gure for a subset of benchmarks \n(Matmul, LU Decomposition, and Heat Di.usion) when running with a small number of threads. We noticed \nthat the steal ratio for Matmul with three threads is a persistent outlier for X10 (Try-Catch), X10WS \n(OffStack), and also Fork-Join and X10WS (Default) (Fig\u00adures 5(a) and 5(b)). We are investigating this \nanomaly.  7.5 Summary These results demonstrate that our approach is extremely ef\u00adfective at reducing \nsequential overheads, and does not do this at the cost of scalability. While the size of the bene.t depends \non the nature of the benchmark, in the cases where our approach does not provide a signi.cant bene.t, \nimpor\u00adtantly it also does not negatively affect performance. 8. Related Work The ideas behind work-stealing \nhave a long history which includes lazy task creation [17] and the MIT Cilk project [9], which offered \nboth a theoretical and practical framework. Languages versus Libraries Work-stealing has been made available \nto the programmer as libraries or as part of lan\u00adguages. Java s fork/join framework [12], Intel s Threading \nBuilding Blocks [18], PFunc [11], and Microsoft s Task Par\u00ad allel Library [13] are all examples of libraries \nthat imple\u00ad ment work-stealing. Users write explicit calls to the library to parallelize their computation, \nas in Figure 1(c). X10, the Cilk-5 runtime [8] and the Habanero runtime [2] on the other hand are all \nexamples of direct language support for work\u00adstealing. In principle, a language supported work-stealing \nimplementation has more opportunities for optimization be\u00adcause it can bind to and leverage internal \nruntime mecha\u00adnisms that are not visible to a library implementation. Con\u00adversely, library implementations \nhave the pragmatic advan\u00adtage of being applicable to pre-existing languages. Work-stealing Deques Several \nprior studies [4, 5, 14, 20] use cut-off strategies to control the recursion depth of func\u00adtion calls \nduring the task generation. This is intended to re\u00adduce the overhead of task creation and deque operations. \nOne of our contributions is to be able to use the worker s Java thread stack as an implicit deque and \nthus eliminate the need to employ cut-off strategies to control sequential over\u00adhead. Cilk introduced \nthe concept of THE protocol [8] to man\u00ad age the deque. Actions by the worker on the head of the deque \ncontribute to sequential overhead, while actions by the thieves on the tail of the deque contribute only \nto non\u00adcritical-path overhead. Almost all modern work-stealing schedulers follow this approach. We do \nnot take this ap\u00adproach. Instead we observe that steals are infrequent and force the victim to yield \nwhen a steal occurs. Although this implies that the victim does some steal-related work, it only does \nso when a steal occurs. As long as the steal ratios are relatively modest the gain in overall system \nperformance from our approach results in better scalability than the tradi\u00adtional approach. Harnessing \nRich Features of a Managed Runtime Man\u00adaged runtimes provide many sophisticated features, which are not \nusually available in a low-level language implemen\u00adtation. A key runtime feature we use in our work is \non-stack replacement [10], which is already employed in Jikes RVM for speculative optimizations and adaptive \nmulti-level com\u00ad  (a) Fibonacci (b) Integrate (c) Jacobi (d) Matmul (e) NQueens (f) Quicksort   \n(g) LU Decomposition (h) Heat Di.usion Figure 8. Speedup relative to sequential Java.  (a) Fibonacci \n(b) Integrate (c) Jacobi (d) Matmul (e) NQueens (f) Quicksort   (g) LU Decomposition (h) Heat Di.usion \nFigure 9. Running time   (c) X10WS (OffStack) steal ratio (d) X10WS (OffStack) failure rate pilation \n[7]. To support on-stack replacement, Jikes RVM s compilers generate machine code mapping information \nfor selected program points that enable extraction of the Java\u00adlevel program state from the machine registers \nand thread stack and the transfer of this state to newly created stack frames. We use these existing \nmechanisms inside Jikes RVM to walk a victim s Java thread stack and extract all the pro\u00adgram state. \nThe thief uses this to establish the necessary con\u00adtext for it to be able to execute stolen work. The \nC++ implementation of X10 performs speculative stack allocation [19]. The victim starts by allocating \nthe frames on a stack. The thief is responsible for copying the stolen frames from the victim s stack \nto the heap. This is not possible in the Java X10 implementation since Java does not support stack allocation. \nHowever we are able to leverage the runtime s stack walking mechanism to achieve an even simpler result \nthe thread state is not preprocessed. There are no frame objects on either the stack or the heap. Instead, \nby using the virtual machine s internal thread stack walking capability, we extract the state directly \nfrom the stack when a steal occur. Our approach radically lowers the memory management load of work-stealing. \nWe are not aware of such functionality in any work-stealing scheduler. 9. Conclusion We believe that \nwork-stealing will be an increasingly impor\u00adtant approach for effectively exploiting software parallelism \non parallel hardware. In this work, we analyzed the sources of sequential overhead in work-stealing schedulers \nand de\u00adsigned and implemented two optimized work-stealing run\u00adtimes that signi.cantly reduce them by \nbuilding upon exist\u00ading runtime services of modern JVMs. Our empirical results demonstrate that we can \nalmost completely remove the se\u00adquential overhead from a work-stealing implementation and therefore obtain \nperformance improvements over sequential code even at modest core counts. We plan to continue exploring \nways in which JVM run\u00adtime mechanisms can be adapted to further improve work\u00adstealing. One avenue of \nexploration is to investigate tech\u00adniques for reducing the stop time on the victim threads by applying \nideas from concurrent garbage collection. For ex\u00adample, stack barriers could be employed to enable state \nex\u00adtraction by the thief to happen mostly concurrently with vic\u00adtim execution. This should reduce stealing \noverheads and en\u00adable the system to tolerate much higher steal ratios without losing performance. References \n[1] Intel@RCilkTM Plus sdk. URL http://software. intel.com/en-us/articles/intel-cilk-plus. [2] V. Cav\u00b4 \ne, J. Zhao, J. Shirako, and V. Sarkar. Habanero-Java: the new adventures of old X10. In Proceedings of \nthe 9th Inter\u00adnational Conference on Principles and Practice of Program\u00adming in Java, PPPJ 11, pages \n51 61, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0935-6. doi: 10.1145/ 2093157.2093165. [3] P. Charles, \nC. Grothoff, V. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. X10: an object\u00adoriented \napproach to non-uniform cluster computing. In Pro\u00adceedings of the 20th annual ACM SIGPLAN conference \non Object-oriented programming, systems, languages, and ap\u00adplications, OOPSLA 05, pages 519 538, New \nYork, NY, USA, 2005. ACM. ISBN 1-59593-031-0. doi: 10.1145/ 1094811.1094852. [4] G. Cong, S. Kodali, \nS. Krishnamoorthy, D. Lea, V. Saraswat, and T. Wen. Solving large, irregular graph problems using adaptive \nwork-stealing. In Proceedings of the 2008 37th Inter\u00adnational Conference on Parallel Processing, ICPP \n08, pages 536 545, Washington, DC, USA, 2008. IEEE Computer So\u00adciety. ISBN 978-0-7695-3374-2. doi: 10.1109/ICPP. \n2008.88. [5] A. Duran, J. Corbal\u00b4an, and E. Ayguad\u00b4e. Evaluation of OpenMP task scheduling strategies. \nIn Proceedings of the 4th international conference on OpenMP in a new era of paral\u00adlelism, IWOMP 08, \npages 100 110, Berlin, Heidelberg, 2008. Springer-Verlag. ISBN 3-540-79560-X, 978-3-540-79560\u00ad 5. URL \nhttp://dl.acm.org/citation.cfm?id= 1789826.1789838. [6] A. Duran, X. Teruel, R. Ferrer, X. Martorell, \nand E. Ayguade. Barcelona OpenMP tasks suite: A set of benchmarks targeting the exploitation of task \nparallelism in OpenMP. In Proceed\u00adings of the 2009 International Conference on Parallel Pro\u00adcessing, \nICPP 09, pages 124 131, Washington, DC, USA, 2009. IEEE Computer Society. ISBN 978-0-7695-3802-0. doi: \n10.1109/ICPP.2009.64. [7] S. J. Fink and F. Qian. Design, implementation and eval\u00aduation of adaptive \nrecompilation with on-stack replacement. In Proceedings of the international symposium on Code generation \nand optimization: feedback-directed and runtime optimization, CGO 03, pages 241 252, Washington, DC, \nUSA, 2003. IEEE Computer Society. ISBN 0-7695-1913- X. URL http://dl.acm.org/citation.cfm?id= 776261.776288. \n[8] M. Frigo, C. E. Leiserson, and K. H. Randall. The imple\u00admentation of the Cilk-5 multithreaded language. \nIn Proceed\u00adings of the ACM SIGPLAN 1998 conference on Programming language design and implementation, \nPLDI 98, pages 212 223, New York, NY, USA, 1998. ACM. ISBN 0-89791-987-4. doi: 10.1145/277650.277725. \n[9] M. Frigo, H. Prokop, M. Frigo, C. Leiserson, H. Prokop, S. Ramachandran, D. Dailey, C. Leiserson, \nI. Lyubashevskiy, N. Kushman, et al. The Cilk project. Algorithms, 1998. [10] U. H\u00a8olzle, C. Chambers, \nand D. Ungar. Debugging optimized code with dynamic deoptimization. In Proceedings of the ACM SIGPLAN \n1992 conference on Programming language design and implementation, PLDI 92, pages 32 43, New York, NY, \nUSA, 1992. ACM. ISBN 0-89791-475-9. doi: 10. 1145/143103.143114. [11] P. Kambadur, A. Gupta, A. Ghoting, \nH. Avron, and A. Lums\u00addaine. PFunc: modern task parallelism for modern high performance computing. In \nProceedings of the Conference on High Performance Computing Networking, Storage and Analysis, SC 09, \npages 43:1 43:11, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-744-8. doi: 10.1145/ 1654059.1654103. \n [12] D. Lea. A Java fork/join framework. In Proceedings of the ACM 2000 conference on Java Grande, JAVA \n00, pages 36 43, New York, NY, USA, 2000. ACM. ISBN 1-58113-288-3. doi: 10.1145/337449.337465. [13] D. \nLeijen, W. Schulte, and S. Burckhardt. The design of a task parallel library. In Proceedings of the 24th \nACM SIG-PLAN conference on Object oriented programming systems languages and applications, OOPSLA 09, \npages 227 242, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-766-0. doi: 10.1145/1640089.1640106. [14] \nH.-W. Loidl and K. Hammond. On the granularity of divide\u00adand-conquer parallelism. In Proceedings of the \n1995 Interna\u00adtional Conference on Functional Programming, FP 95, pages 135 144, Swinton, UK, UK, 1995. \nBritish Computer Soci\u00adety. URL http://dl.acm.org/citation.cfm?id= 2227330.2227343. [15] J. Mellor-Crummey. \nCilk++, parallel performance, and the cilk runtime system. URL http://www. clear.rice.edu/comp422/lecture-notes/ \ncomp422-2012-Lecture5-Cilk++.pdf. [16] MIT. The Cilk project. URL http://supertech. csail.mit.edu/cilk/index.html. \n [17] E. Mohr, D. Kranz, Halstead, and J. R.H. Lazy task cre\u00adation: A technique for increasing the granularity \nof parallel programs. Technical report, Cambridge, MA, USA, 1991. [18] J. Reinders. Intel threading building \nblocks: out.tting C++ for multi-core processor parallelism. O Reilly Media, Inc., 2007. [19] O. Tardieu, \nH. Wang, and H. Lin. A work-stealing scheduler for X10 s task parallelism with suspension. In Proceedings \nof the 17th ACM SIGPLAN symposium on Principles and Practice of Parallel Programming, PPoPP 12, pages \n267 276, New York, NY, USA, 2012. ACM. ISBN 978-1-4503\u00ad1160-1. doi: 10.1145/2145816.2145850. [20] L. \nWang, H. Cui, Y. Duan, F. Lu, X. Feng, and P.-C. Yew. An adaptive task creation strategy for work-stealing \nscheduling. In Proceedings of the 8th annual IEEE/ACM international symposium on Code generation and \noptimization, CGO 10, pages 266 277, New York, NY, USA, 2010. ACM. ISBN 978\u00ad1-60558-635-9. doi: 10.1145/1772954.1772992. \n[21] X. Yang, S. M. Blackburn, D. Frampton, J. B. Sartor, and K. S. McKinley. Why nothing matters: the \nimpact of ze\u00adroing. In Proceedings of the 2011 ACM international con\u00adference on Object oriented programming \nsystems languages and applications, OOPSLA 11, pages 307 324, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0940-0. \ndoi: 10. 1145/2048066.2048092.     \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Work-stealing is a promising approach for effectively exploiting software parallelism on parallel hardware. A programmer who uses work-stealing explicitly identifies potential parallelism and the runtime then schedules work, keeping otherwise idle hardware busy while relieving overloaded hardware of its burden. Prior work has demonstrated that work-stealing is very effective in practice. However, work-stealing comes with a substantial overhead: as much as 2x to 12x slowdown over orthodox sequential code.</p> <p>In this paper we identify the key sources of overhead in work-stealing schedulers and present two significant refinements to their implementation. We evaluate our work-stealing designs using a range of benchmarks, four different work-stealing implementations, including the popular fork-join framework, and a range of architectures. On these benchmarks, compared to orthodox sequential Java, our fastest design has an overhead of just 15%. By contrast, fork-join has a 2.3x overhead and the previous implementation of the system we use has an overhead of 4.1x. These results and our insight into the sources of overhead for work-stealing implementations give further hope to an already promising technique for exploiting increasingly available hardware parallelism.</p>", "authors": [{"name": "Vivek Kumar", "author_profile_id": "81548457656", "affiliation": "Australian National University, Canberra, Australia", "person_id": "P3856086", "email_address": "vivek.kumar@anu.edu.au", "orcid_id": ""}, {"name": "Daniel Frampton", "author_profile_id": "81314488699", "affiliation": "Microsoft, Seattle, WA, USA", "person_id": "P3856087", "email_address": "dframpto@microsoft.com", "orcid_id": ""}, {"name": "Stephen M. Blackburn", "author_profile_id": "81100547435", "affiliation": "Australian National University, Canberra, Australia", "person_id": "P3856088", "email_address": "steve.blackburn@anu.edu.au", "orcid_id": ""}, {"name": "David Grove", "author_profile_id": "81100575938", "affiliation": "IBM T.J. Watson Research, New York, NY, USA", "person_id": "P3856089", "email_address": "groved@us.ibm.com", "orcid_id": ""}, {"name": "Olivier Tardieu", "author_profile_id": "81100366768", "affiliation": "IBM T.J. Watson Research, New York, NY, USA", "person_id": "P3856090", "email_address": "tardieu@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384639", "year": "2012", "article_id": "2384639", "conference": "OOPSLA", "title": "Work-stealing without the baggage", "url": "http://dl.acm.org/citation.cfm?id=2384639"}