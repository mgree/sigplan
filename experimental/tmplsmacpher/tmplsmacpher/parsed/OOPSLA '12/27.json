{"article_publication_date": "10-19-2012", "fulltext": "\n Maple: A Coverage-Driven Testing Tool for Multithreaded Programs Jie Yu Satish Narayanasamy Cristiano \nPereira Gilles Pokam University of Michigan Intel Corporation {jieyu, natish}@umich.edu {cristiano.l.pereira, \ngilles.a.pokam}@intel.com Abstract Testing multithreaded programs is a hard problem, because it is challenging \nto expose those rare interleavings that can trigger a concurrency bug. We propose a new thread inter\u00adleaving \ncoverage-driven testing tool called Maple that seeks to expose untested thread interleavings as much \nas possible. It memoizes tested interleavings and actively seeks to ex\u00adpose untested interleavings for \na given test input to increase interleaving coverage. We discuss several solutions to realize the above \ngoal. First, we discuss a coverage metric based on a set of in\u00adterleaving idioms. Second, we discuss \nan online technique to predict untested interleavings that can potentially be ex\u00adposed for a given test \ninput. Finally, the predicted untested interleavings are exposed by actively controlling the thread schedule \nwhile executing for the test input. We discuss our experiences in using the tool to expose several known \nand unknown bugs in real-world applications such as Apache and MySQL. Categories and Subject Descriptors \nD.2.5 [Software En\u00adgineering]: Testing and Debugging General Terms Design, Reliability Keywords Testing, \nDebugging, Concurrency, Coverage, Idioms 1. Introduction Testing a shared-memory multi-thread program \nand expos\u00ading concurrency bugs is a hard problem. For most concur\u00adrency bugs, the thread interleavings \nthat can expose them manifest only rarely during an unperturbed execution. Even if a programmer manages \nto construct a test input that can trigger a concurrency bug, it is often dif.cult to expose Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October \n19 26, 2012, Tucson, Arizona, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 \nthe infrequently occuring buggy thread interleaving, because there can be many correct interleavings \nfor that input. One common practice for exposing concurrency bugs is stress-testing, where a parallel \nprogram is subjected to ex\u00adtreme scenarios during a test run. This method is clearly inadequate, because \nnaively executing a program again and again over an input tends to unnecessarily test similar thread \ninterleavings and has less likelihood of exposing a rare buggy interleaving. An alternative to stress \ntesting is system\u00adatic testing [13], where the thread scheduler systematically explores all legal thread \ninterleavings for a given test input. Though the number of thread schedules could be reduced by using \npartial-order reduction [10, 12] and by bounding the number of context-switches [32], this approach does \nnot scale well for long running programs. Another recent development is active testing [36, 38, 50]. \nActive testing tools use approximate bug detectors such as static data-race detectors [7, 43] to predict \nbuggy thread in\u00adterleavings. Using a test input, an active scheduler would try to excercise a suspected \nbuggy thread interleaving in a real execution and produce a failed test run to validate that the suspected \nbug is a true positive. Active testing tools target speci.c bug types such as data-races [38] or atomicity \nviola\u00adtions [17, 23, 35, 36, 40], and therefore are not generic. For a given test input, after actively \ntesting for all the predicted buggy thread interleavings, a programmer may not be able to determine whether \nshe should continue testing other thread interleavings for the same input or proceed to test a different \ninput. In this paper, we propose a tool called Maple that em\u00adploys a coverage-driven approach for testing \nmultithreaded programs. An interleaving coverage-driven approach has the potential to .nd different types \nof concurrency bugs, and also provide a metric for the programmers to understand the quality of their \ntests. While previous studies have attempted to de.ne coverage metrics for mulithreaded programs based \non synchronization operations [2] and inter-thread memory dependencies [22, 24, 39], synergistic testing \ntools that can help programmers achieve higher coverage for those metrics have been lacking.  Figure \n1. Overview of the framework. The .rst contribution of this paper is the set of interleav\u00ading idioms \nwhich we use to de.ne coverage for mulithreaded programs. An interleaving idiom is a pattern of inter-thread \ndependencies through shared-memory accesses. An instance of an interleaving idiom is called an iRoot \nwhich is repre\u00adsented using a set of static memory instructions. The goal of Maple is to expose as many \nnew iRoots as possible during testing. We de.ne our set of interleaving idioms based on two hypothesis. \nOne is the well-known small scope hypothe\u00adsis [18, 29] and the other is what we refer to as the value\u00adindependence \nhypothesis. Small scope hypothesis [18, 29] states that most concurrency bugs can be exposed using a \nsmall number of preemptions. CHESS [32] exploits this ob\u00adservation to bound the number of preemptions \nto reduce the test space. We apply the same principle to bound the number of inter-thread memory dependencies \nin our interleaving pat\u00adterns to two. Our empirical analysis of several concurrency bugs in real applications \nshow that a majority of them can be triggered if at most two inter-thread memory dependencies are exposed \nin an execution. Our value-independence hypothesis is that a majority of concurrency bugs gets triggered \nif the errorneous inter\u00adthread memory dependencies are exposed, irrespective of the data values of the \nshared variables involved in the de\u00adpendencies. We leverage this hypothesis to test for an iRoot only \nonce, and avoid testing the same thread interleaving (iRoot) again and again across different test input. \nThus, the number of thread interleavings to test would progressively reduce as we test for more inputs. \nA critical challenge is in exposing untested iRoots for a given test input. To this end, we built the \nMaple testing infrastructure comprised of an online pro.ler and an active scheduler shown in Figure 1. \nMaple s online pro.ler examines an execution for a test input, and predicts the set of candidate iRoots \nthat are fea\u00adsible for that input but have not yet been exposed in any prior test runs. Predicted untested \niRoots are given as input to Maple s active scheduler. The active scheduler takes the test input and \norchestrates the thread interleaving to realize the predicted iRoot in an actual execution using a set \nof novel heuristics. If the iRoot gets successfully exposed, then it is memoized by storing it in a database \nof iRoots tested for the program. We also consider the possibility that certain iRoots may never be feasible \nfor any input. We progressively learn these iRoots and store them in a separate database. These iRoots \nare given a lower priority when there is only limited time available for testing. When the active scheduler \nfor an iRoot triggers a concur\u00adrency bug causing the program produces an incorrect result, Maple generates \na bug report that contains the iRoot. Our active scheduler orchestrates thread schedules on a unipro\u00adcessor, \nand therefore recording the order of thread sched\u00adule along with other non-deterministic system input, \nif any, could allow a programmer to reproduce the failed execution exposed by Maple. We envision two \nusage models for Maple. One usage scenario is when a programmer has a test input and wants to test her \nprogram with it. In this scenario, Maple will help the programmer actively expose thread interleavings \nthat were not tested in the past. Also, a programmer can determine how long to test for an input, because \nMaple s predictor would produce a .nite number of iRoots for testing. Another usage scenario is when \na programmer acciden\u00adtally exposed a bug for some input, but is unable to repro\u00adduce the failed execution. \nA programmer could use Maple with the bug triggering input to quickly expose the buggy interleaving. \nWe helped a developer at Intel in a similar sit\u00aduation to expose an unknown bug using Maple. We built \na dynamic analysis framework using PIN [28] for analyzing concurrent programs. Using this framework, \nwe built several concurrency testing tools including Maple, a systematic testing tool called CHESS [32] \nand tools such as PCT [3] that rely on randomized thread schedulers, which we compare in our experiments. \nWe perform several experiments using open-source appli\u00adcations (Apache, MySQL, Memcached, etc.). Though \nMaple does not provide hard guarantees similar to CHESS [29] and PCT [3], it is effective in achieving \nhigher iRoot coverage faster than those tools in practice. We also show that Maple is effective in exposing \n13 documented bugs faster than these prior methods, which provides evidence that achieving higher coverage \nfor our metric based on interleaving idioms is effective in exposing concurrency bugs. We also discuss \nour experiences in using Maple to .nd 3 unknown bugs in aget, glibc,and CNC. Our dynamic analysis framework \nfor concurrent pro\u00adgrams and all the testing tools we developed are made avail\u00adable to the public under \nthe Apache 2.0 license. They can be downloaded from (https://github.com/jieyu/maple).  2. Coverage-Driven \nTesting Based on Interleaving Idioms In this section we discuss a coverage-driven testing method\u00adology \nfor multithreaded programs. For sequential programs, metrics such as program statement coverage are commonly \nused to understand the effectiveness of a test suite and de\u00adtermine if additional testing is required. \nFor multithreaded programs, however, a practically useful thread interleaving coverage metric has been \nlacking. We de.ne coverage for multithreaded programs based on a set of thread interleaving idioms. Sections \n3 and 4 discuss our Maple tool that can help programmers achieve higher coverage for these interleaving \nidioms. 2.1 Interleaving Idioms An interleaving idiom is a pattern of inter-thread dependen\u00adcies and \nthe associated memory operations. An inter-thread memory dependency (denoted using .) is an immediate \n(read-write or write-write) dependency between two mem\u00adory accesses in two threads. A memory access could \nbe ei\u00adther to a data or a synchronization variable. A dynamic in\u00adstance of an idiom in a program s execution \nis called as an interleaving root (iRoot). A memory access in an iRoot is represented using the static \naddress of the memory instruc\u00adtion. Coverage of a test suite for a program is simply cal\u00adculated as the \nnumber of iRoots exposed in any of the test run. Interleaving idioms should be generic enough that, by \nex\u00adposing their iRoots, most concurrency bugs could be trig\u00adgered. At the same time, the coverage domain \n(number of iRoots that needs to be tested) should be small enough that, the probability of exposing an \nunknown concurrency bug is high when an untested iRoot is exposed. To meet these com\u00adpeting demands, \nwe make an assumption that most concur\u00adrency bugs can be exposed using simple thread interleaving patterns. \nThis assumption is inspired by the small scope hy\u00adpothesis [18, 29]. We study a set of canonical idioms \nthat can be constructed for one or two inter-thread dependencies (which implies there can be only one \nor two shared-variables) involving no more than two threads. Figure 2 enumerates the canonical set of \nidioms for two inter-thread dependencies and two threads. There are six idioms in total. We refer to \nidiom1 as a sim\u00adple idiom, and the rest as compound idioms. For compound idioms, to reduce the coverage \ndomain without signi.cantly compromising the ability to expose a concurrency bug, we include two additional \nconstraints. First, the number of in\u00adstructions executed between two events in the same thread should \nbe less than a threshold. We refer to this threshold as the vulnerability window (vw). Second, in an \nidiom, if atom\u00adicity of two memory accesses in a thread T to a variable V is violated by accesses in \nanother thread, we disallow accesses to V between those two accesses in the thread T .For exam\u00adple, in \nidiom3 we do not allow any access to the variable X Idiom1 Idiom2 Idiom3 .. .. .. .. .. .. .. .. .. \nIdiom4 Idiom5 Idiom6 .. .. .. .. .. .. .. .. .. .. .. .. Figure 2. The canonical idioms for two inter-thread \ndepen\u00addencies and two threads. Initial: a = 0, c = 0 Sufficient Conditions: T1 T2 foo( ){ bar( ){ .. \n.. ..: c:=1 ..: a:=1 if(c)..: Idiom ..:  assert(a!=1) ..: a:=2 ..  ..} } Figure 3. An idiom1 concurrency \nbug. between the two memory accesses AX and DX ,but there could be accesses to X between BX and CX . \nSix idioms in Figure 2 can represent interleavings re\u00adquired to expose a majority of concurrency bugs: \natomicity violations, including both single variable (idiom1, idiom2, idiom3) and multi-variable (idiom4, \nidiom5); typical dead\u00adlock bugs (idiom5), and generic order related concurrency bugs (idiom1, idiom6). \nThese interleaving patterns are more general than the anomalous patterns used in prior studies to .nd \nspeci.c classes of concurrency bugs [36, 38, 40].  2.2 Relation Between iRoots and Concurrency Bugs \nThe iRoot of a concurrency bug provides the minimum set of inter-thread dependencies and the associated \nmemory or synchronization accesses, which if satis.ed, can trigger that bug in an execution. Of course, \nhigher order iRoots may also expose the same concurrency bug, but for the purpose of classifying concurrency \nbugs, we consider the iRoot that provides the minimum set of interleaving conditions. Figure 3 shows \nan example of a concurrency bug. The idiom of the bug is shown in the dashed box. A and B rep\u00adresent \nstatic instructions in the program and X represents a memory location. The arrows denote inter-thread \ndependen\u00adcies. The bug is triggered whenever the inter-thread depen\u00addency I2 . I5 is satis.ed in an execution. \nTherefore, this is an idiom1 bug and its iRoot is I2 . I5. Note that there exists an inter-thread dependency \nI1 . I4 that must also be satis.ed before the iRoot I2 . I5 can be exposed. This dependency affects the \ncontrol .ow of the thread T 2 and de\u00adtermines whether I5 is executed or not. We refer to such con\u00additions \nwhich must be satis.ed in order to satisfy the idiom  .. Figure 4. A real idiom4 concurrency bug from \nMySQL. unlock(&#38;LOCK_log); } } Thread-2 Thread-1 int mysql_insert(..){int generate_table(..){ lock(&#38;LOCK_open); \n.. // delete table entries unlock(&#38;LOCK_open); lock($LOCK_open); .. // insert to table unlock($LOCK_open); \n... Idiom-4 lock(&#38;LOCK_log); // write log .. .. lock(&#38;LOCK_log); // write log unlock(&#38;LOCK_log); \n.. .. ..  Idiom1 Idiom2 Idiom3 Idiom4 Idiom5 Idiom6 Other    7 3 1 4 1 0 1 Table 1. Empirical study \non 17 documented bugs. conditions as pre-conditions. Also notice that I5 . I3 needs to be exposed before \nthe bug can be triggered. However, this condition need not be part of the bug s iRoot (I2 . I5), because \nit is always implied by the bug s iRoot interleaving conditions. Figure 4 shows a real concurrency bug \nin MySQL and its idiom. In this example, two critical sections in Thread-1 are expected to execute atomically, \nbut the programmer did not enforce that constraint explicitly. The bug will be exposed when the critical \nsections in Thread-1 are intercepted with the critical section in Thread-2. The iRoot for this bug is \nof type idiom4 consisting of the two inter-thread dependen\u00adcies between the lock and unlock operations. \nThis example conveys an important observation that even if a concurrency bug is fairly complex involving \nmany different variables and inter-thread dependencies, the iRoot of that bug (minimum set of interleaving \nconditions that need to be satis.ed to trig\u00adger that bug) could be quite simple. Thus, by testing iRoots \nfor a small set of idioms, we can hope to expose a signi.cant fraction of concurrency bugs. 2.2.1 Empirical \nAnalysis To verify our hypothesis that achieving high coverage for our simple set of interleaving idioms \ncould expose a signif\u00adicant fraction of concurrency bugs, we conducted an empir\u00adical study using 17 real \nworld concurrency bugs from vari\u00adous programs including Apache, MySQL, and Memcached. Table 1 presents \nthe results. Except one, the remaining 16 concurrency bugs can be characterized using one of our in\u00adterleaving \nidioms. We could not represent one bug using any of our idioms (Bug#10 in Table 2) because it was value \nde\u00adpendent. We did not .nd any concurrency bug that can be classi.ed as idiom6. Therefore, in this paper, \nwe focus only on exposing iRoots for the .rst .ve idioms.  2.3 Coverage-Driven Testing Using Memoization \nOne of the disadvantages of random testing [3, 6] and sys\u00adtematic testing tools that expose all legal \nthread interleavings for a given test input [29] is that, they may expose the same thread interleaving \n(iRoot) again and again across different test inputs. In our coverage-driven testing method, we pro\u00adpose \nto memoize the thread interleavings (iRoots) exposed in a test run, and use it to target future tests \nto expose untested iRoots. In other words, if an iRoot has been already exposed during an earlier execution \nfor some test input, Maple will not seek to expose the same iRoot again. If a concurrency bug is dependent \non values read or writ\u00adten by memory accesses, then exposing an iRoot once may not be enough. Extending \ninterleaving idioms to include value conditions may address this problem. However, this would signi.cantly \nenlarge the coverage domain and result in the same thread interleaving being unnecessarily tested multiple \ntimes for different values. We hypothesize that most concurrency bugs are such that whenever their erroneous \nin\u00adterleavings are exposed they get triggered. We refer to this hypothesis as the value-independence \nhypothesis. Section 6 provides empirical evidence that supports this hypothesis. We also show that memoizing \ntested iRoots across different test inputs can drastically reduce testing time without signif\u00adicantly \ncompromising Maple s ability in exposing concur\u00adrency bugs. Maple seeks to achieve higher coverage by \nexposing as many different iRoots as possible during testing. Unlike cov\u00aderage metrics such as basic \nblock coverage, it is hard to esti\u00admate the total number of iRoots for a given program. How\u00adever, number \nof exposed iRoots can be used as coverage metric for a saturation-based test adequacy [22, 39]. That \nis, a programmer can decide to stop testing at a point when additional tests are unlikely to expose new \niRoots. We be\u00adlieve saturation-based testing approach is a practical solution for problems such as concurrent \ntesting where estimating the coverage domain is intractable. 3. Online Pro.ling For Predicting iRoots \nIn this section, we discuss the design and implementation of Maple s online pro.ler. Given a program \nand a test input, the pro.ler predicts a set of candidate iRoots that can be tested for the given test \ninput. 3.1 Notations and Terminology As we discussed in Section 2.1, an iRoot for an idiom com\u00adprises \nof a set of inter-thread dependencies between mem\u00adory accesses. A memory access could be either a data \naccess or a synchronization access. For synchronization accesses, we only consider lock and unlock operations \nin this paper. A lock or an unlock operation is treated as a single access when we construct an iRoot, \nand all memory accesses exe\u00adcuted within lock and unlock functions are ignored. Ti (where i = 1,2,3,...) \nuniquely identi.es a thread. Ai represents a dynamic memory access. The super script i uniquely identi.es \na dynamic access, but we usually omit it in our discussion to improve readability. If Ai is a data X \naccess, A stands for the static address of the memory instruc\u00ad  Main Child T1 T2 We improve the accuracy \nof our pro.ler by exploiting the observation that non-mutex happens-before relations fork(child) . . \n  lock(m)  .. .. mostly remain the same across different executions for a unlock(m)  lock(m) given \ninput. A non-mutex happens-before relation is due toanysynchronizationoperationotherthanalock/unlock. \nunlock(m) . . (b) Happens-before relations due to locks tend to change across  . . (a) Figure 5. (a) \nInfeasible iRoots due to non-mutex happens\u00adbefore relations. (b) Infeasible iRoots due to mutual exclu\u00adsion. \ntion and X refers to the memory location accessed. Two data accesses are said to con.ict if both access \nthe same mem\u00adory location and at least one of them is a write. If Ai is a X lock/unlock synchronization \naccess, A stands for the address of the instruction that invoked the lock/unlock operation and X refers \nto the lock variable. Two synchronizations accesses con.ict if one of them is a lock and the other is \nan unlock operation.  3.2 Naive Approach We start with the simpler problem, which is predicting id\u00adiom1 \niRoots for a given test input. One naive way to predict idiom1 iRoots is as follows. First, run the program \nonce and obtain the set of accesses executed by each thread. Then, for any access AX in thread Tc, if \nthere exists a con.ict\u00ading access BX in another thread Tr, we predict two idiom1 iRoots: A . B and B \n. A. For synchronization accesses, we predict A . B only if A is an unlock operation and B is a lock \noperation. Obviously, this approach could produce many infeasible iRoots. an iRoot is said to be infeasible \nfor a program if it can never manifest in any legal execution of that program. In the following sections, \nwe discuss two ma\u00adjor sources of inaccuracy in this naive algorithm and present our solutions. 3.3 Non-Mutex \nHappens Before Analysis The pro.ler predicts iRoots based on a few pro.led execu\u00adtions for a given test \ninput. The predicted iRoots may not ap\u00adpear in any of the pro.led executions, but they are predicted \nto be realizable in some other legal executions. We should avoid predicting iRoots that can never manifest \nin any of the legal executions. We observe that some of the happens-before relations tend to remain the \nsame in all of the legal executions. There\u00adfore, these happens-before relations can be used to .lter \nout infeasible iRoots predicted in the naive approach. For ex\u00adample, in Figure 5(a), an access AX is \nexecuted before the main thread forks the child thread. BX is executed in the child thread. Assuming \nthat AX and BX are con.icting, the naive approach will predict two idiom1 iRoots: A . B and B . A. However, \nit is trivial to observe that in any legal execution, AX executes before BX because of the fork call. \nAs a result, we should not predict the iRoot B . A as a candidate to test. executions, because the order \nin which locks are acquired could easily change. On the contrary, we .nd that non-mutex happens-before \nrelations (e.g. fork-join, barrier and signal\u00adwait) are more likely to remain constant across different \nexecutions. Therefore, the pro.ler predicts an iRoot only if it does not violate the non-mutex happens-before \nrelations in at least one of the pro.led executions. For the program in Figure 5(a), BX cannot happen \nbefore AX any of the exe\u00adcutions according to the non-mutex happens-before relation due to fork. As a \nresult, the pro.ler will not predict B . A as a candidate iRoot to test. Though effective in pruning \ninfeasible iRoots, this analysis is not sound because some non-mutex happens-before relations are not \nguaranteed to remain constant across different executions for an input.  3.4 Mutual Exclusion Analysis \nMutual exclusion constraints imposed by locks could also prevent naively predicted iRoots from manifesting \nin any of the alternate executions. For example, in Figure 5(b), all the accesses (AX , BX and CX ) are \nprotected by the same lock m. Assume that these accesses con.ict with each other. The naive approach \nwould predict A . C (and C . B)tobea candidate iRoot to test. Clearly, A . C (and C . B) is not feasible \nbecause of the mutual exclusion constraint imposed by the lock m. To further improve its accuracy, the \npro.ler is augmented with a mutual exclusion analysis phase to .lter those infea\u00adsible iRoots that are \ncaused by the mutual exclusion con\u00adstraints. To achieve this, the pro.ler needs to collect two types \nof information for each access AX . One is the lock\u00adset information which contains the set of locks that \nare held by Thd(AX ) when executing AX . The other is the critical section information which speci.es \nwhether AX is the .rst or the last access to X in the critical section that contain AX . We now use an \nexample to illustrate how these two types of information can be used to .lter infeasible iRoots caused \nby the mutual exclusion constraints. Consider the example in Figure 5(b). The pro.ler needs to decide \nwhether iRoot A . C is feasible. It .rst checks the locksets of both ac\u00adcesses: AX and CX . If the locksets \nare disjoint, the pro.ler will immediately predict the iRoot to be feasible. If not, the pro.ler will \ngo to the next step. In this example, AX and CX have the same lockset {m}. Therefore, the pro.ler proceeds \nto the next step. In the second step, for each common lock (in our example its m), the pro.ler checks \nwhether the mu\u00adtual exclusion constraint imposed by the common lock will prevent the iRoot from manifesting. \nIt checks whether AX is the last access to X in the critical section that is guarded by the common lock \nm, and whether CX is the .rst access to X in the critical section that is guarded by the common lock \nm. If either of them is not true, the pro.ler will pre\u00addict that the iRoot is infeasible. In our example, \nsince BX is the last access to X in the critical section that is guarded by the common lock m, the iRoot \nA . C is predicted to be infeasible. This analysis is also not sound since control .ow differences between \nexecutions could affect our analysis, but it works well in practice.  3.5 Online Pro.ling Algorithm \nOur pro.ler predicts candidate iRoots to test for a particular idiom using an online mechanism that we \ndescribe in detail in this section. An online algorithm avoids the need to collect large traces. 3.5.1 \nBaseline Algorithm The pro.ler monitors every memory access. For each object, the pro.ler maintains an \naccess history for each thread. We use AHX (Ti) to denote the access history for object X and thread \nTi. Each access AX in the access history AHX (Ti) is ordered by the execution order of Ti,and is associated \nwith a vector clock and an annotated lockset.The vector clock, denoted as VC(AX ), is used to perform \nthe non-mutex happens-before analysis. It is the same as that used in many of the dynamic data race detectors, \nexcept that here we consider non-mutex happens-before relations. The annotated lockset, denoted as AnnoLS(AX \n),is used to perform the mutual exclusion analysis. It consists of a set of locks, each of which is annotated \nwith a sequence number and two bits. The sequence number is used to uniquely identify each critical section \nguarded by the lock, and the two bits indicate whether the access is the .rst or the last access in the \ncorresponding critical section. We say that two annotated locksets are disjoint if no common lock is \nfound between the two sets. Both the vector clock and the annotated lockset are recorded when Thd(AX \n) is executing AX . When an access AX is being executed by Tc, the pro.ler checks the access histories \nfrom all other threads on object X (i.e. AHX (Tr),Tr .= Tc). If there exists a con.icting access BX in \nAHX (Tr), the pro.ler will predict the iRoot B . A if the following conditions are true: (1) BX does \nnot happen after AX by checking VC(BX ) and VC(AX ) (the non-mutex happens-before check). (2) Either \nAnnoLS(AX ) and AnnoLS(BX ) are disjoint, or for each common lock m held by AX and BX , AX is the .rst \naccess to X in the corresponding critical section guarded by m and BX is the last access to X in the \ncorresponding critical section guarded by m (the mutual exclusion check). Similarly, the pro.ler will \nalso predict the iRoot A . B according to the above rules. To make the pro.ling algorithm online, we \nneed to deal with several issues. One issue is that when AX executes, some access, say CX , has not been \nperformed yet. As a result, CX will not be in any access history. However, the pro.ler will still correctly \npredict iRoot A . C and iRoot C . A at the time CX is executed if they are feasible. Another issue with \nthe online algorithm is that when ex\u00adecutes AX , the pro.ler cannot precisely compute the an\u00adnotated \nlockset AnnoLS(AX ) required by the mutual ex\u00adclusion analysis. The reason is because it does not know \nwhether the access AX will be the last access in the cur\u00adrent critical section or not. We solve this \nissue by delaying predicting iRoots for AX until either of the following events happens: (1) another \naccess to X is reached by Thd(AX ). (2) X is about to be deallocated (e.g. free()). (3) Thd(AX ) is about \nto exit. The insight here is that the pro.ler can pre\u00adcisely compute the annotated lockset for AX if \nany of the above events happens.  3.5.2 Optimizations We have designed a few optimizations to make the \nonline algorithm practical for large applications. (1) Condensing access histories. We .nd that it is \nnot necessary to store two accesses in an access history if these two accesses are identical. Two accesses \nfrom the same thread are said to be identical if they are originated from the same static instruction, \nand have the same vector clock and annotated lockset. The reason is because iRoots only consider static \ninstructions (rather than dynamic events). Therefore, an access will not be added to an access history \nif an identical access in the access history can be found. This optimization is sound and can not only \nsave space, but save time as well since each time when predicting iRoots, the number of accesses in the \naccess histories that need to be examined reduces. (2) Caching prediction results. To predict iRoots \nfor an access AX , the pro.ler needs to scan the access histories from all other threads on object X, \nwhich could be a time consuming operation. We call this operation a full scan.We observe that it is not \nalways needed to perform a full scan. If AX does not cause the access history to be updated (i.e. it \ncan be condensed according to the previous optimization), the pro.ler can safely skip the full scan as \nno new iRoot will be predicted even if a full scan is performed. (3) Removing useless access history \nentries. We observe that it is not necessary to keep all access histories from the beginning of the program \nexecution. If an access in the access history can no longer be part of any potentially predicted iRoot, \nwe can safely remove it from the access history. (4) Monitoring only shared instructions. Maintaining \nac\u00adcess histories for each memory location is very expensive. Clearly, the pro.ler does not need to maintain \naccess histo\u00adries for thread private locations. We perform an online analy\u00adsis that runs concurrently \nwith the pro.ler and detects the in\u00adstructions that can access shared locations. The pro.ler uses this \ninformation to create access histories only for the lo\u00adcations accessed by these shared instructions. \nWe omit the   (1) Local Pairs: T1 T2 ,  (2) Idiom1 Prediction Results: , < vw   , Correlate (1) \nand (2), we predict < vw two idiom-4 iRoots: Figure 6. Predicting iRoots for compound idioms. details \nof the shared instruction analysis due to space con\u00adstraint.  3.6 Predicting iRoots for Compound Idioms \nTo predict iRoots for compound idioms, we designed an al\u00adgorithm that leverages the idiom1 prediction \nresults. The ap\u00adproach is generic to all compound idioms de.ned in Sec\u00adtion 2. The algorithm is divided \ninto two parts: identifying local pairs, and correlating with idiom1 prediction results. In this section, \nwe discuss these two parts in detail. 3.6.1 Identifying Local Pairs A local pair, as suggested by its \nname, is a pair of accesses from the same thread. During a pro.ling execution, if the pro.ler .nds two \naccesses AX and BY (X may or may equal to Y ) such that AX and BY are from the same thread, AX is executed \nbefore BY , and the number of dynamic instructions executed between AX and BY is less than a pre-set \nthreshold vw (vw stands for vulnerability window and is speci.ed in the idiom de.nition), it will record \na local pair [AX ,BY ]. For example, Figure 6 shows a pro.ling execution. Accesses AX and BY in T1 are \nexecuted .rst, followed by CX and DY in T2. The pro.ler records two local pairs from this pro.ling execution: \n[AX ,BY ] and [CX ,DY ]. To collect local pairs, the pro.ler uses a rolling window for each thread to \nkeep track of the recent accesses.  3.6.2 Correlating with Idiom1 Prediction Results To predict iRoots \nfor compound idioms, we propose to lever\u00adage the idiom1 prediction results. We use an example to illustrate \nhow to correlate local pairs with idiom1 predic\u00adtion results to predict compound idiom iRoots. Consider \nthe example shown in Figure 6. As mentioned, the pro.ler identi.es two local pairs: [AX ,BY ] and [CX \n,DY ]. Mean\u00adwhile, the pro.ler also records the idiom1 prediction results. For instance, AX and CX can \nproduce two idiom1 iRoots A . C and C . A according to the idiom1 prediction al\u00adgorithm, therefore the \npro.ler records both AX . CX and CX . AX in the idiom1 prediction results 1. Similarly, the pro.ler records \nBY . DY and DY . BY . Now, consider 1 Notice that the idiom1 prediction results are only useful for the \ncurrent pro.ling execution, and will be discarded once the execution .nishes. They are different from \nthe predicted idiom1 iRoots which last across executions. They contain more information than idiom1 iRoots \ndo. the .rst local pair [AX ,BY ]. According to the predicted id\u00adiom1 results, CX can potentially depend \non AX ,and BY can potentially depends on DY . As a result, the pro.ler pre\u00addicts an idiom4 iRoot A . \nC...D . B (assume X .Y ). = Similarly, for another local pair [CX ,DY ], the pro.ler pre\u00addicts another \nidiom4 iRoot C . A...B . D. Currently, the pro.ler performs the correlation part at the end of each pro.ling \nexecution. Similar optimization technique is used to condense local pairs, that is if two local pairs \nfrom the same thread have both their accesses identical, the pro.ler just records one of them. 4. Actively \nTesting Predicted iRoots In this section, we discuss the design and implementation of Maple s active \nscheduler. Maple s pro.ler predicts a set of iRoots that can be realized in an execution using a test \ninput. The goal of Maple s active scheduler is to validate the prediction by orchestrating the thread \nschedule to realize the predicted iRoots in an actual execution for the test input. 4.1 A Naive Approach \nSuppose that we want to expose an idiom1 candidate iRoot A . B. The static instructions A and B are called \ncandidate instructions. In a test run, there might be multiple dynamic accesses associated with a single \ncandidate instruction. We still use AX to denote a dynamic accesses to object X by the candidate instruction \nA. The naive approach works as follows. Whenever a candidate instruction (say AX )is reached by a thread \n(say T1), the active scheduler delays the execution of T1. During the delay, if another thread (say T2) \nreaches the other candidate instruction (say BX ), then the iRoot A . B is exposed by executing AX .rst \nand then executing BX (as shown in Figure 7(a)). This approach is used in several prior studies (e.g. \n[36]). While it is simple, it can lead to several issues, including deadlocks (also referred as thrashing \nin [20]). Consider the example in Figure 7(b). Suppose that T1 reaches AX .rst. The active scheduler, \nin this case, delays the execution of T1, waiting for the other candidate instruction to be reached in \nT2. T2 is blocked when calling the barrier function, leading to a deadlock because no thread can make \nforward progress at that state. One way to mitigate this issue is to make use of timeout. In the example, \nif a timeout is introduced for each delay, T1 will eventually be woken up when the timeout has expired. \nHowever, as discussed in the following sections, this is not enough to address most of the issues.  \n4.2 Non-preemptive and Strict Priority Scheduler There are two problems with a timeout-based approach. \nFirst, it is sensitive to the underlying environment, hence fragile [19]. For instance, the timeout should \nbe set to a larger value when running the program on a slower ma\u00adchine. Second, determining how long \nthe timeout should be is not straightforward. A large timeout is detrimental to per\u00ad  T1 T2 T1 T2 T1 \nT2 barrier(b)  barrier(b)  lock(m) lock(m)   . .  . .  . . Figure 7. (a) The ideal situation \nfor exposing an idiom1 iRoot A . B. (b) The naive approach could deadlock when exposing an idiom1 iRoot \nA . B. (c) The situation in which the watch mode is turned on for exposing an idiom1 iRoot A . B. . . \n .  . unlock(m)  (a) (b) (c) . unlock(m) . formance due to the longer delays, while a shorter timeout \ncould cause unnecessary give ups. An alternative to timeout is to monitor the status of each thread (blocked \nor not) by instrumenting every synchroniza\u00adtion operations and blocking system calls (e.g. [20, 35, 38]). \nFor example, in Figure 7(b), if the active scheduler keeps track of the status of each thread, it should \nknow that T2 is blocked after it calls the barrier function. Thus, T1 will be woken up immediately since \nno other thread in the system can make forward progress at that state. Our approach eliminates the need \nfor monitoring thread status. The main idea is to leverage the non-preemptive and strict priorities provided \nby the underlying operating system (OS). All threads are forced to run on a single processor. Each thread \nis assigned a non-preemptive strict priority. Un\u00adder this scenario, a lower priority thread never gets \nexecuted if there exists a non-blocked higher priority thread. In Linux, the real-time priorities are \nactually non-preemptive strict pri\u00adorities 2. By using non-preemptive strict priorities, the dead\u00adlocks \nwill be automatically detected and resolved by the un\u00adderlying OS since it knows the status of each thread. \nLet us consider the example in Figure 7(b). Initially, T1 has a prior\u00adity Pinit(T1) and T2 has a priority \nPinit(T2). Suppose that Pinit(T1) >Pinit(T2). Therefore, T1 executes .rst. When T1 reaches AX , the active \nscheduler changes the priority of T1 to Plow such that Plow <Pinit(T2). Due to the nature of the non-preemptive \nstrict priorities, T1 is preempted by T2 immediately after the priority change. When T2 calls the barrier \nfunction, it is blocked. At this moment, T1 becomes the only non-blocked thread, and resumes execution \nimme\u00addiately after T2 is blocked. The deadlock situation is natu\u00adrally resolved. Note that the active \nscheduler only needs to monitor the instructions involved in the iRoot being exposed, thus limiting the \nruntime overhead.  4.3 Complementary Schedules Another problem with the approach discussed in Section \n4.1 is that it does not have a mechanism to control the order in which threads get executed. Assume that \nwe want to expose the idiom1 iRoot A . B in the example of Figure 7(c), where AX is in T1 and BX in T2, \nrespectively. Because both AX and BX are protected by the same lock m,if BX is 2 More speci.cally, schedule \npolicy SCHED FIFO. reached by T2 .rst, the iRoot will not be exposed. The delay introduced before BX \nwill not help because T1 will never be able to reach AX due to the fact that T2 is still holding the \nlock m. In order to expose this iRoot, AX must be reached by T1 .rst. However, the naive approach (Section \n4.1) cannot guarantee this as it does not have a mechanism to control the order in which the threads \nget executed. We address this issue using a technique called comple\u00admentary schedules. The idea is to \nuse two test runs on each candidate iRoot. Each newly created thread Ti, including the main thread, is \nassigned with an initial priority Pinit(Ti).In the .rst test run, the initial priorities are assigned \nfrom high to low, following the order of thread creation. To be more precise, we have Pinit(Ti) >Pinit(Tj) \n(Ti has a higher priority than Tj ) if thread Ti is created earlier than Tj.In the second test run, initial \npriorities are assigned from low to high, following the order of threads creation. In order words, Pinit(Ti) \n<Pinit(Tj ) if thread Ti is created earlier than Tj . Using this technique, we increase the likelihood \nthat, in one of the two test runs, AX will be reached .rst in the example shown in Figure 7(c).  4.4 \nWatch Mode Optimization A problem with the naive approach is that it can unnecessar\u00adily give up exposing \nsome iRoot in certain cases. Consider the example in Figure 7(c). We are still interested in expos\u00ading \nthe idiom1 iRoot A . B.If T1 reaches AX .rst, the naive approach gives up exposing the iRoot for AX right \naf\u00adter the timeout. However, giving up is not necessary here be\u00adcause it is still possible that BX could \nexecute later without any access to X in between AX and BX . We use a mechanism called watch mode to \nexposes an iRoot in such case. In watch mode, every memory access is monitored. Consider again the example \nin Figure 7(c). When T1 reaches AX .rst and sets its priority to Plow (Plow is lower than any initially \nassigned priorities), T2 gets control and executes, but is blocked when trying to acquire the lock m. \nAs mentioned above, T1 resumes immediately after T2 is blocked and executes AX . At this moment, instead \nof giving up exposing iRoot for AX , the active scheduler enters the watch mode and monitor every memory \naccess. The active scheduler still keeps the priority of T1 to Plow. Once the lock m is released, T1 \nis preempted by T2 because T2 has a higher priority than T1. Shortly after, T2 reaches BX and no access \nto X is found in between AX and BX . Therefore, the iRoot A . B is exposed.  In the same example, during \nthe watch mode, it is likely that T1 reaches an instruction no matter whether it is a candidate instruction \nor not that accesses X as AX does. In such case, the active scheduler is not able to expose iRoot for \nAX because T1 already has the lowest priority at that moment. It just ends the watch mode and gives up \nexposing iRoot for AX . If the access to X (not instruction B)is from a thread other than T1 (say T3), \nthe active scheduler sets the priority of T3 to Plow. The intuition here is that some other threads may \nmake progress and reach the other candidate instruction B. However, if the con.icting access is eventually \nexecuted by T3 in spite of its lowest priority, the active scheduler ends the watch mode and gives up. \nThe watch mode can be implemented ef.ciently using debug registers or by leveraging the selective instrumenta\u00adtion \nmechanism in PIN [28]. We implement the second ap\u00adproach. For compound idioms, the execution under watch \nmode is usually short given that we have distance constraints in the idiom de.nitions. For idiom1, the \nselective instrumen\u00adtation mechanism in PIN can affect performance depending on how long the active scheduler \nspends in watch mode. The overhead is discussedinSection6.  4.5 Candidate Arbitration There might exist \nmultiple dynamic accesses that correspond to the same candidate instruction during the execution. In \nmany cases, the active scheduler has to decide which of these accesses belongs to the candidate iRoot \nto expose. For ex\u00adample, while the active scheduler is exposing iRoot for AX (seeking candidate instruction \nB in other threads), it is possi\u00adble that another thread also reaches the candidate instruction A, or \nanother thread reaches candidate instruction B,but it happens to access a location other than X (say \nBY ). In either one of these situations, the active scheduler has two choices: either continue to expose \nthe iRoot A . B for AX ,or give up on that attempt and seek to expose the iRoot for latter access of \nA or for BY . We decided to make these choices random with equal probability. We choose not to use a \n.xed policy because it could cause some feasible iRoot to become impossible to expose. We save the random \nseed for repro\u00adduction purpose. We aware that the random arbitration algorithm we use may cause a later \naccess exponentially unlikely to be used as part of a candidate iRoot. This will become an issue when \nwe do want to expose an iRoot for a later access (e.g. only the iRoot that uses this access will lead \nto a bug). Nevertheless, we are still able to expose all the bugs we have analyzed using this strategy \n(Section 6.2). This may be because many of the bugs we analyzed manifest early in their executions, or \nnot many dynamic accesses exist for the candidate instructions in the iRoots that expose the bugs. Even \nif that, we believe this is an important problem and we plan to address it in our future work. Currently, \nwe can think of two possible ways. First, we can associate more information with each candidate instruction \nsuch as its calling context so that some irrelevant accesses that use the same instruction but different \ncontexts will be .ltered out. Second, we can devise a more sophisticated arbitration algorithm using \nmore test runs. 4.6 Dealing with Asynchronous External Events Some applications depend on asynchronous \nexternal events such as network and asynchronous signals. These events are usually dif.cult to deal with \nin the active scheduler because it has no control on when these events are delivered. Con\u00adsider the example \nin Figure 8 where T2 has a higher prior\u00adity initially. When T2 reaches BX , its priority is changed to \nPlow, at which point T1 is scheduled. If T1 is blocked when calling the function sigwait (e.g. because \nthe signal might not have been delivered yet), since all threads except T2 are blocked in the system \nat that time, T2 has to execute BX in spite of its lowest priority; thus giving up the exposition of \niRoot A . B for BX . We observe that if the asynchronous signal in this example is delivered earlier, \nthe active sched\u00aduler might be able to expose the iRoot. To solve this problem, the active scheduler \nintroduces extra time delay where it is about to give up, hoping that the potential external event will \narrive during that period. For instance, in the example of Figure 8, when the active scheduler is about \nto give up by executing BX in T2 after T1 has been blocked, a time delay is injected right before BX \nis executed. During this period, if the asynchronous signal is delivered, the active scheduler can successfully \nexpose the iRoot A . B. For applications that do not depend on asynchronous ex\u00adternal events, there is \nno need for the active scheduler to in\u00adject extra time delay. We detect whether an application de\u00adpends \non asynchronous external events by monitoring sys\u00adtem calls and signals during pro.ling. Even if an application \ndoes depend on asynchronous external events, this might not be true for all the iRoots. During pro.ling, \nwe mark each candidate iRoot with a .ag indicating whether this iRoot de\u00adpends on asynchronous external \nevents or not. The active scheduler uses this .ag to decide whether to inject time delay or not. Finally, \nto ensure forward progress, we set a timeout for each delay. The timeout value can be optimized accord\u00ading \nto the application and the input.  4.7 Compound Idioms A compound idiom iRoot is composed of multiple \nidiom1 iRoots. Our general policy for exposing compound idiom iRoots is to expose each of the idiom1 \niRoot one at a time. Each of the idiom1 iRoot is exposed as described before, but the algorithm for exposing \ncompound idiom iRoots needs to address two more issues that we describe next. First, the active scheduler \nalways enters the watch mode after the .rst candidate instruction in a compound iRoot is executed. To \nunderstand why, consider the example in  unlock(m) lock(m) async signal . .  .. watch mode pre-condition \nsigwait(...) . .  . . .. unlock(m) Figure 8. Problem with asyn-Figure 9. Expose a compound idiom \nchronous external events. iRoot A . B...C . D. when trying to expose iRoot A . B. Figure 9. The goal \nis to expose an idiom4 iRoot A . B...C . D. According to the idiom4 de.nition, we need to make sure that \nthere is no other access to the same locations that A and D access in between them. Therefore, after \nAX is executed in this example, we enter the watch mode. If there is any access to location X before \nDY is reached, the active scheduler stops trying to expose the iRoot for AX because this violates the \nidiom de.nition. One complex aspect of this implementation is that, before DY is reached, we do not know \nwhich location it is going to access. We solve this problem by recording the set of locations that are \naccessed by T1 after AX is executed. This set is checked when DY is reached, to verify that none of the \naddresses touched con.icts with the address accessed by DY . In addition, the active scheduler exits \nthe watch mode and gives up exposing iRoot for AX if the number of dynamic instructions executed by T1 \nafter AX exceeds the pre-de.ned threshold speci.ed in the idiom de.nition. Second, the arbitration is \nbiased after the .rst part of the compound idiom iRoot is exposed. In the example of Figure 9, after \nthe .rst part of the iRoot (A . B) is exposed, if T2 reaches AZ (an access to Z by candidate instruction \nA), we have two choices: (1) ignore AZ and continue looking for the second part of the iRoot; (2) expose \nthe iRoot for AZ and discard the already exposed .rst part. In such a case, we select the .rst choice \nwith a higher probability. Finally, exposing iRoots for idiom5 is slightly different from other compound \nidioms. To expose an idiom5 iRoot A . B...C . D, our strategy is to let two different threads reach A \nand C, respectively; then execute them, and seek B and D in the corresponding threads.  4.8 Limitations \nOne limitation of the current active scheduler is that it can\u00adnot handle pre-conditions. The pre-conditions \nfor an iRoot is the necessary conditions that need to satis.ed to expose the iRoot. For example, in Figure \n10, there exists a pre-condition (from unlock to lock) that needs to be satis.ed so that the iRoot A \n. B can be exposed. Currently, our active sched\u00aduler has no knowledge of these pre-conditions; therefore \nit cannot enforce them. The complementary schedules might alleviate this problem to some extent. To fully \naddress this problem, one possible solution would be to automatically derive pre-conditions for a given \niRoot [21]. We leave this as a future work. 5. Memoization of iRoots Past work on systematic testing \nand active testing ignore the information about the interleavings tested from previous test runs with \ndifferent inputs. We believe that this is crucial in reducing the number of interleavings that need to \nbe tested for a given program input. Therefore, we propose a memoization module in our active testing \ninfrastructure. The memoization module is composed of a database of tested interleavings and a database \nof fail-to-test interleavings for each interleaving idiom as shown in Figure 1. This module is used to \navoid testing the same interleaving again and again across different test inputs. The candidate interleavings \nare pruned out depending on whether previous attempts were made to test them. If a can\u00addidate interleaving \nwas tested before (i.e. it has been exposed by the active scheduler), it is .ltered out by consulting \nthe tested interleavings database. This optimization is sound if the bugs we are targeting are not value \ndependent. Also, if several attempts were made in the past to test a candidate interleaving and the active \ntesting system failed to produce a legal execution exposing the desired interleaving, this can\u00addidate \ninterleaving is .ltered out using the fail-to-test inter\u00adleavings database, which stores all such failed \nto expose can\u00addidate interleavings. This allows us to avoid trying to expose thread interleavings that \ncan never manifest. Unlike mem\u00adoization of tested iRoots, this is an unsound optimization even if a bug \nis not value dependent. However, the number of times a candidate interleaving is actively tested is con.g\u00adurable, \nand a programmer can choose to set it to a very high value if soundness is a concern. 6. Evaluation This \nsection evaluates Maple and discusses our experiences in using the tool for testing real world applications. \nWe .rst describe the con.gurations of Maple we used in our experiments (Section 6.1). Then, we compare \nMaple with other general concurrency testing techniques in two different usage scenarios (Section 6.2 \nand Section 6.3), and show how memoization can be useful in both of these scenarios (Section 6.2.2 and \nSection 6.3.2). Finally, we discuss the ef.ciency and effectiveness of Maple (Section 6.4). 6.1 Maple \nCon.guration Maple is built to be highly con.gurable. We now describe the default con.gurations of Maple \nin our experiments.  In the Pro.ling Phase, the program is pro.led using the best randomized testing \ntechnique (explained later in Sec\u00adtion 6.3.1) a few number of times. For each pro.le run, the pro.ler \nobserves what iRoots are exposed and predicts candidate iRoots to test. The pro.ling phase stops when \nno new candidate iRoot is found for N consecutive pro.le runs (we use an empirical value N =3 throughout \nour exper\u00adiments). Unless otherwise noted, Maple observes and pre\u00addicts all iRoots in the program by \ndefault, including those iRoots from libraries such as Glibc. We believe this is nec\u00adessary because we \ndo .nd bugs that are related to the library code (e.g. Bug#11 and Bug#13 in Table 2). In the Testing \nPhase, candidate iRoots are exposed using the active scheduler. Currently, the active scheduler tests \nall candidate iRoots starting from idiom1 and then proceeds to test iRoots for other idioms in the order \nof their complexity (one to .ve). For each idiom, the active scheduler always chooses to test those iRoots \nthat are from the application code .rst. More sophisticated ranking mechanism may exist, but we leave \nthat to future work. Each candidate iRoot will be attempted at most twice, as mentioned in Section 4.3. \n6.2 Usage Scenario 1: Exposing Bugs with Bug Triggering Inputs One scenario that Maple can be useful \nis when a program\u00admer or a user accidentally exposed a non-deterministic bug for some input, but is unable \nto reproduce the failed execu\u00adtion. In that case, the programmer can use Maple with the bug triggering \ninput to quickly expose the buggy interleav\u00ading. Once the buggy interleaving is found, Maple can also \nre\u00adproduce it faithfully by replaying the same schedule choices, which can be very useful during the \ndebugging process. To evaluate the ability of Maple in exposing bugs in such scenarios, we choose 13 \nbuggy application with their corre\u00adsponding bug triggering inputs (shown in Table 2). Among these benchmarks, \n4 (Bug#1 to Bug#4) are synthetic bugs studied in [27], 1 (Bug#5) is a code snippet extracted from a real \nbuggy program, and 8 (Bug#6 to Bug#13) are real bugs from real executions. We want to know whether Maple \nis able to expose these bugs and how fast it can expose these bugs when compar\u00ading to other general concurrency \ntesting tools. We com\u00adpare Maple with two random testing techniques, PCT and PCTLarge. PCT [3] is a recently \nproposed random testing technique that provides probabilistic guarantee in exposing concurrency bugs. \nIn PCT, threads are randomly assigned a non-preemptive strict priority (similar to that used in the ac\u00adtive \nscheduler of Maple); during execution, PCT changes the priority of the currently running thread to lowest \nat random points d times. The authors state that most of the concur\u00adrency bugs can be exposed with a \nsmall value of d. In our experiment, we choose to use d =3. PCTLarge is a varia\u00adtion of PCT that we proposed. \nIt has the same algorithm as that in PCT except that it uses non-strict priorities instead of strict \npriorities. For instance, in Linux, we use nice values to serve as non-strict priorities. Higher priority \nthreads will have more time quantum than lower priority threads. Inter\u00adestingly, we found that PCTLarge \nusually performs better than PCT. More details are provided in Section 6.3.1. For each bug, we run it \nrepeatedly using its bug triggering input until the bug is triggered. Each time, a different testing \ntechnique is used. We compare the time needed by each test\u00ading technique to expose the bug. For Maple, \nwe assume no previously built memoization database is available. The ef\u00adfect of memoization is discussedinSection6.2.2. \nTable 2 shows the results. As shown in the table, Maple can expose all 13 bugs, including 3 previously \nunknown bugs (Bug#11 to Bug#13). In contrast, PCT and PCTLarge can only ex\u00adpose 7 and 11 bugs respectively \nbefore timeout (24 hours) in reached. Moreover, Maple can expose all the real bugs much faster than PCT \nand PCTLarge. Maple uses more time to ex\u00adpose Bug#5 than PCT and PCTLarge. This is because Bug#5 is an \nidiom4 bug and a lot of time is spent testing irrelevant idiom1, idiom2 and idiom3 iRoots according to \nour current ranking mechanism. We found that PCT or PCTLarge expose bugs faster than Maple on some applications \nwith small ex\u00adecution lengths (e.g. Bug#3). This is expected because the smaller the execution length, \nthe higher the probability to expose the bug, but Maple has to pay a high cost for anal\u00adysis. Nevertheless, \nthe random techniques do not scale for long execution lengths (e.g. Bug#8). Bug#10 does not have an idiom \nbecause it is value dependent. 6.2.1 Experiences in Finding Unknown Bugs We found three previously unknown \nbugs. Bug#11 was ac\u00adcidentally found when testing Bug#9, a documented bug in Aget. We observed a situation \nwhere the program hangs when testing Bug#9. We looked at the iRoot that caused the hang and tried the \nsame iRoot again with the same random seed. In addition, we attached a tracer to the active scheduler \nto record the execution trace. The deadlock happened again in less than 5 runs 3. With the help of the \ntrace, we eventu\u00adally found the root cause of this bug. The thread that handles signals is asynchronously \ncanceled when holding an i/o lock (in printf), causing a deadlock in the main thread when it tries to \nacquire the same lock. Bug#12 is an intermittent bug in an CNC-based appli\u00adcation that manifests as an \nassertion failure. CNC was de\u00adveloped by Intel and stands for Concurrent Collections. The particular \napplication we examined is a server-client appli\u00adcation that builds on Intel Thread Building Blocks (TBB) \nto synchronize threads. This bug was provided to us by a developer at Intel who could not expose it even \nafter at\u00adtaching a software deterministic record and replay tool to it [37]. Maple was able to expose \nthe assertion failure in about 400 test runs, much faster than the two random testing techniques. However, \nbecause we do not have access to the 3 This is because we cannot faithfully replay some non-deterministic \nexter\u00adnal events which are part of the program inputs.  Maple PCT [3] PCTLarge 1 LogProcSweep S Idiom1 \n11 16.5 1 0.6 17.1 98511 86400(TO) 10169 8188.6 3.3K 3 0.1 2 StringBuffer S Idiom1 8 12.0 1 0.8 12.8 \n40 56.4 61 49.1 2.4K 2 0.1 3 CircularList S Idiom3 6 9.5 1 1.0 10.6 6 9.1 18 14.6 3.3K 3 0.1 4 BankAccount \nS Idiom1 6 9.0 1 0.9 10.0 12 17.4 44 35.4 3.6K 3 0.1 5 MySQL-LogMiss E Idiom4 8 13.2 100 120.8 133.9 \n18 29.0 15 13.6 4.9K 3 0.1 6 Pbzip2 R-K Idiom1 8 151.9 2 3.2 155.1 26933 86400(TO) 3336 6144.1 32.1M \n3 0.1 7 Apache #25520 R-K Idiom1 36 580.7 93 1544.2 2124.9 3485 31688.0 12951 86400(TO) 218.5K 5 3.6 \n8 MySQL #791 R-K Idiom1 10 436.5 3975 43097.6 43534.1 11754 86400(TO) 10574 81887.2 1.8M 13 4.4 9 Aget \n#2 R-K Idiom4 9 148.1 11 29.2 177.4 152 355.0 335 619.5 32.0K 3 0.1 10 Memcached #127 R-K N/A 41 304.6 \n4 11.3 316.0 1010 3635.1 306 782.5 89.5K 4 1.2 11 Aget #1 R-U Idiom1 9 74.7 18 123.9 198.6 32075 86400(TO) \n47636 86400(TO) 529.5K 3 0.1 12 CNC R-U Idiom1* 6 50.6 403 4163.8 4214.4 11063 86400(TO) 10012 49046.8 \n209.6K 3 1.1 13 Glibc R-U Idiom1* 30 1120.4 20 36.6 1157.0 39560 86400(TO) 16147 34349.1 28.5M 4 0.1 \n Table 2. Bug exposing capability given bug triggering inputs. All the time reported in the table are \nin seconds. TO stands for timeout (24 hours). In the type column, S stands for synthetic bugs, E stands \nfor extracted bugs, R-K stands for real bugs which are known, and R-U stands for real bugs which are \nunknown. * The root cause of Bug#12 and bug Bug#13 have not been con.rmed yet. They are exposed when \nattempting idiom1 iRoots. source code, we could not help the programmer understand the root cause of \nthe bug using iRoot. TheGlibcbug (Bug#13) was also accidentally discovered when testing Bug#6 on a machine \nwith glibc-2.5. It man\u00adifested as an assertion failure from the free function. We could reproduce the \nbuggy interleaving using the same iRoot and the same random seed. The bug never showed up when a newer \nversion of glibc was used. Since the memory man\u00adagement code in glibc is quite complex, the root cause \nhas not been con.rmed yet.  6.2.2 Memoization Help Expose Bugs Faster We aware that applying memoization \nmay affect the bug ex\u00adposing capability of Maple. For example, if an iRoot cannot be exposed under some \ninputs, it does not mean that it is not feasible under other inputs. Since we put a limit on the to\u00adtal \nnumber of test runs on any iRoot in our current settings, the corresponding iRoot that leads to the bug \nmight not be attempted when the bug triggering input is used, causing the bug to be missed. In order \nto see how memoization can affect the bug exposing capability, we evaluate 4 real bugs from Table 2 (Bug#7 \nto Bug#10). Other real bugs are not cho\u00adsen either because the bugs can be exposed using any input (Bug#6, \nBug#11 and Bug#13), or no other input is available (Bug#12). We .rst test the 4 bugs using inputs that \ndo not trigger the bug to build the memoization databases. Then, we test the bugs using the bug triggering \ninputs. Table 3 shows the results. We can see that all the 4 bugs can be exposed when memoization is \napplied. More importantly, the time required to expose each bug also reduces drastically. For in\u00adstance, \nwe save about 94% of the testing time for Bug#8. In fact, for the server application bugs, we save can \na lot of testing time by memoizing those iRoots that are related to server start and server shutdown, \nclearly showing the bene.t of memoization.      7 Apache #25520 No 36 580.7 93 1544.2 2124.9  \n  8 MySQL #791 No 10 436.5 3975 43097.6 43534.1    9 Aget #2 No 9 148.1 11 29.2 177.4    Memcached \n#127 No 41 304.6 4 11.3 316.0 10 Table 3. Memoization help expose bugs more quickly. All the time reported \nin the table are in seconds.  6.3 Usage Scenario 2: Coverage-Driven Testing Another usage scenario that \nMaple can be helpful is when a programmer has a test input and wants to explore as many interleavings \nas possible for that input within the time bud\u00adget. In this scenario, Maple can be used to cover more \nin\u00adterleavings quickly. Also, memoization can prevent the pro\u00adgrammer from testing the same interleaving \nmultiple times, which helps reduce testing time. 6.3.1 Maple Achieves Higher Coverage Faster The .rst \nquestion we want to address is whether Maple can cover more interleavings faster than other testing techniques. \nTo quantify the coverage on interleavings, we use iRoot coverage as the coverage metric in our experiments. \nThe iRoot coverage is measured using a tuple of numbers, each of which is the number of exposed iRoots \nfor one idiom. For example, the following iRoot coverage (1, 2, 5, 100, 50) means that the test has successfully \nexposed 1 idiom1 iRoot, 2 idiom2 iRoots, 5 idiom3 iRoots, 100 idiom4 iRoots and 50 idiom5 iRoots. We \nhave implemented a tool, called observer, in our dynamic analysis framework to measure the iRoot coverage. \nThe same observer is also reused in the pro.ler to observe exposed iRoots during pro.le runs so as to \navoid testing these iRoots again during the test phase. We compare it with 4 other testing techniques: \nPCT, PCTLarge, RandDelay and CHESS. PCT and PCTLarge have already been introduced in Section 6.2. RandDelay \ninjects random time delay at random points during the execution.  The number of points in which a delay \nis introduced is pro\u00adportional to the execution length (one per 50K non stack memory accesses). The program \nis run on multi-core pro\u00adcessors when RandDelay is used. CHESS [32] is a system\u00adatic testing tool. For \na given program and a given input, it tries to explore all possible thread interleavings that have few \npreemptions. It was originally developed for Windows programs. We implemented it in our framework for \nLinux. Currently, it works for programs that use POSIX threads for synchronization. It employs the sleep-set \nbased partial or\u00adder reduction technique described in [30], and uses a fair scheduler discussed in [31]. \nWe use a preemption bound of 2 throughout our experiments as suggested in [29] 4.Tohan\u00addle a program \nthat has data races, we run a dynamic data race detector .rst to .nd all racy memory accesses in the \nprogram, and then inform the CHESS scheduler so that it can explore different orderings of these racy \nmemory accesses. We use seven bug-free multi-threaded applications in this experiments, among which (fft \nand radix) are scienti.c applications from Splash2 [46], (pfscan, pbzip2, aget)are utility programs, \nand (memcached and apache) are server applications. For scienti.c and utility programs, we use ran\u00addom \ninputs (e.g. random number of thread, random .les and directories, random URLs, etc.). For memcached,we \nwrote our own test cases which perform commonly used opera\u00adtions such as set/get keys and incr/decr keys. \nFor apache, we use SURGE [1] to generate URLs and use httperf to generate parallel requests. Notice that \nwhen testing server applications, each test run consists of starting the server, is\u00adsuing the requests, \nand stopping the server. This process is automated through scripting. To compare Maple with these tools, \nwe attach the same observer to each tool to collect the iRoot coverage after each test run. The current \nimplementation of CHESS cannot iden\u00adtify the low level synchronization operations used in Glibc. Though \nwe can treat those unrecognizable synchronization operations as racy memory accesses and still run CHESS \non it, we believe this approach is unfair to CHESS as the number schedules to explore will increase unnecessarily \ncomparing to the case in which we can recognize those synchronization operations. As a result, we decide \nto only consider iRoots from application code and ignore library code in this exper\u00adiment to ensure a \nfair comparison. Figure 11 shows the iRoot coverage achieved by these tools using the same amount of \ntime as Maple does. We run Maple till its completion. For apache, as we are not able to run Maple till \ncompletion due to its scale, we test it for 6 hours. The observer overhead is excluded from the testing \ntime. Y-axis is normalized to the iRoot coverage achieved by Maple. We are not able to run CHESS on aget, \nmemcached and apache because in these applications, some 4 In fact, 13 out of 14 bugs studied in [29] \nare exposed with a preemption bound of 2. Using a preemption bound larger than 2 will drastically increase \nthe number of test runs and exceed our time budget. non-deterministic events (e.g. network package arrival) \nare not controllable by CHESS 5. From the results shown in Figure 11, we .nd that Maple gains iRoot coverage \nfaster than all the tools we have analyzed. On average, it achieves about 15% more coverage than the \nsecond best tool in our experiment. Also, we .nd CHESS only achieves about 60% of the iRoot coverage \nachieved by Maple using the same amount of time as Maple does. Be aware that the results shown in Figure \n11 do not mean that Maple is able to explore more interleavings than random testing tools and systematic \ntesting tools. In fact, CHESS explores a different interleaving in each test run. The results shown here \nconvey a message that if we believe iRoot coverage is a good coverage metric for concurrent testing, \na specially engineered tool like Maple is able to achieve iRoot coverage faster than a more general testing \ntool such as CHESS and PCT. We also notice that PCTLarge performs better than other random testing techniques \nlike PCT and RandDelay.We believe the reason is because PCTLarge has more con\u00adtext switches than others. \nAs a result, we choose to use PCTLarge to randomize the pro.le runs in Maple. Figure 12 shows the rate \nof increase in iRoot coverage using different testing tools. The X-axis is the number of test runs and \nthe Y-axis is the total number of iRoots exposed so far. We only show results for those applications \non which weareableto run CHESS.We run CHESS till its completion in this experiment. The results in Figure \n12 further justify the fact that Maple is able to gain iRoot coverage faster than random testing tools \nand systematic testing tools. Also, we notice an interesting fact that CHESS experiences a slow start \nin gaining iRoot coverage. We believe this is due to the use of depth-.rst search strategy in CHESS. \nA best-.rst search strategy may alleviate this problem at the cost of storing more states [5].  6.3.2 \nMemoization Help Reduce Testing Time The next question we want to address is how much test\u00ading time we \ncan save when memoization is applied under this usage scenario. To do that, for each bug free applica\u00adtion, \nwe test it using 8 different inputs (inputi,i . [1, 8]). When testing with inputi+1, we compared the \ntesting time between the following two methods: (1) without memoiza\u00adtion database; (2) using the memoization \ndatabase built from input1 to inputi. We choose to memoize both the exposed iRoots and the fail-to-expose \niRoots (we set the threshold to 6, i.e. each iRoot will not be attempted more than 6 test runs). For \nthis experiment, we only test for idiom1 iRoots due to time constraints. Figure 13 shows the results. \nThe Y-axis represents the testing time of the method that uses memoization (normalized to the testing \ntime without mem\u00adoization). The line plotted in red shows the average of the applications we analyzed. \nWe observe that, with memoiza\u00adtion, the testing time reduces gradually when more and more 5 Such programs \nare called non closed programs.  MPLDC MP LDC MPLDC MP LDC MPLD MPLD MPLD MPLD fft radix pfscan pbzip2 \naget memcached apache geomean Figure 11. Comparison with different testing methods using the same amount \nof time. M stands for Maple, P stands for PCT, L stands for PCTLarge, D stands for RandDelay,and C stands \nfor CHESS. 45 100 40  80 # Exposed iRoots # Exposed iRoots # Exposed iRoots # Exposed iRoots 35 60 \n30 40 20 0 25 20 15 Maple  CHESS PCT PCTLarge RandDelay 0 10 20 30 40 50 60 Number of Runs Number \nof Runs (a) fft (b) pfscan 90 400 350 80 300  Maple CHESS PCT 70 60 50 40 30 250 200 150 100 50 0 PCTLarge \n RandDelay 0 100 200 300 400 Number of Runs Number of Runs (c) radix (d) pbzip2 Figure 12. Comparison \nwith different testing methods. X-axis is the number of test runs, and Y-axis is the total number of \niRoots exposed. inputs are tested. For input8, the average saving on testing time is about 90%. This \nclearly shows the bene.t of memo-1.20  apache 0.20   0.00   1 2 3 4 5 6 7 8 Input Figure 13. Memoization \nsaves testing time. Y axis is nor-  ization in reducing testing time.  6.4 Characteristics of Maple \nIn the following, we discuss the characteristics of Maple in terms of its ef.ciency (Section 6.4.1) and \neffectiveness (Section 6.4.2). Testing Time (Normalized) 1.00 fft 0.80 radix pfscan 0.60 pbzip2 aget \n0.40 memcached 6.4.1 Performance Overhead Table 4 shows the average performance overhead of the pro\u00admalized \nto the execution time without memoization. .ler and the active scheduler for each application. We also \ninclude the Pinbase overhead, which is the overhead of PIN Table 4. Runtime overhead of Maple comparing \nto native execution time.  App. Pinbase Pro.ler Active Scheduler fft 6.9X 30.9X 16.3X radix 6.9X 67.7X \n17.8X pfscan 8.3X 31.9X 27.7X pbzip2 9.5X 183.3X 45.4X aget 13.7X 34.4X 98.8X memcached 2.1X 4.8X 4.1X \napache 1.7X 6.2X 6.0X mysql 1.6X 15.7X 2.5X itself without any instrumentation. All the numbers shown \nin Table 4 are normalized to the native execution time. The overhead of the pro.ler varies depending \non the applications. The overhead ranges from 5X (I/O bound applications) to 200X (memory intensive applications), \nand on average is at about 50X. The overhead of the active scheduler also varies, ranging from 3X to \n100X. The average overhead is about 30X. We identify two major factors that contribute to the overhead \nof the active scheduler. One is due to the extra time delay that we introduce to solve the asynchronous \nexternal events problem. The other is because the candidate instruc\u00adtions of some infeasible iRoots are \nreached so frequently. We believe we still have room to improve the performance of the active scheduler. \n 6.4.2 Effectiveness of the Active Scheduler    App. Idiom1 Idiom2 Idiom3 Idiom4 Idiom5    fft \n87.5% 100.0% 40.0% 36.0% 25.0% radix 66.7% 0.0% 0.0% 16.9% 5.6% pfscan 13.3% 6.7% 5.6% 4.8% 6.4% pbzip2 \n23.4% 23.1% 8.0% 5.6% 12.8% aget 13.6% 3.6% 7.0% 2.7% 8.6% memcached 7.8% 1.4% 8.4% 2.7% 7.1% apache \n6.0%* 1.0%* 0.0%* 7.0%* 4.0%* mysql 5.0%* 0.0%* 1.0%* 1.0%* 2.0%* Table 5. The success rate of the active \nscheduler (# success\u00adfully exposed iRoots/ # total predicted iRoots). For apache and mysql, we experimented \nwith 100 randomly selected candidate iRoots. Finally, we discuss how effective the active scheduler is \nin exposing iRoots. For each application and each id\u00adiom, we collect the success rate of the active scheduler \n(# successfully exposed iRoots/ # total predicted iRoots). We run Maple till its completion except for \napache and mysql which exceed our time budget. For these two applications, we randomly sample 100 candidate \niRoots for each idiom and report the success rate. On average, the active scheduler achieves about 28% \nsuccess rate on idiom1, 17% on idiom2, 9% on idiom3, 10% on idiom4 and 9% on idiom5. We re\u00adalize that \nthe success rate for the active scheduler is not sat\u00adisfactory. We identify three major reasons: (1) \nthe pro.ler algorithm in not accurate in the sense that it cannot detect user customized happens before \nrelations, producing many infeasible iRoots; (2) the active scheduler cannot deal with pre-conditions \nwhich might exist for some iRoots. (3) no dy\u00adnamic information being associated with each iRoot com\u00adbining \nwith the fact that the currently candidate arbitration mechanism is not sophisticated enough causes some \niRoots unlikely to be exposed. Nonetheless, Maple succeeds at ex\u00adposing concurrency bugs faster than \nthe state of the art ran\u00addomization techniques, as previously demonstrated. We plan to further improve \nthe accuracy of the pro.ler and the active scheduler in future. 7. Related Work This section places our \nwork in the context of other testing methodologies and bug detection tools. 7.1 Coverage Driven Concurrent \nTesting There have been a few studies on coverage metrics for con\u00adcurrent programs [2, 22, 24, 39, 41, \n47]. Taylor et al. [41] presented a family of coverage criteria for concurrent Ada programs. All-du-path \n[47] is a coverage metric for concur\u00adrent programs that is based on de.nition-use pairs. Sherman et al. \n[39] discusses a few coverage metrics based on syn\u00adchronizations and inter-thread dependencies. However, \nun\u00adlike Maple, none of these work discusses a synergistic set of testing tools that can help programmers \nachieve high cover\u00adage for the proposed coverage metric and analyze its effec\u00adtiveness in exposing concurrency \nbugs.  7.2 Active Testing Recently several active testing methods have been proposed for concurrent \nsoftware testing [17, 20, 23, 35, 36, 38, 40]. A typical active testing tool has two phases: a prediction \nphase and a validation phase. In the prediction phase, these tools use approximate bug detectors to predict \npotentially buggy thread interleavings in a program. In the validation phase, an active scheduler would \ntry to exercise a suspicious buggy interleaving in a real execution to verify whether it is really a \nbug or merely a false positive. In the prediction phase, these tools use either static or dy\u00adnamic analysis \ntechniques to predict certain types of con\u00adcurrency bugs in a program such as data races [38], atom\u00adicity \nviolations [35, 36, 40], atomic-set serializability vio\u00adlations [17, 23], and deadlocks [20]. The interleaving \npat\u00adterns of these tools represent erroneous interleaving patterns and target certain types of concurrency \nbugs. Unlike these tools, our interleaving idioms de.nitions are more general and are used to de.ne a \ncoverage metric. An iRoot is not an anomalous thread interleaving, but just a coverage ele\u00adment that \nneeds to be exposed in an execution during test\u00ading. Maple s goal is to achieve a high coverage for our \nin\u00adterleaving idioms, which we believe is more general for test\u00ading purpose than these bug driven active \ntesting tools. Also, during testing, we memoize interleavings that have been al\u00adready covered and avoid \ntesting the same interleavings again across different inputs. To the best of our knowledge, none of the \nprevious active testing tools use memoization across different inputs. We elaborate more on speci.c differences \nbetween our idioms and the bug patterns used in prior studies. Race\u00adFuzzer [38] targets data races. The \ninterleaving pattern of data races is different from idiom1. One obvious differ\u00adence is that idiom1 not \nonly capture inter-thread depen\u00addencies that are racy, but also capture non-racy dependen\u00adcies. Therefore, \nidiom1 can capture those race free order violation bugs which RaceFuzzer cannot. AssetFuzzer [23] and \nPECON [17] detects atomic-set serializability viola\u00adtions [42] for Java programs. The interleaving patterns \nof atomic set serializability violations are also different from our idioms. The main reason is because \nthese patterns are de.ned over atomic sets and units of work which do not ex\u00adist and cannot be easily \nidenti.ed in C/C++ programs. Dead\u00adlockFuzzer [20] detects deadlocks. Our idiom5 only captures a subset \nof the deadlocks patterns that can be captured by DeadlockFuzzer, which involve only two threads.  Maple \ns prediction algorithm is similar to that in [17, 23, 36, 40] except that Maple uses an online algorithm \nwhile all these tools use trace based of.ine algorithms. We believe an online algorithm is more practical \nin that it does not need to collect execution traces which can potentially be very large for long running \nprograms. There are two common ways in which validation is per\u00adformed. One way is to precisely compute \nan alternate sched\u00adule from the observed schedule to expose the bug and then enforce it [17, 21, 40]. \nHowever, this requires expense analy\u00adsis, which is not suitable if the goal is to achieve interleaving \ncoverage like ours. The other approach is to use heuristics, usually best effort methods, to expose predicted \ninterleav\u00adings [20, 23, 35, 36, 38]. Maple s active scheduler falls into the second category. We believe \nMaple is better due to the following reasons: .rst, Maple uses a novel idea by lever\u00adaging the non-preemptive \nstrict priorities provided by the underlying OS, eliminating the need of monitoring all syn\u00adchronization \noperations and blocking system calls that are required by those previous systems [23, 35, 38], thus is \nsim\u00adpler and less expensive; Second, Maple is more sophisticated in the sense that many of the issues \nthat Maple handles are not handled or not handled well by those tools. For example, RaceFuzzer, CTrigger \nand AtomFuzzer does not solve the deadlock problem (also referred as thrashing [20]) very well. Also, \nnone of them handle asynchronous external events.  7.3 Stress Testing and Random Testing Stress testing \nis still widely used in software industry to\u00adday. A parallel program is subjected to extreme scenarios \nduring test runs hoping to expose buggy interleavings. This method is clearly inadequate since naively \nexecuting a pro\u00adgram again and again over an input tends to unnecessarily test similar thread interleavings. \nA few techniques have been proposed to improve the stress-testing. The main idea is to randomize the \nthread interleavings so that different thread interleavings will be exercised in different test runs. \nThese techniques mainly differ in the way that they randomize the thread interleavings. For example, \nConTest [6] injects ran\u00addom delays at synchronization points. PCT [3] assigns ran\u00addom priority to each \nthread and change priorities at random points during an execution. However, all of these random testing \ntechniques suffer a common problem: the probabil\u00adity of exposing a rare interleaving that can trigger \na concur\u00adrency bug is very low given that the interleaving space is so huge. Comparing to these random \ntesting techniques, our technique has a much higher probability in exposing concur\u00adrency errors due to \nthe following two reasons: .rst, the id\u00adioms guides our tool to test those interleavings that are more \nlikely to cause concurrency errors; second, we have memo\u00adization which would guide us to test untested \ninterleavings .rst. Our results clearly demonstrated the ability of Maple in exposing untested interleavings \nand bugs faster than the best randomization techniques. 7.4 Systematic Testing An alternative to stress \ntesting is systematic testing [13, 16, 32, 44] which tries to explore all possible thread in\u00adterleavings \nfor each test input. Even with partial order re\u00adduction techniques [10, 12], the number of thread interleav\u00adings \nto test for a given input is still huge. Therefore, a few heuristics have been proposed to further reduce \nthe test\u00ading time at the cost of missing potential concurrency errors. CHESS [32] bounds the number of \npreemptions in each test run. HaPSet [44] records observed PSet [48] (which are es\u00adsentially idiom1 iRoots) \nduring testing and guides system\u00adatic search towards those interleavings that can produce new PSet dependencies. \nHowever, even if these heuristics are used, these tools still suffer from scalability problem, espe\u00adcially \nfor programs that have long execution length. Further\u00admore, these tools do not have a way to remember \ntested inter\u00adleavings across different inputs, unlike Maple. Finally, these systematic testing tools \nusually require a closed unit testing environment which is in fact not easy to realize in practice. In \ncontrast, a completely closed unit testing environment is not strictly required while using Maple. However, \nsuch tools do have one distinct advantage over Maple in that they can provide certain guarantees to .nd \na concurrency bug in a program for a given input. 7.5 Test Input Generation Test input generation is \na testing technique that can be used to achieve high code coverage [4, 14, 15, 34]. For a given program, \ntheir goal is to generate test inputs so that testing the program with the generated test inputs can \ncover most of the code in the program. In contrast, the goal of Maple is to orchestrate thread interleavings \nfor a given test input to cover more thread interleavings. Thus, Maple complements test input generators \nand aids in achieving higher iRoot cov\u00aderage.  7.6 Bug Detection Tools Concurrency bug detection tools \ncan be divided into two cat\u00adegories: static bug detection tools and dynamic bug detection tools. Static \nconcurrency bug detection tools [7, 11, 25, 33, 43] analyze programs statically and predict potential \nconcur\u00adrency bugs. Most of the static bug detection tools produce large volume of false positives, thus \npreventing them from being widely used by programmers. MUVI [25] uses static source analysis to predict \nmulti-variable atomicity viola\u00adtions. It can complement Maple s pro.ler for .nding iRoots of complex \nidioms, but unlike Maple, it does not use active scheduler for exposing predicted erroneous interleavings. \n Dynamic bug detection tools [8, 9, 26, 45, 49] usually require bugs to manifest during monitored runs \nand do not actively seek to expose incorrect interleavings. Maple could complement dynamic bug detection \ntools by producing new thread interleavings faster. 8. Conclusion Maple is a new coverage-driven approach \nto test multi\u00adthreaded programs. To this end, we discussed a coverage metric based on a generic set of \ninterleaving idioms. We dis\u00adcussed a pro.le-based predictor that determines the set of untested thread \ninterleavings that can be exposed for a given input, and an active scheduler to effectively expose them. \nA key advantage of our approach over random and systematic testing tools is that we avoid testing the \nsame thread inter\u00adleavings across different test inputs. Our experience in using Maple to test real-world \nsoftware shows that Maple can trig\u00adger bugs faster by exposing more untested interleavings in a shorter \nperiod of time than conventional methods. Acknowledgments We thank the anonymous reviewers for comments \nthat im\u00adproved this paper. This work is supported by the National Science Foundation under the award \nCCF-0916770 and an Intel grant. References [1] P. Barford and M. Crovella. Generating representative \nweb workloads for network and server performance evaluation. In SIGMETRICS, pages 151 160, 1998. [2] \nA. Bron, E. Farchi, Y. Magid, Y. Nir, and S. Ur. Applications of synchronization coverage. In PPOPP, \npages 206 212, 2005. [3] S. Burckhardt, P. Kothari, M. Musuvathi, and S. Nagarakatte. A randomized scheduler \nwith probabilistic guarantees of .nd\u00ading bugs. In ASPLOS, pages 167 178, 2010. [4] C. Cadar, D. Dunbar, \nand D. R. Engler. Klee: Unassisted and automatic generation of high-coverage tests for complex systems \nprograms. In OSDI, pages 209 224, 2008. [5] K. E. Coons, S. Burckhardt, and M. Musuvathi. Gambit: effective \nunit testing for concurrency libraries. In PPOPP, pages 15 24, 2010. [6] O. Edelstein, E. Farchi, Y. \nNir, G. Ratsaby, and S. Ur. Multi\u00adthreaded java program test generation. IBM Systems Journal, 41(1):111 \n125, 2002. [7] D. R. Engler and K. Ashcraft. Racerx: effective, static detec\u00adtion of race conditions \nand deadlocks. In SOSP, pages 237 252, 2003. [8] C. Flanagan and S. N. Freund. Fasttrack: ef.cient and \npre\u00adcise dynamic race detection. Commun. ACM, 53(11):93 101, 2010. [9] C. Flanagan, S. N. Freund, and \nJ. Yi. Velodrome: a sound and complete dynamic atomicity checker for multithreaded programs. SIGPLAN \nNot., 43(6):293 303, 2008. [10] C. Flanagan and P. Godefroid. Dynamic partial-order reduc\u00adtion for model \nchecking software. In POPL, pages 110 121, 2005. [11] C. Flanagan and S. Qadeer. A type and effect system \nfor atomicity. SIGPLAN Not., 38(5):338 349, 2003. [12] P. Godefroid. Partial-Order Methods for the Veri.cation \nof Concurrent Systems -An Approach to the State-Explosion Problem, volume 1032 of Lecture Notes in Computer \nScience. Springer, 1996. [13] P. Godefroid. Model checking for programming languages using verisoft. \nIn POPL, pages 174 186, 1997. [14] P. Godefroid, N. Klarlund, and K. Sen. Dart: directed auto\u00admated random \ntesting. In PLDI, pages 213 223, 2005. [15] P. Godefroid, M. Y. Levin, and D. A. Molnar. Automated whitebox \nfuzz testing. In NDSS, 2008. [16] K. Havelund and T. Pressburger. Model checking java pro\u00adgrams using \njava path.nder. STTT, 2(4):366 381, 2000. [17] J. Huang and C. Zhang. Persuasive prediction of concurrency \naccess anomalies. In ISSTA, pages 144 154, 2011. [18] D. Jackson and C. Damon. Elements of style: Analyzing \na software design feature with a counterexample detector. IEEE Trans. Software Eng., 22(7):484 495, 1996. \n[19] V. Jagannath, M. Gligoric, D. Jin, Q. Luo, G. Rosu, and D. Marinov. Improved multithreaded unit \ntesting. In SIG-SOFT FSE, pages 223 233, 2011. [20] P. Joshi, C.-S. Park, K. Sen, and M. Naik. A randomized \ndynamic program analysis technique for detecting real dead\u00adlocks. In PLDI, pages 110 120, 2009. [21] \nV. Kahlon and C. Wang. Universal causality graphs: A pre\u00adcise happens-before model for detecting bugs \nin concurrent programs. In CAV, pages 434 449, 2010. [22] B. Krena, Z. Letko, and T. Vojnar. Coverage \nmetrics for saturation-based and search-based testing of concurrent soft\u00adware. In RV, 2011. [23] Z. Lai, \nS.-C. Cheung, and W. K. Chan. Detecting atomic-set serializability violations in multithreaded programs \nthrough active randomized testing. In ICSE (1), pages 235 244, 2010. [24] S. Lu, W. Jiang, and Y. Zhou. \nA study of interleaving coverage criteria. In ESEC/SIGSOFT FSE, pages 533 536, 2007. [25] S. Lu, S. Park, \nC. Hu, X. Ma, W. Jiang, Z. Li, R. A. Popa, and Y. Zhou. Muvi: automatically inferring multi-variable \naccess correlations and detecting related semantic and concurrency bugs. In SOSP, pages 103 116, 2007. \n[26] S. Lu, J. Tucek, F. Qin, and Y. Zhou. Avio: detecting atomicity violations via access interleaving \ninvariants. In ASPLOS, pages 37 48, 2006.  [27] B. Lucia, J. Devietti, K. Strauss, and L. Ceze. Atom-aid: \nDetecting and surviving atomicity violations. In ISCA, pages 277 288, 2008. [28] C.-K. Luk, R. S. Cohn, \nR. Muth, H. Patil, A. Klauser, P. G. Lowney, S. Wallace, V. J. Reddi, and K. M. Hazelwood. Pin: building \ncustomized program analysis tools with dynamic in\u00adstrumentation. In PLDI, pages 190 200, 2005. [29] M. \nMusuvathi and S. Qadeer. Iterative context bounding for systematic testing of multithreaded programs. \nIn PLDI, pages 446 455, 2007. [30] M. Musuvathi and S. Qadeer. Partial-order reduction for context-bounded \nstate exploration. Technical Report MSR\u00adTR-2007-12, Microsoft Research, 2007. [31] M. Musuvathi and S. \nQadeer. Fair stateless model checking. In PLDI, pages 362 371, 2008. [32] M. Musuvathi, S. Qadeer, T. \nBall, G. Basler, P. A. Nainar, and I. Neamtiu. Finding and reproducing heisenbugs in concurrent programs. \nIn OSDI, pages 267 280, 2008. [33] M. Naik, C.-S. Park, K. Sen, and D. Gay. Effective static deadlock \ndetection. In ICSE, pages 386 396, 2009. [34] C. Pacheco and M. D. Ernst. Randoop: feedback-directed \nrandom testing for java. In OOPSLA Companion, pages 815 816, 2007. [35] C.-S. Park and K. Sen. Randomized \nactive atomicity violation detection in concurrent programs. In SIGSOFT FSE, pages 135 145, 2008. [36] \nS. Park, S. Lu, and Y. Zhou. Ctrigger: exposing atomicity violation bugs from their hiding places. In \nASPLOS, pages 25 36, 2009. [37] H. Patil, C. Pereira, M. Stallcup, G. Lueck, and J. Cownie. Pinplay: \na framework for deterministic replay and repro\u00adducible analysis of parallel programs. In CGO, pages 2 \n11, 2010. [38] K. Sen. Race directed random testing of concurrent programs. In PLDI, pages 11 21, 2008. \n[39] E. Sherman, M. B. Dwyer, and S. G. Elbaum. Saturation\u00adbased testing of concurrent programs. In ESEC/SIGSOFT \nFSE, pages 53 62, 2009. [40] F. Sorrentino, A. Farzan, and P. Madhusudan. Penelope: weaving threads to \nexpose atomicity violations. In SIGSOFT FSE, pages 37 46, 2010. [41] R. N. Taylor, D. L. Levine, and \nC. D. Kelly. Structural testing of concurrent programs. IEEE Trans. Software Eng., 18(3):206 215, 1992. \n[42] M. Vaziri, F. Tip, and J. Dolby. Associating synchronization constraints with data in an object-oriented \nlanguage. In POPL, pages 334 345, 2006. [43] J. W. Voung, R. Jhala, and S. Lerner. Relay: static race \ndetection on millions of lines of code. In ESEC/SIGSOFT FSE, pages 205 214, 2007. [44] C. Wang, M. Said, \nand A. Gupta. Coverage guided systematic concurrency testing. In ICSE, pages 221 230, 2011. [45] D. Weeratunge, \nX. Zhang, and S. Jagannathan. Analyzing multicore dumps to facilitate concurrency bug reproduction. In \nASPLOS, pages 155 166, 2010. [46] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. The splash-2 \nprograms: Characterization and methodological considerations. In ISCA, pages 24 36, 1995. [47] C.-S. \nD. Yang, A. L. Souter, and L. L. Pollock. All-du-path coverage for parallel programs. In ISSTA, pages \n153 162, 1998. [48] J. Yu and S. Narayanasamy. A case for an interleaving con\u00adstrained shared-memory \nmulti-processor. In ISCA, pages 325 336, 2009. [49] W. Zhang, J. Lim, R. Olichandran, J. Scherpelz, G. \nJin, S. Lu, and T. W. Reps. Conseq: detecting concurrency bugs through sequential errors. In ASPLOS, \npages 251 264, 2011. [50] W. Zhang, C. Sun, and S. Lu. Conmem: detecting severe concurrency bugs through \nan effect-oriented approach. In ASPLOS, pages 179 192, 2010.     \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Testing multithreaded programs is a hard problem, because it is challenging to expose those rare interleavings that can trigger a concurrency bug. We propose a new thread interleaving coverage-driven testing tool called Maple that seeks to expose untested thread interleavings as much as possible. It memoizes tested interleavings and actively seeks to expose untested interleavings for a given test input to increase interleaving coverage. We discuss several solutions to realize the above goal. First, we discuss a coverage metric based on a set of interleaving idioms. Second, we discuss an online technique to predict untested interleavings that can potentially be exposed for a given test input. Finally, the predicted untested interleavings are exposed by actively controlling the thread schedule while executing for the test input. We discuss our experiences in using the tool to expose several known and unknown bugs in real-world applications such as Apache and MySQL.</p>", "authors": [{"name": "Jie Yu", "author_profile_id": "81549201556", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3856119", "email_address": "jieyu@umich.edu", "orcid_id": ""}, {"name": "Satish Narayanasamy", "author_profile_id": "81100556410", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3856120", "email_address": "nsatish@umich.edu", "orcid_id": ""}, {"name": "Cristiano Pereira", "author_profile_id": "81342507330", "affiliation": "Intel Corporation, Santa Clara, CA, USA", "person_id": "P3856121", "email_address": "cristiano.l.pereira@intel.com", "orcid_id": ""}, {"name": "Gilles Pokam", "author_profile_id": "81319499525", "affiliation": "Intel Corporation, Santa Clara, CA, USA", "person_id": "P3856122", "email_address": "gilles.a.pokam@intel.com", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384651", "year": "2012", "article_id": "2384651", "conference": "OOPSLA", "title": "Maple: a coverage-driven testing tool for multithreaded programs", "url": "http://dl.acm.org/citation.cfm?id=2384651"}