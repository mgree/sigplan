{"article_publication_date": "10-19-2012", "fulltext": "\n Eval Begone! Semi-Automated Removal of Eval from JavaScript Programs Fadi Meawad Gregor Richards Flor\u00b4eal \nMorandat JanVitek Purdue University Abstract Eval endows JavaScript developers with great power. It \nallows developers and end-users, by turning text into executable code, to seamlessly extend and customize \nthe behavior of deployed applications as they are running.With great power comes great responsibility, \nthough not in our experience. In previous work we demonstrated through a large corpus study that programmers \nwield thatpowerin rather irresponsible and arbitraryways.We showed that most calls to eval fall into \na small number of very predictable patterns. We argued that those patterns could easily be recognized \nby an automated algorithm and that theycould almost always be replaced with safer JavaScript idioms. \nIn this paper we set out to validate our claim by designing and implementing a tool, which we call Evalorizer, \nthat can assist programmers in getting rid of their unneeded evals. We use the tool to remove eval from \na real-world website and validated our approach over logs taken from the top 100 websites with a success \nrate over 97% under an open world assumption. Categories and Subject Descriptors D.2.3[Software Engi\u00adneering]: \nCodingTools andTechniques Program editors; D.2.7 [Software Engineering]: Distribution, Maintenance, and \nEnhancement Restructuring, reverse engineering, and reengineering General Terms Languages Keywords Dynamic \nLanguages, JavaScript, Re.ection, Dy\u00adnamic Analysis 1. Introduction JavaScript s eval function gives \nprogrammers the ability to run JavaScript code generated at runtime. This gives programmers extraordinary \n.exibility, but at the cost of Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage.To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012,Tucson, Arizona, USA. Copyright c &#38;#169; \n2012ACM 978-1-4503-1561-6/12/10... $10.00 understandability, ef.ciencyand safety. As strings passed to \neval may come from anysource, including computation, user input or another website, and as eval is capable \nof performing anytask1, it can serve as a black hole both for analysis and for maintenance; to understand \nits behavior, one must know every potential argument. Consider the following JavaScript expression: eval( \nx) Depending on the value bound to variable x, the state of any heap-allocated mutable value and the \nbindings of local varia\u00adbles in scope can be modi.ed as a side effect of evaluating this statement. While \nsome languages can enforce some mod\u00adicum of data abstraction, JavaScript has very little in the way of \nencapsulation mechanisms. The impact of an eval can span over the entire heap. The existence of eval \nis a quandary for those wishing to perform static analyses on JavaScript code or enforce any kind of \nsemantic invariants.With an unknown string, eval has no locality guarantees, no time or memory bounds, \nnot even a termination guarantee. Since the strings frequently come from outside sources and are as such \ncompletely unknown, static analyses are forced to assume the worst, losing all potentialgains from the \nanalysis, and dynamic analyses are at best forced to reevaluate every time an eval is encountered. It \nis common for researchers to simply ignore it[1,2, 12, 23], claim it is very rare [9], assume its use \nis innocuous[10], or acknowledge its problemsbut simply produceawarning when it is used[13], resulting \nin unsound or even unsafe results. In security literature, in particular, eval is viewed as a serious \nthreat[22], and it is frequently forbidden[16], .ltered[6]or wrapped[6],but allof these solutions implya \n.exibility or speed penalty. Most dynamic languages have an eval function or similar feature, and in \nmanyit is considered harmful, though not all. TheRprogramming languageis anexample where eval is a keymechanism \nfor languageextensibility[17].In previous work[21], we showed that eval is ubiquitous in the largest \nand most popular websites on the Internet; we speculated that over 1JavaScript provides several other \nsuch functions, such as setTimeout and Function,butwe focuson eval for muchof our discussion.  75% of \nevals could be replaced by other mechanisms within JavaScript. In the present paper, we set out to validate \nthat claim and end up demonstrating that over 95% of the evals invocationsinreal websites canbereplaced \nwith lessgeneral mechanisms with very little work on the programmer s part. Ourworkinghypothesisis that \nno more than5%of all evals are actually needed. Examining the most common use cases of eval, we observe \nthat programmers decide to use eval for one of the following reasons: they use eval to parse JSON and \nmalformed-JSON. For parsing JSON, until recently some browsers did not have native JSON parsers, but \nthis is no longer the case. Current native JSON parsers does not handle malformed-JSON. Others use eval \nto access or modify properties based on user input. It might require some small parsing to handle the \ninput without using eval. And the last category of users is the one executing code coming from third \nparty with no prior knowledge of its shape. Most modern browsers have native JSON parsers thatis usuallyfaster \nthan eval for a JSON string, and it only acceptsJSON strings, thus improving the security. Parsing user \ninput to extract the correct expression might be hard to program manually,but once eval is used we loose \nthe possibility of using static analysis as well as suffer the risk of code injection. When running third \nparty code, we have noticed that it is usually the same or at least has the same shape. But the user \nwould not be able to .gure out what kind of strings are being executed and how to write a parser for \nthem. We see three possible roads to an eval-free Internet: prohi\u00adbition, prevention and migration. Prohibition \nis conceptually the simplest. If calls to eval are simply disallowed, the prob\u00adlemisno more,but this \ncomesata costinexpressivepower. Eval occasionally is the only practical way to achieve a certain degree \nof customizability in the behavior of a website, so for\u00adbidding it would reduce the expressive power \nof JavaScript. In other cases, eval is a way to delay design and implementation decisions, thus allowing \ndevelopers to deploy applications faster. Again, losing that would decrease the usefulness of the language \nfor rapid development. The second path is to pre\u00advent eval by proving, ahead of time, that they are not \nneeded. This can be done through static program analysis techniques which construct approximate models \nof the program and can determine which strings .ow into a particular eval call site. Indeed, there are \nmany constant and quasi-constant strings that are passed in as argument to eval.A static analysis could \nbe coupled with a compiler optimization that compiles the evals to equivalent code. This could be as \nsimple as removing the quotes around a piece of text and splicing in the program where the eval used \nto be. Unfortunately there are inherent limits to static analysis approaches; they are unable to guess, \nfor instance, what a user will type. In our experience, over 40% of strings passed to eval are, at least \nin part, generated by components outside of JavaScript code, such as the user, browser or server.We also \nobserved that many of the most gnarly evals take their argument from outside the JavaScript program. \nThis leaves migration as the third path. This is a path where we attempt wean users off their addiction \nto eval by showing them how to rewrite their code without it. We recognize that eval can be useful either \nbecause the program actually requires the .exibility that it brings to the table or because it is a handy \ncrutch during an early phase of the program s life,but argue thatin most cases, thereisa point where \nthe same functionality can be achieved without it. We propose a simple dynamic approach to get rid of \neval. The technique we present in this paper detects how eval is used through dynamic analysis of calls \nto eval. It categorizes the strings passed as argument to each call site of eval and proposes generic \nreplacements that do not involve calls to eval. We realize our proposal in a tool which we name the Evalorizer. \nThis tool aids in the evolution of eval-utilizing code to eval-free code by presenting replacements that \n.t the real use of each call site. This allows programmers to use eval in the development phase, when \nits .exibility may be most bene.cial, then to gradually remove it in preference of simpler, safer and \nmore readable solutions. We use a grammar inference algorithm to determine the used patterns of anygiven \neval call site as a restricted subset of JavaScript s grammar, then generate succinct code that will \nhandle all the same patterns,but with greater constraint. Additionally to aiding developers in removing \nevals, Evalorizer is also capable of dynamic eval removal. This technique can be used for veri.cation, \nfor measurement, or to perform analyses on otherwise hostile programs without intervention of the original \ndevelopers. Though we focus on JavaScript, eval is certainly not unique to that language. Our implementation \nis speci.c to JavaScript,but our techniques are not, and are applicable to anylanguage which provides \na function similar to eval as well as other, safer re.ective capabilities. We evaluate the bene.ts of \nour approach by successfully migrating .ve real world websites. Furthermore, we use data obtained from \nthe 100 top websites and evaluate the quality of our inference algorithm. We also measure the runtime \nperformance impact of our technique. Our tools and data are freely available at: http://sss.cs.purdue.edu/projects/dynjs \n 2. Use case We motivate Evalorizer with a typical use case. Assume that a web programmer, .nding that \nthe existing calls to eval on CNN.com inhibit maintainability, wants to determine if some of those calls \ncan be removed. The programmer does not have intimate knowledge of all of the components of this site. \nThe JavaScript frontend maybe easily understood, but the part that communicates with server components \nand third-party code is obscure. Because the argument strings of eval are unconstrained and may have \ncome from these sources, the programmer does not know what these calls are doing or even if they are \nactually useful. Some of these calls may be needed, but it isn t clear from looking at the code. CNN.com \nalso Browser Evalorizer Proxy CNN.com Server uses code from a third party domain ATDMT.com that our \n 1 2 developer does not trust entirely.To integrate both websites, 3 she was instructed by ATDMT.com \ns representative to eval the code they are providing,buta safer and more predictable  4 Session 2 Session \n1 mechanism would be preferable. 5 Once launched, Evalorizer acts as an HTTP proxy, inter\u00ad 6 posing \non the traf.c between the browser and the server. 7 There is no speci.c requirement on where it has to \nbe run, as it does not integrate into the server software, so the devel\u00adoper s own machine is suitable. \nThe programmer may use any web browser, so long as it is con.gured to proxy all relevant traf.c through \nEvalorizer. Fig.1 illustratesthis setup, as well as the communication between the different components \nof the system over time for a sample interaction.  8 Evalorizer Patcher 9  10 Browsing the website \nis the only task left to the program\u00ad mer. Ideally she should use as much of the site as is practical, \n11 to exercise every potential eval in the code; the more pages she browses and the more evals are triggered, \nthe more accu\u00adrate the results will be. Once the browsing session is complete, Evalorizer returns a log \nof all evals encountered in the code. For each call site, the user is provided with the arguments, the \n.le name where it was located, and the absolute location within this .le. Our user visits the CNN.com \nhomepage. The request is sent to the proxy(. 1)that in turn forwards the request to the CNN.com server(. \n2). The server returns the homepage to the proxy(. 3). The homepage had, in our example, two eval call \nsites. One was in the HTML page in an embedded script tag, and the other was from an external JavaScript \n.le. The call sites are: .lename line eval call site 1 index.html 105 t = eval(response); 2 lib1.js 1402 \np= eval( window. + x); Inspecting the eval call sites or even its surrounding code does not provide \nany intuition on what eval is performing and what could be a replacement code. In the .rst call site, \nit just evaluates the variable response, that is obtained from an asynchronous call back. Performing \npost .ow analysis for the left-hand-side t may give some intuition on how the return value of eval is \nused,but its side effects (including changes to the lexical environment) will remain obscure.We contend \nthat the second eval call site, despite that part of the argument is a string literal, is exactly as \nobscure as the .rst one. The proxy logs the call site information for future use, called the static log. \nThen it rewrites the eval call sites by invoking a function that will send the eval argument to the proxy, \nthen call eval. The rewriting for our example is: t =( ev1=(response),logEval( ev1, 1),eval( ev1)); p=(ev2 \n= ( window. + x),logEval( ev2,2),eval( ev2)); logEval is a function that sends an asynchronous message \nto the proxy with the argument information; the .rst parameter On the .rst session, the browser requests \na page from the proxy (. 1), that in turn forwards the request to the CNN.com server (. 2). After the \nserver sends the requested page to the proxy(. 3), the later instruments the page and sends it to the \nbrowser (. 4). When an eval is invoked, the browser sends a message to the proxy (. 5, 6, 7). After the \n.rst session is complete, the patcher will acquire the logs form the proxy(. 8), and creates a patch \nthat can be used to update the server(. 9). After patching, the browser communicates with the server \ndirectly(. 10, 11). Figure 1. A typical execution of Evalorizer. is the eval argument, and the second \nis an identi.er that maps the invocation to the call site. The proxy then sends the updated .les to the \nbrowser(. 4). While the page was loading on the browser, the sec\u00adond eval has been invoked twice, with \nthe arguments window.width and window.height.With some interaction with the page, for example by expanding \na collapsible <div>, the .rst call site is invoked with the argument ({ type : news , value :[{ order \n: 2, name : news1 , text : This event has happened }, .... ]}) For each invocation, thelogEval (. 5, \n6, 7)function is called, and therefore the proxy now has a log of the invocations, called the dynamic \nlog. She should attempt to visit every part of the page as well as visiting as manypossible pages. The \nprogrammer can inspect the proxy logs at any time, and it will give her the number of invocations for \neach eval call site if any. Five invocations is usually suf.cient to discover the exact functionality \nof the call site. After this exercise is complete, the user will run the second component of our tool, \nthe patcher. It uses both the static and dynamic logs(. 8), and generates a patching script. The script \ndepends on the policy used, whether you have full con.dence of the replacement, or not, and whether you \nwant to inspect possible hacking attempts. A simpli.ed version of the generated script for both call \nsites is (line numbers omitted):  ---index.html -t = eval(response); +t = JSON.parse(response.substring(1, \n response.length-1)); ---lib1.js -p = eval(\"window.\" + x); +p = window[x]; The programmer now should \napply the patch to his original code(. 9), remove the proxy con.guration(. 10, 11), and start using the \nwebsite eval-free! 3. The State of the Eval This section reviews the semantics and use of eval. Java\u00adScript[7]is \nsupportedby all major web browsers andis an imperative, object-oriented language with Java-like syntax \nand a prototype-based object system. An object is a set of properties that behaves like a mutable map \nfrom strings to values. In JavaScript, eval is a function de.ned in the global scope. When invoked with \na single string argument, it parses and executes the argument. It returns the result of the last evaluated \nexpression. eval can be invoked in two ways: called directly, the eval dcode has access to all variables \nlexically in scope, when called through an alias, the eval d code executes in the global scope[7]. While \neval is powerful, its use is often unnecessary in practice. Consider, eval( r.m + input) For likely values \nof input, the above code could be imple\u00admented as r[ m + input] Rather than invoking the full power of \neval, indexing r returns the desired property with fewer surprises. In previouswork we identi.ed patternsin \nthe10K most popular web sites [21]. Some are industry best practices, but most result from poor understanding \nof the language, repetition of old mistakes, or adapting to browserbugs. These patterns can be detected \nby a simple syntactic check: JSON A JSON string. { x : 1 } JSONP A use of JSON. foo( { x :1}) Library \nFunction de.nitions. function(){...} Read Read of a property. x.foo Assign Assignment. x.foo = 3 Typeof \nType test expression. typeof(x)!= unde.ned Try Trivial try-catch block. try{x=3; }catch{} Call Simple \ncall. x.foo(1,2,3) Empty Blank string. JSON and JSONP are stringsin JSON[7],a literal notation used \nfor data interchange. JSONP pattern is often used for load balancing requests across domains. JSON parsing \nis 100% 90% 80% 70% 60% 50% 40% 30% 20% 10% 0% polymorphic cases were clearly a single centralized eval \nused from manybranches and for manypurposes. The eval strings come from different parts of the program. \nWe have identi.ed seven common provenances. Constant Appears in the source code. Composite Concatenated \nfrom constants and primitives. Synthetic Constant in a nested eval. DOM Obtained from DOM or native calls. \nAJAX Data retrieved from an AJAX call. Cookies Retrieved from a cookie or persistent storage. Input Entered \nby a user into form elements. Fig.2 shows the proportionof the different provenancesby pattern. What \nis striking is that some patterns such as JSON (and the uncategorized grab bag Other)have close to 50% \nof their arguments coming from input. These numbers are relevant as theyprovide a natural upper bound \non the ability of static analysis to infer eval behavior. 4. Translating Eval calls The primary purpose \nof Evalorizer is to replace eval call sites with safer JavaScript idioms. Our approach is to collect \neval call strings at every call site and infer a classi.er suf.cient to match all encountered strings. \nThis concept was chosen asa generalizationof our previouswork[21],in which we showed that most eval strings \n.t into simple categories, and most eval call sites only receive strings in a single category. Input \n Storage  AJAX  DOM  Synthetic  Composite  Constant  Figure 2. Provenance by Pattern. Distribution \nof string provenances across eval categories in each data set.X axis is the pattern that string falls \ninto, Y axis is proportion of provenance in that category. now supported by browsers natively. The Library \npattern captures uses of eval for loading code dynamically. The Read and Assign patterns capture access \nand assignment to local variables and object properties. The Call pattern captures function calls. The \nTypeof, Try and Empty categories are pathological cases etre. We have found with little raison d that \neval call sites are quite consistent with respect to the pattern of the string argument. Across all of \nour data sets, we observed only 399 eval call sites (1.4% of all call sites) with strings in multiple \npattern categories. Many of these  Consider the typical unnecessary use of an eval call site .tting \ninto the Read pattern, shown earlier: var p= eval( window. + x); Intuitively, this could be replaced \nby the following code, using JavaScript s map operator: var p = window[x]; However, this intuition isbuilt \non an assumption about the value of x. Namely, that x is a string, and a valid JavaScript identi.er.Withouthaving \nactually seen thevaluesof x, our na\u00a8ive replacement may break the semantics of this eval call. This holds \neven for broken code; changing the way an error is reported may disrupt a deployed application. Therefore, \nwe generate a recognizer for those eval strings which are actually used at this eval call site.We generate \nthis recognizer by collecting eval strings, parsing them as Java-Script, and .nding their commonalities. \nOnce the recognizer is made, we generate code to match these strings, and per\u00adform the same behavior. \nThe generated code is constrained to accept, at a minimum, only those eval strings which have ac\u00adtually \nbeen seen; in practice it accepts more. These processes are detailed in this section. 4.1 Patterns vs. \ngeneral classi.ers In our previous work, an eval taxonomy was proposed, and recurring patterns[19, 21]were \ndiscovered. In modern Java-Script, nearly none of these patterns are good uses of eval;the Library and \nJSONP patterns may be justi.able, as theymay in certain conditions have no standard JavaScript replacement. \nThe foundationof thiswork comes from the followingkey observation from the previous work: 98.7% of examined \ncall sites have only one pattern as an argument. In other words, the overwhelming majority of eval call \nsites perform the same kind of task each time they are executed. The exact string passed in may differ,but \nit belongs to the same pattern. Detecting patterns is unfortunately not enough to replace eval call sites. \nIndeed, even if an argument to a given eval call belongs consistently to the same pattern, the actual \nvalue may change considerably from one call to another. For instance eval(x) may receive the string foo() \nthen bar() as argument. Both of them belong to the pattern Call, but actually these two calls are quite \ndifferent, so a replacement for all eval call sites in the Call pattern would need to be quite general. \nConversely, the call site may receive the strings document.appendChild(a) and document.appendChild(b) \n, in which case our more general Call replacement would be both overly general and inef.cient. By classifying \nand creating recognizers tailored to each eval call site, we can both avoid unnecessary code and provide \nmore speci.c details about how each site is used.  4.2 Classifying arguments Eval call sites are instrumented \nto record their arguments, and those arguments are collected and analyzed. This data is then used to \ntranslate the general call site into JavaScript code which accepts the same strings and performs the \nsame actions with them, but does not accept unexpected and potentially malicious input. Although static \nanalysis may be an attractive alternative to .nd these classi.cations, we argue that static analysis \nwould be forced to overlook many evals, as indeed many of the values taking part of the computation come \nfrom external websites, user input or from the DOM (Fig. 2). For this reason, we choose to use a dynamic \nanalysis. Every time a new argument is recordedby Evalorizer, the system will parse the string and build \nan abstract syntax tree (AST) for this expression. Every time a call site encounters a distinct new AST, \nit is merged with previously seen trees. This merged tree is used as a recognizer, which represents a \nhighly-restrictive subset of the JavaScript language. Many strategies may be considered for merging these \ntrees, so long as theyencode in someway thevariable partof the strings.We will discussin the following \nsubsections the pros and the cons of some of these strategies. Additionally to AST nodes, the trees may \nhave choice nodes and generalization nodes. Choice nodes are a simple disjunction between two or more \nsubtrees, generalization nodes will be described later. The language recognized by a tree with no choice \nor generalization nodes is a language of size one; with no options, only a single string is accepted. \nThelanguage recognizedbya tree with choice nodesbut no generalizations is .nite and regular. Generalizations \nmatch particular, chosen subsets of the JavaScript language, and allow a recognizer s language to be \nin.nite and, if necessary, non-regular. We describe .rst the creation of choice nodes through merging, \nthen the process of generalization. To describe the behavior of each merging strategy, we will again \nuse the following eval call site as an example: var p= eval( window. + x); We will consider the simple \ncase where x is always either width or height , and so the strings passed to eval are window.width and \nwindow.height . The ASTs for these ex\u00adpressions are shown in Fig. 3. 4.2.1 Choice In order to accept \nmore than one string, the recognizer must contain either choice nodes or generalizations. Choice nodes \nare created by merging the trees generated by multiple invocations of the same eval call site. There \nare several methods by which merging may be performed. Constants. In the simplest case that the site \nis evaluated with the same string in all circumstances, the trees to be merged are identical, so the \nmerging process yields the input tree. The AST itself acts as a recognizer. This AST will only recognize \nthe exact string that was seen during analysis2. 2As the recognition is based on the AST, whitespace \nchange in the string are accepted as well (a) Example trees. (b) Unbounded repetition. (c) Alternative \nnodes. (d) Member access gener\u00adalization.   Figure 3. Example of merging the running window.width-vs-window.height \nexample. Unbounded repetition. The easiest way to merge two differ\u00adent trees is to consider them as outright \nalternatives (Fig. 3b), joining both trees at a single choice node. To match a new string against this \nrecognizer, one simply needs to check it against each tree. This is the simplest, most straightforward \napproach. Unfortunately, because the number of alternatives are unbounded, this leads to an enormously \ninef.cient rec\u00adognizer; we essentially must check each string against every string that has ever been \nseen. This would lead to a size explo\u00adsion in our generated code, which is unacceptable as in Java-Script \nthe code size is considered essential. This technique has been applied for similar purposes in other \nsettings[8].For our simple example, this generates the tree shown in Fig. 3b, which has clear redundancies. \nAlternative nodes. Instead of considering the trees them\u00adselves as full alternatives, they can be merged \nfrom top down to some differing point. When a node does not perfectly match, an alternative (or choice) \nnode is inserted as a parent of these branches (Fig. 3c). The recognizer for this tree needs only to \ndiverge when it encounters the nested choice node. This proposal reduces most of the drawbacks of previous \noption since common parts of trees are shared. However, in theworst caseitisstill unbounded.Ourpreviouswork[21] \ndemonstrated that in most cases a single call site will evaluate stringsofa single, simple pattern[21], \nthus codeexplosion is unlikely.For our simpleexample, this generates the tree shown in Fig. 3c. This \nrecognizer accepts all of the example inputs,but no others. A recognizer, using any of the last3 techniques, \ncan be built for anylanguage, aslong asit has an AST representation.  4.2.2 Generalization Choice nodes \nallow us to capture a .nite number of strings. In practice, this has two obvious problems: as the number \nof inputs becomes large, the number of choices at any choice node will also grow, and it does not adapt \nin anyway to strings not seen in analysis. Generalization is the process of replacing subtrees of a recognizer \ns tree with broader parsers for chosen subsets of the JavaScript language. In principle, any subtree \ncould be replaced with a general parser; in practice, this doesn t work.We could, forexample, replace \nall treesbya parser for the entirety of JavaScript,but then the purpose of Evalorizer would be defeated: \nonce code had been recognized, the only way to evaluate it would be with eval itself. As such, we generalize \nonly those subsets of JavaScript for which there is a clear replacement for eval which accepts the same \nsubset of JavaScript. Although the particular generalizations and replacements are therefore JavaScript-speci.c, \nthe technique is usable in anylanguage with re.ective features. Member expressions. The dot operator \nin JavaScript can generally be replaced by the map operator, and the map operator accepts a string. eval( \nwindow. + x) -. window[x] Because this replacement strategy is available, we may re\u00adplace the right hand \nside of dot operators by generalizations which accept allJavaScript identi.ers. Oursimpleexample .ts \nthis case, so it may be replaced by the tree presented in Fig. 3d. It is important to note that this \ngeneralized tree acceptsa muchlarger(infact, in.nitelylarger) subsetofJava-Script than the previous tree \n(Fig. 3c). Although our approach is to prefer generalization wherever possible, this change in recognized \nlanguage may have security implications. Literal primitives. Numeric and string literals embedded in \nJavaScript strings may be evaluated by Number and JSON.parse3, respectively. eval( 5 ) -. Number( 5 ) \neval( S ) -. JSON.parse( S ) 3Although JSON.parse is intended to parse JSON, which is discussed later, \nstring literals areinfacta subsetof JSON.  Our approach is to replace all numeric and string literals \nby generalizations which accept all JavaScript numbers and strings, respectively. Literal objects. As \nJSON is a safe, pure subset of JavaScript, JSON strings may naturally be passed to eval4. The latest \nversion of the ECMAScript standard adds the JSON.parse function, which parses JSON in a pure, safe way, \nunlike eval. This is the only form of generalization we perform which accepts a non-regular language, \nthough in principle anysubset of JavaScript, regular or otherwise, could be generalized. eval( ({ S :5}) \n) -. JSON.parse( ({ S :5}) ) Because JSON is only a form of literals, having no side\u00adeffects or external \nreferences, this replacement is always safe. Function arguments. Functions may be called with a vari\u00adadic \nnumber of arguments in JavaScript, and infact, may be called with a runtime-generated array of arguments \nof any length with the apply method. If the arguments themselves may be generalized by any of the above \nmethods, then we may further generalize the entire argument list, yielding a recognizer which accepts \nargument lists of anylength. eval( foo(1, 2) ) -. foo.apply(window, [Number( 1 ), Number( 2 )])  4.3 \nCode generation strategy Although having a recognizer for those strings passed to a given eval call site \nis itself useful, the purpose of Evalorizer is to replace eval calls. As such, the next step is to transform \nthe generated recognizers into JavaScript code which will replace eval. In this section we discuss how \nthe replacement code itself is generated. This varies based on the shape of the recognizer tree, so it \nwill be de.ned incrementally. The general shape of the replacement code for an eval call site is straightforward: \nif (arg matches pattern) {// Guard test result = replacement code }else // Fallback case result = default \naction(arg); The way that the matching and replacement code are gen\u00aderated varies by the nature of the \nrecognizer, so will be discussed in the next sections. The default action is a matter of policy. This \naction is generally a fallback to eval(arg), which preserves the original semantics of the eval call \nsite, but when this code is produced by the patcher tool (in strict mode), it will instead throw an exception(throw \nevalorizer exception(arg)). This strict mode allows safe em\u00adbedding of third party code where the entire \nallowed subset of JavaScript is known. 4For historical reasons, JSON is usually wrapped in parentheses \nwhen passed to eval. Removing the parentheses in this case is a trivial optimization. For simplicity \nof presentation, the regular expressions shown in this section are simpli.ed to ignore issues of whites\u00adpace, \nand other non-AST-impacting alterations to JavaScript code. The true regular expressions generated by \nEvalorizer of course are not simpli.ed in this way. 4.4 Constant trees The most straightforward replacement \nis for recognizer trees which contain neither choice nodes nor generalizations. In this case, the tree \nis precisely a JavaScript AST, so Evalorizer can simply deparse it as its own replacement code. All trees \nwithout generalizations represent regular languages, so a reg\u00adular expression can be generated to match \nthe argument; for the case of constant trees, even a regular expression is often more powerful than necessary, \na simple string comparison of the argument may be suf.cient. In a call site which had only been called \nwith the string window.width , for example, the generated code is: if (arg === window.width ) { result \n= window.width; }else result = eval(arg); 4.5 Choices In the presence of choice nodes, it is necessary \nto nest guards and perform different replacements based on the choice. This is not because of the nature \nof the language matched by the recognizer (which is regular),but because the replacement code must be \ndifferent depending on which choice is taken. As such, Evalorizer generates a single match case for the \nentire tree which captures the choice subtree by a grouping within the generated regular expression, \nthen matches the choice subtree in a nested condition. To follow our simple example of eval( window. \n+x), with only width and height as values of x and no generalization performed (i.e., Fig. 3c), the generated \ncode is: var re = / window\\.(width|height)$/; if (match = (re.exec(arg))) { if (match[1] === width ) \n{ result = window.width; }else if (match[1] === height ) { result = window.height; } }else result = \neval(arg); The options may be checked in any order, but since the recognizer was generated from a selection \nof real eval strings, it is logical to order them by the frequency with which particular subtrees were \nseen.  4.6 Generalization Each form of generalization node requires its own strategy for matching and \nreplacement, so each is discussed separately. With the exception of JSON, all generalizations presented \nhere accept only regular languages. As such, regular expres\u00adsions are used to match them. The generalization \nnode within the tree is replaced by a safe eval alternative in the replace\u00adment code, though the particular \nalternative is speci.c to the generalization.  Member expressions. In JavaScript, it is possible to \naccess members of objects by using the map operator. As such, we generalize the right hand side of the \ndot operator to match JavaScript identi.ers5, and generate code using the map operator. Our simple example, \neval( window. +x), with generalization over x, is transformed into the following code: var re = / window\\.([a-zA-Z \n$][a-zA-Z\\d $]*)$/; if(match = re.exec(arg)) { result = window[match[1]]; }else result = eval(arg); Because \nthe identi.eris collected asa string and no choice nodes exist, we need only to generate one replacement, \nrather than one per all possible identi.ers. Literal primitives. The generalization of literal primitives \nis straightforward, as the grammars for both numeric and string literals are regular, and safe alternatives \nto eval for both are known. Consider an example eval( foo( + x + , + y + ) ), where x and y always contain \na string and numeric literal, respectively, and so a recognizer which generalizes over numeric and string \nliterals has been generated. The transformed code for this case is6: var re = / foo\\(( ([ \\\\ ]|\\\\.)* \n),(\\d+)\\)$/; if (match = re.exec(arg)) { result =foo(JSON.parse(match[1]), Number(match[3])); }else \nresult = eval(arg); Literal objects. JSON.parse parses only valid JSON and is hence safe and pure. Our \ncode generator can therefore avoid the complicated parsing of JSON itself, and simply replace eval(arg) \nwith JSON.parse(arg)7. The caveat is that JSON is not a regular language, so matching it becomes complicated. \nBecause JSON cannot be matched correctly by a regular expression, we match it in two steps: First we \nuse a very broad regular expression to match a large superset of JSON8, then we use JSON.parse to validate \nthat the matched substring is infact JSON. This technique has the implication that a recognizer cannot \ncontain multiple JSON 5As JavaScript allows Unicode characters in identi.ers, but JavaScript s regular \nexpression engine is insuf.cient to specify all valid identi.er characters, we only accept ASCII identi.ers. \n 6As the regular expressions for string and numeric literals in JavaScript are actually quite complicated, \nwe present a simpler one here. 7As JSON strings are usually parenthesized when passed to eval to avoid \nbeing parsed as block statements, we strip out these parentheses if present. 8Because this regular expression \nis itself quite complicated, our example shows a univeral match, .*. The ambiguity is still resolved \nby the presence of try/catch around JSON.parse. generalizations within the same choice node, as the \nresultant regular expression would be ambiguous. As an example, consider the common pattern of JSONP, \ne.g. eval( foo( + x + ) ) where x contains only valid JSON strings. Assuming that the recognizer has \nformed a generalizer node for x, the code for the transformed eval call site is: var re = / foo\\((.*)\\)$/; \nif (match = re.exec(arg)) { try { json = JSON.parse(match[1]); result =foo(json); }catch (ex) { result \n= eval(arg); } }else result = eval(arg); Function arguments. Functions may be called with a runtime-de.nedvariadic \nnumberof arguments through the apply method. This generalizer is only used by Evalorizer when the arguments \nthemselves conform to a single general\u00adizer,butin principle couldwork with any regular arguments. An \neval call site which has received the strings foo(1) and foo(2,3) would generate a recognizer which generalizes \nthe argument list to a variadic list of numeric literals, and transform the eval call site as follows: \nvar re = / foo\\(((\\d+,)*\\d+)\\)$/ if (match = re.exec(arg)) { var args1 = match[1].split( , ); var args2 \n= []; for (var i = 0;i < args1.length; i++) args2.push(Number(args1[i])); result =foo.apply(window, args2); \n}else result = eval(arg);  4.7 Preserving original program semantics One of the main concerns of this \nwork is to preserve the original behaviour of websites. The semantics of eval in Java-Script is subject \nto some oddities. When eval is called directly, the code is evaluated within the enclosing lexical scope. \nOn the other hand when it is called indirectly, i.e., through an alias, the code is evaluated in the \nglobal scope. Thus to ensure that our replacement code behaves like the original (variable access and \nhoisting), the generated code must be inlined and nofactorization is possible. Unfortunately, this comes \nat the expense of the code size. Additionally, because it is not generally possible to know whether any \ncall will indirectly dispatch to eval, and we cannot allow eval itself to be an alias, our tool is currently \nunable to handle indirect calls to eval. While eval is capable of evaluating both expressions and statements, \nit is itself an expression. As such, a dif.culty arises when an eval acceptsa statementbutis embedded \ninto a larger expression. The value returned by eval is the value of the lastexpressionevaluated.Fortunately \nthe comma operator, i.e., a sequence separated by comma, has the same behavior and thus can be used as \na substitute. Our replacement code make heavy use of this operator. However, only expressions can be \nused with the comma operator, and eval may contain other forms of statements. In order to handle this \ncase, Evalorizer would need to partially evaluate the context which embeds eval, then inject all of the \nevaluated statements, and .nally .nish the outer context. No such case has been encountered on real website, \nso we currently not support this case,but wedo notexpectdif.cultiesin doing so should the need arise. \n Finally, our replacement code makes use of temporary var\u00adiables. These variables may collide with user \ncode variables. However, since we can only generate legal JavaScript code, these name clashes are unavoidable. \nIt is arguable that some analysis may check for name con.icts,but due to the dynamic nature of JavaScript(and \nparticularly its global scope), this too could be unsafe. Evalorizer s solution is simply to pre.x all \nits own variables by an unusual pattern, making name clashes highly unlikely in practice. 5. Evalorizer \nIn this section we describe the architecture of Evalorizer, starting by explaining the proxy part (Sect. \n5.1) then the patcher in Sect. 5.2. 5.1 The Proxy As the name suggests, the Evalorizer proxy is an HTTP \nproxy. It is implemented in JavaScript and runs on node.js9, a Java-Script runtime based on Google V810, \ndesigned for writing scalable internet applications such as web servers and web proxies. A proxy allows \ndynamic instrumentation of Java-Script code that is browser-independent, as well as being server and \nserver-side language independent. Additionally it can work with all means of including JavaScript, i.e., \ninlined with <script> tags, in event handlers or in external .les. Moreoveritworks with both static and \ndynamic pagesbuilt with any kind of technologies. The proxy does not require any changes to the development, \ntesting or deployment environments since it inspects actual traf.c to and from the website, and does \nnot depend on the original eval call string or how it was originally obtained. Instrumenting a call site. \nThe proxy inspects all the Java-Script code senttothebrowser.Forevery call site,its location is logged. \nFinding the eval call sitesis donebybuilding the AST of all the JavaScript code received from the server. \nThe eval call site node is then instrumented with the logging code (logEval). Logging invocations. When \neval is invoked, the(logEval) code will send an asynchronous HTTP request (XMLHttpRe\u00ad 9http://nodejs.org/ \n10 http://code.google.com/p/v8/ quest) with the argument and the call site identi.er. The proxy receives \nthe request and logs it. 5.1.1 Adding short-circuits. Evalorizer adds to the logEval code a few short-circuits \nfor extremely common uses of eval. Since the great majority of eval invocations are used to read variables \nor deserialize JSON objects, the code for these two cases is also embedded. On a page reload, if the \nproxy has seen at least one argument for that call site, the proxy also embeds the best recognizer created \nyet for that call site. The main purpose of short circuits is to give an instant intuition to the user \nif Evalorizer will be able to resolve that call site properly or not and wether suf.cient logs have been \ncollected or not. The user can .gure out the resolved call sites by inspecting the proxy log. The short-circuits \nare not needed for the creation of the .nal patch.  5.2 ThePatcher Starting form the logs collected \nby the proxy, the patcher applies the techniques discussed in Sect. 4. Since evals are ideally all supposed \nto be replaced, the fallback may be outright removed. We provide two options for removing the fallback. \nIn a strict mode, it is replaced by throwing an exception. This behavior is recommended especially for \neval of third party code, as it reduces the possibility of code injection, XSS attacks or other attacks \nexploiting eval s characteristics. The alternative is to use eval as a fallback, it uses eval if it fails \nto recognize some invoked argument, and this can be used for call sites that were not invoked suf.ciently. \nThe transformation for each page is collected, and applied to the original code. Evalorizer also generates \na patch suitable for the Unix patch command, in uni.ed format. Finally the programmer may choose if he \nwants to apply the patch or copythe whole, updated .le. Furthermore, as this solution is intended to \nallow for sound, gradual replacement, the patch canbe generatedto usea loggingfacilityasitsfallback. \nThis requires that the user hosts a light version of Evalorizer on the site s server,but has the advantage \nof catching missed eval calls with real-world users. 6. Evaluation We validate Evalorizer with three \ndifferent experiments. The .rst one intends to show that our heuristics are legitimate and are more ef.cient \nthan a brute-force approach to the eval replacement problem. For this a large set of logs collected from \nthe top 100 most popular websites is used as well as some live websites which have signi.cant usage of \neval. The second will show that our system performs well even with incomplete sampling of eval strings, \ni.e., under the open world assumption, by using cross-validation techniques. Finally a third experiment \nwill brie.y present some ef.ciencyresults while browsing webpages with eval, during instrumentation, \nand once these pages are patched.  6.1 Corpus In most languages,gatheringa substantial corpusis dif.cult \nbecause accessibility to source code and runtime con.gura\u00adtions is limited by pragmatic and legal issues. \nOn the web, this is generally not the case, as JavaScript is only transmitted in source form. As such, \nour corpus selection is not a matter of accessibility,but utility: we wish to analyze popular pages which \nuse eval in typical ways. The largest corpus we used to validate our methodology is the interactive logs \ncollected in our previous work[21]. These logs represent real interactions with the top 100 most popular \nwebsites at the time, according to the alexa.com list as of March 3, 2011. At least two interactions \nby two different researchers were performed for each site. Each log representsa human interaction, with \neach session lasting1 to 5 minutes and approximating a typical interaction with the website, including \nlogging into accounts where necessary. Of these 100 recorded sites, all use JavaScript, and 82 use eval. \nThe logs represent 204MB of unique JavaScript code, a total of 7078 calls to eval (with an average of \n84 eval calls per trace) and a total of 8.2MB of code passed to the eval function (with anaverageof1210 \nbytes per trace).As this datawas collected from the most popular websites, its extensive use of eval \nis furthermore a validation of the necessity of our technique. Because our largest corpus is recorded \nlogs, and not live web pages, we could only use it to validate the technique, and not the implementation. \nWe selected for further study 4 websites from this list which had considerable use of eval: cnn.com, \nmyspace.com, alibaba.com, ebay. de.We cached a few pages from each website, and hosted them locally to \ntest the complete system without upstream changes. These cached portions include 12.2KB, 1.6K, 3.6K and \n6.4K of JavaScript code, respectively, and generated 738, 42, 24, and 99 calls to eval. Those numbers \nare relatively small,but typical, and theyrepresenta complete set for the cases seen from the logs. The \ncache included more eval call sites,but they were never invoked in this con.guration. In many areas of \nresearch relating to JavaScript, two pop\u00adular suites of benchmarks are used to validate results: Sun-Spider \nand V8. Although they can be useful in some cir\u00adcumstances, our focus is on eval, and their use of eval \nis uninteresting. The former uses eval in four of its included benchmarks, the latter in only one, and \nneither are represen\u00adtative of real world JavaScript behavior [18, 19].  6.2 Distribution of call sites \nThe purpose of this .rst experiment is twofold: we show that the tool works and actually removes eval, \nand demonstrate that our tree representation and our generalizing strategy are well suited and allow \nfor shorter code than a brute-force approach which would directly inline the code of each eval call string. \nConstant Single-pattern Multi-pattern Benchmark cs. invoc. cs. invoc. cs. invoc. CNN 4 37 20 1364 24 \nMySpace 6 1135 3 10 16 Alibaba 15 578 00 17 eBay.de 9 26 39 186 17 All 179 2928 411 9104 112 901 Constant \nstands for a call site where all its invocations have the exact same string, Single-pattern for recognizers \nwhich matches a single pattern, i.e., no choice nodes, and Multi-pattern for all remaining cases. Numbers \nfor actual websites are reported twice, once in their own row, once in All . Table 1. Call sites meta-pattern \ndistribution. Table1 reports how manyrecognizers of each kind have been generated for each call site \n(static) and how many times they are invoked (dynamic). In this table, constant means that the string \nwill be inlined literally and no choice nodes or generalizations exists. Hence those cases are the equivalent \nof the brute force approach and will be handled by a simple eval cache strategy, which is used in most \nJavaScript virtual machines. The single-pattern category represents recognizers which match a pattern \ncontaining some generalization,but no choice nodes; for instance, matching all numbers instead of a single \nconstant. Finally, multi-pattern is the most general, covering all remaining cases. According to these \nnumbers, 25% of the call sites recorded are constants, representing roughly 23% of the invocations. All \nothers recognizers are more general, thus would lead to code explosion if they were handled in this straightforward \nway. This suggests that programmers are still unaware of alternatives to eval. Multiple-pattern while \nless useful, is needed for completeness; without it, 7% of the dynamic calls would fall back to eval \n(and 16% of the call sites). Unsurprisingly, thanks to our approach, once the code is patched, replaying \nthe experiment does not trigger a single eval. Hence the number of mispredictions is always null and \nthus not included in this table. Moreover we classi.ed all eval call sites of the 4 websites manually \nand we were unable to .nd, without changing the surrounding code, a better classi.cation than the one \nreported by our tool. Table2 reports the effect of each generalization.For each generalization, we count \neach generalization node, and how manytime it was invoked. Other nodes report the number of non-generalized \nnodes. Some of the eval call sites were only invoked once despite our best efforts, and we report those \nseparately. We attempted the generalizations on that single argument, since it is very likely that the \ngeneralization holds for anyother still not encountered argument according to statistics of [19]. All \nthese numbers are signi.cant even when compared to non-generalization nodes, hence having special cases \n nodes invoc. nodes invoc. Category appears one Member expressions 2688 7862 149 149 Literal primitives \n12 904 33 825 5908 5908 Literal objects 271 1260 317 317 Function arguments 105 2922 113 113 Other nodes \n13 181 39 498 9289 9289 Table 2. The number of generalization nodesand invoca\u00adtions affected by each \ngeneralization. for these generalizations seems to be a good strategy. The other nodes are used for whole \nexpressions, but most of them are operators standing between generalized categories. From a purely numerical \nstandpoint, the most pro.table generalization, far above all others, are literal primitives. This is \nunsurprising since most eval calls only differ from each other by some constant coming from user input \nor produced by some code generator. More surprisingly, a function arguments generalized node is used \non average 14 times, where member expressions are are used onaverage3 times. However, in absolute terms, \nmember expressions are still triggered much more often than function arguments. This too is unsurprising \nsince in all languages, and JavaScript is not an exception, reads from members are a very common action. \nEven if literal objects serialization is triggered less than all other generalizations, it is still a \ngood idea to have a special case for several reasons. First, it reduces considerably the size of the \nrecognizer and the code generated; literal object strings are generally long and introducea lotofvariability.A \nsecond reason, though a less clear one, is from an ef.ciency standpoint. Since most browsers are faster \nto parse JSON strings than evaluating them, though Safari is a notable exception, replacing eval at the \nparsing sites improves the performance of websites that utlize JSON oftenly. From our perspective, this \nexperiment is a success, since all our generalizations seem adequate as they both reduce the code size \nand increase the power of generalization of Evalorizer.  6.3 Classi.cation stability At heart, Evalorizer \nis a learning algorithm. However, unlike classical learning algorithms,itbuilds anover.tted classi.er \nto assuredly match all input strings. On our dataset, with only the eval strings we ve seen, eval will \nabsolutely never be called, as Evalorizer learns by rote. In order to evaluate Evalorizer s generalization \npower, it must be evaluated on inputs that were not used in the learning phase.To simulate this, we use \nthe k-fold cross-validation technique.For each eval call site, invocation logs are split into k sets \nof equal size. k - 1 of these sets are used as input to train Evalorizer. Once trained, the code is patched \nin strict mode, such that we can determine and catch any failing eval cases. Finally Mispredict % Call \nsites % Method affected Leave-one-out 215 1.7% 99 18.2% Holdout 374 2.89% 128 14% Table 3. Misprediction \nusing two cross-validation meth\u00adods. the unused set is used to validate the patched code. This experiment \nis then repeated until each set has been used as a test set. Two special cases of this technique may \nalso be considered, the Holdout method, where k is equal to two, and the Leave-one-out method, where \nk is equal to the size of the dataset.We applied both holdout and leave-one-out. The former is not to \nour advantage since the training set is very small, and the splitting point has a huge impact. The latter \nmaybe computationally expensivebutin practice takes no more time than computing the residual error and \nit is a much better way to evaluate models. Since at least two samples are needed for this experiment, \ni.e., one as training and one to test, all call sites with only one string available have been removed, \nand only 702 call sites remain, with 12 933 samples. This represents roughly 37% of the whole dataset. \nResults are shown inTable 3. The worst case is with the holdout method. There, 2.89% of input strings \nare mispredicted. The number is encourag\u00ading; clearly most eval replacements are well represented by \nour system even when the training set is unreasonably small. These mispredictions affected 18.2% of the \neval call sites. On the leave-one-out experiment, the results are, unsurpris\u00adingly, better; only 1.7% \nmisprediction, distributed on 14% of call sites. The distribution of percentage of errors by call site \n(Fig. 4a) reveals two extreme cases. Either callsites are perfectly predicted, or Evalorizer fails totally. \nThe straightfor\u00adward conclusion is that these call sites are highly polymorphic, and not enough samples \nwereavailable for them.To under\u00adstand why so many call sites are affected by misprediction, it is necessary \nto look at the number of samples available on call sites having at least one error. Therefore, Fig. 4b \nand Fig. 4c shows, respectively for the leave-one-out and the hold\u00adout methods, a distribution of the \nnumber of samples for a call site which has at least one error. Unsurprisingly, call sites with few samples \ntend to have more errors in both methods, since if the number of samples is two, these methods are equivalent. \nWe argue that a better acquisition of these call sites, i.e., spending more time collecting eval strings, \nwill solve this problem. TheFig.4chasa strangepeakat42.Amanual inspection of this particular call site, \nused on many eBay subdomains, shows that it was used for browser detection. The mispre\u00addicted arguments \nis a call to a helper function which sets up non-standard variables used by some browser. These calls \nonly appear once and very early in a browsing session. How\u00adever, while the holdout method split the data \nin two sets, these  0.00.20.40.60.81.0 0 1020304050 0 1020304050 (a)%of error (leave-one-out) (b) Number \nof samples per call sites (leave-one-out) (c) Number of samples per call sites (holdout) Fig. 4a shows \nthe distribution of misprediction (the x-axis is%of misprediction) by call site for the leave-one-out \nmethod. Fig. 4b and 4c show, respectively for the leave-one-out and holdout methods, the number of inputs \n(on x-axis) for call sites containing at least one misprediction. Figure 4. Distribution of errors. samples \nare never seen in both of them. These errors are hence just statistical artifacts, and in a normal Evalorizer \nsession they would be recorded, and would never be mispredicted. This experiment bolsters our claim that \nour tool s general\u00adization is robust enough to work on true websites. Therefore we consider that our \ntool is suf.cient to remove eval on real codeandmaybe,eveninits current prototypeversion,used in a production \nenvironment.  6.4 Runtime ef.ciency Although the aim of Evalorizer is to advise web developers in the \nreplacement of eval, it is arguable that, to be useful, the runtime overhead of our tool needs to be \nlow. Therefore, we measured the speed of pages while using our tool. However, since most websites do \nnot use eval frequently, and as such would not have their performance affected at all, we only concentrate \non the web page which our previous study showed the most usage of multiple kinds of eval Table 1, namely \nCNN. We claim that if this web page, as the worst case, is not affected, other web page will not be either. \nTo measure runtime of this site, we used JSBench[20]to generate a deterministic benchmark from C NN website, \nand use it to measured run times. This experiment was run on a 2.4 GHz Intel Core i5 Mac with 4GB 1067 \nMHz DDR3 RAM running Mac OS X Lion. Three browser were considered: Google Chrome 18.0.973.0, Safari 5.1.12 \nand Opera 11.60. Table 4 shows that while eval code is instrumented, the performance is degraded, but \nby a small amount. Most of this overhead is actually due to the systematic read and JSON patterns short-circuits. \nA simple solution to most of this overhead could simply consist of disabling these short-circuits in \nthe Evalorizer proxy. Since eval hampers most static analysis made by JavaScript virtual machines, it \nmakes sense to see if once the code is patched some runtime ef.ciency improvement may be expected on \nthe website. Browser Original avg. s Instrumented avg. s ovh. Patched avg. s ovh. Chrome Safari Opera \n139 166 265 50 8 17 224 231 345 72 19 39 61% 39% 30% 115 153 285 27 10 13 -18% -8% 7% Average time(avg.) \nand standarddeviation(s)are in milliseconds. Overhead (ovh.) is expressed as a percentage. Table 4. Runtime \nef.ciency and overheadof Evalorizer on CNN website. This is actually con.rmed on most browsers, opera \nbeing an exception, however the difference is close to the con.dence interval. As this is a prototype, \nwe did not focus on optimizing our code generator. Even as such, once patched, only 369 bytes of eval \nreplacements (less than 0.015% of the total JS code) is added to CNN. We expect this overhead to be acceptable \nfor all practical purposes. 7. RelatedWork The implementation of Evalorizer is based on the JSBench framework[20]. \nThisworkwas motivatedby the resultsof and evaluated by the framework of our previous analysis of how \neval is used in JavaScript[21][19]. The combination of simple patterns and consistency are what motivated \nthe present paper. Work to guide static typing by dynamic instrumentation of eval hasbeendoneinthe contextofRubybyFurretal.[8], \nbut this work makes no attempt to generalize beyond seen eval strings, as in that context the range of \nstrings seen at any call site is very small. Furthermore that work does not to our knowledge make any \nattempt to merge seen strings, resulting in verbose replacement code. Jensen et al. [14] use a technique \nsimilar to ours in static analysis of Java\u00adScript, but their technique does not include a mechanism similar \nto generalizations, and so with the exception of certain specializations, they are only able to accept \nparticular strings which their static analysis framework indicates are possible. Since their technique \nuses static analysis, this list is sound and complete when attainable, but cannot apply to some particularly \ntreacherous cases of eval.We know of no reason why our techniques cannot complement each other to create \na more broadly applicable and robusthybrid analysis approach.  Some papers apply strength reduction \nof eval in order to better perform analyses. The analysis done for AJAX intru\u00adsion detection of Guha \net al.[10]converts unanalyzable eval calls into analyzable JSON parsing by intercepting client requests. \nBut their solution only looks at JSON. Our analy\u00adsis complements this by converting more general classes \nof eval calls into statically analyzable equivalents. They use a similar technique to intercept responses,but \ndo not provide replacements, instead their proxy has to be enabled all the time to prevent the intrusion. \nA different approach to pre\u00advent possible malicious behavior of eval is the sandboxing of unsafe code \nincluding eval from untrusted sources. The solution by Dewald el al.[5]proposes a framework for sand\u00adboxing \nunsafe code,but unlike Evalorizer it requires changes to the browser. The code is not changed by this \napproach, and therefore does not enhance results of static analysis on such code. The runtime will also \nrun slower due to the sandboxing overhead. Manyother works have simply ignored or outlawed eval altogether. \nGatekeeper s[9]enforcement of security policies depends on the outright exclusion of eval, as does Maffeis \net al. s isolation technique[16]. The latter s framework provides the ability to de.ne wrappers for the \neval that parses and rewrites the argument to add the necessary checks for safety to create a safer argument \nfor eval,but making that general would require running a full JavaScript parser written in JavaScript \non the client machine each time eval is invoked, therefore addinga largeoverhead. Unlike the proposalin[16], \nthe .nal patched version from Evalorizer has no overhead, if a slight speed-up, since the analysis itself \nwas done earlier on the proxy during the acquisition phase. Moreover the both of those propositions have \na practical limitation: untrusted code must not use eval. This limitation is severely limiting by design, \nsince untrusted code has to be considered always unsafe and may do whatever it wants. There are methodologies \nsuch as Chugh et al.[4]which take into consideration eval while doing analysis. This staged information \n.ow technique starts by doing a classic static analysis without considering eval, then when eval is actually \ninvoked, its value is propagated and a partial or whole analysis is done again. This hybrid methodology \nhas to punt static analysis to a dynamic one, where the replacement of eval in our approach yields normal \nJavaScript code with few or no eval calls for future static analysis, considerably different goals. Another \nmore subtle concern with eval in research is that although several formal semantics for JavaScript ex\u00adist[15][11], \nnone to our knowledge include support for eval. Although not a signi.cant issue with the semantics them\u00adselves, \nthis has the side effect that anyfurther research using these formal semantics as a base is unlikely \nto have expected results when eval is present. Since our solution is to remove eval from code, the results \nproduced by Evalorizer are com\u00adpatible with these formal semantics. 8. Conclusions Despite the fact that \neval is dangerous and mostly useless, manypeople still use it. This is likely because of its ease of \nuse, legacycode which is dif.cult to change, or because they are not sure what will be evaluated, for \ninstance because it came from third party code. Therefore we proposed in this paper Evalorizer, a tool \nand technique which dynamically inspects arguments of eval and suggests replacements. This tool does \nnot require anyknowledge of the actual code, nor changes to the browser or to the server. This tool is \nbased ona simplekey idea, thata call site generally has only one purpose, which is con.rmed by the observation \nthat most eval call sites receive arguments which are the same or very similar. Evalorizer customizes \neach eval call site according to actual values passed to eval, while trying to generalize it as much \nas possible with JavaScript s features. Evalorizer has been evaluated both on extensive logs from the \n100 most used websites and on four sites which have signi.cant use of eval. This evaluation showed that \nmore than 97% of eval invocations have been replaced by our tool under open world assumption, and this \neven with a small training set. Moreover the slowdown using this tool is reasonable, and once eval has \nbeen de.nitely replaced, slight performance gain may be expected. According to these experiments, we \nconclude that this simple idea works surprisingly very well on true websites and actually and practically \nremoves calls to eval.We see another application to this work to JavaScript virtual machines. Manyvirtual \nmachines cache eval strings for performance reasons. Replacing this simple cache by our technique may \nimprove eval ef.ciency. Acknowledgments. We thank Ben Livshits, Anders M\u00f8ller and Peter Thiemann for \nfruitful discussions, and the anony\u00admous reviewers for their comments. This work was partially supported \nby a SEIF grant from Microsoft Research, a Fel\u00adlowship from the Mozilla Corporation and the NSF grant \nOCI-1047962.  References [1] Christopher Anderson and Sophia Drossopoulou. BabyJ: From object based \nto class based programming via types. Electr. Notes in Theor. Comput. Sci., 82(7):53 81, 2003. doi: 10. \n1016/S1571-0661(04)80802-8. [2] Christopher Anderson andPaola Giannini. Type checking for JavaScript. \nElectr. Notes Theor. Comput. Sci., 138(2):37 58, 2005. doi: 10.1016/j.entcs.2005.09.010. [3] Michael \nBolin. Closure: The De.nitive Guide. O Reilly Series. O Reilly Media, 2010. ISBN 9781449381875. URL http: \n//books.google.ch/books?id=p7uyWPcVGZsC. [4] Ravi Chugh, JeffreyA. Meister, Ranjit Jhala, andSorin Lerner. \nStaged information .ow for JavaScript. In Conference on Programming language design and implementation \n(PLDI), pages 50 62, 2009. doi: 10.1145/1542476.1542483. [5] Andreas Dewald, Thorsten Holz, and Felix \nC. Freiling. AD-Sandbox: sandboxing JavaScript to .ght malicious websites. In Proceedings of the Symposium \non Applied Computing (SAC), 2010. doi: 10.1145/1774088.1774482. [6] Manuel Egele, Peter Wurzinger, Christopher \nKruegel, and Engin Kirda. Defending browsers against drive-by downloads: Mitigating heap-spraying code \ninjection attacks. In Detection of Intrusions and Malware, andVulnerability Assessment, 2009. doi: 10.1007/978-3-642-02918-9_6. \n[7] European Association for Standardizing Information and Communication Systems (ECMA). ECMA-262: EC-MAScript \nLanguage Speci.cation. Fifth edition, Decem\u00adber 2009. URL http://www.ecma-international. org/publications/standards/Ecma-262.htm. \n[8] Michael Furr, Jong-hoon (David) An, and Jeffrey S. Foster. Pro.le-guided static typing for dynamic \nscripting languages. In Conference on Object-Oriented Programming Systems, Languages and Applications \n(OOPSLA), 2009. doi: 10. 1145/1640089.1640110. [9] S. Guarnieri and B. Livshits. Gatekeeper: Mostly static \nenforcement of security and reliability policies for Java-Script code. In USENIX Security Symposium, \n2009. URL https://www.usenix.org/events/sec09/tech/ full_papers/sec09_javascript.pdf. [10] Arjun Guha, \nShriram Krishnamurthi, andTrevor Jim. Using static analysis for Ajax intrusion detection. In Conference \non World wide web (WWW), 2009. doi: 10.1145/1526709. 1526785. [11] Arjun Guha, Claudiu Saftoiu, and Shriram \nKrishnamurthi. The essence of JavaScript. In European Conference on Object-Oriented Programming (ECOOP), \n2010. doi: 10.1007/ 978-3-642-14107-2_7. [12] Dongseok Jang and Kwang-Moo Choe. Points-to analysis for \nJavaScript. In Proceedings of the Symposium on Applied Com\u00adputing (SAC), 2009. doi: 10.1145/1529282.1529711. \n[13] Simon Jensen, Anders M\u00f8ller, and Peter Thiemann. Type analysis for JavaScript. In Symposium on Static \nAnalysis (SAS), 2009. doi: 10.1007/978-3-642-03237-0_17. [14] Simon Holm Jensen, Peter A. Jonsson, and \nAnders M\u00f8ller. Remedying the eval that men do. In Proceedings of the International Symposium on Software \nTesting and Analysis (ISSTA), 2012. doi: 10.1145/2338965.2336758. [15] Sergio Maffeis, John C. Mitchell, \nand Ankur Taly. An oper\u00adational semantics for JavaScript. In Symposium on Program\u00adming Languages and \nSystems (APLAS), 2008. doi: 10.1007/ 978-3-540-89330-1_22. [16] Sergio Maffeis, John Mitchell, and Ankur \nTaly. Isolating JavaScript with .lters, rewriting, and wrappers. In Com\u00adputer Security ESORICS 2009, \n2009. doi: 10.1007/ 978-3-642-04444-1_31. [17] Flor\u00b4eal Morandat, Brandon Hill, Leo Osvald, and Jan Vitek. \nEvaluating the design of the R language. In European Con\u00adference on Object-Oriented Programming (ECOOP), \n2012. doi: 10.1007/978-3-642-31057-7_6. [18] Paruj Ratanaworabhan, Benjamin Livshits, and Benjamin Zorn. \nJSMeter: Comparing the behavior of JavaScript benchmarks with real web applications. In Conference on \nWeb Application Development (WebApps), 2010. URL http://www.usenix.org/events/webapps10/ tech/full_papers/Ratanaworabhan.pdf. \n[19] Gregor Richards, Sylvain Lesbrene, Brian Burg, and JanVitek. An analysis of the dynamic behavior \nof JavaScript programs. In Proceedings of theACM Programming Language Design and Implementation Conference \n(PLDI), 2010. doi: 10.1145/ 1806596.1806598. [20] Gregor Richards, Andreas Gal, Brendan Eich, and JanVitek. \nAutomated construction of JavaScript benchmarks. In Con\u00adference on Object-Oriented Programming Systems, \nLan\u00adguages and Applications (OOPSLA), 2011. doi: 10.1145/ 2048066.2048119. [21] Gregor Richards, Christian \nHammer, Brian Burg, and JanVitek. Theeval that men do:Alarge-scale studyof the useofevalin JavaScript \napplications. In European Conference on Object-Oriented Programming (ECOOP), 2011. doi: 10.1007/ 978-3-642-22655-7_4. \n[22] Konrad Rieck,Tammo Krueger, and AndreasDewald. Cujo: Ef.cient detection and prevention of drive-by-download \nat\u00adtacks. In Annual Computer Security Applications Conference (ACSAC), 2010. doi: 10.1145/1920261.1920267. \n[23] Peter Thiemann. Towards a type system for analyzing Java-Script programs. In European Symposium \non Programming (ESOP), 2005. doi: 10.1007/978-3-540-31987-0_ 28.    \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Eval endows JavaScript developers with great power. It allows developers and end-users, by turning text into executable code, to seamlessly extend and customize the behavior of deployed applications as they are running. With great power comes great responsibility, though not in our experience. In previous work we demonstrated through a large corpus study that programmers wield that power in rather irresponsible and arbitrary ways. We showed that most calls to eval fall into a small number of very predictable patterns. We argued that those patterns could easily be recognized by an automated algorithm and that they could almost always be replaced with safer JavaScript idioms. In this paper we set out to validate our claim by designing and implementing a tool, which we call Evalorizer, that can assist programmers in getting rid of their unneeded evals. We use the tool to remove eval from a real-world website and validated our approach over logs taken from the top 100 websites with a success rate over 97% under an open world assumption.</p>", "authors": [{"name": "Fadi Meawad", "author_profile_id": "81442592443", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856145", "email_address": "fmeawad@cs.purdue.edu", "orcid_id": ""}, {"name": "Gregor Richards", "author_profile_id": "81438595000", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856146", "email_address": "gkrichar@purdue.edu", "orcid_id": ""}, {"name": "Flor&#233;al Morandat", "author_profile_id": "81444607842", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856147", "email_address": "florealm@gmail.com", "orcid_id": ""}, {"name": "Jan Vitek", "author_profile_id": "81100018102", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P3856148", "email_address": "jv@cs.purdue.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384660", "year": "2012", "article_id": "2384660", "conference": "OOPSLA", "title": "Eval begone!: semi-automated removal of eval from javascript programs", "url": "http://dl.acm.org/citation.cfm?id=2384660"}