{"article_publication_date": "10-19-2012", "fulltext": "\n Software Data-Triggered Threads Hung-Wei Tseng and Dean M. Tullsen Department of Computer Science and \nEngineering University of California, San Diego La Jolla, CA, U.S.A. Abstract The data-triggered threads \n(DTT) programming and execu\u00adtion model can increase parallelism and eliminate redundant computation. \nHowever, the initial proposal requires signif\u00adicant architecture support, which impedes existing applica\u00adtions \nand architectures from taking advantage of this model. This work proposes a pure software solution that \nsupports the DTT model without any hardware support. This research uses a prototype compiler and runtime \nlibraries running on top of existing machines. Several enhancements to the ini\u00adtial software implementation \nare presented, which further improve the performance. The software runtime system improves the performance \nof serial C SPEC benchmarks by 15% on a Nehalem proces\u00adsor, but by over 7X over the full suite of single-thread \nappli\u00adcations. It is shown that the DTT model can work in conjunc\u00adtion with traditional parallelism. \nThe DTT model provides up to 64X speedup over parallel applications exploiting tra\u00additional parallelism. \nCategories and Subject Descriptors D.3.4 [Programming Languages]: Run-time environments Keywords Data-triggered \nthreads, parallel programming models 1. Introduction The data-triggered threads (DTT) programming and \nexecu\u00adtion model [16, 17] has been shown to enable applications to exploit parallelism and eliminate \nredundant computation. Unlike traditional execution models, which generate par\u00adallelism based on control \n.ow, the DTT model initiates a thread when the program changes particular memory con\u00adtents. This has \ntwo primary advantages. First, computation depending on the changed data can execute immediately, of\u00adten \nexposing parallelism earlier. Second, untouched or un\u00adchanged data do not generate unnecessary computation. \nThe latter effect, eliminating unnecessary, redundant computa- Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, \nUSA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 tion, is shown to be particularly \npowerful. The DTT model is implemented as extensions to an imperative program\u00adming language, allowing \nthe programmer to declare particu\u00adlar functions as support thread functions, and attach them to variables \nor structure .elds in the code. However, the DTT model, as proposed, requires changes to the instruction \nset architecture and the addition of hardware tables to the pro\u00adcessor, making it unavailable on existing \nprocessors. This paper introduces a software implementation of data\u00adtriggered threads, enabling programmers \nto utilize the DTT model on existing architectures without any hardware sup\u00adport. Software DTT experiences \noverheads (primarily the high cost of spawning threads in current architectures) com\u00adpared to the simulated \nhardware approach, yet still provides opportunity for signi.cant gains. This paper proposes tech\u00adniques \nfor mitigating the software overheads of DTT. The thresholding mechanism, for example, addresses the \nsce\u00adnario where excessive threads are being generated and cause large slowdowns, a case that the prior \nproposal was vulnera\u00adble to. Thus, we signi.cantly increase the generality of data\u00adtriggered threads \nin two ways: (1) we make it available to\u00adday on real hardware, and (2) we free the programmer to use \nDTT for any computation that lends itself to the DTT model, without having to think as carefully about \nthe performance implications. To demonstrate the feasibility of supporting the DTT model without architectural \nsupport, we build a prototype compiler and user-space runtime libraries. The compiler ac\u00adcepts programs \nwritten in C and C++ with DTT extensions and produces code using runtime libraries. The runtime li\u00adbraries \nschedule and execute support thread functions when they detect memory content change to a marked variable. \nA key challenge in designing the runtime system for the DTT model is the high multithreading overhead. \nWe introduce a fast thread spawning mechanism to avoid the thread spawn\u00ading costs. To avoid performance \ndegradations due to multi\u00adthreading hardware, software, or communication overheads, or poorly written \ndata-triggered threads programs, we also design a simple threshold mechanism that automatically and transparently \ndisables the usage of the DTT model dynami\u00adcally. Software data-triggered threads can be used to either \nadd a new type of parallelism to serial code, or to augment tra\u00additionally parallel code. This paper \ndemonstrates both; thus, it is the .rst work to demonstrate the DTT model on ex\u00ad -(,. 3+1*() 2400/13 \n3+1*()  8957- 7$! #7% 25*, 7&#38;! #7%  )5,- 7-6-*98 90- 8*3- 25*, *4, +536:9*9154 1. ' ,-6-4,14/ \n3-357; % 23/1* 1#! \"1$ +549-498 *7- 89122 90- 8*3-\" ' ( &#38; ) (a) The original application (b) \nThe DTT execution model Figure 1. The data-triggered thread execution model isting parallel code. We \nrun serial SPEC benchmarks and both serial and parallel versions of the PARSEC applica\u00adtions on several \nexisting machines. Despite the overheads of our software approach, we achieve 1.15X average perfor\u00admance \nimprovement for the SPEC benchmarks, but an av\u00aderage gain of 7.3X over single-thread execution overall \n(in\u00adcluding the PARSEC results). For just the parallel applica\u00adtions, our speedups over traditional parallel \nexecution with an equivalent number of cores vary, but achieve up to 64X (2-thread DTT compared to 2-thread \ntraditional). This paper makes the following contributions: (1) It presents a pure software framework \nthat allows applica\u00adtions to take advantage of the DTT model. (2) It proposes improvements to the data-triggered \nthreads implementation that mitigates thread spawning overheads. (3) It proposes further improvements \nto the framework to avoid slowdowns from ill-behaved data-triggered threads, enabling effective use of \nthe DTT model on a wider variety of applications. (4) It demonstrates that DTT parallelism can be complementary \nto traditional parallelism, and the combination can dramati\u00adcally outperform traditional parallelism \nalone. This paper is organized as follows. Section 2 provides an overview of the DTT model and discusses \nthe related work. Section 3 details the design of our runtime system that sup\u00adports the DTT model. Section \n4 describes our experimental methodology. Section 5 presents and discusses the experi\u00admental results. \nSection 6 concludes.  2. Background and Related Work This section provides some background on the original \ndata\u00adtriggered threads programming model that is implemented in software in this work, some details on \nthe originally pro\u00adposed hardware implementation, and other work related to this project. 2.1 Data-triggered \nthreads programming andexecution model Data-triggered threads [16, 17] presents a new model of par\u00adallelism \nthat has the potential to expose parallelism more explicitly while avoiding redundant and unnecessary \ncom\u00adputation. Where conventional programming models gener\u00adate parallelism based on control .ow (the program \ncounter reaches a fork instruction, for example), in the DTT model a thread is generated when a particular \nmemory location is changed. The DTT model has two advantages. First, it ex\u00adposes parallelism immediately, \nas soon as the program mod\u00adi.es the source data. Second, it eliminates redundant com\u00adputation if the \ndata is not changed, we do not do the com\u00adputation. The original work demonstrated that about 80% of \nall loads are redundant, meaning that they load the same value that the same load had fetched the last \ntime it accessed this same address. The redundant load often feeds redundant computation that depends \non that load, typically leading to a store that .nally writes a value into memory that has not changed. \nWith DTTs, the programmer can express the re\u00addundant computation in a support thread, which is only trig\u00adgered \nwhen the data has actually been changed. This results in as much as a 6X speedup in one case where the \nprimary gain comes from the ability of DTT to avoid redundant com\u00adputation. Figure 1 details the operation \nof the DTT model. The application in Figure 1(a) consists of code regions A, B, and C where the execution \nof B only depends on the memory contents written in A. In the DTT model (Figure 1(b)) the computation \nof B in the original application (or possibly an incremental form of section B) becomes available for \nparallel execution, in support thread S, immediately once the store producing the data executes. Additionally, \nDTT only performs the dependent computation when the store in A modi.es the memory contents upon which \nB (from the original code) depends. The DTT model will skip the execution of B in the original code and \nimmediately execute C after the support thread S .nishes execution. If the main thread .nishes executing \nA before S completes, the DTT model will stall the main thread until completion of S, before going on \nto C. Therefore, we consider the original location of code region B as an implicit barrier in the DTT \nmodel. When the store instruction in A generates the same mem\u00adory content, or if the program does not \nexecute the store in\u00adstruction at all, the inputs of B remain the same as the last execution, which means \nthe prior execution of S is still valid. The DTT model will skip the computation and memory ac\u00adcesses \nof B of the original application without spawning any support thread. Because the DTT model skips the \nexecution of code section B if the result of the latest support thread computation is still valid, the \nDTT model calls the code sec\u00adtion B from the original application the skippable region . The DTT model \nstill leaves the code of the skippable region in the main thread because the support thread may fail \nto execute. 2.1.1 Data-triggered threads programming model The DTT model is accessed via extensions to \nthe C program\u00adming language. These extensions include pragmas to declare data triggers, pragmas to identify \nsupport thread function, and pragmas to mark the skippable region. (,1 -13.&#38;\" #! 970//.7 ,8#44.7$556\"))%9:6. \n! '.13.&#38; 0.1./(%&#38;\" #! 970//.7 ,8#44.7$556\"))(69&#38;70-. ! '.13.&#38; 01/()&#38;\" #! 970//.7 \n,8#44.7$556\"))(9701. ! '.13.&#38; /$1&#38;\" #! 970//.7 ,8#44.7$556\"))'+9. ! '.13.&#38; 2-*$1(*(13\" #!970//.7 \n,8#44.7$556\"))*52+90209: ! '.13.&#38; -1(+&#38;\" #! 970//.7 ,8#44.7$556\"))%903. ! (a) Array declarations \n## %-#(,'$&#38;!))',\"**+ <@F 6E-@@9D.AAB*221BF0D<79 :BFJB9 \"BFD! K <@F < ( BFD $ EBFBD<793&#38;4' %% \n,9F F;9 <@89I H5>G9 :BFJB9 BD<79' BD<79 ( )>=17;>E+C+GDA/A*<H EBFBD<793<4# EFD<=93<4# D5F93<4# HA>5F<><FJ3<4# \nAF<?93<4# AFJB93<4# &#38;!' BD<79E3<4 ( BD<79' D9FGD@ &#38;' (b) The support thread function =BH 7G5H<F:69JC=9 \n!H=95DHF L =BH =# >&#38; ;DHKD: DF=8:&#38; ;DHKD: DF=8:*:@H6&#38; =BH GH6FH ( H=9 ! BIA.DH=CBG $ B1<F:69G \n&#38; =BH :B9 ( GH6FH \" BIA.DH=CBG $ B1<F:69G &#38; ;CF >(%&#38; >'-2,5/2-0&#38; >\"\" L %+-&#38;* %0\")/($' \n,,(/!--. ;CF =(%&#38; ='BIA.DH=CBG&#38; =\"\" L DF=8: ( )@?08<@G+E+IFC-C*=J GDHDF=8:3=4# GHF=?:3=4# F6H:3=4#JC@6H=@=HK3=4#CH=A:3=4# \nCHKD:3=4# % &#38; DF=8:G3=4 ( DF=8:&#38; M (,'#%+-&#38;* M F:HIFB %&#38; M (c) The original code with \nDTT pragmas Figure 2. DTT extensions to blackscholes. This is a simpli\u00ad.ed version for clarity. We highlight \nDTT pragmas in bold. We show only one of the six nearly-identical support threads that all share the \nsame skippable region. Using these pragmas, the programmer attaches a support thread, de.ned as a function, \nto a variable or a .eld in a structure de.nition. The variable or .eld becomes a data trigger, and any \nchange to that variable, or that .eld in any variable of that type, results in a thread being spawned \nto execute the support thread function. There is no restriction on which variables or data structures \ncan be declared as data triggers. However, since the DTT model bene.ts primarily from parallelism and \neliminating redundant computation, it is most effective when the programmer targets variables or data \nstructure .elds that help to exploit these two bene.ts. The programmer also needs to compose the support \nthread function which describes the computation that de\u00adpends on the data trigger. To simplify the design \nof the DTT execution model, the support thread function takes the ad\u00address that triggers the support \nthread as the only argument. The support thread can communicate with the main thread using shared memory. \nThere are some restrictions when composing support thread functions. First, because threads may be executed \nan unknown number of times before the result is used, the support thread function should be idempotent, \nwhich means the result is not affected by how many times the code is executed. For example, a support \nthread that accumulates a count of some activity is not a good candidate. Second, the current DTT model \ndoes not allow a support thread function to trigger another support thread, although that will likely \nbe added in future work. After all support thread functions .nish execution, the DTT execution model \ncan skip computation rendered unnec\u00adessary by the support threads (the skippable region). When modifying \nexisting code, it is easy to identify this skip\u00adpable region it is the original computation that is \nbeing replaced by the support thread function. When writing new code, the programmer needs to write a \nconventional routine as backup . The skippable region is identi.ed by pragmas which attach it to a particular \nsupport thread function. In an example borrowed from the prior work [16], mcf spends signi.cant time \nin the refresh potential func\u00adtion. That function calculates the potential .elds which depend on data \nin nodes throughout the structure. However, the .elds that the computation depends on tend to be highly \nstatic, rendering most of the calculation redundant. There\u00adfore, we attach support thread functions to \nthe appropriate .elds of the node t data structure. When values in those .elds are changed, potential \nis recalculated. When they do not change, the existing value in memory is still correct and no recalculation \nis necessary. Figure 2 provides an example from blackscholes, with the code somewhat simpli.ed for clarity. \nThe inner loop of the bs thread function (Figure 2(c)) depends on sptprice, strike, rate, volatility, \notime, and otype ar\u00adrays to update the prices array. In other words, a value in the prices array changes \nonly if a value with the same index in arrays of sptprice, strike, rate, volatility, otime, and otype \nchanges. To enable the DTT model on this application, we attach support thread functions to the declaration \nof the arrays as in Fig\u00adure 2(a) and compose support threads for each data trig\u00adger. We present one of \nthe support thread functions that we create in Figure 2(b). The others differ only in func\u00adtion name \nand the calculation of the index from the trig\u00adger address. We declare the inner loop in Figure 2(c) \nas the skippable region using the pragmas. Therefore, if the program changes an element in the sptprice \narray, for example, the DTT model executes the support thread func\u00adtion bsInnerLoopDTTSptPrice to compute \nand up\u00addate only the corresponding element in the prices array, but still leaves all the other elements \nof the destination array untouched. The program can then skip the execution of the inner loop when it \ngets to the block pragma. 2.1.2 Architectural support for data-triggered threadsexecution model The \nprior work supports the DTT model using modi.cations in both hardware and the instruction set architecture. \nOn the hardware side, the processor requires additional hardware tables the thread status table (TST) \nand the thread queue (TQ). The TST contains information associated with each skippable region to indicate \nwhether or not the processor can bypass the execution of the skippable region. The TQ stores information \nto manage active support threads. To support the DTT model, the prior work proposed sev\u00aderal new instructions: \ntstore, tspawn, tcancel, and treturn. The tstore instruction stores a value into a data trigger and checks \nif the stored value is the same as the current data in the target memory. If a tstore instruction detects \na change to memory contents, the following tspawn instruction will transfer the support thread information \nto the TQ. The sup\u00adport thread will execute on a spare hardware context until the support thread terminates \nat a treturn instruction that updates the TST, or a tcancel instruction that simply invalidates the corresponding \nTST entry when the program takes a path that causes the result to not be able to be reused. Note that \nthe im\u00adplementation of the tstore instruction also assumes changes to the load/store pipe and cache to \ndetermine whether the value has been changed in memory. The DTT model only initiates a support thread \nafter a tstore instruction commits. As a result, support threads are always non-speculative threads. \nTherefore, the DTT model does not require any architectural support for thread specu\u00adlation, like speculative \nmultithreading [10] or program de\u00admultiplexing [2].  2.2 Other related work Pure data.ow programming \nmodels [5, 13] that execute pro\u00adgrams based on the generation of data provide native support for .ne-grain \nparallelism. To extract the full power of this execution model, these languages require hardware support \nfor data.ow [1, 7, 12, 14], which can be quite complex. Cilk [8] and CEAL [9] incorporate the concept \nof data.ow programming models into imperative programming lan\u00adguages to take advantage of data.ow constructs \nwithout completely rewriting programs. They each propose exten\u00adsions to the C/C++ programming language \nto trigger com\u00adputation when the program generates new data. DTT shares the idea of triggering parallel \nthreads upon the generation of data with Cilk, but Cilk does not exploit the potential of removing redundant \ncomputation with this model like DTT. CEAL targets incremental recomputation and can also avoid redundant \ncomputation; however CEAL does not incorpo\u00adrate parallelism or threading into its solution. Software \nmemoization [6, 11] is an optimization tech\u00adnique that stores the input and output values of frequent \noperations or functions to reduce redundant computation. Software memoization requires the program to \nallocate ad\u00additional memory to hold input and output values. Software memoization typically requires \nsigni.cant change in algo\u00adrithms, and requires the runtime system to store prior inputs and outputs. \nThe DTT model requires no such storage, as it relies on detecting changes rather than detecting sameness \nor redundancy. Therefore, our software framework works for arbitrary sizes of code regions and unlimited \ndata structure sizes, with no signi.cant storage overhead.  3. Design and implementation of softwaredata-triggered \nthreads In this work, we design a compiler-assisted, software-only framework that accepts programs written \nusing the DTT extensions and executes the compiled programs in the DTT model. To enable the DTT model \nin software, we need to (1) replace hardware tables with software data structures, (2) detect if an \noperation changes memory contents, (3) schedule the thread, and (4) skip over the skippable region, if \nappropriate. This section introduces the basic design of our implementation to achieve these goals. \n 3.1 Software data structures The hardware framework to support the DTT model [16] uses the Thread Status \nTable and Thread Queue to manage execution. In our runtime system, we introduce state vari\u00adables and \na software thread queue to replace these hardware structures. For each skippable region, the compiler \nallocates a state variable that contains information that determines if our run\u00adtime system can skip \nthe execution of that skippable region. In the basic implementation, a state variable contains a valid \nbit, a pending support thread counter, and a cancellation bit (set by the cancel pragma) for a skippable \nregion. The valid bit indicates if the result of the previous DTT execution is still correct. The pending \nsupport thread counter records the number of running and queued support thread events that can potentially \naffect the result of the skippable region. The valid bit tends to change often, as it is always zero \nwhen there are active support threads; the cancellation bit is set very rarely, and stays set until we \nexecute the skippable region. When the cancellation bit is high, the valid bit is always low. The pro\u00adgram \ncan only skip the execution of a skippable region when the valid bit is set and the pending support thread \ncounter is 0. If the counter is positive, the main thread will wait for all support threads to complete. \nAfter that, if the valid bit is set it will skip the skippable region, but if it is reset it will execute \nthe skippable region in place. We can add performance-monitoring counters to allow more advanced management \nof the DTT runtime, as de\u00adscribed in Section 5.4. Even then, we use no more than 40 bytes of memory for \neach state variable. In this paper, we only declare one or two code sections as skippable regions in \neach benchmark, so we only use at most two state vari\u00adables in each application. However, this is not \na limitation of the model; the programmer can express more. When the program needs to generate a support \nthread, we enqueue the necessary information to a software data structure, the thread queue (TQ). We \nstatically allocate the TQ at the beginning of execution. For each TQ entry, we record (1) the current \nsupport thread status which indicates whether the event is executing or waiting to execute, (2) the memory \naddress that triggered the support thread event, (3) the support thread function to execute, and (4) \na pointer to the state variable of the corresponding skippable region. The size of a TQ entry is also \n40 bytes. We allocate 256 entries for the TQ in this paper.  3.2 Detecting changes to memory contents \nThe DTT model spawns support threads when a memory operation changes a value in memory. Therefore, the \nruntime system must be able to detect a change to memory content. To detect whether or not a memory operation \nwrites a new value to an address, the compiler attaches tstore() functions to all assignments that may \nstore a new value to the data triggers. These functions replace the conventional store instructions in \nthose cases. The tstore() function takes the following arguments: the writing memory address, the triggering \naddress, the new value that we are writing, the pointer to the support thread function, and a pointer \nto the state variable of the skippable region. The triggering address is the only argument of the support \nthread function. Our framework uses the base ad\u00address of the writing object as the triggering address \nif the data trigger is attached to a .eld of a data structure (this sim\u00adpli.es the code for the programmer). \nOtherwise, the trigger\u00ading address is identical to the writing address. Because the DTT model allows \nprogrammers to attach data triggers to any variable or .eld of a data structure with arbitrary type, \nwe also need to pass the size of the modifying variable or data .eld to the tstore() function so that \nthe value is stored properly, and so that changes are detected at the right granularity. When the tstore() \nfunction detects a memory change and if the cancellation bit is not set, it enqueues the support thread \nevent with the triggering address, the state variable of the corresponding skippable region, and the \nsupport thread function pointer into the TQ. The DTT runtime system also sets the state of the new support \nthread event to pending. If the TQ does not contain a free entry (all the queued events are still running \nor pending) for storing this information, the runtime system can either (1) execute a queued event on \nthe current processor core to free up a TQ entry or (2) force the main thread to stall until a running \nthread releases a TQ entry. In our preliminary experiments, we found that the TQ rarely becomes full, \nso we choose the latter to simplify the design. After the tstore() function successfully transfers the \nsupport thread information to the TQ, it will .rst increase the pending support thread counter of the \ncorresponding state variable by 1. It will then clear the valid bit of the corresponding state variable \nand return to the main thread. When there is any pending support thread event in the TQ, our framework \nwill try to schedule the execution of the support thread function. The system will select the .rst event \nin the queue that does not have any running support thread event pointing to the same state variable; \nthat is, we serialize the execution of support thread functions that may affect the same skippable region. \nThis restriction is due to our current de.nition of DTT, where the compiler knows the triggering address \nbut not necessarily the outputs of the support thread. Thus, it cannot distinguish between those threads \nthat need to be serialized (e.g., each thread updates the same structure) and those that can be executed \nin parallel if their triggering addresses differ (e.g., if each writes a separate element, such as if \na change to A[i] creates a thread that updates C[i]). This will likely be addressed in future work. For \nnow, this limits our parallelism to two, unless we have multiple skippable regions. The initial baseline \nimplementation spawns a new thread, if possible, when data is modi.ed. However, our experiments (Section \n5.2) .nd that this strategy signi.cantly degrades the performance due to the overhead of spawning new \nthreads. Therefore, we develop a fast thread spawn (Section 5.3) technique, which uses polling threads \nto minimize the thread spawning latency. Once the runtime system starts executing a support thread event, \nit will .rst change the support thread event state to running. The support thread will then run as a \nconven\u00adtional thread until it reaches a dtt return() call. The dtt return() function, inserted by the \ncompiler at all function exit points, will atomically decrease the pending support thread counter by \n1 and update the valid bit in the state variable of the corresponding skippable region to valid if (1) \nno other event currently in the TQ will change the computation result, and (2) the cancellation bit is \nlow. It will then release the occupied TQ entry for future events. In ad\u00addition to the dtt return() function, \nwe also provide a dtt cancel() function that terminates the support thread when the support thread executes \na path that may lead to unwanted results. The dtt cancel() function will inval\u00adidate the valid bit, clear \nall the queued events associated with the state variable, reset the pending support thread counter to \n0, and set the cancellation bit. Once we set the cancellation bit, all later events for the skippable \nregion will be discarded until the main thread executes the skippable region code, which will clear the \ncancellation bit and re-enable the use of DTT on the skippable region. The cancellation bit enables an \nimportant implementation feature by always initializing these bits high, we ensure that the skippable \nregion is exe\u00adcuted the .rst time, and prevent a .urry of support threads being spawned while data structures \nare being initialized. In the DTT model, the skippable region provides an implicit barrier since the \nmain threads stall at the begin\u00adning of a skippable region if any corresponding thread event is executing. \nIn our framework, the compiler inserts a dtt barrier() function right before each skippable re\u00adgion to \nperform the implicit barrier. The dtt barrier() function examines the number of pending threads recorded \nin the corresponding state variable. If any outstanding sup\u00adport thread of this skippable region is running, \nthe dtt barrier() function will stall the main thread until the support thread .nishes execution. If \nthere are no pending support threads associated with the skippable region, the dtt barrier() function \nwill return with the value of the current valid bit of the state variable and allow the main threads \nto continue execution. In this version of DTT, all ac\u00adtive main threads (in parallel code) must share \nthe barrier i.e., they must each contain the skippable region. In this way, there is no possibility \nof a race condition between reading and updating the active thread count.  Just prior to returning \nfrom the dtt barrier(), the valid bit is read. If the valid bit is set, the main thread will jump to \nthe end of the skippable region. Otherwise, the main thread will execute the skippable region code in \nplace. The DTT runtime system will then set the valid bit of the skippable region to valid after the \nprogram executes the skippable region. The DTT model allows support thread functions to mod\u00adify global \ndata. As in many parallel programming paradigms, the programmer needs to ensure that those data updates \nin support thread functions do not incur unwanted data races. The only memory consistency issue we need \nto be careful about is to ensure that the store that happens in tstore() precedes the support thread, \nand that stores in the support thread precede the skippable region. In our software imple\u00admentation, \na memory consistency violation is either impossi\u00adble (any strong consistency system) or highly unlikely \n(weak consistency). In the latter case, a few memory fences suf.ce.  4. Methodology To study the performance \nof software DTTs, we selected three processor architectures and a variety of applications. We describe \nthose processors and applications in this sec\u00adtion. Like the prior work, we focus on modi.cations to \nex\u00adisting, mature code rather than new programs, which enables better comparison between DTT and non-DTT \ncode. 4.1 Processors To investigate the performance of the software implementa\u00adtion of the DTT model, \nwe select three widely available pro\u00adcessors Intel Xeon E5520 (Nehalem), AMD Opteron 2427 (Opteron), \nand Intel Xeon E5420 (Core 2 Quad) as the exper\u00adimental platforms. The memory hierarchies of the processors \nvary signi.cantly. The Core 2 Quad features a shared 6MB L2 cache on each die. In contrast, the Nehalem \nand Opteron have private L2 caches for each core but a shared L3 cache. The Opteron has an exclusive \ncache hierarchy while the oth\u00aders are inclusive. The memory latencies for these processors also differ \nsigni.cantly. Each of the processors features very different microarchitectures including the pipeline \ndesign, the branch predictor, etc., but all these processors can exe\u00adcute multiple threads on the same \ndie. The Nehalem proces\u00adsor also supports simultaneous multithreading (SMT) [18] which can execute two \nthreads within the same processor core to maximize the utilization of functional units. In this work, \nwe also examine the performance of software data\u00adtriggered threads on the SMT con.guration using the \nNe\u00adhalem processor.  4.2 Applications We use gcc-4.1.2 to compile all 15 applications in SPEC 2000 \nthat are written in C or C++ into x86-64 binaries. We use the older SPEC 2000 benchmarks to be able to \ncompare with the prior work. For each benchmark, we use Pin [15] to pro.le the memory instructions that \nfrequently incur re\u00addundant loads, as in prior work [16]. Unlike the prior work, we pro.le the whole \nprogram to determine the potential of applying the DTT model. The pro.ling results help identify the \ndata structures incurring most of the redundant loads. We select very few data structures as data triggers, \nand copy the code that depends on the data triggers to compose sup\u00adport thread functions. Table 1 lists \nour modi.cations to the benchmarks. These modi.cations in some cases are identical or similar to those \nmade in the prior work [16], but in sev\u00aderal cases (equake, gcc, gzip, mesa, perlbmk, and vpr) we target \nother sections of code because we found that the code used in the prior work did not address whole-program \nexe\u00adcution as much as it impacted the short simulated regions in the prior experiments. We also investigate \nthe interaction between DTT and tra\u00additional parallelism for the .rst time in this research. We use the \nPARSEC 2.1 benchmarks [3]. We found that the level of redundant execution varied more widely in PARSEC \nthan it does in SPEC. Therefore, we run the majority of the PAR-SEC benchmarks, but did not attempt to \ntransform those where the pro.led incidence of redundant loads was below 30%. In each case for the remaining \nbenchmarks, we opti\u00admize the single function that contains the most redundancy in the benchmark, but \nretain the traditional parallelism for the rest of the program. In most cases, the DTT parallelism was \nnot redundant with the traditional parallelism, or targeted redundancy rather than parallelism. As a \nresult, the effects of DTT on non-parallel PARSEC was also instructive, particularly in light of our \nbetter access to runtime statistics while exe\u00adcuting the serial versions. Therefore, we also include \nthe single-thread versions of the PARSEC benchmarks with our single-thread analysis and results. For \nblackscholes, our support thread computes a sin\u00adgle value of the prices array in bs thread(), replac\u00ading \ncode that recomputes the entire array. For bodytrack, the support thread calculates the Estimate() method \nonly when a particle .lter object gets updated. For can\u00adneal, we detect the change of element a and b \nto up\u00addate the result of calculate routing cost(). For facesim, we only perform UPBS initialization when \nthe number of elements changes. For .uidanimate, the support thread function computes x, y, and z values \nfor an element in the cell.a array right after the program generates a new value for the element; this \neliminates the need to ex\u00ad ecute ProcessCollisions() after all threads .nish ComputeForcesMT(). For \nswaptions, we recompute the dSimSwaptionMeanPrice and dSimSwaptionStdError .eld of an element that is \nin the swaptions array only when the program changes other .elds in that element that may change those \n.elds. This avoids the redundant execution of the HJM Swaption Blocking() function for the entire array. \nFor vips, we trigger the linear transform computation as soon as the input images are ready. For x264, \nwe examine the type of frame and only regenerate the headers once the type of the upcom\u00ading frame is \ndifferent from the previous frame. For the parallel experiments, we still use just an addi\u00adtional thread \nto execute support thread functions and disable traditional parallelism when the application is using \nDTT. Because we only target one code section in each applica\u00adtion, and exploit redundancy heavily, we \ntend (in the cur\u00adrent implementation) to get nearly all of our DTT gains with just one extra core. This \nmutually exclusive usage of the two models of parallelism (either DTT or traditional parallelism is active) \nis not a part of the DTT programming model tra\u00additional parallel threads should be able to access DTT \ntrig\u00adgers. However, this simpli.cation in the current implemen\u00adtation still allows us to achieve high \nspeedups. Since it is dif.cult to quantify programmer effort re\u00adquired to achieve performance results, \nwe have restricted our changes, both for the serial and parallel applications, to rela\u00adtively minor changes \nwith few skippable regions and support functions. For most benchmarks, we only target at one sup\u00adport \nthread function and one skippable region. This allows us to demonstrate that in many cases very signi.cant \nspeedups are possible with low programmer effort.  5. Results This section quanti.es the software overheads \nof our data\u00adtriggered threads implementation, and shows performance results for the modi.ed applications. \nWe discuss results on three different hardware platforms, results for both a single\u00adthread implementation \nand for a multithreaded implementa\u00adtion, results for three different hardware parallelism scenar\u00adios \n on an SMT core, across cores on a CMP, and across sockets, and results for both serial applications \nand parallel applications. We separate the serial application and paral\u00adlel application results for two \nreasons. First, for comparison with the prior work on hardware DTTs, we run the same SPEC benchmarks \nin similar con.gurations. Second, our performance monitoring library allows us to collect statistics \nmore accurately for the single-threaded case, so we present more comprehensive data for those results. \nSoftware data-triggered threads incur overheads, both in runtime monitoring and especially in multithreading-related \nlatencies, that the hardware-driven approach did not see in the simulated experiments. Even compared \nto conven\u00adtional parallelism, the DTT model tends to create many short threads, and conventional architectures \nare not optimized to execute short threads well [4]. This section will quantify some of those costs and \ndescribe mechanisms that mitigate those overheads, but they do not go away completely. As a result, even \nmore than the prior work, our code modi.cations that exploit DTTs (see Section 4) target redundant execution \nmore than new opportunities for parallelism. This is because if code is redundant, not only do we avoid \nthe redundant computation, we also skip the overheads of spawning and communication associated with that \nsupport thread. To better exploit parallelism in the DTT model, we op\u00adtimize the runtime system design \nto avoid thread spawning overhead and adapt to the overhead in the underlying sys\u00adtems. With the optimized \nruntime system, we also demon\u00adstrate that DTT can be highly complementary with tradi\u00adtional parallelism \nto further improve the performance of par\u00adallelized applications. In our experiments, we .nd that the \ngeneral performance trends are similar across all the platforms we used, so we only present the result \nof the Nehalem machine in .gures to save space, but discuss the result of other platforms in text. We \nuse single-thread unmodi.ed benchmarks as the default baseline unless otherwise speci.ed. 5.1 Runtime \nsystem overhead Without hardware support, the runtime system needs to check if modi.cations change the \nmemory contents of vari\u00adables declared as data triggers. The runtime system also needs to transfer information \nto the TQ and manage the software structures after detecting a change. These are over\u00adheads that impact \nthe main thread of execution. To examine these overheads, we designed a runtime system (just for this \nexperiment) that executes the tstore() functions and queues the support thread events, but does not compute \nsup\u00adport thread functions. Figure 3 presents the overhead of our runtime sys\u00adtem on the Nehalem machine. \nThe average slowdowns for DTT overheads are 1.4%, 2.4% and 1.0% on the Nehalem, Opteron, and Core 2 Quad, \nrespectively. For most bench\u00admarks, the software overhead is less than 2% since we only replace the assignments \nassociated with data triggers, which is typically a very small fraction (lower than 0.01%) of all store \ninstructions. For parser, which is more heavily im\u00adpacted, the program touches data triggers frequently; \nwe Figure 3. The overhead of tracking changes to memory content and managing the internal data structures \nin our runtime system, with the support thread execution disabled. Figure 4. The relative performance \nof the software DTTs implementation with both the main thread and the support thread functions running \nin a single thread on the Nehalem machine.  ammpammp art art bzip2 bzip2 crafty crafty eon eon equake \nequake gcc gcc gzip gzip mcfmcfmesamesaparserparserperlbmkperlbmktwolftwolfvortexvortexvprvprspec2kspec2kaverageaverage \n blackscholesbodytrackbodytrackcannealcannealfacesimfacesim fluidanimate swaptionsvipsvipsx264x264 parsecaverageaverage \n7.27 average  Figure 5. The relative performance of the base software DTTs implementation with the \nsupport threads running on separate cores on the Nehalem machine. observe that the the dynamic instruction \ncount increases by 9% in that program.  5.2 Base implementation While we focus our code changes on regions \nthat exhibit re\u00addundancy, we do exploit both redundancy and parallelism with our DTTs. Following the \nlead of the prior work, we can separate these effects by running both a single-thread version of our \nDTT runtime system and a multi-context version. The latter bene.ts from both effects, while the former \nonly bene\u00ad.ts from redundancy. In addition, for this research, this also isolates the thread spawning \nand communication overheads, since the single-thread version does not incur these. There\u00adfore, for the \ninitial experiments in this section, we create a DTT runtime system that runs in a single thread. In \nthis con\u00ad.guration, when the tstore() function detects a memory modi.cation, the program invokes the \nsupport thread func\u00adtion immediately in the same hardware context. Figure 4 shows the data-triggered \nthreads performance for a single-thread implementation compared with running unmodi.ed (no DTTs) code \non a single core of the Nehalem processor. The experiments show that even without the help of parallelism \nand with no hardware support, our modi.ca\u00adtion can still improve the performance for most serial bench\u00admarks \nfrom SPEC2000, with averages of 6%, 7%, and 8% on the Nehalem, the Opteron, and the Core 2 Quad, respec\u00adtively. \nFor PARSEC benchmarks, our modi.cation achieves signi.cant performance gain, 14.8X and 128.5X for blacksc\u00adholes \nand swaptions on the Nehalem machine. Both blacksc\u00adholes and swaptions contain about 70% redundant loads, \nwhich is somewhat lower than the SPEC benchmarks that we investigated. However, the actual level of redundancy, \nespe\u00adcially for swaptions, is signi.cantly higher. The vast major\u00adity of the non-redundant loads are \nto temporary local struc\u00adtures that the program re-initializes in function calls that de\u00adpend on highly \nstatic data. Thus, they are not recorded by our tools as redundant, but in fact are unnecessary when \nthe global data does not change. These regions are easily identi\u00ad.ed by the programmer due to the presence \nof the redundant loads of the global data. Over all single-thread benchmarks, we achieve average performance \nimprovements of 7.3X, 4.4X, and 7.2X on the Nehalem, the Opteron, and the Core 2 Quad, respectively. \nFor benchmarks eon, equake, mcf, blackscholes, canneal, .uidanimate, and swaptions, which obtain the \nmost speedup in this con.guration, the data-triggered threads model signif\u00adicantly reduces dynamic instruction \ncounts. A phenomenon we see in a few applications is exhibited most strongly for mcf, in which the DTT \nmodel reduces the dynamic instruc\u00adtion count by 21%, yet achieves more than 100% speedup. This happens \nbecause the redundant computation that we eliminate in that application is also the code that incurs \nthe most cache misses and dominates the execution time. However, for parser, because the reduction of \nredun\u00addant computation cannot compensate for the runtime sys\u00adtem overhead, we still slow down the performance \nof this benchmark. In crafty, our modi.cation still increases the dy\u00adnamic instruction count by 6% and \ncauses signi.cant perfor\u00admance degradation. As expected, we see lower performance in general than the \nhardware approach [16]. Unfortunately, we cannot make more speci.c comparisons with the hard\u00adware results, \ndue to differences in machine architectures and ISA, run length (whole program vs short interval), real \nma\u00adchines vs simulations, and different usage of DTTs in some applications. To bene.t from parallelism, \nwe extend our data-triggered threads framework to utilize multiple cores within a pro\u00adcessor. In the \nbaseline multi-threaded DTT framework, the tstore() function creates a new pthread when the system .nds \na support thread event is available for execution. The dtt return() function terminates the thread. Figure \n5 shows the performance of our initial software implementation of multi-core DTT on the Nehalem ma\u00adchine. \nDespite achieving speedup on 9 of 15 SPEC2000 benchmarks and 3 out of 8 PARSEC benchmarks on Ne\u00adhalem, \nthis implementation signi.cantly underperforms the single-thread case. For the SPEC benchmarks, our imple\u00admentation \ndegrades performance vs. the baseline an average of 9% on the Nehalem processor and 10% on Opteron and \nthe Core 2 Quad. For PARSEC, the multi-core DTT on the Nehalem machine still shows 128.2x performance \nimprove\u00adment for swaptions, and marginally better performance for facesim and vips. However, the others \nall drop off consider\u00adably, including blackscholes which achieved 14.8X speedup in single-thread mode. \nFor benchmarks like twolf, crafty, parser, blackscholes, canneal, and .uidanimate, which gen\u00aderate many \nshort support threads, the overhead of multi\u00adthreading is dominant. These results imply that the thread \nspawning overheads, including operating system overhead, warming up the cache, and communicating data, \nhave a large impact on the software DTT performance for several applications. In the following sections, \nwe present optimizations to minimize the thread spawning overhead and allow software DTTs to take better \nadvantage of the available parallelism.  5.3 Fast thread spawning While the initial, naive implementation \nof software data\u00adtriggered threads achieves speedup on some applications, anywhere threads are actually \nbeing generated with high frequency we fail to amortize the high cost of generating and spawning these \nthreads. To minimize the effect of these overheads, we introduce a fast user-level thread spawning mechanism \nin our runtime system. In our fast spawning runtime system, we host a thread on each core that we plan \nto use for executing support threads. This thread, when idle, will monitor the thread queue. If the polling \nthread .nds a queued event that has not executed, it will fetch the queued event and invoke the speci.ed \nsupport thread function. When the support thread function .nishes, the dtt return() function updates \nthe state variable of the corresponding skippable region and goes back to the polling function. Figure \n6 shows the performance of our runtime system with fast thread spawning on the Nehalem machine for all \nbenchmarks we examined. Because we optimize only one skippable region for each benchmark and our current \nframe\u00adwork serializes the support threads associated with the same skippable region, we need only create \none polling thread on a separate processor core. The fast spawn scheme signi.cantly improves the soft\u00adware \ndata-triggered threads implementation. For SPEC2000 benchmarks, the fast spawn scheme improves the perfor\u00admance \nover the baseline architecture (single thread with\u00adout DTT) with an average improvement of 11.5%. The \nfast spawn scheme also improves the SPEC2000 performance over the baseline by 3.7% on the Opteron machine \nand 4.2% Figure 6. The relative performance of the software DTTs implementation with the fast thread \nspawning mechanism (DTT + fast spawn) using a Nehalem machine. main thread only MT/DTT in parallel DTT \nonly Figure 7. The relative execution time of DTT + fast spawn on the Nehalem machine. ammp art bzip2 \ncrafty eon equake gcc gzip mcf mesa parser perlbmk twolfvortex vpr spec2k average blackscholes bodytrack \ncanneal facesim fluidanimate swaptions vips x264 parsec average  ammp art bzip2 crafty eon equake gcc \ngzip mcf mesa parser perlbmk twolf vortex vpr spec2k bodytrack canneal facesim  vips x264 parsec average \n 14.8 14.0 14.8 2.44 2.44 2.44 128.5 127.0 128.3 18.9 18.6 18.9 7.27 7.20 7.32 Speedup over baseline \n2 1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 single thread DTTDTT + fast spawnDTT + fast spawn + threshold \n ammpartbzip2craftyeonequakegccgzipmcfmesaparserperlbmktwolfvortexvpr spec2kaverage blackscholesbodytrackcannealfacesimfluidanimateswaptionsvipsx264 \nparsec average average Figure 8. The relative performance of the software DTTs implementation with fast \nthread spawning and thresholding mechanism (DTT + fast spawn + threshold) on a Nehalem machine, for SPEC. \non the Core 2 Quad machine. For ammp, art, mcf, vortex, and vpr, the fast spawn mechanism allows those \nbenchmarks to take advantage of parallelism and outperform the single\u00adthreaded runtime system. For PARSEC \nbenchmarks, the fast spawn helps exploit the parallelism of the DTT model and achieves signi.cant performance \nimprovement on blacksc\u00adholes, canneal, and .uidanimate over the original implemen\u00adtation. We can see \nthe importance of parallelism to each appli\u00adcation in Figure 7. It presents the relative portion of time \nthat each benchmark spends in running only the main thread (main thread only), running only the support \nthread (DTT only), and running both the main thread and the support thread in parallel (MT/DTT in parallel) \n we only present the execution time breakdown on the Nehalem machine; the results for other machines \nare similar. In fact, we not only see the importance of parallelism (how often both threads are executing), \nbut also the importance of lack of parallelism (how often the support thread is running alone which \nim\u00adplies the main thread is stalled, waiting for the support thread completion). In particular, for crafty \nand parser, where we still suffer slowdown in the fast spawn scheme, we .nd that these benchmarks spend \na signi.cant portion of time where only the support thread function is running. This problem is addressed \nin the next section.  5.4 Thresholding The fast spawn scheme minimizes the overhead in spawning threads. \nHowever, we still see that some benchmarks experi\u00adence signi.cant performance degradation because the \nmain thread stalls frequently. This is a result of two factors. First, the DTT computation is not highly \nredundant if support threads do not execute, the main thread does not wait for them. Second, there is \ninsuf.cient slack to hide the latency of the support thread. Both must be true for the main thread to \nstall. For example, it is still advantageous for the program\u00admer to create support threads with no slack \n(i.e., no paral\u00adlelism with the main code), as long as there is signi.cant redundancy. To avoid this \ncase where we are frequently stalling, we in\u00adtroduce a simple thresholding scheme for our software run\u00adtime \nsystem. For each skippable region, we add two counters to keep track of how many times the program calls \nthe bar\u00adrier function (at the skippable region) and how many times the main thread stalls. This accounts \nfor both factors above as long as the main thread does not have to wait, we don t distinguish whether \nit was because the support thread(s) did not run or because they ran and completed in time. When the \nruntime system invokes the barrier function a certain number of times, it calculates the percentage of \nsupport thread exe\u00adcutions that caused stalls over that period. If the percentage is lower than a threshold, \nthe runtime system will reset the counters. Otherwise, the later invocations of the tstore() function \nwill only invalidate the state variable without en\u00adqueueing any support thread events to the TQ. Thus, \nthe main thread will execute the code in the skippable region instead of using the support threads to \nperform this compu\u00adtation. After a certain period, the runtime system will retry the DTT model. Figure \n8 shows the performance of software DTTs with the thresholding mechanism on the Nehalem machine. For \nthese experiments, we calculate the percentage of stalls ev\u00adery 1000 calls to the barrier function of \neach skippable re\u00adgion. If the threshold mechanism turns off the usage of the DTT model, the system will \nretry using DTT every 10000 calls to the corresponding barrier function. We examined thresholds ranging \nfrom 10% to 90% at increments of 5%. The threshold percentage that achieves the best overall per\u00adformance \ngain is different for each of our hardware plat\u00adforms. This is to be expected, as this value will be \na factor of the relative cost of communication (e.g., thread cold start effects) to computation, which \nwill be impacted by memory latencies, core architectures, inclusive vs. exclusive caches, etc. However, \nwe always .nd a threshold value in each archi\u00adtecture which outperforms all the prior software DTT imple\u00admentations \nwe have investigated. The runtime system per\u00adforms best with 50%, 10%, and 15% threshold values for \n14.8 13.6 14.0 2.44 2.44 2.47 128.3 128.3 123.5 18.9 18.7 18.2 7.32 7.25 7.06 Speedup over baseline 2 \n1.8 1.6 1.4 1.2 1 0.8 0.6 0.4 0.2 0 DTT (Different Cores)DTT (SMT)DTT (Different Sockets) ammpartbzip2craftyeonequakegccgzipmcfmesaparserperlbmktwolfvortexvpr \nspec2kaverage blackscholesbodytrackcannealfacesimfluidanimateswaptionsvipsx264 parsec average average \n Figure 9. The relative performance of the software DTTs implementation with support threads running \non a separate core (different cores), or the same core (SMT) or a different core from another physical \nprocessor (different sockets). the Nehalem, the Opteron, and the Core 2 Quad processor, respectively. \nFor SPEC2000 benchmarks, the runtime system improves the average performance by 14.8% on the Nehalem \nmachine. On the Opteron processor, we achieve 7.3% performance improvement. On the Core 2 Quad processor, \nwe improve performance by 9.3%. Over all single-thread benchmarks, DTT achieves 7.3X speedup on Nehalem. \nFor the threshold values that achieve the best performance on each platform, the scheme successfully \neliminates the performance loss of parser and mitigates the performance loss in crafty. However, it also \ndegrades the performance in ammp by 5% and 2% on Opteron and Core 2 Quad machines. That is the only case \nwhere our threshold appears to be too aggressive and negates some opportunity. For PARSEC benchmarks, \nthe runtime system with thresh\u00adolding mechanism and fast spawn achieves average perfor\u00admance gains of \n18.9X on the Nehalem machine, 10.9X on the Opteron machine, and 18.9X on the Core 2 Quad ma\u00adchine. We \nwill discuss the equivalent (serial main thread plus DTT) PARSEC results in Section 5.6. Since the thresholding \nmechanism (with fast spawn) works the best among all our implementations, we use it as the default software \nDTT implementation in the rest of the paper.  5.5 Adapting to different types of hardwareparallelism \nFor the above multithreaded implementations, we execute support thread functions on a separate core within \nthe same chip. However, our experimental platforms allow us to ex\u00adplore two more options to schedule \nthe execution of the data-triggered support threads. One is to schedule the sup\u00adport thread functions \nin a different hardware context within the same core using simultaneous multithreading, and the other \nis to schedule support thread functions to another core located in a different socket. The former minimizes \ncom\u00admunication and cold-start effects, because caches are shared between SMT contexts, but maximizes \npotential interference between the main thread and the DTT (both in the caches and the execution resources). \nThe latter has the opposite tradeoff. Figure 9 shows the result of executing the support threads on \nthe same core using SMT. Running a support thread within the same core can achieve 12.7% performance \nim\u00adprovement for SPEC2000 and 18.7X performance improve\u00adment for PARSEC over the baseline. Thus, the \nsoftware tech\u00adnique is still effective for multithreading contexts, but just a bit lower than the performance \non separate cores. In this case, the improvements in communication do not fully com\u00adpensate for the increased \ninterference. Notice that the inter\u00adference affects even applications that rarely spawn support threads, \nbecause the fast spawn optimization utilizes polling. We also investigate the impact of running support \nthreads on different sockets in Figure 9. This con.guration elimi\u00adnates nearly all resource sharing between \nthe main thread and support thread, but it also increases the communication latencies between them. Compared \nwith running the support threads on a different core within the same chip, the perfor\u00admance impact of \nincreasing communication latencies is very insigni.cant in most benchmarks. For SPEC2000 bench\u00admarks, \nthe increased communication latency does hurt the performance of gcc, crafty, and parser on some architectures. \nThe average performance loss of running support threads on different sockets is within 0.3% of our baseline \nDTT run\u00adning on different cores on the Core 2 Quad machine. For Nehalem and Opteron, scheduling support \nthreads to a sepa\u00adrate chip is affected more, but still within 3%. For PARSEC benchmarks, running support \nthreads on different sockets signi.cantly hurts the performance of blackscholes, .uidan\u00adimate and swaptions \non all the platforms, but the speedups are still high. Compared with our baseline DTT, schedul\u00ading support \nthreads on a separate chip reduces the average performance by 3.5%, 3.5%, and 1.8% on the Nehalem ma\u00adchine, \nthe Opteron machine, and the Core 2 Quad machine. In general, however, we see that our implementation \nmakes the DTT programming model very tolerant of vary\u00ad   (a) blackscholes (b) bodytrack (c) canneal \n(d) facesim  single-thread 2-thread 4-thread single-thread 2-thread 4-thread (e) .uidanimate (f) swaptions \n(g) vips (h) x264 Figure 10. The performance of PARSEC applications with DTT (DTT) and traditional parallelism \n(pthread only). The DTT version uses a combination of DTT and the original parallelism. ing communication \nlatency. This is in large part due to the thresholding optimization. Thresholding successfully dis\u00adables \nDTT in the cases where the increased latency is enough to push the DTTs past the breakeven point and \nstart hurting performance. We con.rm this in separate experiments we ran on the Nehalem machine. Without \nthresholding, the cost of moving from same-socket to cross-socket parallelism is 8% for SPEC (compared \nto 3% with thresholding).  5.6 Data-triggered threads and multithreadedapplications Having DTT versions \nof parallel programs for the .rst time allows us to investigate a couple of interesting questions. First, \nis DTT parallelism redundant with traditional paral\u00adlelism (exploiting the same phenomena) or is it comple\u00admentary? \nSecond, how do DTT/parallel programs scale with thread count? Figure 10 shows the performance gain of \nthe modi.ed PARSEC benchmarks with and without DTT using different number of threads. Because the performance \ntrends are sim\u00adilar across all our experimental platforms, we only show the result on the Nehalem machine. \nThe results for the parallel benchmarks fall into a few distinct categories. For blackscholes and swaptions, \nDTT speedups are still dramatic. We see in this case that DTT subsumes (and surpasses) traditional parallelism, \ninstead of the other way around. Performance, however, does not scale with the number of threads, because \nwhat little code still gets executed has not been parallelized. In these cases, DTT achieves speedup \nof 8X and 64X, respectively, over the 4\u00adcore parallel version. For .uidanimate, we are able to target \nenough redundant computation (from the ProcessCollisions() func\u00adtion) to get 2.4X speedup from DTT. However, \nwe still see excellent scaling from .uidanimate, even better than tradi\u00adtional parallelism, because the \ncode that remains (after the elimination of the redundant code) is effectively addressed by the traditional \nparallelism. Thus, even in this implementa\u00adtion of DTT which does not scale beyond two threads, DTT can \nstill improve parallel scaling if it removes serial code or parallel bottlenecks, leaving the remaining \ncode more highly parallel than the full original code. For bodytrack, the application has 53% redundant \nloads, so DTT exploits some redundant computation, which im\u00adproves performance by 23% without the help \nof parallelism. When using multiple threads, that redundant code is not on the critical path, and the \nDTT version simply tracks the par\u00adallel version with two or more threads. In canneal, we see the one \ncase where DTT parallelism interferes with traditional parallelism. Our DTT implemen\u00adtation targets code \nwith modest redundancy, which gives us 27% performance gain. However, this is the same region that is \ntargeted by the traditional parallelism, and our current im\u00adplementation does not allow us to exploit \nboth in the same region. Thus, we lose the gains of the original parallelism. We expect this limitation \nto go away in future implementa\u00adtions. For the remaining benchmarks, facesim, vips and x264, we target \ncode that traditional techniques could not or did not parallelize. Although the gains were typically \nsmall, they were complementary with traditional parallelism, such that we always gain more (if only slightly) \nfrom the combination of DTT and parallelism than from parallelism alone. We see from these results that \neven on parallel applica\u00adtions, data-triggered threads enable us to express a new type of parallelism \nthat is often complementary to traditional par\u00adallelism, and in other cases we can use it to express \ntradi\u00adtional parallelism more ef.ciently. Although DTT alone does not currently scale beyond two threads, \nmost of these applications scale well with the combination of DTT and traditional parallelism. When DTT \ntargets and eliminates serial code, it can improve the scaling of traditional parallelism overall.  \n 6. Conclusions This paper presents a pure software approach to support the data-triggered thread programming \nand execution model. Our work improves the generality and portability of the data\u00adtriggered thread model. \nHaving a complete software solu\u00adtion allows the use of the DTT programming model on any existing parallel \nmachine. The presented solutions to miti\u00adgate thread spawning costs and to eliminate the performance \nlost to runaway, serial DTTs frees the programmer to use the DTT constructs without worrying about potential \nperfor\u00admance loss. Our system allows a set of serial programs appli\u00adcations from SPEC 2000 to be sped \nup by 15%, with minor code modi.cation and no hardware support. The complete set of serial applications \n(including single-thread PARSEC) were sped up by 7.3X (arithmetic mean) or 1.6X (geometric mean). We \nalso show that DTT can be highly complemen\u00adtary with traditional parallelism and achieve signi.cant per\u00adformance \ngain, as high as 64X, even over the original parallel version.  Acknowledgments The authors would like \nto thank the anonymous reviewers for their helpful comments. This work was funded in part by NSF grant \nCCF-1018356 and support from AMD.  References [1] Arvind and R. S. Nikhil. Executing a program on the \nMIT tagged-token data.ow architecture. IEEE Transactions on Computers, 39:300 318, March 1990. [2] S. \nBalakrishnan and G. S. Sohi. Program demultiplexing: Data-.ow based speculative parallelization of methods \nin se\u00adquential programs. In 33rd Annual International Symposium on Computer Architecture, volume 0, pages \n302 313, June 2006. [3] C. Bienia and K. Li. PARSEC 2.0: A New Benchmark Suite for Chip-Multiprocessors. \nIn Proceedings of the 5th Annual Workshop on Modeling, Benchmarking and Simulation, June 2009. [4] J. \nBrown, L. Porter, and D. Tullsen. Fast thread migration via cache working set prediction. In Proceedings \nof 17th Interna\u00adtional Symposium on High Performance Computer Architec\u00adture, pages 193 204, February \n2011. [5] D. C. Cann, J. T. Feo, A. D. W. Bohoem, and O. Oldehoeft. SISAL Reference Manual: Language \nVersion 2.0, 1992. [6] D. Citron, D. Feitelson, and L. Rudolph. Accelerating multi\u00admedia processing \nby implementing memoing in multiplication and division units. In Eighth International Conference on Ar\u00adchitectural \nSupport for Programming Languages and Operat\u00ading Systems, pages 252 261, October 1998. [7] D. E. Culler, \nA. Sah, K. E. Schauser, T. von Eicken, and J. Wawrzynek. Fine-grain parallelism with minimal hardwaresupport: \na compiler-controlled threaded abstract machine. In Fourth International Conference on Architectural \nSupport for Programming Languages and Operating Systems, pages 164 175, April 1991. [8] M. Frigo, C. \nE. Leiserson, and K. H. Randall. The implemen\u00adtation of the Cilk-5 multithreaded language. In Proceedings \nof the ACM SIGPLAN 1998 conference on Programming lan\u00adguage design and implementation, pages 212 223, \n1998. [9] M. A. Hammer, U. A. Acar, and Y. Chen. CEAL: a C-based language for self-adjusting computation. \nIn Proceedings of the 2009 ACM SIGPLAN conference on Programming lan\u00adguage design and implementation, \npages 25 37, 2009. [10] P. Marcuello, A. Gonz\u00b4alez, and J. Tubella. Speculative multi\u00adthreaded processors. \nIn Proceedings of the 12th international conference on Supercomputing, pages 77 84, 1998. [11] D. Michie. \nMemo functions and machine learning. Nature, 218:19 22, 1968. [12] R. S. Nikhil. Can data.ow subsume \nvon Neumann comput\u00ading? In 16th Annual International Symposium on Computer Architecture, pages 262 272, \nMay 1989. [13] R. S. Nikhil. ID reference manual, version 90.1. CSG Memo 284-2, September 1990. [14] \nG. Papadopoulos and D. Culler. Monsoon: an explicit token\u00adstore architecture. 17th Annual International \nSymposium on Computer Architecture, pages 82 91, May 1990. [15] H. Patil, R. Cohn, M. Charney, R. Kapoor, \nA. Sun, and A. Karunanidhi. Pinpointing representative portions of large Intel Itanium programs with \ndynamic instrumentation. In 34th International Symposium on Microarchitecture, pages 81 92, Dec. 2004. \n [16] H.-W. Tseng and D. M. Tullsen. Data-triggered threads: Elim\u00adinating redundant computation. In Proceedings \nof 17th Inter\u00adnational Symposium on High Performance Computer Archi\u00adtecture, pages 181 192, Feb. 2011. \n[17] H.-W. Tseng and D. M. Tullsen. Eliminating redundant computation and exposing parallelism through \ndata-triggered threads. IEEE Micro, 32(3):38 47, May 2012. [18] D. Tullsen, S. Eggers, and H. Levy. Simultaneous \nmultithread\u00ading: Maximizing on-chip parallelism. In 22nd Annual Interna\u00adtional Symposium on Computer \nArchitecture, pages 392 403, June 1995.  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>The data-triggered threads (DTT) programming and execution model can increase parallelism and eliminate redundant computation. However, the initial proposal requires significant architecture support, which impedes existing applications and architectures from taking advantage of this model. This work proposes a pure software solution that supports the DTT model without any hardware support. This research uses a prototype compiler and runtime libraries running on top of existing machines. Several enhancements to the initial software implementation are presented, which further improve the performance.</p> <p>The software runtime system improves the performance of serial C SPEC benchmarks by 15% on a Nehalem processor, but by over 7X over the full suite of single-thread applications. It is shown that the DTT model can work in conjunction with traditional parallelism. The DTT model provides up to 64X speedup over parallel applications exploiting traditional parallelism.</p>", "authors": [{"name": "Hung-Wei Tseng", "author_profile_id": "81414608109", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P3856229", "email_address": "h1tseng@cs.ucsd.edu", "orcid_id": ""}, {"name": "Dean Michael Tullsen", "author_profile_id": "81100552297", "affiliation": "University of California, San Diego, La Jolla, CA, USA", "person_id": "P3856230", "email_address": "tullsen@cs.ucsd.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384668", "year": "2012", "article_id": "2384668", "conference": "OOPSLA", "title": "Software data-triggered threads", "url": "http://dl.acm.org/citation.cfm?id=2384668"}