{"article_publication_date": "10-19-2012", "fulltext": "\n E.ciently Combining Parallel Software Using Fine-grained, Language-level, Hierarchical Resource Management \nPolicies Zachary Anderson Systems Group, ETH Z\u00a8urich Z\u00a8 urich, Switzerland zachary.anderson@inf.ethz.ch \nAbstract This paper presents Poli-C, a language extension, runtime li\u00adbrary, and system daemon enabling \n.ne-grained, language\u00adlevel, hierarchical resource management policies. Poli-C is suitable for use in \napplications that compose parallel li\u00adbraries, frameworks, and programs. In particular, we have added \na powerful new statement to C for expressing re\u00adsource limits and guarantees in such a way that programmers \ncan set resource management policies even when the source code of parallel libraries and frameworks is \nnot available. Poli-C enables application programmers to manage any re\u00adsource exposed by the underlying \nOS, for example cores or IO bandwidth. Additionally, we have developed a domain\u00adspeci.c language for \nde.ning high-level resource manage\u00adment policies, and a facility for extending the kinds of re\u00adsources \nthat can be managed with our language extension. Finally, through a number of useful variations, our \ndesign o.ers a high degree of composability. We evaluate Poli-C by way of three case-studies: a scienti.c \napplication, an image processing webserver, and a pair of parallel database join im\u00adplementations. We \nfound that using Poli-C yields e.ciency gains that require the addition of only a few lines of code to \napplications. Categories and Subject Descriptors D.3.3 [Programming Languages]: Language Constructs and \nFeatures Concurrent programming structures; D.1.3 [Programming Techniques]: Concurrent Programming Parallel \nprogramming General Terms Languages, Performance Keywords language extension, resource management, pol\u00adicy, \nhierarchical parallelism, scheduling Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 1. Introduction Due to recent trends in computer architecture, \npreviously single-threaded applications are making increasing use of parallel libraries and frameworks \nin order to achieve steady performance gains. Setting aside the commonly accepted idea that parallel \nprogramming is more di.cult than sequen\u00adtial programming due to concurrency concerns, parallel pro\u00adgramming \nis also di.cult because machine resources such as cores, caches, disks, and network interfaces may not \nbe managed by the operating system in a way that makes sense for the application. This is especially \ntrue when an applica\u00adtion uses multiple parallel libraries or frameworks simulta\u00adneously, as is the case \nboth with new implementations of tra\u00additional scienti.c applications [14, 31], and with upcoming applications \nthat include recognition, mining, and synthe\u00adsis tasks [8, 10, 40]. In this paper we describe Poli-C, \na language extension and runtime that exposes new OS resource management fea\u00adtures to application programmers \nwith the goal of allow\u00ading applications to tailor resource management to their own needs rather than \nrelying on the one-size-.ts-all approach supplied by modern operating systems. In particular, we have \nadded a powerful new statement to C that provides re\u00adsource guarantees and limits in such a way that \nprogrammers can set resource management policies for, and among, hier\u00adarchically parallel applications. \nAdditionally, we have devel\u00adoped a domain-speci.c language for de.ning high-level re\u00adsource management \npolicies, and a facility for extending the kinds of resources that can be managed with our language extensions. \nIn particular, we introduce a new statement to C having the following syntax: require(r(a)) {...}. In \nits most ba\u00adsic form, this statement indicates that inside of the block of code, the program has exclusive \naccess to the amount a of resource r, with the limitation that it cannot exceed that amount. If the requested \nresource is not available, the pro\u00adgram releases the resources it already holds and blocks wait\u00ading for \nit. Additionally, using special functions that we call policies programs may request not only single \nresources, but also sets of resources after querying resource availability and contention at runtime. \n The design of Poli-C provides three main bene.ts. First, it o.ers composability. Using a number of \noptions to the require statement, Poli-C can e.ciently combine o.-the\u00adshelf, unmodi.ed parallel libraries. \nThis paper describes in detail the design and implementation of the require state\u00adment and the ways its \noperation is modi.ed by the options it accepts. Secondly, our design allows .ne-grained management of \nany resource exposed by the operating system, for example cores and IO bandwidth. If the underlying OS \nprovides an API for measuring or managing a resource, then, after writ\u00ading a few lines of glue code, \nthat resource can be managed through the runtime and language extensions provided by Poli-C. Among threads \nin a single process, we achieve this by decoupling resource allocation from thread scheduling through \nthe use of explicit allocation trees. Among all pro\u00adcesses, we achieve this by providing a system-level \ndaemon, policd, that tracks resource usage across the entire system. This paper describes the operation \nof our allocation trees and policd. Exposing this low level of control to the application pro\u00adgrammer \nmight be seen as reversing a trend toward increased abstraction of hardware resources by operating systems \nand virtual machines. This trend likely makes many applications easier to build and maintain. However, \napplications with de\u00admanding performance requirements, such as databases, web\u00adservers, and scienti.c \ncodes, already subvert OS and VM provided abstraction boundaries, albeit in an ad-hoc, system\u00adspeci.c \nway. One important goal achieved by Poli-C is to al\u00adlow this abstraction boundary to instead be shifted \nin a prin\u00adcipled, portable, and reusable way that is made explicit in the source code of these applications \nthrough the require statement. Finally, our design allows us to separate policy from im\u00adplementation. \nWith our approach, policies are declared us\u00ading our policy DSL, but implemented as part of the lan\u00adguage \nruntime, transparent to the application programmer, but still available for modi.cation using declarative \nfeatures that, thanks to Poli-C, are now part of the language. This paper describes our policy DSL, and \nevaluates its use in a number of examples. Our work is inspired by the Lithe hierarchical scheduling \nframework [36]. Since the default OS scheduler is unaware of high-level tasks involving groups of cooperating \nthreads, its thread scheduling is ine.cient. Lithe solves this problem by enabling the allocation of \ncores to high-level tasks, rather than to individual threads. Poli-C solves the same problems as Lithe \nwith three key improvements. First, Poli-C manages other resources in addition to cores. Second, using \npolicd, it manages resources not only within one multithreaded pro\u00adcess, but also across multiple multithreaded \nprocesses. Fi\u00adnally, through the use of options to the require statement, it is possible to use Poli-C \nto compose parallel libraries with\u00adout modifying and creating custom builds of each parallel library \nto be composed. In summary we make the following contributions: The design and implementation of a runtime \nlibrary, lan\u00adguage extension, and system daemon that enable .ne\u00adgrained, language-level, hierarchical \nresource manage\u00adment policies suitable for use among applications that compose parallel libraries and \nframeworks.  The gathering of disjoint OS APIs and interfaces into one coherent tool with a more user-friendly, \nless error-prone, language-based interface  The design and implementation of a domain-speci.c lan\u00adguage \nfor de.ning resource allocation policies.  The evaluation of Poli-C in three di.erent settings: an important \nscienti.c application, an image processing webserver, and a pair of parallel database join imple\u00admentations. \nWe found that using Poli-C yields e.ciency gains requiring the addition of only a few lines of code to \napplications.  The rest of the paper is organized as follows. We begin by describing many of the features \nof our language exten\u00adsions by way of a simple example (Section 2). Then, we discuss some of the high-level \ndesign issues that came up in building Poli-C (Section 3). This is followed by a more detailed description \nof the syntax and semantics of the Poli-C extensions, and the features we have included for man\u00adaging \nnew resources and de.ning new policies (Section 4). Next, through a more in-depth example, we describe \nthe fea\u00adtures of Poli-C that aid composability. Having described all of Poli-C s features, we give a \ndetailed description of the im\u00adplementation of our runtime, which includes a description of our allocation \ntrees, and policd (Section 6). In Section 7 we demonstrate the usefulness of Poli-C, and show some of \nits performance advantages. Finally, we explore related work (Section 8), and conclude with a discussion \nof future direc\u00ad tions (Section 9). 2. Overview This section describes the operation of Poli-C in the \ncontext of a simple example. In particular, we have added a state\u00adment called require to C. The require \nstatement is param\u00adeterized by a list of resource kinds and policies, which we describe below. Based \non this list, the require statement provides both a resource guarantee and a limit to a block of code. \nOur runtime system enforces the property that all e.ects in the block of code under the require statement \nre\u00adceive the resource guarantee and respect the resource limit. Consider the program in Figure 1. In \nthis example, we will use Poli-C to ensure that only two of a program s threads run at any given time, \nand that they do so with exclusive ac\u00adcess to one core each. This may be a desirable arrangement  1 \nvoid *f(void *a) { 2 cores t c; 3 require(c=cores(1)) { 4 int ncores = c.ncores; 5 ... 6 } 7 } 8 9 int \nmain() { 10 require(cpuUtil(0, 1.0), cpuUtil(1, 1.0), 11 cpuUtil(2, 0.0), cpuUtil(3, 0.0)) { 12 for(i= \n0;i <n;i++) 13 spawn f(i); 14 join all(); 15 } 16 } Figure 1. Example program using our extensions to \nC. if, for example, the application programmer wishes to pre\u00advent a thread s cache from being polluted \nby other threads. 2.1 Resource Kinds The main function of this program spawns o. n threads that each \nexecute function f. Poli-C s extensions are used in this program on lines 3 and 10. Assuming that the \ncode in the listing is running on a machine with four cores, the require statement on line 10 indicates \nthat the enclosed code may use up to 1.0, i.e. 100%, of cores 0 and 1, but may not use cores 2 and 3 \nsince the requested utilization for them is 0.0, i.e. 0%. Further, it guarantees that 100% utilization \non cores 0 and 1 will be available for the duration of the code block. Here, cpuUtil is a resource kind. \nThat is, it represents a particular type of resource, and causes the runtime system to use specialized \noperations de.ned for that resource kind. These specialized operations enforce the invariant that a block \nof code does not consume more resources than it has been allocated. The resource kinds are not baked \ninto Poli-C, and new ones may be provided. In Section 3 we discuss who may de.ne resource kinds under \ndi.erent usage scenarios. When there is more than one device of a resource kind, the resource kind is \nparameterized by an identi.er for a spe\u00adci.c device, in this example, a numeric core ID. The map\u00adping \nfrom identi.ers to devices is de.ned when a resource kind is created, which we describe in in Section \n4.2. For now it su.ces to point out that the resource kinds are ini\u00adtialized when a program starts, after \nusing OS calls to query the available hardware, and that cpuUtil is parameterized by core ID and governs \nhow much CPU utilization on a partic\u00adular core is allocated to a block of code. Making requests directly \nthrough resource kinds lacks .exibility. This in.ex\u00adibility is addressed by our policies, described shortly. \n 2.2 Initial State Before a program executes a require statement, it is con\u00adstrained to a limited set \nof resources. These constraints are con.gurable, and are enforced by our system-level daemon, policd, \nwhich we describe in detail in Section 6.3. For now however, in order to simplify discussion of the semantics \nof the require statement for this high-level overview, we as\u00adsume that there is only one process running \nin the system, and that at its start, policd allocates it all available system resources. In Section \n6.3 we discuss how this assumption is eliminated so that Poli-C may provide its invariants to all running \nprocesses. Having made this assumption, before a require state\u00adment is executed, all threads share all \nresources at the discre\u00adtion of the OS scheduler. A require statement has the e.ect of carving out some \nresources from these shared resources for exclusive use in a block of code. That is, if nothing about \na resource kind is mentioned in a require statement, then no change in sharing takes place for that resource \nkind. There\u00adfore, inside of the require statement on line 10, the code has exclusive access to cores \n0 and 1, but if the require state\u00adment had not mentioned the other cores, then those cores would still \nhave been shared among all threads as mediated by the OS. On the other hand, since the require statement \nmakes a request for 0% utilization on cores 2 and 3, code in the require statement is prevented from \nusing them by OS calls made by the implementation of the cpuUtil resource kind.  2.3 Spawning Threads \nInside of the require statement on line 10, the program spawns n threads. Since these threads are spawned \nwithin the require statement, they also have access to the resources speci.ed in the require statement. \nIn particular, they share cores 0 and 1 with each other and with the parent thread, but they may not \nuse cores 2 and 3. When child threads are spawned inside a require state\u00adment, they share the parent \ns guaranteed resources, and in\u00adherit the parent s limitations. That is, the parent thread, and threads \nspawned within the same require statement share evenly the allocation received through the require state\u00adment. \nIf a spawned thread subsequently enters its own re\u00adquire statement, the allocation it receives is no \nlonger shared with any other thread. That is, we maintain the invariant that allocations granted by the \nrequire statement are exclusive until shared by spawned threads, and that shared resources remain shared \nuntil exclusive access is granted by a require statement. If the parent thread leaves the require statement \nwhile the threads spawned within it still exist, the child threads re\u00adtain the guarantees and limitations \nof the require statement. A consequence of this is that the parent thread may not ob\u00adtain a guarantee \nthrough a new require statement for the same resources that were promised to child threads spawned in \nan earlier require statement. Otherwise the resource guar\u00adantees for the child threads could be violated. \nWhen the child threads no longer exist, the parent thread may re-require the resources they were using. \n For example, if the join all() call on line 14 were moved outside of the require statement, the child \nthreads would still have exclusive access to cores 0 and 1, and not would not have access to cores 2 \nand 3. On the other hand, the parent thread would have access to cores 2 and 3, but not 0 and 1 any longer, \nso that the guarantee of exclusive access would still be provided to the child threads.  2.4 Policies \nAt the require statement on line 3 each thread makes a re\u00adquest for exclusive access to a single core. \nHere, cores is a policy. Since requesting a statically determined amount of each individual resource \nis neither convenient nor portable, Poli-C o.ers policies as a way to choose a set of resources at runtime \nbased on availability and contention. Policies are de.ned using a domain-speci.c language that we describe \nin Section 4.3. For now it su.ces to point out that the cores policy examines the unallocated utilization \navailable in all devices of the cpuUtil resource kind, and attempts to re\u00adquire 100% utilization on some \nnumber of them, according to the parameter it is passed. On the other cores, it requests 0% so that they \nwill not be used. The cores policy has the e.ect of requesting that the code in the block have exclusive \naccess to some number of cores. 2.5 Policy Results Since resource levels requested by policies are determined \nat runtime, we provide a facility for communicating the results of a policy back to the application. \nIn particular, policies may de.ne a type to be used for this communication. On line 4, the program reads \nthe .eld ncores of the local c into a local variable. The type cores t is de.ned along with the cores \npolicy, and a variable of this type may be speci.ed in the require statement using the syntax indicated. \nThe variable, having been written by the policy function, is then in scope inside of the require statement. \nBased on the particular resources received through a policy, the application may then make decisions \nabout how to arrange its work. 2.6 Nesting Semantics Since the require statement on line 3 is in the \ncontext of the require statement on line 10, the request must be satis.ed out of the resources listed \nthere. This restriction on nested require statements is true whether or not the nested statement is in \nthe same thread or a child thread spawned in the require statement. Here we also note that the allocations \ngranted are not ad\u00additive. That is, once a particular amount of a resource has been allocated, a larger \namount may not be requested un\u00adtil execution leaves the require statement for the original amount. For \nexample, a thread may not request 0.25 utiliza\u00adtion with one require statement, and then 0.5 with a nested \nrequire statement. When this does occur, we consider it a programming error, and the program aborts with \nan error message1. Other features of Poli-C make this situation easy to avoid. For example, resource \nrequests can also be phrased as a percentage of whatever is currently available. That is, one could write \nrequire(cpuUtil(0,50%)) to obtain exclu\u00adsive access to half of the existing allocation of utilization \non core 0. On the other hand, a nested require statement may make requests for some utilization less \nthat 0.25. In that case, the nested block of code is governed by the lesser amount for the nested require \nstatement until execution leaves it.  2.7 Blocking Only two cores are available after line 10, but an \narbitrary number of threads may be spawned. Since each thread then proceeds to request exclusive access \nto one core, the result is that, unless n is 2 or less, some threads will block waiting for either core \n0 or 1 to be available, and therefore only two threads will run at the same time, on separate cores. \nWhen the resources listed in a require statement are not available, execution blocks until they are. \nTo prevent deadlock, when a thread blocks for any reason other than I/O, it gives up its claim to the \nresources it had already acquired, and attempts to reacquire them when unblocked. It is also important \nto note that for a nested require state\u00adment to succeed, two things must be true. First, there must be \nenough of the resource available in the context, and second, the thread making the nested request must \nbe the only run\u00adning (non-blocked) thread sharing the resources granted by the outer require statement. \nThis restriction is necessary so that there is no need to revoke resources from a thread. That is, we \nassume that threads within a single application be\u00adhave cooperatively. Therefore, in this example, the \nspawned threads block after reaching their own require statements until the parent thread reaches the \ncall to join all(), where it blocks. It signals its blocked children, which wake up and acquire cores \none-by-one. 3. Design Decisions Now that we have presented a brief overview of Poli-C s fea\u00adtures, we \ncan discuss some of the design choices we made in building it. These include the choice between implementing \nPoli-C as a language extension or a library, the system-wide usage model we target, the meaning of a \nresource allocation, the behavior of threads when blocking, and restrictions on the .ow of resources \namong threads. 3.1 Language Extensions vs. A New Library Many of the features of Poli-C could be implemented \neither as language extensions or as part of a well-designed library. Both approaches have advantages \nand disadvantages. In gen\u00aderal, while implementing new features as a library may be possible, we must \nalso consider the design principle that a library which proves very di.cult to use correctly would be \n1 We treat similarly cases where a program attempts to access an unde.ned device, resource kind, or policy. \n better o. as part of the language. In this way the use of its 3.2 Cooperation vs. Enforcement features \ncan be rendered correct-by-construction, thereby re\u00adducing programming errors. Researchers have argued \nthat this is true of a number of existing libraries, e.g., pthreads [11], LRVM [41], explicit locking \n[21], and others. LRVM and explicit locking have the common trait that they require a user to make a \ncall at the beginning of a section of code, and another call at the end of a section of code. However, \nit can be di.cult to remember to place the right calls in the right places, to place only those calls \nactually needed, and to place the ending/closing call at all of the exits from a block of code. Poli-C \nis similarly structured; resource requirements have a beginning and an end. Both the language extension \napproach and the library approach must cope with this problem somehow. A library-based implementation, \nto its credit, would re\u00adquire no custom compiler support, but two problems remain. First, we must ensure \nthat the calls to the library that begin and end a resource requirement are matched correctly. Sec\u00adond, \nwe must generate code to implement our policy DSL. Both of these problems could be addressed by clever \nuses of C++ lambdas, templates, operator overloading, and de\u00adstructors on stack allocated variables. \nHowever, this would then preclude the use of Poli-C for pure C applications un\u00adless we also carried out \nsubstantial by-hand modi.cations of existing application code. On the other hand, if the features of \nPoli-C are imple\u00admented through language extensions in a compiler, then the two problems mentioned above \nare solved by straightfor\u00adward, compile-time code transformations, and Poli-C is still applicable to \nexisting C code. However, the disadvantage of implementing Poli-C with language extensions is the need \nfor this extra support from the compiler, which may compli\u00adcate the tool-chain for an application s build, \nand therefore make the application harder to maintain and distribute. In the design of Poli-C, we have \nchosen to modify the lan\u00adguage instead of providing equivalent features in the form of a library of function \ncalls. We have done this to retain sup\u00adport for existing C code. In particular, Poli-C requires only \nminimal changes to existing applications written in C. We must also justify our choice of C as our base \nlanguage. Aside from the fact that there are many C programs that would ben\u00ade.t from Poli-C s features, \nlanguage use and learning can be hindered by both excessive parsimony, as well as by a glut of features \n[33]. C, due its relatively simple features, is a good choice for trying out new language extensions. \nEven though we have made the choice to implement Poli-C with language extensions, we have made every \nattempt to keep the inter\u00adface to our runtime library simple so that it can be easily integrated into \nother languages. Having made this choice for C, though, we mention that for C++, and possibly also other \nlanguages, a library-based implementation may be more ap\u00adpropriate. Another important issue we consider \nis that of the model under which systems using Poli-C programs are shared. A design wishing to accommodate \nmultiple competing users has di.erent constraints than one wishing to accommodate a single user with \ncompeting applications. For us, this choice is reduced to the choice of who may supply the specialized \noperations used to de.ne resource kinds. In particular, if the specialized operations do not correctly \nuse OS features to mediate access to devices, then Poli-C will fail to provide the guarantees of the \nrequire statement. Under the former sharing model, resource kind de.ni\u00adtions should only be supplied \nor veri.ed by the system ad\u00administrator. However, under the latter sharing model, we must trust that \nthe resource kinds de.ned by the single user behave properly. This makes sense because a single user \nde\u00adrives no bene.ts from a misbehaving resource kind. If an ap\u00adplication supplies a misbehaving resource \nkind, the system administrator or the single user may replace the specialized operations by modifying \napplication source code, or if this is not available, by overriding the application versions with the \ndynamic linker. We have have designed Poli-C with the latter sharing model in mind. However, the former \ncould also be imple\u00admented by requiring elevated privileges to create new re\u00adsource kinds, and by modifying \npolicd to enforce per-user or per-group quotas.  3.3 Resource Allocations It is often the case that \na program can make-do with a range of resource levels. Therefore, in general, resource allocations come \nin two forms, a guarantee of a minimum level of re\u00adsources or a limitation to some maximum level of resources. \nIn Poli-C, the resources made available for use by a block of code through the require statement, what \nwe call an allo\u00adcation, are both a minimum level and a maximum level. At .rst glance, this may seem in.exible. \nHowever, Poli-C s pol\u00adicy functions allow resource levels to be chosen dynamically at runtime. For example, \nan application may provide a pol\u00adicy function parameterized by a range of acceptable resource levels. \nIf the available resources are su.cient to satisfy some value in the range given to the policy function, \nthen the block of code under the require statement receives the guarantee and is subject to the limitation, \nfor which a value was chosen dynamically by the application s policy function. Through our application \ncase-studies, we have found that our policies provide the .exibility needed to allow real-world applica\u00adtions \nto adapt to varying levels of resource availability and contention.  3.4 Blocking Behavior Since our \nrequire statement is not additive, and since threads block at a require statement until su.cient re\u00adsources \nare available, whether already acquired resources are retained or given up when a thread blocks is an \nimpor\u00adtant decision. If threads retain resources on blocking, then deadlock is possible if there is a \nwaits-for cycle. Some such cycles can be prevented at compile time through static analy\u00adsis. However, \neven without considering our policy functions, preventing these deadlocks would require a quite sophisti\u00adcated \n.ow-sensitive, whole-program analysis. Such a cycle can also be avoided by enforcing an order in which \nresources must be requested. However, this violates our composability design goal. In particular, an \napplication developer should be free to use the require statement without worrying about what require \nstatements will be attempted by library code.  Although potential deadlocks are eliminated, other prob\u00adlems \nremain if threads temporarily release resources on blocking. In particular, a thread may acquire exclusive \nac\u00adcess to a device using a require statement with the intention that no other thread should have access \nto it, only to un\u00adknowingly give up exclusive access by blocking in a library call. Additionally, since \nour runtime library must take action when a thread blocks, we must be able to identify every ac\u00adtion \na thread might take that could be construed as blocking. Since our current approach is to use the dynamic \nlinker to override blocking functions in the C Library, we will miss, e.g. threads that busy-wait, or \nwhich access blocking system calls directly through any sort of syscall instruction o.ered by an ISA. \nAt present, we regard the issues presented by this second approach to be less problematic; in fact they \nwere no obstacle to applying Poli-C to the applications we examine in Section 7. In the future, we believe \nthis issue can be addressed by alerting the programmer to the existence of blocking functions inside \nof require statements at compile time, and by allowing the programmer to declare on which blocking operations \nresources should be released.  3.5 Hierarchical Parallelism Threads may grant resources to child threads \nby spawn\u00ading them inside of a require statement. The child threads may then never exceed the resources \nthey receive in this way; they run inside of the parent s require statement for their en\u00adtire lifetime. \nThis design precludes, for example, the follow\u00ading situation. Thread 1 receives an allocation of resources \nthrough a require statement and spawns o. some number of threads to work as a thread-pool. Thread 2 (not \na mem\u00adber of the thread-pool) receives an allocation of resources through a require statement, and wishes \nto have the thread\u00adpool started by Thread 1 perform some work on its behalf. Under our design, it is \nnot possible for Thread 2 to grant its resources to the thread-pool in addition to the resources granted \nby Thread 1. In particular, our design constrains re\u00adsources to be granted only from parent thread to \nchild thread. Resources may not be granted across the thread hierarchy. This restriction simpli.es our \nimplementation, and has not created any problems in our application benchmarks. How\u00adever, in the future, \nit may be worthwhile to create an ab- C Statements Request List Resource Spec Policy Spec Option stmt \nrl rspec pspec o ::= ::= ::= ::= . . . . | require(rl) stmt rdv, rl | pdv, rl | E r(ol, e) | r(d, ol, \ne) p(ol, el) | lv = p(ol, el) {ForChild, ForDescendants, BeforeSpawn, AfterSpawn, Option List Expression \nList ol el e, d lv ::= ::= ::= ::= Private, Shared}o, ol | E e, el | E CExps | percents Clvals r, p . \nIdenti.ers Figure 2. Syntax of our C extensions. straction for thread-pools that is understood by our \nlanguage runtime. 4. Language Extensions In this section, we describe the syntax of the require state\u00adment. \nSince we have already described the semantics of the require statement in Sections 2 and 3, in the rest \nof this section, we focus on the interface provided by Poli-C for de.ning resource kinds, and the domain-speci.c \nlanguage for de.ning policies (e.g. the cores policy used in Section 2) for allocating sets of resources \nbased on resource availability and contention at runtime. 4.1 Syntax Figure 2 shows our addition to \nC s syntax. We add the re\u00adquire statement to the usual set of C statements. The re\u00adquire statement takes \nas an argument a non-empty list of requests, rl. The request list may contain both resource re\u00adquests, \nrspec, and requests based on policies, pspec. The result of the policy function may be assigned to a \nC lvalue. The resources for the require statement may be parameter\u00adized by a C expression specifying \nthe particular device, d, of that resource kind, r. For example, if the resource kind were CPU utilization, \ndisk bandwidth, or network bandwidth, this parameter would specify the CPU ID, the block device, or the \nnetwork interface, respectively. The resources in the list are also parameterized by a pos\u00adsibly empty \nlist of options, ol, and an expression specifying the requested amount of the resource, e, which may \nbe ei\u00adther a C expression or a constant percentage, e.g. 50%. The options list can be used to change \nthe timing and ownership of the resource allocation from the default. The meaning of these will be explained \nin Section 5. The type of the expres\u00ad sion for the amount of the resource requested depends on the resource. \nHowever, the request may be phrased as a raw amount, or as a percentage of the total currently available. \n void register resource(char *name, int n, char **devNames, value t *mins, value t *maxs, struct resource \nops *ops); struct resource ops {uint64 t (*get1)(int d); uint64 t (*get2)(int d); double (*calc)(int \nd, uint64 t t, uint64 t ini1, uint64 t ini2, uint64 t cur1, uint64 t cur2); void (*throttle)(int d, value \nt *v); void (*restrict)(int d, int grant deny); }; Figure 3. Interface for adding custom resource kinds. \nPolicies are also parameterized by a list of options, as well as a list of arguments that are passed \nto the policy functions.  4.2 Resource Kind De.nition Resource kinds may be registered with our runtime \nusing a library call, called register resource. As mentioned ear\u00adlier, resource kind de.nitions are a \ntrusted component of Poli-C, and must maintain certain invariants, which we de\u00adscribe below. Subsequent \nto their registration, resource kinds may be referred to by name in require statements. 4.2.1 register \nresource The interface to resource registration is the function regis\u00adter resource whose prototype is \ngiven in Figure 3. It takes the following arguments. name The name of the resource kind (e.g. cpuUtil) \nused to refer to it in require statements.  n The number of devices of the resource kind.  devNames \n An optional array of strings specifying the names of the devices (e.g. 8:0 for the major:minor identi.er \nfor a block device under Linux, or eth0 for a network device).  mins An array of values indicating \nthe amount of the resource on each device shared by processes that have not yet used a require statement \nto obtain any of it. For example, this parameter can be used to constrain all processes that do not explicitly \nrequest the cpuUtil resource with a require statement to (part of) a single core.  maxs An array of \nvalues indicating the maximum allocation supported by each device.  ops A pointer to a structure that \ncontains .ve func\u00adtion pointers de.ning the specialized operations for a re\u00adsource kind. The value t \ntype is a tagged union with .elds of type char*, uint64 t and double so that the minimum and max\u00adimum \nvalues can be appropriate for the particular device. In a require statement, devices of the resource \nkind may be re\u00adferred to either by an integer giving the index into the array of devices, or by one of \nthe string identi.ers in the names array. We assume that the information required to de.ne re\u00adsource \nkinds can be gathered by a program using features of the OS.  4.2.2 resource ops The functions of resource \nops, each of which is parame\u00adterized by a particular device, have the following meanings. The get1 and \nget2 functions are used to probe the device for cumulative usage statistics. For example, in the case \nof I/O device bandwidth, get1 may return the number of bytes written. Usually only get1 is de.ned, but \nget2 may also be needed when a ratio is being measured. For example, cal\u00adculating the cache-miss rate \ninvolves calculating the ratio of last-level cache misses to cache references. The calc func\u00adtion may \nbe used to calculate a usage rate. Given a duration of time in nanoseconds, t, and the results of get1 \nand get2 at the beginning and end of the time interval, the rate of us\u00adage of the resource can be calculated. \nHowever, this is not required; calc may produce whatever value is meaningful for the device. The throttle \nand restrict functions make use of any available OS features for limiting a thread s use of a device. \nThe throttle function may wrap an OS function for limiting a thread to using a device at a particular \nrate given by the parameter v. Depending on the grant deny parameter, the restrict function grants or \ndenies access by the thread to a device. We distinguish between these two cases for the following reason. \nIf the throttle function is not supplied, any throttling needed for the device (e.g. if a thread has \nrequired only a fraction of the resources available from a device) is supplied by a monitor thread that \nis part of our runtime, which periodically interrupts the thread and causes it to be blocked until its \nresource usage matches the prescribed rate. However, if a thread has only ever required 0% or a 100% \nof a device, the monitor thread does not run. Therefore, for resource kinds without a throttle function, \nthe job of the restrict function, is to maintain the invariant that the OS does not allow a thread to \nuse a device on which it has a 0% allocation. For example, when a thread requests a 0% allocation on \na device of resource kind cpuUtil, the restrict function ensures that the core is removed from the thread \ns CPU set. 4.2.3 Resource Kind Initialization When a program starts, it queries policd for the resource \nkinds that have already been de.ned. If it requires addi\u00adtional resource kinds, it gathers the necessary \ninformation from the OS, and calls register resource. Then, regis\u00adter resource sends the de.nition of \nthe new resource kind to policd, which proceeds to track the resource kind for the entire system. When \na new resource kind is registered using the register resource function (or received from policd), the \nresource kind de.nition is placed in a hash table keyed by the resource name. Then, when a require statement \nmen\u00adtions the name of the resource kind, the name is used to look up the de.nition in the hash table. \n An advantage of this design is that porting our language extensions to a new OS or architecture is \nsimply a matter of reimplementing the functions in resource ops for each resource kind. When the use \nof our extensions does not a.ect the semantics of a program, only the performance, it is safe to move \nan application to a new platform on which only a subset of the resource kinds it refers to are de.ned. \n4.2.4 Examples Consider the cpuUtil resource kind used in the initial exam\u00adple. On Linux, for this resource, \nwe de.ne the get1, calc, and restrict .elds of resource ops. get1 returns the total time spent by a thread \non a CPU. calc calculates the percent utilization of a core by a thread, and restrict adds or re\u00admoves \nthe core from a thread s CPU set. Because the throt\u00adtle .eld is left unde.ned, a monitor thread from \nour runtime is responsible for keeping the thread from using more than its allocation2. Many of the common \nhardware devices .t this way of looking at resources. On Linux, the cgroups [2, 34] inter\u00ad face allows \n.ne-grained control over the share of disk and network utilization each group may use. For memory, sys\u00adtem \ncalls exist to keep particular pages from speci.c banks resident in memory. For caches, various patches \nto Linux im\u00adplement a page-coloring VM subsystem [42] that expose an interface that could be used here \nto grant threads exclusive access to parts of caches, however we have not yet imple\u00admented this. It is \nalso possible to use this formulation of resources to act as a traditional semaphore or monitor. That \nis, instead of allocating concrete hardware resources, resource kind de.\u00adnitions can be written to mediate \naccess to abstract software resources, for example OS resources like .le descriptors, or user-level resources \nlike simultaneous access to a particular software module. In Section 7 we demonstrate one such ex\u00adample \nof the use of these virtual resources.  4.3 Policies As mentioned earlier, requesting access to amounts \nof re\u00adsources based on an explicitly given list of devices is neither convenient nor portable. On the \nother hand, it is useful to make requests, not only for some amount of a particular de\u00advice, but also \nto make requests for sets of allocations from groups of devices depending on availability and contention. \nFor example, a high-level task may require two cores, but may or may not care which cores they are. Therefore, \nwe in\u00adclude in Poli-C a way of specifying these sorts of requests, which we call policies. Policies are \nde.ned using a simple, C-embedded DSL we have developed for the purpose. Fi\u00ad 2 On Solaris, a call could \nbe supplied here for the throttle function in particular one that uses the Fair Share Scheduling feature \nof its Zones [3]. CDecls ::= ... | policy id(own to, void * out,...) {Cstmts} Cstmts ::= ... | res iter(rid) \nCstmts | dev iter(r, did) Cstmts Clvals ::= ... | d.d.eld Cexps ::= ... | dev(r, d) | dev count(r) | \nmax available dev(r) | min waiters dev(r) | total waiters() d.eld .{id, available, request, ownership, \nwaiters} r, d ::= Cexps rid, did . Identi.ers Figure 4. Syntax of our policy de.nition DSL. nally, unlike \nresource kinds, policies are not trusted. That is, new policies can be declared and used by unprivileged \nusers. 4.3.1 Syntax Figure 4 shows the syntax of our policy DSL. We add to C an additional declaration \nfor de.ning policy functions. The .rst two formal parameters to the policy function are manda\u00adtory. The \n.rst, o, gives the requested resulting ownership of the resource set to be computed by the policy function, \ni.e. Shared, Private, or Default. The meanings of these owner\u00adship options are described in Section 5. \nThe second argument to the policy function, out, allows the policy to pass policy\u00adspeci.c information \nabout the results of the policy function back to the application, for example through the binding for \nc in require(c=cores(n)). The remaining arguments are the policy-speci.c parameters provided by the application \nthrough the require statement (e.g. the n of cores(n)). Inside of a policy de.nition, two statements \nare o.ered in addition to the usual C statements: one for iterating through resource kinds (res iter), \nand the second for iterating over the devices of a resource (dev iter). Inside of the block of code under \nres iter(r), r is a pointer to the name of the resource. In dev iter(r,d), r speci.es the name of the \nresources whose devices will be iterated over. Inside the block under the statement, d is bound to an \ninstance of a device structure that contains the following .elds. The id .eld gives either the string \nthat was supplied for the device name if there was one, or a numeric id otherwise. The available .eld \ngives the amount of available resource on the device. The request .eld may be written by the policy function \nto indicate how much of the resource should be requested from the device by the policy function. The \nownership .eld may be written by the policy function to indicated the desired ownership for the requested \namount. Finally, the waiters .eld gives the number of threads waiting for resources on the device to \ncome available.  Additionally, a number of interesting functions are made available by our compiler \ninside of policy de.nitions. The dev(r,d) function returns an instance of the device structure described \nabove, given a string constant for the resource r, and a device identi.er d. The device identi.er can \nbe either a string constant for the name of the device, or an integer giv\u00ading the index for the device. \nWe supply the dev() function so that a policy author can .nd the device structures without us\u00ading our \niteration functions. The dev count() function returns the number of devices for a resource kind. The \nfunctions max available dev() and min waiters dev() return the de\u00advice of a resource kind with the most \nresource available and the fewest waiters, respectively. Finally, the function to\u00adtal waiters() returns \nthe total number of threads waiting for resources across the entire system.  4.3.2 Semantics As the \npolicy de.nition executes, it reads the available and waiters .elds, and writes the request and ownership \n.elds of various device structures. Writing the .elds of de\u00advice structures either bound by the dev iter \nstatement or obtained through the dev function is meaningful. In particu\u00adlar, those amounts of those \ndevices of those resource kinds will be requested by the policy function. If the policy cannot be satis.ed \nby the resources it .nds in the device structures, the policy function must return PolicyFailure. Otherwise, \nthe policy function must return PolicySuccess. The e.ects of require-ing a policy are as follows. Atom\u00adically, \nour runtime attempts to execute the e.ects of a re\u00adquire statement for each of the device structures \nthat were written by the policy function. If the policy function returns PolicyFailure, or if one of \nthese implied require state\u00adments fails, then the executing thread must give up its re\u00adsources and block, \nre-executing the policy function before retrying the implied require statements. Further, the resource \nallocations obtained by a thread through a policy are treated as a unit. They are allocated as a unit, \ndeallocated as a unit, and temporarily given up as a unit when threads block. That is, normally an allocation \nis temporarily returned if none of the threads sharing it are currently running. However, if an allocation \nwas obtained as part of a policy, it is only returned if no threads sharing any of the allocations obtained \nfor the policy are currently running. Figure 5 shows how the cores policy is de.ned using our policy \nDSL. It simply iterates through the available cores, looking for those on which 100% utilization is available. \nIt is easy to see how this policy could be modi.ed to achieve a number of di.erent policies, for example \nallocating some minimum number of cores, and returning the resulting core map in the out parameter. As \nwith resource kind registration, when our compiler front-end .nds a function matching the right signature \nfor 1 typedef struct { 2 int ncores; 3 } cores t; 4 5 policy cores(own t o, void *out, int n) { 6 int \ni, found = 0; 7 cores t *c = (cores t *)out; 8 9 c->bits = 0; 10 dev iter (\"cpuUtil\", d) { 11 if (d.available \n== 1.0 &#38;&#38; found < n) { 12 d.request = 1.0; d.ownership = o; 13 if (c) c->ncores++; 14 found++; \n15 } 16 else { 17 d.request = 0.0; d.ownership = o; 18 } 19 } 20 21 if (found < n) return PolicyFailure; \n22 return PolicySuccess; 23 } Figure 5. The de.nition of our cores policy in terms of the basic resource \ncpuUtil. a policy de.nition, a pointer to the policy function is placed in a hash table keyed by the \npolicy name. When a require statement mentions a policy name, the name is used to look up the policy \nfunction in the hash table. 5. Options for Composability We have implemented a number options that modify \nthe require statement s default behavior. In this section, we take a look at an example that demonstrates \nPoli-C s ability to compose parallel libraries. In particular, we will use a few of Poli-C s options \nto the require statement to cope with unavailable code and idiosyncrasies of a commonly used parallel \nlibrary. In this example, a program divides a workload into several partitions, and spawns a thread to \nwork on each partition. Each of these threads then attempts to work on pieces of each partition in parallel. \nThe structure of this example is analogous to the structure of the scienti.c application we use to evaluate \nPoli-C in Section 7. 5.1 Without Poli-C Figure 6(a) shows the organization of this program without the \nuse of Poli-C or the parallel library (in this example we ll use OpenMP). Instead of C, we use pseudocode \nhere in order to focus on the structure of the program. The main function uses the partition calc function \nto partition the workload C into a number of pieces after examining the number of cores, de.ned by the \nparameter NCORES. Then for each of the partitions, the main function spawns a thread to run the function \ndo part. The do part function iterates through the  void *do task(task t *t): task(t); void *do part(part \nt *P): foreach t in P: spawn(do task, t); int main(): calc t *C; partition calc(C,NCORES); foreach p \nin C: spawn(do part, p); (a) No Poli-C, No OpenMP void *do task(task t *t): require(cores(1)): task(t); \nvoid *do part(part t *P): require(cores(P.cnt)): foreach t in P: spawn(do task, t); int main(): calc \nt *C; cores t c; require(c = min cores(1)): partition calc(C, c.ncores); foreach p in C: spawn(do part, \np); (b) With Poli-C, No OpenMP void *do part(part t *P): require(cores(BeforeSpawn, P.cnt)): omp set \nnum threads(P.cnt); require(cores(AfterSpawn, Private, 1), cores(ForChild, 1)): omp calc(C); int main(): \ncalc t *C; cores t c; require(c = min cores(1)): partition calc(C, c.ncores); foreach p in C: spawn(do \npart, p); (c) With Poli-C, With OpenMP Figure 6. Example program with Poli-C working with OpenMP. tasks \nthat must be performed for the partition P, and spawns a thread to run each in parallel. This design \nhas two main problems. First, in such an application NCORES is usually provided by the user or pro\u00adgrammer \nas either a pre-processor macro, an environment variable, a command-line argument, or by making the right \nsystem call. This is a problem because there is no straight\u00adforward way for the application to know at \nthis stage what other applications are running on the system, and to adjust the partitioning of C accordingly. \nSecondly, the OS sched\u00aduler is unaware that the tasks spawned to work on the same partition are cooperating \nwith each other and competing for resources with the threads for other partitions. In particular, the \nthreads for each partition may wish to meet up at barri\u00aders, or may rely on sophisticated cache optimizations. \nCom\u00adpeting with the threads of other partitions increases the wait time at barriers, and pollutes a thread \ns cache.  5.2 With Poli-C Figure 6(b) shows how the require statement of Poli-C can be used in the program. \nIn the main function, we use the policy min cores. This policy attempts to acquire exclusive access to \nas many cores as possible, with a minimum given by the argument to the policy. If the minimum is not \navail\u00adable the program blocks. When more cores come available, the program is signaled by policd, and \nexecutes the policy function again. The result of the policy function is assigned to c. Its .eld ncores \ngives the number of cores acquired in the require statement. Now, the calculation C can be partitioned \nbased on the actual number of uncontended cores that the program is guaranteed access to. As before, \none thread is spawned for each partition. Each of these threads runs the do part func\u00adtion. This function \n.rst uses the require statement to gain exclusive access to a number of cores equal to the number of \ntasks in the partition3. Since the tasks for each partition will have exclusive access to a core for \neach task, we avoid the problems with barriers and caches su.ered by the original program. Finally, in \ndo task a require statement is used to acquire exclusive access to one core, which has the e.ect of pinning \nthe thread executing task(t) to a single core. This is likely unnecessary; since the core is uncontended, \nthe Linux scheduler will avoid migrating the thread to a di.erent core, but we require the core anyway \njust to be sure.  5.3 Poli-C with GNU OpenMP Finally, in Figure 6(c), we consider what happens when, \nin\u00ad stead of do part manually spawning threads and managing the tasks, the program uses OpenMP to process \neach parti\u00adtion. For this example, we consider the GNU implementation of OpenMP, in which a new thread-pool \nis spawned for each new thread that begins an OpenMP job. Given that, without the Poli-C features that \nwe describe below, the same prob\u00adlems would exist here as with the code in Figure 6(a). In this con.guration, \nthe main function is as before. The do part function, however, is substantially di.erent because we use \noptions to the require statement to manage OpenMP. In particular, it is not incorrect to do so, but we \nwould like to avoid making another request for a single core with the require statement in do part if \nonly a single core was ob\u00adtained with the require statement in main. Therefore, in the .rst require statement \nin do part, we use the BeforeSpawn option. It ensures that the stated resources or policies won t be \nput in place until just before the .rst thread is spawned inside of the require statement. GNU OpenMP \ndoes not spawn a thread-pool if it is instructed to use only one thread. Thus, the require statement \nis only triggered if OpenMP is to use multiple threads. 3 We assume that it is an invariant of partition \ncalc that the number of tasks per partition will not be greater than its second argument, which speci.es \nthe number of cores.  Option Description ForChild ForDescendants Directs spawned threads to make a re\u00adsource \nrequest Directs all descendant threads to make a resource request BeforeSpawn AfterSpawn Delays e.ects \nuntil just before a child thread is spawned Delays e.ects until just after a child thread is spawned \nPrivate Shared Not shared with spawned threads Shared with anyone Table 1. Options for the require statement. \nNow, as in Figure 6(b), we would like to pin each OpenMP thread to its own core. Unfortunately, we no \nlonger have access to a convenient place to put the default require statement, as the OpenMP threads \nare spawned from library code that we do not have access to. However, we can use options to the require \nstatement to achieve the same e.ect. In the second require statement in do part, there are two policies. \nIn the .rst policy we use the options AfterSpawn and Private. Here, we use AfterSpawn so that the require \nstatement won t take e.ect until just after the .rst thread is spawned inside of it. We use the Private \noption so that the parent thread can retain exclusive access to one of the cores allocated by the .rst \nrequire statement. The net result is that just after spawning the .rst thread of the OpenMP thread\u00adpool, \nthe parent thread will get exclusive access to a single core that is not available for allocation to \nany of the thread\u00adpool threads. Finally, in the second policy in the second require statement, we use \nthe ForChild option. This simply directs any child thread spawned in the require statements to apply \nthe cores policy just after they begin. This second require statement has the net e.ect of pinning each \nof the OpenMP threads to its own core. Here we must note that a di.erent OpenMP implementa\u00adtion may have \ndi.erent idiosyncrasies, and therefore require a di.erent arrangement of require statements and options. \nAlthough this situation is not ideal, we feel that it is more portable than the currently available alternatives, \ni.e. relying on by-hand con.guration on each platform the code runs on, or creating custom builds of \nthe di.erent OpenMP imple\u00admentations, as Lithe would require, for example.  5.4 Additional Options This \nexample has shown how the use of Poli-C can avoid the performance problems that may happen when parallel \nli\u00adbraries are composed. The require statement accepts a few other options. Their names and e.ects are \nlisted in Figure 1. Of these, the Shared option possibly requires some explana\u00adtion. A thread that uses \nthe Shared option in a require state\u00adment is subject to the resource limitation for the requested allocation, \nbut receives no guarantee of exclusivity. That is, 1 // Basic resource kind before translation 2 require(cpuUtil(0,100%)) \n{ 3 ... 4 } 1 // Basic resource kind after translation 2 require push(\"cpuUtil\",0,Default,PC,1.0); 3 \n... 4 require pop(); 1 // Policy before translation 2 require(c = cores(2)) { 3 ... 4 } 1 // Policy \nafter translation 2 policy push(\"cores\", Default, &#38;c, n); 3 ... 4 pop policy(); Figure 7. Poli-C \nsyntax before and after translation. the allocation may also be shared with any other thread that requests \nthe resource with Shared ownership. 6. Implementation The implementation of Poli-C has three parts. First, \nthe lan\u00adguage extensions are implemented using a source-to-source compiler for C. Second, within a process, \namong threads, the semantics of Poli-C are enforced by a runtime library that tracks resource allocations \nusing an allocation tree. Fi\u00adnally, we have implemented a system-level daemon, policd, to enforce the \nsemantics of Poli-C among all processes run\u00adning on a system. In this section, we describe each of these \nparts in turn. 6.1 Language Extensions Our extensions to C are implemented in about 1700 lines of OCaml \nusing the CIL source-to-source compiler [35]. The Poli-C compiler front-end has two primary functions. \nFirst, it must translate our require statement into calls into our language runtime. Second, it must \nidentify policy functions, compile them to C, and register them with our language runtime. 6.1.1 require \ncompilation We explain the translation of the require statement by way of two example statements, one \nusing a resource kind and one using a policy. Consider the code fragments in Figure 7. In the .rst two \nfragments, the program requests 100% of de\u00advice 0 of resource kind cpuUtil using the default semantics. \nThis is translated into the require push call in the second fragment. Here, cpuUtil is given as a string, \nthe device is speci.ed as an integer, the options for the require statement are passed as an enum value \n(in this case Default), and the value requested is speci.ed with a .oating point value, with the code \nPC, which indicates the value is a percent.  In the third and fourth fragments, the translation is per\u00adformed \nfor a policy. Our compiler can identify a request for a policy because there must exist in the program \na function with the same name as the policy. In the third fragment, the program requests any two cores \nusing the default options. This is translated into the call to the policy push call in the fourth fragment. \nHere, the name of the policy is given as a string, there are no options, the binding for the result of \nthe policy function is made by passing the address of c, and the arguments to the policy function follow. \n 6.1.2 Policy function compilation Since there may be an arbitrary number of arguments to a policy function, \npolicy push is a variable argument func\u00adtion. Furthermore, since the call to the policy function may \nneed to be delayed if the BeforeSpawn, AfterSpawn, ForChild, or ForDescendant options are used in the \nrequire statement, the arguments to a policy function may need to be saved for later. To accomplish this, \nfor each policy function, our compiler generates helper functions to save and restore the arguments. \nWhen a policy is registered with the Poli-C runtime by adding it to a hashtable keyed by the policy name, \nnot only do we add a pointer to the policy function it\u00adself, but also to the helper functions. This way, \npolicy push can look up all of the functions it may need. Policy functions are identi.ed by the compiler \nas any function having a compatible type, i.e. the return type is the enum type for the PolicyFailure \nand PolicySuccess return codes, and the .rst two argument types are as described in Section 4.3. Two \nelements of our policy DSL require trans\u00ad lation. First, the res iter and dev iter are compiled to sim\u00adple \nfor-loops. Second, writes to the request and ownership .elds of device structures are compiled to calls \nto functions that store the written values into a table. If the policy func\u00adtion returns PolicySuccess, \nthe values from this table indi\u00adcate what resources to request.  6.2 Language Runtime The runtime for \nour extensions is implemented in about 11k lines of C, of which about 2400 lines are the Linux-speci.c \nresource kind de.nitions. The runtime operates by using the dynamic linker to intercept calls into the \npthreads library along with selected system calls. In particular, the runtime must take action when threads \nare created, blocked, and destroyed. In order to implement the semantics described in Sec\u00adtion 2, for \neach resource kind and device, each thread main\u00ad tains a stack that mirrors the nesting structure of \nthe re\u00adquire statements, which we call a resource stack. Further\u00admore, since threads inherit the allocations \nof the threads that spawned them, and these allocations may outlive the dura\u00adtion of the require statement \nin the spawning thread, for each resource kind and device, we must also maintain a ref\u00aderence counted \ntree of allocations, which we call an alloca\u00adtion tree. In order to explain the operation of our language \nFigure 8. Starting at top-left, actions taken on the stacks and allocation tree for entering require \nstatements and spawn\u00ading threads. runtime, we must explain what happens to a thread s re\u00adsource stacks, \nand allocation trees when various operations occur, in particular: entrance into a require statement, \nexit from a require statement, thread blocking, thread spawning, and thread exiting. We explain most \nof these operations by way of the exam\u00adple diagram in Figure 6.2. The diagram shows the modi.ca\u00ad tions \nmade to the stacks and allocation tree for a single re\u00adsource R by the above operations. In the diagram, \nthick, sold arrows labeled by threads and actions indicate state transi\u00adtions. The parenthesized numbers \nnear these arrows are la\u00adbels that we use to refer to the transitions in the text. The thin dotted and \nnon-dotted arrows indicate di.erent kinds of ref\u00aderences among resource stack elements and allocation \ntree nodes. The distinction between dotted and non-dotted refer\u00adence will be described below. Transition \n1: The initial state is in the upper-left corner. Thread 1 s R resource stack contains one element pointing \nto the root node of the allocation tree for R, which contains the maximum available amount of the resource, \nG. If G is found to be 0, and the root node has no children, then the runtime queries policd for resource \navailability. In the .rst step, Thread 1 enters a require statement, making a request for the amount \na of R. Since we will suppose that G is large enough to accommodate the request for a, in response, our \nruntime library pushes an element onto Thread 1 s resource stack that contains a pointer to a new allocation \ntree node. The new allocation tree node contains the value of the new allocation. Its parent is the root \nof the allocation tree. Since a of the resource is held in the new node, the root node s value is decremented \nby a. Further, if resource kind R has a throttle or restriction function de.ned, it is called for Thread \n1 now.  Reference Counting: Notice also that the reference from the original resource stack element \nto the root node of the allocation tree is now represented by a dotted arrow. This in\u00addicates that the \nallocation in that node is no longer used by the thread. An arrow may become dotted either, as in this \ncase, when it comes from a non-top element of the resource stack, when it does come from the top element \nbut the thread is blocked, or when it comes from an allocation node with no incoming non-dotted arrows. \nFurther when an allocation node has no incoming non-dotted arrows, it returns its al\u00adlocation to its \nparent node. In summary, a reference repre\u00adsented by a dotted arrow indicates that the allocation node \nis not currently being used by the holder of the reference, but that it may do so again at some point \nin the future. Transition 2: In the second step of the diagram, Thread 1 spawns a thread. Since Thread \n2 is spawned inside of the require statement initiated by Thread 1, Thread 2 shares the allocation. This \nis indicated by copying the top entry from Thread 1 s stack to be the .rst entry on Thread 2 s stack. \nFurther, throttle and restriction functions are called for Thread 2 using a when it begins running. Transition \n3 &#38; 4: In the next step, Thread 1 blocks. It remembers the value of its allocation, and switches \nits reference from a non-dotted to a dotted arrow. Since the allocation node still has a non-dotted arrow, \nit does not return the allocation to its parent; it is still being used by Thread 2. Next, Thread 2 enters \nits own require statement for an amount b. Since we suppose that a is su.cient to accommodate the allocation, \nin response the runtime library pushes a new element on Thread 2 s resource stack, and creates a new \nallocation tree node, just as it did for Thread 1 s require statement. Transition 5: Following Thread \n2 s require statement, it blocks. Since Thread 2 was the only thread using its most recent allocation, \nthe allocation node returns the value b to its parent, and changes its reference to its parent from non\u00addotted \nto dotted. Now, however, the parent node also no longer has any incoming non-dotted references. This \nmakes sense since neither Thread 1 nor Thread 2 is running. There\u00adfore, its value is also returned to \nits parent, and so the root allocation node returns to its original value G. Transition 6 &#38; 7: Once \nThread 2 has blocked, Thread 1 unblocks. Using the remembered value from the top of its stack, through \nthe allocation tree node referenced there, it reacquires the allocation from its initial require statement. \nFollowing this, Thread 1 attempts a second require state\u00adment for the amount c. Since we suppose that \na is su.cient to accommodate the allocation, a new stack element, and al\u00adlocation tree node are created \nas usual. Transition 8: Then, Thread 2 is unblocked. Two things could happen at this point. First, if \na-c is less than b, then Thread 2 must block. When blocking due to insu.cient re\u00adsources, a thread blocks \non a condition variable held in the .rst allocation tree node it encounters during a traversal to\u00adward \nthe root that has incoming non-dotted references, but which fails to satisfy the request. In this case \nthis would be the allocation tree node holding a-c. However in our dia\u00adgram, we suppose that a-c is large \nenough to accommodate b, and the state is updated accordingly. Epilogue: To .nish our example, although \nit is not pic\u00adtured in our diagram, when a thread leaves a require state\u00adment, it pops o. its topmost \nstack element. If the allocation tree node it refers to no long has any references, its allo\u00adcation is \nreturned to its parent, and the node is deallocated. This process repeats recursively up the tree. If \na thread exits before leaving a require statement, its resource stacks are emptied following this same \nprocedure. Options and Policices: We must mention a few more de\u00adtails that were not covered by the above \nexample. First, the timing options to the require statement are implemented by maintaining multiple stacks \nfor each resource kind and de\u00advice, one for each of the options. For the ownership options, allocation \ntree nodes maintain reference counts and .ags to distinguish among, and keep state for, Private, Shared, \nand Default allocations. Second, as mentioned above, allocations received by way of a policy are treated \nas a unit. This is accomplished by placing each allocation for a policy in a linked list. When deciding \nwhether or not to return an allo\u00adcation to the parent node, we simply traverse the list, check\u00ading reference \ncounts. Finally, we note that our runtime li\u00adbrary is protected by a big global lock. In applying Poli-C \nto highly parallel applications, this did not cause any scaling problems, however it would not be di.cult \nto construct ar\u00adti.cial examples in which it would. We leave elimination of this potential bottleneck \nfor future work. 6.3 policd The above description of our runtime system is e.ective at maintaining the \ninvariants of Poli-C only within a single multi-threaded process. To enforce these invariants across \nan entire system, we use a system-level daemon process called policd. policd has two key functions. First, \nit performs bookkeeping for the resource kinds. Second, it monitors the system for process creation and \nexit events in order to maintain Poli-C invariants. We use policd to make up for the absence on Linux \nof an existing, query-able service that tracks resource allocations. If an OS already has such a service, \nthen policd would likely be unnecessary.  6.3.1 Bookkeeping Programs using Poli-C communicate with policd \nover a lo\u00adcal, UNIX socket using the PASSCRED option to allow mes\u00adsage receivers to verify the identity \nsenders. When a Poli-C program begins, it requests a list of the available resources from the daemon. \nIf it de.nes some resource that does not exist, it is registered with the daemon by sending a descrip\u00adtion \nof the resource over the socket. Then, the daemon tracks how much of the resource is distributed to each \nprocess run\u00adning in the system. When a program executes a require statement, if it has an insu.cient \namount in the root node of its local allocation tree, it makes a request to the daemon. If an insu.cient \namount is available, the program requests that the daemon notify it when the amount changes, and blocks \non a select call on the socket. When the root node of the al\u00adlocation tree has no children, the amount \nof the allocation is returned to the daemon. To aid in writing policy functions, a process can also request \nthat the daemon send it the number of threads currently awaiting a noti.cation about resources from the \ndaemon. 6.3.2 Monitoring When a new resource kind is created, a minimum level of the resource is speci.ed \non each device. This minimum level is shared among processes that have made no resource re\u00adquests. By \nconstraining processes that have made no re\u00adquests to a limited set of resources, we are able to make \nguarantees to those that have made requests. To enforce this minimum level, policd subscribes to events \nemitted by Linux when processes are forked and when they exit. When policd starts, every existing process \nis made to share the minimum resource level4, as is each new process that is forked. When a process is \nallocated resources through the require statement, it no longer uses the minimum amount that was set \naside. Further if a process exits without returning its resources, the daemon notices the exit event \nand reclaims them. 7. Evaluation We evaluate Poli-C by looking at the ease with which it enables performance \nimprovements. We have made small changes to a few applications, which we treat below as case studies. \nFirst, we look at a state-of-the art implementation of sparse matrix QR decomposition, which is used \nin a number of di.erent scienti.c and .nancial applications. Further, we use Poli-C to improve the performance \nof three di.erent web services using the Apache http server. Finally, we apply Poli-C to a state-of-the-art \nimplementation of multicore database join algorithms. We ran all of our experiments on a large 4 This \nnot possible e.g. for Linux kernel-mode threads that are bound to speci.c resources when spawned by the \nkernel. server machine: a 4-socket, 64-core AMD Opteron 6276 running at 2.3GHz having 256GB of main memory \nin four banks and a 2-disk RAID array running Debian Linux with a 3.2.2 kernel. The topology of the machine \nis such that each NUMA-node, as reported by libnuma, consists of 8 cores, which have private L1 caches, \nand shared L2 and L3 caches. 7.1 QR Decomposition We applied Poli-C to a state-of-the-art implementation \nof QR decomposition [14]. QR decomposition is used in many important applications for linear, least-squares \n.tting. The algorithm used in this implementation creates a tree of sub\u00admatrices, each of which may be \nprocessed in parallel. Par\u00adallel linear algebra operations may then be used to operate on each of these \nsub-matrices. Similar strategies are also beginning to be used for other matrix factorization algo\u00adrithms \n[31]. Creation of, and operation over, the tree of sub-matrices is handled by Intel s TBB library [38]. \nThen, the linear algebra operations are performed over the sub-matrices by BLAS matrix subroutines [16]. \nAny implementation of the BLAS subroutines can be linked with the application. We used the Intel MKL \n[25], which uses the GNU OpenMP library to parallelize the matrix operations. By default, the number \nof threads used by TBB, and the number of threads used by OpenMP are both chosen to be equal to the number \nof cores in the system. This is problem\u00adatic, however, because Linux will cause di.erent OpenMP tasks \nworking on di.erent sub-matrices to share the same core. This results in poor performance as threads \npollute the caches of other threads. Further, there are many additional context-switches, and teams of \ncooperating OpenMP tasks fail to arrive at barriers at the same time. To avoid these problems, an end-user \nwould have to con\u00ad.gure TBB and OpenMP by hand to set the number of threads they use to values that prevent \nthis interference. Fur\u00adther, even if these parameters are set to good values, two problems still remain. \nFirst, there is still no assurance that the kernel will not schedule threads in a way that causes undesirable \ninterference. Second, this by-hand con.guration must be redone on each new platform. In our view, an \nend\u00aduser is unlikely to attempt either an initial good con.gura\u00adtion, or any recon.guration for a new \nplatform, especially if the QR decomposition library is only called deep within an application. 7.1.1 \nPoli-C Policy Any automatic approach that prevents the OpenMP jobs from interfering with each other would \ngo a long way to\u00adwards solving the performance problem described above. We would like an automatic approach \nthat requires no end\u00aduser intervention, but we are willing to tolerate changes to a few lines of code. \nTo that end, we devise a Poli-C policy, called numa nodes. The policy is very much like the cores policy \ndescribed in Figure 9. Summary of results for the QR decomposition benchmark.  Matrix LLC Ctxt CPU Time(s) \nMethod Misses Switch Switch landmark Baseline 9.0x107 4.1x105 9.3x104 19.97 ByHand 2.1x107 8.2x103 2.0x102 \n2.65 Poli-C 8.1x106 3.3x103 2.3x102 2.64 deltaX Baseline 2.3x109 9.9x106 2.3x106 188.94 ByHand 2.0x108 \n1.1x104 1.6x102 8.08 Poli-C 1.2x108 6.0x103 2.3x102 7.26 ESOC Baseline   > 1000 ByHand 7.3x108 3.4x104 \n1.7x102 42.01 Poli-C 4.2x108 2.5x104 2.0x102 42.51 Section 4.3 with the exception that instead of attempting \nto acquire exclusive access to some number of cores, it uses libnuma to attempt to acquire exclusive \naccess to some number of NUMA-nodes. Applying Poli-C to the QR decomposition for the man\u00adagement of cores \nrequires only the addition of 2 require statements. In particular, the require statements we added when \nusing the numa nodes policy follow the structure laid out in the discussion of options to the require \nstatement in Section 5. 7.1.2 Results We ran the QR decomposition on three large sparse matrices taken \nfrom the MatrixMarket [12] maintained by the US Na\u00ad tional Institute of Standards and Technology. We \nran the QR decomposition on matrices using the default con.guration with default parameters (Baseline), \nthe default con.guration with hand-tuned parameters (ByHand), and Poli-C. The table in Figure 9 shows \nthe results of running the QR decomposition. We used the matrices landmark , deltaX and ESOC as our workloads. \nFor each of the methods and column, the table lists the average of 10 runs. The standard deviation was \nnever bigger than 10% and usually under 1%, so we omit the exact values. For each matrix, the .rst line \nshows the out-of-the-box performance of the workload using the default options for the numbers of TBB \nand OpenMP threads (Baseline). The second line gives the best performance obtained when the numbers of \nTBB and OpenMP threads were set by hand (ByHand). To arrive at the parameters for the ByHand con\u00ad.guration, \nwe did not conduct an exhaustive search. Instead, we took the suggestion of the developers of the QR \nde\u00adcomposition and chose the parameters so that their product would equal the number of cores. For these \nexperiments we chose 4 TBB threads and 16 OpenMP threads. The third line gives the performance of Poli-C. \n7.1.3 Discussion On our 64-core machine, the default con.guration performs quite poorly. Communication \nand thread creation overhead, in addition to the interferences described above, degrade performance to \nthe extent that we saw no point in waiting for the benchmark to .nish running on the ESOC matrix. Clearly, \nwithout some tool to help the programmer, by-hand tuning would be necessary for this library to be of \nany use on a large machine. We have found that our numa nodes policy performs as well as by-hand tuning \non this machine. In order to achieve this on a range of machines, it may be necessary to devise a more \nsophisticated policy. One of the key advantages of the design of Poli-C is that this policy information \nis transparent and well-organized through the use of our policy DSL. In particular, the author of a library \nlike this QR decomposition could develop a suggested policy to distribute alongside the library itself. \n7.2 Image manipulation web server In this case study we investigate a web server that manip\u00adulates client-uploaded \nimages. The .rst manipulation sup\u00adported by the server is an image blur, consisting of one rel\u00adatively \nexpensive operation. The second manipulation gen\u00aderates several di.erent resizings of an image, consisting \nof multiple relatively inexpensive operations. We use the Apache web server [4], the cgic CGI li\u00ad brary \n[5], and the ImageMagick image manipulation li\u00ad brary [6], which uses OpenMP to parallelize the image \nop\u00ad erations. In its default con.guration on Debian, Apache cre\u00adates a number of server processes each \nof which spawns a number of threads for handling client requests. Therefore, in this case study, we rely \non policd to coordinate resource management among the separate processes. By default, each of the OpenMP \njobs would attempt to use all of the cores. When the webserver is handling more than one client at a \ntime, this arrangement creates the same problem that existed with the QR decomposition. The form of the \nsolution in this case is similar, however there is one important di.erence: the web server must perform \nas well as possible under many di.erent loads. That is, unlike the QR decomposition, we have no control \nover how many concur\u00adrent OpenMP jobs there will be. This di.erence motivates us to use a new Poli-C \npolicy. 7.2.1 Poli-C Policy We experiment with two di.erent policies. In the .rst pol\u00adicy, cores are \ndivided fairly among concurrently arriving re\u00adquests. For example, if there are 4 concurrent requests, \nand 64 available cores, then each request will be allocated 16 cores. This policy is called fair cores. \nThe second policy mirrors the policy for the QR decomposition above. It at\u00adtempts to allocate exclusive \naccess to a NUMA-node to each request. However, if the number of cores divided by the num\u00adber of concurrent \nrequests is smaller than the number of cores in a NUMA-node, then each request will attempt to acquire \nexclusive access to that number of cores on the same NUMA node instead (with a minimum of one core). \nThis policy is called fair numa.  7.2.2 Results Our benchmark consists of ten second bursts of requests. \nClient and server are on the same LAN so that essentially all of the measured latency is due to time \nthe server spends working. We vary the number of requests per second, and measure the average latency \nover all requests in the burst. We do this experiment using an image .le of constant size (about 3MB). \nThe constant size bene.ts less adaptive policies that always use the same number of cores for each request. \nFigure 10 shows the results of the benchmark using a number of di.erent con.gurations. Each plotted point \nis the average of ten runs. Standard deviations were between 1 and 5%, so we omit error bars. The n Cores \ncon.gurations do not use Poli-C, and simply use n cores to process each blur request. Initially, for \nlarge n this provides good perfor\u00admance. However, as request frequency, and thus contention for cores, \nincreases, performance drops o. rapidly. When the speci.ed n is smaller, performance degrades more grace\u00adfully. \nAt high request rates, the adaptive fair numa policy used by Poli-C performs as well as or outperforms \nthe other methods. The fair cores policy eventually outperforms n Cores for n .{64, 32, 16}. 7.2.3 Discussion \nThere are a few interesting features of this experiment. First, we note that the default con.guration \nthat uses all 64 cores again performs quite poorly. When using all cores, com\u00admunication costs drown \nout the performance bene.t of in\u00adcreased parallelism, even when the request rate is low. Per\u00adformance \ndegrades from there as contention increases. At various request frequencies, various of the n Cores methods \noutperform Poli-C. However, with the exception of the .xed single core policy at high frequency5, no \n.xed policy is consistently better. This implies that core alloca\u00adtion policies must provide a mechanism \nfor adaptation to achieve consistent performance. The language extensions provided by Poli-C are an easy \nway to do this. Finally, we note that fair numa outperforms fair cores largely thanks to its NUMA-awareness. \n 7.3 File-upload web server We now consider a case study in which a webserver accepts uploaded .les, \nand in order to provide a high degree of assur\u00adance to clients, does not send a response until the uploaded \n.les are safely in place on permanent storage. In particular, when our CGI script receives an upload, \nit writes it to a .le using the O SYNC and O DIRECT .ags of the open system call. Even with a RAID array \nthat increases write bandwidth, we .nd that performance degrades when many clients are con\u00adcurrently \nwriting large .les in this way. Using the Linux cgroups subsystem, we can devise a policy that guarantees \n(and limits) each client a portion of 5 Since the individual resize operations are small, the di.erence \nbetween Poli-C and the single core policy is essentially measuring the overhead of the Poli-C runtime. \n disk utilization. In particular, the blkio.weight device con\u00adtroller of a cgroup can be written with \na value from 10 -1000 signifying the share of disk utilization that each cgroup may use, with the sum \nof shares for all cgroups in the system being limited to 1000. We abstract this mechanism by creat\u00ading \na DiskWeight Poli-C resource kind. Using this resource kind, we can guarantee to each .le-upload client \nexclusive access to the disk in turn. In the CGI script, we simply wrap the write system call in one \nof our require statements using a request for DiskWrite(1000). We evaluated this policy by uploading \na 200MB .le from an increasing number of clients. The destination .le and directory of each upload was \nunique to avoid e.ects unrelated to contention for the disk. Figure 11 shows the results of this experiment \nboth with and without Poli-C. Each data point shows the average of 10 runs. Error bars show standard \ndeviation. At all request frequencies, Poli-C provides a performance improvement between 1 and 20% increasing \nas request frequency increases. 7.4 Multicore Database Join In this .nal case study, we show how Poli-C \nmay be used to improve the performance of multicore database join al\u00adgorithms. Database systems use a \nnumber of di.erent join algorithms depending on the needs of the overall query and the relations they \nare joining. Di.erent join algorithms have di.erent cache locality properties. The work of Lee et. al \nshows that scheduling a join algorithm with strong cache lo\u00adcality on the same cores as an algorithm \nwith weak cache locality leads to poor performance [28], but that schedul\u00ad ing strong with strong, or \nweak with weak, tends to have a smaller performance penalty. Lee et. al solve this problem through modi.cations \nto the query scheduler of a database system, and through changes to the underlying OS s virtual memory \nsubsystem. We will address the problem in this case study by devising a Poli-C policy to meet the same \nschedul\u00ading constraints. In particular, we examine a highly optimized parallel radix partition join algorithm, \nwhich has strong cache local\u00adity, and a parallel join algorithm that uses a no-partitioning approach, \nwhich has weak cache locality 6. 7.4.1 Poli-C Policy To implement the strategy suggested by Lee et. al, \nwe in\u00adtroduce two virtual resource kinds that we use in our pol\u00adicy to keep track of which cores are \ncurrently being used by a strong join algorithm, and which are being used by a weak join algorithm. We \ncall these resource kinds virtual because they are simply used for bookkeeping rather than being attached \nto actual hardware. The resource kinds are called StrongCore and WeakCore, and each has as many de\u00advices \nas there are cores on a particular machine. Anticipating that the policy we de.ne will allocate to each \njoin algorithm 6 We thank Cagri Balkesen for the implementation of these algorithms. 1 void join(join \nt *j) { 2 struct numa out out; 3 if (j->type == Radix) { 4 require(out = strong numa node()) { 5 radix \njoin(j,out.ncores); 6 }} 7 else { 8 require(out = weak numa node()) { 9 nopartition join(j,out.ncores); \n 10 }}} Figure 12. Application of Poli-C to multicore join algo\u00adrithms using the strong num node and \nweak numa node poli\u00adcies. an amount of 1.0 for each virtual core it will use, we set the maximum available \namount for the virtual cores to be an integer multiple of 1.0 indicating the maximum number of concurrent \njoin algorithms that may share a core. In our experiments we chose 2.0, indicating that at most two con\u00adcurrent \njoin algorithms could share a core. We de.ned two Poli-C policies, weak numa node and strong numa node. \nThese policies are used in the join algo\u00adrithm driver using our require statement as indicated in Fig\u00adure \n12. The strong numa node policy attempts to construct a resource request that meets the following constraints: \n Using Poli-C s Shared option, Request each core in a NUMA-node, preferring .rst nodes that have not \nbeen allocated, and then the least-full node that already has a StrongCore allocation.  If the policy \nfunction notes that it is the .rst to request a particular node, it not only requests a unit (1.0) of \nthe StrongCore virtual resource, but also all of the WeakCore virtual resource. This prevents a weak \njoin algorithm from running on the same core.  Request an allocation of 0.0 on all other cores.  The \nimplementation of weak numa node is symmetric with strong numa node. 7.4.2 Results Before proceeding \nto implement the above policies, we .rst sought to verify the premise on which they are based. In par\u00adticular, \nwe performed the following experiment. We ran two instances of the parallel radix join algorithm concurrently \non the same NUMA node, followed by two instances of the par\u00adallel no-partition join algorithm, followed \nby one instance of each. Each instance operated on separate pairs of rela\u00adtions, one small and one big. \nThe small relation consisted of roughly 16 million tuples occupying 128MiB. The large relation consisted \nof roughly 256 million tuples occupying 2GiB. Figure 13 shows the results of this experiment. Run\u00adning \nthe radix join(strong) concurrently with the no-partition join(weak) harms the performance of the radix \njoin, and slightly helps the performance of the no-partition join.   Figure 14. Throughput in tuples \nper second for two di.er\u00adent methods of mapping multicore join algorithms to cores. Having veri.ed that \nthere is indeed a performance bene.t (at least for the radix join algorithm) in avoiding sharing cores \namong di.erent join algorithms, we now proceed to an experiment that shows the performance bene.ts of \nthe policies described above. In particular, we ran 16 separate joins using the relations described above. \n8 used the radix join algorithm, and 8 used the no-partition join algorithm. The results of this experiment \nare shown in Figure 14. When using Poli-C, the average throughput was about 322 million tuples joined \nper second. When allowing join algorithms to be mapped to cores arbitrarily, the average throughput was \nabout 282 million. Thus, Poli-C enabled a 14% advantage in throughput. This advantage is not as large \nas it could be. A more sophisticated policy could take advantage of many of the strategies suggested \nfor successful database/OS co\u00addesign [20]. We leave implementation of these strategies in the context \nof Poli-C for future work. 8. Related Work The most closely related work to our own is the hierarchi\u00adcal \nscheduling library, Lithe [36]. Lithe is used to e.ciently compose multiple parallel libraries in the \nsame application. While Lithe requires no annotations in application code, it does require modi.cation \nof, and custom builds of, the par\u00adallel libraries that it composes. Given the preponderance of parallel \nlibraries (e.g. OpenMP, pthreads, TBB, etc.), their implementations (e.g. GNU, Intel, Microsoft, Sun), \nand the di.erent versions relied upon by various applications, we view it as less intrusive to allow \na detailed resource allo\u00adcation policy to be declared in the application source code, and to implement \nthe runtime library for these annotations once-and-for-all, using features provided already by operat\u00ading \nsystems for manipulating thread scheduling and resource allocation. On the other hand, the use of custom \nlibraries also has advantages. In Poli-C we have tried to provide features su.\u00adcient to handle the unavailability \nof library code in the form of our options to the require statement. However, it may still be the case \nthat the best place for one of our require statements is not available to the application programmer. \nA runtime that takes advantage of custom parallel libraries avoids this problem because a call to the \nright scheduling primitive can be placed anywhere. The second major di.erence with Lithe is that Poli-C \nis able to manage not only cores, but also any resource whose usage can be measured, or to which the \nOS exposes an inter\u00adface. We accomplish this by decoupling resource allocation from thread scheduling \n(or stacks and continuations as in Lithe) through the use of our explicit allocation trees, which we \ndescribe in Section 6. Finally, Lithe is only able to manage cores within a single multithreaded process. \nThis implies that it would not have been able to act as Poli-C did in the webserver case study above. \nIn particular, our system-level daemon policd allows resource coordination and polices to operate among \nseveral multithreaded processes. Aside from Lithe, researchers have proposed a num\u00adber of other hierarchical \nscheduling solutions for general\u00adpurpose computing, as well as real-time scheduling. These include Psyche \n[32], Converse [26], CPU Inheritance [18], HLS [37], GHC [29], and Manticore [17]. The largest dif\u00ad ference \nbetween these approaches and our own is the ability of our language extensions to manage not only cores, \nbut also other system resources. Additionally, our goal was not enabling the implementation of hierarchical \nschedulers, but rather, among other things, enabling the e.cient compo\u00adsition of existing parallel libraries \nand programs through statements declaring explicit resource management policies. Many recently developed \nlanguages and language exten\u00adsions have sought to ease the expression of parallelism and concurrency, \nfor example Cilk [19], Jade [39], Go [1], Split-C [27], Fortress [7], X10 [15], Chapel [13], and AC [22]. \nOur work di.ers from these projects in that our language ex\u00adtensions allocate resources to various parallel \ntasks, taking for grated that many languages already have features for the expression of parallelism. \nSince the API for our language runtime is simple, it should be easy for Poli-C to be inte\u00adgrated with \nthese languages.  Finally, OS resource management features have also re\u00adcently been used to support \nvirtualization [23, 24]; guest OSs are limited to using only part of the machine, as speci.ed by the \nsystem administrator, so as not to interfere with other guest OSs or applications running on the host \nOS. Because these features are becoming more generally useful, Poli-C gives applications more direct \naccess to them. 9. Conclusion and Future Work In this paper, we presented Poli-C, a language-based inter\u00adface \nfor the composition of parallel software and the .ne\u00adgrained, hierarchical management of resources. Our \ndesign gathers disjoint and confusing OS APIs into a single coher\u00adent interface, and we have demonstrated \nthat it improves the performance of a number of representative benchmarks on a modern server machine \nusing only a few additions to appli\u00adcation code. At present, our implementation is only for Linux, how\u00adever \nall platform speci.c code is wrapped up in our resource kind de.nitions. We hope that this will make \nporting Poli-C to other platforms straightforward. In particular it would be very interesting to implement \na port to Barrel.sh [9], Tesse\u00ad lation [30], FOS [43], or another next-generation, multicore OS in order \nto investigate possible synergies between OS\u00adlevel, and user-level scheduling and resource management. \nWe are investigating ways of making our policy DSL more declarative. Our current approach requires looping \nthrough each device of each desired resource kind. This becomes cumbersome when the requirements of a \npolicy become more complex, for example when requesting that allocated cores be on the same NUMA node, \nas in our case studies in Section 7. We believe that asking programmers for declarative constraints, \nand then using an SMT or constraint logic programing solver to solve for a set of resources that satisfy \nthe constraints will be a more convenient approach. 10. Acknowledgements Many thanks are due to Tim Harris, \nKornilios Kourtis, and Timothy Roscoe for discussions and advice about early drafts of this paper, and \nto Cagri Balkesen for help in set\u00adting up the database joins benchmark. We would also like to thank the \nanonymous reviewers for their valuable feedback and suggestions. References [1] The go programming language, \nOct. 2011. http://golang. org/. [2] lxc: Linux containers, Sept. 2011. http://lxc.sf.net/. [3] System \nadministration guide: Oracle solaris containers\u00adresource management and oracle solaris zones, Sept. 2011. \nhttp://docs.sun.com/app/docs/doc/817-1592. [4] Apache HTTP server project, Apr. 2012. http://httpd. apache.org/. \n[5] cgic: an ANSI C library for CGI programming, Apr. 2012. http://www.boutell.com/cgic/. [6] ImageMagick: \nconvert, edit, and compose images, Apr. 2012. http://www.imagemagick.org. [7] Allen, E., Chase, D., Luchangco, \nV., Jr., J.-W. M. S. R. G. L. S., and Tobin-Hochstadt, S. The fortress language speci.cation version \n1.0, 2008. http://research.sun. com/projects/plrg/fortress.pdf. [8] Asanovic, K., Bodik, R., Catanzaro, \nB. C., Gebis, J. J., Husbands, P., Keutzer, K., Patterson, D. A., Plishker, W. L., Shalf, J., Williams, \nS. W., and Yelick, K. A. The landscape of parallel computing research: A view from berkeley. Tech. Rep. \nUCB/EECS-2006-183, EECS Department, University of California, Berkeley, Dec 2006. [9] Baumann, A., Barham, \nP., Dagand, P.-E., Harris, T., Isaacs, R., Peter, S., Roscoe, T., Sch \u00a8 upbach, A., and Singhania, A. \nThe multikernel: a new os architecture for scalable multicore systems. In SOSP 09, pp. 29 44. [10] Bienia, \nC., Kumar, S., Singh, J. P., and Li, K. The parsec benchmark suite: Characterization and architectural \nimplications. In Proceedings of the 17th International Conference on Parallel Architectures and Compilation \nTechniques (October 2008). [11] Boehm, H.-J. Threads cannot be implemented as a library. In PLDI 05, \npp. 261 268. [12] Boisvert, R. F., Pozo, R., Remington, K., Barrett, R. F., and Dongarra, J. J. Matrix \nmarket: a web resource for test matrix collections. In Proceedings of the IFIP TC2/WG2.5 working conference \non Quality of numerical software: assessment and enhancement (London, UK, UK, 1997), Chapman &#38; Hall, \nLtd., pp. 125 137. [13] Chamberlain, B., Callahan, D., and Zima, H. Parallel programmability and the \nchapel language. Int. J. High Perform. Comput. Appl. 21, 3 (2007), 291 312. [14] Davis, T. A. Algorithm \n915, SuiteSparseQR: Multifrontal multithreaded rank-revealing sparse QR factorization. ACM Transactions \non Mathematical Software 38, 1 (2011). [15] Ebcioglu, K., Saraswat, V., and Sarkar, V. X10: Program\u00adming \nfor hierarchical parallelism and non-uniform data ac\u00adcess. In OOPSLA 04. [16] et al., C. L. Basic linear \nalgebra subprograms for FORTRAN. In Transactions on Mathematical Software (1979). [17] Fluet, M., Rainey, \nM., and Reppy, J. A scheduling framework for general-purpose parallel languages. In ICFP 08, pp. 241 \n252. [18] Ford, B., and Susarla, S. Cpu inheritance scheduling. In OSDI 96, pp. 91 105. [19] Frigo, M. \nMultithreaded programming in cilk. In Proceed\u00adings of the 2007 international workshop on Parallel symbolic \ncomputation (2007), pp. 13 14.  [20] Giceva, J. Database-operating system co-design. Master s thesis, \nETH Z\u00a8 urich, May 2011. [21] Grossman, D. The transactional memory / garbage collection analogy. In OOPSLA \n07, pp. 695 706. [22] Harris, T., Abadi, M., Isaacs, R., and McIlroy, R. AC: Composable asynchronous \nio for native languages. In OOPSLA 11. [23] Hindman, B., Konwinski, A., Zaharia, M., Ghodsi, A., Joseph, \nA., Shenker, S., and Stoica, I. Nexus: A common substrate for cluster computing. Tech. Rep. UCB/EECS-2009-158, \nEECS Department, University of California, Berkeley, 2009. [24] Hindman, B., Konwinski, A., Zaharia, \nM., Ghodsi, A., Joseph, A. D., Katz, R., Shenker, S., and Stoica, I. Mesos: a platform for .ne-grained \nresource sharing in the data center. In NSDI 11, pp. 22 22. [25] Intel. Math kernel library for the linux \noperating system: User s guide, 2007. [26] Kale\u00b4, L. V., Yelon, J., and Knuff, T. Threads for inter\u00adoperable \nparallel programming. In Proceedings of the 9th International Workshop on Languages and Compilers for \nParallel Computing (1997), pp. 534 552. [27] Krishnamurthy, A., Culler, D. E., Dusseau, A., Goldstein, \nS. C., Lumetta, S., von Eicken, T., and Yelick, K. Parallel Programming in Split-C. In SUPERCOM 93, pp. \n262 273. [28] Lee, R., Ding, X., Chen, F., Lu, Q., and Zhang, X. Mcc\u00addb: minimizing cache con.icts in \nmulti-core processors for databases. Proc. VLDB Endow. 2, 1 (Aug. 2009), 373 384. [29] Li, P., Marlow, \nS., Peyton Jones, S., and Tolmach, A. Lightweight concurrency primitives for ghc. In Proceedings of the \nACM SIGPLAN workshop on Haskell workshop (2007), pp. 107 118. [30] Liu, R., Klues, K., Bird, S., Hofmeyr, \nS., Asanovic\u00b4, K., and Kubiatowicz, J. Tessellation: space-time partitioning in a manycore client os. \nIn HotPar 09, pp. 10 10. [31] Mackey, L., Talwalkar, A., and Jordan, M. Divide\u00adand-conquer matrix factorization. \nIn Neural Information Processing Systems (NIPS) (2011). [32] Marsh, B. D., Scott, M. L., LeBlanc, T. \nJ., and Markatos, E. P. First-class user-level threads. In SOSP 91, pp. 110 121. [33] McIver, L., and \nConway, D. Seven deadly sins of introductory programming language design. In Proceedings of the 1996 \nInternational Conference on Software Engineering: Education and Practice (SE:EP 96), pp. 309 . [34] Menage, \nP. Cgroups, July 2011. http://www.kernel.org/ doc/Documentation/cgroups/cgroups.txt. [35] Necula, G. \nC., McPeak, S., and Weimer, W. CIL: Interme\u00addiate language and tools for the analysis of C programs. \nIn CC 04, pp. 213 228. http://cil.sourceforge.net/. [36] Pan, H., Hindman, B., and Asanovic\u00b4, K. Composing \nparallel software e.ciently with lithe. In PLDI 10, pp. 376 387. [37] Regehr, J., and Stankovic, J. A. \nHls: A framework for composing soft real-time schedulers. In Proceedings of the 22nd IEEE Real-Time Systems \nSymposium (2001), pp. 3 . [38] Reinders, J. Intel threading building blocks: out.tting C++ for multi-core \nprocessor parallelism. O Reilly, 2007. [39] Rinard, M. C., and Lam, M. S. The design, implementation, \nand evaluation of jade. ACM Trans. Program. Lang. Syst. 20, 3 (1998), 483 545. [40] Saha, B., Adl-Tabatabai, \nA.-R., Ghuloum, A., Rajagopalan, M., Hudson, R. L., Petersen, L., Menon, V., Murphy, B., Shpeisman, T., \nSprangle, E., Rohillah, A., Carmean, D., and Fang, J. Enabling scalability and performance in a large \nscale CMP environment. In EuroSys 07, pp. 73 86. [41] Satyanarayanan, M., Mashburn, H. H., Kumar, P., \nSteere, D. C., and Kistler, J. J. Lightweight recoverable virtual memory. In SOSP 93, pp. 146 160. [42] \nTaylor, G., Davies, P., and Farmwald, M. The tlb slice: a low-cost high-speed address translation mechanism. \nIn Proceedings of the 17th annual international symposium on Computer Architecture (New York, NY, USA, \n1990), ISCA 90, ACM, pp. 355 363. [43] Wentzlaff, D., and Agarwal, A. Factored operating systems (fos): \nthe case for a scalable operating system for multicores. SIGOPS Oper. Syst. Rev. 43 (April 2009), 76 \n85.     \n\t\t\t", "proc_id": "2384616", "abstract": "<p>This paper presents Poli-C, a language extension, runtime library, and system daemon enabling fine-grained, language-level, hierarchical resource management policies. Poli-C is suitable for use in applications that compose parallel libraries, frameworks, and programs. In particular, we have added a powerful new statement to C for expressing resource limits and guarantees in such a way that programmers can set resource management policies even when the source code of parallel libraries and frameworks is not available. Poli-C enables application programmers to manage any resource exposed by the underlying OS, for example cores or IO bandwidth. Additionally, we have developed a domain-specific language for defining high-level resource management policies, and a facility for extending the kinds of resources that can be managed with our language extension. Finally, through a number of useful variations, our design offers a high degree of composability. We evaluate Poli-C by way of three case-studies: a scientific application, an image processing webserver, and a pair of parallel database join implementations. We found that using Poli-C yields efficiency gains that require the addition of only a few lines of code to applications.</p>", "authors": [{"name": "Zachary Anderson", "author_profile_id": "81332488087", "affiliation": "ETH Zurich, Zurich, Switzerland", "person_id": "P3856231", "email_address": "zachary.anderson@inf.ethz.ch", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384669", "year": "2012", "article_id": "2384669", "conference": "OOPSLA", "title": "Efficiently combining parallel software using fine-grained, language-level, hierarchical resource management policies", "url": "http://dl.acm.org/citation.cfm?id=2384669"}