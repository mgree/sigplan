{"article_publication_date": "10-19-2012", "fulltext": "\n Understanding the Behavior of Database Operations under Program Control Juan M. Tamayo Alex Aiken Nathan \nBronson Stanford University {jtamayo,aiken,nbronson}@cs.stanford.edu Abstract Applications that combine \ngeneral program logic with per\u00adsistent databases (e.g., three-tier applications) often suffer large performance \npenalties from poor use of the database. We introduce a program analysis technique that combines in\u00adformation \n.ow in the program with commutativity analysis of its database operations to produce a uni.ed dependency \ngraph for database statements, which provides programmers with a high-level view of how costly database \noperations are and how they are connected in the program. As an example application of our analysis we \ndescribe three optimizations that can be discovered by examining the structure of the de\u00adpendency graph; \neach helps remove communication latency from the critical path of a multi-tier system. We implement our \ntechnique in a tool for Java applications using JDBC and experimentally validate it using the multi-tier \ncomponent of the Dacapo benchmark. Categories and Subject Descriptors D.2.3 [Software En\u00adgineering]: \nCoding Tools and Techniques Keywords program understanding; three-tier applications 1. Introduction Database \naccess tends to be a performance bottleneck for three-tier applications. In production settings the middle\u00adtier \nsoftware and the database are often physically separated and roundtrip communication latency between \nthe program and the database can become the dominant factor governing overall system performance. However, \nit is currently very dif.cult for programmers to gain an understanding of the bottlenecks in system performance. \nWhile pro.lers can give information about where delays occur in the system (where Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, \n2012, Tuscon, Arizona, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 Mooly \nSagiv Tel-Aviv University sagiv@math.tau.ac.il one component must wait on the results of another), this \nlow\u00adlevel information does not necessarily suggest what higher\u00adlevel reorganization will lead to improved \nperformance. We are interested in providing programmers with a high\u00adlevel picture of program behavior \nuseful for understanding the performance of a three-tier application. The problem is dif.cult because \nthe database operations in an application can depend on each other in complex ways, both in how they \ninteract within the program (e.g., is the output of one query potentially used as input to another query?) \nand in how they interact through the database (e.g., does one database oper\u00adation read tuples that another \ndatabase operation writes?). The relevant database statements might be far apart in the source code, \nthey may be connected by complicated control\u00ad.ow structures or layers of abstraction, or they might be \nsurrounded by many other database operations that make it hard to reason about dependencies between them. \nClassical database and compiler tools are not able to solve this prob\u00adlem, because each only sees at \nmost half of the picture. Stan\u00addard database performance optimizations, such as indexing or query optimization, \ncan give information about individual database operations, but are not able to understand or exploit \nproperties of multiple queries run under program control. A standard compiler has some sense of the data-and \ncontrol\u00ad.ow properties of a program, but has no model of database operations and the interactions they \nmay have through the persistent store. Thus, any systematic approach to understanding and im\u00adproving \ndatabase usage must involve analysis of the .ow of information both within the program and through the \ndatabase. We present a tool that dynamically collects de\u00adpendency information between database statements \nthrough the middle tier (Section 3.1) and through the database (Sec\u00adtion 3.2). Our tool aggregates the \ndata collected and presents a dependency graph between database statements (Section 4) like the ones \nshown in Figures 1 and 6. As an example application, we show how the depen\u00addency graph can be used to \nlocate optimization opportu\u00adnities, as described in Sections 2 and 6. Because we are focused on program \nunderstanding, our tool generates the Figure 1. The dependency graph produced by our tool for the database \nstatements executed in the code of Figure 2. Nodes are database statements, edges are dependencies be\u00adtween \nstatements. There are four kinds of dependency: (1) solid edges are dependencies of data .owing through \nthe middle-tier (the client program), (2) dashed edges repre\u00adsent read-after-write dependencies through \nthe database,   (3) dash-dot edges represent the original sequence between database writes. Write-after-read \ndatabase dependencies do not occur in this example. graphs automatically, but it does not automatically \napply optimizations. (Because we rely on dynamic analysis tech\u00adniques, our approach cannot prove that \nit is safe to per\u00adform the optimizations.) Instead, our method suggests to programmers speci.c, likely \nplaces to look for opportuni\u00adties to improve performance and which optimization to try. We have identi.ed \nthree relatively simple optimizations that can improve database usage: statement batching, in which multiple \ndatabase write statements are submitted as one; asynchronous query execution, where the application avoids \nblocking on database reads; and removing redundant state\u00adments, where a second read of some database \ntuples is re\u00adplaced by simply reusing the results of a previous query. We show through a signi.cant case \nstudy (Section 6) that the cumulative effect of applying all three optimizations can be substantial. \nOur speci.c contributions are: We identify four different kinds of dependencies that are important for \nprogrammers to understand in restructur\u00ading applications that use a database, and give a dynamic analysis \nmethod for capturing the different kinds of de\u00adpendencies.  We present a novel method for summarizing \nthe output of the dynamic analysis, grouping database statements and transactions together based on their \ncalling context to produce concise but context-sensitive graphs illustrating  the frequency, cost, and \ndependencies between database statements within a transaction. We show that opportunities for three \nsimple program optimizations can be identi.ed using our dependency graphs: statement batching, asynchronous \nquery execu\u00adtion, and removing redundant statements. Each of these transformations improves system performance \nby avoid\u00ading or hiding network latency; some also reduce the total work performed.  We present the results \nof a case study on a substantial three-tier application, in which our analysis techniques identify opportunities \nto apply all three optimizations, resulting in an overall reduction of 8 database roundtrips in the most \nimportant operation. Every multi-statement transaction exercised by this benchmark contained an optimization \nopportunity.  2. The Problem Consider the code in Figure 2, which illustrates a buy operation in a stock \ntrading application. The operation (a single method in this case) takes as arguments the stock to buy, \nthe quantity, and the buyer. It then queries the database for details about the user and the stock, computes \nthe total purchase price, creates a new order, calculates the buyer s new account balance, and returns \nan Account object with the buyer s balance updated. Our goal is to improve the performance of this example. \nThe method executes .ve database statements, each requir\u00ading at least one roundtrip to the database. \nWe can remove or hide some of these delays by applying the following opti\u00admizations: Statement batching: \nInstead of emitting one statement at a time and waiting for its result, multiple statements can be batched \nand sent together to the database en\u00adgine, saving one roundtrip per statement batched. For ex\u00adample, \nthe statements in createOrder [line 11] and creditAccount [line 13] could be batched together, saving \none roundtrip.  Asynchronous queries: By emitting queries without im\u00admediately using their results it \nis possible to execute mul\u00adtiple queries simultaneously, thereby hiding the latency of some. In our example, \nthe statements in getAccount-Data [line 4] and getStockData [line 6] could be executed concurrently, \nreducing the total operation la\u00adtency by one roundtrip.  Removing redundant queries: The best way to \nimprove performance is to do less work. In our example, the exe\u00adcution of getAccountData [line 15] is \nunnecessary, since knowing the behavior of creditAccount [line 13] is enough to predict the new account \nbalance. We can remove this query, save one more roundtrip and also reduce the load on the database. \n   1 Account buy(Connection con, String userID, Account buy(Connection con, String userID, String \nsymbol, int quantity) { Future<Account> account = getAccountData(userID); Future<Stock> stock = getStockData(symbol); \ndouble total = 8 stock.getPrice()*quantity stock.get().getPrice()*quantity 9 + account.getOrderFee(); \n+ account.get().getOrderFee(); Statement s = con.createStatement(); s.addBatch(createOrder(account, stock, \nquantity, total)); s.addBatch(creditAccount(account, total)); s.executeBatch(); s.close(); account.setBalance(account.getBalance() \n+ total); return account; } Figure 2. Example code illustrating the optimizations described in this \npaper. (A) Independent statements can be executed concurrently, hiding the latency for some of them. \n(B) Multiple write statements can be batched, sending them as one to the database. (C) Unnecessary queries \ncan be removed. Applying these optimizations requires a high-level view of all queries executed, combined \nwith information about the dependencies between them. Finding these dependencies is not straightforward. \nIn real applications operations are more complex than the illustrative example shown here, and there \ncan be several layers of abstraction that hide the speci.c queries executed. Manually .nding all queries \nand their dependencies is tedious at best. Furthermore, even a complete understanding of all the dependencies \nwithin the program is insuf.cient, because it is unclear what is going on in the database. In particular, \ndo the queries touch the same or distinct tuples in the database? To solve these problems, we propose \na high-level view of the dependencies between all database statements in a transaction. Four kinds of \ndependencies are considered: Data.ow dependencies result when the output of one database statement .ows \nthrough the program (to part of) the input of another database statement.  Database write-write dependencies \nresult when two database statements write at least one common tuple in the database. We conservatively \nassume any pair of database operations that write the database have a write\u00adwrite dependency (see discussion \nin Section 3.2).  Database write-after-read dependencies arise when one database operation reads a tuple \nt and some subsequent database operation writes t.  Database read-after-write dependencies arise when \none database operation writes a tuple t and some subsequent database operation reads t.  The dependency \ngraph for the code in Figure 2 is given in Figure 1. Note that this dependency graph with the multiple, \ndistinct kinds of edges, makes it easy to spot the possible opportunities to apply the three optimizations: \n Batching opportunities appear as sequences of database writes with nothing but database write-write \ndependen\u00adcies connecting them.  Parallelizable reads appear as sets of independent nodes (i.e., with \nno connecting paths of any kind of depen\u00addency).  Duplicate reads appear as multiple independent instances \nof the same database statement or as database read-after\u00adwrite dependencies.  Finding these optimization \nopportunities is reasonably strai\u00adghtforward in graphs like the ones presented in Section 6. For larger \ngraphs an automated tool could scan the graph de.nition and automatically spot the improvement opportu\u00adnities. \n3. Finding Dependencies To generate graphs like the one in Figure 1 we require, for every database statement \nexecuted, the set of statements on which it depends. We use dynamic information .ow tracking (Section \n3.1) to infer dependencies caused by data .owing through the application code, and a commutativity analysis \n(Section 3.2) to infer dependencies through data stored in the database.  3.1 Dynamic Information Flow \nTracking We use dynamic taint analysis, a widely-used technique [20], to .nd data dependencies in the \napplication. In its original form [16], dynamic taint analysis prevents security attacks by labeling \ndata coming from untrusted inputs as tainted, keeping track of the propagation of tainted data as the \npro\u00adgram executes, and detecting when tainted data is used in dangerous ways. More generally, a dynamic \ntaint analysis algorithm spec\u00adi.es a domain of labels used to taint data, a join operator for combining \nlabels, and three policies: which data should be tainted, how taint should be tracked through the program, \nand where the taint of values should be checked. For .nding dependencies between database statements \nin a program we use the following policies: Taint injection: Each database statement execution is as\u00adsigned \na unique identi.er i (described further below) and all data returned from that statement is tainted with \nthe set {i}. Taint tracking: Taint propagates through all explicit data .ows in the program: roughly \nspeaking, the label on the outputs of an operation is the union of the labels of its inputs. We do not, \nhowever, track implicit .ows through conditional branches (i.e., the taint of the predicate of an if-statement \nis not propagated to the branches). We ob\u00adserved that the majority of the conditionals in the pro\u00adgram \ntested for error conditions. While a strict interpre\u00adtation of the taint .ow is that any information \ncomputed following an error check encodes the fact that the compu\u00adtation did not fail, we found that \nthis increased the com\u00adplexity of the resulting dependence graph without provid\u00ading useful information \nto the user. In our benchmark this analysis choice did not suggest any false optimizations. Taint detection: \nTaint is detected at the execution of every database statement: before a database statement s is exe\u00adcuted \nwe record the set of labels used to build s. Thus one component of a label is a set of database state\u00adment \nidenti.ers, the join for which is set union. Two main dif.culties remain. First, because every execution \ninstance of a database statement has a unique identi.er, we must somehow deal with label sets of potentially \nunbounded size. Second, in practice this approach can yield dependencies be\u00adtween any pair of database \nstatement instances, and the over\u00adwhelming majority of these dependencies are not interesting. We take \nadvantage of the following insight to deal with both problems. The interesting relationships are all \nwithin a single transaction; intuitively, database statements within separate transactions are already \nlikely to be concurrent that is exactly why they are wrapped in a transaction and therefore not the source \nof performance bottlenecks. Thus, the dependencies that are useful are just those between database statements \nwithin a transaction, which is a small subset of all the dependencies in a program execution. We exclude \ndependencies between transactions by re.ning the taint labels and the join operation: Identi.ers Every \ntransaction is assigned an integer trans\u00adactionId. Transaction ids monotonically increase in the order \nin which transactions are started by the program. Within a transaction, each database statement is assigned \na queryId. Thus, every database operation executed by the program is associated with a (transactionId,que\u00adryId) \npair. Queries executed outside an explicit transac\u00adtion are assigned a singleton transactionId.  Labels \nA label is a pair (t, S) consisting of a transact\u00adionId and a set of queryId s.  Join. The join of two \nlabels (t1,S1) and (t2,S2) is  . . (t1,S1 . S2) if t1 = t2 (t1,S1) (t2,S2) = (t2,S2) if t1 <t2 . (t1,S1) \nif t1 >t2 If both labels have the same transactionId we re\u00adturn a label with same transactionId and the \nunion of the two sets, which is easily computed by taking the bitwise or of the two labels. If the transactionId \ns are different, we return the label of the one with the larger transactionId, discarding information \nfor ear\u00adlier transactions. This taint analysis captures the data.ow dependencies of the code in Figure \n2. For example, consider the dependency between the query in getStockData [line 6] and the query in creditAccount \n[line 13]. This dependency arises because the total value used by creditAccount [line 13] is computed \nfrom the price in the stock object, which is populated from data queried in getStockData [line 6]. By \ntainting the data queried in getStockData [line 6] and propagating taint as the program executes, we \ndetect the dependency between getStockData [line 6] and the query in creditAccount [line 13]. The full \ndependency chain is: ResultSet (database driver, in . getStockData [line 6]) stock.price (object .eld) \n. total (local variable) . creditAccount (method parameter) . PreparedStatement (database driver, in \ncreditAccount [line 13])  3.2 Finding Database Dependencies As discussed in Section 1, data .ow through \nthe client pro\u00adgram is insuf.cient to fully characterize the dependency graph on database statements. \nTuples modi.ed by a database  --Read 1 (r1) SELECT * FROM STOCKS WHERE SYMBOL = ?; --Write 1 (w1) INSERT \nINTO ORDERS (USERID, AMMOUNT) VALUES (?, ?); --Read 2 (r2) SELECT * FROM ACCOUNTS WHERE USERID = ?; --Write \n2 (w2) UPDATE ACCOUNTS SET BALANCE = BALANCE + ? WHERE ACCOUNTID = ?; Figure 3. Dependent database statements. \nQuestion marks represent placeholders that are dynamically populated by the Java application. There is \na dependency between r2 and w2 if the accounts identi.ed by USERID (in r2) and ACCOUNTID (in w2) are \nthe same. statement might be used by some other statement later in the transaction, creating a dependency \nbetween the two. Because this dependency arises through data stored in the database, it is not possible \nto discover it without analyzing the side effects of database statements. For example, consider the database \nstatements in Fig\u00adure 3. Using the optimizations described in Section 2, there are two possible improvements \nto this code: Option 1: We could move r2 after w2 and batch w1 and w2. This optimization saves one roundtrip \nto the database. However, if the accounts identi.ed by USERID (in r2) and ACCOUNTID (in w2) were the \nsame, this optimization would be incorrect: since w2 writes to a tuple read by r2, executing r2 after \nw2 would change the result of r2. Option 2: We could move r2 before w1, execute r1 and r2 concurrently, \nand batch w1 and w2. This optimization would save two roundtrips to the database. While Option 2 is \nalways preferable in this example, Option 1 illustrates a dif.culty when .nding dependencies between \ndatabase statements: the parameters to r2 and w2 are not known until runtime, and even then the dependencies \narise through the content of the database. Without more informa\u00adtion, a static analysis would have no \nchoice but to conserva\u00adtively assume a dependency exists. In our dynamic analysis we classify database \nstatements as either reads, which do not modify the database, or writes, which do modify the database \ns contents. Consequently, there are three types of database dependencies: read-after\u00adwrite, write-after-read, \nand write-after-write. Two reads are never dependent through the database, even though they might be \nthrough the client application. We conservatively assume there is always a depen\u00addency between two consecutive \nwrites. Little is lost, be\u00adcause neither of the optimizations involving write statements (statement batching \nor redundant query elimination) require changing the order of writes. Furthermore, .nding write\u00adwrite \ndependencies via dynamic analysis is prohibitively expensive, as it requires either knowing the set of \ntuples modi.ed by each write which is impossible without either access to the database implementation \nor effectively sim\u00adulating it or executing the writes in different orders and comparing the entire state \nof the different databases that re\u00adsult to check whether they are the same. Both options are infeasible \nin practice. To .nd dependencies between database reads and writes, we use the fact that if a write changes \nthe result of a read then the statements are dependent. Consider a single read in a sequence of writes: \n... r ... w1 w2 wi wi+1 wn Since we assume that consecutive writes are dependent, the sequence of writes \nis .xed. We are interested in two particu\u00adlar writes: The .rst write w after r that changes the result \nof r. This write indicates how far to the right we can move r without altering the result of the program. \nThere is a write-after\u00adread dependency between r and w.  The last write w' before r to change the result \nof r. This write limits how far to the left we can move r. There is a read-after-write dependency between \nw' and r.  These two dependencies per read, together with the assump\u00adtion that consecutive writes are \ndependent, characterize all dependencies through the database. In the example of Figure 3, the sequence \nof reads and writes for r2 is: w1 r2 w2 w2 limits how far to the right we can move r2 before ob\u00adtaining \nthe wrong result, so there is a write-after-read depen\u00addency between w2 and r2. In this case r2 can move \nfreely to the left so there are no read-after-write dependencies. We implement this database dependency \nanalysis using a proxy JDBC driver that logs all statements executed in a database transaction. Before \nthe transaction is committed, the driver executes the following procedure: 1. Roll back the current transaction. \n2. Separate the state\u00adments executed into a list of reads and a list of writes. 3. Replay the transaction \nby executing all writes in order. Before and after every write execute each read and compute a .ngerprint \nof the read s result. 4. Commit the transaction.  For a transaction with w writes the above procedure \ngen\u00aderates w +1 .ngerprints per read. To determine whether a write changes the result of a read, we compare \nthe read s .ngerprints before and after executing the write. If the .n\u00adgerprints differ, the write changed \nthe result of the read.  4. Graph Construction For each database statement, the dynamic analysis algo\u00adrithms \nof Section 3 output a runtime statement containing: the context in which the database statement was \nexe\u00adcuted, in the form of a stack trace,  the transactionId of the transaction of which it is a part, \nand  the set of runtime statements it depends on.  All runtime statements are part of a transaction, \nwhich is the set of runtime statements with the same transactionId. The major dif.culty in presenting \nthe results of the dy\u00adnamic analysis to a user is that the number of runtime state\u00adments is normally \noverwhelming. Even for applications of moderate complexity many thousands of runtime statements are produced \nfor realistic workloads. In this section we de\u00adscribe our techniques for summarizing this information \nin a form that is both concise and useful. A summary graph groups sets of runtime statements to\u00adgether. \nSpeci.cally, the nodes of the summary graph are sets of runtime statements with the same stack trace \n(i.e., a node of the graph corresponds to a stack trace). There is an edge between two nodes n1,n2 if \na runtime statement in n2 de\u00adpends on a runtime statement in n1. Nodes and edges also have weight, which \nis just the number of runtime statements in a node and the number of underlying dependencies be\u00adtween \ntwo nodes, respectively. If the program contains a loop, a node may represent more than one execution \nof a statement within a single trans\u00adaction. This will be re.ected by the node s weight; if there is \na loop carried dependence this will result in a self-edge or cycle. For example, if the code of Figure \n2 is executed 1000 times, a total of 1000 transactions are logged, with a total of 5000 runtime statements. \nThese 5000 runtime statements are represented by .ve nodes in a summary graph, identi.ed by the following \n.ve stack traces each stack consists in this example consists of just one functionc call: 1. getAccountData \n[line 4] 2. getStockData [line 6] 3. createOrder [line 11] 4. creditAccount [line 13] 5. getAccountData \n[line 15]  These are exactly the nodes presented in Figure 1. As can be seen from this example, grouping \nruntime statements by stack trace dramatically reduces the quantity of data a user needs to inspect while \nstill retaining distinctions based on calling context i.e., the same database statement invoked in two \ncompletely different calling contexts is regarded as belonging to distinct nodes of the dependency graph. \nNote public interface TradeServices { (...) public OrderDataBean buy(String userID, String symbol, double \nquantity, int orderProcessingMode) throws Exception, RemoteException; public OrderDataBean sell(String \nuserID, Integer holdingID, int orderProcessingMode) throws Exception, RemoteException; (...) public QuoteDataBean \ncreateQuote( String symbol, String companyName, BigDecimal price) throws Exception, RemoteException; \npublic QuoteDataBean getQuote( String symbol) throws Exception, RemoteException; public Collection getAllQuotes() \nthrows Exception, RemoteException; (...) } Figure 4. Simpli.ed version of the TradeServices interface, \nafter removing thrown exceptions and comments. This inter\u00adface lists all business operations implemented \nby the middle tier. Each operation runs a different database transaction. that getAccountData is called \ntwice in different places in Figure 2 and thus occurs in two distinct nodes in Figure 1. Our method produces \na set of dependency graphs, where each graph corresponds to a set of executions of the same transaction. \nOf course, we have a similar problem with trans\u00adactions as with runtime statements, namely that we must \nde\u00adcide which runtime transactions belong to the same group. It is not obvious, however, how to group \ntransactions. We observe that even in languages without explicit syntax for beginning and ending transactions, \nprogrammers still delib\u00aderately organize transactions under a particular lexical scope. For example, \nFigure 4 shows some of the services exposed by the business layer of Daytrader, an example online stock \ntrading system we return to in Section 6. Implementations for the methods in the TradeServices interface \ncreate a transaction on method entry, which they commit before re\u00adturning. If all transactions are coded \nin a similar manner, then all queries run within a transaction share a common calling context, namely \nthe method that implements the ser\u00advice exposed by the middle layer. We can use this lexical scope to \nautomatically group transactions into meaningful sets, and create an output graph for each set of transactions. \nNow, the lexical scope for a transaction is not given to us we must infer it from the execution instances \nof the runtime statements. Consider all the runtime statements Rn with transactionId n. The identi.er \nfor transaction n is Figure 5. A set of runtime statements forming a transaction. Each column represents \na runtime statement, identi.ed by its stack trace. The identi.er for the transaction (shown in gray) \nis the common pre.x of all stack traces.  the common pre.x of the stack traces of Rn; an example is \ngiven in Figure 5. Transactions with the same identi.er are grouped, and a single graph is generated \nfor all transactions with the same identi.er. Thus, there are two levels of grouping: sets of runtime \nstatements are grouped into graph nodes, and a second level of grouping organizes nodes into transactions. \nIt is actually more convenient to compute the transaction grouping .rst. The full algorithm for constructing \ngraphs from runtime statements is to perform the following steps in order: 1. Compute the identi.er of \neach runtime transaction; i.e., for all runtime statements with the same transaction-Id, compute the \ngreatest common pre.x of the runtime statements stack traces. 2. Group runtime statements by the identi.er \nof their trans\u00adactionId; let these groups be G1,G2,.... Each Gi pro\u00adduces one dependency graph in the \noutput. 3. The nodes of Gi are the sets of runtime statements in Gi with the same stack trace. There \nis an edge between two nodes if two of their runtime statements are dependent, as described above.  \n Figure 6 gives an example graph for Daytrader s sell method in the TradeServices interface listed in \nFigure 4. Node weights (the number of runtime statements repre\u00adsented by each node) and edge weights \n(the number of run\u00adtime statement dependencies represented by each edge) are shown. 5. Implementation \nof the Analysis We implemented the graph construction algorithm of Sec\u00adtion 4 as a tool that uses bytecode \nrewriting to transparently analyze Java programs that use JDBC directly or indirectly. Our tool can target \nprograms hosted by a pure-Java applica\u00adtion container such as Geronimo, since the underlying layer uses \nJDBC directly. Our tool uses the ASM library [4] to add information .ow tracking to Java, and adds a \nJDBC shim to capture queries and values as they cross the boundary to the database. Our tool provides \nthe full functionality of our algorithm, except that we merge the taint for queryIds larger than 31. \nTo avoid excess object creation we encode each label in a single 64-bit value, with a 32 bit transactionId \nand 32 bits to record the presence or absence of the corresponding queryId in the label s set S. 6. Case \nStudy In this section we report on our experience applying an implementation of our method to Daytrader, \na sample ap\u00adplication included with the Apache Geronimo application server. Daytrader is built around \nthe paradigm of an on\u00adline stock trading system [7] and is designed to be a re\u00adalistic and sophisticated \nthree-tier application. The Dacapo Benchmarks [2], version 2009, include a substantial work\u00adload for \nDaytrader. Our case study is based on this workload, with a few deployment modi.cations. In particular, \nwhen evaluating performance improvements we use a higher\u00adperformance database engine running outside \nthe applica\u00adtion server, instead of the default embedded database. We did not modify the benchmark workload \n(i.e., the sample data and execution script). It is worth mentioning that a dif.culty in evaluating our \nwork is the lack of appropriate benchmarks; while three-tier applications are ubiquitous in practice, \nthey are scarce in the research community. Besides DaCapo, other candidates in\u00adclude the TORPEDO [14] \nand 007 [13] benchmarks. Unfor\u00adtunately, TORPEDO is too small to provide an interesting test of our approach, \nwhich is most useful for complex appli\u00adcations with large databases, and 007 focuses on a hierarchi\u00adcal \nobject-relational database, while we address the issues associated with traditional RDBMSs. However, \nwe believe that DaCapo is suf.ciently involved to be a reasonable rep\u00adresentative of three-tier applications \nfound in practice. The core operations implemented by the business logic layer are de.ned in the TradeServices \ninterface, a simpli\u00ad.ed version of which is shown in Figure 4. We focus here on the sell business operation, \nbecause it is central to the ap\u00adplication s performance, has one of the highest latencies, and illustrates \nall of the optimization strategies of Section 2. The statement dependency graph for the sell operation \nis shown in Figure 6. Our analysis tools caused an order-of-magnitude slowdown over running the benchmark \nwithout instrumenta\u00adtion, which is typical and acceptable for an off-line dynamic technique. Note that \nthe output of our technique does not de\u00adFigure 6. Dependency graph for the sell operation in the Daytrader \nbenchmark. Gray nodes indicate repeated queries; solid edges are dependencies of data .owing through \nthe middle-tier; dashed edges represent read-after-write dependencies through the database; dash-dot \nedges represent the original sequence between writes; dotted edges represent write-after-read dependencies \nthrough the database.   pend on the running time in any way, so the slowdown does not affect the output \nof our tool. In the following subsections we illustrate how a program\u00admer can use the information in \nthe dependency graph to iden\u00adtify opportunities for each of the optimizations, and we also present the \nmore important details of our implementation. Ultimately we are able to eliminate 8 round trips from \nthe sell operation, including 4 from statement batching and 2 each from identifying opportunities for \nasynchronous execu\u00adtion and removing redundant queries. Before applying our system to Daytrader we attempted \nto analyze the application by hand to establish an upper bound on the possible improvements. Our manual \nsearch missed several of the optimizations, and did not .nd any database optimizations that were not \nidenti.ed by our tool. 6.1 Statement Batching As described in Section 2, JDBC allows multiple write state\u00adments \nto be submitted together to the database, instead of submitting them one at a time. Because we assume \nthere is a dependency between consecutive writes, writes form an ordered sequence. Two writes can be \nbatched if the only path between them in the dependency graph consists of only write-write dependencies. \nWe want the longest sequence of writes such that each pair of writes in sequence satis.es this condition. \nThe longer the sequences, the better, because we pay one roundtrip per batch execution, regardless of \nhow many write statements are included. Write statements w1 and w2 can be batched if no data written \nby w1 is used to construct statement w2. For exam\u00adple, Daytrader s sell operation executes the following \nsix writes: 1. createOrder [line 670] 2. updateHoldingStatus [line 1277] 3. creditAccountBalance [line \n1262] 4. completeOrder [line 547] + removeHolding [line 631] 5. completeOrder [line 547] + removeHolding \n[line 639] 6. completeOrder [line 551] + updateOrderStatus [line 1288]  As Figure 6 shows, writes \n3, 4, 5 and 6 do not have any paths between them except write-write dependencies, so they should be batched \nif possible. Batching will reducing the number of roundtrips by three, as it will allow one state\u00adment \nto be sent to the database instead of four. Writes 1 and 2 can also potentially be batched, saving another \nroundtrip. Implementing statement batching can require some code restructuring by the programmer. For \nexample, JDBC does not allow different prepared statements precompiled data\u00adbase statements used to avoid \ninjection attacks and duplicate statement compilation to be executed in the same batch. Thus, the programmer \nis forced to convert any prepared statements into regular database statements before executing them in \nbatch. In addition, although statement batching is a standard in\u00adterface exposed by almost all database \ndrivers, not all drivers implement batching more ef.ciently than regular statements. We found this to \nbe the case for several database drivers, which synchronously executed batched statements one at a time \nfrom inside the client-side JDBC driver. In our ex\u00adperiments we used IBM s DB2, which properly implements \nserver-side batching. Finally, statement batching requires the programmer to maintain the list of statements \nto execute until all the state\u00adments are ready to execute together. In our case study we simply stored \neach statement to be batched in a local vari\u00adable until it was needed. More sophisticated implementa\u00adtions \ncould use a statement queue per connection that is .ushed by the programmer. In some cases statement \nbatching could be at least par\u00adtially automated. In particular, the queue could be .ushed only when the \napplication executes a read to one of the tables to be written by the batched statements, when the transac\u00adtion \ncommits, or when the programmer needs to do so manu\u00adally. The main limitation would be API compatibility: \nJDBC drivers must return the number of rows updated after execut\u00ading a write, even if the application \nhas no immediate need for that information. Extending the API to allow program\u00admers to submit a statement \nwithout waiting for it to execute would solve this problem.  6.2 Asynchronous Query Execution JDBC provides \na synchronous API: Java programs emit queries and block on the result. This is a problem when the result \nof multiple independent queries are required for a computation. In the statement dependency graph, reads \nwith no paths between them are candidates for concurrent issu\u00ading. For example, in the graph of Figure \n6 the following two pairs of queries have no known dependencies: 1. getQuoteData [line 1071] and getAccountData \n[line 987]. 2. completeOrder [line 512] + getAccountProfileData [line 1217] and completeOrder [line \n542] + getHoldingData [line 1103].  Read statement r can be delayed as long as there is no statement \ns that requires its result. A brief inspection of the code con.rms that it is safe to issue both pairs \nconcurrently. Applying these optimizations saves two roundtrips, one for each pair of queries. As with \nstatement batching, there are some important de\u00adtails that must be considered. JDBC drivers are not designed \nfor executing concurrent queries. Database connections use locks extensively to guarantee thread-safety \nand block on network calls. In addition, there is a one-to-one mapping be\u00adtween a database connection \nand its transaction, making it impossible to open multiple database connections within the same transaction. \n To provide the illusion of asynchronous query execution, our implementation maintains a pool of threads, \neach with an open database connection, that execute queries outside the main database connection. Many \napplications, and Day\u00adtrader in particular, wrap queries in methods that execute the database statement, \niterate over the result set, and re\u00adturn the data in wrapper objects. This design encapsulates each database \nstatement, making it reusable across the ap\u00adplication. However, this design also requires the entire \nresult set to be read before the method can return. To minimize the number of changes made to the application, \nwe modify such methods so that they submit the query to the thread pool and return a future to the wrapper \nobject, instead of the object itself. Futures are a standard way of representing the result of an asynchronous \ncomputation in Java. Changing the method s type signature to return a future type creates com\u00adpiler errors \nexactly where the return value of the method is used. At those locations the programmer can either force \nthe future immediately (if there is no point in delaying the exe\u00adcution of the query), or keep a reference \nto the future instead of a reference to the wrapper object, delaying the query eval\u00aduation until it is \nrequired. DB2 s default isolation level (Read Committed) can be satis.ed by queries in our thread pool \nas long as the main transaction has not updated any of the accessed tables. In principle, all connections \nto the database should be auto\u00adcommit, because they never execute more than a single statement. However, \nwe found that executing a single state\u00adment in an auto-commit connection requires two roundtrips: one \nto obtain the result, and one to commit the transaction. Our implementation avoids this latency by arranging \nfor the worker threads to return the result of their read prior to a manual commit of the reading transaction. \nWorker threads only return themselves to the pool after this cleanup is com\u00adpleted. Our concurrent query \nexecution implements a read com\u00admitted isolation level, even if the underlying database state\u00adments are \nexecuted at serializable isolation. All of our other optimizations preserve read committed or serializable \niso\u00adlation. This strategy is correct because DayTrader on DB2 already uses read committed isolation. \nIn general, program transformations may actually streng\u00adthen a weaker isolation level such as read uncommitted. \nIf the application relies on repeating a query as an ad-hoc form of inter-transaction communication, \nfor example, merging duplicate queries may result in livelock. In this particular example examining the \ndata dependence graph will reveal the cycle, but there may be other scenarios where the effect of explicitly \nweak isolation levels is not apparent in the graph. This is an advantage of an approach that guides the \nprogrammer, rather than adding an extra automatic layer that alters and complicates the existing behavior. \n 6.3 Removing Redundant Queries A straightforward way to improve the performance of any application is \nto do less work. In our context, unnecessary database statements should be removed. It is natural to \nask the question, why would there be unnecessary queries in an application in the .rst place? Object-oriented \nprogrammers build programs by com\u00adposing abstractions. They try to make these abstractions self\u00adcontained, \nso they can be reused or extended by other peo\u00adple. For example, in Daytrader the order creation process \nis encapsulated in the createOrder method, which returns an Order object used by different business operations. \nUnnec\u00adessary statements can appear when two of these abstractions are put together. Two signs in the \ngraph point to redundant queries. First, queries that always return the same result are likely re\u00addundant. \nSecond, read-after-write dependencies through the database are always suspicious: Why would an application \nexecute an expensive database query to retrieve data it wrote earlier? If the data is already in memory \nit might not be nec\u00adessary to retrieve it from the database. In the sell operation of Figure 6 two sets \nof statements appear to be redundant: 1. createOrder [line 670]; createOrder [line 674] + getOrderData \n[line 1137]; and completeOrder [line 476]. 2. updateHoldingStatus [line 1277] and completeOrder [line \n542] + getHoldingData [line 110].  These sets of statements point to speci.c places in the source code \nwhere a programmer can look to determine if any of the queries is redundant. Removing redundant queries \nis more application depen\u00addent that either statement batching or concurrent query ex\u00adecution. For Daytrader, \nwe analyzed the two sets of queries given above and found the following: 1. Both the createOrder method \nand the completeOrder methods unnecessarily query the database for an order that is already in memory. \nRemoving these queries saves two roundtrips to the database. 2. The completeOrder [line 542], getHoldingData \n[line 1103] query reads data inserted by updateHold\u00adingStatus [line 1277]. However, this update is used \nto signify the sell is in.ight , according to the devel\u00adoper s comments. We decided not to remove this \nsignal\u00ading mechanism.   login register buy sell update Statement batching 0 1 2 4 0 Concurrent queries \n1 0 2 2 0 Removing queries 0 0 2 2 1 Total 1 1 6 8 1 Original statements 3 2 11 13 2  Figure 7. Improvements \nfor the .ve multi-statement oper\u00adations exercised by the Dacapo benchmark. In all multi\u00adstatement operations \nour tool found optimization opportu\u00adnities. 6.4 Additional Operations In addition to the sell operation, \nwe applied our tool to all operations exercised by the Dacapo benchmark. Figure 7 shows the number of \nroundtrips saved in each operation. In .ve out of nine operations we found optimization opportuni\u00adties, \nwhich we implemented as explained earlier this section. The remaining four operations executed a single \nread. In summary, our tool found optimization opportunities in every multi-statement database transaction \nexercised by Dacapo. 7. Performance Results Figure 9 shows the results of implementing all the opti\u00admizations \ndescribed in previous section for the sell oper\u00adation. Experiments were run on a Dell Precision T7500n \nwith two quad-core 2.66Ghz Intel Xeon X5550 processors and 24GB of RAM. Linux kernel version 2.6.28-16-server \nwas used, and hyper-threading was enabled, yielding a total of 16 hardware thread contexts. We ran our \nexperiments in Sun s Java(TM) SE Runtime Environment, build 1.6.0 21\u00adb06, using the HotSpot 64-Bit Server \nVM. For the database backend we used DB2 Enterprise Server v9.7.0.0, running in the same machine as the \napplication server. Dacapo s workload for Daytrader exercises the business layer directly. To simulate \nconcurrent users it keeps a set of client threads that take operations from a shared queue and execute \nthem against the business layer. We measure the latency of the sell operation as seen from one of these \nclient threads. Optimizations were implemented one after the other, even though they can be implemented \nindependently, to make the results of each optimization clearer. Figure 8 summarizes the number of roundtrips \nsaved for each optimization. Fig\u00adure 9 (top) shows the latency of the sell operation as the network delay \nincreases. Reducing the number of roundtrips to the database reduces the latency of the entire operation, \nas expected. Figure 9 (bottom) shows the latency of the sell operation versus the number of client threads. \nThe curves are reasonably .at, indicating that the machines have not satu\u00adrated and the network latency \ndominates the overall latency of the operation. Expected Estimated from measured improvement 16 ms 8 \nms 4 ms Statement batching Concurrent execution Removing queries 4 2 2 3.75 2.04 2.04 3.55 1.92 2.15 \n2.75 2.13 2.53 Total 8 Figure 8. Number of roundtrips eliminated for the sell operation, by type of \noptimization. We reduce up to eight round-trips to the database. As the network delay increases the measured \nimprovement more closely matches the pre\u00addicted improvements.   8. Related work Previous work on optimizing \nthree-tier applications has fo\u00adcused on automatic techniques for caching and prefetching the results \nof database queries [9, 18, 23]. There is over\u00adlap with the optimizations we have proposed as example \napplications of our dependency graphs: caching can hide the cost of redundant queries and prefetching \neffectively saves roundtrips to the database by eliminating the latency of correctly predicted queries. \nHowever, while automatic tech\u00adniques will work well in many typical situations, they have a limited view \nof the program and will not by themselves achieve acceptable levels of performance in every situation. \nThere is still a need for programmers to understand what is happening in a three-tier program so that \nthey can restruc\u00adture the program to improve performance, either entirely by hand or just enough that \nthe automatic optimizations work as intended. Thus, our approach complements the work on automatic optimization \nof three-tier applications. Several efforts have looked at more tightly integrating the programming language \nand the database, providing a bet\u00adter programming model than SQL statements bolted on to a stock language \n[10, 22]. Besides the better abstractions for the programmer, any such language system would pre\u00adsumably \nhave stronger built-in semantics for the persistent store and thus begin in a better position for tools \nto reason about the performance of multi-tier applications. However, the need for programmers to understand \nthe performance of such applications would not be eliminated, and we expect that dependency graphs much \nlike we propose would be a natural medium of communication with programmers even in such higher-level \nlanguages. Some previous efforts examine different aspects of ana\u00adlyzing multi-language systems, although \nwe are not aware of any that speci.cally target performance in multi-tier sys\u00adtems. Moise and Wong [15] \ndescribe a system to infer source dependencies between multi-language systems, e.g. for a mixed Java/C++ \nprogram they determine which Java classes call a particular C++ function. Strein et al. [21] describe \na prototype IDE that allows refactoring of mixed-language programs (speci.cally, C and Visual Basic). \nSalah et al. [19] describe a system that tags execution traces with a user\u00adde.ned mark, which is useful \nfor .nding which parts of the source code implement which user-facing functionality. Dynamic taint analysis \nis widely-used in security re\u00adsearch. Schwartz et al. [20] present a good overview of the technique and \nformalize it for a simple intermediate lan\u00adguage. They also discuss the trade-offs and challenges when \nchoosing taint injection, propagation and checking policies. As discussed in [5], techniques for taint \ntracking in one lan\u00adguage do not necessarily port well to other languages: there are many intricacies \ninvolved in building a complete, accu\u00adrate taint propagation mechanism. For example, we make essential \nuse of a trick due to [1] to instrument the entire JDK in the presence of Java s dynamic loading. RoadRunner \n[6] is a Java instrumentation framework for rapid prototyping of dynamic analysis tools. RoadRunner has \nbeen successfully used to implement data race and atom\u00adicity violation detection tools. RoadRunner s \nframework is not general enough to implement our taint .ow algorithm, primarily because it lacks shadow \nvalues for local variables and stack operands and it is not possible to propagate taint on arithmetic \noperations and assignments. Luo et al. [12] describe a system in which multiple database statements that \nspecify associative and commu\u00adtative operations (e.g., an increment operation) over a set of tuples are \nmerged into a single statement. They also propose grouping transactions that contain operations over \nthe same set of tuples, thus increasing the number of database state\u00adments that can be merged. Their \nmethod is limited by the fact that most database drivers (and JDBC in particular) provide a synchronous \ninterface to their clients: One query cannot be emitted before the previous one has .nished. Therefore, \nthe database engine will receive only one query at a time. Their method would be well suited for improving \nthe performance of batched database statements like the ones produced after refactoring an application \nusing our tool. Bogle and Liskov [3] propose batched futures, a mecha\u00adnism for reducing the cost of database \ncalls. Instead of exe\u00adcuting calls when the client requests them, they delay queries until their value \nis needed. At that point several calls may have been requested, and they can be executed in batch. Our \nimplementation of concurrent queries has the same purpose, but eagerly executes queries in multiple database \nconnec\u00adtions instead of batching them. The speedup of batched fu\u00adtures is limited by the number of operations \nthat can be de\u00adferred; our tool helps the programmer reorganize the code to maximize the number of deferred \noperations. For Java applications Heath [8] proposes an asynchronous database driver using both thread-pools \nand non-blocking socket I/O. There are asynchronous database drivers for MySQL and Drizzle [17]. Python \ns event-driven network\u00ading engine Twisted has an asynchronous wrapper over the standard (synchronous) \nPython database API [11]. The de\u00adpendency information produced by our method would help a programmer \nport an existing application to use any of these asynchronous database drivers. 9. Conclusion Understanding \nthe performance of database operations in a three-tier system is dif.cult because they may be far apart \nin the source code, hidden behind abstraction barri\u00aders, connected by complicated control-.ow structures, \nor they might interact only through the database. As the num\u00adber of database statements in the application \nincreases, the number of possibilities and the number of ways in which connections can be obfuscated \ngrows super-linearly. Our de\u00adpendency graph solves most of these problems.  Our goal in this work is \nto examine how program analy\u00adsis technology can be used to understand and optimize long latency database \noperations, which are often a performance bottleneck in commercial three-tier applications. We have identi.ed \nthree simple transformations that reduce the num\u00adber of, or hide the latency of, database roundtrips: \nstate\u00adment batching, asynchronous query execution, and redun\u00addant query elimination. Identifying where \nthese transforma\u00adtions may be applied requires dependency information that spans both the application \nand the database. We use dynamic taint analysis to .nd dependencies through the middle-tier, and database \nstatement commuta\u00adtivity to infer dependencies through the database. In addi\u00adtion, we automatically group \nsimilar transactions to present an easier-to-understand result to the programmer. We have also presented \nan extended case study where we applied an implementation of our methods to a three-tier Java applica\u00adtion, \nand found it to be useful in improving its performance by reducing the latency of its operations. References \n[1] W. Binder, J. Hulaas, and P Moret. Reengineering standard Java runtime systems through dynamic bytecode \ninstrumen\u00adtation. In Source Code Analysis and Manipulation, 2007. SCAM 2007. Seventh IEEE International \nWorking Conference on, pages 91 100, Sep 2007. [2] S. Blackburn, R. Garner, C. Hoffmann, A. Khang, K. \nMcKin\u00adley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Guyer, M. Hirzel, A. Hosking, M. Jump, \nH. Lee, J. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, D. von Dinck\u00adlage, and B. Wiedermann. The \nDaCapo benchmarks: Java benchmarking development and analysis. In Proceedings of the 21st annual ACM \nSIGPLAN conference on Object\u00adoriented programming systems, languages, and applications, OOPSLA 06, pages \n169 190. ACM, 2006. [3] P. Bogle and B. Liskov. Reducing cross domain call overhead using batched futures. \nIn Proceedings of the ninth annual con\u00adference on Object-oriented programming systems, language, and \napplications, OOPSLA 94, pages 341 354. ACM, 1994. [4] E. Bruneton, R. Lenglet, and T. Coupaye. ASM: \nA Code Ma\u00adnipulation Tool to Implement Adaptable Systems, Technical report, France Telecom R&#38;D, 2002. \n[5] E. Chin and D. Wagner. Ef.cient character-level taint tracking for Java. In Proceedings of the 2009 \nACM workshop on Secure web services, SWS 09, pages 3 12. ACM, 2009. [6] C. Flanagan and S. N. Freund. \nThe RoadRunner dynamic analysis framework for concurrent programs. In Proceed\u00adings of the 9th ACM SIGPLAN-SIGSOFT \nworkshop on Pro\u00adgram analysis for software tools and engineering, PASTE 10, pages 1 8. ACM, 2010. [7] \nThe Apache Software Foundation. Apache Geronimo v2.0 Documentation : Daytrader. https://cwiki.apache.org/ \nGMOxDOC20/daytrader.html/, 2011. [8] M. Heat. Asynchronous database drivers. Master s thesis, Brigham \nYoung University, April 2011. [9] A. Ibrahim and W. R. Cook. Automatic prefetching by traver\u00adsal pro.ling \nin object persistence architectures. In Proceed\u00adings of the European Conference on Object Oriented Pro\u00adgramming, \npages 50 73, 2006. [10] M. Iu and W. Zwaenepoel. Queryll: Java database queries through bytecode rewriting. \nIn Proceedings of the Interna\u00adtional Conference on Middleware, pages 201 218, 2006. [11] Twisted Metric \nLabs. Twisted Documentation: twisted.enterprise.adbapi: Twisted RDBMS support. http://twistedmatrix.com/documents/current/ \ncore/howto/rdbms.html, 2011. [12] G Luo and M Watzke. . . . Grouping database queries and/or transactions, \nOctober 2010. [13] D. DeWitt M. Carey and J. Naughton. The 007 benchmark. SIGMOD Record, 22(2):12 21, \n1993. [14] B. E. Martin. Uncovering database access optimizations in the middle tier with torpedo. In \nProceedings of the International Conference on Data Engineering, pages 916 929, 2005. [15] D. L. Moise \nand K. Wong. Extracting and representing cross\u00adlanguage dependencies in diverse software systems. In \nPro\u00adceedings of the 12th Working Conference on Reverse Engi\u00adneering, pages 209 218. IEEE Computer Society, \n2005. [16] J Newsome and D Song. Dynamic taint analysis for automatic detection, analysis, and signature \ngeneration of exploits on commodity software. In Proceedings of the 12th Annual Network and Distributed \nSystem Security Symposium, NDSS 05, page 18. Internet Society, 2005. [17] Drizzle Project. Drizzle client \n&#38; protocol library. https: //launchpad.net/libdrizzle, 2011. [18] A. Rama, G. Yorsh, M. Vechev, and \nE. Yahav. Spring: Spec\u00adulative prefetching of remote data. In Proceedings of the Conference on Object-Oriented \nProgramming, Systems, Lan\u00adguages, and Applications, 2011. [19] M. Salah, S. Mancoridis, G. Antoniol, \nand M. Di Penta. Scenario-driven dynamic analysis for comprehending large software systems. In Proceedings \nof the Conference on Soft\u00adware Maintenance and Reengineering, pages 71 80. IEEE Computer Society, 2006. \n[20] E. J. Schwartz, T. Avgerinos, and D. Brumley. All you ever wanted to know about dynamic taint analysis \nand forward symbolic execution (but might have been afraid to ask). In Proceedings of the 2010 IEEE Symposium \non Security and Privacy, SP 10, pages 317 331. IEEE Computer Society, 2010. [21] D. Strein, H. Kratz, \nand W. Lowe. Cross-language program analysis and refactoring. In Proceedings of the Sixth IEEE International \nWorkshop on Source Code Analysis and Manip\u00adulation, pages 207 216. IEEE Computer Society, 2006. [22] \nB. Wiedermann, A. Ibrahim, and W. R. Cook. Interprocedural query extraction for transparent persistence. \nIn Proceedings of the Conference on Object-Oriented Programming, Systems, Languages, and Applications, \npages 19 36, 2008. [23] Woong-Kee Loh Wook-Shin Han and Kyu-Young Whang. Type-level access pattern view: \nEnhancing prefetching perfor\u00admance using the iterative and recursive patterns. Information Science, 180(21), \nNovember 2010.   \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Applications that combine general program logic with persistent databases (e.g., three-tier applications) often suffer large performance penalties from poor use of the database. We introduce a program analysis technique that combines information flow in the program with commutativity analysis of its database operations to produce a unified dependency graph for database statements, which provides programmers with a high-level view of how costly database operations are and how they are connected in the program. As an example application of our analysis we describe three optimizations that can be discovered by examining the structure of the dependency graph; each helps remove communication latency from the critical path of a multi-tier system. We implement our technique in a tool for Java applications using JDBC and experimentally validate it using the multi-tier component of the Dacapo benchmark.</p>", "authors": [{"name": "Juan M. Tamayo", "author_profile_id": "81548357456", "affiliation": "Stanford Computer Science, Stanford, CA, USA", "person_id": "P3856221", "email_address": "jtamayo@cs.stanford.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford Computer Science, Stanford, CA, USA", "person_id": "P3856222", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}, {"name": "Nathan Bronson", "author_profile_id": "81361595088", "affiliation": "Stanford Computer Science, Stanford, CA, USA", "person_id": "P3856223", "email_address": "nbronson@cs.stanford.edu", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81460640494", "affiliation": "Tel-Aviv University, Tel-Aviv, Israel", "person_id": "P3856224", "email_address": "sagiv@math.tau.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384688", "year": "2012", "article_id": "2384688", "conference": "OOPSLA", "title": "Understanding the behavior of database operations under program control", "url": "http://dl.acm.org/citation.cfm?id=2384688"}