{"article_publication_date": "10-19-2012", "fulltext": "\n Optimization Coaching * Optimizers Learn to Communicate with Programmers Vincent St-Amour Sam Tobin-Hochstadt \nMatthias Felleisen PLT @ Northeastern University {stamourv,samth,matthias}&#38;#169;ccs.neu.edu Abstract \nOptimizing compilers map programs in high-level languages to high-performance target language code. To \nmost program\u00admers, such a compiler constitutes an impenetrable black box whose inner workings are beyond \ntheir understanding. Since programmers often must understand the workings of their compilers to achieve \ntheir desired performance goals, they typically resort to various forms of reverse engineering, such \nas examining compiled code or intermediate forms. Instead, optimizing compilers should engage program\u00admers \nin a dialog. This paper introduces one such possi\u00adble form of dialog: optimization coaching. An optimization \ncoach watches while a program is compiled, analyzes the results, generates suggestions for enabling further \ncompiler optimization in the source program, and presents a suitable synthesis of its results to the \nprogrammer. We present an evaluation based on case studies, which illustrate how an optimization coach \ncan help programmers achieve optimiza\u00adtions resulting in substantial performance improvements. Categories \nand Subject Descriptors D.2.6 [Software En\u00adgineering]: Integrated Programming Environments; D.3.4 [Programming \nLanguages]: Processors Compilers Keywords Optimization Coaching, Visualization 1. Compilers: A Dialog \nwith Programmers With optimizing compilers programmers can create fast ex\u00adecutables from high-level code. \nAs Knuth (1971) observed, however, [p]rogrammers should be strongly in.uenced by what their compilers \ndo; a compiler writer ... may in fact know what is really good for the programmer and would * Supported \nin part by NSF grants, the DARPA CRASH program, a grant from the Mozilla Foundation, and an NSERC scholarship \nfor V. St-Amour. Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, USA. Copyright &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. \n. . $10.00 Figure 1: Our optimization coach in action like to steer him towards a proper course. This \nviewpoint has some merit, although it has often been carried to extremes in which programmers have to \nwork harder and make unnatural constructions just so the compiler writer has an easier job. Sadly, the \ncommunication between compilers and program\u00admers has not improved in the intervening 40 years. To achieve \nhigh quality results, expert programmers learn to reverse-engineer the compiler s approach to optimization \nfrom object code generated for some source programs. With an appropriate model of the optimizer, they \ncan then write their programs to take advantage of compiler optimizations. Other programmers remain at \nthe mercy of the compiler, which may or may not optimize their code properly. Worse, if the compiler \nfails to apply an optimization rule, it fails silently, and the programmer may never know. Sometimes \neven experts cannot reliably predict the compiler s behavior. For example, during a recent discussion \nof1 about the perfor\u00admance of a ray tracer, the authors of the compiler publically disagreed on whether \nan inlining optimization had been per\u00adformed, eventually resorting to a disassembling tool. Currently, \nprogrammers seeking maximal optimization from their compilers turn to style guides (Fog 2012; Hagen 2006; \nZakas 2010) on how to write programs that play nicely with the optimizer. Naturally, the advice of such \nguides is limited to generic, program-agnostic advice. They cannot of\u00adfer programmers targeted advice \nabout individual programs. In this paper, we propose an alternative solution to this problem. Speci.cally, \nwe introduce the idea of optimiza\u00ad 1 See Racket bug report http://bugs.racket-lang.org/old/12518   \nFigure 2: Optimization Coach s analysis of our example functions (with focus on get-y-coordinate s body) \ntion coaching and illustrate it with an implementation in the Racket ecosystem (Flatt and PLT 2010). \nA compiler with an optimization coach talks back to the programmer. It ex\u00adplains which optimizations \nit performs, which optimizations it misses, and suggests changes to the source that should trigger additional \noptimization. Figure 1 presents our tool, named Optimization Coach, in action. Here the compiler points \nto a speci.c expression where the optimizer could im\u00adprove the performance of the program, along with \na partic\u00adular recommendation for how to achieve the improvement. As the .gure shows, a compiler with \nan optimization coach is no longer a capricious master but a programmer s assistant in search of optimizations. \n2. Goals and Overview An optimization coach engages the programmer in a dialog. The objective is to gather \ninformation during compilation, to analyze the information, and to present it in an easily accessible \nmanner. More concretely, an optimization coach should report two kinds of information: successes: optimizations \nthat the compiler performed on the current program.  near misses: optimizations that the compiler could \nper\u00adform if it had additional information. Since an optimiza\u00adtion coach may report many such near misses \nfor a large program, it should also rank changes according to how they may affect the overall status \nof optimization.  Throughout, the programmer remains in charge; the opti\u00admization coach merely provides \ninformation and advice. It remains the programmer s responsibility to act on these rec\u00adommendations. \nWe propose the following architecture for optimization coaches, consisting of four main phases: 1. An \ninstrumented compiler logs optimization decisions. 2. The optimization coach analyzes the resulting \nlogs to detect successes and near-misses.  3. From the results of the analysis, it generates advice \nand recommends changes to the program. 4. The optimization coach presents this information in a comprehensible \nmanner, but only on-demand.  Designing and implementing an optimization coach poses challenges at every \nstage, e.g. how to accurately reconstruct the optimization process, how to avoid the false positives \nproblem, how to present complex optimization informa\u00adtion in a comprehensible fashion. In response, we \nidentify several general concepts underlying optimizations, such as optimization failures, optimization \nproximity and irritants, making it possible to discuss optimization decisions ab\u00adstractly. Based on these \nconcepts, we present optimization\u00adand compiler-agnostic solutions to the identi.ed challenges. To validate \nthe usefulness of optimization coaching, we have implemented an optimization coach, named Optimiza\u00adtion \nCoach, that instantiates our framework on two differ\u00adent compilers that re.ect two different approaches \nto opti\u00admization. The .rst, for Typed Racket (Tobin-Hochstadt et al. 2011), handles various local type-driven \ncode specializa\u00adtion optimizations. The second, for the Racket production optimizer, handles inlining \noptimizations. Typed Racket pro\u00adgrams are also Racket programs and Optimization Coach seamlessly composes \nthe results from the two compilers. Optimization coach presents its results via syntax coloring and tool-tips \nin DrRacket (Findler et al. 2002), an integrated development environment for Racket. We then used this \npro\u00adtotype on existing Racket code bases and uncovered opti\u00admizations with minimal effort, greatly improving \nthe per\u00adformance of several benchmarks and programs. The rest of the paper starts with brief sections \non the Racket and Typed Racket compilers. The following sections discuss each of the four phases of our \nproposed architecture by .rst outlining the challenges faced by optimization coaches during that phase, \nthen explaining our solutions to these challenges, and .nally providing concrete examples drawn from \nour two prototype instantiations. The result portion of the paper ends with an empirical evaluation of \nthe effectiveness of Optimization  Figure 3: Optimization near miss involving .xnum arithmetic Figure \n4: Con.rming that the optimization failure is now .xed Coach. The paper concludes with a comparison with \nother techniques with goals similar to our own and a discussion of future work. 3. The Typed Racket Compiler \nThe Typed Racket compiler is a research compiler that com\u00adpiles Typed Racket programs to Racket programs \n(Tobin-Hochstadt et al. 2011). It uses core Racket programs as a high-level intermediate representation. \nThis representation is close to actual source programs, and most source-level information is still present. \nTyped Racket performs source\u00adto-source transformations to implement optimizations. The most important \ntransformations are type-driven code spe\u00adcializations of generic operations. For example, a generic use \nof the multiplication function can be specialized if both its arguments are .oating-point numbers, e.g. \nin de.nitions such as this one: (: add-sales-tax : Float -> Float) (define (add-sales-tax price) (* price \n1.0625)) Since Typed Racket s type system validates that this mutli\u00adplication always receives .oating-point \nnumbers as its argu\u00adments, the compiler may specialize it thus: (: add-sales-tax : Float -> Float) (define \n(add-sales-tax price) (unsafe-fl* price 1.0625)) Similarly, if type information provides guarantees that \na list is non-empty, the compiler may elide checks for null: (: get-y-coordinate : (List Integer Integer \nInteger) -> Integer) (define (get-y-coordinate 3d-pt) (first 3d-pt)) In this case, the type speci.es \nthat the input is a three\u00adelement list. Hence taking its .rst element is always safe: (: get-y-coordinate \n: (List Integer Integer Integer) -> Integer) (define (get-y-coordinate 3d-pt) (unsafe-first 3d-pt)) Figure \n2 shows how Optimization Coach informs the pro\u00adgrammer that the type specialization succeeds for such \nfunc\u00adtions. When the highlight surrounding an optimized region is clicked, the tool brings up a new window \nwith extra informa\u00adtion about that region. Hints also become available when the optimizer cannot exploit \nsome type information. Consider the following function, which indexes into a TCP packet s payload, skipping \nthe headers: (: TCP-payload-ref : Bytes Fixnum -> Byte) (define (TCP-payload-ref packet i) ; skip the \nTCP header (define actual-i (+ i 20)) (bytes-ref packet actual-i))  Figure 5: Surprising optimization \nfailure involving mixed-type arithmetic This program works, but the optimizer cannot eliminate the genericity \noverhead from the addition. Racket s addition function implicitly promotes results to bignums on over.ow, \nwhich may happen for the addition of 20 to a .xnum. There\u00adfore, the Typed Racket compiler cannot safely \nspecialize the addition to .xnum-only addition. Optimization Coach de\u00adtects this near miss and reports \nit, as shown in .gure 3. In addition, Optimization Coach suggests a potential solution, namely, to restrict \nthe argument type further, ensuring that the result of the addition stays within .xnum range. In .gure \n4 we show the result of following Optimization Coach s recommendation. Once the argument type is In\u00addex, \nthe optimizer inserts a .xnum addition for +, and Opti\u00admization Coach con.rms the optimization. Next \nwe consider a surprising optimization failure: (define (define (define IM IA IC 139968) 3877) 29573) \n(define (define (define (set! (/ (* last 42) max 156.8) (gen-random) last (modulo (+ max last) IM)) (* \nlast IA) IC) IM)) This code implements Lewis et al. (1969) s pseudo-random number generator, using mixed-type \narithmetic in the pro\u00adcess. In Racket, mixing integers and .oating-point numbers in arithmetic operations \nusually results in the coercion of the integer argument to a .oating-point value and the return of a \n.oating-point number. Therefore, the author of this code ex\u00adpected the last expression of gen-random \nto be specialized for .oating-point numbers. Unbeknownst to the programmer, however, this code suf\u00adfers \nfrom a special case in Racket s treatment of mixed-type arithmetic. Integer-.oat multiplication produces \na .oating point number, unless the integer is 0, in which case the result is the integer 0. Thus the \nresult of the above multiplication is a .oating-point number most of the time, but not always, making \n.oating-point specialization unsafe. The Typed Racket optimizer knows this fact (St-Amour et al. 2012) \nbut most programmers fail to think of it when they program. Hence, this optimization failure may surprise \nthem and is thus worth reporting. Figure 5 shows how Opti\u00admization Coach explains this failure. Again, \nthe programmer can respond to this recommenda\u00adtion with the insertion of coercions: (define (gen-random) \n(set! last (modulo (+ (* last IA) IC) IM)) (/ (* max (exact->inexact last)) (exact->inexact IM))) Optimization \nCoach con.rms that this change enables fur\u00adther optimization. 4. The Racket Compiler The Racket compiler \nis a mature ahead-of-time compiler that has been in development for 17 years and comes with a sophisticated \noptimizer. It compiles Racket programs to bytecode, which the Racket virtual machine then translates \nto machine code just in time, at the .rst call of each function. This runtime code-generation does not \nperform signi.cant additional optimization. The compiler is written in C and consists of several passes. \nThe .rst pass, which we focus on in this paper, features a large number of optimizations, including inlining \nas well as constant and copy propagation plus constant folding. Sub\u00adsequent passes perform closure conversion \n(Appel and Jim 1989) and lambda lifting. Optimization Coach provides two kinds of information concerning \ninlining transformations: which functions are inlined and how often.  which functions are not inlined, \nand why.  Both kinds of information are associated with function def\u00adinitions. This is no accident; \nwhile inlining is an optimiza\u00adtion that happens at call sites, the only way programmers can control the \nprocess is through the de.nition. Therefore, any  Figure 6: Optimization Coach con.rming that inlining \nis happening (with focus on inS) information that would help the user get the most out of the inliner \nhas to be explained in terms of the de.nition sites. To illustrate the kind of information provided by \nOpti\u00admization Coach, let us consider two functions implementing geometric formulas: inS checks whether \na point is inside a square, and inC checks whether a point is inside a circle. The programmer introduced \nthese helper functions to make the surrounding program readable, but it would be unfortu\u00adnate if this \nrefactoring were to result in extra function calls, especially if they were to hurt the program s performance. \nAs shown in .gure 6, the Racket inliner does inline inS at all of its call sites. In contrast, the inC \nfunction is not inlined to a satisfac\u00adtory level, as evidenced by the red highlight in .gure 7. This \nmay be indicative of low-hanging optimization fruit. One way to resolve the issue is to use a Racket \nmacro to force inlining. When we follow this advice, Optimization Coach con.rms that the manual inlining \nis successful. Breaking up the function into smaller pieces might also work. 5. Optimizer Instrumentation \nIn order to explain an optimizer s results to programmers, we must discover what happens during optimization. \nOne option is to reconstruct the optimizations via analysis of the com\u00adpiler s output. Doing so would \nmimic the actions of highly\u00adexpert programmers with a thorough understanding of the compiler with all \ntheir obvious disadvantages. In addition, it would forgo all the information that the compiler gener\u00adates \nduring the optimization phase. Our proposed alternative is to equip the optimizer with instrumentation \nthat gathers information as optimizations are performed or rejected. De\u00adbugging tools that explain the \nbehavior of compilers or run\u00adtime systems (Clements et al. 2001; Culpepper and Felleisen 2010) have successfully \nused similar techniques in the past. The goal of the instrumentation is to generate a complete log of \nthe optimization decisions that the compiler makes as it compiles a given program. In particular, Optimization \ncoaching is concerned with reporting near misses and unex\u00adpected optimization failures. For this reason, \nmerely logging optimization successes is insuf.cient. We also need to log cases where optimizations were \nnot applied, i.e. optimiza\u00adtion failures. While most of these failures are not interesting to the programmer, \npruning the log is left to a separate phase. The following two subsections explain the instrumenta\u00adtion \nfor the Racket and Typed Racket compilers. 5.1 Instrumentation of Typed Racket As mentioned, the Typed \nRacket compiler mostly performs source-to-source optimizations. These optimizations are im\u00adplemented \nusing pattern matching and templating, e.g.: (pattern (+ f1:float-expr f2:float-expr) #:with opt # (unsafe-fl+ \nf1.opt f2.opt)) Each optimization is speci.ed with a similar pattern con\u00adstruct. The only factors that \naffect whether a term is opti\u00admized are its shape in this case, an AST that represents the application \nof the addition function its type, and the type of its subterms. Instrumenting the optimizer to log optimization \nsuccesses is straightforward; we add a logging statement to each pat\u00adtern describing which optimization \nhappened, the code in\u00advolved, its source location, and the information that affected the optimization \ndecision (the shape of the term, its type and the type of its subterms): (pattern (+ f1:float-expr f2:float-expr) \n#:with opt (begin (log-optimization \"binary float addition\" this-syntax) # (unsafe-fl+ f1.opt f2.opt))) \nWhen we compile the add-sales-tax and get-y\u00adcoordinates functions from .gure 2, the instrumented optimizer \ngenerates this log, con.rming that it applies the optimizations mentioned above:  Figure 7: The inC \nfunction, failing to be inlined (with focus on inC) TR opt: TR-examples.rkt 5:2 (* price 1.0625) --Float \nFloat --binary float multiplication TR opt: TR-examples.rkt 10:2 (first 3d-pt) --(List Integer Integer \nInteger) --pair check elimination To log optimization failures, we add an additional pat\u00adtern form that \ncatches all non-optimized additions and does not perform any optimizations: (pattern (+ n1:expr n2:expr) \n#:with opt (begin (log-optimization-failure \"generic addition\" this-syntax) this-syntax)) ; no change \nThis pattern is general enough to produce super.uous log\u00adging information: to avoid generating excessive \namounts of such information, we restrict failure logging to terms that could at least conceivably be \noptimized. For instance, we log additions that are not specialized, but we do not log all non-optimized \nfunction applications. As in the optimization success case, the logs contain the kind of optimization \nconsidered, the term involved, its type, and the types of its subterms. The following log entry shows \nevidence for the failed optimization in the TCP-payload\u00adref function of .gure 3: TR opt failure: tcp-example.rkt \n5:18 (+ i 20) --Fixnum Positive-Byte --generic addition Overall, the instrumentation is fairly lightweight. \nEach optimization clause is extended to perform logging in addi\u00adtion to optimizing; a few catch-all clauses \nlog optimization failures. The structure of the optimizer is unchanged.  5.2 Instrumentation of the \nRacket Inliner The Racket inliner is based on a design by Serrano (1997) and is a much more sophisticated \noptimizer than the Typed Racket optimizer. Its decision process is guided by multiple factors, including \nthe size of the inlining candidate and the amount of inlining that has already been performed in the \ncontext of the candidate call site. This makes the Racket in\u00adliner similar to other production compiler \noptimizers, and it also makes it unrealistic to log all the factors that contributed to a given optimization \ndecision. An additional constraint is that the inliner is a complex program, and instrumentation should \nnot increase its com\u00adplexity. Speci.cally, instrumentation should not add new paths to the inliner. Concretely, \nthis rules out the use of catch-all clauses. Finally, the Racket inliner is deep enough in the compiler \npipeline that most source-level information is not available in the internal representation. As a result, \nit becomes dif.\u00adcult to correlate optimization decisions with the original pro\u00adgram. Again, these constraints \non the available information are representative of production optimizers. To record optimization successes, \nwe identify all code paths that trigger an inlining transformation and, on each of them, log the name \nof the function being inlined and the location of the original call site. It is worth noting that precise \ninformation about the original call site may not be available at this point in the compiler; the call \nsite may also have been introduced by a previous optimization pass, in which case source location would \nbe meaningless. In general, though, it is possible to locate the call site with at least function-level \ngranularity; that is, we can usually determine the function body where the call site is located. Inlining \nis a suf.ciently general optimization that it could conceivably apply almost anywhere. This makes de.ning \nand logging optimization failures challenging. Since our goal is to ultimately enumerate a list of optimization \nnear misses and surprising failures, we need to consider only op\u00adtimization failures that directly contribute \nto near misses. For example, the failure of a large function to be inlined is an optimization failure \nthat is unlikely to be linked to a near miss or a surprising optimization failure.  Consequently we \nconsider as optimization failures only cases where inlining was considered by the compiler, but ultimately \ndecided against. We identify the code paths where inlining is decided against and add logging to them. \nAs in the case of inlining successes, we log the name of the candidate and the call site. In addition, \nwe also log the cause of failure to provide an explanation to the user. The most likely cause of failure \nis the inliner running out of fuel. To avoid code size explosions, each call site is given a limited \namount of fuel, and inlining a function inside this call site consumes a quantity of fuel proportional \nto the size of the function being inlined. If the inliner runs out of fuel for a speci.c instance, it \nis not performed and the optimization fails. For these kinds of failures, we also log the size of the \nfunction being considered for inlining, as well as the remaining fuel. Here is an excerpt from the inliner \nlog produced when compiling the binarytrees benchmark from section 9: mzc: no inlining, out of fuel #(for-loop \n41:6) in #(main 34:0) size=77 fuel=8 mzc: inlining #(loop 25:2) in #(check 24:0) mzc: inlining #(loop \n25:2) in #(check 24:0) mzc: no inlining, out of fuel #(loop 25:2) in #(check 24:0) size=28 fuel=16 mzc: \ninlining #(check 24:0) in #(for-loop 46:1) mzc: no inlining, out of fuel #(check 24:0) in #(for-loop \n46:18) size=31 fuel=24 6. Optimization Analysis Compiling a program with an instrumented compiler gen\u00aderates \na log of optimization-related decisions. We use this log as a starting point to generate a high-level \noptimization report for the programmer. Optimization logs are not directly suitable for user con\u00adsumption \ndue to several reasons. First, the sheer amount of data in these logs makes it hard to visualize the \noptimization process. Indeed, large portions of that information turn out to be irrelevant. Second, some \noptimizations are related by causality: one optimization can open up further optimization opportunities, \nor an optimization failure may prevent other optimizations from happening. In these cases, presenting \na combined re\u00adport of the related optimizations yields more useful informa\u00adtion than reporting them individually. \nOtherwise, Optimiza\u00adtion Coach would shift the burden of establishing causality to the programmer, which \nwe consider unacceptable. Finally, some optimizations are related by locality: mul\u00adtiple optimizations \nare applied to one piece of code or the same piece of code triggers optimizations in different places. \nAgain, aggregated information about these optimiza\u00adtion events helps summarize information. In the remainder \nof this section, we explain the three main phases of the optimization analysis in general terms. The \nlast two subsections illustrate them with concrete examples. 6.1 Log Pruning The sheer amount of log \ndata calls for criteria that discrimi\u00adnate good opportunities from irrelevant information. For the former, \nwe introduce the notion of optimization proximity; for the latter, we use three ground rules. Optimization \nproximity is an optimization-speci.c metric that measures how close an optimization is from happening. \nFor a particular optimization, it might be derived from the number of program changes that would be necessary \nto trigger the optimization of interest. Log entries with close proximity are retained, others are pruned \nfrom the log. Some log entries are considered incomprehensible be\u00adcause they are about code that is not \npresent in the orig\u00adinal program. Such code might be introduced by macro\u00adexpansion or by previous optimization \npasses. Since the main goal of optimization coaching is to help programmers adapt their code to enable \nfurther compiler optimization, it is of limited usefulness to report about code that is not present in \nsource programs. Other optimization failures are considered irrelevant be\u00adcause the non-optimized semantics \nis likely to be desired by design. For example, reporting that an addition could not be specialized to \n.oating-point numbers is irrelevant if the ad\u00addition is used in a generic way. Presenting recommendations \nthat programmers are most likely to reject is unhelpful, so we prune these kinds of reports. Finally, \nwe prune harmless optimization failures, a kind that we associate with opportunities due to actual opti\u00admizations. \nConsider a loop that is unrolled several times eventually, unrolling must stop. This .nal decision is \nstill reported in the log as an optimization failure, because the optimizer considered unrolling further \nbut decided against it. It is a harmless failure, however, because loop unrolling cannot go on forever. \nThese failures and others like them are therefore pruned from the logs.  6.2 Causality Merging Once \nthe log has been pruned, the optimization analyzer tries to detect clusters of optimization events that \nare related by causality. It consolidates these clusters into a single opti\u00admization event. We call this \nstep causality merging. Our no\u00adtion of causality heavily relies on the concept of irritant. In the case \nof some optimizations, we can relate an optimiza\u00adtion failure for a term to one or several of its subterms. \nThese subterms may have the wrong shape or the wrong type. Since irritants are terms, they can themselves \nbe the sub\u00adjects of optimization failures, meaning they may contain irri\u00adtants. All these related optimization \nfailures are grouped into a tree. Each failure becomes a node, and its irritants become children. Irritants \nthat do not contain irritants form the leaves of the tree; they are the initial causes of the optimization \nfail\u00adure. The optimization failure that is caused by all the others becomes the root of the tree. The \nentire tree can be merged into a single entry, with the root of the tree as its subject and the leaves \nas its irritants. The intermediate nodes of the tree can be safely ignored since they are not responsible \nfor the failure; they only propagate it.  Causality merging reduces the size of the logs and helps pinpoint \nthe program points that cause optimization failures. Furthermore, grouping these optimization events \nenables a ranking of missed optimizations according to their impact on the overall optimization status \nof the program. If we know a subexpression heads a cascade of missed optimizations, .xing it will have \na signi.cant impact. Thus, the size of the irritant tree is used in the merged log entry as a badness \nmeasure to formulate the presentation of the analysis results. In addition to being useful for causality \nmerging, the no\u00adtion of irritant is also useful to help explain optimization fail\u00adures to the programmer \nand to synthesize recommendations.  6.3 Locality Merging Next, the optimization coach synthesizes a \nsingle log entry for clusters of optimization reports that affect the same piece of code. Doing so helps \nformulate a coherent diagnosis about all the optimizations that affect a given piece of code. We call \nthis phase locality merging. Reports that are related by locality but concern unrelated optimizations \nare still grouped together, but the grouping process unions the reports without further analysis. These \ngroupings, while they do not infer new information, improve the user interface by providing all the relevant \ninformation about a piece of code in a single location. This step also aggregates log entries that provide \ninfor\u00admation too low-level to be of use to the programmer directly. New log entries report on these patterns \nand replace the low\u00adlevel entries in the log. 6.4 Analysis of Typed Racket Optimizations Log pruning \nand causality merging are particularly relevant in the context of the Typed Racket optimizer. In this \nsection we illustrate these concepts with concrete examples. For instance, the following function (: \nsum : (Listof Number) -> Number) (define (sum list-of-numbers) (if (null? list-of-numbers) 0 (+ (first \nlist-of-numbers) (sum (rest list-of-numbers))))) uses + in a generic way. This code is not specialized \nby the Typed Racket optimizer, creating the following log entry: TR opt failure: sum-example.rkt 5:6 \n(+ (first list-of-numbers) (sum (rest list-of-numbers))) --Number Number --generic addition Since the \ngeneric behavior is actually desired for sum, it should not be eliminated by optimization. Optimization \nanalysis should therefore detect these cases and remove them from the logs. To trigger a specialization \noptimization, all the arguments to a generic operation must be convertible to the same type, and that \ntype needs to be one for which a specialized version of the operation is available. We de.ne optimization \nproxim\u00adity for this optimization to be the number of arguments that would need to have a different type \nin order to reach a state where optimization could happen. For example, the combination of argument types \n(+ Float Real) is 1-close to being optimized, while this one (+ Float Real Real) is 2-close, and thus \nfurther from being optimized, since addition cannot be specialized for Real numbers, but can be for Floats. \nOnly optimization failures within a speci.c proximity threshold are kept. Our prototype uses a threshold \nof 1 for this optimization, which works well in practice. The generic addition in the sum function above \nhas a 2-close measure and is therefore ignored by Optimization Coach. Our Typed Racket prototype also \nprovides examples of causality merging. Let us consider the last part of the pseudo-random number generator \nfrom section 3: (/ (* max last) IM) While max is of type Float, last and IM have type In\u00adteger. Therefore, \nthe multiplication cannot be specialized, which causes the type of the multiplication to be Real. This \ntype, in turn, causes the division to remain generic. Here is the relevant excerpt from the logs: TR \nopt failure: prng-example.rkt 11:5 (* max last) --Float Integer --generic multiplication TR opt failure: \nprng-example.rkt 11:2 (/ (* max last) IM) --Real Integer --generic division The subject of the multiplication \nentry is an irritant of the division entry. Optimization Coach joins the two entries in a new one: the \nentire division becomes the subject of the new entry with last and IM as irritants.  6.5 Analysis of \nRacket Inlining Optimizations Optimization log entries from the Racket inliner provide two main pieces \nof information: which function is inlined, and at which call site it is inlined. On one hand, this information \nis not especially enlightening to users. On the other hand, for widely used functions, it is likely that \nthere is a large number of such reports; the number of inlining reports grows with the number of call \nsites of a function. Clusters of log entries related to inlining the same func\u00adtion are a prime target \nfor locality merging. Locality merging provides an aggregate view of a function s inlining behavior. \nBased on the ratio of inlining successes and failures, Opti\u00admization Coach decides whether the function \nas a whole is successful with regards to inlining or not.  Figure 8: Optimization Coach ordering near \nmisses by predicted importance (with focus on add-discount-and-sales\u00adtax); the most important near miss \nis in a darker shade of red and the least important one is in a lighter shade of red A function with \na high success to failure ratio is reported as an inlining success. In contrast, a function for which \nin\u00adlining failures outnumber sucesses is reported as a potential point for improvement. This ratio is \nalso used to compute the badness measure with which Optimization Coach ranks the importance of optimization \nevents. We determined the threshold ratios for successes and failures empirically; thus far, they have \nbeen accurate in practice. In the case of failures the instrumented compiler also logs the size of the \nfunction and the amount of fuel left. This information can be used to assign a weight to speci.c inlining \nfailures before computing the above ratio. When little fuel is left, only small functions can be inlined. \nIn these cases, the blame for the inlining failure lays more with the call site than with the function \nbeing called. Therefore, these failures should count for less than other failures; applying such discount \navoids penalizing the relevant function. Since Racket is a mostly functional language, loops are expressed \nas tail recursive functions; therefore, function in\u00adlining also expresses loop unrolling. An important \nrole of optimization analysis for the Racket inliner is to determine which log entries were produced \nas part of loop unrolling and which were produced as part of traditional inlining. This analysis can \nonly be performed post-facto because the same code paths apply to both forms of inlining. Considering \nunrolling and inlining separately has two bene.ts. First, as previously mentioned, unrolling failures \nare not usually problematic; any series of unrollings needs to end in a failure. These failures should \nnot be confused with inlining failures, which may actually be problematic. Treating failed unrollings \nseparately avoids false positives, which would lead to overly pessimistic reports. Second, ab\u00adsence of \ninlining successes for a given function is usually more alarming than absence of unrollings. Confusing \ninlin\u00ading and unrolling successes can lead to false negatives. A function that is never inlined, but \ndoes get unrolled several times, would be reported as an overall success. Fortunately, considering an \nunrolling of a recursive function is a simple and highly effective heuristic to identify loop unrollings. \nIt is accurate in practice and leads to improved analysis results. 7. Generating Recommendations After \noptimization analysis, reports of success and failure are at the appropriate level of abstraction. In \nthis shape, they are suitable for presentation to the programmer. Fur\u00adthermore, it is now possible to \ngenerate concrete advice from these reports when possible to help programmers elimi\u00adnate optimization \nfailures. To identify the expressions for which to recommend changes, the optimization coach reuses the \nconcept of ir\u00adritant. Recall that irritants are terms whose structure caused optimization to fail. If \nthese terms were changed to have an appropriate structure, the optimizer would be able to ap\u00adply transformations. \nIrritants are thus ideal candidates for recommendation targets. Determining the best change to a given \nirritant relies on optimization-speci.c logic. Since each optimization has its own failure modes, general \nrules do not apply here. In some cases, generating recommendations is impossi\u00adble, mostly due to the \nnature of an optimization or to the particular term. Irritants are reported nonetheless; knowing the \ncause of a failure may still help the programmer. 7.1 Recommendations for Typed Racket In the context \nof Typed Racket, Optimization Coach s rec\u00adommendations suggest changes to the types of irritants. The \n.xes are determined as part of the optimization analysis phase. Recommendation generation for Typed Racket \nis therefore a straightforward matter of connecting the facts and presenting the result in an informative \nmanner. For example, when presented with the .xnum arithmetic example from .gure 3, Optimization Coach \nrecommends changing the type of i to Byte or Index. In the case of the pseudo-random number generator \nfrom .gure 5, Optimiza\u00adtion Coach recommends changing the types of the irritants to Float to conform \nwith max.  Figure 9: A function affected by both an optimization success and an optimization near miss \n 7.2 Recommendations for Inlining Inlining is not as easy to control through changes in the source program \nas Typed Racket s type-driven optimiza\u00adtions. Therefore, recommendations relating to inlining op\u00adtimizations \nare less precise than those for Typed Racket. Since lack of fuel is the main reason for failed inlinings, \nreducing the size of functions is the simplest recommenda\u00adtion. In some cases, it is possible to give \nthe programmer an estimate of how much to reduce the size of the function, using its current size and \nthe remaining fuel. For languages with macros, an optimization coach can also recommend turning a function \ninto a macro, i.e., inlin\u00ading it manually at all call sites. To avoid in.nite expansion, Optimization \nCoach recommends this action only for non\u00adrecursive functions. Figure 7 shows examples of Optimiza\u00adtion \nCoach s recommendations related to inlining. 8. User Interface The integration of an optimization coach \ninto the program\u00admer s work.ow requires a carefully designed tool for pre\u00adsenting relevant information. \nWhile the tool must present some of the information immediately, it must hide other pieces until there \nis a demand for it. In both cases, the pre\u00adsentation must remain easily comprehensible. Optimization \nCoach is a plug-in tool for DrRacket (Find\u00adler et al. 2002). As such a tool, Optimization Coach has ac\u00adcess \nto both the Racket and Typed Racket compilers and can easily collect instrumentation output in a non-intrusive \nfash\u00adion. At the press of a button, Optimization Coach compiles the program, analyzes the logs, and presents \nthe results. As our screenshots show, Optimization Coach highlights regions that are affected by either \noptimizations or near misses. To distinguish the two, green boxes highlight opti\u00admized areas and red \nboxes pinpoint areas affected by near misses. If a region is affected by both optimizations and missed \noptimizations, a red highlight is used; missed opti\u00admizations are more cause for concern than optimizations \nand should not be hidden. The scale-y-coordinate function from .gure 9 contains both an optimization \nsuccess taking the .rst element of its input list and an optimization near miss scaling it by a .oating-point \nfactor. The user interface uses different shades of red to express an ordering of near misses according \nto the number of op\u00adtimization failures involved. Optimization Coach uses the badness measure introduced \nin section 6.2 to generate this ordering. Figure 8 shows ordering in action; the body of add-sales-tax \ncontains a single near miss and is there\u00adfore highlighted with a lighter shader of red, distinct from \nthe body of add-discount-and-sales-tax, which con\u00adtains two near misses. Clicking on a region brings \nup a tool-tip that enumerates and describes the optimization events that occurred in the re\u00adgion. The \ndescription includes the relevant code, which may be a subset of the region in the case of nested optimizations. \nIt especially highlights the irritants and, with these, explains the event. Finally, the window also \ncomes with recommen\u00addations when available. 9. Evaluation To validate our optimization coach empirically, \nwe con\u00adducted two experiments to con.rm that our optimization coach can .nd critical optimizations and \nthat following its recommendation improves the performance of programs. First, we ran Optimization Coach \non non-optimized ver\u00adsions of .ve small benchmarks from the Computer Lan\u00adguage Benchmark Game,2 followed \nthe recommendations, measured the performance of the resulting programs, and 2 http://shootout.alioth.debian.org \n  compared it to versions of the programs that had been hand\u00adoptimized by experts. Second, we identi.ed \ntwo small but performance-intensive Racket applications and used Opti\u00admization Coach to identify missed \noptimizations.3 9.1 Benchmarks The selected benchmark programs were highly tuned by ex\u00adpert Racket programmers. \nFor each program, we also bench\u00admark the non-optimized version from which the highly\u00adoptimized version \nis derived. Then we used the optimization coach to construct a third version of each benchmark. Specif\u00adically, \nwe ran Optimization Coach on the non-optimized ver\u00adsion and followed all of its recommendations. Figure \n10 compares the running time of the three ver\u00adsions, and illustrates how close we get to the highly-tuned, \nhand-optimized version of each benchmark just by follow\u00ading Optimization Coach s recommendations. On \nthree of the benchmarks, Optimization Coach s recommendations trans\u00adlate to signi.cant performance improvements. \nOn two of these three, the Optimization Coach-optimized version is al\u00admost as fast as the hand-optimized \nversion. In addition to this quantitative evaluation, we performed a qualitative evaluation by comparing \nthe speci.c opti\u00admizations recommended by Optimization Coach to those performed by experts. For all the \nbenchmarks, the hand\u00adoptimized version includes all the optimizations recom\u00admended by Optimization Coach. \nNo false positives and no novel optimizations were found. Experts also performed op\u00adtimizations outside \nthe domains covered by Optimization Coach, suggesting potential areas for future improvements. Here are \nspeci.c comments on the benchmark programs: 3 See http://github.com/stamourv/oopsla2012-benchmarks for \nthe complete code for the experiments. All our benchmarking timings report the average of 10 runs on \na 2.50GHz Intel Core 2 Quad 8300 processor with 2 GB of memory running Ubuntu 11.10. binarytrees Optimization \nCoach does not provide useful recommendations for improving optmization. However, it does con.rm that \nall helper functions are inlined as expected. The performance gains observed in the hand-optimized ver\u00adsions \ncome almost entirely from a single optimization: all the data structures were changed from structures \nto vectors, which have a lower allocation overhead. This transforma\u00adtion is not worth recommending in \ngeneral because struc\u00adture semantics is often desirable. However, an optimization coach with access to \npro.ling information could recommend changing structures to vectors if structures are allocated in a \nhot loop. This integration is a key area of future work. heapsort Optimization Coach .nds most of the \noptimiza\u00adtions present in the hand-optimized version. The heapsort implementation is written in Typed \nRacket and most opti\u00admizations are type-related. Optimization Coach successfully recommends changing \ntypes and inserting coercions in all the places where the expert programmer inserted them. Man\u00adual vector \nbounds-check elimination explains the remaining performance gains in the hand-optimized version. mandelbrot \nThe hand-optimized version bene.ts from two major optimizations: the manual inlining of a helper function \ntoo large to be inlined by the optimizer and the manual un\u00adrolling/specialization of an inner loop. Optimization \nCoach detects that the helper function is not being inlined and rec\u00adommends using a macro to force inlining, \nwhich is exactly what the expert did. Optimization Coach does not recom\u00admend manually unrolling the loop; \nits recommendation gen\u00aderation process does not extend to unrolling. moments The expert-optimized version \nof the moments benchmark, which also uses Typed Racket, includes replac\u00ading types with low optimization \npotential with types with greater potential. In places, this transformation involves adding dynamic coercions \nto the desired type when the type system cannot guarantee the desired type bound. Optimiza\u00adtion Coach \nrecommends the same type changes performed by the expert. However, Optimization Coach s recommenda\u00adtions \ndo not place the coercions in the same locations as the expert s. The expert placed the coercions in \nlocations where their runtime overhead is low, which is responsible for the performance gains of the \nhand-optimized version over the Optimization Coach-optimized version. nbody Optimization Coach identi.es \nexpressions where the Typed Racket compiler must conservatively assume that sqrt may produce complex \nnumbers, leading to optimiza\u00adtion failures. Following Optimization Coach s recommenda\u00adtions, we replaced \nuses of sqrt by flsqrt, which is guar\u00adanteed to return .oating-point numbers, leading to another optimization \nvisible in the hand-optimized version. The rest of the performance improvements in the hand-optimized \nver\u00adsion are due to replacing structures and lists with vectors, as in binarytrees.  9.2 Full Applications \nTo measure Optimization Coach s effectiveness in a realis\u00adtic setting, we applied it to two Racket applications: \na simple video chat client written by Tony Garnock-Jones (Northeast\u00adern University Programming Research \nLab) and a ray tracer for rendering icons written by Neil Toronto (Brigham Young University). Messrs. \nGarnock-Jones and Toronto are highly experienced Racket programmers, and both worked hard to make their \napplications performant. Our experiment proceeded as follows. First, we ran Opti\u00admization Coach on each \ncode base. Second, we modi.ed the programs following the recommendations that Optimization Coach considered \nmost important. Finally, we measured the performance impact of the changes using the same setup as the \nbenchmark measurements. Figure 11 shows our perfor\u00admance measurements, in the same format as .gure 10. \nVideo Chat The .rst application we studied is a simple video chat client. For scale, the chat client \nis 860 lines of Racket code. We focused our effort on the simple differential video coder-decoder (codec) \nin the client. To measure the performance of each version of the codec, we timed the decoding of 600 \npre-recorded video frames. The code mostly consists of bitwise operations on pixel values. Optimization \nCoach uncovered additional opportunities for the Racket optimizer. The decoding computation is spread \nacross several helper functions. Optimization Coach con.rmed that Racket in\u00adlines most of these helper \nfunctions, avoiding extra func\u00adtion call overhead and enabling other optimizations. How\u00adever, the three \nlargest helper functions (kernel-decode, clamp-pixel and integrate-delta) were either not in\u00adlined to \na satisfactory level or not inlined at all. Optimization Coach predicted that inlining kernel-decode \nand clamp\u00adpixel would have the biggest impact on performance and highlighted them in a darker shade of \nred than integrate\u00addelta, as decribed in section 8. We followed these two recommendations, turning both \nhelper functions into macros. Each change was local, re\u00adquired only a change to a single line of code \nand did not require understanding the behavior of the function or its role in the larger computation. \nThese two changes are solely re\u00adsponsible for the speedup shown in .gure 11. Afterwards, we followed \nthe last, weaker, recommenda\u00adtion: inlining integrate-delta. The impact on perfor\u00admance was negligible. \nThis suggests that, at least for this program, Optimization Coach s recommendation ordering heuristics \nare accurate and useful in practice. However, the tool can order recommendations only relative to each \nother. After the important recommendations were followed, it be\u00adcame impossible to tell whether the impact \nof the last one would be signi.cant. This points out an interesting limi\u00adtation. Running Optimization \nCoach on programs with lit\u00adtle potential for optimization is likely to yield only low\u00adimpact recommendations, \nbut programmers cannot learn this by looking at the output of the tool alone. Addressing this limitation \ncalls for future work, possibly the integration of a pro.ling tool. Ray Tracer The second application \nwe studied is a ray tracer that is used to render the logos and icons used by DrRacket and the Racket \nwebsite. For scale, the ray tracer is 3,199 lines of code. It supports a wide variety of features, such \nas refraction, multiple highlight and re.ection modes, plus text rendering. Over time, most icons in \nthe Racket codebase have been ported to use this ray tracer, and it eventually became a bot\u00adtleneck in \nthe Racket build process. Its author spent signi.\u00adcant time and effort4 to improve its performance. To \ndetermine whether an optimization coach would have helped him, we attempted to optimize the original \nversion of the ray tracer ourselves using Optimization Coach. For our measurements, we rendered a 600 \n\u00d7 600 pixel logo using each version of the ray tracer. Starting from the original version of the ray \ntracer, we ran Optimization Coach on the .les containing the core of the computation. Optimization coach \nidenti.ed three helper functions that Racket failed to inline. We followed its rec\u00adommendations by turning \nthe three functions into macros. As with the video codec, the changes were local and did not require \nknowledge of the code base. Furthermore, diagnosis was entirely automatic; the tool pinpointed the exact \nlocation of the recommended change. The ray tracer s author had independently performed the same changes, \nwhich were responsible for most of the speedups over his original ray tracer. Optimization Coach successfully \nidenti.ed the same sources of the performance 4 Leading to the discussion mentioned in the introduction. \n bugs as a Racket expert and provided solutions, making a strong case for the effectiveness of optimization \ncoaching. 10. Related Work Optimization coaching assists programmers with .nding missed optimizations \nand gives suggestions on how to en\u00adable them. We brie.y survey previous efforts that pursue related goals. \n10.1 Pro.lers Programmers use pro.lers (Altman et al. 2010; Ammons et al. 2004; Jovic et al. 2011) heavily \nwhen diagnosing perfor\u00admance issues. Pro.lers answer the question Which pieces of the program take a \nlong time? but programmers still need to determine which parts of a pro\u00adgram take an abnormally long \ntime before they can address performance issues. Optimization coaches, in contrast, answer a different \nquestion, namely Which pieces could still be optimized? and the answers identify the location of potential \ncode im\u00adprovements. Optimization coaches are unaware of which parts of the program are heavily executed, \nthough, and may suggest recommendations about infrequently used regions. Such recommendations are unlikely \nto lead to signi.cant performance improvements. Similarly, optimization coaches may recommend performing \noptimizations that, despite ap\u00adplying to hot code, have only minimal effect on performance. It is up \nto programmers to judge which recommendations are likely to have a signi.cant impact on the resulting \nper\u00adformance of the program and to focus on those. This duality suggests that pro.lers and optimization \ncoaches are comple\u00admentary tools and should be used together. Combining the two into a single, uni.ed \ntool is a direction for future work. Furthermore, for pro.lers to produce meaningful output, they need \nto run programs with representative input, but performance-heavy inputs may appear only after deploy\u00adment, \nat which point .xing performance bugs becomes sig\u00adni.cantly more expensive than during development. In \ncon\u00adtrast, optimization coaching operates statically, in the ab\u00adsence of program input. Then again, an \noptimization coach reports observations about the optimizer s omissions, and a .x may or may not impact \nthe performance of the program. Finally, pro.lers also do not interpret their results to ex\u00adplain what \ncauses speci.c program regions to be slow nor do they provide recommendations for improvement. The Zoom5 \nsystem-wide pro.ler is an exception, providing hints to programmers about possibly slow operations. However, \nZoom describes their operations at the assembly instruc\u00adtion level, which makes it challenging for programmers \n5 http://www.rotateright.com/zoom.html especially for non-experts to act on these recommenda\u00adtions using \na high-level language. 10.2 Analysis Visualization A large number of tools exist for visualizing analysis \nresults, of which MrSpidey (Flanagan et al. 1996) is an early ex\u00adample. Some of these tools focus on \nhelping programmers understand and debug their programs; others help compiler writers understand and \ndebug their analyses. Two recent efforts additionally aim to help programmers optimize their programs. \nLhot\u00e1k s work (Lhot\u00e1k et al. 2004; Shaw 2005) introduces a plug-in for the Eclipse IDE that displays \nthe results of static analyses computed by Soot (Vall\u00e9e-Rai et al. 2000), an optimization framework for \nJava bytecode. While most of these visualizations are targeted at compiler researchers or students learning \nabout compilers, their visualization (Shaw (2005), page 87) of Soot s array bounds check analysis (Qian \net al. 2002) informs program\u00admers about provably safe array accesses. Similarly, their vi\u00adsualization \nof loop invariants (Shaw (2005), page 99) high\u00adlights expressions that, according to Soot s analysis, \ncan safely be hoisted out of the loop by the compiler. Although these visualizations are relevant for \nprogram\u00admers concerned with optimization, they differ from those of an optimization coach in two major \nways. First, they report the results of an analysis, not those of the optimizer. The two are closely \nrelated,6 but analysis information is only a proxy for the decisions of the optimizer. Second, while \nLhot\u00e1k s tool reports potentially unsafe array accesses and explains which bounds are to blame it does \nnot attempt to distinguish between expected failures and near misses. In contrast to Optimization Coach, \nit also fails to issue recommendations concerning which piece of code needs to change to eliminate the \narray bounds check. JIT Inspector7 is a Firefox extension that provides opti\u00admization metrics for JIT-compiled \nJavascript code. Most im\u00adportantly, it provides information about whether operations are executed in \nJIT code or whether they require calls to the runtime system. In addition, JIT Inspector gives programmers \nsome in\u00adsight into the type inference (Hackett and Guo 2012) process that the SpiderMonkey JIT compiler \nuses to guide optimiza\u00adtion. Like the Soot-Eclipse plug-in, however, JIT Inspector presents the results \nof an analysis not those of the optimizer. 10.3 Compiler Logging Several compilers implement logging \nin their optimizers, e.g., GCC (The Free Software Foundation 2012) supports the -ftree-vectorizer-verbose \n.ag for its vector\u00adizer. Similarly, GHC (The GHC Team 2011) provides the 6 We ignore the fact that, as \nimplemented, the plugin uses its own instance of Soot (Shaw (2005), page 15), that may not re.ect the \nanalyses performed by the compiler. 7 http://addons.mozilla.org/firefox/addon/jit-inspector/  -ddump-rule-firings \n.ag that enumerates the rewrit\u00ading rules that it applies to its input program. SBCL (The SBCL Team 2012) \ngoes one step further and also logs some optimization failures, such as failures to specialize generic \noperations or to allocate some objects on the stack. The logged information of these compilers is similar \nto the result of the logging phase of our optimization coach. This information can be useful to expert \nprogrammers with a solid understanding of the compiler internals, but without the interpretive phases \nof an optimization coach, it is not suitable for non-expert programmers or for casual use. The FeedBack \ncompiler (Binkley et al. 1998), based on lcc (Fraser and Hanson 1995), improves on compiler log\u00adging \nby visualizing the optimizations it applies to programs. It was primarily designed to help students and \ncompiler writers understand how speci.c optimizations work. Implic\u00aditly, it also informs programmers \nof the optimizations ap\u00adplied to their programs. The Feedback Compiler s visual\u00adizations illustrate two \noptimizations: a stepper-like interface that replays common subexpression elimination events and a graphical \nrepresentation of array traversals affected by it\u00aderation space reshaping (Wolfe 1986). While the output \nof the FeedBack compiler is easier to understand than a textual log, the tool suffers from most of the \nsame problems as optimization logging. It directly re\u00adports optimization events, without post-processing \nor .lter\u00ading them, leaving the interpretation step to programmers. It also does not detect or report \nnear misses; optimization fail\u00adures must be inferred by programmers from the absence of optimization \nreports. 10.4 Other Tools Jin et al. (2012) extract source-based rules from known per\u00adformance bugs \nin various applications, then use these rules to detect previously unknown performance bugs in other \nap\u00adplications. When their rules uncover matches, their tools rec\u00adommend the change that .xed the original \nbug as a potential solution for the newly discovered bug. Their corpus of rules successfully discovers \npotential per\u00adformance bugs, which their suggestions can .x. Their work focuses on algorithmic and API \nusage-related performance bugs and is complementary to optimization coaching. Kremlin (Garcia et al. \n2011) is a tool that analyses pro\u00adgram executions and generates recommendations concern\u00ading parallelization \nefforts. Like an optimization coach, the Kremlin tool issues program-speci.c recommendations but, in \ncontrast to an optimization coach, it works dynamically. Like a pro.ler, it requires representative input \nto produce meaningful results, which exposes it to the same limitations and criticisms as pro.lers. Finally, \nKremlin is a special\u00adpurpose tool; it is unclear whether its techniques would ap\u00adply to other optimization \ndomains. 11. Future Work Optimization coaching should apply beyond the optimiza\u00adtions covered here. In \nthis section, we discuss several stan\u00addard optimizations and describe how optimization coaching may apply. \nFor each optimization, we explain its purpose and prerequisites. We then propose a way of detecting near \nmisses of this kind of optimization, which is usually but not always based on optimization proximity. \nFinally, we sketch ideas for how a programmer could react to a near miss. Common Subexpression Elimination \n(CSE) CSE (Much\u00adnick (1997), \u00a713.1) is only valid if the candidate expressions do not depend on variables \nthat may be mutated. An optimization coach can detect cases where an ex\u00adpression is computed multiple \ntimes and is a candidate for CSE but a reference to a mutated variable prevents the op\u00adtimization from \nhappening. The optimization coach could recommend that the programmer reconsider mutating the relevant \nvariable. Test Reordering Conditionals with multiple conjuncts can be optimized by performing the cheaper \ntests (e.g. integer comparisons) before the more expensive ones (e.g. unknown function calls). This optimization \n(Muchnick (1997), \u00a712.3) is only valid if the reordered tests are pure. Counting the number of impure \ntests should identify near misses; speci.cally, the lower the ratio of impure tests to the total number \nof tests, the closer the optimizer is to triggering the reordering. To further rank missed optimizations, \nan optimization coach can take into account the size of the body of the conditional because the cost \nof the condition matters more for small conditionals. When reporting near misses, the optimization coach \ncan recommend making the problematic tests pure or reordering them manually. Scalar Replacement A scalar \nreplacement optimization (Muchnick (1997), \u00a720.3) breaks up aggregates, e.g., struc\u00adtures or tuples, \nand stores each component separately. Like inlining, scalar replacement is mostly useful because it opens \nup further optimization possibilities and reduces allocation. Scalar replacement is performed only when \nthe target aggre\u00adgate does not escape. Typed Racket performs scalar replacement on complex numbers. Optimization \nCoach reports when and where the optimization triggers, but it does not currently detect near misses \nor provide recommendations. An optimization coach could use the ratio of escaping use sites to non-escaping \nuse sites of the aggregate as a possible optimization proximity metric. This metric can be re.ned by \nconsidering the size of the aggregate: breaking up larger ag\u00adgregates may enable more optimizations than \nbreaking up smaller ones. When it does discover near misses, the opti\u00admization coach can recommend eliminating \nescaping uses or manually breaking up the aggregate. Loop-Invariant Code Motion (LICM) A piece of code \nis loop-invariant (Muchnick (1997), \u00a713.2) if all reaching def\u00adinitions of variables in the piece of \ncode come from outside the loop. If one or more relevant de.nitions come from in\u00adside the loop, the optimization \nis inapplicable.  A potential proximity metric for LICM would measure how many problematic assignments \nin the body of the loop could potentially be avoided. This can be re.ned by also considering the ratio \nof assignments and references inside the loop for problematic variables. Variables that are used often \nand mutated rarely are more easily made loop-invariant than those that are mutated often. When presenting \nsuch near misses, the optimization coach could recommend avoiding the problematic assignments or performing \nthe code motion manually. Assignments to loop index variables cannot be avoided, however; attempting \nto make them loop-invariant would defeat the purpose of using a loop. Such necessary assignments can \nbe recognized and should not count towards the optimization proximity metric. Devirtualization In object-oriented \nlanguages, a method call can be turned into a direct procedure call if its receivers belong to one single \nclass. The transformation avoids the cost of runtime dispatch (Dean et al. 1995). If the receivers belong \nto a small set of classes, the compiler can inline a small type-case at the call site, which is still \ncheaper than full-.edged method dispatch. If the set of classes is too large, the compiler uses general \ndispatch. An obvious proximity metric for devirtualization would be the number of receiver classes minus \nthe maximum for which the optimizer generates a type-case. In cases where this number is small, the optimization \ncoach can recommend using more precise type annotations to narrow down the type of the receiver or writing \nthe type-case manually. Reducing Closure Allocation When a compiler decides whether and where to allocate \nclosures, they take into account a number of factors (Adams et al. 1986). A closed function corresponds \nto just a code pointer. A non-escaping function can have its environment allocated on the stack, avoiding \nheap allocation. Ef.cient treatment of closures is a key optimization for functional languages. An optimization \ncoach could warn programmers when the compiler needs to allocate closures on the heap. For instance, \nstoring a function in a data structure almost always forces it to be allocated on the heap. An optimization \ncoach can remind programmers of that fact, and encourage them to avoid storing functions in performance-sensitive \ncode. To reduce the number of false positives, the optimization coach can discard reports that correspond \nto common pat\u00adterns where heap allocation is desired, such as functions that mutate variables from their \nlexical context. Specialization of Polymorphic Containers Polymorphic containers, such as lists, usually \nrequire a uniform repre\u00adsentation for their contents, in order for operations such as map to operate \ngenerically. In some cases, however, a spe\u00adcialized representation e.g., a list of unboxed .oating-point \nnumbers, can be more ef.cient. Leroy (1992) and Shao and Appel (1995) propose optimizations that allow \nspecialized representations to be used where possible and to fall back on uniform representations when \nnot. To avoid excessive copy\u00ading when converting between representations, these opti\u00admizations are typically \napplied only when the element types are suf.ciently small. For example, a list of 4-tuples may be specialized, \nbut not a list of 5-tuples. An optimization coach could communicate to the pro\u00adgrammer which datatypes \nare not specialized and report how much they need to be shrunk to enable optimization. Case-of-Case Transformation \nThe case-of-case transfor\u00admation (Peyton Jones 1996) rewrites nested pattern match\u00ading of the form (in \nHaskell syntax) case (case E of {P1 -> R1; P2 -> R2}) of {Q1 -> S1; Q2 -> S2} to this form case E of \n{P1 -> case R1 of {Q1 -> S1; Q2 -> S2}; P2 -> case R2 of {Q1 -> S1; Q2 -> S2}} which potentially exposes \nmore optimization opportunities. This transformation may lead to code duplication just as with inlining \nand may generate extra closures for join points. Whether it is worth performing depends on the op\u00adtimizations \nit enables, and compilers must resort to heuris\u00adtics. As with inlining, an optimization coach can report \ncases where the case-of-case transformation is possible but ulti\u00admately not performed, along with the \ncause of the failure. 12. Conclusion In this paper, we identify an un.lled niche in the program\u00adming \necosystem feedback from the compiler s optimizer concerning successes and failures of speci.c optimizing \ntransformations. Currently, if a programmer wishes to un\u00adderstand how the compiler views a program, the \nbest option is to study the compiler s generated code. We propose an alternative technique: optimization \ncoaching. Our .rst opti\u00admization coach operates at compile time and provides de\u00adtailed, program-speci.c \ninformation about the optimizer s actions and omissions. In addition to reporting optimization successes \nand fail\u00adures, our tool generates recommendations as to what changes would allow optimizations. By applying \nthese suggestions to a variety of programs, we show that optimization coach\u00ading can automatically identify \ncrucial optimization opportu\u00adnities. Reacting to its recommendations can often recover many of the performance \ngains available to manual opti\u00admization by an expert. Acknowledgments We gratefully acknowledge Matthew \nFlatt s help with Racket s optimizer. Robby Findler sug\u00adgested working with the Racket inliner. The OOPSLA \nre\u00adviewers, including James Noble, helped us hone our ideas.  Software Optimization Coach is available \nin version 5.3 of DrRacket (August 2012) at http://racket-lang.org. Bibliography Norman Adams, David \nKranz, Richard Kelsey, Jonathan Rees, Paul Hudak, and James Philbin. ORBIT: an optimizing compiler for \nScheme. In Proc. Symp. on Compiler Construction, 1986. Erik Altman, Matthew Arnold, Stephen Fink, and \nNick Mitchell. Performance analysis of idle programs. In Proc. Conf. Object-Oriented Programming Systems, \nLanguages, and Applications, pp. 739 753, 2010. Glenn Ammons, Jong-Deok Choi, Manish Gupta, and Nikhil \nSwamy. Finding and removing performance bottlenecks in large systems. In Proc. European Conf. on Object-Oriented \nProgram\u00adming, pp. 172 196, 2004. Andrew Appel and Trevor Jim. Continuation-passing, closure\u00adpassing style. \nIn Proc. Symp. on Principles of Programming Languages, pp. 293 302, 1989. David Binkley, Bruce Duncan, \nBrennan Jubb, and April Wielgosz. The FeedBack compiler. In Proc. International Works. on Pro\u00adgram Comprehension, \npp. 198 206, 1998. John Clements, Matthew Flatt, and Matthias Felleisen. Modeling an algebraic stepper. \nIn Proc. European Symp. on Programming, pp. 320 334, 2001. Ryan Culpepper and Matthias Felleisen. Debugging \nhygienic macros. Science of Computer Programming 75(7), pp. 496 515, 2010. Jeffrey Dean, David Grove, \nand Craig Chambers. Optimization of object-oriented programs using static class hierarchy analysis. In \nProc. European Conf. on Object-Oriented Programming, pp. 77 101, 1995. Robert Bruce Findler, John Clements, \nCormac Flanagan, Matthew Flatt, Shriram Krishnamurthi, Paul Steckler, and Matthias Felleisen. DrScheme: \na programming environment for Scheme. J. of Functional Programming 12(2), pp. 159 182, 2002. Cormac Flanagan, \nMatthew Flatt, Shriram Krishnamurthi, Stephanie Weirich, and Matthias Felleisen. Catching bugs in the \nweb of program invariants. In Proc. Conf. on Programming Language Design and Implementation, pp. 23 32, \n1996. Matthew Flatt and PLT. Reference: Racket. PLT Inc., PLT-TR\u00ad2010-1, 2010. http://racket-lang.org/tr1/ \n Agner Fog. Software optimization resources. 2012. http://www. agner.org/optimize/ Christopher W. Fraser \nand David R. Hanson. A Retargetable C Compiler: Design and Implementation. Addison-Wesley, 1995. Saturnino \nGarcia, Donghwan Jeon, Chris Louie, and Michael Bed\u00adford Taylor. Kremlin: rethinking and rebooting gprof \nfor the multicore age. In Proc. Programming Language Design and Im\u00adplementation, pp. 458 469, 2011. Brian \nHackett and Shu-Yu Guo. Fast and precise type inference for JavaScript. In Proc. Conf. on Programming \nLanguage Design and Implementation, pp. 239 250, 2012. William von Hagen. The De.nitive Guide to GCC. \nApress, 2006. Guoliang Jin, Linhai Song, Xiaoming Shi, Joel Scherpelz, and Shan Lu. Understanding and \ndetecting real-world performance bugs. In Proc. Conf. on Programming Language Design and Implementation, \npp. 77 88, 2012. Milan Jovic, Andrea Adamoli, and Matthias Hauswirth. Catch me of you can: performance \nbug detection in the wild. In Proc. Conf. Object-Oriented Programming Systems, Languages, and Applications, \npp. 155 170, 2011. Donald E. Knuth. An empirical study of FORTRAN programs. Software Practice and Experience \n1, pp. 105 133, 1971. Xavier Leroy. Unboxed objects and polymorphic typing. In Proc. Symp. on Principles \nof Programming Languages, pp. 177 188, 1992. Peter A. W. Lewis, A. S. Goodman, and J. M. Miller. A pseudo\u00adrandom \nnumber generator for the System/360. IBM Systems Journal 8(2), pp. 136 146, 1969. Jennifer Lhot\u00e1k, Ond.rej \nLhot\u00e1k, and Laurie J. Hendren. Integrating the Soot compiler infrastructure into an IDE. In Proc. Symp. \non Compiler Construction, pp. 281 297, 2004. Stephen S. Muchnick. Advanced Compiler Design and Implemen\u00adtation. \nMorgan-Kaufmann, 1997. Simon L Peyton Jones. Compiling Haskell by program transfor\u00admation a report from \nthe trenches. In Proc. European Symp. on Programming, pp. 18 44, 1996. Feng Qian, Laurie J. Hendren, \nand Clark Verbrugge. A compre\u00adhensive approach to array bounds check elimination for Java. In Proc. Symp. \non Compiler Construction, pp. 325 342, 2002. Manuel Serrano. Inline expansion: when and how. In Proc. \nInter\u00adnational Symp. on Programming Language Implementation and Logic Programming, pp. 143 147, 1997. \nZhong Shao and Andrew Appel. A type-based compiler for stan\u00addard ML. In Proc. Conf. on Programming Language \nDesign and Implementation, pp. 116 129, 1995. Jennifer Elizabeth Shaw. Visualisation Tools for Optimizing \nCom\u00adpilers. MS dissertation, McGill University, 2005. Vincent St-Amour, Sam Tobin-Hochstadt, Matthew \nFlatt, and Matthias Felleisen. Typing the numeric tower. In Proc. Inter\u00adnational Symp. on Practical Aspects \nof Declarative Languages, pp. 289 303, 2012. The Free Software Foundation. GCC 4.7.0 Manual. 2012. The \nGHC Team. The Glorious Glasgow Haskell Compilation Sys\u00adtem User s Guide, Version 7.4.1. 2011. The SBCL \nTeam. SBCL 1.0.55 User Manual. 2012. Sam Tobin-Hochstadt, Vincent St-Amour, Ryan Culpepper, Matthew Flatt, \nand Matthias Felleisen. Languages as libraries. In Proc. Programming Language Design and Implementation, \npp. 132 141, 2011. Raja Vall\u00e9e-Rai, Etienne Gagnon, Laurie J. Hendren, Patrick Lam, Patrice Pominville, \nand Vijay Sundaresan. Optimizing Java bytecode using the Soot framework: is it feasible? In Proc. Symp. \non Compiler Construction, pp. 18 34, 2000. Michael Wolfe. Loop skewing: the wavefront method revisited. \nJ. of Parallel Programming 15(4), pp. 279 293, 1986. Nicholas C. Zakas. High Performance JavaScript. \nO Reilly, 2010.     \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Optimizing compilers map programs in high-level languages to high-performance target language code. To most programmers, such a compiler constitutes an impenetrable black box whose inner workings are beyond their understanding. Since programmers often must understand the workings of their compilers to achieve their desired performance goals, they typically resort to various forms of reverse engineering, such as examining compiled code or intermediate forms. Instead, optimizing compilers should engage programmers in a dialog. This paper introduces one such possible form of dialog: optimization coaching. An optimization coach watches while a program is compiled, analyzes the results, generates suggestions for enabling further compiler optimization in the source program, and presents a suitable synthesis of its results to the programmer. We present an evaluation based on case studies, which illustrate how an optimization coach can help programmers achieve optimizations resulting in substantial performance improvements.</p>", "authors": [{"name": "Vincent St-Amour", "author_profile_id": "81485646095", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P3856051", "email_address": "stamourv@ccs.neu.edu", "orcid_id": ""}, {"name": "Sam Tobin-Hochstadt", "author_profile_id": "81319502825", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P3856052", "email_address": "samth@ccs.neu.edu", "orcid_id": ""}, {"name": "Matthias Felleisen", "author_profile_id": "81100323458", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P3856053", "email_address": "matthias@ccs.neu.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384629", "year": "2012", "article_id": "2384629", "conference": "OOPSLA", "title": "Optimization coaching: optimizers learn to communicate with programmers", "url": "http://dl.acm.org/citation.cfm?id=2384629"}