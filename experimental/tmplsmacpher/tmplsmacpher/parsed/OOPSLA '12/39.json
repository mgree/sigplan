{"article_publication_date": "10-19-2012", "fulltext": "\n An Empirical Study of the In.uence of Static Type Systems on the Usability of Undocumented Software \n Clemens Mayer, Stefan Hanenberg Romain Robbes * , \u00c9ric Tanter Andreas Ste.k Southern Illinois University \nUniversity of Duisburg-Essen, Institute for Computer Science and Business Information Systems, University \nof Chile, Computer Science Dept (DCC), Santiago de Chile, Chile Edwardsville, Department of Computer \nScience, Edwardsville, IL Essen, Germany rrobbes@dcc.uchile.cl aste.k@siue.edu clemens.mayer@stud.uni-due.de \netanter@dcc.uchile.cl stefan.hanenberg@icb.uni-due.de Abstract Although the study of static and dynamic \ntype systems plays a major role in research, relatively little is known about the impact of type systems \non software development. Perhaps one of the more common arguments for static type sys\u00adtems in languages \nsuch as Java or C++ is that they require developers to annotate their code with type names, which is \nthus claimed to improve the documentation of software. In contrast, one common argument against static \ntype sys\u00adtems is that they decrease .exibility, which may make them harder to use. While these arguments \nare found in the liter\u00adature, rigorous empirical evidence is lacking. We report on a controlled experiment \nwhere 27 subjects performed pro\u00adgramming tasks on an undocumented API with a static type system (requiring \ntype annotations) as well as a dynamic type system (which does not). Our results show that for some tasks, \nprogrammers had faster completion times using a static type system, while for others, the opposite held. \nWe conduct an exploratory study to try and theorize why. Categories and Subject Descriptors D.3.3 [Programming \nLanguages]: Language Constructs and Features General Terms Human Factors, Languages Keywords programming \nlanguages, type systems, empiri\u00adcal research * R. Robbes is partially funded by FONDECYT Project 11110463. \n\u00c9. Tanter is partially funded by FONDECYT Project 1110051. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, \nUSA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 1. Introduction For decades, \ntype systems (cf. [4, 18]) have played an es\u00adsential role in software education, research, and industry. \nWhile a large number of programming languages with indus\u00adtrial relevance include a static type system \n(e.g., Java, C++, Haskell, or Scala), a number of programming languages especially those used in web \ntechnologies include a dy\u00adnamic type system (such as Ruby, PHP, JavaScript, or Smalltalk). Thus, a lingering \nquestion is whether one sys\u00adtem or another, either static or dynamic, has a larger bene.t for the humans \nthat use them. Although there is an ongoing debate about the pros and cons of static type systems where \ndifferent authors strongly argue for or against static type systems, it is rare that such ar\u00adguments \nare backed by empirical observations. In contrast, such claims are often backed by personal experience, \nspecu\u00adlation, or anecdote. To help summarize the debate both for and against static type systems, typical \narguments against include reasoning such as: Type systems are in.exible: not all valid programs can \nbe handled by a given type systems. As such program\u00admers often use workarounds, such as type casts, to \nbypass the limitations of the type system.  For the same reason, type systems impede rapid proto\u00adtyping, \nwhere, for instance, an object passed as an argu\u00adment may not comply with a given type, but will work \nin practice if it implements an acceptable subset of the pro\u00adtocol it should respond to. Inserting type \nannotations and making objects conform to the expected type distracts the programmer from the task at \nhand.  On the other hand, typical arguments in favor of static type systems often include:  Type systems \nimprove the program structure. Bird and Wadler emphasize that type systems help in the de\u00adsign of clear \nand well-structured programs where logical errors can be detected by any compiler that enforces a type \nsystem (see [1, p. 8]).  Type systems act as a form of documentation. Quoting Pierce: The type declarations \nin procedure headers and module interfaces constitute a form of documentation, giving useful hints about \nbehavior [18, p. 5]1. Bruce gives a similar argument by saying that a type annota\u00adtion provides documentation \non constructs that make it easier for programmers to determine how the constructs can or should be used \n[4, p. 7].  Interestingly, more recent programming languages such as Dart2 sidestep this question by \nproviding a gradual type sys\u00adtem (see [26]) which permits developers to use static and dy\u00adnamic typing \nin combination. While this approach appears to be a compromise at .rst glance, such a technique essentially \npushes the decision onto developers themselves. While de\u00advelopers using a gradual type system have the \nfreedom to choose the type system, they (still) suffer from the problem of determining in what situations \neach type system is appro\u00adpriate. In order to address the question of pros and cons of static and dynamic \ntype systems, this paper focusses on the second of the above mentioned arguments the static type system \ns potential bene.t in documentation. We present an empiri\u00adcal study on 27 human subjects that we asked \nto perform .ve different programming tasks on an API which was not initially known to the subjects and \nwhich was documented only via its source code (without any additional documenta\u00adtion such as comments, \nprogramming examples, handbooks, etc.). While the expected outcome of this experiment was that the static \ntype system has, at least, no negative impact on the usability of such APIs, our empirical observations \nre\u00advealed that for three of the .ve tasks, static type systems had a positive impact on development time \nwhile for two programming tasks, we found that dynamic type systems af\u00adforded faster development times. \nWe start this paper with a discussion of related work in section 2. Then, section 3 describes the experimental \ndesign, the programming tasks given to the subjects, the execution of the experiment, and the threats \nto its validity. Section 4 describes the results of the experiment. As the results we observed did not \nconform to our initial expectations, we performed exploratory studies that try to explain the result \nof the experiment using further measurements (section 5). We discuss this work in section 6. Then, we \nconclude the paper in section 7, discuss future work in section 8 and include additional empirical data \nin an appendix. 1 By accident, a previous paper [15] did not correctly cite the text word for word. 2 \nSee http://www.dartlang.org/ 2. Related Work In an experiment in the late 1970s, Gannon was able to \nshow a positive impact of static type systems [8]. Thirty-eight subjects participated in a two-group \nexperiment where each group had to solve a programming task twice, with a stat\u00adically type checked language \nand a language without type checking. The study revealed an increase in programming reliability for subjects \nusing the language with the static type checking. The programming languages used were arti.cially designed \nfor the experiment. By using the programming languages ANSI C and K&#38;R C, Prechelt and Tichy studied \nthe impact of static type check\u00ading on procedure arguments [22]; ANSI C is a language which performs \ntype checking on arguments of procedure calls while K&#38;R C does not. The experiment divided 34 sub\u00adjects \ninto four groups. Each group had to solve two program\u00adming tasks, one using ANSI C and one using K&#38;R \nC. The groups differed with respect to the ordering of the tasks and with respect to what task had to \nbe ful.lled with what tech\u00adnique. While the experiment tested several hypotheses with slightly mixed \nresults, it measured with respect to develop\u00adment time for one of the two programming tasks a signi.\u00adcant \nimpact of the programming language in use for one task subjects using the statically type checked ANSI \nC were faster in solving the programming task. For the other pro\u00adgramming task, no signi.cant difference \nwas measured. In a qualitative pilot-study on static type systems Daly et al. observed programmers who \nused a new type system for an existing language (a statically typed Ruby [6]). The au\u00adthors concluded \nfrom their work that the bene.ts of static typing could not be shown, however, the study did not in\u00adclude \na quantitative analysis, making it dif.cult for other re\u00adsearch teams to replicate their .ndings at other \ninstitutions. Another study, which did not directly focus on static type systems compared seven different \nprogramming languages [20]. Although the focus was not explicitly on static or dy\u00adnamic type systems, \none .nding was that programs writ\u00adten in scripting languages (Perl, Python, Rexx, or Tcl), took half \nor less time to write than equivalent programs writ\u00adten in C, C++, or Java. However, it should be emphasized \nthat this experiment had some potential problems, includ\u00ading: 1) the development times for the scripting \nlanguages as explicitly emphasized by the author were not directly measured on subjects (which was the \ncase for the languages C, C++, and Java), 2) the subjects were permitted to use tools of their choice \n(e.g., IDE, testing tools), and 3) the programs subjects wrote were not of equivalent length. Despite \nthese potential problems, Prechelt concluded that humans writ\u00ading in programming languages with a dynamic \ntype system (the scripting languages) were afforded faster development times. Our studies. The study \npresented here is part of a larger experiment series that analyzes the impact of static type systems \non software development (see [12]). So far, we performed four other experiments ([10, 15, 27, 28]).3 \n The study by Hanenberg studied the impact of statically and dynamically typed programming languages \nto imple\u00adment two programming tasks within approximately 27 hours [10]. Forty-nine students were divided \nin two groups, one solving programming tasks using a statically typed language and the other a dynamically \ntyped language. Subjects us\u00ading the dynamically typed programming language had a sig\u00adni.cant positive \ntime bene.t for a smaller task, while no signi.cant difference could be measured for a larger task. However, \nthe experiment did not reveal a possible explana\u00adtion for the bene.t of the dynamically typed group for \nthe smaller task. The object-oriented programming languages being used were arti.cially designed for \nthe experiment. In a different experiment, Stuchlik and Hanenberg ana\u00adlyzed to what extent type casts, \nwhich are a language fea\u00adture required by languages with a static type system, in.u\u00adenced the development \nof simple programming tasks [28]. Twenty-one subjects divided into two groups, took part in a within-subjects \ndesign participants completed tasks two times, once with static typing and once with dynamic typing. \nIt turned out that type casts do in.uence the development time of rather trivial programming tasks in \na negative way, while code longer than eleven lines of code showed no sig\u00adni.cant difference. The programming \nlanguages being used were Java (for the statically typed language) and Groovy (where Groovy was only \nused as a dynamically typed Java without using Groovy s additional language features). A further experiment \nperformed by Steinberg and Hanen\u00adberg analyzed to what extent static type systems help to iden\u00adtify and \n.x type errors as well as semantic errors in an appli\u00adcation [27]. The study was based on 31 subjects \n(again based on a two-group within-subject design). The result of the ex\u00adperiment was that static type \nsystems have a positive impact on the time required to .x type errors (in comparison to the time for \n.xing equivalent no-such-method exceptions). With respect to the time required to .x semantic errors, \nthe exper\u00adiment did not reveal any signi.cant differences between the statically and dynamically typed \nlanguages. Again, the pro\u00adgramming languages Java and Groovy were used. Finally, Kleinschmager et al. \nperformed an experiment on 33 subjects which combined repetitions of previous exper\u00adiments (with Java \nand Groovy as languages) [15]. Among other tested hypotheses, it was con.rmed that .xing type er\u00adrors \nin a statically typed language is faster than correspond\u00ading .xes in a dynamically typed language, while \nno differ\u00adences for .xing semantic errors was measured. As we see from our literature review, there is \nnot yet an empirically rigorous consensus amongst the few studies we are aware of. As such, further experiments \nare needed. 3 The work by Steinberg and Hanenberg is not yet published [27]. 3. Experiment Description \nWe begin by stating the underlying hypotheses of our exper\u00adiment and then discuss initial considerations \nwhen running such studies. After introducing issues such as which pro\u00adgramming languages, programming \nenvironments, and APIs we used, we describe our programming tasks in detail. Then, we state our expectations \nabout the performance of the de\u00advelopers in the two groups, the experimental design, and why we chose \nthat design. Finally, as all experiments have limi\u00adtations, we discuss the threats to validity. 3.1 \nHypotheses Our experiment tests the belief that a static type system helps developers use an undocumented \nAPI. By helping , we mean that a static type system either requires less effort in comparison to a dynamic \ntype system in order to ful.ll the same programming tasks, or, with the same effort, that more programming \ntasks can be ful.lled with the aid of static type systems. The second perspective differs from the .rst \none in that the effort is .xed, while it is a variable in the .rst one. In our experiment, we decided \nto use development time as a variable, as it is easier to measure the time required to ful.ll a certain \ntask than determining how much work has been accomplished within a given amount of time; Devel\u00adopment \ntime is a common measurement for effort in pure programming experiments (see [8, 13, 20 22, 25, 29]). \nHence, the .rst difference hypothesis to be tested is: Null Hypothesis 1: The development time for complet\u00ading \na programming tasks in an undocumented API is equivalent when using either a static type system or a \ndy\u00adnamic type system. Given that our .rst null hypothesis only takes into account the design of an undocumented \nAPI as a whole, it is desir\u00adable to formulate a second null hypothesis that takes poten\u00adtial confounding \nfactors into account. For example, it seems reasonable that given a larger undocumented API, static type \ninformation may help the user more than a similar task us\u00ading a smaller API, where using the API may \nbe more obvious because there are fewer classes or methods4. Accordingly, we formulate a second null \nhypothesis as follows: Null Hypothesis 2: There is no difference in respect to development time between \nstatic and dynamic type sys\u00adtems, despite the number and complexity of type decla\u00adrations in an undocumented \nAPI. Note that the second hypothesis only takes a different num\u00adber of classes and types into account \nbut does not try to ex\u00adplain the relationship between the number of types and de\u00ad 4 According to programming \nlanguages such as Java or C++ we assume a close connection between class hierarchy and type hierarchy. \nFurthermore, we assume here that the dynamically typed API does not contain any type annotations.  velopment \ntime. Finally, both hypotheses focus on the de\u00advelopment time of programming tasks. Hence, if either \nof these hypotheses can be rejected through standard statistical inference techniques, we may gain insight \ninto the relative bene.ts or consequences of static and dynamic typing.  3.2 Initial considerations \nfor the experiment In order to build an experiment that tests the previously de.ned hypotheses, it was \nnecessary to .nd: Two comparable languages (with a static and a dynamic type system);  An undocumented \npiece of software that should be used within the experiment;  Appropriate programming tasks that should \nbe ful.lled by the subjects; and  A reasonable experimental design.  At the time the experiment was \nplanned, we knew that volun\u00adteers from the University of Duisburg-Essen, Germany, and the University \nof Chile would participate as subjects in the experiment. In order to obtain participation from these \nvol\u00adunteers, we agreed that that the experiment should last for no more than one day. Further, we estimated \nupfront that the number of subjects would be around thirty. Hence, the exper\u00adimental setup and the experimental \ndesign should take into account that the expected effect size should be measurable for a rather small \nsample size. 3.3 Programming languages and environment The broad goal for our experiment was to compare \ntwo dif\u00adferent languages one with a static type system (which re\u00adquires type annotations in the code) \nand one with a dynamic type system (without any type annotations). In order to ad\u00address the problem that \ndifferent languages have different lan\u00adguage features and that consequently differences in an ex\u00adperiment \ncannot be reduced to the factor type system, it is necessary to .nd two very similar languages. According \nto previous experiments (see [15, 27, 28]) we decided to use Java and Groovy [16]. While Groovy has a \nnumber of language features in addition to Java, it can also be used as a dynamically typed Java : all \ntype declarations of variables, .elds and parameters can be annotated with the keyword def (without referring \nto the corresponding nomi\u00adnal type). In this case, Groovy does not perform any static type checking on \nthese elements but dynamically performs the type check at runtime. Consequently, a pure reduction of \nGroovy on the language features of Java permitted us to have two similar languages which only differ \nwith respect to the type system. One further argument for using Java and Groovy was that the subjects \nwe expected to participate already had some programming skills in Java. Consequently, it was not nec\u00adessary \nto perform any exhaustive training for the subjects. This implies that we did not intent to introduce \nGroovy as a new programming language. Instead, we only introduced Groovy as a Java version where the \ndeclarations of vari\u00adables, parameters, etc. only required the keyword def. To maximize the similarity \nof the two experimental set\u00adtings, it was necessary to exclude another language feature: Method overloading. \nWhen overloaded methods have the same number of parameters, parameter types must be ex\u00adplicit in order \nto distinguish between the methods. Hence, if no static types are available, method overloading cannot \nbe used. An alternative would have been to use in the dynami\u00adcally typed methods with different names, \nwhere the caller of the method is responsible for choosing the right one. How\u00adever, it was unclear whether \nthis alternative would introduce other experimental confounds, as the two APIs would be dif\u00adferent. Thus \nwe decided to ignore method overloading in the experiment. For the programming environment used by the \nsubjects, our intention was (again) to provide a comparable situation for both Java and Groovy. While \nat .rst glance IDEs such as Eclipse provide tool support for Java as well as Groovy, Java tool support \nin Eclipse is much more mature than that of Groovy. Consequently, using Eclipse would probably have been \nan advantage for Java developers and would have con\u00adfounded the measurements: the intention of the experiment \nwas to measure the impact of the language feature type sys\u00adtem, not the maturity level of tool support. \nHence, we de\u00adcided to use a plain text editor with syntax highlighting, as has been done in previous \nexperiments [15, 27]. This custom editor permitted the user to compile programs, run applica\u00adtions, and \nexecute test cases. This simple IDE also logged data for the experiment such as when subjects were about \nto run their code, or when they ful.lled a given programming task.  3.4 Experimental design The experiment \nwas designed as a 2-group within-subject design (see [13, 21, 29] for a more detailed introduction into \nsuch kinds of experiments) where the subjects were randomly assigned to one of both groups. The groups \nshould be balanced with respect to the number of subjects, i.e. both groups should have a comparable \nnumber of subjects. Each group solved the programming tasks in both languages, i.e. in Java as well as \nin Groovy. Both groups differ with respect to the ordering of both languages: While the .rst group (group \nGROOVYFIRST) solved all programming tasks in Groovy and then solved all programming tasks in Java, the \nother group (group JAVAFIRST) did it the other way around. The main motivation for this kind of design \nis, that the problem of deviation among different subjects is reduced (see section A.1 for a more detailed \ndiscussion). Obviously, learning effects are a potential problem in ex\u00adperimental designs that have subjects \ncomplete more than one task. Once a programming task is solved using lan\u00adguage A, it becomes easier to \nsolve the same program\u00adming task with language B (if both languages are simi\u00ad  lar). Consequently, it \nseems obvious at .rst that a within-3.5 Description of the APIs subject measurement, where each subject \nsolves a program\u00adming task twice, cannot lead to valid results. However, apart from the fact that this \nexperimental design has been already successfully applied (see for example [28]), there are sev\u00aderal \nvalid statistical approaches for fairly analyzing data in such studies (e.g., a repeated measures ANOVA). \nHere we roughly discuss the design a more detailed description and discussion can be found in the appendix \n(section A.1). Figure 1. Impact of within-subject design for one single task on experiment results (taken \nwith adaptations from [28]) Figure 1 illustrates the assumed effect on two very similar subjects in both \ngroups the .rst measurement and then the second measurement which includes the learning effect as well \nas the language effect. It should be emphasized that only the .rst and the second measurement is visible \nto the experimenter. To what extent the factors learning effect and language effect individually in.uenced \nthe difference cannot be determined by the two measurements. As the .gure shows, the second measurement \nis still able to reveal the effect of the language, if the learning effect is smaller than the language \neffect (in the lower part of Figure 1 the second measurement is still larger then the .rst mea\u00adsurement). \nIt is even acceptable if the learning effect is sim\u00adilar to the language effect (which means that no \ndifference between the .rst and the second measurement is found for group B) but the design fails, if \nthe learning effect is larger than the language effect (see section A.1 for a more detailed discussion). \nAPI design considerations. Our aim was to provide two versions of an undocumented API one Java version \nand one Groovy version as similar as possible. The only dif\u00adference should be that the pure Java version \nmakes use of static types while the Groovy version does not. Similarly, the API should be simple enough \nto understand how to use it in a relatively small amount of time. However, the API must not be so trivial \nthat all subjects immediately .nish the tasks (a ceiling effect). Since the time difference between static \nand dynamic type systems potentially depends on the searching for the right types or reading source code \nin dif\u00adferent .les, the source code should be large enough that a difference could be measured. Finally, \nwe needed to take the previously described within-subject design into account. Since this design poten\u00adtially \nsuffers from learning effects, it is desirable to consider some precautions against it. For example, \nwe would gener\u00adally expect that subjects using the API a second time would be better at doing so. As \nsuch, our statistical procedures must take this into account by conducting comparisons across .rst time \nusers of both static and dynamic typing, in addition to secondary use. Structurally identical APIs. With \nthese factors in mind, we provided two structurally identical APIs, where the sec\u00adond one derived from \nthe .rst by renaming classes, meth\u00adods, variables, etc. This was done in a manner that ensured the two \nAPIs, despite having the same complexity, seemed to address two entirely different problems. We designed \nan API consisting of 35 classes, in four packages, having 1279 lines of code. One version of the API \nhandled the implementation of UI tree views, the other version handled SQL queries (as trees of SQL elements) \nand results. The tree view API was built .rst and from it we derived the SQL API. The chosen names for \nparameters and variables were still meaningful, although they did not directly refer to a type name. \nFinally, we replaced the type names in the UI tree view implemen\u00adtation by the keyword def (in order \nto gain a dynamically typed version of this API).  3.6 Description of programming tasks The programming \ntasks should directly re.ect on the hy\u00adpotheses as introduced in section 3.1. In order to do so, we decided \nto design programming tasks that differ with respect to whether the API is statically or dynamically \ntyped (hy\u00adpothesis 1). They also should differ with respect to the com\u00adplexity of type declarations needed \nin order to ful.ll the pro\u00adgramming tasks (hypothesis 2). Task complexity. Since we are interested in \nthe effect of typing on API usage, we wish to measure the difference with respect to typing and not the \npossible in.uence of type sys\u00adtems on different language constructs, or on the complexity of the code \nto write. While complexity measurements such as lines of code, Halstead, or cyclomatic complexity are \nof\u00adten used [7], we decided to use a different approach. In order to maximize the effect of using the \nAPI, and reduce the ef\u00adfect of the complexity of the programming task itself, we kept the complexity \nof the code snippets subjects have to write similar. The programs that use the APIs should only consist \nof single methods which do not make use of non\u00adtrivial constructs such as loops or conditions. Instead, \nthe in\u00adtention was to use only assignments, variable reads, method calls and object creation operators. \nThe complexity in the tasks only stems from the number of types to identify and the relationships between \nthe types.  Task completion. We instructed the subjects to move to the next task only when they successfully \ncompleted the cur\u00adrent task. For each task, we de.ned several tests that verify that the task has been \nindeed completed. These were Unit Tests, run each time the code snippet is compiled success\u00adfully. Only \nwhen all the tests passed were the subjects al\u00adlowed to go to the next task. The subjects did not have \naccess to the tests. Warm-up task. We included a simple warm-up task so that the subjects would have \ntime to familiarize themselves with the development environment, the testing process to de\u00adtermine correctness, \nand the Groovy programming language. We asked that subjects solve three vary basic programming tasks \n(e.g., summing a list of numbers). This task was not included in the analysis and is not described below. \nTask description. In the following section, we describe the programming tasks from the following perspectives: \n(1) what the expected solution of the task is; (2) what the char\u00adacteristics of the programming tasks \nare; and (3) how many unique types are required in order to ful.ll the program\u00adming tasks. We show the \nsolution from the .rst API UI tree views the subsequent ones have similar solutions. 3.6.1 Task 1 (easy, \none class to identify) In the .rst programming task, developers were asked to return an instance of a \ncertain class (class AbstractTree-Factory). In order to do so, they were told the name of the abstract \nsuperclass, but not the name of the concrete subclass (where only one such concrete subclass exists in \nthe project). Hence, in both cases (dynamically and statically typed) it was necessary to identify one \ntype (which corresponds to only one class AbstractTreeViewFactory) in the project. The code consists \nof a single line, which returns the object being created (see Figure 2). 3.6.2 Task 2 (easy, three classes \nto identify) In the second programming task, developers were required to create an initialized TreeView \nobject with the name sampletree. This object is created by invoking a method in the previously described \nfactory. Additionally, an initial\u00adizer needed to be passed to the factory. This initializer itself required \na Configuration object which contains the ti\u00adtle of the tree (an instance of class Title); altogether, \nsub\u00ad jects had to identify Initializer, Configuration and Title. public static AbstractTreeViewFactory \ntask1 () { return new TreeViewFactory ( ) ; } public static TreeView task2 () { AbstractTreeViewFactory \nstv ; Initializer init = new Initializer (); Configuration conf = new Configuration (); Title t = new \nTitle (); t .setName(\"sampletree\"); conf. setTitle (t ); init .setConfig(conf); stv = new TreeViewFactory(init \n); return stv . createTree (); } Figure 2. Example solutions for programming tasks 1 &#38; 2 3.6.3 \nTask 3 (medium, three classes to identify) The third task required developers to create a transformer \nof the tree view. A corresponding class Transformer was given within the API which performs such a transformation. \nHowever, it was necessary to create .rst a graph from the tree view and pass it to the transformer. Then, \na walker must be created and passed to the transformer. Finally, a start node for the transformation \n(the tree s root node) is provided; it needs to be extracted by converting it into a TreeViewNode (by \nsending a corresponding message to the graph object). Figure 3 illustrates a possible solution for the \ntask. Altogether, the types Graph, Walker and TreeViewNode needed to be identi.ed. 3.6.4 Task 4 (dif.cult, \nthree classes to identify) The fourth task required developers to add a new node (with name sampletreenode) \nto a graph which can be ac\u00adcomplished by parameterizing an initializer correctly. The problem with this \ntask is that the design of the initializer object is not trivial. The initializer must be parameterized \nwith instances of an IdentifiedSequence. This se\u00adquence contains a tree node identi.er (the node s name) \nand a sequence of pairs consisting of an identi.er and an IdentifiedSequence each child of a tree node \nitself is identi.ed by a corresponding identi.er. In that way, it is possible to parameterize the initializer \nalready with a tree of identi.ers. Again, three classes need to be identi.ed: Identified-Sequence, TreeNodeIdentifier, \nand Pair. Due to the underlying recursive de.nition of the items, we sus\u00adpected this code to be rather \ndif.cult to understand (if no static type system directly reveals the underlying design of the API). \n 3.6.5 Task 5 (easy, six classes to identify) For the .nal task, a menu with a corresponding command \nshould be created for the tree view. A command, represented  public static Transformer <TreeViewNode> \ntask3(TreeView tv) { GraphFactory gf = new GraphFactory (); Graph <TreeViewNode ,Edge<TreeViewNode>> \ng = gf.createGraphFromTreeView(tv ); Transformer <TreeViewNode> t = new Transformer <TreeViewNode >(); \nt . setTransformableStructure(g); Walker w = new Walker (); t .setWalker(w); TreeViewNode s =  g.getNodeFromContent(tv.getRootNode \n()); t . setStartNode(s); t .doTransformation ();  return t; } public static void task4(Initializer \ninit) { IdentifiedSequence <TreeNodeIdentifier > z = new IdentifiedSequence <TreeNodeIdentifier >(); \nTreeNodeIdentifier t = new TreeNodeIdentifier (); t .setName(\"samplerootnode\"); Pair <TreeNodeIdentifier \n, IdentifiedSequence < TreeNodeIdentifier >> p = new Pair <TreeNodeIdentifier , IdentifiedSequence <TreeNodeIdentifier \n> >(); p. setFirst (t ); p. setSecond ( new IdentifiedSequence < TreeNodeIdentifier >());  z . add(p \n); init .setItems(z); } Figure 3. Example solutions for programming tasks 3 &#38; 4 by a String, is passed \nto the method. It should be added to a corresponding Command object (which needs to be created). The \ncommand object needs to be passed to the menu. Further objects (LayoutPolicy, BackgroundImage, Font) \nshould also be passed to the menu object in order to fully specify the command. The task required six \nclasses to be identi.ed: Menu, MenuItem, SingleCommand, LayoutPolicy, Back\u00adgroundImage, Font. We suspected \nthat this task might be simpler than others, since the relations between the types is much more straightforward \nthan the preceding task.  3.7 Assumed Disadvantage of Dynamic Type Systems The programming tasks were \ndesigned in a way that we as\u00adsumed that for all tasks (except task 1) the static type system would show \na measurable positive impact; we explain the assumed reasons for this and the assumed behavior of devel\u00adopers \nin the following. We use DT developers to describe those developers that use the dynamically typed API, \nand ST developers for those using the statically typed API. public static Menu task5 ( String cmd) { \nMenu m = new Menu ( ) ; MenuItem mi = new MenuItem ( ) ; mi. setTitle (\"samplecommand\"); m. add(mi); \nSingleCommand cc = new SingleCommand ( ) ; mi. setCmd(cc ); cc .setCommand(cmd); LayoutPolicy l = new \nLayoutPolicy (); BackgroundImage bg = new BackgroundImage ( ) ; Font f= new Font(\"Arial\" , 12, Font.Style.DEFAULT); \n m.setLayout(l ); l . setBgImage ( bg ) ; l . setFont(f );  return m; } Figure 4. Example solution \nfor programming tasks 5 3.7.1 Task 1 For the .rst task, DT developers as well as ST developers have the \nsame problem: they need to become aware that the class mentioned in the task description is abstract. \nIn both cases, developers need to scan the code in order to .nd a concrete class that implements the \nmentioned one. As such, we expected no signi.cant difference between the groups. 3.7.2 Task 2 For programming \ntask 2 we suppose that DT developers have a disadvantage in comparison to ST developers. We assumed that \ndevelopers .rst will have to investigate the class TreeViewFactory in order to determine how a new tree \nview can be created. DT developers then .nd a parameter named init in the constructor (which is then \npassed to an instance variable named init). DT devel\u00adopers should need additional time in order to determine \nthat an Initializer instance is required here. Then, the type Initializer needs to be understood. While \nthe ST developers directly can see in the code that a type Configuration is required, DT developers only \nsee that there is a .eld named config. Again, DT developers might assume that the tree name should be \nassigned to config. Finally, while ST developers directly get the information that a Configuration object \nrequires a Title object (which contains the String object representing the tree s name), DT developers \nneed to .nd it out on their own perhaps as\u00adsuming that they can directly pass the String to the con.g \nobject.  3.7.3 Task 3 Task 3 is slightly harder to solve than task 2. The main dif\u00adference lies in the \nway the API can be read. In task 2, it was possible to study a class in order to determine what needs \nto be done next. In task 3, DT developers only know that they need a Transformer object, but no TreeView \nis per\u00admitted as parameter. Here, developers need to detect that a different class GraphFactory converts \nthe tree view into a graph which then can be passed to the transformer. The same is true for the start \nnode, which needs to be returned from the graph in order to be passed to the transformer. We assume that \nthe relationships between these different tasks are harder to detect for DT developers, advantaging ST \nde\u00advelopers.  3.7.4 Task 4 Task four is the hardest task for the DT developers. We think that developers \neasily become aware that the items need to be set in the Initializer object. For the ST developers, the \nmethod directly reveals the required (generic) type which guides developers to type IdentifiedSequence. \nFrom the types, ST developers see that a TreeNodeIdentifier is required and that an object of type Pair \nneeds to be added to the sequence. Hence, the static type system directly docu\u00adments the recursive de.nition \nof IdentifiedSequence. In contrast, DT developers have to discover this either by reading the API s source \ncode or by interpreting the error messages when they used the wrong classes in their code. We think that \nthis is a very time consuming task and expect that DT developers will require more time than ST develop\u00aders. \n 3.7.5 Task 5 We suspected that Task 5 would be less dif.cult to under\u00adstand (no recursive type de.nition, \netc.), even though it had the largest number of classes that have to be identi.ed. We assume that ST \ndevelopers have an advantage over DT de\u00advelopers due to the number of identi.ed classes (and not, as \nin task 4, the complexity of the type de.nitions).  3.8 Summary of the programming tasks The programming \ntasks are trivial code, algorithmically speaking: no loops, conditions, or advanced language fea\u00adtures \nare used. This is intentional; the tasks construct data structures. Each task requires the developer \nto identify the classes to instantiate and to pass them to other classes. However, the tasks have different \nlevels of dif.culty. While tasks 1, 2, and 5, are relatively trivial programming tasks, tasks 3 and 4 \nare slightly more complex. For task 3, the conversion of some objects need to be understood and task \n4 requires the understanding of a recursive data struc\u00adture. Both tasks make use of generic types in \nJava, but from our point of view, task 4 is more complex, as the recursive data de.nition is directly \ndocumented by the generic types for ST developers. In sum, we designed our tasks such that there was \na variety of different complexity levels. In doing so, we attempted to provide insight into how this \ncharacter\u00adistic might in.uence our goal of learning the pros and cons of static and dynamic type systems. \n 3.9 Experiment Execution / Subjects We recruited 33 subjects for our experiment. All subjects were (undergraduate \nas well as graduate) students from the the University of Duisburg Essen, Germany or the University of \nChile all of the students were trained as part of their studies in the programming language Java. Three \nsubjects did not complete the programming tasks and abandoned the experiment (two starting with Groovy \n.rst, one starting with Java .rst); their results were excluded from the analysis. Another subject was \nremoved from the experiment, be\u00adcause after watching the measurements, it was revealed that the student \nspent a very large amount of time in reading the complete source code while working on task 2 and then \nsolved the tasks 3 and 4 quickly the subject con.rmed in a following interview that he worked like that. \nWe removed the subject because we considered our measurement method (time until a programming task was \ncompleted) to be insuf\u00ad.cient in this situation. This is because it was not possible to determine how \ntime should be now considered for the differ\u00adent tasks. For similar reasons, we removed two further sub\u00adjects \nfrom the experiment, because they abandoned one task, switched then to another one and then came back \nto the orig\u00adinal one. We .nally considered 27 subject for the analysis .fteen from Duisburg-Essen and \ntwelve from Chile.  3.10 Threats to validity As in any experiment, there is a number of threats to va\u00adlidity. \nMost of the threats are in common with previous ex\u00adperiments [10, 28] (such as chosen subjects, background \nof subjects, etc.) and will not be discussed here. Internal Threat: Experimental Design. As already mentioned \nbefore, a general threat is that the underlying design assumes that the (possible) main effect (the effect \nof the type system) is larger than the possible learning effect in the within-subject measurement. If, \nhowever, the learn\u00ading effect is very large in comparison to the main effect, the chosen design will \nnot be able to detect the existence of the main effect. Internal Threat: Measurement. The chosen program\u00adming \ntasks largely depend on the underlying system. It might be the case that the underlying system is so \nsimple that the effect undocumented API does not play any role. We tried to avoid this problem by using \nan implementation which is, from our subjective point of view, complex enough although we cannot argue \nexactly what complex enough means. For example, we may have chosen tasks that are ei\u00adther too simple \nor too hard. While this is true, we have care\u00adfully documented the tasks we used so that other research \ngroups can reproduce them, modify them, or at least com\u00adpare them in future work. External Threat: Dynamically \nTyped API. The under\u00adlying system is created by creating a statically typed system .rst and then by removing \nthe type annotations (i.e. replac\u00ading the type annotations with the keyword def and by re\u00admoving interfaces \nand implements declarations). Conse\u00adquently, the system does not make use of any possible fea\u00adtures of \ndynamically typed languages. It could be possible that the existence of the dynamic type system would \nhave a large impact on the overall system s architecture. From our point of view, the dynamically typed \nsystem still represents a reasonable design while we believe in general that API design is in.uenced \nby the underlying type system, we do not think that in the special situation of this experiment the dynamic \ntype system would have had any impact of the re\u00adsulting structure.  External Threat: Arti.ciality of \nthe tasks. At .rst glance, some tasks may seem to be intentionally complex, to the point of being arti.cial. \nHowever, we have found similar examples in the .eld. For instance, using the XML DOM API to write a document \nto a .le, one must instantiate a DocumentBuilderFactory catch various exceptions in the process , then \ncreate a Transformer (through a second factory), and .nally get a NodeList of elements of interest wrapped \ninside a NodeSource that is passed to the Transformer. Similar tasks to ours may happen in the .eld; \nwe do not regard the tasks we designed as arti.cial.5 External Threat: Tool support. The subjects only \nused a very simple IDE without features such as code completion. Especially the absence of code completion \nprobably leads to high development times. Consequently, the measured devel\u00adopment times cannot be considered \nas representative devel\u00adopment times for the same tasks in the .eld. There are au\u00adthors who state that \nsuch tools based on a static type system are better than corresponding implementations based on a dynamic \ntype system in such a case the type system would indirectly in.uence development time (reduced development \ntime caused by better tool support caused by the type sys\u00adtem). The experiment does not permit to (and \ndoes not try to) make any statements about the possible impact of tool\u00ading. External Threat: Programming \nLanguages. While the experiment has a focus on the language feature type system, it is possible (and \nlikely) that a type system plays a different role for different kinds of language features. Hence, it \nmight be the case that for special language constructs the in.uence of the type system is larger or less. \nConsequently, if different programming languages would have been chosen (instead of Java and Groovy) \nit is possible that the results would have been different. Conclusion Threat: Type System. The experiment \nuses a special kind of type system a (nominal) type system where types are annotated in the code. It \nmight be possi\u00adble that different type systems, such as those ones based on type inference (see for example \n[19]) have a different impact on the development time. Consequently, it might be possible 5 The full \nexample is available at: http://docs.oracle.com/javaee/1.4/tutorial/doc/JAXPXSLT4.html that this experiment \nrejects a hypothesis although the hy\u00adpothesis holds for a different kind of type system. 4. Experiment \nResults This section illustrates the analysis of the experiment data. In order to ease the reading of \nthe paper, we decided to shift parts of the analysis to the appendix. This choice was made to focus on \nthe most important parts of the analysis which is otherwise quite systematic and long and on the results \nthemselves. In this section, we focus only on the time taken to solve the tasks. As the subjects did \nswitch tasks only when they successfully passed a task, we do not need to analyze the correctness of \nthe tasks; the solutions are by de.nition correct. Subjects unable to complete some tasks were discarded \nfrom the analysis (see section 3.9). Structure of the results. We .rst give an overview of the results \nwith descriptive statistics of the data (Section 4.1). We then conduct a between-subject analysis, which \nconcludes that the results are signi.cantly in.uenced by the tasks (Section 4.2). Given the variability \nof performance be\u00adtween subjects and tasks, the between-subject analysis does not .nd a main effect of \nthe type systems. This is however the goal of the following within-subject analysis, which as\u00adcertains \nwhich language (and by extension type system) is better performing on a task-by-task basis (Section 4.3). \nWe continue the analysis to conclude whether the number and complexity of the types to identify has a \nrelationship with the difference we observe between the tasks (Section 4.4). We .nally wrap up the results \nwith a discussion motivating the need for a further exploratory analysis (Section 4.5). 4.1 Measurements \nand descriptive statistics We start with a high-level analysis of the measurements of task completion \ntime. A .rst view on the boxplot (see Figure 5) representing the measured data (see appendix A.2) seems \nto support the following statements: while for task one, four and .ve there is a tendency that the Groovy \nsolutions re\u00adquired more time, task three (and to a certain extent, task two) required more time in Java. \nHowever, the raw data, the descriptive statistics and the visual representation only give a rough overview \nof the mea\u00adsurements. For instance, the boxplot does not consider that each subject is measured twice. \nTo understand whether there is a signi.cant difference, it is necessary to apply appropriate statistical \ntests (see [9, 23] for an introduction).  4.2 Repeated measures ANOVA We start our analysis by running \ntwo repeated measures ANOVAs. The .rst was run on tasks 1-5 for the groups that used static or dynamic \ntyping for the .rst time. The sec\u00adond ANOVA compared the groups that had switched from static to dynamic, \nor vice versa, for tasks 1-5 again. This analysis does not bene.t from the within-subject measure\u00adment \nof each individual task; i.e., it cannot detect the effect Figure 5. Boxplot for all measured data (combined \nround 1 Figure 7. Boxplot for second round (no repeated measure\u00adand round 2) grouped by tasks ment of \nsame tasks)  of the static or dynamic type system on each subject for a .rst round (p<0.0016 , . 2 \np =.354) as well as for the second given task. As explained above, this analysis is also sensi\u00adround \n(p<0.001, . 2 p =.396). An interesting observation here tive to the variability of the performance between \nsubjects; is, that the . 2 p values are very similar (which indicates that it can, however, detect differences \nin performance due to the tasks themselves. Figures 6 and 7 show the boxplots for the .rst respectively \nsecond round where we group the re\u00ad sults by the programming tasks. We perform a repeated measures ANOVA \non program\u00ad ming tasks and programming language. This analysis con\u00adthe impact of the task on the variance \nof the corresponding development time is similar). As a .rst conclusion, we can say that the individual \ntask has a strong in.uence on the development time; this .nding is in accordance with the existing literature \n[10, 22, 28]. A second observation is that there is a signi.cant inter\u00adsiders programming task and programming \nlanguage as in\u00adaction between the factor task and programming language dependent variables while we have \nthe development times (in the .rst round p<0.025, . 2 p =.167; in the second round as dependent variable. \nThe different programming tasks are p<0.001, . 2 p =.195). This corresponds to our expectations: the \nconsidered within-subject, i.e. each individual s difference in development times for different tasks \nis considered. Figure 6. Boxplot for .rst round (no repeated measurement of same tasks) We .rst observe \nthat, in both the .rst and the second round, the dependent variable programming time is different for \nthe different programming tasks the ANOVA reveals a signi.cant impact of the factor programming task \nfor the experimental design assumes that the impact of the type sys\u00ad tem is different for the different \ntasks, since their complexity varies. With regard to the factor programming language, the .rst round \ndid not reveal a signi.cant impact (not even any tendency, since p>0.90!) while the second round reveals \na close to signi.cant difference (p<0.06). Consider what this means in plain English. At .rst glance, \nit appears that we cannot reject our Null Hypothesis 1. Since no signi.cant effect has been demonstrated, \nit seems that the impact of the programming language if any is so small that it is hidden by the de\u00adviation \namongst tasks, developers, or any other confounds we have not considered here. This is however a weakness \nof this particular analysis, in cases such as programming experiments where the between-subject variability \nis high. This is the case here: as shown in the appendix, the stan\u00addard deviation of the task completion \ntime is comparable to the median a common occurrence in programming exper\u00adiments. Hence, the between-subject \nanalysis is not able to take into account the (possible) effect of the programming language on each individual; \nas argued in our experimen\u00ad 6 The Greenhouse-Geisser was applied since the sphericity test turned out \nto be signi.cant with p<0.02.  tal design, we go on in the analysis with a within-subject analysis. \n 4.3 Analysis of groups and tasks: Null hypothesis 1 We now study the tasks and the groups in isolation, \nand combine the results of the analyses afterwards. Such a sepa\u00adrated analysis means that we study, for \neach task, the differ\u00adences between the Java and the Groovy development times for group GROOVYFIRST, \nand group JAVAFIRST. Accord\u00ading to section 3.4 we then combine the results of the tasks where one group \nreveals a signi.cant difference while the other one either shows the same or no signi.cant difference. \nIn case both groups show contradicting differences for one task, it is not possible to interpret the \nresult for this task. The complete analysis of the separate groups is in ap\u00adpendix A.3. The combination \nof the analyses shows a signif\u00adicant difference for each task. For no task, the signi.cance test revealed \nthe same results, i.e., for no task did group GROOVYFIRST have the same positive impact of one lan\u00adguage \nas group JAVAFIRST. Additionally, no contradiction between the results appeared, i.e., for no task did \none group have a different signi.cant result than the other group. Table 1 shows the results of the analysis \nfor null hypothe\u00adsis 1 that development time for completing a programming tasks in an undocumented API \nis equivalent when using ei\u00adther a static or a dynamic type system and the correspond\u00ading p-values (see \nsection A.3). For tasks 1, 4, and 5 we .nd a positive impact of Java; however, we .nd a positive impact \nof Groovy for tasks 2 and 3. Consequently, we reject the null hypothesis in all cases, but for tasks \n2 and 3, the dominating language is contrary to our expectations. Task Task 1 Task 2 Task 3 Task 4 Task \n5 P-values GROOVYFIRST <.001 .93 .45 .03 <.001 JAVAFIRST .35 .01 .01 .34 .86 Dominating language Java \nGroovy Groovy Java Java Table 1. Experiment results for hypothesis 1 Combination of both analysis for \nboth groups 4.4 Analysis of complexity: Null hypothesis 2 Given the results from our task by task analysis, \nwe cannot reject our second Null hypothesis: For the easy tasks (task one, two, and .ve) we .nd that \nthe dynamic type system had a positive impact for task two, while it had a negative impact for tasks \none and .ve. This contradicts our hypothesis that the number of types to identify is the main factor, \nsince task two, where the dynamic type system prevailed, required to identify more types than task one, \nand fewer than task .ve.  For tasks three and four with the same number of types to be identi.ed but \nwith a different complexity we ob\u00adserved another result that contradicts our hypothesis,  since the \ndynamic type system prevailed in a task of medium dif.culty (and did not prevail in all the easy tasks). \nConsequently, a pure reduction of the programming tasks to the number and the dif.culty of the types \nto be identi.ed cannot be a main effect on the difference between static and dynamic typing; there are \nclearly other factors at work.  4.5 Discussion The results show that the chosen experimental design \nwas appropriate for the research question and the given task: the learning effect did compensate the \nmain effect for the group starting with Java (no signi.cant differences for task 1, 4 and 5) but it did \nnot lead to contradictory results (such as one group showing positive effects for one language, while \nthe other shows a positive effect for the other). However, the study revealed unexpected results: While \nthere was a signi.cant positive impact of the static type system for tasks one, four, and .ve, there \nwas a signi.cant positive impact of the dynamic type system on tasks two and three. This result is surprising \nin several ways. First, it is unclear why there should be a signi.cant differ\u00adence for task one: the \ntask was only to instantiate a class no parameters needed to be sent, nor anything that seems to be related \nto the question of type system seems to be rel\u00adevant here. The subjects (which were mainly familiar with \nJava) could have been slightly surprised by seeing the key\u00adword def in the code snippet. Following this \nargumentation leads to the conclusion that the Java solutions had a bene.t in comparison to the Groovy \nsolutions, possibly due to the subjects being more accustomed to Java than Groovy, de\u00adspite the presence \nof the warmup task. We were not surprised to see a positive impact of Java in tasks four and .ve: For \ntask four, the static type system ex\u00adplicitely documents the recursive type de.nition and gives in that \nway developers a bene.t in comparison to the dynami\u00adcally typed version where this information was missing. \nFor task .ve, a large set of types were necessary in order to ini\u00adtialize the required Menu object. While \ndevelopers having the annotations of the static type system could directly see from the class de.nitions \nwhich objects needed to be instan\u00adtiated, developers with the dynamic type system need to .nd out on \ntheir own which classes possibly match. However, tasks two and three revealed a completely dif\u00adferent \npicture: in both cases the developers with the dynamic type system were faster. Since our IDE gathers \nmore infor\u00admation than the completion time of the tasks, we conducted an exploratory study to better \nunderstand this result. 5. Exploratory Study For some tasks the impact of the static or dynamic type \nsystem is contrary to our intuitions; we investigated whether this contrary effect might be explained \nby other factors.  A possible in.uencing factor is the number of builds and test runs that were performed \nduring the implementation of each task. Assuming that people with the dynamically typed solutions need \nto explore on their own what kind of objects are required by the API, they would require more compilations \nand test runs in order to situate themselves. Another in.uencing factor is the number of .les devel\u00adopers \nwere watching while working on a solution. People working on the dynamically typed solutions probably \nhave to spend more time on different class de.nitions in order to .nd out what types might be expected \nby the corresponding API. A third and related data point is the number of .le switches that occur during \nthe session. These data points are recorded by our IDE; we study them in turn. First, we analyze how \noften people built and ran the code. Second, we analyze the number of .les that were consulted for each \ntask. Finally, we analyze how much developers have navigated between .les. As we did previously, we shifted \nparts of the analysis to the appendix in order to ease the reading of the paper (see appendix A.4, A.5, \nand A.6). 5.1 Number of Builds and Test Runs The numbers of builds and test runs we count here is not \nonly the number of test runs on code that compiles, but the number of times a developer pressed the start-button \nfrom the IDE; this involved compiling and running the test cases. For statically typed solutions this \ntest run also implies the type check, which potentially fails. The potential difference between such \ntest runs could mean that people working on the statically typed solutions gain more information about \nthe expected types than the people using the dynamically typed solutions. In contrast, the users of the \ndynamic type system need to explore on their own which types are required by the API, possibly leading \nto more run-time errors. Task Task 1 Task 2 Task 3 Task 4 Task 5 P-Values GROOVYFIRST .23 .70 .03 .01 \n.81 JAVAFIRST .23 .03 .78 .48 .68 Fewer test runs - Groovy Java Java - Table 2. Wilcoxon tests for number \nof test runs We apply the same analysis as in the previous section by studying the tasks and groups in \nseparation (see appendix A.4). The result is in Table 2, which shows the language which had fewer builds \nand test runs. While the result is similar to the comparison of the development times for tasks two and \nfour, no signi.cant difference was found for tasks one and .ve. The result for task three is the opposite \nof the development time analysis: although task three required fewer test runs using Java, the development \ntime was still higher. At .rst glance, it seems that the number of test runs do not provide an appropriate \nexplanation for the measured development time; we will elaborate on this in the general discussion. \n5.2 Number of Watched Files Next, we analyze the number of .les being viewed by the developer. The number \nof watched .les might be an indi\u00adcator of the quantity of source code one has to read before one solves \nthe task. A reason for differences would be that developers using a dynamic type systems are more likely \nto look into sources which are not related to the current task. If one assumes that dynamically typed \nlanguage do not directly guide developers to those types which are needed by the API, many different \n.les are opened by the developer the more .les are opened, the larger the number of .les which are not \nrelated to a given task. Performing a separate analysis for the tasks and the lan\u00adguage (see appendix \nA.5) reveals the results which are sum\u00admarized in Figure 3: for all tasks (with the exception of task \ntwo) the number of watched .les is higher for the dynami\u00adcally typed solutions; for task two, it is the \nopposite. Task Task 1 Task 2 Task 3 Task 4 Task 5 P-Values GROOVYFIRST .02 .35 .05 .01 <.001 JAVAFIRST \n.67 .01 .85 .04 .05 Fewer watched .les Java Groovy Java Java Java Table 3. Wilcoxon tests for number \nof watched .les Hence, it looks like that the number of watched .les seems to provide similar results \nas the development time measurement, with the exception of task 3, where Groovy users watched more .les \nthan Java users, despite taking less time overall.  5.3 Number of File Switches The number of .le switches \ndetermines how often a devel\u00adoper switches between different .les; the .rst opening of a .le is already \nconsidered as a .le switch. If for a given task a developer has only one .le switch, then this means \nthat he has solved the tasked without switching to another .le. We can see the number of .le switches \nas a measure of the amount of exploration that a developer has to perform in or\u00adder to solve a task. \nThe underlying assumption from the task design was that the use of the dynamically typed language would \ncause the developer to switch more often between dif\u00adferent .les as he or she needs to consult more code \nin order to decide which types are needed. The difference between the number of .le switches and the \n(previously analyzed) number of watched .les is, that in the previous analysis each .le that is opened \nmore than once is only counted as one watched .le. Performing a separate analysis for the tasks and the \nlan\u00adguage reveals the results shown in Table 4 (see appendix A.6 for the details of the analysis).  \nTask Task 1 Task 2 Task 3 Task 4 Task 5 P-Values GROOVYFIRST <.001 .78 .55 .01 <.001 JAVAFIRST .48 .01 \n.02 .17 .36 Fewer switches Java Groovy Groovy Java Java Table 4. Wilcoxon tests for number of .le switches \nThe result of this analysis directly corresponds to the analysis of the development times (again, see \nTable 1): For task one, four and .ve the developers require fewer .le switches for the statically typed \nsolutions; they require more .le switches for task two and three. What s even more inter\u00adesting, the \np-values for the different groups are quite similar. Hence, the number of .le switches seem to be a plausible \nindicator for the resulting development time. We will inves\u00adtigate the reasons for this in the discussion. \n6. Discussion We start with a summary of our .ndings in the main exper\u00adiment and the exploratory analysis. \nTable 5 reports on all results of the measurements we investigated. Aspect Task 1 Task 2 Task 3 Task \n4 Task 5 Less Development Time Java Groovy Groovy Java Java Fewer Builds/Runs  Groovy Java Java Fewer \nFiles watched Java Groovy Java Java Java Fewer File switches Java Groovy Groovy Java Java Our Expectations \n Java Java Java Java Table 5. Summary of measurements we performed, and our expectations before carrying \nout the experiment. Unex\u00adpected results are shown in bold. A priori expectations. Initially, we expected \nto see de\u00advelopers using the static type system (Java users) perform the tasks faster, with the exception \nof task one, where a neg\u00adligible difference would be seen. Similarly, we expected sub\u00adjects using the \nstatic type system to hold an advantage for tasks two to .ve, in all other metrics: number of builds \nand runs indicator of ad-hoc explorations and trial-and-error; number of .le switches indicator of the \namount of explo\u00adration; and number of .les watched indicator of quantity of code read to .nish the task. \nResults. As shown above, we found some surprising re\u00adsults: task 1 shows an unexpected advantage to Java; \ntask 2, an unexpected and consistent advantage to Groovy; task 3, a time advantage to Groovy, re.ected \nin .le switches, but not in build and runs and .les watched. On the other hand, task 4 yields consistently \nexpected results, and task 5 yields expected results (except in builds and runs where there is no clear \nadvantage to Java). We continue with a task-based discussion of the possible reasons for the results \nwe observe. Task 1. We measured a positive effect of the static type system although one would expect \nthat typing hardly plays any role for such a simple task. A possible explantion would be the background \nof the subjects, who all had experience with Java, but not with Groovy. We included a warm-up task to \nalleviate this effect, but it still could be present. Task 2. In all cases the results were better for \nthe dynami\u00adcally typed solutions (time, build and runs, .le switches, and .les watched). This is even \nmore surprising if one accepts the argument put forward for task one that the subjects had, because of \ntheir background, some advantage in Java. A possible reason for the unexpected result is that the task \nis simple enough that types hinders more than they help. The design of the API having an Initializer \nobject and additionally having a Configuration object which receives a string may be intuitive; people \ncan simply use it correctly without committing any obvious error. In case this is not quite suf.cient, \nthe message-not\u00adunderstood exception for the dynamically typed solutions may easily guide developers \nto the correct class; the error speci.es which message is not understood, hinting at the correct class. \nIn contrast, the subjects using the statically typed solution may be enticed to browse the classes they \nknow they will use, in order to get more familiar with their API, even if the full API of these classes \nwill not be used. Hence, instead of trying to understand the class de.ni\u00adtion in detail, trial-and-error \nseems like a reasonable way of solving the task and in simple cases, it may be ef.cient for developers \nto behave that way, instead of consistently read\u00ading class de.nitions. Task 3. Contrary to task two, \nGroovy developers indeed watched more .les than Java developers, even if they spent less time to solve \nthe task. The Java developers also had fewer builds and runs, and fewer .les watched. This .nding is \nconsistent with the interpretation of task two: Java developers spend more time reading API classes but \nread fewer API classes. They read these .les with more attention, covering more of the API than Groovy \ndevelopers. In contrast, Groovy developers seem to jump around the system , more frequently compiling \nand running the system, and browsing more .les in the process (including .les less relevant to the task). \nHence, it is possible that the same trial and error , and partial program comprehension approach that \nworked for task 2 still works for task 3. We already see the limits of the approach, as the slow but \ndeliberate approach used in the Java solutions ends up requiring fewer builds and runs fewer runtime \nerrors , and less investigation of unrelated .les. The task may also be simple enough that subjects easily \nknow where to start: From the task de.nition, subjects were already aware that some kind of transformation \nis needed; the initialization of the tranformer object, and the construc\u00adtion of the graph may be intuitive \nenough for the trial and error approach to work.  On the other hand, Java users may have been confused \nby the presence of the generic types. Although they had the ben\u00ade.t that they can read directly from \nthe Transformer s class de.nition that a graph object is required, they possibly spent more time on the \nde.nition of the generic types without a signi.cant bene.t. Task 4. The argument that complex types reveal \nmore about the structure of the program, but are harder to interpret would also explain the difference \n(pro Java) for task four. Generic types were also required; while the generic type de.nitions reveals \nan important characteristic the recursive type de.nitions which directly help to understand the de\u00adsign \nof the underlying class. this was not the case in task three. In task three, it might have been helpful \nto see that an object of type Graph is required, but the additional type parameters may have reduced \neven negated the possible positive impact of the static type system. In contrast, in task four, the recursive \ntype de.nition is hard to understand without type information. Groovy users hence needed to read more \n.les, more .le switches, more builds and runs, and more time overall. According to our theory, the trial-and-error \napproach, appropriate for simple types, clearly shows its limit for complex types. Task 5. Task .ve is \nalso interesting in constrast to task two. Task .ve required more types to be identi.ed, but the types \nto identify were of a similar complexity. Thus one could conclude that the more types need to be found, \nthe more the statically typed solution is advantageous; less .le browsing is necessary, and fewer .le \nswitches as well. There are no differences in builds and runs, however. This could be due to the simplicity \nof the type de.nitions themselves (in comparison to task four). Summary. Analyzing the tasks through \nthe various met\u00adrics, we built a working theory of why we saw the results we observed: The difference \nin task one could be due to the experience of the subjects with Java. For tasks two on\u00adwards, simple \ntype de.nition may be easier to understand through trial-and-error than through static typing. Static \ntyp\u00ading encourages subjects to consult the class de.nition that are mentioned, whereas the users of the \ndynamic type sys\u00adtem employ a more partial and less systematic program com\u00adprehension process. This approach \nshows its limits for more complex tasks, when more types, or more complex types are identi.ed. These \nneed more browsing, .le switches, and pro\u00adgram runs (especially for complex type de.nitions), than the \nstatically typed version. This is only a working theory; it needs to be con.rmed by other controlled \nexperiments, and qualitative and quanti\u00adtative program comprehension studies. Further, we are cur\u00adrently \nnot able to formulate this theory more precisely: Al\u00adthough we suspect that, for example, task two and \nfour differ with respect to their complexity, and although we think that the type system in task four \ndocuments better the design of the underlying classes, we cannot describe this in a way that we could \ndetermine to what extent this documentation is bet\u00adter. Likewise, the apparent correlation between .le \nswitches and development time warrants further investigation. However, this working theory is a .rst \nstep towards for\u00admulating a theory that describes differences in developer per\u00adformance in statically \nand dynamically typed programs. 7. Conclusion This paper described an experiment comparing static and \ndy\u00adnamic type systems for programming tasks in an undocu\u00admented API. We gave 27 subjects .ve programming \ntasks and found that the type systems had a signi.cant impact on the development time: for three of .ve \ntasks me measured a positive impact of the static type system, for two tasks we measured a positive impact \nof the dynamic type system. Based on the previous discussion, our overall conclusions for the use of \nstatic/dynamic type systems in undocumented APIs are the following: 1. There is no simple answer to the \nstatic vs. dynamic typing question: The question of whether or not static types are helpful for developers \ncannot be generally an\u00adswered without taking the programming tasks into ac\u00adcount. In fact, this is completely \nconsistent with the results of previous experiments, such as Prechelt and Tichy s [22], or our own experiments[10, \n28]. 2. The type system has an in.uence on the development time: The choice of the static or dynamic \ntype system had an in.uence on the development time for all program\u00adming tasks in the experiment. Again, \nthis is consistent with previous experiments (such as [11, 22, 28]). 3. Dynamic type systems potentially \nreduce develop\u00adment times for easier tasks: Although we are currently not aware of how to determine exactly \nwhat easy and hard means, it seems that if a dynamic type systems has a positive impact, it is for easier \ntasks (which is consistent with the experiments described in [10, 28]). Although there was one counter \nexample in the experi\u00adment (task 1), we think that the result for this task is a consequence of the chosen \nsubjects low familiarity with the dynamic language, Groovy (despite the presence of a warmup task). \n4. Static type systems reduce development times if  (a) the type annotations explicitely document design \nde\u00adcisions, or (b) the number of classes to identify in the programming tasks is relatively high.  \nWe argued for a) based on the fourth programming task (also in comparison to task two and three) and \nfor b) based on the .fth programming task (in comparison to taks two and three). However, we also showed \nthat a pure reduction to the number of classes is not valid (see section 4.4).  Strong empirical evidence \nis needed to back the arguments in favor of static or dynamic type systems. From that perspec\u00adtive, we \nconsider this experiment as a valuable contribution. We also think that much more experimentation is \nneeded in order to strengthen the evidence the current state of exper\u00adimentation in the area of programming \nlanguages, and type systems in particular, is weak. We hope that more experi\u00adments and replications will \nbe performed by the program\u00adming language community at large. Comparison to related work. The experiment \nby Klein\u00adschmager et al. [15] should also be taken into account; there, for no single task a positive \nimpact of the dynamic type sys\u00adtem could be shown. Comparing the tasks of the experiment presented here \nand the one in [15], we think that (again) the difference lies in the complexity of the programming tasks. \nWhile the programming tasks in this experiment re\u00adquire to instantiate and con.gure objects, the programming \ntasks in [15] required more interactions between the objects. We think that it is necessary in future \nexperiments not only to think about complexity from the perspective of number of unknown artifacts that \nshould be used but also from the per\u00adspective of complexity of the required interaction between the developer \nand the API classes . 8. Future Work While there is already some strong evidence for our .rst two conclusions \n(as they agree with several previous exper\u00adiments in the literature), we think that much more research \nis required in order to give more evidence for the conclu\u00adsions three and four. We plan to do so in subsequent \nstud\u00adies. Rather than closing the issues, we see this experiment as starting point that opens a number \nof questions for following experiments. Our discussions (and even the original research question) were \nstrongly related to the idea that the static type system has a positive impact on the documentation. \nIn fact, this is related to the kind of static type system as provided by pro\u00adgramming languages such \nas Java where the type system also implies the existence of corresponding type annotations in the code. \nAnnotations without type checks. In a follow up experi\u00adment, we will study if the positive impact we \ndetected for the static type system, as shown in the programming tasks four and .ve, can also be achieved \nonly with type annotations in the code without static type checks. Such an experiment would measure the \neffect of types as pure documentation. Static type checks without annotations. A related ex\u00adperiment \nwould be to evaluate the bene.ts of a statically typed language using type inference instead of annotations \n(such as ML). The motivation for such an experiment is sim\u00adilar to the previous one but from a different \nperspective: as\u00adsessing whether the positive effect of static typing, as shown in the programming tasks \nfour and .ve, can be achieved al\u00adthough the type annotations are missing. Comparing type systems. One \nfurther experiment is to check whether a type system effect can be achieved by dif\u00adferent static type \nsystems. We are currently investigating whether generic types as introduced in Java 1.5 have a mea\u00adsurable \nimpact on development time in comparison to Java 1.4 code. Further experiments are imaginable that test \ndiffer\u00adent extensions of static type systems. Impact of documentation. Another question is to what extent \nthe type system plays a role if the API is well doc\u00adumented. A good documentation may reduce the effect \nof the (static or dynamic) type system on the developer perfor\u00admance. Works such as [14, 24] provide \nadditional insights about other possible in.uencing factors for the use of APIs, which might have a larger \neffect than the type system of the underlying programming language. Tool support. Finally, tooling needs \nto be analysed. A common statement is that such tools are better if they are based on a static type system. \nHence, a comparison of dif\u00adferent tools (e.g., code completion) for dynamically typed languages making \nuse of type inference versus actual static types would be interesting. Acknowledgements The authors would \nlike to thank the volunteers from the University of Duisburg-Essen and from the University of Chile that \nparticipated in the experiment. References [1] BIRD, R., AND WADLER, P. An introduction to functional \nprogramming. Prentice Hall International (UK) Ltd., Hert\u00adfordshire, UK, UK, 1988. [2] BORTZ, J. Statistik: \nf\u00fcr Human-und Sozialwissenschaftler, 6., vollst. \u00fcberarb. u. aktualisierte au.. ed. Springer, Septem\u00adber \n2005. [3] BROOKS, R. E. Studying programmer behavior experimen\u00adtally: the problems of proper methodology. \nCommun. ACM 23 (April 1980), 207 213. [4] BRUCE, K. B. Foundations of object-oriented languages: types \nand semantics. MIT Press, Cambridge, MA, USA, 2002. [5] CURTIS, B. Five paradigms in the psychology of \nprogram\u00adming. In Handbook of Human-Computer Interaction, M. He\u00adlander, Ed. Elsevier (North-Holland), \n1988, pp. 87 106. [6] DALY, M. T., SAZAWAL, V., AND FOSTER, J. S. Work in progress: an empirical study \nof static typing in ruby. Workshop on Evaluation and Usability of Programming Languages and Tools (PLATEAU),Orlando, \nFlorida, October 2009 (2009). [7] FENTON, N. E., AND PFLEEGER, S. L. Software Metrics: A Rigorous and \nPractical Approach. PWS Publishing Co., Boston, MA, USA, 1998. [8] GANNON, J. D. An experimental evaluation \nof data type conventions. Commun. ACM 20, 8 (1977), 584 595. [9] GRAVETTER, F., AND WALLNAU, L. Statistics \nfor the Be\u00adhavioral Sciences. Cengage Learning, 2008.  [10] HANENBERG, S. An experiment about static \nand dynamic type systems: Doubts about the positive impact of static type systems on development time. \nIn Proceedings of the ACM international conference on Object oriented programming systems languages and \napplications (New York, NY, USA, 2010), OOPSLA 10, ACM, pp. 22 35. [11] HANENBERG, S. Faith, hope, and \nlove: An essay on soft\u00adware science s neglect of human factors. In Proceedings of the ACM international \nconference on Object oriented pro\u00adgramming systems languages and applications (Reno/Tahoe, Nevada, USA, \nOctober 2010), OOPSLA 10, pp. 933 946. [12] HANENBERG, S. A chronological experience report from an initial \nexperiment series on static type systems. In 2nd Workshop on Empirical Evaluation of Software Composition \nTechniques (ESCOT) (Lancaster, UK, 2011). [13] JURISTO, N., AND MORENO, A. M. Basics of Software Engineering \nExperimentation. Springer, 2001. [14] KAWRYKOW, D., AND ROBILLARD, M. P. Improving API usage through \nautomatic detection of redundant code. In ASE 2009, 24th IEEE/ACM International Conference on Automated \nSoftware Engineering, Auckland, New Zealand, November 16-20, 2009 (2009), IEEE Computer Society, pp. \n111 122. [15] KLEINSCHMAGER, S., HANENBERG, S., ROBBES, R., TAN-TER, \u00c9., AND STEFIK, A. Do static type \nsystems improve the maintainability of software systems? An empirical study. In IEEE 20th International \nConference on Program Comprehen\u00adsion, ICPC 2012, Passau, Germany, June 11-13, 2012 (2012), pp. 153 162. \n[16] KOENIG, D., GLOVER, A., KING, P., LAFORGE, G., AND SKEET, J. Groovy in Action. Manning Publications \nCo., Greenwich, CT, USA, 2007. [17] MCCONNELL, S. What does 10x mean? Measuring varia\u00adtions in programmer \nproductivity. In Making Software: What Really Works, and Why We Believe It, A. Oram and G. Wilson, Eds., \nO Reilly Series. O Reilly Media, 2010, pp. 567 575. [18] PIERCE, B. C. Types and programming languages. \nMIT Press, Cambridge, MA, USA, 2002. [19] POTTIER, F., AND R\u00c9MY, D. The essence of ML type in\u00adference. \nIn Advanced Topics in Types and Programming Lan\u00adguages, B. C. Pierce, Ed. MIT Press, 2005, ch. 10, pp. \n389 489. [20] PRECHELT, L. An empirical comparison of seven program\u00adming languages. IEEE Computer 33 \n(2000), 23 29. [21] PRECHELT, L. Kontrollierte Experimente in der Soft\u00adwaretechnik. Springer, Berlin, \nMarch 2001. [22] PRECHELT, L., AND TICHY, W. F. A controlled experiment to assess the bene.ts of procedure \nargument type checking. IEEE Trans. Softw. Eng. 24, 4 (1998), 302 312. [23] RICE, J. A. Mathematical \nStatistics and Data Analysis. Duxbury Press, Apr. 2001. [24] ROBILLARD, M. P. What makes APIs hard to \nlearn? answers from developers. IEEE Software 26, 6 (2009), 27 34. [25] SHNEIDERMAN, B. Software Psychology: \nHuman Factors in Computer and Information Systems. Winthrop Publishers, August 1980. [26] SIEK, J. G., \nAND TAHA, W. Gradual typing for objects. In ECOOP 2007 -Object-Oriented Programming, 21st Euro\u00adpean Conference, \nBerlin, Germany, July 30 -August 3, 2007, Proceedings (2007), vol. 4609 of Lecture Notes in Computer \nScience, Springer, pp. 2 27. [27] STEINBERG, M., AND HANENBERG, S. What is the impact of static type \nsystems on debugging type errors and semantic errors? An empirical study of differences in debugging \ntime using statically and dynamically typed languages -yet pub\u00adlished work. [28] STUCHLIK, A., AND HANENBERG, \nS. Static vs. dynamic type systems: An empirical study about the relationship be\u00adtween type casts and \ndevelopment time. In Proceedings of the 7th symposium on Dynamic languages (Portland, Oregon, USA, 2011), \nDLS 11, ACM, pp. 97 106. [29] WOHLIN, C., RUNESON, P., H\u00d6ST, M., OHLSSON, M. C., REGNELL, B., AND WESSL\u00c9N, \nA. Experimentation in soft\u00adware engineering: an introduction. Kluwer Academic Pub\u00adlishers, Norwell, MA, \nUSA, 2000. A. Appendix   A.1 Experimental Design A general question when designing an experiment is \nwhether it should be done as a between-subject (without repeated measurement) or a within-subject (with \nrepeated measure\u00adment on each subject) experiment. The main reason for choosing a within-subject design \nis that programming tasks are typically quite complex. The subjects do not only have to manage the intellectual \neffort of the programming tasks, they must additionally be able to handle a development en\u00advironment \nsuch as an editor, understand compiler or run\u00adtime exceptions, etc. We are far from ideal situations, \nsuch as measuring reaction times to simple stimuli. As a con\u00adsequence, programming tasks have a large \ndeviation (see for example [5, 17] for a more detailed discussion on this) that potentiallys hide the \nmain effect that should be stud\u00adied in the experiment. This does not happens because there is no main \neffect; this happens because the individual dif\u00adferences among subjects are much larger than the main \nef\u00adfect. Within-subject design help to overcome this problem (see [3]) although the potential problem \nof learning effects needs to be considered. We assumed for the experiment a (positive) impact of the \nstatic type system on the measured results, but we also as\u00adsumed a positive impact of the learning effect. \nConsequently, the repeated measurement for each subject contains both, the learning effect as well as \nthe type system effect. Figure 1 in section 3.4 illustrates the implications of the within-subject design \n whose measurement implies the learning as well as the language effect in more detail: the upper and \nthe lower part of the .gure represents two (very similar) subjects in the two different groups. The .gure \ncon\u00adtains the .rst as well as the second measurement. For group one (Groovy .rst) this means that the \n.rst measurement is the development time for the Groovy solution, while for the second group (Java .rst) \nthe .rst measurement is the de\u00advelopment time required for the Java solution. The second measurement \n(and hence the difference between the .rst and the second measurement) represents the development time \nrequired for the second language. The second measurement also contains the learning effect, which in \nboth cases reduces the development time of the .rst measurement.  Under the assumption that the static \ntype system reduces the development time, the second measurement for a sub\u00adject in the group starting \nwith Groovy must be much smaller than the .rst measurement, because learning effect as well as the language \neffect reduce the development time. Under the assumption that the learning effect is smaller than the \nlanguage effect, the second measurement for a subject in the group starting with Java is higher than \nthe .rst measurement. Hence, if for each subject in the sample the .rst and the sec\u00adond measurements \nare similar to the ones shown in Figure 1, it can be concluded that the development time using the static \ntype system in Java requires less time than the devel\u00adopment time using the dynamic type system in Groovy. \nKeep in mind that the resulting analysis is performed on a sample; it is not necessary that the assumptions \nhold for each subject in the sample, they should hold only in the average. Figure 8. Potential problem \nin the experimental design: large learning effect The experimental design would fail if the learning \neffect is much larger than the effect of the type system. In such a situation, the experiment would reveal \nfor both groups a signi.cant decrease of development times. Figure 8 illus\u00adtrates this potential problem: \nIn both cases the language ef\u00adfect is rather small in comparison to the learning effect, i.e. the learning \neffect dominates the language effect. Conse\u00adquently, for the subjects in both groups the second measure\u00adment \nwould be lower than the .rst measurement. In such a situation the experiment would not reveal anything \nwith respect to type systems in both situations the second mea\u00adsurement is lower than the .rst measurement. \nBecause of the above mentioned large deviations, the differences between the .rst and the second measurements \nwill probably not be signi.cant. Hence, it would only be possible to conclude that subjects who perform \na programming tasks for the second time, are quicker in the second round which is irrelevant for the \nresearch question followed by the experiment. Importantly, the design does not require the learning ef\u00adfect \nto be much smaller than the language effect. It is valid if both effects have a comparable size. In such \na situation, the group with the accumulated effects will show (signi.cant) differences between the .rst \nand the second measurement while the group with the abrogated effects does not.  A.2 Measured Development \nTime and Descriptive Statistics Table 6 contains all measured data for all 27 subjects in the experiment. \nIn addition to the raw data we also include the differences for each subject and each task in the table \n(Groovy times -Java times), i.e. a negative value means that the subject required more time using Java \nthan using Groovy. Table 7 shows the descriptive statistics for the measure\u00adments. It turns out, that \nthe standard deviation for the differ\u00adent tasks is quite large (in most cases comparable to the me\u00addian) \n a phenomenon that can be often found in program\u00adming experiments which from our point of view strength\u00adens \nthe experimental design based in a within-subject com\u00adparison (see sections 3.4, A.1 and 3.10). For task \none 19 subjects required more time for the Groovy solution than for the Java solution. For task two and \nthree, just 10 subjects required more time for the Groovy solution, for task four 22 subjects required \nmore time for the Groovy solution, and .nally 20 subjects required for task 5 more time for Groovy than \nfor Java. For task one, four and .ve a majority required more time using the dynamic type system while \nfor task two and three it is the opposite.  A.3 Within-Subject Analysis Figure 9 illustrates the boxplot \nfor the within-subject mea\u00adsurement of group GROOVYFIRST (the group that started .rst with the dynamically \ntyped tasks). The differences to the between-subject measurement from Figure 10 are ob\u00advious: tasks four \nand .ve reveal differences, task one and three show potential differences and task two probably does \nnot reveal any difference in the development time of Groovy and Java. Furthermore, the differences correspond \nto the ex\u00adpectations: in all cases the median of the Groovy develop\u00adment times is larger than the median \nof the Java develop\u00adment times. As argued in section 3.4 this difference consists not only of the difference \nbetween the type systems but also of the learning effect.  Task 1 Task2 Task3 Task4 Task5 Sums No Group \nJava Groovy Diff Java Groovy Diff Java Groovy Diff Java Groovy Diff Java Groovy Diff Java Groovy Diff \n 1 B 897 124 -773 793 1336 543 2362 1390 -972 3079 2225 -854 1113 941 -172 8244 6016 -2228 2 B 179 58 \n-121 1152 373 -779 817 1703 886 1307 2414 1107 949 1342 393 4404 5890 1486 3 B 175 944 769 2868 1210 \n-1658 2447 1271 -1176 1887 1834 -53 1669 1399 -270 9046 6658 -2388 4 B 196 110 -86 1451 644 -807 4823 \n607 -4216 899 1554 655 1148 1220 72 8517 4135 -4382 5 B 89 60 -29 596 691 95 2191 278 -1913 1877 824 \n-1053 605 1126 521 5358 2979 -2379 6 B 366 121 -245 2842 2708 -134 1348 1234 -114 1761 2942 1181 2296 \n1120 -1176 8613 8125 -488 7 B 122 139 17 1021 310 -711 2444 805 -1639 886 2942 2056 893 887 -6 5366 5083 \n-283 8 B 82 69 -13 454 633 179 845 1419 574 2817 3113 296 890 607 -283 5088 5841 753 9 A 86 448 362 2429 \n1977 -452 1108 924 -184 1054 2238 1184 1326 1484 158 6003 7071 1068 10 A 58 228 170 348 492 144 634 518 \n-116 455 1671 1216 644 1025 381 2139 3934 1795 11 A 105 144 39 461 345 -116 621 805 184 382 1144 762 \n532 548 16 2101 2986 885 12 A 45 225 180 1380 951 -429 1134 1604 470 652 2625 1973 577 1774 1197 3788 \n7179 3391 13 A 63 539 476 404 358 -46 917 477 -440 738 2473 1735 387 2705 2318 2509 6552 4043 14 A 83 \n248 165 944 2190 1246 515 1247 732 1514 4359 2845 862 1652 790 3918 9696 5778 15 A 62 177 115 374 1266 \n892 338 515 177 697 3714 3017 435 759 324 1906 6431 4525 16 B 139 181 42 648 362 -286 1558 328 -1230 \n1557 4179 2622 988 615 -373 4890 5665 775 17 B 904 76 -828 4644 437 -4207 4963 1378 -3585 5795 2663 -3132 \n1296 1078 -218 17602 5632 -11970 18 B 483 730 247 4787 608 -4179 5747 768 -4979 2801 4614 1813 1238 5953 \n4715 15056 12673 -2383 19 B 183 243 60 1673 683 -990 2344 500 -1844 1848 2012 164 709 812 103 6757 4250 \n-2507 20 B 255 155 -100 2298 1449 -849 2398 1472 -926 2144 2649 505 1236 2142 906 8331 7867 -464 21 A \n44 144 100 1097 1775 678 225 1059 834 272 8042 7770 407 2955 2548 2045 13975 11930 22 A 273 1195 922 \n1602 847 -755 527 866 339 1392 5185 3793 1170 2310 1140 4964 10403 5439 23 A 107 6411 6304 4081 147 -3934 \n1088 1388 300 1001 1682 681 897 1596 699 7174 11224 4050 24 A 169 190 21 408 1060 652 1040 936 -104 4107 \n1555 -2552 517 813 296 6241 4554 -1687 25 A 234 1011 777 2449 665 -1784 2955 1564 -1391 1296 3558 2262 \n1235 2612 1377 8169 9410 1241 26 A 288 610 322 789 1180 391 2015 6676 4661 1559 6341 4782 567 1387 820 \n5218 16194 10976 27 A 75 216 141 473 1059 586 3534 2399 -1135 1292 3190 1898 645 735 90 6019 7599 1580 \n Table 6. Development times for all 27 subjects; times in seconds, differences are Groovy times -Java \ntimes; Group A = GROOVYFIRST; Group B = JAVAFIRST Task 1 Task2 Task3 Task4 Task5 No Java Groovy Diff \nJava Groovy Diff Java Groovy Diff Java Groovy Diff Java Groovy Diff min 44.0 58.0 -828.0 348.0 147.0 \n-4207.0 225.0 278.0 -4979.0 272.0 824.0 -3132.0 387.0 548.0 -1176.0 max 904.0 6411.0 6304.0 4787.0 2708.0 \n1246.0 5747.0 6676.0 4661.0 5795.0 8042.0 7770.0 2296.0 5953.0 4715.0 mean 213.4 548.0 334.6 1572.8 953.9 \n-618.9 1886.6 1264.1 -622.5 1669.2 3027.5 1358.3 934.5 1540.6 606.1 median 139.0 190.0 100.0 1097.0 691.0 \n-286.0 1348.0 1059.0 -184.0 1392.0 2649.0 1184.0 893.0 1220.0 324.0 std. dev. 224.1 1212.8 1252.7 1314.5 \n634.6 1452.5 1473.2 1189.4 1834.0 1215.4 1622.4 2153.1 431.9 1106.5 1143.1 Table 7. Descriptive statistics \nfor development time (time in seconds for all but standard deviation) The signi.cance tests con.rm the \nprevious impressions concerning differences for the .ve tasks. Because the data is now based on a within-subject \nmeasurement, the Java and Groovy development times should not be tested for normal\u00adity separately. Instead, \nthe differences should be checked (see [2]). Table 8 shows the results for the Shapiro-Wilk-test in order \nto check the normality assumption, and the corre\u00adsponding p-values for the Wilcoxon-test and t-test. \nFor task 1, 4 and 5, the differences are signi.cant. Figure 10 is the boxplot for the group JAVAFIRST. \nFor task 1 and 2, there is a potential bene.t for Groovy; for task 3, the Groovy bene.t is obvious; for \ntask 4 and 5, a potential advantage to Java. We perform the same analysis as before: we check the differences \nin measurements for normality, Table 8. Signi.cance tests for the within-subject compari\u00adson of Group \nGROOVYFIRST Shapiro-Wilk p-value <.001 .01 <.001 .29 .05 Applied test Wilcoxon Wilcoxon Wilcoxon t-Test \nWilcoxon p-value <.001 .93 .45 .03 <.001 dominating language Java - - Java Java then we perform either \nthe Wilcoxon-test or the t-test (Table 9).  A.4 Test Runs Table 10 contains the descriptive statistics \nfor the number of test runs, and Figure 11 shows the boxplot. It seems as Figure 9. Boxplot for the within-subject \ncomparison of Figure 11. Boxplot for test runs Group GROOVYFIRST  ni.cant (p<.07 and . 2 p =.092). \nAgain, the factor program\u00ad ming language is not signi.cant (p>.99). This is similar to the results for \nthe measured development time. However, the second round reveals different results. Neither the fac\u00adtor \nprogramming task nor the interaction between task and programming language are signi.cant (p=.101, respectively \np=.188). Instead, the factor programming language turns out to be signi.cant (p<.01, . 2 p =.30). A non-parametric \ntest shows signi.cant differences for tasks 2, 3, and 4 (Table 11). Group GROOVYFIRST Task Task 1 Task \n2 Task 3 Task 4 Task 5 p-value .23 .70 .03 .01 .81 Fewer test runs - - Java Java -  Group JAVAFIRST \np-value .23 .03 .78 .48 .68 Fewer test runs - Groovy - - - Table 11. Wilcoxon tests for number of test \nruns  A.5 Watched Files The repeated measures ANOVA on the number of watched .les reveals for the second \nround a signi.cant factor of pro\u00ad gramming task (p<.001 and . 2 p =.31), a signi.cant interac-Table 9. \nSigni.cance tests for the within-subject compari\u00ad Task Task 1 Task 2 Task 3 Task 4 Task 5 Shapiro-Wilk \np-value .08 <.001 .43 .60 <.001 Applied test t-Test Wilcoxon t-Test t-Test Wilcoxon p-value .35 .01 .01 \n.34 .86 dominating language - Groovy Groovy - - son of group JAVAFIRST tion between the programming \ntask and the group (p<.02 and . 2 p =.116) and non-signi.cant factor group (p=.19). For if the boxplot \n(which does not take into account that each the second round, the factor programming task is signif\u00ad \n2 p =.367) the interaction is signi.cant icant (p<.001 and .subject is measured twice) already indicates \nthat there is no (p<.01 and . 2 p =.176) as well as the factor group (p<.001 and general tendency with \nrespect to the number of test-runs: 2 p for task one, three, and four there seems to be a tendency .=.35). \nThe results of the Wilcoxon-test for both groups is that the number of test runs is higher for the dynamically \ntyped solutions. For task two and .ve it is unclear whether there is a difference between the number \nof test runs for the dynamically typed or the statically typed solutions. Performing a repeated measures \nANOVA on the test runs reveals a .rst interesting characteristic. For the .rst round, in Table 12.  \nA.6 File Switches Table 13 contains the descriptive statistics for the number of test runs, and Figure \n12 shows the boxplot. The boxplot shows that the number of .le switches is comparable to the with p<.01 \nand . 2 p =.142 there is a signi.cant impact of the development time. programming tasks. However, the \ninteraction between the The repeated measures ANOVA reveals for the .rst round factor task and programming \nlanguage is only close to sig-a signi.cant factor of programming task (p=.0 and . 2 p =.36), Table 10. \nDescriptive statistics for number of test runs  Task 1 Task2 Task3 Task4 Task5 No Java Groovy Diff Java \nGroovy Diff Java Groovy Diff Java Groovy Diff Java Groovy Diff min 1 1 0 4 2 -2 1 3 2 2 8 6 4 1 -3 max \n44 70 26 51 24 -27 66 57 -9 72 83 11 49 63 14 mean 7.67 12.15 4.48 16.78 10.63 -6.15 12.33 20.81 8.48 \n15.67 25.89 10.22 12.48 11.48 -1.00 median 4 10 6 11 10 -1 4 19 15 13 20 7 10 6 -4 std. dev. 9.17 13.99 \n4.82 13.96 6.04 -7.92 18.14 14.64 -3.51 14.37 16.68 2.31 9.80 14.25 4.45 Task 1 Task2 Task3 Task4 Task5 \nNo Java Groovy Diff Java Groovy Diff Java Groovy Diff Java Groovy Diff Java Groovy Diff min 1 1 26 1 \n-25 24 30 6 27 57 30 35 31 -4 max 29 202 173 366 162 -204 412 285 -127 444 375 -69 201 299 98 mean 9.67 \n23.63 13.96 90.33 52.96 -37.37 113.33 80.41 -32.93 91.59 156.37 64.78 71.74 102.07 30.33 median 8 11 \n3 73 43 -30 73 60 -13 71 156 85 64 91 27 std. dev. 7.13 40.36 33.23 75.43 35.94 -39.49 93.14 65.10 -28.04 \n82.28 76.41 -5.86 32.81 60.69 27.88 Table 13. Descriptive statistics for number of .le switches Group \nGROOVYFIRST Group GROOVYFIRST Task Task 1 Task 2 Task 3 Task 4 Task 5 p-value .02 .35 .05 .01 <.001 more \n.les Groovy - Groovy Groovy Groovy Task Task 1 Task 2 Task 3 Task 4 Task 5 p-value <.001 .78 .55 .01 \n<.001 more switches Groovy - - Groovy Groovy Group JAVAFIRST Group JAVAFIRST p-value .48 .01 .02 .17 \n0.36 more switches - Java Java - - p-value .67 .01 .85 .04 .05 more .les - Java - Groovy Groovy Table \n14. Wilcoxon tests for number of .le switches Table 12. Wilcoxon tests for number of watched .les the \ngroup (p<.01 and . 2 p =.173) and a non-signi.cant fac\u00ad tor group (p>.75). For the second round, the \nfactor program\u00ad ming task is signi.cant (p=.0 and . 2 p =.412) the interaction is signi.cant (p<.01 \nand . 2 p =.157) and the factor group is close to signi.cant (p<.06). The Wilcoxon-test on both groups \nis shown in Table 14.  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Abstract Although the study of static and dynamic type systems plays a major role in research, relatively little is known about the impact of type systems on software development. Perhaps one of the more common arguments for static type systems in languages such as Java or C++ is that they require developers to annotate their code with type names, which is thus claimed to improve the documentation of software. In contrast, one common argument against static type systems is that they decrease flexibility, which may make them harder to use. While these arguments are found in the literature, rigorous empirical evidence is lacking. We report on a controlled experiment where 27 subjects performed programming tasks on an undocumented API with a static type system (requiring type annotations) as well as a dynamic type system (which does not). Our results show that for some tasks, programmers had faster completion times using a static type system, while for others, the opposite held. We conduct an exploratory study to try and theorize why.</p>", "authors": [{"name": "Clemens Mayer", "author_profile_id": "81548570356", "affiliation": "University of Duisburg-Essen, Essen, Germany", "person_id": "P3856163", "email_address": "clemens.mayer@stud.uni-due.de", "orcid_id": ""}, {"name": "Stefan Hanenberg", "author_profile_id": "81100540390", "affiliation": "University of Duisburg-Essen, Essen, Germany", "person_id": "P3856164", "email_address": "stefan.hanenberg@icb.uni-due.de", "orcid_id": ""}, {"name": "Romain Robbes", "author_profile_id": "81331502594", "affiliation": "University of Chile, Santiago de Chile, Chile", "person_id": "P3856165", "email_address": "rrobbes@dcc.uchile.cl", "orcid_id": ""}, {"name": "&#201;ric Tanter", "author_profile_id": "81100346970", "affiliation": "University of Chile, Santiago de Chile, Chile", "person_id": "P3856166", "email_address": "etanter@dcc.uchile.cl", "orcid_id": ""}, {"name": "Andreas Stefik", "author_profile_id": "81314486348", "affiliation": "Southern Illinois University Edwardsville, Edwardsville, IL, USA", "person_id": "P3856167", "email_address": "astefik@siue.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384666", "year": "2012", "article_id": "2384666", "conference": "OOPSLA", "title": "An empirical study of the influence of static type systems on the usability of undocumented software", "url": "http://dl.acm.org/citation.cfm?id=2384666"}