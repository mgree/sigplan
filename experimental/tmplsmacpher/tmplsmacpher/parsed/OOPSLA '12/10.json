{"article_publication_date": "10-19-2012", "fulltext": "\n Adaptive Multi-Level Compilation in a Trace-based Java JIT Compiler Hiroshi Inoue , Hiroshige Hayashizaki \n, Peng Wu , and Toshio Nakatani IBM Research - Tokyo {inouehrs, hayashiz, nakatani}@jp.ibm.com Abstract \nThis paper describes our multi-level compilation tech\u00adniques implemented in a trace-based Java JIT compiler \n(trace-JIT). Like existing multi-level compilation for method-based compilers, we start JIT compilation \nwith a small compilation scope and a low optimization level so the program can start running quickly. \nThen we identify hot paths with a timer-based sampling profiler, generate long traces that capture the \nhot paths, and recompile them with a high optimization level to improve the peak performance. A key to \nhigh performance is selecting long traces that effectively capture the entire hot paths for upgrade recom\u00adpilations. \nTo do this, we introduce a new technique to generate a directed graph representing the control flow, \na TTgraph, and use the TTgraph in the trace selection engine to efficiently select long traces. We show \nthat our multi\u00adlevel compilation improves the peak performance of programs by up to 58.5% and 22.2% on \naverage compared to compiling all of the traces only at a low optimization level. Comparing the performance \nwith our multi-level compilation to the performance when compiling all of the traces at a high optimization \nlevel, our technique can reduce the startup times of programs by up to 61.1% and 31.3% on average without \nsignificant reduction in the peak performance. Our results show that our adaptive multi\u00adlevel compilation \ncan balance the peak performance and startup time by taking advantage of different optimization levels. \nCategories and Subject Descriptors D.3.4 [Processors]: Compilers, Optimization, Run-time environments \nGeneral Terms Algorithms, Experimentation, Perfor\u00admance Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that copies bear this notice and the full \ncitation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to \nlists, requires prior specific permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, \nUSA. Copyright &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10 $15.00. IBM Research - T.J. Watson Research \nCenter pengwu@us.ibm.com Keywords Trace selection and compilation, JIT compiler, profiling, Java 1. \nIntroduction Trace-based compilation uses dynamically-identified frequently-executed code sequences (traces) \nas the basic units for compilation and execution as opposed to methods for traditional method-based compilers \n(method-JIT). At the heart of a trace-based JIT compiler (trace-JIT) lies trace selection, the component \nthat forms traces out of executed instructions. Trace selection is a challenging problem because a trace-based \nsystem needs to balance two conflict\u00ading requirements, code quality and code size. In general, generating \nlonger traces is the key to achieving better performance for a trace-JIT because it increases the opportunities \nfor compiler optimizations and reduces the overhead in transitioning between compiled traces. How\u00adever, \ngenerating long traces often causes more duplication among traces. The duplication significantly increases \ncode size and compilation times and results in poor startup performance [1]. When contrasting a trace-JIT \nagainst a method-JIT, generating longer traces has a similar effect to applying more aggressive method \ninlining in a method-JIT. To achieve both fast startup performance and high peak performance in a method-JIT, \nadaptive multi-level compi\u00adlation is widely used in JVMs [2-5]. With adaptive multi\u00adlevel compilation, \na JVM first compiles most of the methods with a low optimization level and less aggressive method inlining \nto yield fast startup performance. Then the JVM identifies hot methods by profiling and recompiles them \nwith a higher optimization level and more aggressive method inlining to achieve high performance in the \nsteady state. This paper presents the design and implementation of our adaptive multi-level compilation \nscheme implemented in a trace-based Java JIT compiler. In a trace-JIT, the compiler is responsible for \nselecting the new compilation scope for upgrade recompilations. Each trace compilation can start and \nend at any point without regard to the method boundaries. Due to this flexibility of trace selection, \nidentifying hot code paths and selecting long traces that efficiently capture the hot code paths for \nrecompilation is not a trivial task. We describe how the trace selection engine, the timer-based sampling \nprofiler, and the code generator work together for effective recompilation. Although we confirmed that \nour basic adaptive multi\u00adlevel compilation scheme significantly improved the peak performance over compiling \nall of the traces at a low optimization level, we discovered that the basic trace selection technique \nsometimes generates a sub-optimal compilation scope for recompilation. For additional performance gains \nfrom recompilations, we introduced new techniques in the timer-based profiler and the trace selec\u00adtion \nengine. In the trace selection engine, we build a weighted directed graph representing the control flow \namong the traces to decide on a better program location to start a long trace for upgrade recompilation. \nWe call this a trace-transition graph (TTgraph). Each node in the TTgraph represents a trace and an edge \nrepresents the transfer of control between two traces. We show how we build the TTgraph based on a timer-based \nlow-overhead sampling mechanism, and then explain how we use the TTgraph to select good traces. We determine \nthe location to start a new trace by traversing the edges in the TTgraph backward to effectively capture \nthe entirety of each candidate hot path. We implemented our multi-level compilation scheme with the TTgraph-based \ntrace selection algorithm in a trace-based Java JIT compiler [6]. Our results showed that our multi-level \ncompilation improved the peak perfor\u00admance by up to 58.5% and 22.2% on average over the base condition \nthat does not employ upgrade recompilations from the initial optimization level. In these improvements, \nwe showed that the advanced technique using TTgraph gave additional improvements in the peak performance \nby up to 6.5% and 1.8% on average. When comparing the performance with our multi-level compilation against \nthe case compiling all of the traces with a high optimization level, the peak performance was still 3.3% \nslower on average but the startup time was significantly reduced by 31.3% on average and up to 61.1%. \nThis paper makes the following contributions. (1) We describe the design and implementation of our adaptive \nmulti-level compilation scheme for a trace-based JIT compiler. As far as we know, this is the first full-fledged \nworking implementation of adaptive multi-level compila\u00adtion in a trace-based system. (2) We evaluate \nthe performance of trace-JIT with and without adaptive multi\u00adlevel compilation and show that the multi-level \ncompilation is a practical way to balance the startup time and peak performance. (3) We show that our \nnew technique based on the TTgraph effectively improves the performance over the basic recompilation \nscheme. In summary, selecting a better compilation scope for upgrade recompilation is an im\u00adportant problem \nfor overall performance in addition to just using the different optimization level in the JIT compiler. \n The rest of the paper is organized as follows. Section 2 gives an overview of our adaptive multi-revel \ncompilation scheme. Section 3 describes our advanced trace selection technique for upgrade recompilation \nusing TTgraph. Section 4 describes our experimental results. Section 5 covers previous techniques. Section \n6 summarizes this work. 2. Basic Trace Recompilation In this section, we give an overview of our adaptive \nmulti\u00adlevel compilation scheme with emphasis on how we select a new compilation scope for recompilation, \nwhich is a problem unique to trace-based systems. Currently, our scheme employs two optimization levels, \nthe cold level used in the initial compilation and the hot level used in the upgrade recompilation. Here \nthe optimization level in\u00adcludes both the trace selection parameters and the optimization strategy in \nthe code generation. The cold level compilation focuses on the startup time of the JVM, whereas the hot \nlevel compilation focuses on the peak performance. Hence the hot level compilation seeks to use much \nlarger compilation scopes than the cold level and also uses more time-consuming optimizations in the \ncode generation. Our system starts executing the program by using the interpreter with the runtime system \nmonitoring the program execution to find frequently executed code paths and compiles them as traces at \nthe cold level. Then the runtime identifies the especially hot paths by using the timer-based sampling \nand generates new traces for upgrade recompila\u00adtion at the hot level. The upgrade recompilation in our \ntrace-JIT has 3 steps: 1. Find the hot traces that consume large amounts of CPU time by using timer-based \nsampling, 2. Record a new, longer trace that spans the identified hot traces for upgrade recompilation, \nand  3. Compile the selected trace using the hot level. Step 2 is a key component in our multi-level \ncompilation scheme to achieve high performance without a significant increase in code size or compilation \ntime.  We use timer-based sampling to identify the hot traces. In our system, each trace has a recompilation \ncounter to track the number of timer ticks in the trace. When the timer-based profiler samples a compiled \ntrace, it incre\u00adments the counter for the trace and checks the value. If the counter value reaches a \nrecompilation threshold within a certain time interval, the trace is identified as a hot trace. After \na hot trace is identified, we select a compilation scope that spans the identified hot trace for upgrade \nrecompilation. Because the cold-level compilation focuses on quick startup of the program, the compilation \nscope of each trace compiled at the cold level is typically quite small. This means recompiling a hot \ntrace using a higher optimi\u00adzation level without expanding the compilation scope is inadequate for sufficiently \nhigh peak performance. To generate a larger compilation scope, we first invali\u00addate the current compiled \ntrace. Then the next time the execution reaches the trace head of the invalidated trace, we record the \nexecution path starting from that location using a larger maximum trace length than the initial compilation \nto find a long trace. Alternatively, we could generate a lerger compilation scope by concatenating existing \nshort traces, but our tests indicated this approach was inferior. An advantage of the trace-JIT over \nthe method-JIT is that each trace is specialized for a specific execution path and hence the profile \nresults are more important in the trace-JIT. Concatenating multiple traces may inefficiently mix traces \nfrom different execution paths, and this may cause the profiling results to be inaccurate or even illegal. \nIn our system, we again profile for a new trace at the time of upgrade recompilation. After the new trace \nis recorded for recompilation, it is compiled by the compilation thread using the hot optimiza\u00adtion level. \nAt the time of trace invalidation, we also invalidate the branch instructions that jump into the invalidated \ntrace. These branch instructions were originally generated by trace linking optimization [7], which directly \ndispatches the next compiled trace at the trace exit, and they revert to their original states. After \ngenerating the new trace, those branch instructions are rewritten again to jump into the new trace at \nthe next execution.  3. Trace Recompilation via Trace-Transition Graph As shown in Section 4, the basic \nscheme for the adaptive multi-level compilation can improve the peak performance significantly compared \nto the case of using only the cold\u00adlevel compilation. However we found that there were still more opportunities \nto improve the trace selection engine to generate better compilation scopes for recompilation. To address \nthe limitation of the basic trace recompila\u00adtion, we introduce a new recompilation scheme based on the \ntrace-transition graph (TTgraph), which is a control flow graph using traces as nodes. We use the TTgraph \nto select the longest possible traces that capture the hot code paths without causing significant code \nduplication. 3.1 Motivation Although our basic recompilation technique described in Section 2 successfully \nimproved the peak performance, we discovered that it sometimes generated a sub-optimal compilation scope. \nFigure 1 shows an example of the program Trace-Transition graph (TTgraph)  desired trace selection for \nrecompilation. In this example, the program executes a loop, which consists of BB1 and BB2, and the following \nlong linear hot path. When we start to select and compile traces with our multi-level compila\u00adtion, we \ngenerate lots of short and fragmented traces to achieve faster start up by avoiding excess JIT compilation \noverhead. In this example, we limit the trace length up to two basic blocks and hence each trace includes \nonly two basic blocks. We present the status of the selected traces using a weighted directed graph called \na Trace-Transition graph (TTgraph). The TTgraph resembles a control flow graph but uses a trace as a \nnode and the transfer of the control between the two traces as an edge. The weight of the edge shows \nthe relative frequency of that transfer of control. We generate the TTgraph at runtime based on trace \nlinking information and timer-based sampling. Then we use the TTgraph for better trace selection. The \nmiddle of Figure 1 shows the TTgraph of the pro\u00adgram after the hot path was initially compiled as short \ntraces. Because the code is a long linear hot path, we want to generate a long linear trace capturing \nthe entire hot path as shown in the figure on the right. The basic recompilation technique, however, \nmay pick a different location as the trace head for the new trace based on the timer-based sampling. \nFor example, if a sufficient number of timer ticks occur in linear trace 2, the new trace starts from \nthe head of the linear trace 2, excluding linear trace 1. Such trace selection may miss optimization \nopportunities. Also, if a sufficient number of timer ticks occur later in linear trace 1 after generating \nthe new trace starting from linear trace 2, then another trace may be generated from the head of linear \ntrace 1. This could cause duplicated traces. By using the TTgraph, we aim to generate better traces \nfor upgrade recompilation as shown in the example. In a nutshell, we determine where to start a new trace \nby traversing the edges in the TTgraph backward to capture the entire hot path. We will explain the detailed \nalgorithm of the backward traversal in Section 3.4.  3.2 Profiling Trace Transition To construct the \nTTgraph, we need to profile trace transi\u00adtioning events. We extended the timer-based sampling profiler \nused to identify hot traces to profile the transitions between the compiled traces. Here is an explanation \nof the timer-based sampling. The runtime system configures a hardware timer for periodic interrupts. \nWhen the timer interrupt occurs during the execution of compiled code, a thread-local flag called the \nyield flag is set to show that this thread needs to stop at the next yield point, where the runtime system \ncan safely stop the thread and identify the exact program location. The locations of the yield points \ndetermine the accuracy and overhead of the profiler [8]. To accurately identify the trace executed when \na timer interrupt occurs, yield points are required for the trace entry point and for all of the exit \npoints, as shown in Figure 2(a). We use this approach in our basic recompilation described in Section \n2. We do not insert yield points at the trace exit points for handling exceptions to avoid code size \nbloat. Most of these exits will never be triggered, but they would require a large amount of yield-point \ncode. To generate the TTgraph, we use the timer interrupt to capture the frequent transitions between \ntraces. We do not insert yield points at the exit points of the traces, but we use the branch-and-link \ninstruction, which records the instruction pointer in the link register, to implement a jump between \ntraces when we do trace linking optimization [7]. The branch-and-link instruction is typically used for \nmethod invocation in the method-JIT. When the execution stops at the yield point at the entry of the \nnext trace, we can identify the previously executed trace by looking at the value in the link register. \nUnlike a method-JIT, the execu\u00adtion never returns to a previously executed trace in the trace-JIT, and \nhence we use the value in the link register only for generating the TTgraph. Since the execution never \nreturns to a previously executed trace, the trace-JIT does not need to maintain any information such \nas an execution stack to record the execution history. This means we cannot identify previously executed \ntraces by walking the stack. One potential drawback of this approach is that the frequent use of the \nbranch-and-link instruction without matching method returns could confuse the return-address\u00adstack-based \nbranch prediction of the processor. However (a) yield points in compiled traces for basic recompilation \nscheme (b) yield points in compiled traces for TTgraph-based recompilation entry   entry cyclic trace \nlinear trace  trace linking optimization uses branch\u00adand-link exit  exit instruction to jump into \nthe next trace exit Figure 2. Locations of yield points in compiled traces. we did not observe any \nsignificant increase in the number of branch mispredictions. Figure 2(b) depicts the locations of the \nyield points to generate the TTgraph. 3.3 Generating Trace-Transition Graph To generate the TTgraph, \nwe first generate the basic structure of the TTgraph as we link the traces. Then we adjust the weights \nof the edges using timer-based sampling. Figure 3 shows an example of the steps to generate a TTgraph. \nOur trace-JIT, like many other trace-based systems, starts generating traces to compile by selecting \na hot loop as a cyclic trace. Then the trace-JIT selects a frequently taken exit point of the first trace \nas a trace head for the next trace and starts a trace from that trace head. When a trace is generated \njust after an exit point of another trace, we can link the two traces. At the same time, we add a directed \nedge between each pair of linked traces with the weight of 1 in the TTgraph as shown in Figure 3(a). \nBy repeating this process, the trace-JIT can generate the TTgraph with sufficiently high coverage of \nthe compiled traces for the frequently executed code. A node in a TTgraph (a trace) may have multiple \nincom\u00ading edges, like the linear trace 2 in the example. Because a trace is a single-entry-multiple-exit \nblock in our system, all incoming edges must jump to the head of the trace. A node may also have multiple \noutgoing edges such as the linear (a) create basic structure of the TTgraph generated as we link the \ntraces 1 1 1 1 1 1 1 When a new trace is generated after an exit 1 point of a previous trace, an edge \nwith the weight of 1 is added between the two traces (b) increment weight of edges by timer-based sampling \n1) a timer interrupt occurs while executing linear trace 1   1 22 1 4) set the yield flag to stop \nat the next yield point forbursty sampling  1 1  (c) repeat incrementing the weight until a sampling \ntermination condition is satisfied (e.g. the execution hits a cyclic trace)  5) execution stops at the \nyield point at the head of the next traces 4 ) set the yield flag again to stop at the next yield point \nrepeat 5) and 4 ) at each trace until the burst terminates 22 1 Figure 3. Steps to generate TTgraph. \ntrace 3 in the example. An outgoing edge may cause an exit from the middle of the trace or from the end \nof the trace. After building the basic structure of the TTgraph, we adjust the weights of the edges in \nthe TTgraph to assess the hot paths in the program. As described in Section 3.2, we use the timer-based \nsampling profiler to find the transitions between traces. When the profiler samples a transition between \ntwo traces, we increment the weight of the edge representing this transition. For example, in Figure \n3(b), a timer interrupt occurs while the thread is executing linear trace 1 and then the execution stops \nat the yield point at the head of the next trace, linear trace 2. The runtime identifies the current \ntrace from the current instruction pointer value and the previous trace by the value in the link register. \nThen the weight of the edge between linear trace 1 and linear trace 2 is incremented by 1. To more quickly \nbuild a sufficiently accurate TTgraph, we sample multiple edges to increment the weights at each timer \ninterrupt. This is similar to bursty tracing [9], which allows low-overhead temporal profiling by sampling \na sequence of consecutive events (a burst) instead of just one event when it starts sampling. We sample \na sequence of transitions between traces by repeatedly stopping the execution at the yield point at the \nhead of each trace. To implement this behavior, we set the yield flag again when the execution stops \nat any yield point in the trace head, which then halts at the next trace head. In the example of Figures \n3(b) and 3(c), first the yield flag is set when the timer interrupt occurs, and then the execution stops \nat the head of linear trace 2. After incrementing the weight of the edge between linear trace 1 and linear \ntrace 2, the yield flag is set again to stop at the next yield point. The execution stops at the head \nof the linear trace 3 and the weight of the edge between linear trace 2 and linear trace 3 is increment\u00aded. \nBy repeating this steps until one of the following burst termination conditions is satisfied, we can \nsample the consecutive transitions without increasing the frequency of the timer interrupts. The burst \ntermination conditions we used are: (1) the number of transitions in this burst reaches a predefined \nthreshold (32 in the current implementation), (2) the execution reaches a trace already encountered \nin this burst, (3) the execution reaches a cyclic trace or already recompiled trace, or (4) the execution \nuses a trace exit at which no next trace is linked. Once most of the hot paths are recompiled, most bursts \nquickly terminate at an already recompiled trace and so the overhead due to the sampling is not significantly \nlarger than the basic sampling method without repeated stops in the steady state. TTgraph consumes 96 \nbytes of memory per node (a trace) in the graph. To reduce the memory consumption, (a) Incrementing \na recompilation counter based on TTgraph (b) Incrementing a recompilation counter in basic recompilation \n(without using TTgraph) Figure 4. Recompilation using the TTgraph. we track only a few of the hottest \nincoming and outgoing edges if the number of the edges becomes large. Also, we allocate these data associated \nwith a trace only on demand during the TTgraph construction, instead of allocating it at the JIT compilation \ntime.  3.4 Selecting New Trace Heads for Recompilation Based on Trace-Transition Graph We use the generated \nTTgraph to decide on better locations to start new compilation scopes for the upgrade recompila\u00adtion. \nIn this section, we assume that the TTgraph already exists as described in Section 3.3 for ease of explanation. \nIn the real implementation we generate and use the TTgraph at the same time. Figure 4(a) is an example \ndepicting how we select a trace to charge the execution time using the TTgraph in the timer-based sampling \nprofiler. When execution stops at a yield point due to a timer interrupt, we traverse the TTgraph backward \nstarting from the current location of the yield point, and decide on the trace whose execution time should \nbe increased (by incrementing the recompilation counter). We stop the backward traversal if there is \nno incoming edge in a node or if the edge to track starts from a cyclic trace, an invalidated trace, \nor an already recompiled trace. For the example in Figure 4(a), we start the back\u00adward traversal from \nlinear trace 4 and stop the traversal at linear trace 1. The recompilation counter of linear trace 1 \nis incremented. If the counter value reaches the recompilation threshold, then we will use the trace \nhead address for linear trace 1 as the starting point to start recording a long trace for upgrade recompilation. \nWe do not continue the back\u00adward traversal into the cyclic trace in this example, because our trace-JIT \nallows only cyclic or linear traces as compila\u00adtion units, and hence a cyclic execution path and a linear \nexecution path cannot be captured in the same trace even if we allow very long traces for upgrade recompilation. \n To contrast to our technique, Figure 4(b) shows how the basic recompilation works. Note that we do not \ngenerate the TTgraph for the basic recompilation, so the TTgraph was only provided to clarify this explanation. \nIn this example, we charge the execution time for linear trace 3 where the timer interrupt occurred. \nThis behavior is similar to that of the timer-based sampling profiler for a method-JIT. If the compilation \nscope for recompilation starts from the head of linear trace 3, then the new trace does not cover linear \ntraces 1 and 2, which is a sub-optimal selection. In the TTgraph, a node may have multiple incoming edges \nor outgoing edges. Figure 5 shows how we handle a node with multiple edges during the backward traversal. \nIf a node has multiple incoming edges, we track all of the edges with sufficiently large weights. We \nused 20% of the total weight of the incoming edges as the threshold. In the example of Figure 5(a), linear \ntrace 2 has two incoming edges, from linear traces 1 and 1'. We track both edges because they have weights \nlarger than the threshold. When tracking multiple edges from a node, the recompilation (a) Handling \nof multiple incoming edges  (b) Handling of multiple outgoing edges  Figure 5. Handling of a node having \nmultiple incoming or outgoing edges. (a) TTgraph after a trace is invalidated for upgrade recompilation \nWhen the recompilation counter value reaches the recompilation threshold, the trace is invalidated. The \nnext execution starting from the head of the invalidated trace is recorded for upgrade recompilation. \nThe node in the TTgraph is marked as invalidated.  (no incoming \u00b7\u00b7\u00b7 (b) TTgraph after the new trace \nis generated  1 (no incoming \u00b7\u00b7\u00b7  Figure 6. Trace invalidation and recompilation. counters of multiple \ntraces may be incremented. Here the linear trace 4' has a weight smaller than the threshold. counters \nfor both linear traces 1 and 1' are incremented. Therefore, we do not track this edge during the backward \nIf a node has multiple outgoing edges and some of their traversal of the TTgraph. The rationale for this \nis that if an weights are less significant than the others, we can ignore edge has a much smaller weight, \nthen the edge is likely to those edges while traversing the TTgraph. We again used represent a rarely-executed \npath. Therefore, a trace starting 20% as the threshold value. For example in Figure 5(b), from linear \ntrace 3 (or its predecessors) will not cover linear trace 3 has two outgoing edges, while the edge to \nlinear trace 4' and its successors. Hence we do not track timer_interrupt_handler() { // this method \nis called when the timer interrupt occurs set the yield flag of the current thread;  burstCounter = \n0;  }  yield_point_handler() { // this method is called at a yield point if the yield flag is set \nreset the yield flag of the current thread; if (the current yield point is at the head of a trace) { \n // Recompilation Based on TTgraph (see Section 3.4 for detail) if (burstCounter == 0) { // if this \nis the first yield point in this burst // charge the execution time to trace(s) based on the TTgraph \n identify the trace(s) to charge the execution time by the backward traversal from the current trace; \n increment the recompilation counter of the identified trace(s);  invalidate the trace(s) for recompilation \nif the recompilation counter exceeds the threshold; } // Generating TTgraph (see Section 3.3 for detail) \nif (the previous trace can be identified by the link register value) { increment the weight of the TTgraph \nedge between the previous trace and the current trace;if (none of the burst termination conditions is \nsatisfied) { // continue bursty tracing set the yield flag of the current thread; }} burstCounter++; \n} else if (the current yield point is in a compiled trace) { // at a loop backedge if (burstCounter \n== 0) { // if this is the first yield point in this burst increment the recompilation counter of the \ncurrent trace;  invalidate the current trace for recompilation if the recompilation counter exceeds \nthe threshold;  } } } Figure 7. Pseudocode of the TTgraph generation and the TTgraph-based recompilation. \nthis edge to more efficiently capture the hot path in the new trace. When the recompilation counter for \na trace reaches the recompilation threshold, we mark the node as invalidated and remove all of the related \nedges in the TTgraph. The next time the execution reaches the invalidated trace, the runtime will start \nto record that execution starting from the trace head address of the invalidated trace as a new trace \nfor upgrade recompilation, as already described in Section 2. After the new trace is generated, we add \nthe new trace in TTgraph as a node marked as already recompiled. Figure 6 explains how the TTgraph changes \nwhen a trace is invali\u00addated and then the new trace is generated by upgrade recompilation. To summarize \nhow we generate the TTgraph based on the timer-based sampling and use it for trace recompilation, Figure \n7 shows simplified pseudocode for our algorithm as executed at a yield point when the execution stops \ndue to the timer interrupt.   4. Performance Results 4.1 Implementation We implemented our adaptive \nmulti-level compilation scheme with the TTgraph-based trace selection in our trace-JIT [1, 6, 10], which \nis based on the IBM J9/TR Java VM and JIT compiler [3]. Figure 8 shows a component view of our trace-JIT. \nIn our system, traces are formed out of Java bytecode and compiled by our trace-JIT. As already described, \nour trace compiler has two optimization levels, a cold level and a hot level. The cold level is used \nin the initial compilation and focuses on balancing the compila\u00adtion time and the code quality to achieve \nfaster JVM startup. The cold-level compilation executes some basic optimiza\u00adtions including value propagation, \ndead store elimination, and common subexpression elimination. The hot level is used in the upgrade recompilation \nand puts the emphasis on the code quality over the compilation cost. Hence the hot\u00ad Table 1. Summary \nof two optimization levels. optimization level maximum trace length compiler optimizations used in Focus \ncold 16 basic blocks value propagation, dead store elimination, and common subexpression elimination \netc initial compilation fast startup (balanced compilation cost and code quality) hot 256 basic blocks \noptimization used in cold + loop optimizations, escape analysis and global register allocation upgrade \nrecompilation best peak performance (code quality) level compilation performs more costly optimizations, \nsuch as global register allocation, escape analysis and loop optimizations, to achieve higher peak performance. \nWe also use different values for the maximum trace length in the cold-level and hot-level compilations. \nWe use 16 basic blocks for the cold level and 256 basic blocks for the hot level. The maximum trace length \nis one of the most im\u00adportant parameters for the overall performance. We selected the maximum trace length \nfor cold traces (16 BBs here) to minimize the startup time with no upgrade recom\u00adpilation. After fixing \nthe maximum length for cold traces, we select the maximum length for hot trace (256 BBs) to maximize \nthe peak performance. From our experience, using trace lengths larger than 256 BBs did not produce obvious \nperformance gains. The effects of the other parameters, including the threshold for bursty tracing and \ntimer interrupt interval, were not important compared to the maximum trace length. Table 1 summarizes \nthe configura\u00adtions for the two optimization levels. For the recompilation threshold, we used 5 based \non our experiments, so the upgrade recompilation starts when five timer ticks are hit within one trace, \nto balance the peak performance and the compilation time. We ran the evaluation on an IBM BladeCenter \nJS22 using four 4.0-GHz POWER6 cores [11] with 2 SMT threads per core. The system has 16 GB of system \nmemory and runs AIX 6.1. The size of the Java heap was 1 GB using 16-MB pages and we used the generational \ngarbage collector. In our evaluation, we used DaCapo 9.12 [12] running with the default data size in \nour tests. We did not include the tradesoap benchmark because the baseline system with the method-JIT \nsometimes encountered an error with this benchmark. For each result, we report the average of 16 runs \nalong with the 95% confidence interval.  4.2 Performance With and Without Recompilation In this section, \nwe describe the performance of the trace-JIT with our adaptive multi-level compilation, which recompiles \nselected code using the TTgraph. For compari\u00adson, we used configurations with only one optimization level. \nOne configuration compiles all traces at the cold level, which is the baseline in all of the graphs, \nand the another configuration compiles all traces at the hot level,  Figure 8. Overview of our trace-JIT \nsystem architecture. which yields higher peak performance in exchange for slower startup. In addition, \nwe show the performance of the method-JIT. For the method-JIT, we added the JVM option -Xjit:count=1000,bcount=250,milcount=1 \nonly for Jython to avoid pathological behavior of the method-JIT, in which the method-JIT does not compile \nmany methods over a long time period. Figure 9 compares the steady-state performance and the startup \ntime (the execution time of the first iteration). With the adaptive multi-level compilation, the steady-state \nperformance was improved by up to 58.5% and 22.2% on average (geometric mean) for all of the benchmarks \ncompared to the case compiling all of the traces at the cold level. Compiling all of the traces at the \nhot level further improved the steady-state performance. However, as shown in Figure 9(b), using only \nthe hot-level compilation greatly slowed the startup, by up to 2.57x and 1.46x on average. In contrast, \nthe adaptive multi-level compilation offers startup times comparable to the baseline times. These results \nshowed that the adaptive multi-level compilation benefited from both optimization levels, with faster \nstartup times at the cold level and higher peak performance at the hot level. Comparing the performance \nof our trace-JIT using the adaptive multi-level compilation against the method-JIT, the trace-JIT was \n5.3% slower in the steady state and 2.0% slower in the startup than the method-JIT on average. One reason \nfor the performance differences was that our current system uses only linear traces and cyclic traces \nto avoid join points in the control flow in middle of a (a) steady-state performance 1.8  relative \nsteady-state performance over single-level compilation (cold level). 1.6 1.4 1.2 1.0 0.8 0.6 0.4 0.2 \n0.0 higher isfaster  Figure 9. Steady-state performance and startup time with and without adaptive \nmulti-level compilation. (a) total compilation time (b) JIT-compiled code size trace. Hence, even with \nvery large maximum trace length, loop counter, and hence lots of optimization opportunities the longest \ntrace will be limited. Allowing more complicat-are missed. Another reason for the slower startup was \ned control flow may create more optimization opportunities lower performance of the interpreter in the \ntrace-JIT due to in exchange for additional compilation work. Allowing the trace selection overhead. \ncomplicated control flow in a trace will not only enlarge Figure 10 shows the total compilation time \nand the total the compilation scope but also help loop optimizations. size of the JIT-compiled code. \nThese graphs are quite This is because a cyclic trace does not include code similar to those of the startup \ntimes except for the method\u00adsequence just before the loop, such as initialization of the JIT because \nthe startup time is included most of the compi\u00ad (a) breakdown of the number of compiled traces into \nthe optimization levels (cold and hot) 1. 4 1.2 1 0.8 0.6 0.4 0.2 0 number of compiled traces  Figure \n11. Numbers of compiled traces and total compilation times for each optimization level. lation times \nand longer compilation times often lead to For further insight into the characteristics of the adap\u00adslower \nstarts. The adaptive multi-level compilation con-tive multi-level compilation, Figure 11 is a breakdown \nof sumed about 71.1% more compilation time on average than the number of compiled traces and total compilation \ntime the single-level compilation at the cold level due to the for the two optimization levels. There \nare three bars for additional compilation time consumed for the upgrade each benchmark. The left bar \nrepresents the single-level recompilations. However, the compilation time of the compilation at the cold \nlevel, the center bar represents adaptive multi-level compilation was much shorter than the single-level \ncompilation at the hot level, and the right bar compilation time when compiling all of the traces at \nthe hot represents our adaptive multi-level compilation. level. Similarly, the JIT-compiled code size \nwas increased As shown in Figure 11(a), the number of traces recom\u00adby using the upgrade recompilation \nbut was still much piled at the hot level was not significant compared to the smaller than when using \nonly the hot level. The compila-total number of traces compiled. The number of upgrade tion time and \ncode size of the method-JIT were compilations was 6.0% of the all compilations on average significantly \nlarger because the method-JIT has more (though up to 15.6%). Such relatively small numbers of optimization \nlevels for recompilation and hence one hot upgrade compilations gave significant performance im\u00admethod \nwas compiled repeatedly. Another reason for the provements in the steady state, as shown in Figure 9. \nshorter compilation times of the trace-JIT was that optimi-Although the number of traces compiled at \nthe hot level zations in the trace-JIT were less costly than the same was only 6.0% on average, the compilation \ntime used for optimizations in the method-JIT due to the simpler control upgrade compilations was 34.8% \non average and up to flow in the trace compilation scope. 68.8% of the total compilation time. This mens \nthat one OS native libraries JIT-compiled code hot cold left bar: center bar: right bar: single-level \ncompilation (cold level) single-level compilation (hot level) adaptive multi-level compilation (TTgraph-based \nrecompilation) 0.9 1. 0 0.8me er 0.7 fast 0.6 normalized CPU ti is 0.3 0.4 0.5 shorter 0.2 0.1 0. 0 \n Figure 12. CPU time breakdown into JIT compiled code for the hot and cold levels, native library, and \nOS. upgrade compilation at the hot level used about five times more compilation time than one cold-level \ncompilation due to the more aggressive optimizations and larger compila\u00adtion scope. Despite the visible \nincrease in the total compilation time due to the upgrade compilations, the startup time with multi-level \ncompilation was not increased much, as shown in Figure 9(b). The upgrade compilations were invoked by \nthe timer-based profiler and hence the compilation time of the upgrade compilation was typically excluded \nfrom the startup time. Unlike our multi-level compilation, the startup time of the single-level compila\u00adtion \nusing the hot level was badly affected by the increase in the compilation time because costly hot-level \ncompila\u00adtions frequently occurred during the startup phase of the JVM. Figure 12 shows profiles of the \nCPU time broken down into JIT compiled code at the cold level, JIT compiled code at the hot level, native \nlibrary in the JVM, and the OS, based on how much active CPU time was spent in each component as measured \nby using the hardware perfor\u00admance monitor of the processor. Only in this figure, we averaged the results \nof four measurements. On average, 85.9% of the CPU time for JIT compiled code is spent in hot-level compiled \ncode, while only 14.1% of the time is spent in cold-level compiled code. These results show that our \nmulti-level compilation technique successfully cap\u00adtured really hot code paths and recompiled them by \nusing the higher optimization level.  4.3 Comparison of the basic recompilation and the TTgraph-based \nrecompilation This section focuses on how our recompilation technique using the TTgraph described in \nSection 3 improved the overall performance compared to the basic recompilation. In this section, we use \nthe multi-level compilation with the basic recompilation as the baseline for all of the figures. In addition, \nthe TTgraph-based recompilation tends to use more compilation time than the basic recompilation. For \nthe fair comparison, we tested the basic recompilation with a lower recompilation threshold to use roughly \nthe same amount of the total compilation time on average (labeled basic recompilation with aggressive \nthreshold in figures). Figure 13 shows the speedup in the steady-state perfor\u00admance over the basic recompilation. \nA positive value means better performance than the basic recompilation. The TTgraph-based recompilation \nimproved the perfor\u00admance by up to 6.5% and 1.8% on average. Even with the same basic recompilation, \nusing the aggressive threshold gave the performance improvement of 0.5% on average. However the improvement \nwas smaller than the TTgraph\u00adbased recompilation in all benchmarks. Figure 14 depicts the improvements \nin the startup per\u00adformance. The performance of the TTgraph-based recompilation spanned from 8.0% slower \nto 3.5% faster than the basic recompilation. The average for all of the benchmarks for startup performance \nwas not much differ\u00adent from that of the basic recompilation. It was only 0.4% faster. The startup times \nfor the basic recompilation with the aggressive threshold were also quite similar to the baseline. Our \nresults showed that with the given total compilation time, the TTgraph-based recompilation technique \nachieved better steady-state performance than the basic recompila\u00adtion with almost similar startup performance. \n basic recompilation basic recompilation with aggressive threshold TTgraph-based recompilation performance \nimprovement over basic recompilation . 8.0% 7.0% 6.0% 5.0% 4.0% 3.0% 2.0% 1.0% 0.0% -1.0% -2.0% -3.0% \n faster than basic recompilation    Figure 13. Changes in steady-state performance with multi-level \ncompilation using various recompilation techniques.  5. Related Work Upgrade compilation is a standard \ntechnique that is sup\u00ad ported in many modern dynamic compilers for Java and other languages, such as \nSun Hotspot JVM [2], IBM JVM [3, 4], Jikes RVM [5], and SELF [13]. In all of these systems, upgrade compilation \nis triggered by hotness detection, after which recompilation selects an optimization strategy. In our \nwork, the hotness and optimization strategy selection used by the trace-JIT are similar to those used \nin the method-JIT. But upgrade compilation in the trace-JIT has another degree of freedom in deciding \nhow to re-record a trace for recompilation, such as the selection of trace 6.0% 4.0% 2.0% 0.0% -2.0% \n-4.0% -6.0% -8.0% -10.0% performance improvement over basic recompilation . head during trace re-recording. \nThe trace-based compilation was first introduced by binary translators and optimizers [7], where method \nstructures are not available. Recently, trace-based compila- Figure 14. Changes in startup performance \nwith multi-level compilation using various recompilation techniques. paths by using timer-tick-based \nsampling. Hence, applying tion has gained popularity in dynamic compilers because it can potentially \nprovide more opportunities for specializa\u00adtion [14, 15] with smaller resource requirements [16, 17] compared \nto the method-based compilation. costly optimizations for recompilation can be justified. Some existing \ntrace-JIT support recompilation. In [18-Most of the trace-based systems, including our trace-JIT, 20], \na trace that is aborted from the compiled code into the have an interpreter to monitor the program execution. \nIn interpreted execution too frequently may be recompiled to contrast, SPUR [21] and Maxpath [22] do \nnot have inter\u00ad include a trace starting from the side exit point in the preters and use instrumented \nnon-optimized compiled code compilation scope. Such recompilation is used to limit the generated by \na method-JIT to monitor the execution. After penalty of a badly formed trace by extending the coverage \nthe hot paths are detected by the instrumented compiled by the compiled code, not for upgrading compilation \nfor code, the detected hot paths are recompiled as a trace by an really hot traces. Because the trace \nrecompilation is optimizing trace-JIT. Therefore, the goals of recompilation invoked by taken side exit \nin these existing systems, the are different from ours. Also, we do not need to insert recompiled traces \nare not necessarily hotter than other instrumented code in the compiled code to monitor execu\u00ad traces \nand hence using a higher optimization level for them tion. Instead, we use lightweight timer-based sampling \nto cannot be justified. In contrast, the hotness-based upgrade detect the hot paths for recompilation. \nrecompilation described in this paper identifies really hot  6. Summary In this paper, we described \nour adaptive multi-level compilation technique for a trace-based JIT compiler. As in the existing multi-level \ncompilation for a method-JIT, we start JIT compilation with a small compilation scope and a low optimization \nlevel to achieve fast startup. Then we identify hot paths with a timer-based sampling profiler, generate \nlong traces that capture the hot paths, and recom\u00adpile them with a higher optimization level to yield \nbetter peak performance. Our multi-level compilation technique successfully identified and recompiled \na few traces at a higher optimization level. Our multi-level compilation accelerated the code by up to \n58.5% in the steady state and 22.2% on average without degrading the startup perfor\u00admance compared to \ncompiling all of the traces at the cold level. We also showed that our technique to build and exploit \nthe TTgraph for selecting the compilation scope improved the performance over the basic recompilation \ntechnique. Our results showed that the adaptive multi-level compi\u00adlation is a practical technique in \ntrace-based systems to balance the steady-state performance and startup time by taking advantage of different \noptimization levels.  References [1] P. Wu, H. Hayashizaki, H. Inoue, and T. Nakatani, \"Reducing Trace \nSelection Footprint for Large-scale Java Applications with no Performance Loss\", in Proceedings of the \nACM Object-Oriented Programming, Systems, Languages &#38; Applications, pp. 789 804, 2011. [2] M. Paleczny, \nC. Vick, and C. Click, The Java Hotspot(tm) Server Compiler , in Proceedings of the USENIX Java Virtual \nMachine Research and Technology Symposium, pp. 1 12, 2001. [3] N. Grcevski, A. Kielstra, K. Stoodley, \nM. Stoodley, and V. Sundaresan. Java just-in-time compiler and virtual machine improvements for server \nand middleware applications . In Proceedings of the USENIX Virtual Machine Research and Technology Symposium, \npp. 151 162, 2004. [4] T. Suganuma, T. Yasue, M. Kawahito, H. Komatsu, and T. Nakatani, A dynamic optimization \nframework for a Java just-in-time compiler , in Proceedings of the ACM Conference on Object-Oriented \nProgramming Systems, Languages, and Applications, pp. 180 195, 2001. [5] M. Arnold, S. Fink, D. Grove, \nM. Hind, and P. F. Sweeney. Adaptive optimization in the Jalape\u00f1o JVM , in Proceedings of the ACM SIGPLAN \nconference on Object\u00adoriented programming, systems, languages, and applications. pp. 47 65. 2000. [6] \nH. Inoue, H. Hayashizaki, P. Wu, and T. Nakatani, \"A Trace\u00adbased Java JIT Compiler Retrofitted from a \nMethod-based Compiler\", in Proceedings of the International Symposium on Code Generation and Optimization, \npp. 246 256, 2011. [7] V. Bala, E. Duesterwald, and S. Banerjia, Dynamo: A Transparent Runtime Optimization \nSystem, in Proceedings of the ACM Programming Language Design and Implementation, pp. 1 12, 2000. [8] \nT. Mytkowicz, A. Diwan, M. Hauswirth, and P. F. Sweeney. Evaluating the accuracy of Java profilers , \nin Proceedings of the ACM SIGPLAN conference on Programming language design and implementation. pp. 1879 \n197, 2010. [9] M. Hirzel, and T. M. Chilimbi, Bursty tracing: a framework for low-overhead temporal profiling \n, in Proceedings of the 4th Workshop on Feedback-Directed and Dynamic Optimization, pp. 117 126, 2001. \n[10] H. Hayashizaki, P. Wu, H. Inoue, M. Serrano, and T. Nakatani, \"Improving the Performance of Trace-based \nSystems by False Loop Filtering\", In Proceedings of Sixteenth International Conference on Architectural \nSupport for Programming Languages and Operating Systems, pp. 405 418, 2011. [11] H. Q. Le, W. J. Starke, \nJ. S. Fields, F. P. O Connell, D. Q. Nguyen, B. J. Ronchetti, W. M. Sauer, E. M. Schwarz, and M. T. Vaden. \nIBM POWER6 microarchitecture . IBM Journal of Research and Development, Vol. 51 (6), pp. 639 662, 2007. \n [12] S. M. Blackburn et al., The DaCapo Benchmarks: Java Benchmarking Development and Analysis , in \nProceedings of the ACM conference on Object-Oriented Programming, Systems, Languages, and Applications, \npp. 169 190, 2006. [13] U. Holzle and D. Ungar, A third generation self implementation: Reconciling responsiveness \nwith performance , in Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages, \nand Applications, pp. 229 243, 1994. [14] C. Bolz, A. Cuni, M. Fijalkowski, and A. Rigo, Tracing the \nMeta-Level: PyPy s Tracing JIT Compiler , in Proceedings of the 4th workshop on the Implementation, Compilation, \nOptimization of Object-Oriented Languages and Programming Systems, pp. 18 25, 2009. [15] A. Gal, B. Eich, \nM. Shaver, D. Anderson, D. Mandelin, M. Haghighat, B. Kaplan, G. Hoare, B. Zbarsky, J. Orendorff, J. \nRuderman, E. W. Smith, R. Reitmaier, M. Bebenita, M. Chang, and M. Franz., Trace-based Just-In-Time Type \nSpecialization for Dynamic Languages , in Proceedings of the ACM SIGPLAN conference on Programming language \ndesign and implementation, pp. 465 478, 2009. [16] A. Gal, C. Probst, and M. Franz, HotPathVM: An Effective \nJIT Compiler for Resource-constrained Devices , in Proceedings of the International Conference on Virtual \nExecution Environments, pp. 144 153, 2006. [17] B. Cheng and B. Buzbee, A JIT Compiler for Android's \nDalvik VM , Google I/O developer conference, 2010. http://www.google.com/events/io/2010/sessions/jit-compiler\u00adandroids-dalvik-vm.html \n[18] C. H\u00e4ubl and H. M\u00f6ssenb\u00f6ck, Trace-based Compilation for the Java HotSpot Virtual Machine , in Proceedings \nof the International Conference on the Principles and Practice of Programming in Java, pp. 129 138, \n2011. [19] C. H\u00e4ubl, C. Wimmer, and H. M\u00f6ssenb\u00f6ck, Evaluation of trace inlining heuristics for Java , \nin Proceedings of the Annual ACM Symposium on Applied Computing, pp. 1971 1876, 2011. [20] A. Gal, Efficient \nbytecode verification and compilation in a virtual machine , PhD thesis, University of California, Irvine, \n2006. [21] M. Bebenita, F. Brandner, M. Fahndrich, F. Logozzo, W. Schulte, N. Tillmann, and H. Venter, \nSPUR: A trace-based JIT compiler for CIL , in Proceedings of the ACM international conference on Object \noriented programming systems languages and applications, pp. 708 725, 2010. [22] M. Bebenita, M. Chang, \nG. Wagner, A. Gal, C. Wimmer, and M. Franz, Trace-based compilation in execution environments without \ninterpreters , in Proceedings of the 8th International Conference on the Principles and Practice of Programming \nin Java, pp. 59 68, 2010.  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>This paper describes our multi-level compilation techniques implemented in a trace-based Java JIT compiler (trace-JIT). Like existing multi-level compilation for method-based compilers, we start JIT compilation with a small compilation scope and a low optimization level so the program can start running quickly. Then we identify hot paths with a timer-based sampling profiler, generate long traces that capture the hot paths, and recompile them with a high optimization level to improve the peak performance. A key to high performance is selecting long traces that effectively capture the entire hot paths for upgrade recompilations. To do this, we introduce a new technique to generate a directed graph representing the control flow, a TTgraph, and use the TTgraph in the trace selection engine to efficiently select long traces. We show that our multi-level compilation improves the peak performance of programs by up to 58.5% and 22.2% on average compared to compiling all of the traces only at a low optimization level. Comparing the performance with our multi-level compilation to the performance when compiling all of the traces at a high optimization level, our technique can reduce the startup times of programs by up to 61.1% and 31.3% on average without significant reduction in the peak performance. Our results show that our adaptive multi-level compilation can balance the peak performance and startup time by taking advantage of different optimization levels.</p>", "authors": [{"name": "Hiroshi Inoue", "author_profile_id": "81100619710", "affiliation": "IBM Research - Tokyo, Tokyo, Japan", "person_id": "P3856054", "email_address": "inouehrs@jp.ibm.com", "orcid_id": ""}, {"name": "Hiroshige Hayashizaki", "author_profile_id": "81381610007", "affiliation": "IBM Research - Tokyo, Tokyo, Japan", "person_id": "P3856055", "email_address": "hayashiz@jp.ibm.com", "orcid_id": ""}, {"name": "Peng Wu", "author_profile_id": "81549581256", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P3856056", "email_address": "pengwu@us.ibm.com", "orcid_id": ""}, {"name": "Toshio Nakatani", "author_profile_id": "81100311827", "affiliation": "IBM Research - Tokyo, Tokyo, Japan", "person_id": "P3856057", "email_address": "nakatani@jp.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384630", "year": "2012", "article_id": "2384630", "conference": "OOPSLA", "title": "Adaptive multi-level compilation in a trace-based Java JIT compiler", "url": "http://dl.acm.org/citation.cfm?id=2384630"}