{"article_publication_date": "10-19-2012", "fulltext": "\n GPUVerify: A Veri.er for GPU Kernels * Adam Betts1 Nathan Chong1 Alastair F. Donaldson1 Shaz Qadeer2 \nPaul Thomson1 1Department of Computing, Imperial College London, UK 2Microsoft Research, Redmond, USA \n{abetts,nyc04,afd,pt1110}@imperial.ac.uk Abstract We present a technique for verifying race-and divergence\u00adfreedom \nof GPU kernels that are written in mainstream ker\u00adnel programming languages such as OpenCL and CUDA. \nOur approach is founded on a novel formal operational se\u00admantics for GPU programming termed synchronous, \ndelayed visibility (SDV) semantics. The SDV semantics provides a precise de.nition of barrier divergence \nin GPU kernels and allows kernel veri.cation to be reduced to analysis of a sequential program, thereby \ncompletely avoiding the need to reason about thread interleavings, and allowing existing modular techniques \nfor program veri.cation to be leveraged. We describe an ef.cient encoding for data race detection and \npropose a method for automatically inferring loop invari\u00adants required for veri.cation. We have implemented \nthese techniques as a practical veri.cation tool, GPUVerify, which can be applied directly to OpenCL \nand CUDA source code. We evaluate GPUVerify with respect to a set of 163 kernels drawn from public and \ncommercial sources. Our evaluation demonstrates that GPUVerify is capable of ef.cient, auto\u00admatic veri.cation \nof a large number of real-world kernels. Categories and Subject Descriptors F3.1 [Logics and Meanings \nof Programs]: Specifying, Verifying &#38; Reason\u00ading about Programs Keywords Veri.cation, GPUs, concurrency, \ndata races, barrier synchronization 1. Introduction In recent years, massively parallel accelerator processors, \nprimarily graphics processing units (GPUs) from companies * This work was supported by the EU FP7 STEP \nproject CARP (project number 287767), by EPSRC project EP/G051100/2, and by two EPSRC\u00adfunded PhD studentships. \nPart of the work was carried out while Alastair Donaldson was a Visiting Researcher at Microsoft Research \nRedmond. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n12, October 19 26, 2012, Tucson, Arizona, USA. Copyright c &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. \n. . $10.00 qadeer@microsoft.com such as AMD and NVIDIA, have become widely available to end-users. Accelerators \noffer tremendous compute power at a low cost, and tasks such as media processing, medical imaging and \neye-tracking can be accelerated to beat CPU performance by orders of magnitude. GPUs present a serious \nchallenge for software develop\u00aders. A system may contain one or more of the plethora of devices on the \nmarket, with many more products anticipated in the immediate future. Applications must exhibit portable \ncorrectness, operating correctly on any GPU accelerator. Software bugs in media processing domains can \nhave serious .nancial implications, and GPUs are being used increasingly in domains such as medical image \nprocessing [37] where safety is critical. Thus there is an urgent need for veri.ca\u00adtion techniques to \naid construction of correct GPU software. This paper addresses the problem of static veri.cation of GPU \nkernels written in kernel programming languages such as OpenCL [17], CUDA [30] and C++ AMP [28]. We focus \non two classes of bugs which make writing correct GPU kernels harder than writing correct sequential \ncode: data races and barrier divergence. In contrast to the well-understood notion of data races, there \ndoes not appear to be a formal de.nition of barrier di\u00advergence for GPU programming. Our work begins \nby giving a precise characterization of barrier divergence via an oper\u00adational semantics based on predicated \nexecution, which we call synchronous, delayed visibility (SDV) semantics. While predicated execution \nhas been used for code generation by GPU kernel compilers, our work is the .rst to use predicated operational \nsemantics for the purpose of speci.cation and veri.cation. Founded on the SDV semantics, we design a \nveri.cation method which reduces analysis of concurrent GPU threads to reasoning over a transformed sequential \nprogram. This completely avoids reasoning about thread interleavings, and enables reusing existing modular \nveri.cation techniques for sequential programs. We present novel heuristics for auto\u00admatically inferring \nloop invariants required for veri.cation. We have developed GPUVerify, a veri.er for GPU ker\u00adnels that \ncan be applied directly to OpenCL and CUDA source code. We have used GPUVerify to analyse a set of 163 \nOpenCL and CUDA kernels. We divided this set into train\u00ading kernels and evaluation kernels, such that \nnone of our team had any experience with kernels in the evaluation set. We used the training benchmarks \nto design and tune GPU\u00adVerify s invariant inference and performance. We then ran GPUVerify blindly on \nthe evaluation set, .nding that fully automatic veri.cation was achieved for 49 out of 71 ker\u00adnels (69 \n%). We also compare GPUVerify experimentally with PUG, a a recent formal analysis tool for CUDA ker\u00adnels \n[21]. GPUVerify performs competitively with PUG for veri.cation of correct kernels and rejects buggy \nkernels in several cases where PUG reports false negatives. Addition\u00adally, GPUVerify supports a .ner \nshared state abstraction than PUG, allowing veri.cation of real-world kernels for which PUG reports false \npositives.  GPUVerify, and all the non-commercial benchmarks used for our evaluation, are available \nonline.1 In summary, our paper makes the following contributions: Synchronous, delayed visibility semantics: \na formal op\u00aderational semantics for GPU kernels based on predicated execution, data-race freedom, and \ndivergence freedom  A veri.cation method for GPU kernels based on auto\u00admatic abstraction followed by \ngeneration of veri.cation conditions to be solved via automated theorem proving  A method for inferring \nautomatically the invariants needed for our veri.cation method  An extensive evaluation of our veri.er \non a collection of 163 publicly available and commercial GPU kernels; to our knowledge, this experimental \nevaluation is signif\u00adicantly larger, in terms of number of kernels, than any previously reported evaluation \nof a tool for GPU kernel analysis.  We begin by giving an overview of GPU kernel program\u00adming and the \nmost pressing dif.culties faced by kernel pro\u00adgrammers. 2. GPU kernel programming A typical GPU (see \nFigure 1) consists of a large number of simple processing elements (PEs), sometimes referred to as cores. \nSubsets of the PEs are grouped together into multi\u00adprocessors, such that all PEs within a multiprocessor \nexe\u00adcute in lock-step, in single instruction multiple data (SIMD) fashion. Distinct multiprocessors on \na GPU can execute in\u00addependently. Each PE is equipped with a small private mem\u00adory, and PEs located on \nthe same multiprocessor can access a portion of shared memory dedicated to that multiproces\u00adsor. All \nPEs on the GPU have access to a large amount of off-chip memory known as global memory, which is usually \nseparate from main CPU memory. Today, there are three major GPU programming mod\u00adels: OpenCL, an industry \nstandard proposed by the Khronos Group and widely supported (in particular, OpenCL is AMD s primary high-level \nGPU programming model) [17]; 1 http://multicore.doc.ic.ac.uk/tools/GPUVerify/ Figure 1: Schematic overview \nof a typical GPU architecture Term CUDA OpenCL C++ AMP thread thread work-item thread group thread block \nwork-group tile sub-group warp N/A N/A Figure 2: Equivalent terms for thread, group and (where applicable) \nsub-group in CUDA, OpenCL and C++ AMP CUDA, from NVIDIA [30]; and C++ AMP, from Mi\u00ad crosoft [28]. Threads \nand groups. All three programming models pro\u00advide a similar high-level abstraction for mapping computa\u00adtion \nacross GPU hardware, centered around the notion of a kernel program being executed by many parallel threads, \ntogether with a speci.cation of how these threads should be partitioned into groups. The kernel is a \ntemplate speci\u00adfying the behavior of an arbitrary thread, parameterized by thread and group id variables. \nExpressions over these ids al\u00adlow distinct threads to operate on separate data and follow differing execution \npaths through the kernel. Threads in the same group can synchronize during kernel execution, while threads \nin distinct groups execute completely independently. The runtime environment associated with a GPU pro\u00adgramming \nmodel must interface with the driver of the avail\u00adable GPU to schedule execution of kernel threads across \nprocessing elements. Typically each group of threads is as\u00adsigned to one of the GPU s multiprocessors, \nso that distinct groups execute in parallel on different multiprocessors. If the number of threads in \na group is N and the number of PEs in a multiprocessor is M, then a group is divided into I N l sub- \nM groups, each consisting of up to M threads. Execution of a single group on a multiprocessor then proceeds \nby round\u00adrobin execution of the sub-groups. Each thread in a given sub-group is pinned to a distinct \nPE, and all threads in the same sub-group execute together in lock-step, following ex\u00adactly the same \ncontrol path. Distinct sub-groups may follow different control paths. Figure 2 summarizes the speci.c \nterms used by the three main GPU programming models to refer to threads, groups and (in the case of CUDA) \nsub-groups. OpenCL and C++ AMP aim for portability across GPUs from mul\u00adtiple vendors, so do not allow \na kernel to query the device\u00ad  Program fragment Predicated form if(lid > N) p = (lid > N); x = 0; p \n=> x = 0; else !p => x = 1; x = 1; p = (i < x); while(i < x) { while(. t :: t.p) { i++; p => i++; } p \n=> p = (i < x); } Figure 3: Predicated forms for conditionals and loops speci.c size or structure of \nthread sub-groups. As CUDA is NVIDIA-speci.c, CUDA programmers can write ker\u00adnels that make assumptions \nabout the division of threads into sub-groups. However, such kernels will not easily port to general-purpose \nGPU programming languages, and may break when executed on future generations of NVIDIA hard\u00adware that \nuses a different sub-group size. Thus GPU kernels that do not make assumptions about the size of thread \nsub\u00adgroups are preferable. Predicated execution. Recall that the PEs in a GPU mul\u00adtiprocessor execute \nin lock-step, as a SIMD processor array. Threads within a sub-group occupy a multiprocessor s PEs, and \nthus must also execute in lock-step. Conditional state\u00adments and loops through which distinct threads \nin the same sub-group should take different paths must therefore be sim\u00adulated, and this is achieved \nusing predicated execution. Consider the conditional statement in the top-left of Fig\u00adure 3, where lid \ndenotes the local id of a thread within its group and x is a local variable stored in private memory. \nThis conditional can be transformed into the straight-line code shown in the top-right of the .gure, \nwhich can be executed by a sub-group in lock-step. The meaning of a statement predicate=>command is that \na thread should execute com\u00admand if predicate holds for that thread, otherwise the thread should execute \na no-op. All threads evaluate the condition lid > N into a local boolean variable p, then execute both \nthe then and else branches of the conditional, predicated by p and !p respectively. Loops are turned \ninto predicated form by dictating that all threads in a sub-group continue to execute the loop body until \nthe loop condition is false for all threads in the sub\u00adgroup, with threads for whom the condition does \nnot hold becoming disabled. This is illustrated for the loop in the bottom-left of Figure 3 (where i \nand x are local variables) by the code fragment shown in the bottom-right of the .gure. First, the condition \ni<x is evaluated into local variable p. Then the sub-group loops while p remains true for some thread \nin the sub-group, indicated by .t :: t.p. The loop body is predicated by p, and thus has an effect only \nfor enabled threads. We present precise operational semantics for predicated execution in Section 3. \nBarrier synchronization. When a thread t1 writes to an ad\u00address in shared or global memory, the result \nof this write is not guaranteed to become visible to another thread t2 unless t1 and t2 synchronize. \nAs noted above, there is no mech\u00adanism for threads in distinct groups to synchronize during kernel execution.2 \nThreads in the same group can synchro\u00adnize via barriers. Intuitively, a kernel thread belonging to group \ng waits at a barrier statement until every thread in g has reached the barrier. Passing the barrier guarantees \nthat all writes to shared and global memory by threads in g oc\u00adcurring before execution of the barrier \nhave been committed. Our analysis through writing GPU kernels and talking to GPU developers is that there \nare two speci.c classes of bugs that make writing correct GPU kernels more dif.cult than writing correct \nsequential code: data races and barrier divergence. 2.1 Data races We distinguish between two kinds \nof data races in GPU kernels. An inter-group data race occurs if there are two threads t1 and t2 from \ndifferent groups such that t1 writes to a location in global memory and t2 writes to or reads from this \nlocation. An intra-group data race occurs if there are two threads t1 and t2 from the same group such \nthat t1 writes to a location in global or shared memory, t2 writes to or reads from this location, and \nno barrier statement is executed between these accesses. Races can lead to nondeterministic kernel behavior, \nand computation of incorrect results. In this work, we restrict our attention to intra-group data races \nbecause we consider them to be the more signi.cant problem for GPU programmers. Threads across groups \ncan\u00adnot synchronize and consequently the argument for absence of inter-group data races is usually based \non globally disjoint memory access patterns. Threads within a group can syn\u00adchronize in sophisticated \nways using the barrier operation; consequently, the correctness argument is more complicated and errors \nmore likely.  2.2 Barrier divergence If threads in the same group diverge, reaching different bar\u00adriers \nas in the following kernel fragment: if((lid % 2) == 0) barrier(); // Even threads hit .rst barrier else \nbarrier(); // Odd threads hit second barrier then kernel behavior is unde.ned. According to CUDA [30]: \nexecution is likely to hang or produce unintended side effects . While there is clarity across all programming \nmodels for what barrier divergence means in loop-free code, the situation is far from clear for code \nwith loops. Consider the example kernel shown on the left of Figure 4. This kernel is intended to be \nexecuted by a group of four threads, and 2 Atomic operations on global memory are available in some GPU \narchi\u00adtectures, but cannot reliably implement inter-group synchronization due to lack of progress guarantees \nbetween groups.  shared int A[2][4]; ... void kernel() { p = (i < x); int buf, x, y, i, j; while(. \nt :: t.p) { x = (lid == 0 ? 4 : 1); p => j = 0; y = (lid == 0 ? 1 : 4); q = p &#38;&#38; (j < y); buf \n= i = 0; while(. t :: t.q) { while(i < x) { q => barrier(); j = 0; q => A[1-buf][lid] = while(j < y) \n{ A[buf][(lid+1)%4];  barrier(); q => buf = 1 -buf; A[1-buf][lid] = q => j++; A[buf][(lid+1)%4]; q => \nq = p &#38;&#38; (j < y); buf = 1 -buf; } j++; p => i++; } p => p = (i < x); i++; } }} Figure 4: Illustration \nof the subtleties of barriers in nested loops declares an array A of two shared buffers, each of size \nfour. Local variable buf is an index into A, representing the current buffer. The threads execute a nest \nof loops. On each inner loop iteration a thread reads the value of the current buffer at index lid+1 \nmodulo 4 and writes the result into the non\u00adcurrent buffer at index lid. A barrier is used to avoid data \nraces on A. Notice that local variables x and y are set to 4 and 1 respectively for thread 0, and to \n1 and 4 respectively for all other threads. As a result, we expect thread 0 to perform four outer loop \niterations, each involving one inner loop iteration, while other threads will perform a single outer \nloop iteration, consisting of four inner loop iterations. According to the guidance in the CUDA documentation \nsuch a kernel appears to be valid: all threads will hit the barrier statement four times. Taking a snapshot \nof the array A at each barrier and at the end of the kernel, we might expect to see the following: A \n= {{0, 1, 2, 3}, {-, -, -, -}} . {{0, 1, 2, 3}, {1, 2, 3, 0}} . {{2, 3, 0, 1}, {1, 2, 3, 0}} . {{2, 3, \n0, 1}, {3, 0, 1, 2}} . {{0, 1, 2, 3}, {3, 0, 1, 2}} However, consider the predicated version of the \nkernel shown in part on the right of Figure 4. This is the form in which the kernel executes on an NVIDIA \nGPU. The four threads comprise a single sub-group. All threads will enter the outer loop and execute \nthe .rst inner loop iteration. Then thread 0 will become disabled (q becomes false) for the in\u00adner loop. \nThus the barrier will be executed with some, but not all, threads in the sub-group enabled. On NVIDIA \nhardware, a barrier is compiled to a bar.sync instruction in the PTX (Parallel Thread Execution) assembly \nlanguage. According to the PTX documentation [31], if any thread in a [sub\u00adgroup] executes a bar instruction, \nit is as if all the threads in the [sub-group] have executed the bar instruction . Thus threads 1, 2 \nand 3 will not wait at the barrier until thread 0 re\u00adturns to the inner loop: they will simply continue \nto execute past the barrier, performing three more inner loop iterations. This yields the following sequence \nof state-changes to A: A = {{0, 1, 2, 3}, {-, -, -, -}} . {{0, 1, 2, 3}, {1, 2, 3, 0}} Architecture Final \nstate of A NVIDIA Tesla C2050 {{0, 1, 0, 1}, {1, 0, 1, 0}} AMD Tahiti {{0, 1, 2, 3}, {1, 2, 3, 0}} ARM \nMali-T600 {{0, 1, 2, 3}, {3, 0, 1, 2}} Intel Xeon X5650 {{*, *, *, 1}, {3, 0, 1, 2}} Figure 5: The litmus \ntest of Figure 4 yields a range of results across varying platforms . {{0, 3, 0, 1}, {1, 2, 3, 0}} . \n{{0, 3, 0, 1}, {1, 0, 1, 0}} . {{0, 1, 0, 1}, {1, 0, 1, 0}} After the inner loop exits, thread 0 becomes \nenabled, but all other threads become disabled, for a further three outer loop iterations, during each \nof which thread 0 executes a single inner loop iteration. The state of A thus remains {{0, 1, 0, 1}, \n{1, 0, 1, 0}}. The OpenCL standard [17] gives a better, though still informal de.nition, stating: If \na barrier is inside a loop, all [threads] must execute the barrier for each iteration of the loop before \nany are allowed to continue execution beyond the barrier , which at least can be interpreted as rejecting \nthe example of Figure 4. To investigate this issue in practice, we implemented the litmus test of Figure \n4 in both CUDA and OpenCL and (with help from contacts in the GPU industry; see Acknowledge\u00adments) ran \nthe test on GPU architectures from NVIDIA, AMD and ARM, and on an Intel Xeon CPU (for which there is \nan OpenCL implementation). Our .ndings are reported in Figure 5. Observe that the test result does not \nagree between any two vendors. The NVIDIA results match our above pre\u00addiction. The AMD result also appears \nto stem from pred\u00adicated execution. ARM s Mali architecture does not work using predicated execution \n[25], so perhaps unsurprisingly gives the intuitive result we might expect. For Intel Xeon, we found \nthat different threads reported different values for certain array elements in the .nal shared state, \nindicated by asterisks in Figure 5, which we attribute to cache effects. The example of Figure 4 is contrived \nin order to be small enough to explain concisely and examine exhaustively. It does, however, illustrate \nthat barrier divergence is a subtle issue, and that non-obvious misuse of barriers can compro\u00admise correctness \nand lead to implementation-dependent re\u00adsults. Clearly a more rigorous notion of barrier divergence is \nrequired than the informal descriptions found in the CUDA and OpenCL documentation. We give a precise, \noperational de.nition for barrier di\u00advergence in Section 3 which clears up this ambiguity. In essence, \nour de.nition states that if a barrier is encountered by a group of threads executing in lock-step under \na pred\u00adicate, the predicate must hold uniformly across the group, i.e., the predicate must be true for \nall threads, or false for all threads. This precise de.nition facilitates formal veri.cation of divergence-freedom. \n 3. Operational semantics for GPU kernels Our aim is to verify race-and divergence-freedom for GPU kernels. \nIn order to do this, we require an operational seman\u00adtics for GPU kernels that speci.es precisely the \nconditions under which races and divergence occur. For checking barrier divergence, the least conservative \nas\u00adsumption we can safely make is that a thread group consists of a single sub-group, i.e., all threads \nin a group execute in lock-step. This will indeed be the case, e.g., for a group of 32 threads executing \non an NVIDIA GPU. We can then de\u00ad.ne barrier divergence to occur if the thread group executes a barrier \nand the threads are not uniformly enabled: the current predicate of execution holds for some threads \nbut not others. Clearly if we can prove divergence-freedom for a kernel un\u00adder this tight assumption, \nthe kernel will also be divergence\u00adfree if thread groups are actually divided into sub-groups with a \n.ner level of granularity. For race checking, the scenario is reversed: the least con\u00adservative safe \nassumption is that threads in the same group interleave completely asynchronously between pairs of bar\u00adriers, \nwith no guarantees as to the relative order of statement execution between threads. This is the case \non ARM s Mali GPU architecture [25] (so that essentially every sub-group consists of just a single thread). \nClearly if race-freedom can be proved under this most general condition, then a ker\u00adnel will remain race-free \nif, in practice, certain threads in a group execute synchronously. We propose a semantics which we call \nsynchronous, delayed visibility (SDV). Under SDV, group execution is synchronous, allowing precise divergence \nchecking. Each thread s shared memory accesses are logged, and the visibil\u00adity of writes to shared memory \nby one thread to the group is delayed until a barrier is reached. Delaying the visibility of writes ensures \nthat threads do not see a synchronized view of shared and global memory between barriers, catering for \nthe fact that execution might not really be fully synchronous. Logging accessed locations allows racing \naccesses to be de\u00adtected when threads synchronize at a barrier. To describe the SDV semantics formally, \nwe de.ne Ker\u00adnel Programming Language (KPL), which captures the es\u00adsential features of mainstream languages \nfor writing GPU kernels. KPL describes execution of a single group of GPU threads. Kernels in real GPU \nprogramming languages can have multiple groups, but it suf.ces for checking divergence\u00adand intra-group \nrace-freedom to model the execution of a single arbitrary group.  3.1 Syntax The syntax for KPL is shown \nin Figure 6. A KPL kernel declares the total number of threads in the group that will execute the kernel \n(threads: number), and the group s id (group: number), catering for the fact that in practice, the group \nmay be one of many. This is followed by a list of procedure declarations followed by a main statement. \nEach Figure 6: Syntax for Kernel Programming Language kernel ::= threads: number group: number * proc \nmain: stmt proc ::= procedure name var stmt stmt basic stmt ::= |||||||::= basic stmt | stmt; stmt local \nname stmt if local expr stmt else stmt while local expr stmt while local expr stmt name(local expr) barrier \nbreak | continue | return loc := local expr local expr loc name ||::= ||::= ::= loc := rd(local expr) \nwr(local expr, local expr) gid | lid | loc constant literal of type Word local expr op local expr name \n| V any valid C name procedure has a name, a single parameter and a body; for brevity we do not model \nmultiple parameters or return val\u00adues. The .nal element in the program is a main statement. For simplicity, \nbut without loss of generality, threads have access to a single shared array which we refer to as shared \nmemory. Since our goal is to verify absence of only intra\u00adgroup data races, we make no distinction between \nshared and global memory (c.f. Figure 1) in our programming language. We assume that every local variable \nand each indexable ele\u00adment of shared memory has type Word, the type of memory words. We assume that \nany value in Word can be interpreted as an integer and a boolean. In practice, Word will also rep\u00adresent \n.oating point numbers, and structured data will be represented by sequences of values of type Word. A \nthread may update one of its local variables by per\u00adforming a local computation, or by reading from the \nshared state (v := rd(e), where e is an expression over local vari\u00adables determining which index to read \nfrom). A thread may also update the shared state (wr(e1,e2), where e1,e2 are expressions over local variables, \nwith e1 determining which index to write to, and e2 the value to be written). For sim\u00adplicity, we assume \nall local variables are scalar. Compound statements are constructed via sequencing, conditional branches, \nlocal variable introduction, loops, and procedure calls in the standard way. KPL provides a few other \nstatements: barrier, which is used to synchronize threads; break, which causes execution to break out \nof the closest enclosing loop; continue, which causes execu\u00adtion to jump to the head of the closest enclosing \nloop; and return, which causes execution to return from the closest enclosing procedure call.  Figure \n6 speci.es two syntactic elements which should not appear directly in a KPL program; they are used in \nthe semantic rules of Figure 8 which we will explain in Section 3.2. These are: a special while statement, \nused to model the dynamic semantics of while loops in which we have to distinguish between the .rst and \nsubsequent iterations of a loop, and a set V of locations from which storage for local variables is allocated \nas they come into scope dynamically. We assume that the program is well-formed according to the usual \nrules, e.g., statements should only refer to declared variables and variable introduction should not \nhide a variable introduced earlier in an enclosing scope. An extra require\u00adment important for our semantics \nis that the main statement must not contain a return and must end with a barrier. We do not formalize \nfeatures of GPU kernels such as multi-dimensional groups and arrays. However, our veri.\u00adcation method \nand implementation, described in Section 4, handles both.  3.2 Semantics Notation. Given a function \nf : A . B and elements a . A, b . B, we write f[a := b] to denote the function g : A . B such that g(x)= \nf(x) for all x . A \\ {a}, and g(a)= b. We abbreviate f[a := b][c := d] to f[a := b, c := d]. By viewing \na tuple with named components as a function mapping names to element values, we also use this notation \nto specify updates to tuples. We use (s1,s2,...,sk) to denote a sequence of length k, and write () for \nthe empty sequence. We write s@ss for a sequence whose .rst element is s, and whose remaining elements \nform the sequence ss. We overload the @ operator and write ss@tt for the concatenation of sequences ss \nand tt. Thread and group states. To model the delayed visibility aspect of SDV, shared state is distributed: \neach thread is equipped with a shadow copy of shared memory. At the start of kernel execution, every \nthread s shadow memory is identical. During execution, a thread reads and modi.es its shadow memory locally, \nand logs read and write sets recording which addresses in shared memory the thread has accessed. When \na barrier statement is reached with all threads enabled, the read and write sets are checked for data \nraces. If a race has occurred, execution aborts. Otherwise the write sets are used to build a consistent \nview of shared memory, the shadow memories are all reset to agree with this view, and the read and write \nsets are cleared. In what follows let P be a KPL kernel. Let n denote the number of threads in the group \nexecuting P , speci.ed via threads: n in the de.nition of P .A thread state for P is a tuple (lid, l, \nsh, R, W ) where: lid : Word is the thread s id within the group.  l : V . Word is the storage for \nthe thread s local variables (recall that V is a set of locations).  sh : N . Word is the thread s \nshadow copy of shared memory.  R, W . Nare the thread s read and write sets, recording the shared addresses \nthe thread has accessed since the last barrier.  We use s to denote a thread state, and s.lid, s.l, \ns.sh, etc., to refer to the components of s. The set of all thread states is denoted ThreadStates. For \nlocal expression e and s thread state s, we write efor the result of evaluating e according to s.lid \nand s.l. We do not provide a concrete de.nition of es, which depends on the nature of the base type Word \nand the available operators, except we specify s that v= s.l(v) (where v . V is a storage location), \nlids = s.lid, and gids = x where group: x is speci.ed in the de.nition of P . A predicated statement \nis a pair (s, e), where s . stmt and e . local expr. Intuitively, (s, e) denotes a statement s that should \nbe executed if e holds, and otherwise should have no effect. The set of all predicated statements is \ndenoted PredStmts. A group state for P is a tuple (S, ss) where: S=(s0,...,sn-1) . ThreadStatesn records \na thread state for each thread in the group  ss . PredStmts * is an ordered sequence of program statements \nto be executed by the group  The set of all group states is denoted GroupStates. Given a tuple of thread \nstates S=(s0,...,sn-1), we use S(i) to denote si. A group state (S, ss) is a valid initial state of P \nif: ss = ((s, true)), where s is declared in P via main : s.  S(i).lid = i and S(i).R = S(i).W = \u00d8 \n(0 = i<n).  S(i).l(v)= false for all v . V (0 = i<n).  S(i).sh = S(j).sh (0 = i,j < n).  The .rst \ntwo requirements are straightforward. The third re\u00adquirement ensures that local variables are initialized \nto the value in Word corresponding to false. The .nal requirement ensures that threads have a consistent \nbut arbitrary initial view of the shared state. Our state representation does not in\u00adclude a single, \nde.nitive shared state component: the shared state is represented via the shadow copies held by individual \nthreads, which are initially consistent, and made consistent again at each barrier. Predicated group \nexecution. The rules of Figure 7 de.ne a binary relation .t . (ThreadStates \u00d7 PredStmts) \u00d7 ThreadStates \ndescribing the evolution of one thread state into another under execution of a predicated statement. \nFor readability, given a thread state s and predicated statement (s, p), we write (s, s, p) instead of \n(s, (s, p)). Rule T-DISABLED ensures that a predicated statement has no effect if the predicate does \nnot hold, indicated by  s \u00acp (T-DISABLED) (s, basic stmt,p) .t s s ' s pl= s.l[v := e ] (T-ASSIGN) (s, \nv := e, p) .t s[l := l'] s ' s' s pl= s.l[v := s.sh(e )] R= s.R .{e } (T-RD) (s, v := rd(e),p) .t s[l \n:= l',R := R'] sss s W ' p sh' = s.sh[e1 := e2 ]= s.W .{e1 } (T-WR) (s, wr(e1,e2),p) .t s[sh := sh',W \n:= W '] Figure 7: Rules for predicated execution of basic statements \u00acps in the rule s premises; T-ASSIGN \nupdates s.l according to the assignment; T-RD updates the thread s local store with an element from the \nthread s shadow copy of shared memory, and records the address that was read from; rule T-WR is analogous. \nFigure 8 de.nes a binary relation .g . GroupStates \u00d7 (GroupStates .{error}), where error is a designated \nerror state. This relation de\u00adscribes the evolution of a group as it executes a sequence of predicated \nstatements. Collective execution of a predi\u00adcated basic statement is achieved by every thread execut\u00ading \nthe statement, the order in which they do so is irrelevant (G-BASIC). If the group is due to execute \na barrier statement un\u00adder predicate p but not all threads agree on the truth of p, the error state is \nreached (G-DIVERGENCE). This precisely captures the notion of barrier divergence discussed in Sec\u00adtion \n2. Execution of barrier when all threads are disabled has no effect (G-NO-OP). Intra-group races are \ndetected via rule G-RACE. This rule states that if a group is due to execute a barrier statement and \nall threads are enabled, then when we compare the read and write sets computed by each thread, we must \nnot .nd distinct threads i and j such that the write set for thread i intersects with either the read \nor write set for thread j. If this scenario occurs, the error state is reached. The predicate races(S) \nis de.ned as follows: races(S) = . 0 = i = j <n. (S(i).R . S(i).W ) n S(j).W = \u00d8 The most intricate \nrule of Figure 8 is G-SYNC which cap\u00adtures the effect of a barrier synchronization in the absence of \ndata races. A new thread state S'(i) is constructed for each thread i, with the same local component \nl as before the bar\u00adrier. The barrier enforces a consistent view of shared mem\u00adory across the group by \nsetting the shared shadow memories sh identically in each S'(i). This is achieved using a func\u00adtion merge. \nIf thread i has recorded a write to shared mem\u00adory location z, i.e. z . S(i).W , then merge(S) maps z \nto the value at address z in thread i s shadow memory, i.e. to ' . 0 = i<n. (S(i), basic stmt,p) .t S(i) \n(G-BASIC) (S, (basic stmt,p)@ss) .g (S' , ss) S(i) S(j) . 0 = i .\u00acp = j<n.p (G-DIVERGENCE) (S, (barrier,p)@ss) \n.g error . 0 = i<n. \u00acpS(i) (G-NO-OP) (S, (barrier,p)@ss) .g (S, ss) . 0 = i<n.pS(i) races(S) (G-RACE) \n(S, (barrier,p)@ss) .g error . 0 = i<n.pS(i) \u00acraces(S) ' . 0 = i<n. S(i) = (S(i).lid, S(i).l, merge(S), \n\u00d8, \u00d8) (G-SYNC) (S, (barrier,p)@ss) .g (S' , ss) (S, (S1; S2,p)@ss) .g (S, (S1,p)@(S2,p)@ss)(G-SEQ) fresh \nv . V (G-VAR) (S, (local x S,p)@ss) .g (S, (S[x . v],p)@ss) fresh v . V (G-IF) (S, (if eS1 else S2,p)@ss) \n.g (S, (v := e, p)@(S1,p . v)@(S2,p .\u00acv)@ss) fresh v . V (G-OPEN) (S, (while e S,p)@ss) .g (S, (while \ne belim(S, v),p .\u00acv)@ss) . 0 = i<n. (p . e)S(i) fresh u, v . Vq = p . u .\u00acv (G-ITER) (S, (while e S,p)@ss) \n.g (S, (u := e, p)@(celim(S, v),q)@(while e S,p)@ss) . 0 = i<n. \u00ac(p . e)S(i) (G-DONE) (S, (while e S,p)@ss) \n.g (S, ss) fresh u, v . VS = Body(f)[Param(f) . u] (G-CALL) (S, (f(e),p)@ss) .g (S, (u := e; relim(S, \nv),p .\u00acv)@ss) Figure 8: Rules for lock-step execution of a group S(i).sh(z). Formally, merge(S) is a \nmap satisfying the fol\u00adlowing constraints: z . S(i).W 0 = i<n . 0 = i<n.z /. S(i).W merge(S)(z) = S(i).sh(z) \nmerge(S)(z) = S(0).sh(z) Because races(S) is false (a premise of the rule), merge(S) is unique. Finally, \nthe read and write sets of all threads are cleared. The remaining rules in Figure 8 describe predicated \nexe\u00adcution for compound statements. Rule G-SEQ is straightfor\u00adward. Rule G-VAR creates storage for a \nnew local variable x by allocating a fresh location v in V and substituting all occurrences of x in S \nby v; we use the notation S[x . v] to denote this substitution. Rule G-IF decomposes a condi\u00adtional statement \ninto a sequence of predicated statements: the conditional s guard is evaluated into a new location v, \nthe then branch S1, is executed by all threads under predi\u00adcate p . v (where p is the predicate of execution \nalready in place on entry to the conditional), and the else branch S2, is executed by all threads under \npredicate p .\u00acv.  The rules G-OPEN, G-ITER and G-DONE together model predicated execution of a while \nloop. In what follows, we say that a break or continue statement is top-level in a loop if the statement \nappears in the loop body but is not nested inside any further loops. Rule G-OPEN converts a while loop \ninto a while loop by creating fresh storage to model break statements. A fresh location v is selected; \nv records whether a thread has executed a break statement associated with the while loop. Like all local \nstorage, v has initial value false: no thread has executed break on loop entry. The function belim is \napplied to the loop body. This function takes a statement S and a lo\u00adcation v and replaces each top-level \nbreak statement inside S by the statement v := true. Furthermore, the predicate for the execution of \nthe while loop becomes p .\u00acv to model that statements in the loop have no effect subsequent to the execution \nof a break statement. A similar technique is used to model break statements in [14]. The G-ITER rule \nmodels execution of loop iterations, and handles continue statements. Two fresh local storage loca\u00adtions \nu and v are selected (both initialized to false). Location u is used to store the valuation of the loop \nguard. Location v is used to record whether a thread has executed a top-level continue statement during \nthe current loop iteration; v is initially false because no thread has executed a continue statement \nat the beginning of an iteration. First, the statement u := e (executed under enclosing predicate p) \nevaluates the loop guard into u. Then function celim is applied to the loop body; this function takes \na statement S and a location v and replaces each top-level continue statement inside S by the statement \nv := true. The loop body, after elimina\u00adtion of continue statements, is executed under the predicate \np . u .\u00acv (denoted q in rule G-ITER): a thread is enabled during the current iteration if the incoming \npredicate holds (p), the loop guard evaluated to true at the start of the itera\u00adtion (u) and the thread \nhas not executed a continue state\u00adment (\u00acv). (Note that, due to rule G-OPEN, the incoming predicate p \nincludes a conjunct recording whether the thread has executed a break statement.) After the loop body, \nthe while construct is considered again. Thus, all threads continuously execute the loop body us\u00ading \nG-ITER until, for for every thread, a) the enclosing pred\u00adicate p becomes false, either because this \npredicate was false on loop entry or because the thread has executed break, or b) the loop condition \nno longer holds for the thread. When a) or b) is the case for all threads, loop exit is handled by rule \nG-DONE. The rule G-CALL models the execution of a call to a pro\u00adcedure f. This involves executing the \nstatement correspond\u00ading to the body of the called procedure (Body(f)) after re\u00adplacing all occurrences \nof its formal parameter (Param(f)) with the given actual parameter expression. All threads ex\u00adecute the \nentire body of a procedure in lock-step. A fresh storage location v is used to record whether a thread \nhas ex\u00adecuted a return statement within the procedure body. Initially this location is set to false, \nand function relim replaces each return statement in Body(f) with the statement v := true. The procedure \nbody is executed under the predicate p .\u00acv (where p is the existing predicate of execution at the point \nof the call) so that execution of a return statement by a thread is simulated by the thread becoming \ndisabled for the remainder of the procedure body. 4. Veri.cation method Armed with the SDV semantics \nof Section 3, we now con\u00adsider the problem of verifying that a GPU kernel is race\u00adand divergence-free. \nFor this purpose, we have designed a tool, GPUVerify, built on top of the Boogie veri.cation sys\u00adtem \n[2]. Boogie takes a program annotated with loop invari\u00adants and procedure contracts, and decomposes veri.cation \ninto a set of formulas to be checked automatically by the Z3 theorem prover [8]. We describe challenges \nassociated with automatically translating OpenCL and CUDA kernels into a Boogie intermediate representation \n(Section 4.1), a tech\u00adnique for transforming this Boogie representation of the ker\u00adnel into a standard \nsequential Boogie program whose cor\u00adrectness implies race-and divergence-freedom of the orig\u00adinal kernel \n(Section 4.2), and a method for automatically inferring invariants and procedure contracts to enable \nauto\u00admatic veri.cation (Section 4.3). 4.1 Translating OpenCL and CUDA into Boogie To allow GPUVerify \nto be applied directly to source code we have implemented a compiler that translates GPU ker\u00adnels into \nan intermediate Boogie form. Our compiler is built on top of the CLANG/LLVM infrastructure [24] due to \nits existing support for both OpenCL and CUDA language ex\u00adtensions. There were effectively three challenges \nwith respect to this compilation. First, many industrial applications utilise features of OpenCL and \nCUDA not present in vanilla C, such as declaring variables as vector or image types, or calling in\u00adtrinsic \nfunctions; we therefore invested signi.cant engineer\u00ading effort into designing equivalent Boogie types \nand func\u00adtions. Second, the Boogie language does not support .oating point values directly, thus we modelled \nthem abstractly via uninterpreted functions. This sound over-approximation can in principle lead to false \npositives, but in our extensive eval\u00aduation (Section 5) we have only encountered one instance of this, \nwhere race-freedom depends on concrete .oating point values. The third issue, namely handling of pointers, \nis more interesting technically and is now discussed in depth.  Source Generated Boogie p = A; p = int_ptr(A_base, \n0); q = p; q = p; foo(p); foo(p); p = q + 1; p = int_ptr(q.base, q.offset + 1); p[e] = d; if(p.base == \nA_base) A[p.offset + e] = d; else if(p.base == B_base) B[p.offset + e] = d; else assert(false); x = p[e]; \nif(p.base == A_base) x = A[p.offset + e]; else if(p.base == B_base) x = B[p.offset + e]; else assert(false); \n Figure 9: Translating pointer usage into Boogie Modelling pointers. Boogie is a deliberately simple \ninter\u00admediate language, and does not support pointer data types natively. We have devised an encoding \nof pointers in Boogie which we explain using an example. For readability we use C-like syntax rather \nthan the Boogie input language. Suppose a kernel declares exactly two integer arrays (in any memory space) \nand two integer pointers: int A[1024], B[1024]; int *p, *q; In this case GPUVerify generates the following \ntypes: enum int_ptr_base = { A_base, B_base, null, none }; struct int_ptr { int_ptr_base base; int offset; \n}; Thus an integer pointer is modelled as a pair consisting of a base array, or one of the special values \nnull or none if the pointer is null or uninitialized, respectively, and an integer offset from this base. \nThe offset is in terms of number of elements, not bytes. Pointers p and q can be assigned to offsets \nfrom A or B, to null, or can be left uninitialized. Figure 9 shows how uses of p and q are translated \ninto Boogie. Statement p=q+1 demonstrates that pointer arith\u00admetic is straightforward to model using \nthis encoding. Pointer writes and reads are modelled by a case split on all the possible bases for the \npointer being dereferenced. If no base matches then the pointer is either uninitialized or null. These \nillegal dereferences are captured by an assertion failure. This encoding exploits the fact that in GPU \nkernels there are a .nite, and usually small, number of explicitly declared pointer targets. We deal \nwith stack-allocated local variables whose ad\u00addresses are taken by rewriting these variables as arrays \nof length one, and transforming corresponding accesses to such variables appropriately. This is made \npossible by the fact that GPU kernel languages do not permit recursion. Points-to analysis. The case-split \nassociated with pointer dereferences can hamper veri.cation of kernels with pointer\u00admanipulating loops, \nrequiring loop invariants that disam\u00adbiguate pointer dereferences. To avoid this, we have imple\u00admented \nSteensgaard s .ow-and context-insensitive pointer analysis algorithm [36]. Although this over-approximates \nthe points-to sets, our experience of GPU kernels is that aliasing is scarce and therefore precision \nis high. Returning to the above example, suppose points-to analysis determines that p may only refer \nto array A (or be null or uninitialized). In this case, the assignment p[e] = d is translated to: if(p.base \n== A_base) A[p.offset + e] = d; else assert(false); As well as checking for dereferences of null or uninitialized \npointers, the assert(false) case ensures that potential bugs in our points-to analysis do not lead to \nunsound veri.cation.  4.2 Reducing race-and divergence-checking to sequential program veri.cation Having \ncompiled an OpenCL or CUDA kernel into corre\u00adsponding Boogie form, GPUVerify attempts to verify the kernel. \nWe describe the veri.cation strategy employed by GPUVerify using a worked example. Consider the kernel \nof Figure 10a, adapted from part of a C++ AMP application that computes the transitive closure of a graph \nusing Warshall s algorithm, and simpli.ed for ease of presentation. The kernel is written for a single, \n2\u00addimensional group of SZ\u00d7SZ threads. A thread s local id is 2D, with x and y components lidX and lidY, \nrespectively. The kernel declares a 2D shared array of booleans, gr, rep\u00adresenting the adjacency matrix \nof a graph. Access logging instrumentation. A kernel is .rst instru\u00admented with calls to procedures that \nwill log accesses to shared arrays. Figure 10b shows the example kernel of Fig\u00adure 10a after access logging \ninstrumentation. Observe for example that the condition gr[lidY][k] &#38;&#38; gr[k][lidX]3 involves \ntwo read accesses to gr, thus is pre-pended by two calls to LOG_RD_gr. Reduction to a pair of threads. \nAfter access logging, the kernel must be translated into a form which models the predicated execution \nof multiple threads in a group. Ini\u00adtially, we attempted to directly encode the SDV semantics of Section \n3, modeling lock-step execution of all threads in a group. Unfortunately, modeling in this way required \nheavy use of quanti.ers, especially for implementing the G-SYNC rule of Figure 8 and associated merge \nfunction. This led to Boogie programs outside the decidable theory supported by the Z3 theorem prover. \nAs a result, veri.cation of micro ker\u00adnels took in the order of minutes, while veri.cation attempts for \nlarge kernels quickly exhausted memory limits. Both the properties of race-and divergence-freedom are \nstated pairwise: a race occurs when accesses by two threads con.ict, and divergence occurs when a barrier \nis executed in a state where one thread is enabled and another disabled. 3 For ease of presentation we \ntreat the operands of &#38;&#38; as being evaluated si\u00admultaneously. In reality, short-circuit evaluation \nintroduces an extra branch.  void barrier(); void barrier(); void LOG RD gr(int y, int x); void LOG \nWR gr(int y, int x); shared bool gr[SZ][SZ]; shared bool gr[SZ][SZ]; void kernel() { int k = 0; void \nkernel() { while(k < SZ) int k = 0; { while(k < SZ) { if(!gr[lidY][lidX]) LOG RD gr(lidY, lidX); { \nif(!gr[lidY][lidX]) { if(gr[lidY][k] &#38;&#38; LOG RD gr(lidY, k); gr[k][lidX]) LOG RD gr(k, lidX); \n{ if(gr[lidY][k] &#38;&#38; gr[lidY][lidX] gr[k][lidX]) { = true; LOG WR gr(lidY, lidX); } gr[lidY][lidX] \n= true; } } barrier(); } k++; barrier(); } k++; } }} (a) Example kernel (b) Kernel after race instrumentation \nvoid barrier(bool en1, bool en2); void LOG RD gr(bool en1, int y1, int x1, bool en2, int y2, int x2); \nvoid LOG WR gr(bool en1, int y1, int x1, bool en2, int y2, int x2); bool gr1[SZ][SZ], gr2[SZ][SZ]; void \nkernel() { int k1, k2; bool LC1, LC2, P1, P2, Q1, Q2; // Predicates // Assume that the pair of threads \nare distinct assume(lidX1 != lidX2 || lidY1 != lidY2); // Not shown: assume that thread ids lie in appropriate \nrange k1, k2 = 0, 0; LC1, LC2 = k1 < SZ, k2 < SZ; while(LC1 || LC2) { LOG RD gr(LC1, lidY1, lidX1, LC2, \nlidY2, lidX2); P1, P2 = LC1 &#38;&#38; !gr1[lidY1][lidX1], LC2 &#38;&#38; !gr2[lidY2][lidX2]; LOG RD \ngr(P1, lidY1, k1, P2, lidY2, k2); LOG RD gr(P1, k1, lidX1, P2, k2, lidX2); Q1, Q2 = P1 &#38;&#38; gr1[lidY1][k1] \n&#38;&#38; gr1[k1][lidX1], P2 &#38;&#38; gr2[lidY2][k2] &#38;&#38; gr2[k2][lidX2]; LOG WR gr(Q1, lidY1, \nlidX1, Q2, lidY2, lidX2); gr1[lidY1][lidX1], gr2[lidY2][lidX2] = Q1 ? true : gr1[lidY1][lidX1], Q2 ? \ntrue : gr2[lidY2][lidX2]; barrier(LC1, LC2); k1, k2 = LC1 ? k1 + 1 : k1, LC2 ? k2 + 1 : k2; LC1, LC2 \n= LC1 &#38;&#38; k1 < SZ, LC2 &#38;&#38; k2 < SZ; }} (c) Kernel after transformation to 2-thread predicated \nform Figure 10: Example illustrating how GPUVerify transforms a kernel into two-threaded, predicated \nform for veri.cation Based on this observation, we can consider transforming a kernel into a form where \nthe predicated execution of only two threads is modeled. If we can prove a kernel race-and divergence-free \nfor a pair of distinct but otherwise arbitrary threads, we can conclude correctness of the kernel. The \nde\u00adsign of the PUG veri.er for CUDA kernels also hinges on this observation [21]. Because a two-threaded \npredicated program with lock-step execution is essentially a sequen\u00adtial program consisting of parallel \nassignments to pairs of variables, reasoning about GPU kernels at this level com\u00adpletely avoids the problem \nof exploring interleavings of con\u00adcurrent threads, and allow us to leverage existing techniques for modular \nreasoning about sequential programs. For this approach to be sound, we must approximate rule G-SYNC of \nFigure 8, abstracting the values written to the shared state by threads that are not modeled. This can \nbe achieved in multiple ways. We have considered the following strategies: Adversarial abstraction: \nThe shared state is completely removed; reads are replaced with non-deterministic as\u00adsignments.  Equality \nabstraction: Both threads manipulate a shadow copy of the shared state. At a barrier, the shadow copies \nare set to be arbitrary, but equal. Thus on leaving the bar\u00adrier, the threads have a consistent view \nof shared memory.  We have found several example kernels (including the kernel of Figure 10a) where \nrace-freedom hinges on threads agreeing on the value of certain shared locations. In these cases, adversarial \nabstraction is too strong for successful veri.cation. However, in many such cases, it does not matter \nwhat speci.c value is stored in shared memory, only that all threads see the same value. The equality \nabstraction suf.ces for such cases. Our use of equality abstraction allows us to improve upon the precision \nof prior work [21] which is limited to adversarial abstraction. Using adversarial abstraction, when it \nsuf.ces, proves to be more ef.cient than equality abstraction. GPUVerify ap\u00adplies abstraction on array-by-array \nbasis. We have imple\u00admented an inter-procedural control dependence analysis to over-approximate those \nshared arrays whose values may in\u00ad.uence control .ow. Arrays which may in.uence control .ow are handled \nusing equality abstraction and all others us\u00ading adversarial abstraction. We have found this heuristic \ntyp\u00adically leads to equality abstraction being applied only when it is required. While the equality or \nadversarial abstractions suf.ce for veri.cation of the vast majority of kernels we have stud\u00adied, equality \nabstraction is not suf.cient when correctness depends upon richer properties of the shared state. For \nin\u00adstance, suppose a kernel declares shared arrays A and B, and includes a statement: A[B[lid]] = ... \nWrite-write race freedom of A requires that B[i] != B[j] for all distinct i and j. In practice, we have \nfound that this prohibits veri.cation of kernels which perform a pre.x sum operation into an array B, \nand then use B to index into an array A as shown above. We plan to investigate richer shared state abstractions \nto overcome this limitation in future work. Figure 10c shows the result of transforming the access\u00adinstrumented \nversion of the kernel (Figure 10b) into a form where the predicated execution of a pair of arbitrary, \ndis\u00adtinct threads is modeled, using the equality abstraction. (The transformation using adversarial abstraction \nis identical, ex\u00adcept that the arrays gr1 and gr2 are eliminated, and reads from these arrays are made \nnondeterministic.)  The id of the .rst thread is represented by the pair lidX1, lidY1, and similarly \nfor the second thread. The assume state\u00adment dictates that at least one of lidX and lidY should differ \nbetween the threads; we omit an additional precondition en\u00adsuring that the id components lie in the range \n[0..SZ-1]. Local variable k is duplicated, and the assignment k=0 replaced with a parallel assignment, \nsetting k1 and k2 to zero. The kernel declares fresh boolean variables LC, P and Q (duplicated for each \nthread). These are used to model predicated execution of the while loop (LC) and the outer and inner \nconditionals (P and Q respectively). In the examples of Section 2, and in the operational semantics of \nSection 3, we speci.ed that under predicated execution a while loop should continue to execute while \nthere exists a thread for which the condition holds. In the presence of just two threads, existential \nquanti.cation turns into disjunction, hence the loop condition LC1 || LC2. In Figure 10c, parameters \nto the LOG_RD_gr and LOG_WR_gr procedures are duplicated, with a parameter being passed for each thread. \nIn addition, a predicate parameter, en, is passed for each thread, recording whether the thread is enabled \ndur\u00ading the call (c.f. the incoming predicate p in the G-CALL rule of Figure 8). If LOG_RD_gr is called \nwith false as its en1 pa\u00adrameter, this indicates that the .rst thread is not enabled, and thus a read \nshould not be logged for this thread. Similarly, barrier is equipped with a pair of predicate parameters, \nen1 and en2. Proof sketch for two-thread reduction. First, consider an alternative semantics with a different \nversion of the rule G-SYNC in which the shadow states for each thread are either set to a completely \narbitrary value (adversarial ab\u00adstraction) or an arbitrary but consistent value (equality ab\u00adstraction). \nIt is easy to see that either of these two alterna\u00adtive semantics is an abstraction of the original semantics: \n(1) all states reachable via race-free and divergence-free ex\u00adecutions at barrier operations are preserved; \n(2) all diver\u00adgences and data-races are preserved. Now suppose there is a data-race between two threads \nwith indices i and j. Since the two-thread program is based on the abstract version of rule G-SYNC and \nthe indices of the two threads in this program are arbitrary symbolic constants, the two-thread program \ncan simulate the behavior of threads i and j all the way up to the data-race. Therefore, the two-thread \nprogram will also have a data-race. The argument for divergence is similar. Handling multiple procedures. \nDuring the transformation to two-threaded form, the parameter list of each user-de.ned procedure is duplicated, \nand (as with the LOG and barrier procedures) enabled predicates are added for each thread. The procedure \nbody is then translated to two-threaded, pred\u00adicated form, with every statement guarded by the enabled \npredicate parameters. Correspondingly, actual parameters are duplicated at call sites, and the current \npredicates of ex\u00ad ecution passed as enabled parameters. Checking divergence. Under the two-thread encoding, \nin\u00adserting a check for barrier divergence is trivial: the barrier procedure merely asserts that its arguments \nen1 and en2 are equal. This two-threaded version of rule G-DIVERGENCE (Figure 8) precisely matches the \nnotion of barrier divergence presented formally in Section 3. We may wish to only check divergence-freedom \nfor a kernel, if verifying race-freedom proves too dif.cult. This is sound under adversarial ab\u00adstraction, \nwhere every read from the shared state returns an arbitrary value. A kernel that can be shown divergence\u00adfree \nunder this most general assumption is guaranteed to be divergence-free under any schedule of shared state \nmodi.\u00adcations. If we prove divergence-freedom for a kernel under the equality abstraction, we can conclude \na weaker property than divergence-freedom: that barrier divergence cannot oc\u00adcur unless a data race has \noccurred. Note that our divergence checking is stricter than that attempted by the PUG veri\u00ad.er [21] \nwhich merely requires threads which follow dif\u00adferent conditional paths through a kernel to pass the \nsame number of barriers.4 While PUG reports micro-kernels ex\u00adhibiting the divergence bugs discussed in \nSection 2 as suc\u00adcessfully veri.ed, such kernels are rejected by GPUVerify. Race checking. The LOG_RD \nand LOG_WR procedures are responsible for manipulating a read and write set for each thread, for each \nof the kernel s shared arrays. According to the semantics of Section 3 (rule G-RACE of Figure 8), race \nchecking then involves asserting inside barrier for each array A that the read and write sets for A do \nnot con.ict between threads. Alternatively, we can immediately assert race-freedom whenever an access \nis logged. GPUVerify em\u00adploys this eager method, which we have found leads to faster analysis. We encode \nread and write sets ef.ciently by exploiting nondeterminism, similar to a method used in prior work [9, \n10]. For each shared array A with index type T we introduce the following variables for each thread i \nunder consideration (where i .{1, 2}): WR exists Ai : bool  WR elem Ai : T  RD exists Ai : bool  \nRD elem Ai : T  Boolean WR exists Ai is set to true if and only if thread i s write set for A is non-empty. \nIn this case, WR elem Ai represents one element of this write set: an index into A. The corresponding \nRD variables for read sets are similar. Initially WR/RD exists Ai is false for each thread be\u00adcause the \nread/write sets are empty. The LOG_WR_A proce\u00addure then works as follows: for each thread i, if i is \nenabled 4 In a subsequent paper on dynamic symbolic execution of CUDA ker\u00adnels [23] the authors of [21] \nimprove this check to restrict to textually aligned barriers.  on entry to the procedure (predicate \nparameter eni is true), then the thread nondeterministically chooses to do nothing, or to set WR exists \nAi to true and WR elem Ai to the index being logged. Procedure LOG_RD_A operates similarly. This strategy \nensures that if WR exists Ai holds, WR elem Ai is the index of an arbitrary write to A performed by thread \ni. Checking absence of write-write races can then be achieved by placing the following assertion in the \nLOG_WR_A proce\u00addure: assert(!(WR exists A1 . WR exists A2. WR elem A1 == WR elem A2)); Procedure LOG_WR_A \nworks analogously, and a similar asser\u00adtion is used to check read-write races. Because this encoding \ntracks an arbitrary element of each read and write set, if the sets can have a common, con.icting element \nthis will be tracked by both threads along some execution trace, and the generated assertion will fail \nalong this trace. If we can prove for every array that the associated assertions can never fail, we can \nconclude that the kernel is race-free. At a barrier, read and write sets are cleared via assuming that \nevery WR/RD exists is false, i.e., by terminating all execution paths along which read or written elements \nwere logged. Tolerating benign write-write races. In practice it is quite common for threads to participate \nin benign write-write races, where identical values are written to a common lo\u00adcation without synchronization. \nWhen equality abstraction is used, GPUVerify tolerates this kind of race by adding a conjunct to the \nabove assertion to check that the values writ\u00adten are not equal.  4.3 Invariant inference GPUVerify \nproduces a Boogie program akin to the trans\u00adformed kernel of Figure 10c, together with implementations \nof barrier and all LOG_RD/WR procedures. This program must be veri.ed to prove race-and divergence-freedom \nof the original kernel. Veri.cation hinges on .nding inductive invariants for loops and contracts for \nprocedures. We have found that invariant generation using abstract interpretation over standard domains \n(such as intervals or polyhedra) is not effective in verifying GPU kernels. This is partly due to the \ndata access patterns exhibited by GPU ker\u00adnels and discussed in detail below, where threads do not tend \nto read or write from contiguous regions of memory, and also due to the predicate nature of the programs \nproduced by our veri.cation method. Instead, we use the Houdini [12] algorithm as the basis for inferring \ninvariants and contracts. Houdini is a method to .nd the largest set of inductive invariants from amongst \na user-supplied pool of candidate invariants. Houdini works as a .xpoint procedure; starting with entire \nset of invariants, it tries to prove that the current candidate set is inductive. The invariants that \ncannot be proved are dropped from the candidate set and the procedure is repeated until a .xpoint is \nreached. We discuss the relationship between Houdini and other invariant generation techniques brie.y \nin Section 6. Through manually deducing invariants for a set of kernels (the training set described in \nour experimental evaluation, Section 5) we have devised a number of candidate genera\u00adtion rules which \nwe outline below. We emphasise that the candidate invariants generated by GPUVerify are just that: candidates. \nThe tool is free to speculatively generate can\u00addidates that later turn out to be incorrect: these are \nsimply discarded by Houdini. A consequence is that incorrect or unintended candidates generated due to \nbugs in GPUVerify cannot compromise the soundness of veri.cation. Our candidate generation rules are \npurely heuristic. The only fair way to evaluate these carefully crafted heuristics is to evaluate GPUVerify \nwith respect to a large set of unknown benchmarks. We present such an evaluation in Section 5.1. Candidate \ninvariant generation rules. In what follows, lid and SZ denote a thread s local id, and the size of the \nthread group, respectively, and ==> denotes implication. For clarity, we present the essence of each \nrule; the GPUVerify implementation is more .exible (e.g., being insensitive to the order of operands \nto commutative operations, and detecting when a thread s id has been copied into another local vari\u00adable). \nFor each of the rules associated with shared memory writes, there is an analogous rule for reads. Rule: \naccess at thread id plus offset. Condition: A[lid + C] = ... occurs in loop Generated candidate: WR_exists_A \n==> WR_elem_A -C == lid Rationale: It is common for a thread to write to an array using its thread id, \nplus a constant offset (which is often zero) as index; this access pattern is illustrated as follows: \n Rule: access at thread id plus strided offset. Conditions: A[lid + i*SZ + C] = ... occurs in loop i \nis live at loop head Generated candidate: WR_exists_A ==> ((WR_elem_A -C) % SZ) == lid Rationale: When \nprocessing an array on a GPU, it is typi\u00adcally ef.cient for threads in a group to access data in a coa\u00adlesced \nmanner as in the following example: for(i = 0; i < 256; i++) A[i*SZ + lid + C] = ...; This access pattern \nis illustrated as follows:  Rule: access at thread id plus strided offset, with strength reduction. \nConditions: i = lid appears before loop A[i+C] = ... occurs in loop i = i + SZ appears in loop body i \nis live at loop head Generated candidates: (i % SZ) == lid WR_exists_A ==> ((WR_elem_A -C) % SZ) == lid \nRationale: Same as the previous rule. However, GPU pro\u00adgrammers commonly apply the strength reduction \noperation manually, rewriting the above code snippet as follows: for(i = lid; i < 256*SZ; i += SZ) A[i \n+ C] = ...; In this case, the write set candidate invariant will not be inductive in isolation: the invariant \n(i % SZ) == lid is re\u00adquired in addition. Rule: access contiguous range. Conditions: A[lid*C + i] = ... \noccurs in loop i is live at loop head Generated candidates: WR_exists_A ==> lid*C <= WR_elem_A WR_exists_A \n==> WR_elem_A < (lid + 1)*C Rationale: It is common for threads to each be assigned a .xed-size chunk \nof an array to process. This access pattern is illustrated as follows: Rule: variable is zero or a power \nof two. Conditions: i = i*2 or i = i/2 occurs in loop i is live at loop head D is the smallest power \nof 2 with D = SZ Generated candidates: i&#38;(i-1)==0, i!=0, i<1, i<2, i<4, ... , i<D Rationale: GPU \nkernels frequently perform tree reduction operations on shared memory, as in the code snippet be\u00adlow. \nRace-freedom is ensured through the use of a bar\u00adrier, together with a guard ensuring that threads which \nhave dropped out of the reduction computation do not write to shared memory. Verifying race-freedom requires \nan invari\u00adant that the loop counter is a power of two in a pre.x\u00adclosed range, possibly including zero. \nThe above rule gen\u00aderates a linear number of candidates which capture all rele\u00advant pre.x-closed ranges. \nThe access pattern for such a tree reduction with respect to a group of 8 threads (SZ == 8) is illustrated \nbelow. A grey square containing a thread id indi\u00adcates a memory access by the associated thread; dark \ngrey indicates both a read and a write, while light grey indicates a read only. for(i = 1; i < SZ; i \n*= 2) { if((lid % (2*i)) == 0) { A[lid] += A[lid + i]; } barrier(); } GPUVerify includes a number of \nadditional candidate generation rules that are intimately related to the details of our transformation \nof a kernel to a predicated sequential program. We omit details of these rules as they are very speci.c \nand less intuitive. We have also designed rules to generate candidate pre\u00adand post-conditions for procedures. \nWe do not discuss these rules: although they allow us to perform modular veri.cation of some GPU kernels, \nwe .nd that for our current bench\u00admarks (which are representative of the sizes of today s GPU kernels), \nfull procedure inlining yields superior performance to modular analysis. 5. Experimental evaluation Benchmarks. \nWe evaluate GPUVerify using four bench\u00admark suites, comprising 163 kernels in total: AMD SDK: AMD Accelerated \nParallel Processing SDK v2.6 [1], 71 publicly available OpenCL kernels  CUDA SDK: NVIDIA GPU Computing \nSDK v2.0 [29], 20 publicly available CUDA kernels  C++ AMP: Microsoft C++ AMP Sample Projects [27], \n20 publicly available kernels, translated to CUDA  Basemark: Rightware Basemark CL v1.1 [34], 52 com\u00admercial \nOpenCL kernels, provided to us under academic license  To our knowledge, this benchmark set makes our \nevalua\u00adtion signi.cantly larger, in terms of number of kernels ana\u00adlyzed, than any previously reported \nevaluation of a tool for GPU kernel analysis. We consider the somewhat out-of-date v2.0 version of the \nNVIDIA SDK to facilitate a direct comparison of GPUVer\u00adify with PUG [21], the only existing veri.er for \nCUDA ker\u00adnels, since PUG is not compatible with more recent versions of the CUDA SDK. For this reason, \nwe restrict our attention to the benchmarks from this SDK which were used to eval\u00aduate PUG in [21]. Because \nGPUVerify cannot directly anal\u00adyse C++ AMP code, we retrieved the set of C++ AMP sam\u00adples available online \n[27] on 3 February 2011, and manually extracted and translated the GPU kernel functions into cor\u00adresponding \nCUDA kernels. This mechanical extraction and translation was straightforward. We scanned each benchmark \nsuite and removed kernels which are immediately beyond the scope of GPUVerify, either because they use \natomic operations (7 kernels) or because they involve writes to the shared state using double\u00adindirection \nas discussed in Section 4.2 (12 kernels). We plan to investigate supporting atomic operations, and design \nricher shared state abstractions to handle double-indirection, in future work. Experiments are performed \non a PC with a 3.4GHz Intel Core i7-2600 CPU, 8GB RAM running Windows 7 (64\u00adbit), using revision 2490 \nof Boogie, and Z3 v3.3. All times reported are averages over 3 runs.  GPUVerify, together with all our \nnon-commercial bench\u00admark kernels, are available from our web page.5 5.1 Evaluation of GPUVerify Methodology. \nThe practical utility of GPUVerify for prov\u00ading correctness of GPU kernels depends largely on the ef\u00adfectiveness \nof the tool s invariant inference technique. In\u00advariant inference must be precise enough to allow automatic \nveri.cation of typical kernels, and semi-automatic veri.ca\u00adtion of especially intricate kernels. Inference \nmust not com\u00adpromise ef.cient analysis: whether veri.cation succeeds or fails (in the latter case due \nto the kernel being incorrect, or the inferred invariants being too weak), the runtime associ\u00adated with \nveri.cation should be as low as possible. Ideally, GPUVerify should be suitably ef.cient that it can \nrun as a background process in an IDE such as Eclipse, to provide immediate feedback to GPU kernel developers. \nWe have used the following methodology to design and evaluate our invariant inference technique. We divided \nour benchmarks into two similarly-sized sets: a training set and an evaluation set, such that details \nof the evaluation set were previously unknown to all members of our team. We chose the CUDA SDK, C++ \nAMP and Basemark benchmarks as the training set (92 kernels) and the AMD SDK benchmarks as the evaluation \nset (71): members of our team had looked previously at the CUDA SDK and C++ AMP benchmarks, but not at \nthe AMD SDK or Basemark benchmarks; how\u00adever, we wanted to make the evaluation set publicly avail\u00adable, \nruling out Basemark. We manually analysed all benchmarks in the training set, determining invariants \nsuf.cient for proving race-and divergence-freedom. We then distinguished between be\u00adspoke invariants: \ncomplex, kernel-speci.c invariants re\u00adquired by individual benchmarks; and general invariants, conforming \nto an identi.able pattern that cropped up across multiple benchmarks. The general invariants led us to \ndevise the invariant inference heuristics described in Section 4.3. We implemented these heuristics in \nGPUVerify and tuned GPUVerify to maximise performance on the training set. We then applied GPUVerify \nblindly to the evaluation set. We report below the extent to which our inference technique enabled fully \nautomatic analysis of the AMD SDK kernels. We believe that this approach of applying GPUVerify unassisted \nto a large, unknown set of benchmarks provides a fair evaluation of the tool s automatic capabilities. \nCharacteristics of the training and evaluation sets. Fig\u00adure 11 provides an overview of the sizes of \nbenchmarks in the training and evaluation sets. We indicate the number of effective lines of code (ELOC) \n(this excludes comments and whitespace, and counts a statement spanning multiple lines as a single effective \nline), number of procedures and number of loops. The largest kernels we analysed consist of 100 and 5 \nhttp://multicore.doc.ic.ac.uk/GPUVerify ELOC =30 31-60 61-120 121-180 max=576 Training 78 13 1 0 0 Evaluation \n49 10 6 5 1 #procs 1 2-3 4-5 6-7 max=8 Training 69 18 2 3 0 Evaluation 55 10 5 0 1 #loops 0 1 2 3 4-5 \nTraining 44 24 19 2 3 Evaluation 21 27 20 0 3 Figure 11: Summary of size, in terms of ELOC, number of \nprocedures and number of loops, of benchmarks in the training and evaluation sets  576 ELOC for the \ntraining and evaluation sets, respectively. The size and complexity of our benchmark kernels are rep\u00adresentative \nof GPU kernels in practical use. In all experiments, we run GPUVerify using a timeout of .ve minutes \nper benchmark. Full inlining of procedures is used, as discussed in Section 4.3. Results for the training \nset. Figure 12 is a cumulative his\u00adtogram showing the performance of GPUVerify with respect to the training \nset. The x-axis plots time (in seconds, on a log scale), and the y-axis plots number of kernels. A point \nat position (x, y) indicates that for y of the kernels, veri.cation took x seconds or fewer. The results \nshow that GPUVerify is capable of rapidly analysing the vast majority of the training set kernels: 85 \nout of 92 were veri.ed in 10 seconds or fewer. The longest veri.cation time was 105 seconds; this is \nfor a CUDA Pre.xSum kernel which contains a complex bespoke invariant. In no cases did veri.cation time \nout. Running GPUVerify with race checking disabled, the tool was able to prove barrier divergence freedom \nfor all training benchmarks, in under 10 seconds per kernel.  We had to add invariants manually to \na number of the training set benchmarks to enable veri.cation. In two cases these were bespoke invariants, \nas mentioned above. In the remaining cases we had to add a general sort of relational invariant commonly \nrequired for kernels that perform tree reductions. Tree reductions are often written in the following \nform (where SZ denotes group size and is assumed to be a power of two): int offset = 1; for (int d = \nSZ; d > 0; d >>= 1) { barrier(); offset *= 2; if (lid < d) { // Write to shared array using function \nof offset and lid } } In this case, invariants asserting that offset and d are in\u00addividually powers of \ntwo, as in the .nal rule discussed in Section 4.3, do not suf.ce. A relational invariant between offset \nand d is also required: (d==SZ &#38;&#38; offset==1 ) || (d==SZ/2 &#38;&#38; offset==2) || ... || (d==1 \n&#38;&#38; offset==SZ) || (d== 0 &#38;&#38; offset==2*SZ)) We have not yet devised a general-purpose \nheuristic for inferring this sort of relational invariant. Results for the evaluation set. Figure 13 \nsummarises anal\u00adysis times for GPUVerify applied to the evaluation set. This plot shows three cumulative \nhistograms. The cumulative his\u00adtogram whose points are crosses relates to benchmarks for which veri.cation \nsucceeded: a cross with coordinates (x, y) indicates that for y kernels, veri.cation succeeded in x sec\u00adonds \nor fewer. The cumulative histogram whose points are circles relates to benchmarks for which veri.cation \nfailed, either due to the kernel being incorrect, or due to invariant inference being too weak. A circle \nwith coordinates (x, y) indicates that for y kernels, veri.cation failed in x seconds or fewer. Finally, \nthe cumulative histogram whose points are squares relates to benchmarks in either of the previous two \ncategories. This indicates the responsiveness of GPUVerify: a square at (x, y) indicates that for y benchmarks, \nGPUVer\u00adify terminated, reporting either success or failure, within x seconds. Thus the y coordinate of \neach square is the sum of the y coordinates of the associated cross and circle: if ver\u00adi.cation succeeded \nand failed in x seconds or fewer for ys and yf kernels, respectively, then veri.cation terminated in \nx seconds or fewer for ys + yf kernels. Using the inference techniques devised with respect to the training \nset (c.f. Section 4.3), GPUVerify was able to verify 49 out of the 71 evaluation set kernels (69 %) fully \nautomat\u00adically. Of these kernels, 48 were veri.ed in 10 seconds or fewer, and the longest veri.cation \ntime was 17 seconds. As for the training set, with race checking disabled, GPUVerify was able to prove \ndivergence freedom fully automatically for all evaluation benchmarks, in under 10 seconds per kernel. \nMany modern static analysis tools achieve low false alarm rates via a careful mixture of deliberately \nintroduced unsoundness in the analysis and ad hoc warning suppression. GPUVerify does not follow this \napproach: the tool attempts to be a real veri.er, and thus will report veri.cation fail\u00adure for a kernel \nunless it was possible to construct a proof of correctness in a sound manner, under bit-level accuracy. \nWith this in mind, we believe that being able to verify 49 out of 71 evaluation kernels is a good result. \nThe results also show that, with one exception, the re\u00adsponse time of GPUVerify, whether or not veri.cation \nsuc\u00adceeds, is reasonable. The top cumulative histogram shows that veri.cation terminated within 10 seconds \nfor 68 of the kernels, and the response time for 70 out of the 71 kernels was less than 59 seconds. We \nbelieve that response time is critically important for practical uptake of the tool. Given that GPUVerify \nwill frequently be applied to incorrect ker\u00adnels, and accepting that invariant inference cannot be per\u00adfect, \nit is encouraging that GPUVerify s runtime is relatively impervious to whether veri.cation succeeds or \nfails. Veri.\u00adcation of one kernel timed out: this is a loop-free FFT im\u00adplementation consisting of 576 \nELOC. After translation to Boogie, reduction to a sequential program and full proce\u00addure inlining, the \nresulting Boogie program is almost 10,000 lines, resulting in a huge veri.cation condition. Our current \ninference rules for procedure contracts were not suf.cient to enable modular veri.cation of this kernel. \nOver all kernels that were successfully veri.ed, 81 % of the candidate invariants speculated by GPUVerify \nproved to be true. This relatively high percentage indicates that our invariant generation rules (Section \n4.3) are usually .ring in cases where generated candidates turn out to be useful. In addition to the \nFFT example, we have manually in\u00adspected the other 21 kernels for which GPUVerify reported veri.cation \nfailure. Figure 14 summarises 13 cases where veri.cation would succeed with more sophisticated invari\u00adant \ninference. A further kernel veri.es with a complex be\u00adspoke invariant, which does not appear to conform \nto a gen\u00aderal pattern. Veri.cation of .ve kernels failed due to missing preconditions on kernel parameters. \nOne example is a kernel whose race freedom depends upon the identity a + b = a, where a and b are positive \nvalues read from the shared state. If a and b are suf.ciently large then, with 32-bit integers, this \nidentity does not hold. Without a precondition stating that the contents of the shared state are suitably \nbounded, GPUVerify reports a data race. We believe that GPUVerify can be a useful as a tool to help GPU \nkernel programmers explicitly understand and state the preconditions their ker\u00adnels assume. For one kernel \nGPUVerify reports a read-write race which we discovered to be benign: the writing thread is guaranteed \nto write the same value which the reading thread is about to read. Finally, we encountered one false \npositive arising from an array index being derived from .oating point input data.  Reason for veri.cation \nfailure Solution Slight variations of the access patterns recognised by our current inference rules are \nused (7 kernels) Generalise existing inference rules Access patterns should be recognised by our inference \nrules, but rules do not .re because components of an integer vector, rather than integer variables, are \nused for indexing (3 kernels) Enhance existing inference rules to be sensitive to vector components Kernel \nemploys tree reduction in the form described above, and veri.es with corresponding relational invariant \n(3 kernels) Design inference rules for tree reduction invariants Figure 14: Common causes of veri.cation \nfailure for evalu\u00adation set kernels, and planned improvements to inference  Detection of a bug in previous \nCUDA SDK example. Us\u00ading GPUVerify we discovered a write-write data race in the N-body example that shipped \nwith the CUDA SDK v2.3. This example uses multiple CUDA kernels to numerically approximate a system of \nN interacting bodies [33]. This is an ideal problem for parallelisation since interactions be\u00adtween each \npair of bodies can be calculated independently. The CUDA implementation of this example decomposes the \nN2 pair-interactions into smaller k \u00d7 k tiles, each of which is assigned to a one-dimensional group of \nk threads. Within each group, every thread is assigned to a distinct body (a row of the tile) and sequentially \nconsiders the interactions associ\u00adated with this body to compute an updated state for the body. The kernel \nimplements an optimisation for small values of N where threads are arranged in two-dimensional groups, \nand multiple threads within a group are assigned to the same body. Consequently, the interactions calculated \nby threads assigned to the same body must be summed. A barrier en\u00adsures that each thread has completed \nits sub-calculation, and then a conditional is used to ensure that a single master thread performs the \nsummation. However, a data race could occur because a similar condition was not in place to ensure that \nonly this master thread would perform a .nal update to the position and velocity of the body. As a result, \nit was possible for the master thread s .nal update, using the full summation, to be overwritten by partial \nresults computed by other threads. We reported this data race to Lars Nyland at NVIDIA who con.rmed that \nIt was a real bug, and it caused real issues in the results. It took signi.cant debugging time to .nd \nthe problem. [32]. NVIDIA had subsequently .xed this bug in v3.0 of the CUDA SDK.  5.2 Comparison of \nGPUVerify with PUG We present a head-to-head comparison of PUG and GPU-Verify on our CUDA benchmarks: \nthe CUDA SDK and C++ AMP suites (40 kernels). Recall that we removed kernels from these suites which \nuse atomic operations or write to the shared state using double-indirection: PUG is also in\u00adcapable of \nreasoning about these features. The CUDA SDK benchmarks were previously used to evaluate PUG [21]. The \nC++ AMP benchmark suite consists of kernels we translated manually from C++ AMP to CUDA, which we then \nadapted to allow for documented limitations of PUG s front-end. We made a special effort not to modify \nthese kernels in any way which would make them easier for GPUVerify to handle. Both GPUVerify and PUG \nrepresent integers using bit\u00advectors. GPUVerify always uses 32 bits for variables of int type, as this \nis required by both CUDA and OpenCL. PUG allows the user to specify which bit width should be used for \nintegers. In the evaluation of [21], custom bit widths were chosen on a benchmark-by-benchmark basis. \nTo make the current comparison fair, we always run PUG in 32-bit mode: we believe that this is essential \nfor veri.cation purposes as smaller bit widths can change the semantics of kernels under analysis. For \nall experiments, we use a .ve minute timeout for both PUG and GPUVerify. Results for correct benchmarks. \nWe found that PUG re\u00adported false positive for three kernels. These are kernels whose correctness depends \nupon threads agreeing on the contents of the shared state. GPUVerify is able to reason about these kernels \nusing the equality abstraction (Sec\u00adtion 4.2). PUG s shared state abstraction is equivalent to our adversarial \nabstraction, which is not suf.cient for these kernels. Note that GPUVerify decides automatically which \nshared state abstraction to use. Figure 15 compares the performance of GPUVerify and PUG on the 37 kernels \nthat both tools could handle. A point with coordinates (x, y) corresponds to a kernel which took x and \ny seconds to be veri.ed by GPUVerify and PUG respectively. Points lying above/below the diagonal corre\u00adspond \nto kernels where GPUVerify performed better/worse than PUG. Points at the very top of the graph correspond \nto kernels for which PUG timed out. The axes use a log scale.  The plot shows that PUG is on average \nfaster than GPU-Verify, but that PUG s worst-case performance is signi.\u00adcantly worse than GPUVerify s: \nthe timeout of 5 minutes is reached by PUG for six kernels, but never reached by GPU-Verify. We also \n.nd that PUG runs extremely quickly, taking only a tiny fraction of a second, for six kernels. We con\u00adjecture \nthat in these cases PUG may be able to infer race freedom though cheap syntactic checks, without invoking \nits constraint solver. Results for buggy benchmarks. We have also compared GPUVerify and PUG to see how \nquickly they report proof failures when applied to buggy kernels. We randomly in\u00adjected a mutation into \neach kernel in the CUDA SDK and C++ AMP benchmark suites. First, we used a script to choose, for each \nkernel, a random mutation and a random location within the kernel to apply the mutation. These mu\u00adtations \nwere chosen to elicit either a data race (for example, removing a barrier or adding a racy access) or \nbarrier diver\u00adgence (for example, adding a barrier where control .ow is non-uniform). The script places \nits suggestions as comments within each kernel. Secondly, we took each kernel and ex\u00adamined the suggested \nmutation. If it was sensibly placed and would give rise to buggy behaviour we implement the mu\u00adtation \nby hand; otherwise, we reran the script to generate a fresh mutation suggestion, repeating the process \nuntil a suitable mutation was generated. We found that GPUVerify s proof attempts generally failed within \naround 5 seconds, whereas PUG s proof at\u00adtempt failed usually within half a second: an order of mag\u00adnitude \nfaster. However, for seven buggy kernels we found that PUG reported false negatives: wrongly reporting \ncor\u00adrectness of the kernel. Of these false negatives, one mutation was an injected barrier divergence \nwhile the remaining six were data races. GPUVerify reported no false negatives. 6. Related work There \nare numerous existing dynamic and static techniques for data-race detection in programs using lock-based \nsyn\u00adchronization or fork-join parallelism; a full discussion of these techniques is beyond the scope \nof this paper. We note however that this paper is concerned with proving race\u00adand divergence-freedom \nin data-parallel programs in which the primary challenges barrier synchronization and dis\u00adjoint access \npatterns based on clever array indexing are different from those encountered in lock-based and fork-join \nprograms. In the rest of this section, we discuss papers that explicitly handle data-parallel or GPU \nprograms. We con\u00adclude the section with a brief discussion of invariant genera\u00adtion techniques. PUG. \nThe closest work to GPUVerify is the PUG an\u00adalyzer for CUDA kernels [21]. Although GPUVerify and PUG \nhave similar goal, scalable veri.cation of GPU ker\u00adnels, the internal architecture of the two systems \nis very different. GPUVerify .rst translates a kernel into a sequen\u00adtial Boogie program that models the \nlock-step execution of two threads; the correctness of this program implies race\u00adand divergence-freedom \nof the original kernel. Next, it in\u00adfers and uses invariants to prove the correctness of this se\u00adquential \nprogram. Therefore, we only need to argue sound\u00adness for the translation into a sequential program; the \nsound\u00adness of the veri.cation of the sequential program follows di\u00adrectly from the soundness of contract-based \nveri.cation. On the other hand, PUG performs invariant inference simulta\u00adneously with translation of \nthe GPU kernel into a logical formula. PUG provides a set of built-in loop summarisa\u00adtion rules which \nreplace loops exhibiting certain shared array access patterns with corresponding invariants. Unlike GPU-Verify, \nwhich must prove or discard all invariants that it gen\u00aderates, the loop invariants inserted by PUG are \nassumed to be correct. While this approach works for simple loop pat\u00adterns, it has dif.culty scaling \nto general nested loops in a sound way resulting in various restrictions on the input pro\u00adgram required \nby PUG. In contrast, GPUVerify inherits .ex\u00adible and sound invariant inference from Houdini regardless \nof the complexity of the control structure of the GPU kernel. Formal semantics for GPU kernels. A recent \npaper study\u00ading the relationship between the lock-step execution model of GPUs and the standard interleaved \nsemantics for threaded programs presents a formal semantics for predicated execu\u00adtion [14]. This semantics \nshares similarities with the SDV semantics we present in Section 3, but the focus of [14] is not on veri.cation \nof GPU kernels. Symbolic execution and bounded-depth veri.cation. The GKLEE [23] and KLEE-CL [6] tools \nperform dynamic sym\u00adbolic execution of CUDA and OpenCL kernels, respectively, and are both built on top \nof the KLEE symbolic execution engine [5]. A method for bounded veri.cation of barrier\u00adfree GPU kernels \nvia depth-limited unrolling to an SMT formula is presented in [38]; lack of support for barriers, present \nin most non-trivial GPU kernels, limits the scope of this method. Symbolic execution and bounded unrolling \ntechniques can be useful for bug-.nding both GKLEE and KLEE-CL have uncovered data race bugs in real-world \nexamples and these techniques have the advantage of gen\u00aderating concrete bug-inducing tests. A further \nadvantage of GKLEE and KLEE-CL is that because they are based on KLEE, which works on LLVM bytecode, \nthey can be ap\u00adplied to GPU kernels after optimization and thus have the potential to detect bugs that \nresult from incorrect compiler optimizations. The major drawback to these methods is that they cannot \nverify freedom of defects for non-trivial kernels.  The GKLEE tool speci.cally targets CUDA kernels, \nand faithfully models lock-step execution of sub-groups of threads, or warps as they are referred to \nin CUDA (see Fig\u00adure 2). This allows precise checking of CUDA kernels that deliberately exploit the warp \nsize of an NVIDIA GPU to achieve high performance. In contrast, GPUVerify makes no assumptions about \nsub-group size, making it useful for checking whether CUDA kernels are portable, but incapable of verifying \nkernels whose correctness depends on implicit warp-level synchronization. Both GKLEE and KLEE-CL explicitly \nrepresent the number of threads executing a GPU kernel. This allows for precise defect checking, but \nlimits scalability. A recent ex\u00adtension to GKLEE uses the notion of parametric .ows to soundly restrict \ndefect checking to consider only certain pairs of threads [22]. This is similar to the two-thread ab\u00adstraction \nemployed by GPUVerify and PUG, and leads to scalability improvements over standard GKLEE, at the ex\u00adpense \nof a loss in precision for kernels that exhibit inter\u00adthread communication. Dynamic analysis. Dynamic \nanalysis of CUDA kernels for data race detection has been proposed [4]. A recent pa\u00adper [20] reports \na technique that combines dynamic and static data race analysis: a CUDA kernel is simulated with dynamic \nrace checking. If no races are detected, .ow analy\u00adsis is used to determine whether the control-.ow taken \ndur\u00ading dynamic execution was dependent on input data; if not, the kernel can be deemed race free, otherwise \nthe technique is inconclusive. It appears that this approach can handle ker\u00adnels that are veri.able using \nour adversarial abstraction. Kernels which GPUVerify can verify only with the equal\u00adity abstraction, \ndue to threads testing input data, are not be amenable to analysis using the technique of [20]. Other \napproaches. A recent approach to construction of correct parallel programs is based on thread contracts \n[16]. A programmer speci.es the coordination and data sharing strategy for their multi-threaded program \nas a contract, ade\u00adquacy of the speci.cation for ensuring race-freedom is then checked statically, while \nadherence to the speci.cation by the implementation is ascertained via testing. Adapted to the setting \nof barrier synchronization rather than lock-based coordination, this technique might enable analysis \nof more complex GPU kernels for which automatic contract infer\u00adence is infeasible. Invariant generation. \nAs described in Section 4.3, we use the Houdini algorithm [12] to generate loop invariants for veri.cation. \nHoudini was introduced as an annotation as\u00adsistant for the Java Extended Static Checker [19]. Related \ntemplate-based invariant generation techniques include [15, 18, 35]. As discussed in the related work \nsection of [12], Houdini can be viewed under the framework of abstract in\u00adterpretation [7] where the \nabstract domain is conjunctions of predicates drawn from the set of candidate invariants. Com\u00adpared with \nstandard predicate abstraction [13], which con\u00adsiders arbitrary boolean combinations of predicates (and \nis thus more precise), veri.cation using Houdini requires a lin\u00adear instead of exponential number of \ntheorem prover calls. In our context, the key advantage of the Houdini approach over traditional abstract \ninterpretation using a .xed abstract domain is .exibility. We can easily extend GPUVerify with a richer \nlanguage of predicates by adding further candidate in\u00advariant generation rules; there is no need for \ncareful redesign of an abstract domain. The main problem with our invariant generation method is that \nits success is limited by the scope of our candidate in\u00advariant generation rules. Interpolation and counterexample\u00adguided \nabstraction re.nement can be used incrementally generate invariants in response to failed or partial \nveri.\u00adcation attempts [3, 26], while the Daikon technique [11] allows program-speci.c invariants to be \nspeculated through dynamic analysis. We plan to draw upon these techniques to improve GPUVerify s invariant \ninference in future work. 7. Conclusions We have provided an operational semantics for GPU kernels, and \nused this semantics to design a novel technique for for\u00admal veri.cation of race-and divergence-freedom. \nThrough a large experimental evaluation we have demonstrated that our implementation of this technique, \nGPUVerify, is effective in verifying and falsifying real-world OpenCL and CUDA GPU kernels. Future work \nwill involve extending the reach of GPU-Verify by supporting atomic operations, investigating more sophisticated \nstrategies for inferring loop invariants and pro\u00adcedure speci.cations, and devising richer shared state \nab\u00adstractions. Acknowledgements Our thanks to Guodong Li and Ganesh Gopalakrishnan for providing us with \nthe latest version of PUG, and for an\u00adswering numerous questions about the workings of this tool.  Thanks \nto Matko Botin.can, Mike Dodds, Hristina Palikareva and the anonymous reviewers for their insightful \nfeedback on an earlier draft of this work. We are grateful to several people working in the GPU in\u00addustry \nfor their assistance with this work: Anton Lokhmotov (ARM) for providing us with barrier divergence results \nfor ARM s Mali architecture; Lee Howes (AMD) for providing such results for AMD s Tahiti architecture; \nTeemu Uotila and Teemu Virolainen (Rightware) for providing access to Basemark CL, and answering our \nqueries about these ker\u00adnels; Yossi Levanoni (Microsoft) for providing early access to C++ AMP samples. \nReferences [1] AMD. AMD Accelerated Parallel Processing (APP) SDK. developer.amd.com/sdks/amdappsdk/pages/default.aspx \n[2] M. Barnett, B. Chang, R. DeLine, B. Jacobs, and K. R. M. Leino. Boogie: A modular reusable veri.er \nfor object-oriented programs. In FMCO, 2005.  [3] D. Beyer, T. A. Henzinger, R. Majumdar, and A. Ry\u00adbalchenko. \nPath invariants. In PLDI, 2007. [4] M. Boyer, K. Skadron, and W. Weimer. Automated dynamic analysis of \nCUDA programs. In STMCS, 2008. [5] C. Cadar, D. Dunbar, and D. Engler. KLEE: Unassisted and automatic \ngeneration of high-coverage tests for complex systems programs. In OSDI, 2008. [6] P. Collingbourne, \nC. Cadar, and P. H. J. Kelly. Symbolic testing of OpenCL code. In HVC, 2011. [7] P. Cousot and R. Cousot. \nAbstract interpretation: a uni.ed lattice model for static analysis of programs by construction or approximation \nof .xpoints. In POPL, 1977. [8] L. de Moura and N. Bj\u00f8rner. Z3: An ef.cient SMT solver. In TACAS, 2008. \n[9] A. Donaldson, D. Kroening and P. R\u00a8ummer. Automatic anal\u00adysis of scratch-pad memory code for heterogeneous \nmulticore processors. In TACAS, 2010. [10] A. Donaldson, D. Kroening and P. R\u00a8 ummer. Automatic analy\u00adsis \nof DMA races using model checking and k-induction. For\u00admal Methods in System Design 39(1):83-113. [11] \nM. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant, C. Pacheco, M. S. Tschantz, and C. Xiao The Daikon \nsystem for dynamic detection of likely invariants. Sci. Comput. Program. 69(1-3):35-45. [12] C. Flanagan \nand K. R. M. Leino. Houdini, an annotation assistant for ESC/Java. In FME, 2001. [13] S. Graf and H. \nSa idi. Construction of abstract state graphs with PVS. In CAV, 1997. [14] A. Habermaier and A. Knapp. \nOn the correctness of the SIMT execution model of GPUs. In ESOP, 2012. [15] T. Kahsai, Y. Ge, and C. \nTinelli. Instantiation-based invariant discovery. In NFM, 2011. [16] R. K. Karmani, P. Madhusudan, and \nB. Moore. Thread con\u00adtracts for safe parallelism. In PPOPP, 2011. [17] Khronos OpenCL Working Group. \nThe OpenCL speci.ca\u00adtion, version 1.1, 2011. Document Revision: 44. [18] S. K. Lahiri and S. Qadeer. \nComplexity and algorithms for monomial and clausal predicate. In CADE, 2009. [19] K. R. M. Leino, G. \nNelson, and J. B. Saxe. ESC/Java user s manual. Technical Note 2000-002, Compaq Systems Re\u00adsearch Center, \nOctober 2000. [20] A. Leung, M. Gupta, Y. Agarwal, R. Gupta, R. Jhala, and S. Lerner. Verifying GPU kernels \nby test ampli.cation. In PLDI, 2012. [21] G. Li and G. Gopalakrishnan. Scalable SMT-based veri.ca\u00adtion \nof GPU kernel functions. In FSE, 2010. [22] G. Li, P. Li, G. Gopalakrishnan. Parametric .ows: automated \nbehaviour equivalencing for symbolic analysis of races in CUDA programs. In SC, 2012. [23] G. Li, P. \nLi, G. Sawaya, G. Gopalakrishnan, I. Ghosh, and S. P. Rajan. GKLEE: concolic veri.cation and test generation \nfor GPUs. In PPOPP, 2012. [24] llvm.org. clang: a C language family frontend for LLVM. clang.llvm.org \n[25] A. Lokhmotov. Mobile and embedded computing on Mali GPUs. In UK GPU Computing Conference, 2011. \n[26] K. L. McMillan. Lazy abstraction with interpolants. In CAV, 2006. [27] Microsoft Corporation. C++ \nAMP sample projects for down\u00adload (MSDN blog). blogs.msdn.com/b/nativeconcurrency/archive/2012/01/ 30/c-amp-sample-projects-for-download.aspx \n[28] D. Moth and Y. Levanoni. Microsoft s C++ AMP unveiled. www.drdobbs.com/windows/231600761 [29] NVIDIA. \nCUDA Toolkit Release Archive. developer.nvidia.com/cuda-toolkit-archive [30] NVIDIA. CUDA C programming \nguide, v4.0, 2011. [31] NVIDIA. PTX: Parallel thread execution ISA, v2.3, 2011. [32] L. Nyland. Personal \ncommunication, April 2012. [33] L. Nyland, M. Harris, and J. Prins. Fast N-body simulation with CUDA. \nGPU Gems 3, Chapter 31. Addison-Wesley, 2007. [34] Rightware Oy. Basemark CL. www.rightware.com/en/ Benchmarking+Software/Basemark%99+CL \n[35] S. Srivastava and S. Gulwani. Program veri.cation using templates over predicate abstraction. In \nPLDI, 2009. [36] B. Steensgaard. Points-to analysis in almost linear time. In POPL, 1996. [37] S. Stone, \nJ. Haldar, S. Tsao, W. Hwu, B. Sutton, and Z. Liang. Accelerating advanced MRI reconstructions on GPUs. \nJ. Parallel Distrib. Comput., 68(10):1307 1318, 2008. [38] S. Tripakis, C. Stergiou, and R. Lublinerman. \nChecking non\u00adinterference in SPMD programs. In HotPar, 2010.   \n\t\t\t", "proc_id": "2384616", "abstract": "<p>We present a technique for verifying race- and divergence-freedom of GPU kernels that are written in mainstream kernel programming languages such as OpenCL and CUDA. Our approach is founded on a novel formal operational semantics for GPU programming termed synchronous, delayed visibility (SDV) semantics. The SDV semantics provides a precise definition of barrier divergence in GPU kernels and allows kernel verification to be reduced to analysis of a sequential program, thereby completely avoiding the need to reason about thread interleavings, and allowing existing modular techniques for program verification to be leveraged. We describe an efficient encoding for data race detection and propose a method for automatically inferring loop invariants required for verification. We have implemented these techniques as a practical verification tool, GPUVerify, which can be applied directly to OpenCL and CUDA source code. We evaluate GPUVerify with respect to a set of 163 kernels drawn from public and commercial sources. Our evaluation demonstrates that GPUVerify is capable of efficient, automatic verification of a large number of real-world kernels.</p>", "authors": [{"name": "Adam Betts", "author_profile_id": "81314480874", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P3856042", "email_address": "abetts@imperial.ac.uk", "orcid_id": ""}, {"name": "Nathan Chong", "author_profile_id": "81548601456", "affiliation": "Imperial College London, Lonon, United Kingdom", "person_id": "P3856043", "email_address": "nyc04@imperial.ac.uk", "orcid_id": ""}, {"name": "Alastair Donaldson", "author_profile_id": "81318493370", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P3856044", "email_address": "afd@imperial.ac.uk", "orcid_id": ""}, {"name": "Shaz Qadeer", "author_profile_id": "81100286660", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P3856045", "email_address": "qadeer@microsoft.com", "orcid_id": ""}, {"name": "Paul Thomson", "author_profile_id": "81549015856", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P3856046", "email_address": "pt1110@imperial.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384625", "year": "2012", "article_id": "2384625", "conference": "OOPSLA", "title": "GPUVerify: a verifier for GPU kernels", "url": "http://dl.acm.org/citation.cfm?id=2384625"}