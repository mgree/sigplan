{"article_publication_date": "10-19-2012", "fulltext": "\n Exploiting Inter-Sequence Correlations for Program Behavior Prediction Bo Wu* Zhijia Zhao* Xipeng Shen* \nThe College of William and Mary, The College of William and Mary, The College of William and Mary, Williamsburg, \nVA, USA Williamsburg, VA, USA Williamsburg, VA, USA bwu@cs.wm.edu zzhao@cs.wm.edu xshen@cs.wm.edu Yunlian \nJiang Yaoqing Gao Raul Silvera Google, Mountain View, USA IBM Toronto Lab, Toronto, Canada IBM Toronto \nLab, Toronto, Canada yunlian@gmail.com ygao@ca.ibm.com rauls@ca.ibm.com Abstract Prediction of program \ndynamic behaviors is fundamental to program optimizations, resource management, and archi\u00adtecture recon.gurations. \nMost existing predictors are based on locality of program behaviors, subject to some inherent limitations. \nIn this paper, we revisit the design philosophy and systematically explore a second source of clues: \nstatisti\u00adcal correlations between the behavior sequences of differ\u00adent program entities. Concentrated \non loops, we examine the correlations existence, strength, and values in enhanc\u00ading the design of program \nbehavior predictors. We create the .rst taxonomy of program behavior sequence patterns. Wedevelopa newformof \npredictors, named sequence pre\u00addictors, to effectively translate the correlations into large\u00adscope, proactive \npredictions of program behavior sequences. We demonstrate the usefulness of the prediction in dynamic \nversion selection and loop importance estimation, showing 19% average speedup on a number of real-world \nutility ap\u00adplications. By taking scope and timing of behavior predic\u00adtion as the .rst-order design objectives, \nthe new approach overcomes limitations of existing program behavior predic\u00adtors, opening up many new \nopportunities for runtime opti\u00admizationsatvariouslayersof computing. Categories and Subject Descriptors \nD.3.4[Programming Languages]: Processors optimization,compilers General Terms Languages, Performance \nPermission to make digital or hard copies of all or part of this work for personal or classroomuseisgrantedwithout \nfeeprovidedthat copies arenot madeordistributed forpro.tor commercialadvantage andthatcopiesbearthisnotice \nand thefull citation onthe .rstpage.To copy otherwise,to republish,topostonserversorto redistribute tolists, \nrequirespriorspeci.cpermission and/ora fee. OOPSLA 12, October19 26,2012,Tucson,Arizona,USA. Keywords \nProgram behavior prediction, Dynamic opti\u00admizations, Dynamic version selection, Just-In-Time Com\u00adpilation, \nSequence prediction, Correlation 1. Introduction Effective prediction of program dynamic behaviors such \nas, loop trip-counts, function calling frequencies, dynamic dependences has been long considered fundamental \nfor program optimizations, resource management, and architec\u00adture recon.gurations (e.g., voltage scaling.) \nItsimportance is underlined by the advents of multicore and cloud comput\u00ading, which critically rely on \nbehavior prediction for adapt\u00ading to their complex process interplays, resource provision, and power \nmanagement. Many kinds of behavior predictors have beenimplemented,in bothhardware andsoftware.Ex\u00adamples \ninclude last value predictors[3, 10, 20], stride pre\u00addictors[10, 22], .nite context predictors [11, 22], \nparameter stride [15], and memorization[21]. Despite design differences, all these existing predictors \nare essentially based on a common philosophy: to exploit the locality of the target behavior. A last \nvalue predictor, for instance, uses the returnvalueofthe previousinvocation of a function as the predicted \nvalue of the current invoca\u00adtion [20] what it exploits is the value locality (i.e., adja\u00adcent invocations \nshare the same return values) of the target function.A stridepredictor,onthe otherhand,exploitspat\u00adtern \nlocality by assuming that the return value changes by thesame amount asit doesin thepreviousinvocationof \nthe function [22]. Locality exploitation has been the essence of almost all runtime predictors. The key \nobservation in this work is that the locality\u00adcentric philosophy is powerful but incomplete. We use dy\u00adnamic \ncode version selection to illustrate its limitations. In this type of optimization, the runtime optimizer, \nat each in\u00ad c Copyright &#38;#169;2012ACM978-1-4503-1561-6/12/10...$10.00 vocationofa function,predictswhichof \nseveralversionsof Speedup 1.6 1.5 1.4 1.3 1.2 1.1 1 0.9 Figure 1. Speedups brought by dynamic version \nselection (detailsin Section 3.1.) the function that have different optimizations applied will work the \nbest in this upcominginvocation[8].Alocality\u00adbased predictor, for its reliance on recent invocations \nof the function as the clue for its prediction, faces two challeng\u00adingscenarios. Figure2(a)illustrates \nthe .rst. Thefunction cluster in the program consumes more than 20% of the to\u00adtal execution time of a \ndata mining program DataMine, but is invoked only once in each run. For lack of recent invo\u00adcations, \nlocality-based predictors are inapplicable1.Thesec\u00adond scenario is when locality is missing, illustrated \nin Fig\u00adure2(b).Thegraphshowsthesequenceofthe bestversions for 20 invocations offunction prepare to parse() \nin one run of an English parsing tool Parser in SPEC CPU bench\u00admarksuites [14].The randomness causeslocality-based \npre\u00addictors to fail the highest prediction accuracy is 33% in all locality-based predictors we have tried(detailed \nin Sec\u00adtion 3.1.) The limitations throttle the power of dynamic op\u00adtimizations signi.cantly, as shownbythegaps \nbetween the locality-based and optimal speedup bars of six utility applicationsin Figure 1. At a deeper \nlevel, the two examples re.ect two inher\u00adent limiting properties that the locality-centric philosophy \nbrings to existing predictors. The .rst is reactivity:Locality\u00adbased predictors are reactive to recent \ninvocations of a func\u00adtionand hencefailsinthe.rst scenario, wherenopriorinvo\u00adcations exist. The second \nis a small prediction scope: Lo\u00adcality often weakens when the scope of prediction increases, because \nthe more code the scope covers, the larger the be\u00adhaviorvariety usually is.At thescopeof thefunctionin \nthe Parser example, the locality is so weak that the predictors fail to work. In this study, we advocate \na new paradigm for predictor construction, the principle of which is to useinter-sequence correlations \nto complement locality to address the limita\u00adtions of existing predictors. Inter-sequence correlations \nare de.ned as statistical cor\u00adrelations among behavior sequences of different program 1The discussion \nhere concentrates on runtime predictors, which are more broadly adopted than the alternative, of.ine \npro.ling-based predictors due toinput-sensitivity as Section 3.2willshow. constructs. A program construct \nhere refers to a program component,such asaloop, function, statement. The behav\u00adiors of two constructs \nhave a strong inter-sequence correla\u00adtion if their sequences show some statistical correlations thatis,thesequencesof \none constructvaryinaway notex\u00adpected on the basis of chance alone but related with the se\u00adquencesof the \nother construct.A simpleexampleisthatloop L2 alwaysiteratesmore or less quadraticallyas much asloop L9 \ndoes. While such correlations sometimes arise from data .ows, they are of a statistical property beyond \ndata .ows. Some may not correspond to a data .ow at alland some may appear nondeterministic. But nonetheless, \ntheyhold statisti\u00adcally(i.e., with alarge probability) across instances of the constructs, as well as \nacross executions of the program. Figure3 showsanexample.Theshown while and for loops reside in two separate \nfunctions in Parser, which try to .ndrespectively an underscore anda decimal pointfrom a string. Both \nfunctions are invoked many times on differ\u00adent stringsin onerun of the program.The two graphsshow the \ntrip-counts or numbers of iterations of the two loops in an execution. Visually, we see large similarity \nbetween the graphs. Quantitatively, in most cases, the two loops have the same trip-counts in their correspondinginvocations, \ndespite that they are invoked far from each other and no clear data .owsexist between them.Manual analysis \nuncovers therea\u00adson for the correlation: Most input strings have no under\u00adscores, while at the same time \ndecimal points often show up only atthe endofasentence(as periods.)So the trip-counts of both loops tend \nto equal the string length. Correlations have been observed before (e.g., [16]). An example is the use \nof correlations among branches for branch prediction [27]. The target of such analysis and pre\u00addiction \nhas been individualinstances (e.g., which branch the next instance of a conditional instruction will \ntake), rather than behavior sequences (e.g., what the trip-count sequence of a loop will look like in \na run.) Focusing on behavior sequences gives large scope and proactivity for behavior prediction, critical \nfor overcoming the limitations of prior locality-centric predictors. It distinguishes the goal of this \ncurrent work from those of prior studies on behavior corre\u00adlations. That focus is also the source of \nmany new challenges. As illustratedbythetrip-count sequencesinFigure3,abehavior sequence typically consists \nof either categorical or numeri\u00adcal behaviors manifested by hundreds or millions of consec\u00adutiveinstancesofa \nprogram construct.Becausedifferent in\u00adstances may be subjectto different contexts,instances in one sequence \nmay show dramatically different behavior values. Generally, when the target of consideration moves from \na single instancetoa sequence,signi.cant complexity and pat\u00adternvariety arise, as illustratedby the sequencesin \nFigure3 and more sequences in Figure 5. Many questions follow, in\u00adcluding whether inter-sequence correlationsexist \nbroadlyin a program, how to deal with intra-and inter-sequence be\u00ad ... ... // clustering 3 CG = cluster(); \nfor each group g of CG for each customer c in g if c.subTime>12 then loadActivity (c); ... ... // processing \n... ... for each group g of CG best version number 2 process (bUseGPU, g); 1 0 2 4 6 8 101214161820 \nnum. of invocation ... ... (a) (b) Figure 2. (a) Skeleton of a data mining program,DataMine, with an \nimportant function cluster() invoked once only.(b) Best versions of function prepare to parse() in Parser. \nint contains_underbar(char * s) { /* finding an underscore */int numberfy(char * s){ Loop 1: while(*s \n!= '\\0') { /* finding a decimal point */ if (*s == '_') return TRUE;Loop 2: for (; (*s != '\\0') &#38;&#38; \n(*s != '.'); s++) s++; ... }} } 25 25 20 20 Trip-count 15 10 Trip-count 15 10 5 5 0 0 0 0.51 1.52 \n2.53 3.54 0 0.51 1.52 2.53 3.54 4 Loop  invocation x 10Loop  invocation x 104 (a) loop1 trip-count \nsequence (b) loop2 trip-count sequence Figure 3. Trip-count sequencesoftwoloopsin Parser. havior variations, \nhow to detect inter-sequence correlations, andhowto use themforlarge-scope behaviorpredictionand dynamic \noptimizations. This paper describes our investigation into these open problems. We .rst examine the existence \nof inter-sequence correlations in large scopes by measuring hundreds of pro\u00adgram executions, billions \nof invocations to thousands of loops, and a large variety of patterns and code span. The measurement \nreveals not only the broad existence of correla\u00adtions among program behavior sequences,but also produces \nthe .rst taxonomy of program behavior sequence patterns, and uncovers some important properties of them. \nWe then analyze the complexities for detecting inter\u00adsequence correlations and capitalization of them \nfor program behavior prediction and optimizations. The investigation leads to a new type of runtime behavior \nprediction, named behavior sequence prediction, which, by complementing lo\u00adcality with correlations, \naddresses thelimitationsofexisting predictorsinbothpredictiontimingandscope.Fortheprob\u00adlemin Figure2 \n(a), for instance,it detectsthata loopin the loadData function correlates with somemajorloopsin func\u00adtion \ncluster in theDataMine program; the correlation makes it possible to replace the reactivity of prior \npredictions with proactivity, predicting the behavior of cluster from load-Data even before cluster is \never invoked in this execution. For theParser inFigure2(b),it.nds thata loopin function maxcost of sentence() \ncorrelates with the loops in function prepare to parse(); using the correlation, it is able to over\u00adcome \nthe randomness large-scoped behaviors often exhibit and predict the best versions of prepare to parse()at \nmost of its invocations. As shown in the third bar group of Figure 1, the speedup almost doubles for \na majority of the programs, equal or close to the optimal. In summary, this work makes four-fold major \ncontribu\u00adtions: It unveils the broad existence and some important prop\u00aderties of inter-sequence correlations. \n It creates the .rst taxonomy of program behavior se\u00adquence patterns.  It proposes a new approach, \nbehavior sequence predic\u00adtion, that systematically exploits inter-sequence correla\u00adtionsfor large-scoped \nprogram behavior prediction.  By taking scope and proactivity as the .rst-order design objectives, the \nnew approach overcomes limitations of existing predictors. It yields 19% speedup through func\u00adtion version \nselection and opens up many new opportu\u00adnitiesforruntime optimizationsatvariouslayersof com\u00adputing. \n In the rest of this paper, we .rst examine the existence of inter-construct correlations andintroducea \nsequence pre\u00addictor to translate the correlations into predicted program be\u00adhaviors (Secton2.)Wethenexploresthe \nusesin programop\u00adtimizations (Section 3.) After a review on related work, we conclude the paper with \na short summary.  2. Inter-Sequence Correlations and Sequence Predictor In this section, we examine \nhow broadly inter-sequence cor\u00adrelations exist and how they can be captured and formulated. Along the \nway, we develop a sequence predictor to system\u00adatically translate the correlationsinto predicted program \nbe\u00adhaviors. The predictor is used for assessing the strength of correlations,but hasamuch broader usage \nas Section3 will show. 2.1 Methodologyfor Data Collection Our measurement focuses on loops,the most \nimportant con\u00adstructs in many programs. We add a pass in LLVM [19] to do instrumentation. The current \nframework works for C programs only. What we collect are the loop trip-count se\u00adquencesofallCprogramsinSPECCPU2006.Our \nmeasure\u00adment covers more than two hundred runs of eleven C pro\u00adgrams withbillionsofinvocationsofthousandsof \nloops. The program, lbm,is not included because allbut one ofits loops have constant trip-counts.We did \nnot include perlbench be\u00adcause we have dif.culties in making extra inputs work for it. Four programs \nin the benchmark suite, gromacs, cactu\u00adsADM, calculix, wrf, are claimed asCprograms,but contain Fortran \ncomponents, and are hence not supported.We addi\u00adtionallyincludea SPEC CPU2000 program, parser, because \nthis study started with it. Table 1 lists all used programs, along with the numbers of non-trivial loops \nthat is, those Table 1. Benchmarks and pattern distributions Prog. time var. #of loops C1 C2.1 Pattern \nDistribution C2.2 C3.1 C3.2 C3.3 milc 40x 124 0.72 0 0.25 0.02 0.01 0 gobmk 9x 590 0.02 0.02 0 0.41 0.5 \n0.05 hmmer 39x 68 0.64 0 0.14 0.09 0.05 0.08 sjeng 58x 92 0 0.07 0 0.14 0.43 0.36 h264ref 81x 371 0.84 \n0.02 0.01 0.05 0.06 0.02 sphinx3 4x 299 0.59 0.02 0.1 0.08 0.04 0.17 bzip2 16x 174 0.5 0 0.03 0.26 0.09 \n0.12 gcc 11x 1884 0.14 0.03 0 0.64 0.08 0.11 libqu\u00adantum 117x 40 0.63 0.04 0 0.25 0 0.08 mcf 25x 37 0.13 \n0.06 0.06 0.19 0.06 0.5 parser 8x 399 0.1 0.04 0.02 0.16 0.24 0.44 that have more than .ve instances \nin any run, and whose trip-countisnota constant.The machineweuseis equipped withIntelXeon E5310 processors,runningLinux \n2.6.22. As a statistical approach, the sequence prediction requires a number of runs on different inputs \nper program to observe statisticproperties.We hence collect someextra inputs based on our understanding \nof the typical usage of the program, learned through reading the source code, document, and default inputs. \nWe .nally have 20 inputs for each program, which stimulate different behaviors. The second column in \nTable 1 reports the ratios between the running times of the longest and shortest executions of the programs, \nre.ecting the large differences in the input set. 2.2 Observations on Behavior Sequences For understanding \ninter-sequence correlations, the .rst step is to understand behavior sequences themselves. This sub\u00adsection \nreports four important properties of behavior se\u00adquences we observe from the 78,340 trip-count sequences \nwe collected. Among these properties, two form obstacles for sequenceprediction, whilethe othertworeveal \nsomeop\u00adportunities. Intra-Run Value Variation Due to the in.uence of invo\u00adcation contexts, most sequences \nexhibit certain value vari\u00adations. Of the 78,340 sequences, only 34% have a single value throughout the \nsequence. On average, the maximum trip-count in a sequence is 3620 times larger than the mini\u00admum. Inter-Run \nValue Variation Most loops exhibit variations in the sequence values and lengths across different runs \nof the program. Of all 4078 loops, only 4% are counter\u00adexamples. On average, across the 20 runs of a \nprogram, a loop shows 6710% changes in the sequence length and 764% changes in the sequence mean value. \nThese two types of variations complicate the analysis of statistical correlations among the sequences, \nand form barriers for sequence prediction. But we also observe two favorable features of the sequences. \nBehavior Sequence Taxonomy After manually examining a large number of sequences, we observe that although \nthe a behavior sequence C1: a single C2: multiple phases C3: others primary pattern C1.1: constant \nC2.1: non-repetitive phases C2.2: repetitive phases C1.2: logarithm  C2.2.1: C2.2.3: \u00a4stu qibtf C1 \notherwiseC1.3: square root C2.2.2: C1.4: linear \u00a4stu qibtf C2.1.1 C1.5: square   C3.1: C3.3: C2.1.1: \nC2.1.2: having a partner otherwise each phase C1 C3.2: otherwise dominant values Figure 4. The taxonomy \nof program behavior sequences. sequencesvary much, manyof themfall into oneofa hand\u00adful categories in \nterms of the overall shape or pattern. It sheds an insight for using a divide-and-conquer strategy to \ntackle the sequence complexities: If we can categorize se\u00adquences into a handful classes, we may be able \nto design customized abstracts and solutions for each category. After going through the sequences, we \ncreate a hierarchi\u00adcal taxonomy of program behavior sequences, as shown in Figure4 and illustrated in \nFigure 5. The design of the taxonomy is based on the complexity of the sequences (with predictability \nin mind.) There are three top classes in the taxonomy. A sequence belongs to class C1 ifit canbe modeledwith \nasingleprimary pattern. The type of the primary pattern then puts the sequence into C1.x, where x can \nbe 1, 2, 3, 4, 5, corresponding to the .ve types of primary patterns we concern in this work: constant, \nlogarithm, square root, linear, and square relations with the occurrence number of a behavior instance. \nThese .ve kinds of relations cover most of the patterns we have observed. Figure 5 (a) exempli.es a loop \ntrip-count sequence of a linear pattern.Forasequencein C1, predicted values of the parameters of its \nprimary pattern are enough for predicting the whole sequence. A sequence belongs to class C2 if the sequence \nconsists of multiple phases. Depending on whether the phases are repetitive, C2 isfurther divided into \nC2.1 and C2.2. For a sequence with non-repetitive phases(C2.1), if ev\u00adery phase of it belongs to some \nclass in C1, the sequenceis essentially a combination of multiple C1 sub-sequences and can be hence represented \nby a series of parameters of the cor\u00adresponding primary patterns. Such sequences form the sub\u00adclass C2.1.1, \nexempli.ed by Figure 5 (b). Other sequences in C2.1 constitute C2.1.2,exempli.edbyFigure5(c). The modeling \nand prediction of C2.1.2 are similar to C3, dis\u00adcussed laterinthissection. A sequence with repetitive \nphases(C2.2) can be easily predicted onceitsvaluesin the .rst phase are known. Based on the patterns \nof its .rst phase, it can be further classi.ed into three sub-classes. It falls into C2.2.1 if its .rst \nphase belongs to C1,exempli.edby Figure5(d).Inthis case,that phase can be modeled as a sequence in C1. \nThe sequence falls into the second sub-class, C2.2.2, if its .rst phase can be modeled as a sequence \nin C2.1.1 that is, its .rst phase itself contains multiple non-repetitive sub-phases and each of them \nis in a primary pattern. The .rst phase can be then modeled as a C2.1.1 sequence. Figure5(e) shows an \nexample of C2.2.2. The C2.2.3 class consists of all C2.2 sequences that belong to neither C2.2.1. The \nclass C3 subsumes all sequences that cannot .t into either C1 or C2. These sequences usually show extreme \nir\u00adregularity. They cannot be modeled with a primary pattern, a combination of patterns, or phases. However, \nfor some of them, their corresponding program constructs happen to have one or more partners in the program; \nhere, a partner of a program construct refers to another construct in the pro\u00adgram whose sequences always \nshow high similarity with those of this construct. For instance, the two loops in Fig\u00adure3 arepartnersofeachother.These \nsequencesformasub\u00adclass C3.1. They can be predicted from one to another, de\u00adspite their irregularities.Some \nother sequences have several (e.g. less than8) dominantvalues, asexempli.edby Figure5 (f). Although the \ntime order in which these values occur ap\u00adpears random, knowing these dominant values may be still helpful \nin certain scenarios. They form the class C3.2. The other sequences in C3 constitute C3.3, which are \nconsid\u00aderedas unmanageableinthiswork. Figure5(g)showssuch an example. The sequences collected in our \nexperiment can be well mapped into the taxonomy.The seven rightmost columnsin Table 1 show the distribution \nof the loop sequences among the classes. Such categorization lays the foundation for studying inter-sequence \ncorrelations, as the following sec\u00adtions will show. Cross-Run Stableness of Patterns The last but not \nleast property of the behavior sequences we observe is that while the sequences of a program construct \nmay vary much across different runs in value and length, but not in shape or pat\u00adtern. Figure 6 shows \nthe .ve sequences of three loops in three programs. The sequencesofdifferentloopsshow large differences \nin shape. However, they all clearly exhibit cer\u00adtain shape stableness in the sense that the sequences \nin dif\u00adferent runs appear to have similar shapes and patterns, even though the differences in the program \ninputs cause consid\u00aderable variations in the sequence length and height of the curves. These loops are \nnot exceptions. We say that a loop isa stable loop if allitssequencesfallintoasingle category other than \nthe unmanageable category C3.3 in the taxonomy. Of all the 11 programs, seven have more than 88% of their \nloops being stable. Sphinx3 has 83% stable loops, and the rest all have more than 50% stable loops (64% \nfor sjeng, 56%for mcf, and 50%for parser.)Adetailed analysis shows that the main reason for therelativelylow \npercentageof the threeprogramsis not unstableness across runsbut the irregu-larityofthesequences.RecallallloopsinC3.3 \nareirregular. They are all automatically labeled as non-stable loops, even 100 5k 15 15 10 trip-count \n10 trip-count 80 60 40 trip-count 4k 3k 2k trip-count 5 5 20 1k 0 0 0 0 0 0 10 20 30 40 50 0 500 100015002000 \n0 0.5 1 1.5 2 2.55 10 15 50 num. of invocations num. of invocations num. of invocations x 104num. of \ninvocations (a) C1.4 (libquantum) (b) C2.1.1 (h264ref) (c) C2.1.2 (gobmk) (d) C2.2.1 (mcf) 7 60 200 6 \n trip-count trip-count 40 trip-count 30 20 10 1 0 0 0 24 4 0 0 1000 2000 3000 4000 0 100 200 300 \n400 500 x10 num. of invocation num. of invocations num. of invocations (e)C2.2.3 (milc) (f)C3.2 (gcc) \n(g) C3.3(gcc) Figure 5. Sequence categories(givenin Figure4)and someexamplesthat appearinthe collectedlooptrip-count \nsequences of the programs (namesin brackets) listedinTable 1. 20 20 2020 5 4 3 2 trip-count trip-count \n10 trip-count 10 trip-count 10 trip-count 10 trip-count 10 0 0 10 20 num. of invocation 30 0 0 10 20 \nnum. of invocation 30 0 0 20 num. of invocation 40 0 0 20 num. of invocation 40 0 0 20 40 num. of invocation \n60 400 1000 (a) A loop in h264ref (line 1502 of mbuffer.c) 1000 1000 200 trip-count 500 trip-count 400 \n200 trip-count 500 trip-count 500 0 0 50 num. of invocation 100 0 0 50 100 num. of invocation 150 0 \n0 50 100 num. of invocation 150 0 0 100 num. of invocation 200 0 0 100 200 num. of invocation 300 10000 \n2 x 10 4 (b) A loop in h264ref (line 79 of memalloc.c) 10000 2 x 10 4 4 x 10 4 trip-count 5000 trip-count \n1 trip-count 5000 trip-count 1 trip-count 2 0 0 500 1000 num. of invocation 1500 0 0 500 1000 num. of \ninvocation 1500 0 0 1000 num. of invocation 2000 0 0 1000 2000 num. of invocation 3000 0 0 2000 4000 \nnum. of invocation 6000 (c) A loop in mcf (line 52 of pstart.c) Figure 6. Trip-count sequences in .ve \ndifferent runs. though stableness can often be seen in these loops: All their sequences tend to be irregular \nin all runs. The cross-run stableness reinforces the divide-and-conquer ideafor addressing sequence complexity, \nin the sense thatif we treat program constructs category by category, we need notworry aboutthe cases \nthata constructmay belongtodif\u00adferent categories in different runs. Moreover, the cross-run stableness \nsuggests the promise of using of.ine training for construct classi.cation and correlation analysis. \n 2.3 Measuring Inter-Sequence Correlations After achieving some understanding of behavior sequences, \nwe are ready to examine the correlations among the se\u00adquences. One option for measuring the sequence \ncorrelations between two constructs is to compute the standard correla\u00adtion coef.cients between the behavior \nsequences of the two constructs. It cannot meet our needs because these coef.\u00adcients capture only certain \nkinds of relationships (e.g., linear relationship by Pearson s coef.cient or monotonic relation\u00adship \nby Spearman s coef.cient)between the two construct s behaviors. Recall that the main purpose of studying \nthe cor\u00adrelation is to enhance behavior prediction, useful relation\u00adshipsfor which are apparently not \nonly linear ones. Considering the ultimate usage, we choose to use predic\u00adtive capability to measure \nthe strength of correlations. Con\u00adstructA strongly correlates with constructBifA s behavior sequences, \nalthough showing certain variations, can be ac\u00adcurately predicted from B s behavior sequences. If the \npre\u00addiction accuracy of construct A s sequences is higher from construct B than from construct C, we \nsay A has a stronger correlation withB than with C. Thismetricsuggeststhatapredictor needstobebuiltbe\u00adtween \nconstruct sequences to get the prediction accuracy, and the better the predictor is, the closer the obtained \nac\u00adcuracy re.ects the strength of the correlation. Creating the best predictor is dif.cult if ever possible. \nBut an important insight is that because the best predictor gets no worse ac\u00adcuracythana non-perfect \npredictor,a positive conclusion on the existence of a correlation by a non-perfect predictor must be \ncorrect. Drivenby thisinsight, wedevelop thefollowing correlation-based predictor.  2.4 SequencePredictor \nWe name the predictor asequence predictor becauseits out\u00adput is a behavior sequence rather than the value \nof the next instance as mostexisting predictors output. Themuchlarger scope of prediction is a result \nof its exploitation of inter\u00adsequence correlations. Although the predictoris designed for examining theexistenceof \ninter-sequence correlations,ithas a much broader usage as Section3 will show. Thedesignofthepredictoris \nbasedonthetaxonomyand cross-run pattern stableness presented in Section 2.2. As Fig\u00adure 7 shows, the \npredictor is built by of.ine training and is used for online prediction. The of.ine training is a synergy \nof intra-sequence pattern recognition andinter-sequence cor\u00adrelation analysis. It happens on the sequences \ncollected on many training runs. It .rst recognizes the sequence pattern of a loop, and represents its \nsequences with a few pattern parameters (Section 2.4.1.)Working onthe conciserepre\u00adsentation, it then \nuses statistical analysis to .nd the con\u00adstructsrelated with one another, andbuildspredictive models \namongthem (Section2.4.2.)The modelsleadtolarge-scope sequence predictions during run time (Section 2.4.3.) \n 2.4.1 PatternRecognition The goal of this step is to categorize a construct into one of the classes \nin the pattern taxonomy, and then represent each sequence with a feature tuple. The tuple contains the \nclass number of the construct and its corresponding pattern parameters. We start by explaining the concept \nof feature tuples. Feature Tuples A feature tuple offers a way to concisely represent a sequence. It \ncontains all important characteristics ofasequence such thatfromit,the sequence canberegener\u00adated easily(withfewexceptions.)Every \nnon-C3.3 sequence can be represented by afeature tuple: (c, .p), where, c is the number of the leaf class \nin the taxonomy tree that the se\u00adquence belongs to, and p.is the pattern vector, consisting of the parameters \nof the pattern of that class. Table 2 summa\u00adrizes the format of the pattern vectors of each major class, \nwhere, f (n) is one of the primarypatterns listedin category C1, {0, logn, n0.5, n, n2}, and n is the \ninstance number.For example,thepatternvectorsforthe sequenceinFigure5(a) is (17, 0, 1), where, according \nto the second column in Ta\u00adble 2, 17 corresponds to the length of the sequence, and the other two elements \ncome from the pattern of the sequence, trip count = number of invocation. Apparently, with such a vector, \nwe can easily regenerate the entire sequence. In the samevein,the patternvectorforthe sequencein Figure5(b) \nis computed as (9, 1.4, 32, -1, 20, 1.1, 22, 0), where the .rst four elements correspond to the .rst \nphase in the sequence with the second number, 1.4, indicating that the phase is in subclass C1.4having \na linear pattern; the other elements cor\u00adrespond to the second phase, which has a constant pattern. \nRecognition Algorithm Figure 8 presents the algorithm for determining feature tuple, (c, .p). Each iteration \nof the outer loop of the function seqClassify processes one con\u00adstruct. It .rst calls seqClassify to \nclassify each sequence of the construct, and then checks whether all sequences of a loop belong to a \nsingle class other than C3 (by invoking the function hasDiffClass()).Ifso,it has .nishedprocessing that \nconstruct. Otherwise, it checks whether the construct be\u00adlongs to C3.1 by trying to .nd a partner of \nit through com\u00adparisons between its sequences and the sequences of other loops whose IDs are smaller \nthan this loop s. (Loop IDs are givenin orderoftheir .rstinvocationsina typicalrun; obser\u00advationsonaloopwithalowerID \nmaybe henceusedtohelp predict the behavior of a loop with a higher ID as shown in Section 2.4.3.) If \nit .nds a partner for a construct currently labeled as C3.2, it relabels it to C3.1; we give C3.1 prece\u00addence \nbecause the partnership usually gives better predic\u00adtion accuracy according to our experience. Other \nconstructs are labeled to C3.3. We brie.y explain how a sequence is classi.ed by the function seqClassify \n. The function .rst applies regression to the sequence by invoking the curveFit function, which uses \nthe standard curve-.tting method [13] to try to .t the sequencewith eachofthe.ve primary patterns(i.e., \nthe.ve patterns in class C1.)Ifthe .tting error ofthe best of the .ve is smallerthana threshold (e.g., \n10%), thesequenceis con\u00adsidered asa sequencein the corresponding sub-classinC1, and the .tting result \nis taken as its pattern vector. Otherwise, seqClassify tries to see whether the sequence has repetitive \nphasesbyinvoking thefunction repPhaseTest. At a positive answer, it further checks the patterns of the \n.rst phase to determine which sub-class of C2.2 suites the sequence. At a negative answer, it invokes \nthe function phaseDet to deter\u00admine whether the sequence consists of non-repetitive phases, and identi.es \nthe patterns in each phase if so. Otherwise, Pg jof Online   Figure 7. The three-step approach to \nenabling sequence prediction. Table 2. Formats of pattern vectors class C1.1 C1.5 C2.1.1 C2.1.2 C2.2.1 \nC2.2.2 C2.2.3 C3.1 C3.2 C3.3 format (l, p1, p2) (l1, c1, p11, p12, l2, c2, p21, p22, \u00b7 \u00b7 \u00b7 ) - (l, n, \nc1, p1, p2) (l, n, l1, c1, p11, p12, l2, c2, p21, p22, \u00b7 \u00b7 \u00b7 ) (l, n) (p) (l, v1, v2, \u00b7 \u00b7 \u00b7 ) - note \nl: seq. length; p1 + p2 * f (n). li: length of phase i; ci: class of phase i; pi1 + pi2 * f (n) for phase \ni - l: phase length; n: # of phases; c1: class of phase 1; p1 + p2 * f (n) for phase 1. l: phase length; \nn: # of phases; ci: class of phase i; pi1 + pi2 * f (n) for phase i. l: phase length; n: # of phases. \np: partner ID. l: seq. length; vi: the ith most frequent value. - 2 Note: f (n) .{0, logn, n0.5 , n, \nn } it checks whether the sequence contains several dominant values through the function domValueDet, \nwhich simply ex\u00adamines whether the n most frequent values in the sequence together cover over 99% of \nthe sequence. If so, it labels the sequence with C3.2. The proper value of n depends on the use of the \nprediction. Our experiment sets it to seven. The rightmost column of Figure8outlines the two phase\u00addetection \nalgorithms. Unlike many existing algorithms[23, 24], the two algorithms distinguish the detections of \nrepeti\u00adtive and non-repetitive phases. It is initially for the need of sequence prediction, but later \nwe .nd such distinction also simpli.es the detection problem. The function repPhaseTest uses a simple \nbut effective approach we design to detect repetitive phases. Its basic idea is that for a sequence consisting \nof K repetitions of a phase, we should get k mutually similar segments if we cut the sequence into k \neven-sized partitions as long as k is an integer divider of K. The dif.culty is to .nd a good k value. \nAn insight taken by the function is that checking prime numbers suf.ces because any integer is the product \nof a series of prime numbers. If all prime numbers below a number (say m) are checked, it is guaranteed \nto cover all possible values of K whose smallest prime divider is no greater than m. The function phaseDet \nis a simpli.ed version of the wavelet-based phase detection algorithm proposed by Shen and others[23]. \nThe second-level differentiation is equiva\u00adlent to a second-level Haar wavelets transformation. It helps \nexposethe drastic form changesin the sequence, which typ\u00adically suggest phase boundaries(asillustratedby \nFigure5 (e)). 2.4.2 ModelConstruction After the sequences are represented withpattern vectors, the second \nstep for building sequence predictors is to construct predictive models among the pattern vectors of \ndifferent con\u00adstructs. It uses a regression framework, working on the pat\u00adternvectorsof allthe sequencesinthe \ntrainingset.Figure9 outlines the high-level algorithm. At the core is a function corRegress , which conducts \nregression between X and y, and reportsthe best model and regression error.The regres\u00adsion models it \ntries include the standard LMS for linear re\u00adlations and Regression Trees for non-linear relations two \n // inputs: // S: all sequences // S.nseq[i]: # of seq. of behavior i // S.seq[i]: set of seq. of behavior \ni // B: # of behaviors // output: rst // rst[i][j].class: class of the jth seq. // of behavior i // rst[i][j].pv: \npattern vector of the // jth seq. of behavior i function seqClassify(S, B, &#38;rst) for (i=0; i<B; i++) \n// classify each seq for (j=0; j<S.nseq[i]; j++) seqClassify_(S.seq[i][j], rst[i][j]);  end for if \n(hasDiffClass(rst[i]) || rst[i][0].class== C3.2 || rst[i][0].class==null) class3.add (i); end if end \nfor // deal with class 3 behaviors for (i=0; i<class3.size; i++) b = class3.beh[i]; for (j=0; j<i; j++) \n if (isPartner(S.seq[i], S.seq[j]))  rst[b].class = C3.1 ; rst[b].pv = j; end if end for if (rst[b].class==null) \n rst[b].class = C3.3 ; end if end for end // classify s to C1, C2.2, C2.1, C3.2, or others function \nseqClassify_(s, &#38;rst) // C1 rst1 = curveFit(s); if (rst1.minErr < H1) rst.class = C1. +rst1.bestModel; \nrst.pv = getPatternVec(rst1); return;  end if // C2.2 rst2 = repPhaseTest(s); if (rst2.yes) rst3 = \ncheckPhase1(s, rst2); rst.class = C2.2. +rst3.class; rst.pv = getPatternVec(rst2, rst3); return;  end \nif // C2.1 rst4 = phaseDet(s); if (rst4.yes)  rst5 = modelEachPhase(s,rst4); rst.class = rst5.pure? \nC2.1.1 : C2.1.2 ; rst.pv = getPatternVec(rst4, rst5); return; end if // C3.2 rst5 = domValueDet(s); \nif (rst5.yes) rst.class = C3.2 ; rst.pv = rst5.values; end if end // test if s consists of repetitive \nphases function repPhaseTest(s) rst.yes=0; rst.phN = 1; for (i=0; i<PR.size; i++) dif[i] = avg disparity \namong PR[i] even-size partitions of s; rdif[i] = dif[i] normalized by mean value of partitions  end \nfor n > \u00a4oeNjo)ejg*< if (dif[m]<H2 || rdif[m]<H3) rst.yes = 1; k = s.size/m; // recursively minimize \nphase granularity rst1 = repPhaseTest(s[0:k-1]); rst.phN = m*rst1.phN; end if return rst; end // test \nif s consists of clear phases function phaseDet(s) phaseN = 0; dif2 = second level differentiation of \ns m = mean(dif2); d = std(dif2); for (i=0; i<dif2.size; i++) if (dif2[i] > m+d) phaseBounds[phaseN] = \ni; phaseN++; end if end for end Figure 8. Algorithm for sequence pattern recognition. // A: the training \ndata set for each construct b for each construct b that b .id<b.id for each dimension d of b s pattern \nvector Let y be a vector containing all values of d in A Let X be a matrix containing all pattern vectors \nof b in A  Do regression: corRegress(y, X, err, model); if (err < minErr) minErr=err; b.partner[d]=b \n; b.model[d]=model; end if end for end for end for Figure 9. Sequence correlation detection. ef.cient \nmodels with broad applicability and interpretable results [13]. The regression error is calculated through \n10-fold cross validation [17]. It uses 90% of X and y for model construc\u00adtion and the remaining 10% for \ntesting. It repeats this pro\u00adcess 10 times, with a different partition between the two sets eachtime. \nTheaverage erroristaken as theregression error for that model. As shown in Figure 9, after this step, \neach construct gets a list of predictive models, which each corresponds to one element in its pattern \nvector. In addition, the construct gets a list of partners. The ith partner is a construct whose pattern \nvector strongly correlates with the ith dimension of the pattern vector of the current construct.  2.4.3 \nRuntimePrediction Using the constructed predictors for runtime prediction is straightforwardfor most \nclasses of sequences.Forinstance, suppose the algorithm has recognized that a loop L1 is the partner \nof another loop L2 in terms of their sequence corre\u00adlation.Letf () represent thepredictive modelmappingfrom \nthe pattern vectors of L1 to those of L2. In a new run, after a numberofinvocationsofL1,itssequencepatternvectoris \ncomputedfromthe observed iterationsof thoseinvocations. Putting the vector into the predictive model \nf () then pro\u00adduces the pattern vector of L2. From that vector, the whole sequence of L2 s behavior can \nbe immediately generated. The prediction exhibits a large scope and does not need to waitforthetarget \nconstructto occurinthe currentrun,which are appealing properties over locality-based predictors. Some \nspecial treatment is needed during the sequence prediction for classes C2.1.2, C2.2.3, C3.2, and C3.3 \ndue to their complexities. For sequences in class C2.2.3, the prediction starts after the .rst phase \n.nishes. Based on the repetitiveness of the phase, the prediction is simply n - 1 copies of the .rst \nphase, where n is the number of phases, predicted using the of.ine constructed predictive models. For \nthese sequences, the proactivity is not complete, but usuallyhigh still. For sequences in class C3.2, \nas the order in which the dominant values appear is hard to determine, the dominant values are predicted,but \nno sequences are generated.For se\u00adquences in classC2.1.2 orC3.3, no predictionis conducted. From Table \n1, we see that for most programs, only a small portion ofloopsfall into those special classes. Theruntimepredictionofpattern \nparametersistypically lightweight, involving just the computation of a linear ex\u00adpression oratraverseofa \nseverallevel regressiontree.Com\u00adparedto many runtime optimizations(e.g.,JITcompilation), the overhead \nis negligible.  2.5 Prediction Accuracy: Con.rmingExistence of Correlations We apply the sequence predictor \nto the loop trip-count se\u00adquences of the programs mentioned in Section 2.1. A rea\u00adsonable accuracy will \nhelp con.rm the existence of inter\u00adsequence correlations. Before detailing the result, we explain the \ncalculation of the accuracy of a predicted sequence. One option is to com\u00adpare,fromlefttoright,eachpairof \ncorresponding instances in the predicted and true sequences. But if one instance is missing in the predicted \nsequence, the misalignment may make the following instances appear erroneous even if they are correctly \npredicted. In practice, some difference in se\u00adquence length may notimpair the use in optimization much. \nWe choose to use real sequence length during the genera\u00adtion of a predicted sequence, and present the \naccuracy of sequence length prediction separately. Ten-fold cross vali\u00addation is used so that testing \nruns differ from training runs. Figure 10 (a) reports the average accuracies. The accu\u00ad 1 n racy of a \npredicted sequence is de.ned as (1 - n i=1 |v ' (i) - v(i)|/max(v ' (i),v(i))), where n is the length \nof the true sequence, and v(i) and v ' (i) represent the values of the i th instance in the predicted \nand true sequences respec\u00adtively.The dominator ensuresa 0-100% range.The .rsttwo bars of each benchmark \nshow the accuracy of the predicted sequence lengthand pattern parameters. Most programs are above 90% \non both,including thelargest software, gcc. The program sjeng gives the lowest accuracybecause of its \nlarge number of branches and recursivefunction calls. The third bar of each benchmark shows the accuracy \nof predicted sequences. Most programs are above 85%; the overallaverageis 91%.Toputthe result into context,weap\u00adply \na pure locality-based predictor to predict the sequences. It generates the sequence of a loop purely \nbased on its earlier invocations. The design maintains a prediction scope com\u00adparable with the sequence \nprediction for a fair comparison. The accuracy is shown by the rightmost bar of each bench\u00admark. On average, \nthe accuracy is 76%, 15% lower than the sequence predictor s result. Becausewehave20 inputsforevery \nbenchmarkandthe prediction accuracies on their corresponding runs are often different from one another, \nit is worthwhile to examine the distributions of the prediction accuracies. We show the dis\u00adtributionsthrough \nboxplotsinFigure10(b).In each boxplot, the short line segments at its top and bottom show the accu\u00adracy \nrange, and the middle square covers the medium 50% of the accuracies. A large gap shows up between the \ntwo boxplots of every benchmark. Statistical signi.cance test (T\u00adtest)gives all zero p-values, further \ncon.rming the consis\u00adtent, signi.cantly better accuracies from sequence prediction than from locality-based \nprediction, indicating the value of the correlations exploited by the sequence predictor. The prediction \naccuracy by the sequence predictor re\u00ad.ectsthe broadexistenceofinter-sequencecorrelations.But two important \nquestions remain open. Are the correlations strong enough for actual uses in program optimizations? Is \nthe sequence predictor effective enough for translating the correlations into what the optimizations \nneed? We imple\u00adment two uses to answer these questions, as described next.  3. Uses of Correlation-Based \nSequence Predictors In this section, we .rst use dynamic function version se\u00adlection to demonstrate the \nusefulness of the correlation\u00adbased prediction for optimizing some large applications.We thenshow its \nusesfor loopimportance estimation on SPEC CPU2006 benchmarks written in C. Section 4 will brie.y discuss \nuses in other contexts. 3.1 Dynamic FunctionVersion Selection Dynamic function version selection is a \nkind of runtime op\u00adtimization. For a given function, multiple versions are gen\u00aderated during of.ine compilation. \nThey perform well in dif\u00adferent scenarios. Dynamic version selection predicts the best version for an \nupcoming invocation ofthefunction. This op\u00adtimization is especially usefulfor utilityprograms whose in\u00adput \nconsists of a sequence of different tasks, such as a com\u00adpiler compiling many different functions, a \nparser parsing manydifferent sentences. These tasks stimulate different be\u00adhaviors of the program, hence \ncreating the needs for dynamic version selection. In ourexperiments, we use thefeedback-driven compila\u00adtionofGCCto \ncreatethreeversionsforthe functionsin each of the benchmarks. The feedback-driven compilation uses the \npro.le of a training run of a program to reoptimize the program. The three versions are the results of \nthe feedback\u00ad seq. pattern sequence by sequence by length parameter sequence pred locality-basd pred \n 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 Prediction Accuracy ing thesumofits predictedtrip-count sequence.We \ncall the vector the predicted behavior signature of the coming call. There is a behavior signature of \nfoo prepared for each ver\u00adsion duringthe creationofthethreeversions.By comparing the predicted behaviorsignature \nwiththem,the dynamicver\u00adsion selection procedure selects the version whose signature is closest to the \npredicted one. The closeness is de.ned by .5 ' '' |v - vi |/(1 + v ), where v and vi are the elements \ni=1 i ii of the predicted and reference signatures. The normalization ' by (1 + v ) reduces biases \nby the absolute trip-count values i (adding one avoids division by 0.) We compare with two alternatives. \nThe .rst is static compilation at the highest optimization level. The second is an implementation of \na prior dynamic version selection scheme [8], which uses locality-based prediction, working as follows. \nDuring the initial several invocations of a func\u00adtion, the technique tries a different version at each \ninvoca\u00ad (a) Average accuracy of the predicted sequence lengths and tion. By comparing the running times \nof the several invo\u00ad pattern parametersby the sequence predictor(lefttwo bars), cations, it selects \nthe version with the shortest running time and average accuracy of predicted sequences (right two and \nuses that version for a number of upcoming invocations bars.) ofthe function.For better adaptation, \nthe sampling of differ\u00ad sequnce-pred Prediction Accuracy 1 0.8 0.6  ent versions happen periodically. \nWe experiment with four sampling periodlengths:6,12,24, andallinvocations. We next report our .ndings \non six utility applications. They mostly come from real-world applications with thou\u00adsands or hundreds \nof thousands of lines of source code. They are representatives of the domains ranging from natural lan\u00ad \n0.4 guage processing, program compilation, scripting language interpretation, to data mining. One of \nthem, DataMine,is a 0.2 CPU-GPU mixed program and has some special aspects for version selection. We \npostpone it to the end of our discus\u00ad 0 sion. In the experiments, we focus on only one or several major \nfunctions that consume a signi.cant part of the total running time of the program. The reported performance \nis the average of .ve runs on the Xeon E5310 machine with (b) Boxplots showing the distributions of the \nsequence ac-GCC 4.4.1 installed. Figure 11 summarizes the result. Sleator-Temperley Link Parser v2.1 \n(Parser) According to SPEC2K web site, this program is a syntactic parser of curacies. Upper box: by \nsequence predictor; lower box: by locality-based predictor. Figure 10. Average accuracy(a) and statistical \ndistributions of the accuracies of the predicted sequences(b). driven compilation when three different \ntraining runs on dif\u00adferent inputs are used. We use our correlation-based prediction to help dynamic \nversion selection. The idea is to take its predicted loop se\u00adquences asindicationsof the behaviorsofthe \nupcoming in\u00advocation of a function. For example, there is a call to func\u00adtion foo in a utility program \nand this function contains .ve loops.To apply dynamicversion selectionto that call,a call to a version \nselection procedure is inserted before it. Using the predictive models built of.ine, the procedure predicts \nwhat trip-count sequences the .ve loops of foo will pro\u00adduce in the coming invocation. It then derives \na 5-element vector with each element corresponding to one loop, equal-English, based on link grammar \nwith about 60000 word forms. It has 11,391 lines of C code. We use its batch\u00adprocessing mode, in which, \nan input contains a number of sentences for parsing. Different sentences trigger different behaviors \nof the functions in the program. The three train\u00ading .les we use each contain 2, 5, 20 sentences respec\u00adtively;althoughthesentencesin \none .le areofsimilar length and complexity, the sentences in different .les differ signi.\u00adcantly.The \ntesting inputwe use consistsof 10,000 sentences of various length and complexity. The parsing of one \nsentence goes through a few stages. The correlationswe .nd are between loopsinthe .rst stage and those \nin the other stages. The dynamic version selec\u00adtionisontheselaterstages.The correlation-basedprediction \nyields 89% average prediction accuracy on the signatures. Because .nding the version with the closest \nsignature is a qualitative problem,versionselection hasa quitelarge error tolerance. The prediction gives \na perfect version selection accuracy, producing a speedup of as much as 17.7% com\u00adpared to the best static \ncompilation result. In comparison, thelocality-based dynamicversionselectionsfail to .nd the best versions \nat more than halfof the invocations, producing 10.9% speedup. GNU Compiler Collection v3.2 (GCC) Asa \nwidelyadopted compiler, GCC, written in 517K lines of code, takes C or C++ source code as its input and \ncompiles the contained functions one by one. The compilation of different functions may trigger the invocations \nof different procedures in GCC and stimulate different behaviors of GCC.Thethree training inputs we use \nconsist of loop-intensive, branch-intensive, and straight-line code respectively. The test input contains \n40,299linesofC codewith 322 functionsofvarious kinds. The compilation of a function goes through a number \nof compilation passes. The correlations we .nd are between theloops in the earlier passes and later passes. \nThe accuracy for function signatures is 90% and the best version can be found for 93%of allinvocations.Theresultingspeedupis \nas much as 9.4% over the static compilation result, 4.4% more than the locality-based dynamic version \nselections. SQL Database Engine SQLite v3.7.9 (Sqlite) Sqlite is one of the most widely deployed SQL \ndatabase engines in the world[1]. It has 110K lines ofCcode.We use three types of select queries as training \ninputs, with some containing a simple single selection condition, some containing multiple conditions \nwith AND and OR relations, some contain\u00ading conditions involving numerical comparison and compu\u00adtations. \nThe differences in the three types of queries trigger the invocation of different parts of Sqlite. As \na result, the threeversionsproducedby thefeedback-driven compilation of GCC are quite different from \none another, each suiting one type of the queries much better than the others and the statically optimizedversion.Comparedtothestaticallyopti\u00admized \nversion, the three versions show 34 65% speedup on the corresponding types of queries. The query .lefortesting \nconsistsof 10,000 queries with all three types of queries included. The sequence prediction\u00adbased dynamic \nselection is able to choose the best versions forallthe queries,producinganaverage speedupofasmuch as \n52%, 12% more than the locality-based version selection. Bzip2 v1.0.3 and Sphinx-3 Bzip2 is a widely \nused data compressor in 8,293 lines of code. Its inputs may be .les of different kinds. We use an image, \na novel, and a set of XML .les as training inputs to create three versions. We use a .le in rich format \ncontaining all three kinds of content asthe testing input.Ourversionselectionfocuses on a buffer compression \nfunction. The locality-based selection give similar performance as our approach, both less than 1.8% \naway from the static optimization result. We .nd that eventhe perfectversion selectiongives only2% speedup.An \nSpeedup 1.6 1.5 1.4 1.3 1.2 1.1 1 0.9  Figure 11. The performance broughtbylocality-based and correlation-based \nversion selection. Baseline is static opti\u00admization result. analysisofthe source codeshowsthatthe programseparates \nthe input into equal-sized buffers. The compression on the buffers behave quite similarly, regardless \nthe type of input, hence the insigni.cant speedups. Sphinx3 is a widely known speech recognition system \nwith 23K linesof code.Itshares similar properties as Bzip2, showing insigni.cant speedup in all cases. \nCustomer Relationship Management (DataMine) DataMine is a program we derived for customer relationship \nmanage\u00adment.Figure2(a)has outlineditscodeskeleton.ItusesK\u00admeansto cluster customers intoa numberof groups.Itthen \nloads the customer activities if necessary and processes the customer information of each group to extract \nsome high\u00adlevel knowledge about the customer group. The version selection for this program differs from \nthe other applications we have described. This program is in\u00adtendedto runonheterogeneous computingsystems.Itcomes \nwith two versions of the central workload-handling function process(), one for CPU, the other for Graphic \nProcessing Units(GPU), determinedby the booleanvariable bUseGPU. The dynamic version selections try to \nselect the appropriate version for each invocation of the function. The selection mainly depends on the \namount of computation involved in the workload handling, determined by the size of the cluster but also \nthe fraction of the customers in the cluster that have subscribedthewebserviceformorethan12 months.The \nre\u00adlation between the amount of computation and the best ver\u00adsion to use has been determined through \nof.ine pro.ling. The detected correlations are between the loops in the clustering(includingaloop inthe \nloadActivity function) and the loops in the core function process (CPU version). The predicted signature \nof that function re.ects the amount of computation in the nextinvocation of the corefunction.The testhas \n1000 clusterswithsizerangingfrom15to 354,286. The accuracy of signature prediction is 100%, leading to \nperfect version selection. The speedup is 8.5% over the all-CPU execution and 7.6% over the all-GPU execution, \nsig\u00adni.cantlyoutperforming all locality-based versions. (In Fig\u00adure 11, the all-GPU is used as the baseline.) \n Importance Accuracy 1 0.8 0.6 0.4 0.2 0 Figure 12. Prediction of theimportance of loops. Runtime Overhead \nThe overhead of correlation-based version selection consists of three parts: recording the trip\u00adcount \nsequence pattern parameters of some predictor loops, applying the of.ine built models (a linear function \nor a several-level decision tree) to produce the predicted trip\u00adcount sequence of the target loops, and \nreducing the se\u00adquences to signatures. The second and third parts involve only a few, simple calculations \nand have negligible overhead compared to the large number of operationsin the optimiza\u00adtiontarget.The \namountofoverheadof the .rst part depends on the types of the predictor loops. If their sequences are \ncomposed of primary patterns, only a few samples (i.e., trip\u00adcounts) are suf.cient for computing their \npattern parame\u00adters (e.g., two samples are enough for a linear pattern.) For Class C3.1, however, the \nwhole loop sequence needs to be recorded.Animportant insightforminimizingtheoverhead is that for predicting \nthe correct version to use, it is usually unnecessary to predict the trip-counts of all loops. We can \nconstruct the signature by selecting only the loops that do not need detailed instrumentation for prediction \nand mean\u00adwhile have large variances in their sequence values across different runs.  3.2 Prediction \nof Loop Importance In this experiment, we use the predicted behavior sequences to estimate the importance \nof a loop in a run, which is de.ned as its total trip count divided by the sum of all loops trip counts. \nSuch information is essential for guid\u00ading loop optimizations to concentrate on important loops. Meanwhile, \nas it re.ects the relative frequency of different memory references andfunctions, it is bene.cial for \nguiding prefetching and function inlining as well [2]. We compare three methods on all the manageable \nloops (i.e., non-C3.3 loops.) The .rst computes the importance directly from our predicted loop trip-count \nsequences. The second is through a direct extension of locality-based pre\u00addiction. It computes the importance \nbased on the .rst in\u00advocations of all loops in the current execution. This exten\u00adsion maintains a prediction \nscope comparable with the other two methods forfair comparison. The thirdisa pureof.ine pro.ling-based \napproach, using the pro.les in the test run for estimation of real runs. Ourexperiment usesalltheCprogramsin \nSPEC CPU2006 that Section 2.1 has described. Figure 12 reports the estima\u00adtion accuracy on the ref runs \nof the benchmarks. The accu\u00ad racy is de.ned as (1 -(|m[i] -m ' [i]|)/2), where, m[i] i and m ' [i] are \nthe real and predicted importance of the ith loop. Division by two is to normalize the accuracy to the \nrange of 0 100%. The of.ine pro.ling approach is subject to input sensitivity, hence showing the lowest \naccuracy.The pure locality-based method achieves high accuracy on six of the eleven benchmarks.Onaverage, \nthe sequence predic\u00adtiongives 98% accuracy, 17% higher thanthe locality-based method and the pro.ling \nmethod.  4. Discussion In this paper, we have used loop trip-count sequences to demonstrate the existence \nof inter-sequence correlations in program dynamic behaviors, and showed the feasibility of sequence prediction \nand its potential for program optimiza\u00adtions. Loops are amongthe most important constructsin pro\u00adgrams. \nButthere aremanyother kindsof program constructs and behavior sequences, such as function return values, \nref\u00aderenced memory addresses, data reuse distances, and so on. The inter-sequence correlation study described \nin this pa\u00adper and the ensuing sequence predictor construction are po\u00adtentially extensible to these types \nof behaviors. They may open up many new opportunities, not justfor program-level optimizations, but also \nfor optimizations at other levels of software execution stacks. At the Operating System level, for instance, \nthe enhanced scope, timing, and accuracy of program behavior prediction may better reveal the resource \nrequirement of an application in its different phases, and hence better guide resource provision in data \ncenters[12] and job (co-)scheduling on non-uniform or heterogeneous architecture [28, 30]. Atthe architecture \nlevel, the predic\u00adtion may better help dynamic voltage scaling and cache par\u00adtition/recon.guration toavoidlocal \noptimum traps withits large prediction scope.Weexpect that the promisingresults observed in this current \nwork will help stimulate a broader interest of the community in exploring and exploiting inter\u00adsequence \ncorrelations and sequence predictors in these other settings. Directly characterizing the data .owing \ninto a function may also help predict function behaviors, but automatically doing so is dif.cult, subject \nto the complexity of data and needs for domain knowledge. Correlation-based prediction is a complement \nrather than replacement of locality-based prediction.Thelatteris useful for re.ning predictionresults \nwith localinformation.Howto best combine themisworth futher exploration.  5. RelatedWork Program behavior \nprediction has been studied broadly. Ex\u00adamples include branch prediction [27], load value predic\u00adtion \n[4], and return value prediction [15, 21]. These pre\u00addictors are typically based on the history instances \nof a be\u00adhavior. Some consider onlyrecent instances,such as thelast (N)valuemethod[3, 20],stride method \n[10, 22]. Some con\u00adcern calling contexts, such as .nite context method [11, 22]. Some further consider \nthe arguments of functions, such as parameter stride [15], memorization[21].Some others com\u00adbine multiple \ntypes of predictors into one [5, 21]. They are all based on locality of behaviors.There are some studies \non behavior histogram prediction[9, 29]. Although more com\u00adplex than aninstance,a histogram differsfrom \nsequencesin that it has no temporal order. Program phase prediction [23, 24] also tries to reveal some \nlarge-scope behavior patterns. However, its goal is to predict phase shift, rather than a behavior sequence. \nPhase shift is one of the many patterns exploited in sequence pre\u00addiction. Some recent studies try to \nenable proactive behav\u00adior prediction by drawing the heuristics from program in\u00adputs [25]. However, their \nprediction target is still one in\u00adstance of a behavior (e.g., the appropriate optimization level of a \nJava method). Recent years have seen a body of work that uses Ma\u00adchine Learning(ML)techniquesto assist \nprogram optimiza\u00adtions.Examplesincludepredictionofthe optimizationlevels for a Java method [6], prediction \nof suitable parallelization schemes[26], and so on. This currentworkis complemen\u00adtary to these previous \ntechniques. These studies typically build predictive models mapping from program code fea\u00adtures(e.g.,portionsofvariousinstructions \ncontained)toop\u00adtimization decisions, rather than exploit inter-sequence cor\u00adrelations. We are not aware \nof a prior work in those stud\u00adies that predicts large-scope behavior sequences. Such pre\u00addictions are \nimportant for optimizations that rely on large\u00adscope, dynamic behaviorsofa program.For instance,forthe \ndynamic version selection in Section 3.1, the suitable ver\u00adsions to use at an invocation of a function \ndepends on how it will run in that invocation rather than what code it con\u00adtains.The previous ML-based \nmethod[6] is not applicable for such uses. On the other hand, the output from our se\u00adquencepredictors \nmaybe usedto enrichthefeaturevectors used in constructions of ML-based optimization models, as exempli.ed \nby our dynamic program version selection ex\u00adperiment. The correlation analysis of this current work is \nenlight\u00adened by a previous study [16], which uses correlations to predict average values of a behavior, \nrather than a sequence. Their study is oblivious to the sequence complexities cov\u00adered in this work. \nSome branch predictors use correlations among branches. But the exploitation has been both im\u00adplicit \nandsimple,ina manner that hardware can accommo\u00addate. And the prediction target is usually the next branch \nrather than a large-scoped sequence of dynamic behaviors. Some prior studies have used compression tools, \nsuch as SE-QUITUR,for .nding hot path or datastreams [7, 18].Hot streams capture some often seen short \nsequences of discrete events(e.g., variable c is likely to follow an access sequence bde), rather than \nlarge-scoped sequences of numerical be\u00adhaviors (e.g., the loop trip-count sequences in this paper.) Overall, \nwe are not aware of a prior study that analyzes the taxonomy of program behavior sequences, or systematically \nexploits inter-construct correlationsfor large-scoped predic\u00adtion of a sequence of numerical program \nbehaviors.  6. Conclusions This paper presents an exploration on inter-construct corre\u00adlations. It examines \ntheir existence, and creates the .rst hi\u00aderarchical taxonomyfor categorizing behavior sequences. It develops \na new form of predictors, namelysequence predic\u00adtor, to translate the correlations into large-scoped, \nproactive prediction of program behaviors. Experiments on loop trip\u00adcount sequences demonstrate the strength \nof inter-construct correlations, and the effectiveness of sequence predictorsin producing accurate prediction \nof behavior sequences. This study concludes the broad existence of inter-construct corre\u00adlations, reveals \ntheir large values in enhancing program be\u00adhavior predictors proactivity and scope, and demonstrates \nthe promiseofsequence predictorsin opening new opportu\u00adnities for program and system optimizations. \n Acknowledgement We owe the anonymous reviewers our gratitude for their helpful suggestions on the paper. \nThis material is based upon work supported by the National Science Foundation underGrant No. 0811791 \nandCAREERAward, DOE Early Career Award, and IBM CAS Fellowship. Any opinions, .ndings, and conclusions \nor recommendations expressed in this material are those of the authors and do not necessarily re.ect \nthe views of the National Science Foundation, DOE, orIBM. References [1] Sqlite. http://http://www.sqlite.org/. \n[2] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman. Compil\u00aders: Principles, Techniques, and Tools. \nAddison Wesley, 2nd edition, August 2006. [3] M. Burtscher and B. G. Zorn. Exploring last n value predic\u00adtion. \nIn PACT, 1999. [4] M. Burtscher and B. G. Zorn. Prediction outcome history\u00adbased con.dence estimation \nfor load value prediction. Journal of Instruction-Level Parallelism,1999. [5] M. Burtscher and B. G. \nZorn. Hybrid load-value predictors. IEEE Transactions on Computers, 2002.  [6] J. Cavazos and M. O Boyle. \nMethod-speci.c dynamic compi\u00adlation using logistic regression. In Proceedings of ACM SIG-PLAN Conference \non Object-Oriented Programming Systems, Languages and Applications, 2006. [7] T. M. Chilimbi and M. Hirzel. \nDynamic hot data stream prefetching for general-purpose programs. In Proceedings of ACM SIGPLAN Conference \non Programming Language Design and Implementation, Berlin,Germany, June 2002. [8] P. Chuang, H. Chen, \nG. Ho.ehner, D. Lavery, and W. Hsu. Dynamic pro.le driven code version selection. In Proceed\u00adings of \nthe 11th Annual Workshop on the Interaction between Compilers and Computer Architecture, 2007. [9] C. \nDing and Y. Zhong. Predicting whole-program locality with reuse distance analysis. In PLDI, 2003. [10] \nF. Gabbay. Speculative execution based on value prediction. Technical Report 1080, Israel Institute ofTechnology,1996. \n[11]B.Goeman,H.Vandierendonck,andK.de Bosschere.Differ\u00adential fcm:Increasingvalue prediction accuracyby \nimproving table usage ef.ciency. In HPCA, 2001. [12] S.Govindan,J. Choi,B.Urgaonkar,A.Sivasubramaniam,and \nA.Baldini. Statisticalpro.ling-basedtechniques foreffective power provisioning in data centers. In EuroSys, \n2009. [13] T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning. Springer, \n2001. [14] J. Henning. Spec2000: measuringcpu performance in the new millennium. IEEE Computer, 2000. \n[15]S.Hu,R.Bhargava,andL.K. John. Theroleofreturnvalue prediction in exploiting speculative method-level \nparallelism. Journal of Instruction-Level Parallelism, 2003. [16] Y. Jiang, E. Zhang, K. Tian, F. Mao, \nM. Geathers, X. Shen, and Y. Gao. Exploiting statistical correlations for proactive predictionof programbehaviors.In \nCGO, 2010. [17] P. Jonathan, W. J. Krzanowski, and W. V. McCarthy. On the use of cross-validation to \nassess performance in multivariate prediction. Statistics and Computing, 10(3), 2000. [18] J. R. Larus. \nWhole program paths. In Proceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, \nAtlanta, Georgia,May1999. [19] C. Lattner. LLVM: An Infrastructure for Multi-Stage Opti\u00admization. PhD \nthesis, Computer Science Dept., Univ. of Illi\u00adnois at Urbana-Champaign, 2002. [20] M.H. Lipasti and J.P. \nShen. Exceeding the data.ow limit via value prediction. In MICRO, 1996. [21] C. Pickett, C. Verbrugge, \nand A. Kielstra. Adaptive software return value prediction. Technical Report 1, McGill Univer\u00adsity, 2009. \n[22]Y. Sazeides andJ.E. Smith.The predictabilityof datavalues. In MICRO, 1997. [23] X. Shen, Y. Zhong, \nand C. Ding. Locality phase prediction. In ASPLOS, 2004. [24] T. Sherwood, S. Sair, and B. Calder. Phase \ntracking and prediction. In ISCA, 2003. [25] K. Tian, Y. Jiang, E. Zhang, and X. Shen. An input-centric \nparadigm for program dynamic optimizations. In OOPSLA, 2010. [26] Z. Wang and M. O Boyle. Mapping parallelism \nto multi\u00adcores: a machine learning based approach. In PPOPP 09: Proceedings of the fourth ACM SIGPLAN \nsymposium on Prin\u00adciples and practice of parallel programming, pages 75 84, 2009. [27] T. Yeh and Y. \nN. Patt. A comparison of dynamic branch predictors that use two levels ofbranch history. In ISCA, 1993. \n[28] E. Z. Zhang, Y. Jiang, and X. Shen. Does cache sharing on modern cmp matter to the performance of \ncontemporary multithreaded programs? In PPoPP,2010. [29] Y. Zhong, X. Shen, and C. Ding. Program locality \nanalysis using reuse distance. ACM Transactions on Programming Languages and Systems,31(6),2009. [30] \nS. Zhuravlev, S. Blagodurov, and A. Fedorova. Address\u00ading shared resource contention in multicore processors \nvia scheduling. In ASPLOS,2010.  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Prediction of program dynamic behaviors is fundamental to program optimizations, resource management, and architecture reconfigurations. Most existing predictors are based on locality of program behaviors, subject to some inherent limitations. In this paper, we revisit the design philosophy and systematically explore a second source of clues: statistical correlations between the behavior sequences of different program entities. Concentrated on loops, it examines the correlations' existence, strength, and values in enhancing the design of program behavior predictors. It creates the first taxonomy of program behavior sequence patterns. It develops a new form of predictors, named sequence predictors, to effectively translate the correlations into large-scope, proactive predictions of program behavior sequences. It demonstrates the usefulness of the prediction in dynamic version selection and loop importance estimation, showing 19% average speedup on a number of real-world utility applications. By taking scope and timing of behavior prediction as the first-order design objectives, the new approach overcomes limitations of existing program behavior predictors, opening up many new opportunities for runtime optimizations at various layers of computing.</p>", "authors": [{"name": "Bo Wu", "author_profile_id": "81544128056", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P3856190", "email_address": "bwu@cs.wm.edu", "orcid_id": ""}, {"name": "Zhijia Zhao", "author_profile_id": "81538929656", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P3856191", "email_address": "zzhao@cs.wm.edu", "orcid_id": ""}, {"name": "Xipeng Shen", "author_profile_id": "81452603368", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P3856192", "email_address": "xshen@cs.wm.edu", "orcid_id": ""}, {"name": "Yunlian Jiang", "author_profile_id": "81384618613", "affiliation": "Google, Mountain View, CA, USA", "person_id": "P3856193", "email_address": "yunlian@gmail.com", "orcid_id": ""}, {"name": "Yaoqing Gao", "author_profile_id": "81548601056", "affiliation": "IBM Toronto Lab, Toronto, Canada", "person_id": "P3856194", "email_address": "ygao@ca.ibm.com", "orcid_id": ""}, {"name": "Raul Silvera", "author_profile_id": "81351590885", "affiliation": "IBM Toronto Lab, Toronto, Canada", "person_id": "P3856195", "email_address": "rauls@ca.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384678", "year": "2012", "article_id": "2384678", "conference": "OOPSLA", "title": "Exploiting inter-sequence correlations for program behavior prediction", "url": "http://dl.acm.org/citation.cfm?id=2384678"}