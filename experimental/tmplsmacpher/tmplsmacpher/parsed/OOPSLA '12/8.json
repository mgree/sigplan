{"article_publication_date": "10-19-2012", "fulltext": "\n Mitigating the Compiler Optimization Phase-Ordering Problem using Machine Learning Sameer Kulkarni \nJohn Cavazos University of Delaware {skulkarn,cavazos}@cis.udel.edu Abstract Today s compilers have \na plethora of optimizations to choose from, and the correct choice of optimizations can have a sig\u00adni.cant \nimpact on the performance of the code being opti\u00admized. Furthermore, choosing the correct order in which \nto apply those optimizations has been a long standing problem in compilation research. Each of these \noptimizations inter\u00adacts with the code and in turn with all other optimizations in complicated ways. \nTraditional compilers typically apply the same set of optimization in a .xed order to all functions in \na program, without regard the code being optimized. Understanding the interactions of optimizations is \nvery important in determining a good solution to the phase\u00adordering problem. This paper develops a new \napproach that automatically selects good optimization orderings on a per method basis within a dynamic \ncompiler. Our approach for\u00admulates the phase-ordering problem as a Markov process and uses a characterization \nof the current state of the code being optimized to creating a better solution to the phase ordering \nproblem. Our technique uses neuro-evolution to construct an arti.cial neural network that is capable \nof pre\u00addicting bene.cial optimization ordering for a piece of code that is being optimized. We implemented \nour technique in Jikes RVM and achieved signi.cant improvements on a set of standard Java benchmarks \nover a well-engineered .xed order. General Terms Compilers, Optimization, Code genera\u00adtion, Connectionism \nand neural nets Keywords Phase Ordering, Compiler optimization, Ma\u00adchine learning, Neural Networks, Java, \nJikes RVM, Code Feature Generation Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tuscon, Arizona, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 1. Introduction Selecting the best ordering of compiler \noptimizations for a program has been an open problem in compilation research for decades. Compiler writers \ntypically use a combination of experience and insight to construct the sequence of optimiza\u00adtions found \nin compilers. In this approach, compromises must be made, e.g., should optimizations be included in a \ndefault .xed sequence if those optimizations improve per\u00adformance of some benchmarks, while degrading \nthe perfor\u00admance of others. For example, GCC has around 250 passes that can be used, and most of these \nare turned off by default. The developers of GCC have given up in trying to include all optimizations \nand hope that a programmer will know which optimizations will bene.t their code. In optimizing compilers, \nit is standard practice to apply the same set of optimizations phases in a .xed order on each method \nof a program. However, several researchers [2, 3, 15], have shown that the best ordering of optimizations \nvaries within a program, i.e., it is function-speci.c. Thus, we would like a technique that selects the \nbest ordering of optimizations for individual portions of the program, rather than applying the same \n.xed set of optimizations for the whole program. This paper develops a new method-speci.c technique that \nautomatically selects the predicted best ordering of opti\u00admizations for different methods of a program. \nWe develop this technique within the Jikes RVM Java JIT compiler and automatically determine good phase-orderings \nof optimiza\u00adtions on a per method basis. Rather than developing a hand\u00adcrafted technique to achieve this, \nwe make use of an arti.\u00adcial neural network (ANN) to predict the optimization order likely to be most \nbene.cial for a method. Our ANNs were automatically induced using Neuro-Evolution for Augment\u00ading Topologies \n(NEAT) [23]. A trained ANN uses input properties (i.e., features) of each method to represent the current \noptimized state of the method and given this input, the ANN outputs the opti\u00admization predicted to be \nmost bene.cial to the method at that state. Each time an optimization is applied, it poten\u00adtially changes \nthe properties of the method. Therefore, af\u00adter each optimization is applied, we generate new features \nof Figure 1. We used genetic algorithms to .nd tuned optimization sequence for benchmarks in the SPECjvm98 \nand Java Grande benchmark suites. In a .rst experiment, we obtained an optimization ordering that performed \nwell over all the benchmarks by using the running time of all the benchmarks as a .tness function. In \na second experiment, we used running time of each individual benchmark to evolve the best optimization \nordering for each benchmark.  the method to use as input to the ANN. The ANN then pre\u00addicts the next \noptimization to apply based on the current op\u00adtimized state of the method. This technique solves the \nphase\u00adordering problem by taking advantage of the Markov prop\u00aderty of the optimization problem. That \nis, the current state of the method represents all the information required to choose an optimization \nto be most bene.cial at that decision point. We discuss the Markov property and our approach in more \ndetail in Section 3. The application of machine learning to compilation has received a lot of attention. \nHowever, there has been little ef\u00adfort to learn the effect that each optimization has on the code and \nto use that knowledge to choose the most appro\u00adpriate optimization to apply. To the best of our knowledge, \nthe technique described in this paper is the .rst to automati\u00adcally induce a heuristic that can predict \nan overall optimiza\u00adtion ordering to individual portions of a program. Our tech\u00adnique learns what order \nto apply optimizations rather than tuning local heuristics, and it does this in a dynamic compila\u00adtion \nsetting.1 Furthermore, we show signi.cant performance improvement over an existing well-engineered compilation \nsystem. We make the following contributions: We present a method of evolving an ANN to be used for phase-ordering, \nwhich, to the best of our knowledge, is the .rst technique that learns from characteristics of code being \noptimized what is the best ordering of optimizations to apply.  We show that our phase-ordering technique \ncan achieve signi.cant speedup over the traditional approach of ap\u00adplying a .xed optimization sequence. \n 1 The same technique can be applied in a static compilation setting in a straight-forward manner. \nWe compare our ANN-based phase-ordering technique to the current state-of-the-art phase-ordering technique, \ni.e., using genetic algorithms (GAs). Moreover, we show that our technique is much more practical for \nphase-ordering than using GAs.  We present results optimizing several Java benchmark programs to illustrate \nthat optimization order is impor\u00adtant.  We show that our trained ANN generates customized method-speci.c \nphase-orderings in the compiler for a va\u00adriety of different methods in SPECjvm2008 and DaCapo benchmarks \n  2. Overview Compiler optimizations transform the code being optimized, thus the application of each \noptimization potentially affects the bene.t of downstream optimizations. One of the most prominent examples \nof this is the phase-ordering problem between register allocation and instruction scheduling. How\u00adever, \nany set of optimizations can potentially interact with each other and can therefore participate in a \nphase-ordering problem. These code interactions are an integral part of com\u00adpiler optimizations, so it \nis important to understand the ef\u00adfects of the optimizations in order to arrange them in a way that can \ndeliver the most bene.t. 2.1 Phase-Ordering with Genetic Algorithms Most compilers apply optimizations \nbased on a .xed order that was determined to be best when the compiler was being developed and tuned. \nHowever, programs require a speci.c ordering of optimizations to obtain the best performance. To demonstrate \nour point, we use genetic algorithms (GAs), the current state-of-the-art in phase-ordering optimizations \n[3,  6 9, 14, 16, 17], to show that selecting the best ordering of 2.2 Issues with Current State-of-the-Art \noptimizations has the potential to signi.cantly improve the running time of dynamically-compiled programs. \nWe used GAs to construct a custom ordering of optimiza\u00adtions for each of the Java Grande [22] and SPEC \nJVM 98 benchmarks.2 In this GA approach, we create a population of strings (called chromosomes), where \neach chromosome corresponds to an optimization sequence. Each position (or gene) in the chromosome corresponds \nto a speci.c optimiza\u00adtion from Table 2, and each optimization can appear multi\u00adple times in a chromosome. \nFor each of the experiments be\u00adlow, we con.gured our GAs to create 50 chromosomes (i.e., 50 optimization \nsequences) per generation and to run for 20 generations. We ran two different experiments using GAs. \nThe .rst experiment consisted of .nding the best optimization se\u00adquence across our benchmarks. Thus, \nwe evaluated each op\u00adtimization sequence (i.e., chromosome) by compiling all our benchmarks with each \nsequence. We recorded their execu\u00adtion times and calculated their speedup by normalizing their running \ntimes with the running time observed by compiling the benchmarks at the O3 level. That is, we used average \nspeedup of our benchmarks (normalized to opt level O3) as our .tness function for each chromosome. This \nresult corre\u00adsponds to the Best Overall Sequence bars in Figure 1. The purpose of this experiment was \nto discover the optimization ordering that worked best on average for all our benchmarks. The second \nexperiment consisted of .nding the best op\u00adtimization ordering for each benchmark. Here, the .tness function \nfor each chromosome was the speedup of that opti\u00admization sequence over O3 for one speci.c benchmark. \nThis result corresponds to the Best Sequence per Benchmark bars in Figure 1. This represents the performance \nthat we can get by customizing an optimization ordering for each benchmark individually. The results \nof these experiments con.rm two hypothe\u00adses. First, signi.cant performance improvements can be ob\u00adtained \nby .nding good optimization orders versus the well\u00adengineered .xed order in Jikes RVM. The best order \nof op\u00adtimizations per benchmark gave us up to a 20% speedup (FFT) and on average 8% speedup over optimization \nlevel O3. Second, as shown in previous work, each of our bench\u00admarks requires a different optimization \nsequence to obtain the best performance. One ordering of optimizations for the entire set of programs \nachieves decent performance speedup compared to O3. However, the Best Overall Sequence degrades the performance \nof three benchmarks (LUFact, Series,and Crypt) compared to O3. In contrast, search\u00ading for the best custom \noptimization sequence for each benchmark, Best Sequence for Benchmark , allows us to outperform both \nO3 and the best overall sequence. 2 We choose these benchmarks because they run for a short time. This \nallowed us to evaluate thousands of different optimization sequences using GAs. Using genetic algorithms \nis the current state-of-the-art in obtaining good optimization orderings, and they can bring signi.cant \nperformance improvements for some programs. However, using GAs has several issues that impede their widespread \nadoption in traditional compilers. Expensive Search: GAs and other search techniques are inherently expensive \nbecause they need to evaluate a variety (typically hundreds) of different optimization orders for each \nprogram and are therefore only applicable when compilation time is not an issue, e.g., in an iterative \ncompilation scenario. And, because there is no transfer of knowledge, the search space corresponding \nto the potential optimization orders has to be explored anew for each new benchmark or benchmark suite. \nWe show empirical results in Section 7 describing the time it took to for GAs to construct optimization \norderings. Method-speci.c dif.culty: Using GAs to .nd custom or\u00adderings of optimizations for speci.c \ncode segments within a program (e.g., for each method) is non-trivial. An or\u00adder of optimization speci.c \nto each piece of code requires a separate exploration of the optimization ordering space for that code. \nThis requires obtaining .ne-grained execu\u00adtion times for each piece of code after it is optimized with \na speci.c phase-ordering. Fine-grained timers produce notori\u00adously noisy information and can be dif.cult \nto implement. 3 Note that exhaustive exploration to .nd the optimal order of optimizations is not practical. \nFor example, if we consider 15 optimizations and an optimization sequence length of 20, the number of \nunique sequences exhaustive exploration that would have to be evaluated is enormous (1520). Thus, the \ncurrent state-of-the-art is to intelligently explore a small fraction of this space using genetic algorithms \nor some other search algorithm.  2.3 Proposed Solution Instead of using expensive search techniques \nto solve the phase-ordering problem, we propose to use a machine\u00adlearning based approach which automatically \nlearns a good heuristic for phase-ordering. This approach incurs a one\u00adtime expensive training process, \nbut is inexpensive to use when being applied to new programs. There are two poten\u00adtial techniques we \ncould use to predict good optimization orders for code being optimized. 1. Predict the complete sequence: \nThis technique requires a model to predict the complete sequence of optimizations that needs to be applied \nto the code just by looking at characteristics of the initial code to be optimized. This is a dif.cult \nlearning task as the model would need to 3 Evaluating optimization orders for a method outside of an \napplication con\u00adtext [19] can simplify .ne-grained timing, but has the potential to identify optimization \nsequences that do not perform well when the method is used in its original context.  Figure 2. The \n.gure above represents the framework used to evolve a neural network using NEAT to guide the compi\u00adlation \nof a given method. The Figure 4 describes the way the neural network was used to guide the compilation \nprocess. understand the complex interactions of each optimization in the sequence. 2. Predict the current \nbest optimization: This method would use a model to predict the best single optimization (from a given \nset of optimizations) that should be applied based on the characteristics of code in its present state. \nOnce an optimization is applied, we would reevaluate characteris\u00adtics of the code and again predict the \nbest optimization to apply given this new state of the code. In this paper we choose the second approach; \nwe believe this is an easier learning problem to solve. We used a technique called Neuro-Evolution for \nAug\u00admenting Topologies to automatically construct a heuristic that can generate customized optimization \norderings for each method in a program. The process of developing this heuris\u00adtic is depicted in Figure \n2 and described in detail in Sec\u00adtion 3. This approach involves continually interrogating a neural network \nto predict which optimization would produce the best results as a method is being optimized. Our network \nuses as input features characterizing the current state of the code being optimized and correlates those \nfeatures with the best optimization to use at particular point in the optimiza\u00adtion process. As we are \nconsidering dynamic JIT compila\u00adtion, the neural network and the feature generator must incur a small \noverhead, otherwise the cost of applying the network to perform phase-ordering might outweigh any bene.ts \nof the improved optimization orders. 1. ANN generated by NEAT is loaded into the JikesRVM s opti\u00admization \ndriver. 2. Evaluate ANN at the task of phase-ordering optimizations by performing the following steps \nfor each method selected for dynamic compilation. (a) Generate a feature vector of current method s \nstate (b) Use ANN to predict the best optimization to apply (c) Apply optimization and jump to step \n(a) unless the ANN predicts to stop any more optimizations.  3. Run benchmarks and obtain feedback \nfor NEAT (a) Record execution time for each benchmark optimized using the ANN (b) Obtain speedup by \nnormalizing each benchmark s running time to running time using default optimization heuristic (e.g., \nopt level O3)   Figure 3. NEAT performs neuro-evolution to construct a neural network to be used for \nphase-ordering. The above steps describe the process of evaluating an ANN as part of neuro-evolution. \nAnother approach would be to handcraft a heuristic based on experimentation and analysis. This is undesirable \nbecause it is an arduous task and speci.c to a compiler, and if the platform were to change, the entire \ntuning process of the heuristic would have to be repeated.  3. Approach This section gives a detailed \noverview of how neuro\u00adevolution based machine learning is used to construct a good optimization phase-ordering \nheuristic for the optimizer within Jikes RVM. The .rst section outlines the different ac\u00adtivities that \ntake place when training and deploying a phase\u00adordering heuristic. This is followed by sections describing \nhow we use NEAT to construct an ANN, how we extract features from methods, and how these features and \nANNs allow us to learn a heuristic that determines the order of optimizations to apply. Figure 3 outlines \nour technique. 3.1 Overview of Training and Deployment There are two distinct phases, training and deployment. \nTraining occurs once, off-line, at the factory and is equiv\u00adalent to the time spent by compiler writers \ndesigning and implementing their optimization heuristics. Deployment is the act of applying the heuristic \nat dynamic compilation time to new unseen programs. As part of the training phase, NEAT generates an \nANN that is used to control the order of optimizations within Jikes RVM. The ANN is evaluated by applying \ndifferent optimiza\u00adtion orderings to each method within each training program and recording the performance \nof the optimized program. The ANN takes as input a characterization (called a feature vector) of current \nstate of the method being optimized and outputs a set of probabilities corresponding to the bene.t of \napplying each optimization. The optimization with the high\u00adest probability is applied to the method. \nAfter an optimiza\u00adtion is applied, the feature vector of the method is updated and fed into the network \nfor another round of optimization. One output of the network corresponds to stop optimizing, and the \noptimization process continues until this output has the highest probability.  Once the best ANN is \nevolved, it is installed into the Jikes RVM compiler and used at runtime as an optimization heuristic. \nThe next sections describe these stages in more detail.  3.2 Markov Property Most compilers apply optimizations \nin a .xed order, and this order is tuned for a particular set of benchmarks. This tuning process is performed \nmanually and is tedious and relatively brittle. Also, the tuning procedure needs to be repeated each \ntime the compiler is modi.ed for a new platform or when a new optimization is added to the compiler. \nMost impor\u00adtantly, we have empirical evidence that each method within a program requires the application \nof a speci.c order of op\u00adtimizations to achieve the best performance. In this paper, we propose to use \nmachine learning to mitigate the compiler optimization phase-ordering problem. Determining the correct \nphase ordering of optimizations in a compiler is a dif.cult problem to solve. In the absence of an oracle \nto determine the correct ordering of optimizations, we must use a heuristic to predict the best optimization \nto use. We formulate the phase-ordering problem as a Markov Process.Ina Markov Process, the heuristic \nmakes a decision on what action to perform (i.e., optimization to apply) based on the current state of \nthe environment (i.e., the method being optimized). In order to perform learning, the state must conform \nto the Markov Property, which means that the state must represent all the information needed to make \na decision of what action to perform at that decision point. In our framework, the current state of the \nmethod being optimized serves as our Markov state because it succinctly summarizes the important information \nabout the complete sequence of optimizations that led to it.  3.3 Neuro-Evolution Overview In this paper, \nwe use Neuro-Evolution of Augmenting Topologies (NEAT) to construct our neural networks to be used for \nphase-ordering. NEAT uses a process of natural se\u00adlection to construct an effective neural network to \nsolve a particular task. This process starts by randomly generating an initial population (or generation) \nof neural networks and evaluating the performance of each network at solving the speci.c task at hand. \nThe number of neural networks present in each genera\u00adtion is set to 60 for our experiments. Each of these \n60 neural networks is evaluated by using them to optimize the bench\u00admarks in the training set. A .tness \nis associated with each Figure 4. This .gure represents the phase-ordering process. The process starts \nwhen the Jikes RVM optimizer receives a method to optimize. We iterate over the instructions of the method \nto generate the features, and then provide these features to the neural network. The neural network then \nprovides a set of outputs, which represent the probabilities of each optimization being bene.cial. The \noptimization with the highest probability is applied to the code. One of the outputs of the network corresponds \nto stop optimizing. When the probability of this output is highest, the optimizer stops applying optimizations \nto the method. network as described in Section 3.5.3. Once the initial set of generated neural networks \nare evaluated, the 10 best neural networks from this set are propagated to the next generation and are \nalso used to produce new neural networks in this generation. This process continues and each successive \ngeneration of neural networks produces networks that performs better than the networks from the previous \ngeneration. New net\u00adworks are created using mutation and crossover of the best networks from the previous \ngeneration. During the process of constructing new networks, we mutate the topology of a progenitor network. \nMutation can involve adding a neuron to an existing edge in a network s hidden layer. We set the probability \nof adding a neuron to a low value (.1%) to keep our networks small and ef.cient. Mutation can also involve \nadding a new edge (probability .5%) or deleting an existing Table 1. Method features being collected. \nTo reduce the length of the table several (different) features have been placed in logical groups e.g. \nsimple, long and real are three different features.  Feature Meaning bytecodes Number of bytecodes in \nthe method locals space Number of words allocated for locals synch Method is synchronized exceptions \nMethod has exception han\u00addling code leaf Method is a leaf (contains no calls) .nal Method is declared \n.nal private Method is declared private static Method is declared static Category Fraction of bytecodes \nthat ... aload, astore are Array Loads and Stores primitive, long are Primitive or Long compu\u00adtations \n(e.g., iadd, fadd) compare are Compares (e.g., lcmp, dcmpl) branch are Branches (forward/back\u00adward/cond/uncond) \njsr are a JSR switch are a SWITCH put/get are a PUT or GET invoke are an INVOKE new are a NEW arraylength \nare an ArrayLength athrow,checkcast,monitor are an Athrow, checkcast, or monitor multi newarray are a \nMulti Newarray simple, long, real are a Simple,Long, or Real Conversions edge (probability .9%). These \nprobabilities are within the ranges suggested by the authors of NEAT. Neurons are re\u00admoved when the last \nedge to or from that neuron is removed. The mutation probabilities were manually-tuned for our spe\u00adci.c \ntask. For our present experiments, we stopped after 300 generations, which was when the performance of \nthe net\u00adworks no longer improved at the task of phase-ordering for our training benchmarks. Figure 2 \ndepicts the process of constructing a neural network using NEAT to replace the op\u00adtimization heuristic \nin Jikes RVM.  3.4 Feature Extraction Determining the properties of a method that predict an opti\u00admization \nimprovement is a dif.cult task. As we are operating in a dynamic compilation environment, we chose features \nthat are ef.cient to calculate and intuitively relevant. Com\u00adputing these features requires a single \npass over the instruc\u00adtions of the method. Table 1 shows the 26 features used to de\u00adscribe the current \nstate of each method being optimized. The values of each feature will be an entry in the 26-element feature \nvector x associated with each method. The .rst 2 en\u00adtries are integer values de.ning the size of the \ncode and data of the method. The next 6 are simple boolean properties (rep\u00adresented using 0 or 1) of \nthe method. The remaining features are simply the percentage of bytecodes belonging to a partic\u00adular \ncategory (e.g., 30% loads, 22% .oating point, 5% yield points, etc.).  3.5 Applying NEAT There are many \ncharacteristics (i.e., features) that can in.u\u00adence the phase-ordering decision, and these factors may \nhave complex interdependencies between them. In order to effec\u00adtively model the non-linear behavior of \nthese features, our neural networks are multilayer perceptrons. 3.5.1 Why NEAT? NEAT can be used to solve \nchallenging tasks because it can evolve networks of unbounded complexity from a minimal starting point. \nThis method has been shown to outperform the best .xed-topology method on challenging reinforce\u00adment \nlearning tasks [23] The reason that NEAT is faster and better than typical reinforcement learning is \nthree-fold: 1) it incrementally grows networks from a minimal structure, 2) it protects structural innovation \nusing natural selection, and 3) it employs a principled method of crossover of different topologies. \nNeural networks are traditionally trained using super\u00advised learning algorithms, which require a labeled \ntraining set. The labeled training set consists of a feature vector that is used as input, which characterizes \na particular decision point and the correct label or desired output the network should produce when given \nthis input. In the case of the phase order\u00ading problem, we would need a feature vector corresponding \nto the code being optimized and the desired output would be the sequence of optimizations to apply to \nthat code. Generat\u00ading this labeled dataset requires knowing the right sequence of optimizations to apply \nto a method is dif.cult as discussed in Section 2.3.  3.5.2 Structure of the network In our neural networks, \neach feature or characteristic of the method is fed to an input node, and the layers of the network can \nrepresent complex nonlinear interaction between fea\u00adtures. Each output node of the network controls a \nparticular optimization that could be applied. The outputs are numbers between 0 or 1 depending on whether \nthe optimization is predicted to be bene.cial to the state of the code currently being optimized. We \napply the optimization pertaining to the output that is closest to 1 indicating the optimization that \nthe network predicts will be most bene.cial. One of the outputs of the ANN tells the optimizer to stop \noptimizing. When the probability of this output is highest, the optimizer stops ap\u00adplying optimizations \nto the method. Figure 4 shows the pro\u00adcess of phase-ordering.  3.5.3 Fitness Functions The .tness value \nwe used for the NEAT algorithm is the arithmetic mean of the performance of the benchmarks in the training \nset. That is, the .tness value for a particular performance metric is: .s.S Speedup(s)Fitness(S)= |S| \nwhere S is the benchmark training suite and Speedup(s)is the metric to minimize for a particular benchmark \ns,which in our case is the run time (i.e., running time of the benchmark without compile time). Speedup(s)= \nRuntime(sde f )/Runtime(s) where sde f is a run of benchmark s using the default optimization order of \noptimization level O3. The goal of the learning process is to create a heuristic that determines the \ncorrect order of optimizations to apply to a particular method thereby reducing the running time of the \nsuite of benchmarks in the training set.  4. Infrastructure + Methodology In this section we describe \nthe platform, the benchmarks, and the methodology employed in our experiments. 4.1 Platform For our experiments \nin this paper, we modi.ed version 3.1.1 of the Jikes Research Virtual Machine [4]. The VM was run on \nan Intel x86 based machine, supporting two AMD Opteron 2216 dual core processors running at 2.6GHz with \nan L1 and L2 cache and RAM of 128K, 1M and 8GB, re\u00adspectively. The operating system on the machine was \nLinux, running kernel 2.6.32. We used the FastAdaptiveGenMS con.guration of Jikes RVM, indicating that \nthe core vir\u00adtual machine was compiled by the optimizing compiler at the most aggressive optimization \nlevel and the generational mark-sweep garbage collector was used. 4.2 Benchmarks For the present set \nof experiments we used four benchmark suites. For our training set, we used seven benchmarks from the \nJava Grande benchmark suite [29]. These benchmarks were used for training primarily due to their short \nexecution times. For the test set, we used the SPECjvm98 [28], the SPECjvm2008 [27], and the DaCapo benchmark \n[1] suites. We used all the benchmarks from SPECjvm98 and the sub\u00adset of benchmarks from SPECjvm2008 \nand DaCapo that we could correctly compile with Jikes RVM. We used the largest inputs for all benchmarks. \n4 The SPEC JVM bench\u00admarks have been designed to measure the performance of the Java Runtime Environment \n(JRE) and focus on core Java functionality. The DaCapo benchmark suite is a collection of programs that \nwere designed for various different Java performance studies. The results in Section 5 come from the \nbenchmarks in our test set. 4.3 Optimization Levels We ran our experiments in two scenarios, .rst using \nonly the optimizing compiler in a non-adaptive scenario and second using the adaptive compilation mode. \nIn the optimizing com\u00adpilation scenario, we set the initial compiler to be the opti\u00admizing compiler and \ndisable any recompilation. This forces the compiler to compile all the loaded methods at the high\u00adest \noptimization level. Under the adaptive scenario, all dy\u00adnamically loaded methods are .rst compiled by \nthe baseline compiler that converts bytecodes straight to machine code without performing any optimizations. \nThe resultant code is slow, but the compilation times are fast. The adaptive opti\u00admization system then \nuses online pro.ling to discover the subset of methods where a signi.cant amount of the pro\u00adgram s running \ntime is being spent. These hot methods are then recompiled using the optimizing compiler. During this \nprocess these methods are .rst compiled at optimization level O0, but if they continue to be important \nthey are re\u00adcompiled at level O1, and .nally at level O2 if warranted. Available optimizations are divided \ninto different optimiza\u00adtion levels based on their complexity and aggressiveness. When using the neural \nnetwork in the adaptive scenario, we disabled the optimizations that belonged to a higher level than \nthe present optimization level being used.  4.4 Measurement In a dynamic compiler like Jikes RVM, there \nare two types of execution times that are of interest, total time and run\u00adning time. The total time of \na program is the time that the dynamic compiler takes to compile the code from bytecodes to machine code, \nand then to actually run the machine code. The running time of a program is considered to be just the \ntime taken to run the machine code after it has been com\u00adpiled by the dynamic compiler during a previous \ninvocation. For programs with short running times the total time is of interest, as the compilation process \nitself is the larger chunk of the execution time. However for programs that are likely to run for longer \ndurations, e.g. programs that perform heavy computation or server programs that are initialized once \nand remain running for a longer period of time, it is important to highly optimize the machine code being \ngenerated. This is true even at the expense of potentially greater compile time, as the compilation time \nis likely to be overshadowed by the 4 Note that for the benchmark FFT in SPECjvm2008, we used the small \ninput size because the large input size required more memory than was available on our experimental platform. \n OptKey Meaning Optimization Level O0 CSE Local common sub expression elimination CNST Local constant \npropagation CPY Local copy propagation SA CFG Structural Analysis ET Escape Transformations FA Field \nAnalysis BB Basic block frequency estimation Optimization Level O1 BRO Branch optimizations TRE Tail \nrecursion elimination SS Basic block static splitting SO Simple optimizations like Type prop, Bounds \ncheck elim, dead-code elim, etc. Optimization Level O2 LN Loop normalization LU Loop unrolling CM Coalesce \nMoves Table 2. The set of optimizations that were used to perform phase ordering in our experiments. \nexecution of the machine code that has been generated by the dynamic compiler. The time taken to execute \nthe bench\u00admark for the .rst invocation is taken as the total time. This time includes the time taken \nby the compiler to compile the bytecodes into machine code and the running of the machine code itself. \nThe running time is measured by running the benchmark over .ve iterations and taking the average of the \nlast three execution times, this ensures that all the required methods and classes had been preloaded \nand compiled. To compare our performance we normalize our running times and total times with the default \noptimization setting. This default compilation scenario acts as our baseline, which is the average of \ntwenty running times and twenty total times for each benchmark. The noise for all benchmarks in this \npaper was less than 1.2% and the average noise was 0.7%.  4.5 Evaluation Methodology As is standard \npractice, we evolve our neural network over one suite of benchmarks, commonly referred to in the ma\u00adchine \nlearning literature as the training set. We then test the performance of our evolved neural network over \nanother unseen suite of benchmarks, that we have not trained on, referred to as the test set.  5. Results \nIn this section, we present our results of using the neural net\u00adwork that performed best on the training \nset. We used this network to determine good optimization orders for methods in programs from the SPECjvm98, \nSPECjvm2008, and Da-Capo benchmark suites in both an adaptive and optimizing compilation scenario. 5.1 \nAdaptive Compiler In the adaptive compilation scenario, we allowed the adap\u00adtive compiler to decide the \nlevel of optimization to be used to optimize methods as described in Section 4.3. However, at each optimization \nlevel we used the induced neural network to decide to order of optimizations to apply at that level. \nIn this scenario, we obtained an average speedup of 8% in run\u00adning time and 4% improvement in the total \nexecution time over all the benchmarks versus the default adaptive mode in Jikes RVM 5.1.1 SPECjvm98 \nRunning time Using our neural network for phase-ordering, we were able to obtain an average speedup of \n10% across the seven benchmarks of the SPECjvm98 benchmark suite on the running time. We got signi.cant \nimprovements over default on mpegaudio (20%), compress (14%), and javac (11%). Total time We observed \na modest increase in performance of 3% on average on the SPECjvm98 benchmarks. However, it is important \nto note that we achieved these speedups de\u00adspite of the overhead of feature extraction and the execution \nof the neural network. The javac program gave us the best total time speedup at around 7%. 5.1.2 SPECjvm2008 \nRunning Time We achieved an average running time speedup of 6.4% on the SPECjvm2008 benchmarks. The fft \nbenchmark did give us a slowdown of a little less than 5%. Interestingly, we discovered that the neural \nnet\u00adwork used very short optimization sequences to optimize that benchmark. This helps to explain the \nimprovement in the total time for this benchmark as described in the next section. Total Time Our average \nperformance improvement over all .ve SPECjvm2008 benchmarks was around 4%. We achieved a performance \nimprovement of up to 7% on the benchmark sor with our ANNs. 5.1.3 DaCapo Running Time The running time \nperformance improve\u00adment of the programs in the DaCapo benchmark suite (at 6.8%) was not as high as the \nother two benchmark suites, but their performance on the total time of 6% was much bet\u00adter than the average \nof the other two SPECjvm benchmark suites. Total Time  5.2 Optimizing Compiler When running Jikes RVM \nin a non-adaptive mode, all the methods are compiled directly at the highest optimization level. The \naverage speedup when just measuring running time was 8.2%, and we improved the total time by over 6%. \n  Figure 5. Adaptive: The graph above represents the speedup achieved by using NEAT when used by Jikes \nRVM in adaptive mode to optimize each benchmark in the test set. We compare our result with the performance \nof each of the benchmarks when using the default adaptive compilation scenario. Figure 6. Optimization \nLevel O3: The graph above represents the speedup achieved by using NEAT when used in the non\u00adadaptive \nmode in Jikes RVM to optimize each benchmark in the test set. We compare our result with the performance \nof each of the benchmarks when using the default non-adaptive compilation scenario. 5.2.1 SPECjvm98 performing \nbenchmark was again mpegaudio at 11% speedup. Running time In SPECjvm98, we achieve up to a speedup of \n24% on mpegaudio. On average, we improved the run\u00ad 5.2.2 SPECjvm2008 ning time performance of this benchmark \nsuite by 10%, Running Time We achieved an average running time which is a signi.cant improvement. speedup \nof 7% over all the .ve benchmarks of the SPECjvm2008 Total time When measuring total time, we observed \na benchmark suite. The best performing benchmark from the modest increase in performance of around 3.4%. \nThe best SPECjvm2008 suite was sor with a speedup of almost 12%. Total Time An interesting observation \nhere is the perfor\u00admance of the fft benchmark. In all other cases this bench\u00admark had a minor slowdown. \nWe realized that the average optimization sequence length suggested by the neural net\u00adwork was 11. This \nis very short compared to the default .xed order sequence length of 23. This reduction in the sequence \nlength helped to reduce the amount of compilation required, and thus improves total time performance. \n 5.2.3 DaCapo Running Time Using the Jikes RVM in a non-adaptive mode, we were able to get some signi.cant \nspeedups of 17% for pmd and 10.6% for lusearch. There were no signif\u00adicant slowdowns and on average we \nobserved a speedup of 7.3% on the DaCapo benchmark suite. Total Time We saw signi.cant speedups across \nDaCapo with 14% speedups on xalan, luindex and lusearch, and speedups of 5%, 8%, and 9% on the three \nother pro\u00adgrams. On average, we had an improvement 11%.  6. Exploration of Phase ordering bene.t In \nthis section, we tried to analyze the optimization orderings that our neural network came up with. We \nran the bench\u00admarks and collected the pro.ling runs, which gave us an idea of which methods were most \nimportant. Looking at the neural network does not typically give any intuition of the phase-ordering \nheuristic, however it may help to understand the rough complexity of the .nal solution. The neural network \nfound interesting combinations of transformations that helped in improving the performance of some of \nthe benchmarks. We explain two such situations in this section. 6.1 Branch Optimization and CFG structural \nanalysis. Changing the ordering of these two optimizations signif\u00adicantly impacted the performance of \na benchmark. In the scimark.lu.small benchmark in SPEC2008 in Fig\u00adure 7, we look at the code that was \naffected and the exact change in the code that caused the change in performance. The pseudo code shown \nin Listing 1 is the hottest method in the scimark.lu.small benchmark. Code generated by ap\u00adplying CFG \nStructural Analysis and then Branch Optimiza\u00adtion (i.e., the ordering obtained from the default optimization \nlevel) is shown in Listing 2. On the other hand the neural net\u00adwork asks us to perform Branch Optimization \nand then CFG Structural Analysis and the code that is generated is shown in Listing 3. In the slower \ncode, the loops are represented as while loops, and the code that worked best had loops that are represented \nas do-while loops, even though it had more un\u00adconditional branch statements. This small difference in \nthe machine code gave an improvement of approximately 7% in the running time of the scimark.lu benchmark. \nBe\u00adcause the original code had a large fraction of unconditional branch statements, it triggered the \nneural network to apply CFG Structural Analysis. 6.2 Loop Unrolling and CFG structural analysis. Analyzing \nanother benchmark, scimark.sparse,which performs sparse matrix multiplication, we see another simi\u00adlar \nphenomena. We looked at the sparse.SparseCompRow.matmul method, which is the hottest method in the benchmark \nand has multiple nested loops as represented in Listing 4 in Figure 8. Considering the number of nested \nloops in this method, Loop Unrolling could potentially be an optimization to this method. How\u00adever we \nrealized that our neural network applied CFG Struc\u00adtural Analysis before it applied Loop Unrolling.This \norder\u00ading helped in improving the quality of the code, improving the total running time by almost 14%. \nAgain, this particu\u00adlar ordering is not present in the default ordering present in the JikesRVM compiler. \nThere were some differences in the machine code that were generated. The exact change in the machine \nthat caused this huge speedup cannot be pinpointed, however we found a few instances of machine code \nthat were less than optimum Listing 5 shows such a piece of machine code. When looking at this particular \ninstance we quickly realized that the code placement was needlessly complex. For example, if only the \ntarget of the .rst conditional jump was set to LABEL1, we would not need the last three un\u00adconditional \njumps. Intuitively, a compiler writer would try to .x the problem by applying another optimization like \nBranch Optimization or applying CFG Structural Analysis once more. But, in this particular case repeating \nCFG Struc\u00adtural Analysis or applying another instance of Branch opti\u00admization did not improve the performance \nof the code. Such kind of situations can be better catered to by using machine learning similar to our \nmethod of phase ordering.  7. Improvements over state of art Training any machine learning heuristic \nrequires one to pro\u00advide .tness values (measure of the goodness of a solution) to each of the heuristics \nbeing tested. In a labeled dataset the .tness values are already provided. However in our case a labeled \ndataset does not exist, and the only way to calculate the effectiveness of a solution is to apply the \nheuristic calcu\u00adlate the running time and compare it to the default running time. This makes the execution \ntime of the benchmark the bottleneck in our experiments. At present the best way to tune phase ordering \nis to use GA to optimize the search. This paper compares the proposed solution of using NEAT with Genetic \nAlgorithms. When using GAs one can try to provide the solution in two different forms. Finding a single \nsolution using Genetic Algorithm for all benchmarks in a benchmark suite is the .rst way and .nding a \ncustomized solution for each benchmark by using Genetic Algorithm per benchmark of a suite would be the \nsecond.  factor () { for (...) {arithematic operation over an array for (...) { arithematic operation \nover an array } } ... if (...) {for (...) {arithematic operation over an array } } if (...) { for (...) \n{for (...) {arithematic operation over an array } } }  } Listing 1. Pseudo-code for scimark.lu.LU.factor, \nthe hottest method for the SPEC2008 lu benchmark LABEL1 in ifcmp <CONDITION> GOTO LABEL2 ... GOTO LABEL1 \nLABEL2  Listing 2. Slow Code (CFG SA applied before BRO): The following code corresponds to a while \nloop,where n iterations of the loop require n conditional jumps and n unconditional jumps. This is the \ncode produced by using optimization level O3. in ifcmp <!CONDITION> GOTO LABEL2 LABEL1 ... in ifcmp \n<CONDITION> GOTO LABEL1 LABEL2  Listing 3. Fast Code (BRO applied before CFG SA): The following code \ncorresponds to a do-while loop, where n iterations of the loop would require n+1 con\u00additional jumps but \nno unconditional jumps, this improves the performance. This is the code produced using the op\u00adtimization \nordering produced using our neural network. Figure 7. Listing 1 shows the pseudo code of the scimark.lu.LU.factor \nmethod that is compiled by the optimizing compiler. The two HIRs generated for scimark.lu.LU.factor by \nthe two different optimization orderings are shown in Listing 2 and Listing 3. Changing the order that \nthe transformations are applied changes the running time by almost 7% matmul () { for (...) {arithematic \noperation over an array for (...) { for (...) {arithematic operation over an array } } } ... if (...) \n{for (...) {arithematic operation over an array } } }  Listing 4. Pseudo-code for matmult, the hottest \nmethod for the SPEC2008 sparse benchmark.  LABEL1 ... int ifcmp <CONDITION> GOTO LABEL3 goto LABEL2 \nLABEL2 goto LABEL4 LABEL3 goto LABEL1 LABEL4   Listing 5. Slow Code (LU applied before SA): Here the \nnum\u00adber. of unconditional statements are unnecessary, and hampers the performance of the code. This is \nthe code produced by using optimization level O3. LABEL1 ... int ifcmp <CONDITION> GOTO LABEL1   \nListing 6. Fast Code (SA applied before LU): During compi\u00adlation, CFG Structural Analysis was applied \nbefore Loop Un\u00adrolling, which gave the compiler a chance to clean up the code before the loop unrolling \nwas applied. This is the code produced using the optimization ordering produced using our neural net\u00adwork. \nFigure 8. The .nal HIR generated for sparse.SparseCompRow.matmult by the two different optimization orderings. \nThe code generated by applying CFG Structural Analysis before Loop Unrolling shown in Listing 6 performs \nbetter in terms of running time and achieved a speedup of almost 14%.. When looking at the other characteristics, \nthe number of unconditional jumps were reduced by 33% and there was a 10% reduction in the number of \nbasic blocks.  Figure 9. Genetic Algorithm versus Neural Network: The graph above represents the speedup \nachieved by the best optimization sequence found by the genetic algorithm for all the benchmarks in the \ntraining set, when applied to the test set (JikesRVM in non-adaptive mode). We compare our result with \nthe performance of each of the benchmarks when using the default non-adaptive compilation scenario. Program \nGA NEAT Java Grande 4.4 4.91 Jolden 7 8.3 Total 11.4 13.2 Table 3. Time taken in days to train the training \nset, to provide the reults in Figure 9 In the .rst case of .nding a single solution for a complete suite \nmeans that we have a collection of benchmarks that we train on, and them apply the solution that we .nd \nevery piece of code that come across after that. This situation is somewhat similar to our case (of using \nNEAT) where we train on a training set and test on a test set (code that we have not seen before). The \namount of training time required is shown in Table 3, we can see that GA was able to .nd a solution in \naround 13% less time, however the solutions proposed by NEAT as shown in the Neural Network bar consistently \noutperforms a single solution proposed by the GA as shown by the Genetic Algorithm bar in Figure 9. The \nbar represented by Genetic Algorithm per benchmark in Figure 9 outperforms NEAT, but is not very practical \nfor general use. The amount of training time required to .nd the right sequence for each benchmark is \nshown in Table 4, spending 70 days may not be possible for an end user, nor can the sequence found during \nthis method be used for any Table 4. This table shows the average time that we have taken if we evolved \nan optimization ordering using Genetic Algorithm per benchmark individually. other code. This makes \nthis method impractical for most purposes, but can be used as an approximate upper bound on the bene.t \nthat can be obtained from phase ordering.  8. Discussion In this section, we brie.y describe the neural \nnetwork that we used for the experiments and discuss some observations Table 5. The table shows average \nnumber of optimizations that were applied by the neural network (17) which is 6 optimizations less than \nthe number of optimizations that are applied by O3 optimization level.  Program Avg. Seq. length Program \nAvg. Seq. length SPECjvm2008 contd. SPECjvm98 sparse sor 20 16 javac mpegaudio jess compress raytrace \njack 18 19 16 19 18 17 DaCapo avrora luindex lusearch pmd sunflow xalan 19 16 16 18 16 17 SPECjvm2008 \nfft lu monte carlo 11 18 17 Average 17 Default O3 23  (e.g., the reduction in the optimization sequence \nlength, a case of repeated optimizations, and handling of relatively .at pro.les.) Neural Network We \nused one neural network for all the results shown in Table 6 and Figures 5 and 6. This network had 30 \ninputs, 14 outputs, 24 hidden nodes, and 503 total connections. Reduction of optimization sequence length \nFrom our ex\u00adperiments, we were able to demonstrate two achievements. Intelligent ordering of the sequences \nprovided us with sig\u00adni.cant speedups. We also show that intelligently applying the right optimizations \nhelps in improving the compile time by not having to apply optimizations that have little impact on a \nmethod s performance. This would reduce the compila\u00adtion burden on the system, and directly improve the \nsystem performance in terms of total execution time. A detailed analysis of the phase orderings suggested \nby the ANN is shown in the Table 6. We typically applied 16-20 optimizations while the default optimizing \ncompiler applied 23. We believe that this is signi.cant. That is, we were able to apply the right optimizations \nand thus more effectively utilize the optimization resources available to us. Repeating optimizations \nIn some cases the optimizations get repeated back to back. For example, the sequence shown in the fourth \nrow of Table 6, the network predicted to ap\u00adply Static Splitting twice in succession. This situation \narises when applying a particular optimization does not change the feature vector. We could potentially \nbe stuck in an in.nite loop where the feature vector remains the same, thus inad\u00advertently causing the \nneural network to apply the same opti\u00admization, which causes an in.nite loop. In order to overcome this \nsituation, if the network predicts that applying the same optimization again would be bene.cial, we allow \nfor a maxi-Figure 10. Speedup based on method importance:The plot above represents the speedup achieved \nby evolving an optimization sequence using genetic algorithm per bench\u00admark and NEAT. Each data point \nin the plot corresponds to a benchmark, and the plot depicts the number of methods that constitute 60% \nof the running time for a particular bench\u00admark versus the speedup obtained for that benchmark. mum of \n5 such repetitions, and then instead apply the second best optimization. Flat-pro.led benchmarks For \nsome benchmarks, the run\u00adning time of the benchmark is equally divided among mul\u00adtiple methods (i.e., \na .at pro.le), while in other benchmarks have the majority of the execution time is spent in just one \nor a few methods. Finding a good phase ordering in case of benchmarks with one single hot method is relatively \nstraight-forward. We would simply be searching for an op\u00adtimization sequence that was bene.cial for the \none impor\u00adtant method of the benchmark. Since the execution time is dominated by a single method, we \nwould see an overall im\u00adprovement in the performance of the benchmark even if the method-speci.c phase \nordering negatively affects the perfor\u00admance of the other methods. In order to demonstrate our point, \nwe conducted an exper\u00adiment where we allowed the genetic algorithm to search for the best optimization \nsequence to be applied to each bench\u00admark. This was the method proposed by Cooper et al. [6] and was \nshown to .nd good optimization sequences for a pro\u00adgram. Figure 10 shows the speedup achieved by both \nGAs and neural networks on each benchmark as it relates to the number of hot methods that constitute \n60% of the running time for a particular benchmark. In this .gure, we see that the GA is better at .nding \ngood speedups when the 60% of  Benchmark Hot method Percent of Size Optimization Total Calls Sequence \nfft(small) lu FFT.transform internal() LU.factor() SPECjvm 2008 86.93% 390 CNST,CPY,CPY,LU,BB,SS,BB,CSE,LN,CNST,LN \n72.59% 277 TRE,CNST,CPY,SS,SS,BRO,SA,ET,SO, ET,LU,SS,LU,TRE,SS,SS,SO,CNST,FA,FA monte carlo MonteCarlo.integrate() \n25.31% 68 BB,CPY,BB,TRE,CNST,BB,CSE, CSE,LU,CSE,SS,SA,LU,FA sparse sor SparseCompRow. matmult() SOR.execute() \n80.79% 161 SO,BB,LU,CNST,TRE,LN,CPY,TRE,SS,CPY, SO,SO,SS,FA,BB,CNST,CPY,TRE,CNST 86.51% 184 SO,SO,BB,SO,SS,CPY,ET,TRE,CPY,LN,CSE, \nCSE,SO,LN,SA,SA,SA,BB,TRE,CNST Table 6. This table gives information about the hottest methods in SPECjvm2008 \nand the optimization sequences obtained from our neural network for each of these methods. The abbreviations \nused to described the sequences are explained in Table 2. the execution time is concentrated in just \none method. How\u00adever, our NEAT-evolved networks are able to achieve good speedup when the execution time \nis distributed over multi\u00adple methods. This conclusion is reaf.rmed by the speedups of javac and mpegaudio \nin Figure 9, which both have rela\u00adtively .at pro.les, and in both cases the individually trained GA phase \nordering did not do as well as the Neural network. 9. Related Work Auto-tuning: An area that is closely \nrelated to this paper is the study of automatic code generation and optimization for different computer \narchitectures (auto-tuning), which has been explored in many existing studies for many different applications. \nA number of library generators automatically produce high-performance kernel routines [21, 26, 30]. Re\u00adcent \nresearch efforts [12, 18] expand automatic code genera\u00adtion to routines whose performance depends not \nonly on ar\u00adchitectural features, but also on input characteristics. These systems are a signi.cant step \ntoward automatically opti\u00admizing code for different computer architectures. Recently, Ganapathi [11] \net al. presented some preliminary results on the application of machine learning to auto-tuning for multi\u00adcores. \nThey showed that auto-tuning of stencil codes, with the assistance of machine learning, was able to surpass \nper\u00adformance of tuning by a domain expert. This research dis\u00adplays the great potential for machine learning \nand search in an auto-tuning environment. However, these prior works have all been largely focused on \nsmall domain-speci.c ker\u00adnels and still neglect exploring the bene.ts of learning from a knowledge base \nof previously explored applications and architectures. Machine learning applied to Compilation: Machine \nlearn\u00ading and search techniques applied to compilation has been studied in many recent projects [5, 8, \n9, 14, 20, 24, 25, 31]. These previous studies have developed machine learning\u00adbased algorithms to ef.ciently \nsearch for the optimal se\u00adlection of optimizing transformations, the best values for the transformation \nparameters, or the optimal sequences of com\u00adpiler optimizations. Generally, these studies customize op\u00adtimizations \nfor each program or local code segments, some based on code characteristics. The proposed research in \nthis paper is motivated by these studies and makes a signi.cant step forward: the compiler will not only \nuse program char\u00adacteristics, but will also learning to decide the right ordering of optimizations. Several \nresearchers have looked at searching for the best sequence of optimizations for a particular program \n[6 8, 13, 16, 17], for example the work by Cooper et al. [6] used genetic algorithms to solve the compilation \nphase ordering problem. They were concerned with .nding good com\u00adpiler optimization sequences that reduced \ncode size. Their technique was successful at reducing code size by as much as 40%. Unfortunately, their \ntechnique was application\u00adspeci.c, i.e., a genetic algorithm had to be retrained to .nd the best optimization \nsequence for each new program. Also, Cooper et al. [8] propose a technique called virtual execu\u00adtion \nto reduce the cost of evaluating different optimization orderings. Virtual execution consists of running \nthe program one time and predicting the performance of different opti\u00admization sequences without running \nthe code again. These approaches give impressive performance improvements, but has to be performed each \ntime a new application is compiled. While this is acceptable in embedded environments, it is not suitable \nfor typical compilation. Kulkarni et al. [17] exhaustively enumerated all distinct function instances \nfor a set of programs that would be pro\u00adduced from different phase-orderings of 15 optimizations. This \nexhaustive enumeration allowed them to construct probabilities of enabling/disabling interactions between \ndif\u00adferent optimization passes in general and not speci.c to any program. In contrast, the techniques \nin this paper charac\u00adterized methods being optimized; therefore, the techniques described here learn \nwhich optimizations are bene.cial to apply to unseen methods with similar characteristics. Many researchers \nhave also looked at using machine learning to construct heuristics that control compiler op\u00adtimizations. \nCavazos et al. [5] used logistic regression to control what optimizations to apply in JikesRVM. However, \nthey do not attempt to control the order of optimizations and instead only turn on and off optimizations \ngiven the hand-tuned .xed order of optimizations. For the SPECjvm98 benchmarks, they were not able to \nachieve signi.cant im\u00adprovements for running time under both non-adaptive and adaptive scenarios likely \nbecause the .xed-order of opti\u00admizations in Jikes RVM had been highly tuned and there was little room \nfor improvement on top of this ordering by simply turning optimizations on and off. In contrast, we achieve \ngood improvements on SPECjvm98 benchmarks by applying method-speci.c optimization orderings.  Stephenson \net al. [25] used genetic programming to tune heuristic priority functions for three compiler optimizations \nwithin the Trimaran s IMPACT compiler. For one of the opti\u00admizations, register allocation, they were \nonly able to achieve on average a 2% increase over the manually tuned heuristic. Monsifrot et al. [20] \nused a classi.er based on decision tree learning to determine which loops to unroll showing a few percent \nimprovement on two different machines. The results in these papers highlight the diminishing results \nobtained when only controlling a single optimization. In contrast, this research will control numerous \noptimizations available in the compiler. Agakov et al. [2] describe two models to improve the search \nfor good optimization orders to apply to programs. The .rst model, called the independent identically \ndis\u00adtributed model, produces a probability vector correspond\u00ading to probability that a transformation \noccurs in a good sequence for a particular program. When optimizing a new program, a nearest neighbor \nalgorithm is used to choose the probability vector of the program in the training set closest to the \nprogram to be optimized. This probability vector is then used to choose optimizations for the new program. \nThe second model, called the Markov model simply creates a probability matrix where the probability of \nan optimization being bene.cial depends upon the optimizations that have been previously applied. These \nmodels were developed to focus the search for good optimization orderings during iter\u00adative compilation. \nTherefore, these techniques suffers from the same limitations as described in Section 2.1. Addition\u00adally, \nthese models use simple nearest neighbor algorithms using the characteristics of the original unoptimized \ncode. Therefore, these models do not take advantage of important characteristics of the code as it is \nbeing optimized. Fursin et al. [10] (as part of the MILEPOST project) have integrated machine learning \nalgorithms in GCC to control these optimizations applied. They show good results on three different architectures, \ncompared to random search of opti\u00admizations sequences. However, the machine learning algo\u00adrithms in MILEPOST \ndo not learn good optimization order\u00adings because as the authors state this requires detailed in\u00adformation \nabout dependencies between passes to detect legal orders . 10. Conclusion This paper has shown that method-speci.c \noptimization orderings can give signi.cant performance improvements within the Jikes RVM JIT compiler. \nIt has also demonstrated that a technique of neuro-evolution can automatically de\u00adrive a neural network \nthat gives signi.cant performance improvements over a well-engineered optimization order\u00ading. We show \ntotal execution time improvements of up to 20%. To the best of our knowledge, this is the .rst paper \nto demonstrate that machine-learning models can be success\u00adfully used to choose optimization orders for \nmethods within a compiler. The present study is promising as it provides a fresh prospective to the problem \nof phase ordering which has been studied for decades. The amount of improvement that can be found from \nthis method has not yet reached its full potential, and we propose to improve the machine learning algorithm \nto provide better improvements. For future work, we would like to implement similar phase-ordering techniques \nin a static compiler, in order to understand the behavior of other environments on our setup. There is \nnothing about the technique that makes it speci.c to dynamic compilation. In addition, we would also \nlike to incorporate pro.le information into the feature set, which allow us to improve our predictions. \n  References [1] Dacapo benchmark suite. URL http://dacapobench.org/benchmarks.html. [2] F. Agakov, \nE. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. F. P. O Boyle, J. Thomson, M. Toussaint, and C. K. I. \nWilliams. Using machine learning to focus iterative optimiza\u00adtion. In CGO 06: Proceedings of the International \nSympo\u00adsium on Code Generation and Optimization, pages 295 305, Washington, DC, USA, 2006. IEEE Computer \nSociety. [3] L. Almagor, K. D. Cooper, A. Grosul, T. J. Harvey, S. W. Reeves, D. Subramanian, L. Torczon, \nand T. Waterman. Find\u00ading effective compilation sequences. In Proceedings of the 2004 ACM SIGPLAN/SIGBED \nconference on Languages, compilers, and tools for embedded systems, LCTES 04, pages 231 239. ACM, 2004. \n[4] B. Alpern, C. R. Attanasio, J. J. Barton, M. G. Burke, P.Cheng, J.-D. Choi, A. Cocchi, S. J. Fink, \nD. Grove, M. Hind, S. F. Hummel, D. Lieber,V.Litvinov,M.F. Mergen, T.Ngo,J.R. Russell, V.Sarkar,M. J.Serrano, \nJ.C.Shepherd, S.E.Smith, V. C. Sreedhar, H. Srinivasan, and J. Whaley. The Jalape no virtual machine. \nIBM Systems Jounrnal, 39(1), 2000. [5] J. Cavazos and M. F. P. O Boyle. Method-speci.c Dynamic Compilation \nUsing Logistic Regression. In OOPSLA 06: Proceedings of the 21st annual ACM SIGPLAN conference on Object-oriented \nprogramming systems, languages, and ap\u00adplications, pages 229 240, New York, NY, USA, 2006. ACM Press. \n[6] K. Cooper, P. Schielke, and D. Subramanian. Optimizing for reduced code space using genetic algorithms. \nIn Proceedings  of the ACM SIGPLAN 1999 workshop on Languages, compil\u00aders, and tools for embedded systems, \npages 1 9. ACM, 1999. [7] K. Cooper, D. Subramanian, and L. Torczon. Adaptive opti\u00admizing compilers for \nthe 21st century. The Journal of Super\u00adcomputing, 23(1):7 22, 2001. [8] K. Cooper, A. Grosul, T. Harvey, \nS. Reeves, D. Subramanian, L. Torczon, and T. Waterman. ACME: adaptive compilation made ef.cient. In \nLCTES 05: Proceedings of the 2005 ACM SIGPLAN/SIGBED conference on Languages, compilers, and tools for \nembedded systems, volume 40, pages 69 77. ACM, 2005. [9] K. D. Cooper, A. Grosul, T. J. Harvey, S. Reeves, \nD. Subrama\u00adnian, L. Torczon, and T. Waterman. Exploring the Structure of the Space of Compilation Sequences \nUsing Randomized Search Algorithms. J. Supercomputing, 36(2):135 151, 2006. [10] G. Fursin, C. Miranda, \nO. Temam, M. Namolaru, E. Yom-Tov, A. Zaks, B. Mendelson, P. Barnard, E. Ashton, E. Courtois, F. Bodin, \nE. Bonilla, J. Thomson, H. Leather, C. Williams, and M. O Boyle. Milepost gcc: machine learning based \nresearch compiler. In Proceedings of the GCC Developers Summit, June 2008. [11] A. Ganapathi, K. Datta, \nA. Fox, and D. Patterson. A case for machine learning to optimize multicore performance. First USENIX \nWorkshop on Hot Topics in Parallelism (HotPar 09), 2009. [12] S.-C. Han, F. Franchetti, and M. P\u00a8uschel. \nProgram Genera\u00adtion for the All-pairs Shortest Path Problem. In PACT 06: Proceedings of the 15th international \nconference on Paral\u00adlel architectures and compilation techniques, pages 222 232, New York, NY, USA, 2006. \nACM Press. [13] M. R. Jantz and P. A. Kulkarni. Eliminating false phase interactions to reduce optimization \nphase order search space. In CASES, pages 187 196. ACM, 2010. [14] P. Kulkarni, S. Hines, J. Hiser, D. \nWhalley, J. Davidson, and D. Jones. Fast Searches for Effective Optimization Phase Se\u00adquences. In PLDI \n04: Proceedings of the ACM SIGPLAN 2004 conference on Programming language design and im\u00adplementation, \npages 171 182. ACM Press, 2004. [15] P. A. Kulkarni, D. B. Whalley, G. S. Tyson, and J. W. Davidson. \nExhaustive optimization phase order space explo\u00adration. In Fourth Annual IEEE/ACM Interational Conference \non Code Generation and Optimization, pages 306 318, New York City, NY, March 2006. [16] P. A. Kulkarni, \nD. B. Whalley, and G. S. Tyson. Evaluating heuristic optimization phase order search algorithms. In CGO, \npages 157 169. IEEE Computer Society, 2007. [17] P. A. Kulkarni, D. B. Whalley, G. S. Tyson, and J. W. \nDavid\u00adson. Practical exhaustive optimization phase order exploration and evaluation. TACO, 6(1), 2009. \n[18] X. Li, M. J. Garzar\u00b4an, and D. Padua. Optimizing Sorting with Genetic Algorithms. In In Proc. of \nthe International Symposium on Code Generation and Optimization (CGO), pages 99 110, March 2005. [19] \nC. Liao, D. J. Quinlan, R. W. Vuduc, and T. Panas. Effective source-to-source outlining to support whole \nprogram empiri\u00adcal optimization. In LCPC 09, pages 308 322, 2009. [20] A. Monsifrot, F. Bodin, and R. \nQuiniou. A machine learning approach to automatic production of compiler heuristics. In AIMSA 02: Proceedings \nof the 10th International Conference on Arti.cial Intelligence: Methodology, Systems, and Applica\u00adtions, \npages 41 50, London, UK, 2002. Springer-Verlag. [21] M. P\u00a8uschel, J. Moura, J. Johnson, D. Padua, M. \nVeloso, B. Singer, J. Xiong, F. Franchetti, A. Gacic, Y. Voronenko, K. Chen, R. W. Johnson, and N. Rizzolo. \nSPIRAL: Code Generation for DSP Transforms. In Proc. of the IEEE, spe\u00adcial issue on Program Generation, \nOptimization, and Plat\u00adform Adaptation, 93(2):232 275, February 2005. [22] L. A. Smith, J. M. Bull, and \nJ. Obdrz\u00b4alek. A parallel Java Grande benchmark suite. In ACM, editor, SC2001: High Per\u00adformance Networking \nand Computing. Denver, CO, November 10 16, 2001. ACM Press and IEEE Computer Society Press, 2001. ISBN \n1-58113-293-X. [23] K. O. Stanley and R. Miikkulainen. Ef.cient rein\u00adforcement learning through evolving \nneural network topologies. In Proceedings of the Genetic and Evo\u00adlutionary Computation Conference (GECCO-2002), \npage 9, San Francisco, 2002. Morgan Kaufmann. URL http://nn.cs.utexas.edu/?stanley:gecco02b. [24] M. \nStephenson and S. Amarasinghe. Predicting Unroll Fac\u00adtors Using Supervised Classi.cation. In CGO 05: \nProceed\u00adings of the international symposium on Code generation and optimization, pages 123 134, Washington, \nDC, USA, 2005. IEEE Computer Society. [25] M. Stephenson, S. Amarasinghe, M. Martin, and U.-M. O Reilly. \nMeta Optimization: Improving Compiler Heuristics with Machine Learning. In Proc. of Programing Language \nDesign and Implementation, June 2003. [26] R. Vuduc, J. W. Demmel, and K. A. Yelick. OSKI: A Library \nof Automatically Tuned Sparse Matrix Kernels. Journal of Physics Conference Series, 16:521 530, Jan. \n2005. [27] Website. Specjvm 2008, . URL http://www.spec.org/jvm2008/. [28] Website. Specjvm 98, . URL \nhttp://www.spec.org/jvm98/. [29] Website. Java grande benchmarks, . URL http://www2.epcc.ed.ac.uk/computing/ \nresearch activities/java grande/. [30] R. C. Whaley, A. Petitet, and J. J. Dongarra. Automated Empirical \nOptimizations of Software and the ATLAS Project. Parallel Computing, 27(1-2):3 35, 2001. [31] K. Yotov, \nX. Li, G. Ren, M. Cibulskis, G. DeJong, M. Garzaran, D. Padua, K. Pingali, P. Stodghill, and P. Wu. A \nComparison of Empirical and Model-driven Optimization. In Proc. of Programing Language Design and Implementation, \npages 63 76, June 2003.  \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Today's compilers have a plethora of optimizations to choose from, and the correct choice of optimizations can have a significant impact on the performance of the code being optimized. Furthermore, choosing the correct order in which to apply those optimizations has been a long standing problem in compilation research. Each of these optimizations interacts with the code and in turn with all other optimizations in complicated ways. Traditional compilers typically apply the same set of optimization in a fixed order to all functions in a program, without regard the code being optimized.</p> <p>Understanding the interactions of optimizations is very important in determining a good solution to the phase-ordering problem. This paper develops a new approach that automatically selects good optimization orderings on a per method basis within a dynamic compiler. Our approach formulates the phase-ordering problem as a Markov process and uses a characterization of the current state of the code being optimized to creating a better solution to the phase ordering problem. Our technique uses <i>neuro-evolution</i> to construct an artificial neural network that is capable of predicting beneficial optimization ordering for a piece of code that is being optimized. We implemented our technique in Jikes RVM and achieved significant improvements on a set of standard Java benchmarks over a well-engineered fixed order.</p>", "authors": [{"name": "Sameer Kulkarni", "author_profile_id": "81490643157", "affiliation": "University of Delaware, Newark, DE, USA", "person_id": "P3856049", "email_address": "skulkarn@cis.udel.edu", "orcid_id": ""}, {"name": "John Cavazos", "author_profile_id": "81100096445", "affiliation": "University of Delaware, Newark, DE, USA", "person_id": "P3856050", "email_address": "cavazos@cis.udel.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384628", "year": "2012", "article_id": "2384628", "conference": "OOPSLA", "title": "Mitigating the compiler optimization phase-ordering problem using machine learning", "url": "http://dl.acm.org/citation.cfm?id=2384628"}