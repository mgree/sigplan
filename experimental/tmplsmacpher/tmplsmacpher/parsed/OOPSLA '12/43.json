{"article_publication_date": "10-19-2012", "fulltext": "\n Integrating Task Parallelism with Actors Shams Imam Rice University shams@rice.edu Abstract This paper \nintroduces a uni.ed concurrent programming model combining the previously developed Actor Model (AM) \nand the task-parallel Async-Finish Model (AFM). With the advent of multi-core computers, there is a renewed \ninterest in programming models that can support a wide range of parallel programming patterns. The proposed \nuni\u00ad.ed model shows how the divide-and-conquer approach of the AFM and the no-shared mutable state and \nevent-driven philosophy of the AM can be combined to solve certain classes of problems more ef.ciently \nand productively than either of the aforementioned models individually. The uni\u00ad.ed model adds actor \ncreation and coordination to the AFM, while also enabling parallelization within actors. This paper describes \ntwo implementations of the uni.ed model as ex\u00adtensions of Habanero-Java and Habanero-Scala. The uni.ed \nmodel adds to the foundations of parallel programs, and to the tools available for the programmer to \naid in productivity and performance while developing parallel software. Categories and Subject Descriptors \nD.1.3 [Programming Techniques]: Concurrent Programming Parallel program\u00adming General Terms Design, Languages, \nPerformance Keywords Parallel Programming, Actor Model, Fork-Join Model, Async-Finish Model, Habanero-Java, \nHabanero-Scala 1. Introduction Current mainstream programming languages provide lim\u00adited support for \nexpressing parallelism. Programmers need parallel programming models and constructs that can pro\u00adductively \nsupport a wide range of parallel programming pat\u00adterns. This has led to a renewed interest in parallel \nprogram\u00adming models in the research community. Programs exhibit Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, \nUSA. Copyright &#38;#169; 2012 ACM 978-1-4503-1561-6/12/10. . . $15.00 Vivek Sarkar Rice University vsarkar@rice.edu \n varying degrees of task, data, and pipeline parallelism [9] and extensions thereof e.g., event-driven \nparallelism as an extension of pipeline parallelism. In this paper, we focus on two such models: The \nAsync-Finish Model (AFM), as exempli.ed by the async and finish constructs [4, 5] and the data-driven \nfuture extension [28], which is well-suited to exploit task parallelism in divide-and-conquer style and \nloop-style programs.  The Actor Model (AM) which promotes the no-shared mutable state and an event-driven \nphilosophy.  We introduce a uni.ed parallel programming model that integrates the previously developed \nAsync-Finish Model [5] and Actor Model [1, 12]. This integration not only adds actors as a new coordination \nconstruct in the AFM, but also enables parallelization of message-processing within actors1. It also \nsimpli.es detecting termination and man\u00adaging synchronous operations in actors. We developed two reference \nimplementations of this uni.ed model by extend\u00ading Habanero-Java (HJ) [4] and Habanero-Scala (HS) [14]2. \nBoth HJ and HS include an implementation of uni.ed ac\u00adtors using data-driven controls (explained in Section \n6.2.1) which we call light actors. Habanero-Scala also includes a heavy actor implementation that extends \nthe standard Scala actor library [10] which uses exceptions for control .ow. The proposed uni.ed model \nshows how the AFM and the AM can be combined to solve certain classes of problems more productively than \neither of the aforementioned mod\u00adels individually. In our performance evaluation, we include a summary \nof application characteristics that can be more ef.\u00adciently solved using the uni.ed model compared to \nthe AFM or AM and show that benchmarks exhibiting such character\u00adistics can execute up to 30% faster \nusing constructs from the uni.ed model in our implementations relative to the AFM or AM. The paper is \norganized as follows: in Section 2 we give a brief description of the AFM and the AM and some limita\u00ad \n1 This subsumes the parallelization (while processing different messages) offered by the become primitive \n[1] and enables parallelization while pro\u00ad cessing a single message in actors. 2 Previous versions of \nHJ and HS supported only the async-finish style computations without support for actors. tions of the \ntwo models. Section 3 summarizes the syntax of some of the parallel constructs from these models used \nin the rest of the paper. We introduce the proposed uni.ed model in Section 4, which de.nes how actors \nand async-finish tasks can be integrated. Section 5 presents some of the new capabilities in the uni.ed \nmodel including parallelization of message processing. In Section 6, we describe our reference implementations \nand compare them with implementations of other JVM based actor frameworks in Section 7. Section 8 discusses \nrelated work and we summarize our conclusions and future work in Section 9. 2. Background 2.1 The Async-Finish \nModel (AFM) The AFM is a task parallel model and a variant of the Fork-Join Model. The central features \nof any AFM implementa\u00adtion on multicore architectures include the abilities to cre\u00adate lightweight tasks \nand to ef.ciently manage the synchro\u00adnization constraints among tasks. In the AFM, a parent task can \nfork (async) multiple child tasks which can execute in parallel. In addition, these child tasks can recursively \nfork even more tasks. A parent/ancestor task can selectively join (finish) on a subset of child/descendent \ntasks. The task executing the join has to wait for all tasks created in the finish scope to terminate \nbefore it can proceed. This is the primary form of synchronization among tasks in the AFM. The child \ntasks are said to execute in the finish scope rep\u00adresented by the aforementioned join. In the AFM, each \ntask is guaranteed to have a unique dynamic Immediately Enclos\u00ading Finish (IEF) which may be the implicit \nfinish construct for the entire program. An example AFM program is dis\u00adcussed later in Figure 2. 2.1.1 \nDesirable Properties Async-.nish style computations are guaranteed to be dead\u00adlock free [5]. In addition, \nin the absence of data races, these programs also have the extremely desirable property that they are \ndeterministic [21]. Two well-known manifestations of the AFM can be found in the X10 [5] and Habanero-Java \n[4] languages. The new task-parallel constructs in OpenMP 3.0 [19] also represent a variant of the AFM. \n  2.2 The Actor Model The Actor Model (AM) was .rst de.ned in 1973 by Carl He\u00adwitt et al. during their \nresearch on Arti.cial Intelligent (AI) agents [12]. It was designed to address the problems that arise \nwhile writing distributed applications. Further work by Henry Baker [13], Gul Agha [1], and others added \nto the theoretical development of the AM. The AM is different from task parallelism in that it is primarily \nan asynchronous message-based concurrency model. An actor is the central entity in the AM that de.nes \nhow computation proceeds. The key idea is to encapsulate mutable state and use asyn\u00adchronous messaging \nto coordinate activities among actors. An actor is de.ned as an object that has the capability to process \nincoming messages. It has a well-de.ned life cycle and restrictions on the actions it performs in the \ndifferent states. During its life cycle an actor is in one of the following three states: new: An instance \nof the actor has been created; however, the actor is not yet ready to receive or process messages.  \nstarted: An actor moves to this state from the new state when it has been started using the start operation. \nIt can now receive asynchronous messages and process them one at a time. While processing a message, \nthe actor should continually receive any messages sent to it with\u00adout blocking the sender.  terminated: \nThe actor moves to this state from the started state when it has been terminated and will not process \nany messages in its mailbox or new messages sent to it. An actor signals termination by using the exit \noperation on itself while processing some message.  Typically, the actor has a mailbox to store its \nincoming messages. Other actors act as producers for messages that go into the mailbox. An actor also \nmaintains local state which is initialized during creation. After creation, the actor is only allowed \nto update its local state using data from the mes\u00adsages it receives and from the intermediate results \nit com\u00adputes while processing the message. The actor is restricted to process at most one message at \na time. There is no restriction on the order in which the actor decides to process incom\u00ading messages, \nthereby leading to non-determinism in actor systems. As an actor processes a message, it is allowed to \nchange its state and behavior affecting how it processes the subsequent messages. While processing a \nmessage, an actor may perform a .nite combination of the following steps: 1. Asynchronously send a message \nto another actor whose address is known; 2. Create a new actor providing all the parameters required \nfor initialization; 3. Become another actor, which speci.es the replacement behavior to use while processing \nthe subsequent mes\u00adsages [20].  2.2.1 Desirable Properties The only way an actor conveys its internal \nstate to other ac\u00adtors is explicitly via messages and responses to messages. This property obtains bene.ts \nsimilar to encapsulation in object-oriented programming and encourages modularity. The encapsulation \nof the local state also helps prevent data races because only the actor can modify its local state. Due \nto the asynchronous mode of communication, the lack of restriction on the order of processing messages \nsent from different actors, and the absence of synchronization via en\u00adcapsulation of local data, actors \nexpose inherent concurrency and can work in parallel with other actors.  2.3 Limitations of the AFM \nand the AM The AFM is well-suited to exploit parallelism from de\u00adterministic interaction patterns; however \nmany algorithms and applications involve interaction patterns that are non\u00addeterministic. For example, \nin producer-consumer appli\u00adcations the production or consumption of an individual item may exhibit deterministic \nparallelism; however in\u00adteractions between multiple producers and consumers are non-deterministic in \ngeneral. Another example is Quick\u00adsort which exhibits both deterministic (creating the left and right \nfragments around the partition element) and nonde\u00adterministic (availability of sorted left and right \nfragments) forms of task parallelism. General AFM implementations (e.g. CnC [3], DDFs [28]) cannot exploit \nthe inherent non\u00ad determinism in the arrival of results from the subtasks (see Figure 3 for an example). \nThe AM can exploit this nonde\u00ad terminism in the arrival of results from subtasks as well as guarantee \nsynchronized access while computing partial re\u00adsults. The uni.ed model can be used to exploit both forms \nof parallelism fairly simply by creating relatively inexpen\u00adsive asyncs when required and using actors \nto manage the non-determinism in the availability of partial results. In the AM, simulating non-blocking \nsynchronous replies requires some amount of effort mainly due to the lack of a guarantee of when a given \nmessage will be processed and the need to temporarily disable processing of other messages at the actor \nwaiting for the reply. Similarly, achieving global consensus among a group of actors is a non-trivial \ntask; for example, one approach requires implementing coordinator actors (which can become a sequential \nbottleneck) for track\u00ading the .ow of messages in the coordinated actors. Such co\u00adordination patterns \nare simple in the AFM, for example by using finish to wrap asyncs or by using phasers [26] as communication \nbarriers. Additionally, these AFM constructs are implemented in a scalable manner to avoid bottlenecks. \nAs another example, the pipeline pattern is a natural .t with the AM since each stage can be represented \nas an ac\u00adtor. The single-message-processing rule ensures that each stage/actor processes one message \nat a time before hand\u00ading it off to the next actor in the pipeline. However, the amount of concurrency \n(parallelism) in a full pipeline is lim\u00adited by the number of stages. One way to increase the avail\u00adable \nparallelism, apart from creating more stages, is to in\u00adtroduce parallelism within the stage, achieved \nin the uni.ed model by spawning asyncs. The slowest stage is usually the bottleneck in a pipeline, increasing \nthe parallelism can help speed up the slowest stage in the pipeline and improve per\u00adformance. An example \nfor such a pipeline is the Filterbank benchmark (from StreamIt [30]) in which the .nite impulse response \n(FIR) stage is the slowest stage and is discussed further in Section 3.4. 3. Overview of Parallel Constructs \nIn this section, we brie.y summarize the syntax (in Habanero-Java and Habanero-Scala) of the parallel \nconstructs we use in the following sections to explain various features of the uni.ed model. The three \nmain constructs are: async and finish,  data-driven futures, and  actors.  In the uni.ed model, \neach of these constructs has .rst-class status and can be arbitrarily composed with the others. We end \nthis section with a motivating example displaying a composition of these constructs. 3.1 async and finish \nIn the uni.ed model (and the AFM), tasks are created at fork points using the async keyword. The statement \nasync (stmt) causes the parent task to create a new child task to execute (stmt) (logically) in parallel \nwith the parent task [4]. The scheduling of tasks created by asyncs on actual threads is done by the \nruntime and is transparent to the user and to the tasks in the program. Figure 1: Fork-Join Parallelism \nachieved by forking new tasks and joining before proceeding. Note that un\u00adtil all forked tasks (Task \nA, Task B, Task B1, and Task B2) reach the join point, Task C cannot be executed. [source=http://www.coopsoft.com/ar/ForkJoinArticle.html]. \n1 /*** Habanero-Java code *** / 2 public class ForkJoinPrimer { 3 // An implicit global finish wraps \nmain() which 4 // must wait for all nested tasks to terminate 5 public static void main ( String args \n[]) { 6 System . out . println ( Task O ); // Task-O 7 finish { 8 async { / / Task-A 9 System . out . \nprintln ( Task A );  10 } 11 async { / / Task-B 12 System . out . println ( Task B ); 13 async { / / \nTask-B1 created by Task-B 14 System . out . println ( Task B1 ); 15 } 16 async { / / Task-B2 created \nby Task-B 17 System . out . println ( Task B2 ); 18 }}} // Wait for tasks A, B, B1 and B2 to finish 19 \nSystem . out . println ( Task C ); // Task-C 20 }} Figure 2: HJ version of the Fork-Join program from \nFigure 1  The finish keyword is used to represent a join opera\u00adtion. The task executing finish (stmt) \nhas to wait for all child tasks created inside (stmt) to terminate before it can proceed. A program is \nallowed to terminate when all tasks nested inside the global .nish terminate. Figure 2 shows an example \nHJ program using the async and finish con\u00adstructs to represent the fork and join constraints of tasks \nin Figure 1.  3.2 Data-Driven Futures (DDFs) DDFs are an extension to futures to support the data.ow \nmodel [28]. They support a single assignment property in which each DDF must have at most one producer \nand any async can register on a DDF as a consumer causing the ex\u00adecution of the async to be delayed until \na value becomes available in the DDF. There are three main operations al\u00adlowed on a DDF: put(some-value): \nassociates a value with the DDF. Only a single put() is allowed on the DDF during the execution of the \nprogram.  await(): used by asyncs to delay their execution until some other task has put() a value into \nthe DDF.  get(): used to retrieve the value stored in the DDF. It can legally be invoked by a task that \nwas previously awaiting on the DDF. This guarantees that if such a task is now executing, there was already \na put() and the DDF is now associated with a value.  The exact syntax for an async waiting on DDFs is \nas fol\u00adlows: asyncAwait(ddf1, ..., ddfN ) (stmt). Figure 3 shows the implementation of Quicksort using \nasyncs and DDFs.  3.3 Actors In the uni.ed model, actors are de.ned by extending an ac\u00adtor base class. \nConcrete sub-classes are required to imple\u00adment the method used to process messages3. Actors are like \nother objects and can be created by a new operation on con\u00adcrete classes. An actor is activated by the \nstart() method, after which the runtime ensures that the actor s message pro\u00adcessing method is called \nfor each message sent to the ac\u00adtor s mailbox. The actor can terminate itself by calling the exit() method \nwhile processing a message. Messages can be sent to actors from actor code or non-actor code by in\u00advoking \nthe actor s send() method using a call as follows, someActor.send(aMessage).A send() operation is non\u00adblocking \nand the recipient actor processes the message asyn\u00adchronously. As in the AM, there are no guarantees \non the order of message delivery in the uni.ed model. However, in our implementations (HJ and HS) the \nruntime preserves the order of messages with the same sender task and receiver ac\u00adtor, but messages from \ndifferent senders may be interleaved 3 This method is named process() in HJ light actors while it is \nnamed act() and behavior() in HS heavy and light actors, respectively. 1 2 3 4 5 6 7 8 9 10 11 13 14 \n15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 /* ** Habanero-Scala code *** / object QuicksortApp extends \nHabaneroApp {val input : ListBuffer [ Int ] = ... val resDdf = ddf [ ListBuffer [ Int ]]() finish { asyncAwait \n( resultDdf ) { val sortedList = resDdf . get () ... } quicksort ( input , resDdf ) } private def quicksort \n( data , resultDdf )= { if ( data . length < 1) { resultDdf . put ( data ) } else {val pivot = ... val \n( ddfL , ddfR ) = (ddf () , ddf ()) async { // asynchronously sort left fragment quicksort ( filter (<, \ndata , pivot ), ddfL ) }async { // asynchronously sort right fragment quicksort ( filter (>, data , pivot \n), ddfR ) } val eqs = filter (== , data , pivot ) asyncAwait ( ddfL , ddfR ) { // wait for both left \nand right to complete val res = ddfL . get () ++ eqs ++ ddfR . get () resultDdf . put ( res ) }}}} Figure \n3: HS version of Quicksort using DDFs. The async at line 26 is triggered only after a value is put into \nboth the DDFs (at line 15 or line 29 by the recursive calls) it is awaiting on. As mentioned in Section \n2.3, we cannot nondeterministically precom\u00ad pute partial results depending on whether the left or right \nfragments are available early. in an arbitrary order. This is similar to the message order\u00ading provided \nby ABCL [35]. Figure 4 shows a HelloWorld example using HJ actors. 1 /*** Habanero-Java code *** / 2 \npublic class HelloWorld { 3 public static void main ( final String [] args ) { 4 final UnifiedActor printActor \n= new PrintActor () ; 5 finish { 6 printActor . start () ; 7 printActor . send ( Hello ) ; 8 printActor \n. send ( World ) ; 9 actor . send ( PrintActor . STOP_MSG );  10 } // wait until actor terminates 11 \nSystem . out . println ( PrintActor has terminated ); 12 }} 14 class PrintActor extends UnifiedActor \n{ 15 static final Object STOP _MSG = new Object () ; 16 protected void process ( final Object msg ) { \n17 if ( STOP _MSG . equals ( msg )) { 18 exit (); 19 } else { 20 System . out . println ( msg ); 21 }}} \n Figure 4: HelloWorld using HJ actors. We are guaranteed or\u00addered sends, i.e. though Hello and World \nwill be processed asyn\u00adchronously, they will be processed in that order. 1 /*** Habanero-Scala code \n*** / 2 object FilterBankApp extends HabaneroApp { 3 finish { 4 ... 5 val sampler = ... 6 val fir = new \nFirFilter (... , sampler ). start ()  7 ... 8 }}  9 class FirFilter (... , nextStage : UnifiedActor \n) 10 extends UnifiedActor { 11 ... 12 def behavior () = { 13 case FirItemMessage ( value , coeffs )=> \n 14 ... 15 val numHelpers = ... // number of helper tasks 16 // allocate the DDFs to use 17 val stores \n= Array . tabulate ( numHelpers ) { 18 index => ddf [ Double ]() } 19 finish { 20 // compute the sum \nusing divide -and-conquer 21 (0 until numHelpers ) foreach { helperId => 22 val myDdf = stores ( helperId \n) 23 async { 24 val ( start , end ) = ... 25 var sum : Double = 0.0 26 start until end foreach { index \n=>  * 27 sum += buffer ( index ) coeffs ( index ) 28 } 29 myDdf . put ( sum ) 30 }} 31 ... 32 // wait \nfor the partial results 33 asyncAwait ( stores ) { 34 // propagate the sum down the pipeline 35 val sum \n= stores . foldLeft (0.0) { 36 ( acc , loopDdf )=> acc + loopDdf . get () 37 } 38 nextStage . send ( \nDataItemMessage ( sum )) 39 }} 40 case ... => ... 41 }} Figure 5: HS version of the FIR stage in the \nFilter Bank pipeline. In this example, the computation of the dot product between the coef.cients and \na local buffer has been parallelized to speedup this stage in the application.  3.4 Composing the constructs \nWe now discuss an example application in which these constructs can be composed in ways that cannot easily \nbe achieved with current programming models. This example, shown in Figure 5, is the FIR stage of the \nFilter Bank appli\u00ad cation. The application represents a pipeline and each of the stages can be represented \nusing actors. The FIR stage is the slowest stage in the pipeline and limits the pipeline rate, the performance \ncan be improved by speeding up this stage by parallelizing it. We do so by partitioning the computation \nof the dot product using asyncs (line 23 in the example). Each async computes the dot product of a partition \nbefore writ\u00ading back the result into its assigned DDF (line 29), avoiding the possibility of a data races. \nThe async at line 33 awaits on the results in the DDFs to be available before computing the .nal result \nand propagating the value to the next stage in pipeline. All the spawned asyncs between line 22 and line \n39 join to their IEF, the finish at line 19. This ensures the FIR stage does not start processing the \nnext message until it has completed processing the current message and has prop\u00adagated values to the \nnext stage in the pipeline. While such parallelism in the FIR stage could be simulated in the AM, it \nrequires distributing the logic among multiple actors and signi.cantly complicates the code as the actor \nrepresenting the FIR stage needs to maintain additional state to track the arrival of partial results \nand maintain the order of values it passes along the pipeline (the AM does not guarantee the order in \nwhich messages will be serviced). In addition, there will be overhead associated with the data copying \nrequired to send the data fragments to the helper actors. Compar\u00adatively, the use of asyncs and finish \navoids such draw\u00adbacks making the code easier to maintain and helping with productivity. 4. The Uni.ed \nModel Although both the AFM and AM have existed as indepen\u00addent parallel programming models for a while \nnow, we are unaware of previous efforts to systematically combine these two models. We integrate the \nAFM and the AM so as to get the bene.ts of actor coordination construct in the AFM and also of parallelizing \nmessage-processing within actors. In this section, we describe how actor message processing and async-finish \ntasks can be integrated in the uni.ed model and the bene.ts of this integration. 4.1 Coordination of \nActors in the Uni.ed Model Integrating actors and tasks requires understanding how the actor life cycle \ninteracts with task creation and termination events. The creation of an actor is a simple operation and \ncan be performed synchronously inside the task executing the action. Similarly, terminating the actor \nis a synchronous operation that can be effected by an actor on itself while it is processing a message. \nOnce an actor enters the terminated state, it avoids processing any messages sent to it without blocking \nthe sender (such messages are effectively no-ops and do not need to be placed in the mailbox). Since \ntasks always execute inside an enclosing .nish scope, both these operations are easily mapped to the \nAFM. The more interest\u00ading case is handling the actions of the actor while it is active in the started \nstate and processing messages. Starting an actor activates it and allows it to continu\u00adously receive \nmessages and to process these messages one at a time. This operation can hence be represented as an asynchronous \ntask whose body is a long-running loop which keeps processing one message at a time from its mailbox \nun\u00adtil it is terminated. This newly spawned asynchronous task inherits the IEF (as per normal async-finish \nsemantics) of the task which started the actor. The long-running loop in the task enforces the IEF to \nblock until all actors started in\u00adside it terminate. In Section 6.1 we present the lingering task technique \nwhich avoids having to explicitly use the long\u00adrunning loop mentioned above (with its accompanying over\u00adheads \nwhen the mailbox is empty). 1 /*** Habanero-Java code *** / 2 public class HelloWorld2 { 3 public static \nvoid main ( final String [] args ) { 4 final UnifiedActor printActor = new . PrintActor () . ; 5 async \n{ 6 finish { // F1, IEF for printActor 7 ... 8 printActor . start () ; // similar to an async  9 ... \n10 }  11 System . out . println ( PrintActor terminated ); 12 } 13 async { 14 finish { // F2 15 ... \n16 // task T2 17 printActor . send ( Hello ) ; 18 printActor . send ( World ) ; 19 printActor . send \n( PrintActor . STOP_MSG );  20 ... 21 }  22 System . out . println ( Done sending messages ); 23 }}} \n Figure 6: HelloWorld example with printActor, executing in finish scope F1, receiving messages from \na different finish scope, F2. We now discuss the actions to be performed after the ac\u00adtor has started \nand is receiving and processing messages. By de.nition, actors process the messages they receive asyn\u00adchronously. \nThis translates to the creation of a new task that processes the message and runs in parallel with the \ntask that initiated the send of the message. Under normal async\u00adfinish semantics, this means that both \nthese tasks share the same IEF. Now, consider the case where an actor is receiv\u00ading messages from a task/actor \nexecuting in a different IEF, as shown in Figure 6. Under normal async-finish seman\u00adtics, when T2 sends \na message to printActor it places the message in printActor s mailbox and creates a new task, say T3, \nto process this message. This causes F2 to unneces\u00adsarily (and incorrectly) block until T3 completes. \nSince the message will end up in printActor s mailbox, the process\u00ading of the message is done by printActor \nand semantically T3 should have F1 as its IEF as opposed to F2. Hence, when T2 sends a message to printActor, \nthe new asynchronous task must be spawned in the finish scope of printActor. In the uni.ed model, this \ngeneralizes to all asynchronous tasks spawned to process a message inheriting the IEF of the recipient \nactor. Note that this ability to attach a different finish scope while spawning a task is a feature of \nthe uni\u00ad.ed model which is unavailable in the general AFM. The use of newly spawned tasks to send messages \nis also facilitated by the fact that no message-ordering restrictions apply in the AM and these spawned \ntasks can thus be executed in any or\u00adder. In addition, since the new task inherits the finish scope of \nthe recipient actor, it allows the sender to be any arbitrary task executing under the uni.ed model. \n Mapping the entire life cycle of actors into the AFM provides a clean and transparent mechanism to detect \nthe termination of actors. Some actor implementations on the Java VM (JVM) (e.g., Scala Actors library \n[10], Kilim [27], Jetlang [23]) require the user to write explicit code to detect whether an actor has \nterminated before proceeding with the rest of the code in the control .ow. A common pattern is to explicitly \nuse countdown latches and wait on the latch until the count reaches zero. In programs written using the \nAFM, a similar effect is achieved by joining tasks inside their finish scope without the programmer having \nto worry about low-level synchronization constructs such as latches. Consequently, mapping actors to \na finish scope provides a transparent mechanism to detect actor termination and relieves the user from \nwriting boiler plate code. Figure 7 shows a simple PingPong example using the uni.ed actors and the finish \nconstruct to detect termination easily. The Scala version (7a) needs to maintain a latch and pass it \naround to the different actors, while the main thread waits on the latch. In addition, actors need additional \nlogic to decrement the count on the latch. The use of such shared latches breaks the pure actor model \nby not encapsulating state. On the other hand, the uni.ed model example (7b) bene.ts from the finish \nconstruct. Terminating the actor using the call to exit noti.es the IEF that the actor has terminated \nand the statements following the finish are free to proceed (when all other spawned tasks inside the \nfinish scope have also completed). The actor no longer worries about the cross-cutting concern of invoking \nmethods on a latch. 4.2 Desirable Properties Actors in the uni.ed model continue to encapsulate their \nlo\u00adcal state and process one message at a time. Thus the bene.ts of modularity are still preserved. Similarly, \nthe data locality properties of the AM continue to hold. Actors also introduce a means of a new coordination \nconstruct in the AFM in ad\u00addition to the existing constructs such as futures, DDFs, and phasers. With \nactors inside the AFM, it is now possible to create arbitrary computation DAGs impossible in the pure \nAFM. Since actors have been integrated into the AFM, ac\u00adtors can co-exist with any of the other constructs \nin the AFM, and they can be arbitrarily nested. The implementation of the receive operation using DDFs \n(mentioned in Section 5.2) is an example of this. 1 /*** Scala code *** / 2 object ScalaActorApp extends \nApp { 3 val latch = new CountDownLatch (2) 4 val pong = new PongActor ( latch ) . start () 5 val ping \n= new PingActor ( msgs , pong , latch ) . start () 6 ping ! StartMsg 7 latch . await () 8 println ( Both \nactors terminated ) 9 } 10 // class PingActor not displayed 11 class PongActor ( latch : CountDownLatch \n) extends Actor { 12 var pongCount =0 13 def act () { 14 loop { react { 15 case PingMessage => 16 sender \n! PongMessage 17 pongCount = pongCount +1 18 case StopMessage => 19 latch . countDown () 20 exit (' stop \n) 21 }}}} (a) Actor Model (Scala) 1 /*** Habanero-Scala code *** / 2 object LightActorApp extends HabaneroApp \n{ 3 finish { 4 val pong = new PongActor () . start () 5 val ping = new PingActor ( msgs , pong ). start \n() 6 ping . send ( StartMessage () ) 7 } 8 println ( Both actors terminated ) 9 } 10 // class PingActor \nnot displayed 11 class PongActor extends UnifiedActor { 12 var pongCount =0 13 override def behavior \n() = { 14 case PingMessage ( sender )=> 15 sender . send ( PongMessage () ) 16 pongCount = pongCount \n+1 17 case StopMessage => 18 exit () 19 }} (b) Uni.ed Model (Habanero-Scala) Figure 7: Implicit actor \ntermination detection using finish in the uni.ed model (Figure 7b). Note the elegant syntax for Habanero-Scala \nwith pattern matching as opposed to the use of instanceof which are required in Habanero-Java. 5. New \nCapabilities in the Uni.ed Model With the uni.ed model in place, there are a number of constructs that \ncan now be supported in the AFM. The key to each of these constructs is the ability to reason about the \nenclosing finish under which the actors execute. Some of these constructs are presented below. 5.1 Parallelization \ninside Actors The requirement that the actor must process at most one message at a time is often misunderstood \nto mean that the processing must be done via sequential execution. In fact, there can be parallelism \nexposed even while processing mes\u00adsages as long as the invariant of processing at most one mes\u00adsage at \na time is maintained. One advantage of integrating the AFM and the AM is that it allows us to use async\u00adfinish \nconstructs inside the message-processing code to expose this parallelism. There are two main ways in \nwhich this is achieved, discussed below: Using finish constructs during message processing and  Allowing \nescaping async tasks.  5.1.1 Using finish during message processing The traditional actor model already \nensures that the actor processes one message at a time. Since no additional restric\u00adtions are placed \non the message-processing body (MPB), we can achieve parallelism by creating new async-finish constructs \ninside the MPB. In this approach, we can spawn new tasks to achieve the parallelism at the cost of block\u00ading \nthe original message-processing task at the new finish. Since the main message-processing task only returns \nafter all spawned tasks inside the finish have completed, the invariant that only one message is processed \nat a time is maintained. Figure 8 shows an example code snippet that achieves this. Note that there is \nno restriction on the con\u00adstructs used inside the newly constructed finish. As such, all the async-finish \ncompliant coordination constructs can also be used. 1 /*** Habanero-Scala code *** / 2 class ParallelizedActor \n() extends UnifiedActor { 3 override def behavior () = { 4 case msg : SomeMessage => 5 // optional preprocessing \nof the message 6 finish { // ensures spawned tasks complete 7 async { ... /* processing in parallel */ \n} 8 async { ... /* more parallel processing */ } 9 }10 // optional post processing after finish 11 ... \n12 }} Figure 8: An actor exploiting the async-finish parallelism inside actors message-processing body. \nThe nested finish ensures no spawned tasks escape, thereby ensuring that an actor does not process multiple \nmessages at a time. 5.1.2 Allowing escaping asyncs during message processing Requiring all spawned asyncs \nto be contained in a single MPB instance is too restrictive. This constraint can be re\u00adlaxed based on \nthe observation that the at most one message\u00adprocessing rule is required to ensure there are no internal \nstate changes of an actor being affected by two or more message-processing tasks of the same actor. As \nlong as this rule is obeyed, escaping asyncs (tasks) can be allowed in\u00adside the MPB. We can achieve this \ninvariant by introducing a paused state in the actor life cycle and by adding two new operations: pause \nand resume. In the paused state, the ac\u00adtor is not processing any messages from its mailbox. The actor \nis simply idle as in the new state; however, the actor can continue receiving messages from other actors. \nThe ac\u00adtor will resume processing its messages, at most one at a time, when it returns to the started \nstate. The pause oper\u00adation takes the actor from a started state to a paused state while the resume operation \nachieves the reverse. The actor is also allowed to terminate from the paused state using the exit operation. \nThe pause and resume operations are sim\u00adilar to the wait and notify operations in Java threads for coordination. \nSimilar to the restriction that thread coordina\u00adtion operations can only be executed by the thread owning \nthe monitor, pause and resume operations on an actor can only be executed in tasks spawned within an \nactor either ex\u00adplicitly by the user or implicitly by the runtime to process messages (e.g., the MPB \ntask). However, unlike the thread coordination operations neither the pause nor the resume operations \nare blocking, they only affect the internal state of the actor that coordinates when messages are processed \nfrom the actor s mailbox.  Figure 9: Actor life cycle extended with a paused state. The actor can now \ncontinually switch between the started and paused states using the pause and resume operations. With \nthe two new operations, we can now allow spawned tasks to escape the MPB task. These spawned tasks are \nsafe to run in parallel with the next message-processing task of the same actor as long as they are not \nconcurrently affecting the internal state of the actor. The actor can be suspended in a paused state \nwhile these spawned tasks are executing and can be signaled to resume processing messages once the spawned \ntasks determine they will no longer be modifying the internal state of the actor and hence not violating \nthe one message-processing rule. Figure 10 shows an example in which the pause and resume operations \nare used to achieve parallelism inside the MPB while delaying the processing of the next message in the \nactor s mailbox. 1 /*** Habanero-Scala code *** / 2 class EscapingAsyncsActor () extends UnifiedActor \n{ 3 override def behavior () = { 4 case msg : SomeMessage => 5 async { /* do some processing in parallel \n*/ } 6 // preprocess the message 7 pause () // delay processing the next message 8 // pause/resume is \nnot thread blocking 9 async { 10 // do some more processing in parallel 11 // safe to resume processing \nother messages 12 resume () 13 // some more processing 14 } ... 15 }} Figure 10: An actor exploiting \nparallelism via asyncs while avoid\u00ading an enclosing finish. The asyncs escape the MPB, but the pause \nand resume operations control processing of subsequent messages by the actor. Unfortunately, the ability \nto spawn new tasks inside the actor s MPB creates the potential to introduce data races, since multiple \ntasks can be working on the actor s local data. In fact, data races are also possible in AM implementations \nwhich do not guarantee data isolation. We plan on extending the Scalable Parallel Dynamic Datarace Detection \n(SPD3) algorithm [22] for the AFM to the uni.ed model for data race detection. Introducing the pause \nand resume operations also increases the possibility of reaching deadlocks. If an actor is never resumed \nafter it has been paused, the actor will never terminate and hence the IEF will block inde.nitely. Like \nthe AM, under the uni.ed model it is required that every actor terminate, e.g., by a call to exit. Terminating \nactors explicitly is required so that the IEF for an actor does not block inde.nitely.  5.2 Non-blocking \nreceive operations Implementing the synchronous receive operation4 often in\u00advolves blocking and can limit \nscalability in virtual machines that do not allow explicit call stack management and con\u00adtinuations. \nFor example, the implementation of receive in the Scala actor library involves blocking the currently \nexe\u00adcuting thread and degrades performance. The alternate ap\u00adproach requires use of exceptions to unwind \nthe stack and maintain control .ow, as in Scala s react construct, and is also relatively expensive. \n1 /*** Habanero-Scala code *** / 2 class ActorPerformingReceive extends UnifiedActor { 3 override def \nbehavior () = { 4 case msg : SomeMessage => 5 ... 6 val theDdf = ddf [ ValueType ]() 7 anotherActor \n! new Message ( theDdf ) 8 pause () // delay processing next message 9 asyncAwait ( theDdf ) {  10 val \nresponseVal = theDdf . get () 11 // process the current message 12 ... 13 resume () // enable next message \nprocessing 14 } 15 // return in paused state 16 }} Figure 11: An actor in the uni.ed model that uses \nData-Driven Fu\u00adtures (DDFs) to perform the receive operation without blocking. The actor that processes \nthe message needs to perform a put of a value on the DDF to trigger the waiting async (in asyncAwait). \nWhen the async is triggered, the actor processes the value in the DDF and performs the resume operation \nto continue processing subsequent messages. With the support for pause and resume, the receive op\u00aderation \ncan now be implemented in the uni.ed model with\u00adout blocking threads or using exceptions. This requires \nsup\u00adport of the DDF coordination construct. DDFs allow the ex\u00adecution of the async to be delayed until \na value is avail\u00adable in the DDF. A DDF can be passed along to the actor which .lls the result on the \nDDF when it is ready. Mean\u00adwhile the actor that sent the DDF can pause and create an async which waits \nfor the DDF to be .lled with a value 4 Synchronous receives are called now type messages in ABCL [35]. \n and can resume itself. Figure 11 shows an example of a non-blocking receive implementation. This presents \nan in\u00adstance of the sender and the recipient actors coordinating with each other without explicit message-passing \nand thus violates the pure AM. Non-blocking receives present an excellent case in which constructs from \nthe two different models, AFM and AM, can work together to ease the im\u00adplementation of other nontrivial \nconstructs. Figure 16 intro\u00ad duces syntactic sugar that can be used to implement the syn\u00adchronous receive \noperation.  5.3 Stateless Actors Stateless actors can process multiple messages simultane\u00adously since \nthey maintain no mutable internal state. It is easy to create such actors in the uni.ed model using escap\u00ading \nasyncs as shown in Figure 12. There is no need to use the pause operation, and the escaping async tasks \ncan pro\u00adcess multiple messages to the same actor in parallel. State\u00adless actors can be used to implement \nconcurrent reads in a data structure which would not be possible in most actor im\u00adplementations since \nthe message-processing would be com\u00adpletely serialized. 1 /*** Habanero-Scala code *** / 2 class StatelessActor \n() extends ParallelActor { 3 override def behavior () = { 4 case msg : SomeMessage => 5 async { processMessage \n( msg ) } 6 if ( enoughMessagesProcessed ) { exit () } 7 // return immediately to process next message \n 8 }} Figure 12: A simple stateless actor created using the uni.ed model. The message processing body \nspawns a new task to process the cur\u00adrent message and returns immediately to process the next message. \nBecause the async tasks are allowed to escape, the actor may be processing multiple messages simultaneously. \n6. Implementation We developed two implementations of the uni.ed model described in Section 4, namely \nHabanero-Java (HJ) and Habanero-Scala (HS). We extended both HJ and HS to in\u00adclude the uni.ed actor coordination \nconstruct. In our imple\u00admentations, any of the AFM compliant constructs can be ar\u00adbitrarily nested with \nthese uni.ed actors and vice versa. Both the actor implementations rely on the use of lingering tasks \nto integrate actors into the AFM. We explain the lingering tasks technique before introducing the two \nimplementations. 6.1 Lingering Tasks Section 4.1 explained how to map actors to tasks. There starting \nan actor was likened to a long-running asynchronous task processing one message at a time. However, such \na long-running task would waste resources as it would be involved in some sort of busy waiting mode until \na message arrives. The purpose of this long-running task is to attach the actor s MPB to an IEF; a more \nef.cient technique is to use a lingering task. A lingering task is a task with an empty body that attaches \nitself to an IEF like a normal asynchronous task spawned inside a .nish scope. Thus, the .nish scope \nis aware of the existence of this task and will block until the task is scheduled and executed. However, \nthe lingering task does not make itself available for scheduling immediately (unlike normal asynchronous \ntasks) and thus forces the IEF to block under the constraints of the AFM5. At some later point in time, \nthe lingering task will be scheduled and executed, allowing the .nish scope to complete execution and \nmove ahead. The lingering task provides a hook into its .nish scope that may be used to spawn more tasks. \nAll these spawned tasks execute under the same IEF as the lingering task. When a uni.ed actor is started, \na lingering task is created by the runtime and stored in the actor. This allows the actor to con\u00adtinue \nspawning subsequent tasks under the same IEF when it asynchronously processes messages sent to it. When \nthe ac\u00adtor terminates, the runtime schedules the lingering task for execution. Once the lingering task \nhas been scheduled, the actor stops creating any further asynchronous tasks realiz\u00ading that the IEF may \nno longer be available to spawn tasks. This is consistent with the notion that termination of an actor \nis a stable property, and the actor is not allowed to process messages once it terminates. Any messages \nsent to a termi\u00adnated actor are ignored and no task is created to process the message.   6.2 Habanero-Java \nHabanero-Java (HJ) already provides AFM constructs, we extended HJ with an actor coordination construct \nwhich we refer to as light actors. Light actors support all the features in the uni.ed model discussed \nin Section 5. Light actors are started using a call to start() and the MPB is triggered only on the messages \nthey receive. Light actors do not use exceptions to manage the control .ow and require the user to implement \na process() method to determine the steps to execute while processing a message. If any uncaught ex\u00adception \nis thrown by process(), then the actor terminates itself by calling exit() and the exception is collected \nby its enclosing finish scope which then throws a MultiExcep\u00adtion [5]. An actor can explicitly terminate \nitself by calling exit() while processing a message. The implementation of light actors relies on the \nuse of data-driven controls to implement the mailbox. The mailbox supports a push-based implementation \nwhere asyncs are created without the runtime having to poll (i.e. pull-based) the actor s mailbox to \ndecide when to launch an async to process messages. 5 A .nish scope can only complete after all its transitively \nspawned tasks have completed. 6.2.1 Data-Driven Controls (DDCs) A Data-Driven Control (DDC) lazily binds \na value and a block of code called the execution body (EB). When both these are available a task that \nexecutes the EB using the value is scheduled. Both the value and the EB follow the dynamic single assignment \nproperty ensuring data-race freedom. Un\u00adtil both .elds are available, the scheduler is unaware of the \nexistence of the task. Figure 13 shows a simpli.ed imple\u00ad mentation of a DDC excluding synchronization \nconstructs. The DDC may be implemented using an asynchronous or a synchronous scheduler. Light actors \nuse both forms of DDCs: asynchronous execution of the task by involving the Habanero scheduler and synchronous \nexecution of the EB. 1 /*** Habanero-Java code *** / 2 class DataDrivenControl { 3 private ValueType \nvalue = null ; 4 private ExecBody execBody = null ; 6 void addValue ( ValueType theValue ) { 7 if (! \nvalueAvailable () ) { 8 value = theValue ; 9 // resume awaiting task 10 if ( execBody != null ) {11 execBody \n. scheduleWith ( value ); 12 }}} 14 void addResumable ( ExecBody theBody ) { 15 if ( valueAvailable () \n) { 16 // value is available , execute immediately 17 theBody . scheduleWith ( value ); 18 } else { 19 \n// need to wait for the value 20 execBody = theBody ; 21 }}} Figure 13: Simpli.ed implementation of \na DDC not including syn\u00adchronization constructs or validations. Both the value and the exe\u00adcution body \ncan be lazily attached. The execution body determines whether scheduling happens synchronously or asynchronously. \nDDCs differ from Tas\u00b8irlar s DDFs [28] in that only a single task may be associated with a value at a \ntime. DDFs apply the dynamic single assignment property only to the value and allow multiple tasks to \nbe waiting for the value. In addition, the scheduler is aware of the existence of these data-driven tasks \n(DDTs) and causes the finish scopes of the tasks to block until the DDTs are scheduled. In contrast, \nwith DDCs the scheduler is unaware of the existence of the task until it is scheduled, and only then \nwill it schedule and execute the task. This may lead to issues with the finish scope of the activity \nin the asynchronous scheduler, but we will see below that coupling the DDC with the lifespan of the lingering \ntask avoids the potential problem.  6.2.2 The Mailbox: Linked List of DDCs The mailbox for the light \nactors is implemented as a linked list of DDCs (Figure 14). As messages are sent to the actor, the chain \nof DDCs are built with each message populating the value .eld of a DDC. The linked list is concurrent \nand multiple messages can be sent to an actor safely. Light actors guarantee that the order of the messages \nsent from the same actor will be preserved in the mailbox. No guarantee is pro\u00advided for the order of \nmessages in the mailbox for messages sent from different actors.  Figure 14: The actor mailbox is represented \nas a linked list of DDCs. The message head determines where the next message is stored, while the body \nhead determines which message is being processed currently. Once the actor has started (via the call \nto start()), it proceeds to traverse the messages in the mailbox, one at a time, lazily attaching some \nexecution logic as the EB of the DDC. As each EB executes it attaches new execution logic to the next \nDDC in the list. Since at any time only one DDC is actively executed, the guarantee that only one message \nis processed at a time is provided. Attempts are made to synchronously execute the EB when messages are \navailable in the DDC. If a message is unavailable at the DDC pointed by the body head (Figure 14), the \nEB is set up to execute asynchronously in a task when a message arrives. The lingering activity gains \naccess to the finish scope to which this asynchronous task needs to join. When the asynchronous task \nis ultimately scheduled and executed, the body head of the mailbox is moved ahead and the next DDC processed. \nIf the actor was terminated via a call to exit() in the EB of the DDC the actor stops processing messages \nfrom its mailbox, no more asynchronous tasks are scheduled by the actor, the lingering task is scheduled \nand subsequent messages sent to the actor are ignored. 6.2.3 Supporting pause and resume with DDCs Light \nactors support the pause and resume operations ex\u00adplained in Section 5.1.2 using synchronous DDCs. A \ncall to pause() changes the state of the actor to paused. Be\u00adfore processing the next message in the \nmailbox, we check whether the actor is in a paused state. If so, the next message from the mailbox is \nnot processed. Instead, a block of code to process the next message from the mailbox is created and setastheEBof \na pause-resume DDC. When resume() is called, the state of the actor is reset, and the pause-resume DDC \nis provided a value to synchronously trigger the exe\u00adcution of the EB.  6.3 Habanero-Scala Habanero-Scala \n(HS) is an extension of the Scala lan\u00adguage [18] with AFM compliant constructs. In HS, the AFM constructs \nwere added as a library and an existing actor im\u00adplementation (standard Scala actors) extended to support \nthe uni.ed model. We refer to these uni.ed actors as heavy ac\u00adtors. Heavy actors provide support the \noperations presented in the uni.ed model excluding the pause and resume op\u00aderations. In addition, we \nhave also ported the light actor implementation into HS. An important reason to choose Scala is its \nsupport for powerful abstractions to express various programming con\u00adstructs. One such construct, pattern-matching, \nis an elegant way to write actor code since the MPB needs to pattern\u00admatch on the messages received by \nthe actor. Scala also has a relatively lenient constraint on the naming of methods, which coupled with \nits expressiveness makes it extremely easy to create domain-speci.c languages. This allows for easy transition \nof HJ constructs into Scala without the need to build a front-end compiler. Most of the Habanero work\u00adsharing \nruntime can be reused in HS since both HJ and Scala run on the Java Virtual Machine. 6.3.1 Heavy Actors \nHeavy actors are an extension of the standard Scala actors. These are called heavy actors since their \nimplementation involves more overhead than the light actors presented in Section 6.2. To support operations \nlike receive (called react for event-based Scala actors) and to avoid blocking, Scala actors throw exceptions \nto roll back the call stack and to allow the underlying thread to process messages of other actors. The \nneed to throw and then ultimately catch these exceptions, even without the overhead of building the stack \ntrace, is relatively expensive compared to an implementation that does not rely on the use of exceptions \nfor control .ow. 1 /*** Habanero-Scala code *** / 2 package edu . rice . habanero 3 ... 4 trait HabaneroActor \nextends Actor { 6 // custom scheduler to create asynchronous 7 // tasks under IEF of lingering activity \n8 var lingeringActivity : HabaneroActivity = null 9 val habaneroExecutor = ... 10 override def scheduler \n= habaneroExecutor 11 ... 12 override def start () = { 13 // activity causes the IEF to wait on this \nactor 14 lingeringActivity = ... 15 // delegate to the parent implementation 16 super . start () 17 \n} 18 override def exit () : Nothing = { 19 // schedule activity allowing IEF to terminate 20 resumeWaitingActivity \n( lingeringActivity ) 21 // delegate to the parent implementation 22 super . exit () 23 }} Figure 15: \nHeavy actors in HS extend the standard Scala Actor trait. The start and exit events are used to maintain \nsome book\u00adkeeping for the heavy actors and to interact with the Habanero runtime to schedule and execute \ntasks. The lingering activity is explained in Section 6.1. The heavy actor in HS is implemented as a \ntrait that extends the standard Scala Actor trait (Figure 15). HS heavy actors do not support the pause \nand resume operations explained in Section 5.1.2. However, they support all the other AFM compliant constructs \ninside the MPB including finish, async, futures, etc. HS heavy actors still need to rely on exceptions \nfor control .ow and explicit management of the actor continuations, both implemented in the standard \nactors, and are thus more expensive to operate than the corresponding light actors. 6.3.2 Light Actors \nHS also includes an implementation of light actors. They extend the features of HJ light actors by using \nScala s pat\u00adtern matching construct to represent the message processing body. While the pattern matching \nconstruct is more elegant it also entails a performance penalty compared to using sim\u00adple instanceof \nchecks used in the HJ actor implementa\u00adtion. Pattern matching also allows us to abstract away the synchronous \nreply operation (Section 5.2) so that the user does not have to manually manage the calls to pause() \nand resume() as shown in Figure 16. 1 /*** Habanero-Scala code *** / 2 abstract class HabaneroReactor \nextends Actor [ Any ] { 3 // updated as each reply message is processed 4 private var replyDdf : DataDrivenFuture \n[ Any ] = null 5 def reply ( msg : Any ): Unit = { 6 if ( replyDdf ne null ) { 7 replyDdf . put ( msg \n) 8 } else { 9 // report error ...  10 }} 11 def awaitReply ( receiver : HabaneroReactor , msg : Any \n, 12 handler : PartialFunction [ Any , Unit ]) : Unit = { 13 // create DDF and message to send to the \nactor 14 val replyMsg = new ReplyMessage ( msg , ddf [ Any ]() ) 15 // disable processing messages from \nthe mailbox 16 receiver . send ( replyMsg ) ; pause () 17 // await reply from the receiver actor 18 asyncAwait \n( replyMsg . replyDdf ) { 19 // process the response message 20 handler ( replyDdf . get () ) 21 // continue \nprocessing further messages 22 resume () 23 }}} Figure 16: Light actors in HS abstract away the synchronous \nreply operation, end-users use the awaitReply() and reply() method invocations in their actor code. HS \nlight actors also support the become and unbecome operations. The become primitive speci.es the behavior \nthat will be used by the actor to process the next message allow\u00ading the actor to dynamically change \nits behavior at runtime. If no replacement behavior is speci.ed, the current behavior will be used to \nprocess the next message. In the pure AM, actors are functional and the become operation provides the \nability for the actor to maintain local state by creating a new actor and becoming this new actor. In \nScala, the same ef\u00adfect can be achieved by having dynamic pattern matching constructs which work in conjunction \nwith mutable member variables. HS light actors support the become and unbecome opera\u00adtions to allow \nthe actor to change its behavior as it processes messages. In addition, the light actor is required to \nde.ne the behavior() operation that provides a default behavior to use while processing messages. All \nthese behaviors are presented as partial functions which Scala provides native support for. The behavior \nhistory is maintained in a stack and the old behavior can be retrieved by an unbecome operation. The \nsupport for become and unbecome is an improvement over the standard Scala actors in which the user has \nto rely the arithmetic mean of thirty execution times (last three from each invocation) are reported. \nThis method is inspired from [8] and the last three execution times are used to ap\u00ad proximate the steady \nstate behavior. In the bar charts, the error bars represent one standard deviation. All actor im\u00adplementations \nof a benchmark use the same algorithm and mostly involved renaming the parent class of the actors (in \nthe Scala and Habanero-Scala versions) to switch from one implementation to the other. 7.2 Microbenchmarks \ncomparing Actor frameworks on manipulation of local state or explicit management of be\u00adhaviors to simulate \nthe same operations. If at any point, the current behavior cannot process a message (i.e. the partial \nfunction is not de.ned for the message), that actor terminates and throws an exception by default; users \ncan customize this and avoid throwing exceptions and terminating. 7. Experimental Results The actor frameworks \nused for comparison with our imple\u00ad mentations all run on the JVM and include Jetlang [23], Average Execution \nTime (in secs) 150 100 50 0 Kilim [27], Scala actors [10], and Akka [31]. Jetlang pro\u00ad vides a low-level \nmessaging API in Java that can be used to build actors with the onus of ensuring the single message processing \nrule delegated to the user. The use of batching while processing actor messages instead of creating a \nnew asynchronous task to process each message in our imple\u00admentation of light actors is inspired by Jetlang. \nKilim is an Number of pings (in millions) Jetlang Kilim  Akka Standard Scala HS heavy HS light (a) \nScala versions which use pattern matching. actor implementation that ensures data isolation as required \nin the AM. Our actor implementations, however, do not sup\u00adport data isolation in messages. Scala includes \nan actor li\u00adbrary that provides event-based actors which allow multiple actors to run on a thread. Our \nactor API is inspired from Scala s event-based actors, however we do not use excep\u00ad tions to maintain \ncontrol .ow and use a push-based imple\u00ad mentation using DDCs for light actors. Akka is a framework for \nbuilding event-driven applications on the JVM and has support for highly performant lightweight actors. \nWe chose not to include Erlang [34] since it does not run on the JVM, but has already been shown to have \nperformance competitive to Kilim and Jetlang [15]. Number of pings (in millions) 7.1 Experimental Setup \nThe benchmarks were run on a 12-core (two hex-cores) 2.8 GHz Intel Westmere SMP node with 48 GB of RAM \nper node (4 GB per core), running Red Hat Linux (RHEL 6.0). Each core had a 32 kB L1 cache and a 256 \nkB L2 cache. The software stack includes a Java Hotspot JDK 1.7, Habanero-Java 1.3.1, Habanero-Scala \n0.1.3, and Scala 2.9.1\u00ad 1. Each benchmark used the same JVM con.guration .ags (-Xmx8192m -XX:MaxPermSize=256m \n-XX:+UseParallelGC -XX:+UseParallelOldGC -XX:-UseGCOverheadLimit) and was run for ten iterations in ten \nseparate JVM invocations, Jetlang Kilim Akka HJ light (b) Java versions which use instanceof operator. \n Figure 17: The PingPong benchmark exposes the throughput and latency while delivering messages. There \nis no parallelism to be exploited in the application. The .rst benchmark (Figure 17) is the PingPong \nbench\u00admark in which two processes send each other messages back and forth. The benchmark was con.gured \nto run using two workers since there are two concurrent actors. This bench\u00admark tests the overheads in \nthe message delivery implemen\u00adtation for actors. The original version of the code was ob\u00adtained from \n[29] and ported to use each of the different ac\u00ad tor frameworks. Scala actors and HS heavy actors have \nthe same underlying messaging implementation but use differ\u00adent schedulers. The HS heavy actors bene.t \nfrom the thread binding support in the Habanero runtime. HS light actors perform better than Scala and \nHS heavy actors because it avoids the use of exceptions to maintain control .ow (as dis\u00adcussed in Section \n6.3.1). Both the Scala and Java versions of Kilim, Jetlang, Akka and light actors bene.t from avoid\u00ading \ngenerating exceptions to maintain control .ow. The Java versions follow the same pattern with Akka and \nlight actors performing the best. In general, the Akka and light actor ver\u00adsions bene.t from the use \nof fork-join schedulers as opposed to threadpool schedulers available in standard implementa\u00adtions of \nKilim and Jetlang actors. Jetlang s Scala version is much slower than the Java version as the Scala implementa\u00adtion \npays the overhead for pattern matching twice as opposed to once in Kilim, Akka and light actors. The \nChameneos benchmark, shown in Figure 18, tests the effects of contention on shared resources (the mailbox \nim\u00adplementation) while processing messages. The Scala imple\u00admentation was obtained from the public Scala \nSVN reposi\u00adtory [11]. The other actor versions were obtained in a man\u00ad ner similar to the PingPong benchmark. \nThe benchmark was run with 500 chameneos (actors) constantly arriving at a mall (another actor) and it \nwas con.gured to run us\u00ading twelve workers. The mailbox implementation of the mall serves as a point \nfor contention. In this benchmark, the bene\u00ad.ts of thread binding are neutralized since the contention \non the mailbox is the dominating factor and since both the Scala and HS heavy actors share the same implementation \nthey show similar performance. Both the Scala and Java versions of Kilim, Jetlang, Akka and light actors \nbene.t from batch\u00adprocessing messages inside tasks and from avoiding generat\u00ading exceptions to maintain \ncontrol .ow. The light actor im\u00adplementations that uses DDCs (Section 6.2.1) outperforms the linked list \nimplementation in actors. Jetlang, which uses iterative batch-processing of messages sent to the mall, \nis in general faster than the light actor implementation which uses recursive batch processing of messages. \nThe Java Grande Forum Fork-Join benchmark [7], shown in Figure 19, measures the time taken to create \nand destroy actor instances. Each actor does a minimal amount of work processing one message before it \nterminates. The Akka implementation is noticeably slower while the Jetlang implementation quickly runs \nout of memory as it uses an ArrayList to maintain the work queue. The heavy actor implementation again \nbene.ts from thread binding support compared to standard Scala actors. The light actor imple\u00admentation \nwhich uses lightweight async tasks to implement actors performs best. Average Execution Time (in secs) \nAverage Execution Time (in secs) 100 50 0 Number of meetings (in millions)  Jetlang Kilim  Akka Standard \nScala heavy HS light (a) Scala versions which use pattern matching. 40 20 0 2 4 6 810  Number of meetings \n(in millions) Jetlang Kilim Akka HJ light (b) Java versions which use instanceof operator. Figure \n18: The Chameneos benchmark exposes the effects of con\u00adtention on shared resources. The Chameneos benchmark \ninvolves all chameneos constantly sending messages to a mall actor that co\u00adordinates which two chameneos \nget to meet. Adding messages into the mall actor s mailbox serves as a contention point. 200 150 100 \n50 0 Average Execution Time (in secs) Average Execution Time (in secs) Number of tasks forked (in millions) \n Jetlang Kilim  Akka Standard Scala HS heavy HS light (a) Scala versions which use pattern matching. \n150 100 50 0 2 4 6 810 Number of tasks forked (in millions) Jetlang Kilim Akka HJ light (b) Java versions \nwhich use instanceof operator. Figure 19: The Java Grande Forum Fork-Join benchmark ported for actors. \nIndividual invocations were con.gured to run using twelve workers. Both Jetlang versions run out of memory \non larger problem sizes.  7.3 Application Benchmarks In this section, we compare the performance of \nthe actor frameworks on applications displaying different parallel pat\u00adterns. We also analyze the bene.ts \nof parallelizing the actor message processing in the uni.ed model in some applica\u00adtions. Each application \nbenchmark was run with the sched\u00adulers set up to use 12 worker threads. 7.3.1 General Applications Compared \nFigure 20 displays results of running different applications using the different actor frameworks. The \n.rst two applica\u00adtions, Sudoku Constraint Satisfaction (Sudoku-CS) and Pi Precision (PiPrec), represent \nmaster-worker style actor pro\u00adgrams where the master incrementally discovers work to be done and allocates \nwork fragments to the workers. Work\u00aders only have at most one message pending in their mail\u00adbox and there \nis no scope for batch processing messages. The master is the central bottleneck in such applications \nand all frameworks perform similarly. The next application, All-Pairs Shortest Path (APSP), represents \na phased computation where all actor effectively join on a barrier in each iteration of the outermost \nloop in Floyd-Warshall s algorithm before proceeding to the next iteration. In each iteration the slow\u00adest \nactor dominates the computation and as a result we see similar execution times for all the frameworks. \n The next three applications have relatively larger mem\u00adory footprints and we see the bene.ts of thread \nbinding as well as ef.cient implementation for throughput. HS heavy is faster than standard Scala actors. \nSimilarly the light and Akka actors outperform the other actor frameworks. The ac\u00adtor implementation \nof Successive Over-Relaxation (SOR) represents a 4-point stencil computation and was ported from SOTER \n[32]. The next two applications, Concurrent Sorted Linked-List (CSLL) and Prime Sieve (PSieve), use a \npipeline pattern to expose some parallelism. CSLL mea\u00adsures the performance of adding elements, removing \nele\u00adments, and performing collective operations on a linked-list. The implementation maintains a list \nof helper actors with each actor responsible for handling request for a given value range for individual \nelement operations. Collective oper\u00adations, such as length or sum, are implemented using a pipeline starting \nfrom the head of the list of the helper actors and only the tail actor returning a response to the requester. \nThere are multiple request actors requesting various oper\u00adations on the linked-list and non-con.icting \nrequests are processed in parallel. The PSieve application represents a dynamic pipeline in which a .xed \nnumber of local primes are buffered in each stage. Every time the buffer over.ows, a new stage is created \nand linked to the pipeline, thus grow\u00ading the pipeline dynamically. There is overhead in .lling and draining \nitems in the pipeline for each stage and thus a buffered solution with multiple primes per stage performs \nbetter. In summary, the geometric means of the execution times in seconds for the different actor frameworks \nin sorted or\u00adder are as follows: HS light (8.47), Akka (9.51), HS heavy (14.35), Kilim (15.99), Jetlang \n(16.64), and standard Scala (21.59). The HS light is more than 10% faster than Akka and more than 33% \nfaster than the other actor frameworks while using sequential message processing in actors. 7.3.2 Quicksort \nQuicksort lends itself to divide-and-conquer strategy and is a good .t for the AFM, however as mentioned \nin Section 2.3 it exposes some amount of non-determinism in availability of partial results which cannot \nentirely be captured by the AFM. Figure 21 compares the uni.ed actor implementations  0 5 1015202530354045 \nAverage Execution Time (in secs) Jetlang Kilim Akka  Standard Scala HS heavy HS light  Sudoku-CS: \nSudoku Constraint Satisfaction SOR: Successive Over Relaxation  PiPrec: Pi Precision CSLL: Concurrent \nSorted Linked List  APSP: All-Pairs Shortest Path (Floyd-Warshall) PSieve: Prime Sieve  Figure 20: \nComparison of implementations of some applications using different JVM actor frameworks (Scala version). \n 16.67 14.27 13.6 12.25 0 5 1015  Jetlang Kilim Akka Standard Scala HJ DDFs HS heavy (sequential) \nHS heavy (parallel) HS light (sequential) HS light (parallel) Figure 21: Results of the Quicksort benchmark \non input of length 11 million. in HJ with previously existing async-finish extensions such as isolated \nand DDFs. Pure actor implementations in HJ involve sequential message-processing. The light actor implementation \nis faster than the DDF-based implementa\u00adtion as it can make progress computing the partial result from \nfragments. In the uni.ed model, parallelization inside the ac\u00adtor is achieved by performing the left \nand right splits around the partition in parallel for arrays with sizes larger than a con.gured threshold. \nThe parallelized uni.ed actor imple\u00admentations perform better than the implementation that use sequential \nmessage processing by around 10% and 14% for light and heavy actors, respectively. The HS light (parallel) \nactor is the best-performing and is around 10% faster than other actor implementations and more than \n23% faster than DDF implementation.  7.3.3 Filter Bank for multirate signal processing Filter Bank has \nbeen ported from the StreamIt [30] set of benchmarks. It is used to perform multirate signal processing \nand consists of multiple pipeline branches. On each branch the pipeline involves multiple stages including \nmultiple de\u00adlay stages, multiple FIR .lter stages, and sampling. Since Filter Bank represents a pipeline, \nit can easily be imple\u00admented using actors. The FIR .lter stage is stateful, appears early in the pipeline, \nand is a bottleneck in the pipeline. Par\u00adallelizing the computation of the weighted sum to pass down \nthe pipeline in this FIR stage shortens the critical length of 0 1020304050 Average Execution Time (in \nsecs) Jetlang Kilim Akka Standard Scala HS heavy Sequential HS light Sequential HS light Parallel \nFigure 22: Filter Bank benchmark results con.gured to use three branches. the pipeline and helps speed \nup the application. Figure 22 compares the performance of the actor implementations of the Filter Bank \nbenchmark with a uni.ed implementation which parallelizes the FIR stage. The HS light parallel ver\u00adsion \nis at least 30% faster than the other actor implementa\u00adtions. 7.3.4 Online Hierarchical Facility Location \nFacility Location algorithms are used to decide when and where to open facilities in order to minimize \nthe cost of opening a facility and the cost of servicing customers. In the online version, the locations \nof customers are not known beforehand and the algorithm needs to make these decisions on-the-.y. One \nalgorithm for this problem is the Online Hi\u00aderarchical Facility Location [2]. The algorithm exposes a \nhi\u00ad erarchical tree structure (quadrants in the algorithm) while performing the computation. Information \nin the form of cus\u00adtomer locations initially .ows down the tree. In the algo\u00adrithm, each node maintains \na list of customers it plans to service and this list is partitioned at decision points to form new child \nnodes. In addition, the decision to create child nodes needs to be propagated up the tree and to selected \nsib\u00adlings. A speculatively parallel version of this algorithm can be mapped to use actors. Each actor \nrepresents a quadrant and are arranged in a tree structure. Each quadrant maintains a cost value as it \nreceives customers. When the threshold is exceeded, the customers are partitioned and transferred to \nnewly formed child quadrants. In the uni.ed model async\u00adfinish additional parallelism is achieved while \npartition\u00ading the customers to prepare the child nodes. The pure AM variants do not support such parallelism \nwhile processing a message and need to split the child nodes sequentially.  Average Execution Time (in \nsecs) Jetlang Kilim Akka Standard Scala HS heavy Sequential HS light Sequential HS light Parallel Figure \n23: Online Hierarchical Facility Location benchmark re\u00adsults. Results displayed for 6 million customers \nand an alpha value of 5. Figure 23 compares the performance of the actor imple\u00ad mentations of the Facility \nLocation benchmark with a uni\u00ad.ed implementation. In Online Hierarchical Facility Loca\u00adtion, parallelism \nfrom the uni.ed model is used when a quad\u00adrant (actor) splits and creates its four children. The split \nhap\u00adpens based on a threshold determined by the value of al\u00adpha, which is an input to the program. A \nsmaller value of alpha means there are larger number of splits and the tree is deeper. The performance \nof the HS light with parallelized splits is better than the HS light actor implementation by about 27% \nand is comfortably better than Jetlang, Kilim, Akka, and Scala. 8. Related Work Sch\u00a8afer et al. have \nproposed the notion of Parallel Actor Monitors (PAM) [25] to extend the actor model with support for \nintra-actor parallelization. In PAM, the end user speci\u00ad.es schedulers, separate from the actor s message \nprocess\u00ading body (MPB), that control when an actor is able to pro\u00adcess multiple messages safely. For \nexample, a scheduler in PAM might allow an actor to process multiple messages for read requests in parallel \nbut only allow a single message for a write request to be in .ight by an actor. Similarly, it is trivial \nto express stateless actors in PAM by writing a scheduler that allows all messages to be processed in \nparallel. In the actor model, only one message for an actor would be in .ight at a time. Our approach \nallows us to specify similar intra-actor parallelism constraints by allowing escaping asyncs but cur\u00adrently \nrequires modi.cation of the actor s MPB. Addition\u00adally, our model allows expressing parallelization inside \nthe actor s MPB, for example, exploiting data parallelism while processing a message as seen in the Filterbank \nexample in Figure 5. Such parallelism cannot be expressed with PAM. The CoBox model [24] proposed by \nSchafer et al. is in\u00ad spired by the actor model and exposes parallelism among asynchronously communicating \nobjects. Objects are parti\u00adtioned into separate concurrently executing CoBoxes and al\u00adlocated dynamically \nbut never leave their host CoBox. A task performs synchronous operations on co-located objects. CoBoxes \ncan have multiple ready tasks, but actively exe\u00adcute a single task at a time thus ensuring data race \nfree\u00addom. An active task can cooperatively decide to suspend itself and activate another task in the \nsame CoBox when it discovers some condition which prevents its progress. This notion of isolating data \ninto different partitions and coop\u00aderative execution has also been used by Lublinerman et al. in the \nChorus programming model [16]. Chorus is used for applications with irregular data parallelism where \nthe parti\u00adtions, called Object Assemblies, can merge or split dynam\u00adically when new data isolation constraints \nare discovered in synchronously communicating objects. Our approach differs from these models in that \nwe can expose parallelism inside the MPB which would be equivalent to multiple tasks ex\u00adecuting simultaneously \nin a CoBox/Object Assembly, but at the cost of possible data races. We are planning on extending the \nSPD3 algorithm [22] for HJ s .nish, async and isolated constructs to also detect races in our combination \nof actor and task parallelism. Non-blocking receive operations between actors are avail\u00adable in the E \nlanguage [17] under the form of promises to futures. These are created every time a message is asyn\u00adchronously \nsent to an actor. Actions can be registered to the promise using the when clause; these actions are triggered \nwhen the promise resolves, i.e. when the message sent to the actor is processed. This is similar to how \nDDFs work with the asyncAwait clause. The difference is that in our model DDFs need to resolved explicitly \nby putting values into the DDF, though this resolution could be done automat\u00adically by a runtime system \nin response to certain events such as when an actor processes a message. AmbientTalk [6], in\u00ad spired \nfrom E, also supports creation of futures on message sends and requires explicit resolution of future \nvalues. In ad\u00addition to the use of DDFs, the pause and resume operations in our model allow us to implement \nnon-blocking receive operations while preventing the next message in an actor s mailbox from being processed. \nSALSA [33] also supports non-blocking receive opera\u00ad tions using the notion of tokens (similar to implicit \nfutures) whose resolved value gets passed automatically to registered actions; the tokens themselves \ncannot be passed in messages to other actors. As mentioned previously, in our model DDFs are resolved \nexplicitly and can be passed around in messages to other actors. SALSA also support join tokens which \ncan register an action to execute only after all messages sent in\u00adside the join token has been processed. \nIn our model we can achieve this by registering on multiple DDFs. The join block is also similar to the \nfinish construct in our model, the dif\u00adference being that a statement following a finish may not execute \nuntil all nested async tasks have completed execu\u00adtion. In SALSA, the join token only delays the execution \nof the action registered on the token. 9. Conclusions and Future Work This paper focuses on a uni.ed \nmodel that integrates the Async-Finish model (AFM) and the Actor model (AM). To the best of our knowledge, \nthis is the .rst effort to system\u00adatically combine these two models. The uni.ed model al\u00adlows for parallelism \ninside actors while also making termi\u00adnation detection easier in actor programs. It also allows ar\u00adbitrary \ncoordination patterns among tasks in the AFM, in an arguably more productive manner than other extensions, \nsuch as phasers and DDFs. The uni.ed model allows for eas\u00adier implementation of certain constructs: for \nexample, the normally blocking receive can be implemented in a non\u00adblocking manner in the uni.ed model. \nThe paper also studies properties of applications that can bene.t from the uni.ed model. We also present \ntwo implementations of this uni.ed model in Habanero-Java (HJ) and Habanero-Scala (HS). HJ is a mature \nAFM implementation which we extend with support for the uni.ed actors. On the other hand, HS is an extension \nof Scala in which we ported AFM constructs and modi.ed the existing actor implementation to work under \nthe uni.ed model. In addition, HS provides a faster actor implementation than the standard Scala actor \nlibrary. These implementations served as tools to run experiments that cor\u00adroborate the claim that uni.ed \nsolutions to certain problems are more ef.cient than solutions that exclusively use the AFM or the AM. \nThe uni.ed model suffers from the possibility of data races when the message processing inside actors \nis paral\u00adlelized. In fact, data races can also exist in many actor im\u00adplementations on the JVM as they \ndo not enforce data isola\u00adtion. Data race detection in the uni.ed actors is an interesting area for future \nresearch and we plan to extend the SPD3 al\u00adgorithm [22] for data race detection in the uni.ed model. \nAvailability Public distributions of Habanero-Java and Habanero-Scala, including code examples, are available \nfor download at http://habanero.rice.edu/hj.html and http://habanero-scala.rice.edu/, respectively. Acknowledgments \nWe are grateful to Vincent Cav\u00b4 gnak e, Dragos\u00b8 Sb irlea and Sa.Tas\u00b8irlar for discussions on the Habanero \nJava runtime sys\u00adtem, phasers and DDFs, respectively. We thank Carlos Varela and Travis Desell for feedback \non an earlier draft of this paper, and for general discussions on designing and implementing actor languages \nand runtimes as well as spe\u00adci.c details on the SALSA language. We also thank Philipp Haller for his \nfeedback on an earlier draft of this paper. Fi\u00adnally, we are grateful to Jill Delsigne at Rice University \nfor her assistance with proof-reading an earlier draft of this pa\u00adper. This work was supported in part \nby the U.S. National Science Foundation through awards 0926127 and 0964520. References [1] G. Agha. \nActors: a model of concurrent computation in distributed systems. MIT Press, Cambridge, MA, USA, 1986. \nISBN 0-262-01092-5. [2] A. Anagnostopoulos, R. Bent, E. Upfal, and P. V. Hentenryck. A simple and deterministic \ncompetitive algorithm for online facility location. Inf. Comput., 194:175 202, November 2004. ISSN 0890-5401. \n[3] Z. Budimli\u00b4c, M. Burke, V. Cav\u00b4e, K. Knobe, G. Lowney, R. Newton, J. Palsberg, D. Peixotto, V. Sarkar, \nF. Schlimbach, and S. Tas\u00b8irlar. Concurrent Collections. Sci. Program., 18: 203 217, August 2010. ISSN \n1058-9244. [4] V. Cav\u00b4 e, J. Zhao, Y. Guo, and V. Sarkar. Habanero-Java: the New Adventures of Old X10. \n9th International Conference on the Principles and Practice of Programming in Java (PPPJ), August 2011. \n[5] P. Charles, C. Grothoff, V. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. \nX10: An Object-Oriented Approach to Non-uniform Cluster Computing. SIG-PLAN Not., 40:519 538, Oct. 2005. \nISSN 0362-1340. doi: 10.1145/1094811.1094852. [6] J. Dedecker, T. Van Cutsem, S. Mostinckx, T. D&#38;#39;Hondt, \nand W. De Meuter. Ambient-Oriented Programming in Am\u00adbientTalk. In Proceedings of the 20th European Conference \non Object-Oriented Programming, ECOOP 06, pages 230 254, Berlin, Heidelberg, 2006. Springer-Verlag. ISBN \n3-540\u00ad35726-2, 978-3-540-35726-1. doi: 10.1007/11785477 16. URL http://dx.doi.org/10.1007/11785477_16. \n[7] EPCC. The Java Grande Forum Multi-threaded Bench\u00admarks. URL http://www2.epcc.ed.ac.uk/computing/ \nresearch_activities/java_grande/threads/ s1contents.html. [8] A. Georges, D. Buytaert, and L. Eeckhout. \nStatistically Rigor\u00adous Java Performance Evaluation. In Proceedings of the 22nd annual ACM SIGPLAN conference \non Object-oriented pro\u00adgramming systems and applications, OOPSLA 07, pages 57 76, New York, NY, USA, \n2007. ACM. ISBN 978-1-59593\u00ad786-5. [9] M. I. Gordon, W. Thies, and S. Amarasinghe. Exploit\u00ading Coarse-Grained \nTask, Data, and Pipeline Parallelism in Stream Programs. SIGOPS Oper. Syst. Rev., 40:151 162, Oc\u00adtober \n2006. ISSN 0163-5980. [10] P. Haller and M. Odersky. Scala Actors: Unifying thread-based and event-based \nprogramming. Theo\u00adretical Computer Science, 410(2 3):202 220, 2009. ISSN 0304 3975. doi: 10.1016/j.tcs.2008.09.019. \nURL http://www.sciencedirect.com/science/article/  pii/S0304397508006695. Distributed Computing Tech\u00adniques. \n[11] Haller, Philipp. chameneos-redux.scala Fish-Eye: browsing scala-svn, 2011. URL https: //codereview.scala-lang.org/fisheye/browse/ \nscala-svn/scala/branches/translucent/docs/ examples/actors/chameneos-redux.scala?hb=true. [12] C. Hewitt, \nP. Bishop, and R. Steiger. Arti.cial Intelligence A Universal Modular ACTOR Formalism for Arti.cial Intelli\u00adgence. \nProceedings of the 3rd International Joint Conference on Arti.cial Intelligence, Stanford, CA, August \n1973. [13] Hewitt, Carl and Baker, Henry G. Actors and Continu\u00adous Functionals. Technical report, Massachusetts \nInstitute of Technology, Cambridge, MA, USA, February 1978. [14] Imam, Shams and Sarkar, Vivek. Habanero-Scala: \nAsync-Finish Programming in Scala. In The Third Scala Workshop (Scala Days 2012), April 2012. [15] R. \nK. Karmani, A. Shali, and G. Agha. Actor Frameworks for the JVM Platform: A Comparative Analysis. In \nProceed\u00adings of the 7th International Conference on Principles and Practice of Programming in Java, PPPJ \n09, pages 11 20, New York, NY, USA, 2009. ACM. ISBN 978-1-60558-598\u00ad 7. doi: 10.1145/1596655.1596658. \nURL http://doi.acm. org/10.1145/1596655.1596658. [16] R. Lublinerman, S. Chaudhuri, and P. Cerny. Parallel \nPro\u00adgramming with Object Assemblies. In Proceedings of the 24th ACM SIGPLAN conference on Object oriented \nprogramming systems languages and applications, OOPSLA 09, pages 61 80, New York, NY, USA, 2009. ACM. \nISBN 978-1-60558\u00ad766-0. doi: 10.1145/1640089.1640095. URL http://doi. acm.org/10.1145/1640089.1640095. \n[17] M. S. Miller, E. D. Tribble, and J. Shapiro. Concurrency Among Strangers: Programming in E as Plan \nCoordination. In Proceedings of the 1st International Conference on Trustwor\u00adthy Global Computing, TGC \n05, pages 195 229, Berlin, Hei\u00addelberg, 2005. Springer-Verlag. ISBN 3-540-30007-4, 978-3\u00ad540-30007-6. \nURL http://dl.acm.org/citation.cfm? id=1986262.1986274. [18] M. Odersky, P. Altherr, V. Cremet, I. Dragos, \nG. Dubochet, B. Emir, S. Mcdirmid, S. Micheloud, N. Mihaylov, M. Schinz, and et al. An Overview of the \nScala Programming Language Second Edition. System, (Section 2):15 30, 2006. [19] OpenMP Architecture \nReview Board. OpenMP Applica\u00adtion Program Interface -Version 3.0 May 2008. URL www. openmp.org/mp-documents/spec30.pdf. \n[20] N. Raja and R. K. Shyamasundar. Actors as a Coordinating Model of Computation. In Proceedings of \nthe 2nd Interna\u00adtional Andrei Ershov Memorial Conference on Perspectives of System Informatics, pages \n191 202. Springer-Verlag, 2004. ISBN 3-540-62064-8. [21] R. Raman, J. Zhao, V. Sarkar, M. Vechev, and \nE. Yahav. Ef\u00ad.cient Data Race Detection for Async-Finish Parallelism. In Proceedings of the First international \nconference on Runtime veri.cation, RV 10, pages 368 383, Berlin, Heidelberg, 2010. Springer-Verlag. ISBN \n3-642-16611-3, 978-3-642-16611-2. [22] R. Raman, J. Zhao, V. Sarkar, M. Vechev, and E. Yahav. Scal\u00adable \nand Precise Dynamic Data Race Detection for Structured Parallelism. In PLDI, 2012. [23] Rettig, Mike. \njetlang: Message based concurrency for Java. URL http://code.google.com/p/jetlang/. [24] J. Sch\u00a8afer \nand A. Poetzsch-Heffter. JCoBox: Generalizing Active Objects to Concurrent Components. In Proceedings \nof the 24th European conference on Object-oriented pro\u00adgramming, ECOOP 10, pages 275 299, Berlin, Heidelberg, \n2010. Springer-Verlag. ISBN 3-642-14106-4, 978-3-642\u00ad14106-5. URL http://dl.acm.org/citation.cfm?id= \n1883978.1883996. [25] C. Scholliers, E. Tanter, and W. D. Meuter. Parallel Actor Monitors. In 14th Brazilian \nSymposium on Programming Languages, 2010. [26] J. Shirako, D. M. Peixotto, V. Sarkar, and W. N. Scherer. \nPhasers: a Uni.ed Deadlock-Free Construct for Collective and Point-to-Point Synchronization. In Proceedings \nof the 22nd annual international conference on Supercomputing, ICS 08, pages 277 288, New York, NY, USA, \n2008. ACM. ISBN 978-1-60558-158-3. [27] S. Srinivasan and A. Mycroft. Kilim: Isolation-Typed Ac\u00adtors \nfor Java (A Million Actors, Safe Zero-Copy Communica\u00adtion). European Conference on Object Oriented Programming \nECOOP 2008, 5142/2008:104 128, 2008. [28] S. Tas\u00b8irlar and V. Sarkar. Data-Driven Tasks and their Imple\u00admentation. \nIn Proceedings of the International Conference on Parallel Processing (ICPP) 2011, September 2011. [29] \nThe Scala Programming Language. pingpong.scala. URL http://www.scala-lang.org/node/54. [30] W. Thies, \nM. Karczmarek, and S. P. Amarasinghe. StreamIt: A Language for Streaming Applications. In Computational \nComplexity, pages 179 196, 2002. [31] Typesafe Inc. Akka. URL http://akka.io/. [32] UIUC. SOTER project. \nURL http://osl.cs.uiuc.edu/ soter/. [33] C. Varela and G. Agha. Programming Dynamically Re\u00adcon.gurable \nOpen Systems with SALSA. ACM SIGPLAN Notices, 36(12):20 34, Dec. 2001. ISSN 0362-1340. doi: 10.1145/583960.583964. \nURL http://doi.acm.org/10. 1145/583960.583964. [34] R. Virding, C. Wikstr\u00a8om, M. Williams, and J. Armstrong. \nConcurrent programming in ERLANG (2nd ed.). Prentice Hall International (UK) Ltd., Hertfordshire, UK, \nUK, 1996. ISBN 0-13-508301-X. [35] A. Yonezawa, J. Briot, and E. Shibayama. Object-Oriented Concurrent \nProgramming in ABCL/1. In Conference pro\u00adceedings on Object-oriented programming systems, languages and \napplications, OOPLSA 86, pages 258 268, New York, NY, USA, 1986. ACM. ISBN 0-89791-204-7. doi: 10. 1145/28697.28722. \nURL http://doi.acm.org/10.1145/ 28697.28722.   \n\t\t\t", "proc_id": "2384616", "abstract": "<p>This paper introduces a unified concurrent programming model combining the previously developed Actor Model (AM) and the task-parallel Async-Finish Model (AFM). With the advent of multi-core computers, there is a renewed interest in programming models that can support a wide range of parallel programming patterns. The proposed unified model shows how the divide-and-conquer approach of the AFM and the no-shared mutable state and event-driven philosophy of the AM can be combined to solve certain classes of problems more efficiently and productively than either of the aforementioned models individually. The unified model adds actor creation and coordination to the AFM, while also enabling parallelization within actors. This paper describes two implementations of the unified model as extensions of Habanero-Java and Habanero-Scala. The unified model adds to the foundations of parallel programs, and to the tools available for the programmer to aid in productivity and performance while developing parallel software.</p>", "authors": [{"name": "Shams M. Imam", "author_profile_id": "81498656089", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P3856172", "email_address": "shams@rice.edu", "orcid_id": ""}, {"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P3856173", "email_address": "vsarkar@rice.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384671", "year": "2012", "article_id": "2384671", "conference": "OOPSLA", "title": "Integrating task parallelism with actors", "url": "http://dl.acm.org/citation.cfm?id=2384671"}