{"article_publication_date": "10-19-2012", "fulltext": "\n Kitsune: Ef.cient, General-purpose Dynamic Software Updating for C Christopher M. Hayden Edward K. Smith \nMichail Denchev Michael Hicks Jeffrey S. Foster University of Maryland, College Park, USA {hayden,tedks,mdenchev,mwh,jfoster}@cs.umd.edu \nAbstract Dynamic software updating (DSU) systems allow programs to be updated while running, thereby \npermitting developers to add features and .x bugs without downtime. This paper introduces Kitsune, a \nnew DSU system for C whose design has three notable features. First, Kitsune s updating mecha\u00adnism updates \nthe whole program, not individual functions. This mechanism is more .exible than most prior approaches \nand places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune \nmakes the impor\u00adtant aspects of updating explicit in the program text, making the program s semantics \neasy to understand while minimiz\u00ading programmer effort. Finally, the programmer can write simple speci.cations \nto direct Kitsune to generate code that traverses and transforms old-version state for use by new code; \nsuch state transformation is often necessary, and is signi.cantly more dif.cult in prior DSU systems. \nWe have used Kitsune to update .ve popular, open-source, single-and multi-threaded programs, and .nd \nthat few program changes are required to use Kitsune, and that it incurs essentially no performance overhead. \nCategories and Subject Descriptors C.4 [Performance of Systems]: Reliability, availability, and serviceability \nGeneral Terms Design, Languages Keywords dynamic software updating 1. Introduction Running software systems \nwithout incurring downtime is very important in today s 24/7 world. Dynamic software updating (DSU) services \ncan update programs with new code to .x bugs or add features without shutting them down. The research \ncommunity has shown that general- Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 12, October 19 26, 2012, Tucson, Arizona, USA. Copyright c &#38;#169; \n2012 ACM 978-1-4503-1561-6/12/10. . . $10.00 purpose DSU is feasible: systems that support dynamic up\u00adgrades \nto running C, C++, and Java programs have been ap\u00adplied to dozens of realistic applications, tracking \nchanges ac\u00adcording to those applications release histories [1, 5, 9, 12 14, 16, 17, 19]. Concurrently, \nindustry has begun to package DSU support into commercial products [2, 20]. The strength of DSU is its \nability to preserve program state during an update. For example, servers for databases, media, FTP, SSH, \nand routing can maintain client connec\u00adtions for unbounded time periods. DSU can allow those active connections \nto immediately bene.t from important program updates (e.g., security .xes), whereas traditional updating \nstrategies like rolling upgrades cannot. Servers may also maintain signi.cant in-memory state; examples \nin\u00adclude memcached (a caching server) and redis (a key-value server). DSU techniques can maintain this \nin-memory state across the update, whereas traditional upgrade techniques will lose it (memcached) or \nmust rely on an expensive disk reload that degrades performance (redis). We are interested in supporting \ngeneral-purpose DSU for single-and multi-threaded C applications. While progress made by existing DSU \nsystems is promising, a truly prac\u00adtical system must be in harmony with the main reasons de\u00advelopers \nuse C: control over low-level data representations; explicit resource management; legacy code; and, perhaps \nabove all, performance. In this paper we present Kitsune, a new DSU system for C that is the .rst to \nsatisfy these motiva\u00adtions while supporting general-purpose dynamic updates in a programmer-friendly \nmanner. (We compare in detail against related systems in Section 5.) Kitsune operates in harmony with \nC thanks to three key design and implementation choices. First, Kitsune uses en\u00adtirely standard compilation. \nAfter a translation pass to add some boilerplate calls to the Kitsune runtime, a Kitsune pro\u00adgram is \ncompiled and linked to form a shared object .le (via a simple Make.le change). When a Kitsune program \nis launched, the runtime starts a driver routine that loads the .rst version s shared object .le and \ntransfers control to it. When a dynamic update becomes available (only at speci.c program points, as \ndiscussed shortly), the program longjmps back to the driver routine, which loads the new application \nversion and calls the new version s main function. Thus, ap\u00adplication code is updated all at once, and \nas a consequence, Kitsune places no restrictions on coding idioms or data rep\u00adresentations; it allows \nthe application s internal structure to be changed arbitrarily from one version to another; and it does \nnot inhibit any compiler optimizations.  Second, Kitsune gives the programmer explicit control over \nthe updating process, which is re.ected as three kinds of additions to the original program: (1) a handful \nof calls to kitsune update(...), placed at the start of one or more of the program s long-running loops, \nto specify update points at which dynamic updates may take effect; (2) code to ini\u00adtiate data migration, \nwhere old-version data is assigned to new-version variables (possibly after transformation to re\u00ad.ect \nprogram changes); and (3) code to perform control mi\u00adgration, which redirects execution to the corresponding \nup\u00addate point in the new version. In our experience, these code additions are small (see below) and fairly \neasy to write be\u00adcause of Kitsune s simple semantics. (Section 2 explains Kit\u00adsune s use in detail.) \nFinally, Kitsune includes a novel tool called xfgen that assists the programmer in writing code to migrate \nand trans\u00adform old program state to be compatible with a new pro\u00adgram version. The input to xfgen is \na series of programmer\u00adprovided transformation speci.cations ( transformers for short), one per changed \ntype or variable, that describe in in\u00adtuitive notation how to translate data from the old to new format. \nThe output of xfgen is C code that performs state mi\u00adgration, executing transformation wherever it is \nneeded. At a high level, the generated code operates analogously to a trac\u00ading garbage collector, traversing \nthe heap starting at global variables and locals marked by the programmer. When the traversal reaches \ndata requiring transformation, it allocates new memory cells and initializes them using the transform\u00aders, \ntaking care to maintain the shape of the heap. The old version s copies of any transformed data structures \nare freed once the update is complete. Kitsune s approach is easy to use, relative to other DSU systems; \nit adds no overhead dur\u00ading the non-updating portion of execution; and it does not change data layout. \n(Section 3 describes xfgen.) We have implemented Kitsune and used it to update three single-threaded \nprograms vsftpd, redis, and Tor and two multi-threaded programs memcached and icecast. For each application, \nwe considered from three months to three years worth of updates. We found that the number of code changes \nwe needed to make for Kitsune was generally small, between 57 and 159 LoC total, across all versions \nof a program. The change count is basically stable, and not generally related to the application size, \ne.g., 134 LoC for 16 KLoC icecast vs. 159 LoC for 76 KLoC Tor. xfgen was also very effective, allowing \nus to write transformers with similarly small speci.cations totaling between 27 and 200 lines; the size \nhere depends on the number of data structure changes across the sequence of updates. We tested that all \nprograms behaved correctly under our updates. We measured Kitsune s performance overhead and found it \nranged from -2.2% to +2.35%, which is in the noise on modern systems [15]. We also measured the overhead \nthat Ginseng [16, 17] and UpStare [13], two other general\u00adpurpose DSU systems for C, imposed on some \nof the same programs and found it to be as high as 18.4% for the former and 41.6% for the latter. We \nalso measured that the time re\u00adquired to perform an update using Kitsune at typically less than 40ms. \nOne program, icecast, took ~1s to update; this is due to internal timing constraints and does not adversely \naffect the application. (See Section 4 for full details.) Kitsune s design adopts the best ideas from \nexisting sys\u00adtems while it eschews their shortcomings; we drew sig\u00adni.cant inspiration from UpStare and \nGinseng. From the former, Kitsune adopts the notion of whole-program up\u00addates, rather than per-function \nupdates, and from the lat\u00adter it adopts the idea of updating only at explicitly speci\u00ad.ed update points, \nrather than at arbitrary positions. Whole\u00adprogram updating eliminates the need to refactor programs to \nsupport certain updates (e.g., loop extraction [17] to en\u00adable updating long-running loops), while update \npoints sim\u00adplify the task of testing and otherwise reasoning about an update, since far fewer program \nstates need to be considered. On the other hand, Kitsune speci.cally rejects other ele\u00adments of these \nsystems designs to better balance competing concerns. For example, both UpStare and Ginseng require nontrivial \ncompilers to enable updating UpStare compiles the entire program specially to enable stack reconstruction, \na mechanism that reduces the control migration problem to specifying a stack mapping at update-time, \nwhile Ginseng s compiler inserts extra levels of indirection, adds slop space to struct de.nitions, inserts \nread/write barriers to support on-the-.y control and data migration, and performs a static analysis to \nensure that these compilation changes will not break the program (e.g., due to tricky uses of typecasts). \nKitsune s design requires the programmer to write slightly more code in the worst case compared to Ginseng \nand Up-Stare, but in general, confers several advantages, including: (1) signi.cantly better performance, \nsince control migra\u00adtion code is localized to program paths that rarely intersect with normal program \nexecution, while Ginseng s and Up\u00adStare s compilation changes are pervasive; (2) simpli.ed update understanding, \nsince the programmer can just read the code without having to mentally apply a stack mapping and/or indirection \nmodel to it; (3) a simpler implementation, since no special compiler is needed; and (4) greater .exi\u00adbility \nand scalability, since it does not require (conservative, slow) whole-program analysis that would prohibit \ncertain programming idioms. In essence, Kitsune makes DSU a .rst-class program feature that is implemented \nand main\u00adtained by the programmer, a task that is made simpler and more manageable thanks to the careful \ndesign of the Kitsune run-time library and tool suite.  Section 5 provides a thorough, mechanism-level \ncompar\u00adison to related DSU systems for C. Considered as a whole, we .nd Kitsune to be the most .exible, \nef.cient, and easy to use (and deploy) DSU system for C developed to date. 2. Kitsune A Kitsune application \ns execution goes through three phases: Normal execution. When started for the .rst time, and while no \ndynamic update is available, the application executes normally. Update preparation. Once a dynamic update \nbecomes available, the application thread(s) must reach a state in which the update can be safely applied. \nThe programmer will insert calls to the function kitsune update at program points at which an update \nis permitted to take effect; such calls are dubbed update points [12]. When an update is avail\u00adable, \nthe kitsune update function starts the update process. If the program is single-threaded, the new program \nis loaded and the next phase, update execution, begins. If the program is multithreaded, each thread \nblocks until all reach an update point, and then update execution begins. Update execution. The threads \nrunning the old code have their stacks unwound, the entire new program is loaded, and its main function \nis called by the main thread. Since main also executes during normal startup, the programmer adds a few \nKitsune API calls to direct it to behave differently during an update. This added code will do two things: \n(1) migrate and transform the old version s data, and (2) direct control to a point in the new version \nthat is equivalent to the point at which the update took place in the old version, identi.ed by calls \nto kitsune update. We call these two activities data mi\u00adgration and control migration, respectively. \nOnce all threads have reached their update points, the update is complete, re\u00adsources are freed, and \nnormal execution resumes. In what follows, we explain how we build programs to implement this semantics \n(Section 2.1), and how data and control migration is orchestrated by the programmer using the Kitsune \nAPI (Section 2.2). For simplicity, we start by assuming we are working with a single-threaded program, \nand conclude by describing how we handle multi-threaded programs (Section 2.3). 2.1 Implementing dynamic \nupdating The process of building a Kitsune application is illustrated in Figure 1. There are two inputs \nprovided by the program\u00admer: the main application s .c source .les (upper left) and an xfgen . xf speci.cation \n.le for transforming the running state during an update (not needed for the initial version). The source \n.les are processed by the Kitsune compiler kitc to add some boilerplate calls derived from programmer \nan\u00adnotations. Rather than compile and link the resulting .c .les to a standalone executable, these .les \nare compiled to be Figure 1. Kitsune build chain position independent (using gcc s -fPIC .ag) and linked, \nalong with the Kitsune runtime system rt.a, into a shared object library app.so. (For the best performance \nwe also use gcc s -fvisibility=hidden option to prevent application symbols from being exported, since \nexported symbols incur heavy overhead when called.) When building an updating version of the program, \nthe . xf .le is compiled by xfgen to C code and linked in as well. Processing the . xf requires .ts type \nsummary .les produced by kitc for the old and current versions (described in detail in Section 3.2). \nThe .rst version of a program is started by executing kitsune app.so args... where args... are the program \ns usual command-line arguments. The kitsune executable is Kitsune s application-independent driver routine, \nwhich dy\u00adnamically loads the shared library and then performs some initialization. Among other things, \nthe driver installs a sig\u00adnal handler for SIGUSR2,1 which is later used to signal that an update is available. \nThe driver also calls setjmp, and then transfers control to the (globally visible) kitsune init func\u00adtion \nde.ned in rt.a. This function performs some setup and calls the application s (non-exported) main function; \nat this point, normal execution begins. The kitsune driver is only 109 lines of C code and is the sole \npart of a program that cannot be dynamically updated. When SIGUSR2 is received, the handler sets a global \n.ag; this starts the update preparation phase. The kitsune update function will notice the .ag has been \nset and call longjmp to return to the driver, which then dynamically loads the new program version s \nshared object library. Since the longjmp call will reset the stack, the kitsune update function copies \nany local variables marked for migration to the heap before jumping back to the driver. Thus, just after \nan update, the old version s full state (e.g., its heap, open .les and connec\u00adtions, process/parent ID, \netc.) is still available. At this point, kitsune init is invoked to start the new version, beginning \nthe update execution phase. The programmer will have in\u00adserted calls to various Kitsune API functions \nto perform data 1 The exact method for signaling that an update is available is left to the discretion \nof the programmer. However, Kitsune provides a sensible default of SIGUSR2, which works well in most \ncases. In programs we worked with, only Tor, which had a previously existing control framework, required \na different mechanism.  1 /* con.g variables set by load con.g () (code not shown) */ 2 int con.g foo \n, con.g bar , con.g size ; /* automigrated */ 3 4 typedef int data; 5 data *mapping; /* automigrated \n*/ 6 7 int op count=0; /* automigrated */ 8 struct dispatch item 9 { char *key; dispatch fn *fun; } \ndispatch tab 10 attribute (( kitsune no automigrate )) 11 = {{ get , &#38;handle get }, { set , &#38;handle \nset }}; 12 13 void handle set ( int sock) { 14 key = recv int(sock); 15 val = recv int(sock); 16 mapping[key] \n= val; 17 send response( %d> ok , op count); 18 } 19 void handle get( int sock) { 20 key = recv int(sock); \n21 send response( %d> %d=%d , op count, key, mapping[key]); 22 } 23 void client loop ( int sock) { 24 \nwhile (1) { 25 kitsune update( client ); 26 char *cmd = read from socket(sock); 27 if (!cmd) break; 28 \ndispatch fn *cmd handler = lookup(dispatch tab, cmd); 29 op count++; 30 cmd handler(sock); } 31 } 32 \nint main()  33 int main sock, client sock ; 34 kitsune do automigrate (); 35 36 load con.g (); 37 mapping \n= malloc(con.g size * sizeof (data )); } 38 if (!MIGRATE LOCAL(main sock))  39 main sock = setup connection(); \n40 41 42 43 while (1) { 44 kitsune update( main ); 45 client sock = get connection(main sock); 46 client \nloop ( client sock ); } 47 }  Figure 2. Example; Kitsune additions highlighted and control migration \nduring this phase, which we illustrate in detail next. We consider xfgen in Section 3.  2.2 Example \nTo use Kitsune, the programmer must slightly modify her application to insert update points, and to add \ncode to per\u00adform control and data migration. This subsection illustrates what these modi.cations look \nlike, and describes the Kitsune API, using an example. Consider the C program in Figure 2, which implements \na simple key-value server. Clients connect to the server and send either get i to get the integer value \nassociated with index i, or set in to associate index i with value n. In the .gure we have highlighted \nthe extra code we need to perform data and control migration. Let us ignore the highlighted code for \nthe moment so that we can discuss the program s core operation. Execution begins with main() on line \n32. After de.ning some local variables, the function calls load con.g () (code not shown) which initializes \nthe three global con.guration variables de\u00ad.ned on line 2 and then allocates an empty mapping. Next, \nmain() calls setup connection() (code also not shown) to begin listening on main sock. Finally, main() \nenters the main loop on lines 43 47. This loop repeatedly waits for a connection and then calls client \nloop () to handle that connection. The client loop () function repeatedly reads a command from the socket; \n.nds the handler (a func\u00adtion pointer) for that command in dispatch tab (created on lines 9 11); increments \na global counter op count that tracks the number of requests; and then dispatches to handle set or handle \nget. If the client disconnects, the function exits the loop on line 27 and returns. While this code is \nvery simple, many server programs share this same general structure a main loop that listens for connections; \na client loop that dispatches different commands; and handler functions that implement those commands. \nNow consider the highlighted code, which the developer has added to the program to implement Kitsune \ncontrol and data migration. This code makes use of several primitive operations that Kitsune provides \nwhich we have summarized in Figure 3 and discuss here. We should emphasize that because this example \nis tiny, the amount of highlighted code is disproportionately large (see Section 4). Migrating control. \nA dynamic update is initiated when the program calls kitsune update(name), where name identi\u00ad.es the \nupdate point, which can be queried when the new program version is launched. In Figure 2 we have added \nupdate points on lines 25 and 44, i.e., we have one update point at the start of each long-running loop. \nThese are good choices for update points because the program is quiescent, i.e., in between events, when \nupdate-relevant state is not in the middle of being modi.ed [8, 17].2 The kitsune driver will load the \nnew version and call its main function. During update execution, the program will direct control toward \nthe equivalent update point in the new version. To do this, it will branch on kitsune is updating (), \nwhich returns true if the program is being run as a dynamic update (or its variant kitsune is updating \nfrom (name) for updates triggered at that named update point), to distinguish update execution from normal \nstartup. 2 Note that our de.nition of quiescent differs from (and is not comparable to) that of some \nprior work, which de.nes it to mean that all updated functions are inactive, i.e., not running.  In \nFigure 2, the conditional on line 35 prevents the con.g\u00aduration from being reloaded and mapping from \nbeing real\u00adlocated when run as an update, since in this case the pro\u00adgram will migrate that state from \nthe old version instead (discussed below). If the update was initiated from the client loop, then on \nline 40 the program migrates client sock from the previous version and then goes straight to that loop. \nNo\u00adtice that when control returns from this call, the program will enter the beginning of the main loop, \njust as if it had returned from the call on line 46. Also notice we do not speci.cally test for an update \nfrom the main update point, as in that case the control .ow of the program naturally falls through to \nthat update point. Migrating data. When a program using Kitsune starts up\u00addate execution, critical data \nfrom the previous version of the program remains available in memory. The programmer is responsible for \nidentifying what portion of that data should be migrated to the new version and specifying how that mi\u00adgration \nis to take place. The .rst step is to identify the global and local variables that should be migrated. \nAll global variables are migrated by default (that is, automigrated ), and the programmer can identify \nany exceptions. For our example, migration occurs for the con.guration variables on line 2 and for mapping \non line 5. We use the kitsune no automigrate attribute on line 10 to prevent dispatch tab from being \nautomigrated, so that it is initialized normally with pointers to new version functions rather than overwritten \nwith old version data. Local variables are not automigrated the programmer must annotate a function with \nthe kitsune note locals attribute (c.f. main()) to support migration of its local variables. To facilitate \ndata migration, kitc generates a per-.le do registration () function that registers the names and addresses \nof all global variables, including statics, and records for each one whether it is automigratable. The \ndo registration () function is marked as a constructor so it is called automatically by dlopen. Similarly, \nkitc in\u00adtroduces code in each of the functions annotated with kitsune note locals to register (on function \nentry) and deregister (on function exit) the names and addresses of local variables (in thread-local \nstorage). The second step is to indicate when data should be mi\u00adgrated after the new version starts. \nCalling kitsune do\u00ad automigrate() (line 34) starts migration of global state, calling a migration function \nfor each registered variable that is automigratable. These functions traverse data structures, transforming \nthem wherever necessary, and are produced by xfgen automatically, when migratable data is unchanged be\u00adtween \nversions, or else according to programmer speci.ca\u00adtions. Each function follows a particular naming convention, \nand the runtime .nds it in the new program version using dlsym(). Within a function annotated with kitsune \nnote locals , the user calls MIGRATE LOCAL(var) to migrate (via the appropriate migration function) the \nold version of var to the new version, e.g., as used on line 41 to migrate client sock . MIGRATE LOCAL() \nreturns true if the program was started as a dynamic update; on line 38 we test this result to decide \nwhether to initialize main sock. Our overall design for data migration re.ects our expe\u00adrience that we \ntypically need to migrate all, or nearly all, global variables, whereas we need only migrate a few lo\u00adcal \nvariables only locals up to the relevant update point are needed, and of these, most contain transient \nstate. We also assume that data that should be migrated is reachable from the application s local and \nglobal variables. In our experi\u00adments, this assumption was true except for memcached, in which pointers \nto some application data were stored only in a library. We solved this problem by caching such pointers \nin the main application; see Section 4.2. Cleaning up after an update. After updating, Kitsune re\u00adclaims \nspace taken up by the old program version. Since control and data migration are under programmer control \nin Kitsune, we need to specify the point at which the update is complete. That point is when the new \nprogram version reaches the same update point at which the update occurred (c.f. the branch on line 42 \nof Figure 2, which then reaches the update point on line 25). Kitsune then unloads the code and stack \ndata from the previous program version; to be safe, the programmer must ensure there are no stale point\u00aders \nto these locations. For example, programmers must en\u00adsure any strings in the data segment that need to \nmigrate are copied to the heap (which can be done in state transformers, or with strdup in the program \ntext). Kitsune also frees any heap memory that xfgen-generated migrations have marked as freeable. Finally, \ncontrol returns to the new version.  2.3 Multi-threading Updating a multi-threaded program is more challenging \nsince the programmer must migrate control and data for every thread. We could require the programmer \nto write this code manually, but we have observed that when the set of threads before and after the update \nis the same, a little ad\u00additional support can make it easier to migrate those threads automatically. \nTo make a pthreads program Kitsune-enabled, the pro\u00adgrammer modi.es all thread creation sites to use \na wrap\u00adper for pthread create called kitsune pthread create .A thread created with kitsune pthread create \n( tid , f , arg) has its thread id tid , thread function f, and f s argument arg atomically added to \na global list kitsune threads of live threads. When a thread exits normally, it removes its entry from \nkitsune threads . Once an update becomes available, each non-main thread stops itself when it reaches \nan update point, recording the name of the update point in its kitsune threads entry. When all threads \nhave reached their update points, the main thread starts updating as described in Section 2.2, and continues \nun\u00ad  API call / attribute Semantics during normal execution Semantics during update execution kitsune \nupdate( label ) kitsune is updating () kitsune is updating from ( label ) kitsune do automigrate () Begins \nthe update process when called, if a dynamic update is available Returns false Returns false Does nothing \nMarks the completion of an update (so resources can be freed, etc.) Returns true Returns true if the \nupdate began from an update point with argument label Runs migration code to initialize the automigratable \nglobal variables attribute (( kitsune no automigrate )) Global variables without this attribute are noted \nso that they are migrated during the call to kitsune do automigrate () attribute (( kitsune note locals \n)) Local variables in a function with this attribute have their addresses registered when the function \nis called so they can be migrated should a new update begin before the function returns. If no update \noccurs, the addresses are deregistered when the function returns MIGRATE LOCAL(localvar) Returns false \nInvokes migration code to initialize lo\u00adcal variable localvar from its old ver\u00adsion; returns true MIGRATE \nGLOBAL(globalvar) Returns false Invokes migration code to initial\u00adize global variable globalvar with \nkitsune no automigrate attribute; re\u00adturns true MIGRATE LOCAL STATIC(funcname, localvar) MIGRATE GLOBAL \nSTATIC(globalvar) As above, but for static global/local variables Figure 3. Kitsune primitives til it \n.nally reaches its own update point in the new version. Then the run-time system iterates through kitsune \nthreads and relaunches each thread, calling the new version of the recorded thread function with its \nrecorded argument. If needed, the developer can provide a special transformation function to modify the \nset of threads or transform a thread s entry function and argument. Each of those threads then ex\u00adecutes, \nperforming whatever control and data migration is needed. Each thread pauses when it reaches the update \npoint where it was stopped. Once all threads have paused, the Kit\u00adsune runtime cleans up the old program \nversion, releasing its code and data as usual, and resumes the main thread and all paused threads. To \nupdate a multi-threaded program with Kitsune, that program should meet or be modi.ed to meet several \nre\u00adquirements. First, each long-running thread must periodi\u00adcally reach an update point. Typically this \nmeans a thread needs an update point in any long running loop and should avoid (uninterruptible) blocking \nI/O and similar operations. Second, threads should not hold resources, such as locks, at update points, \nsince the thread could be killed and restarted at that point. This requirement is in keeping with the \ngen\u00aderal criterion for choosing update points, which stipulates that little or no state should be in-.ight. \nThird, the program should be insensitive to the order in which the threads are restarted in the new version. \nWe expect this holds because the main thread will likely migrate any shared state, which would otherwise \nbe the main source of contention between threads. Finally, recreating threads changes their thread IDs, \nand so the program should not store those IDs in memory. (We could extend Kitsune to relax this requirement.) \nAll programs we considered in our experiments satisfy these requirements. In separate work, we performed \na study to determine whether Kitsune s approach would work for four additional programs, i.e., whether \neach thread would reach an update point in a timely manner [11]. In that study, we implemented additional \nruntime support to allow threads to wake from blocking calls (e.g., I/O and condition\u00advariable wait operations) \nand made small modi.cations to the programs (e.g., to break out of blocking calls in the libevent and \nlibpcap libraries). We found that after these modi.cations, all of the threads in those programs were \nable to reach update points quickly (usually in less than 1ms). Additionally, in our experiments we did \nnot encounter concurrency-related problems during state migration. How\u00adever, if such problems do occur \nin other programs, the devel\u00adoper can add synchronization to avoid interference. In sum, all of the concurrency \npatterns we have observed so far have been compatible with Kitsune s requirements. 3. xfgen As mentioned \nbrie.y in Section 2.2, Kitsune s runtime in\u00advokes migration functions for each automigrating variable, \nfollowing a naming convention to locate the appropriate mi\u00adgration function. Kitsune includes a tool, \nxfgen, that pro\u00adduces migration functions automatically for variables and types that have not changed, \nand generates migration func\u00adtions for those that have according to speci.cations the pro\u00adgrammer expresses \nin a simple, domain-speci.c language.  INIT new var: {action} $in , $out old/new type or var E PTRARRAY(S) \n size of ptd-to array INIT new type: {action} $old/newsym(x) x in old/new prog. E ARRAY(S) size of \narray old var . new var: {action} $old/newtype(t) t in old/new prog. E OPAQUE non-traversed pointer \nold type . new type: {action} $base containing struct E FORALL(@t) polymorphism intro. old var . new \nvar $xform(old, new) xformer function E VAR(@t) refer to type var old type . new type from old to new \ntype/var E INST(typ) instantiate poly. type (a) transformers (b) special variables (c) type annotations \n Figure 4. xfgen speci.cation language and type annotations We refer to such explicitly speci.ed migration \nfunctions as transformers, since they are used to transform the data from an old representation to a \nnew one. The design of xfgen is based on our experience applying DSU to C [8, 9, 12, 16, 17], and aims \nto make common kinds of state transformers easy to write while maintaining the .exibility to implement \narbitrary transformations. This section presents the xfgen speci.cation language, giving a series of \nexamples, and then describes how xfgen generates migration functions. 3.1 Transformer speci.cations \nFigures 4(a) and (b) summarize xfgen s speci.cation lan\u00adguage. Each transformer has one of the forms \nshown in part (a). The INIT transformers describe how to initialize new variables or values of new types, \nand the . transform\u00aders describe how to transform variables or types that have changed and/or been renamed. \nHere {new,old} var is either a local or global variable name and {new,old} type is either a regular C \ntype name or a struct .eld (we will see exam\u00adples below). The transformer action consists of arbitrary \nC code that may reference the special xfgen variables shown in Figure 4(b). These variables refer to \nentities from the old or new program version. A . transformation without an action identi.es a variable/type \nrenaming. Example 1. Suppose we wrote a new version of the pro\u00adgram in Figure 2 in which we removed the \nvariable op count and replaced it with two new variables get count and set count that record per-operation \ncounts. These variables will need to be initialized during the update. We do not know exactly how many \nget and set operations have oc\u00adcurred, but we do have their sum in op count, so we might over-approximate \neach with the sum, as follows: INIT get count: { $out = $oldsym(op count); }INIT set count : { $out = \n$oldsym(op count); } Here we are initializing new variables, so we use an INIT transformer, and the action \nuses $oldsym(op count) to refer to the old version s value of op count and $out to refer to the output \nof the transformer, i.e., get count and set count in the new version. Alternatively, we might wish to \npreserve the total count, and in lieu of precise information we might assume most calls are gets, and \nsome are sets: INIT get count: { $out = (int) .oor ($oldsym(op count)*0.9); }INIT set count : { $out \n= (int) ceil ($oldsym(op count)*0.1); } In general, how a programmer writes a transformer depends on \nan application s invariants and the desired properties of the updated application s behavior [10]. Example \n2. While transformers are often simple, xfgen is powerful enough to express more complicated changes. \nFor example, suppose we change line 5 in Figure 2 so that, rather than an array, mapping is a linked \nlist: struct list { int key; data val; struct list *next; }*mapping; Then we can specify the following \ntransformer: 1 mapping . mapping: { 2 int key; 3 $out = NULL; 4 for (key = 0; key < $oldsym(con.g size \n); key++) { 5 if ($in[key] != 0) { 6 $newtype(struct list ) *cur = 7 malloc( sizeof ($newtype(struct \nlist ))); 8 cur. key = key; 9 cur. val = $in[key ]; 10 cur. next = $out; 11 $out = cur; 12 }}} Here mapping \n. mapping indicates this is a transformer for the new version of mapping (the occurrence to the right \nof the arrow, referred to as $out within the action) from the old version of mapping (referred to as \n$in). The body of the transformer loops over the old mapping array (whose length is stored in old version \ns con.g size ), allocating and initializing linked list cells appropriately. In the call to malloc, we \nuse $newtype(struct list ) to refer to the list type in the new program version. Example 3. Finally, \nsuppose the programmer wants to change type data from int to long, and at the same time extend mapping \nwith .eld int cid to note which client es\u00adtablished a particular mapping: typedef long data; struct list \n{ int key; data val; int cid ; struct list *next; }*mapping; The programmer can specify that val should \nsimply be copied over and cid should be initialized to -1: typedef data . typedef data: { $out =(long) \n$in ; } INIT struct list . cid { $out = -1; }  Because the type of mapping changed, xfgen will use \nthese speci.cations to generate a function that traverses the mapping data structure, initializing the \nnew version of mapping along the way. As we will see shortly, this is pos\u00adsible because there is a structural \nrelationship between ele\u00adments in the old list and elements in the new list, and because by default xfgen-created \nmigrations stop traversal at NULL, the list terminator. (We could not use this approach for the previous \narray-to-list change because the data elements were not related in a simple structural manner.) Other \nspecial variables. In the examples so far, we have seen uses of all but the last two special variables \nin Fig\u00adure 4(b). The variable $base refers to the struct whose .eld is being updated. For example, in \nINIT struct s.x: { $out = $base.y } new .eld x of struct s is initialized to .eld y in the same struct. \nVariable $xform(old,new) names the migration function between types old and new. This variable is useful \nwhen de.ning a transformer for a container datastructure, so that an action can recursively call the \nmigration for each con\u00adtained object. For example, suppose we merged Examples 2 and 3 into a single update \nthat transformed mapping to a list and changed data s type to long. Then we could use the transformer \nfrom Example 2, changing line 9 to XF INVOKE($xform(data, data), &#38;$in[key], &#38;cur. val); $xform \nlooks up (or forces the creation of) the migration between its argument variables/types. This migration \nis re\u00adturned as a closure that takes pointers to the old and new object versions and can be called using \nXF INVOKE.  3.2 Migration generation xfgen generates code to perform migration. At a high level, the \ngenerated code will traverse the heap, starting from the migratable global and local variables in the \nold version s stack and data segment, and assign the (possibly trans\u00adformed) data to the corresponding \nvariables in the new ver\u00adsion. With xfgen, the programmer is able to focus on de.n\u00ading what transformation \nshould be used for data represen\u00adtations that have changed, and is relieved of the tedium of how to .nd \nall of the old values, preserve the structure of the heap, and manage memory. xfgen generates migration \ncode based on the contents of the developer-provided . xf speci.cation .le and the Kitsune-maintained \n. ts type summary .les for the old and new versions (see Figure 1). A type summary .le contains all of \nthe type de.nitions (e.g., struct, typedef) and global and local variable declarations from its corresponding \n.c source .le, noting which are eligible for migration (according to the rules given in Section 2.2). \nTo assist the programmer, xfgen can check that . xf .les are complete: an . xf .le is rejected if it \nfails to de.ne a transformer that applies to migratable data that was added or whose type changed between \nversions. xfgen uses type information to generate migration func\u00adtions for migratable types and variables \n(or portions thereof) that have not changed these functions will iterate over the variable/type in question, \nrecursively invoking migration functions on the variable/type s subcomponents. Migrat\u00adable data includes \nmigrated local and global variables, their types, and types they transitively reference; e.g., struct \nbar is migratable if struct foo is migratable and contains a pointer to a struct bar. xfgen sometimes \nneeds additional programmer-provided type information to work correctly; e.g., it may need to know the \nlengths of arrays. In the remainder of this section, we describe how migra\u00adtion code is generated for \nvariables and types, and how an\u00adnotations are used to ensure the generated code is complete. Migrating \nvariables. For each migrated variable listed in the new version s . ts .le, if that variable is named \nexplicitly in an old var . new var speci.cation, then xfgen gener\u00adates C code from the speci.ed action, \nsubstituting references appropriately. For example, $in and $out are replaced by values returned from \nkitsune lookup old and new, respec\u00adtively, which return a pointer to a symbol in the old or new program \nversion, respectively, or NULL if no such symbol exists. Thus xfgen will produce the following C code \nfrom the overapproximating speci.cations in Example 1, above: void kitsune migrate get count () { int \n*o op count = (int*) kitsune lookup old ( op count ); int *n get count = (int*)kitsune lookup new( get \ncount ); *n get count = *o op count; }void kitsune migrate set count () { /* as above */ } For each remaining \nmigrated variable x, xfgen will con\u00adsult the y. x renaming rule if one exists to determine the source \nsymbol y; otherwise it assumes x s name is un\u00adchanged. In that case, as we noted previously, the lack \nof a symbol x in the old version code, or the lack of an explicit transformer if x s type has changed, \nwill cause xfgen to re\u00adject the . xf .le. Assuming the old version symbol has type old type and the new \nversion has type new type, xfgen s gen\u00aderated function for x will simply call the migration function \nfor old type . new type; if old type and new type have the same de.nition (and no explicit transformer \nhas been spec\u00adi.ed) then xfgen will generate C code for this function. We describe this process next. \nMigrating types. xfgen generates transformer code from old type . new type speci.cations in approximately \nthe same manner as for variables. For speci.cations involving only a single struct .eld (as for the cid \n.eld in Example 3), xfgen will generate code for the rest of the .elds in the manner described below, \nand then insert the hand-written code for the new or changed .eld. A generated function for an unchanged \ntype simply re\u00adcursively invokes the migration functions for the immediate children of the type. For \nexample, suppose we generated a migration for struct list in Example 3. Then, xfgen would produce code \nthat retains the old key value by copying it (generated migrations for primitive types are simple assign\u00adments); \nrecursively invokes the user-provided transformer for data for the val .eld; inserts the user-provided \ncode to assign to the cid .eld; and recursively invokes itself on the target of the next .eld, assuming \nit is not NULL.  Pointers must be handled carefully by the generated code. For non-NULL pointers, the \ngenerated code checks a global migration map to see if the pointer has been migrated be\u00adfore; if so it \nreturns the old target. Doing so maintains the shape of the heap and avoids in.nite loops when traversing \ncyclic datastructures. Otherwise it calls the appropriate mi\u00adgration for the pointer s target. If the \npointer is to a global or local variable, then the address of this variable is different in the new version. \nAs such, the code will look up the cor\u00adresponding address and redirect to it. If the pointer target s \ntype has truly changed (and so must have an explicit trans\u00adformer), the generated code mallocs space \nto store the result. In that case, the old-version pointer is added to a list of ad\u00addresses to be reclaimed \nonce the update is complete. This is what happens for next in Example 3 since the struct in\u00adcreased in \nsize, new memory is allocated for it and all .elds must be copied. As an optimization, since the generated \ncode for an unchanged struct reuses the old memory, migrations ignore non-pointer .elds, retaining the \nold values. Once the source and target addresses (which may be the same) have been determined, they are \nadded to the migration map. Note that using malloc/ free for memory management during mi\u00adgration precludes \nprograms that use custom allocators. We hope to address this limitation in future work. xfgen-generated \ncode may uselessly traverse portions of the heap that do not contain changed data. If the programmer \nknows that a particular data structure contains only pointers into the heap (and not to global or local \nvariables), and that no pointed-to objects require transformation, she can create transformers that truncate \nthe traversal. For example, if .eld f of struct foo (transitively) points only to unchanged heap data, \nthe developer could write struct foo. f . struct foo. f : { $out = $in; } to shallow-copy the .eld. We \ndid this for redis-mod (Fig\u00adure 5, discussed in Section 4.3). The migrations generated by xfgen assume \nthere are no pointers into the middle of migratable objects. To help check this assumption, we provide \nan execution mode in which the created migrations use an interval tree to record the start and end of \neach object they encounter. A migration reports an error if it is ever asked to migrate an object that \noverlaps with, but does not exactly match the bounds of, a previously migrated object. Supporting pointers \nto the interior of objects is future work. Type annotations. xfgen sometimes needs type informa\u00adtion \nbeyond what is normally available in C. For example, without further guidance, xfgen would generate an \nincorrect migration for mapping in Example 1. It would assume that mapping points to a single data element, \nrather than an array of elements. In Kitsune, this extra information is provided by the programmer as \nannotations, shown in Figure 4(c). kitc recognizes these annotations and adds the information sup\u00adplied \nby them to the . ts .les. The annotations, inspired by Deputy [6], are straightfor\u00adward. E PTRARRAY(S) \nprovides a size S for a pointed-to array. For example, we would change line 5 of Figure 2 to data * E \nPTRARRAY(con.g size) mapping; By default, xfgen assumes that t* values for all types t are annotated \nwith E PTRARRAY(1); explicit annotations override this default. Annotation E ARRAY(S) provides a size \nS for array .elds at the end of a struct (such .elds can be left unsized in C). For both of these annotations, \nS can be an integer constant, a global variable, or a co-located struct .eld. E OPAQUE annotates pointers \nthat should be copied as values, rather than recursed inside during traversals (so we could use this \nannotation on the foo. f .eld to truncate traversal in the above example, rather than de.ne a manual \ntransformation). Finally, xfgen includes annotations to handle some id\u00adiomatic uses of void* to encode \nparametric polymorphism (a.k.a. generics). For example, the following de.nition in\u00adtroduces a struct \nlist type that is parameterized by type variable @t, which is the type of its contents: struct list { \nvoid E VAR(@t) *val; struct list E INST(@t) *next; } E FORALL(@t); E FORALL(@t) introduces polymorphism, \nE VAR(@t) refers to type variable @t, and E INST(@t) instantiates a polymor\u00adphic type with type @t. For \ncomparison, this example is equivalent to the following Java generic linked list: class List <T> { // \nlike E FORALL(@T) T val; // like E VAR(@T) List <T> next; // like E INST(@T) } With generics, we can \nwrite struct list E INST(int) *x to declare that x is alistof ints. Generated migration code will invoke \nthe migration function that is appropriate for the list elements instantiated type. 4. Experiments To \nevaluate Kitsune, we used it to develop dynamic updates for .ve widely deployed server programs. To quantify \nthe programming effort of using Kitsune, we tallied the number and kinds of changes we made to the pro\u00adgrams, \nand the number and variety of xfgen speci.cations we wrote for state transformation. Overall, we made \nfew code changes between 57 and 159 LoC to support up\u00addating, with most changes only to the initial version. \nLike\u00adwise, xfgen speci.cations were generally small, averaging  Program # Vers LoC Upd Ctrl Data E * \nOth S v. v t. t S xf LoC vsftpd 14 (1.1.0 2.0.6) 12,202 6 26 17+8 6+14 28+8 83+30 9 21 30 101 redis 5 \n(2.0.0 2.0.4) 13,387 1 2 3 43 8 57 0 4 4 37 Tor 13 (0.2.1.18 0.2.1.30) 76,090 1 39 37+6 19 57 153+6 16 \n15 31 189 memcached* 3 (1.2.2 1.2.4) 4,181 4 9 13 20 66 112 12 10 22 27 icecast* 5 (2.2.0 2.3.1) 15,759 \n11+1 22+3 14+9 32+3 39 118+16 25 50 75 200 *Multi-threaded Table 1. Kitsune benchmark programs, and \nmodi.cations to support updating 3 4 lines per changed variable or type. These numbers are comparable \nto prior work. To assess the performance overhead of using Kitsune, we measured the slowdown on normal \nexecution for the Kitsune versions of the servers compared to the originals. We also measured the time \ntaken to perform an update, from signaling to completion. We .nd that there is essentially no overhead \non normal execution, a result uniformly better than prior work. We found that the time required to apply \nan update ranges from 2ms up to 1s, depending on the program; in all cases, the times seem acceptable \nfor typical use. 4.1 Benchmarks We chose a suite of benchmark programs that maintain in\u00adprocess state \nthat would be bene.cial to preserve during an update. Vsftpd is a popular open-source FTP server. Re\u00addis \nis a key-value database used by several high-traf.c ser\u00advices, including guardian.co.uk and craigslist.org. \nTor is a popular onion-router that provides anonymous Inter\u00adnet access. Memcached is a widely used, high-performance \ndata caching system employed by sites such as Twitter and Wikipedia. Icecast is a popular music streaming \nserver. All of these programs maintain persistent network connections that an of.ine update would interrupt. \nRedis and memcached also maintain potentially large volumes of in-memory data that would either be would \nbe lost (memcached) or expen\u00adsive to restore (redis) following an update. Vsftpd also serves as a useful \nbenchmark because several other DSU systems have used it for evaluation [5, 9, 13, 17]. The left portion \nTable 1 lists for each program the length of the version streak we looked at (for n versions, there are \nn-1 updates), which versions we considered, and the number of source lines of the last version as computed \nby sloccount. We consider at least three months of releases per program; for Tor we cover two years and \nfor vsftpd we cover three. We tested that all programs behaved correctly before, during, and after updates \nwere applied.  4.2 Programmer effort Here we describe the manual effort that was required to prepare \nthese programs for updating with Kitsune, and to craft updates corresponding to releases of these programs. \nThe center portion of Table 1 summarizes the Kitsune\u00adrelated changes we made to these programs, tabulating \nthe number of update points added; the number of lines of code needed for control migration and data \nmigration;3 the number of type annotations for xfgen; the number of lines changed for other reasons; \nand their sum. Each column shows the number of changes in the .rst version, followed by +n where n is \nthe sum of changes in all subsequent ver\u00adsions; if this is omitted, no further changes were needed. One \nstriking trend in the table is that most required changes occurred in the .rst version. Control migration \nand update points were particularly stable, essentially be\u00adcause the top-level control structure of the \nprograms rarely changed. Data migration code and annotations were occa\u00adsionally added along with new \ndata structures. Another in\u00adteresting trend is that the magnitude of the changes required is not directly \nproportional to either the code size or num\u00adber of versions considered, e.g., 134 LoC for 16 KLoC ice\u00adcast \nvs. 159 LoC for 76 KLoC Tor. On re.ection, this trend makes sense. Changes to support control migration \ndepend on the number and location of update points, and data an\u00adnotations depend on the type and number \nof data structures; none of these characteristics scales directly with code size. Together, these numbers \nshow that with Kitsune, DSU can be viewed as a stable program feature that is added to the program and \nmaintained (with minimal comparative effort) as the program evolves. The rightmost columns of the table \nlist the xfgen speci.\u00adcations we wrote for each program s updates. In particular, we list the number \nof variable transformers (v. v) and type transformers (t. t), across all versions, and their sum. We \nalso list the total number of lines of transformer code we wrote, across all versions. We can see that, \non average, 3 4 lines of xfgen code were needed for each transformer. Overall, we found both the control \nand data migration code relatively easy to write. Following the structure of the example given in Section \n2.2, we added conditionals to avoid re-initializing data we wished to preserve across the update, and \nto direct control back to the correct update points. State transformation code was also relatively easy \nto write. Most state required either no or very simple transformation along the lines of Example 1 in \nSection 3.1. Perhaps the most tricky part was adding type annotations to data structures. 3 Speci.cally, \ncontrol migration changes comprise calls to kit\u00adsune is updating and kitsune is updating from. Data migration \nchanges include calls to kitsune do automigrate, MIGRATE GLOBAL, and MIGRATE LOCAL, and uses of the kitsune \nno automigrate attribute.  While in many cases these annotations were obvious (spec\u00adifying generic types \nor bounded arrays) or prompted by xf\u00adgen, a missing annotation was sometimes hard to debug. For example, \nfailing to annotate a pointer as an array would re\u00adsult in the generated code not migrating all of the \nstate, ul\u00adtimately leading to a later crash. Fortunately, since Kitsune uses normal compilation and linking, \nwe could use gdb to debug these problems directly. Now we consider the particulars of each program, and \nwhen possible, compare the magnitude of these changes against those required by prior systems. In general, \nusing Kitsune seems to require more program changes than prior systems, but the total sum of changes \nis still small. Vsftpd. Many of the changes we made to vsftpd were typ\u00adical across our benchmarks: we \nadded type information for generics and inserted control .ow changes to avoid overwrit\u00ading OS state when \nupdated. We added one update point for each of the .ve long-running loops in the program, which comprise \nthe connection acceptance loop, two loops to im\u00adplement privilege-isolated logins, the main FTP command \nprocessing loop, and a loop left running in a privileged par\u00adent process that implements commands such \nas chown. The most interesting change we made to vsftpd was to handle I/O. Vsftpd replaces calls to recv \nwith calls to a wrapper that restarts the actual read if it is interrupted, e.g., by the receipt of a \nsignal. We inserted one update point in the wrapper so that interruption can initiate an update. To simplify \nthe control-.ow changes needed, rather than give the update point its own name, we reused the name of \nthe update point in the loop that initiated the wrapped call; this is safe is because this loop will \nreinitiate the call when the update completes. Other DSU systems. Neamtiu et al. [17] applied Ginseng, \nanother DSU system, to vsftpd. They updated a subset of the version streak we did (.nishing at version \n2.0.3). Even though their changes support just one update point (versus our six, which permit updating \nin many more situations), the effort was comparable: They report 50 LoC changed and 162 lines for state \ntransformation, compared to 113 LoC changed and 101 lines of state transformation for Kitsune. Makris \nand Bazzi [13] also updated vsftpd using UpStare for a shorter streak. They say that some manual initializa\u00adtion \nof new variables and struct .elds was required, along with 11 user-de.ned continuation mappings, but \nprovide no detail as to their overall size. The UpStare distribution includes 14 mapping .les, containing \n4,644 lines of C code between them. Much of this code appears auto-generated. The manually written portion \nis chie.y used to transform variables and types. Redis. Redis required few modi.cations to support updat\u00ading. \nWe placed a single update point in its main event loop and added one check to avoid some reinitialization. \nThe vast majority of redis s state is stored in a single global variable, server , so few variables needed \nmigration. Redis makes ex\u00adtensive use of linked lists and hash tables, and we used xf\u00adgen s generics \nannotations to model their types precisely. The version streak we considered included only code modi\u00ad.cations, \nbut we still needed xfgen to migrate data structures that reference global variables (whose addresses \nchange with each updated version). For example, we wrote a 19-line speci.cation to direct the traversal \nthrough a void* .eld by consulting an integer key to determine the .eld s type; while long, this code \nwas straightforward. Finally, redis uses a cus\u00adtom allocator, which xfgen does not support, so we modi.ed \nthe redis header .les with preprocessor directives to redirect custom allocator calls to malloc and free \n. We are unaware of prior work applying DSU to redis. Tor. Tor is the largest of our benchmark programs, \nat ~76 KLoC. Adding DSU support required one update point in Tor s main loop. The larger number of control \n.ow changes in comparison to other programs is a result of Tor s modular design. Twelve of the 33 modules \nmodule init functions required simple two-line changes to prevent re\u00adinitialization of updated state. \nThe remaining changes were made along the path from main to Tor s main loop, simi\u00adlarly to our other \nbenchmarks. Most data changes were kitsune no automigrate attributes, directing Kitsune to skip over \nvarious constant strings used in parsing Tor s con.gura\u00adtion .le. We automigrated most global variables \nat the start of update execution, but manually migrated Tor s network consensus data structure (via a \ncall to MIGRATE GLOBAL) which, for an update that added a new .eld, required other global state to be \nconsistent .rst. The rest of our changes (counted above as other ) to Tor were primarily to add sup\u00adport \nto initiate an update via Tor s control interface, rather than a command-line signal, for easier testing. \nWe wrote eight xfgen rules, corresponding to 16 variables and 15 types. All the transformers for Tor \ndata structures were straightforward, since type representation changes were rare in the streak we considered. \nInvestigating further, we found that Tor s data representations tended to remain stable because most \nTor data represents protocol messages, and these rarely change so as to preserve backward compat\u00adibility. \nThe bulk of the remaining xfgen rules update func\u00adtion pointers for event handling stored in libevent \nand/or OpenSSL datastructures. We are unaware of prior work applying DSU to Tor. Memcached. Memcached \nis a multi-threaded server im\u00adplemented using libevent. Unlike Tor, memcached s main loop is in libevent \ns event base loop function. To work with Kitsune, we had to change main to install a libevent callback \non SIGUSR2 to handle update noti.cations. When the main thread is noti.ed of an update, it uses pipes, \nvia libevent it\u00adself, to notify each of the child threads of the update; each thread then terminates \nin response. The main thread then performs the update and all threads work back to calls to event base \nloop in the new code. Interestingly, we needed to sandwich such calls with like-named update points: \n kitsune update ( upd ); /* complete any active update */ event base loop ( libevent base , 0); /* \npass control to libevent */ kitsune update ( upd ); /* a new update upon return */ When a thread is \nsignaled, the event base loop call returns and initiates the update. When the program restarts, the update \nwill complete when it reaches the update point (of the same name) just prior to the event base loop call \nin the new code. There are two additional challenges in updating due to libevent: First, as with Tor, \nwe needed to reinstall new function pointers in libevent after an update. Second, mem\u00adcached installs \nthe data associated with active connections in libevent, but does not retain its own pointers to that \ndata. To enable updates to transform that data, we added code to maintain, in memcached itself, a list \nof active connections, which we can then use for state transformation. Other DSU systems. Neamtiu and \nHicks [16] updated memcached using Ginseng. They needed 26 lines of pro\u00adgram changes and 12 lines for \nstate transformation. Kit\u00adsune required more changes in part because we did not change libevent itself, \nwhich in Neamtiu and Hicks setup was merged into the main program (and thus was updatable). Their changes \nalso created a problem with reaching update points suitably often due to intervening blocking calls; \nplac\u00ading the update point outside libevent avoided this issue. Icecast. Icecast is another multi-threaded \nprogram, with separate threads for connection acceptance, connection han\u00addling, .le serving, receiving \na stream from another server, sending statistics, and more. To modify icecast to support DSU with Kitsune, \nwe located each of the thread creation points and then inspected the thread entry function and re\u00adlated \ncode to make two types of changes. First, we identi.ed the thread s long-running, event-handling loop \nto which we added a call to kitsune update. Second, we added the neces\u00adsary annotations to migrate local \nvariables or skip initializa\u00adtion during thread startup. The most complex icecast patch added a new thread \nto handle authentication (which required us to add an update point to the new authentication thread s \ncode) and reduced the number of connection threads. To im\u00adplement this patch, we wrote transformation \ncode that uses a C API provided by the Kitsune runtime that exposes the set of active threads at the \ntime the update was taken (including the entry function, initialization argument, and update point taken) \nand allows the developer to add and remove threads or modify the properties of existing threads. Other \nDSU systems. Neamtiu and Hicks [16] also con\u00adsidered updates to the same streak of icecast versions. \nThey changed 154 LoC and wrote 80 lines of state transformation code. For Kitsune we changed 134 lines \nof the main pro\u00adgram, and wrote 200 lines of xfgen speci.cations. A large proportion of these speci.cations \nwere simple rules to ini\u00adtialize added variables or .elds. However, they also included more complex rules \nfor adding and removing a thread, as Program Orig (siqr) Kitsune Ginseng UpStare 64-bit, 4\u00d72.4Ghz E7450 \n(6 core), 24GB mem, RHEL 5.7 vsftpd 2.0.6* memchd 1.2.4 redis 2.0.4 icecast 2.3.1 6.55s (0.04s) +0.75% \n 59.30s (3.25s) +0.51%  46.83s (0.40s) -0.31%  10.11s (2.27s) -2.18% 32-bit, 1\u00d73.6Ghz Pentium D (2 \ncore), 2GB mem, Ubuntu 10.10 vsftpd 2.0.3* vsftpd 2.0.3 memchd 1.2.4 redis 2.0.4 icecast 2.3.1 5.96s \n(0.01s) +2.35% +11.3% +41.6% 14.03s (0.02s) +0.29% +1.47% +6.64% 101.40s (0.35s) -0.49% +18.4% 43.88s \n(0.16s) -1.21%  35.71s (0.68s) +1.18% -0.28% *CD+LS benchmark, .le download benchmark Table 2. Normal \nexecution performance overhead mentioned above. Ginseng s patches do not attempt to re\u00admove this extra \nconnection thread.  4.3 Performance Normal execution overhead. We measured the overhead Kitsune adds \nto normal execution for all programs except Tor, discussed separately below. For comparison, we also \nmeasured the overhead of Ginseng and UpStare on programs they had previously benchmarked: vsftpd, memcached, \nand icecast for Ginseng, and vsftpd for UpStare. We used the following workloads: For memcached, we ran \nmemslap (2.5M operations using memslap s default workload). For redis, we used redis-benchmark (1M GET \nand 1M SET operations), and for a fair comparison, we modi.ed the non-updating version of redis to use \nthe stan\u00addard memory allocation functions, as we had done to sup\u00adport xfgen. (Note that switching to \nstandard malloc slightly improves redis performance, since the custom allocator per\u00adformed extra bookkeeping \nfor tracking memory usage.) For vsftpd, we performed two separate tests. First, we measured the time \nto perform the following interaction 2K times: con\u00adnect to the server, change directories, and retrieve \na directory listing. Second, for the 32-bit platform, we measured the time to establish a connection \nand download a 32-byte bi\u00adnary .le 200 times. This test closely resembles tests reported in the UpStare \nand Ginseng papers [13, 17]. For icecast, we used a benchmark originally developed for Ginseng [16] that \nmeasures the time taken for 16 simultaneous clients to download 7 music .les, each roughly 2MB in size. \nFor all programs, we ran the client and server on the same machine, to factor out network latency. Table \n2 reports the results. We ran each benchmark 21 times and report the median time for the unmodi.ed pro\u00adgrams \nalong with the semi-interquartile range (SIQR), and the slowdowns for Kitsune, Ginseng, and UpStare (the \nme\u00addian time for each compared to the median original time). The top of the table gives results on a \n24 core, 64-bit ma\u00adchine, and the bottom gives results on a 2 core, 32-bit ma\u00adchine. Ginseng only works \nin 32-bit mode and UpStare is  Program Med. (siqr) Min Max 64-bit, 4\u00d72.4Ghz E7450 (6 core), 24GB mem, \nRHEL 5.7 vsftpd .2.0.6 memcached .1.2.4 redis .2.0.4 icecast .2.3.1 icecast-nsp .2.3.1 tor .0.2.1.30 \n2.99ms (0.04ms) 2.62 3.09 2.50ms (0.05ms) 2.27 2.68 39.70ms (0.98ms) 36.14 82.66 990.89ms (0.95ms) 451.73 \n992.71 187.89ms (1.77ms) 87.14 191.32 11.81ms (0.12ms) 11.65 13.83 32-bit, 1\u00d73.6Ghz Pentium D (2 core), \n2GB mem, Ubuntu 10.10 vsftpd .2.0.3 memcached .1.2.4 redis .2.0.4 icecast .2.3.1 tor .0.2.1.30 2.62ms \n(0.03ms) 2.52 2.71 2.44ms (0.08ms) 2.27 3.12 38.83ms (0.64ms) 37.69 41.80 885.39ms (7.47ms) 859.00 908.87 \n10.43ms (0.46ms) 10.08 12.98  Table 3. Kitsune update times only distributed in binary packages, which \nare only available for 32-bit systems. From this data, we can see that Kitsune adds essentially no overhead \nto normal execution: the performance differ\u00adences range from -2.18% to 2.35%, which is well within the \nnoise on modern systems [15]. In contrast, the overhead for Ginseng and UpStare is more signi.cant: for \nGinseng it is 11.3% and 18.4% for memcached and the .rst vsftpd bench\u00admark, respectively, and for UpStare \nit is 41.6% for the .rst vsftpd benchmark. For vsftpd, this overhead is higher than previously reported \n[13, 17]: Ginseng s overhead was re\u00adported at 3% and UpStare s at 16%. These results are close to those \nof our second vsftpd benchmark 1.47% overhead for vsftpd and 6.64% for UpStare giving us con.dence that \nwe are making a fair comparison. We conjecture that the higher overhead in the .rst benchmark is due \nto it spending a higher proportion of time executing application code than the sec\u00adond, and thus is more \nimpacted by UpStare s and Ginseng s special compilation. For comparison, UpStare s previously reported \noverhead on PostgreSQL (with data stored in mem\u00adory to spend less time in OS code) was over 40%. Tor. \nWhile we did not measure the overhead of Kitsune on Tor directly, we did test it by running a Tor relay \nin the wild. We dynamically updated this relay from version 0.2.1.18 to version 0.2.1.30 (13 versions) \nas it was carrying traf.c for Tor clients. We initiated several dynamic updates during periods of load, \nwhen as many as four thousand connections carrying up to 11Mb/s of traf.c (up and down) were live. No \nclient connections were disrupted (which would have been indicated by broken or renegotiated TLS sessions). \nOver the course of this experiment, our relay carried 7TB of traf.c. Time required for an update. We \nalso measured the time it takes to deploy an update with Kitsune, i.e., the total elapsed time for an \nupdate s preparation and execution, starting from when the update is signaled to when it has completed. \nTa\u00adble 3 summarizes the results for the last update in each streak, giving the median+SIQR, minimum, \nand maximum update times. For each program, we picked a suitable work- Figure 5. State size vs. update \ntime (color plot) load during which we did the update. For vsftpd, we up\u00addated after an FTP client had \nconnected to and interacted with the server; for redis and memcached, we inserted 1,000 and 15,000 key-value \npairs, respectively, prior to update; and for icecast, we established one connection to a music source \nand 10 clients receiving that stream prior to updating. For Tor, we fully bootstrapped as a client, establishing \nmultiple circuits through the network and communicating with direc\u00adtory servers, and then applied the \nupdate. For all programs except icecast, the update times are quite small. For icecast, most of the nearly \n1 second delay occurs while the Kitsune runtime waits for each thread to reach an update point. This \ntime was lengthened by one-second sleeps sprinkled throughout several of these threads. The line in the \ntable labeled icecast-nsp measures the update time when these sleeps are removed, and shows the resulting \ntime is much shorter. Because the sleeps are there, we conjecture icecast can tolerate the pause for \nupdates; we did not observe a noticeable stop in the streamed music during the update. In recent work \n[11], we have developed techniques to support faster update times, showing signi.cant improvements for \nicecast in particular. We plan to port these ideas to Kitsune in the near future. Recall from Section \n3.2 that xfgen-generated migrations may traverse signi.cant portions of the heap, and thus for some updates \nthe update time may vary with the size of the program state. Among our programs, the most likely to exhibit \nthis issue are redis and memcached, as they may accumulate signi.cant amounts of state. Figure 5 graphs \nthe update time for these two programs versus the number of key-value pairs stored. For redis, the update \ntime grows linearly because we traverse each of the data items on the heap, since some contain pointers \nto global variables that must be redirected to the new version s data segment. On the other hand, memcached \ns update times remain relatively constant because it stores its data in arrays that we treat opaquely, \nremoving the need to traverse each instance.  Examining redis more closely, we observed that the point\u00aders \nthat force us to traverse the heap in fact point to a small, .nite set of static locations. Thus, we \ncreated a modi.ed (42 LoC changed) version of redis, labeled redis-mod, that stores integer indices into \na table in place of those pointers. This obviates the need for a full heap traversal for all the updates \nin our streak, allowing updates times to remain constant for the tested heap sizes. Programs that use \nKitsune may ben\u00ade.t from a similar transformation if they maintain a large amount of state containing \nstatic pointers. 5. Related Work In this section we consider recent work that supports DSU for programs \nwritten in C and C++; these languages im\u00adpose stringent constraints on a DSU system s design. Ta\u00adble \n4 characterizes the mechanisms used to implement Kit\u00adsune and other recent C/C++ DSU systems. Ekiden \n[9], Gin\u00adseng [17], OPUS [1], POLUS [5], and UpStare [13] target applications, while Ksplice [2], K42 \n[3], LUCOS [4], and DynaMOS [14] support (or are) OS kernels. (LUCOS is essentially a version of POLUS \nthat uses VMMs to effect changes in operating systems; all comments we make about the latter apply to \nthe former.) We discuss tradeoffs result\u00ading from these mechanism choices, and argue that Kitsune provides \nthe greatest .exibility and best performance with modest programmer effort. The footnotes in the table \nsum\u00admarize the discussion below. (A direct comparison to two systems, Ginseng and UpStare, appears at \nthe end of the in\u00adtroduction.) Code updates. Most systems effect code updates at the granularity of individual \nfunctions (or objects). As noted in the .rst column, Ksplice, OPUS, DynaMOS, and POLUS insert, at run-time, \na trampoline in the old function to jump to the function s new version.4 As noted in the second column, \nGinseng and K42 use indirection: Ginseng compiles direct function calls into calls via function pointers, \nwhile the K42 OS s object handles are indirected via a hand-coded object translation table (OTT); updates \ntake effect by redirecting indirection targets to the new versions. There are several drawbacks to using \nthese mechanisms. Trampolines require a writable code segment, which makes the application vulnerable \nto code injection attacks. Tram\u00adpoline-based updating may break programs optimized using inlining, since \nit presumes to know where the start of a func\u00adtion is, so POLUS and OPUS both forbid inlining. (Ksplice \nis able to account for the compiler s inlining decisions.) Us\u00ading indirect calls adds overhead to normal \nexecution and also inhibits inlining. Most onerously, neither trampolines nor in\u00addirections support updating \nfunctions that never (or rarely) exit, such as main, which changes relatively frequently [8], or functions \nthat contain event-handling loops, such as the 4 The .rst instruction of the function is replaced by \na jump to a small piece of code, the trampoline, that executes the replaced instruction and then jumps \nto the function s new version. Code upd trampindprog Data upd replshdwwrap Timing nonactvupd pts DynaMOS4 \n\u00d7 \u00d7 Ekiden \u00d7 \u00d7 \u00d7 Ginseng12345 \u00d7 \u00d7 \u00d7 K4225 \u00d7 \u00d7 \u00d7 Ksplice \u00d7 \u00d7 \u00d7 OPUS2 \u00d7   \u00d7 LUCOS/POLUS245 \u00d7 \u00d7 UpStare23 \n\u00d7 \u00d7 (\u00d7) Kitsune \u00d7 \u00d7 \u00d7 1needs deep analysis 4mixes old and new code 2inhibits optimizations 5relaxed \nthread sync. 3pervasive instrumentation Table 4. Comparing DSU systems for C/C++ scheduling loop in the \nOS. In the best case, programmers must refactor the program to place long-running loop bodies in separate \nfunctions (e.g., using loop extraction [17]). The remaining three systems, UpStare, Ekiden, and Kit\u00adsune \nsupport more general changes by updating at the granu\u00adlarity of the whole program rather than individual \nfunctions. UpStare loads in code for the new program and then per\u00adforms stack reconstruction: the running \nprogram automati\u00adcally unwinds the current stack one function at a time back to main, and then rewinds \nthe stack to a new-version program point speci.ed by the programmer. In contrast, Kitsune re\u00adlies on \nthe programmer to migrate control to the equivalent new-version program point. Kitsune s manual approach \npays dividends in both bet\u00adter performance and simpler semantics. To allow updates to happen at any program \npoint, UpStare s compiler adds un\u00adwinding/rewinding code to all functions; while convenient, this code \nadds substantial performance overhead to normal execution. Moreover, to exploit UpStare s .exibility, \na de\u00adveloper must carefully de.ne how to map from all possible old-version thread stacks to new-version \nequivalents. Up-Stare reduces this burden allowing the programmer to limit updates to fewer program points, \njust as Kitsune does. But then the value of general-purpose stack reconstruction is less clear. Kitsune \nallows all compiler optimizations5 and code to support control migration imposes no overhead during normal \nexecution since such code only appears on program paths leading to update points, and these paths tend \nnot to intersect with normal execution paths. Moreover, expressing control migration in the code rather \nthan in a speci.cation to the side is arguably advantageous: with only a few update points there is very \nlittle code to write, and its presence in the program makes the update semantics explicit and easier \nto understand. Ekiden, the precursor of Kitsune, effects up\u00ad 5 Compiling the updatable program to use \nposition-independent code (PIC) sacri.ces a register. However, modern servers are often compiled with \nPIC to enable address-space layout randomization.  dates by transferring state to a new-version process; \nit em\u00adploys roughly the same API as Kitsune and enjoys its ben\u00ade.ts of high .exibility and low overhead, \nbut updates take longer and require more memory. Data updates. Returning to the table, we can see that \nmost systems handle data structure representation changes by us\u00ading object replacement, in which the \nprogrammer, or sys\u00adtem, can allocate replacement objects and initialize them us\u00ading data from the old \nversion. Ksplice and DynaMOS leave the old objects alone but allocate shadow data structures that contain \nonly the new .elds. Ginseng uses an approach called type wrapping wherein programs are compiled so that \nstructs have an added version .eld and extra slop space to allow for future growth. Calls to mediator \nfunctions are inserted to access updatable objects and these calls initiate transformation of those objects \nthat are not up to date. Shadow data structures have the bene.t that fewer func\u00adtions are changed by \nan update: if we add a new .eld to a struct, then only code that uses that .eld is affected, rather than \nall code that uses the struct. But programmers must write additional code to deal with shadow .elds and \nmanage their lifetimes, which imposes run-time overhead and clut\u00adters the software over time. Type wrapping \nhas the bene.t that there is no need to .nd objects in order to update them; rather, object transformation \nwill occur lazily as the new version executes. But type wrapping has several limitations: (1) mediator \nfunctions slow normal execution; (2) the added slop space hurts performance (e.g., cache locality) and \nmay prove insuf.cient for some changes; (3) the change in repre\u00adsentation forbids certain coding idioms \n(e.g., involving type\u00adcasts to/from void*); and (4) the whole-program static anal\u00adysis underlying Ginseng \ns type wrapping has trouble scaling. Object replacement adds the least overhead to normal ex\u00adecution, \nbut there must be a way to .nd all instances of changed objects (e.g., by chasing pointers from global \nvari\u00adables) and redirect these pointers to newly allocated, trans\u00adformed objects. K42 s coding style \nmakes this easy the system can just traverse the OTT but most applications are not written this way. \nKitsune s xfgen tool is able to generate traversal code given relatively small speci.cations and some \ntype annotations; in other systems, the programmer burden is much higher. Note that DSU for type-safe \nlanguages can avoid xfgen s traversal generation: the garbage collector can automatically .nd and initiate \ntransformation of changed ob\u00adjects [7, 19] without need of further type annotations. That said, object \nreplacement in Kitsune is not a panacea. For one, xfgen does not currently support migrating a pointer \nto the interior of an object before the object itself has been migrated. Type wrapping is not limited \nin this way because the address of the interior object will not change between versions. However, type \nwrapping does suffer from a related problem where other data structures may hold onto an ad\u00address inside \na wrapped type (what Neamtiu and Hicks call an abstraction-violating alias [17]), which will delay the \nupdate. Another problem with object replacement in Kit\u00adsune is that all migration takes place at update \ntime, poten\u00adtially producing a lengthy pause in execution. Type wrapping postpones transformation of \ndata until it is accessed, which amortizes the transformation cost over the post-update exe\u00adcution. However, \ntype wrapping s lazy approach could have the undesirable of effect of delaying the pause until the ap\u00adplication \nis performing a time-critical request. Timing. Returning to the table, we consider how systems determine \nwhen an update may take effect. Ksplice, K42, and OPUS only permit an update when changed code is not \nactive; that is, no thread is running that code, and no thread s stack refers to it. While this restriction \nreduces post-update errors, it does not eliminate them [8], and moreover imposes strong restrictions \non the form of an update and how quickly it can be applied. For increased .exibility, other systems allow \nupdates to active code. Kitsune and UpStare updates take place when all threads reach a programmer-designated \nupdate point (for UpStare, such points may be system-determined). We have found this simple approach \nworks quite well in practice. In contrast, Ginseng allows an update to take effect so long as it appears \nas though it occurred when all threads were at update points [16]. This approach accelerates update times, \nbut the static analysis that underlies it scales poorly and is conservative, requiring awkward code restructurings. \nPO-LUS allows threads to update immediately, and thus because POLUS updates take effect at function calls, \nafter an up\u00addate a program may wind up running bits of old and new code at the same time; a study using \nGinseng showed mixing code versions substantially increases the odds of errors [8]. Moreover, POLUS data \nstructures are versioned, with ver\u00adsion N of the code accessing version N of the data, so the programmer \nde.nes callbacks (invoked via virtual memory page protection support) to keep the copies in sync. There\u00adfore, \nthe programmer must understand the impact of multiple code versions accessing the same data, which we \nimagine could be tricky when a datastructure change corresponds to a change in semantics. Our experience \nwith the simple bar\u00adrier approach suggests these more sophisticated approaches, with higher programmer \ndemands, may be unnecessary [11]. Checkpointing. Checkpoint-and-restart systems [18] al\u00adlow programs \nto be relaunched in the middle of execution from a checkpoint. At a high-level this bears some similarity \nto DSU, but checkpointing systems do not provide support for changing code or data representations on \nrestart. Eki\u00adden effects an update by serializing and transferring state between an old-version process \nand a fresh new-version process i.e., Ekiden works like a checkpointing system that does permit code \nand data modi.cation. However, we found that the cost of transferring state in Ekiden was signif\u00adicant, \nand hence moved to the Kitsune model, which has a similar programming API but allows in-place code and \ndata changes, for better performance.  6. Conclusions We have presented Kitsune, a new system for dynamically \nupdating C programs. Kitsune works by updating the entire program at once, thus avoiding the restrictions \nimposed by other DSU systems on data representations, programming idioms, and compiler optimizations. \nKitsune s design allows program changes for updatability to be simple and informa\u00adtive, and xfgen makes \nwriting state transformers much eas\u00adier. Our results from applying Kitsune to single-and multi\u00adthreaded \nbenchmarks show that Kitsune has essentially no performance overhead, and code changes required to use \nKitsune are comparable to, or only slightly more than, prior systems. We believe that the ideas and insights \nbehind Kit\u00adsune could also be applied to C++ programs, though extend\u00ading to kitc and xfgen to C++ would \nrequire non-trivial ef\u00adfort. We believe that Kitsune s careful balancing of .exibil\u00adity, ef.ciency, and \nease-of-use makes it a major step forward in practical dynamic software updating for C. Acknowledgments \nThis research was supported in part by NSF grant CCF\u00ad0910530 and the partnership between UMIACS and the \nLab\u00adoratory for Telecommunication Sciences. We would like to thank Karla Saur and Jonathan Turpie for \nhelp in the devel\u00adopment and testing of Kitsune. Emery Berger, Miguel Cas\u00adtro, JP Martin, Cristi Zam.r, \nand the anonymous referees provided helpful comments on drafts of this paper. Kristis Makris helped us \nwith the UpStare benchmarks. References [1] G. Altekar, I. Bagrak, P. Burstein, and A. Schultz. OPUS: \nOnline patches and updates for security. In Proc. USENIX Security, 2005. [2] J. Arnold and M. F. Kaashoek. \nKsplice: automatic rebootless kernel updates. In Proc. EuroSys, 2009. [3] A. Baumann, J. Appavoo, D. \nD. Silva, J. Kerr, O. Krieger, and R. W. Wisniewski. Providing dynamic update in an operating system. \nIn Proc. USENIX ATC, 2005. [4] H. Chen, R. Chen, F. Zhang, B. Zang, and P.-C. Yew. Live updating operating \nsystems using virtualization. In Proc. VEE, 2006. [5] H. Chen, J. Yu, C. Hang, B. Zang, and P.-C. Yew. \nDynamic software updating using a relaxed consistency model. IEEE Transactions on Software Engineering, \n37(5), 2011. [6] J. Condit, M. Harren, Z. Anderson, D. Gay, and G. C. Necula. Dependent types for low-level \nprogramming. In Proc. ESOP, 2007. [7] S. Gilmore, D. Kirli, and C. Walton. Dynamic ML without dynamic \ntypes. Technical Report ECS-LFCS-97-378, LFCS, University of Edinburgh, 1997. URL http://www.dcs.ed. \nac.uk/home/stg/DynamicML/dynamic.ps.gz. [8] C. M. Hayden, E. K. Smith, E. A. Hardisty, M. Hicks, and \nJ. S. Foster. Evaluating dynamic software update safety using ef.cient systematic testing. IEEE Transactions \non Software Engineering, 99(PrePrints), Sept. 2011. [9] C. M. Hayden, E. K. Smith, M. Hicks, and J. S. \nFoster. State transfer for clear and ef.cient runtime upgrades. In Proc. HotSWUp, 2011. [10] C. M. Hayden, \nS. Magill, M. Hicks, N. Foster, and J. S. Foster. Specifying and verifying the correctness of dynamic \nsoftware updates. In Proc. International Conference on Veri.ed Soft\u00adware: Theories, Tools, and Experiments \n(VSTTE), 2012. [11] C. M. Hayden, K. Saur, M. Hicks, and J. S. Foster. A study of dynamic software update \nquiescence for multithreaded pro\u00adgrams. In Proc. HotSWUp, 2012. [12] M. Hicks and S. Nettles. Dynamic \nsoftware updating. ACM TOPLAS, 27(6), 2005. [13] K. Makris and R. Bazzi. Immediate Multi-Threaded Dynamic \nSoftware Updates Using Stack Reconstruction. In USENIX ATC, 2009. [14] K. Makris and K. D. Ryu. Dynamic \nand Adaptive Updates of Non-Quiescent Subsystems in Commodity Operating System Kernels. In Proc. EuroSys, \n2007. [15] T. Mytkowicz, A. Diwan, M. Hauswirth, and P. F. Sweeney. Producing wrong data without doing \nanything obviously wrong! In Proc. ASPLOS, 2009. [16] I. Neamtiu and M. Hicks. Safe and timely dynamic \nupdates for multi-threaded programs. In Proc. PLDI, 2009. [17] I. Neamtiu, M. Hicks, G. Stoyle, and M. \nOriol. Practical dynamic software updating for C. In Proc. PLDI, 2006. [18] E. Roman. A survey of checkpoint/restart \nimplementations. Technical report, Lawrence Berkeley National Laboratory, Tech, 2002. [19] S. Subramanian, \nM. Hicks, and K. S. McKinley. Dynamic Software Updates: A VM-centric Approach. In Proc. PLDI, 2009. [20] \nZeroTurnaround. LiveRebel. http://www. zeroturnaround.com/liverebel.   \n\t\t\t", "proc_id": "2384616", "abstract": "<p>Dynamic software updating (DSU) systems allow programs to be updated while running, thereby permitting developers to add features and fix bugs without downtime. This paper introduces Kitsune, a new DSU system for C whose design has three notable features. First, Kitsune's updating mechanism updates the whole program, not individual functions. This mechanism is more flexible than most prior approaches and places no restrictions on data representations or allowed compiler optimizations. Second, Kitsune makes the important aspects of updating explicit in the program text, making the program's semantics easy to understand while minimizing programmer effort. Finally, the programmer can write simple specifications to direct Kitsune to generate code that traverses and transforms old-version state for use by new code; such state transformation is often necessary, and is significantly more difficult in prior DSU systems. We have used Kitsune to update five popular, open-source, single- and multi-threaded programs, and find that few program changes are required to use Kitsune, and that it incurs essentially no performance overhead.</p>", "authors": [{"name": "Christopher M. Hayden", "author_profile_id": "81447599362", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P3856075", "email_address": "hayden@cs.umd.edu", "orcid_id": ""}, {"name": "Edward K. Smith", "author_profile_id": "81487652432", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P3856076", "email_address": "tedks@cs.umd.edu", "orcid_id": ""}, {"name": "Michail Denchev", "author_profile_id": "81548956356", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P3856077", "email_address": "mdenchev@cs.umd.edu", "orcid_id": ""}, {"name": "Michael Hicks", "author_profile_id": "81100060959", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P3856078", "email_address": "mwh@cs.umd.edu", "orcid_id": ""}, {"name": "Jeffrey S. Foster", "author_profile_id": "81338488852", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P3856079", "email_address": "jfoster@cs.umd.edu", "orcid_id": ""}], "doi_number": "10.1145/2384616.2384635", "year": "2012", "article_id": "2384635", "conference": "OOPSLA", "title": "Kitsune: efficient, general-purpose dynamic software updating for C", "url": "http://dl.acm.org/citation.cfm?id=2384635"}