{"article_publication_date": "08-25-2003", "fulltext": "\n Functional Automatic Differentiation with Dirac Impulses Henrik Nilsson Department of Computer Science \nYale University Henrik.Nilsson@yale.edu ABSTRACT Functional Reactive Programming (FRP) is a framework \nfor reactive programming in a functional setting. FRP has been applied to a number of domains, such as \ngraphical an\u00adimation, graphical user interfaces, robotics, and computer vision. Recently, we have been \ninterested in applying FRP\u00adlike principles to hybrid modeling and simulation of physical systems. As \na step in that direction, we have extended an existing FRP implementation, Yampa, in two new ways that \nmake it possible to express certain models in a very natural way, and reduces the amount of work needed \nto put model\u00ading equations into a suitable form for simulation. First, we have added Dirac impulses that \nallow certain types of dis\u00adcontinuities to be handled in an easy yet rigorous manner. Second, we have \nadapted automatic di.erentiation to the setting of Yampa, and generalized it to work correctly with Dirac \nimpulses. This allows derivatives of piecewise contin\u00aduous signals to be well-de.ned at all points. This \npaper re\u00adviews the basic ideas behind automatic di.erentiation, in particular Jerzy Karczmarczuk s elegant \nversion for a lazy functional language with overloading, and then considers the integration with Yampa \nand the addition of Dirac impulses.  Categories and Subject Descriptors D.3.2 [Programming Languages]: \nLanguage Classi.ca\u00adtions functional languages, data-.ow languages; I.6.2 [Sim\u00adulation And Modeling]: \nSimulation Languages; I.6.8 [Sim\u00adulation And Modeling]: Types of Simulation continu\u00adous, discrete event \n General Terms Languages, Algorithms  Keywords FRP, Haskell, functional programming, synchronous data.ow \nlanguages, modeling languages, hybrid modeling, automatic di.erentiation, distribution theory, Dirac \nimpulses Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n03 August 25 27, 2003, Uppsala, Sweden. Copyright 2003 ACM 1-58113-756-7/03/0008 ...$5.00. 1. INTRODUCTION \nConsider a highly simpli.ed model of a bouncing ball as shown in .gure 1(a). It is dropped from an initial \nheight y0 and falls toward the .oor at y = 0 under the in.uence of the gravitational force -mg, where \nm is the mass of the ball. For simplicity s sake, we want to model the ball as if it were a point mass. \nMoreover, we do not want to model the physics of the interaction when the ball hits the .oor in any great \ndetail. Instead we assume that the impact is fully elastic and that it occurs instantaneously. Under \nthese assumptions, the velocity of the ball will simply change sign whenever it bounces o. the .oor. \nFigure 1(b) shows the position y and velocity y.as functions of time when a ball modeled in this fashion \nis released from y0 = 1 m. This is an example of a hybrid model; that is, a model having both continuous \nand discrete aspects. The behavior of the falling ball is described continuously through New\u00adton s laws \nof motion, whereas the instantaneous interaction between the ball and the .oor are isolated events occurring \nat discrete points in time. While the present model is trivial, the kind of modeling simpli.cations it \nexempli.es are very common in practice, and a major reason for why models of physical systems often become \nhybrid models. Hybrid models are described by a set of systems of dif\u00adferential equations. Each individual \nsystem of equations de\u00adscribes the continuous behavior of the modeled system in a particular mode of \noperation by relating time-varying, con\u00adtinuous variables and their time derivatives. Discrete aspects \nof the behavior are expressed by switching among the modes of operation, governed by predicates specifying \nswitching events and suitable initial conditions for the continuous state variables of the mode being \nactivated. Hybrid automata [7] are one way to formalize these notions. These models can then be used \nin di.erent ways, but evaluation of the behav\u00adior over time through simulation based on numerical inte\u00adgration \nis the most common application. Explicit mode switching is not always the most intuitive and clear way \nto describe a hybrid model. In the example of the bouncing ball, most modeling languages require a mode \nswitch when the ball bounces o. the .oor. The initial con\u00addition of the new mode will have the same position \nfor the ball (on the .oor) but negate the velocity of the ball so that it is moving up. In systems like \nthis one where the sole pur\u00adpose of mode switching is an abrupt change in one of the state variables, \nexplicit switching leads to an unnecessarily complicated description of the system. In cases like these, \nDirac impulses [19, 6] o.er an alterna\u00adtive way to account for abrupt changes in continuous vari\u00ad 5 4 \n3 y 2 1 y0 0 -1 -2 mg -3 -4 -5 0 (a) Ball over .oor (b) Position and velocity of the ball Figure \n1: Idealized bouncing ball ables. Informally, a Dirac impulse can be understood as an in.nitely high \nand narrow impulse whose area is .nite. This is exactly what is needed to model the force of interaction \nbetween the ball and the .oor in this case in which the in\u00adteraction is assumed to take place instantaneously. \nThis idea has obviously been considered in the modeling community, but does not yet seem to be available \nin any mainstream systems for hybrid modeling and simulation [12]. As will become clear, mode switching \nalso has serious im\u00adplications for another aspect of many simulation systems: di.erentiation. It is often \nnecessary compute derivatives of various functions in a model. One reason is the use of sophis\u00adticated \nnumerical integration methods that solve non-linear equations at each time step. The need also arises \nwhen one time-varying quantity in a model is the time derivative of another, and the causality, i.e., \nthe direction of data .ow, in a particular mode of operation is such that the former needs to be computed \nfrom the latter through di.erentia\u00adtion, rather than the latter from the former through numer\u00adical integration. \nEither way, automatic di.erentiation [2] is a key technique for computing derivatives for this kind of \napplications. It is an algebraic method based on computing the value of a func\u00adtion along with the derivative \nof that function with respect to some speci.c variable at the same point. Thus, unlike symbolic di.erentiation, \none does not obtain a closed, sep\u00adarate expression for the derivative. In return, the method is quite \ngeneral and capable of di.erentiating functions ex\u00adpressed using arbitrary programming language constructs, \nsuch as conditionals and iteration. Naturally automatic di.erentiation only yields correct re\u00adsults if \nthe function being di.erentiated is continuously dif\u00adferentiable at the point of di.erentiation. If we \nconsider time derivatives of variables in the continuous part of a model, it should be clear that we \nhave a problem in a hybrid setting since each mode switch can introduce arbitrary discontinu\u00adities in \na variable and its derivatives. That is, the variables are only piecewise continuous, and we cannot trust \nthe re\u00adsults of automatic di.erentiation at all points in time. However, the theory of Dirac impulses \nwas developed pre\u00adcisely to deal with problems associated with discontinuities. Speci.cally, the unit \nimpulse can be seen as the derivative of the unit step function. Thus, could we not again turn to Dirac \nimpulses to remedy the situation? In this paper, we will show that this indeed is possible. Moreover, \nwe will show that a lazy functional language like Haskell allows a rather elegant and general solution. \nThe de\u00advelopment is carried out in the context of Yampa [13, 8], a recent member in the family of languages \nbased on the ideas of Functional Reactive Programming (FRP). FRP, in vari\u00adous incarnations, has been \napplied to a number of domains, such as graphical animation [4], graphical user interfaces [3], robotics \n[17, 16], and computer vision [18]. Recently, we in the Yale Haskell group have been interested in apply\u00ading \nFRP-like principles to hybrid modeling and simulation of physical systems, where our ultimate goal is \na system in which models can be expressed through non-directed equa\u00adtions, so called non-causal (or object \noriented 1) modeling [5, 1, 14]. The work presented here is a step in that direction, even if Yampa only \nallows causal modeling. The rest of the paper is organized as follows. Sections 2 and 3 review the basic \nideas behind automatic di.eren\u00adtiation, in particular Jerzy Karczmarczuk s elegant version for a lazy \nfunctional language with overloading [10], and the fundamental concepts of Yampa. Piecewise continuous, \ntime-varying variables are known as signals in Yampa, and as a .rst step in the actual development, section \n4 spe\u00adcializes Karczmarczuk s method to computing time deriva\u00adtives of signals and considers the interplay \nbetween this and Yampa s facilities for numerical integration. Sections 5 and 6 then pave the way for \na correct treatment of discontinuities in signals by moving to generalized signals, where a signal no \nlonger is regarded as a function of time, but as a gen\u00aderalized function or distribution, such as a Dirac \nimpulse. 1 Not to be confused with object-oriented programming lan\u00adguages. Concepts like classes and \ninheritance may be part of an object-oriented modeling language, but methods and imperative variables \nare not. The automatic di.erentiation machinery is then adapted to work on generalized signals. The .nal \nstep in the develop\u00adment is taken in section 7, which considers the integration of generalized signals \ninto Yampa. The end result is a uni.ed framework in which impulses are available for the bene.t of the \nmodeler, and in which generalized automatic di.erentia\u00adtion guarantees correct results for time derivatives \nof signals at all points. 2. AUTOMATIC DIFFERENTIATION Automatic di.erentiation [2] is the technique \nof choice for computing derivatives in many application areas. Its advan\u00adtages include the ability to \ndi.erentiate arbitrary program code (as long as the code implements di.erentiable func\u00adtions), exact \nresults within the limitations of .oating point arithmetic, and good performance. In contrast, symbolic \ndif\u00adferentiation has much more limited applicability, and when applicable often yields unwieldy results \nand thus bad per\u00adformance. Numeric di.erentiation is fraught with problems stemming from the fact that \nit does not yield exact results. Automatic di.erentiation is a purely algebraic method, and the key idea \nis to augment every computation in a code frag\u00adment so that derivatives with respect to a chosen variable \nare computed along with the main result. This is also the main drawback of the method: unlike symbolic \ndi.erentia\u00adtion, the end result is not a separate, self-contained expres\u00adsion for the derivative that \ncan be used to compute the value of the derivative at arbitrary points. Instead, the value of the derivative \nis obtained in conjunction with the value of the function at whichever point the latter is evaluated. \nThe following example illustrates the basic idea. Consider the code fragment z1 = x + y z2 = x * z1 \nIf we assume that the code fragment that assigned some values to x and y has been augmented so that the \nderiva\u00adtives of these variables with respect to some common chosen variable of di.erentiation is available \nin the variables x and y respectively, then the above code fragment can be aug\u00admented to compute the \nderivatives of z1 and z2 with respect to that same variable of di.erentiation as follows: z1 =x+y z1 \n= x + y z2 =x*z1 z2 = x * z1 + x * z1 Methods for automatic di.erentiation can be quite a bit more involved, \nespecially for multi-variate cases, but the ba\u00adsic approach outlined above is enough for our purposes. \nThe next question is how to actually go about augment\u00ading a program. One possibility is to employ source-to-source \ntransformations. Another is to overload arithmetic opera\u00adtors and functions so that they compute derivatives \nalong with the main result. Jerzy Karczmarczuk has described a particularly elegant formulation for a \nlazy functional lan\u00adguage with overloading, where lazy evaluation is exploited to, conceptually, compute \nderivatives of all orders, not just the .rst one [10]. That is the approach we are going to use. First, \nwe need a numeric domain for our di.erential alge\u00adbra. Elements in this domain represent values of continuous \nfunctions at some point along with derivatives of all orders at that same point: dataC = C DoubleC valC(Ca_) \n=a derC(C_ x )=x Note that the name C (suggesting continuous) is used both for the type itself and for \nthe value constructor. Elements of type C are pairs, where the .rst .eld, here of type Double2 , represents \nthe value of some function, and the second, re\u00adcursively of type C, represents the .rst derivative. But \nthat means the derivative can be di.erentiated yielding the sec\u00adond derivative, again of type C, and \nso on. Thus the repre\u00adsentation includes all derivatives, to be computed lazily. The value of the constant \nzero is 0 everywhere, and so is its derivative. The representation of zero in C is thus: zeroC :: C zeroC \n= C 0.0 zeroC zeroC can be used to de.ne a function that computes the representation of arbitrary constants, \nwhich in turn de.nes the meaning in C of Haskell s overloaded numerical literals: constC :: Double -> \nC constC a = C a zeroC The derivative of the variable of di.erentiation with respect to itself is always \n1, and the representation of its value in C at some arbitrary point is thus given by: dVarC :: Double \n-> C dVarC a = C a (constC 1.0) The de.nitions of arithmetic operators are straightforward: instance \nNum C where (C a x )+ (C b y )= C (a + b)(x + y ) x@(Ca x )* y@(Cb y )= C (a*b)(x *y+x*y ) Note that \nthese de.nitions are recursive since the overloaded operators + and * are used at type C for computing \nthe derivative of the result. Further operators and mathematical functions like sin and cos can be de.ned \nwith equal ease. As an illustration, suppose we have y = t2 + k and that we want to compute y, y., and \ny\u00a8for t = 2 and k = 1. This can be done by simply transliterating these equations into Haskell: k = constC \n1 t = dVarC 2 y=t*t+k We now have: valC y = 5 valC (derC y) = 4 valC (derC (derC y)) = 2 3. YAMPA This \nsection gives a short introduction to Yampa, a lan\u00adguage embedded in Haskell for describing reactive, \nhybrid systems [13, 8]. A fundamental notion in Yampa is that of signals. A signal is, conceptually, \na function of time, or, 2 In a more thorough implementation, C would be parame\u00adterized w.r.t the numerical \ncarrier type. Figure 2: System of interconnected signal functions with varying structure equivalently, \na time-varying value (sometimes called .uent). Thus, intuitively, for some suitable type Time representing \ncontinuous time: Signal a . Time a However, signals are not .rst class entities in Yampa: they only \nexist indirectly through the notion of signal functions introduced below. Moreover, an executable Yampa \nimple\u00admentation can only approximate this conceptual signal model since continuous-time signals necessarily \nare evaluated for only a discrete set of sample points. Conceptually, the domain of a signal can either \nbe contin\u00aduous or discrete. In the former case, the signal is de.ned at every point in time. In the latter \ncase, the signal is a partial function, only de.ned at discrete points in time associated with the occurrence \nof some event. In Yampa, we have cho\u00adsen to blur this distinction. The notion of discrete-time sig\u00adnals \nis captured by lifting the range of of continuous-time signals using an option type called Event. This \ntype has two constructors: NoEvent, representing the absence of a value; and Event, representing the \npresence of a value: data Event a = NoEvent | Event a A discrete-time signal carrying elements of type \nA can thus be thought of as a function of type Signal (Event A). The next important Yampa notion is that \nof signal func\u00adtions. A signal function is a pure function that maps an in\u00adput signal onto a output signal. \nBy changing the perspective slightly, a signal function can also be seen as a time-indexed instantaneous \nmapping from signal values to signal values. If the mapping in fact is time-invariant, the signal function \nis said to be stateless, otherwise it is said to be stateful. Unlike signals, signal functions are .rst \nclass entities in Yampa. The type of a signal function mapping a signal of type a onto a signal of type \n( is written SF a(. Intuitively, we have SF a( . Signal a Signal ( If more than one input or output \nsignal are needed, tuples are used for a or ( since a continuous signal of tuples is isomorphic to a \ntuple of continuous signals. A Yampa system consists of a number of interconnected signal functions, \noperating on the system input and pro\u00adducing the system output. The signal functions operate in parallel, \nsensing a common rate of time .ow. The structure of a Yampa system may evolve over time. For example, \nsignal functions can be added or deleted; see .gure 2. These structural changes are known as mode switches. \nThe .rst class status of signal functions in combination with powerful switching constructs make Yampa \nan unusually .exible language for describing hybrid systems [13]. Yampa s signal functions are an instance \nof the arrow framework proposed by Hughes [9]. Two central combina\u00adtors from that framework are arr, \nwhich lifts an ordinary function to a stateless signal function, and <<<, which com\u00adposes two signal \nfunctions: arr ::(a->b)->SFab (<<<):: SF b c ->SF a b -> SFa c Yampa also provides a combination of \nthe two, the arrow\u00adcompose combinator: (^<<):: (b ->c) ->SF a b -> SFa c Through the use of these and \nrelated plumbing combinators, arbitrary signal function networks can be expressed. Paterson s syntactic \nsugar for arrows [15] e.ectively al\u00adlows signals to be named, despite signals not being .rst class values. \nThis eliminates a substantial amount of plumbing, resulting in much more legible code. In fact, the plumbing \ncombinators will rarely be used in the examples in this pa\u00adper. In this syntax, an expression denoting \na signal function has the form: proc pat -> do [ rec ] pat1 <-sfexp1 -< exp1 pat2 <-sfexp2 -< exp2 ... \npat<-sfexp-< exp n nn returnA -< exp The keyword proc is analogous to the A in A-expressions, pat and \npati are patterns binding signal variables pointwise by matching on instantaneous signal values, exp \nand expi are expressions de.ning instantaneous signal values, and sfexpi are expressions denoting signal \nfunctions. The idea is that the signal being de.ned pointwise by each expi is fed into the corresponding \nsignal function sfexpi, whose output is bound pointwise in pati. The overall input to the signal function \nde\u00adnoted by the proc-expression is bound by pat, and its output signal is de.ned by the expression exp. \nThe signal variables bound in the patterns may occur in the signal value expres\u00adsions, but not in the \nsignal function expressions (sfexpi). If the optional keyword rec is used, then signal variables may \noccur in expressions that textually precedes the de.nition of the variable, allowing recursive de.nitions \n(feedback loops). Finally, let pat = exp is shorthand for pat <-arr id -< exp allowing easy binding of \ninstantaneous values. To illustrate Yampa and the arrow notation, we provide a bouncing ball model according \nto the speci.cation given in the introduction. The following are the most important Yampa primitives \nused in the model: integral :: SF Double Double edge :: SF Bool (Event ()) switch :: SF a (b, Event c) \n-> (c -> SF a b) ->SF a b The signal function integral integrates its input. edge is a signal function \nthat generates an event whenever the input signal goes from False to True. The signal function switch \nswitches from one subordinate signal function into another when a switching event occurs. Its .rst argument \nis the signal function that initially is active. It outputs a pair of signals. The .rst de.nes the overall \noutput while the initial signal function is active. The second signal carries the event that will cause \nthe switch to take place. Once the switch\u00ading event occurs, switch applies its second argument to the \nvalue tagged to the event and switches into the resulting signal function. The code for the model is \nas follows. It does not use any of the new facilities described in this paper. bouncing :: Position -> \nSF () (Position, Velocity) bouncing y0 = bouncing y0 0.0 where bouncing y0 yd0 = switch (bouncing0 y0 \nyd0) $ \\(y, yd) -> bouncing y (-yd) bouncing0 y0 yd0 = proc () -> do yd <-(yd0 +) ^<< integral -< -9.81 \ny <-(y0 +) ^<< integral -< yd hit<-edge -<y<=0 returnA -< ((y, yd), hit tag (y, yd)) Here, bouncing0 \nrealizes the physics of a falling ball and detects the collision event. The event is tagged with the \nvalues of the state variables, i.e. the height above the .oor and the velocity, at the point of impact, \nenabling the switch to pass the state on to the subsequent mode. In this case, we switch back into the \nsame mode, using the same height (to ensure continuity in position) but negating the velocity to obtain \nthe initial value for the state variables in the new mode. 4. INTEGRATING AUTOMATIC DIFFER-ENTIATION \nINTO YAMPA We now turn to computing time derivatives of signals in Yampa through automatic di.erentiation. \nIn Yampa, signals are only evaluated for the current time, and as signals are not .rst class entities, \nthis is done implicitly. Since we are only interested in time derivatives of signals, there is no thus \nneed for the function dVarC from section 2: as long as signal functions that are sources of time-varying \nsignals construct their output correctly in the domain C, signals can be added, multiplied, or transformed \nin other ways using operations on C, and di.erentiation will just work. Since the signal function integral \nis the main source of continuous time-varying signals, we will focus on that. The output signal y(t) \nobtained by applying integral to an in\u00adput signal x(t) is de.ned by:3 t y(t)=x(p )dp (1) 0 According \nto the fundamental theorem of calculus, the deriva\u00adtive of y(t) is simply x(t). Thus, all we need to \ndo to achieve our goal, is to de.ne a version integralC of integral that works on signals of type C and \nthat ensures that the deriva\u00adtive of the output signal is equal to the input signal. 3 Time is local \ntime, measured from the time at which a signal function is switched in, i.e., applied to its input signal. \n Non-broken mode: x = l sin(\u00a2) y = -l cos(\u00a2) ml2 \u00a2\u00a8+ mgl sin(\u00a2)= u . Initial conditions: \u00a20 = 1/4, \u00a20 \n=0 Broken mode: mx\u00a8=0 my\u00a8 = -mg Initial cond.: x, y, x., y.continuous. Figure 3: Breaking pendulum Like \nin other simulation systems or synchronous data .ow languages, signals in Yampa are represented by streams \nof instantaneous signal values and signal functions are state\u00adful stream processors. A signal function \nis implemented as a function that maps the time di.erence since the previ\u00adous sample time and the current \ninstantaneous input value to a pair of a new signal function (a kind of continuation, carrying any state) \nand an instantaneous output value:4 dataSFa b=SF(DTime->a->(SFa b,b)) The signal function integralC is \nde.ned as follows: integralC :: SF C C integralC = SF tf0 where q0 =0.0 tf0 _ x0 = (igrlAuxC q0 x0, C \nq0 x0) igrlAuxC q_prev x_prev = SF tf where tf dt x = (igrlAuxC q x, C q x) where q = q_prev + dt * valC \nx_prev Note that the output value, e.g. Cqx after the .rst step, is de.ned so that the instantaneous \nvalue of the derivative x is the instantaneous input value, as required by the fun\u00addamental theorem of \ncalculus. Also note that the value part q of the output does not depend on the current input value, only \non the previous one. This is crucial to make recur\u00adsive equations involving integrals well-de.ned. For \nillustra\u00adtive purposes, the employed integration method is simple Euler integration. A more sophisticated \napproach such as a Runge-Kutta method could be used. Since we have access to derivatives of arbitrary \norder, another possibility would be to use Taylor methods [11]. However, it is unclear if variable step-size \nmethods could be used in the Yampa setting. To illustrate automatic di.erentiation in the context of \nYampa, consider modeling the pendulum in .gure 3. It con\u00adsists of a mass m attached to the end of a sti., \nmassless rod l, .xed to some supporting structure at the other end, where an external torque u also can \nbe applied. The angle \u00df gives the deviation of the rod from the plumb line. At some time the rod could \nbreak causing the mass to fall freely. Thus the system has two modes, described by the equations given \nin the .gure along with initial conditions. 4 This is a simpli.ed account; see [13] for details. The \nmost interesting aspect of the model for our purposes is that the state of the system is best described \nby di.er\u00adent variables in the two modes: in the non-broken case by . the angle of deviation \u00df and its \nderivative \u00df; in the broken mode by the position and velocity of the mass in Cartesian co-ordinates. \nHowever, in a simulation, we may be inter\u00adested in the latter information regardless of the mode, e.g. \nto ensure continuity when the pendulum breaks. Comput\u00ading the Cartesian position given the angle of deviation \nis easy enough using basic trigonometry, whereas computing the velocity is most naturally achieved by \nsimply di.erenti\u00adating the position. The automatic di.erentiation machinery allows the user to do the \nlatter directly. Without it, the user would either have had to calculate the derivatives symbol\u00adically \nby hand, or he would have had to reformulate the model to use Cartesian state also in the non-broken \nmode. To develop a Yampa model for the non-broken mode, we .rst have to determine the state variables \nand the causality, or data .ow direction, and rework the model equations into causal (directed) form, \nwhere the state variables are com\u00ad . puted by integration. As was noted above, \u00df and \u00df are our state \nvariables in this mode. If we consider the dot notation more as a way to name variables rather than a \ndi.erentiation operator, we obtain: \u00df = . + .\u00df dt x = l sin(\u00df) 4 y = -l cos(\u00df) .\u00df = \u00a8 \u00df dt .x = d x dt \n\u00a8 \u00df = u - mgl sin(\u00df) ml2 .y = d dt y We can now obtain a Yampa model for the non-broken mode by transliterating \nthese equations into Haskell, em\u00adploying the syntactic sugar for arrows. Despite some syntac\u00adtic noise, \nthe one-to-one correspondence should be obvious. nonBroken :: SF Torque (Position2, Velocity2) nonBroken \n= proc u -> do rec phi <-(pi/4 +) ^<< integralC -< phid phid <-integralC -< phidd let phidd = (u -m*g*l*sin \nphi) / m*l*l x =l*sinphi y =-l*cosphi xd =derCx yd =derCy returnA -< ((x, y), (xd, yd)) Figure 4 shows \nthe simulation results with no externally applied torque and the pendulum breaking after 9 s. 5. DISTRIBUTION \nTHEORY There are many functions that do not have derivatives in the classical sense. For example, consider \nthe unit step function (or Heaviside step function) H(t): 0 if t< 0 H(t)= {(2) 1 if t ~ 0 The derivative \nof H(t) in the usual sense simply is not well\u00adde.ned at t = 0.  0123456 78910 time Figure 4: Simulation \nof the breaking pendulum Nevertheless, if we consider a sequence fn(t) of ever nar\u00adrower and taller impulses \nwhose area is exactly 1, for exam\u00adple like in .gure 5(a) with p = 0, it should be clear that the sequence \nof functions Hn(t) de.ned by t Hn(t)= fn(p )dp (3) - will become better and better approximations of \nH(t) as n .. Thus it is, according to the fundamental theorem of calculus, intuitively appealing to think \nof the limit of a function sequence like fn(t) as the derivative of the unit step function. This limit \nis called the (Dirac) delta function (despite not being a function in the usual sense) or unit impulse, \nand is denoted by a(t). In a similar way, the limit of a sequence of functions like those shown in .gure \n5(b) is an intuitive way to understand the derivative a'(t) of the unit impulse. When drawing diagrams, \nit is conventional to represent impulses with an arrow, as in .gure 5(a). As impulses can be scaled, \nthe height of the arrow represents the area or strength of the impulse. Similarly, as shown in .gure \n5(b), we will draw the impulse derivative using a double-headed arrow, whose height corresponds to the \nstrength of the dif\u00adferentiated impulse. The second impulse derivative will be drawn as a triple-headed \narrow. By analyzing the limits of operations on sequences of func\u00adtions that tend to the unit impulse, \nproperties that should (a) Dirac impulse (b) Impulse derivative Figure 5: Dirac impulse a(t - p ) and \nits derivative a ' (t - p ) as the limits of function sequences hold for reasons of consistency emerge. \nFor example: b 1 if0 . (a, b) a(t)dt = {(4) a 0 if0 =. [a, b] More generally, given any function \u00df(t) \ncontinuous at the origin, a central property is that a(t)\u00df(t)dt = \u00df(0) (5) - The theory of distributions \nor generalized functions [19, 6] is a rigorous formalization of the ideas outlined above. The key step \nis to consider functions that map a certain class of functions to numbers (i.e., a kind of higher-order \nfunctions), instead of functions that map numbers to numbers. This allows a consistent theory to be developed \nin which every distribution has a derivative that is also a distribution. Formally, a distribution is \na linear and continuous func\u00adtional. A functional is a function that maps so called test functions, in.nitely \ndi.erentiable functions that vanish out\u00adside a .nite interval, to numbers. The application of a func\u00adtional \nT to a test function \u00df is conventionally written =T,\u00df8 or sometimes =T (t),\u00df(t)8. Two functionals S and \nT are de\u00ad.ned to be equal if =S, \u00df8 = =T,\u00df8 (6) for every test function \u00df. A functional T is linear if \n=T, c1\u00df1 + c2\u00df2 8 = c1 =T,\u00df1 8 + c2 =T,\u00df2 8 (7) Continuity is analogous to the usual notion of continuity, \nbut in terms of limits of sequences of test functions instead of limits of sequences of numbers. Every \nlocally integrable function f induces a distribution Tf through the following de.nition: =Tf ,\u00df8 = f \n(t)\u00df(t)dt (8) - Such distributions are said to be regular. Now, the unit im\u00adpulse is not a function in \nthe usual sense, so (8) is not ap\u00adplicable. But (5) shows what to expect in the limit for a sequence \nof functions tending to the impulse. Thus, for rea\u00adsons of consistency, the delta distribution is de.ned \nas =a, \u00df8 = \u00df(0) (9) Di.erentiation is de.ned through =T ' ,\u00df8 = -=T,\u00df ' 8 (10) Thus, in particular, \nwe have that the impulse derivative is =a ' ,\u00df8 = -=a, \u00df ' 8 = -\u00df ' (0) (11) or in general =a(k)\u00df(k) \n,\u00df8 =(-1)k=a, \u00df(k) 8 =(-1)k(0) (12) Shifting is de.ned by =T (t - p ),\u00df(t)8 = =T (t),\u00df(t + p )8 (13) \nUnfortunately, the theory of distributions does not gener\u00adalize the theory of classical functions in \nevery respect. For example, it is not possible to de.ne the product of two dis\u00adtributions in general. \nHowever, it is possible to de.ne mul\u00adtiplication with a C (in.nitely di.erentiable) function: =gT, \u00df8 \n= =T, g\u00df8 (14) provided g . C .  6. GENERALIZED SIGNALS In this section, we will see how the theory \nof distributions allows us to apply automatic di.erentiation to signals in a hybrid simulation setting. \nNote that we are not trying to develop a generalized di.erential algebra independently of that setting. \nThe present development focus on the easier problem where discontinuities ultimately stem from mode switching \nor explicit impulses.5 Let us .rst see how the theory of distributions helps us di.erentiate piecewise \ncontinuous functions. 2 tif t< 1 f (t)= {-(2 - t)2 if t ~ 1 ' 2t if t< 1 f (t)= {- 2a(t - 1) 4 - 2t if \nt ~ 1 '' 2 if t< 1 f (t)= {- 2a ' (t - 1) -2 if t ~ 1 ''' '' f (t)= -4a(t - 1) - 2a (t - 1) Figure 6 \nshows these (generalized) functions graphically. Note how the derivatives are described by the sum of, \non the one hand, a classical function, and, on the other, impulses (and impulse derivatives) to account \nfor discontinuities (and impulses) in the di.erentiated function. Turning to our original problem, di.erentiation \nof piece\u00adwise continuous signals in Yampa (and, in turn, their deriva\u00adtives), it should now be clear \nthat we can address this if we conceptually regard signals not as functions of time, but as generalized \nfunctions of time of the following form: mn . S(t)= s(t)+ aij a(i) (t - pj ) (15) i=0 j=1 5 As suggested \nby one of the anonymous reviewers, it may be possible to develop such a generalized di.erential algebra. \nThat would allow functions like abs that introduce disconti\u00adnuities to be handled properly. Currently \nfunctions like abs should not be used. Substitutes, de.ned in terms of switch\u00ading, are provided, but \nthose substitutes are consequently stateful signal functions as opposed to pure functions in a di.erential \nalgebra.  Figure 6: Di.erentiation of generalized functions where s(t) is a regular signal (piecewise \ncontinuous function) and m is the order of the highest impulse derivative. We call this a generalized \nsignal. pj , j . [1,n] are the points in time at which the generalized signal or its derivatives have \ndiscontinuities. At the implementation level, what we need to represent is samples of generalized signals. \nLet . be the set of points of discontinuity: .= { pj | j . [1,n] } (16) Let St be the representation \nof a sample of a generalized signal S as de.ned by (15) at time t. For sample times t/. . we have St \n= s(t) (17) For t = pj . . we choose to represent the sample by a pair of the left limit of the regular \npart s at t, together with a list of the strengths of those impulses and impulse derivatives that are \nnon-zero at t, where the nth list element represents the strength of the impulse derivative of order \nn: St =(s(t-), [a0j ,a1j ,...,amj ]) (18) What about the right limit and representing discontinuities? \nWe will return to that question shortly. Underlying this representation is an assumption that . includes \na point for every theoretical point of discontinu\u00adity, and that this point is su.ciently close to its \ntheoretical value. Ensuring this is a hard but separate problem that has received widespread attention \nin the hybrid modeling and simulation literature. For automatic di.erentiation, as described earlier, \nthe rep\u00adresentation of a sample must include the value of the signal as well as the values of the derivatives \nof the signal at the sample point. We choose to keep the regular part and the impulse part of a sample \nseparate, allowing us to reuse the the type C introduced in section 2, and giving us an easy way to test \nfor the absence of impulses.The following type G represents a sample of a generalized signal: dataG = \nG C I dataC = C DoubleC dataI = NI | I [Double] I C is exactly as before; the C-part of G represents \nthe value or left limit of the regular part s and its derivatives according to (17) or (18). Since we \nassume that . includes all points of discontinuity, these derivatives are well-de.ned. The I-part accounts \nfor the impulses and impulse derivatives. It is either NI for no impulse in case (17) applies, or it \nis a .nite list of impulse strengths as de.ned in (18) together with a value of type I representing the \nimpulse part of the derivative of the entire generalized signal at the sample point. It turns out to \nbe useful to maintain the invariant that the list of impulse strengths never end with zero. Unlike C, \nI should not be considered to have a meaning separate from G. This is because di.erentiation of the regular \npart can require further impulses to be added to the impulse part. As an added bene.t of explicitly including \nthe value of the derivative in the representation, we get a way to repre\u00adsent points of discontinuity; \ni.e. points where the right limit is di.erent from the left limit. We exploit the fact that a discontinuity \ngives rise to an impulse in the derivative of a signal, and take such an impulse to be the representation \nof a discontinuity. The right limit can thus be computed from the left limit by adding the strength of \nany impulse in the derivative. Two basic selectors for I are strengthI and derI: strengthI :: I -> Double \nstrengthI NI = zeroStrength strengthI (I [] _) = zeroStrength strengthI (I (a:_) _) = a derI::I ->I derINI \n=NI derI(I_ i )=i Note that derI yields the impulse part of the derivative of the generalized signal \nsample of which a particular entity of type I is part. stepI and impulseI computes the impulse parts \nfor a step and an impulse at the point of discontinuity: stepI :: Double -> I stepI a | isZeroStrength \na = NI | otherwise = I [] (impulseI a) impulseI :: Double -> I impulseI a | isZeroStrength a = NI | \notherwise = impAux [a] where impAux as = I as (impAux (zeroStrength:as)) Addition of impulse parts is \nstraightforward since distri\u00adbutions are linear: addI::I ->I->I addIi NI=i addINIj =j addI(Ias i )(I \nbsj ) = I (addI as bs) (addI i j ) where addI as [] = as addI [] bs = bs addI (a:as) (b:bs) = consStrengths \n(a + b) (addI as bs) consStrengths is a list constructor that maintains the in\u00advariant that lists of \nimpulse strengths should not end in zero (recall that the list of impluse strengths is .nite). discI \ncomputes the impulse part at a point of disconti\u00adnuity given the left and right limits: discI:: C -> \nC ->I discI(C a x )(C b y )= stepI (b -a) addI (I [] (discI x y )) The product of a C function and an \nimpulse or im\u00adpulse derivative is considerably more interesting. At .rst, one might think that multiplication \njust amounts to a point\u00adwise scaling. But a closer study of equations (12) and (14) reveals that that \nis only true if the C function is a con\u00adstant; i.e., when its derivatives are identically zero. Let us \nstudy the product of a function f (t) E C and a(n) (t - p ). Our goal is to .nd the strengths of the \nresulting impulses and impulse derivatives at point p , enabling us to construct a correct representation \nof the sample of the product signal at that point. The following derivation is just a straightforward \napplication of equations (7) to (14): =f (t)a(n) (t - p ),\u00df(t)8 =(-1)n[f (p )\u00df(p )](n) ( n n f (k) (p \n)\u00df(n-k) =(-1)n (p ) k) k=0 ( n n f (k) =a(n-k) =(-1)n (p )(-1)(k-n)(t - p ),\u00df(t)8 k) k=0 ( n n k (k)(n-k) \n=(-1)f (p )=a(t - p ),\u00df(t)8 (19) k) k=0 Thus, multiplying a C function f by an impulse deriva\u00adtive of \norder n shifted by p , results in a sum of impulse derivatives of order 0 to n, all shifted by the same \namount, where the strength for the impulse derivative of order n - k n is (-1)kk f (k) (p ). There is \na technical problem. The derivation above as\u00adsumes that f is a C function; i.e., neither f nor its deriva\u00adtives \nhave any discontinuities. In our implementation, the role of f is played by some signal, and since the \nimplemen\u00adtation is sampled, that implies that we only know whether a signal is C or not in the immediate \nneighborhood of the current sample point. Everywhere else, the signal could be arbitrarily ill behaved: \nthere is no recollection of the past, and the future is unknown. However, intuitively, since only the \nbehavior of a C function at the point of the impulse is relevant for the result, and since an impulse \nis identically zero everywhere else, the behavior of the signal elsewhere should not matter. But we have \nnot yet proved this formally. We now know how to obtain the impulse strengths for the product of a C \nfunction and arbitrary impulse derivatives. Thanks to linearity, we can thus multiply the impulse part \nof a generalized signal with a C function. But in our setting, we also need to construct the impulse \npart of the derivative of the product, and we would like to do that using only the impulse part of the \ngeneralized signal. Let us denote the regular part of a generalized signal S by . S and the impulse part \nby r S. Thus S = . S + r S (20) It is easy to show that the derivative of a product of a func\u00adtion f \nE C and a distribution S can be rewritten in the expected way: [f (t)S(t)] ' = f (t)S ' (t)+ f ' (t)S(t) \n(21) The impulse part of the derivative of the product can then be obtained as follows: r [f (t)S(t)] \n' = r [f (t)S ' (t)+ f ' (t)S(t)] = r [f (t)S ' (t)] + r [f ' (t)S(t)] '' ' = r [f (t)(. S (t)+ r S (t))] \n+ r [f (t)(. S(t)+ r S(t))] = r [f (t) . S ' (t)] + r [f (t) r S ' (t)] no impulses +r [f ' (t) . S(t)] \n+ r [f ' (t) r S(t)] no impulses = r [ f (t) r S ' (t)]+ r [ f ' (t) r S(t)] only impulses only impulses \n= f (t) r S ' (t)+ f ' (t) r S(t) (22) Now we are at long last in a position to de.ne the product of \na sample of a C signal and the impulse part of a sample of a generalized signal. mulCI:: C -> I ->I mulCI_ \nNI = NI mulCI x@(C _ x ) i@(I as i ) = I (mciAux x as) (addI (mulCI x i ) (mulCI x i)) where mciAux x \nas = loop1 pascal as where loop1_ []= [] loop1 (ps:pss) aas@(a:as) = consStrengths (loop2 1 ps x aas) \n(loop1 pss as) loop2___[]= 0 loop2 sign (p:ps) (C b x ) (a:as) = sign * fromIntegral p * a * b + loop2 \n(negate sign) ps x as pascal :: [[Int]] pascal = (repeat 1) : map (scanl1 (+)) pascal mciAux computes \nthe impulse part of the value of the prod\u00aduct according to (19), whereas the impulse part of the deriva\u00adtive \nof the product is computed by invoking mulCI recur\u00adsively according to (22). The summation ordering in \nmciAux has been optimized for reasons of e.ciency. pascal is a ver\u00adsion of Pascal s triangle that provides \nbinomial coe.cients in an order suitable for mciAux. The following equality holds: ( m + n pascal !! \nm !! n = (23) n ) Let us .nally de.ne some operations on G. leftLimit and rightLimit are the left and \nright limits of a signal at the current point. The left limit is simply a projection by (17) and (18). \nThe right limit is the same as the left limit if there are no impulses at the point in question. Otherwise \nit is computed from the left limit by adding the strength of any impulse in the .rst derivative, as explained \nbefore. leftLimit :: G -> C leftLimit (G x _) = x rightLimit :: G -> C rightLimit (G x NI) = x rightLimit \n(G (C a x ) (I _ i )) = C (a + strengthI i ) (rightLimit (G x i )) We want to allow the user to explicitly \nintroduce impulses. Since events and impulses share the property that they both occur at a speci.c point \nin time, and since Yampa has a rich sub-language for events, it seems natural to do this through a function \nmapping events to impulses. We chose to let the value tagged to the event specify the strength of the \nresulting impulse. impulse :: Event C -> G impulsee= G0.0i where i = casee of NoEvent -> NI (Event x) \n-> impulseI (valC x) De.ning arithmetic operators like + and * on G is straight\u00adforward. The corresponding \noperators on C are used for the regular part and operations such as addI and mulCI for the impulse part. \nFor multiplication, four cases are distin\u00adguished. The .rst two cover the cases where at least one of \nthe factors and all its derivatives are free from impulses at the point in question. The third concerns \nthe case where two regular signals that do not have regular derivatives are multiplied. This is de.ned \nby multiplying left and right lim\u00adits separately, and using discI to compute an impulse part that accounts \nfor any resulting discontinuities. The last case is an error case. However, it would be possible to generalize \nmultiplication a bit further. For example, it is possible to de.ne the product of a coinciding step and \nimpulse. instance Num G where (Gxi)+ (Gyj)=G(x+y)(i addI j) (Gx NI)*(Gyj) =G(x*y)(x mulCI j) (Gx i) *(GyNI)=G(x*y)(y \nmulCI i) u * v | isRegular u &#38;&#38; isRegular v = G lluv (discI lluv rluv) | otherwise = error \"Illegal \nmult.\" where lluv = leftLimit u * leftLimit v rluv = rightLimit u * rightLimit v  7. INTEGRATING GENERALIZED \nSIGNALS INTO YAMPA In this section, we will see how to .t generalized signals and the associated generalized \nautomatic di.erentiation ma\u00adchinery into Yampa. Again, integration is one of the fun\u00addamental capabilities \nthat needs to be adapted. The main novelty compared with the development in section 4 is in\u00adtegration \nof impulses and impulse derivatives. As the anti\u00adderivative of an impulse is a step, integration across \nan im\u00adpulse should cause a jump in the value of the integral, the size of which is given by the strength \nof the impulse. The sig\u00adnal function integralG integrates generalized signals (com\u00adpare the implementation \nof integralC in section 4): integralG :: SF G G integralG = SF tf0 where q0 = 0.0 tf0 _ ~u@(G x0 i0) \n= (igrlAuxG (q0 + strengthI i0) (rightLimit u), G (C q0 x0) (integrateI i0)) igrlAuxG q_prev x_prev = \nSF tf where tf dt ~u@(Gx i) = (igrlAuxG (q + strengthI i) (rightLimit u), G (C q x) (integrateI i)) where \nq = q_prev + dt * valC x_prev integrateI :: I -> I integrateI NI = NI integrateI i@(I [] _) = I [] i \nintegrateI i@(I (_:as) _) = I as i The (recursive) calls to igrlAuxG are fairly straightfor\u00adward. For \nexample, consider the code fragment igrlAuxG (q + strengthI i) (rightLimit u) The strength of any impulse \non the input is added to the internal integral state causing the desired jump (strengthI yields 0 if \nthere is no impulse). Note that the right limit of the input is used for passing on what will be the \nprevious value of the regular part of the input at the next step. The construction of the output value \nis more subtle. For example, consider the fragment G (C q x) (integrateI i) If there is no impulse, the \nauxiliary function integrateI yields NI, and the value of the output signal and its deriva\u00adtives at this \npoint is simply given by the regular part Cqx, where q is the value of the integral and x is the regular \npart of the input. This is exactly as for integralC in section 4. If there is an impulse, then Cqx denotes \nthe left limit of the output, and integrateI accounts for the jump by putting the input impulse into \nthe derivative of the impulse part of the output. This is how a step is represented, and the right limit \nwill thus have the correct value. Finally, any im\u00adpulse derivatives in the input has to be integrated \nby putting their anti-derivatives into the output. This is accomplished by simply reducing the order \nof all impulse derivatives by one, which in our representation amounts to a simple tail operation (in \nintegrateI) on the list of impulse strengths. There is one crucial di.erence in the behavior of integralG \ncompared with integralC (and integral). Recall that the output of integralC at any point in time only \ndepends on inputs at earlier points in time. Recursive equations like the following (in arrow notation) \nare thus well de.ned: x <- integralC -< x + 1 (In this particular case, the result is an exponentially \ngrow\u00ading signal as one would expect.) In the same way, the left limit of the output of integralG has \ncarefully been de.ned to only depend on earlier input values. In contrast, the impulse part of the output, \nand thus also the right limit of the output, by necessity depends on the impulse part of the input at \nthe same point in time. Thus integralG cannot be used in a recursive equation like the one above: the \nresult would be bottom. However, the ability to write recursive de.nitions is ab\u00adsolutely critical, especially \nin a modeling context since any interesting system of di.erential equations gives rise to a set of recursive \nde.nitions involving integrals. Our solution is to appeal to modeling knowledge in order to break the \nbad recursion. Certain variables (signals) in a model are usually required to be continuous or at least \nfree from impulses be\u00adcause of the underlying physical reality. For example, the trajectories of physical \nbodies moving in space are typically required to be continuous. If such knowledge is stated ex\u00adplicitly, \nthis can be exploited to make a signal independent of its impulse part since it is then known that there \nare no impulses. To catch modeling mistakes, it should be checked that there in fact are no impulses, \nbut if this check is delayed until the following time step, everything will work out .ne. The following \nsignal function allows the user to assert that a signal is free from impulses, and uses this knowledge \nto make the output independent of the impulse part of the input at the current time step: assertNoImpulseG \n:: SF G G assertNoImpulseG = SF tf0 where tf0_ (G x0 i0)= (aniAux i0, G x0 (noImp i0)) aniAux i_prev \n= SF tf where tf _ (G x i)= (aniAux i, seq (checkNoImp i_prev) (G x (noImp i))) --A promise that there \nare no impulses. noImp i = (I [] (derI i)) --Check that we kept the promise. checkNoImp NI = () checkNoImp \n(I [] _) = () checkNoImp _ = error \"assertion failed\" Ideally, assertions like this one should be inserted \nautomati\u00adcally where needed based on declaratively stated continuity assumptions, but that would probably \nrequire a more so\u00adphisticated language implementation strategy than an em\u00adbedding in Haskell, as is currently \nthe case for Yampa. As noted in section 1, switching is what introduces dis\u00adcontinuities in signals in \nthe .rst place (along with explic\u00aditly introduced impulses). The following version of the ba\u00adsic Yampa \nswitching combinator switch (see section 3) en\u00adsures that any discontinuities resulting from switching \nis ac\u00adcounted for by introducing impulses in the overall output: switchG :: SF a (G, Event b) -> (b -> \nSF a G) -> SFa G The arguments are as for switch. The impulse part of the output at the point of switching \nis computed by applying discI to the left and right limit of the output signal at that point; i.e., the \nlast output from the signal function be\u00ading switched out and the .rst output from the signal func\u00adtion \nbeing switched in. How to handle any impulses in the outputs of the subordinate signal functions at the \npoint of switching has not yet been satisfactorily resolved. Currently such impulses are ignored. The \nimplementation details are omitted. Finally, let us look at an example that makes use of some of the \nnew capabilities introduced above. We return to the bouncing ball example from the introduction. By using \nim\u00adpulses we can model the ball very concisely. The variable y represents the height above the .oor, \nand its derivative y.the velocity. The constant y0 is the initial height. We allow ourselves to use the \nnotation a(y . 0) for introducing im\u00adpulses whenever the predicate y . 0 becomes true, and the notation \n.y(t-) for the left limit of .y. y = y0 + y.dt y.= -9.81 + (-2) .y(t-)a(y . 0) dt Thus, whenever the \nball hits the .oor, it will be subjected to a force that instantaneously accelerates it by twice its \ncurrent velocity in the direction opposite to the current ve\u00adlocity. Note that current velocity really \nmeans the velocity immediately prior to impact. That is why the left limit of .y has to be used. Transliterating \nthese equations into Yampa yields: bouncing :: Position -> SF () (Position, Velocity) bouncing y0 = proc \n() -> do rec y <-(y0 +) ^<< integralG -< yd_ni hit<-edge -<y<=0 yd <-integralG -< -9.81 + impulse (hit \ntag (-2*leftLimit yd)) yd_ni <-assertNoImpulseG -< yd returnA -< (y, yd) Figure 1(b) is a plot of the \nsimulation result from this model. Ignoring the distinction between yd and yd ni, the corre\u00adspondence \nbetween the equations for y and .y and the code above is pretty direct. The only major di.erence is the \nuse of assertNoImpulseG. As discussed above, integralG cannot be used in recursive de.nitions without \nasserting that one of the recursively de.ned signals is free from impulses. Here we have chosen to make \nthis assertion for the velocity, yd, yielding the signal yd ni that safely can be used recursively since \nit is known that it is free from impulses. Without impulses, the bouncing ball would have to be modeled \nusing explicit mode switching, for example as shown in section 3. That model has a considerably more \ncompli\u00adcated structure and is arguably less declarative than the impulse-based one.  8. CONCLUSIONS \nThis paper showed how to add support for Dirac impulses to a causal hybrid simulation system, and how \nto integrate this with the technique of automatic di.erentiation, ensuring everywhere correct derivatives \neven for signals that are only piecewise continuous. The development required introduc\u00ading the notion \nof generalized signals, i.e. signals that con\u00adceptually are distributions rather than classical functions, \nand extending arithmetic operations to work pointwise on such signals, computing the value of the result \nas well as its derivatives at each point. Multiplication turned out to be a somewhat tricky case and \nrequired extra care. The paper also demonstrated the utility of impulses and automatic di.erentiation \nfor causal (hybrid) modeling. We expect that these techniques could be even more useful in a non-causal \nsetting, which is our ultimate goal [14], where the techniques would be employed transparently behind \nthe scenes as needed depending on the choice of state variables and the resulting causality. For computationally \ndemanding simulation applications, the present system is really only a proof of concept: lazy evaluation \nin itself would usually be considered too expen\u00adsive, and the somewhat elaborate representation of samples \nof generalized signals does not make it any more e.cient. However, in a compilation-based implementation \nof a sim\u00adulation language, it ought to be possible to infer statically how many derivatives that needs \nto be computed for the various variables, and the extent to which it is necessary to keep track of impulses. \nPerhaps a suitable type system could help. It should then be possible to generate reasonably e.\u00adcient \nsimulation code. Nevertheless, functional programming and lazy evaluation made it possible to implement \nquite sophisticated simulation techniques remarkably concisely. As a consequence, explor\u00ading various \ndesign alternatives was also easy and did not require too much work, something that was very important \nduring the development. 9. ACKNOWLEDGEMENTS The author would like to thank the members of the Yale Haskell \ngroup, in particular John Peterson and Antony Court\u00adney, as well as Valery Trifonov and Ana Bove, who \nall in their own way have contributed to this paper. Thanks also to the anonymous reviewers for very \ndetailed and construc\u00adtive feedback. 10. REFERENCES [1] Fran\u00b8cois E. Cellier. Object-oriented modelling: \nMeans for dealing with system complexity. In Proceedings of the 15th Benelux Meeting on Systems and Control, \nMierlo, The Netherlands, pages 53 64, 1996. [2] George F. Corliss. Automatic di.erentiation bibliography. \nIn Andreas Griewank and George F. Corliss, editors, Automatic Di.erentiation of Algorithms: Theory, Implementation, \nand Application, pages 331 353. SIAM, Philadelphia, Pennsylvania, USA, 1991. Updated on-line version: \nhttp://liinwww.ira.uka.de/bibliography/\u00adMath/auto.diff.html [3] Antony Courtney and Conal Elliott. Genuinely \nfunctional user interfaces. In Proceedings of the 2001 ACM SIGPLAN Haskell Workshop, Firenze, Italy, \nSeptember 2001. [4] Conal Elliott and Paul Hudak. Functional reactive animation. In Proceedings of ICFP \n97: International Conference on Functional Programming, pages 163 173, June 1997. [5] Hilding Elmqvist, \nFran\u00b8cois E. Cellier, and Martin Otter. Object-oriented modeling of hybrid systems. In Proceedings of \nESS 93 European Simulation Symposium, pages xxxi xli, Delft, The Netherlands, 1993. [6] G. Friedlander \nand M. Joshi. Introduction to the theory of distributions. Cambridge University Press, 1998. [7] Thomas \nA. Henzinger. The theory of hybrid automata. In Proceedings of the 11th Annual IEEE Symposium on Logics \nin Computer Science (LICS 1996), pages 278 292, 1996. [8] Paul Hudak, Antony Courtney, Henrik Nilsson, \nand John Peterson. Arrows, robots, and functional reactive programming. In Johan Jeuring and Simon Peyton \nJones, editors, Advanced Functional Programming, 4th International School 2002, volume 2638 of Lecture \nNotes in Computer Science, pages 159 187. Springer-Verlag, 2003. [9] John Hughes. Generalising monads \nto arrows. Science of Computer Programming, 37:67 111, May 2000. [10] Jerzy Karczmarczuk. Functional \ndi.erentiation of computer programs. Higher-Order and Symbolic Computation, 14(1):35 57, March 2001. \n[11] John H. Mathews. Numerical methods for mathematics, science, and engineering. Prentice-Hall, 2nd \nedition edition, 1992. [12] Pieter J. Mosterman. An overview of hybrid simulation phenomena and their \nsupport by simulation packages. In Fritz W. Vaadrager and Jan H. van Schuppen, editors, Hybrid Systems: \nComputation and Control 99, number 1569 in Lecture Notes in Computer Science, pages 165 177, 1999. [13] \nHenrik Nilsson, Antony Courtney, and John Peterson. Functional reactive programming, continued. In Proceedings \nof the 2002 ACM SIGPLAN Haskell Workshop (Haskell 02), pages 51 64, Pittsburgh, Pennsylvania, USA, October \n2002. ACM Press. [14] Henrik Nilsson, John Peterson, and Paul Hudak. Functional hybrid modeling. In Proceedings \nof PADL 03: 5th International Workshop on Practical Aspects of Declarative Languages, volume 2562 of \nLecture Notes in Computer Science, pages 376 390, New Orleans, Lousiana, USA, January 2003. Springer-Verlag. \n[15] Ross Paterson. A new notation for arrows. In Proceedings of the 2001 ACM SIGPLAN International Conference \non Functional Programming, pages 229 240, Firenze, Italy, September 2001. [16] Izzet Pembeci, Henrik \nNilsson, and Greogory Hager. System presentation functional reactive robotics: An exercise in principled \nintegration of domain-speci.c languages. In Principles and Practice of Declarative Programming (PPDP \n02), pages 168 179, Pittsburgh, Pennsylvania, USA, October 2002. [17] John Peterson, Greg Hager, and \nPaul Hudak. A language for declarative robotic programming. In Proceedings of IEEE Conference on Robotics \nand Automation, May 1999. [18] John Peterson, Paul Hudak, Alastair Reid, and Greg Hager. FVision: A declarative \nlanguage for visual tracking. In Proceedings of PADL 01: 3rd International Workshop on Practical Aspects \nof Declarative Languages, pages 304 321, January 2001. [19] Ian Richards and Heekyung Youn. Theory of \ndistributions: a non-technical introduction. Cambridge University Press, 1990.   \n\t\t\t", "proc_id": "944705", "abstract": "Functional Reactive Programming (FRP) is a framework for reactive programming in a functional setting. FRP has been applied to a number of domains, such as graphical animation, graphical user interfaces, robotics, and computer vision. Recently, we have been interested in applying FRP-like principles to hybrid modeling and simulation of physical systems. As a step in that direction, we have extended an existing FRP implementation, <i>Yampa</i>, in two new ways that make it possible to express certain models in a very natural way, and reduces the amount of work needed to put modeling equations into a suitable form for simulation. First, we have added <i>Dirac impulses</i> that allow certain types of discontinuities to be handled in an easy yet rigorous manner. Second, we have adapted <i>automatic differentiation</i> to the setting of Yampa, and <i>generalized</i> it to work correctly with Dirac impulses. This allows derivatives of piecewise continuous signals to be well-defined at all points. This paper reviews the basic ideas behind automatic differentiation, in particular Jerzy Karczmarczuk's elegant version for a lazy functional language with overloading, and then considers the integration with Yampa and the addition of Dirac impulses.", "authors": [{"name": "Henrik Nilsson", "author_profile_id": "81100060854", "affiliation": "Yale University", "person_id": "PP39025384", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/944705.944720", "year": "2003", "article_id": "944720", "conference": "ICFP", "title": "Functional automatic differentiation with dirac impulses", "url": "http://dl.acm.org/citation.cfm?id=944720"}