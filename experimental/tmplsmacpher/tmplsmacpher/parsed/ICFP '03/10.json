{"article_publication_date": "08-25-2003", "fulltext": "\n Iterative-Free Program Analysis Mizuhito Ogawa * Zhenjiang Hu * Isao Sasano mizuhito@jaist.ac.jp hu@mist.i.u-tokyo.ac.jp \nsasano@jaist.ac.jp Japan Advanced Institute of Science and Technology, The University of Tokyo, and *Japan \nScience and Technology Corporation, PRESTO Abstract Program analysis is the heart of modern compilers. \nMost control .ow analyses are reduced to the problem of .nding a .xed point in a certain transition system, \nand such .xed point is commonly computed through an iterative procedure that repeats tracing until convergence. \nThis paper proposes a new method to analyze programs through re\u00adcursive graph traversals instead of iterative \nprocedures, based on the fact that most programs (without spaghetti GOTO) have well\u00adstructured control \n.ow graphs, graphs with bounded tree width. Our main techniques are; an algebraic construction of a control \n.ow graph, called SP Term, which enables control .ow analysis to be de.ned in a natural recursive form, \nand the Optimization The\u00adorem, which enables us to compute optimal solution by dynamic programming. We \nillustrate our method with two examples; dead code detection and register allocation. Different from \nthe traditional standard it\u00aderative solution, our dead code detection is described as a simple combination \nof bottom-up and top-down traversals on SP Term. Register allocation is more interesting, as it further \nrequires opti\u00admality of the result. We show how the Optimization Theorem on SP Terms works to .nd an \noptimal register allocation as a certain dynamic programming. Categories and Subject Descriptors D.1.1 \n[Applicative (Functional) Programming]: Functional Pro\u00adgramming; D.1.2 [Automatic Programming]: Program \nGenera\u00adtion; D.3.3 [Language Constructs and Features]: Programming with Graphs General Terms Algorithms, \nLanguage  Keywords Program Analysis, Control Flow Graph, Register Allocation, Tree Width, SP Term, Dynamic \nProgramming, Catamorphism. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. ICFP 03, August 25 29, 2003, Uppsala, Sweden. Copyright 2003 ACM 1-58113-756-7/03/0008 \n...$5.00 1 Introduction Program analysis is the heart of modern compilers. Most control .ow analyses \nare reduced to the problem of .nding a .xed point in a certain transition system. Ordinary method to \ncompute a .xed point is an iterative procedure that repeats tracing until convergence. Our starting observation \nis that most programs (without spaghetti GOTO) have quite well-structured control .ow graphs. This fact \nis formally characterized in terms of tree width of a graph [25]. Thorup showed that control .ow graphs \nof GOTO-free C programs have tree width at most 6 [33], and recent empirical study shows that control \n.ow graphs of most Java programs have tree width at most 3 (though in general it can be arbitrary large) \n[16]. Once a graph has bounded tree width, we can construct a graph in an algebraic way [3, 4]. This \nsuggests that .nding a .xed point would be computed by recursive traversals on the algebraic struc\u00adture, \nand the optimal solution would be obtained with a dynamic programming. Unfortunately, the existing results \nare not suf.cient for our purpose. For instance, the algebraic construction of graphs with bounded tree \nwidth treats only undirected graphs [3]. This problem can be eas\u00adily coped with, but a more serious problem \nis that it has too many recursive constructors, k(k + 1)(k + 2)/6 for tree width k, which makes it hard \nto write recursive de.nitions over it. This paper proposes a new algebraic construction SP Term of graphs \nwith bounded tree width, and a new method to analyze programs through recursive graph traversals instead \nof iterative procedures, based on the fact that most programs (without spaghetti GOTO) have well-structured \ncontrol .ow graphs. Our main theoretical result (Theorem 2) is that a (directed) graph G can be represented \nby an SP Term in SPk if and only if G has tree width at most k (and has at least k nodes). Note that \nSP Term con\u00adstruction reduces the number of recursive constructors to 2 (regard\u00adless of the size of tree \nwidth k), at the cost of increase of k2 - k + 1 constants. These constants express either diedges from \nthe i-th spe\u00adcial node (called terminal) to the j-th, or a graph with no edges, and they can be treated \nin a uniform way. This makes writing recursive de.nitions on SP terms feasible. We illustrate our methodology \nwith two examples: dead code detec\u00adtion and register allocation. Different from the traditional standard \niterative solution, our dead code detection is described as a simple combination of bottom-up and top-down \ntraversals on an SP Term. Register allocation is more interesting, as it further requires opti\u00admality \nof the result. We solve it as an instance of maximum marking problems [26, 27, 5]; mark the nodes of \na control .ow graph under a certain condition such that the sum of weight of marked nodes is maximum \n(or, minimum). We make use of Optimization Theorem from our previous work [26, 27], and show how it works \nto .nd an optimal register allocation as a certain dynamic programming on SP Terms. The rest of the paper \nis organized as follows. We start by an overview of our basic idea in Section 2, through an example of \ndead code detection on a simple .owchart program without GOTO. Its control .ow graph has tree width \nat most 2, i.e., the class of series-parallel graphs. Section 3 presents an optimal register allocation \nwith the .xed num\u00ad ber of registers for a .owchart program. The core of our technique is Optimization \nTheorem [26, 27], which automatically gives an ef\u00ad.cient solution for maximum marking problems by certain \ngeneric dynamic programming. The advantage and the problem of our method are also brie.y discussed. \nSection 4 introduces the general de.nition of SP Term, and demon\u00adstrates how to extend dead code detection \nto a program that has a control .ow graph with larger tree width. We show that once the reachability \ndescription is given, the description of dead code de\u00adtection will be uniformly extended to larger tree \nwidth. Section 5 discusses related work, and Section 6 concludes the pa\u00adper. Throughout the paper, we \nconsider only intra-procedural con\u00adtrol .ow analyses (0-CFA), and describe algorithms in Haskell-like \nnotations.  2 Dead Code Detection without Iteration In this section, we explain our idea through a \nsimple case study, dead code detection of .owchart programs. This class of graphs corresponds to the \ncontrol .ow graphs of structured (in strict sense) programs, i.e., programs that consist of single-entry \nand single-exit blocks. The syntax of .owchart programs is described below. At the end of the whole program, \nthe end statement is assumed to be added. Prog := x := e assignment | input x input statement | output \nx output statement | Prog; Prog sequence | if e then Prog else Prog . conditional statement | while e \ndo Prog od while loop Our key to the dead code detection without an iterative procedure is the algebraic \nconstruction of control .ow graphs, called SP Term. After translation from a .owchart program to an SP \nTerm, we show how to compute the sets of used and newly de.ned variables in each program fragment by \na single bottom-up traversal over an SP Term, and explain how to compute the set of live variables at \neach terminal in each (sub) SP Term by a single top-down traversal over an SP Term. 2.1 SP Terms for \nControl Flow Graphs of Flowchart Programs Algebraic Construction of Series-Parallel Graphs Control .ow \ngraphs of .owchart programs are graphs with tree width at most 2, which are known as series-parallel \ndirected graphs (digraphs) [32]. Such graphs can be speci.ed in terms of SP Term. Note that the following \nde.nition is somewhat simpli.ed compared to that in Section 4.1 for general cases. DEFINITION 1. An SP \nTerm is a pair of a ground term t and a 1 11 l1 l1 l1 + - (e,(l1,l2)) . 2 Constants (e,(l1,l2)) . l2 \nl2 l2 2 22 S . . 1 . Series composition ; l2 l1 2  l; l; l1 G1G22 l1 2 1 match(l1,l1;)P Parallel \ncomposition 2 match(l2,l2;) G1 G2 P2 G1 G2 + Figure 1. Interpretation of e, e-,2, S, and P. tuple (l1,l2) \nof labels, de.ned as the following.1 + SP2:=(e, (l1,l2)) - | (e, (l1,l2)) | (2, (l1,l2)) | (SSP2 SP2, \n(l1,l2)) | (PSP2 SP2, (l1,l2)) An SP Term is interpreted as a pair of a 2-terminal series-parallel di\u00adgraph \nand a tuple of 2-labels; a 2-terminal digraph is a digraph with a tuple of two nodes, called terminals. \nWe can regard the .rst ter\u00adminal as the single-entry, and the second terminal as the next node of the \nsingle-exit. Labels (l1,l2) are the identi.ers of terminals. Let match(l,l;) be the function that returns \n. . l if l = l; or l; = * l; if l = * . . otherwise (i.e., accept the special label * as a wild card \nduring matching). + The constant (e, (l1,l2)) is interpreted as a diedge from the .rst - terminal to \nthe second terminal, (e, (l1,l2)) as a diedge from the second to the .rst terminal, and 2 as two isolated \nterminals. The series composition S (t1,(l1,l2)) (t2,(l1;,l2;)) fuses the sec\u00adond terminal in t1 and \nthe .rst terminal of t2 if match(l2,l1;)= ., and regard the .rst terminal in t1 as the .rst and the sec\u00adond \nterminal in t2 as the second. The parallel composition P (t1,(l1,l2)) (t2,(l1;,l2;)) fuses each .rst \nand second terminals in t1 and t2 if match(l1,l1; ),match(l2,l2;)= ., and label match(l1,l1;) on the \n.rst terminal and match(l2,l2;) on the second terminal. The interpretation of each function symbol and \nconstant is de\u00adscribed in Fig. 1; a terminal is presented as a double circle, and labels l1,l2 are associated \nto terminals. We prepare the function chT that exchanges the order of the two terminals of a graph. chT \n:: SP2 . SP2 + - chT (e, (l1,l2)) =(e, (l2,l1)) - + chT (e, (l1,l2)) =(e, (l2,l1)) chT (2, (l1,l2)) \n=(2, (l2,l1)) chT (Sxy, (l1,l2)) =(S (chT y)(chT x), (l2,l1)) chT (Pxy, (l1,l2))=(P (chT x)(chT y), (l2,l1)) \n + - 1In Section 4.1, e, e, P2 are denoted by e2(1,2), e2(2,1), and P2 respectively. For readability, \nwe set St1 t2 = S2 (chT t2) t1, where Sk is uniformly de.ned in Section 4.1 for k = 2.    1 1 1 1 \n 1 P S 2 Pe+ chT . .   2 2 2 2 2 . ++ 2 2 e trans p echT (trans p) Figure 2. Translation of \nwhile statement to an SP Term. Translation from Programs to SP Terms We add labels to each statements \nin Prog to identify each node in a control .ow graph. We denote the set of such labeled programs by LProg. \nThe implementation trans of the transformation from a labeled program to an SP Term is given below. trans \n:: LProg .SP2 + trans (l : x := e)=(e,(l,*)) + trans (l : input x)=(e,(l,*)) + trans (l : out put x)=(e,(l,*)) \ntrans (p1; p2)=(S (trans p1)(trans p2), (l,*)) where l is the starting line of p1 trans (l : if e then \np1 else p2 .) + =(P (S (e,(l,*)) (trans p1, (l,*))) + (S (e,(l,*)) (trans p2, (l,*))), (l,*)) trans \n(l : while e do p od) + =(P (e,(l,*)) + (S (P (e,(l,*)) (chT (trans p)(*,l + 1)), (l,l + 1)) (2,(*,*)), \n(l,*)), (l,*)) Program 1: input n; 2: i := 0; 3: S := 0; 4: c := True; 5: while c do 6: i := i + 1; 7: \nc := False; 8: S := S + i; 9: c := i <= n; od; 10 : out put S Figure 3. An example of control .ow graph \nand its transforma\u00adtion to SP Term For instance, the translation of while-statement proceeds as in Fig. \n2. Intuition behind the wild character label * is; for each fragment of a program, the .rst label denotes \nthe entry of the frag\u00adment, and the second label, which is always * during transforma\u00adtion, denotes the \nnext control point. Note that each program frag\u00adment has the unique node labeled with * . At the end, \n*is replaced with the label for the end statement, i.e., the end of the program. CFG SP-term . (1,2) \n. .  . S (5,10) . 2 (6,10) . S (5,6) - (7,6) - (8,7) . e . -- (9,8) (5,9) ee Leaf nodes in an SP \nTerm are either e+ , e-, and 2. Each edge in + a control .ow graph uniquely corresponds to either eor \ne-, and each while loop uniquely corresponds to 2. Thus, the number of leaves in an SP Term is equal \nto the sum of the number of edges and while loops, which is proportional to the size of a program.2 This \nconcludes that transformation from a program to an SP Term has (at most) linear growth in size. Fig. \n3 describes the control .ow graph of the example program (in Section 1), which computes the sum of 1,2,\u00b7\u00b7\u00b7,n \nfor an input n, and its transformation to an SP Term by trans. In Fig. 3, a tuple associated to each \nsubtree is a tuple of terminals at the interpretation of the subtree. Note that the description of a \ncontrol .ow graph by an SP Term is not unique. For instance, gives an alternative description of the \nsame program in Fig. 3 (Transformation trans is already nondeter\u00administic for p1; p2. Fig. 3 is obtained \nby the left most decomposi\u00adtion, and Fig. 4 is by the righter most decomposition) (1,3 ) + e (1,2) S \n(8,6) - - e e (8,7) (7,6) Figure 4. Another equivalent SP Term description  2.2 Dead Code Detection \nof Flowchart Pro\u00adgrams Our target is dead code detection, i.e., whether de.ned variables are used before \nrede.ned. We use the following functions to extract information from a node labeled l. defv l = {x} if \nthe node is an assignment x := e or a input statement input x. = if the node is just an expression. usev \nl = FV (e) if the node is either an assignment x := e or an expression e. = {x} if the node is an output \nstatement out put x. Detecting Used and De.ned Variables in a Fragment We .rst prepare the functions \nuse1 g, use2 g, de f1.2 g, and de f2.1 g that detect which variables are used and/or de.ned in a sub \nSP Term of g. use1 g returns the set of variables that are used 2Assuming that tree width is at most \nk, |E|= k|V | where V, E are the set of nodes and edges, respectively [23]. ({c} ,f,f,Var) ({S}n,i,S},f,Var,Var) \n({2 (f,f,Var,Var) ({i}(f,{n,i,S},Var,{S,c}) (f,{e-(f,f,Var,{,c}) (f,{-(f,{S,c},Var,{S}) - (f,{c},Var,f) \nee-(f,{n,i},Var,{c}) (use1,use2,de f1.2,de f2.1)  before being rede.ned in some path in g starting from \nterminal 1.3 de f1.2 g returns the set of variables that are newly de.ned in all paths in g from terminal \n1 to terminal 2 (if terminal 2 is reach\u00adable from terminal 1); and returns Var (the set of all variables)4, \notherwise. We omit the complementary de.nitions for use2 and de f2.1 g. + use1 (e, (l1,l2)) = usev l2 \n- use1 (e, )= f use1 (2, )= f use1 (Sxy, )= use1 x .(use1 y \\de f1.2 x) use1 (Pxy, )= use1 x .(use2 y \n\\de f1.2 x) . use1 y .(use2 x \\de f1.2 y) + de f1.2 (e, (l1,l2)) = defv l2 - de f1.2 (e, )= Var de f1.2 \n(2, )= Var de f1.2 (Sxy, )= de f1.2 x . de f1.2 y de f1.2 (Pxy, )= de f1.2 x n de f1.2 y Note that by \ntupling use1, use2, de f1.2, and de f2.1, we can com\u00adpute the sets of live variables at terminal 1 and \n2 in each sub SP Term of g by a single bottom-up traversal on g [18]. Live Variable Detection without \nIteration After the computation of use1, use2, de f1.2, and de f2.1, we as\u00ad sume that each sub SP Term \nin g has additional information of the results of these functions. Next we give a function addLive that \nassociates the information of live variables to each terminal in each sub SP Term in g. The function \naddLlive takes an SP Term g and two sets of variables vs1 and vs2 (both with the initial value of f), \nwhere vs1 denotes live variables outgoing from g at terminal 1 and vs2 denotes live variables outgoing \nfrom g at terminal 2. It returns a pair of an SP Term and a tuple of the sets of variables that are alive \nat the terminal 1 and 2. + addLive (e, (l1,l2)) vs1 vs2 + =(e, (l1, l2, vs1 .usev l2 .(vs2 \\defv l2), \nvs2)) - addLive (e, (l1,l2)) vs1 vs2 - =(e, (l1, l2, vs1, vs2 .usev l1 .(vs1 \\defv l1))) addLive (2, \n(l1,l2)) vs1 vs2 =(2, (l1,l2, vs1,vs2)) 3use1 g omits the used variables at terminal 1. 4Var satis.es \nX nVar = X, X .Var = Var, and X \\Var = f for each set X of variables. 1 : input n; {n} 2 : i := 0; \n{n,i} 3 : S := 0; {n,i,S}  4 : c := True; {n,i,S,c} 5 : while c {n,i,S}do 6 : i := i + 1; {n,i,S} \n 7 : c := False; {n,i,S} 8 : S := S + i; {n,i,S}  9 : c := i <= n; {n,i,S,c}od; 10 : output S f  \n Detected live variables 1: input n; .de f v(l1)= {n}.{n} 2: i := 0; .de f v(l2)= {i}.{n,i} 3: S := 0; \n.de f v(l3)= {S}.{n,i,S} 4: c := True; .de f v(l4)= {c}.{n,i,S,c} 5: while c do 6: i := i + 1; .de f \nv(l6)= {i}.{n,i,S} 7: c := False; .de f v(l7)= {c}.{n,i,S} 8: S := S + i; .de f v(l8)= {S}.{n,i,S} 9: \nc := i <= n; .de f v(l9)= {c}.{n,i,S,c}od; 10 : out put S Figure 6. Dead code detection of a .ow chart \nprogram addLive (Sxy, (l1,l2)) vs1 vs2 =(S (addLive x vs1 (use1 y .(vs2 \\de f1.2y))) (addLive y (use2 \nx .(vs1 \\de f2.1x)) vs2), (l1, l2, vs1, vs2)) addLive (Pxy, (l1,l2)) vs1 vs2 =(P (addLive x (vs1 .use1 \ny .((vs2 .use2 x) \\de f1.2 y)) (vs2 .use2 y .((vs1 .use1 x) \\de f2.1 y))) (addLive y (vs1 .use1 x .((vs2 \n.use2 y) \\de f1.2 x)) (vs2 .use2 x .((vs1 .use1 y) \\de f2.1 x))), (l1, l2, vs1, vs2)) With the assumption \nthat use1, use2, de f1.2, and de f2.1 are com\u00adputed and their results are stored, addLive is done in \na single top\u00addown traversal on g. Fig. 5 shows computation of (use1,use2,de f1.2,de f1.2) and addLive \non a control .ow graph in Fig. 3. At the terminals in a leaf in an SP Term, the detected set of live \nvariables at each node is obtained. Dead Code Detection of Flowchart Programs Now that the set of live \nvariables at each node in a control .ow graph has been computed, dead code detection is straightforward. \nA variable is dead if it is not live. Dead code is an assignment that assigns a value to a dead variable. \nThus, in the example in Fig. 5, the assignment Z:=X+2 at line 8 is a dead code, since Z is dead as in \nshown in Fig. 6. Figure 5. Examples of use1, use2, de f1.2, de f1.2, and addLive y x z  y x z  y z \n y x z  x z y y x z  y x z u   3 Register Allocation for Flowchart Program In this section, we \nshow how to .nd an optimal register allocation as an instance of a maximum marking problem. Our strategy \nis, .rst write down the .nite mutumorphic speci.cation checking whether marking represents correct register \nallocation, and the weight w that counts the number of required LOAD/STORE instructions. Second, transform \nchecking to the form with f oldSP by tupling transfor\u00admation [18]. Then, if w is homomorphic, Optimization \nTheorem (Theorem 1 [27, 26]) automatically gives how to detect an opti\u00admal register allocation with certain \ngeneric dynamic programming (i.e., a single traversal on an SP Term), assuming the live variables are \npre-computed. Note that we restrict ourselves to control .ow graphs with bounded tree width, and do not \nintend P =NP, where the conventional optimal register allocation based on graph color\u00ading [10] is NP-complete. \nFor simplicity, we consider a .owchart programs (without GOTO) as in Section 2. We assume that functions \ndefv l, usev l, and live variables at terminal labeled l are pre-computed (as in Section 2). 3.1 Register \nAllocation In a real computer, an instruction is executed with values on limited number of registers. \nIf needed inputs are not on registers, then they must be loaded from memory; and if there are no room \nfor them, some values on registers must be stored. These LOAD/STORE in\u00adstructions are usually expensive, \nand register allocation is an opti\u00admization that under .xed number of registers, .nd an optimal regis\u00adter \nusage, i.e., a program execution with the minimum use of LOAD (from memory to register) and STORE (from \nregister to memory). Basic operations on registers are either LOAD, STORE, move,or execution of an instruction. \nWhen the number of registers is 4, for instance, we have LOAD x r2 y z . STORE x r2 . r4:=r1 (move) \n. . r4:=r1 +r2 where u =y +x For simplicity, we only concentrate on the number of LOAD/STORE instructions, \nand do not care on the number of move. Fig. 7 shows the optimal register allocation for a simple program \n(appeared in Section 1), which computes the sum of 1 to n with 3 registers.5 Here, each tuple of three \nvariables represents a regis\u00adter allocation just before each instruction is executed. The special symbol \nmeans that the register is either empty or permitted to overwritten. Note that between line 7 and 8, \nSTORE c needs not to be inserted; instead we just overwrite S on c. This is correct, because c is dead \nat line 7. 3.2 Maximum Marking Problem Maximum marking problem (MMP for short) can be speci.ed as follows: \nGiven a data structure x, the task is to .nd a way to mark elements in x such that the marked data satis.es \na certain property p and has the maximum (or, equivalently, minimum) value with re\u00adspect to certain weight \nfunction w. This means that no other mark\u00ading of x satisfying p can produce a larger value with respect \nto w. 5Strictly speaking, LOAD/STORE instruction must be inserted at the machine code level, but for \nsimplicity we just insert LOAD/STORE instructions into a .owchart program.  instruction register live \nvariables 1: 2: 3: input n; i :=1; S :=0; STORE S (, , ) (n, , ) (n,i, ) (n,i,S) {n}{n,i}{n,i,S} 4: 5: \nc :=True; while c do (n,i, ) (n,i,c) {n,i,S,c}{n,i,S} 6: 7: i :=i +1; c :=False; LOAD S (n,i,c) (n,i,c) \n(n,i,c) {n,i,S}{n,i,S} 8: S :=S +i; STORE S (n,i,S) (n,i,S) {n,i,S} 9: c :=i <=n; od; (n,i, ) {n,i,S,c} \nLOAD S (n,i,c) 10 : out put S (n,i,S) {} Figure 7. An example of optimal register allocation MMP includes \nmany interesting problems, such as knapsack prob\u00adlems, and optimized range problems in data mining [28]. \nOf course, it is not expected that every MMP problem can be solved ef.ciently. In fact, MMP includes \nNP-hard problems, such as the knapsack problem. However, for instance, the knapsack problem restricted \nto integer weight can be computed in linear time. Let us consider more formally. The speci.cation of \nMMP is de\u00adscribed as follows, where constraints are expressed by a boolean\u00advalued function p and a weight \nfunction w. mmpw p =selectmax w . filter p . gen The function gen generates all possible marking on elements: \ngen : D .{D*} D* is the data structure derived from D where each node is attached with a mark. The function \nfilter p takes a set of marked data and selects ones that satisfy the property p. The function selectmax \nw takes a set of marked data and select one that has the maximum value with respect to the weight function \nw. Then, we can derive a linear time algorithm mechanically if the property p is de.ned by .nite mutumorphisms, \nand the weight function w is homomor\u00adphic [26]. Mutumorphism is a set of mutually recursive functions, \namong which no nested function calls occur and each argument of recur\u00adsive call is a sub-structure of \nthe input [15]. Note that by tupling transformation, mutumorphism is transformed to a single catamor\u00adphism \n[18]. Although mutumorphism is de.ned on more general data structures, from now on, we will consider \nSP Terms only. DEFINITION 2(FINITE MUTUMORPHIC PROPERTY [27]). A property p is .nite mutumorphic if it \nis de.ned by p : SP*. Bool + p (e, a)= fe+ a - p (e, a)= fe- a p (2, a)= f2 a p (Sx1 x2, a)= fS (hx1)(hx2)a \np (Px1 x2, a)= fP (hx1)(hx2)a where h x =(px, f1 x, f2 x,..., fm x), which may use auxiliary functions \nf1,..., fm each of which has.nite range ofCi. fi : SP*.Ci + fi (e, a) = fie+ a - fi (e, a)= fie- a fi \n(2, a)= fi2 a fi (Sx1 x2, a)= fiS (hx1)(hx2)a fi (Px1 x2, a)= fiP (hx1)(hx2)a If p is .nite mutumorphic, \ntupling transformation [18] will yield a catamorphism for h. Therefore a .nite mutumorphic property p \ncan be described in the form of p = fst . f oldSP pe+ pe- p2 pS pP where fst is the function that takes \nthe .rst element in a tuple and the fold (catamorphism) operation f oldSP on SP terms is de.ned below. \nf oldSP .e+ .e- .2 .S .P =. + where . (e, a)=.e+ a - . (e, a)=.e- a . (2, a)=.2 a . (Sx1 x2, a)=.S (. \nx1)(. x2)a . (Px1 x2, a)=.P (. x1)(. x2)a To be concrete, recall the dead code detection in Section 2.2, \nwhere we have reached the point that each node is added with a set of live variables. Assume that some \nnodes in the graph are marked (which can be checked by isM.) Now we may de.ne the property md by f oldSP \nthat all marked nodes in the graph are dead. md = f oldSP .1 .1 .1 .2 .2 where .1 (l1,l2,vs1,vs2)=valid \n(l1,vs1).valid (l2,vs2) .2 p1 p2 a =p1 .p2 ..1 a Here valid is to determine whether a marked terminal \nnode is dead, i.e., valid (l,vs)=if isM l then defv l .vs else True. DEFINITION 3(HOMOMORPHIC WEIGHT \nFUNCTION [26]). A weight function w is homomorphic if w is de.ned as a fold w : SP*.Weight w = f oldSP \n.e+ .e- .2 .S .P where .S and .P is described as a summation in a form like .Sr1 r2 a =r1 +r2 +vS a .Pr1 \nr2 a =r1 +r2 +vP a for some functions vS and vP. Continuing with the dead code detection problem, we \nmay de.ne a weight function nd to count the number of the marked dead nodes6 nd = f oldSP .1 .1 .1 .2 \n.2 where .1 (l1,l2,vs1,vs2)=cl1 +cl2 .2 p1 p2 a =p1 +p2 Here cl returns 1 if the node l is marked, and \n0 otherwise. THEOREM 1(OPTIMIZATION THEOREM [26, 27]). If the property p is .nite mutumorphic and the \nweight function w is homomorphic, MMP speci.ed by spec : SP .SP* spec = mmpw p 6This is not exactly true. \nIn fact, all marked dead nodes except for the two terminal nodes of the whole graph are counted twice. \nhasan O(|C;|\u00b7n)algorithm described as opt .e+ .e- .2 .S .P fst pe+ pe- p2 pS pP where C; =C1 \u00d7\u00b7\u00b7\u00b7\u00d7Cm \nand n is the size of an input. The core of Optimization Theorem is a generic dynamic program\u00adming. The \nidea is; during data traversal, compute intermediate max\u00adima for all possible states that may contribute \nto the .nial maxi\u00admum. Finite mutumorphisms f1,..., fm describe state transition, and .niteness of their \nranges guarantee that such states are .nite. For the de.nition of opt and detail, refer to [27, 26]. \nIt follows from this theorem that we can detect all dead codes with the property md and the weight function \nnd by the following pro\u00adgram. opt .1 .1 .1 .2 .2 id .1 .1 .1 .2 .2 We will see a more interesting application \nof the theorem in the next Section. 3.3 Optimal Register Allocation as MMP As an application of Optimization \nTheorem, we demonstrate the register allocation problem. Check Whether Each Terminal Has a Correct Mark \nLet Var be the set of variables that appears in a program, and let be the special symbol that means a \nregister is either empty or ready to overwrite. The set Reg of register allocations (we consider the \nsize of registers is three) is de.ned as: Reg = {(v1,\u00b7\u00b7\u00b7,vn)| vi .Var .{ }, vi =vj . vi =vj = if i = \nj}. An element in Reg is labeled to each terminal in an SP Term as a mark, which represents the register \nallocation state just before the instruction at the terminal being executed. Below, we will describe \n checking, which checks whether each terminal has a correct mark, and  w, which counts the number of \nrequired LOAD/STORE in\u00ad  structions under a certain marking of the program. A mark in Reg is a tuple, \nand we use the following operations (as analogy to set operations). Let r =(v1,\u00b7\u00b7\u00b7,vn), r; =(v;1,\u00b7\u00b7\u00b7,v;). \nnReg. r \\\\ r; =(v;;1 ,\u00b7\u00b7\u00b7,vn;;) where vi;;=if vi =v;i then else vi RV r ={v1,\u00b7\u00b7\u00b7,vn}\\{ } The function \nchecking g takes a marked SP-term associated with the line numbers in a program (denoted by l1,l2), the \nsets of live variables (denoted by vs1 and vs2), and the marking that represents (pre-execution) register \nstatus (denoted by m1 and m2) at terminal 1 and 2, and returns a Boolean value. + checking (e, ((l1,l2,vs1,vs2), \n(m1,m2))) = ch ((l1,l2, vs1,vs2), (m1,m2)) - checking (e, ((l1,l2,vs1,vs2), (m1,m2))) = ch ((l1,l2, vs1,vs2), \n(m1,m2)) checking (2, ((l1,l2, vs1,vs2), (m1,m2))) = ch ((l1,l2, vs1,vs2), (m1,m2)) checking (Sxy, ((l1,l2, \nvs1,vs2), (m1,m2))) = let (m;1,m2;)= getMarks x (m;;1 ,m;; 2 )= getMarks y in checking x . checking \ny ;;;;;; . m1 = m1 . m2 = m. m2 = m 21 checking (Pxy, ((l1,l2,vs1,vs2), (m1,m2))) = let (m;1,m2;)= getMarks \nx (m;;1 ,m;; 2 )= getMarks y in checking x . checking y 1 . m1 = m;;2 . m2 = m;; . m1 = m;1 . m2 = \nm;2 ch ((l1,l2, vs1,vs2), (m1,m2)) = usev l1 . RV m1 . (vs1 . usev l1) . usev l2 . RV m2 . (vs2 . usev \nl2) . |defv l1| + |RV m1 n vs1 \\ defv l1|= n . |defv l2| + |RV m2 n vs2 \\ defv l2|= n getMarks (t,a)= \nsnd a The judgment usev l1 . RV m1 corresponds to the pre-condition of the instruction l1 at terminal \n1 in g, i.e., each variable used in the instruction must be in some register in m1, and |defv l1| + |(m1 \nn vs1) \\ defv l1|= n corresponds to the post-condition, i.e., m1 has a room to write de.ned variables \n(defv l1); otherwise, some live variables in m1 except for those de.ned at l1 will be overwritten before \nstored. Notice the obvious optimizing conditions RV m1 . vs1 . usev l1 RV m2 . vs2 . usev l2 in ch ((l1,l2,vs1,vs2),(m1,m2)), \nwhich mean that live variables in registers are as many as possible. The checking property is de.ned \nas .nite mutumorphisms with the function getMarks. By tupling transformation, we get the following form: \n checking = fst . f oldSP pe+ pe- p2 pS pP where pe+(x, a)=(ch a, snd a) pe- (x, a)=(ch a, snd a) p2 \n(x, a)=(ch a, snd a) pS xya 1 ,m;; = let (m1;,m;2)= snd x (m;;2 )= snd y (m1,m2)= snd a 1 . m2 = m;;2 \n= m;; in ( fst x . fst y . m1 = m;2 . m;1 , (m1,m2)) pP xya 1 ,m;; = let (m1;,m;2)= snd x (m;;2 )= snd \ny (m1,m2)= snd a ;;; in ( fst x . fst y . (m1 = m1 . m1 = m1 ) 2 . m2 = m;; . (m2 = m;2 ), (m1,m2)) Here \nwe include obvious optimizing conditions RV m1 . vs1 . usev l1 RV m2 . vs2 . usev l2 to ch (l1,l2,vs1,vs2,m1,m2), \nwhich mean that live variables in reg\u00adisters are as many as possible. Weight Counts the Number of Required \nLOAD/STORE The weight function w is de.ned as follows. + w (e, ((l1,l2,vs1,vs2), (m1,m2))) = count l1 \nvs1 m1 m2 - w (e, ((l1,l2,vs1,vs2), (m1,m2))) = count l2 vs2 m2 m1 w (2, ((l1,l2,vs1,vs2), (m1,m2))) \n= 0 w (Sxy, ((l1,l2,vs1,vs2), (m1,m2))) = wx + wy w (Pxy, ((l1,l2,vs1,vs2), (m1,m2))) = wx + wy count \nlvsmm; = let V =(RV m n vs) . defv l inif V . RV m; then max|(V \\ RV m;) n vs| + |RV m;\\V | else if |V \n| < n then |RV m;\\V |else if RV (m; \\\\ m)= f then 0 else 2 The function w counts the number of required \nLOAD/STORE at + each edge (uniquely represented by eor e-), and just sums up for recursive constructors \nS and P. The intuition for count is: V is the set of variables that are placed on the registers after \nan instruction is executed. If V is not included in the next register status m;, then their difference \nmust be stored and loaded. Between STORE and LOAD operations, we can reorder the positions of variables \nby move operations. Assume V is included in the next register status m;.If |V | < n, this means there \nexists a register with , and we can reorder variables in V . Otherwise, V = RV m;, and if RV (m; \\\\ m)= \nf we need to make room by a pair of STORE and LOAD operations for reordering. The above de.nition of \nthe weight function w is homomorphic, and w is de.ned by f oldSP as follows. .e+ a = let ((l1,l2,vs1,vs2), \n(m1,m2)) = a in count l1 vs1 m1 m2 .e- a = let ((l1,l2,vs1,vs2), (m1,m2)) = a in count l2 vs2 m2 m1 .2 \na = 0 .Sxya = x + y .Pxya = x + y Applying Optimization Theorem, and Discussion With all the above, Theorem \n1 automatically derives the solution for optimal register allocation. At last, two points are worth remarking: \n 1. In real compilers, there are often practical requirements of hardware, such as, some instruction \nmust use some speci.c registers, some register must be used together with some spe\u00adci.c registers, or \nthe result of some instruction must be writ\u00adten in a different register. These requirements are hard \nfor the conventional graph coloring method [10], but our method is easy to handle them by modifying the \nfunction checking. 2. We obtain an optimal register allocation without iteration. The core of the technique \nis dynamic programming on SP Terms. The cost to pay is huge marking space, which grows expo\u00adnentially \nto the number of registers. However, since checking can be judged locally (like forall in Haskell), most \nof marking is avoided by default. We expect demand-driven computa\u00adtion.helps the situation.   4 Analyzing \nControl Flow Graphs with Larger Tree Width In this section, we discuss how our method can be extended \nto wider class of programs. The example is again dead code detection; but for a program that has a control \n.ow graph with tree width larger than 2. For simplicity, we mostly consider control .ow graphs with tree \nwidth at most 3. Our construction is uniform and extension for larger tree width is straight forward, \nif we assume the description on reachability among terminals. Due to lack of space, we do not explain \ntree decomposition, which gives the original de.nition of tree width [25]. Instead, we treat a (di)graph \nwith tree width at most k as a (di)graph denoted by an SP Term in SPk.   4.1 SP Terms In Section \n2.1, we show how an SP Term of a control .ow graph of a .owchart program (i.e., a series-parallel graph) \nis computed in linear time. In this section, we give de.nition of general SP Term G4 . .  1 (for \ngraphs with larger tree width) and show that translation will be done in linear time. DEFINITION 4. An \nSP Term is a pair of a ground term t and a  1 . 2 4  3 tuple (l(1),\u00b7\u00b7\u00b7,l(k)), de.ned as the following. \n SPk :=(ek(i, j), (l(1),\u00b7\u00b7\u00b7,l(k))) (i = j) G1 | (k, (l(1),\u00b7\u00b7\u00b7,l(k))) G1 | (Sk SPk ... SPk, (l(1),\u00b7\u00b7\u00b7,l(k))) \nshift ' v-\" k  | (Pk SPk SPk, (l(1),\u00b7\u00b7\u00b7,l(k))) Here, l(1),\u00b7\u00b7\u00b7,l(k) are labels, Pk is the parallel composition, \nand  Sk is the series composition. SP Terms (ek(i, j),(l(1),\u00b7\u00b7\u00b7,l(k))) and (k,(l(1),\u00b7\u00b7\u00b7,l(k))) are interpreted \nas k-terminal digraphs G, G; with terminals (l(1),\u00b7\u00b7\u00b7,l(k))) and 1 . 3 2 G2 ...  2 5  G1 3 V (G)= \n{l(1),\u00b7\u00b7\u00b7,l(k)}, E(G)= {(l(i),l( j)}, V (G;)= {l(1),\u00b7\u00b7\u00b7,l(k)}, E(G;)= f. i.e., k-nodes l(1),\u00b7\u00b7\u00b7,l(k) \nwith one diedge from l(i) to l( j), and i.e., k isolated nodes l(1),\u00b7\u00b7\u00b7,l(k), respectively (See Fig. \n8). G1 G1 fuse  3 3   .  3 i k 1 2 5 2   3 remove j 2  3 3  ek(i, j) k   . 2 Figure 8. \nInterpretation of constants ek(i, j) and k 1 2 2  Series composition (Sk t1 \u00b7\u00b7\u00b7 tk, (l(1),\u00b7\u00b7\u00b7,l(k))) \nis interpreted in S4 G1 G2 G3 G4 S2 G1 G2 S3 G1 G2 G3 3 steps. See Fig. 9 (In Fig. 9 and 10, a double \ncircle expresses a Figure 9. Interpretation of series composition S2, S3, S4 terminal). Let ti =(ti;, \nli(1),\u00b7\u00b7\u00b7,li(k)). 1. Shift the numbering of terminals in ti, i.e., the j-th terminal to the j + 1-th \nterminal for each j with i = j =k.  1  2. Fuse each terminal of the same numbering and put a label \n  1 2 match (l1(i -1),\u00b7\u00b7\u00b7,li-1(i -1),li+1(i),\u00b7\u00b7\u00b7,lk(i)) 2 match(l1(i -1),\u00b7\u00b7\u00b7,li-1(i -1),li+1(i),\u00b7\u00b7\u00b7,lk(i)) \n=..  1 2 3 to the i-th terminal if 3 4 G1 G2 G1 G2 G1 G2 3. Remove the last terminal (labeled $ in \nFig. 9). P2 P3 P4 where match (l(1),\u00b7\u00b7\u00b7,l(k)) is an abbreviation of  match (l(1),match (l(2),\u00b7\u00b7\u00b7,match \n(l(k -1),l(k))\u00b7\u00b7\u00b7). Parallel composition (Pk t1 t2, (l(1),\u00b7\u00b7\u00b7,l(k))) is interpreted similar to P in Section \n2; fuse each terminal of the same numbering in t1 = P2 G1 G2 (t1;, l1(1),\u00b7\u00b7\u00b7,l1(k)) and t2 =(t2;, l2(1),\u00b7\u00b7\u00b7,l2(k)), \nand put a label match (l1(i),l2(i)) to the i-th terminal if match (l1(i),l2(i)) =.. P3 G1 G2 See Fig. \n10. Intuition behind is; like that the parallel composition pk constructs any subgraph in a complete \ngraph Kk, the series composition Sk P4 G1 G2 combines such components and produces a clique of the size \nk + 1 Figure 10. Interpretation of parallel composition P2, P3, P4 (i.e., an embedding of Kk+1). EXAMPLE \n1. In Fig. 9, the digraph s3(G1,G2,G3) has tree width 3, and G1,G2.G3 have tree width 2. The SP Terms \nof G1,G2,G3 are described as G1 = S3(P3(e3(2,3),e3(3,1)),P3(e3(1,2),e3(1,3)),3) G2 = S3(P3(e3(1,2),e3(1,3)),e3(3,1),3) \nG3 = S3(3,e3(1,2), S3(P3(e3(1,2),e3(1,3)),P3(e3(1,2),e3(3,1)),3)) REMARK 1. SP2 (series parallel graphs) \nallows several choices for de.nition of the series composition. For instance, de.nition of S in Section \n2 is different from S2 here; they can be related as Sxy = chT (S2 x (chT y)). S2 is the part of uniform \nway to de.ne the series composition for each SPk; however, for readability, we used the simpli.ed version \nS in Section 2. REMARK 2. The de.nition of series composition Sk and parallel composition Pk are given \nby Arnborg, et.al. in a different aspect of an algebraic construction of graphs with bounded tree width \n[3]. Note that if once an SP Term t is given, tree decomposition [25] of a corresponding graph is straightforward: \na backborn tree T as V (T )= {s |s .t}and a covering Xs for s .V (T ) as the set of terminals in s. THEOREM \n2. Let G be a digraph with twd(G) = k and |V (G)|=k for k =2. Then, an SP Term is computed in linear \ntime (wrt |V (G)|) such that its interpretation is a pair of k-terminal digraph G and a tuple of k-terminals \nwith G = G by neglecting ter\u00adminals. In general, deciding the tree width of a graph is NP-complete; how\u00adever, \nfor .xed k, whether a graph has tree width at most k is decid\u00adable in linear time [7, 24]. Fortunately, \nwe already know the upper bound of the tree width of control .ow graphs of some speci.c pro\u00adgramming \nlanguages. This shows the general method to compute an SP Term from a con\u00adtrol .ow graph via tree decomposition. \nThis is done in linear-time, but not so ef.cient linear time. However, a direct translation (such as \ntrans in Section 2.1) from a program will be much more ef.\u00adcient, because a control .ow graph loses the \nparsing information of an original program. For a simple imperative language with GOTO, such a translation \nis shown in Appendix A. 4.2 Alternative De.nition of Dead Code De\u00adtection on SP2 Before the extension, \nwe give the alternative de.nitions of the func\u00adtions use1 g, use2 g, addLive g vs1 vs2 for SP2 in Section \n2). Recall that the original de.nition is as follows (taking account into the modi.cation of S instead \nof S2): use1 (e2(1,2), (l1,l2)) = usev l2 use1 (e2(2,1), )= f use1 (2, )= f use1 (S2 xy, )= use1 y .(use2 \nx \\de f1.2 y) use1 (P2 xy, )= use1 x .(use2 y \\de f1.2 x) . use1 y .(use2 x \\de f1.2 y) de f1.2 (e2(1,2), \n(l1,l2)) = defv l2 de f1.2 (e2(2,1), )= Var de f1.2 (2, )= Var de f1.2 (S2 xy, )= de f1.2 y .de f2.1 \nx de f1.2 (P2 xy, )= de f1.2 x nde f1.2 y addLive (e2(1,2), (l1,l2)) vs1 vs2 =(e2(1,2), (l1, l2, vs1 \n.usev l2 .(vs2 \\defv l2), vs2)) addLive (e2(2,1), (l1,l2)) vs1 vs2 =(e2(2,1), (l1, l2, vs1, vs2 .usev \nl1 .(vs1 \\defv l1))) addLive (2, (l1,l2)) vs1 vs2 =(2, (l1,l2, vs1,vs2)) addLive (S2 xy, (l1,l2)) vs1 \nvs2 =(S2 (addLive y vs1 (use2 x .(vs2 \\de f2.1x))) (addLive x vs2 (use2 y .(vs1 \\de f2.1y))), (l1, l2, \nvs1, vs2)) addLive (P2 xy, (l1,l2)) vs1 vs2 =(P2 (addLive x (vs1 .use1 y .((vs2 .use2 x) \\de f1.2 y)) \n(vs2 .use2 y .((vs1 .use1 x) \\de f2.1 y))) (addLive y (vs1 .use1 x .((vs2 .use2 y) \\de f1.2 x)) (vs2 \n.use2 x .((vs1 .use1 y) \\de f2.1 x))), (l1, l2, vs1, vs2)) The alternative de.nition below contains some \nredundant computa\u00adtion. This is because generality of the de.nition, if one considers to extend to general \nk. Note that distributivity of nwrt .and inclusion like use2 g \\de f1.2 g .use1 g can absorb the differences. \nuse1 (e2(1,2), (l1,l2)) = usev l2 use1 (e2(2,1), (l1,l2)) = f use1 (2, (l1,l2)) = f use1 g@(S2 xy, (l1,l2)) \n = use1 x .(use1 y \\de f1.2 g) .((use2 x .use2 y) \\de f1.2 x) use1 g@(P2 xy, (l1,l2)) =(use1 x .use1 \ny) .((use2 x .use2 y) \\de f1.2 g) addLive (e2(1,2), (l1,l2)) vs1 vs2 =(e2(1,2), (l1, l2, vs1 .usev l2 \n.(vs2 \\defv l2), vs2)) addLive (e2(2,1), (l1,l2)) vs1 vs2 =(e2(2,1), (l1, l2, vs1, vs2 .usev l1 .(vs1 \n\\defv l1))) addLive (2, (l1,l2)) vs1 vs2 =(2, (l1, l2, vs1,vs2)) addLive g@(S2 xy, (l1,l2)) vs1 vs2 \n=(S2 (addLive y vs1 use2 x .((vs2 .use1 x) \\de f2.1x)) (addLive x vs2 use2 y .((vs1 .use1 y) \\de f2.1y)), \n(l1, l2, vs1,vs2)) addLive g@(P2 xy, (l1,l2)) vs1 vs2 = let vs;1 = vs1 .((vs2 .use2 x .use2 y) \\de f1.2 \ng) ; vs2 = vs2 .((vs1 .use1 x .use1 y) \\de f2.1 g) in (P2 (addLive x (use1 y .vs1;)(use2 y .vs2;)) (addLive \ny (use1 x .vs1;)(use2 x .vs2;)), (l1, l2, vs1, vs2))  4.3 Dead Code Detection for Larger Tree Width \nUsed/De.ned Variables in a Fragment in SP3 usev l j if i = 1 use1 (e3(i, j), (l1,l2,l3)) = f otherwise \nuse1 (3, (l1,l2,l3)) = f use1 g@(S3 xyz, (l1,l2,l3)) = use1 y .use1 z . ((use1 x .use2 z) \\de f1.2 g) \n. ((use2 x .use2 y) \\de f1.3 g) . ((use3 x .use3 y .use3 z) \\de f1.$ g) use1 g@(P3 xy, (l1,l2,l3)) =(use1 \nx .use1 y) . ((use2 x .use2 y) \\de f1.2 g) . ((use3 x .use3 y) \\de f1.3 g) The basic idea is the same \nas that in use1 g for SP2 except for the modi.cation ((use3 x . use3 y . use3 z) \\ de f1.$ g) in use1 \ng@(Sxyz, (l1,l2,l3)). $ is the newly introduce symbol that represents the removed terminal in Sxyz, i.e., \nterminal 3 in x,y,z. For SP2, de f1.$ (Sxy) coincides with de f1.2 x, because paths from terminal 1 to \nterminal 2 in x are only paths from terminal 1 to $in g without loops. Thus, for SP2, the need for $ \nis hidden. de f1.2 (e3(i, j), (l1,l2,l3)) defv l2 if i = 1, j = 2 = Var otherwise de f1.2 (3, (l1,l2,l3)) \n= Var de f1.2 (S3 xyz, (l1,l2,l3)) = de f1.2 z n (de f1.2 y . de f2.1 x) n (de f1.3 z . de f3.1 x) n \n(de f1.3 y . de f3.2 z) n (de f1.3 y . de f3.1 x) n (de f1.3 z . de f3.2 y . de f2.1 x) n (de f1.2 y \n. de f2.3 x . de f3.2 z) de f1.2 (P3 xy, (l1,l2,l3)) = de f1.2 x n de f1.2 y n (de f1.3 y . de f3.2 x) \nn (de f1.3 x . de f3.2 y) de f1.$ (Sxyz, (l1,l2,l3)) = de f1.3 y n de f1.3 z n (de f1.2 y . de f2.3 x) \nn (de f1.3 z . de f3.3 x) n (de f1.2 y . de f2.1 x . de f2.3 z) n (de f1.2 z . de f1.2 x . de f2.3 y) \nThe de.nition of de f1.2 in SP3 is quite complex especially for Sxyz. The intuition can be obtained by \nreplacing .,n with .,., respectively. Then, by setting values for base cases as True if (i, j)=(1,2)reach1.2 \n(e3(i, j), (l1,l2,l3)) = False otherwise reach1.2 (3, (l1,l2,l3)) = False the same de.nition gives us \nthe judgment of reachability from ter\u00adminal 1 to terminal 2 in g. Live Variables Detection for SP3 Now, \nwe give de.nition of addLive to detect live variables for SP3. addLive g@(e3(1,2), (l1,l2,l3)) vs1 vs2 \nvs3 =(e3(1,2), (l1, l2, l3, vs1 . usev l2 . (vs2 \\ defv l2), vs2, vs3)) addLive (3, (l1,l2,l3)) vs1 vs2 \nvs3 =(3, (l1, l2, l3, vs1, vs2, vs3)) addLive g@(S3 xyz, (l1,l2,l3)) vs1 vs2 vs3 = let vs;1 =(vs1 . use1 \ny . use1 z) . ((vs2 . use1 x . use2 z) \\ de f1.2 g) . ((vs3 . use2 x . use2 y) \\ de f1.3 g) . ((use3 \nx . use3 y . use3 z) \\ de f1.$ g) vs;2 =((vs1 . use1 y . use1 z) \\ de f2.1 g) . (vs2 . use1 x . use2 \nz) . ((vs3 . use2 x . use2 y) \\ de f2.3 g) . ((use3 x . use3 y . use3 z) \\ de f2.$ g) vs;3 =((vs1 . use1 \ny . use1 z) \\ de f3.1 g) . ((vs2 . use1 x . use2 z) \\ de f3.2 g) . (vs3 . use2 x . use2 y) . ((use3 x \n. use3 y . use3 z) \\ de f3.$ g) vs; =((vs1 . use1 y . use1 z) \\ de f$.1 g) . ((vs2 . use1 x . use2 z) \n\\ de f$.2 g) . ((vs3 . use2 x . use2 y) \\ de f$.3 g) . ((use3 x . use3 y . use3 z) in (S (addLive x vs;2 \nvs3;vs;)(addLive y vs;1 vs3;vs;) ; (addLive z vs;1 vs2 vs;), (l1, l2, l3, vs1, vs2, vs3)) addLive g@(P3 \nxy, (l1,l2,l3)) vs1 vs2 vs3 = let vs;1 =(vs1 . use1 x . use1 y) . ((vs2 . use2 x . use2 y) \\ de f1.2 \ng) . ((vs3 . use3 x . use3 y) \\ de f1.3 g) vs;2 =((vs1 . use1 x . use1 y) \\ de f2.1 g) . (vs2 . use2 \nx . use2 y) . ((vs3 . use3 x . use3 y) \\ de f2.3 g) vs;3 =((vs1 . use1 x . use1 y) \\ de f3.1 g) . ((vs2 \n. use2 x . use2 y) \\ de f3.2 g) . (vs3 . use3 x . use3 y) ;;;; in (P (addLive x vs;1 vs2 vs3)(addLive \ny vs1;vs2 vs3), (l1, l2, l3, vs1, vs2, vs3)) Discussion Here, we present our study only for SP3, i.e., \ntree width at most 3. It is worth mentioning the analogy between de fi. j and reachi. j, and the dif.culty \nto extend to graphs with larger tree width is fo\u00adcused on the reachability description among terminals. \nCurrent our description is not parametric wrt tree width, i.e., we must describe, say, dead code detection \nfor each SPk. However, we have a basic feel that there would be some generic skeleton-like structure \nregard\u00ading reachability. That is, with the description of reachability for SPk and the description of \nan analysis for SP2, we can generate the de\u00adscription of an analysis for general SPk. Of course, with \nthe increase of tree width, the number of functions rapidly grows. But, recall that most JAVA programs \n(and possi\u00adbly other imperative programs) have a control .ow graph with tree width at most 3 [16]. Thus, \neven for relatively small tree width, our method would cover quite large portion of real programs.  \n5 Related Work Many researches have been devoted to the declarative approaches to program analyses. Steffen \nand Schmidt [31, 30] showed that tem\u00adporal logic is well suited to describe data dependencies and other \nproperties exploited in classical compiler optimization. Lacey, et.al. [21] formalized program optimization \nas rewriting systems with temporal logic side conditions (described in CTL-FV) and shows that CTL-FV \nplays a crucial role in the proofs of correct\u00adness of classical optimizations. Instead of temporal logic, \nde Moor, et.al. [12] proposed another functional approach to control .ow analyses. Their speci.cation \nlanguage is the regular path condition, but the ef.ciency of derived programs is not discusses. There \nare several functional approaches for computation on graphs. For instance, Fegaras and Sheard [14] treat \ngraphs with embed\u00added functions, i.e., graphs are treated as functions that generates all paths in a \ngraph. Erwig introduces the active pattern matching, which is a conditional pattern matching mechanism \n[13]. Their ap\u00adproaches are interesting in description, but the existence of strong side conditions limits \nthe chance to optimize. Instead, we restrict ourselves to graphs with bounded tree width, in which many \nNP\u00adhard graph problems are solved in linear-time [11, 9]. The concept of a graph with bounded tree width \n[25] independently appeared from early 80 s; partial k-tree in terms of cliques, some algebraic construction \nof k-terminal graphs [4, 3], and in terms of separators, and they are all equivalent. The class of graphs \nwith bounded tree width is quite restrictive; but the signi.cant tread\u00adoff is: The class of graphs with \nbounded tree width frequently has a linear time algorithm for graph problems. For graphs with bounded \ntree width, there have been lots of work on automatic gen\u00aderation of linear-time algorithms from speci.cation \nin monadic sec\u00adond order formulae, which are frequently NP-complete for general graphs [11, 9]. Our starting \nobservation is that most programs (without spaghetti GOTO) have control .ow graphs with bounded tree \nwidth, and many control .ow analyses can be speci.ed in temporal logic (such as CTL-FV). By combining \nthem, it seems easy to obtain (almost) linear-time algorithm for control .ow analyses. This is true in \nthe\u00adory, but not in practice; each existence of quanti.ers in a formula causes the exponential explosion \nof the constant factor. Our ap\u00adproach is, directly write functional speci.cation on the simple data structure, \nSP Term. This approach drastically reduces the constant factor [27]. Further, an SP Term is more approachable \nespecially from programming point of view, and it does not refuse to capture better algorithmic ideas. \nFor an algebraic construction of graphs, one of the early work for .owchart scheme is found in [29]. \nBauderon and Courcelle [4] are also pioneers, and our SP Term is greatly in debt to the work by Arnborg, \net.al. [3]. However, their constructions do not .t to our purpose; for instance, the construction by \nArnborg, et.al. [3] re\u00adquires the recursive constructors lij,rj,sj, pj with 1 = i = j = k for graphs \nwith tree width at most k. Thus, the number of their re\u00adcursive constructors becomes k(k + 1)(k + 2)/6, \nand this makes us dif.cult to write recursive de.nitions. We proposed another con\u00adstruction, SP Term, \nwhich has only 2 recursive constructors Sk,Pk regardless of the size of k. The number of constants ek(i, \nj) has square growth, but they are interpreted as diedges from the i-th to the j-th terminal. For these \nconstants, writing functional speci.ca\u00adtion (base cases) is easy; even in uniform way. Thorup [33] showed \nthat a structured imperative program have a control .ow graph with relatively small tree width. He also \ninves\u00adtigated on .nding near optimal register allocation by the conven\u00adtional graph coloring on an intersection \ngraph. It is well known that register allocation is equivalently reduced to the graph coloring problem \n[10], which is known to be NP-complete. For precise solu\u00adtion, it seems pessimistic; Kannan and Proesbsting \nshowed that the number of minimal coloring (thus deciding the minimum number of registers that can be \nallocated without spilling) is NP-complete even for SP2 (series parallel graphs) [19]. However, if we \nfurther assume that the number of registers is .xed, we can obtain an ef.cient solution. Bodlaender, \net.al. showed a linear-time algorithm to decide whether a program can be executed without spilling for \na .xed number of registers [8]. This is elegant in theory; however, their estimation includes the blow \nup of tree width of an intersection graph of a family of subgraphs. Thus, their constant factor explodes. \nOur method based on Optimization Theorem could also have a huge constant factor, which can grow to the \npower of the number of live variables. However, there is possibility to tame it. For instance, we could \nexpect the number of live variables at each program point are not so large, and most of markings would \nbe avoided imme\u00addiately. These observation suggest that, in practice, there seems a room to improve the \nconstant factor drastically by demand-driven computation and other program transformation techniques, \nwhich are available for functional programs. For instance, Ohori proposed another register allocation \nby proof transformation on a typed as\u00adsembly language, which reduces the number of candidates of opti\u00admal \nregister allocations [22]. The combination with such methods would be worth exploring. We should mention \nanother classical ef.cient solution under cer\u00adtain restriction of control .ow graphs: reducible .ow graph \n[2, 1]. If a program has a reducible control .ow graph, one can construct n (log n) algorithms for program \nanalyses, such as common subex\u00adpression detection. Knuth showed that most FORTRAN programs have reducible \ncontrol .ow graphs by an empirical study [20]. SP Term is independent to the concept of a reducible .ow \ngraph; for instance, Hecht and Ullman showed a graph is reducible if and only if the left-hand-side .gure \nin Fig. 11 is contained [17]. How\u00adever, that graph is easy to treat from tree width point of view; it \nis described in SP2 as S2 e2(1,2)(P2 (S2 (P2 e2(1,2) e2(2,1)) e2(1,2)) e2(2,1)) (with terminal 1 at the \ntop and terminal 2 at the rightmost node). In contrast, a complete directed acyclic graph (DAG) with \nm-nodes (as in the right-hand-side .gure in Fig. 11) is described in SPm, proportional to the size m. \nHowever, any DAG is reducible. \u00ac reducible . SP2  6 Conclusion and Future Work In this paper, we proposed \nan iterative-free approach to program analysis, based on the fact that control .ow graphs of most prac\u00adtical \nprograms are well structured. Our main contributions can be summarized as follows. We de.ned a simple \nbut powerful algebraic construction of digraphs called SP Terms, on which program analyses can be naturally \ndescribed as catamorphisms (or mutumorphism). As catamorphism enjoys many nice algebraic rules such as \nfusion and tupling for algorithmic optimization [6], this catamorphic formalization of program analyses \nmakes it possible to sys\u00adtematically derive ef.cient analysis algorithms, which has not been really recognized \nso far.  We identi.ed that many program analyses can be considered as the maximum marking problems. \nBy making use of the optimization theorem for them, we are able to obtain ef.cient analysis algorithms. \n As demonstrated by two examples, our method is quite power\u00adful. In fact, many program analysis examples \nin the compiler textbook can be cast into this framework.  This research is just at the beginning, and \nthere are lots of subjects to conquer. As pointed in Section 3, the table for dynamic programming technique \ncan easily explode. Although our method drasti\u00adcally improves constant factor compared to starting from \nfor\u00admulae [27], still this is quite true. However, we only need the computation that can reach to the \nresult satisfying given con\u00adstraints, and, from our experience, computation in most part of the table \ndoes not contribute to obtain such results. There\u00adfore, we hope demand-driven computation will improve \nthe situation, and would like to con.rm it by experiments.  Currently, our description of analyses is \nnot parametric wrt tree width k. However, as Section 4 suggests, the reachability description would work \nas a generic skeleton-like structure. We have a strong feel about it, but it must be more concrete. \n A k-terminal graph may have multiple representations by SP Terms. This means whether the user de.ned \nfunctional spec\u00adi.cation is consistent with the interpretation of SP Terms to k-terminal graphs is up \nto the user s responsibility. For in\u00adstance, in Fig. 5, different occurrences in the SP Term of a node \nin the control .ow graph have the same set of live vari\u00adables. This is guaranteed by user s semantic \nconsideration. From its own theoretical interest and possible better support, we hope to give the complete \naxiomatization of SP Terms un\u00adder this interpretation.  Acknowledgments The authors thank Oege de Moor \nand Jeremy Gibbons for stimu\u00adlating discussions during their visit at the University of Tokyo, and thank \nAki Takano for his helpful suggestions. We also thank anony\u00admous referees and Fritz Henglein for their \nvaluable comments and suggestions. Last but not least, we thank Masato Takeichi for his continuous support. \n 7 References [1] A. Aho, R. Sethi, and J.D. Ullman. Compilers Principles, Techniques, and Tools. Addison-Wesley, \n1986. [2] F.E. Allen. Control .ow analysis. ACM SIGPLAN Notices, 5(7):1 19, 1970. [3] S. Arnborg, B. \nCourcelle, A. Proskurowski, and D. Seese. An algebraic theory of graph reduction. Journal of the Associa\u00adtion \nfor Computing Machinery, 40(5):1134 1164, 1993. [4] M. Bauderon and B. Courcelle. Graph expressions and \ngraph rewritings. Mathematical System Theory, 20:83 127, 1987. [5] R. Bird. Maximum marking problems. \nJournal of Functional Programming, 11(4):411 424, 2001. [6] R. Bird and O. de Moor. Algebra of Programming. \nPrentice Hall, 1996. [7] H.L. Bodlaender. A linear-time algorithm for .nding tree\u00addecompositions of small \ntreewidth. SIAM Journal Computing, 25(6):1305 1317, 1996. [8] H.L. Bodlaender, J. Gustedt, and J.A. Telle. \nLinear-time reg\u00adister allocation for a .xed number of registers. In Proc. 9th ACM-SIAM Symposium on Discrete \nAlgorithms, SODA 1998, pages 574 583. ACM Press, 1998. [9] R.B. Borie, R.G. Parker, and C.A. Tovey. Automatic \ngenera\u00adtion of linear-time algorithms from predicate calculus descrip\u00adtions of problems on recursively \nconstructed graph families. Algorithmica, 7:555 581, 1992. [10] G.J. Chaitin. Register allocation &#38; \nspilling via graph coloring. In Proc. ACM Symposium on Compiler Construction, pages 98 105. ACM Press, \n1982. [11] B. Courcelle. Graph rewriting: An algebraic and logic ap\u00adproach. In J. van Leeuwen, editor, \nHandbook of Theoretical Computer Science, volume B, chapter 5, pages 194 242. El\u00adsevier Science Publishers, \n1990. [12] O. de Moor, D. Lacey, and E. van Wyk. Universal regular path queries. to appear in High Order \nSymbolic Computation, 2002. [13] M. Erwig. Functional programming with graphs. In Proc. 1997 ACM SIGPLAN \nInternational Conference on Functional Programming, pages 52 65. ACM Press, 1997. SIGPLAN Notices 32(8). \n[14] L. Fegaras and T. Sheard. Revisiting catamorphisms over datatypes with embedded functions (or, programs \nfrom outer space).In Proc. 23rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, \npages 284 294. ACM Press, 1996. [15] M. Fokkinga. Tupling and mutumorphisms. Squiggolist, 1(4), 1989. \n[16] J. Gustedt, O.A. M\u00e6hle, and A. Telle. The treewidth of Java programs. In Proc. 4th Workshop on Algorithm \nEngineering and Experiments, ALENEX 2002, pages 86 97, 2002. Lecture Notes in Computer Science, Vol. \n2409, Springer-Verlag. [17] M.S. Hecht and J.D. Ullman. Characterizations of reducible .ow graphs. Journal \nof the ACM, 21(3):367 375, 1974. [18] Z. Hu, H. Iwasaki, M. Takeichi, and A. Takano. Tupling cal\u00adculation \neliminates multiple data transversals. In Proc. 2nd ACM SIGPLAN International Conference on Functional \nPro\u00adgramming, pages 9 11. ACM Press, 1997. [19] S. Kannan and T. Proebsting. Register allocation in structured \nprograms. Journal of Algorithms, 29:223 237, 1998. [20] D.E. Knuth. An empirical study of FORTRAN programs. \nSoftware Practice and Experience, 1(2):105 134, 1971. [21] David Lacey, Neil D. Jones, Eric Van Wyk, \nand Carl Christian Frederiksen. Proving correctness of compiler optimizations by temporal logic. In Proc. \n29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, pages 283 294. ACM Press, 2002. \n[22] A. Ohori. Register allocation by program transformation. In Programming Languages and Systems, 12th \nEuropean Sym\u00adposium on Programming, ESOP03, pages 399 413, 2003. Lecture Notes in Computer Science, Vol. \n2618, Springer-Verlag. [23] L. Perkovi\u00b4c and B. Reed. An improved algorithm for .nding tree decompositions \nof small width. In Widmayer et al., edi\u00adtor, WG 99, pages 148 154, 1999. Lecture Notes in Computer Science, \nVol. 1665, Springer-Verlag. [24] L. Perkovi\u00b4c and B. Reed. An improved algorithm for .nding tree decompositions \nof small width. International Journal of Foundations of Computer Science, 11(3):365 371, 2000. [25] N. \nRobertson and P.D. Seymour. Graph minors II. algorith\u00admic aspects of tree-width. Journal of Algorithms, \n7:309 322, 1986. [26] I. Sasano, Z. Hu, and M. Takeichi. Generation of ef.cient programs for solving \nmaximum multi-marking problems. In Proc. 2nd International Workshop in Semantics, Applications, and Implementation \nof Program Generation, volume 2196, pages 72 91. Springer-Verlag, 2001. Lecture Notes in Com\u00adputer Science. \n[27] I. Sasano, Z. Hu, M. Takeichi, and M. Ogawa. Make it prac\u00adtical: A generic linear-time algorithms \nfor solving maximum\u00adweightsum problems. In Proc. 5th ACM SIGPLAN Interna\u00adtional Conference on Functional \nProgramming, pages 137 149. ACM Press, 2000. [28] I. Sasano, Z. Hu, M. Takeichi, and M. Ogawa. Derivation \nof linear algorithm for mining optimized gain association rules. Computer Software, 19(4):39 44, 2002. \n[29] H. Schmeck. Algebraic characterization of reducible .owcharts. Journal of Computer System Science, \n27(2):165 199, 1983. [30] D.A. Schmidt. Data .ow analysis is model checking of ab\u00adstract interpretations. \nIn Proc. 25th ACM SIGPLAN-SIGACT symposium on Principles of Programming Languages, pages 38 48. ACM Press, \n1998. [31] B. Steffen. Data .ow analysis as model checking. In Theo\u00adretical Aspects of Computer Science, \nvolume 526 of Lecture Notes in Computer Science, pages 346 364. Springer-Verlag, 1991. [32] K. Takamizawa, \nT. Nishizeki, and N. Saito. Linear-time computability of combinatorial problems on series-parallel graphs. \nJournal of the Association for Computing Machinery, 29:623 641, 1982. [33] M. Thorup. All structured \nprograms have small tree width and good register allocation. Information and Computation, 142:159 181, \n1998. A Computing SP Terms directly from Imper\u00adative Programs To show the direct translation from an \nimperative program with GOTO to an SP Term, we de.ne a simple imperative language. The de.nition, given \nin Figure 12, is similar to that in [21], except for the additional while construct. For simplicity, \nthis language has no exceptions or procedures. P ::= I; P program I ::= l : C instruction C ::= x := \ne assignment l ||||||: input x out put x if e then P else P . while e do P od goto l break label input \nstatement output statement conditional statement while loop goto statement break statement Figure 12. \nA Simple Imperative Language  Let us consider a program in this language with at most n-GOTO. The translation \ntransG to an SP Term is given below. The basic idea is; construct an SP Term by ignoring GOTO and memorize \ntheir source nodes as additional terminals. Then, scan the SP Term again, and add an edge by the parallel \ncomposition at some subterm in which the destination node eventually becomes a terminal. (Note that each \nnode in a control .ow graph becomes a terminal of some subterm of an SP Term.) Let prog be a program \nwritten in the language in Fig. 12. As in Section 2.1, we .rst preprocess prog to lprog by labeling each \nline of prog. Let ((so1,des1),\u00b7\u00b7\u00b7,(son,desn)) be the tuple of n-pairs of the source and destination nodes \nof each goto in lprog (We assume soi = desi for each i). Let lprog; be a program obtained from lprog \nby replacing goto with a null command skip. Then, lprog; is regarded as a .owchart program in Section \n2.1, and addG (nlift (trans l prog;)) where functions addG, nli ft, and trans are de.ned below. nlift \n:: SP2 .SPn+2 + nlift (e, (l1,l2)) = (en+2(1,n + 2), (l1,so1,\u00b7\u00b7\u00b7,son,l2)) - nlift (e, (l1,l2)) = (en+2(n \n+ 2,1), (l1,so1,\u00b7\u00b7\u00b7,son,l2)) nlift (2, (l1,l2)) = (2, (l1,so1,\u00b7\u00b7\u00b7,son,l2)) nlift (S2 xy, (l1,l2)) =(Sn+2 \n(permT (nlift x)) (n + 2, (l1,so2,so3,\u00b7\u00b7\u00b7,son,l2,$)) (n + 2, (l1,so1,so3,\u00b7\u00b7\u00b7,son,l2,$)) \u00b7\u00b7\u00b7 (n + 2, (l1,so1,so2,\u00b7\u00b7\u00b7,son-1,l2,$)) \n(nlift y), (l1,so1,\u00b7\u00b7\u00b7,son,l2)) nlift (P2 xy, (l1,l2) =(Pn+2 (nlift x)(nlift y), (l1,so1,\u00b7\u00b7\u00b7,son,l2)) \npermT :: SPn+2 .SPn+2 permT (Sn+2 x1 \u00b7\u00b7\u00b7 xn+2, (l1,\u00b7\u00b7\u00b7,ln+2)) =(Sn+2 (permT xn+1)(permT x1) \u00b7\u00b7\u00b7 (permT \nxn)(permT xn+2), (ln+1,l1,\u00b7\u00b7\u00b7,ln,ln+2)) permT (Pn+2 xy, (l1,\u00b7\u00b7\u00b7,ln+2)) =(Pn+2 (permT x)(permT y), (ln+1,l1,\u00b7\u00b7\u00b7,ln,ln+2)) \npermT (en+2(i, j), (l1,\u00b7\u00b7\u00b7,ln+2)) =(en+2((perm i), (perm j)), (ln+1,l1,\u00b7\u00b7\u00b7,ln,ln+2)) permT (n + 2, (l1,\u00b7\u00b7\u00b7,ln+2)) \n=(n + 2, (ln+1,l1,\u00b7\u00b7\u00b7,ln,ln+2)) perm :: Nat .Nat perm m = if m == (n + 2) then m else if m == (n + 1) \nthen 1 else m + 1 addG :: SPn+2 .SPn+2 addG (Sn+2 x1 \u00b7\u00b7\u00b7 xn+2, (l1,\u00b7\u00b7\u00b7,ln+2)) = addE (Sn+2 (addG x1) \n\u00b7\u00b7\u00b7 (addG xn+2), (l1,\u00b7\u00b7\u00b7,ln+2)) addG (Pn+2 xy, (l1,\u00b7\u00b7\u00b7,ln+2), (l1,\u00b7\u00b7\u00b7,ln+2)) = addE (Pn+2 (addG x)(addG \ny), (l1,\u00b7\u00b7\u00b7,ln+2)) addG (en+2(i, j), (l1,\u00b7\u00b7\u00b7,ln+2)) = addE (en+2(i, j), (l1,\u00b7\u00b7\u00b7,ln+2)) addG (n + 2, (l1,\u00b7\u00b7\u00b7,ln+2)) \n= addE (n + 2, (l1,\u00b7\u00b7\u00b7,ln+2)) addE :: SPn+2 .SPn+2 addE x@(t, (l,so1,\u00b7\u00b7\u00b7,son,l;)) = if (l == desj) || \n(l; == desj) then (Pn+2 x (en+2(so j,desj), (l,so1,\u00b7\u00b7\u00b7,son,l;)), (l,so1,\u00b7\u00b7\u00b7,son,l;)) else x The function \nnlift insert labels of goto statements as new n\u00adterminals between the .rst and the second (original) \nterminal in an SP Term; permT permutes except for the last terminal to adapt to the series composition. \nNext, addG adds an edge between the source and destination nodes of each goto-statement. Note that if \neach block has at most m-goto then instead of n (the sum of the numbers of goto) we can similarly transform \na control .ow graph to an SP Term in SPm+2.  \n\t\t\t", "proc_id": "944705", "abstract": "Program analysis is the heart of modern compilers. Most control flow analyses are reduced to the problem of finding a fixed point in a certain transition system, and such fixed point is commonly computed through an <i>iterative procedure</i> that repeats tracing until convergence.This paper proposes a new method to analyze programs through <i>recursive graph traversals</i> instead of iterative procedures, based on the fact that most programs (without <i>spaghetti</i> GOTO) have well-structured control flow graphs, <i>graphs with bounded tree width</i>. Our main techniques are; an algebraic construction of a control flow graph, called <i>SP Term</i>, which enables control flow analysis to be defined in a natural recursive form, and the <i>Optimization Theorem</i>, which enables us to compute optimal solution by dynamic programming.We illustrate our method with two examples; dead code detection and register allocation. Different from the traditional standard iterative solution, our dead code detection is described as a simple combination of bottom-up and top-down traversals on SP Term. Register allocation is more interesting, as it further requires optimality of the result. We show how the Optimization Theorem on SP Terms works to find an optimal register allocation as a certain dynamic programming.", "authors": [{"name": "Mizuhito Ogawa", "author_profile_id": "81100412378", "affiliation": "Japan Advanced Institute of Science and Technology", "person_id": "PP31041154", "email_address": "", "orcid_id": ""}, {"name": "Zhenjiang Hu", "author_profile_id": "81100253989", "affiliation": "The University of Tokyo", "person_id": "PP15027466", "email_address": "", "orcid_id": ""}, {"name": "Isao Sasano", "author_profile_id": "81100091875", "affiliation": "Japan Science and Technology Corporation, PRESTO", "person_id": "PP31026818", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/944705.944716", "year": "2003", "article_id": "944716", "conference": "ICFP", "title": "Iterative-free program analysis", "url": "http://dl.acm.org/citation.cfm?id=944716"}