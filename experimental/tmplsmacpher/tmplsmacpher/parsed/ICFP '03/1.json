{"article_publication_date": "08-25-2003", "fulltext": "\n Scripting the Type Inference Process Bastiaan Heeren Jurriaan Hage S. Doaitse Swierstra Institute of \nInformation and Computing Sciences, Utrecht University P.O.Box 80.089, 3508 TB Utrecht, The Netherlands \n {bastiaan,jur,doaitse}@cs.uu.nl Abstract To improve the quality of type error messages in functional \npro\u00adgramming languages, we propose four techniques which in.uence the behaviour of constraint-based type \ninference processes. These techniques take the form of externally supplied type inference di\u00adrectives, \nprecluding the need to make any changes to the compiler. A second advantage is that the directives are \nautomatically checked for soundness with respect to the underlying type system. We show how the techniques \ncan be used to improve the type error messages reported for a combinator library. More speci.cally, how \nthey can help to generate error messages which are conceptually closer to the domain for which the library \nwas developed. The techniques have all been incorporated in the Helium compiler, which implements a large \nsubset of Haskell. Categories and Subject Descriptors D.3.2 [Programming Languages]: Applicative (Functional) \nPro\u00adgramming; D.3.4 [Programming Languages]: Processors de\u00adbuggers; F.3.3 [Logics and Meanings of Programs]: \nStudies of Program Constructs type structure General Terms Languages  Keywords constraints, type inference, \ntype errors, directives, domain-speci.c programming 1 Introduction The important role of type systems \nin modern, higher-order, func\u00adtional languages such as Haskell and ML is well-established. Type Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 03, August \n25 29, 2003, Uppsala, Sweden. Copyright 2003 ACM 1-58113-756-7/03/0008 ...$5.00 systems not only guide \nthe novice programmer by pointing out er\u00adrors at compile-time, but they are equally indispensable to \nthe ad\u00advanced programmer, when writing complex programs. Unfortunately, clarity and conciseness are often \nlacking in the type errors reported by modern compilers. In addition, it is often not ap\u00adparent what \nmodi.cations are needed to .x an ill-typed program. For example, when using standard inference algorithms, \nthe re\u00adported error site and its actual source can be far apart. As a result, the beginning programmer \nis likely to be discouraged from pro\u00adgramming in a functional language, and may see the rejection of \nprograms as a nuisance instead of a blessing. The experienced user might not look at the messages at \nall. The problem is exacerbated in the case of combinator languages. Combinator languages are a means \nof de.ning domain speci.c lan\u00adguages embedded within an existing programming language, using the abstraction \nfacilities present in the latter. However, since the domain speci.c extensions are mapped to constructs \npresent in the underlying language, all type errors are reported in terms of the host language, and not \nin terms of concepts from the combinator library. In addition, the developer of a combinator library \nmay be aware of various mistakes which users of the library can make, something which he can explain \nin the documentation for the library, but which he cannot make part of the library itself. We have identi.ed \nthe following problems that are inherent to com\u00admonly used type inference algorithms. i. A .xed order \nof uni.cation: Typically, the type inferencer tra\u00adverses a program in a .xed order, and this order strongly \nin\u00ad.uences the reported error site. Moreover, a type inferencer checks type correctness for a given construct, \nsay function ap\u00adplication, in a uniform way. However, for some function ap\u00adplications it might be worthwhile \nto be able to depart from this .xed order. To overcome this problem, it should be possible to override \nthe order in which types are inferred by delaying certain uni.cations or changing the order in which \nsubexpres\u00adsions are visited. ii. The size of the mentioned types: Often, a substantial part of the types \nshown in a type error message is not relevant to the actual problem. Instead, it only distracts the programmer \nand makes the message unnecessarily verbose. The preservation of type synonyms where possible reduces \nthe impact of this problem. iii. The standard format of type error messages: Because of the general format \nof type error messages, the content is often not very poignant. Specialized explanations for type errors \naris\u00ading from speci.c functions would be an improvement for the following reasons. Firstly, domain speci.c \nterms can be used in explanations, increasing the level of abstraction. Secondly, depending on the complexity \nof the problem and the expected skills of the intended users, one could vary the verbosity of the error \nmessage. iv. No anticipation for common mistakes: Often, the designer of a library is aware of the common \nmistakes and pitfalls that occur when using the library. The inability to anticipate these pitfalls is \nregretful. This might take the form of providing additional hints, remarks and suggested .xes that come \nwith a type error. By way of example, consider the set of parser combinators by Swierstra [14], which \nwe believe is representative for most (com\u00adbinator) libraries. Figure 1 contains a type incorrect program \nfrag\u00adment to parse a lambda abstraction using the parser combinators (see Section 2 for a description \nof the latter). In the example, an expression is either an expression with operators which have the same \npriority as the boolean and operator (pAndPrioExpr), or the expression is a lambda abstraction which \nmay abstract a number of variables.1 The most likely error is indicated in comments in the example itself: \nthe subexpression <* pExpr indicates that an ex\u00adpression, the body of the lambda, should be parsed at \nthis point, but that the result (in this case an Expr) should immediately be dis\u00adcarded (as is the case \nwith the \"\\\\\" and the \"->\"). As a result, the constructor Lambda is only applied to a list of patterns, \nso that the second alternative in pExpr has result type Expr -> Expr.How\u00adever, the .rst alternative of \npExpr yields a parser with result type Expr and here the con.ict surfaces. Consider the type errors reported \nby Hugs in Figure 2 , and by GHC in Figure 3. Comparing the two messages with the third message which \nwas generated using our techniques, we note the following. It refers to parsers and not to more general \nterms such as func\u00adtions or applications. It has become domain speci.c and can solve problem (iii) to \na large extent.  It only refers to the incompatible result types.  The third message includes precise \nlocation information.  In addition, Hugs displays the unfolded non-matching type, but it does not unfold \nthe type of pAndPrioExpr, which makes the dif\u00adference between the two seem even larger. This is an instance \nof problem (ii). Note that if the Parser type had been de.ned using newtype (or data), then this problem \nwould not have oc\u00adcurred. However, a consequence of such a de.nition is that wher\u00adever parsers are used, \nthe programmer has to pack and unpack them. This effectively puts the burden on him and not on the compiler. \nIn the case of GHC, the error message explicitly states what the non\u00admatching parts in the types are. \nOn the other hand, it is not evident that the expected type originates from the explicit type signature \nfor pExpr. The expressions in the error message include paren\u00adtheses which are not part of the original \nsource. It is striking that the same long expression is listed twice, which makes the message more verbose \nwithout adding any information. Note that if, instead of applying the constructor Lambdato the result \nof the parser, we immediately apply an arbitrary semantic function, then the messages generated by Hugs \nand GHC become more com\u00adplex. Again, we see an instance of problem (ii). 1We assume here that we are \ndealing with a list of tokens, and not characters, but this is no essential difference. In Figure 5 we \nhave shown the absolute extreme of concision: when our facility for specifying so-called sibling functions \nis used, the in\u00adferencer discovers that replacing <* by the similar combinator <*> yields a type correct \nprogram. The fact that <*> and <* are sib\u00adlings is speci.ed by the programmer of the library, usually \nbecause his experiences are that these two are often mixed up. This kind of facility helps to alleviate \nproblem (iv). Please note that it is better to generate a standard type error here, and to give the probable \n.x as a hint. There is always the possibility that the probable .x is not the correct one, and then the \nuser needs more information. In the light of the problems just described, we present an approach that \ncan be used to overcome the problems for a given library. An important feature of our approach is that \nthe programmer of a such a library need not be familiar with the type inference process as it is implemented \nin the compiler: everything can be speci.ed by means of a collection of type inference directives, which \ncan then be distributed as part of the combinator library. If necessary, a user of such a library may \nadapt the directives to his own liking. An additional bene.t is that the type inference directives can \nbe auto\u00admatically checked for soundness with respect to the type inference algorithm present in the compiler. \nWe have implemented our techniques in the Helium compiler [8], which implements a large subset of Haskell; \nthe most notable omission is that of type classes. This compiler was constructed at Utrecht University \nwith a focus on generating high quality er\u00adror messages, and is used in an introductory course on functional \nprogramming. We expect that our techniques can be quite easily in\u00adcorporated into compilers which have \na constraint-based type infer\u00adence process with a clear separation between generating and solving constraints. \nThe paper is structured as follows. After a minimal introduction to the parser combinator library in \nSection 2, we propose solutions for the four problems just identi.ed (Section 3), and describe how to \nspecify the necessary type inference directives. In Section 4, we ex\u00adplain some technical details that \nare essential in making this work. Finally, Section 5 discusses related work, and Section 6 concludes \nthis paper.  2 Preliminaries In this section we brie.y describe the parser combinators which we use \nin our examples. Whenever necessary we explain why they are de.ned as they are, especially where this \ndeparts from what might be the more intuitive way of de.ning them. The parser com\u00adbinators [14] were \nde.ned to correspond as closely as possible to (E)BNF grammars, although the complete library provides \ncombi\u00adnators for many other often occurring patterns. Consider for the remainder of this section the \nHaskell declarations in Figure 6. The type Parser s a describes a parser which takes a list of symbols \nof type s and delivers a list of possible results (to cope with failing and ambiguous parsings). A result \nconsists of (the semantics of) whatever was parsed at this point, which is of type a, and the remainder \nof the input. The main combinators in our language are the operators <*>, <|> and <$>, and the parser \nsym. The .rst of these is the sequential composition of two parsers, the second denotes the choice between \ntwo parsers. We may recognize terminal symbols by means of the sym parser, which takes the symbol to \nrecognize as its parameter. To be able to parse a symbol, we have to be able to test for equal\u00ad data \nExpr = Lambda Patterns Expr --can contain more alternatives type Patterns = [Pattern] type Pattern = \nString pExpr :: Parser Token Expr pExpr = pAndPrioExpr <|> Lambda <$ pKey \"\\\\\" <*> many pVarid <* \npKey \"->\" <* pExpr --<* should be <*>  Figure 1. Type incorrect program ERROR \"Example.hs\":7 -Type \nerror in application  *** Expression : pAndPrioExpr <|> Lambda <$ pKey \"\\\\\" <*> many pVarid <* pKey \n\"->\" <* pExpr *** Term : pAndPrioExpr *** Type : Parser Token Expr *** Does not match : [Token] -> [(Expr \n-> Expr,[Token])]  Figure 2. Hugs, version November 2002 Example.hs:7: Couldn t match Expr against \nExpr -> Expr Expected type: [Token] -> [(Expr, [Token])] Inferred type: [Token] -> [(Expr -> Expr, [Token])] \n In the expression: (((Lambda <$ (pKey \"\\\\\")) <*> (many pVarid)) <* (pKey \"->\")) <* pExpr In the second \nargument of (<|>) , namely (((Lambda <$ (pKey \"\\\\\")) <*> (many pVarid)) <* (pKey \"->\")) <* pExpr Figure \n3. The Glasgow Haskell Compiler, version 5.04.3 Compiling Example.hs (7,6): The result types of the parsers \nin the operands of <|> don t match left parser : pAndPrioExpr result type : Expr right parser : Lambda \n<$ pKey \"\\\\\" <*> many pVarid <* pKey \"->\" <* pExpr result type : Expr -> Expr Figure 4. Helium, version \n1.1 (type rules extension) Compiling Example.hs (11,13): Type error in the operator <* probable fix: \nuse <*> instead  Figure 5. Helium, version 1.1 (type rules extension and sibling functions) infixl 7 \n<$>, <$ infixl 6 <*>, <* infixr 4 <|> type Parser s a = [s] -> [(a,[s])] <$>:: (a ->b) -> Parsersa \n-> Parsers b <*> :: Parser s (a -> b) -> Parser s a -> Parser s b <|> :: Parser s a -> Parser s a -> \nParser s a <$ ::a ->Parsersb->Parsersa <* :: Parser s a -> Parser s b -> Parser s a sym ::(s->s->Bool)->s \n->Parserss tok :: (s -> s -> Bool) -> [s] -> Parser s [s] option :: Parser s a -> a -> Parser s a many \n:: Parser s a -> Parser s [a] symbol :: Char -> Parser Char Char token :: String -> Parser Char String \n Figure 6. The parser combinators ity. In Haskell this is usually done by way of type classes, but we \nhave chosen to include the predicate explicitly. For the tech\u00adniques described in this paper, this makes \nno essential difference. For notational convencience we introduce symbol which works on characters. We \nnow have all we need to implement BNF grammars. For in\u00adstance the production P . QR | a might be transformed \nto the Haskell expression pP =pQ <*> pR <|> symbol a . The type of each of the parsers pQ <*> pR and \nsymbol a must of course be the same, as evidenced by the type of <|>. The type of the combinator <*> \nspeci.es that the .rst of the two parsers in the composition delivers a function which can be applied \nto the result of the second parser. An alternative would be to return the tupled results, but this has \nthe drawback that, with a longer sequence of parsers, we obtain deeply nested pairs which the semantic \nfunctions have to unpack. Usually, one uses the <$> combinator to deal with the results that come from \na sequence of parsers: pP =f <$> pQ <*> pR <|> symbol a , where f is a function which takes a value of \nthe result type of pQ and a second parameter which has the result type of pR, delivering a value which \nshould have type Char (because of the symbol a parser). The operator <$> has a higher priority than <*>, \nwhich means that the .rst alternative for <|> should be read as ((f <$> pQ) <*> pR). A basic property \nof <$> is that it behaves like function application, consuming its arguments one by one. How\u00adever, this \nis generally not the way parser builders read their parsers. Usually, the parser is constructed .rst, \nand the semantic functions are added afterwards. This difference in perception is one of the sources \nof confusion when people use the parser combinators. In the following section we describe how our techniques \ncan help to alleviate this problem. For <$> and <*>, we have variants <$ and <*, which discard the result \nof their right operand. This is useful when what is parsed needs only to be recognized, but is not used \nlater on. An example of this can be found in the code of Figure 1, which throws away whatever comes out \nof either pKey application. This means that we can simply apply the constructor Lambda, instead of a \nfunction which takes four parameters and throws two of them away. In the same example, we see an application \nof the many combina\u00adtor which recognizes a list of things, in this case pVarIds. Here, pVarId is a prede.ned \nparser which recognizes an identi.er. The pKey combinator is used to recognize keywords and reserved \noper\u00adators (which have been tokenized already so we do not have to deal with whitespace). Finally, the \ntoken combinator is a combination of many and symbol, and the option combinator is used when a parser \ncan recognize the empty word. For example, option (token \"hello!\") \"\" either recognizes the string \"hello!\" \nand returns it, or it succeeds without consuming any input. In the latter case, the option parser returns \nthe empty string instead. Note that in Haskell strings are de.ned to be equivalent to lists of characters. \n  3 Type inference directives This section describes four techniques that help to improve the qual\u00adity \nof type error messages. To start with, we present a notation to de.ne your own type rules with their \nspecialized type error mes\u00adsages. Next we explain why .exibility in the order of uni.cation is absolutely \nnecessary in order to get appropriate messages. The .nal two examples deal with common mistakes. We have \nimplemented all four techniques in the Helium com\u00adpiler [8]. In Helium, the directives for a module Name.hs \nare col\u00adlected in the .le Name.type, which is automatically loaded when Name.hs is imported. 3.1 Specialized \ntype rules This section describes how to write specialized type rules and ex\u00adplains how this in.uences \nthe error reporting in case a type rule fails. There are serious disadvantages to incorporating these \nrules directly in a type inferencer. It requires training and experience to extend an existing type inferencer, \nand it implies a loss of composi\u00adtionality and maintainability of the implemented type rule. Since the \ncorrectness of a type inferencer is quite a subtle issue, it is no longer possible to guarantee that \nthe underlying type system re\u00admains unchanged. Instead, we follow a different approach in which the type \nrules can be speci.ed externally. This makes it relatively easy to specify a type rule and to experiment \nwith it without having to change any code of the type inference engine. Specializing a type rule Let \nus take a closer look at a traditional type inference rule for in.x application. An in.x application \nis type correct if the types of the two operands .t the type of the operator. Clearly, in.x application \nis nothing but syntactic sugar for normal pre.x function application. Applying the type rule for function \napplication twice in succession results in the following. G nHM op : t1 . t2 . t3 G nHM x : t1 G nHM \ny : t2 G nHM x op y : t3 Here, G nHM e : t means that under the type environment G we can assign type \nt to expression e [4]. Instead of using this general type rule to deal with in.x applications, one could \ncome up with a more speci.c type rule for a particular operator, for instance <$>. Let <$> be part of \nthe type environment G, and have type signature (a .b) .Parser s a .Parsersb. Then we can write down \nthe following specialized type rule. G nHM x : t1 .t2 G nHM y : Parser t3 t1 G nHM x <$> y : Parser t3 \nt2 If we encounter a local de.nition of <$>, then the above type rule will not be used within the scope \nof that local de.nition, even if the new de.nition of <$> has the same type as the old one. To avoid \nconfusion, we only want to use the type rule if the very same operator, here <$>, is used. The type rule \ndoes not adjust the scope, as can be concluded from the fact that the same type environment G is used \nabove and below the line. In the rest of this paper we will only consider specialized type rules with \nthis property. This limitation is necessary to avoid complications with monomorphic and polymorphic types. \nSince the type environment remains unchanged, we will omit it from now on. In general, a type rule contains \na number of constraints, on each of which a type inferencer may fail. For instance, the inferred types \nfor the two operands of <$> are restricted to have a speci.c shape (a function type and a Parser type), \nthe relations between the three type variables constrain the inferred types even further and, lastly, \nthe type in the conclusion must .t into its context. To obtain a better understanding why some inferred \ntypes may be inconsistent with this type rule, let us reformulate the type rule to make the type constraints \nmore explicit. . . t1 =a .b x : t1 y : t2 t2 =Parsersa x <$> y : t3 . t3 =Parsersb An equality constraint, \nwritten t =t', can be thought of as the uni\u00ad.cation of two types. Algorithms that determine the most \ngeneral uni.er of two types are well understood. In addition to the type variables introduced in the \ntype rule, three more type variables are introduced in the constraint set, namely a, b, and s. The order \nin which the constraints are solved is irrelevant for the success or failure of the type inference process. \nHowever, the order chosen does determine where the type inferencer .rst no\u00adtices an inconsistency. Typically, \nthe order is determined by the type inference algorithm that one prefers, e.g., the standard bottom\u00adup \nalgorithm W [4] or the folklore top-down algorithm M [10]. To acquire additional information, we split \nup each constraint in a number of more basic type constraints. The idea of these small uni.cation-steps \nis the following: for a type constraint that cannot be satis.ed, the compiler can produce a more speci.c \nand detailed error message. The example now becomes . . t1 =a1 .b1 s1 =s2 x : t1 y : t2 t2 =Parser s1 \na2 a1 =a2 x <$> y : t3 . t3 =Parser s2 b2 b1 =b2 The de.nition of a type rule, as included in a .type \n.le, consists of two parts, namely a deduction rule and a list of constraints. The deduction rule consists \nof premises, which occur above the line, and a conclusion below the line. A premise consists of a single \nmeta-variable, which matches with every possible expression, and a type. On the other hand, the conclusion \nmay contain an arbitrary expression, except that lambda abstractions and let expressions are not allowed, \nbecause they modify the type environment. There is no restriction on the types in the premises and the \nconclusion. Below the deduction rule, the programmer can list a number of equality constraints between \ntypes. Each of these is followed by a corre\u00adsponding error message. Example 1. We present a special type \nrule for the <$> combinator. Each of the constraints is speci.ed with an error message that is reported \nif the constraint cannot be satis.ed. The order in which the constraints are listed determines the order \nin which they shall be considered during the type inference process. x :: t1; y :: t2; x <$> y :: t3; \n t1 == a1 -> b1 : left operand is not a function t2 == Parser s1 a2 : right operand is not a parser t3 \n== Parser s2 b2 : result type is not a parser s1 == s2 : parser has an incorrect symbol type a1 == a2 \n: function cannot be applied to parser s result b1 == b2 : parser has an incorrect result type Now take \na look at the following function de.nition, which is clearly ill-typed. test :: Parser Char String test \n= map toUpper <$> \"hello, world!\" Because it is pretty obvious which of the six constraints is vi\u00adolated \nhere (the right operand of <$> is not a parser, hence, t2 =Parser s1 a2 cannot be satis.ed), the following \nerror is re\u00adported. Type error: right operand is not a parser Note that this type error message is still \nnot too helpful since impor\u00adtant context speci.c information is missing, such as the location of the \nerror, pretty-printed parts of the program, and con.icting types. To overcome this problem, we use attributes \nin the speci.cation of error messages. Error message attributes A .xed error message for each constraint \nis too simplistic. The main focus of a message should be the contradicting types that caused the uni.cation \nalgorithm to fail. To construct a clear and concise message, one typically needs the following information. \n The inferred types of the subexpressions: One should be able to refer to the actual type of a type variable \nthat is mentioned in either the type rule or the constraint set. In the special case that a subexpression \nis a single identi.er which is assigned a polymorphic type, then we prefer to display this generalized \ntype instead of simply using the instantiated type.  A pretty-printed version of the expression and \nits sub\u00adexpressions: This should resemble the actual code as closely as possible, and should (preferably) \n.t on a single line.  Position and range information: This also includes the name and location of the \nsource .le at hand.  Example 2. To improve the error message of Example 1, we re\u00adplace the annotation \nof the type constraint t2 == Parser s1 a2 : right operand is not a parser by the following error message, \nwhich contains attributes. t2 == Parser s1 a2 : @expr.pos@: The right operand of <$> should be a parser \n expression : @expr.pp@ right operand : @y.pp@ type : @t2@ does not match : Parser @s1@ @a2@ In the \nerror message, the expression in the conclusion is called expr. We can access its attributes by using \nthe familiar dot no\u00adtation, and surrounding it by @ signs. For example, @expr.pos@ refers to the position \nof expr in the program source. Similarly, pp gives a pretty printed version of the code. The speci.cation \nof a type constraint and its type error message is layout-sensitive: the .rst character of the error \nreport (which is a @ in the example above) determines the level of indentation. The de.nition of the \nerror report stops at the .rst line which is indented less. As a result, the error report for the de.nition \nof test in Ex\u00adample 1 now becomes: (2,21): The right operand of <$> should be a parser expression : \nmap toUpper <$> \"hello, world!\" right operand : \"hello, world!\" type : String does not match : Parser \nChar String For a given expression (occurring in the conclusion of a type rule), the number of type \nconstraints can be quite large. We do not want to force the user to write out all these constraints and \ngive corre\u00adsponding type error messages. For this reason, the user is allowed to move some constraints \nfrom the list below the type rule to the type rule itself, as we illustrate in the next example. Example \n3. We continue with Example 1. Because we prefer not to give special error messages for the case that \nthe result type is not a parser, we may as well give the following type rule. x :: t1; y :: t2; x <$> \ny :: Parser s b; t1 == a1 -> b : left operand is not a function t2 == Parser s a2 : right operand is \nnot a parser a1 == a2 : function cannot be applied to parser s result At this point, only three of the \noriginal type constraints remain. If any of the removed constraints contributes to an inconsistency, \nthen a standard error message will be generated. These constraints will be considered before the explicitly \nlisted constraints. Order of the type constraints In the type rule speci.cations we have so far only \nlisted the con\u00adstraints for that rule and the order in which they should be consid\u00adered. In principle, \nwe do not assume that we know anything about how the type inferencer solves the constraints. The only \nthing a type rule speci.es is that if two of the constraints contribute to an incon\u00adsistency, then the \n.rst of these will be considered to be the source of the error. An error report will be generated for \nthis constraint, after which the type inference process continues. The situation is not as simple as \nthis. Each of the meta-variables in the rule corresponds to a subtree of the abstract syntax tree for \nwhich sets of constraints are generated. How should the constraints of the current type rule be ordered \nwith respect to these constraints? <*> <*> r <$>  <$> qf <*> f p pqr Figure 7. Abstract syntax tree \n(left) compared with the concep\u00adtual structure (right) Example 4. If we want the constraints generated \nby the subexpres\u00adsion y to be considered after the constraint t1 == a1 -> b, then we should change the \ntype rule in Example 3 to the following. x :: t1; y :: t2; x <$> y :: Parser s b; constraints x t1 \n== a1 -> b : left operand is not a function constraints y t2 == Parser s a2 : right operand is not a \nparser a1 == a2 : function cannot be applied to parser s result Note that in this rule we have now explicitly \nstated at which point the constraints of x and y should be considered. By default, the sets of constraints \nare considered in the order of the correspond\u00ading meta-variables in the type rule, to be followed afterwards \nby the constraints listed below the type rule. Hence, we could have omitted constraints x. By supplying \ntype rules to the type inferencer we can adapt the behaviour of the type inference process. It is fair \nto assume that the extra type rules should not have an effect on the underlying type system, especially \nsince an error in the speci.cation of a type rule is easily made. We have made sure that user de.ned \ntype rules that con.ict with the default type rules are automatically rejected at compile time. A more \nelaborate discussion of this subject can be found in Section 4.2. 3.2 Phasing Recall the motivation \nfor the chosen priority and associativity of the <$> and <*> combinators: it allows us to combine the \nre\u00adsults of arbitrary many parsers with a single function in a way that feels natural for functional \nprogrammers, and such that the number of parentheses is minimized in a practical situation. However, \nthe abstract syntax tree that is a consequence of this design principle differs considerably from the \nview that we suspect many users have of such an expression. Unfortunately, the shape of the abstract \nsyn\u00adtax tree strongly in.uences the type inference process. As a con\u00adsequence, the reported site of error \nfor an ill-typed expression in\u00advolving these combinators can be counter-intuitive and misleading. Ideally, \nthe type inferencer should follow the conceptual perception rather than the view according to the abstract \nsyntax tree. Phasing by example Let f be a function, and let p, q, and r be parsers. Consider the following \nexpression. f <$> p <*> q <*> r Figure 7 illustrates the abstract syntax tree of this expression and \nits conceptual view. How can we let the type inferencer behave according to the conceptual structure? \nA reasonable choice would be to treat it in a similar way as a non-curried function application, that \nis, .rst infer a type for the function and all its arguments, and then unify the function and argument \ntypes. We can identify four steps if we apply the same idea to the parser combinators. Note that the \nfour step process applies to the program as a whole. 1 Infer the types of the expressions between the \nparser combi\u00adnators. 2 Check if the types inferred for the parser subexpressions are indeed Parser types. \n3 Verify that the parser types can agree upon a common symbol type. 4 Determine whether the result types \nof the parser .t the func\u00adtion. One way to view the four step approach is that all parser related uni.cations \nare delayed. Consequently, if a parser related con\u00adstraint is inconsistent with another constraint, then \nthe former will be blamed. Example 5. The following example presents a type incorrect at\u00adtempt to parse \na string followed by an exclamation mark. test :: Parser Char String test = (++) <$> token \"hello world\" \n <*> symbol ! The type error message of Hugs is not too helpful here. ERROR \"Phase1.hs\":4 -Type error \nin application *** Expression : (++) <$> token \"hello world\" <*> symbol ! *** Term : (++) <$> token \"hello \nworld\" *** Type : [Char] -> [([Char] -> [Char],[Char] )] *** Does not match : [Char] -> [(Char -> [Char],[Char])] \n The four step approach might yield: (1,7): The function argument of <$> does not work on the result \ntypes of the parser(s) function : (++) type : [a] -> [a] -> [a] does not match : String -> Char -> \nString Observe the two major improvements. First of all, it focuses on the problematic function, instead \nof mentioning an application. Sec\u00adondly, the types do not involve the complex expanded Parser type synonym, \nnor do they contain the symbol type of the parsers, which in this example is irrelevant information. \nAssigning phase numbers Delaying the satisfaction of constraints can be achieved by annota\u00adtions with \na phase number. This phase number in.uences the order in which the constraints are solved. The constraints \nin phase num\u00adber i are solved before the constraint solver continues with the con\u00adstraints of phase i \n+ 1. Consequently, phasing has a global effect on the type inference process. Adding the keyword phase, \nfollowed by a phase number, will as\u00adsign the constraints after this directive to this phase. By default, \nconstraints are assigned to phase 5, leaving space to introduce new phases. Of course, the constraints \nof a type rule can be assigned to different phases. Example 6. We introduce phases numbered from 6 to \n8 for the steps 2, 3, and 4 respectively. We assign those phase numbers to the constraints in the specialized \ntype rule for <$>. Note that step 1 takes place in phase 5, which is the default. No constraint gener\u00adated \nby the following type rule will be solved in phase 5. x :: t1; y :: t2; x <$> y :: t3; phase 6 t2 == \nParser s1 a2 : right operand is not a parser t3 == Parser s2 b2 : result type is not a parser phase 7 \ns1 == s2 : parser has an incorrect symbol type phase 8 t1 == a1 -> b1 : left operand is not a function \na1 == a2 : function cannot be applied to parser s result b1 == b2 : parser has an incorrect result type \n One may wonder what happens when the sets constraints x and constraints yare included among the listed \nconstraints. Because phasing is a global operation, the constraints in these sets continue to keep their \nown assigned phase number. Sometimes the opposite approach is desired: to verify the correct\u00adness of \nthe parser related uni.cations before continuing with the rest of the program. This technique is similar \nto pushing down the type of a type declaration as an expected type, a useful technique applied by, for \ninstance, the GHC compiler. Example 7. Let us take another look at the ill-typed function de.\u00adnition \ntest in Example 1. test :: Parser Char String test = map toUpper <$> \"hello, world!\" If the constraints \nintroduced by the type rule for <$> are assigned to an early phase, e.g. 3, then, effectively, the right \noperand is im\u00adposed to have a Parser type. Since \"hello, world!\" is of type String, it is at the location \nof this literal that we report that a dif\u00adferent type was expected by the enclosing context. By modifying \nthe .type .le along these lines, we may obtain the following error message. (2,21): Type error in string \nliteral expression : \"hello, world!\" type : String expected type : Parser Char String  3.3 Sibling \nfunctions This section and the next one deal with anticipating common mis\u00adtakes. Although some mistakes \nare made over and over again, the quality of the produced error reports can be unsatisfactory. In some \ncases it is possible to detect that a known pitfall resulted in a type error message. If so, a more speci.c \nmessage than the standard one should be presented, preferably with hints to solve the problem. One typical \nmistake that leads to a type error is confusing two func\u00adtions that are somehow related. For example, \nnovice functional pro\u00adgrammers have a hard time remembering the difference between inserting an element \nin front of a list (:), and concatenating two lists (++). Even experienced programmers may mix up the \ntypes of curry and uncurry now and then. Similar mistakes are likely to occur in the context of a combinator \nlanguage. We will refer to such a pair of related functions as siblings. The idea is to suggest replacing \na function with a sibling function if this resolves the type error. The types of two siblings should \nbe distinct, since we cannot distinguish the differences based on their semantics. Example 8. Consider \nthe parser combinators from Section 2, and, in particular, the special variants that ignore the result \nof the right operand parser. These combinators are clearly siblings of their basic combinator. A closer \nlook to the program in Figure 1 tells us that the most likely source of error is the confusion over the \ncombinators <*> and <*. The observation that replacing one <*combinator by <*> results in a type correct \nprogram paves the way for a more appropriate and considerably simpler error message. A function can have \nmultiple sibling functions, but the sibling re\u00adlation is not necessarily transitive. Furthermore, a sibling \nfunction should only be suggested if replacement completely resolves a type error. Moreover, the suggested \nfunction should not only match with its arguments, but it should also .t the context to prevent misleading \nhints. Implementing this in a traditional type inference algorithm can be quite a challenge. A practical \nconcern is the runtime be\u00adhaviour of the type inferencer in the presence of sibling functions. Ideally, \nthe presence of sibling functions should not affect the type inference process for type correct programs; \nonly for type incor\u00adrect programs is some extra computation performed, and only for operators that contribute \ndirectly to the type error. In Section 4.3 we discuss type graphs, a .exible data structure that is powerful \nenough to handle sibling functions. A set of sibling functions can be declared in the .le containing \nthe type inference directives by giving a comma separated list of func\u00adtions. siblings <$> , <$ siblings \n<*> , <* The type error that is constructed for the program in Figure 1 can be found in Figure 5. A \nmore conservative approach would be to show a standard type error message, and add the probable .x as \na hint. 3.4 Permuted arguments Another class of problems is the improper use of a function, such as \nsupplying the arguments in a wrong order, or mistakenly pair\u00ading arguments. McAdam discusses the implementation \nof a system that tackles these problems by unifying types modulo linear iso\u00admorphism [12]. Although we \nare con.dent that these techniques can be incorporated into our own system, we limit ourselves to a small \nsubset, that is, permuting the arguments of a function. Example 9. The function option expects a parser \nand a result that should be returned if no input is consumed. But in which order should the arguments \nbe given? Consider the following program and its type error message. test :: Parser Char String test \n= option \"\" (token \"hello!\") ERROR \"Swapping.hs\":2 -Type error in application *** Expression : option \n\"\" (token \"hello!\") *** Term : \"\" *** Type : String *** Does not match : [a] -> [([Char] -> [(String,[Char]) \n],[a])] The error message does not guide the programmer in .xing his pro\u00adgram. In fact, it assumes the \nuser knows that the non-matching type is equal to Parser a (Parser Char String). Instead of having to \nspecify for each function, whether you want the type inferencer to attempt to resolve an inconsistency \nby per\u00admuting the arguments to the function, our type inferencer does this by default. A conservative \ntype error message for the program of Example 9 would now be: (2,8): Type error in application expression \n: option \"\" (token \"hello!\") term : option type : Parser a b -> b -> Parser a b does not match : String \n-> Parser Char String -> c probable fix : flip the arguments If, for a given type error, both the method \nof sibling functions and permuted arguments can be applied, then preference is given to the former. In \na class room setting, we have seen that the permuted arguments facility gives useful hints in many cases. \nHowever, we are aware that sometimes it may result in misleading information from the compiler. During \na functional programming course we have col\u00adlected enough information to determine how often this occurs \nin a practical setting. The data remains to be analyzed.  4 Technical details In this section we brie.y \ndiscuss the way in which our type rules are applied, and the machinery that we use to test for sibling \nfunctions and permuted function arguments. 4.1 Applying specialized type rules In Section 3.1 we introduced \nnotation to de.ne specialized type rules for combinator libraries. Typically, a set of type rules is \ngiven to cover the existing combinators and possibly some more complex combinations of these. Since we \ndo not want to forbid overlapping patterns in the conclusions of the type rules, we have to be more speci.c \nabout the way we apply type rules to a program. The abstract syntax tree of the program is used to .nd \nmatching patterns. We look for .tting patterns starting at the root of this tree, and continue in a top-down \nfashion. In case more than one pattern can be applied at a given node, we select the type rule which \noccurs .rst in the .type .le. Consequently, nested patterns should be given before patterns that are \nmore general. This .rst-come .rst\u00adserved way of dealing with type rules also takes place when two combinator \nlibraries are imported: the type rules of the combinator library which is imported .rst have precedence. \nExample 10. Matching the patterns on the abstract syntax tree of the program involves one subtle issue. \nConsider the following pat\u00adterns, in the given order. name pattern meta-variables R1 f <$> p <*> qf , \np, and q R2 p <*> q <*> rp, q, and r R3 f <$> pf and p What happens if we apply these rules to the code \nfragment test = fun <$> a <*> b <*> c. At .rst sight, rule R1 seems to be a possible candidate to match \nthe right-hand side of test. However, following the chosen priority and associativity of the operators, \nthe second rule matches the top node of the abstract syntax tree, which is the rightmost <*>. Meta\u00advariable \np in R2 matches with the expression fun <$> a. Since this subexpression matches rule R3, we apply another \ntype rule. 4.2 Correctness of specialized type rules In Heeren, Hage and Swierstra [7] we proved the \ncorrectness of the underlying type system. Before using a specialized type rule, we verify that it does \nnot change the type system. We think that such a feature is essential, because a mistake is easily made. \nWe do this by ensuring that for a given expression in the deduction rule, the con\u00adstraints generated \nby the type system and the constraints generated from the specialized type rule are equivalent. A specialized \ntype rule allows us to in.uence the order in which constraints are solved. The order of solving constraints \nis irrelevant except that the constraints for let de.nitions should be solved before continuing with \nthe body [7]. If we make sure that all our special\u00adized type rules respect this fact, then the correctness \nof the new type rules together with the underlying type system is guaranteed. Validation of a specialized \ntype rule A type rule is validated in two steps. In the .rst step, a type rule is checked for various \nrestrictions. The (expression) variables that oc\u00adcur in the conclusion can be divided into two classes: \nthe variables that are present in a premise, which are the meta-variables, and the variables that solely \noccur in the conclusion. Each meta-variable should occur exactly once in the conclusion, and it should \nnot be part of more than one premise. Every non-meta-variable in the con\u00adclusion should correspond to \na top-level function inside the scope of the type rule s module. The type signature of such a function \nshould be known a priori. If none of the restrictions above is violated, then we continue with the second \nstep. Here, we test if the type rule is a specialized ver\u00adsion of the standard type rules present in \nthe type system. Two types are computed: one for which the type rule is completely ignored, and one type \naccording to the type rule. The type rule will be added to the type system if and only if the types are \nequivalent (up to the renaming of type variables). Before we discuss how to check the soundness of a \ntype rule, we present an example of an invalid type rule. Example 11. Take a look at the following type \nrule. x :: t1; y :: t2; x <$> y :: Parser s b; t1 == a1 -> b : left operand is not a function t2 == \nParser s a2 : right operand is not a parser Because the programmer forgot to specify that xshould work \non the result type of y, the type rule above is not restrictive enough. Thus, it is rejected by the type \nsystem with the following error message. The type rule for \"x <$> y\" is not correct the type according \nto the type rule is (a -> b, Parser c d, Parser c b) whereas the standard type rules infer the type (a \n-> b, Parser c a, Parser c b) Note that the 3-tuple in this error message lists the type of x, y and \nx <$> y, which re.ects the order in which they occur in the type rule. To determine this, we .rst ignore \nthe type rule, and use the default type inference algorithm. Let G be the current type environment to \nwhich we add all meta-variables from the type rule, each paired with a fresh type variable. For the expression \ne in the conclusion, the type inference algorithm will return a type t and a substitution S such that \nSGnHM e : St. Let f be (S\u00df1,...,S\u00dfn,St), where \u00dfi is the fresh type variable of the ith meta-variable. \nThe order of the meta-variables is irrelevant, but it should be consistent. Example 11 (continued). Let \ne =x <$> y, and construct a type environment . . <$>:.s,a,b . (a .b).Parsersa .Parser s b G = x :\u00df1 . \ny:\u00df2 Given G and e, the default type inference algorithm returns a most general uni.er S and a type t. \nS =[\u00df1:=\u00df3 .\u00df4,\u00df2:=Parser \u00df5 \u00df3] t =Parser \u00df5 \u00df4 As a result we .nd that f =(\u00df3 .\u00df4,Parser \u00df5 \u00df3,Parser \n\u00df5 \u00df4) Next, we use the type rule and ignore the standard type system. Let G'be the type environment \ncontaining all top-level de.nitions of which the type is known at this point, and let C be the set of \ntype constraints given in the type rule. Compute a most general substi\u00adtution S that satis.es C , and \nlet . be (St1,...,Stn,St), where ti is the type of the ith meta-variable, and t is the type of the conclusion. \nThe type rule is consistent with the default type system if and only if f and . are equivalent up to \nvariable renaming. Example 11 (continued). Let G'be a type environment that con\u00adtains <$> with its type \nscheme, and let C be the two constraints speci.ed in the type rule. Next, we compute S =[t1 :=a1 .b,t2 \n:=Parser s a2] and, consequently, . =(a1 .b,Parser s a2,Parsersb). Because fa. we reject the examined \ntype rule. = In theory, there is no need to reject a type rule that is more speci.c than the default \ntype rules, since it will not make the type system unsound. Our implementation of specialized type rules, \nhowever, cannot cope with more speci.c type rules for the reason that the constraints implied by the \ntype rule replace the constraint collection from the standard type system. Permitting a type rule that \nis too restrictive can result in the rejection of type correct programs. Example 12. Although the following \nspecialized type rule is sound, it is rejected because it is too restrictive in the symbol type. It is \nimportant to realize that applying the type rule to expressions of the form x <$> y is not in.uenced \nby the type mentioned in the type rule, here Parser Char b. Note that this specialized type rule has \nan empty set of type constraints. x :: a -> b; y :: Parser Char a; x <$> y :: Parser Char b; Phasing \nand let expressions Phasing the type inference process gives a great degree of freedom to order the constraints, \nbecause it is a global operation. How\u00adever, this freedom is restricted by the correct treatment of let\u00adpolymorphism. \nThe type scheme of an identi.er de.ned in a let should be computed before it is instantiated for occurrences \nof that identi.er in the body. This imposes a restriction on the order since the constraints corresponding \nto the let-de.nition should be solved earlier than the constraints that originate from the body. Example \n13. Consider the following function. maybeTwice = let p = map toUpper <$> token \"hello\" in option ((++) \n<$> p <*> p) [] A type scheme should be inferred for p, before we start inferring a type for maybeTwice. \nTherefore, all type constraints that are col\u00adlected for the right-hand side of p are considered before \nthe con\u00adstraints of the body of the let, thereby ignoring assigned phase num\u00adbers. Of course, phasing \nstill has an effect inside the local de.nition as well as inside the body. Note that if we provide an \nexplicit type declaration for p, then there is no reason to separate the constraint sets of p and the \nbody of the let. 4.3 Type graphs To conclude our technical discussion, we want to say something about \nthe use of type graphs which allowed us to easily implement our scriptable type inferencer. The implementation \nis based on type graphs which we describe in a technical report [7]. Essentially, a type graph is an \nadvanced data structure to represent substitutions, which also keeps track of the reasons for a uni.cation. \nType graphs allow us to solve the collected type constraints in a more global way, so that we suffer \nless from the notorious left-to-right bias present in most type inference algorithms. Type graphs have \nthe following advantages. All the justi.cations for a particular shape of an inferred type remain available. \nSimilarly, all the uni.cation steps that con\u00adtribute to a type error are still accessible in a type graph. \nFor example, it is easy to compute a minimal set of program points contributing to a given type error, \nor to trace the origin of a type constructor.  Since we solve a set of constraints at once, we can discover \nglobal properties.  Because a type graph can be in an inconsistent state, the con\u00adstruction of a type \ngraph is separated from the removal of in\u00adconsistencies. This makes it easier to plug in heuristics, \nsuch as the approach of Walz [15].  Type graphs allow us to locally add and remove sets of con\u00adstraints. \nIf the use of (++) in an expression contributes to an error, then we can remove the corresponding constraints \nand add those of one of its siblings, e.g. (:), instead to see whether this solves the problem. Permuted \narguments can also be implemented by local modi.cations to the type graph.  However, there is a tradeoff \nbetween quality and performance. Ob\u00adviously, the extra overhead caused by the type graph increases the \ncompile time of the system. For normal sized programs, the price to be paid remains within reasonable \nproportions. For instance, type graphs have been implemented successfully in a concrete educa\u00adtional \nsetting, and are part of the Helium Compiler [8]. Although benchmarks with up to 1400 lines of code have \nbeen typed within reasonable time, it is unknown to us to what extent the overhead becomes problematic \nfor larger modules.  5 Related work The poor type error messages produced by most modern compil\u00aders \nare still a serious obstacle to appreciate higher-order, functional programming languages. Several approaches \nhave been proposed to improve the quality of type error messages. Because recent ex\u00adtensions to the type \nsystem appear to have a negative effect on the clarity of the error reports, it is becoming more important \nto under\u00adstand the dif.culties of type inference in full detail. It is well understood that the reported \nsite of error is greatly in.u\u00adenced by the order in which types are uni.ed. Several modi.cations of the \nuni.cation order in the standard algorithm W [4] have been presented, among which the top-down algorithm \nM [10]. General\u00adizations of these algorithms exist [7, 11]. To remove the left-to-right bias, Yang [9] \npresents algorithm UAE, which uni.es types in the assumption environment. Nevertheless, an algorithm \nwith a .xed order of uni.cation can never be satisfactory for all inputs. A number of papers attempt \nto explain the type inference process by tracing type uni.cations [2, 5]. Unfortunately, lots of type \nvariables are involved in the explanations, which tend to be lengthy and ver\u00adbose. Chitil [3] discusses \na tool to navigate through an explanation graph to inspect types. Although it is generally impossible \nto blame a single location for a type inconsistency, we believe that the most appropriate site should \nbe selected and reported, rather than enumerating all contributing sites. For instance, this decision \ncould be based on the number of justi.cations for a particular type constant, as was suggested by Walz \n[15]. Separating the collection of constraints and solving a set of constraints allows us to perform \na global analysis of a program. Formulating the type inference process as a constraint problem is not \nnew [1, 13]. Recently, Haack and Wells [6] have shown how to compute a minimal set of program locations \nthat contributed to a type inconsistency from a set of constraints. The type error slices in this paper \nare a nice and compact notation to present the error paths in our type graph. The focus of most type \nerror messages is to explain the con.icting types in a program, but little attention has been devoted \nto include suggestions on how to repair a type incorrect program. As a re\u00adsult, programmers have to extract \nthe corrections that are required to resolve a type error without any help. The only paper we are aware \nof that addresses repairing type incorrect programs considers uni.cation of types modulo linear isomorphism \n[12]. 6 Conclusion We have shown how the four techniques for externally modifying the behaviour of a \nconstraint-based type inferencer can improve the quality of type error messages. The major advantages \nof our ap\u00adproach can be summarized as follows. Type directives are supplied externally. As a result, \nno de\u00adtailed knowledge of how the type inference process is imple\u00admented is necessary.  Type directives \ncan be concisely and easily speci.ed by any\u00adone familiar with type systems. Consequently, experimenting \neffectively with the type inference process becomes possible.  The directives are automatically checked \nfor soundness. The major advantage here is that the underlying type system re\u00admains unchanged, thus providing \na .rm basis for the exten\u00adsions.  For combinator libraries in particular, it becomes possible to report \nerror messages which correspond more closely to the conceptual domain for which the library was developed. \n We have shown how our techniques can be applied to a parser com\u00adbinator library. We think it is clear \nthat our techniques can be ap\u00adplied equally well to other libraries, including the standard Haskell Prelude. \nIn fact, we are currently in the process of constructing type inference directives for the Helium Prelude. \nOur techniques have all been implemented in the Helium compiler, but they can be applied in other compilers \nas well. We want to point out that implementing our ideas was relatively easy, mainly as a consequence \nof using a constraint-based approach to type inference. It might seem that the directives we have discussed \nwork only for type graphs. This is not the case. Specialized type rules and phasing both work equally \nwell when using a greedy constraint solver (com\u00adparable in behaviour to, e.g., W and M ). (This is in \nfact possible in Helium.) The sibling functions and permuted function arguments were straightforwardly \nimplemented using type graphs. We think that implementing them for greedy solvers is a bigger challenge, \nwhich probably involves some mechanism for backtracking. Finally, our specialized type rules do not allow \na change of scope. We believe that an extension to allow this raises more complications than it is worth. \nThere are still a number of possibilities to be examined in future work. The .rst of these is to build \nbacktracking into our type infer\u00adence mechanism so that greedy constraint solvers can also bene.t from \nsibling functions and the permutation of function arguments. As a second possibility, we expect to extend \nthe facility for per\u00admuted function arguments to handle isomorphic types in general. In the near future \nwe shall extend Helium with type classes. As a result, we expect to be able to extend our framework along \nsimilar lines. Two other extensions which we are taking into consideration are adding .exibility in specifying \nthe priority of specialized type rules, and extending the facilities for phasing; at this point, phasing \nis a purely global operation, which might be too coarse for some applications. By their nature, the specialized \ntype rules follow the structure of the abstract syntax tree. A different approach, is to include a facility \nto discover whether changing the structure of an ill-typed expression, e.g., by moving some of the parentheses \nof the expression, results in a well-typed expression. This information can then be included as a hint. \nAcknowledgements We thank Dave Clarke and Arjan van IJzendoorn for their sugges\u00adtions and comments. \n7 References [1] A. Aiken and E. Wimmers. Type inclusion constraints and type inference. In Proceedings \nof Functional Programming Languages and Computer Architecture, pages 31 41, 1993. [2] M. Beaven and R. \nStansifer. Explaining type errors in poly\u00admorphic languages. In ACM Letters on Programming Lan\u00adguages, \nvolume 2, pages 17 30, December 1993. [3] O. Chitil. Compositional explanation of types and algorith\u00admic \ndebugging of type errors. In Proceedings of the Sixth ACM SIGPLAN International Conference on Functional \nPro\u00adgramming, pages 193 204, September 2001. [4] L. Damas and R. Milner. Principal type schemes for func\u00adtional \nprograms. In Principles of Programming Languages, pages 207 212, 1982. [5] D. Duggan and F. Bent. Explaining \ntype inference. In Science of Computer Programming 27, pages 37 83, 1996. [6] C. Haack and J. B. Wells. \nType error slicing in implicitly typed higher-order languages. In Proceedings of the 12th Eu\u00adropean Symposium \non Programming, pages 284 301, April 2003. [7] B. Heeren, J. Hage, and S. D. Swierstra. Generalizing \nHindley-Milner type inference algorithms. Technical Report UU-CS-2002-031, Institute of Information and \nComputing Science, University Utrecht, Netherlands, July 2002. [8] A. van IJzendoorn, D. Leijen, and \nB. Heeren. The Helium compiler. http://www.cs.uu.nl/helium. [9] J. Yang. Explaining type errors by .nding \nthe sources of type con.icts. In G. Michaelson, P. Trindler, and H.-W. Loidl, ed\u00aditors, Trends in Functional \nProgramming, pages 58 66. Intel\u00adlect Books, 2000. [10] O. Lee and K. Yi. Proofs about a folklore let-polymorphic \ntype inference algorithm. ACM Transanctions on Program\u00adming Languages and Systems, 20(4):707 723, July \n1998. [11] O. Lee and K. Yi. A generalized let-polymorphic type in\u00adference algorithm. Technical Memorandum \nROPAS-2000-5, Research on Program Analysis System, Korea Advanced In\u00adstitute of Science and Technology, \nMarch 2000. [12] B. J. McAdam. How to repair type errors automatically. In 3rd Scottish Workshop on Functional \nProgramming, pages 121 135. Stirling, U.K., August 2001. [13] M. Sulzmann, M. Odersky, and M. Wehr. Type \ninference with constrained types. Research Report YALEU/DCS/RR\u00ad1129, Yale University, Department of Computer \nScience, April 1997. [14] S. D. Swierstra. Combinator parsers: From toys to tools. In G. Hutton, editor, \nElectronic Notes in Theoretical Computer Science, volume 41. Elsevier Science Publishers, 2001. [15] \nJ. A. Walz and G. F. Johnson. A maximum .ow approach to anomaly isolation in uni.cation-based incremental \ntype infer\u00adence. In Conference Record of the 13th Annual ACM Sympo\u00adsium on Principles of Programming \nLanguages, pages 44 57, St. Petersburg, FL, January 1986.  \n\t\t\t", "proc_id": "944705", "abstract": "To improve the quality of type error messages in functional programming languages,we propose four techniques which influence the behaviour of constraint-based type inference processes. These techniques take the form of externally supplied type inference directives, precluding the need to make any changes to the compiler. A second advantage is that the directives are automatically checked for soundness with respect to the underlying type system. We show how the techniques can be used to improve the type error messages reported for a combinator library. More specifically, how they can help to generate error messages which are conceptually closer to the domain for which the library was developed. The techniques have all been incorporated in the Helium compiler, which implements a large subset of Haskell.", "authors": [{"name": "Bastiaan Heeren", "author_profile_id": "81100374595", "affiliation": "Utrecht University, Utrecht, The Netherlands", "person_id": "P600478", "email_address": "", "orcid_id": ""}, {"name": "Jurriaan Hage", "author_profile_id": "81100273210", "affiliation": "Utrecht University, Utrecht, The Netherlands", "person_id": "P151577", "email_address": "", "orcid_id": ""}, {"name": "S. Doaitse Swierstra", "author_profile_id": "81100040645", "affiliation": "Utrecht University, Utrecht, The Netherlands", "person_id": "PP14026122", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/944705.944707", "year": "2003", "article_id": "944707", "conference": "ICFP", "title": "Scripting the type inference process", "url": "http://dl.acm.org/citation.cfm?id=944707"}