{"article_publication_date": "08-25-2003", "fulltext": "\n Optimistic Evaluation: An Adaptive Evaluation Strategy for Non-Strict Programs Robert Ennals Computer \nLaboratory, University of Cambridge Robert.Ennals@cl.cam.ac.uk ABSTRACT Lazy programs are beautiful, \nbut they are slow because they build many thunks. Simple measurements show that most of these thunks \nare unnecessary: they are in fact always evaluated, or are always cheap. In this paper we describe Optimistic \nEvaluation an evaluation strategy that ex\u00adploits this observation. Optimistic Evaluation complements \ncompile-time analyses with run-time experiments: it evalu\u00adates a thunk speculatively, but has an abortion \nmechanism to back out if it makes a bad choice. A run-time adaption mechanism records expressions found \nto be unsuitable for speculative evaluation, and arranges for them to be evalu\u00adated more lazily in the \nfuture. We have implemented optimistic evaluation in the Glas\u00adgow Haskell Compiler. The results are encouraging: \nmany programs speed up signi.cantly (5-25%), some improve dra\u00admatically, and none go more than 15% slower. \n Categories and Subject Descriptors D.3.2 [Programming Languages]: Language Classi.ca\u00adtions Applicative \n(functional) languages, Lazy Evaluation; D.3.4 [Programming Languages]: Processors Code gen\u00aderation, \noptimization, pro.ling  General Terms Performance, Languages, Theory  Keywords Lazy Evaluation, Online \nPro.ling, Haskell 1. INTRODUCTION Lazy evaluation is great for programmers [16], but it car\u00adries signi.cant \nrun-time overheads. Instead of evaluating a function s argument before the call, lazy evaluation heap\u00adallocates \na thunk,or suspension, and passes that. If the function ever needs the value of that argument, it forces \nthe Permission to make digital or hard copies of all or part of this work for personal or classroom use \nis granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n03, August 25-29, 2003, Uppsala, Sweden. Copyright 2003 ACM 1-58113-756-7/03/0008 ...$5.00. Simon Peyton \nJones Microsoft Research Ltd, Cambridge simonpj@microsoft.com thunk. This argument-passing mechanism \nis called call-by\u00adneed, in contrast to the more common call-by-value. The extra memory tra.c caused by \nthunk creation and forcing makes lazy programs perform noticeably worse than their strict counterparts, \nboth in time and space. Good compilers for lazy languages therefore use sophisti\u00adcated static analyses \nto turn call-by-need (slow, but sound) into call-by-value (fast, but dangerous). Two basic analy\u00adses \nare used: strictness analysis identi.es expressions that will always be evaluated [40]; while cheapness \nanalysis lo\u00adcates expressions that are certain to be cheap (and safe) to evaluate [7]. However, any static \nanalysis must be conservative, forc\u00ading it to keep thunks that are probably unnecessary but not provably \nunnecessary . Figure 1 shows measurements taken from the Glasgow Haskell Compiler, a mature op\u00adtimising \ncompiler for Haskell (GHC implements strictness analysis but not cheapness analysis, for reasons discussed \nin Section 7.1.). The Figure shows that most thunks are either always used or are cheap and usually used \n. The exact de.nition of cheap and usually-used is not important: we use this data only as prima facie \nevidence that there is a big opportunity here. If the language implementation could somehow use call-by-value \nfor most of these thunks, then the overheads of laziness would be reduced signi.cantly. This paper presents \na realistic approach to optimistic eval\u00aduation, which does just that. Optimistic evaluation decides at \nrun-time what should be evaluated eagerly, and provides an abortion mechanism that backs out of eager \ncomputa\u00adtions that go on for too long. We make the following contri\u00adbutions: The idea of optimistic evaluation \nper se is rather ob\u00advious (see Section 7). What is not obvious is how to (a) make it cheap enough that \nthe bene.ts exceed the costs; (b) avoid potholes: it is much easier to get big speedups on some programs \nif you accept big slow\u00addowns on others; (c) make it robust enough to run full-scale application programs \nwithout modi.cation; and (d) do all this in a mature, optimising compiler that has already exploited \nmost of the easy wins. In Section 2 we describe mechanisms that meet these goals. We have demonstrated \nthat they work in prac\u00adtice by implementing them in the Glasgow Haskell Compiler (GHC) [29]. Optimistic \nevaluation is a slippery topic. We give an operational semantics for a lazy evaluator enhanced with optimistic \nevaluation (Section 5). This abstract machine allows us to re.ne our general design choices into a precise \nform. The performance numbers are encouraging: We have pro\u00adduced a stable extended version of GHC that \nis able to com\u00adpile arbitrary Haskell programs. When tested on a set of reasonably large, realistic programs, \nit produced a geomet\u00adric mean speedup of just over 15% with no program slow\u00ading down by more than 15%. \nPerhaps more signi.cantly, naively written programs that su.er from space leaks can speed up massively, \nwith improvements of greater than 50% being common. These are good results for a compiler that is as \nmature as GHC, where 10% is now big news. (For example, strictness analysis buys 10-20% [31].) While \nthe baseline is mature, we have only begun to explore the design space for optimistic evaluation, so \nwe are con.dent that our initial results can be further improved. 2. OPTIMISTIC EVALUATION Our approach \nto compiling lazy programs involves mod\u00adifying only the back end of the compiler and the run-time system. \nAll existing analyses and transformations are .rst applied as usual, and the program is simpli.ed into \na form in which all thunk allocation is done by a let expression (Section 5.1). The back end is then \nmodi.ed as follows: Each let expression is compiled such that it can either build a thunk for its right \nhand side, or evaluate its right hand side speculatively (Section 2.1). Which of these it does depends \non the state of a run-time ad\u00adjustable switch,one foreach let expression. The set of all such switches \nis known as the speculation con.gu\u00adration. The con.guration is initialised so that all lets are speculative. \n If the evaluator .nds itself speculatively evaluating a very expensive expression, then abortion is \nused to sus\u00adpend the speculative computation, building a special continuation thunk in the heap. Execution \ncontinues with the body of the speculative let whose right-hand side has now been suspended (Section \n2.2).  Recursively-generated structures such as in.nite lists can be generated in chunks, thus reducing \nthe costs of laziness, while still avoiding doing lots of unneeded work (Section 2.3).  Online pro.ling \nis used to .nd out which lets are ex\u00adpensive to evaluate, or are rarely used (Section 3). The pro.ler \nthen modi.es the speculation con.guration, by switching o. speculative evaluation, so that the o.end\u00ading \nlets evaluate their right hand sides lazily rather than strictly.  Just as a cache exploits the principle \nof locality, op\u00adtimistic evaluation exploits the principle that most thunks are either cheap or will \nultimately be evaluated. In practice, though, we have found that a realistic implementation re\u00adquires \nquite an array of mechanisms abortion, adaption, online pro.ling, support for chunkiness, and so on to \nPercentage of dynamically allocated thunks % Always Used % Cheap and Usually Used 100% 75% 50% 25% \n 0% Figure 1: How thunks are commonly used make optimistic evaluation really work, in addition to tak\u00ading \ncare of real-world details like errors, input/output, and so on. The following sub-sections introduce \nthese mech\u00adanisms one at at time. Identifying and addressing these concerns is one of our main contributions. \nTo avoid con\u00adtinual asides, we tackle related work in Section 7. We use the term speculative evaluation \nto mean an evaluation whose result may not be needed , while optimistic eval\u00aduation describes the entire \nevaluation strategy (abortion, pro.ling, adaption, etc). 2.1 Switchable Let Expressions In the intermediate \nlanguage consumed by the code gen\u00aderator, the let expression is the only construct that allocates thunks. \nIn our design, each let chooses dynamically whether to allocate a thunk, or to evaluate the right-hand \nside of the let speculatively instead. Here is an example: let x = <rhs> in <body> The code generator \ntranslates it to code that behaves log\u00adically like the following: if (LET237 != 0) { x = value of <rhs> \n}else{ x = lazy thunk to compute <rhs> when needed } evaluate <body> Our current implementation associates \none static switch (in this case LET237)with each let. There are many other possible choices. For example, \nfor a function like map,which is used in many di.erent contexts, it might be desirable for the switch \nto take the calling context into account. We have not explored these context-dependent possibilities \nbecause the complexity costs of dynamic switches seem to overwhelm the uncertain bene.ts. In any case, \nGHC s aggressive inlin\u00ading tends to reduce this particular problem. If <rhs> was large, then this compilation \nscheme could result in code bloat. We avoid this by lifting the right hand sides of such let expressions \nout into new functions. 2.2 Abortion The whole point of optimistic evaluation is to use call\u00adby-value \nin the hope that evaluation will terminate quickly. It is obviously essential to have a way to recover \nwhen this optimism turns out to be unfounded. That is the role of abortion. If the evaluator detects \nthat a speculation has been going on for a long time, then it aborts all current speculations (they can \nof course be nested), resuming after the let that started the outermost speculation. Detecting when a \nspeculation has been running for too long can be done in several ways; the choice is not impor\u00adtant, \nso long as it imposes minimal overheads on normal execution. One way would be to have the running code \nbe aware of how long it has been running, and actively call into the run-time system when it has run \nfor more than a .xed amount of time. Another approach, which we currently im\u00adplement, is to have periodic \nsample points which look at the state of the running program. If execution remains within the same speculation \nfor two sample points, then we consider the speculation to have gone on for too long. Abortion itself \ncan also be implemented in many di.erent ways. Our current scheme shares its implementation with that \nalready used for handling asynchronous exceptions [21]. A suspension is created in the heap containing \nthe aborted computation. If the result is found to be needed, then this computation will resume from \nthe point where it left o.. Abortion turns out to be a very rare event, with only a few tens of abortions \nin each program run, so it does not need to be particularly e.cient. Abortion alone is enough to guarantee \ncorrectness; i.e. that the program will deliver the same results as its lazy counterpart. However, we \ncan optionally also turn o. the static switch for one or more of the aborted lets, so that they run lazily \nin future (many strategies are possible). From this we get a crude bound on the total time and space \nconsumed by aborted computations. Since there are .nitely many lets in a program, turning one o. at each \nabortion bounds how many abortions can take place during the life of a program; in the limit, the entire \nprogram runs lazily. Further, the wasted work done by each abortion is bounded by twice the sample interval. \n 2.3 Chunky Evaluation Consider the following program, which generates an in.\u00adnite stream of integers. \nfrom :: Int -> [Int] from n =let n1 =n+1 in let rest = from n1 in (n : rest) If we used speculative \nevaluation for the rest thunk, the evaluation would be certain to abort. Why? Because evalu\u00adating from \nn would speculatively evaluate from (n+1),wh\u00adich would speculatively evaluate from (n+2),and so on. It \nwill not be long before we turns o. speculative evaluation of rest,making from completely lazy. This \nis a big lost opportunity: when lazily evaluating the lazily generated list, the tail of every cell returned \nby from is sure to be evaluated, except the last one! (The n1 thunks are easily caught by optimistic \nevaluation, if n is cheap.) What we would really like instead is to create the list in chunks of several \nelements. While creating a chunk, rest is evaluated speculatively. However once the chunk is .n\u00adished \n(for example, after 10 elements have been created) rest switches over to lazy evaluation, causing the \nfunction to terminate quickly. This approach largely removes the overheads of laziness, because only \na few thunks are lazy but it still allows one to work with in.nite data structures. This chunky behaviour \ncan be useful even for .nite lists that are completely evaluated. Lazy programmers often use a generate-and-.lter \nparadigm, relying on laziness to avoid creating a very large intermediate list. Even if the com\u00adpiler \nknew that the intermediate list would be completely evaluated, it would sometimes be a bad plan to evaluate \nit strictly. Chunky evaluation is much better. One way to implement chunky evaluation is to limit how \ndeeply speculation is nested. The code for a let then behaves semantically like the following (note that \nthis generalises the code given in Section 2.1): if (SPECDEPTH < LIMIT237){ SPECDEPTH = SPECDEPTH + 1 \nx = value of <rhs> SPECDEPTH = SPECDEPTH -1 }else{ x = lazy thunk for <rhs> } evaluate <body> Here, SPECDEPTH \nis a count of the number of nested spec\u00adulations that we are currently inside and LIMIT237 is a limit \non how deeply this particular let can be speculated. The intuition is that the deeper the speculation \nstack, the less likely a new speculation is to be useful. LIMIT237 can be adjusted at run-time to control \nthe extent to which this let may be speculated. 2.4 Exceptions and Errors Consider the following function: \nf x y = let bad = error \"urk\" in if x then bad else y In Haskell, the error function prints an error \nmessage and halts the program. Optimistic evaluation may evaluate bad without knowing whether bad is \nactually needed. It is ob\u00adviously unacceptable to print \"urk\" and halt the program, because lazy evaluation \nwould not do that if x is False. The same issue applies to exceptions of all kinds, including divide-by-zero \nand black-hole detection [26]. In GHC, error raises a catchable exception, rather than halting the program \n[32]. The exception-dispatch mecha\u00adnism tears frames o. the stack in the conventional way. The only change \nneeded is to modify this existing dispatch mech\u00adanism to recognise a speculative-evaluation let frame, \nand bind its variable to a thunk that re-raises the exception. A let thus behaves rather like a catch \nstatement, preventing exceptions raised by speculative execution from escaping. 2.5 Unsafe Input/Output \nOptimistic evaluation is only safe because Haskell is a pure language: evaluation has no side e.ects. \nInput/output is safely partitioned using the IO monad [33], so there is no danger of speculative computations \nperforming I/O. How\u00adever, Haskell programs sometimes make use of impure I/O, using the function unsafePerformIO. \nSpeculatively evalu\u00adating calls to this function could cause observable behaviour di.erent to that of \na lazy implementation. Moreover, it is not safe for us to abort IO computations because an IO op\u00aderation \nmay have locked a resource, or temporarily put some global data structures into an invalid state. For \nthese reasons, we have opted to disallow speculation of unsafePerformIO and unsafeInterleaveIO.Any specu\u00adlation \nwhich attempts to apply one of these functions will abort immediately.  3. ONLINE PROFILING Abortion \nmay guarantee correctness (Section 2.2), but it does not guarantee e.ciency. It turns out that with the \nmechanisms described so far, some programs run faster, but some run dramatically slower. For example \nthe constraints program from NoFib runs over 30 times slower. Why? De\u00adtailed investigation shows that \nthese programs build many moderately-expensive thunks that are seldom used. These thunks are too cheap \nto trigger abortion, but nevertheless aggregate to waste massive amounts of time and space. One obvious \nsolution is this: trigger abortion very quickly after starting a speculative evaluation, thereby limiting \nwast\u00aded work. However, this approach fails to exploit the thunks that are not cheap, but are almost always \nused; nor would it support chunky evaluation. In order to know whether an expression should be specu\u00adlated \nor not, we need to know whether the amount of work potentially wasted outweighs the amount of lazy evaluation \noverhead potentially avoided. We obtain this information by using a form of online pro.ling. 3.1 Idealised \nPro.ling We begin by describing an idealised pro.ling scheme, in which every let is pro.led all the time. \nFor every let,we maintain two static counters: speccount is incremented whenever a speculative evalua\u00adtion \nof the right hand side is begun. wastedwork records the amount of work wasted by as-yet\u00adunused speculations \nof the let. The quotient (wastedwork/speccount) is the wastage quo\u00adtient -the average amount of work \nwasted work per spec\u00adulation. If the wastage quotient is larger than the cost of allocating and updating \na thunk, then speculation of the let is wasting work, and so we reduce the extent to which it is speculated. \n(In fact, the critical threshold is tunable in our implementation.) By work we mean any reasonable measure \nof execution cost, such as time or allocation. In the idealised model, we just think of it as a global \nregister which is incremented regularly as execution proceeds; we discuss practical imple\u00admentation in \nSection 3.4. The wastedwork we record for a speculation includes the work done to compute its value, \nbut not the work of com\u00adputing any sub-speculations whose values were not needed. Operationally, we may \nimagine that the global work regis\u00adter is saved across the speculative evaluation of a let right\u00adhand \nside, and zeroed before beginning evaluation of the right-hand side. But what if a sub-speculation is \nneeded? Then the pro.ler should attribute the same costs as if lazy evaluation had been used. For example, \nconsider: let x = let y = <expensive> in y+1 in ... The evaluation of x will speculatively evaluate <expensive>, \nbut it turns out that y is needed by x,so the costsof <expen\u00adsive> should be attributed to x,not y.Speculating \ny is perfectly correct, because its value is always used. How do we implement this idea? When the speculation \nof y completes, we wrap the returned value in a special indi\u00adrection closure that contains not only the \nreturned value v, but also the amount of work w done to produce it, together with a reference to the \nlet for y. When the evaluation of x needs the value of y (to perform the addition), it evaluates the \nclosure bound to y (justas itwould if y were bound to a thunk). When the indirection is evaluated, it \nadds the work w to the global work counter, and subtracts it from y s static wastedwork counter, thereby \ntransferring all the costs from y to x. There is one other small but important point: when spec\u00adulating \ny we save and restore the work counter (for x), but we also increment it by one thunk-allocation cost. \nThe mo\u00adtivation is to ensure the cost attribution to x is the same, regardless of whether y is speculated \nor not. This scheme can attribute more cost to a speculative eval\u00aduation than is fair . For example: \nfv =let x =v+1 in v+2 The speculative evaluation of x will incur the cost of eval\u00aduating the thunk for \nv; but in fact v is needed anyway, so the real cost of speculating x is tiny (just incrementing a value). \nHowever, it is safe to over-estimate the cost of a speculation, so we simply accept this approximation. \n 3.2 Safety We would like to guarantee that if a program runs more slowly than it would under lazy evaluation, \nthe pro.ler will spot this, and react by reducing the amount of speculation. Our pro.ler assumes that \nall slowdown due to specula\u00adtion is due to wasted work1in speculations. If work is being wasted then \nat least one let must have a wastage quotient greater than one thunk allocation cost. When the pro.ler \nsees a let with this property, it reduces the amount of spec\u00adulation done for that let. Eventually, the \nspeculation con\u00ad.guration will either settle to one that is faster than lazy evaluation, or in the worst \ncase, all speculation will be dis\u00adabled, and execution reverts back to lazy evaluation. This safety property \nalso holds for any approximate pro.l\u00ading scheme, provided that the approximate scheme is conser\u00advative \nand overestimates the total amount of wasted work. 3.3 Random Sampling It would be ine.cient to pro.le \nevery let all the time, so instead we pro.le a random selection of speculations. At regular intervals, \ncalled sample points, wepro.leeach ac\u00adtive speculation, from the sample point until it .nishes.We double \nthe work measured to approximate the complete ex\u00adecution cost of the speculation. There are two sources \nof error. First, since we sample randomly through time, we are likely to see expensive spec\u00adulations \nmore frequently than inexpensive ones. However, 1In fact, as we see in section 6.2, space usage is often \na major factor. Sample Point Speculation of y Speculation of x Time Elasped Stack Stack Figure 2: Underestimating \nWasted Work Sample Point  Speculation of y Speculation of x Time Elasped Figure 3: Overestimating Wasted \nWork we believe that in practice, this should not be a problem, particularly as it is the expensive speculations \nthat are likely to be the ones that are wasting the most work. 3.4 Implementing Pro.ling We implement \npro.ling by overwriting return frames on the stack. At a sample point, we walk down the stack and .nd \nthe return frames for all active speculations. Each of these return frames is overwritten with the address \nof a pro\u00ad.ling system routine, while the real return address is squir\u00adreled away for safe-keeping. When \nthe speculation .nishes, it will jump back into the pro.ling system through this hijacked return point. \nThe pro.ler now calculates an estimate of the work done during the speculation. Available timers are \nnot accurate enough to allow for an accurate measure of the time elapsed during a speculation, so we \nuse the amount of heap allocated by the speculation instead. This seems to be a fairly reasonable estimate, \nprovided that we ensure that all recursions allocate heap.  4. IS THIS TOO COMPLEX? By this time, the \nreader may be thinking isn t this all rather complicated? . Does one really need abortion, switch\u00adable \nlet expressions, chunky evaluation, special handling for IO, and online pro.ling? Our answer is two-fold. \nFirst, just as a compiler needs a lot of bullets in its gun, to tackle widely varying programs, we believe \nthat the same is true of optimistic evaluation. To make this idea work, in practice, on real programs, \nin a mature compiler, there Second, we can reasonably assume that the sample point is uniformly distributed \nbetween the moment a speculation begins and the moment it ends; however, theworkticks at\u00adjust are a lot \nof cases to cover. One of the contributions of this paper is precisely that we have explored the idea \ntributed to that speculation may not be uniformly distributed deeply enough to expose these cases. Section \n6.4 quanti.es over that period. For example: the e.ect of removing individual bullets . let x = if <expensive1> \nthen let y = <expensive2> in (y,y) else ... Most of the work attributed to x is the cost of evaluating \n<expensive1>; the work of evaluating <expensive2> will be attributed to y. Furthermore, evaluation of \n<expensive1> precedes the evaluation of <expensive2>. Hence, if we were to place our sample point in \nthe middle of the speculation for x, then we would get an under-estimate of the work done. This is illustrated \nby Figure 2, in which work counted is shaded and work ignored is unshaded. Alas, under-estimates are \nnot safe! However, it turns out that this is not a problem, for a rather subtle reason. In our example, \nthe speculation of x does no work after the speculation of y has .nished. Let us assume that <expensive1> \ndoes exactly the same amount of work W as <expensive2>. Hence, the sample point will have a 50% chance \nof falling before the start of the spec\u00adulation of y, and a 50% chance of falling after it. If the sample \npoint falls after the speculation of y starts, then we will attribute no work to x, thus underestimating \nthe work by W units (Figure 2). However, if the sample point falls before the speculation of y starts, \nthen we will not pro.le the speculation of y and so will attribute all of the work done Second, the actual \nimplementation in GHC is not overly complex. The changes to the compiler itself are minor (ar\u00adound 1,300 \nlines in a program of 130,000 or so). The changes to the run-time system are more signi.cant: In a run-time \nsystem of around 47,000 lines of C, we have added around 1000 lines and altered around 1,600 lines certainly \nnot trivial, but a 3% increase seems reasonable for a major in\u00adnovation.  5. AN OPERATIONAL SEMANTICS \nAs we have remarked, optimistic evaluation is subtle. We found it extremely helpful to write a formal \noperational se\u00admantics to explain exactly what happens during specula\u00adtive evaluation, abortion, and \nadaption. In this section, we brie.y present this semantics. 5.1 A Simple Language The language we work \nwith is Haskell. While the exter\u00adnally visible Haskell language is very complex, our compiler reduces \nit to the following simple language: 0 E ::= x variable | C {xi}n constructor app case analysis 0 | \ncase E of {Pi}n | let x = E in E. thunk creation by <expensive2> to x; thus overestimating the work by \nan .x.E abstraction | average of 2W units (Figure 3) the factor of 2 comes from | doubling the measured \ncost, as mentioned above. The net | Ex application 00 exn exception or error C {xi}n C {xi}n e.ect \nis that we still over-estimate x s cost. This example, in which all x s work is front-loaded, is an \nP . E pattern match ::= extreme case. The informal argument we have given can be generalised to show \nthat our sampling approach can only V | .x.E | exn values ::= over-estimate costs. (VAR1) G[x . V ]; \nx; s -. S G[x . V ]; V ; s (VAR2) G[x . E]; x; s -. S G[x . E]; E;#x : s if E is not a value (UPD) G; \nV ;#x : s -. S G[x . V ]; V ; s (APP1) G; Ex; s -. S G; E;( x): s (APP2) G; .x.E;( x'): s -. S G; E[x'/x]; \ns (APP3) G; exn;( i): s -. S G; exn; s (CASE1) (CASE2) (CASE3) G; case E of { Pi} n 0 ; s G; C { x'i} \nn 0 ; { Pi} n 0 : s G; exn; { Pi} n 0 : s -. S -. S -. S G; E; { Pi} n 0 : s G; E[{ x'i/xi} n 0 ]; s \nG; exn; s where Pk = C { xi} n 0 . E (LAZY) G; let x = E in E'; s -. S G[x' . E]; E'[x'/x]; s if S(x) \n= D(s)and x' is new (SPEC1) G; let x = E in E'; s -. S G; E; x /E' : s if S(x) >D(s) (SPEC2) G; V ; x \n/E : s -. S G[x' . V ]; E[x'/x]; s where x' is new Figure 4: Operational Semantics: Evaluation The binder \nx is unique for each let. This allows us to uniquely refer to a let by its binder. Functions and constructors \nare always applied to vari\u00adables, rather than to expressions. It follows that let is the only pointatwhich \na thunk mightbecreated. (A case ex\u00adpression scrutinises an arbitrary expression E, but it does not .rst \nbuild a thunk.). We restrict ourselves to non-recursive let expressions. This does not restrict the expressiveness \nof the language, as the user can use a .xed point combinator. It does how\u00adever simplify the semantics, \nas recursive let expressions are problematic when speculated. We omit literals and primi\u00adtive operators \nfor the sake of brevity. Adding them intro\u00adduces no extra complications. Exceptions and errors (Section \n2.4) are handed exactly as described in [32], namely by treating an exception as a value (and not as \na control operator).  5.2 The Operational Framework We describe program execution using a small step \nopera\u00adtional semantics, describing how the program state changes as execution proceeds. The main transition \nrelation, -. , takes the form: ' G; E; s -. S G'; E'; s ' meaning that the state G; E; s evolves to G'; \nE'; sin one step using S. The components of the state are as follows. G represents the heap. It is a \nfunction mapping names to expressions.  E is the expression currently being evaluated.  s is a stack \nof continuations containing work to be done, each taking one of the following forms (c.f. [11]):  x \n/E Speculation return to E, binding x { Pi} 0 n Case choose a pattern Pi x Application use arg x #x Update \nof thunk bound to x We write D(s) to denote the number of speculation frames on the stack s. We refer \nto this as the specula\u00adtion depth. Sisthe speculation con.guration.S maps a let binder x to a natural \nnumber n. This number says how deeply we can be speculating and still be allowed to create a new speculation \nfor that let.If the let is lazy, then n will be 0. Online pro.ling may change the mapping S while the \nprogram is running. The transition rules for -. are given in Figure 4. The .rst nine are absolutely conventional \n[35], while the last three are the interesting ones for optimistic evaluation. Rule (LAZY) is used if \nthe depth limit S[x] is less than the current specula\u00adtion depth D(s). The rule simply builds a thunk \nin the heap '' G, with address x, and binds x to x.Rule (SPEC1) is used if S[a] is greater than the current \nspeculation depth D(s); it pushes a return frame and begins evaluation of the right\u00adhand side. On return, \n(SPEC2) decrements d and binds x to (the address of) the computed value. At a series of sample points, \nthe online pro.ler runs. It can do two things: .rst, it can change the speculation con\u00ad.guration S; and \nsecond, it can abort one or more running speculations. The process of abortion is described by the . \ntransitions given in Figure 5. Each abortion rule removes one continuation from the stack, undoing the \nrule that put it there. It would be sound to keep applying these rules until the stack is empty, but \nthere is no point in continuing when the last speculative-let continuation (let x = in E)has been removed \nfrom the stack, and indeed our implementa\u00adtion stops at that point. One merit of having an operational \nsemantics is that it allows us to formalise the pro.ling semantics described in Section 3. We have elaborated \nthe semantics of Figure 4 to describe pro.ling and cost attribution, but space prevents us showing it \nhere.  6. PERFORMANCE We measured the e.ect of optimistic evaluation on several programs. Most of the \nprograms are taken from the real subset of the NoFib [28] suite, with a few taken from the spectral subset. \nThese programs are all reasonably sized, realistic programs. GHC is 132,000 lines, and the mean of the \nother benchmarks is 1,326 lines. Programs were selected before any performance results had been obtained \nfor them. ''' ' (!SPEC) G; E; x /E ' : s .G[x . E]; E [x /x]; s where x is new. (!CASE) G; E; {Pi}n \n: s .G; case E of {Pi}n 0 ; s (!APP) G; E;( x): s .G; Ex; s (!UPD) G; E;#x : s .G[x . E]; x; s 0 Figure \n5: Operational Semantics : Abortion We ran each program with our modi.ed compiler, and with the GHC compiler \nthat our implementation forked from. The results for normal GHC were done with all optimisa\u00adtions enabled. \nTests were performed on a 750MHz Pentium III with 256Mbytes of memory. The implementation bench\u00admarked \nhere does not remember speculation con.gurations between runs. To reduce the e.ect of this cold start, \nwe arranged for each benchmark to run for around 20 seconds. We will now summarise the results of our \ntests. For full results, please refer to Appendix A. 125% 100% 75% 50% 25% 0% Figure 6: Benchmark Run-Time \nrelative to Normal GHC 6.1 Execution Time Figure 6 shows the e.ect that optimistic evaluation has on \nrun time. These results are very encouraging. The average speedup is just over 15%, some programs speed \nup dramati\u00adcally and no program slows down by more than 15%. As one would expect, the results depend \non the nature of the pro\u00adgram. If a program has a strict inner loop that the strictness analyser solves, \nthen we have little room for improvement. Similarly, if the inner loop is inherently lazy, then then \nthere is nothing we can do to improve things, and indeed the extra overhead of having a branch on every \nlet will slow things down. In the case of rsa speculation had virtually no e.ect as rsa spends almost \nall of its time inside a library written in C. These results re.ect a single .xed set of tuning parame\u00adters, \nsuch as the thunk-cost threshold (Section 3.1), which we chose based on manual experimentation. Changing \nthese parameters can improve the run-time of a particular pro\u00adgram signi.cantly (up to 25% or so), but \nonly at the cost of worsening another. Whether a more sophisticated pro.l\u00ading strategy could achieve \nthe minimal run-time for every program remains to be seen.  6.2 Heap Residency Some programs are extremely \nine.cient when executed lazily, because they contain a space leak. People often post such programs on \nthe haskell mailing list, asking why they are going slowly. One recent example was a simple word counting \nprogram [24]. The inner loop (slightly simpli.ed) was the following: count :: [Char] -> Int -> Int -> \nInt -> (Int,Int) count [] _ nw nc = (nw, nc) count (c:cs) new nw nc = case charKind c of Normal -> count \ncs 0 (nw+new) (nc+1) White -> count cs 1 nw (nc+1) Every time this loop sees a character, it increments \nits accu\u00admulating parameter nc. Under lazy evaluation, a long chain of addition thunks builds up, with \nlength proportional to the size of the input .le. By contrast, the optimistic version evaluates the addition \nspeculatively, so the program runs in constant space. Optimistic evaluation speeds this program up so \nmuch that we were unable to produce an input .le that was both small enough to allow the lazy implementa\u00adtion \nto terminate in reasonable time, and large enough to allow the optimistic implementation to run long \nenough to be accurately timed! The residency column of Appendix A shows the e.ect on maximum heap size \non our benchmark set, with a mean of 85% of the normal-GHC .gure. Not surprisingly, the im\u00adprovement \nis much smaller than that of count perhaps because these real programs have already had their space leaks \ncured and some programs use more space than be\u00adfore. Nevertheless, optimistic evaluation seems to reduce \nthe prevalence of unexpectedly-bad space behaviour, a very common problem for Haskell programmers, and \nthat is a welcome step forward.  6.3 Code Size Code size increases signi.cantly (34% on average). This \nis due to the need to generate both lazy and strict versions of expressions. We have not yet paid much \nattention to code bloat, and are con.dent that it can be reduced substantially, but we have yet to demonstrate \nthis.  6.4 Where the Performance comes from Where does the performance improvement come from? Could \nweget thesameperformance resultsfrom a simpler system? Figure 7 shows the performance of several simpli.ed \nvari\u00adants of our system relative to our full optimistic implemen\u00adtation. Figure 8 shows the performance \nof several altered versions of normal GHC relative to normal GHC. Chunky evaluation. The No Chunky bars \nin Figure 7 show the e.ect on switching o. chunky evaluation. When No Chunky No Strictness No Semi No \nProfile 150% 125% 100% 75% 50% 25% 0% gram run slower. In short, optimistic evaluation turns semi\u00adtagging \nfrom a mixed blessing into a consistent, and some\u00adtimes substantial, win. This is a real bonus, and one \nwe did not originally anticipate. Strictness Analysis. The No Strictness bars show the e.ect of turning \no. GHC s strictness analysis. Fig\u00adure 8 shows that strictness analysis is usually a very big win for \nnormal GHC, while in Figure 7 we see that the ef\u00adfect of switching o. strictness analysis in an optimistically\u00adevaluated \nimplementation is far smaller. Hence, in the ab\u00adsence of strictness analysis, the win from optimistic \nevalua\u00adtion would be far greater than the ones we report in Figure 6. Pro.ling. The No Pro.le bars in \nFigure 7 show the ef\u00adfect of disabling online pro.ling. Most programs are largely una.ected, but a few \nprograms such as constraints and fulsom slow down massively. Programs like these justify our work on \npro.ling (Section 3); our goal is to give accept- Figure 7: Run times of Simpli.ed Implementations able \nperformance in all cases, without occasional massive and unpredictable slow-downs. Overheads. The All \nlazy bars in Figure 8 show what Semitagging No Strictness happens when we pay all the costs of optimistic \nevaluation, but get none of the bene.ts. In this experiment, we set All Lazy 125% 100% 75% 50% 25% 0% \nS to map every let to 0, so that all lets are done lazily. Comparing this to Normal GHC, the baseline \nfor this graph, shows the overheads that the pro.ler and the switchable-let mechanism impose on normal \nevaluation; they are always less than 25%.  7. RELATED WORK 7.1 Static Analyses Where static analysis \nis possible, it is much to be pre\u00adferred, because the results of the analysis can often enable a cascade \nof further transformations and optimisations, and static choices can be compiled into straight line code, \nwith better register allocation. GHC has a fairly sophisticated strictness analyser, and all our results \nare relative to a baseline in which strictness analysis is on. (When we switch it o., the speedups from \noptimistic evaluation are much greater.) The other promis\u00ading static analysis is Fax\u00b4en s cheap eagerness \nanalysis [7], which attempts to .gure out which thunks are guaranteed to be cheap to evaluate, so that \ncall-by-value is sound, and will not waste much work even if the result is not used. A further development, \ndynamic cheap eagerness [8], uses a more complicated analysis to add an extra depth parameter to selected \nrecursive functions, plus an explicit cut-o. test, to achieve an e.ect similar to chunky evaluation. \nCheap eagerness is built on a very sophisticated whole\u00adprogram .ow analysis. Is a thunk for x+1 cheap? \nIt depends on whether x is evaluated; and if x is an argument to a function, we need to examine all calls \nto the function and that is not straightforward in a higher-order program. Worse, a whole-program analysis \ncauses problems for sepa\u00adrate compilation, and that is a big problem when shipping pre-compiled libraries. \nThese problems may be soluble for example by compiling multiple clones of each function, each suitable \nfor a di.erent evaluation pattern but the additional implementation complexity would be signi.cant. That \nis why we did not implement cheap eagerness in GHC for comparison purposes. A su.ciently-clever whole-program \nstatic analysis might well discover many of the cheap thunks that we .nd with Figure 8: E.ect of Changes \nto Normal GHC chunky evaluation is turned o., a let expression can only be on or o., with no reduction \nin depth allowed for in-between cases. Some programs are largely una.ected by this, but others su.er \nsigni.cantly. constraints actually speeds up. Semi-tagging. Our implementation of optimistic eval\u00aduation \nuses an optimisation called semi-tagging, which we brie.y explain. When evaluating a case expression, \nGHC normally jumps to the entry point of the scrutinee, passing it a vector of return addresses, one \nfor each possible con\u00adstructor. If the scrutinee is a value, then the entry point will simply return \nto the relevant return address. An alter\u00adnative plan is called semi-tagging [30]: before jumping to the \nscrutinee s entry point, test whether the scrutinee is already evaluated. In that case, we can avoid \nthe (slow) indirect call and return. The Semitagging bars in Figure 8 show the e.ect of enabling semi-tagging \nonly relative to baseline GHC (i.e. no optimistic evaluation). Under normal lazy evaluation, the scrutinee \nis often unevaluated, so while semi-tagging is a win on average, the average speedup is only 2%. However, \nunder optimistic evaluation most scrutinees are evaluated, and semi-tagging gives a consistent improvement. \nThe No Semi barsin Figure7 aregiven relative to the full optimistic evaluator, and show that switching \no. semi\u00adtagging almost always makes an optimistically-evaluated pro\u00adour online pro.ler. The critical \ndi.erence is that we can .nd all the cheap thunks without being particularly clever, without requiring \na particularly complex implementation, and without getting in the way of separate compilation. Fax\u00b4en \nreports some promising speedups, generally in the range 0-25% relative to his baseline compiler, but \nit is not appropriate to compare these .gures directly with ours. As Fax\u00b4en is careful to point out, \n(a) his baseline compiler is a prototype, (b) his strictness analyser is very simple , and (c) all his \nbenchmarks are small. The improvements from cheapness analysis may turn out to be less persuasive if \na more sophisticated strictness analyser and program op\u00adtimiser were used, which is our baseline. (Strictness \nanalysis does not require a whole-program .ow analysis, and readily adapts to separate compilation.) \nThere exist many other static analyses that can improve the performance of lazy programs. In particular, \nthe GRIN project [3] takes a di.erent spin on static analysis. A whole program analysis is used to discover \nfor each case expression the set of all thunk expressions that might be being evalu\u00adated at that point. \nThe bodies of these expressions are then inlined at the usage sites, avoiding much of the cost of lazy \nevaluation. Such transformations do not however prevent lazy space leaks. 7.2 Eager Haskell Eager Haskell \n[20, 19] was developed simultaneously, but independently, from our work. Its basic premise is identical: \nuse eager evaluation by default, together with an abortion mechanism to back out when eagerness turns \nout to be over\u00adoptimistic. The implementation is rather di.erent, however. Eager Haskell evaluates absolutely \neverything eagerly, and periodically aborts the running computation right back to the root. Abortion \nmust not be too frequent (lest its costs dominate) nor too infrequent (lest work be wasted). The abortion \nmechanism is also di.erent: it allows the compu\u00adtation to proceed, except each function call builds a \nthunk instead of making the call. The net e.ect is somewhat sim\u00adilar to our chunky evaluation, but appears \nto require an entirely new code generator, which ours does not. The main advantage of our work over Eager \nHaskell is that we adaptively decide which let expressions are appropriate to evaluate eagerly while \nEager Haskell always evaluates ev\u00aderything eagerly. On some programs eager evaluation is a good plan, \nand Eager Haskell gets similar speedups to Op\u00adtimistic Evaluation. On other programs, though, laziness \nplays an important role, and Eager Haskell can slow down (relative to normal GHC) by a factor of 2 or \nmore. In an ex\u00adtreme case (the constraints program) Eager Haskell goes over 100 times slower than normal \nGHC, while with Opti\u00admistic Evaluation it slows by only 15%. Eager Haskell users are encouraged to deal \nwith such problems by annotating their programs with laziness annotations, which Optimistic Evaluation \ndoes not require. It is much easier to get good speedups in some cases by accepting big slow-downs in \noth\u00aders. It is hard to tell how much Eager Haskell s wins will be reduced if it were to solve the big-slow-down \nproblem in an automated way. 7.3 Speculative Parallelism The parallel programming community has been \nmaking use of speculation to exploit parallel processors for a long time [5]. There, the aim is to make \nuse of spare processors by arranging for them to evaluate expressions that are not (yet) known to be \nneeded. There is a large amount of work in this .eld, of which we can only cite a small subset. Several \nvariants of MultiLisp allow a programmer to sug\u00adgest that an expression be evaluated speculatively [12, \n27]. Mattson [22, 23] speculatively evaluates lazy expressions in parallel. Local Speculation [23, 6] \ndoes some speculations on the local processor when it would otherwise be waiting, reducing the minimum \nsize for a useful speculation. Hunt\u00adback [17] speculatively evaluates logic programming expres\u00adsions \nin parallel, with the user able to annotate speculations with a priority. A major preoccupation for speculative \npar\u00adallelism is achieving large enough granularity; otherwise the potential gain from parallelism is \ncancelled out by the cost of spawning and synchronisation. In our setting, the exact reverse holds. A \nlarge thunk might as well be done lazily, because the cost of allocating and updating it are swamped \nby its evaluation costs; it is the cheap thunks that we want to speculate! Haynes and Friedman describe \nan explicit engines pro\u00adcess abstraction that can be used by the programmer to spawn a resource-limited \nthread [14]. An engine has a cer\u00adtain amount of fuel; if the fuel runs out, the engine returns a continuation \nengine that can be given more fuel, and so on. Engines are rather coarse-grain, and under explicit user \ncontrol, both big di.erences from our work. Another strand of work takes eager parallelism as the baseline, \nand strives to aggregate, or partition, tiny threads into larger compound threads [38, 34]. In some ways \nthis is closer to our work: call-by-need is a bit like parallel evaluation (scheduled on a uniprocessor), \nwhile using call\u00adby-value instead aggregates the lazy thread into the par\u00adent. However, the issues are \nquite di.erent; as Schauser puts it the di.culty is not what can be put in the same thread, but what \nshould be ... given communication and load-balancing constraints . Furthermore, thread partition\u00ading \nis static, whereas our approach is dynamic. Yet another strand is that of lazy thread creation,where \none strives to make thread creation almost free in the case where it is not, in the end, required [25]. \nA very success\u00adful variant of this idea is Cilk [10], in which the parent thread saves a continuation \nbefore optimistically executing the spawned child; if the child blocks, the processor can re\u00adsume at \nthe saved continuation. Other processors can also steal the saved continuation. The big di.erence from \nour work is that in Cilk all threads are assumed to be required, whereas the possibility that a lazy \nthunk is not needed is the crux of the matter for us. In summary, despite the common theme of speculative \nevaluation in a declarative setting, we have found little over\u00adlap between the concerns of speculation-for-parallelism \nand optimistic evaluation. 7.4 Other Work Stingy Evaluation [39] is an evaluation strategy designed \nto reduce space leaks such as the one described in Section 6.2. When evaluating a let expression, or \nduring garbage collection, the evaluator does a little bit of work on the ex\u00adpression, with the hope \nof evaluating it, and avoiding having to build a thunk. As with Eager Haskell, all expressions are eagerly \nevaluated, however the amount of evaluation done before abortion is signi.cantly smaller, with only very \nsimple evaluations allowed. Often this small amount of work will not be useful, causing some programs \nto run slower. Stingy evaluation was implemented in the LML [1] compiler. Branch Prediction [36] is present \nin most modern proces\u00adsors. The processor will speculatively execute whichever side of a branch that \nis thought most likely to be used. As with optimistic evaluation, observation of the running program \nis used to guide speculation. Static analysis can be used to guide branch prediction [2]. Online Pro.ling \nis used in many existing language imple\u00admentations, including several implementations for the Ja\u00adva [18] \nlanguage, such as HotSpot [37] and Jalapeno [4]. One of the .rst implementations to use such techniques \nwas for Self [15]. These systems use similar techniques to optimistic evaluation, but do not apply them \nto laziness. Feedback Directed Optimisation [9] is a widely used tech\u00adnique in static compilers. A program \nis run in a special pro.ling mode, recording statistics about the behaviour of the program. These statistics \nare then used by the compiler to make good optimisation choices when compiling a .nal version of the \nprogram. Many commercial compilers use this technique. In principle we could do the same, compiling the \ncon.guration S into the program, rather than making it adapt at run-time, but we have not done so.  \n8. CONCLUSIONS AND FUTURE WORK Our goal is to allow a programmer to write programs in a lazy language \nwithout having to use strictness annotations in order to obtain good performance. Our measurements show \nthat optimistic evaluation can improve overall perfor\u00admance by 14%, or more, relative to a very tough \nbaseline. Our implementation represents just one point in a large design space that we are only just \nbeginning to explore. It is very encouraging that we get good results so early; things can only get better! \nIn particular, we are looking at the following: Heap Usage Pro.ling. As shown in Section 6.2, optimis\u00adtic \nevaluation often speeds programs up by preventing them from .lling the heap up with thunks. However our \ncurrent system does not take heap behaviour into account when deciding which let expressions should be \nevaluated optimistically, and so could be missing opportunities to speed programs up. We plan to ex\u00adplore \nways of making better decisions, by taking into account the behaviour of the heap. Proving Worst Case \nPerformance. We have stressed the importance of avoiding bad performance for par\u00adticular programs. But \nexperiments can never show that no program will run much slower under our sys\u00adtem than with normal GHC. \nInstead we would like to prove this property. Based on the operational se\u00admantics in Section 5, preliminary \nwork suggests that we can indeed prove that the online pro.ler will even\u00adtually .nd all let expressions \nthat waste work, and hence that optimistic evaluation is at most a constant factor worse than ordinary \ncall-by-need (because of its .xed overheads), once the speculation con.guration has converged [Paper \nin preparation]. There are hundreds of papers about cunning compilation techniques for call-by-value, \nand hundreds more for call-by\u00adneed. So far as we know, this paper and Maessen s recent independent work \n[19] are the .rst to explore the rich terri\u00adtory of choosing dynamically between these two extremes. \nThe implementation described in this paper is freely avail\u00adable in the GHC CVS, and readers are encouraged \nto down\u00adload it and have a look at it themselves. We plan to include Optimistic Evaluation in a release \nversion of GHC soon. Acknowledgments We are grateful for the generous support of Microsoft Re\u00adsearch, \nwho funded the studentship of the .rst author. We would also like to thank Manuel Chakravarty, Fergus \nHen\u00adderson, Jan-Willem Maessen, Simon Marlow, Greg Mor\u00adrisett, Alan Mycroft, Nick Nethercote, Andy Pitts, \nNorman Ramsey, John Reppy, and Richard Sharp, who provided use\u00adful suggestions.  9. REFERENCES [1] L. \nAugustsson and T. Johnsson. The chalmers lazy-ML compiler. The Computer Journal, 32(2):127 141, Apr. \n1989. [2] T. Ball and J. R. Larus. Branch prediction for free. In ACM Conference on Programming Languages \nDesign and Implementation (PLDI 93), pages 300 313. ACM, June 1993. [3] U. Boquist. Code Optimisation \nTechniques for Lazy Functional Languages. PhD thesis, Chalmers University of Technology, Sweden, April \n1999. [4] M.G.Burke,J.-D. Choi, S. Fink, D. Grove, M. Hind, V. Sarkar, M. J. Serrano, V. Sreedhar, H. \nSrinivasan, and J. Whaley. The Jalapeno dynamic optimizing compiler for Java. In Proceedings of the ACM \nJava Grande Conference, 1999. [5] F. W. Burton. Speculative computation, parallelism and functional programming. \nIEEE Trans Computers, C-34(12):1190 1193, Dec. 1985. [6] M. M. T. Chakravarty. Lazy thread and task creation \nin parallel graph reduction. In International Workshop on Implementing Functional Languages, Lecture \nNotes in Computer Science. Springer Verlag, 1998. [7] K.-F. Fax\u00b4en. Cheap eagerness: Speculative evaluation \nin a lazy functional language. In ACM SIGPLAN International Conference on Functional Programming (ICFP \n00), Montreal, Sept. 2000. ACM. [8] K.-F. Fax\u00b4en. Dynamic cheap eagerness. In Proceedings of the 2001 \nWorkshop on Implementing Functional Languages. Springer Verlag, 2001. [9] ACM Workshop on Feedback Directed \nand Dynamic Optimization (FDDO), 2001. [10] M. Frigo, C. E. Leiserson, and K. H. Randall. The implementation \nof the Cilk-5 multithreaded language. In ACM Conference on Programming Languages Design and Implementation \n(PLDI 98), volume 33, pages 212 223, Atlanta, May 1998. ACM. [11] J. Gustavsson. A type-based sharing \nanalysis for update avoidance and optimisation. In ACM SIGPLAN International Conference on Functional \nProgramming (ICFP 98), volume 34(1) of ACM SIGPLAN Notices, Baltimore, 1998. ACM. [12] R. H. Halstead. \nMultilisp -a language for concurrent symbolic computation. ACM Transactions on Programming Languages \nand Systems, 7(4):501 538, Oct. 1985. [13] K. Hammond and J. O Donnell, editors. Functional Programming, \nGlasgow1993, Workshops in Computing. Springer Verlag, 1993. [14] C. Haynes and D. Friedman. Engines build \nprocess abstractions. In Conference Record of the 1984 ACM Symposium on Lisp and Functional Programming, \n1984. [15] U. H\u00a8olzle. Adaptive optimization for Self: reconciling high performance with exploratory \nprogramming. Ph.D. thesis, Computer Science Department, Stanford University, Mar. 1995. [16] J. Hughes. \nWhy functional programming matters. The Computer Journal, 32(2):98 107, Apr. 1989. [17] M. Huntback. \nSpeculative computation and priorities in concurrent logic languages. In ALPUK Conference, 1991. [18] \nH. M. J Gosling. The Java Language Environment: a White Paper. Technical report, Sun Microsystems, 1996. \n[19] J.-W. Maessen. Eager Haskell: Resource-bounded execution yields e.cient iteration. In The Haskell \nWorkshop, Pittsburgh, 2002. [20] J.-W. Maessen. Hybrid Eager and Lazy Evaluation for E.cient Compilation \nof Haskell.PhD thesis, Massachusetts Institute of Technology, June 2002. [21] S. Marlow, S. Peyton Jones, \nA. Moran, and J. Reppy. Asynchronous exceptions in Haskell. In ACM Conference on Programming Languages \nDesign and Implementation (PLDI 01), pages 274 285, Snowbird, Utah, June 2001. ACM. [22] J. Mattson. \nAn e.ective speculative evaluation technique for parallel supercombinator graph reduction. Ph.D. thesis, \nDepartment of Computer Science and Engineering, University of California, San Diego, Feb. 1993. [23] \nJ. S. Mattson Jr and W. G. Griswold. Local Speculative Evaluation for Distributed Graph Reduction. In \nHammond and O Donnell [13], pages 185 192. [24] L. Maurer. Isn t this tail recursive? Message posted \nto the Haskell mailing list: http://haskell.org/ pipermail/haskell/2002-March/009126.html, Mar. 2002. \n[25] E. Mohr, D. Kranz, and R. Halstead. Lazy task creation -a technique for increasing the granularity \nof parallel programs. IEEE Transactions on Parallel and Distributed Systems, 2(3), July 1991. [26] A. \nMoran, S. Lassen, and S. Peyton Jones. Imprecise exceptions, co-inductively. In Higher Order Operational \nTechniques in Semantics: Third International Workshop, number 26 in Electronic Notes in Theoretical Computer \nScience, pages 137 156. Elsevier, 1999. [27] R. Osbourne. Speculative computation in Multilisp. PhD thesis, \nMIT Lab for Computer Science, Dec. 1989. [28] W. Partain. The nofib benchmark suite of Haskell programs. \nIn J. Launchbury and P. Sansom, editors, Functional Programming, Glasgow1992, Workshops in Computing, \npages 195 202. Springer Verlag, 1992. [29] S. Peyton Jones, C. Hall, K. Hammond, W. Partain, and P. Wadler. \nThe Glasgow Haskell Compiler: a technical overview. In Proceedings of Joint Framework for Information \nTechnology Technical Conference, Keele, pages 249 257. DTI/SERC, Mar. 1993. [30] S. Peyton Jones, S. \nMarlow, and A. Reid. The STG runtime system (revised). Technical report, Microsoft Research, February \n1999. Part of GHC source package. [31] S. Peyton Jones and W. Partain. Measuring the e.ectiveness of \na simple strictness analyser. In Hammond and O Donnell [13], pages 201 220. [32] S. Peyton Jones, A. \nReid, C. Hoare, S. Marlow, and F. Henderson. A semantics for imprecise exceptions. In ACM Conference \non Programming Languages Design and Implementation (PLDI 99), pages 25 36, Atlanta, May 1999. ACM. [33] \nS. Peyton Jones and P. Wadler. Imperative functional programming. In 20th ACM Symposium on Principles \nof Programming Languages (POPL 93), pages 71 84. ACM, Jan. 1993. [34] K. Schauser, D. Culler, and S. \nGoldstein. Separation constraint partitioning: a new algorithm for partitioning non-strict programs into \nsequential threads. In 22nd ACM Symposium on Principles of Programming Languages (POPL 95), pages 259 \n271. ACM, Jan. 1995. [35] P. Sestoft. Deriving a lazy abstract machine. Journal of Functional Programming, \n7, 1997. [36] J. E. Smith. A study of branch prediction strategies. In International Symposium on Computer \nArchitecture, 1981. [37] Sun Microsystems. The Java HotSpot Virtual machine, White Paper, 2001. [38] \nK. Traub. Sequential implementation of lenient programming languages.Ph.D. thesis, MIT Lab for Computer \nScience, 1988. [39] C. von Dorrien. Stingy evaluation. Licentiate thesis, Chalmers University of Technology, \nMay 1989. [40] P. Wadler and J. Hughes. Projections for strictness analysis. In G. Kahn, editor, Functional \nProgramming Languages and Computer Architecture. Springer Verlag LNCS 274, Sept. 1987. A Table of Test \nStatistics   \n\t\t\t", "proc_id": "944705", "abstract": "Lazy programs are beautiful, but they are slow because they build many thunks. Simple measurements show that most of these thunks are unnecessary: they are in fact always evaluated, or are always cheap. In this paper we describe Optimistic Evaluation --- an evaluation strategy that exploits this observation. Optimistic Evaluation complements compile-time analyses with run-time experiments: it evaluates a thunk speculatively, but has an abortion mechanism to back out if it makes a bad choice. A run-time adaption mechanism records expressions found to be unsuitable for speculative evaluation, and arranges for them to be evaluated more lazily in the future.We have implemented optimistic evaluation in the Glasgow Haskell Compiler. The results are encouraging: many programs speed up significantly (5-25%), some improve dramatically, and none go more than 15% slower.", "authors": [{"name": "Robert Ennals", "author_profile_id": "81540726056", "affiliation": "University of Cambridge, Cambridge, UK", "person_id": "PP31037577", "email_address": "", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research Ltd, Cambridge, UK", "person_id": "PP43121273", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/944705.944731", "year": "2003", "article_id": "944731", "conference": "ICFP", "title": "Optimistic evaluation: an adaptive evaluation strategy for non-strict programs", "url": "http://dl.acm.org/citation.cfm?id=944731"}