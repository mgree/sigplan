{"article_publication_date": "08-25-2003", "fulltext": "\n Compiling Regular Patterns Michael Y. Levin University of Pennsylvania milevin@cis.upenn.edu ABSTRACT \nPattern matching mechanisms based on regular expressions feature in a number of recent languages for \nprocessing tree\u00adstructured data such as XML. A compiler for such a lan\u00adguage must address not only the \nfamiliar problems of pattern optimization for ML-style algebraic datatypes and pattern matching, but \nalso some new ones, arising principally from the use of recursion in patterns. We identify several fac\u00adtors \nplaying a signi.cant role in the quality of the generated code, propose two pattern compilers one generating \nback\u00adtracking target programs, the other non-backtracking and sketch proofs of their correctness.  Categories \nand Subject Descriptors D.3.4 [Programming Languages]: Processors compil\u00aders, optimization General Terms \nLanguages, Performance  Keywords pattern matching, compilation 1. INTRODUCTION A critical issue in programming \nlanguages for XML pro\u00adcessing is the design of high-level mechanisms for inspecting and decomposing tree-structured \ndata. Most recent designs can be classi.ed into two broad categories: those incorpo\u00adrating the XPath \nstandard ([7], [6]) and those based on the pattern matching primitives of functional languages. In the \nlatter category, a series of languages including XDuce [13] and its successors CDuce [4] and Xtatic [11] \nhave demon\u00adstrated the utility of regular patterns [12] ML-like patterns extended to tree regular expressions \nfor writing complex tree transducers. Regular patterns simplify the disassembly of documents into their \nsubcomponents, retrieval of elements Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. ICFP 03, August 25 29, 2003, Uppsala, Sweden. Copyright 2003 ACM 1-58113-756-7/03/0008 \n...$5.00. from the depths of complex structures, and dynamic docu\u00adment validation. Tasks that would consume \nmany error\u00adprone lines of code in a conventional language can often be encoded concisely using regular \npatterns. Naturally, these bene.ts come at the cost of more challenging compilers. Compilation of ML-style \npatterns has been addressed ex\u00adtensively in the literature (see Section 6). The main issue in translating \nthese patterns is how to minimize the number of tests performed during pattern matching while keeping \nthe size of the output code small. Regular patterns, being more expressive than ML patterns, raise the \nsame issues and add some new ones in particular, the handling of recursion. Whereas the number of tests \nrequired to determine whether a given input value matches an ML pattern is bounded by a function of the \nsize of the pattern, matching against a recursively de.ned pattern may involve a number of tests depending \non the size of the value. (For example, the recur\u00adsive pattern X de.ned by X = () | a[X],X describes \ntrees of arbitrary depth whose nodes are all labeled by a;check\u00ading that a given tree matches X involves \nexploring all of its nodes.) Since XML documents may be large, the designer of a regular pattern compiler \nmust be particularly sensitive to the performance of the generated code with respect to the size of input \nvalues. Algorithms for high-quality pattern compilation in this domain are somewhat complex, and we have \nfound it use\u00adful to spend signi.cant e.ort on developing careful proofs of correctness. To simplify these \nproofs as well as the pre\u00adsentation of the algorithms themselves we separate com\u00adpilation conceptually \ninto three phases. The .rst phase converts source program patterns into a simpler, less am\u00adbiguous form \ncorresponding to states of a non-deterministic, top-down tree automaton. The structure of tree automata, \nhowever, is both too rigid and too high level to suggest a direct way of generating equivalent target \ncode; we need a more .exible automata model to bridge the gap between the source and target languages. \nThis leads us to introducing matching automata, which extend tree automata with vari\u00adable binding, subroutines, \nand integer indices tracking which of a set of patterns match an input value. The second com\u00adpilation \nphase converts collections of tree automaton states into equivalent matching automata; the third phase \ngener\u00adates target language procedures from these matching au\u00adtomata. The matching problem investigated \nin this paper is similar to the membership problems addressed in regular tree lan\u00adguages and tree automata \ntheory. The related work section will provide a detailed comparison of these two perspectives. The contributions \nof this work are as follows. We explore a series of examples and identify several issues that greatly \nin.uence the quality of generated pattern matching code. Based on these observations, we propose two \ncompilation al\u00adgorithms for regular patterns one generating backtracking code, and the other a non-backtracking \nvariant. To present these developments, we introduce matching automata a model of target programs, allowing \nus to elide the speci.cs of the target language and reason about properties of the compilation algorithms \nat a more abstract level. The remainder of the paper is organized as follows. Sec\u00adtion 2 introduces the \nsource language of regular patterns and the low-level target language to be used as a back-end for both \nthe backtracking and the non-backtracking com\u00adpilers and previews two compilation approaches discussed \nlater in the paper. Section 3 reviews the de.nition of binary top-down tree automata, introduces matching \nautomata, de.nes two particular forms of matching automata, called simple backtracking and simple non-backtracking \n, and analyzes examples highlighting a number of important is\u00adsues arising in regular pattern compilation. \nSection 4 ex\u00adplains how to build matching automata in backtracking and non-backtracking forms. Section \n5 sketches our implementa\u00adtion and discusses some preliminary experiments. Section 6 presents related \nwork. Space constraints prevent the inclu\u00adsion here of proofs (and details of some technical de.nitions); \nthese will be provided in a companion technical report. 2. SOURCE AND TARGET LANGUAGES We begin by describing \nthe source and target languages of our pattern compiler and outlining two methods of com\u00adpiling the former \ninto the latter. The subsequent sections will develop a more formal treatment of the compilation ap\u00adproaches \noutlined here. 2.1 Source Language The data values operated on by both the target and source languages \nare sequences of elements, where each element has the form l[s],with l a label and s a sequence of child \nelements. The empty sequence is written (), but we omit the parentheses if it is delimited by a label, \nwriting just l[] instead of l[()]. Element sequences can be used in a high-level language design to represent \n(attribute-less) XML documents. For in\u00adstance, the XML element <a><b/><c/></a> can be encoded by the \nvalue a[b[],c[]]. For simplicity, we avoid dealing with attributes and base type data; while the latter \ndoes not present any additional challenges, designing and compiling attribute patterns is a signi.cant \ntask and falls outside of the scope of this paper. Besides pattern matching, the source language includes \na variety of standard constructs such as conditional and boolean expressions, let binders, value constructors, \nand mu\u00adtually recursive second-class functions. Aside from pattern matching, these forms can be compiled \ntrivially and need not be discussed here any further. Pattern matching expressions have the form match \nt with p1 . t1 | ... |pn . tn else t0; they are evaluated by .rst evaluating t,matchingthe re\u00adsult against \nthe patterns p1 ... pn, and evaluating the right hand side of the .rst clause containing a matching pattern \nor t0 if no patterns matched. If the default clause is omit\u00adted, the match expression is said to be exhaustive.In \nsuch cases, we can assume that any input value matches at least one of the patterns. (This is ensured \nstatically by the source language s type checker.) Patterns are described by the following grammar: p \n::= () | l[p] | p1,p2 | p1|p2 | X These denote the empty sequence pattern, a labeled ele\u00adment pattern, \nsequential composition and union of two pat\u00adterns, and a pattern variable. Pattern variables are intro\u00adduced \nby top-level mutually recursive declarations of the form def X=p. Top-level declarations induce a function \ndef that maps variables to the associated patterns (e.g. the above declaration implies def(X) = p.) The \nsemantics of patterns is de.ned by a binary relation v . p over values and patterns. The relation is \ndescribed by the following rules: .rst, () . (); second, l[v] . l[p] if v . p;third, v . X if v . def(X);fourth, \nv . p1,p2 if v is the concatenation of two sequences v1 and v2 such that v1 . p1 and v2 . p2; and .nally, \nv . p1|p2 if v . p1 or v . p2. The patterns presented here are not designed to bind vari\u00adables; they \ncan only check whether a value matches a cer\u00adtain shape or not. Extending patterns with term variable \nbinders does not introduce and signi.cant conceptual com\u00adplications, but it makes the formal development \nsubstan\u00adtially more tedious. Our implementation handles full pat\u00adterns with binders, but we only cover \nbinder-less patterns in this paper. Example 1. The following program de.nes two mutually recursive patterns, \nX (matching a[]; a[a[a[]],a[]];etc.) and Y (matching (); a[a[]],a[]; etc.), and a function that checks \nwhether its argument matches X or Y and returns 1 or 2, respectively, or 0 if the argument matches neither \npattern. fun F(x) = match x with def X = a[Y],Y |X . 1 def Y= ()| a[X],X |Y . 2 else . 0 We will use \nabbreviations p? and p* to denote respectively () |p and X where X is de.ned by the top-level declaration \ndef X= () |p,X. A more detailed discussion of the source language can be found in [13, 12]. 2.2 Target \nLanguage We employ a target language that is similar to the inter\u00adnal language of OCaml [10]. Most of \nthe source language constructs have a direct counterpart in the target language, but pattern matching \nis realized by a simpler case construct. Case expressions have the form case x of p1 . e1 ... pn . en \nelse e0 where patterns must be non-overlapping and can only be of two kinds: (), which matches the empty \nse\u00adquence; and l[x],y, which matches a sequence starting with an element tagged by l and binds the contents \nof the .rst element to x and the sequence of the remaining elements to y. Evaluation of case expressions \nproceeds similarly to eval\u00aduation of match expressions the input is matched against the patterns until \nthe .rst matching pattern is found and the corresponding right hand side is evaluated.  2.3 Two Compilation \nSchemes We intend to study in detail two compilation schemes, dif\u00adfering in their handling of recursive \npatterns. In the back\u00adtracking approach, every recursive pattern induces a target fun F(x) = fun F(x) \n= case x of casexof |() . 2 |() . | a[x],y . 2 let pr1 = XY(x) in | a[x],y . let pr2 = XY(y) in if Y(x) \n&#38;&#38; Y(y) if p2(pr1)&#38;&#38; p2(pr2) then 1 then 1 else else if X(x) &#38;&#38; X(y) if p1(pr1)&#38;&#38; \np1(pr2) then 2 then 2 else 0 else 0 else 0 else 0 fun Y(x) = fun XY(x) = casexof casexof |() . 1 |() \n. [0,1] | a[x],y . | a[x],y . if X(x) &#38;&#38; X(y) let pr1 = XY(x) in then 1 let pr2 = XY(y) in else0 \n[p2(pr1)&#38;&#38; p2(pr2), else 0 p1(pr1)&#38;&#38; p1(pr2)] (a) (b) Figure 1: Backtracking (a) and \nnon-backtracking (b) target programs language helper function that returns true (1) if its input matches \nthe pattern or false (0) if it does not. Figure 1(a) shows the result of compiling the source program \nof Exam\u00adple 1 using the backtracking approach. The two mutually recursive functions X and Y correspond \nto the source pat\u00adterns with the same names. (For brevity, we omit X;it is similar to Y, exceptthatit \ndoes not checkfor theempty sequence.) This program is backtracking because the tests of the else if branch \nof F involve traversing the values that are also processed during the tests of the if branch. In the \nnon-backtracking approach, helper functions cor\u00adrespond to sets of recursive patterns. Instead of returning \nbooleans, they return tuples of booleans [t1,...,tn] in\u00addicating which of the set of patterns match the \nfunction s input. Figure 1(b) contains the result of compiling the sam\u00adple program in the non-backtracking \napproach. The helper function XY returns a pair whose .rst and second compo\u00adnents correspond to patterns \nX and Y respectively. The advantages of the backtracking approach are that op\u00aderations on boolean values \nare more e.cient than operations on tuples of boolean values and that the number of helper functions \nis guaranteed to be at most linear in the size of the patterns. The price of this is backtracking and \nsubopti\u00admal performance for some matching problems. Conversely, the non-backtracking approach generates \nprograms that em\u00adploy more complex operations and potentially exponentially many helper functions, but \nthat are always guaranteed to run in time at worst linear in the size of the input. We now proceed to \na more formal development of these compilation schemes.  3. MATCHING AUTOMATA In this section, we review \nstandard top-down tree au\u00adtomata, introduce matching automata, de.ne special back\u00ad Figure 2: Tree Automaton \ntracking and non-backtracking forms of matching automata, and present examples of matching automata illustrating \nsev\u00aderal important compilation issues. 3.1 Tree Automata The semantics of source patterns described in \nthe previous section does not directly lead to an e.cient pattern match\u00ading algorithm because of associativity \nand distributivity of pattern formation constructors and non-determinism in the rule for matching against \npattern concatenations. To give us a better starting point for generating e.cient pattern matching code, \nwe convert source patterns into a form with a substantially simpler pattern matching seman\u00adtics. This \nform of patterns can be described by states of a non-deterministic top-down tree automaton. De.nition \n1. A non-deterministic top-down tree automa\u00adton is a tuple A = (S,T),where S is a set of states and T \nis a set of transitions consisting of empty transitions of the form s . () and label transitions of the \nform s . l[s1],s2, where s,s1,s2 . S and l is a label. The acceptance rela\u00adtion on values and states, \ndenoted v . s, is de.ned by the following rules: s . l[s1],s2 . T s . () . Tv1 . s1 v2 . s2 (TA-Emp)(TA-Lab) \n() . s l[v1],v2 . s The patterns of Example 1 shown in the previous section can be converted into a tree \nautomaton with two states, S = {s1,s2} (corresponding to patterns X and Y respectively), two label transitions, \ns1 . a[s2],s2 and s2 . a[s1],s1, and an empty transition s2 . (). Let us derive a[a[]],a[] . s2. (1) \na[a[]],a[] . s2 by TA-Lab from 2 instances of (2) (2) a[] . s1 by TA-Lab from 2 instances of (3) (3) \n() . s2 by TA-Emp Tree automata can be depicted by graphs whose nodes and edges represent states and \ntransitions as follows. For any transition of the form s1 . l[s2],s3, there is an edge labeled l from \nthe node corresponding to s1 toabar which has two outgoing edges: one, labeled in, leads to the node \ncorresponding to s2, and the other, labeled out, leads to the node corresponding to s3. An empty transition \nof the form s . () is represented by a dangling edge labeled by (). Figure 2 shows the tree automaton \ndiscussed above. From now on, we will assume that source program pat\u00adterns have been converted into a \ntree automaton, and the subsequent algorithms will deal with the states of this au\u00adtomaton. Hosoya and \nPierce [12] give a detailed description of the algorithm converting a collection of source patterns into \nstates of a tree automaton. Essentially, it transforms patterns into a disjunctive normal form by applying \nassocia\u00adtivity of concatenation and distributivity of concatenation with respect to union. 3.2 Matching \nAutomata Tree automata are a good .rst step, but several factors make them inappropriate for representing \nlow-level pattern matching code. The .rst has to do with handling values of the form l[v1],v2. While \na tree automaton processes v1 and v2 independently, a target program should be able to handle them sequentially \nand use information obtained during inspection of v1 to drive processing of v2.The sec\u00adond concerns the \ntreatment of recursive patterns. While, in tree automata, circularities entailed by recursive patterns \nare implicit in the transition relation, in the target language they must be implemented by recursive \nprocedures. Since there may be multiple ways of achieving this goal, tree au\u00adtomata transitions alone \nare not su.cient for modeling tar\u00adget language. The third issue arises from the fact that tree automata \nare designed to match against a single pattern, whereas, to implement match expressions, we need a mecha\u00adnism \nfor matching e.ciently against collections of patterns. For more e.ective processing of subtrees, we \nintroduce transitions with variables. This new kind of transitions has a source state, a target language \npattern that determines when the transition is applicable and binds the subtrees of the current value, \nand a set of destination pairs that spec\u00adify the continuation of the transition. For instance, a tree \nautomaton transition s1 . a[s2],s3 canbe rewrittenas a transition with variables s1 : a[x],y .{x . s2,y \n. s3}. When s1 receives a value a[v1],v2, the automaton binds x to v1 and y to v2 and transfers control \nto the destination states sending the contents of x to s2 and the contents of y to s3. Once we introduce \nvariables, v1 and v2 do not have to be processed immediately; instead, the automaton can pro\u00adceed examining \none of them while keeping the other stored in a variable for future reference. The following example \ndemonstrates that such .exibility can be advantageous. Example 2. It can be shown that there does not \nexist a deterministic top-down tree automaton that implements the pattern a[],b[] | a[b[]]. With the \nhelp of transitions with variables, however, it is possible to recognize it de\u00adterministically. Consider \nan automaton (S,T) where S = {s1,s2,s3,s2},and T contains the following transitions: s1 : a[x],y .{x \n.s2} s2 : b[w],z .{w .s4,z .s4,y .s4} s2 : () .{y .s3} s3 : b[x],y .{x .s4,y .s4} s4 : () .\u00d8 The transition \noriginating in s1 saves subtrees v1 and v2 of the input value a[v1],v2 in x and y and sends the con\u00adtents \nof x to s2.In s2, the automaton examines the shape of x and, depending on the result, processes the contents \nof y:if x contains a b-labeled element, y is sent to s4;if x con\u00adtains the empty sequence, y is sent \nto s3. Observe that the above automaton is deterministic since no state is a source of multiple transitions \nwith the same label. In addition to simple transitions with variables discussed in the above example, \nwe introduce subroutine transitions to make automata look more like target language code with respect \nto handling recursive patterns. A subroutine tran\u00adsition has the form s : A .{y1 . s1,...,yk . sk} where \nA is the name of a subroutine automaton. When s receives avalue v, the subroutine automaton A is invoked, \nand, if it accepts v, the destination pairs are evaluated as in simple transitions. To support matching \nagainst multiple patterns, we intro\u00adduce index sets and index mapping relations. The idea is for an automaton \nnot to simply accept or reject its input, but also to output an integer index in case of acceptance. \nFor in\u00adstance, an automaton for a matching problem based on pat\u00adterns p1, ..., pk would output an index \ni .{1,...,k}i. its input matches pi. To accommodate computing with indices, we enrich simple transitions \nwith index sets and subroutine transitions with index mapping relations. Thus, a simple I transition \nhas the form q : p .{y1 . q1,...,yn . qn}where I is a set integer indices, and a subroutine transi\u00ad s \ntion has the form q : A .{y1 .q1,...,yn .qn}where s is a binary relation on indices. The index set in \na simple transition indicates which of the original patterns can still match the input value when the \ntransition is taken. The index mapping relation in a subroutine transition serves a similar function: \nthe pattern pk can still be matched when the transition is taken as long as the subroutine automaton \naccepts the current value returning j and (j .k) .s. The following de.nition summarizes all of the above \ncon\u00adcerns. We write E[v1/x,v2/y] to denote an environment mapping x to v1 and y to v2 and agreeing with \nE on all other variables and E\\y to denote an environment which is unde.ned on y and otherwise equal \nto E. De.nition 2. A matching automaton is a tuple (Q,qs,R), where Q is a set of states, qs is a start \nstate, and R isaset of transitions. There are two kinds of transitions: simple and subroutine. They have \nthe following structure: I q : p .{y1 .q1,...,ym .qm} (simple) s q : A .{y1 .q1,...,ym .qm} (subroutine) \nBoth types of transitions have a source state q and a set of destination pairs {y1 .q1,...,ym .qm}. A \ndestination pair consists of a destination variable yi and a destination state qi. A simple transition \ncontains a target language pat\u00adtern p which can be of the form () or l[x],z and a set of integer indices \nI. A subroutine transition contains a sub\u00adroutine automaton name A and a relation s mapping indices to \nindices. Let M be a mapping of automaton names to matching automata and let A = (Q,qs,R) be a matching \nautomaton. The acceptance relation E fv .q .k is de.ned on envi\u00adronments, values, states, and indices \nby the following rules. I q : () .{y1 .q1,...,ym .qm}.R k .I .i.{1,...,m}.E\\yi fE(yi) .qi .k (MA-Emp) \nE f() .q .k I q : l[x],z .{y1 .q1,...,ym .qm}.R k .I E' = E[v1/x,v2/z] .i .{1,...,m}.E'\\yi fE'(yi) .qi \n.k (MA-Lab) E fl[v1],v2 .q .k s q : B .{y1 .q1,...,ym .qm}.R E fv .M(B) .j (j,k) .s .i.{1,...,m}.E\\yi \nfE(yi) .qi .k (MA-Sub) E fv .q .k Avalue v is accepted by the automaton A with an index k in an environment \nE, written E f v . A . k,if it is accepted by the automaton s start state: E fv .qs .k. The rule MA-Emp \nsays that the empty sequence () is accepted by a state q in an environment E returning an index k if \nthere is a transition of the form q : () . ... and for each destination pair yi . qi, the value E(yi)is \naccepted by qi returning k in an environment obtained from E by removing y s binding. MA-Lab describes \nhow a state can accept a value l[v1],v2 using a transition of the form q : l[x],z . .... It is similar \nto MA-Emp except that the environments used for checking the destination pairs are extended with bindings \nof x to v1 and z to v2. MA-Sub deals with subroutine transitions. A value is accepted by a state q in \nan environment E producing an index k if there is a transition of the form q : B . ..., the automaton \nM(B) accepts v in E producing an index j such that (j .k)is in the transition s index mapping relation, \nand the destination pairsare checked asin MA-Emp. The index mapping relations in subroutine transitions \nserve two purposes. First, they allow us to reduce the number of subroutine automata since we can avoid \nbuilding isomor\u00adphic automata that only di.er in their indices. Second, and more importantly, they are \nessential for creating matching automata that represent non-backtracking target programs. Let us consider \na matching automaton which implements Example 1. This automaton, let us call it XY, consists of states \nq1 (the start state) and q2 and transitions I1 q1 : () .\u00d8 where I1 = {2}, I2 q1 : a[x],y .{x .q2} where \nI2 = {1,2},and s1 q2 : IC .\u00d8 where s1 = {1 .1,2 .2} The subroutine automaton IC contains states q3 (its \nstart state) and q4 and subroutine transitions s2 q3 : XY .{y .q4} and s2 q4 : XY .\u00d8 where s2 = {1 .2,2 \n.1} Diagrams of matching automata are similar to those of tree automata except that they must account \nfor the addi\u00adtional annotations on transitions. Edges must be annotated with index sets in case of simple \ntransitions and index map\u00adping relations in case of subroutine transitions. The parts of edges connecting \na bar to a destination state are labeled by the corresponding destination variable instead of the key\u00adwords \nin and out used in tree automata .gures. Figure 3(b) shows the automata discussed in the above example. \nThe goal of XY is to output 1 if its input matches X or 2 if its input matches Y. Let us derive \u00d8fa[a[]],a[] \n.XY .2. (1) \u00d8fa[a[]],a[] .XY .2 by De.nition 2 from (2) (2) \u00d8fa[a[]],a[] .q1 .2by MA-Lab from (3)  \n(3) E1 fa[] .q2 .2 by MA-Sub from (4) E1 = \u00d8[a[]/y] (4) E1 fa[] .IC .2 by De.nition 2 from (5) (5) E1 \nfa[] .q3 .2 by MA-Sub from (6,7) (6) E1 fa[] .XY .1 derived similarly to (1) (7) \u00d8fa[] .q4 .2 by MA-Sub \nfrom (8) (8) \u00d8fa[] .XY .1 derived similarly to (1)  3.3 Special Forms of Matching Automata The de.nition \nof matching automata is very .exible. For instance, tree automata can be viewed as a special case of \n z  Y a[z],y y Y X= {1} {1 .1} {1 .1} (a) XY = z IC a[z],y {1 .1,2.2} XY y XY IC = {1 .2,2.1} \n{1 .2,2.1} (b) Figure 3: Matching Automata in Simple Backtrack\u00ading (a) and Non-Backtracking (b) Forms \nmatching automata that implement single clause match ex\u00adpressions. As a result of this .exibility, not \nevery match\u00ading automaton can be easily mapped to a target language program. The purpose of the forthcoming \ndevelopment is to identify matching automata for which this mapping is straightforward. Speci.cally, \nwe will de.ne two sets of re\u00adstrictions ensuring that matching automata correspond di\u00adrectly to either \nbacktracking or non-backtracking target lan\u00adguage programs introduced in the previous section. We say \nthat automata satisfying the former set of restrictions are in simple backtracking form, and automata \nsatisfying the latter set of restrictions are in simple non-backtracking form. As we introduce various \nrestrictions, we will show how parts of matching automata correspond to target language expressions. \nWe will use the matching automata of Figure 3 and the corresponding programs of Figure 1 to illustrate \nthis connection. We begin by discussing restrictions that are pertinent to both backtracking and non-backtracking \nstyles. We say that a matching automaton is sequential if each of its transitions has at most one destination \npair. The automata of Figure 3 are sequential, for example, but the automaton shown in Figure 4(b) below \nis not: its non-.nal transitions have two destination pairs. We say that a matching automaton is disjoint \nif for any state of the automaton, simple transitions originating in this state are non-overlapping. \nAutomata of Figure 3 are dis\u00adjoint; the automaton of Figure 4(b) is not since q1 is a source of two a-transitions. \nA state of a sequential and disjoint automaton can be con\u00adverted to a target language case expression, \neach outgoing transition corresponding to a case branch. For example, ob\u00adserve how states q1 and q8 of \nFigure 3(a) and q1 of Figure 3(b) correspond to the case expressions appearing in Figure 1. The remaining \nrestrictions are related to subroutine tran\u00adsitions. We say that an automaton has separated transitions \nif whenever a state has an outgoing subroutine transition, all other transitions originating in this \nstate are subroutine transitions as well. States of such automata can be par\u00adtitioned into subroutine \nand simple states; each serving as a source of only the corresponding kind of transitions. Con\u00adsider, \nfor instance, the automaton Y showninFigure 3(a). It has a simple state q8 and subroutine states q9 and \nq10. The next concept is speci.c to the backtracking form of matching automata. We say that a matching \nautomaton is boolean if, for any I of its simple transitions q1 : p .... and for any of its s subroutine \ntransitions q2 : A ....,it is the case that I s = range(s)= {1}. A subroutine transition q : A .... is \nboolean if dom(s)= {1}. Since boolean matching automata involve a single index, their function, like \ntree automata, is either to accept or to reject the input value. Thus, in the target language boolean \nautomata can be represented by boolean functions. The backtracking compilation approach employs two kinds \nof matching automata: a matcher implements a source match expression; a boolean acceptor implements a \nparticular pat\u00adtern and returns 1 if it matches the input value. Only the latter kind of automata are \nused as subroutines in the back\u00adtracking method. The automata shown in Figure 3(a) are in simple back\u00adtracking \nform. In particular, F is a matcher and X and Y are boolean acceptors. Subroutine states q2, q6,and q9 \ncorre\u00adspond to the if expressions in the target program. Each of these states is the start of one or \nmore call tails sequences of subroutine transitions sharing the same index mapping relation s. A call \ntail corresponds to an if branch: the conjunction of the subroutine calls constitutes the test, and the \nindex occurring in range(s) is returned if the test succeeds. The following de.nition summarizes the \nabove restrictions. De.nition 3. A collection of matching automata is said to be in simple backtracking \nform (SBF)if itcan be partitioned into matchers and acceptors, all of which are sequential and disjoint \nautomata with separated transitions. Furthermore, only acceptors may serve as subroutines, and acceptors \nand subroutine transitions must be boolean. In the non-backtracking scheme, subroutine states must have \nat most one outgoing subroutine transition. (This is what ensures that there is no backtracking!) To \nsatisfy this condition, we remove the restriction requiring subroutines to be boolean (so there is no \nlonger a distinction between acceptors and matchers) and introduce an additional kind of matching automata \ncalled index converters whose purpose is to make a sequence of subroutine calls and convert the returned \nindices. The automata shown in Figure 3(b) are in simple non-backtracking form; XY is a matcher, and \nIC is an index converter. Note that in this example, the index converter is unneces\u00adsary; we can inline \nthe subroutine call invoking the converter by substituting q3 for q2 without changing the meaning of \nXY. Such inlining, however, is not always possible. Example 5 below will involve an essential index converter \nthat can\u00adnot be eliminated. The following de.nition formalizes the simple non-backtracking form. De.nition \n4. A collection of matching automata is said tobein a simple non-backtracking form (SNBF) if it can be \npartitioned into a collection of matchers and index con\u00adverters, all of which are sequential and disjoint \nautomata with separated transitions. Furthermore, converters may only call matchers and vice versa; any \nsubroutine state may be the source of exactly one transition; subroutine transi\u00adtions in matchers must \nbe .nal; and index converters may only contain subroutine states. In the target language, matchers are \nimplemented by func\u00adtions that return tuples of booleans. The elements of the tu\u00adple correspond to the \ndi.erent indices output by the matcher. The matcher outputs an index i. the function returns a tu\u00adple \nwith the corresponding element set to true. For example, matcher XY outputs 1 or 2; thus, the corresponding \ntarget function returns a pair. A subroutine state in a matcher gives rise to a let expression whose \ncomponents are gener\u00adated from the matcher s subroutine transition as well as the subroutine transitions \nof the index converter. 3.4 Examples While designing our regular pattern compiler, we found that several \nfactors play a major role in the quality of the output code. Sometimes we were surprised by a dramatic \ne.ect of some seemingly innocuous change to the compiler on either the performance or size of the generated \ncode. The following series of examples is an extract of what we believe are the most important lessons \nlearned from our ex\u00adperiments. Example 3. Our .rst example illustrates two points. First, we show how \nwe can create a matching automaton by a simple modi.cation of the tree automaton corresponding to the \nmatching problem s patterns. We then show how the obtained matching automaton can be converted into a \nsub\u00adstantially more e.cient matching automaton. Consider the following source program fragment: match \nx with def A = () | a[A],A | a[a[]],a[A] .1 | a[A] .2 A tree automaton built from the patterns of this \nexpres\u00adsion is shown in Figure 4(a). Its states s1 and s3 correspond to the .rst and second patterns \nrespectively. A correspond\u00ading matching automaton can be constructed directly from the states and transitions \nof the tree automaton. First, we must account for the di.erence in the structure of transi\u00adtions by modifying \ntree automaton transitions of the form s1 .a[s2],s3 and s1 .() into transitions with vari\u00adables s1 : \na[x],y .{x .s2,y .s3}and s1 : () .\u00d8 respectively. We also must create a start state that com\u00adbines the \ntransitions originating in s1 and s3,the states corresponding to the patterns of the match expression. \nFi\u00adnally, we must annotate the transitions with appropriate in\u00addex sets. The result of this transformation \nis the matching automaton shown in Figure 4(b). It succeeds, outputting 1 or 2, if its input matches \nthe .rst or second pattern of the match expression, respectively; if the input matches neither, the automaton \nfails. in (a) (b)  x a[_],_ a[x],y y {1,2} {2} (c) Figure 4: Example 3 This matching automaton can \nbe improved. Observe that the match expression of this example is exhaustive (it has no else branch), \nso we may assume that the matching automa\u00adton will never receive an input value that does not match either \nsource pattern. For values that match one of the patterns, it is su.cient to count the number of top-level \nel\u00adements: if there are two, then the input value matches the .rst pattern; if one, then the second pattern. \nThis is imple\u00admented by the matching automaton shown in Figure 4(c). In q1, it receives a value of the \nform a[v1],v2 and stores v1 in x and v2 in y; then, in q2 it investigates the contents of y, and, if \nit is of the form a[v3],v4, returns 1, or, if it is of the form (), returns 2. Note, that investigating \nthe contents of x before the contents of y would not immediately reveal the answer since learning that \nv1 is of the form a[v3],v4 does not tell us whether the input value matches the .rst or the second source \npattern. We now move to a discussion of issues related to subrou\u00adtine automata and subroutine transitions \nthe issues that in.uenced most of the essential design choices during the development of our compiler. \nExample 4. In this example, we discuss a program whose most obvious implementation has exponential running \ntime. Consider the following fragment: def A = a[B] | a[C] match x with defB=b[]|a[A] |A . 1 defC=c[]|a[A] \nelse0 Letus try to convertthis match expression into a matching automaton in SBF. Our .rst instinct is \nto associate each B y () x a[x],y A= C  () x () B= () xA (a) y ()  x () b[x],y a[x],y y () x A= \ny () xA (b) Figure 5: Example 4 of the three recursive patterns with a separate subroutine automaton; \nFigure 5(a) shows the corresponding solution. (We omit C since it is similar to B. Also, since the match \nexpression of this example consists of a single clause, all index sets and index mapping relations in \ntransitions are vacuously {1} and {1 . 1} respectively; so, we omit them from the .gure.) Observe that \nA, if executed sequentially, will take expo\u00adnentially many steps to reject a value of the form a[a[ ... \n[d[]] ... ]]. The source of this ine.ciency lies in the fact that there are two subroutine transitions \noriginating in q3, and the automaton will backtrack, repeatedly trying one of the transitions, failing, \nand trying the other transition. We can obtain a linear matching automaton by observ\u00ading that it is not \nnecessary to associate a subroutine with each pattern de.ned recursively in this example. Figure 5(b) \ndisplays a solution in which only A has a corresponding matching automaton. Instead of having subroutine \ncalls to B and C, this automaton incorporates their states and tran\u00adsitions directly. The new automaton \nis non-backtracking since it does not have a state with more than one outgoing subroutine transition. \nThis example shows the bene.t of minimizing the number of subroutine automata. Later, however, we will \nsee that this strategy should not be applied naively because it can lead to a huge size explosion. Example \n5. This example shows that, for some matching problems, using boolean subroutines is not enough. Con\u00adsider \nthe following program: def X = b[] | a[Y],Z | a[Z],Y match x with defY=c[]|a[Z],X|a[X],Z |X . 1 def Z \n= d[] | a[X],Y | a[Y],X else 0 A boolean matching automaton for this match expression can be constructed \nsimilarly to the automaton of Figure 5(a) and, similarly, it will exhibit exponential running time. Un\u00adlike \nthat example, however, it is not clear how to transform the exponential automaton into a more e.cient \nboolean au\u00adtomaton. In such cases, we can fall back to using more  () XYZ = {1} () {2} () {3} {3} \n XYZ y XYZ IC = {1 . 4,1 . 5, {1 . 3,1 . 6, 2 2 . 2,2 . 5, . 1,2 . 6, 3 3 . 1,3 . 4} . 2,3 . 3}  \nFigure 6: Example 5 general subroutine automata of SNBF. The automaton XYZ shown in Figure 6 implements \nthis matching problem it outputs 1, 2, or 3 if the input matches X, Y,or Z respec\u00adtively. XYZ, like any \nautomaton in SNBF, is linear. As we have mentioned before, in this example, it is not possible to achieve \nthe desired behavior by circumventing the index converter and making the two XYZ calls from q2. Such \nan automaton would not distinguish values matching a[Y],Z and a[Z],Y they should be accepted and 1 should \nbe returned from values matching a[Y],Y and a[Z],Z they should be rejected. Example 6. We conclude with \nan example showing a po\u00adtential drastic explosion of the size of matching automata. Consider the following \nprogram: def D = d[D?] def E = e[E?] def A1 =a1[C1] match x with def A2 =a2[C2] |C3 . 1 def C1 =D|E else0 \ndef C2 =A1,C1 def C3 =A2,C2 Only patterns D and E are recursive, and, so, it is reason\u00adable to associate \nsubroutine matching automata with these and only these patterns. Let us try to build matching au\u00adtomata \nfor C1, C2,and C3 in succession. Figure 7(a) shows a matching automaton for C1; it has two .nal states \nwith sub\u00adroutine transitions. The same .gure displays a signi.cantly larger matching automaton for C2. \n(The omitted part of C2 indicated by ... is similar to the part above it: q6 is isomor\u00adphic to q5.) Automaton \nC2 has four .nal states. If we build a matching automaton for C3 following the same pattern, it will \nhave sixteen .nal states and will not .t on the page. The size grows double-exponentially in the size \nof the source pattern! To avoid the above size explosion, it is su.cient to asso\u00adciate a subroutine automaton \nwith C1 as well as with D and E. Figure 7(b) shows a compact matching automaton for C3 that takes advantage \nof C1 s subroutine automaton. Armed with various insights from these examples, we now proceed to a description \nof our compilation algorithms.  4. COMPILATION Section 3.1 discussed how patterns of a source program \ncan be converted into states of a top-down nondeterministic tree automaton A = (S,T). This section describes \nthe com\u00adpilation algorithm that builds matching automata in simple backtracking or non-backtracking forms \nfor matching prob\u00adlems speci.ed in terms of elements of S.In particular, we will show how, given an ordered \nsequence of tree automa\u00adton states s1 ... sn, we can build a matching automaton, in either of the two \nspecial forms, that on an input value v outputs k i. v .sk. We start by giving a top-level overview of \nthe SBF compi\u00adlation algorithm. We then discuss in more detail several key techniques employed in the \nalgorithm and state several of its properties. We conclude the section with an outline of the SNBF compilation \nalgorithm and summarizing complexity of the presented algorithms. 4.1 Outline of SBF compilation The \ncompilation algorithm will manipulate a data struc\u00adture that is a generalization of sequences of tree \nautomaton states. In this data structure, tree automaton states are arranged into a matrix whose rows \nand columns are associ\u00adated with integer indices and variables respectively. More formally: De.nition \n5. Let A = (S,T) be a tree automaton. A con.guration over S consists of a tuple of distinct variables \n<x1,. ..,xn> and a set of tuples {(s11,...,s1n,j1),...,(sm1, ..., smn,jm)} each associating a collection \nof states from S to an integer index. A con.guration can be depicted as follows: x1 ... xn C = s11 ... \ns1n j1 ... sm1 ... smn jm We say that an environment E satis.es C yielding an index jr, written E |= \nC .jr,if E(xi) .sri for all i .{1,...,n}. A con.guration C is satis.able by an environment E if there \nexists an index k such that E |= C .k. Because of space considerations, we cannot give a detailed presentation \nof the compilation algorithm. Instead, we will discuss the algorithm s important aspects by considering \na series of examples. We start by describing the algorithm that generates matching automata in SBF, and \nthen point out how it can be modi.ed to generate matching automata in SNBF. The core of the SBF algorithm \nis a recursive function sbf that takes a con.guration and produces a matching automa\u00adton. Every invocation \nof sbf associates its parameter con\u00ad.guration C with a state of the generated matching automa\u00adton and \nconstructs the transitions originating in this state by expanding C. Expansion produces a collection \nof smaller residual con.gurations that are then processed recursively. The algorithm terminates when \nthe current con.guration has no columns. At the top level, the SBF algorithm proceeds as follows. It \n.rst builds an initial con.guration corresponding to each match expression of the source program. To \nensure that there is a subroutine automaton for any generated subrou\u00adtine transition, the algorithm also \ncreates a single-state con\u00ad.guration for every state of the input tree automaton. The algorithm then \ninvokes sbf on all of the obtained con.gu\u00ad  (a) (b)  Figure 7: Example 6 rations thus producing \nthe resulting collection of matching automata. The following three sections provide more details. First, \nwe describe two expansion techniques: one for generating simple transitions, and the other for generating \nsubroutine transitions. Then, we show how sbf determines which of the two expansion techniques should \nbe applied to a given con.guration and, in a related development, address the size explosion concern \nraised in Example 6. We then describe several techniques for optimizing con.gurations that lead to smaller \nand more e.cient matching automata.  4.2 Two Con.guration Expansion Techniques Example 7. Let us illustrate \nthe expansion method pro\u00adducing simple transitions. We refer to this kind of expansion as expansion by \nlabel. Consider the following diagram that illustrates the .rst step in the construction of the matching \nautomaton shown in Figure 4(c). x a[x],y s1 1 s3 2 {1,2} C1 ? xy s2 s3 1 s5 s4 2 C2 Here, C1 is the \ninitial con.guration that corresponds to the source state of the matching automaton. It includes the \ntree automaton states s1 and s3 that represent the source pat\u00adterns of the underlying match expression \n(see Figure 4(a)). To expand a con.guration by label, we must select one of its columns and a label appearing \nin some transition whose start state is an element of the selected column. There is only one column in \nC1; so, it is selected for expansion (as indicated by the vertical arrow.) Both of the states in the \nselected column are sources of a single a-labeled transition. Therefore, there is only one way to expand \nC1 by label, and the result of this expansion is the residual con.guration C2. C2 is obtained from C1 \nas follows. First we remove the selected column and the rows whose state in the selected column does \nnot have an a-labeled transition. (No rows are removed in our case.) Then we generate two variables that \ndo not appear in the obtained con.guration and add two new columns consisting of these variables and \nthe successor states of the a-labeled transitions. In particular, the con\u00adtents of the two new columns \nin the .rst row of C2 arises from the transition s1 . a[s2],s3, and the contents of the second row arises \nfrom the transition s3 . a[s5],s4. C1 and C2 correspond to the states q1 and q2 of the match\u00ading automaton \nshown in Figure 4(c). The pattern part of the transition between C1 and C2 consists of the label a and \nthe variables used in the expansion. The index set consists of the indices appearing in the destination \ncon.guration. The destination variable is unknown at this stage; it will be de\u00adtermined by the column \nthat is selected for expansion of C2. The following .gure shows how C2 is expanded. y xy s2 s3 1 s5 \ns4 2 C2 C4 Let us base expansion of C2 on the second column (hence y becomes the destination variable \nof the transition generated in the previous paragraph.) There are two distinct labels occurring in transitions \nwhose source states are in the sec\u00adond column: () and a.Hence, we must expand C2 twice: once with respect \nto each label. Expanding C2 by a is done similarly to how we expanded C1. Expanding by () is even simpler: \nit follows the same steps as expanding by a but does not involve generating variable names or introducing \nnew columns. Example 8. The second kind of expansion produces sub\u00adroutine transitions and is called expansion \nby state.Consider the following con.gurations corresponding to states q2, q3, and q4 of Figure 3(a). \n xy s2 s2 1 s1 s1 2 C1 C3 in ... out out a2 a1 ()  ... Figure 8: Tree Automaton for Example 6 Just \nas in expansion by label, we must select a column whose states will serve as the basis for expansion. \nThe .rst column of C1 was selected in this example. There are two distinct states in the .rst column, \nand, so, C1 is expanded two times: by s2 resulting in C2,and by s1 resulting in C3. C2 is obtained from \nC1 by removing the selected column and the rows whose state in the selected column is not equal to s2. \nC3 is produced similarly. In the generated transitions, M<s1> and M<s2> denote subroutine matching automata \ncor\u00adresponding to states s1 and s2. The compilation algorithm constructs these automata using initial \nsingle-state con.gu\u00adrations (<x>,{(s1, 1)})and (<x>,{(s2, 1)}) respectively. 4.3 Loop Breakers To help \nus determine whether a con.guration should be expanded by label (as in Example 7) or by state (as in \nEx\u00adample 8), we introduce the following concept. De.nition 6. Let A = (S,T) be a tree automaton. We say \nthat Rec . S is a set of loop breakers for A if removing the transitions originating in Rec ensures that \nthe remaining transition relation is acyclic. The initial con.guration that corresponds to the start \nstate of the matching automaton is always expanded by la\u00adbel. A non-initial con.guration is expanded \nby state if all of its columns contain a loop breaker. If a non-initial con\u00ad.guration contains columns \nthat have no loop breakers, one of such columns is selected for expansion and the con.gu\u00adration is expanded \nby label. This strategy ensures that the compilation algorithm terminates. The goal of the initial con.guration \nrule is to prevent generating a non-terminating matching automaton that calls itself (or some other automa\u00adton) \nrecursively without making any progress. The above de.nition speci.es a necessary condition for a set \nof loop breakers, but does not tell us how to compute it. Let us consider several alternatives. The .rst \none is the set of all states of a tree automaton; it is the maximal set of loop breakers. If the compilation \nalgorithm uses the maxi\u00admal set of loop breakers, the size of the generated matching automaton is guaranteed \nto be at worst linear in the size of the input tree automaton. The disadvantage of this ap\u00adproach is \nthat it generates too many subroutine transitions resulting in more backtracking and extra cost of subroutine \ninvocation. At the other extreme are minimal sets of loop breakers. They lead to more e.cient matching \nautomata by minimiz\u00ading the number of subroutine transitions as shown in Exam\u00adple 4. Example 6 demonstrates, \nhowever, that minimal sets of loop breakers can result in a matching automaton with a double exponential \nnumber of states. To understand what causes the double exponential blowup, consider Figure 8 that shows \nthe tree automaton associated with Example 6. Observe, that the transitions originating in s1 and s2 \nduplicate the destination state and that s3 has two outgoing transitions. Both these factors contribute \nto the blowup. In view of this, we consider two other approaches to com\u00adputing the set of loop breakers: \na multiple predecessor set of loop breakers is the union of a minimal set and the set of states with \nmultiple incoming transitions (such as s2 and s3); a multiple successor set of loop breakers is the union \nof a minimal set and the set of states with multiple outgoing transitions (such as s3). Using multiple \nsuccessor or mul\u00adtiple predecessor loop breakers ensures that the size of the generated automaton is \nat worst exponential or polynomial respectively. For the tree automaton of Figure 8, for exam\u00adple, the \nmultiple successor set of loop breakers {s3, s4, s6}leads to the matching automaton shown in Figure 7(b). \nOur current compiler implementation uses the same loop breaker set throughout compilation. Tests have \nshown that using either multiple predecessor or multiple successor sets of loop breakers results in satisfactory \ntarget programs that are almost as fast as those generated with the minimal set of loop breakers and \nalmost as small as those generated with the maximal set of loop breakers. In the future, we would like \nto consider an adaptive strategy that starts with the minimal set of loop breakers and switches to a \nmore size conscious set of loop breakers if the generated program ex\u00adceeds a certain size threshold. \n 4.4 Optimizing Con.gurations Depending on whether the input match expression is ex\u00adhaustive or not, \nthe sbf operates in either the exhaustive or non-exhaustive mode. In the exhaustive mode, sbf can take \nadvantage of the fact that only values that match one of the alternatives can be given as input to the \ngenerated matching automaton. The .rst exhaustiveness optimization simpli.es con.gurations all of whose \nrows contain the same index into zero-column con.gurations. Consider, for instance, C3 and C4 of Example \n7. For any satisfying environments, these con\u00ad.gurations are equivalent to the zero-column con.gurations \n(<>,{1})and (<>,{2}) respectively. Using this optimization, we can .nalize the matching automaton generated \nin Exam\u00adple 7 by making the transitions originating in C2 .nal (c.f. the matching automaton of Figure \n4(c)). Another exhaustive mode optimization involves eliminat\u00ading columns containing the same state. \nAgain we refer to con.gurations C3 and C4 of Example 7. These con.gura\u00adtions are subject to the described \noptimization since they have only one row. Removing the columns will result in the same zero-column con.gurations \nthat we obtained by apply\u00ading the optimization described in the previous paragraph. 4.5 Properties We \nnow describe the important properties of the SBF al\u00adgorithm. First, we summarize the type of sbf. In \naddi\u00adtion to the current con.guration, its parameters are: the underlying tree automaton, a set of loop \nbreakers, and a boolean indicator of whether we are in the exhaustive or non-exhaustive mode. The result \nof sbf is the generated matching automaton and a variable indicating which row of the input con.guration \nwas selected for expansion. The .rst lemma states that the algorithm terminates. Lemma 1. Let A = (S,T) \nbe a tree automaton and let Rec . S be a loop breaker set. Then sbf(C, A, Rec, exh) terminates for any \ncon.guration C over S and any boolean value exh. Proof sketch. The way in which we characterize sets \nof loop breakers in De.nition 6 allows us to de.ne a measure that can be shown to decrease as sbf invokes \nitself recur\u00adsively. The next lemma establishes that the matching automaton generated by sbf is indeed \nin simple backtracking form. Lemma 2. Let A = (S,T) be a tree automaton; let Rec . S be a loop breaker \nset, and let (M,y) = sbf(C, A, Rec, exh) where C is a con.guration over S and exh is a boolean value. \nThen M is in SBF. Proof sketch. The way in which we de.ned the two ex\u00adpansion techniques ensures that \nthe matching automaton is sequential and disjoint. Subroutine automata are boolean since they are generated \nfrom single-state con.gurations. The generated automaton has separated transitions since expansion by \nlabel creates only simple transitions, and ex\u00adpansion by state creates only subroutine transitions. The \n.nal lemma shows that sbf is sound. Observe that the statement is stronger for non-exhaustive mode since \nthen the equivalence holds for both matched and unmatched inputs; whereas, in the exhaustive mode, because \nof the two optimizations described in the previous subsection, the equivalence only holds for matched \ninputs. Lemma 3. Let A = (S,T) be a tree automaton; let Rec . S be a loop breaker set; let E be an environment, \nand let (M,y) = sbf(C,A,Rec,exh) where C is a con.guration over S and exh is a boolean value. Then, if \nexh = true and C is satis.able by E or if exh = false,we have E |= C . k i. E\\y f E(y) . M . k. Proof \nsketch. The proof of the if statement proceeds by induction on the size of values in E; the proof of \nthe only if direction proceeds by induction on the size of the right hand side derivation. 4.6 The SNBF \nAlgorithm There are two di.erences between the SNBF and SBF al\u00adgorithms. The .rst concerns handling con.gurations \nthat cannot be expanded by label. Instead of doing expansion by state as it was described for sbf, the \nSNBF algorithm gen\u00aderates a fresh index converter that makes a subroutine tran\u00adsition for every column \nof the current con.guration. Sub\u00adroutine automata used in these transitions are based on the states appearing \nin the corresponding column. Since the SNBF algorithm employs subroutine automata that are based on collections \nof tree automaton states, rather than just one state, at the top level, the algorithm gener\u00adates a subroutine \nfor every subset of S. (Our implementation does not generate a subroutine automaton unless it encoun\u00adters \na call to it while constructing another automaton.) 4.7 Summary of Complexity Results Let us consider \nthe running time and the size of the gen\u00aderated matching automata for the compilation algorithms described \nin this section. Example 5 shows that backtrack\u00ading matching automata generated by SBF can exhibit ex\u00adponential \nrunning time in the size of the input value. Non\u00adbacktracking matching automata generated by SNBF are \nat worst linear. We have not studied running time complex\u00adity of matching automata in relation to the \nsize of the tree automaton given as input to SBF and SNBF. In our appli\u00adcation domain of large XML documents \nand relatively small patterns, this question is less important than complexity in the size of the value. \nSpace complexity involves two components: the number of generated subroutine matching automata and the \nsize of an individual matching automaton. The SBF algorithm gener\u00adates at most a linear number of subroutine \nautomata in the size of the input tree automaton. The SNBF algorithm can result in exponentially many \nsubroutine automata. The size of an individual matching automaton depends on the strategy for selecting \nloop breakers. The maximal set of loop breakers results in a matching automaton whose size is at worst \nlinear in the size of the input tree automaton. As Example 6 shows, a minimal set of loop breakers can \nre\u00adsult in a double exponential matching automata. Finally, we can show that using multiple predecessor \nor multiple succes\u00adsor loop breaker sets ensures that the size of the generated automaton is no worse \nthan polynomial and exponential re\u00adspectively.  5. EXPERIMENTS This section describes our preliminary \nexperiments. We did not put a great deal of e.ort into optimizing the data structures and low level algorithmic \ndetails of our proto\u00adtype implementation. The generated target code is evalu\u00adated by a straight forward \nunoptimized interpreter. Both the compiler and the target language interpreter are run in an interpreted \nversion of Scheme. Under these circum\u00adstances, it is meaningless to time our experiments directly since \nthe eventual implementation is likely to behave com\u00adpletely di.erently. Instead, to have some indication \nof the relative speed of the output code, we count the number of evaluation steps function calls, variable \nlookups, primitive applications, etc. performed by the target language inter\u00adpreter. In addition to the \ntime related measurements, we estimate the size of the generated code by the number of nodes in the AST \nof the output program. We analyze three test programs. The .rst one, rng2xt, is a 500 line program (2,200 \nAST nodes, 63% of which are in patterns) that converts a Relax NG schema into a collection of XDuce regular \ntypes. It is run on a 900 line XML doc\u00adument. The second, format html, is a 3,000 line program (7,400 \nAST nodes, 92% of which are in patterns) that tra\u00adverses an html page, .nds all of its headings and makes \na table of contents with references to them. The third, for\u00admat bibtex, is a 1,200 line program (4,400 \nAST nodes, 55% of which are in patterns) that reads a bibtex .le and converts it into an html page displaying \nthe .le s contents. The .rst experiment compares di.erent methods of select\u00ading sets of loop breakers. \nIn the table shown in Figure 10, max Rec,succ Rec, and pred Rec denote the maximal, multi\u00adple successor, \nand multiple predecessor sets of loop breakers respectively. For the .rst two programs, all loop breaker \nselection strate\u00adgies result in programs of roughly the same size. For for\u00admat bibtex, using the maximal \nset of loop breakers produces a substantially smaller program as discussed in Section 4.3. Maximal sets \nof loop breakers generally lead to slower pro\u00adgrams. The fastest programs, for our tests, were generated \nusing multiple predecessor loop breaker sets. Apparently, this strategy introduces the least number of \nsubroutine func\u00adtions, and, hence, incurs the least amount of penalty arising from function calls. Selecting \nminimal sets of loop breakers failed becauwe of a dramatic code size explosion.  max succ pred rng2xt \nRec Rec Rec size of gen d code 8,788 7,758 9,169 #of eval steps 955,714 580,813 455,270 format html size \nof gen d code 18,127 15,577 18,357 #of eval steps 8,484 9,269 7,384 format bibtex size of gen d code \n24,729 41,518 52,856 #of eval steps 131,642 45,104 22,892 rng2xt backtracking non-backtracking size \nof gen d code 9,169 17,096 #of eval steps 455,270 542,753 format html size of gen d code 18,357 23,138 \n#of eval steps 7,384 10,234 format bibtex size of gen d code 52,856 34,207 #of eval steps 22,892 34,722 \nFigure 10: Comparison of SBF vs. SNBF  Figure 9: Comparison of various loop breaker sets The table shown \nin Figure 10 compares the backtrack\u00ading and non-backtracking compilation algorithms. In both cases, we \nuse multiple predecessor loop breaker sets. The backtracking approach results in faster programs for \nall the test cases. It seems that the cost of backtracking occurring in the programs generated by this \nmethod is far outweighed by the cost of operations on tuples of boolean values employed in the programs \ngenerated by the non\u00adbacktracking approach. Finally, we would like to note that despite our prototype \ns obvious ine.ciencies, its performance is somewhat reason\u00adable: the slowest of the examples takes 30 \nseconds to compile and 10 seconds to run on a Pentium III laptop. In a pro\u00adduction version of the compiler, \nwe expect these numbers to improve by at least two orders of magnitude. 6. RELATED WORK The XDuce programming \nlanguage [13, 12], provided the starting point for our project and is the source language of our compilers. \nAs mentioned in Section 3.1, our compiler uses XDuce s algorithm for converting source patterns into \nstates of a binary top-down tree automaton. Hosoya and Pierce [12] provide a detailed account of this \nalgorithm. Compilation of datatype-based pattern matching has been researched extensively in the past. \nPapers in this .eld usu\u00adally distinguish two general approaches. Baudinet and Mac-Queen [3] describe \na method based on decision trees (used in the SML/NJ compiler) that is geared toward producing e.\u00adcient \nnon-backtracking code but might su.er from an occa\u00adsional blowup of the output code size. Conversely, \nAugusts\u00adson [2] introduces a backtracking approach that restrains the size of the output code at the \nexpense of its e.ciency. Le Fessant and Maranget [10] combine the advantages of both approaches by introducing \nseveral optimizations (used in the OCaml compiler) to the backtracking technique that result in more \ne.cient but still compact output code. (On a slightly di.erent note, Sestoft [18] shows how instrumenta\u00adtion \nand partial evaluation can help derive a reasonably e.\u00adcient pattern match compiler from a simple pattern \nmatch\u00ading algorithm. The resulting compiler, however, is less ad\u00advanced than the compilers mentioned \nabove.) A similar trade-o. between performance and space con\u00adsiderations is present in the compilation \nalgorithms of this paper. By varying loop breaker sets used in the algorithm, we can either minimize \nthe code size but introduce more subroutine function calls and hence more backtracking, or, conversely, \nminimize the number of subroutine calls at the risk of generating very large code. Since our work focuses \non recursive patterns and the many issues that arise as a result of dealing with recursive patterns, \nour algorithms can be viewedas anextension of datatype basedpattern com\u00adpilation. Compilers for logic \nprogramming languages employ op\u00adtimizations focused on avoiding backtracking and sharing common tests \nperformed by di.erent branches of conditional expressions. Like datatype pattern matching optimizations, \nthese techniques ([15] and [9]) do not handle recursive pat\u00adterns and, thus, cannot be used directly \nin our compiler. The topic of procedure inlining optimization ([14], [1], [19]) is relevant to our work. \nOur compilation algorithm is similar to procedure inliners in that it examines a poten\u00adtially cyclic \nstructure a tree automaton and determines which nodes of that structure can be implemented inline and \nwhich nodes must correspond to procedures. Peyton Jones and Marlow [14] introduce the notion of loop \nbreak\u00aders, show how selecting di.erent loop breakers can have a signi.cant e.ect on the quality of the \ngenerated code, and describe a heuristic for locating loop breakers. A great deal of research has been \nconducted in the area of regular tree languages. This .eld studies properties of regular tree and forest \nlanguages. Two kinds of trees are considered: ranked in which any label has an arity, and the number \nof child subtrees in a node is determined by the arity of the node s label ([8]); and unranked in which \nany node can have arbitrary number of children regardless of the label ([5], [16].) One of the problems \ninvestigated in this area, the mem\u00adbership problem, is relevant to our research. Its goal is to check \nwhether a tree belongs to a particular regular tree language speci.ed by a regular tree grammar. A standard \nsolution described in the literature involves converting the grammar into a non-deterministic bottom-up \ntree automa\u00adton (NTA), building an equivalent bottom-up determinis\u00adtic tree automaton (DTA), and matching \nthe input value against the obtained DTA. The .rst and third components of the above process can be accomplished \nin linear time in the size of the input. The process of determinization, however, can result in a DTA \nwhose size is exponential in the size of the original NTA. Seidl and Neumann [17] in\u00adtroduce pushdown \nforest automata bottom-up automata with a top-down twist that exhibit better determinization characteristics. \nBottom-up automata do not give us a natural framework for modeling target language code. It is unclear \nhow to read-o. a target language program from a bottom-up au\u00adtomaton in such a way that this program \ncan be further optimized by inlining and other low-level transformations. For this reason, we base our \ncompilation algorithms on the top-down approach, and, hence, bottom-up techniques can\u00adnot be applied \nfor our purposes directly. Furthermore, our matching automata are not meant at least at this time to \ncompete with the automata theoretic approaches on the terms that are of interest to that com\u00admunity. \nIn our framework, we are interested in and can express transformations that may give us constant factor \nimprovements in the performance of the generated program for common source programs rather than asymptotic \ncom\u00adplexity improvements for certain rare cases. This interest is re.ected in the design of matching \nautomata that features subroutine transitions to help us examine code inlining ap\u00adproaches; indices to \nhelp us implement pattern matching with multiple patterns and experiment with exhaustiveness optimization; \nand variables in transitions for more .exibility in scheduling evaluation of subtrees of the input tree. \nIt can be shown [8] that word automata minimization al\u00adgorithms can be generalized to tree automata. \nMinimization of tree automata can be employed in conjunction with the compilation algorithms discussed \nin this paper. Recall that the .rst stage of our compiler converts source patterns into states of a tree \nautomaton. Currently, the algorithm used in this stage [12] does not produce a minimal tree automaton. \nIn the future, we would like to experiment with employing existing minimization algorithms at this stage \nto give us a better starting point for SBF and SNBF. Another question concerns minimizing matching automata \nproduced by SBF and SNBF. This problem is hard. Because of subroutine transitions, minimizing matching \nautomata is not unlike trying to .nd an optimal solution to function inlining, and we are not aware of \na function inliner that boasts optimality; they all are based on heuristics. Sim\u00adilarly, our compilation \nalgorithms do not claim to produce minimal matching automata, but rather matching automata that are good \nenough in practice. Acknowledgments I am grateful to Benjamin C. Pierce, Alan Schmitt, Vladimir Gapeyev, \nGeo.rey A. Washburn, Stephen Tse, Haruo Hosoya, Ralf Hinze, and the ICFP reviewers for reading this paper \nand suggesting numerous improvements. 7. REFERENCES [1] A. W. Appel and T. Jim. Shrinking lambda Expressions \nin Linear Time. Journal of Functional Programming, 7(5):515 540, 1997. [2] L. Augustsson. Compiling \npattern matching. In J.-P. Jouannaud, editor, Functional Programming Languages and Computer Architecture, \npages 368 381. Springer-Verlag, Berlin, DE, 1985. [3] M. Baudinet and D. MacQueen. Tree pattern matching \nfor ML. unpublished paper, 1985. [4] V. Benzaken, G. Castagna, and A. Frisch. CDuce: a white paper. \nIn Informal Proceedings of the Workshop on Programming Language Technologies for XML (PLAN-X), 2002. \n[5] A. Bruggemann-Klein, M. Murata, and D. Wood. Regular tree languages over non-ranked alphabets, 1998. \n [6] A. S. Christensen, A. Moller, and M. I. Schwartzbach. Static analysis for dynamic XML. http://www.brics.dk/ \nmis/planx.ps, 2002. Workshop on Programming Language Technologies for XML (PLAN-X). [7] J. Clark and \nS. DeRose. XML path language (XPath). http://www.w3.org/TR/xpath. [8] H. Common, M. Dauchet, R. Gilleron, \nF. Jacquemard, D. Lugiez, S. Tison, and M. Tommasi. Tree automata techniques and applications. Draft \nbook; http://www.grappa.univ-lille3.fr/tata. [9] S. Dawson, C. R. Ramakrishnan, S. Skiena, and T. Swift. \nPrinciples and practice of uni.cation factoring. ACM Transactions on Programming Languages and Systems, \n18(5):528 563, 1996. [10] F. L. Fessant and L. Maranget. Optimizing pattern-matching. In Proceedings \nof the 2001 International Conference on Functional Programming. ACM Press, 2001. [11] V. Gapeyevand B. \nC. Pierce. Regular object types. In European Conference on Object-Oriented Programming (ECOOP), 2003. \nA preliminary version was presented at FOOL 03. [12] H. Hosoya and B. Pierce. Regular expression pattern \nmatching. In ACM Symposium on Principles of Programming Languages (POPL), London, England, 2001. Full \nversion to appear in Journal of Functional Programming. [13] H. Hosoya and B. C. Pierce. XDuce: A statically \ntyped XML processing language. ACM Transactions on Internet Technology, 2003. To appear. [14] S. P. Jones \nand S. Marlow. Secrets of the Glasgow Haskell Compiler Inliner. In IDL, 1999. revised version to appear \nin Journal of Functional Programming. [15] A. Krall. Implementation techniques for Prolog. In N. E. Fuchs, \neditor, 10. Workshop Logische Programmierung, Bericht, pages 1 15, Z\u00a8urich, 1994. Universit\u00a8at Z\u00a8urich. \n[16] M. Murata. Hedge Automata: a Formal Model for XML Schemata. Web page, 2000. [17] A. Neumann and \nH. Seidl. Locating matches of tree patterns in forests. In Foundations of Software Technology and Theoretical \nComputer Science, pages 134 145, 1998. [18] P. Sestoft. ML pattern match compilation and partial evaluation. \nLecture Notes in Computer Science, 1110:446 ??, 1996. [19] O. Waddell and R. K. Dybig. Fast and E.ective \nProcedure Inlining. In Static Analysis Symposium, pages 35 52, 1997.  \n\t\t\t", "proc_id": "944705", "abstract": "Pattern matching mechanisms based on regular expressions feature in a number of recent languages for processing tree-structured data such as XML. A compiler for such a language must address not only the familiar problems of pattern optimization for ML-style algebraic datatypes and pattern matching, but also some new ones, arising principally from the use of recursion in patterns. We identify several factors playing a significant role in the quality of the generated code, propose two pattern compilers---one generating backtracking target programs, the other non-backtracking---and sketch proofs of their correctness.", "authors": [{"name": "Michael Y. Levin", "author_profile_id": "81100404720", "affiliation": "University of Pennsylvania", "person_id": "PP31040811", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/944705.944712", "year": "2003", "article_id": "944712", "conference": "ICFP", "title": "Compiling regular patterns", "url": "http://dl.acm.org/citation.cfm?id=944712"}