{"article_publication_date": "08-25-2003", "fulltext": "\n Disjunctive Normal Forms and Local Exceptions Emmanuel Beffara Universit\u00b4Equipe PPS \u00b42 place Jussieu \n75251 Paris Cedex 05, France beffara@pps.jussieu.fr e Paris 7, Abstract All classical .-terms typable \nwith disjunctive normal forms are shown to share a common computational behavior: they implement a local \nexception handling mechanism whose exact workings de\u00adpend on the tautology. Equivalent and more ef.cient \ncontrol com\u00adbinators are described through a specialized sequent calculus and shown to be correct. Categories \nand Subject Descriptors D.1.1 [Programming Techniques]: Functional Programming; D.3.3 [Programming Languages]: \nLanguage Constructs and Fea\u00adtures control structures; F.3.3 [Logics and Meanings of Pro\u00adgrams]: Studies \nof Program Constructs control primitives; F.4.1 [Mathematical Logic and Formal Languages]: Mathematical \nLogic lambda calculus and related systems  General Terms Theory, Languages Keywords Control structures, \ndisjunctive normal forms, classical realizability 1 Introduction We show in this paper that to each disjunctive \nnormal form G corre\u00adsponds a computational behavior shared by all terms of type G.An indication that \ndisjunctive normal forms (DNFs) might be inhab\u00adited by interesting computational behaviors comes from \nprevious work by the second author and Krivine [4] where some disjunctive tautologies are set in correspondence \nwith a family of synchroniza\u00adtion schemes. As a consequence of the intuitionistic disjunction property, \nnone of the non trivial formulas of that class is intuition\u00adistically valid. DNFs are actually the next \nsimplest class with this Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. ICFP 03, August 25 29, 2003, Uppsala, Sweden. Copyright 2003 ACM 1-58113-756-7/03/0008 \n...$5.00 Vincent Danos Universit\u00b4Equipe PPS \u00b42 place Jussieu 75251 Paris Cedex 05, France danos@pps.jussieu.fr \ne Paris 7, anti-intuitionistic feature, so it seems natural to investigate these as well. Nevertheless, \none could fear that such a vast set of types as DNFs would be too loose to actually provide an interesting \nspeci.cation problem. But it is not so. They all do specify a quite distinc\u00adtive control mechanism, and \nmore sophisticated than with disjunc\u00adtive tautologies. Proving this is the .rst contribution of the pa\u00adper. \nWe go beyond giving mere descriptions of those behaviors, by actually extending our basic language, a \ncall-by-name typed .-calculus with control, with primitive instructions directly imple\u00admenting these. \nProving these are correct is the second contribution. We use Krivine s classical realizability (as documented \nin his recent papers [9, 10]) to do this. The new computation rules are, we believe, simple enough to \nun\u00adderstand, even though they use a tamed form of dynamic binding, and can be rendered in graphic notation \nas control charts. Notation is an issue here, in that to have the new instructions actually used by a \nprogrammer, one has to accompany the formal operational seman\u00adtics with an intuitive notation that will \nhelp in building a working representation of whatever control scheme is described. We also develop a \nproof-theoretic description via a specialized sequent cal\u00adculus, as well as an ordinary programming-language-like \nsyntax to suggest how our new control combinators would .t in the real pic\u00adture. This last piece of syntax \nis smoothly extending the CAML [2] notation for exception handling. It must be made clear that the control \nstructures we consider here are not exceptions in the ML sense of the word. For one thing, all the structures \nwe manipulate are local, in the sense that they are de.ned at the point where they are used and that \na given exception is always associated to one speci.c handler. Besides, the ambient language being call-by-name, \ncontrol is of quite a different and sim\u00adpler nature than it is in ordinary call-by-value programming. \nYet it should be possible to rerun our methods in the call-by-value world. This is an important question \nwhich we leave for future exploration. Another point which might need some clari.cation is the follow\u00ading: \nof course any propositional formula is classically equivalent to a DNF, but this does not mean we are \ngiving all formulas a computa\u00adtional interpretation! What this says is that the equivalence between generic \npropositional formulas and DNFs has to be a compilation into our multi-exception handling combinators. \nWe intend to ex\u00adplore the matter further. 2 Preliminaries We .rst state the de.nitions of our calculus \nand the logic that types it. We also state the notion of realizability that we use later on. 2.1 The \nCalculus Our computational framework is ..-calculus, a variant on Fel\u00adleisen s .C-calculus, with a deterministic \ncall-by-name evaluation strategy. We de.ne ., the set of terms (or ..-terms), and ., the set of stacks, \nas well as the set . \u00d7. of executables, by terms t,u ::=x |tu |.x.t |.x.t |kp stacks p ::=e |t \u00b7p executables \ne ::=t *p Stacks should be understood as evaluation contexts in call-by-name strategy: an executable \nt *t1 \u00b7\u00b7\u00b7tn is seen as the term t applied to arguments t1 \u00b7\u00b7\u00b7tn, with the next reduction step occurring \nat the po\u00adsition of t. The evaluation relation . over executables is de.ned as the re.exive transitive \nclosure of the following set of rules: push tu *p .t *u \u00b7p pop .x.t *u \u00b7p .t[u/x]*p save .x.t *p .t[kp/x]*p \nrestore kp *t \u00b7p'.t *p The push and pop rules implement usual \u00df-reduction while save and restore provide \nthe control mechanism. The term .x.t captures the current evaluation context p as a term kp, and applying \nthe latter restores this context. Binders .x and .x are thus dual in the sense that one substitutes terms \nwhile the other substitutes stacks, in the form of the terms kp which can also be called continuations. \n 2.2 The Typing System Types are second order propositional formulas. Given a set Var of propositional \nvariables, the typing rules are the following: [axiom] G,x : A fx : A G,x : A ft : B G ft : A .B G fu \n: A [.intro][.elim] G f.x.t : A .B G ftu : B G ft : AX .FV (G) G ft : .XA [.intro][.elim] G ft : .XA \nG ft : A[B/X] These .rst .ve rules give a standard presentation, known as natural deduction, for second \norder propositional intuitionistic logic. The typing rule for the . binder is G,x : A .B ft : A [Peirce] \nG f.x.t : A With this sixth rule, known as Peirce s law, we get one possible nat\u00adural deduction presentation \nof second order propositional classical logic, as it was .rst noted by Grif.n [6] in the context of Scheme \nwith a call/cc operator. Note that we do not provide any typing rule for continuations kp.A reason for \nthis is that it would require giving types to stacks. This would be possible but it would lead to unnecessary \ncomplication here. Another deeper reason is that typable terms are seen as proofs of their type, and \nthis interpretation would not make sense anymore when writing continuations explicitly. From a computational \npoint of view, typable terms are programs that should be able to be exe\u00adcuted in any context, and this \nforbids them to contain explicit ones. As usual we de.ne the absurdity as . :=.XX and the negation as \n\u00acA :=A ... All types considered later on live actually in a fragment of this system that corresponds \nto simple types augmented with the . constant, so types are implicitly considered universally quanti.ed \nin every variable. 2.3 Realizability Realizability builds polarized models of the propositional logic \nabove by associating truth values to each formula. Positive truth values are sets of terms while negative \nones are sets of stacks. These models are parametrized by a given observation: let ... . \u00d7. be a set \nof executables closed by anti-reduction (i.e. if e . e' and e' ... then e ...). The executables in this \nset are called the ob\u00adservables and they are said to realize the absurd. In the de.nitions below, we \nassume such a . has been chosen. From . we deduce a notion of orthogonality between terms and stacks, \nand subsequently between positive and negative truth val\u00adues: a set X of terms is said to be orthogonal \nto a set Z of stacks (which we can write X . Z) if for any term t . X and any stack p .Z we have t *p \n.... The orthogonal of a given set of terms or stacks is then the largest set orthogonal to it, i.e. \nX. ={p |X . p} and Z. ={t |t . Z} (note that we use the same notation in both cases even if it applies \nto objects of different kinds). We associate truth values to the types of our system. For a given type \nA, the positive value |A|will be, intuitively, the set of terms that behave well as members of this type. \nThe corresponding negative value [A], orthogonal to |A|, will contain the contexts, i.e. the stacks, \nwhere terms of type A behave well. Negative truth values are de.ned inductively given a valuation of \npropositional variables. Let e be a function from propositional vari\u00adables into the powerset P (.). The \nnegative truth value [A]e associ\u00adated to a given type A in the valuation e is de.ned as [X]e :=e(X) [A \n.B]e :=[A]. \u00b7[B]e e [ [.XA]e :=[A]e[Z/X] Z.. where e[Z/X] is the environment e where value Z has been \nas\u00adsigned to X. These de.nitions correspond to the semantic meaning of the types. For closed formulas \nwe can write [A]instead of [A]e since the value is environment-independent (more generally, [A]e depends \nonly on the values e takes on variables free in A). We then call interpretation of A the positive truth \nvalue |A|e =[A]. , i.e. the e set of terms orthogonal to [A]e, and we say that a term t realizes a type \nA (in environment e), which we write t FeA,if t is in the interpretation of A. We write this t F A when \nA has no free variable. The value of some particular types is worth explaining. For the absurd .=.XX, \nthe negative value is the whole set of stacks, i.e. [.]=.; the associated positive value |.| is thus \nthe set of terms t such that for any stack p the executable t *p is in . , that is terms who reduce into \n. whatever the context. It is also interesting to describe the values of negated types: the negative \nvalue [\u00acX]= [X ..]turns out to be the positive value |X|concatenated with .. Subsequently, if p is a \nstack in [X]and t is a term in |X|, then t *p is in . by de.nition, and anti-reduction implies that kp \n*t \u00b7p' is also in . for any p', which means that p .[X]implies kp F \u00acX. In turn, a consequence is that \nkpt realizes ., and actually it is more or less an executable on its own. This F is a semantic relation \nbetween terms and types, while the typing provided a more syntactic relation. For any choice of . , the \ntyping relation is actually a subset of the realization relation: THEOREM 1(ADEQUACY). Let x1: A1,...,xn \n: An ft : Bbe a derivable typing judgement and let . be a set of observables. For any valuation e : Var \n.P (.), any family t1,...,tn of terms such that ti .|Ai|e for each i and any stack p .[B]e, the executable \nt[t1/x1,...,tn/xn]*p is in . . This theorem is standard, but we include its proof anyway, for co\u00adherence \nand as an illustration of the kind of reasoning we will use later on. PROOF. We proceed by induction \non the typing derivation. Call G the typing environment x1: A1,...,xn : An and consider a given valuation \ne, a given family ti .|Ai|e and a given stack p .[B]e.For any term t, call t\u00afthe substituted term t[t1/x1,...,tn/xn]. \naxiom: We have the judgement x : A fx : A so if t .|A|e we have t *p ...by de.nition. application: Let \nt and u be terms such that G ft : A .B and G f u : A are derivable. By induction we have t\u00af.|A .B|e and \nu\u00af.|A|e,so \u00afu \u00b7p is in |A|e \u00b7[B]e =[A .B]e, therefore t\u00af*u\u00af\u00b7p is in . , and so is tu *p since . is closed \nby anti-reduction. abstraction: Assume G,x : A ft : B is derivable. By induction, for any term u .|A|e, \nwe know that t\u00af[u/x]*p is in . , and so is .x.t *u \u00b7p by anti-reduction, so we can actually deduce that \n.x.t is orthogonal to |A|e \u00b7[B]e =[A .B]e. continuation: If G,x : A .B ft : A is derivable, then, by \ninduc\u00adtion, for any term u .|A .B|e and any stack p .[A]e we have t\u00af[u/s]*p .... Besides, for any stack \nv \u00b7p'.[A .B]e the term v is in |A|e,so v *p is in . , and so is kp *v \u00b7p' by anti-reduction, which proves \nthat kp is in |A .B|e, so the exe\u00adcutable t\u00af[kp/x]*p is in . , as well as .x.t *p by anti-reduction. \nquanti.cation: Write B =.XA and suppose that G ft : A is deriv\u00adable. From the de.nition of [.XA]e we \ndeduce the existence of a stack set Z for which [A]e[Z/X] contains p. Since the vari\u00ad able X does not \nappear in any of the Ai, the value of each |Ai|e does not depend on e(X), so for each i we have ti .|Ai| \ne[Z/X], and therefore t\u00af*p is in . by induction hypothesis. un-quanti.cation : Assume G ft : .XA, this \nmeans that for every stack set Z and every stack in [A]e[Z/X] we have t *p .... This is true in particular \nfor Z =[B]e for a given type B.If we prove that the valuation of A[B/X]in e is equal to the valuation \nof A in e[[B]e/X], then we get the expected result, and this substitution lemma is proved easily by induction \non A, from the de.nition of substitution. Adequacy thus holds for all types. In the case of closed formulas, \nit reduces into the following ade\u00adquacy lemma: PROPOSITION 2. For any term t and any type A, if ft : \nA is deriv\u00adable then t F A holds for any . . The idea of realizability is derived from Girard s concept \nof re\u00adducibility candidate, and it can actually be used to prove normaliza\u00adtion results. Once the adequacy \nlemma is established, these proofs derive easily, even in our context where the calculus is enriched \nwith global control. Adequacy is also the key argument in our speci.cation proofs. The general idea is \nto de.ne . as the closure by anti-reduction of a given executable e. That a set of terms X and a set \nof stacks Z are orthogonal values in the resulting model then means that applying a term in X to a stack \nin Z gives an executable that reduces into e. As an example, let us consider the following speci.cation \nfor Church booleans. For each b .{0,1}, let us de.ne the type for boolean b as Bb =.X0.X1(X0 .X1 .Xb) \nThen we have the following speci.cation: PROPOSITION 3. Let t be a term typable as Bb for some b . {0,1}. \nFor any couple of terms (u0,u1) and any stack p, the ex\u00adecutable t *u0 \u00b7u1 \u00b7p reduces into ub *p. PROOF. \nLet u0 and u1 be two terms and p be a stack. De.ne . as the closure of {ub *p}by anti-reduction. Since \nft : Bb is derivable by hypothesis, adequacy proves that t F Bb. As a consequence, for any environment \ne : Var .P (.), i.e. for any value of e(X0) and e(X1), we have t FeX0 .X1 .Xb. De.ne e(Xb)as {p}and e(X1-b) \nas the empty set. Then ub F Xb by de.nition of . and trivially u1-b F X1-b, so the stack u0 \u00b7u1 \u00b7p is \nin [X0 .X1 .Xb]e, and subsequently t *u0 \u00b7u1 \u00b7p is in . , which means that it reduces into ub *p. Of \ncourse, this result could be obtained by a direct proof, using subject reduction arguments. However, \nthe proof above, though voluntarily detailed, is very simple. The pattern is always the same: carefully \nchoose the observation . and the value of type variables (thanks to universal quanti.cation), and let \nthe adequacy do the work. We will use the same statement and nearly the same proof in proposition 12 \nas a consistency argument for the calculus enriched with control combinators.  3 Speci.cation Theorems \nThe class of formulas we are considering here is the so-called dis\u00adjunctive normal forms (or DNFs), that \nis disjunctions of conjunc\u00adtions of literals. Given a set Var of propositional variables, we write Lit \nfor the set of literals over Var, i.e. Lit =Var .\u00acVar. Two literals X and \u00acX are said to be opposite.A \nclause is a conjunction of liter\u00adals, or equivalently a .nite subset of Lit, and a DNF is a disjunction \nof clauses, or a .nite subset of the set P.n(Lit)of .nite parts of Lit. We could use multisets instead \nof sets, however this would lead to undue notational complication, so we don t. We will write clauses \nand DNFs indifferently as formulas with .and .or as sets of sets of literals, whichever is more suited \nto the context. We will study the speci.cation problem for this class of formulas. However, simple types \nare built with .as the only connector, so we must de.ne how DNFs are converted into types. For any type \nt and any ordered clause {L1,...,Lk}we de.ne: {L1,...,Lk}.t := L1 .\u00b7\u00b7\u00b7.Lk .t The ordering we use will \nbe either clear from the context or indiffer\u00adent. Likewise, given a formula G ={c1,...,cn}and a fresh \nvariable Z, we interpret G as a type by de.ning G := .Z (c1 .Z).\u00b7\u00b7\u00b7.(cn .Z).Z again using an appropriate \nordering. Any term of type G will thus take n functions as arguments. Take note that G seen as a type \nis intuitionistically isomorphic to G as a DNF, so conversion is com\u00adputationally neutral. 3.1 A First \nSpeci.cation Our purpose now is to identify a computational behavior common to all ..-terms that are \ntypable using a given DNF. For this purpose, we characterize tautologies among DNFs using a notion of \nsection: De.nition 1. Let G =c1 .\u00b7\u00b7\u00b7.cn be a disjunctive normal form. A section of G is an element of \nthe product c1 \u00d7\u00b7\u00b7\u00b7\u00d7cn. One can rephrase this by saying that a section is a choice of one literal in \neach clause. We then write s(c) for the literal chosen in clause c and L .s if literal L is chosen in \nsome clause. A section can be interpreted as a potential counter-example, and therefore it comes as no \nsurprise that the formula is true exactly when there is no such counter-example: PROPOSITION 4. A disjunctive \nnormal form G is a tautology if and only if every section of G contains two opposite literals. PROOF. \nSuppose that G is a tautology and there is a section s that does not contain opposite literals. We can \nthen de.ne a boolean val\u00aduation v of the propositional variables such that v(X)=Tif \u00acX .s and v(X)=.if \nX .s. This valuation thus makes each clause of G false, which is contradictory. Suppose now that G is \nnot a tautology, so there is a valuation v such that v(G) is false. For each clause c, since vc =., there \nis a literal s(c) such that v(s(c))=., and this de.nes a section in which all literals are false in v, \ntherefore s cannot contain opposite literals. Sections are clearly connected to provability by the proposition \nabove, but they also happen to play a r ole in the speci.cation. Be\u00adfore embarking on the precise statement, \nlet us explain intuitively what is happening. Let G be a DNF tautology, let t be a term of type G and \nf be a sequence of arguments, with c .G. What t does is to pass each fc a freshly created set of exceptions, \nindexed by the literals in c.If all of the fc raise one of the exceptions they were given, then t selects \ntwo of these with opposite types X and \u00acX and runs their arguments one against the other. That there \nalways must be two opposite exceptions is precisely what the proposition above says, and therefore the \narguments have compatible types. Note, in the statement of the theorem below, that there is no mention \nof exceptions. What we assume is that the terms fc apply one of their arguments, whatever it is. The \nintuition about exceptions is only a way of saying, and it will be formalized only in the next section. \nIn the theorem, we write f to represent a family of terms indexed over G and fc for the element of index \nc in this family, similarly we write u to represent any family of terms indexed over the literals in \na given clause of G and uL for members of such a family. THEOREM 5. Let G be a tautology in disjunctive \nnormal form and lett bea ..-term of type G. Let p be a stack and f a family of terms indexed by the clauses \nof G. Suppose there exists a section s of G and a family of terms vc and stacks pc such that for each \nclause c, s(c).\u00acVar ..u .p' fc *u \u00b7p us(c)*vc \u00b7p' s(c).Var ..u fc *u \u00b7p us(c)*pc where u is any family \nof terms indexed on the literals of c. Then '' there exists a pair of clauses (c,c )such that s(c)=\u00acs(c \n)and t *f \u00b7p vc *pc ' . PROOF. First we split G into G+={c |s(c).Var}and G- ={c |s(c).\u00acVar}. De.ne . \nas the closure by anti-reduction of '' {vc *pc '|c .G- ,c .G+ ,s(c)=\u00acs(c )} and instantiate the propositional \nvariables by '' [Z]={p} and [X]={pc '|c .G+ ,s(c )=X} where Z is the fresh variable used to translate \nG into a type and X ranges over variables occurring in G. Let c be a clause in G- . For any family (uL) \nof terms such that uL F \u00acL for each literal L .c, by hypothesis, fc *u \u00b7p reduces into us(c)*vc \u00b7p' for \nsome p' . Here s(c) is a negative literal \u00acX, and the term vc realizes X since for each element pc ' \nin [X] we have ' s(c)=\u00acs(c )and so vc *pc ' ..., therefore us(c)*vc \u00b7p' is in . , so is fc *u \u00b7\u00b7p by \nanti-reduction, and so fc F c .Z. Similarly, let c ' be a clause in G+ . For any family u of terms such \n' that uL F L for each clause L .c , by hypothesis fc '*u \u00b7p reduces ' into as(c ')*pc ' where s(c )is \nsome variable X. This executable is '' in . since pc '.[s(c )], therefore fc ' F c .Z. By hypothesis, \nft : G is derivable, so the adequacy lemma proves that t realizes G. We have just shown that for each \nclause c, the term fc realizes the type c .Z, and by de.nition p is in [Z],so f \u00b7p is in [G],so t *f \n\u00b7p is in . , which means that it reduces into some ' vc *pc ' with s(c)=\u00acs(c ). 3.2 Sharper Speci.cations \nThe speci.cation extracted above is actually quite shallow. There is an important restriction, namely \nthat the values vc and the stacks pc that the processes pass when raising exceptions may not contain \nany occurrence of the functions arguments (the uL in the quanti.\u00adcations). We can do better by relaxing \nthe conditions on the argu\u00adments, i.e. by assuming that there exists a family of parametrized terms vc[]and \nparametrized stacks pc[]such that, for each clause c, s(c).\u00acVar ..u .p' fc *u \u00b7p us(c)*vc[u]\u00b7p' s(c).Var \n..u fc *u \u00b7p us(c)*pc[u] and proving that the reduction of t *f \u00b7p leads to vc[a]*pc '[b]for some families \nof terms a andb. Proving just this is mainly the same as proving theorem 5. However, proving something \nabout the terms in the families a andb is harder. As an illustration, let us consider he particular case \nof the excluded middle, i.e. G =\u00acX .X. Its translation as a type, as explained above, is (\u00acX .Y ).(X \n.Y ).Y . Here the following develop\u00adment result holds: PROPOSITION 6. Let t be a term of type \u00acX .X. \nLet p be a stack and let f and g be two terms. Suppose there is a stack pg and a family of contexts (vi[])0=i=n \nsuch that .u .p' f *u \u00b7p u *v0[u]\u00b7p' .u g *u \u00b7p u *pg .i < n .u .p' vi[u]*pg u *vi+1[u]\u00b7p' then there \nexists a term u such that t * f \u00b7g\u00b7p reduces into vn[u]*pg. PROOF. De.ne . as the closure by anti-reduction \nof {vn[u]*pg |u ..} and instantiate the propositional variables as [X]:={pg} and [Y ]:={p}. Then obviously \nfor any term u the we have vn[u]F X. Let i be an integer with 0 =i =n-1. Assume that for any term u in \n|\u00acX|the term vi+1[u]realizes X. Then for any such u and any stack p', the executable u*vi+1[u]\u00b7p' is \nin . , therefore vi[u]*pg is also in . , by anti-reduction using the third hypothesis. As a consequence, \nfor any u, u F \u00acX implies vi[u]F X. By recurrence, we thus have this property for the context v0[]. For \nany u F \u00acX and any stack p' , u *v0[u]\u00b7p' is in . as we just proved, so by anti-reduction f *u \u00b7p is \nalso in . , therefore f real\u00adizes \u00acX .Y . Besides, one trivially proves that g realizes X .Y , and t \nrealizes (\u00acX .Y ).(X .Y ).Y by the adequacy lemma, so t * f \u00b7g \u00b7p is in . , which means that it reduces \ninto vn[u]*pg for some term u. This allows values passed to exceptions to contain occurrences of exceptions \n(however without lifting the constraint on the stacks). The speci.cation means that the reduction of \nt * f \u00b7g \u00b7p leads to something equivalent to v0[kpg ], i.e. that the terms the combinator puts inside \nthe contexts vi[]behave like a continuation kpg . This can be understood as the fact that the exception \nX can be raised several times and that it will always be caught the same way by g. Lifting also the constraint \non stacks is harder, in particular it seems to impose some restrictions on the allowed observations, \nthough we did not get a proof of this. These results could probably be extended to the general case, \nbut the task seems notationally daunting.  4 Synthesizing Combinators Of course all DNF tautologies \nare provable in our system, and there\u00adfore one can .nd ..-terms that will have them as types. Compared \nto the speci.cation as described in the theorem above, they appear to implement it in a quite clumsy \nway. It is tempting to have new combinators in the calculus doing the same job, only better. In order \nto synthesize new control primitives, we now de.ne a logic with an associated sequent calculus to prove \ntautologies with, and deduce a computational structure from the proofs, by using a spe\u00adcialized Curry-Howard \ncorrespondence. 4.1 The Or-And Logic The logic L.. is de.ned as follows, given a set Var of propositional \nvariables: variables X .Var literals clauses formulas sequents L ::=X |\u00acX c ::=T|L .....L G ::=.|c,...,c \nI G Again, clauses are understood as .nite sets of literals and formulas are .nite sets of clauses, \nand T and . are their respective empty sets. In particular, ignoring order and duplication, we derive \nse\u00adquents using a single n-ary rule: I G1,. \u00b7\u00b7\u00b7 I Gn,. I (\u00acX1 .\u00b7\u00b7\u00b7.\u00acXn), (X1 .G1), ...,(Xn .Gn),. with \nn = 0, where the notation L .G represents the distribution of the literal L over the clauses in G, i.e. \nL .(c1,...,cn):=(L .c1),...,(L .cn) For n =0 our unique rule reduces to an axiom I T,., noticing that \nT is the conjunction of zero literals. First of all, we have to show that this system is complete, i.e. \nthat it actually proves our tautologies. For this we need the following lemma: LEMMA 7. Every tautology \nin disjunctive normal form has a clause with only negative literals. PROOF. Let G be a DNF. Suppose that \nG has no such clause, then each clause has at least one positive literal, which de.nes a section with \nonly positive literals. From proposition 4, we conclude that G is not a tautology. We also need the following \nnotion of quotient: De.nition 2. Let G be a disjunctive normal form and X be a vari\u00adable. The quotient \nof G by X is de.ned as G/X ={c \\{X}|c .G,\u00acX .c}. For instance, {{X,Y },{\u00acX,Z},{Z}}/X ={{Y },{Z}}. This \noperation provides a reduction of G under the assumption that variable X is true. Indeed we have the \nfollowing consistency result: PROPOSITION 8. Let G be a disjunctive normal form and X be a variable. \nIf G is a tautology, so is G/X. PROOF. By construction, G/X has its variables in Var \\{X}. Let v be a \nvaluation of this set of variables. De.ne v ' as the extension of v to Var such that v '(X)=T. Since \nG is a tautology, v '(G)is true, so there is a clause c in G such that v '(c)=T. Since v '(X)=T, c does \nnot contain \u00acX,so c \\{X} is a clause of G/X, and v(c \\{X})=T, so v validates G/X. PROPOSITION 9(COMPLETENESS). \nLet G be a disjunctive nor\u00admal form. If G is a tautology, then I G can be derived in L... PROOF. We actually \nprove a slightly stronger result, namely that under the same conditions, for any set of clauses . the \nsequent I G,. is derivable in L... We proceed by induction on the number of variables in G: if there \nis no variable, G is reduced to one trivial clause T, and I T,. is derived using the nullary rule. Otherwise, \nlemma 7 proves that G has a totally negative clause, so we can write G =(\u00acX1 .\u00b7\u00b7\u00b7.\u00acXk),G' We will thus \nderive I G,. by applying the k-ary rule to the k for\u00admulas we get by supposing that one of these variables \nis true. For each i, de.ne Gi ={c |c .Xi .G,\u00acXi .c} .i ={c |c .G,Xi .c,\u00acXi .c} The union of these two \nformulas is actually the quotient G/Xi, and from proposition 8 we know that the formula Gi,.i is a tautology. \nSince it contains strictly fewer variables than G, the induction hy\u00adpothesis proves that the sequent \nI Gi,.i,G' ,. is derivable. Besides, from this de.nition, obviously .i is a subset of G', so the sequent \nI Gi,G' ,. is derivable. Moreover, since for each i the formula Xi .Gi is also included in G', we can \nwrite I G1,G' ,. \u00b7\u00b7\u00b7 I Gn,G' ,. I (\u00acX1 .\u00b7\u00b7\u00b7.\u00acXn),G' ,. which concludes the proof. We also have to prove \nthat this is actually a proof system, in the sense that the formulas it derives are (classical) tautologies. \nThis can be proved using the characterization with sections: PROPOSITION 10 (SOUNDNESS). If a sequent \nI G is derivable in L.. then the disjunctive normal form G is a tautology. PROOF. Weprovebyinductiononthederivationthatanysectionof \nG contains opposite literals. First we can remark that if G contains the empty clause T, it has no section \nand the result holds trivially. Otherwise, let s be a section of G. The last rule is I G1,. \u00b7\u00b7\u00b7 I Gn,. \nI (\u00acX1 .\u00b7\u00b7\u00b7.\u00acXn), (X1 .G1), ...,(Xn .Gn),. for some n = 1, so s contains a \u00acXi. If it also contains \nXi, then the proof is .nished. Otherwise, its restriction to Xi .Gi,. is actu\u00adally a section of Gi,., \nwhich contains opposite literals by induction hypothesis. As said, none of these tautologies is intuitionistically \nvalid, except in the particular case where they contain the trivial clause T, there\u00adfore any behavior \nthey may specify is fundamentally concerned with global control. 4.2 Charts and Combinators Proofs in \nthe logic L.. can be interpreted as strategies in a counter\u00adexample game: in a node that proves G, the \nconclusion contains a distinguished totally negative clause \u00acX1 .\u00b7\u00b7\u00b7.\u00acXk. If we play this clause and \nthe opponent refutes it, he does so by providing a proof of an Xi, so we can deduce that G is equivalent \nto G/Xi and the proof may go down this branch. We can materialize this with a graphical notation that \nwe call control charts: where the label Xi on an edge represents the passing of a proof of Xi. The nullary \nrule is then interpreted as a simple leaf: T As illustrated below, it suf.ces to write the distinguished \ntotally negative clause in each node, then the charts are read down from the top. If the root node contains \na non-trivial clause \u00acX1 .\u00b7\u00b7\u00b7.\u00acXk, then play it. If the opponent refutes the literal \u00acXi with a value \nai of type Xi, then follow the edge labeled Xi and remember the value ai. By construction, when reaching \na node labeled T, the opponent cannot refute. 4.2.1 Example: a Twofold Excluded-Middle At each step, \nthe important clause is the distinguished purely nega\u00adtive one, so we write only this one. Taking for \ninstance the complete DNF for variables X and Y we have G ={\u00acX .\u00acY, X .\u00acY, \u00acX .Y, X .Y } The unique proof \nof this formula corresponds to the following con\u00adtrol chart, according to the informal description above: \n\u00acX .\u00acY XY \u00acY \u00acX Y X TT The point of this new notation is that it can also be understood as describing \na combinator CG: CG *f \u00b7p aX *a \u00b7p' aY *b \u00b7p' aXY *c \u00b7p' aYX *d \u00b7p' f\u00acX.\u00acY *aX \u00b7afX.\u00acY *a \u00b7aXY f\u00acX.Y \n*b \u00b7aYX fX.Y *a \u00b7c \u00b7p fX.Y *d \u00b7b \u00b7p Y \u00b7p \u00b7p \u00b7p The family a above corresponds to the edges in the chart, \nwhile f corresponds to the nodes. This de.nition is rather informal because it hides information: the \nterms ac are not constants since they are related to a particular instance of CG, moreover they must \nremem\u00adber the argument vector f and the initial stack p, as well as an a or a b in the case of aXY and \naYX . For instance, aXY should be written like aXY,G,but we avoid this notation for readability. f ,p,a \nAnother, more subtle point, is that these a and b may contain occur\u00adrences of aX and aY respectively \nand raise them again after aXY or aYX has been triggered. Though this behavior is type-theoretically \ncorrect, we can do better and prevent such non-linear behavior by rebinding the exceptions on the .y: \naX *a \u00b7p' fX.\u00acY *(.aX .a)\u00b7aXY \u00b7p aY *b \u00b7p' f\u00acX.Y *(.aY .b)\u00b7aYX \u00b7p aXY *c \u00b7p' fX.Y *(.aX .a)\u00b7(.aXY .c)\u00b7p \naYX *d \u00b7p' fX.Y *(.aYX .d)\u00b7(.aY .b)\u00b7p We observe that this is actually closer to exception handling as \nfound in functional languages, where exceptions are caught only once. The use of such rebinding is not \npurely syntactical, as it corresponds to what the speci.cation in proposition 6 meant: an exception aXa \nis always caught the same way, by calling the function fX.\u00acY with a as argument on the stack p, and if \nthis leads to a *pX then applying ' aX to another value a will also lead to a '*pX , so replacing aX \nby kpX will behave the same way. This is the argument for the introduction of .aX .a, as we will see \nin the proof of theorem 11 below. 4.2.2 General De.nition We are left now with the mission to de.ne, \nfor any chart, the asso\u00adciated combinator and to prove its correctness. Given a chart C over G, starting \nwith clause c = \u00acX1 .\u00b7\u00b7\u00b7.\u00acXk and with immediate sub-charts CX1 ,...,CXk , we de.ne CC inductively: CC \n*f \u00b7p fc *aX1 \u00b7\u00b7\u00b7aXk \u00b7p (1) ' aXi *v \u00b7p' CCXi *f \u00b7p (2) where the aXi are new language constructs (and \nnot variables, for instance), and where f ' is indexed on the clauses of G/Xi and de\u00ad.ned as ' f = fc \nif c .G (3) c ' f = fXi.c(.x.v[x/aXi ]) if Xi .c .G (4) c assuming, in the second case, that the type \nof fXi.c has the literal Xi in .rst position. Note that the notation aXi is not formally enough, because \nthe new rules imply that these objects have a memory of the chart C and the terms fc. To be precise, \nwe would need to put all required informa\u00adtion in the indices, however this would lead to a heavy notation \nthat we avoid by making them implicit. In a way, the .x.v[x/aXi ] represents what has been learned from \nraising the exception aXi , and we see that this exception will never be raised again, since aXi is no \nlonger free in the right-hand side of (2). In the particular case where c is the trivial clause T, the \nde.nition downs to CC *f \u00b7p fT*p so the combinator is a pure projection, indeed an intuitionistic con\u00adstruct. \nFresh symbols are needed at step (1) because at step (4) we have to be sure that what we bind was actually \ncreated at the corresponding step (1). This is crucial in the correctness argument below.  4.3 Correctness \nSo we have a class of combinators with clearly de.ned reduction rules, but we still have to prove that \nthey actually realize the type they are intended to implement. The dynamic binding, while mak\u00ading the \ncombinator s de.nition valid, imposes a restriction on the validity of realization. De.nition 3. An observable \nset . is called sequential if it is not empty, closed under reduction and validates the substitution \nprop\u00aderty: for any context e and any term t,if e[t] ... and if e[x] does not reduce into an x *p, then \ne[u] ... for all terms u. These conditions arise rather naturally in the correctness proof. Be\u00adsides, \nthe observables used in various speci.cation theorems are all of this form. Therefore, these speci.cations \nwill still hold in our en\u00adriched calculus. We examine below the simple example of Church booleans. Another \nexample, that of computational consistency in the presence of arithmetic constructs, is given in related \nwork of the .rst author [1]. THEOREM 11. Let G be a tautology in disjunctive normal form and C be a chart \nover G. For any sequential . , the combinator CC realizes the type G. PROOF. We proceed by induction \non the height of the chart C . When C is a leaf, we have G = T,c1,...,cn, which is the type Z . (c1 . \nZ) .\u00b7\u00b7\u00b7 . (cn . Z) . Z. As said, the de.nition of CC gives CC *f \u00b7p fT*p so for any . and any value for \n[Z],if fT F Z then for any p in [Z] we have CC *f \u00b7p fT*p ..., thus CC F G. Suppose C starts with a negative \nclause n = \u00acX1 .\u00b7\u00b7\u00b7.\u00acXk and let . be a sequential set of observables. Assume for each variable X a valuation \n[X] .., and call Z the variable used as the return type. For each clause c in G, let fc be a term that \nrealizes c . Z.By de.nition we have CC *f \u00b7p fn *aX1 \u00b7\u00b7\u00b7aXk \u00b7p so if we prove that the right member is \nin . we can conclude that CC realizes G. If the reduction of fn *a \u00b7p never places any aX in head position, \nfn *t \u00b7p will also ignore t for any family of terms, so it holds in particular if tX F \u00acX (the family \nis indexed on the variables in n), in which case fn *t \u00b7p is in . . Then by the substitution property \nwe deduce that fn *a \u00b7p is in . , since none of the |\u00acX| is empty (because . = 0/). Otherwise these exists \na propositional variable X, a multi-hole term context vX [] and a stack p' such that we have ' fn *a \n\u00b7paX *vX [aX ] \u00b7p' CCX *f \u00b7p using the notations of section 4.2.2, thus proving that the third exe\u00ad cutable \nis in . is enough to conclude. By induction hypothesis, the ' combinator CCX realizes G/X, so we have \nto prove that f realizes c ' c . Z for each c in G/X.If c is a clause of G, we have f = fc c and fc F \nc .Z by hypothesis. Otherwise X .c is a clause of G and ' f = fX.c(.x.vX [x]). By hypothesis, we know \nthat fX.c realizes c X . c .Z, so we can conclude if .x.vX [x] realizes X. Here is the key argument. \nLet us write n = X .m, assuming the .rst argument to fn has type X, and look closely at the reduction \nthat produces the context vX []. Let pX be a stack in [X]. Obviously the term kpX realizes \u00acX,so fnkpX \nrealizes m .Z, so we have these two convergent reductions, where a is any family indexed over m such \nthat \u00dfL F L for each literal L in m: fn *kpX \u00b7\u00df \u00b7p kpX *vX [kpX ] \u00b7p' vX [kpX ] *pX .x.vX [x] *pX vX \n[kpX ] *pX Observe that since aX is fresh when passed to fn, the instances of aX in aXvX [aX ] are exactly \nall the residuals of that fresh aX , there\u00ad fore the .rst reduction holds. We have fn F X .m .Z, kpX \nF X and\u00df F m and the stack p is in [Z], therefore the .rst executable is in . , and since . is closed \nunder both reduction and anti-reduction, .x.vX [x] *pX is also in . , thus .x.vX [x] realizes X as required. \nThe import of this correctness result is that one can safely extend the ..-calculus with the family CC \nand assign type G to CC when C is a chart over G, while keeping the coherence of the whole system, as \nwe will prove now. Tji is understood as an exception, i.e. an object of negative type that one throws \nwith value v by writing raise (Tv),or simply (Tv),  Cijx j is understood as a co-exception, i.e. an \nobject that re\u00adceives a value of a positive type thrown by an associated ex\u00adception of the same name \nCij and binds it in ti to the variable  xj. The corresponding DNF is k pk nk ^ \u00ac Ti j . 4.4 Computational \nConsistency Ci j _^ G = Showing that new combinators integrate well in the tying system is good thing, \nand showing that the calculus itself still makes sense is also useful. To this end, we show that some \nnon-trivial speci.ca\u00adtion result, namely the one for Church booleans exposed in propo\u00adsition 3, still \nholds in the enriched calculus. For each b .{0,1},we de.ned the type boolean b as Bb = .X0.X1(X0 .X1 \n.Xb) Then we have the following speci.cation: PROPOSITION 12. Let t be a term typable as Bb for some \nb . {0,1}. For any couple of variables (u0,u1) the executable t *u0 \u00b7 u1 \u00b7e reduces into ub *e. PROOF. \nLet u0 and u1 be two variables not appearing in t. De.ne . as the closure of {ub *e} by anti-reduction, \nwhich is clearly a sequential observable set. Since ft : Bb is derivable by hypothesis, adequacy proves \nthat t F Bb. De.ne [Xb] to be {e} and [X1-b] to be empty. Then ub F Xb and u1-b F X1-b,so t *u0 \u00b7u1 \u00b7e \nis in . , which means that it reduces into ub *e. This proposition, as compared to its previous statement, \ndeals with variables as arguments and requires an empty stack. The reason for this is that the closure \nby anti-reduction of {ub *e} is then a sequential observation. Stating a general result would require \nto de.ne . as the smallest sequential observation that contains ub *p (in the notations of proposition \n3). The conclusion would be that t * u0 \u00b7u1 \u00b7p reduces not into ub *p, but into something observationally \nequivalent, namely a reduction of it, potentially with a substitution performed on terms that never reach \nthe head. However, by reasoning about the stability of variable substitution with respect to the reduction \nrelation, we can get back to the original speci.cation. We will not formalize this, because it is beyond \nthe point of our remark, and because it would require some amount of work, in particular the introduction \nof stack variables.  5 A Possible Syntax To get an idea of the meaning of the combinators CC as program\u00adming \nconstructs, we sketch a syntax for them in an ML-like lan\u00adguage. On the model of constructs like the \ntry body with Exn x . handler of exceptions, let us write sync catch C1 1 x1,...,C1 p1 xp1 throw T 1 \n1 ,...,T 1 n1 do t1 . . . catch Ck 1x1,...,Ck pk xpk throw T k 1 ,...,T k nk do tk as a syntactic sugar \naround CC applied to terms t1,...,tk. All the Tji and Cij belong to the same set of names, indexed by \nthe variables of G: i=1 j=1 j=1 The corresponding chart is constructed inductively by picking the .rst \ncompletely negative clause (that catches nothing) and using the quotient construction. Of course, this \nmight not succeed if G is not a tautology, and this is part of the type-checking of this structure. If \nit does type-check, we know from the correctness result that no exception can escape. In a programming \nsetting, using names in place of the propositional variables is mandatory since pure type inference cannot \ndiscover the whole structure, in particular because different propositional vari\u00adables may be assigned \nthe same type in a particular instantiation. 5.1 Back to the Example Returning to the example of the \ntwofold excluded middle (as seen in section 4.2.1), the syntax sync throw A,B do t1 catch Ax throw B \ndo t2 catch By throw A do t3 catch Ax,By do t4 will then represent the term CC (.A.B.t1)(.x.B.t2)(.y.A.t3)(.x.y.t4). \nThe semantics of this construct will be to execute t1 and return its return value, unless it raises one \nof the exceptions. If the execution of t1 leads to raise (Aa), then exception A is caught; the value \na is sent to all the cases that are able to catch A (here t2 and t4) and those that throw A are discarded \n(here t3, and of course t1). The result is that the execution continues with sync throw B do t2[a/x] \ncatch By do t4[a/x] Note that with this particular tautology, there is only one possible control chart, \nso the order in which clauses are written is purely cosmetic. As shown in the reduction above, in the \nsimple case of the excluded middle, the syntax boils down to sync throw Exn do body catch Exn x do handler \nwhich is not too far from the usual try ...with presentation, except that we are dealing here with a \nlocally de.ned exception and that we provide strong typing.  6 Conclusions Information .ows mostly from \nprogramming language design to theory and not in the other direction. The story of control oper\u00adators, \nof how they were discovered to correspond to Peirce s law by Grif.n [6] and Felleisen, and only later \nwere put on the logic workbench [5, 6, 12, 13, 3], makes this absolutely clear. We have presented here \nan analysis of disjunctive normal forms leading to the synthesis of a new family of multi-exception han\u00addlers \nwhich we can present also in a reasonable programming-style syntax. Free with the method, based on running \nCurry-Howard backwards and using Krivine s realizability, comes the means of asserting the correctness \nof these combinators. The realizability approach shows really strong here in allowing us to deal with \na con\u00adtrolled form of dynamic binding and to smoothly extend the typing system. The combinators we have \nconstructed are all based on the idea of using only negative exceptions. But, DNF always have a purely \npositive clause as well, not just a purely negative one. Building on this idea, it is possible to develop \na completely symmetric world of co-exception based combinators, or even to mix styles, and this still \nneeds to be explored. Another question is whether the analogy between control charts and winning strategies \ncan be made rigorous and whether there is a con\u00adnection to Kreisel s famous no-counter-example interpretation \nor other game-based explanations of classical truth. The theory of realizability shows to smoothly extend \nto more so\u00adphisticated calculi, with constants and data types [1]. This suggests that our results will \nstill hold in settings closer to real programming languages, the .rst example being that of PCF with \ncontrol. On the way towards realistic programming settings, extensions of .\u00adcalculus with other forms \nof control, like partial continuations [7], catch/throw [11, 8] could be studied with our techniques. \nFinally, another challenging question is whether one can do the same analysis in a call-by-value scenario \nand get new, clean and ab\u00adstract control forms that one can prove to be correct and that would be more \nmeaningful for an actual programming language. We think it is possible. 7 References [1] E. Beffara. \nRealizability with constants. Workshop on Formal Methods and Security, Nanjing, China, 2003. [2] G. Cousineau \nand M. Mauny. The Functional Approach to Programming with Caml. Cambridge University Press, 1998. [3] \nV. Danos, J.-B. Joinet, and H. Schellinx. A new deconstructive logic: Linear logic. Journal of Symbolic \nLogic, 62:755 807, 1996. [4] V. Danos and J.-L. Krivine. Disjunctive tautologies as syn\u00adchronisation \nschemes. In P. Clote and H. Schwichtenberg, ed\u00aditors, Proceedings of CSL 00, number 1862 in Lecture Notes \nin Computer Science, pages 292 301, Fischbachau, 2000. Springer Verlag. [5] J.-Y. Girard. A new constructive \nlogic: Classical logic. Math\u00adematical Structures in Computer Science., 1992. [6] T. G. Grif.n. A formulae-as-types \nnotion of control. In 17th Symposium on Principles of Programming Languages, pages 47 58. ACM, Jan. 1990. \n[7] Y. Kameyama. A type-theoretic study on partial continua\u00adtions. In J. van Leeuwen, O. Watanabe, M. \nHagiya, P. D. Mosses, and T. Ito, editors, Theoretical Computer Science: Exploring New Frontiers of Theoretical \nInformatics, volume 1872 of Lecture Notes in Computer Science, pages 489 504. Springer Verlag, 2000. \n[8] Y. Kameyama and M. Sato. Strong normalizability of the non\u00addeterministic catch/throw calculi. Theoretical \nComputer Sci\u00adence, 272(1 2):223 245, 2002. [9] J.-L. Krivine. Typed lambda-calculus in classical Zermelo-Fr\u00e6nkel \nset theory. Archive in Mathematical Logic, 40(3):189 205, 2001. [10] J.-L. Krivine. Dependent choice, \nquote and the clock. The\u00adoretical Computer Science, to appear. [11] H. Nakano. A constructive formalization \nof the catch and throw mechanism. In Symposium on Logic in Computer Sci\u00adence (LICS 92), pages 82 89, \nSanta Cruz, California, 1992. [12] C.-H. L. Ong and C. A. Stewart. A Curry-Howard founda\u00adtion for functional \ncomputation with control. In Proceeding of POPL 97, 1997. [13] M. Parigot. Strong normalization for second-order \nlambda\u00admu calculus. In Proceedings of LICS 93, 1993.  \n\t\t\t", "proc_id": "944705", "abstract": "All classical ?-terms typable with disjunctive normal forms are shown to share a common computational behavior: they implement a local exception handling mechanism whose exact workings depend on the tautology. Equivalent and more efficient control combinators are described through a specialized sequent calculus and shown to be correct.", "authors": [{"name": "Emmanuel Beffara", "author_profile_id": "81100404694", "affiliation": "Universitat Paris 7, Equipe PPS, Paris Cedex, France", "person_id": "PP16000351", "email_address": "", "orcid_id": ""}, {"name": "Vincent Danos", "author_profile_id": "81100647621", "affiliation": "Universitat Paris 7, Equipe PPS, Paris Cedex, France", "person_id": "PP14221896", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/944705.944724", "year": "2003", "article_id": "944724", "conference": "ICFP", "title": "Disjunctive normal forms and local exceptions", "url": "http://dl.acm.org/citation.cfm?id=944724"}