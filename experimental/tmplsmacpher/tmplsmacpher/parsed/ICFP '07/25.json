{"article_publication_date": "10-01-2007", "fulltext": "\n Feedback Directed Implicit Parallelism Tim Harris Microsoft Research, Cambridge, UK tharris@microsoft.com \n Abstract In this paper we present an automated way of using spare CPU re\u00adsources within a shared memory \nmulti-processor or multi-core ma\u00adchine. Our approach is (i) to pro.le the execution of a program, (ii) \nfrom this to identify pieces of work which are promising sources of parallelism, (iii) recompile the \nprogram with this work being performed speculatively via a work-stealing system and then (iv) to detect \nat run-time any attempt to perform operations that would reveal the presence of speculation. We assess \nthe practicality of the approach through an imple\u00admentation based on GHC 6.6 along with a limit study \nbased on the execution pro.les we gathered. We support the full Concur\u00adrent Haskell language compiled \nwith traditional optimizations and including I/O operations and synchronization as well as pure com\u00adputation. \nWe use 20 of the larger programs from the no.b bench\u00admark suite. The limit study shows that programs \nvary a lot in the parallelism we can identify: some have none, 16 have a potential 2x speed-up, 4 have \n32x. In practice, on a 4-core processor, we get 10-80% speed-ups on 7 programs. This is mainly achieved \nat the addition of a second core rather than beyond this. This approach is therefore not a replacement \nfor manual paral\u00adlelization, but rather a way of squeezing extra performance out of the threads of an \nalready-parallel program or out of a program that has not yet been parallelized. Categories and Subject \nDescriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming Parallel programming General Terms \nAlgorithms, Languages, Performance Keywords Implicit Parallelism, Functional Programming, Haskell 1. \nIntroduction Parallel processing has now reached consumer desktop machines: all major vendors offer multi-core \nprocessors which are capable of executing 2, 4, and soon 8 threads in parallel. Where will we .nd enough \npro.table work for these threads to do so that a user perceives a 2-core machine as being better than \na uni-processor, or a 4-core system better than a 2? In this paper we return to an old idea: can we .nd \nthis kind of parallelism automatically in existing programs? If we can do this then it would avoid programmers \nneeding to grapple with explicit abstractions for parallel programming. Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 07, October 1 3, 2007, Freiburg, Germany. \nCopyright c &#38;#169; 2007 ACM 978-1-59593-815-2/07/0010. . . $5.00 Satnam Singh Microsoft Research, \nCambridge, UK satnams@microsoft.com We work with programs written in Haskell (Peyton Jones et al. 1996), \na pure, lazy, functional language which supports monadic I/O. In principle this language is a great .t \nfor multi-core hardware: purity means that the compiler or run-time system can evaluate multiple parts \nof a program in parallel without needing to worry about data races. In practice we encounter .ve problems: \n Programs vary in the amount of parallelism that is actually available. As we show, some have a lot but \nsome have very little.  Even in programs with abundant parallelism, the work must be at a suf.ciently \ncoarse granularity that the parallel speed up compensates for the overheads introduced in managing the \nwork.  In languages with lazy evaluation, it is not immediately clear which pieces of computation will \nactually contribute to the real work of the program. Performing un-needed work can harm performance \nfor example it can allocate a lot of memory and trigger extra garbage collections.  Even if the source \ncode for a piece of work appears pure, its compiled implementation may contain side effects (e.g. to \nperform memoisation).  Although the core of a language may be pure, practical pro\u00adgrams written in it \nare likely to involve some kinds of I/O or updates to mutable storage locations.  We base our work around \nthe use of thunks allocated by optimized Haskell programs. Thunks provide a natural abstraction with \nwhich to look for implicit parallelism: in principle the run-time system can evaluate any of the thunks \nthat have been allocated. We quantify this with a limit study examining the parallel speed-up we would \nachieve if we could evaluate all thunks as soon as they are allocated (Section 2). This limit study is \nclearly unrealistic from a practical point of view it assumes there is an in.nite number of processors, \nand that there are no overheads introduced by running thunks in parallel. Our results show that many \nprograms do actually exhibit substantial degrees of implicit parallelism in this simulated environment: \n16 of our 20 test programs show at least a 2x speed-up and 4 show at least an 32x speed-up. Can we achieve \nsuch a speed-up in practice? To do that we must be able to predict which thunks are likely to be good \ncandidates for parallel execution. We want to select thunks that are likely to be needed by the program \nand which will run for long enough that the parallelism gained compensates for the overheads introduced. \nIn Section 3 we show that, for most programs, we can make these predictions based on a thunk s allocation \nsite. Restricting ourselves to selecting long-running thunks loses some potential parallelism, but we \nstill see simulated 2x speed-ups on 7 of our tests. In Section 4 we show how we use these predictions \nin practice in the Glasgow Haskell Compiler (GHC). When we predict that a thunk allocation site will \nproduce useful work we spark the thunk, putting it into a shared pool of work that can be performed speculatively. \nWe modi.ed the GHC run-time system to add (i)a work-queue mechanism for managing sparked thunks, (ii)anew \nthunk-locking mechanism to prevent duplication of work between speculative execution and direct execution, \nand (iii) a mechanism to detect and prevent I/O operations that may reveal the presence of speculation. \nSection 5 summarizes the results from our implementation of feedback-directed implicit parallelism (FDIP). \nWe see performance improvements in all of the programs that our simulator predicted, but the practical \noverheads eat into the parallelism, leaving 10\u00ad80% speed-ups on the 7 programs that offered the potential \nfor feedback-directed implicit parallelism. We show results for all of our benchmarks, including those \nwhich get a negligible speed-up and those which are slowed down. In practice we would use the simulation \nresults from Section 4 to select whether or not to use FDIP for a particular program to try to counter \nthis speculation risk. As we conclude in Section 7, FDIP can .nd useful parallelism in some programs \nand get useful performance improvements on real commodity hardware. However, this approach is clearly \nnot a silver bullet for parallel programming: it provides a way of squeez\u00ading extra performance out of \nthe threads of an already-parallel pro\u00adgram or out of a program that has not yet been parallelized. In \nparticular, we make the following contributions: We introduce a low-overhead mechanism for building \nparal\u00adlelism pro.les from optimized compiled code.  We introduce heuristics that use these pro.les to \npredict which thunk-allocation sites are likely to represent coarse-grained sources of parallelism. \n We show how these predictions can be used with full-run feed\u00adback to improve the performance of real \nlarge applications run\u00adning on commodity multi-core hardware.   2. Pro.ling In this section we examine \nthe amount of implicit parallelism avail\u00adable in optimized programs compiled by GHC (speci.cally, we \nuse -O on the GHC command line). This is a limit study in which we ignore practical matters such as the \noverhead of scheduling work on multiple processors: as we will show, much of this parallelism is at too \n.ne a granularity for it to be exploitable on stock hardware. However, the results are still important \n the limit study gives an upper bound on what we could hope to achieve in practice. Section 2.1 outlines \nthe implementation of lazy evaluation in GHC; the techniques it uses are central to our de.nition of \nimplicit parallelism. Then, in Sections 2.2 2.5 we describe our techniques for tracing compiled Haskell \nprograms and measuring the implicit parallelism that they actually contain. 2.1 Lazy evaluation in GHC \nLazy evaluation in GHC is based on the allocation and execution of thunks which represent suspended computations \nwhose results may or may not be needed. Consider this expression as an example1: let x = f(1) --T1 y \n= f(2) --T2 iny+x For clarity in these short examples we assume that let is imple\u00admented by allocating \na thunk for each of the variables it introduces, and that evaluating + will require its left argument \nbefore its right one. In this example thunk T1 will compute the value of f(1),and 1 The syntax -- causes \nthe rest of a line to be treated as a comment. Haskell Header Result Payload words stack word word (if \nany) Entry code (a) An unevaluated thunk consists of a header word that points to the thunk s implementation, \na result word which will be updated to hold the thunk s result once it has been computed, and zero or \nmore payload words which provide values for the free variables used by the thunk. Haskell Header Result \nPayload words stack word word (if any)  thunk (b) When a thread requires the value in the thunk it \nbranches to the function that the thunk s header word points to. This entry code pushes an update frame \nonto the thread s stack, identifying the thunk under evaluation. Haskell Header Result Payload words \nstack word word (if any) (c) The thunk s evaluation is complete when execution returns to the update \nframe. The thunk is then updated, placing its value in the result word and replacing the header word \nwith a pointer to indirection entry code (IND). If the thunk s value is needed again then the IND code \nreturns the contents of the thunk s result word. Figure 1. Thunk evaluation in GHC. similarly for T2 \nand f(2). In this case the value of y is needed al\u00admost immediately: the thunk is said to be entered \n(causing f(2) to be evaluated) and then, assuming f(2) terminates, the thunk is updated (overwritten \nin memory so that the result of f(2) is imme\u00addiately available if it is needed again). Figure 1 shows \nthe life-cycle of a thunk in more detail. Many of the transformations made by an optimizing compiler \nfocus on eliminating thunk creation in this kind of code: this is worthwhile because it avoids heap-allocating \nthe thunk itself, elim\u00adinates the book-keeping work on entry and update, and can expose opportunities \nfor further traditional optimizations (for instance if the body of f is inlined and f(2) can then be \nevaluated at compile time). In our example one would not expect a thunk to be created for T2 because \nthe let expression will always need the result of f(2). As we said in the introduction, our implementation \nand per\u00adformance measurements are all designed to work with code with these optimizations applied. As \nwe discuss in the conclusions in T1 letx=f(1)--T1 T2 y = f(2) --T2 iny+x T (a) T allocates thunks T1 \nand T2 and then evaluates both of them. T3 letx=f(1) --T3 T4 y=f(2)+x--T4 in y T (b) T allocates T3 \nand T4. T evaluates T4 which in turn requires T3 to be evaluated. T5 letx=f(1) --T5 T6 y=x+ f(2) --T6 \nin y T   (c) T allocates T5 and T6. T evaluates T6 which immediately requires T5 to be evaluated. Figure \n2. Examples of thunk allocation and lazy evaluation. Section 7, there is an interesting trade-off between \nparallelism and optimization opportunities. Figure 2 illustrates some examples of lazy evaluation. In \neach case the let expression computes f(1)+f(2). Looking .rst at Figure 2(a), following our .rst example, \nthe code in the left column allocates two thunks on entry to the let expression. Evaluating y+x in the \nbody of the let forces T2 to be evaluated and then T1.The center column of Figure 2(a) shows what happens \nat run-time. The boxes show pieces of work involved in the implementation of each thunk, represented \nin execution order from left to right. If the let expression is in thunk T then execution starts with \nshort steps that allocate thunks T1 and T2. Execution then branches to T2 which computes f(2) before \nreturning to T. Execution proceeds to T1 which computes f(1) and returns again to T. We return to the \nright column of the .gure later. In Figure 2(b) the addition is moved into one of the thunks. This is \nshown in the center by the fact that execution proceeds directly from T4 to T3 rather than back to T \nin between. Finally, in Figure 2(c), the operands of the addition are swapped over. At run-time this \nmeans that T6 almost immediately enters T5 and the body of T6 only executes when the result from T5 has \nbeen computed.  2.2 Computing available parallelism In the examples Figure 2(a)-(c) the total amount \nof work performed at run-time is the same. However, they differ in the amount of implicit parallelism \nthat is available at the granularity of thunks. In Figure 2(a) thunks T1 and T2 can start being evaluated \nin parallel as soon as they have been allocated. The same is true for most of the execution of the thunks \nin Figure 2(b), aside from the very last step of T4 that must wait until T3 has been completed. Conversely \nthere is little parallelism available in Figure 2(c): the very .rst thing that T6 does is to enter T5 \nand so the bulk of T6 cannot proceed until T5 has been evaluated. Do real programs behave like examples \n(a) and (b), or do they behave like example (c)? We can answer this question by tracing the execution \nof real programs and examining how they use thunks. Our basic approach is (i) to modify GHC so that programs \ncan be run in a tracing mode which records the dependencies between different pieces of thunks execution, \n(ii) from this trace build a graph whose nodes are pieces of work labeled with the work s duration, and \nwhose edges represent the dependencies that execution must respect, (iii) .nd the critical path through \nthis graph from the thunk representing the start of execution to that computing the program s .nal result. \nThe critical path length gives us a lower bound on how fast this same execution could have run on a multi-processor \nmachine. If there is little parallelism in this idealized view then there is cer\u00adtainly none in practice. \nConversely, even if the critical path suggests that there is abundant parallelism then, of course, we \nmust remain sceptical of the result; this limit study assumes an unbounded num\u00adber of cores are available, \nthat no additional book-keeping over\u00adheads are introduced, and no further slow-down is caused by ad\u00additional \npressure on the memory management subsystem or other resources in the run-time system or hardware. At \na high level there are three kinds of dependency between pieces of work involved in evaluating thunks: \n Pieces of work from the same thunk must execute serially. For example, in Figure 2(a), all of the pieces \nof T execute in order.  A thunk cannot start being executed until it has been allocated. For example, \nin Figure 2(a), the thunks T1 and T2 cannot start to be evaluated until they have been allocated by T. \n If a thunk U requires the result of a thunk V then U s execu\u00adtion cannot proceed past that point until \nV has been completed and the result supplied. This is what harms parallelism in Fig\u00adure 2(c): T6 requires \nthe result of T5 early in its own execution.  The right hand column of Figure 2 illustrates these dependencies \nfor our three examples. The blocks represent the individual steps of execution that each thunk is broken \nup into, and the dashed lines represent the dependencies that exist. For example, in Figure 2(a), the \n.rst step of T allocates T1, the second step allocates T2.The third step of T enters T2. Notice how this \ndoes not introduce a dependency from T to T2: T2 was available for execution at any point since its allocation. \nHowever, the fourth step of T is dependent on receiving the result of T2, and the .nal step of T on receiving \nthe result of T1. 2.3 Implementation It is vital that we minimize the impact that our trace-collection \nhas on the program under test; many steps are very small and so there is a risk that the probe effect \nintroduced by tracing will affect theresults.Wedothisbytryingtoperform mostworkas part of garbage collection \n(GC). The current garbage collectors in GHC are all stop-the-world designs in which all mutator threads \nare suspended during collection. GC will already drastically perturb the contents of the processor s \ncaches and so it provides a good time to add extra work associated with pro.ling. In this limit study \nour input programs are all sequential. How\u00adever, our tracing infrastructure does support programs using \nmul\u00adtiple Haskell threads multiplexed over a single operating system thread. Events. We add tracing to \neach thunk at each stage of its allocation\u00adentry-update lifecycle. We batch up trace-events in a simple \nin\u00admemory buffer between GCs. To gather the step-dependency graphs we need to trace .ve kinds of event, \n(i) thunk allocation, (ii) thunk entry, (iii) thunk update, (iv) Haskell thread switches, and (v) thunk \nrelocation events generated at GC time. The .rst three kinds of event correspond to the steps in the \nthunk life-cycle in Figure 1. Each event includes the processor cycle-counter value when it is generated. \nIn each case thunks are identi.ed by their address in the heap and, in thunk allocation events, each \nstatic allocation site is identi.ed by a unique integer ID allocated sequentially by the compiler. Thread \nswitch events are generated when the user-level sched\u00aduler in the run-time system switches to or from \na given Haskell thread. A special thread ID is used to represent switching to work in the run-time system \nitself; this avoids us accounting work in (e.g.) the GC to the thunk which happened to be active when \nthe collec\u00adtion was triggered. Thunk relocation events are generated at GC time and include the old and \nnew locations of the thunk. Trace processing. At GC time we process the in-memory buffer to generate \na text .le on disk that can be processed off-line to determine the implicit parallelism available. In \ngenerating the on-disk .le format we map each thunk to a unique sequence number; this provides a stable \nidenti.er for the thunk, meaning that trace-processing tools do not need to track the location of the \nthunk if it is relocated by the GC. The mapping from thunk addresses to unique IDs is held in a hashtable \nseparate from the Haskell heap. This avoids perturbing the in-memory represen\u00adtation of thunks by extending \nthem with space to hold the ID. Each line in the on-disk .le represents either (i) a relationship between \ntwo steps, or (ii) execution of a step, along with the cycle\u00adcount timing of that step. We use the notation \nX.Y to mean step number Y of thunk number X. For example, the execution shown in Figure 2(a) would be \nrepresented as follows, assuming that thunk T is numbered 100, T1 101, T2 102:  100.1 101.0 A-1234 100.1 \n100.2 S-700 100.2 102.0 A-1235 100.2 100.3 S-500 100.3 101.0 E 101.0 101.1 S-5000  101.1 100.4 U 100.4 \n100.5 S-500 100.5 102.0 E 102.0 102.1 S-6000 102.1 100.6 U The A-lines indicate allocation dependencies, \ne.g. the .rst line shows that step 100.1 .nished by allocating thunk 101. The number 1234 identi.es the \nstatic allocation site in the program. The S-lines indicate execution steps, e.g. the second line shows \nthat step 100.1 took 700 cycles to execute measured with the user\u00admode processor cycle counter instruction. \nAs Figure 2(a) shows, execution starts by T allocating the other thunks in a series of short steps. The \nE-lines record when a thunk is .rst entered in sequential execution. They are super.uous from the point \nof view of the limit study but allow us to record the path taken by sequential execution for comparison \nwith the possible parallel execution. Finally, the U-lines record when one thunk completes execution \nby being updated. For example, the line 101.1 100.4 U shows that execution returns from thunk T1 (number \n101) to thunk T (number 100). Validation. We validated the tracing infrastructure by having it maintain \na shadow stack of the thunks it believed were under evaluation in each Haskell thread. A thunk entry \nevent pushes a new thunk onto the stack. A thunk update event pops a thunk from the stack. We report \nan error if (i) the thunk popped from the shadow stack does not match the thunk supplied as a parameter \nto thunk update,(ii) a thunk is entered before an allocation event is seen for it, (iii) a thunk is updated \nbefore an entry event is seen for it. This approach was invaluable in .nding places in the run-time system \nwhere thunks were manipulated without recording the nec\u00adessary trace events2. Ultimately we could compile \ntraced versions of GHC itself (including the Haskell libraries it uses) and run these without any errors \nbeing reported. Validation is often overlooked when collecting traces (Jain 1991) and our experience \nis that, with\u00adout validation, subtle problems would have remained undetected and skewed the results by \ncausing work to be attributed to the wrong thunk. 2.4 Replaying execution We measure the implicit parallelism \navailable in a program s exe\u00adcution by using the log to reconstruct a graph of the kind shown on the \nright hand side in Figure 2. Each execution step becomes a node in the graph and each allocation or update \nevent becomes a dependence edge between steps from different thunks. For this limit study we .nd the \nfastest possible execution of the trace by using a simple discrete event simulator modeling a machine \nwith an in.nite number of processors. Simulation events correspond to the completion of a step and, when \nan event .res, any subsequent step that is now eligible to run is started and an event scheduled for \nthat step s completion. The simulator outputs a summary of the execution time on this idealized parallel \nmachine and a trace showing how the number of active cores varies over time. 2.5 Results Figure 3 summarises \nthe test programs that we used and their thunk usage. For each program we recorded (i) the total run-time \nof the original compiled without any instrumentation, (ii) the total run\u00adtime that our instrumentation \naccounts to the evaluation of thunks, (iii) the mean thunk size, and (iv) the fraction of allocated thunks \n2 For readers familiar with the GHC run-time system: (i) when dealing with static thunks, (ii) thunks \nallocated by the run-time system to raise asynchronous exceptions, (iii) AP STACK thunks generated from \nupdate frames whose execution is suspended. Eval time (109 cycles) Thunk size Thunks Baseline Instrumented \n(cycles) needed atom Floating point simulation, no.b/spectral 0.34 0.64 (185%) 254 94% boyer Gabriel \nsuite boyer benchmark, no.b/spectral 0.47 1.09 (232%) 284 61% bsort-1 Sorting circuit model, locally \nwritten 0.77 1.18 (152%) 725 99% bsort-2 Sorting circuit model, locally written 1.68 2.64 (157%) 1245 \n99% cacheprof Cache pro.ling tool, no.b/real 1.82 2.81 (154%) 1369 97% calendar Prints a given year s \ncalendar, no.b/spectral 0.68 1.27 (186%) 585 99% circsim Circuit simulator, no.b/spectral 0.66 1.56 (237%) \n315 84% clausify Put propositions into clausal form, no.b/spectral 0.58 0.85 (146%) 405 93% compress \nText compression algorithm, no.b/real 4.61 5.30 (114%) 1969 99% fft2 Fourier transforms, no.b/spectral \n0.48 0.59 (123%) 1382 99% .bheaps Fibonacci heaps, no.b/spectral 0.26 0.25 (96%) 765 98% hidden Line \nrendering, no.b/real 0.70 1.02 (146%) 441 83% lcss Hirschberg s LCSS algorithm, no.b/spectral 0.33 0.47 \n(142%) 324 99% multiplier Binary-multiplier simulator, no.b/spectral 0.67 1.70 (254%) 213 99% para Paragraph \nformatting, no.b/spectral 1.98 3.26 (164%) 1301 92% primetest Primality testing, no.b/spectral 2.33 2.21 \n(95%) 9266 99% rewrite Equational rewriting system, no.b/spectral 0.29 0.23 (80%) 1059 82% scs Circuit \nsimulator, no.b/real 0.78 0.98 (125%) 862 84% simple Hydrodynamics and heat-.ow, no.b/spectral 1.80 2.87 \n(159%) 526 99% sphere ray tracer, no.b/spectral 0.70 0.74 (106%) 1832 85% Figure 3. Summary of the test \nprograms used. Figure 4. Implicit parallelism for our test programs with different execution thresholds. \nThe y-axis shows the parallelism achieved, so 1 means the same as sequential execution , and 2 twice \nas fast as sequential execution . that are needed by the computation. The difference between the .rst \ntwo numbers gives an upper bound on how much our instrumenta\u00adtion adds to the time spent evaluating thunks; \nas expected we tend to see larger differences when thunks are short. Our test programs were selected \nas follows. We started with the full real and spectral 3 sections of the no.b benchmark suite, along \nwith a number of other locally-written programs. We re\u00admoved programs which could not be immediately \nbuilt with our tracing system (for example because of dependencies on Haskell packages we had not installed). \nWe also removed programs which we could not readily con.gure to run for at least 1s so that the run\u00adning \ntime is large compared with our measurement precision. Fi\u00ad 3 Real contains real applications written \nin Haskell. Spectral contains substantial kernels of applications. nally, we examined the tests and made \nsure that they did not simply repeat a small code fragment in a way that would lead our system to conclude \nthat each repetition could run in parallel; it would be misleading to generalize from a test program \nwhere repeating an operation 500 times allows a speed up of a factor of 500 through parallelism. We did \nthis by con.rming that the parallelism was in\u00addependent of the number of repetitions. We preliminarily \nexperimented with whole-program instrumen\u00adtation, using an instrumented version of the Haskell libraries \nalong with instrumentation in the program under test. Using instrumented libraries did not affect the \nimplicit parallelism in these tests and so, in all the measurements reported here, we use ordinary uninstru\u00admented \nlibraries. From a practical viewpoint, this means that our results correspond to applying FDIP without \nneeding to recompile or modify libraries on a per-program basis. 6 4 21 0 0 2000 4000 6000 8000 10000 \n12000 14000 16000 Figure 5. An allocation site that should not be sparked despite hav\u00ading a high mean \nexecution time. The x-axis shows the thunks cre\u00adated at the site in allocation order, the y-axis shows \ntheir execution time in 1M-cycle units. Figure 4 summarizes our results, showing the parallelism achieved \nfor each of the programs and for a range of execution time thresholds. The programs are shown on the \nx-axis, and the speed-up achieved is shown on a logarithmic scale on the y-axis: 1 means the same as \nsequential execution , 2 means twice as fast as sequential execution , and so on. For each program we \nshow a cluster of bars corresponding to different thresholds on thunks execution times: only thunks that \ntake longer than this threshold are executed in parallel, shorter thunks are executed sequentially at \nthe point that they are .rst entered. We de.ne the execution-time of a thunk as the number of cycles \nin the sequential trace between when it is entered and when it is updated. The lowest threshold of 100 \ncycles causes all thunks to be executed in parallel as soon as they are allocated: the per-thunk logging \noperations add around 100 cycles to each thunk s execution. Behavior clearly varies between programs. \nSome programs show virtually no possible parallelism even for very small thresh\u00adolds boyer, calendar, \nand rewrite. Others provide some paral\u00adlelism, but only with very small thresholds fft2 and sphere. \nIt is unlikely that this could be exploited in practice; the book-keeping cost of dispatching a thunk \nfor execution on another thread is likely to be at least 10k cycles. However, some programs do continue \nto offer parallelism with thresholds over 10K: atom, bsort-1, bsort-2, hidden, lcss, primetest and simple \nall show a potential speed up of 2x or more even when using a 1M cycle threshold. 3. Feedback generation, \nrecompilation The results from the limit study show that several programs do provide appreciable amounts \nof implicit parallelism, even when restricting ourselves to large thunks of 1M cycles or more. Can we \nactually obtain any of this parallelism in practice? The problem is that, to obtain this parallelism \nat run-time, we need to predict which thunks will be worth running concurrently with application threads. \nTo take a contrived example, suppose that the limit study shows that a program has 4-way implicit parallelism. \nThis is easy to exploit if the program allocates exactly 4 long running thunks simply enqueue all of \nthe thunks for concurrent execution. However, Figure 3 shows that real programs allocate a vast number \nof thunks, some long running, some short running, and some which are allocated but never evaluated. To \ngenerate good\u00adquality feedback we must predict which thunks are (i)likelyto represent pieces of work \nthat are needed by the application, and (ii) likely to represent large pieces of work. Our basic approach \nis to use static allocation sites to predict whether or not thunks will meet these criteria and to make \na binary spark / not-spark prediction for each site. Intuitively thunks allo\u00adcated at the same point \nin the program may be expected to be used in similar ways. Earlier work in Haskell (Ennals 2004) has \nsug\u00adgested that this is true in practice, and earlier work on storage man\u00adagement in other languages \nhas shown static allocation sites to be a predictor of properties of the data s usage (Harris 2001). \nWe jus\u00adtify this decision later in this section by comparing the parallelism achieved by our per-allocation-site \ndecisions with that achieved in our limit study. Selecting thunks that will be needed. We do not want \nto spark thunks that are not needed: even if there are idle cores available then the extra work will \nadd pressure on the garbage collector. We select allocation sites based on a simple threshold on the \nfraction of thunks allocated at that site in the pro.ling run which were eventually entered. In our results \nthis parameter has little effect on the feedback quality over a wide range: our results use a threshold \nof 3/4 but the number of sparked thunks is unaffected up to a ratio of at least 255/256, and the run-time \nperformance is unaffected by requiring a 1/1 ratio. One explanation of this, at least in our test programs, \nis that allocation sites producing long-running thunks are also sites that produce thunks that are always \nneeded. Selecting thunks that provide coarse-grained parallelism. The key problem, however, is selecting \nthunks that are likely to repre\u00adsent a substantial piece of work at run-time: we want to ensure that \neach thunk we spark will provide enough work to compensate for the overhead of sparking it. In Section \n2 we de.ned a thunk s execution time as the num\u00adber of processor cycles in the sequential trace between \nwhen the thunk was entered and when it is updated. We initially hoped to use an allocation site s average \nexecution time to select sites to spark. However, this does not work well. For example, consider this \nfunc\u00adtion: noRealWork :: Int -> Int noRealWork0=0 noRealWork x = let t = noRealWork (x-1) --T1 in t The \nfunction noRealWork recurses deeply before returning 0. If called with a large parameter then a lot of \nthunks will be allocated at T1 and, if the recursion is deep enough, the mean execution time will make \nT1 look as though it is worth sparking. We might ideally spark the .rst thunk allocated in a given recursion, \nbut not the subsequent ones. However, this cannot be expressed by a binary spark / not-spark decision \nat the allocation site. Figure 5 shows a second problematic example taken from an actual program (bsort-2). \nThe graph plots the execution time of 15 000 thunks from a given allocation site in units of 1M cycles. \nThe execution times vary from around 0 up to 6.2M cycles al\u00adthough some short-running thunks are allocated \nat this site, the site as a whole still has a mean workload substantially over 1M cycles. Examining the \nbehavior of this allocation site in more detail shows that the .rst peak comprises 4 000 thunks which \nare allocated to\u00adgether near the start of the program. Thunk 4 000 is entered .rst. Thunk 4 000 then \nenters thunk 3 999, which enters thunk 3 998 and so on in turn. Only a few hundred cycles of work are \nperformed at each stage. The long running time of e.g. thunk 3 999 represents almost exactly the work \nas that of 4 000. However, while this case is similar to noRealWork, we would ideally spark the last \nthunk allocated in the recursion rather than the .rst thunk. We therefore took the approach of not sparking \nthunks created by allocation sites like these. Instead we try to identify allocation sites where each \nsparked thunk provides its own real work. We do this by considering the total work of an allocation site \nwhich we Figure 7. Simulated performance for sparking based on allocation site. We vary the mean work-size \nthreshold between 100K-1M cycles with a .xed 3/4 needed-thunk threshold.  de.ne as the time during which \nat least one thunk allocated at that site is under evaluation, and then .nd the mean work by dividing \nthis by the number of thunks allocated. For example, in Figure 5, a total of around 6.2M cycles is spent \nduring the evaluation of the .rst 4 000 thunks, and so the mean contribution of these thunks is only \n6.2M / 4 000 = 1.5K cycles. Figure 6 shows how we compute an allocation site s total work in a single \npass over the trace .le. We keep various mappings in\u00addexed by thunks and by allocation sites, updating \nthese as the al\u00adgorithm runs. ThunkToSite maps thunks to the allocation site that created them. EntryTime \nrecords the earliest sequential execution time at which any currently-active thunk at a given site was \nentered. EntryCount records the number of currently-active thunks at a given site. TotalWork records \nthe total work accounted to the allo\u00adcation site. CurTime records the current simulated time, advanced \nwhen processing each step in the trace. This approach deals with allocation sites generating thunks which \nrecursively enter other thunks from the same site. What about different sites? For example: inner :: \nInt -> Int inner x = let t = realWork x --T2 in t outer :: Int -> Int outer x = lett= innerx --T3 in \nt If we assume realWork is a long-running computation then T2 and T3 will both appear to generate long-running \nthunks even though T3 itself generates little work beyond its calls to inner. Do we need to avoid sparking \nboth allocation sites? We consid\u00adered an extension to the algorithm in Figure 6 which took a tentative \nselection of thunks and replayed the execution trace to identify the work provided by each tentatively \nselected thunk above and beyond that provided by other tentatively selected thunks that it recursively \nentered. We have not yet implemented that extension. There are two reasons. First, it requires two passes \nover the trace which would prevent us from applying it on-line in future work. Second, the depth of recursion \nthrough wrappers, and hence the number of small wrapper thunks created, is bounded above by the number \nof thunk allocation sites that we select for sparking beyond this depth one of the sites involved must \nbe providing real work. 3.1 Simulating the performance of our selections We re-ran our limit study to \nexplore the impact of the different mean-work thresholds. Figure 7 illustrates this. We use the same \naxes as our earlier results from the limit study and, for each pro\u00adgram, we include two bars from the \nlimit study for comparison: the parallelism achieved by sparking all thunks ( Limit all ) and the parallelism \nachieved by sparking all thunks whose execution time is 100k cycles or more ( Limit 100k+ ). We then \nadd four bars showing the speed-up when we select allocation sites using 100k, 250k, 500k and 1M-cycle \nmean-work thresholds. In many cases the simulated results are identical for 100k-500k thresholds. This \nis because the sparking decisions are identical: the parallelism is coming from a particular allocation \nsite or set of allocation sites, and disappears entirely when the threshold exceeds this site s mean \nwork. We lose some parallelism going from the limit study to the per\u00adallocation-site predictions. We \nexamined why this is in atom which is the program with the largest drop when comparing Limit 100k+ with \nSelect 100k . In this case it is because of thunk allocation sites that generate thunks with a variety \nof work times rather than because of allocation sites that generate thunks that are not always needed. \nWe did not study the other tests in detail, but high fractions of needed thunks in Figure 3 suggest that \nthis same conclusion is likely to be true in all of the programs where we lost parallelism. Figure 8 \nshows the number of allocation sites selected for spark\u00ading with a 100k threshold, the number of thunks \nthat they allocate, and the fraction of these that are needed by the program. The low numbers both of \nsparked sites and sparked thunks justify our decision not to avoid sparking short wrapper thunks: the \ntotal num\u00adber of wrapper thunks must be low. 4. Run-Time system The previous section showed that making \nper-allocation-site pre\u00addictions of which thunks to spark is able to achieve a parallel speed\u00adup in several \nof our test programs. In this section we show how we achieve a speed-up in practice rather than just \nin a simula\u00ad ThunkToSite :: Thunk -> AllocationSite EntryTime :: AllocationSite -> Time EntryCount :: \nAllocationSite -> Int TotalWork :: AllocationSite -> Time CurTime :: Time foreach record r { // Update \nthe total sequential work performed if (r is step) { CurTime += r.size; } // Thunk allocation: record \nmapping from this // thunk to its allocation site if (r is allocate-thunk) { ThunkToSite(r.thunk) = r.site \n} // Thunk entry: check whether we are already // evaluating a thunk from this allocation site if (r \nis enter-thunk) { site = ThunkToSite(r.thunk); if (EntryCount(site) == 0) { EntryTime(site) = CurTime \nEntryCount(site) = 1 } else { EntryCount(site) ++; } } // Thunk completion (update): check whether \n// we are still evaluating other thunks from // this allocation site if (r is update-thunk) { site = \nThunkToSite(r.thunk); EntryCount(site) --; if (EntryCount(site == 0)) { // Last thunk at this site: \nrecord // elapsed time since entering the // first thunk TotalWork(site) += CurTime -EntryTime(site) \n} } } Figure 6. Pseudo-code to compute the total work of an allocation site in a single pass over a trace \n.le. tor. The main idea is to add additional Haskell threads whose sole job is to evaluate sparked thunks \nspeculatively (Section 4.1). The sparked thunks are managed with a simple work-stealing system (Section \n4.2) with the addition of locks to prevent duplication of the same piece of work between a speculative \nthread and an applica\u00adtion thread (Section 4.3). These mechanisms replace GHC s exist\u00ading support for \nsparking thunks on shared-memory multi-processor machines. We summarize the differences and compare the \nperfor\u00admance of these two approaches in Section 4.4. We must be careful to preserve the semantics of \nthe original program. The problem here is that GHC provides two operations through which I/O can be preformed \noutside the normal monadic I/O system: unsafePerformIO :: IO a -> a unsafeInterleaveIO :: IO a -> IO \na Sites sparked Thunks sparked Thunks needed atom 3 801 100.00% boyer 1 1 100.00% bsort-1 5 4097 100.00% \nbsort-2 14 469 100.00% cacheprof 21 9161 100.00% calendar 1 400 100.00% circsim 11 3604 100.00% clausify \n1 20 100.00% compress 4 4 100.00% fft2 15 2573 100.00% .bheaps 2 17 100.00% hidden 15 7284 92.77% lcss \n5 776 100.00% multiplier 10 4008 99.98% para 5 31351 100.00% primetest 4 602 100.00% rewrite 4 20 100.00% \nscs 62 8805 98.17% simple 104 532 100.00% sphere 3 3 100.00% Figure 8. The number of thunks sparked during \nthe simulation, and the percentage of sparked thunks that are actually needed. Evaluating unsafePerformIO \nx causes the I/O action x to be per\u00adformed immediately. This back door can be used for ad-hoc pro\u00ad.ling \nand debugging, e.g. to probe which order different expres\u00adsions are evaluated. It can also be used to \nencapsulate I/O opera\u00adtions which the programmer asserts are free from side effects and independent of \ntheir environment. Performing unsafeInterleaveIO y provides a way to lazily defer I/O: the action y is \nperformed when the value yielded by unsafeInterleaveIO is demanded. This is used in the GHC libraries \nto implement lazy .le reading. We must be careful not to perform unsafe I/O inside speculative thunks: \nwe do not know whether or not the speculative work should be performed and, even if it should be performed, \nwe do not want to re-order the I/O operations that are executed. We deal with this problem by dynamically \ndetecting attempts to perform unsafe I/O operations while speculating. If an unsafe I/O operation is \nattempted then we suspend speculation of the current spark. The suspension mechanism means that if the \nthunk is subsequently entered then evaluation will resume at the I/O operation. We implement this by \nde.ning a new I/O action: nonSpeculatively :: IO () This has no side effect in an application thread, \nbut it suspends speculation if it is attempted during speculation. We then rede.ne unsafePerformIO and \nunsafeInterleaveIO in terms of the original versions of these operations: --Perform IO action m only \nif running in an --application thread doNonSpeculatively :: IO a -> IO a doNonSpeculatively m = do { \nnonSpeculatively;m} unsafePerformIO :: IO a -> a unsafePerformIO x = oldUnsafePerformIO (doNonSpeculatively \nx) unsafeInterleaveIO :: IO a -> IO a unsafeInterleaveIO y = oldUnsafeInterleaveIO (doNonSpeculatively \ny) Although these two unsafe operations are similar, note the asym\u00admetry in what is being prevented. \nSpeculative work must not be allowed to call unsafePerformIO.Incontrast,speculative work can call unsafeInterleaveIO \nbut evaluating the result of type a must be prevented. 4.1 Speculative worker threads The GHC run-time \nsystem performs its own user-level scheduling of Haskell threads. This lets it support a much larger \nnumber of Haskell threads than most OS threading libraries would provide. The maximum number of Haskell \nthreads that can run in parallel is supplied as a start-up parameter to the run-time system. In the terminology \nof the GHC run-time system, this creates a number of capabilities. Each capability holds resources that \nshould be per\u00adcore primarily a run queue of Haskell threads being scheduled over that capability and \na local memory allocation region. Haskell threads migrate between capabilities over long timescales for \nload balancing. The number of capabilities is typically tuned by the user to the number of cores available \non the machine. To deal with speculation we add an additional speculative work thread to each capability. \nIn most respects this is an ordinary lightweight Haskell thread which loops attempting to pick up and \nevaluate thunks that have been sparked for speculative execution. It differs from ordinary threads in \nthat (i) it has lower priority than other Haskell threads on the same capability, (ii)ifitattempts an \noperation that would block then the speculative evaluation it was attempting is suspended instead of \nthe thread waiting. The intuition behind this is that the blocking operation represents a long delay \nand so the speculative thread should continue performing useful work from another spark rather than actually \nwaiting. Speculation is suspended by updating the thunks under evalua\u00adtion with new thunks that will \nresume the computation at the point it was suspended. This mechanism is already used in several places \nin the GHC run-time system e.g. to save partial evaluation of thunks that are interrupted by the delivery \nof an asynchronous exception. 4.2 Managing sparked thunks We manage the sparked thunks with a basic work-stealing \nsystem. Each capability has its own spark-pool into which threads run\u00adning on it publish references to \nthe thunks that they spark. They do this by passing the newly-allocated thunk to a new function sparkSpeculation \nexported by the GHC run-time system. These calls are intended to only by added by the FDIP tool, not \ndirectly by the programmer. If the capability s speculative work thread runs then it preferentially takes \nsparks from its own pool. Otherwise it takes sparks from a random capability s pool. We take a random \nthunk from the pool. Since we assume that sparks generated by speculation represent large pieces of work \nwe use simple per-spark-pool mutexes and a steal-one policy. The bene.ts of a .ner-grained work-stealing \nsystem (Blumofe and Leiserson 1994; Hendler et al. 2005) would be negated by the thunk-locking mechanism \ndescribed below. 4.3 Preventing duplicate evaluation The existing implementation of thunk evaluation \nin GHC is de\u00adsigned to be thread-safe (Harris et al. 2005). The approach taken there is to make it safe \nfor concurrent threads to enter, evaluate, and update the same thunk at the same time. This can lead \nto dupli\u00adcate evaluation. However, referring back to Figure 1, such duplicate evaluation will only occur \nif a second thread enters a thunk in the window between a .rst thread entering it (Figure 1(a)) and updating \nit (Figure 1(c)). This approach is effective in practice because it avoids the cost of locking thunks \nwhile they are under evaluation. However, this re\u00adlies on a number of assumptions about how thunks are \nused. First, many programs are single-threaded: duplicate evaluation cannot occur in these. Second, in \nprograms using multiple threads, differ\u00adent threads are often working on different parts of a problem. \nThis can lead to an af.nity between threads and thunks. Third, many thunks are short-running and so the \nwindow during which duplicate evaluation is possible is short (our statistics in Figure 3 recon.rm this) \nand the cost of occasional duplication is low. The thunks sparked by FDIP do not behave like this. Most \nim\u00adportantly, we deliberately attempt to spark long-running thunks: the cost of duplicate evaluation \nis high, and the window during which it is possible is long. Furthermore, the work-stealing model intro\u00adduces \ncorrelations between the thunks being entered speculatively and those entered by application threads. \nFor example, this hap\u00adpens if an application allocates a series of thunks, adds these to the spark-pool, \nand then proceeds to evaluate one of the thunks itself. To avoid these problems we introduce locking \non thunks, but only on ones that are added to the spark-pool. As our simulation results show in Figure \n8, the number of sparked thunks is low compared with the total number. Sparked thunks are locked when \nthey are under evaluation, either by a speculative thread or by an application thread. If an application \nthread tries to enter a locked thunk then it blocks until the thunk s value is available. If a speculative \nthread attempts to enter a locked thunk then that suspension is suspended and it can resume speculation \nfrom another spark. The intuition behind this is that the locked thunk represents a large piece of work \n(because it was selected for sparking) and so the speculative thread should continue performing useful \nwork from another spark rather than waiting. We introduce this locking in the sparkSpeculation function \nthat is called at the allocation sites selected for feedback-directed sparking. Figure 9 shows what we \ndo. The original thunk (Fig\u00adure 9(a)) is cloned and overwritten by a shim-lock object (Fig\u00adure 9(b)). \nRemember that we call sparkSpeculation on a newly\u00adallocated thunk: it is thread-local at this point. \nOverwriting the thunk means that all references which would have referred to the original thunk will \nnow refer to the shim lock. When the shim-lock is entered the locking code is executed instead of the \noriginal thunk s entry code. This uses an atomic compare-and-swap on the header word of the clone, attempting \nto replace the pointer to its entry code with a pointer to a new locked\u00adthunk entry. The .rst thread \nto attempt this will succeed: it has locked the thunk. The lock holder branches to the original entry \ncode to evaluate the thunk. Other threads branch to the locked\u00adthunk entry code which blocks them (in \nthe case of application threads) or suspends their speculation (in the case of speculating threads). \nThe shim-lock is released by the normal thunk-update operation: as Figure 9(d) shows, the update overwrites \nthe header word once again. As an optimization, the lock holder can overwrite the shim\u00adlock s entry code \nwith an indirection. This avoids any subsequent threads from attempting to acquire the lock: they will \ndereference the indirection to either (i) enter a further indirection to the thunk s result value, (ii) \nenter the locked-thunk entry code. 4.4 Comparison with GHC 6.6 We compared the performance of the new \nimplementation of sparked thunks with the existing implementation in GHC 6.6. The existing implementation \nis used in Concurrent Haskell to build the par combinator. The primary differences between the imple\u00admentations \nare that (i) we introduce locking around sparked thunks, whereas GHC 6.6 does not, (ii) our speculative \nthreads pro-actively steal work from other spark pools, whereas GHC 6.6 periodically pushes work from \none spark pool to another pool that is empty, (iii) we use long-running Haskell worker threads for speculation, \nHeader Result Payload words word word (if any) Thunk to spark Entry code (a) Original thunk passed to \nsparkSpeculation. Thunk to spark (b) The thunk is cloned and the original overwritten by a shim-lock \nobject. Entering the shim-lock will execute the locking code. Thunk to spark  (c) The locking code (i) \nlocks the sparked thunk using atomic compare-and-swap on its header word, (ii) overwrites the shim\u00adlock \ns header word as an indirection so future threads branch to the locked thunk entry until the result is \navailable. Thunk to spark Entry Locked code thunk entry (d) The normal thunk-update operation releases \nthe shim-lock, leaving a double-indirection to be removed at GC time. Figure 9. Shim-lock implementation. \n4.5 4 3.5 3 2.5 2 1.5 1 0.5 0 0 10 20 30 40 Figure 10. Parallel speed-up on a 4-core processor (y-axis) \nagainst thunk granularity (x-axis) for GHC 6.6 and for the modi.cations we introduced to support FDIP. \nFor this workload the GHC 6.6 implementation provides a better parallel speed-up for very short\u00adrunning \nthunks, but the FDIP mechanisms provide a near-optimal 4-fold speed-up for the longer thunks that we \nare interested in. whereas GHC 6.6 creates a new Haskell thread for each spark that is .shed from a pool. \nFor our comparison we used a naive parallel implementation of fib n, computing the n th Fibonacci number \nby simple recur\u00adsive evaluation of fib n-1 and fib n-2. The call to fib n-1 is sparked and so, in principle, \neach level of recursion introduces par\u00adallelism. The parallel work becomes very .ne grained if we con\u00adtinue \nsparking thunks all the way down to fib 1 and so we set a threshold below which we switch to sequential \nevaluation. This lets us control the minimum size of the thunks we spark. Figure 10 shows the parallel \nspeed-up achieved for various thresholds in the computation of fib 38 (selected to take around 1s on \nour test machine). GHC 6.6 achieves no parallel speed-up for high thresholds: the spark-pushing algorithm \nis tied to allocation work and does not run frequently with this workload. Speed-ups of 2x -3x are achieved \non a 4-core machine for thresholds from around 25 (mean thunk size 4M cycles) down to 5 (mean thunk size \n2.5k cycles). The modi.cations we made achieve near-optimal 4x speed-ups for larger thunks because work \ncan be stolen by idle cores as soon as it has been sparked. This continues down to around threshold 17 \ncorresponding to 50k cycle thunks. Below this performance falls off rapidly and is worse than GHC 6.6. \nHowever, as we showed in the limit study in Figure 7 we are primarily interested in thunks above this \nsize where the FDIP modi.cations perform well. The performance of the FDIP run-time modi.cations led \nus to pick 100k as the threshold for selecting a thunk allocation site for sparking. There are two reasons \nfor GHC 6.6 performing better with smaller thunks: (i) we incur run-time and space costs from locking \nsparked thunks whereas GHC 6.6 does not, (ii) our simple work\u00adstealing system involves locking the local \nspark pool when sparking a thunk whereas GHC 6.6 s spark pools are each entirely thread local. We believe \nthe peak speed-up in GHC 6.6 is lower because of the cost of creating a new Haskell thread for each spark \n.shed from the pool. The two schemes are clearly complementary and we would envisage combining them based \non thunk size. 5. Results We tested our FDIP implementation using the same benchmarks we used in the \nlimit study in Section 2. These do not ordinarily in\u00adclude separate input sets for testing feedback-directed \ntechniques. We therefore modi.ed the input sets, where practical, to provide different input problems \nfor our timed experiments from our pro\u00ad.ling runs. This is to ensure that sources of parallelism we .nd \nare due to the structure of the program under test rather than simply due to properties of the training \ninput. Figure 11 summarizes these changes. atom Longer simulation run boyer Unchanged bsort-1 Larger \ninput bsort-2 Larger input cacheprof Unchanged calendar More repetitions circsim Longer simulation run \nclausify More repetitions compress Longer input text fft2 Larger input problem .bheaps Larger input problem \nhidden More complex scene to render lcss Larger input problem multiplier Size of multiplication inputs \npara Larger input text primetest Unchanged rewrite Unchanged scs Reduce simulation timesteps simple Unchanged \nsphere Scene size Figure 11. Differences between training and test data.  Figure 12 shows the performance \nmeasurements running on a machine with two 4-core CPUs. We measure the total wall-clock time spent outside \ngarbage collection and normalize this to the time spent outside garbage collection in a single-threaded \nexecution without any work sparking thunks. We omit garbage collection time because the GHC garbage collector \nis single-threaded. We plot the best-of-5 runs to avoid interaction with other services on the machine. \nFor each program we plot the performance with the GHC run-time system con.gured to use 1..4 cores. The \nresults show that our actual implementation can still achieve a parallel speed-up, although at a much \nreduced level than the simulated performance in Figure 7. There are several possible explanations for \nthe difference. First, the simulation ignores con\u00adtention within the GHC run-time system for example \nfor access to the lock that protects the storage manager from which threads replenish their local allocation \nbuffers. Second, the simulation ig\u00adnores the overheads of sparking work and the cache effects of mov\u00ading \ndata from a sparking core to one running work speculatively. Third, the simulation ignores the overheads \nof the shim-lock im\u00adplementation. We have not yet had time to investigate the relative contributions \nof these factors. In practice we would use the simulation results from Figure 7 to select whether or \nnot FDIP is likely to bene.t a given application. If it is not then we would run the application on an \nordinary version of the GHC run-time system. In our example programs a simple comparison of the predicted \n2-core performance against a 10% improvement threshold would identify the programs that could bene.t \nfrom FDIP. However, for this paper, we show results for all of the programs to assess the costs introduced \nby FDIP when parallelism is not found. We compared the number of bytes allocated under FDIP with the \nnumber of bytes allocated by the same program compiled with\u00adout any calls to sparkSpeculation. On 1-core \nruns, calls to sparkSpeculation add less than 0.01% to the amount allocated by each program. On most \nprograms they add less than 1% on 1-4 core runs. The worst case is 5% in bsort-2 and hidden.Thismay be \ndue to duplicate evaluation of thunks that are not sparked. Finally, we examined one of the benchmarks \nto see how the allo\u00adcation sites selected for sparking corresponded to pieces of the orig\u00adinal program. \nWe selected bsort-2 because of our local knowl\u00adedge in the algorithms it uses. The program is a sequential \nimple\u00admentation of bitonic sorting networks (Batcher 1969) applied to 32 streams of integers. It models \na pipelined hardware implementa\u00adtion in which each clock cycle inputs a new set of 32 integers and outputs \na completed set of 32 integers. The sorting network has a hierarchical structure, with the 32\u00adway sorter \nbeing built from two 16-way sorters. This is expressed in the source code using a combinator two: two \n:: ([a] -> [b]) -> [a] -> [b] two r = halve >-> toBoth r r >-> unhalve This combinator takes a function \nr from lists of a to lists of b and produces a function which splits its input list into two halves, \napplies r to each half, and then merges the results to form a list of b.The >-> combinator passes pairs \nof lists between these steps. FDIP identi.es the two places where bsort-2 uses two, sparking one half \nof the work so that it can be computed in parallel with the other half. In this case the decision of \nwhere to spark work corresponds closely to what would be done manually to parallelize the same program \nusing the par and seq combinators. We have not yet had time to investigate the other programs to see \nwhether or not this is true in general. 6. Related work Hammond s introduction to parallel functional \nprogramming intro\u00adduces the main concepts and history (1994), highlighting the vital importance of introducing \nparallelism at the right granularity: too coarse and there will be idle processors, too .ne and the overheads \nwill be unacceptable. Mechanisms to express speculative evaluation have been present in many functional \nprogramming languages. For example, Osborne (1990) explored the use of speculation in Multilisp, a language \nwith explicit side effects and futures as the mechanism for expressing parallel tasks. Speculative tasks \nare explicitly created by the pro\u00adgrammer. Explicit operations are also provided to control groups of \ntasks for instance to cull speculative computation that is no longer needed. Our work aims to provide \nan entirely automatic mechanism for speculation. Roe (1991) examined the use of the par and seq combinators \nto control parallelism. Our approach can be seen as an automated identi.cation of where to use par; the \nplacement in bsort-2 we examined in Section 5 coincides with sensible manual placement. Of course, we \nanticipate that careful manual use of these combina\u00adtors and evaluation strategies (Trinder et al. 1998) \nbuilt over them will yield better performance than automated placement. Mattson (1993) describes the \nuse of speculative evaluation of Haskell programs on the BBN butter.y multiprocessor machine, using the \npar combinator to identify non-speculative parallelism and additional priority annotations to identify \npossible speculative work. Mattson writes that ideally the compiler employs heuristics to annotate the \nprogram graph automatically... heuristics to auto\u00admatically determine safe and effective speculative \nannotations have not yet been developed . Our work provides one heuristic for doing this, although we \ndo not reify the annotations in the original source program. Languages like Id90 and pH (Nikhil and Arvind \n2001) com\u00adbine non-strictness with eager evaluation. This lenient evalua\u00adtion (Traub 1991) provides abundant \n.ne-grained parallelism. For example, given let x=E in E , a lenient language will create separate tasks \nfor E and E , and similarly the body of a function and all of its arguments. Arvind et al (1988) used \nan idealized interpreter to show that many traditional algorithms exhibit ample parallelism in this kind \nof model and that it is effective both for using parallelism across processing elements and for masking \nlarge, unpredictable memory latencies. This form of parallelism is particularly effective on data\u00ad.ow \nhardware such as Monsoon (Traub et al. 1991) which can support .ne-grained concurrency. This is a much \n.ner granularity of parallelism than our limit study identi.es.  Ennals and Peyton Jones introduced \nthe idea of optimistic evalu\u00adation for non-strict languages (Ennals and Peyton Jones 2003; En\u00adnals 2004). \nThe idea is to identify let expressions whose right\u00adhand sides are quick to evaluate and likely to be \nneeded. These can be optimistically evaluated without creating a thunk. The selection of where to use \noptimistic evaluation is based on dynamic feedback of which let expressions generate suitable thunks: \noptimistic eval\u00aduation is disabled for let expressions that generate long-running thunks or ones that \nare frequently un-needed. Our decision to make spark / not-spark choices for each static allocation site \nwas partly based on Ennals and Peyton Jones de\u00adcision to make optimistic / lazy evaluation decisions \nfor each let expression. It is interesting to consider whether dynamic feedback would be capable of making \nspark / not-spark decisions. It is not obvious that this is the case the number of thunks sparked (Fig\u00adure \n7) is a tiny fraction of the total number created: probabilistic or burst-based pro.ling is likely to \nmiss these, whereas pro.ling every thunk will have a high performance cost because many thunks are small \n(Figure 3). GranSim (Loidl 1998) is a .exibile simulator for studying the performance of prorgams written \nin Glasgow Parallel Haskell. We developed a new tracing tool and simple simulator (Section 2) because \nwe wished to study sequential benchmarks which had not yet been parallelised. It would be interesting \nto use GranSim to study the behavior of the parallel programs resulting from FDIP. 7. Conclusions and \nfuture work In this paper we have shown that, for some programs, we can achieve 10-80% parallel speed-ups \nautomatically on stock multi\u00adcore hardware. Furthermore, we can use a simulator to identify which programs \nare likely to have such speed-ups and avoid intro\u00adducing any overheads in programs that will not. This \nclearly does not provide a silver bullet for using the full computational power of these machines effectively, \nbut it suggests that perhaps paral\u00adlelizing a single application thread across two cores is a practical \nproposition. The key new technique in achieving this speed-up is the pre\u00addiction of which thunk-allocation \nsites in the program are likely to produce long-running pieces of work based on pro.ling informa\u00adtion \ncollected from earlier program runs. A clear question for fu\u00adture work is whether we can adapt this technique \nto work within a single program run, rather than needing a pro.le-collection phase. A further interesting \ndirection to explore is the relationship be\u00adtween optimizations and available parallelism. As researchers \nhave previously explored, there are several tensions here: optimizations for non-strict languages often \ntry to avoid or delay thunk allocation, whereas we exploit thunk allocation as a potential source of \nparal\u00adlelism. It would be interesting to explore the impact of reducing the level of optimization on \nthe parallelism seen in our limit study and on the eventual performance that we achieve in practice. \nPerhaps it is practical to gather pro.ling data from an un-optimized program, identify pro.table thunk-allocation \nsites, and then re-compile the program with some annotations to prevent these sites from being removed \nby optimizations. This would require tighter integration between the compiler and feedback-generation \nsystem than we are using at the moment. Finally, although the goal of our work has been automatic par\u00adallelization, \nour techniques could also be applied to guide manual parallelization. As we showed with bsort-2, the \nselected thunk\u00adallocation sites may correspond to meaningful points in the pro\u00adgram source code, and \nso simply identifying these points from an execution trace could be a valuable programming aid. Acknowledgments \nWe would like to thank John DeTreville for feedback and advice while starting this work, Maurice Herlihy, \nJan-Willem Maessen, Simon Marlow and Simon Peyton Jones for discussions, the anony\u00admous reviewers for \ntheir suggestions, and Andrew Birrell for help preparing the .gures. References Arvind, David E. Culler, \nand Gino K. Maa. Assessing the bene.ts of .ne-grained parallelism in data.ow programs. Technical Report \n279, Computation Structures Group, MIT, March 1988. K. E. Batcher. Sorting networks and their applications. \nIn Proceedings of the AFIPS Spring Joint Computing Conference, 1969. R. Blumofe and C. Leiserson. Scheduling \nmultithreaded computations by work stealing. In Proceedings of the 35th Annual Symposium on Foundations \nof Computer Science, Santa Fe, New Mexico., pages 356 368, November 1994. Robert Ennals. Adaptive Evaluation \nof Non-Strict Programs. PhD thesis, Cambridge University Computer Laboratory, 2004. Robert Ennals and \nSimon Peyton Jones. Optimistic evaluation: an adaptive evaluation strategy for non-strict programs. In \nICFP 03: Proceedings of the eighth ACM SIGPLAN international conference on Functional programming, pages \n287 298. ISBN 1-58113-756-7. Kevin Hammond. Parallel functional programming: An introduction. In International \nSymposium on Parallel Symbolic Computation, Hagen-berg/Linz, Austria, September 1994. Tim Harris. Dynamic \nadaptive pre-tenuring. In International Symposium on Memory Management (ISMM 00), volume 36(1) of ACM \nSIGPLAN Notices, pages 127 136, January 2001. Tim Harris, Simon Marlow, and Simon Peyton Jones. Haskell \non a shared\u00admemory multiprocessor. In Haskell 05: Proceedings of the 2005 ACM SIGPLAN workshop on Haskell, \npages 49 61, September 2005. Danny Hendler, Yossi Lev, Mark Moir, and Nir Shavit. A dynamic-sized nonblocking \nwork stealing deque. Technical Report TR-2005-144, Sun Microsystems Laboratories, 2005. Raj Jain. The \nart of computer systems performance analysis. Wiley, 1991. H-W. Loidl. Granularity in Large-Scale Parallel \nFunctional Programming. PhD thesis, Department of Computing Science, University of Glasgow, March 1998. \nJames Stewart Mattson. An effective speculative evaluation technique for parallel supercombinator graph \nreduction. PhD thesis, University of California at San Diego. Rishiyur S Nikhil and Arvind. Implicit \nParallel Programming in pH. Morgan Kaufman, 2001. Randy B. Osborne. Speculative computation in multilisp. \nIn LFP 90: Proceedings of the 1990 ACM conference on LISP and functional pro\u00adgramming, pages 198 208. \nISBN 0-89791-368-X. Simon Peyton Jones, Andrew Gordon, and Sigbjorn Finne. Concurrent haskell. In POPL \n96: Proceedings of the 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages \n295 308, New York, NY, USA, 1996. ISBN 0-89791-769-3. Paul Roe. Parallel Programming Using Functional \nLanguages. PhD thesis, Department of Computing Science, University of Glasgow, 1991. Kenneth R. Traub. \nImplementation of non-strict functional programming languages. MIT Press, Cambridge, MA, USA, 1991. ISBN \n0-262\u00ad70042-5. Kenneth R. Traub, Gregory M. Papadopoulos, Michael J. Beckerle, James E. Hicks, and Jonathan \nYoung. Overview of the Monsoon project. In ICCD 91: Proceedings of the 1991 IEEE International Conference \non Computer Design on VLSI in Computer &#38; Processors, pages 150 155. ISBN 0-8186-2270-9. P. W. Trinder, \nK. Hammond, H.-W. Loidl, and S. L. Peyton Jones. Algo\u00adrithm + strategy = parallelism. J. Funct. Program., \n8(1):23 60, 1998. ISSN 0956-7968.   \n\t\t\t", "proc_id": "1291151", "abstract": "<p>In this paper we present an automated way of using spare CPU resources within a shared memory multi-processor or multi-core machine. Our approach is (<i>i</i>) to profile the execution of a program, (<i>ii</i>) from this to identify pieces of work which are promising sources of parallelism, (<i>iii</i>) recompile the program with this work being performed speculatively via a work-stealing system and then (<i>iv</i>) to detect at run-time any attempt to perform operations that would reveal the presence of speculation.</p> <p>We assess the practicality of the approach through an implementation based on GHC 6.6 along with a limit study based on the execution profiles we gathered. We support the full Concurrent Haskell language compiled with traditional optimizations and including I/O operations and synchronization as well as pure computation. We use 20 of the larger programs from the 'nofib' benchmark suite. The limit study shows that programs vary a lot in the parallelism we can identify: some have none, 16 have a potential 2x speed-up, 4 have 32x. In practice, on a 4-core processor, we get 10-80% speed-ups on 7 programs. This is mainly achieved at the addition of a second core rather than beyond this.</p> <p>This approach is therefore not a replacement for manual parallelization, but rather a way of squeezing extra performance out of the threads of an already-parallel program or out of a program that has not yet been parallelized.</p>", "authors": [{"name": "Tim Harris", "author_profile_id": "81406593835", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "PP39085998", "email_address": "", "orcid_id": ""}, {"name": "Satnam Singh", "author_profile_id": "81100280060", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "PP37044302", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291192", "year": "2007", "article_id": "1291192", "conference": "ICFP", "title": "Feedback directed implicit parallelism", "url": "http://dl.acm.org/citation.cfm?id=1291192"}