{"article_publication_date": "10-01-2007", "fulltext": "\n Lazy Call-By-Value Evaluation * Bernd Bra\u00dfel Sebastian Fischer Michael Hanus Frank Huch Institute of \nComputer Science, CAU Kiel, Germany. {bbr,sebf,mh,fhu}@informatik.uni-kiel.de Abstract Designing debugging \ntools for lazy functional programming lan\u00adguages is a complex task which is often solved by expensive \ntrac\u00ading of lazy computations. We present a new approach in which the information collected as a trace \nis reduced considerably (kilobytes instead of megabytes). The idea is to collect a kind of step infor\u00admation \nfor a call-by-value interpreter, which can then ef.ciently re\u00adconstruct the computation for debugging/viewing \ntools, like declar\u00adative debugging. We show the correctness of the approach, discuss a proof-of-concept \nimplementation with a declarative debugger as back end and present some benchmarks comparing our new \nap\u00adproach with the Haskell debugger Hat. Categories and Subject Descriptors D.1.1 [Programming Tech\u00adniques]: \nApplicative (Functional) Programming; D.2.5 [Software Engineering]: Testing and Debugging General Terms \nLanguages, Theory Keywords Laziness, debugging techniques. 1. Introduction The demand-driven nature of \nlazy evaluation is one of the most ap\u00adpealing features of modern functional languages like Haskell (Pey\u00adton \nJones 2003). Unfortunately, it is also one of the most complex features one should face in order to design \na debugging tool for these languages. In particular, printing the step-by-step trace of a lazy computation \nis generally useless from a programmer s point of view, mainly because arguments of function calls are \noften shown unevaluated and because the order of evaluation is counterintuitive. There are several approaches \nthat improve this situation by hid\u00ading the details of lazy evaluation to the programmer. The main such \napproaches are: Freja (Nilsson and Sparud 1997) and Buddha (Pope and Naish 2003), which are based on \nthe declarative debugging technique from logic programming (Shapiro 1983), Hat (Sparud and Runciman 1997b), \nwhich enables the exploration of a compu\u00adtation backwards starting at the program output or error message, \n* This work has been partially supported by the EU (FEDER) and the Spanish MEC under grant TIN2005-09207-C03-02, \nand by the DFG under grant Ha 2457/1-2. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. ICFP 07 October 1 3, 2007, Freiburg, Germany. Copyright c . 2007 ACM \n978-1-59593-815-2/07/0010. . . $5.00 Germ\u00b4 an Vidal DSIC, Technical University of Valencia, Spain gvidal@dsic.upv.es \nand Hood (Gill 2000), which allows the programmer to observe the data structures at given program points. \nMany of these approaches are based on recording a tree or graph structure representing the whole computation, \nlike the Evaluation Dependence Tree (EDT) for declarative debugging or the redex trail in Hat. For .nding \nbugs, this recorded structure is represented in a user-friendly (usually innermost-style) way to the \nprogrammer in a separate viewing phase. Unfortunately, this structure dramati\u00adcally grows for larger \ncomputations and can contain several mega-, even gigabytes of information. In this paper, we introduce \nan alternative approach to debugging lazy functional programs. Instead of storing a complete redex trail \nor EDT, we memorize only the information necessary to guide a call-by-value interpreter to produce the \nsame results. To avoid un\u00adnecessary reductions, similarly to the lazy semantics, the call-by\u00advalue interpreter \nis controlled by a list of step numbers determining which redexes should not be evaluated. If every redex \nis evaluated even by a lazy strategy, the list of step numbers reduces to a sin\u00adgle number the total \nnumber of reduction steps in the complete computation which demonstrates the compactness of our repre\u00adsentation. \nFurthermore, we are able to prove the correctness of our approach, in contrast to the existing approaches \nin which the com\u00adpression of stored information is only motivated as an implementa\u00adtion issue. We illustrate \nour approach with a small example. EXAMPLE 1.1. Consider the following simple (but erroneous) Haskell \nprogram (where the concrete code for fib, which com\u00adputes the corresponding Fibonacci number, is omitted): \ndata Nat = Zero | S Nat take Zero _ = [] take(Sx) (y:ys)=y: takexys length [] = Zero length (_:xs) = \nlength xs fibsx=fibx :fibs(Sx) main = length (take (S (S Zero)) (fibs Zero)) The lazy evaluation of main \ndoes not evaluate the individual list el\u00adements. This behavior is represented by the step list [2,1,0,14]. \nThis list is interpreted by a call-by-value interpreter to perform (in innermost order) two steps, then \ndiscard the next innermost redex (i.e., replace it by some value representing an unevaluated thunk), \nperform one step, discard the next redex and the following one, and .nally perform 14 reduction steps. \nThe three discarded redexes cor\u00adrespond to the partial evaluation of the expression (fibs Zero) to (_:_:_) \n(where _ denotes a discarded redex). This example demonstrates the compactness of our representation \nthat usually only requires a fairly limited amount of memory (kilo\u00adbytes instead of megabytes). The step \nlist is used in the subsequent tracing/debugging session to control the call-by-value interpreter Lam \nG : .y.e .. G : .y.e Con G : C xn .. G : C xn App G : x1 .E1 . : .y.e G : x1 @r x2 . : e l[x2/y] .E2 \n.E1\u00b7r..[|el|]\u00b7E2 T : z T : z l G[y .. e1[y/x]] : e2[y/x] .E .: z Let G: letr x = e1 in e2 . l .: z r..[|e2|]\u00b7E \n G: x .E1 .: Ci xki .: eil[xki /yki ] .E2 T: z Case G: caser x of {Cn ykn .. en}.E1\u00b7r..[|eil|]\u00b7E2 T: \nz G: e .E .: z Var G[x .. e]: x .E .[x .. z]: z where e l = l(e) where y is a fresh variable and e2 l \n= l(e2) where eil = l(ei) Figure 2. Instrumented lazy semantics x . Var z . Value ::= .x.e | Cxn e . \nExp ::= .x.e | Cxn | x1 @ x2 | x | let x = e1 in e2 | case x of {Cn xkn .. en} Figure 1. Syntax of normalized \nexpressions that shows the original evaluation in a more comprehensible order. In a nutshell, we trade \ntime for space in our approach. This paper is organized as follows. The next section introduces an instrumented \nversion of Launchbury s natural semantics for lazy evaluation so that it produces a simple trace of the \ncomputation. Then, Section 3 presents a lazy call-by-value semantics that is driven by (a compressed \nform) of the trace produced by the instru\u00admented lazy semantics. A prototype implementation of a debugging \ntool for Haskell that follows the ideas presented in this paper is de\u00adscribed in Section 4. Section 5 \ndiscusses some related work before we conclude in Section 6. The proofs of some technical results are \nomitted but can be found in a technical report (Bra\u00dfel et al. 2007). 2. Instrumented Lazy Semantics In \nthis section, we consider the natural semantics of Launchbury (1993) for lazy evaluation. In this semantics, \nlaziness is modeled in two steps. First, the input expression is normalized such that all arguments of \napplications and case expressions are variables. This can easily be achieved by introducing additional \nlet bindings. Then, a semantics for normalized expressions is given, where one can easily de.ne the semantics \nof sharing, i.e., laziness. We do not go into details of normalization and directly assume normalized \nlambda expressions extended with (recursive) lets and constructors, as shown in Figure 1, for the syntax \nof our expres\u00adsions. Here and in the following the notation on is used to denote a sequence of objects \nof the form o1,...,on. In addition to the normalization of Launchbury (1993), we assume that the .rst \nargu\u00adment of an application is a variable and we restrict to lets which de.ne only one variable. With \nthese restrictions, it will be easier to relate the lazy and the call-by-value evaluation order. The \n.rst re\u00adquirement can easily be achieved by introducing additional lets for the .rst argument of @. The \nsecond can be obtained by construct\u00ading a tuple containing all mutually recursive de.nitions within a \nlet and selecting the corresponding arguments, cf. (Bra\u00dfel et al. 2007). In the examples, we use lets \nwith multiple bindings. Since these examples do not contain mutually recursive de.nitions, these lets \ncan simply be interpreted as syntactic sugar for nested lets. The lazy semantics is shown in Figure 2 \n(ignore the indices labeling the arrows and the superscripts l, r for the moment). Our rules obey the \nfollowing naming conventions: G, ., T, O . Heap = Var . Exp z . Value A heap is a partial mapping from \nvariables to expressions (the empty heap is denoted by []). The value associated to variable x in heap \nG is denoted by G[x]. G[x .. e] denotes a heap with G[x]= e, i.e., we use this notation either as a condition \non a heap G or as a modi.cation of G. In the examples, we will also use the notation G \\ x for a heap \nwith G \\ x[y] = G[y] if y . = x and G[x] unde.ned. We use judgments of the form G: e . .: z which are \ninterpreted as the expression e in the context of the heap G evaluates to the value z with the (possibly \nmodi.ed) heap . , according to the rules of Figure 2. We brie.y explain the more complex rules of our \nsemantics: (App) This rule allows us to evaluate function applications of the form x1@x2.1 For this purpose, \nx1 is .rst evaluated to a lambda abstraction so that a \u00df-reduction can be performed. (Var) This rule \nis used to look up the bindings of variables in the heap. When the variable x is bound to an expression \ne, this e is evaluated to a value z so that the binding x .. z replaces the original binding x .. e in \nthe heap. This is essential to achieve the effect of sharing, since subsequent attempts to evaluate x \nwill use the value z instead of repeating the evaluation of e. Note that e is evaluated in a heap which \ndoes not include the binding x .. e. This is done to detect black holes, i.e., non\u00adterminating computations \nbecause the evaluation of x requires again the evaluation of the same x. (Let) In order to reduce a let \nexpression, we add the bindings to the heap and proceed with the evaluation of the let expression. Note \nthat we rename the variables introduced by the let construct with fresh names in order to avoid variable \nname clashes. (Case) This rule is used to evaluate a case expression. For this pur\u00adpose, the case argument \nshould .rst be reduced to a constructor call. Then, the evaluation continues by selecting the matching \nbranch of the case expression and by applying the correspond\u00ading matching substitution. The proof of \na judgment is a proof tree using the rules of Figure 2. 1 The reason to write the application symbol \n@ explicitly will become apparent later when we present the instrumentation of the semantics. Labeling \nof expressions (function l): l(.x.e)= .x.e l(Cxn)= Cxn l(x)= x l(x1 @ x2)= x1 @r x2 l(let x = e1 in e2)= \nletr x = l(e1) in e2 l(case x of {Cn ykn .. en})= case r x of {Cn ykn .. en} where r is always a fresh \nreference Extraction of references (function [||]): [|.x.e|]= . [|Cxn|]= . [|x|]= . [|x1 @r x2|]= r [|letr \nx = e1 in e2|]=[|e1|] \u00b7 r [|case r x of {Cn ykn .. en}|]= r Figure 3. Labeling and extraction of references \n 2.1 Collecting Events The basic idea of collecting events representing a call-by-value reduction is \nto keep track of a trace of the evaluated redexes in the correct left-to-right innermost order by means \nof an instrumented version of the lazy semantics. Here, the evaluation of an expression e starts with \nan empty heap and the labeled expression l(e) where the labeling function l is shown in Figure 3. The \nlabeling adds a reference from some domain Ref with .. Ref to every reducible symbol of a given expression \n i.e., applications, lets and cases. References need to be identi.able and the distinguished reference \n. is never considered fresh. In the examples we use the natural numbers augmented with .. However, we \ndo not need any order or arithmetic operations on references. As shown in Figure 3, we only label the \ntopmost evaluable symbol in every expression. Thus, nothing is labeled in a value or a variable. Also \ncase branches are not labeled until they are demanded by the computation. During evaluation, the instrumented \nsemantics will dynamically generate new references for each newly introduced expression. These references \nare then used to extract the history of the evalua\u00adtion in form of a sequence of events. These events \nwill later be con\u00adsumed to compute a step list, cf. Example 1.1 and De.nition 3.9. We collect the references \nby means of an extraction function [||] de.ned in Figure 3. Since our aim is a call-by-value evaluation, \nthis function takes a labeled expression and extracts a sequence of references following a left-to-right \ninnermost order. As we will see, collecting references in this order will enable us to replay the lazy \nevaluation employing a sort of call-by-value strategy. References are collected in the form of events. \nEach event maps a reference r0 to a (possibly empty) sequence of references. Infor\u00admally, we collect \nan event r0 .. r1 \u00b7 ... \u00b7 rn whenever the ex\u00adpression whose topmost reference is r0 reduces to the expression \nlabeled with the references r1 \u00b7 ... \u00b7 rn. In the call-by-value eval\u00aduation, the reduction corresponding \nto r0 will be performed before the reduction corresponding to r1, which will be performed before the \nreduction corresponding to r2, and so on. Because of the na\u00adture of events, there cannot be cycles nor \nmore than one reference mapping to the same reference. DEFINITION 2.1 (Sequence, * , ||, \u00b7, ., { } ). \nLet M be a set. Then M * is the set of .nite sequences over M with the concatenation operator \u00b7 . I.e., \nM * := {a1 \u00b7 ... \u00b7 an |{a1,...,an}. M}. Furthermore, we denote the empty sequence by . and use u, v, \nw to denote sequences. For a given sequence v := a1 \u00b7 ... \u00b7 an we denote by { v} the set {a1,...,an} \nand by |v| the length n of v. DEFINITION 2.2 (Set of Events, Sequence of Events). 2Ref \\{.}..Ref * Let \nM : where .. Ref be a .nite partial mapping of references different from . to sequences of references. \nWe write r0 .. r1 \u00b7 ... \u00b7 rn . M to denote that in M , r0 maps to r1 \u00b7 ... \u00b7 rn. Moreover, we de.ne M \nas the smallest set satisfying [n M(r)=(M(ri)) .{rn} i=1 for all r .. r1 \u00b7 ... \u00b7 rn . M . We say that \nM is a set of events iff there is no reference r with r . M (r) and  r1 .. u \u00b7 r \u00b7 v, r2 .. u . \u00b7 r \n\u00b7 v . . M implies  r1 \u00b7 u \u00b7 v = r2 \u00b7 u \u00b7 v or (r = .. u \u00b7 v \u00b7 u \u00b7 v = .). Furthermore, we denote by \nref (M) the set of all references in M, formally ref (M) := .r..r1...rn.M {r, r1,...,rn} . A sequence \nof reference mappings E is a sequence of events iff { E} is a set of events. To understand sets of events \nit is often useful to view them as forests, i.e., sets of trees because their de.nition requires that \nthere is no undirected cycle in the structure of events. M (r) denotes all references reachable from \nr by following the structure of events, similar to a transitive closure. Note that not every reference \noccurring in a set of events E is mapped to a sequence. Informally, events are generated only for those \nreferences which label expressions that were reduced during the lazy evaluation. The special reference \n. will later be used to generate an event r .. . for a reference which was not reduced. In our framework \nthis is only possible in the (non-deterministic) call\u00adby-value semantics that we will introduce in the \nnext section. In the context of lazy evaluation such an event can only be produced by a so-called .nalizer \n, a side-effect triggered by the garbage collector. We will not consider garbage collection in this paper. \nThe instrumented semantics for computing sequences of events is presented in Figure 2. Whenever the evaluation \nof a new expres\u00adsion is started in the rules performing relevant steps (rules App, Let, and Case), we \nintroduce a new labeling for this new expres\u00adsion. For instance, in rule App, we label the body of the \nfunction (e) with fresh references (e l). This reduction has to be performed in the call-by-value evaluation \nas well, which will possibly introduce further innermost redexes represented by the references in [|e \nl|]. Following Launchbury (1993), we write judgments sequentially as follows: if G: e .E .: z, we write \nG: e a sub-proof another sub-proof .: z where the elements of the proof tree are depicted to the right \nof the corresponding step. As an additional abbreviation, we omit copying the result .: z, if it stays \nunchanged, like in the rules App, Let, and Case. Furthermore, instead of writing the whole sequence of \nevents in each proof step we only annotate the new events added to the sequence to the right hand-side \nof the proof step. The following example illustrates the instrumented semantics using this notation. \nEXAMPLE 2.3. Consider the data type for natural numbers intro\u00adduced in Example 1.1 and the expression \nmain = let { const = let {w = .a..b..c.b} in w @ w; z = Zero; id = .x.x; s = id @ z; f = const @ z } \nin f @ s []: let0 {const = let1 {w = .a..b..c.b} in w @ w; z = Zero; id = .x.x; s = id @ z; f = const \n@ z} in f @ s 0 .. 2 G1 = [const .. let1 {w = .a..b..c.b} in w @ w]: let2{z = Zero; id = .x.x; s = id \n@ z; f = const @ z} in f @ s 2 2 2 .. 3 = const @ z} in f @ s 3 .. 5 \u00b7 4 4 .. 7 \u00b7 6 G2 = G1[z .. Zero]: \nlet3{id = .x.x; s = id @ z; f G3 = G2[id .. .x.x]: let4{s = id @5 z; f G4 = G3[s .. id @5 2 2 2 6666666666666666666666666666666664 \n66666666666666666666666666666664 666666666666666666666666666664 6666666666666666666666666664 66666666666666666666666664 \nz]: let6{f G5 = G4[f .. const @7 2 = const @ z} in f @ s = const @7 z} in f @ s 6 .. 8 z]: f @8 s G5 \n: G4 : G4 : 2 2 2 66666666666664 f const @7 666666666666666664 z const G6 = G4 \\ const : let1 {w = .a..b..c.b} \nin w @ w 1 .. 9 G7 = G6[w .. .a..b..c.b]: w @9 w 2 2 6666666664 666664 6664 4 G7 : w G6 : .a..b..c.b \nG7 : .a..b..c.b 9 .. .  G7 : .b..c.b G8 = G7[const .. .b..c.b]: .b..c.b G8 : .c.z 7 .. . G9 = G8[f \n.. .c.z]: .c.z 2 G9 : z G9 \\ z : Zero 8 .. . 4 G9 : Zero G9 : Zero Figure 4. Example of instrumented \nlazy evaluation The complete lazy derivation of evaluating l(main) is shown in values, heap, and events. \nGraphically: Figure 4. The example derivation includes the generated sequence G of events on the right \nhand side of the .gure. Considering that the references in the initial expression are 0 and 1 (remember \nthat no further references are created since the outer let is an abbreviation e1 . ....... E1 e2 e2 \ne1 E2 Our second result states a key property for our development: LEMMA 2.4. Let G: e .E .: z be a lazy \nderivation. Then E is a sequence of events. COROLLARY 2.6. Proof. Cf. (Bra\u00dfel et al. 2007). . If G[y \n.. ey]: e .Ee .[y .. zy]: z, ey .= zy for let const = ... in let z = ...), the events in this sequence \n. T can be interpreted as follows: every evaluable expression has been reduced but the one labeled with \nreference 5. It is easy to see that the sequences produced by the semantics are E O E 2 1 indeed sequences \nof events in the sense of De.nition 2.2. and G: ey .Eey then zy T: z Now, we prove a basic property of \nthe instrumented semantics: the computed events are preserved up to renaming of references y and T[y \n.. zy = z ]: e .E.[y .. zy]: z y e e and Ee =bEey \u00b7 E if closures are evaluated immediately (in rule \nLet), as long as their evaluation would have been eventually required in the lazy computation. This property \nis stated in two steps. In the following, we say that two sets of events are equal up to renaming of \nreferences, denoted M1 =bM2, if they have exactly the same shape and only differ in the names of references \nof their nodes. Formally, M1 =bM2 if there exists a bijective mapping s : ref (M1) .. ref (M2) such that \nr .. r1 ...rn . M1 iff s(r) .. s(r1) ...s(rn) . M2. We also write E1 =bE2 for two sequences of events \niff { E1} =b{ E2} . Our .rst result is the following.2 THEOREM 2.5. If G: e1 .E1 .: z1 and G: e2 .E2 \nT: z2 Informally speaking it states that, given a let expression of the form let y = ey in e, we can \nevaluate .rst the closure ey and then e in the resulting heap with the updated binding for y, and we \nstill produce the same value, heap and events as when the standard lazy evaluation order is followed, \nprovided that the evaluation of ey was demanded during the evaluation of e. The following diagram depicts \nthis property: G[y .. ey] ey Eey e T[y .. zy] then T: e1 .E O: z1 and .: e2 .EO: z2 ey 1 2 and E2 =bE1 \n\u00b7 E\u00b7 E e 1 2 Ee . . E Basically this result states that, given a heap G and two expressions .[y .. \nzy] e e1 and e2, they can be evaluated in any order, producing the same Proof. Applying rule Var, we \nhave: y ]: z y Discard G: e .(r1...)\u00b7...\u00b7(rn...) G: _ where e .. Value . Var and r1 \u00b7 ... \u00b7 rn =[|e|] \nG: e1[y/x] .E1 .: z .[y .. z]: e2l [y/x] .E2 T: z . Let where y fresh and e l = l(e2) G: letr x = e1 \nin e2 .E1\u00b7(r..[|el 2|])\u00b7E2 T: z. 2 Figure 5. Non-deterministic Call-by-Value Rules Since e and y are \nreducible with the same heap G[y .. ey], we can apply Theorem 2.5 which yields the existence of two dual \nderivations. One of them is very simple: .: zy .. .: zy .[y .. zy]: y .. .[y .. zy]: zy . This already \nallows us to deduce that zy = zy . and simpli.es the conclusion of the other derivation as intended: \nT[y .. zy]: e .E. .[y .. zy]: z e Finally, we have Ee \u00b7 . =bEey \u00b7 Ee. . . Corollary 2.6 implies that, \nfor any lazy computation, we can construct an equivalent computation that follows the call-by-value evaluation \norder with the exception that expressions are only eval\u00aduated as much as needed in the original lazy \ncomputation (i.e., it is basically a reordering of the lazy computation). In the next section, we will \nintroduce a call-by-value semantics that is driven by the events of an associated lazy computation. 3. \nLazy Call-By-Value Evaluation In this section, we introduce a lazy call-by-value semantics. It is call-by-value \nbecause the arguments of a function are evaluated before the function is called. It is still lazy because \nevery argument is only evaluated as much as needed in the corresponding lazy evaluation. Our .rst step \ntowards this goal is to present a semantics which discards unevaluated expressions non-deterministically. \n3.1 Non-deterministic Call-By-Value Evaluation Assume that we replace rule Let from Figure 2 with the \nrules of Figure 5. A derivation obtained by this new set of rules will be called a non-deterministic \ncall-by-value derivation in the follow\u00ading. This is because the new version of rule Let implements a \ncall\u00adby-value semantics where closures are evaluated as soon as they are introduced. On the other hand, \nrule Discard allows us to non\u00addeterministically discard the evaluation of a closure when its value is \nnot needed in the considered computation. In this case, we intro\u00adduce a fresh value denoted by _, i.e., \n_ is a constructor which does not appear in the initial expression. The rule Discard is only applicable \nto reducible expressions whose topmost symbol is a let,a case or an application @, i.e., it is only applicable \nwhere the call-by-value reduction would perform super.uous steps from the perspective of the lazy computation. \nEXAMPLE 3.1. Consider again the program of Example 2.3. The unique non-deterministic call-by-value computation \nthat produces an equivalent sequence of events is shown in Figure 6. In this case, the associated sequence \nis: (1 .. 2) \u00b7 (2 .. .) \u00b7 (0 .. 3) \u00b7 (3 .. 4) \u00b7 (4 .. 6 \u00b7 5) \u00b7 (6 .. .) \u00b7(5 .. 8 \u00b7 7) \u00b7 (8 .. .) \u00b7 (7 \n.. 9) \u00b7 (9 .. .) Apart from event (6 .. .), it is equivalent to the sequence of Example 2.3. The following \nresult states the equivalence between the original lazy semantics and the non-deterministic call-by-value \nversion. In the following, given a heap G, we denote by G_ any heap that can be obtained from G by replacing \nunevaluated closures x .. e by x .. _. Likewise we denote by E. a sequence of events that can be obtained \nfrom E by replacing all events r .. . by .. THEOREM 3.2. If G: e .E .: z is a lazy derivation then there \nis one and only one non-deterministic call-by-value derivation G: e .E. ._ : z such that E =bE.. . Proof. \nBy repeatedly applying Corollary 2.6 to the initial deriva\u00adtion G: e .E .: z, we know that there exists \na derivation G: e .E2 .: z such that E =bE2 and for every let expression let x = e in e ., the closure \ne was evaluated before e . if needed. From this derivation we can construct a non-deterministic call-by\u00advalue \nderivation G: e .E. T: z. We only need to apply rule Dis\u00adcard for every closure which is not evaluated \nin .. Because neither rule Var nor rule Discard produce any events adding to E.. , we have E2 = E... \nAlso, by de.nition of Discard, . and T only differ in the value _ which yields ._ =T. Now, we should \nprove that there is no other non-deterministic = E.. call-by-value derivation G: e .E.. ._ : z such that \nE b. The only source of non-determinism introduced by the rules of Figure 5 is the overlapping of rule \nDiscard with the rules Let, App and Case. All three rules produce an event adding to E.. whereas rule \nDiscard does not. As the labeling function l always introduces fresh labels, every reference occurring \nin . only occurs once. As the rule Discard eliminates a labeled expression from the heap, the remaining \nderivation cannot generate an event for the discarded references. This means that applying any of the \nthree rules instead of Discard necessarily produces a different sequence of events. . A direct consequence \nof this theorem is that we can use the events produced by a lazy evaluation and construct, using this \nin\u00adformation, the unique corresponding call-by-value derivation. The order in which the events were produced \ndoes not matter for this theorem. However, this order will become important for the con\u00adsiderations in \nthe next sub sections. Concluding the section, we make a simple but useful observa\u00adtion about the heaps \noccurring in non-deterministic call-by-value derivations, DEFINITION 3.3 (Call-by-Value Heap). A call-by-value \nheap is a partial mapping from variables to values rather than expressions. The notation for heaps carries \nover to call-by-value heaps. PROPOSITION 3.4. Let [] : e .E .: z be a non-deterministic call-by-value \nderivation. Then each heap occurring in that deriva\u00adtion is a call-by-value heap. Proof. The only rules \nmanipulating the heap are the rule Let of Figure 5 and rule Var of Figure 2. Both rules introduce only \nbindings to values. .  3.2 From Events to Step List In the introduction an example of using a step list \nwas given. This section describes how to obtain a step list from a sequence of events and how to use \nthis step list to drive a lazy call-by-value semantics. []: let0 {const = let1 {w = .a..b..c.b} in w \n@ w; z []: let1 {w = .a..b..c.b} in w @ w []: .a..b..c.b 1 .. 2  = Zero; id = .x.x; s = id @ z; f = \nconst @ z} in f @ s w events by a root, according to De.nition 3.5. This is done by bringing together \nthe references labeling the initial expression. LEMMA 3.7. Let G: e .E .: z and r .. ref ({ E} ) a fresh \nreference. Then for M. := { (r .. [|e|]) \u00b7 E} holds ref (M.)= {r}. M.(r). Proof. Since all references \nr1 \u00b7 ... \u00b7 rn created dynamically during the derivation G: e .E .: z are immediately mapped to other \nreferences by an event r .. r1 \u00b7 ... \u00b7 rn upon creation, S we have ref ({ E} )= q.[|e|] { E } (q) .{q}. \nThus, adding the event r .. [|e|] yields the desired property. . Lemma 3.7 essentially means that we \ncan use the ordering <{ E} on the events generated by our instrumented semantics. This en\u00adables us to \nformulate our theorem that the non-deterministic call\u00adby-value derivation generates the events in exactly \nthat order. THEOREM 3.8. Let G: e .E .: z be a non-deterministic call\u00adby-value derivation, . a reference \nwith . .. ref ({ E} ) and M. = { (. .. [|e|]) \u00b7 E} . Then r<M. q holds for all u, r, u . , v, q, v . \n,w with E = u \u00b7 (r .. u .) \u00b7 v \u00b7 (q .. v .) \u00b7 w. Proof. By induction on the structure of the derivation: \nThe base cases, rules Lam and Val, are immediate as [|e|]= E = .. The base case for rule Discard holds, \nsince the generated sequence (r1 .. .) \u00b7 ... \u00b7 (rn .. .) is constructed in correspondence to [|e|]= r1 \n\u00b7 ... \u00b7 rn. Inductive cases: Var: The claim directly stems from the induction hypothesis, since the \nsequence of events is unchanged in this rule. App: By Proposition 3.4, the event sequence of the left \nantecedence for applications of this rule is always .. By the induction hypothe\u00adsis, the claim holds \nfor the evaluation of the body .: e l[x2/y] .E2 T: z. Since this expression is freshly labeled, adding \nthe event The main idea is that, by considering only a given set of events, one can see in which sequence \nreferences would be mapped by the lazy call-by-value semantics and where in this order references are \ndiscarded. A sequence of reduced references means that ordinary rules, i.e., not Discard, were applied. \nSimply counting the reduced references between the discarded ones tells us how many ordinary rules can \nbe applied in the call-by-value derivation. According to this main idea, we .rst de.ne an ordering on \nreferences for a given set of events. The additional condition on the set of events E is equivalent to \nrequiring that the mappings in E form a tree (rather than a forest). The de.ned ordering is then a standard \npre.x ordering. DEFINITION 3.5 (Root, Rooted, Reference Ordering <M ). Let M be a set of events containing \na reference r such that ref (M)= {r}. M(r). Then we call M rooted and r the root of M and de.ne the reference \nordering <M as: p<M q :. (q . M(p) . (.s, u, sp, v, sq,w : s .. u \u00b7 sp \u00b7 v \u00b7 sq \u00b7 w . M . p . M (sp) \n.{sp}. q . M(sq) .{sq})) LEMMA 3.6. For a rooted set of events M, ordering <M is linear. Proof. <M is \nirre.exive and antisymmetric because M contains no cycles. <M is transitive as M is a transitive closure. \n<M is total because there exists a reference r . M such that ref (M)= {r}. M (r), yielding that any two \nreferences p, q must be comparable by choosing r for the existential proposition. . We will show that \nthis pre.x order on the references in (special) sets of events is exactly the order in which events are \ngenerated in the non-deterministic call-by-value derivation, i.e., if the generated sequence is of the \nform u \u00b7 (r .. ...) \u00b7 v \u00b7 (q .. ...) \u00b7 w then r<{ E} q. Before showing this, we need to extend the generated \n G1 = [w .. .a..b..c.b]: w @2 G1 : w []: .a..b..c.b G1 : .a..b..c.b 2 .. . G1 : .b..c.b 0 .. 3  G3 = \nG2[z .. Zero]: let4 {id = .x.x; s = id @ z; f = const @ z} in f @ s G3 : .x.x 4 .. 6 \u00b7 5 \u00bb 2 G6 = G5[f \n.. .c.z]: f @9 G6 : f G6 \\ f : .c.z G6 : .c.z 9 .. . 2 ]: let7 {f = const @8 G5 : const @8 z 2 G5 = G4[s \n.. G5 : const 2 6664 4 42 G6 : z G6 \\ z : Zero G6 : Zero 4 2 66666664 2 66666666666666666664 4 66666666666666666666666664 \n 6664 66666664 G2 = G1[const .. .b..c.b]: let3 {z = Zero; id = .x.x; s = id @ z; f = const @ z} in f \n@ s G2 : Zero 3 .. 4 G4 = G3[id .. .x.x]: let5 {s = id @6 z; f = const @ z} in f @ s G4 : id @6 z 6 .. \n. G4 :5 .. 8 \u00b7 7 z} in f @ s G6 : Zero G5 \\ const : .b..c.b G5 : .b..c.b 8 .. . G5 : .c.z 7 .. 9 s \nFigure 6. Example of non-deterministic call-by-value computation Figure 7. Lazy call-by-value semantics \nDiscard 0 : ns, G : e . G : _, ns where e .. Value . Var and e .= let x = e1 in e2 Lam ns, G : .y.e . \nG : .y.e, ns Con App ns, G : C xn . G : C xn, ns n : ns, G[x1 .. .y.e] : e[x2/y] . T : z, ms n + 1 : \nns, G[x1 .. .y.e] : x1 @ x2 . T : z, ms Var ns, G : x . G : G[x], ns Let1 ns, G : e1[y/x] . . : z, m \n+ 1 : ms m : ms, .[y .. z] : e2[y/x] . T : z ., ks ns, G : let x = e1 in e2 . T : z., ks where y fresh \nLet2 ns, G : e1[y/x] . . : z, 0 : ms ns, G : let x = e1 in e2 . .[y .. z] : _, ms where y fresh Case \nn : ns, G[x .. Ci xki ] : ei[xki /yki ] . . : z, ms n + 1 : ns, G[x .. Ci xki ] : case x of {Cn ykn .. \nen} . . : z, ms r .. [|e l[x2/y]|] means that r<M. r . for all r . . ref ({ E2} ). Since r is reduced \nin this application. i.e., before the body e l[x1/y], this is the desired property. Case: As Proposition \n3.4 also holds for case expressions, this case is analogous to the previous case App. Let: An application \nof this rule is of the following form: G[y .. e1[y/x]]:y .E1 .: z .[y .. z]: e2l [y/x] .E2 T:z . G: \nletr x = e1 in e2 .E1\u00b7(r..[|el 2|])\u00b7E2 T: z. where y fresh and e2 l = l(e2). Analogous to the previous \ncases, adding the event r .. [|e2l |] yields the desired property with respect to the evaluation of the \nbody e2l . Moreover, by de.nition of func\u00adtion [||] the event . .. [|letr x = e1 in e2|] is equal to \n. .. [|e1|]\u00b7r which means that r . <M. r for all r . . ref ({ E1} ). As the ex\u00adpression e1 is indeed \nevaluated before reducing letr x = e1 in e2, this concludes the proof. . Theorem 3.8 directly justi.es \nhow to de.ne the step list. Following the linear sequence provided by <M. , we count the length of reference \nsequences which do not contain a discarded reference, i.e., one that is mapped to .. DEFINITION 3.9 (Step \nList). Let E be a sequence of events. Then the step list st(E) is de.ned as st(v \u00b7 (r .. .) \u00b7 w)= |v| \n: st(w) if v = v . st(v)= |v| : [] if v = v . EXAMPLE 3.10. Consider the events from Example 3.1. With \nE1 = (1 .. 2) \u00b7 (2 .. .) \u00b7 (0 .. 3) \u00b7 (3 .. 4) \u00b7 (4 .. 6 \u00b7 5) E2 = (5 .. 8 \u00b7 7) \u00b7 (8 .. .) \u00b7 (7 .. 9) \n\u00b7 (9 .. .) we produce the following step list: st(E)= st(E1 \u00b7 (6 .. .) \u00b7 E2)=5: st(E2) = [5, 4] The step \nlist can be interpreted as follows: apply the rules of the call-by-value semantics so that the .rst .ve \npre-redexes should be evaluated, the next one should be discarded, and the remaining ones (four) should \nbe evaluated.  3.3 Step-driven Call-by-Value Evaluation A simple observation will be useful in the following. \nPROPOSITION 3.11. Let r be a reference, v .. a sequence of = references and E a sequence of events. Then \nthere exist a natural number n and a list of numbers ns such that st(E)= n : ns and st((r .. v) \u00b7 E)= \nn +1: ns. Proof. Immediate consequence of De.nition 3.9. . We now introduce our lazy call-by-value semantics \nthat is driven by the computed step list. The rules of the semantics are shown in Figure 7. The judgments \nare extended with step lists denoted as ns. We brie.y explain the most relevant rules: Rule Discard can \nonly be applied when the step list begins with 0. This means that a number in the list has been consumed \nand, thus, no innermost reduction should be performed. In the rules App, Var and Case observe that we \nassume that the variables are already bound in the current heap to a value. This is justi.ed by Proposition \n3.4.  Rule Let is split into rules Let1 and Let2. This is motivated by the fact that an expression may \ncontain nested let bindings labeled with different references. Then, if the outermost let is to be discarded, \none should .rst also discard the inner lets, and this is precisely the reason to introduce the new rule \nLet2. The choice between the two rules is determined by the outgoing step list of the .rst premise. \n There is one main difference between the non-deterministic call\u00adby-value semantics and the lazy call-by-value \nsemantics. The non\u00addeterministic discard rule can replace a complex expression, i.e., one with more than \none label, by _, whereas the lazy call-by-value semantics needs to discard every labeled sub-expression \nseparately. In the following lemma we show that we can construct a lazy call\u00adby-value derivation consisting \nof applications of Let2 and Discard to discard complex expressions. LEMMA 3.12 (Discarding complex expressions). \nLet n> 0 be a natural number, e an expression with |[|l(e)|]| = n, G a call-by\u00advalue heap and ns an arbitrary \nsequence of natural numbers. Then there exists a lazy call-by-value derivation 0: ... :0 : ns, G: e . \n.: _, ns such that G . .. | {z } n Proof. Cf. (Bra\u00dfel et al. 2007) . Now we can prove the correspondence \nbetween the non-determinis\u00adtic and the lazy call-by-value semantics. To compare heaps between the different \nderivations we denote by Gl the heap G without labels. THEOREM 3.13. Let e be an expression, e l := l(e), \nG a call-by\u00advalue heap and G: e l .E .: z a non-deterministic call-by-value derivation. Then for all \nsequences of events E. there exists a step\u00addriven derivation st(E \u00b7 E.), Gl : e . .. : z, st(E.) with \n.l . .. . Proof. We inductively construct the step-driven derivation from the non-deterministic derivation. \nBase cases: Lam: The derivation is G: .x.e. .. G: .x.e. . Since for all E. , st(. \u00b7 E.)= st(E.), we can \nconstruct the derivation st(E.), Gl : .x.e. . Gl : .x.e., st(E.). Con: This case is analogous to the \nprevious case. Discard: An application of the rule Discard looks as follows: lG: e .(r1...)\u00b7...\u00b7(rn...) \nG: _ where r1 \u00b7 ... \u00b7 rn =[|e l|]. Now we have st((r1 .. .) \u00b7 ... \u00b7 (rn .. .) \u00b7 E.)=0: ... :0 : st(E.) \n| {z } n Therefore, an application of Lemma 3.12 yields the existence of .l ..l . 0: ... :0 : st(E), \nG: e . .: _, st(E) where G. .. | {z } n Inductive cases: Var: Both variants of this rule obviously neither \nchange the set of events nor the step lists. Due to Proposition 3.4 the heap also stays unchanged. Therefore, \nwe can omit evaluating the binding of x in G and the claim directly stems from the inductive hypothesis. \nApp: By Proposition 3.4 the derivation has the form G: .x.e. .. G: .x.e. G. G. : x1 .. G. : .x.e. : \ne l .E .: z .: z G. : x1 @ x2 .(r..[|el|])\u00b7E where G. = G[x1 .. .x.e.] and e l = l(e .[x2/x]). By induction \nhypothesis we can construct a derivation for all E.: ..l ... st(E \u00b7 E), G: e [x2/x] . .: z, st(E) where \n.l . ... By Proposition 3.11, st((r .. [|e l|]) \u00b7 E \u00b7 E.) is of the form n +1: ns. Hence we can construct \nthe derivation n : ns, G.l : e .[x2/x] . .. : z, st(E.) n +1: ns, G.l : x1 @ x2 . .. : z, st(E.) Case: \nAs Proposition 3.4 also holds for case expressions, this case is analogous to the previous case App. \nLet: The non-deterministic derivation has the form ll . G: e1[y/x] .E1 .: z .[y .. z]: e2[y/x] .E2 T: \nz G: letr x = e1 l in e2 .E1\u00b7(r..[|el 2|])\u00b7E2 T: z. where y is a fresh variable and e2 l = l(e2). ll \nl We have [|e1[y/x]|] \u00b7 r =[|e1|] \u00b7 r =[|letr x = e1 in e2|]. Further\u00admore, by induction hypothesis we \ncan construct the derivations for all E.,E.. \u00b7 E.. st(E1 ), Gl : e1[y/x] . .. : z, st(E..) and st(E2 \n\u00b7 E.), ..[y .. z]: e2[y/x] . T. : z, st(E.) where .l . .. and Tl . T.. We choose E.. =(r .. [|e l 2|]) \n\u00b7 E2 \u00b7 E. and conclude that by Proposition 3.11 st(E..) is of the form m +1: ms. Therefore, we can construct \nthe derivation \u00b7 E.. st(E1 ), Gl : e1[y/x] . .. : z, m +1: ms m : ms, ..[y .. z]: e2[y/x] . T. : z, st(E.) \nst(E1 \u00b7 E..), G: let x = e1 in e2 . T. : z, st(E.) This completes the proof. . EXAMPLE 3.14. Consider \nagain the expression of Example 2.3 and the step list computed in Example 3.10: [5, 4]. The associated \nlazy call-by-value computation according to the rules of Figure 7 is shown in Figure 8.  3.4 Consuming \nEvents in Arbitrary Top-Down Order The algorithm to compute the step list given in De.nition 3.9 con\u00adsumes \nevents generated by a call-by-value derivation. We already know that the same set of events is also computed \nby the lazy derivation. Furthermore, we know how to order these events to ob\u00adtain the call-by-value sequence, \ncf. Lemma 3.6. A naive way to implement the approach would therefore collect all events E gen\u00aderated \nby the lazy semantics then order them according to <{ E} and .nally compute the step list. However, we \ncan do better and process each event as soon as its exact structure is known but be\u00adfore we know its \nexact position in the complete sequence. This last step substantially increases the ef.ciency with which \nthe step list is generated in our approach. This algorithm is also straight forward to implement as described \nin the next section. In this section, we .rst give an alternative de.nition of how to compute the step \nlist that is less dependent on the order in which the events are generated. After that we show that the \nsequence of events generated by the lazy semantics meets the requirements of this alternative de.nition. \nThe events generated by both our semantics have an important property. Each reference occurring during \nthe evaluation of an expression e was either contained in e or is introduced on the right hand side of \nan event before being reduced on the left hand side of an event. We denote this property of introduction \nbefore usage as top-down because in the view of events as trees it means that parents always come before \nall of their children. Formally, the top\u00addown property is de.ned as follows. DEFINITION 3.15 (Top-Down \nSequence). Let E be a sequence of events. We call E top-down iff for all sequences u, v, w and all references \nr holds If E = u \u00b7 (r .. v) \u00b7 w and u .= . then there exist a reference q and sequences u1,u2,v1,v2 such \nthat u = u1 \u00b7 (q .. v1 \u00b7 r \u00b7 v2) \u00b7 u2. The challenge now is to compute the unique call-by-value step \nlist from a different sequence of events and in the presence of missing information of the form r .. \n.. The algorithm that meets this challenge is de.ned as follows. DEFINITION 3.16. Let E =(r .. v) \u00b7 w \nbe a top-down sequence of events and . .. ref { E} a fresh reference. Then, the associated step list \nis computed from as(E) := list(count(E. , (0,r) \u00b7 (0, .))), [5, 4], = const @ z} in f @ s []: let {const \n= let {w = .a..b..c.b} in w @ w; z = Zero; id = .x.x; s = id @ z; f [5, 4], []: let {w = .a..b..c.b} \nin w @ w \u00bb [5, 4], []: .a..b..c.b [4, 4], G1 = [w .. .a..b..c.b]: w @ w [3, 4], G1 : .b..c.b 2 [2, 4], \n G2 = G1[const .. .b..c.b]: let {z = Zero; id = .x.x; s = id @ z; f [2, 4], G2 : Zero = const @ z} in \nf @ s 2 66666666666666666664 [1, 4], = const @ z} in f @ s 6666666666666664 G3 = G2[z .. Zero]: let \n{id = .x.x; s = id @ z; f [1, 4], G3 : .x.x 2 [0, 4], = const @ z} in f @ s 666666666664 G4 = G3[id \n.. .x.x]: let {s = id @ z; f [0, 4], G4 : id @ z [4], G4 : \u00bb 2 [3], G5 = G4[s .. [3], \u00bb ]: let {f = \nconst @ z} in f @ s G5 : const @ z 666664 [2], G5 : .c.z 2 [1], G6 = G5[f .. .c.z]: f @ s [0], G6 : \nz [0], G6 : Zero \u00bb 4 [0], G6 : Zero Figure 8. Example of lazy call-by-value computation with the step \nlist [5,4] where the functions count and list are de.ned as follows: count((r .. .) \u00b7 E, v \u00b7 (n, r) \u00b7 \n(o, s) \u00b7 w)= count(E, v \u00b7 (n +1+ o, s) \u00b7 w) count((r .. r0 \u00b7 r1 \u00b7 ... \u00b7 rn) \u00b7 E, v \u00b7 (n, r) \u00b7 w)= count(E, \nv \u00b7 (n +1,r0) \u00b7 (0,r1) \u00b7 ... \u00b7 (0,rn) \u00b7 w) count(., w)= w list((n, r) \u00b7 w)= n : list(w) list(.) =[] LEMMA \n3.17 (Soundness of count). Let E =(r .. v) \u00b7 w be a top-down sequence of events and E. the sequence where \nthe events in E are sorted according to <{ E} . Then st(E.)= as(E). Proof. First, observe that E. is \nindeed unique as <{ E} is a linear ordering by Lemma 3.6. Furthermore, since E is a top-down sequence \nand since count processes this sequence left to right, each ri in a given event r .. r1 \u00b7...\u00b7rn cannot \nyet have been processed. As neither the root r nor the fresh . appear on the right hand side of an event, \nthis property also holds for the initial argument. We extend <{ E} such that . is the largest reference \nand ob\u00adserve that in the initial argument the references in the second se\u00adquence are ordered w.r.t. <{ \nE} . This ordering is maintained by the algorithm. In the .rst rule of count this is obvious as no new \nrefer\u00adences are introduced to the second sequence. In the second rule the introduced references are greater \nthan those in v as r is greater than the references in v and smaller than each ri. The introduced refer\u00adences \nri are also smaller than the references in w because those are not contained in E (r) because E is a \ntop-down sequence. Furthermore, each number n in (m, q) \u00b7 (n, r) within the sec\u00adond sequence exactly \ncounts all references s which have been elim\u00adinated and for which q<{ E} s<{ E} r holds. This is immediate, \nsince the second sequence is ordered and the elimination of a ref\u00aderence x is added to the counter directly \nbehind x in that sequence. Finally, by de.nition of E. , count eliminates and counts all references except \nthose r for which E contained an event r .. .. All in all, count computes the number of events between \nr .. . events, as st() does. . The only thing left to show is that the sequence of events generated by \nthe lazy semantics is indeed top-down. LEMMA 3.18. Let [] : e .E .: z be a lazy derivation and r be a \nreference not occuring in ref ({ E} ). Then (r .. [|e|]) \u00b7 E is a sequence of events which is top down. \nProof. Cf. (Bra\u00dfel et al. 2007). . We are ready to prove our main theorem. THEOREM 3.19. Let e be an \nexpression e l := l(e) and [] : e l .E .: z be a lazy derivation, and 1+ m : ms = as(E). Then there exists \na lazy call-by-value derivation m : ms, [] : e . .. : z, [0]. Proof. By Theorem 3.2 there exists a non-deterministic \ncall by value derivation [] : e l .E. .- : z with E =bE.. Since st(.)= [0], by Theorem 3.13 there exists \na lazy call by value derivation st(E.), [] : e . .. : z, [0]. Let r be a reference not occuring in ref \n({ E} ). Then by Lemma 3.18, E.. := (r .. [|e|]) \u00b7 E. is top-down and therefore Lemma 3.17 yields st(E..)= \nas(E..). Finally, by Proposition 3.11 st(E..) is of the form 1+ m : ms with m : ms = st(E.). . 4. Implementation \nWe have implemented a debugging tool for a kernel lazy functional language (basically, Haskell without \nthe IO monad)3. The tool consists of two main components: 1. A program transformation (cf. Section 4.1) \nthat associates to each source program an instrumented program that behaves as the source program but \nadditionally stores the step list from the sequence of events during its execution. 2. A combined tracer/declarative \ndebugger that executes the source program in a call-by-value manner w.r.t. the step list. To ensure an \nef.cient execution of the program during debugging time, the source program is compiled into a strict \nlanguage where the functions have additional arguments to pass the step list.  4.1 Trace Generation \nIn this section, we describe the generation of the step list represent\u00ading the information about laziness. \nWe call this generation tracing because the step list is produced by side effects during the execution \nof a program. In this sense, the step list is a signi.cantly condensed 3 At the moment, we also do not \nprovide classes. However, in contrast to the IO monad, supporting classes will not demand a conceptual \nextension. version of the information retrieved by debugging approaches like the ART of Hat (Sparud and \nRunciman 1997b) or the EDT of Freja (Nilsson and Sparud 1997) and Buddha (Pope and Naish 2003). Tracing \na given program is implemented by transforming the pro\u00adgram into an instrumented version. Executing this \nnew version will generate a .le containing the list of steps. One of the main goals of the presented \napproach is the ef.cient generation of the trace. To achieve this, we will not generate the whole sequence \nof events produced by the instrumented lazy se\u00admantics. Rather, we collapse the recorded data as much \nas possible during the execution of the program. The structure we keep at run time is at all times only \nthe list of numbers which is built according to the operation count (De.nition 3.16) and, therefore, \nat any time reduced as much as possible. We represent the step list as a doubly\u00adlinked list where pointers \nare modeled as IORefs in order to allow ef.cient destructive updates of the list structure. The data \ntypes for list nodes and references to them are de.ned as follows: type Ref = IORef Node data Node = \nEmpty | Node Ref StepCnt Ref type StepCnt = Int Empty is a special constructor used to mark the beginning \nand the end of a linked list. We provide an interface that re.ects the structure of the events that are \nreleased during the computation. In order to modify the original program as little as possible, we use \na projection function with side effects to release events. The function event :: Ref -> [Ref] -> a -> \na can be used to release an event of the form r .. r1 \u00b7 r2 ... \u00b7 rn before the evaluation of some expression \ne using the call event r [r1,r2,...,rn] e Corresponding to the de.nition of the operation count (De.ni\u00adtion \n3.16), we can consume each event directly when it is released and do not have to consider a speci.c order \nin which they are gener\u00adated. The .rst two rules of De.nition 3.16 can be directly translated into Haskell \ncode. The .rst de.ning rule of the function event cor\u00adresponds to the event r .. . so it removes r from \nthe linked list and adds its increased step count to its successor. event r [] x = unsafePerformIO (do \nNode v n w <-readIORef r updSucc v w Node _ o s <-readIORef w writeIORef w (Node v (n+1+o) s) return \nx) We employ the impure function unsafePerformIO to modify the list structure as a side effect. The \nsecond rule releases an event r .. s \u00b7 rn with a non-empty list of references: event r (s:rs) x = unsafePerformIO \n(do Node v n w <-readIORef r updSucc v s link v (n+1) (s:rs) w return x) Here, we employ the action \nlink :: Ref -> StepCnt -> [Ref] -> Ref -> IO Ref to replace the reference given as .rst argument to event \nby the given list of references. link is only de.ned for non-empty lists and the given step count is \nadded to the head of this list. link vn[r]w=do writeIORef r (Node v n w) updPred w r link v n (r:s:rs) \nw = do writeIORef r (Node v n s) link r 0 (s:rs) w The auxiliary functions updPred and updSucc update \nthe prede\u00adcessor or the successor of a reference in a linked list respectively: updPred, updSucc :: Ref \n-> Ref -> IO () updPred r p = do Node _ n s <-readIORef r writeIORef r (Node p n s) updSucc r s = do \nNode p n _ <-readIORef r writeIORef r (Node p n s) To compute a list of step numbers from a linked list \nrepresented by pointers, we de.ne an action list, which is the direct translation of list in De.nition \n3.16: list :: Ref -> IO [Int] list r = do node <-readIORef r case node of Empty -> return [] Node _ns-> \ndons<-lists return (n:ns) The function list is called after the execution of the transformed program \nand the resulting list of step numbers is then written into a .le. During the execution, the linked list \nrepresented by references is held in main memory. This list is small compared to traces generated by \nother debugging approaches, because it is compressed on the .y whenever a subcomputation is executed \ncompletely. The computation is started with a reference that points to distinguished start and end nodes \n(cf. Theorem 3.19). The constant initialRef returns the never collapsed start reference. Its successor \nis later supplied to the instrumented program: initialRef :: Ref initialRef = unsafePerformIO (do [start,hd,end,empty] \n<-sequenceM (replicate 4 (newIORef Empty)) writeIORef start (Node empty 0 hd) writeIORef hd (Node start \n0 end) writeIORef end (Node hd 0 empty) return start) Our de.nition of the function event can be improved. \nWhenever a reference is replaced by a non-empty list of references, the orig\u00adinal reference is deleted \nfrom the list by updating the successor of its predecessor. We can modify our approach to reuse references \nwhenever such an event occurs: the original reference can be reused for the .rst reference in the list \nof replacements. As a consequence, the predecessor of this reference can be left unchanged and we gen\u00aderate \nless fresh references during the execution of the instrumented program. In our implementation, we use \nthree specialized functions instead of the single function event to release events: collapse :: Ref -> \na -> a onlyInc :: Ref -> a -> a extend :: Ref -> [Ref] -> a -> a These functions behave as follows: collapse \ndeletes the given reference from the linked list and adds its incremented step count to its successor. \nIt releases events of the form r .. .. onlyInc only increments the step count stored in the given reference. \nIt releases events of the form r .. r . in order to reuse the IORef of r for r . . extend increments \nthe step count in the .rst reference and inserts the list of references after this reference. It implements \nevents r .. rn where n> 1 and reuses the IORef of r for r1. Note that the call onlyInc r needs to modify \nonly a single IORef, while event r [r ] accesses four. The speci.c function to be applied is statically \nknown during the program transformation. Thus, there is no additional overhead for dispatching to one \nof these functions at run time. The reuse of references helps to save both memory and run time because \nless references need to be created and less references are accessed in order to process the events. \n4.2 Program Debugging In this section, we show a concrete example for a debugging ses\u00adsion by considering \nthe program of Example 1.1. When we run our debugging tool with this example, the program is transformed \nso that the step list [2,1,0,14] is generated during the evaluation of the initial expression main. As \nalready discussed in Example 1.1, this step list points out that there are three expressions that should \nbe discarded during the evaluation of main due to the partial eval\u00aduation of the expression (fibs Zero) \nto (_:_:_). The tracer/debugger uses the step list to control the call-by\u00advalue evaluation. It starts \nwith the result of the initial expression: main --> Zero The result is computed during debugging time \naccording to our lazy call-by-value evaluation. Although we trade here computation time during the debugging \nsession against memory for storing the trace, the evaluation time required during debugging is acceptable \ndue to the ef.cient innermost execution and the time required by the user to decide the next interaction. \nAfter printing an expression and its value, the user has the op\u00adtion to proceed like a (call-by-value) \ntracer, i.e., a typical procedural debugger, or like a declarative debugger. For the latter, he has the \noptions c (stating that the result is correct w.r.t. his intended mean\u00ading) or w (indicating a wrong \nresult). For tracing, the user can move to the next reduction step or skip the entire computation. Tracing \nis useful when the user is undecided about a computation w.r.t. the intended semantics. A debugging session \nis .nished when the bug has been located, i.e., when there is an application of a rule where the evaluations \nof all functions occurring in the right-hand side are correct. If the end of a computation is reached \nwithout locating a bug (e.g., when the user has skipped over some potentially wrong subcomputation), \nthe session is restarted from the beginning. In our example, the entire computation is buggy so that \nwe type w. Since the expression (fibs Zero) is innermost in the right\u00adhand side of the rule for main, \nits (partial) evaluation is shown next: fibs Zero --> _:_:_ Here we type s in order to skip over this \ncomputation (which has not enough information for a de.nitive decision at this point). Thus, we see the \nnext subexpression in innermost order: take (S (S Zero)) (_:_:_) --> [_,_] Although this looks .ne, we \ntype s to see the next subexpression: length [_,_] --> Zero Since this is de.nitely a wrong length, we \ntype w and obtain: length [_] --> Zero We type w again and obtain length [] --> Zero which is intended \nso that we type c. Immediately, we get the report that the bug is in the rule that reduces the redex \nlength [_]. Looking at the source code for this rule, we can now see that an application of the constructor \nS is missing in the right-hand side. Similar to other debugging tools, our tool is also equipped with \na source code viewer that always shows the current rule applied to the outermost function of the expression \nunder consideration. The algorithm for bug location is similar to that in other declarative debugging \ntools (Shapiro 1983). However, note that the call-by\u00advalue view is important to obtain the questions \nand trace order in a comprehensible manner. Although the example was simple due to lack of space, the \nde\u00adbugging tool also covers higher-order features (as already shown in the semantics) and primitive functions \nlike arithmetic operations. Currently, the tool does not handle I/O actions. However, this is not a principle \nlimitation: I/O actions can be treated by storing their re\u00adsult in the step list that is then used during \nthe debugging session (instead of executing them again). The details about their repre\u00adsentation are \nnon-trivial (e.g., the effect of getChar and putChar should be visualized in some console window) so \nthat we leave this part of the implementation for future work. 4.3 Practical Experience In this section \nwe compare our debugger with the Haskell Tracer Hat (Wallace et al. 2001). The time that is needed to \ngenerate our trace is comparable to the time needed to generate Hat s ART. Obviously, our trace .le is \nmuch smaller than the ART. The call\u00adby-value evaluation steps are performed reasonably fast. We achieve \nthis performance with a modest implementation ef\u00adfort. We neither use highly optimized external functions4 \nto gen\u00aderate the trace, nor do we need sophisticated random access .le operations in order to navigate \nour trace .le. Although our trace generation is already acceptably ef.cient, we plan to optimize it using \nHaskell s foreign function interface to access a C implemen\u00adtation. First experiments show that such \nan implementation will be about .ve times faster than the current implementation. As we only prototypically \nimplemented our approach, we can not make signi.cant comparisons with Hat. We expect that the main performance \noverhead of our approach is the re-computation in call-by-value order. We choose an example with a lot \nof compu\u00adtation steps in order to compare the time spent in re-computing the results with the time necessary \nto look them up in a trace .le. EXAMPLE 4.1. The following example program would print the 16th prime \nnumber if we would not have introduced a bug: main = print (primes !! 15) primes :: [Int] primes = sieve \n[2..] sieve :: [Int] -> [Int] sieve (x:xs) = x : sieve (filter ((==0) . ( mod x)) xs) If we run this \nprogram, it prints the surprisingly big number 65536 which is de.nitely not a prime number. We use Hat \nand our de\u00adbugger in order to debug the evaluation of the main expression (primes !! 15). We measure \nthe time that is used for generating the traces and also compare their size. Our benchmarks were run \non an AppleTM PowerBook with an 1.33 GHz PowerPC G4 processor and 768 MB DDR SDRAM main memory. Hat s \nART is generated within 7 seconds and is 4.7 MB big. Our trace is generated within about 5 seconds and \nis about 100 bytes big. So, our trace gener\u00adation is as ef.cient as the generation of Hat s ART although \nit is completely implemented in Haskell. Hat s trace .le is about 50000 times bigger than ours which \ncan be printed on a few lines: 4 We only use unboxed Ints to represent the step count. [327673,0,1179615,589791,294879,147423, \n73695,36831,18399,9183,4575,2271,1119, 543,255,111,39,2,0,0,184] We can see that up to a million steps \ncan be performed in innermost order and only 20 suspensions are needed during the computation. We used \nthe declarative debugger hat-detect and our step debug\u00adger and compared their response times. Unfortunately, \ndebugging the program in Example 4.1 was not feasible in both debuggers, so we replaced the number 15 \nby 10 for this test. Hat-detect poses the following question almost immediately: primes = 2:4:8:16:32:64:128:256:512:1024:2048:_ \nWe can see that we compute powers of 2 instead of primes. Stating that this is wrong, it takes more than \n10 sec until the next question appears. Subsequent questions are generated within a few seconds. Our \ntool shows every computation step in less than a second. The time to show each step depends on how long \nit takes to compute the arguments and the result of the displayed call. Recomputing these values seems \nto be as feasible as looking them up in a redex trail. 5. Related Work As already mentioned in the introduction, \nmany of the approaches to debug lazy functional programs are based on recording the com\u00adputation in a \nredex trail (Sparud and Runciman 1997b) or an EDT (Nilsson and Sparud 1997). The size of these structures \ncrucially grows with the length of the computation to be debugged. As a so\u00adlution to this problem, some \ndifferent approaches were proposed. Sparud and Runciman (1997a) present one of the .rst ap\u00adproaches to \nreduce the size of redex trails. It is based on not record\u00ading trusted computations (e.g., the evaluation \nof Prelude functions) and on pruning trails. Unfortunately, considering trusted functions does not reduce \nmemory consumption as expected, since trusted functions are applied to expressions for which debug information \nhas to be recorded, cf. Pope and Naish (2003). Pruning trails was not considered further since the resulting \ntrail is incomplete and the buggy computation can be cut from the recorded information. In the further \ndevelopment of Hat (Wallace et al. 2001), the problem of recording large redex trails was not really \ntackled. All information is written to a large .le which results in a slowdown when generating this .le \nand analyzing it in viewer components. The piecemeal tracing approach of Nilsson (1999, 2001) and Pope \nand Naish (2003) was de.ned for declarative debugging by means of an Evaluation Dependence Tree (EDT) \nused in Freja (Nilsson and Sparud 1997) or Buddha (Pope 2005). In this ap\u00adproach, only a piece of the \nentire EDT is initially generated, and new pieces are computed only if they are needed by re-executing \nthe entire program. Hence, the saving of space is purchased by ad\u00additional run-time during debugging. \nIn contrast, our approach is directly space ef.cient and only stores a minimal amount of infor\u00admation \nat execution time. Furthermore, their approach is basically oriented to declarative debugging in contrast \nto step lists. Pope and Naish (2003) propose an optimization of the piece\u00admeal EDT construction in which \nit is not necessary to restart the whole computation to compute new pieces of the EDT. Instead computations \nare stored which allow the generation of the miss\u00ading parts of the EDT. However, there exists no evaluation \non how much memory is needed for storing these computations yet (Pope and Naish 2003). Furthermore, the \nimplementation highly depends on the internal structure of the Glasgow Haskell Compiler (ghc) and the \nunderlying C heap in which structures are stored to pre\u00advent them from garbage collection. This makes \nit non-portable to other Haskell systems. The whole approach is motivated from the implementation perspective, \nwithout any correctness proofs. 6. Conclusions We have presented a novel approach to debug lazy functional \npro\u00adgrams by re-executing them in the context of call-by-value evalu\u00adation. To avoid unnecessary computation \nsteps, we have designed an instrumented lazy semantics that produces the information nec\u00adessary to drive \nthe call-by-value evaluator so that it discards those expressions whose evaluation is not needed. Furthermore, \nwe have shown that this information can be obtained by program transfor\u00admation. We have formally de.ned \nthe resulting lazy call-by-value semantics and have proved its correctness. Our .rst experiments with \na prototypical implementation of a debugger are encouraging. The approach is a completely new technique \nof relating lazy and call-by-value computations. Although we developed it in the context of debugging \nit should be applicable to arbitrary analyzes of the run-time behavior of lazy functional language. As \nfuture work, we expect many fruitful applications of this technique and want to improve the implementation \nas well as the available back ends. References B. Bra\u00dfel, S. Fischer, M. Hanus, F. Huch, and G. Vidal. \nLazy Call-By-Value Evaluation. Technical report, CAU Kiel, 2007. A. Gill. Debugging Haskell by Observing \nIntermediate Data Structures. In Proc. of the 4th Haskell Workshop. Technical report of the University \nof Nottingham, 2000. J. Launchbury. A Natural Semantics for Lazy Evaluation. In Proc. of the ACM Symp. \non Principles of Programming Languages (POPL 93), pages 144 154. ACM Press, 1993. H. Nilsson. Tracing \nPiece by Piece: Affordable Debugging for Lazy Func\u00adtional Languages. In Proc. of the 1999 Int l Conf. \non Functional Pro\u00adgramming (ICFP 99), pages 36 47. ACM Press, 1999. H. Nilsson. How to look busy while \nbeing as lazy as ever: the implementa\u00adtion of a lazy functional debugger. Journal of Functional Programming, \n11(6):629 671, 2001. H. Nilsson and J. Sparud. The Evaluation Dependence Tree as a Basis for Lazy Functional \nDebugging. Automated Software Engineering, 4(2): 121 150, 1997. S.L. Peyton Jones, editor. Haskell 98 \nLanguage and Libraries The Revised Report. Cambridge University Press, 2003. B. Pope. Declarative Debugging \nwith Buddha. In V. Vene and T. Uustalu, editors, Advanced Functional Programming, 5th International School, \nAFP 2004, volume 3622 of Lecture Notes in Computer Science, pages 273 308. Springer Verlag, September \n2005. ISBN 3-540-28540-7. B. Pope and Lee Naish. Practical aspects of declarative debugging in Haskell-98. \nIn Fifth ACM SIGPLAN Conference on Principles and Practice of Declarative Programming, pages 230 240, \n2003. ISBN:1\u00ad58113-705-2.  P.M. Sansom and S.L. Peyton Jones. Formally Based Pro.ling for Higher-Order \nFunctional Languages. ACM Transactions on Programming Lan\u00adguages and Systems, 19(2):334 385, 1997. E. \nShapiro. Algorithmic Program Debugging. MIT Press, Cambridge, Massachusetts, 1983. J. Sparud and C. \nRunciman. Complete and Partial Redex Trails of Func\u00adtional Computations. In Proc. of the 9th Int l Workshop \non the Imple\u00admentation of Functional Languages (IFL 97), pages 160 177. Springer LNCS 1467, 1997a. J. \nSparud and C. Runciman. Tracing Lazy Functional Computations Us\u00ading Redex Trails. In Proc. of the 9th \nInt l Symp. on Programming Lan\u00adguages, Implementations, Logics and Programs (PLILP 97), pages 291  308. \nSpringer LNCS 1292, 1997b. M. Wallace, O. Chitil, T. Brehm, and C. Runciman. Multiple-View Tracing for \nHaskell: a New Hat. In Proc. of the 2001 ACM SIGPLAN Haskell Workshop. Universiteit Utrecht UU-CS-2001-23, \n2001.   \n\t\t\t", "proc_id": "1291151", "abstract": "<p>Designing debugging tools for lazy functional programming languages is a complex task which is often solved by expensive tracing of lazy computations. We present a new approach in which the information collected as a trace is reduced considerably (kilobytes instead of megabytes). The idea is to collect a kind of step information for a call-by-value interpreter, which can then efficiently reconstruct the computation for debugging/viewing tools, like declarative debugging. We show the correctness of the approach, discuss a proof-of-concept implementation with a declarative debugger as back end and present some benchmarks comparing our new approach with the Haskell debugger Hat.</p>", "authors": [{"name": "Bernd Bra&#223;el", "author_profile_id": "81100307117", "affiliation": "CAU Kiel, Kiel, Germany", "person_id": "P869014", "email_address": "", "orcid_id": ""}, {"name": "Michael Hanus", "author_profile_id": "81100022601", "affiliation": "CAU Kiel, Kiel, Germany", "person_id": "PP39023651", "email_address": "", "orcid_id": ""}, {"name": "Sebastian Fischer", "author_profile_id": "81330491065", "affiliation": "CAU Kiel, Kiel, Germany", "person_id": "PP37042023", "email_address": "", "orcid_id": ""}, {"name": "Frank Huch", "author_profile_id": "81100099102", "affiliation": "CAU Kiel, Kiel, Germany", "person_id": "PP37023706", "email_address": "", "orcid_id": ""}, {"name": "Germ&#225;n Vidal", "author_profile_id": "81100420507", "affiliation": "Technical University of Valencia, Valencia, Spain", "person_id": "PP39086759", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291193", "year": "2007", "article_id": "1291193", "conference": "ICFP", "title": "Lazy call-by-value evaluation", "url": "http://dl.acm.org/citation.cfm?id=1291193"}