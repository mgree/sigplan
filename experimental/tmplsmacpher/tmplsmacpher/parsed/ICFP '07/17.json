{"article_publication_date": "10-01-2007", "fulltext": "\n Compiling with Continuations, Continued Andrew Kennedy Microsoft Research Cambridge akenn@microsoft.com \nAbstract We present a series of CPS-based intermediate languages suitable for functional language compilation, \narguing that they have practi\u00adcal bene.ts over direct-style languages based on A-normal form (ANF) or \nmonads. Inlining of functions demonstrates the bene\u00ad.ts most clearly: in ANF-based languages, inlining \ninvolves a re\u00adnormalization step that rearranges let expressions and possibly in\u00adtroduces a new join \npoint function, and in monadic languages, commuting conversions must be applied; in contrast, inlining \nin our CPS language is a simple substitution of variables for variables. We present a conti.cation transformation \nimplemented by sim\u00adple rewrites on the intermediate language. Exceptions are modelled using so-called \ndouble-barrelled CPS. Subtyping on exception constructors then gives a very straightforward effect analysis \nfor ex\u00adceptions. We also show how a graph-based representation of CPS terms can be implemented extremely \nef.ciently, with linear-time term simpli.cation. Categories and Subject Descriptors D.3.4 [Programming \nLan\u00adguages]: Processors Compilers General Terms Languages Keywords Continuations, continuation passing \nstyle, monads, op\u00adtimizing compilation, functional programming languages  1. Introduction Compiling \nwith continuations is out of fashion. So report the au\u00adthors of two classic papers on Continuation-Passing \nStyle in recent retrospectives: In 2002, then, CPS would appear to be a lesson aban\u00ad doned. (McKinley \n2004; Shivers 1988) Yet, compiler writers abandoned CPS over the ten years following our paper anyway. \n(McKinley 2004; Flanagan et al. 1993) This paper argues for a reprieve for CPS: Compiler writers, give \ncontinuations a second chance. This conclusion is borne of practical experience. In the MLj and SML.NET \nwhole-program compilers for Standard ML, co\u00adimplemented by the current author, we adopted a direct-style, \nmonadic intermediate language (Benton et al. 1998, 2004b). In part, we were interested in effect-based \nprogram transformations, Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. ICFP 07, October 1 3, 2007, Freiburg, Germany. Copyright c . 2007 ACM 978-1-59593-815-2/07/0010. \n. . $5.00 so monads were a natural choice for separating computations from values in both terms and types. \nBut, given the history of CPS, prob\u00adably there was also a feeling that CPS is for call/cc , something \nthat is not a feature of Standard ML. Recently, the author has re-implemented all stages of the SML.NET \ncompiler pipeline to use a CPS-based intermediate lan\u00adguage. Such a change was not undertaken lightly, \namounting to roughly 25,000 lines of replaced or new code. There are many bene.ts: the language is smaller \nand more uniform, simpli.ca\u00adtion of terms is more straightforward and extremely ef.cient, and advanced \noptimizations such as conti.cation are more easily ex\u00adpressed. We use CPS only because it is a good place \nto do opti\u00admization; we are not interested in .rst-class control in the source language (call/cc), or \nas a means of implementing other features such as concurrency. Indeed, as SML.NET targets .NET IL, a \ncall\u00adstack-based intermediate language with support for structured ex\u00adception handling, the compilation \nprocess can be summarized as transform direct style (SML) into CPS; optimize CPS; transform CPS back \nto direct style (.NET IL) . 1.1 Some history CPS. What s special about CPS? As Appel (1992, p2) put it, \nContinuation-passing style is a program notation that makes ev\u00adery aspect of control .ow and data .ow \nexplicit . An important consequence is that full \u00df-reduction (function inlining) is sound. In contrast, \nfor call-by-value languages based on the lambda cal\u00adculus, only the weaker \u00df-value rule is sound. For \nexample, \u00df\u00adreduction cannot be applied to (.x.0) (fy) because fy may have a side-effect or fail to terminate; \nbut its CPS transform, fy (.z.(.x..k.k 0) zk) can be reduced without prejudice. There are obvious drawbacks: \nthe complexity of CPS terms; the need to eliminate administrative redexes introduced by the CPS transformation; \nand the cost of allocating closures for lambdas in\u00adtroduced by the CPS transformation, unless some static \nanlysis is .rst applied. In fact, these drawbacks are more apparent than real: the complexity of CPS \nterms is really a bene.t, assigning use\u00adful names to all intermediate computations and control points; \nthe CPS transformation can be combined with administrative reduc\u00adtion; and by employing a syntactic separation \nof continuation-and source-lambdas it is possible to generate good code directly from CPS terms. ANF. \nIn their in.uential paper The Essence of Compiling with Continuations , Flanagan et al. (1993) observed \nthat fully devel\u00adoped CPS compilers do not need to employ the CPS transformation but can achieve the \nsame results with a simple source-level transfor\u00admation . They proposed a direct-style intermediate language \nbased on A-normal forms, in which a let construct assigns names to every intermediate computation. For \nexample, the term above is repre\u00adsented as let z = fy in (.x.0) z,to which \u00df-reduction can be ap\u00adplied, \nobtaining the semantically equivalent let z = fy in 0.This style of language has become commonplace, \nnot only in compilers, but also to simplify the study of semantics for impure functional languages (Pitts \n2005, \u00a77.4). Monads. Very similar to ANF are so-called monadic languages based on Moggi s computational \nlambda calculus (Moggi 1991). Monads also make sequencing of computations explicit through a let x . \nM in N binding construct, the main difference from ANF being that let constructs can themselves be let-bound. \nThe sepa\u00adration of computations from values also provides a place to hang effect annotations (Wadler \nand Thiemann 1998) which compilers can use to perform effect-based optimizing transformations (Ben\u00adton \net al. 1998). 1.2 The problem A-Normal Form is put forward as a compiler intermediate language with \nall the bene.ts of CPS (Flanagan et al. 1993, \u00a76). Unfor\u00adtunately, the normal form is not preserved under \nuseful compiler transformations such as function inlining (\u00df-reduction). Consider the ANF term M = let \nx =(.y.let z =ab in c)d in e. Now naive \u00df-reduction produces let x =(let z =ab in c)in e which is not \nin normal form. The .x is to de.ne a more complex notion of \u00df-reduction that re-normalizes let constructs \n(Sabry and Wadler 1997), in this case producing the normal form let z =ab in (let x =c in e). In contrast, \nthe CPS transform of M, namely (.y..k.ab(.z.k c))d (.x.k e), simpli.es by simple \u00df-reduction to ab (.z.(.x.k \ne)c). As Sabry and Wadler explain in their study of the relationship be\u00adtween CPS and monadic languages, \nthe CPS language achieves this normalization using the metaoperation of substitution which traverses \nthe CPS term to locate k and replace it by the contin\u00aduation thus effectively pushing the continuation \ndeep inside the term (Sabry and Wadler 1997, \u00a7 8). Monadic languages permit let expressions to be nested, \nbut incorporate so-called commuting conversions (cc s) such as let y . (let x . M in N)in P . let x . \nM in (let y . N in P ). ANF can be seen as a monadic language in which \u00df-reduction is combined with cc-normalization \nensuring that terms remain in cc\u00adnormal form. All of the above seems quite benign; except for two things: \n1. Commuting conversions increase the complexity of simplifying intermediate language terms. Reductions \nthat strictly decrease the size of the term can be applied exhaustively on CPS terms, the number of reductions \napplied being linear in the size of the term. The equivalent ANF or monadic reductions must neces\u00adsarily \ninvolve commuting conversions, which leads to O(n 2) reductions in the worst case. Moreover, as Appel \nand Jim (1997) have shown, given a suitable term representation, shrinking re\u00adductions on CPS can be \napplied in time O(n); it is far from clear how to amortize the cost of commuting conversions to obtain \na similar measure for ANF or monadic simpli.cation. 2. Real programming languages include conditional \nexpressions, or, more generally, case analysis on datatype constructors. These add considerable complexity \nto reductions on ANF or  monadic terms. Consider the term let z =(.x.if x then a else b)c in M This \nis in ANF, but \u00df-reduction produces let z =(if c then a else b)in M, which is not in normal form because \nit contains a let-bound conditional expression. To reduce it to normal form, one must either apply a \nstandard commuting conversion that duplicates the term M, producing if c then let z =a in M else let \nz =b in M, or introduce a join-point function for term M,togive let kz =M in if c then let z =a in kz \nelse let z =b in kz. Observe that k is simply a continuation! In our CPS language, k is already available \nin the original term, being the (named) continuation that is passed to the function to be inlined. The \nde\u00adsire to share subterms almost forces some kind of continuation construct into the language. Better \nto start off with a language that makes continuations explicit. 1.3 Contribution Much of the above has \nbeen said before by others, though not al\u00adways in the context of compilation; in this author s opinion, \nthe most illuminating works are Appel (1992); Danvy and Filinski (1992); Hatcliff and Danvy (1994); Sabry \nand Wadler (1997). One contribution of this paper, then, is to draw together these observa\u00adtions in a \nform accessible to implementers of functional languages. As is often the case, the devil is in the details, \nand so another purpose of this paper is to advocate a certain style of CPS that works very smoothly for \ncompilation. Continuations are named and mandatory (just as every intermediate value is named, so is \nevery control point), are second-class (they re not general lambdas), can represent basic blocks and \nloops, can be shared (typically, through common continuations of branches), represent exceptional control \n.ow (using double-barrelled CPS), and are typeable (but can be used in untyped form too). By re.ning \nthe types of exception values in the double-barrelled variant we get an effect system for exceptions \nfor free . We make two additional contributions. Following Appel and Jim (1997), we describe a graph-based \nrepresentation of CPS terms that supports the application of shrinking \u00df-reductions in time lin\u00adear in \nthe size of the term. We improve on Appel and Jim s selec\u00adtive use of back pointers for accessing variable \nbinders, and em\u00adploy the union-.nd data structure to give amortized near-constant\u00adtime access to binders \nfor all variable occurrences. This leads to ef\u00ad.cient implementation of .-reductions and other transformations. \nWe present benchmark results comparing our graph-CPS represen\u00adtation with (a) an earlier graphical representation \nof the original monadic language used in our compiler, and (b) the original func\u00adtional representation \nof that language. Lastly, we show how to transform functions into local continu\u00adations using simple term \nrewriting rules. This approach to contif\u00adication avoids the need for a global dominator analysis (Fluet \nand Weeks 2001), and furthermore supports nested and .rst-class func\u00adtions.  2. Untyped CPS We start \nby de.ning an untyped continuation-passing language .U that supports non-recursive functions, the unit \nvalue, pairs, CPS and tagged values. Even for such a simple language, we can cover many of the issues \nand demonstrate advantages over alternative, direct-style languages. Grammar (terms) CTm . K,L ::= letval \nx= V in K | let x= pi xin K | letcont kx= K in L | kx | fkx | case xof k1 . k2 (values) CVal . V,W ::= \n() | (x,y) | ini x| .kx.K Well-formed terms G . V ok G,x;. . K ok (let) G; . . letval x= V in K ok G,x;. \n. K ok G; .,k . L ok (letc) G; . . letcont kx= K in Lok x. GG,y;. . K ok (proj) i . 1,2 G; . . let \ny = pi xin K ok k . .,x . G k . .,f,x . G (appc) (app) G; . . kxok G; . . fkx ok x. G,k1,k2 . . (case) \nG; . . case xof k1 . k2 ok Well-formed values x,y . G x. G (pair) (tag) i . 1,2 G . (x,y) ok G . ini \nxok G,x; k . K ok (unit) (abs) G . () ok G . .kx.K ok Well-formed programs (prog) {}; halt . K ok Figure \n1. Syntax and scoping rules for untyped language .U CPS In Section 3, we add recursive functions, types, \npolymorphism, exceptions, and effect annotations. At that point, the language re\u00adsembles a practical \nCPS-based intermediate language of the sort that could form the core of a compiler for SML, Caml, or \nScheme. Figure 1 presents the syntax of the untyped language. Here or\u00addinary variables are ranged over \nby x, y, f,and g, and continuation variables are ranged over by k and j. Indices irange over 1,2. We \nspecify scoping of variables using well-formedness rules for values and terms. Here G . V ok means that \nvalue V is well-formed in the scope of a list of ordinary variables G,and G; . . K ok means that term \nK is well-formed in the scope of a list of continuation variables . and a list of ordinary variables \nG. Complete programs are well-formed in the context of a distinguished top-level contin\u00aduation halt. \n(For the typed variant of our language there will be typing rules with G and . generalized to typing \ncontexts.) We describe the constructs of the language in turn. The expression letval x = V in K binds \na value V to a variable x in the term K. Thisisthe only way a value V can be used in a term; arguments \nto functions, case scrutinees, components of pairs, and so on, must all be simple variables. Even the \nunit value () must be bound to a variable before being used (in the full language, the same holds even \nfor constants such as 42). This means that there is no need for a general notion of substitution: we \nonly substitute variables for variables. Notice also that there is no notion of redundant binding such \nas let x. y in K. The expression let x = pi y in K projects the i th component of a pair y and binds \nit to variable xin K.  The expression letcont kx = K in L introduces a local continuation k whose single \nargument is x and whose body is K,to beusedinterm L. It corresponds to a labelled block in traditional \nlower-level representations. In Section 3 we extend local continuations with support for recursion, and \nso represent loops directly.  A continuation application kxcorresponds to a jump (if k is a local continuation) \nor a return (if k is the return continuation of a function value). As with values, continuations must \nbe named: function application expressions and case constructs do not have subterms, but instead mention \ncontinuations by name. We need only ever substitute continuation variables for continuation variables. \n Local continuations can be applied more than once, as in letcont jy = K in letcont k1 x1 =(letval x= \nV1 in jx) in . letcont k2 x2 =(letval x= V2 in jx) in case z of k1 . k2 Here j is the common continuation, \nor join point for branches k1 and k2. The expression fkx is the application of a function f to an argument \nxand a continuation k whose parameter receives the result of applying the function. If k is the return \ncontinuation for the nearest enclosing ., then the application is a tail call . For example, consider \nthe function value .kx.(letcont jy = gky in fjx). Here gis in tail position, and f is not. In effect, \nwe are de.ning .x.g(f(x)). The construct case x of k1 . k2 expects x to be bound to a tagged value ini \ny and then dispatches to the appropriate continuation ki, passing yas argument.  Values include the \nunit value (),pairs (x,y) and tagged values ini x. Function values .kx.K include a return continuation \nk and argument x. Note carefully the well-formedness rule (abs): its continuation context includes only \nthe return continuation k, thus enforcing locality of continuations introduced by letcont.  The semantics \nis given by environment-style evaluation rules, presented in Figure 2. As is conventional, we de.ne a \nsyntax of run-time values, ranged over by r, supporting the unit value, pairs, constructor applications, \nand closures. Environments map variables to run-time values, and continuation variables to continuation \nval\u00adues. Continuation values are represented in a closure form, which gives the impression that they \nare .rst-class. An alternative would be to model stack frames more directly and thereby demonstrate that \ncontinuations are in fact just code pointers. For the purpose of simply de.ning the meaning of programs \nwe prefer the closure\u00adbased semantics. The function .\u00b7. . interprets a value expression in an environ\u00adment \n.. Terms are evaluated in an environment .; the only obser\u00advations that we can make of programs are termination, \ni.e. the ap\u00adplication of the top-level continuation halt to a unit value. 2.1 CPS transformation To illustrate \nhow the CPS-based language can be used for func\u00adtional language compilation, consider a fragment of Standard \nML Runtime values: r ::= () |(r1,r2) |ini r |.., .k x.K. Continuation values: c ::= .., .x.K. Environments: \n. ::= |.,x ..r |., k ..c  Interpretation of values: .(). . =() .(x,y). . =(.(x),.(y)) .ini V . . = ini \n(.(x)) ..k x.K. . = .., .k x.K.  Evaluation Rules: .,x . ..V . . .K . (e-let) . .letval x = V in K . \n.,k . ..., .x.K..L . (e-letc) . .letcont kx = K in L . ., y . .ri .K . (e-proj) .(x)=(r1,r2) . .let \ny = pi x in K . .. ,y ...(x) .K . (e-appc) .(k)= ...,.y.K. . .kx . .. ,y ..r .K . .(x)= ini r (e-case) \n. .case x of k1 . k2 . .(ki)= ...,.y.K. .. ,j .. ..(k),y ..(x) .K . (e-app) .(f)= ...,.j y.K. . .fkx \n. (e-halt) . .halt x . Figure 2. Evaluation rules for .U CPS .\u00b7. : ML .(Var .CTm) .CTm .x. . = .(x) .(). \n. = letval x =() in .(x) .e1 e2. . = .e1. (.z1. .e2. (.z2. letcont kx = .(x) in z1 kz2)) .(e1,e2). . \n= .e1. (.z1. .e2. (.z2. letval x =(z1,z2) in .(x))) .inie. . = .e. (.z.letval x = ini z in .(x)) .#ie. \n. = .e. (.z.let x = pi z in .(x)) .fn x => e. . = letval f = .k x..e. (.z.k z) in .(f) .let val x = e1 \nin e2 end. . = letcont jx = .e2. . in .e1. (.z.j z) .case e of in1 x1 => e1| in2 x2 => e2. . = .e. (.z.letcont \nk1 x1 = .e1. . in letcont k2 x2 = .e2. . in case z of k1 . k2) Figure 3. Naive CPS transformation of \ntoy ML into .U CPS whose expressions (ranged over by e) have the following syntax: ML .e ::= x |ee . \n|fn x => e |(e,e .) |#ie |() |inie |let val x = e in e . end |case e of in1 x1 => e1| in2 x2 => e2 We \nassume a datatype declared by datatype ( a, b) sum = in1 of a | in2 of b Expressions in this language \ncan be translated into untyped CPS terms using the function shown in Figure 3. This is an adapta\u00adtion \nof the standard higher-order one-pass call-by-value transfor\u00admation (Danvy and Filinski 1992). An alternative, \n.rst-order, trans\u00adformation is described by Danvy and Nielsen (2003). The transformation works by taking \na translation-time func\u00adtion . as argument, representing the context into which the trans\u00adlation of the \nsource term is embedded. For our language, the con\u00adtext s argument is a variable, as all intermediate \nresults are named. Note some conventions used in Figure 3: translation-time lambda abstraction is written \nusing and translation-time application is .written .(...), to distinguish from . and juxtaposition used \nto de\u00adnote lambda abstraction and application in the target language. Also note that any object variables \npresent in the target terms but not in the source are assumed fresh with respect to all other bound vari\u00adables. \nThe translation is one-pass in the sense that it introduces no administrative reductions (here, \u00df-redexes \nfor continuations) that must be removed in a separate phase, except for let constructs (to avoid these \nalso would require analysis of the let expression; we prefer to apply simplifying rewrites on the output \nof the transfor\u00admation). However, the translation is naive in two ways. First, it in\u00adtroduces .-redexes \nfor continuations when translating tail function applications. For example, .fn x => f (x,y). . produces \n letval g = .k x.(letval p =(x,y) in letcont jz = kz in fjp) in .(g) whose .-redex (highlighted) can \nbe eliminated to obtain the more compact letval g =(.k x.letval p =(x,y) in fkp) in .(g). Second, the \ntranslation of case duplicates the context; consider, for example, f(case x of in1 x1 => e1| in2 x2 => \ne2) whose translation involves two calls to f. The more sophisticated translation scheme of Figure 4 \navoids both these problems; again, this is based on Danvy and Filinski (1992). The translation function \n.\u00b7. is as before, except (a) it in\u00adtroduces a join point continuation to avoid context duplication for \ncase, and (b) for terms in tail position it uses an alternative trans\u00adlation function .\u00b7. that takes \nan explicit continuation variable as argument instead of a context. 2.2 Rewrites After translating from \nsource language to intermediate language, most functional language compilers perform a number of optimiza\u00adtion \nphases that are implemented as transformations on intermedi\u00adate language terms. Some phases are speci.c \n(for example, arity\u00adraising of functions, or hoisting expressions out of loops) but usu\u00adally there is \nsome set of general rewrites based on standard re\u00adductions in the lambda-calculus. Figure 5 presents \nsome general rewrites for our CPS-based language. The rewrites look more com\u00adplicated than the equivalent \nreductions in the lambda-calculus be\u00adcause the naming of intermediate values forces introduction and \nelimination forms apart. For example, \u00df-reduction on pairs, which in the lambda calculus is simply pi \n(e1,e2) . ei, has to support an intervening context C. In practice, the rewrites are not hard to im\u00adplement. \nIn functional style, value bindings (e.g. pairs) are stored in an environment which is accessed at the \nreduction site (e.g. a pro\u00adjection). In imperative style, bindings are accessed directly through pointers, \nas we shall see in Section 4.1. The payoff from this style of rewrite is the selective use of \u00df rules. \nFor example, in a lambda-calculus extended with a let con\u00adstruct, one might perform the reduction let \np =(x, y) in M . M[(x, y)/p] but this would be undesirable unless every substi\u00adtution of (x,y) for p \nin M produced a redex. In our language, letval p =(x,y) in ...k p... let z = p1 p in K reduces to .\u00b7. \n: ML .(Var .CTm) .CTm .fn x=> e. . = letval f = .kx. .e. k in .(f) .let val x= e1 in e2 end. . = letcont \njx = .e2. .in .e1. j .case eof in1 x1 => e1| in2 x2 => e2. . = .e. (.z. letcont jx = .(x) in letcont \nk1 x1 = .e1. j in letcont k2 x2 = .e2. j in case z of k1 . k2) .\u00b7. : ML .CVar .CTm .x. k = kx .e1 e2. \nk = .e1. (.x1..e2. (.x2.x1 kx2)) .fn x=> e. k = letval f = .jx..e. j in kf .(e1,e2). k = .e1. (.x1..e2. \n(.x2.letval x=(x1,x2) in kx)) .inie. k = .e. (.z.letval x= ini z in kx) .(). k = letval x=() in kx .#ie. \nk = .e. (.z.let x.pi z in kx) .let val x= e1 in e2 end. k = letcont jx = .e2. k in .e1. j .case eof in1 \nx1 => e1| in2 x2 => e2. k = .e. (.z.letcont k1 x1 = .e1. k in letcont k2 x2 = .e2. k in case z of k1 \n. k2) Figure 4. Tail CPS transformation (changes and additions only shown) C::= [] |letval x= V in C|let \nx= pi y in C|letval x= .kx.Cin K |letcont kx= Cin K |letcont kx= K in C DEAD-CONT letcont kx= L in K \n.L (knot free in K) DEAD-VAL letval x= V in K .K (xnot free in K) \u00df-CONT letcont kx= K in C[ky] . letcont \nkx= K in C[K[y/x]] \u00df-FUN letval f = .kx.K in C[fjy]  . letval f = .kx.K in C[K[y/x,j/k]] \u00df-CASE letval \nx= ini y in C[case xof k1 . k2]  . letval x= ini y in C[ki y] \u00df-PAIR letval x=(x1,x2) in C[let y = pi \nxin K]  .letval x=(x1,x2) in C[K[xi/y]] \u00df-CONT-LIN letcont kx= K in C[ky] .C[K[y/x]] (if knot free in \nC) \u00df-FUN-LIN letval f = .kx.K in C[fjy] .C[K[y/x,j/k]] (f =.y, f not free in C) .-CONT letcont kx= jxin \nK . K[j/k] .-FUN letval f = .kx.g k xin K . K[g/f] .-PAIR let xi = pi xin C[let xj = pj x in C.[letval \ny =(x1,x2) in K]] .let xi = pi xin C[let xj = pj x in C.[K[x/y]]] ({i,j}= {1,2}) .-CASE letcont ki x1 \n=(letval y1 = ini x1 in ky1) in C[letcont kj x2 =(letval y2 = inj x2 in ky2) in C.[case xof k1 . k2]] \n.letcont ki x1 =(letval y1 = ini x1 in ky1) in C[letcont kj x2 =(letval y2 = inj x2 in ky2) in C.[kx]] \n({i,j}= {1,2}) Figure 5. General rewrites for .U CPS letval p =(x,y) in ...k p...K[x/z] which applies \nthe \u00df-PAIR rule to p1 pbut preserves other occurrences of p. It is easy to show that all rewrites preserve \nwell-formedness of terms. In particular, the scoping of local continuations is respected. The \u00df-FUN and \n\u00df-CONT reductions are inlining transforma\u00adtions for functions and continuations. The remainder of the \nreduc\u00adtions we call shrinking reductions, as they strictly decrease the size of terms (Appel and Jim \n1997). The \u00df-CONT-LIN and \u00df-FUN-LIN reductions are special cases of \u00df-reduction for linear uses of a \nvari\u00adable, in effect combining DEAD-and \u00df-reductions. Shrinking re\u00adductions can be applied exhaustively \non a term, and are typically used to clean up a term after some special-purpose global trans\u00adformation \nsuch as arity-raising or monomorphisation. Clearly the number of such reductions will be linear in the \nsize of the term; moreover, using the representation of terms described in Section 4 it is possible to \nperform such reductions in linear time.  2.3 Comparison with a monadic language The original implementations \nof the MLj and SML.NET compil\u00aders used monadic languages inspired by Moggi s computational lambda calculus \n(Moggi 1991). Figure 6 presents syntax for a monadic language .mon and selected reduction rules. The \nde.ning feature of monadic languages is that sequencing of computations is made explicit through the \nlet construct; val\u00adues are converted into trivial computations using the val construct. Monadic languages \nshare with CPS languages the property that fa\u00admiliar \u00df-reduction on functions is sound, as evaluation \nof the func\u00adtion argument is made explicit through let. But there are drawbacks, as we outlined in the \nIntroduction. (An orthogonal issue as for CPS based languages is whether values can appear anywhere \nex\u00adcept inside val.In .mon, for ease of presentation, we permit values to be embedded in applications, \npairs, and so on, whereas for .U CPS we insist that they are named. The difference shows up in the re\u00adduction \nrules, which in .U make use of contexts. It should be CPS noted that the drawbacks of monadic languages \nthat we are about to discuss are unaffected by this choice.) Problem 1: need for let/let commuting conversion. \nThe basic reductions listed in Figure 5 have corresponding reductions in CPS. The let construct itself \nhas \u00df and . rules which correspond to \u00df-CONT and .-CONT for .U (consider the CPS transforms of CPS the \nterms). In contrast to CPS-based languages, though, monadic Problem 2: quadratic blowup. The CC-LET \nreduction seems in-Grammar nocent enough. But observe that it is not a shrinking reduction so MTm . \nM, N ::= val v | let x . M in N | vw | pi v it s not immediately clear whether reduction will terminate. \nFortu\u00ad| case v of in1 x1.M1 . in2 x2.M2 nately, the combination of CC-LET and shrinking \u00df/.-reductions \nMVal . v, w ::= x | .x.M | (v, w) | ini v | () of Figure 6 does terminate (Lindley 2005), and moreover \nthere is a formal correspondence between the reductions of the monadic Reductions language and CPS (Hatcliff \nand Danvy 1994). Unfortunately, the \u00df-LET let x . val v in M . M[v/x] order in which conversions are \napplied is critical to the ef.ciency .-LET let x . M in val x . M of simpli.cation by reduction. Consider \nthe following term in .mon: CC-LET let x2 . (let x1 . M1 in M2) in N let fn . val (.xn.let yn . gxn in \ngyn) in CC-CASE let x . (case v of in1 x1.M1 . in2 x2.M2) in N . let x1 . M1 in (let x2 . M2 in N) let \nfn-1 . val (.xn-1.let yn-1 . fn xn-1 in gyn-1) in . . let f . val .x.N . . in case v of in1 x1.let x \n. M1 in fx let f1 . val (.x1.let y1 . f2 x1 in gy1) inf1 a . in2 x2.let x . M2 in fx If (linear) \u00df-FUN \nis applied to all functions in this term, followed \u00df-PAIR pi (v1,v2) . vi by a sequence of CC-LET reductions, \nthen no redexes remain \u00df-FUN (.x.M) v . M[v/x] after O(n) reductions. If, however, the commuting conversions \n\u00df-CASE case ini v of in1 x1.M1 . in2 x2.M2 . Mi[v/xi] are interleaved with \u00df-FUN,then O(n 2) reductions \nare required. Figure 6. Syntax and selected rewrites for monadic language .mon (There are other examples \nwhere it is better to apply commuting conversions .rst.) Although this is a pathological example, the \nsimpli.er was a major bottleneck in the MLj and SML.NET languages include a so-called commuting conversion, \nexpressing compilers (Benton et al. 2004a), in part (we believe) because of associativity for let: the \nneed to perform commuting conversions. CC-LET let x2 . (let x1 . M1 in M2) in N Solution 2: Use CPS. \nIt is interesting to note that monadic terms . let x1 . M1 in (let x2 . M2 in N) can be translated into \nCPS in linear-time; shrinking reductions can This reduction plays a vital role in exposing further reductions. \nbe applied exhaustively there in linear-time (see Section 4); and the Consider the source expression \nterm can be translated back into CPS in linear-time. Therefore the quadratic blowup we saw above is not \nfundamental, and there may #1 ((fn x => (gx,x)) y) be some means of amortizing the cost of commuting \nconversions Its translation into .mon is so that exhaustive reductions can be peformed in linear time. \nNev\u00adertheless, it is surely better to have the term in CPS from the start, let z2 . (.x.let z1 . gx in \nval (z1,x)) y in p1 z2. and enjoy the bene.t of linear-time simpli.cation. Now suppose that we apply \n\u00df-FUN,to get Problem 3: need for let/case commuting conversion. Matters let z2 . (let z1 . gy in val \n(z1,y)) in p1 z2. become more complicated with conditionals or case constructs. Consider the source expression \nIn order to make any further progress, we must use CC-LET to get let z1 . gy in let z2 . val (z1,y) \nin p1 z2.g .(g((fn x => case x of in1 x1 => (x1,x3)| in2 x2 => g .. x) y)) Now we can apply \u00df-LET and \n\u00df-PAIR to get let z1 . gy in z1 Its translation into .mon is which further reduces by .-LET to gy. .. \nlet z . (.x.case x of in1 x1.val (x1,x3) . in2 x2.g x) y in Solution 1: Use CPS. Now take the original \nsource expression let z . . gz in g . z . . and translate it into our CPS-based language, with k representing \nThis reduces by \u00df-FUN tothe enclosing continuation. let f = .j1 x. let z . (case y of in1 x1.val (x1,x3) \n. in2 x2.g .. y) in (letcont j2 z1 =(letval z2 =(z1,x) in j1 z2) in gj2 x) let z . . gz in g . z . . \nin letcont j3 z3 =(let z4 = p1 z3 in kz4) At this point, we want to .oat the case expression out of the \nlet. in fj3 y The proof-theoretic commuting conversion that expresses this Applying \u00df-FUN-LIN gives \nthe following, with substitutions high-rewrite is lighted: let x . (case v of in1 x1.M1 . in2 x2.M2) \nin N letcont j3 z3 =(let z4 = p1 z3 in kz4) . in letcont j2 z1 =(letval z2 =(z1,y ) in j3 z2) in gj2 \ny case v of in1 x1.(let x . M1 in N) . in2 x2.(let x . M2 in N) This can have the effect of exposing \nmore redexes; unfortunately, and by \u00df-CONT-LIN on j3 we get it also duplicates N which is not so desirable. \nSo instead, compil\u00adletcont j2 z1 = ers typically adopt a variation of this commuting conversion that \n(letval z2 =(z1,y) in let z4 = p1 z2 in kz4) shares M between the branches, creating a so-called join \npoint in gj2 y. function: Finally, use of \u00df-PAIR and DEAD-VAL produces letcont j2 z1 = CC-CASE let x \n. (case v of in1 x1.M1 . in2 x2.M2) in N kz1 in gj2 y which reduces by .-CONT to gky. All reductions \n. let f . val .x.N were simple uses of \u00df and . rules, without the need for the addi\u00ad in case v of in1 \nx1.let x . M1 in fx tional administrative reduction CC-LET. . in2 x2.let x . M2 in fx Applying this to \nour example produces the result let f . val (.z.let z . . gz in g . z .) in case xof in1 x1.(let z . \nval (x1,x3) in fz) . in1 x2.(let z . g .. xin fz). As observed earlier, join points such as f are just \ncontinuations. Solution 3: Use CPS. Consider the CPS transformation of the original source expression, \nwith k being the enclosing return con\u00adtinuation. letcont j. z . = g . kz. in letcont jz = gj. z in letval \nf = .j.. x. (letcont k1 x1 =(letval z =(x1,x3) in j.. z ) in .. j.. letcont k2 x2 = gxin case xof k1 \n. k2) in fjy Applying \u00df-FUN-LIN immediately produces the following term, with substitutions highlighted: \nletcont j. z . = g . kz. in letcont jz = gj. z in letcont k1 x1 =(letval z .. =(x1,x3) in jz..) in letcont \nk2 x2 = g .. jy in case y of k1 . k2 There is no need to apply anything analogous to CC-CASE,or to introduce \na join point: the original term already had one, namely j, which was substituted for the return continuation \nj.. of the function. The absence of explicit join points in monadic languages is an annoyance in itself. \nBy representing join points as ordinary functions, it is necessary to perform a separate static analysis \nto determine that such functions can be compiled ef.ciently as basic blocks. Explicitly named local continuations \nin CPS have the advantage that locality is immediate from the syntax, and preserved under transformation; \nfurthermore traditional intra-procedural compiler optimizations (such as those performed on SSA representations) \ncan be adapted to operate on functions in CPS form.  2.4 Comparison with ANF Flanagan et al. (1993) \npropose an alternative to CPS which they call A-Normal Form, or ANF for short. This is de.ned as the \nimage of the composition of the CPS, administrative normalization and inverse CPS transformations. CS \n CPS . A\u00df-normalization A(CS) . un-CPS The source language CS is Core Scheme (corresponding to our \nfragment of ML), and their CPS transformation composed with \u00df\u00adnormalization is equivalent to our one-pass \ntransformation .\u00b7. of Figure 4. The language A(CS) corresponds precisely to CC-LET/CC-CASE normal forms \nin .mon. We can express these normal forms Instead of going via a CPS language, the transformation into \nANF can be performed in one pass, as suggested by the dotted line Ain the diagram above.1 A similar transformation \nhas been studied by Danvy (2003). by a grammar: ATm . A,B ACmp . R AVal . v,w ::= ::= ::= R | let x. \nR in A | case v of in1 x1.A1 . in2 x2.A2 vw | pi v | v x| .x.A| (v,w) | ini v | ()  As Flanagan et al. \n(1993) suggest, the back end of an A-normal form compiler can employ the same code generation techniques \nthat a CPS compiler uses . However, as we mentioned in the In\u00adtroduction, it is not so apparent whether \nANF is ideally suited to optimization. After all, it is not even closed under the usual rule for \u00df reduction \n(.x.A) v . A[v/x]. As Sabry and Wadler (1997) later explained, it is necessary to combine substitution \nwith re-normalization to get a sound rule for \u00df-reduction: essentially the repeatedapplicationof CC-LET.Theydonotconsiderconditionals \nor case constructs, but presumably to maintain terms in ANF in it is necessary to normalize with respect \nto CC-LET and CC-CASE following function inlining. It is clear, then, that ANF suffers all the same problems \nthat af\u00adfect monadic languages: the need for (non-shrinking) commuting conversions, quadratic blowup \nof linear reductions, and the ab\u00adsence of explicit join points.  3. Typed CPS with exceptions We now \nadd types and other features to the language of Section 2. In the untyped world, we can model recursion \nusing a call-by-value .xed-point combinator. For a typed language, we must add ex\u00adplicit support for \nrecursive functions which, in any case, is more practical. Moreover, we would like to express recursive \ncontinu\u00adations too, in order to represent loops. Finally, to support excep\u00adtions, functions in the extended \nlanguage take two continuations: an exception-handler continuation, and a return continuation. This is \nthe so-called double-barrelled continuation-passing style (Thi\u00adelecke 2002). Figure 7 presents the syntax \nand typing rules for the extended language .T CPS. Types of values are ranged over by t, sand include \nunit, a type of exceptions, products, sums and functions. (To save space, we omit constructs for manipulating \nexception values.) Con\u00adtinuation types have the form \u00act which is interpreted as continua\u00adtions accepting \nvalues of type t . Note that for simplicity of presen\u00adtation we do not annotate terms with types; it \nis an easy exercise to add suf.cient annotations to determine unique typing derivations. Typing judgments \nfor values have the form G . V : t in which G maps variables to value types. Judgments for terms have \nthe form G; . . K ok in which the additional context . maps continua\u00adtion variables to continuation types. \nComplete programs are typed in the context of a single top-level continuation halt accepting unit values. \nWe consider each construct in turn. The letval construct is as before, with the obvious typing rule \nand associated value typing rules. Likewise for projections.  The letcont construct is generalized to \nsupport mutually recur\u00adsive continuations. These represent loops directly. Local con\u00adtinuations are also \nused for exception handlers.  The letfun construct introduces a set of mutually recursive functions; \neach function takes a return continuation k, an excep\u00adtion handler continuation h, and an argument x. \nAs a language construct, there is nothing special about the handler continua\u00adtion except that its type \nis .xed to be \u00acexn, and so a function type t . s is constructed from the argument type t and the type \n\u00acs of the return continuation. What really distinguishes  1 Though, curiously, the A-normalization algorithm \nin (Flanagan et al. 1993, Fig. 9) does not actually normalize terms, as it leaves let-bound conditionals \nalone. Grammar (value types) t,s ::= unit | exn | t \u00d7 s | t + s | t . s (values) CVal . V,W ::= () \n| (x,y) | ini x (terms) CTm . K,L ::= letval x= V in K | let x= pi xin K | letcont C in K | letfun F \nin K | kx | fkhx | case xof k1 . k2 (function def.) FunDef . F ::= fkhx = K (cont. def.) ContDef . C \n::= kx= K  Variables x:t . G k:\u00act . . (var) (contvar) G . x: t . . k : \u00act  Well-typed terms {G,xi:ti;.,k1:\u00act1,...,kn:\u00actn \n. Ki ok}1.i.n G; .,k1:\u00act1,...,kn:\u00actn . L ok (letc) G; . . letcont k1 x1 = K1,...,kn xn = Kn in L ok \n{G,xi:ti,f1:t1 . s1,...,fn:tn . sn; ki:\u00acsi,hi:\u00acexn . Ki ok}1.i.n G,f1:t1 . s1,...,fn:tn . sn;. . L ok \n(letrec) G; . . letfun f1 k1 h1 x1 = K1,...,fn kn hn xn = Kn in L ok G . V : t G,x:t;. . K ok G . x: \nt . . k : \u00act G . x: t1 \u00d7 t2 G,y:ti;. . K ok (letv) (appc) (proj) i . 1,2 G; . . letval x= V in K ok G; \n. . kxok G; . . let y . pi xin K ok G . x: t1 + t2 . . k1 : \u00act1 . . k2 : \u00act2 G . f : t . s . . k : \u00acs \n. . h: \u00acexn G . x: t (case) (app) G; . . case xof k1 . k2 ok G; . . fkhxok Well-typed values Well-typed \nprograms G . x: t G . y : s G . x: ti (pair) (tag) i . 1,2 (unit) (prog) G . (x,y): t \u00d7 s G . ini x: \nt1 + t2 G . () : unit {}; halt:\u00acunit . K ok Figure 7. Syntax and typing rules for typed language .T CPS \nexceptions is (a) their role in the translation from source lan\u00adguage into CPS, and (b) typical strategies \nfor generating code. Continuation application kx is as before. Now there are four possibilities for \nk: it may be a recursive or non-recursive occur\u00adrence of a letcont-bound continuation, compiled as a \njump, it may be the return continuation, or it may be a handler continu\u00adation, which is interpreted as \nraising an exception.  Function application fkhx includes a handler continua\u00adtion argument h.If k is \nthe return continuation for the near\u00adest enclosing function, and h is its handler continuation, then \nthe application is a tail call. If k is a local continuation and h is the handler continuation for the \nenclosing function, then the application is a non-tail call without an explicit excep\u00adtion handler so \nexceptions are propagated to the context. Otherwise, h is an explicit handler for exceptions raised by \nthe function. (Other combinations are possible; for example in letfun fkhx = C[ghhy] in K the function \napplication is essentially raise (g y) in a tail position.)  Branching using case is as before.  3.1 \nCPS transformation We can extend the fragment of ML described in Section 2.1 with exceptions and recursive \nfunctions: ML . e ::= ...| raise e| e1 handle x=> e2 | let fun din eend MLDef . d ::= fx = e The revised \nCPS transformation is shown in Figure 8 (see (Kim et al. 1998) for the selective use of a double-barrelled \nCPS trans\u00adformation). Both .\u00b7. and .\u00b7. take an additional argument: a contin\u00aduation hfor the exception \nhandler in scope. Then raise eis trans\u00adlated as an application of h.For e1 handle x=> e2 a local handler \ncontinuation h. is declared whose body is the translation of e2;this is then used as the handler passed \nto the translation function for e1.  3.2 Rewrites CPS, and extended with transformations such as loop \nunrolling : The rewrites of Figure 5 can be adapted easily to .T \u00df-REC letfun f1 k1 h1 x1 = C[fi khx] \nf2 k2 h2 x2 = K2 ... fn kn hn xn = Kn in K  . letfun f1 k1 h1 x1 = C[Ki[k/ki,h/hi,x/xi]] f2 k2 h2 x2 \n= K2 ... fn kn hn xn = Kn in K \u00df-RECCONT letcont k1 x1 = C[ki x] k2 x2 = K2 ... kn xn = Kn in K . letcont \nk1 x1 = C[Ki[x/xi]] k2 x2 = K2 ... kn xn = Kn in K There are no special rewrites for exception handling, \ne.g. corre\u00adsponding to (raise M) handle x.N . let x . M in N.Stan\u00addard \u00df-reduction on functions and continuations \ngives us this for free. For example, the CPS transform of let fun fx = raise xin fy handle z => (z,z) \nend is letfun fk. h. x= h. x in letcont jz =(letval z . =(z,z) in kz.) in fkjy which reduces by \u00df-FUN \nand \u00df-CONT to letval z . =(y,y) in kz. . .\u00b7. : ML . CVar . (Var . CTm) . CTm .x. h. = .(x) .e1 e2. h. \n= .e1. h(.x1..e2. h(.x2.letcont kx= .(x) in x1 khx2)) .fn x=> e. h. = letfun fkh. x= .e. h. k in .(f) \n.(e1,e2). h. = .e1. h(.x1..e2. h(.x2.letval x=(x1,x2) in .(x))) .inie. h. = .e. h(.z.letval x= ini z \nin .(x)) .(). h. = letval x=() in .(x) .#ie. h. = .e. h(.z.let x. pi z in .(x)) .let val x= e1 in e2 \nend. h. = letcont jx = .e2. h. in .e1. hj .let fun d in eend. h. = letfun .d. in .e. h. .raise e. h. \n= .e. h(.z.hz) .e1 handle x=> e2. h. = letcont jx = .(x) in letcont h. x= .e2. hj in .e1. h. j .case \neof in1 x1 => e1| in2 x2 => e2. . = .e. h(.z.letcont jx = .(x) letcont k1 x1 = .e1. hj in letcont k2 \nx2 = .e2. hj in case z of k1 . k2) .\u00b7. : MLDef . FunDef .fx= e. = fkhx = .e. hk .\u00b7. : ML . CVar . CVar \n. CTm .x. hk = kx .e1 e2. hk = .e1. h(.x1..e2. h(.x2.x1 khx2)) .fn x=> e. hk = letval f = .jx..e. hj \nin kf .(e1,e2). hk = .e1. h(.x1..e2. h(.x2.letval x=(x1,x2) in kx)) .inie. hk = .e. h(.z.letval x= ini \nz in kx) .(). hk = letval x=() in kx .#ie. hk = .e. h(.z.let x. pi z in kx) .let val x= e1 in e2 end. \nhk = letcont jx = .e2. hk in .e1. hj .let fun din eend. hk = letfun .d. in .e. hk .raise e. hk = .e. \nh(.z.hz) .e1 handle x=> e2. hk = letcont h. x= .e2. hk in .e1. h. k .case eof in1 x1 => e1| in2 x2 => \ne2. hk = .e. h(.z.letcont k1 x1 = .e1. hk in letcont k2 x2 = .e2. hk in case z of k1 . k2) Figure 8. \nTail CPS transformation for .T CPS Likewise, commuting conversions are not required, in contrast with \nmonadic languages, where in order to de.ne well-behaved conversions it is necessary to generalize the \nusual M handle x . N construct to try y . M in N1 unless x. N2, incorporating a success continuation \nN1 (Benton and Kennedy 2001).  3.3 Other features It is straightforward to extend .T with other features \nuseful for CPS compiling full-scale programming languages such as Standard ML. Recursive types of the \nform \u00b5a.t can be supported by adding suitable introduction and elimination constructs: a value fold x \nand a term let x= unfold y inK.  Binary products and sums generalize to the n-ary case. For opti\u00admizing \nrepresentations it is common for intermediate languages to support functions with multiple arguments \nand results, and constructors taking multiple arguments. This is easy: function de.nitions have the form \nfkhx = K, and continuations have the form kx = K and are used for passing multiple results and for case \nbranches where the constructor takes multiple ar\u00adguments.  Polymorphic types of the form .a.t can be \nadded. Typing con\u00adtexts are extended with a set of type variables V. Then to sup\u00adport ML-style let-polymorphism, \neach value binding construct (letval, letfun, and projection) must incorporate polymorphic  generalization. \nFor example: V,a;G . V : t V;G,x:.a.t;. . K ok (letv) V;G;. . letval x= V in K ok For elimination, we \nsimply adapt the variable rule (var) to incorporate polymorphic specialization: x:.a.t . G (var) G . \nx: t[s/a] 3.4 Effect analysis and transformation The use of continuations in an explicit handler-passing \nstyle lends itself very nicely to an effect analysis for exceptions. Suppose, for simplicity, that there \nare a .nite number of exception constructors ranged over by E. We make the following changes to .T CPS: \n We introduce exception set types of the form {E1,...,En}, representing exception values built with any \nof the construc\u00adtors E1,...,En. Set inclusion induces a subtype ordering on exception types, with top \ntype exn representing any exception, and bottom type {} representing no exception.  The type of handler \ncontinuations in function de.nitions are re.ned to describe the exceptions that the function is permitted \nto throw. For example: (1) letfun fk(h:\u00ac{}) x= K in ... (2) letfun fk(h:\u00acexn) x= K in ... (3) letfun \nfk(h:\u00ac{E,E.}) x= K in ...   The type of (1) tells us that K never raises an exception, in (2) the function \ncan raise any exception, and in (3) the function might raise E or E. . Now that handlers are annotated \nwith more precise types, the function types must re.ect this too. We write t.s. s for the type of functions \nthat either return a result of type sor raise an exception of type s. <:exn. Subtyping on function types \nand continuation types is speci.ed by the following rules: t2 <:t1 s1 <:s2 s1 . <:s2 . s2 <:s1 t1.s1 \n. s1 <:t2.s2 . s2 \u00acs1 <:\u00acs2 Exception effects enable effect-speci.c transformations (Benton and Buchlovsky \n2007). Suppose that the type of f is t .{E1} s. Then we can apply a dead-handler rewrite on the following: \nletcont h:\u00ac{E1,E2} x=(case xof E1.k1 . E2.k2)in fkhy . letcont h:\u00ac{E1} x=(case xof E1.k1)in fkhy In fact, \nthere is nothing exception-speci.c about this rewrite: it is just employing re.ned types for constructed \nvalues. The use of continuations has given us exception effects for free .   4. Implementing CPS Many \ncompilers for functional languages represent intermediate language terms in a functional style, as instances \nof an algebraic datatype of syntax trees, and manipulate them functionally. For ex\u00adample, the language \n.U CPS can be implemented by an SML datatype, here using integers for variables, with all bound variables \ndistinct: type Var = int and CVar = int datatype CVal = Unit |Pair of Var *Var |Inj of int * Var | Lam \nof CVar * Var * CTm and CTm = LetVal of Var * CVal * CTm | LetProj of Var * int * Var * CTm | LetCont \nof CVar * Var * CTm * CTm | AppCont of CVar * Var | App of Var * CVar * Var | Case of Var * CVar * CVar \nRewrites such as those of Figure 5 are then implemented by a function that maps terms to terms, applying \nas many rewrites as possible in a single pass. Here is a typical fragment that applies the \u00df-PAIR and \nDEAD-VAL reductions: fun simp census env S K = case K of LetVal(x, V, L) => if count(census,x) = 0 (* \nDead-Val *) then simp census env S L else LetVal(x, simpVal census env S V, simp census (addEnv(env,x,V)) \nS L) | LetProj(x, 1, y, L) => let val y = applySubst S y in case lookup(env, y ) of (* Beta-Pair *) Pair(z,_) \n=> simp census env (extendSubst S (x,z)) L |_ => LetProj(x, 1, y , simp census env S L) end In addition \nto the term K itself, the simpli.er function simp takes a parameter env that tracks letval bindings, \na parameter S used to substitute variables for variables and a parameter census that maps each variable \nto the number of occurrences of the vari\u00adable, computed prior to applying the function. The census becomes \nout-of-date as reductions are applied, and this may cause reductions to be missed until the census is \nrecalcu\u00adlated and simp applied again. For example, the \u00df-PAIR reduction may trigger a DEAD-VAL in an \nenclosing letval binding (consider letval x =(y1,y2)in ...let z =p1 xin ...where xoccurs only once). \nMaintaining accurate census information as rewrites are per\u00adformed can increase the number of reductions \nperformed in a single pass (Appel and Jim 1997), but even with up-to-date census infor\u00admation, it is \nnot possible to perform shrinking reductions exhaus\u00adtively in a single pass, so a number of iterations \nmay be required be\u00adfore all redexes have been eliminated. In the worst case, this leads to O(n 2)behaviour. \nWhat s more, each pass essentially copies the entire term, leav\u00ading the original term to be picked up \nby the garbage collector. This can be expensive. (Nonetheless, the simplicity of our CPS lan\u00adguage, with \nsubstitutions only of variables for variables, and the lack of commuting conversions as are required \nin ANF or monadic languages, leads to a very straightforward simpli.er algorithm.) 4.1 Graphical representation \nof terms An alternative is to represent the term using a graph, and to perform rewrites by destructive \nupdate of the graph. Appel and Jim (1997) devised a representation for which exhaustive application of \nthe shrinking \u00df-reductions of Figure 5 takes time linear in the size of the term. We improve on their \nrepresentation to support ef.cient .\u00adreductions and other transformations. The representation has three \ningredients. 1. The term structure itself is a doubly-linked tree. Every subterm has an up-link to its \nimmediately enclosing term. This supports constant time replacement, deletion, and insertion of subterms. \n 2. Each bound variable contains a link to one of its free occur\u00adrences, or is null if the variable is \ndead, and the free occurrences themselves are connected together in a doubly-linked circular list. This \npermits the following operations to be performed in constant time:  Determining whether a bound variable \nhas zero, one, or more than one occurrence, and if it has only one occurrence, locating that occurrence. \n Determining whether a free variable is unique.  Merging two occurrence lists.   Furthermore, we \nseparate recursive and non-recursive uses of variables; in essence, instead of letfun fkhx =K in L we \nwrite let f = rec gkhx.K[g/f] in L. This lets us detect DEAD-.and \u00df-.-LIN reductions. 3. Free occurrences \nare partitioned into same-binder equivalence classes by using the union-.nd data structure (Cormen et \nal. 2001)2. The representative in each equivalence class (that is, the root of the union-.nd tree) is \nlinked to its binding occurrence. This supports amortized near-constant time access to the binder (the \n.nd operation) and merging of occurrence lists (the union operation). Substitution of variable x for \nvariable y is implemented in near\u00adconstant time by (a) merging the circular lists of occurrences so that \nx now points to the merged list, and (b) applying a union operation so that the occurrences of y are \nnow associated with the binder for x. Consider the following value term, with doubly-linked tree structure \nand union-.nd structure implicit but with binder-to-free 2 Readers familiar with type inference may recall \nthat union-.nd underpins the almost-linear time algorithm for term uni.cation (Baader and Nipkow 1998). \npointer shown as a dotted arrow and circular occurrence lists shown as solid arrows: .kx . let p =( . \nx,y ) in ... p . ... . p . ... . ....... ... let z = p1 . p . in ... z . ... . z ... . p Now suppose \nthat we wish to apply \u00df-PAIR to the projection p1p. Using the .nd operation on the union-.nd structure \nwe can locate the pair (x,y)in near constant time. Now we substitute xfor z by disconnecting z s binder \nfrom its circular list and connecting x s occurrence list in its place, and merging the two lists, in \nconstant time. At the same time, we apply the union operation to merge the binder equivalence classes \n(not shown). .kx . let p =( . x,y ) in ...p ... p . . ... . ........ ... let z = ..p1 . p in ... x . \n... . x ... . p Finally we remove the projection itself, deleting the occurrence of p from the circular \nlist, again in constant time: .kx . let p =( . . x,y ) in ...p ...p . . ... ... x . ... . x ... . p One \nissue remains: the classical union-.nd data structure does not support deletion. There are recent techniques \nthat extend union-.nd with amortized near-constant time deletion (Kaplan et al. 2002). However, the representation \nis non-trivial, and might add unaccept\u00adable overhead to the union and .nd operations, so we chose instead \na simpler solution: do nothing! Deleted occurrences remain in the union-.nd data structure, possibly \nas root nodes, or as nodes on the path to the root. In theory, the ef.ciency of rewriting is then depen\u00addent \non the peak size of the term, not its current size, but we have not found this to be a problem in practice. \nEach of the shrinking reductions of Figure 5 can be imple\u00admented in almost-constant time using our graph \nrepresentation. To put these together and apply them exhaustively on a term, we fol\u00adlow Appel and Jim \n(1997): First sweep over the term, detecting redexes and collecting them in a worklist.  Then pull \nitems off the worklist one at a time (in any order), applying the appropriate rewrite, and adding new \nredexes to the worklist that are triggered by the rewrite. For example, the removal of a free occurrence \n(as can happen for multiple variables when applying DEAD-VAL) can induce a DEAD-. reduction (if no occurrences \nremain) or a \u00df-.-LIN reduction (if only a single occurrence remains).  In the current implementation, \nthe worklist is represented as a queue, but it should be possible to thread it through the term itself. \nShrinking reductions could then be performed with constant space overhead.  4.2 Comparison with Appel/Jim \nThe representation of Appel and Jim (1997) did not make use of union-.nd to locate binders. Instead, \n(a) the circular list of variable occurrences included the bound occurrence, thus giving constant time \naccess to the binder in the case that the free variable is unique, and (b) for letval-bound variables, \neach free occurrence contained an additional pointer to its binder. When performing a substitution operation, \nthese binder links must be updated, using time linear in the number of occurrences; fortunately, for \nany particular variable this can happen only once during shrinking reductions, as letval\u00adbound variables \ncannot become rebound. Thus the cost is amortized across the shrinking reductions. Unfortunately the \nlack of binder occurrences for non-letval\u00adbound variables renders less ef.cient other optimizations such \nas .-reduction. Take an instance of .-PAIR: let x1 =p1 xin C[let x2 =p2 xin C.[letval y =(x1,x2)in K]] \n. let x1 =p1 xin C[let x2 =p2 xin C.[K[x/y]]] Just to locate the binder for x1 and x2 would take time \nlinear in the number of occurrences. Our use of union-.nd gives us ef.cient implementation of all shrinking \nreductions, and of other transformations too; moreover, when analysing ef.ciency we need not be concerned \nwhether vari\u00adables are letval-bound or not.  4.3 Performance results We have modi.ed the SML.NET compiler \nto make use of a typed CPS intermediate language only mildly more complex than that shown in Figure 7. \nIt employs the graphical representation of terms described above; in particular, the simpli.er performs \nshrinking reductions exhaustively on a term representing the whole program, and it is invoked a total \nof 15 times during compilation. Table 1 presents some preliminary benchmark results show\u00ading average \ntime spent in simpli.cation, time spent in monomor\u00adphisation, and time spent in unit-removal (e.g. transformation \nof unit*int values to int). We compare (a) the released version of SML.NET, implementing a monadic intermediate \nlanguage (MIL) and functional-style simpli.cation algorithm, (b) the Appel/Jim\u00adstyle graph representation \nadapted to MIL terms implemented by Lindley (Benton et al. 2004a; Lindley 2005), and (c) the new graph\u00adbased \nCPS representation with union-.nd. Tests were run on a 3Ghz Pentium 4 PC with 1GB of RAM running Windows \nVista. The SML.NET compiler is implemented in Standard ML and com\u00adpiled using the MLton optimizing compiler, \nwhich generates high quality code from both functional and imperative coding styles so giving both techniques \na fair shot. As can be seen from the .gures, the graph-based simpli.er for the monadic language is signi.cantly \nfaster than the functional sim\u00adpli.er and although all times are small, bear in mind that the simpli.er \nis run many times during compilation. Unit removal is roughly comparable in performance across implementations. \nInter\u00adestingly, the graph-based CPS implementation of monomorphisa\u00adtion runs up to twice as slowly as \nthe functional monadic imple\u00admentation. We conjecture that this is because monomorphisation necessarily \ncopies (and specializes) terms, and CPS terms tend to be larger than MIL terms, and the graph representation \nis larger still. These .gures come with a caveat: the comparison is somewhat apples and oranges . There \nare differences between the MIL, g-MIL and g-CPS representations that are unrelated to monads or Table \n1. Optimization times (in seconds) Benchmark Lines Phase MIL g-MIL g-CPS raytrace 2,500 Simp 0.12 0.01 \n0.01 mlyacc 6,200 Simp 0.44 0.02 0.02 smlnet 80,000 Simp 7.29 0.29 0.15 Mono 0.75 n/a 1.41 Deunit 0.76 \n1.3 0.6 hamlet 20,000 Simp 0.97 0.08 0.04 Mono 0.15 n/a 0.19 Deunit 0.12 0.16 0.14 CPS. Future work \nis to make a fairer comparison, implementing a functional version of the CPS terms, and perhaps also \na precise monadic analogue.  5. Conti.cation Our CPS languages make a syntactic distinction between \nfunctions and local continuations. The former are typically compiled as heap\u00adallocated closures or as \nknown functions, whilst the latter can al\u00adways be compiled as inline code with continuation applications \ncompiled as jumps. For ef.ciency it is therefore desirable to trans\u00adform functions into continuations, \na process that has been termed conti.cation (Fluet and Weeks 2001). Functions can be conti.ed when they \nalways return to the same place. Consider the following code written in the subset of SML studied in \nSection 2: letfunfx =... in g(case d of in1 d1=> f y| in2 d2 => fd2) end If f returns at all, it must \npass control to g. Here, this is obvious, but for more complex examples it is not so apparent. Now consider \nits CPS transform: letval f =(.kx.\u00b7\u00b7\u00b7k\u00b7\u00b7\u00b7) in letcont k0 w = grw in letcont j1 d1 = fk0 y in letcont \nj2 d2 = fk0 d2 in case dof j1 . j2 It is clear that f is always passed the same continuation k0 and so, \nunless it diverges, it must return through k0 andsopass control to g. We can transform f into a local \ncontinuation, as follows: letcont k0 w = grw in letcont jx = \u00b7\u00b7\u00b7k0 \u00b7\u00b7\u00b7in letcont j1 d1 = jy in letcont \nj2 d2 = jd2 in case dof j1 . j2 We have done three things: (a) we have replaced the function f by a continuation \nj, deleting the return continuation at both de.nition and call sites, (b) we have substituted the argument \nk0 for the formal k in the body of f, and (c) we have moved j so that it is in the scope of k0. Fluet \nand Weeks (2001) use the dominator tree of a program s call graph to contify programs that consist of \na collection of mutually-recursive .rst-order functions. They show that their al\u00adgorithm is optimal: \nno conti.able functions remain after applying the transformation. Their dominator-based analysis can \nbe adapted to our CPS languages, and is simpler to describe in this context be\u00adcause all function de.nitions \nand uses have a named continuation (Fluet and Weeks use named continuations only for non-tail calls). \nWhen applied to top-level functions, the transformation is simpler too, but in the presence of .rst-class \nfunctions and general block structure the transformation becomes signi.cantly more complex to describe. \nWe prefer an approach based on incremental transformation, in essence repeatedly applying the rewrite \nillustrated above until no further rewrites are possible. We consider .rst the case of non\u00adrecursive \nfunctions, then generalize to mutually-recursive func\u00adtions, and conclude by relating our technique to \ndominator-based conti.cation. 5.1 Non-recursive functions In the untyped language .U without recursion, \nit is particularly CPS straightforward to spot conti.able functions: they are those for which all occurrences \nare applications with the same continuation argument. We de.ne the following rewrite: CONT (f not free \nin C, Dand Dminimal): letval f = .kx.K in C[D[fk0 x1,...,f k0 xn]] . C[letcont jx = K[k0/k] in D[jx1,...,j \nxn]] Here C is a single-hole context as presented in Figure 5 and Dis a multi-hole context whose formalization \nwe omit. The CONT rewrite combines three actions: (a) the function f is replaced by a continuation j, \nwith each application replaced by a continuation application; (b) the common continuation k0 is substituted \nfor the formal continuation parameter k in the body K of f; and (c) the new continuation j is pulled \ninto the scope of the continuation k0. The multi-hole context D is the smallest context enclosing all \nuses of f, which ensures that j is in scope after transformation. The analysis is trivial (just check \ncall sites for common continuation arguments), yet iterating this transformation leads to optimal conti.cation, \nin the sense of Fluet and Weeks (2001). Here is an example adapted from loc. cit. \u00a75.2, letval h = .kh \nxh.\u00b7\u00b7\u00b7in letval g1 = .k1 x1.\u00b7\u00b7\u00b7hk1 z1 \u00b7\u00b7\u00b7k1 z8 \u00b7\u00b7\u00b7in letval g2 = .k2 x2.\u00b7\u00b7\u00b7hk2 z2 \u00b7\u00b7\u00b7in letval f = .kf \nxf .\u00b7\u00b7\u00b7g1 kf z3 \u00b7\u00b7\u00b7g2 kf z4 \u00b7\u00b7\u00b7g2 kf z5 \u00b7\u00b7\u00b7in letval m= .km xm.\u00b7\u00b7\u00b7fj1 z6 \u00b7\u00b7\u00b7fj2 z7 in ... We can immediately \nsee that g1 and g2 (but not h)are always passed the same continuation kf , and so we can apply CONT to \ncontify them both: letval h = .kh xh.\u00b7\u00b7\u00b7in letval f = .kf xf . (letcont kg1 x1 = \u00b7\u00b7\u00b7hkf z1 \u00b7\u00b7\u00b7kf z8 \u00b7\u00b7\u00b7in \nletcont kg2 x2 = \u00b7\u00b7\u00b7hkf z2 \u00b7\u00b7\u00b7in \u00b7\u00b7\u00b7kg1 z3 \u00b7\u00b7\u00b7kg2 z4 \u00b7\u00b7\u00b7kg2 z5 \u00b7\u00b7\u00b7) in letval .mkm.xm \u00b7\u00b7\u00b7fj1 z6 \u00b7\u00b7\u00b7fj2 \nz7 = in ... Now hcan be conti.ed as it is always passed kf : letval f = .kf xf . (letcont kh xh = \u00b7\u00b7\u00b7in \nletcont kg1 x1 = \u00b7\u00b7\u00b7kh z1 \u00b7\u00b7\u00b7kf z8 in letcont kg2 x2 = \u00b7\u00b7\u00b7kh z2 \u00b7\u00b7\u00b7in \u00b7\u00b7\u00b7kg1 z3 \u00b7\u00b7\u00b7kg2 z4 \u00b7\u00b7\u00b7kg2 z5 \u00b7\u00b7\u00b7) \nin letval .mkm.xm \u00b7\u00b7\u00b7fj1 z6 \u00b7\u00b7\u00b7fj2 z7 = in ...  5.2 Recursive functions Generalizing to recursive functions \nand continuations is a little trickier. Suppose we have a .T CPS term of the form letfun f1 k1 h1 x1 \n= K1 \u00b7\u00b7\u00b7 fn kn hn xn = Kn in K. A set of functions F .{f1,...,fn}can be conti.ed collectively, written \nConti.able(F), if there is some pair of continuations k0 and h0 such that each occurrence of f . F is \neither a tail call within F or is a call with continuation arguments k0 and h0.In\u00adtuitively, each function \n(eventually) returns to the same place (k0), or throws an exception that is caught by the same handler \n(h0), though control may pass tail-recursively through other functions in F. There may be many such subsets \nF; we assume that F is in fact strongly-connected with respect to tail calls contained within it (or \nis a trivial singleton with no tail calls). Then for a given letfun function f with return continuation \nk,if f is passed around as a .rst-class value then create an edge from root to k;otherwise,for each application \nfjxcreate an edge from j to k. Finally, for each local continuation kcreate an edge from root to k. The \nnon-recursive CONT rewrite has the effect of merging two nodes in the graph, as follows: . .... . = k \n...... ...... ...... ...... . .... . . k0 k0 ...... ...... term there is a unique partial partition \nof the functions into disjoint subsets satisfying Conti.able(-). Let F = {f1,...,fm}. De.ne a translation \non function appli\u00ad ....... cations The recursive RECCONT and RECCONT2 rewrites are similar, except that \nin place of k we have a strongly-connected component ji x if f = fi .F (fkhx). = fkhx otherwise andextendthis \ntoall terms. Assuming that Conti.able(F) holds, {k1,...,km}. there are two possibilities. ....... .... \n. . k1 = ...... ...... ...... ...... . .... . . k0 ...... ...... . .... . . ... . k0 ki 1. All applications \nof the form fk0 h0 x for f . F are in the term K. Then we can apply the following rewrite, which is the \ndirect analogue of CONT. RECCONT (f1,...,fm not free in C,and K minimal):    .... . km letfun f1 \nk1 h1 x1 = K1 \u00b7\u00b7\u00b7 fn kn hn xn = Kn in C[K] . letfun fm+1 km+1 hm+1 xm+1 = Km+1 \u00b7\u00b7\u00b7 fn kn hn xn = Kn \nin C[letcont j1 x1 = K1 [k0/k1,h0/h1] = K. \u00b7\u00b7\u00b7 jm xmm[k0/km,h0/hm] in K] 2. Otherwise, all applications \nof the form fk0 h0 x for f . F are in the body of one of the functions outside of F; without loss of \ngenerality we assume this is fn. RECCONT2(f1,...,fm not free in C,and Kn minimal): letfun f1 k1 h1 x1 \n= K1 \u00b7\u00b7\u00b7 fn-1 kn-1 hn-1 xn-1 = Kn-1 fn kn hn xn = C[Kn] in K . letfun fm+1 km+1 hm+1 xm+1 = Km+1 \u00b7\u00b7\u00b7 \nfn-1 kn-1 hn-1 xn-1 = Kn-1 fn kn hn xn = C[letcont j1 x1 = K1 [k0/k1,h0/h1] \u00b7\u00b7\u00b7 jx= K. [k0/km,h0/hm] \nmm m in Kn] in K For an example of the latter, more complex, transformation, consider the following \nSML code: let fun unif(Ap(a,xs),Ap(b,ys)) = (unif(a,b);unifV(xs,ys)) | unif(Ar(a,b),Ar(c,d)) = unifV([a,b],[c,d]) \nand unifV(x::xs,y::ys) = (unif(x,y);unifV(xs,ys)) | unifV([],[]) = () in unif end The function unifyV \ncan be conti.ed into the de.nition of unif:it tail-calls itself, and its uses inside unif have the same \ncontinuation.  5.3 Comparing dominator-based conti.cation The dominator-based approach of Fluet and \nWeeks (2001) can be recast in our CPS language as follows. (For simplicity we do not consider exception \nhandler continuations here). First construct a continuation .ow graph for the whole program. Nodes consist \nof continuation variables and a distinguished root node. Then for each Conversely, any part of the .ow \ngraph matching the left-hand-side of this diagram corresponds to a conti.able subset of functions in \na letfun to which the RECCONT or RECCONT2 rules can be applied. It is immediately clear that exhaustive \nrewriting terminates, as the .ow graph decreases in size with each rewrite, eventually producing a graph \nwith no occurrences of the pattern above. The algorithm described by Fluet and Weeks (2001) conti.es \nk if it is strictly dominated by some continuation j whose immediate dominator is root. It can be shown \nthat if a rooted graph contains such a pair of nodes j and k, then some part of the graph matches the \npattern above. Hence exhaustive rewriting has the same effect as as optimal conti.cation based on dominator \ntrees.  6. Related work and conclusion The use of continuation-passing style for functional languages \nhas its origins in Scheme compilers (Steele 1978; Kranz et al. 1986). It later formed the basis of the \nStandard ML of New Jersey com\u00adpiler (Appel 1992; Shao and Appel 1995). In early compilers, lambdas originating \nfrom the CPS transfor\u00admation were not distinguished from lambdas present in the source, so some effort \nwas expended at code generation time to determine which lambdas could be stack-allocated and which could \nbe heap\u00adallocated. Later compilers made a syntactic distinction between true functions and second-class \ncontinuations introduced by CPS; and sometimes transformed one into the other (Kelsey and Hudak 1989), \nthough conti.cation was not studied formally. A number of more recent compilers use what has been called \nalmost CPS. The Sequentialized Intermediate Language (SIL) em\u00adployed by Tolmach and Oliva (1998) is a \nmonadic-style language in which a letcont-like feature is used to introduce join points. Some\u00adwhat closer \nto our CPS language is the First Order Language (FOL) of the MLton compiler (Fluet and Weeks 2001). It \ngoes further than SIL in making use of named local continuations in all branch con\u00adstructs and non-tail \ncalls. However, functions are not parameterized on return (or handler) continuations, and there is special \nsyntax for tail calls and returns. This non-uniform treatment of continuations complicates transformations \n inlining of non-tail functions must replace all return points with jumps, and the conti.cation analy\u00adsis \nand transformation must treat tail and non-tail calls differently. We have found the uniform treatment \nof continuations in our CPS language to be a real bene.t, not only as a simplifying force in implementation, \nbut also in thinking about compiler optimizations: conti.cation, in particular, is dif.cult to characterize \nin the absence of a notion of continuation passing. As far as we are aware, we are the .rst to implement \nlinear\u00adtime shrinking reductions in the style of Appel and Jim (1997). An earlier term-graph implementation \nby Lindley was for a monadic language and had worst-case O(n 2) behaviour due to commuting conversions \n(Benton et al. 2004a; Lindley 2005). Shivers and Wand (2005) have proposed a rather different graph representation \nfor lambda terms, with the goal of sharing subterms after \u00df-reduction. Their representation does bear \nsome resemblance to ours, though, with up-links from subterms to enclosing terms, and circular lists \nthat connect the sites where a term is substituted for a variable. This paper would not be complete without \na mention of Static Single Assignment form (SSA), the currently fashionable interme\u00addiate representation \nfor imperative languages. As is well known, SSA is in some sense equivalent to CPS (Kelsey 1995) and \nto ANF (Appel 1998). Its focus is intra-procedural optimization (as with ANF, it s necessary to renormalize \nwhen inlining functions, in contrast to CPS) and there is a large body of work on such op\u00adtimizations. \nFuture work is to transfer SSA-based optimizations to CPS. We conjecture that CPS is a good .t for both \nfunctional and imperative paradigms. Acknowledgments I would like to thank Nick Benton, Olivier Danvy, \nSam Lindley, Simon Peyton Jones and Claudio Russo for fruitful discussions on compiler intermediate languages. \nGeorges Gonthier suggested the use of union-.nd in the graphical representation of terms. References \nAndrew W. Appel. Compiling with Continuations. Cambridge University Press, 1992. Andrew W. Appel. SSA \nis functional programming. SIGPLAN Notices,33 (4):17 20, 1998. Andrew W. Appel and Trevor Jim. Shrinking \nlambda expressions in linear time. Journal of Functional Programming, 7(5):515 540, 1997. Franz Baader \nand Tobias Nipkow. Term Rewriting and All That. Cambridge University Press, 1998. Nick Benton and Peter \nBuchlovsky. Semantics of an effect analysis for exceptions. In ACM SIGPLAN International Workshop on \nTypes in Language Design and Implementation (TLDI), pages 15 26, 2007. Nick Benton and Andrew Kennedy. \nExceptional syntax. Journal of Func\u00adtional Programming, 11(4):395 410, 2001. Nick Benton, Andrew Kennedy, \nand George Russell. Compiling Standard ML to Java bytecodes. In 3rd ACM SIGPLAN International Conference \non Functional Programming. ACM Press, September 1998. Nick Benton, Andrew Kennedy, Sam Lindley, and Claudio \nRusso. Shrink\u00ading reductions in SML.NET. In 16th International Workshop on Imple\u00admentation and Application \nof Functional Languages (IFL), 2004a. Nick Benton, Andrew Kennedy, and Claudio Russo. Adventures in interop\u00aderability: \nThe SML.NET experience. In 6th International Conference on Principles and Practice of Declarative Programming \n(PPDP), 2004b. Thomas Cormen, Charles Leiserson, Ronald Rivest, and Clifford Stein. Introduction to Algorithms. \nMIT Press, second edition, 2001. Olivier Danvy. A new one-pass transformation into monadic normal form. \nIn 12th International Conference on Compiler Construction (CC 03), 2003. Olivier Danvy and Andrzej Filinski. \nRepresenting control: A study of the CPS transformation. Mathematical Structures in Computer Science,2 \n(4):361 391, 1992. Olivier Danvy and Lasse R. Nielsen. A .rst-order one-pass CPS transfor\u00admation. Theor. \nComput. Sci., 308(1-3):239 257, 2003. Cormac Flanagan, Amr Sabry, Bruce F. Duba, and Matthias Felleisen. \nThe essence of compiling with continuations (with retrospective). In McKinley (2004), pages 502 514. \nMatthew Fluet and Stephen Weeks. Conti.cation using dominators. In ICFP 01: Proceedings of the Sixth \nACM SIGPLAN International Con\u00adference on Functional Programming, pages 2 13. ACM Press, Septem\u00adber 2001. \nJohn Hatcliff and Olivier Danvy. A generic account of continuation-passing styles. In Principles of Programming \nLanguages (POPL), pages 458 471, 1994. Haim Kaplan, Nira Shafrir, and Robert E. Tarjan. Union-.nd with \ndeletions. In SODA 02: Proceedings of the thirteenth annual ACM-SIAM sympo\u00adsium on Discrete algorithms, \npages 19 28, Philadelphia, PA, USA, 2002. Society for Industrial and Applied Mathematics. ISBN 0-89871-513-X. \nRichard Kelsey. A correspondence between continuation passing style and static single assignment form. \nIn Intermediate Representations Workshop, pages 13 23, 1995. Richard A. Kelsey and Paul Hudak. Realistic \ncompilation by program transformation. In Principles of Programming Languages (POPL). ACM, January 1989. \nJung-taek Kim, Kwangkeun Yi, and Olivier Danvy. Assessing the overhead of ML exceptions by selective \nCPS transformation. In ACM SIGPLAN Workshop on ML, pages 112 119, 1998. Also appears as BRICS techni\u00adcal \nreport RS-98-15. David A. Kranz, Richard A. Kelsey, Jonathan A. Rees, Paul Hudak, and James Philbin. \nORBIT: an optimizing compiler for scheme. In Proceed\u00adings of the ACM SIGPLAN symposium on Compiler Construction, \npages 219 233, June 1986. Sam Lindley. Normalisation by evaluation in the compilation of typed func\u00adtional \nprogramming languages. PhD thesis, University of Edinburgh, 2005. Kathryn S. McKinley, editor. 20 Years \nof the ACM SIGPLAN Conference on Programming Language Design and Implementation 1979-1999, A Selection, \n2004. ACM. Eugenio Moggi. Notions of computation and monads. Information and Computation, 93:55 92, 1991. \nA. M. Pitts. Typed operational reasoning. In B. C. Pierce, editor, Advanced Topics in Types and Programming \nLanguages, chapter 7, pages 245 289. The MIT Press, 2005. Amr Sabry and Philip Wadler. A re.ection on \ncall-by-value. ACM Trans\u00adactions on Programming Languages and Systems (TOPLAS), 19(6):916 941, November \n1997. ISSN 0164-0925. Zhong Shao and Andrew W. Appel. A type-based compiler for Standard ML. In Proc. \n1995 ACM SIGPLAN Conference on Programming Lan\u00adguage Design and Implementation (PLDI), pages 116 129, \nLa Jolla, CA, Jun 1995. Olin Shivers. Higher-order control-.ow analysis in retrospect: lessons learned, \nlessons abandoned (with retrospective). In McKinley (2004), pages 257 269. Olin Shivers and Mitchell \nWand. Bottom-up \u00df-reduction: Uplinks and .-DAGs. In European Symposium on Programming (ESOP), pages 217 \n232, 2005. Guy L. Steele. RABBIT: A compiler for SCHEME. Technical Report AI\u00adTR-474, MIT, May 1978. Hayo \nThielecke. Comparing control constructs by double-barrelled CPS. Higher-Order and Symbolic Computation, \n15(2/3):141 160, 2002. Andrew P. Tolmach and Dino Oliva. From ML to Ada: Strongly-typed language interoperability \nvia source translation. Journal of Functional Programming, 8(4):367 412, 1998. Philip Wadler and Peter \nThiemann. The marriage of effects and monads. In ACM SIGPLAN International Conference on Functional Programming \n(ICFP), 1998.  \n\t\t\t", "proc_id": "1291151", "abstract": "<p>We present a series of CPS-based intermediate languages suitable for functional language compilation, arguing that they have practical benefits over direct-style languages based on <i>A</i>-normal form (ANF) or monads. Inlining of functions demonstrates the benefits most clearly: in ANF-based languages, inlining involves a re-normalization step that rearranges let expressions and possibly introduces a new 'join point' function, and in monadic languages, commuting conversions must be applied; in contrast, inlining in our CPS language is a simple substitution of variables for variables.</p> <p>We present a contification transformation implemented by simple rewrites on the intermediate language. Exceptions are modelled using so-called 'double-barrelled' CPS. Subtyping on exception constructors then gives a very straightforward effect analysis for exceptions. We also show how a graph-based representation of CPS terms can be implemented extremely efficiently, with linear-time term simplification.</p>", "authors": [{"name": "Andrew Kennedy", "author_profile_id": "81100450709", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "PP39043337", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291179", "year": "2007", "article_id": "1291179", "conference": "ICFP", "title": "Compiling with continuations, continued", "url": "http://dl.acm.org/citation.cfm?id=1291179"}