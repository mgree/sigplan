{"article_publication_date": "10-01-2007", "fulltext": "\n Stream Fusion From Lists to Streams to Nothing at All Duncan Coutts1 Roman Leshchinskiy2 Don Stewart2 \n1 Programming Tools Group 2 Computer Science &#38; Engineering Oxford University Computing Laboratory \nUniversity of New South Wales duncan.coutts@comlab.ox.ac.uk {rl,dons}@cse.unsw.edu.au Abstract This paper \npresents an automatic deforestation system, stream fu\u00adsion, based on equational transformations, that \nfuses a wider range of functions than existing short-cut fusion systems. In particular, stream fusion \nis able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing \nfea\u00adture of the framework is its simplicity: by transforming list func\u00adtions to expose their structure, \nintermediate values are eliminated by general purpose compiler optimisations. We have reimplemented the \nHaskell standard List library on top of our framework, providing stream fusion for Haskell lists. By \nal\u00adlowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion \nin typical Haskell programs. We present benchmarks documenting time and space improvements. Categories \nand Subject Descriptors D.1.1 [Programming Tech\u00adniques]: Applicative (Functional) Programming; D.3.4 \n[Program\u00adming Languages]: Optimization General Terms Languages, Algorithms Keywords Deforestation, program \noptimisation, program trans\u00adformation, program fusion, functional programming 1. Introduction Lists \nare the primary data structure of functional programming. In lazy languages, such as Haskell, lists also \nserve in place of tra\u00additional control structures. It has long been recognised that com\u00adposing list functions \nto build programs in this style has advantages for clarity and modularity, but that it incurs some runtime \npenalty, as functions allocate intermediate values to communicate results. Fusion (or deforestation) \nattempts to remove the overhead of pro\u00adgramming in this style by combining adjacent transformations on \nstructures to eliminate intermediate values. Consider this simple function which uses a number of interme\u00addiate \nlists: f :: Int . Int fn = sum [k * m | k . [1..n], m . [1..k]] Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 07, October 1 3, 2007, Freiburg, Germany. \nCopyright &#38;#169; 2007 ACM 978-1-59593-815-2/07/0010. . . $5.00.  No previously implemented short-cut \nfusion system eliminates all the lists in this example. The fusion system presented in this paper does. \nWith this system, the Glasgow Haskell Compiler (The GHC Team 2007) applies all the fusion transformations \nand is able to generate an ef.cient worker function f. that uses only unboxed integers (Int #) and runs \nin constant space: f. :: Int # . Int # f. n = let go :: Int # . Int # . Int # go z k = case k > n of \nFalse . case 1 > k of False . to (z + k)k (k + 1)2 True . go z (k + 1) True . z to :: Int # . Int # . \nInt # . Int # . Int # to zkk. m = case m > k of False . to (z +(k * m)) kk. (m + 1) True . go zk. in \ngo 01 Stream fusion takes a simple three step approach: 1. Convert recursive structures into non-recursive \nco-structures; 2. Eliminate super.uous conversions between structures and co\u00adstructures; 3. Finally, \nuse general optimisations to fuse the co-structure code.  By transforming pipelines of recursive list \nfunctions into non\u00adrecursive ones, code becomes easier to optimise, producing better results. The ability \nto fuse all common list functions allows the pro\u00adgrammer to write in an elegant declarative style, and \nstill produce excellent low level code. We can .nally write the code we want to be able to write without \nsacri.cing performance! 1.1 Short-cut fusion The example program is a typical high-level composition \nof list producers, transformers and consumers. However, extensive opti\u00admisations are required to transform \nprograms written in this style into ef.cient low-level code. In particular, naive compilation will produce \na number of intermediate data structures, resulting in poor performance. We would like to have the compiler \nremove these intermediate structures automatically. This problem, deforesta\u00adtion (Wadler 1990), has been \nstudied extensively (Meijer et al. 1991; Gill et al. 1993; Takano and Meijer 1995; Gill 1996; Hu et al. \n1996; Chitil 1999; Johann 2001; Svenningsson 2002; Gibbons 2004). To illustrate how our approach builds \non previous work on short-cut fusion, we review the main approaches. build/foldr The most practically \nsuccessful list fusion system to date is the build/foldr system (Gill et al. 1993). It uses two combina\u00adtors, \nfoldr and build , and a single fusion rule to eliminate adjacent occurrences of the combinators. Fusible \nfunctions must be written in terms of these two combinators. A range of standard list func\u00adtions, and \nlist comprehensions, can be expressed and effectively fused in this way. There are some notable exceptions \nthat cannot be effectively fused under build/foldr: left folds (foldl ) (functions such as sum that consume \na list using an accumulating parameter), and zips (functions that consume multiple lists in parallel). \ndestroy/unfoldr A more recent proposal (Svenningsson 2002) based on unfolds rather than folds addresses \nthese speci.c short\u00adcomings. However, as proposed, it does not cover functions that handle nested lists \n(such as concatMap) or list comprehensions, and there are inef.ciencies fusing .lter -like functions, \nwhich must be de.ned recursively. stream fusion Recently, we proposed a new fusion framework for operations \non strict arrays (Coutts et al. 2007). While the perfor\u00admance improvements demonstrated for arrays are \nsigni.cant, this previous work describes fusion for only relatively simple opera\u00adtions: maps, .lters \nand folds. It does not address concatenations, functions on nested lists, or zips. In this paper we extend \nstream fusion to .ll in the missing pieces. Our main contributions are: an implementation of stream \nfusion for lists (Section 2);  extension of stream fusion to zips, concats, appends (Section 3) and \nfunctions on nested lists (Section 4);  a translation scheme for stream fusion of list comprehensions \n(Section 5);  an account of the compiler optimisations required to remove intermediate structures produced \nby fusion, including functions on nested lists (Section 7);  an implementation of stream fusion using \ncompiler rewrite rules and concrete results from a complete implementation of the Haskell list library \n(Section 8).   2. Streams The intuition behind build/foldr fusion is to view lists as sequences represented \nby data structures, and to fuse functions that work directly on the natural structure of that data. The \ndestroy/unfoldr and stream fusion systems take the opposite approach. They convert operations over the \nlist data structure to instead work over the dual of the list: its unfolding or co-structure. In contrast \nto destroy/unfoldr, stream fusion uses an explicit rep\u00adresentation of the sequence co-structure: the \nStream type. Separate functions, stream and unstream , are used to convert between lists and streams. \n 2.1 Converting lists to streams The .rst step in order to fuse list functions with stream fusion is \nto convert a function on list structures to a function on stream co\u00adstructures (and back again) using \nstream and unstream combina\u00adtors. The function map, for example, is simply speci.ed as: map :: (a . b). \n[a]. [b] mapf = unstream \u00b7 maps f \u00b7 stream which composes a map function over streams, with stream conver\u00adsion \nto and from lists. While the natural operation over a list data structure is a fold, the natural operation \nover a stream co-structure is an unfold. The Stream type encapsulates an unfold, wrapping an initial \nstate and a stepper function for producing elements. It is de.ned as: data Stream a = .s. Stream (s \n. Step a s) s data Step a s = Done | Yield a s | Skip s Note that the type of the stream state is existentially \nquanti.ed and does not appear in the result type: the stream state is encapsu\u00adlated. The Stream data \nconstructor itself is similar to the standard Haskell list function unfoldr (Gibbons and Jones 1998), \nStream :: .sa. (s . Step a s) . s . Stream a unfoldr :: .sa. (s . Maybe (a,s)) . s . [a] Writing functions \nover streams themselves is relatively straight\u00adforward. map, for example, simply applies its function \nargument to each yielded stream element, when the stepper is called: maps :: (a . b). Stream a . Stream \nb maps f (Stream next0 s0)= Stream next s0 where next s = case next0 s of Done . Done ' Skip s' . Skip \ns ' Yield x s' . Yield (fx)s The stream function can be de.ned directly as a stream whose elements \nare those of the corresponding list. It uses the list itself as the stream state. It is of course non-recursive \nyielding each element of the list as it is unfolded: stream :: [a]. Stream a stream xs0 = Stream next \nxs0 where next [] = Done next (x : xs)= Yield x xs The unstream function unfolds the stream and builds \na list structure. Unfolding a stream to produce a list is achieved by repeatedly calling the stepper \nfunction of the stream, to yield the stream s elements. unstream :: Stream a . [a] unstream (Stream next0 \ns0)= unfold s0 where unfold s = case next0 s of Done . [] ' . ' Skip sunfold s ' Yield x s' . x : unfold \ns In contrast to unfoldr , the Stream stepper function has one other alternative, it can Skip, producing \na new state but yielding no new value in the sequence. This is not necessary for the semantics but as \nwe shall show later, is crucial for the implementation. In particular it is what allows all stepper functions \nto be non-recursive. 2.2 Eliminating conversions Writing list functions using compositions of stream \nand unstream is clearly inef.cient: each function must .rst construct a new Stream, and when it is done, \nunfold the stream back to a list. This is evident in the de.nition of map from the previous sec\u00adtion. \nInstead of consuming and constructing a list once: stream consumes a list, allocating Step constructors; \nmaps consumes and allocates more Step constructors; .nally, unstream consumes the Step constructors and \nallocates new list nodes. However, if we compose two functions implemented via streams: map f \u00b7 mapg \n= unstream \u00b7 maps f \u00b7 stream \u00b7 unstream \u00b7 maps g \u00b7 stream we immediately see an opportunity to eliminate \nthe intermediate list conversions! Assuming stream \u00b7 unstream as the identity on streams, we obtain the \nrewrite rule: (stream/unstream fusion). s . stream (unstream s) . s The Glasgow Haskell Compiler supports \nprogrammer-de.ned rewrite rules (Peyton Jones et al. 2001), applied by the compiler during compilation. \nWe can specify the stream fusion rule as part of the list library source code without changing the compiler. \nWhen the compiler applies this rule to our example, it yields: unstream \u00b7 maps f \u00b7 maps g \u00b7 stream Our \npipeline of list transformers has now been transformed into a pipeline of stream transformers. Externally, \nthe pipeline still con\u00adsumes and produces lists, just as the direct list implementation of map \u00b7 map \ndoes. However, internally the maps f \u00b7 maps g pipeline is the composition of (simple, non-recursive) \nstream func\u00adtions. It is interesting to note that the stream /unstream rule is not really a classical \nfusion rule at all. It only eliminates the list allo\u00adcations that were introduced in converting operations \nto work over streams.  2.3 Fusing co-structures Having converted the functions over list structures \ninto functions over stream co-structures, the question now is how to optimise away intermediate Step \nconstructors produced by composed func\u00adtions on streams. The key trick is that all stream producers are \nnon-recursive. Once list functions have been transformed to compositions of non-recursive stepper functions, \nthere is an opportunity for real fusion: the compiler can relatively easily eliminate intermediate Step \nconstructors produced by the non-recursive steppers, using existing general purpose optimisations. We \ndescribe this process in detail in Section 7.  3. Writing stream combinators Figure 1 shows the de.nitions \nof several standard algorithms on .at streams which we use throughout the paper. For the most part, these \nde.nitions are essentially the same as those presented in our previous work (Coutts et al. 2007). In \nthe following, we discuss some of the combinators and highlight the principles underlying their implementation. \nNo recursion: .lter Similarly to maps, the stepper function for .lters is non-recursive which is crucial \nfor producing ef.cient fused code. In the case of .lters, however, a non-recursive imple\u00admentation is \nonly possible by introducing Skip in place of elements that are removed from the stream the only alternative \nis to recur\u00adsively consume elements from the input stream until we .nd one that satis.es the predicate \n(as is the case for the .lter function in the destroy/unfoldr system). As we are able to avoid this recursion, \nwe maintain trivial control .ow for streams, and thus never have to see through .xed points to optimise, \nyielding better code. Consuming streams: fold The only place where recursion is al\u00adlowed is when we consume \na stream to produce a different type. The canonical examples of this are foldrs and foldls which are \nde\u00ad.ned in Figure 1. To understand this it is helpful to see composi\u00adtions of stream functions simply \nas descriptions of pipelines which on their own do nothing. They require a recursive function at the \nend of the pipeline to unroll sequence elements, to actually con\u00adstruct a concrete value. Recursion is \nthus only involved in repeatedly pulling values out of the stream. Each step in the pipeline itself requires \nno recursion. Of course because of the possibility that a single step might skip it may take many steps \nto actually yield a value. Complex stream states: append Many operations on streams en\u00adcode complex control \n.ow by using non-trivial state types. One .lters :: (a . Bool). Stream a . Stream a .lters p (Stream \nnext0 s0)= Stream next s0 where next s = case next0 s of Done . Done ' ' Skip s . Skip s ' Yield x s \n' | px . Yield x s ' | otherwise . Skip s returns :: a . Stream a returns x = Stream next True where \nnext True = Yield x False next False = Done  enumFromTos :: Enum a . a . a . Stream a enumFromTos lh \n= Stream next l where next n |n > h = Done |otherwise = Yield n (succ n )  foldrs :: (a . b . b). b \n. Stream a . b foldrs fz (Stream next s0)= go s0 where go s = case next s of Done . z '' Skip s . go \ns ' Yield x s . fx (go s ' ) foldls :: (b . a . b). b . Stream a . b foldls fz (Stream next s0)= go \nz s0 where go z s = case next s of Done . z '' Skip s . goz s '' Yield x s . go (fzx)s appends :: Stream \na . Stream a . Stream a appends (Stream nexta sa0)(Stream nextb sb0)= Stream next (Left sa0) where next \n(Left sa )= case nexta sa of Done . Skip (Right sb0) '' Skip s . Skip (Left s ) aa '' Yield x s . Yield \nx (Left s ) aa next (Right sb)= case nextb sb of Done . Done ' ' Skip sb . Skip (Right s b) ' ' Yield \nx s b . Yield x (Right s b) zips :: Stream a . Stream b . Stream (a, b) zips (Stream nexta sa0)(Stream \nnextb sb0)= Stream next (sa0, sb0, Nothing ) where next (sa , sb, Nothing )= case nexta sa of Done . \nDone '' Skip s . Skip (s , sb, Nothing ) aa '' Yield a s . Skip (s , sb, Just a ) aa ' next (s , sb, \nJust a )= a case nextb sb of Done . Done ' '' Skip s . Skip (s , sb, Just a ) ba ' '' Yield b s . Yield \n(a, b)(s , sb, Nothing ) ba Figure 1: Flat stream combinators concatMaps :: (a . Stream b). Stream a \n. Stream b concatMaps f (Stream nexta sa0)= Stream next (sa0, Nothing ) where next (sa , Nothing )= case \nnexta sa of Done . Done '' Skip s . Skip (s , Nothing ) aa '' Yield a s . Skip (s , Just (fa)) aa next \n(sa , Just (Stream nextb sb)) = case nextb sb of Done . Skip (sa, Nothing ) '' Skip sb . Skip (sa, Just \n(Stream nextb sb)) '' Yield b sb . Yield b (sa, Just (Stream nextb sb)) Figure 2: De.nition of concatMaps \non streams example is appends which produces a single stream by concate\u00adnating two independent streams, \nwith possibly different state types. The state of the new stream necessarily contains the states of the \ntwo component streams. To implement concatenation we notice that at any moment we need only the state \nof the .rst stream, or the state of the second. The next function for append thus operates in two modes, \neither yield\u00ading elements from the .rst stream, or elements from the second. The two modes can then be \nencoded as a sum type, Either sa sb, tagging which mode the stepper is in: either yielding the .rst stream, \nor yielding the second. The modes are thus represented as Left sa or Right sb and there is one clause \nof next for each. When we get to the end of the .rst stream we have to switch modes so that we can start \nyielding elements from the second stream. This is another example where it is convenient to use Skip. \nIn\u00adstead of immediately having to yield the .rst element of the second stream (which is impossible anyway \nsince the second stream may skip) we can just transition into a new state where we will be able to do \nso. The rule of thumb is in each step to do one thing and one thing only. What is happening here of course \nis that we are using state to encode control .ow. This is the pattern used for all the more complex stream \nfunctions. Section 7.1 explains how code in this style is optimised. Consuming multiple streams: zip \nFunctions that consume mul\u00adtiple stream in parallel, such as zips, also require non-trivial state. Unsurprisingly \nthe de.nition of zips on streams is quite similar to the equivalent de.nition in the destroy/unfoldr \nsystem. The main difference is that the stream version has to cope with streams that produce Skips , \nwhich complicates matters slightly. In particular, it means that the we must cope with a situation where \nwe have an element from the .rst stream but cannot immediately (i.e., non\u00adrecursively) obtain an element \nfrom the second one. So rather than trying to extract an element from one stream, then from another in \na single step, we must pull from the .rst stream, store the element in the state and then move into a \nnew state where we attempt to pull a value from the second stream. Once the second stream has yielded \na value, we can return the pair. In each call of the next function we pull from at most one stream. Again \nwe see that in any single step we can do only one thing.  4. Functions on nested streams The last major \nclass of list functions that we need to be able to fuse are ones that deal with nested lists. The canonical \nexample is concatMap, but this class also includes all the list comprehensions. In terms of control structures, \nthese functions represent nested recursion and nested loops. T . [E | ] . = return E T . [E | B,Q] . \n= guard B (T . [E | Q] .) T . [E | P . L,Q] . = let fP = True f = False gP = T . [E | Q] . hx = guard \n(fx)(gx) in concatMap h L T . [E | let decls,Q] . = let decls in T . [E | Q] . Figure 3: Translation \nscheme for list comprehensions The ordinary list concatMap function has the type: concatMap ::(a . [b]) \n. [a]. [b] For each element of its input list it applies a function which gives another list and it concatenates \nall these lists together. To de.ne a list concatMap that is fusible with its input and output list, and \nwith the function that yields a list, we will need a stream-based concatMaps with the type: concatMaps \n::(a . Stream b). Stream a . Stream b To get back the list version we compose concatMaps with stream \nand unstream and compose the function argument f with stream : concatMap f = unstream . concatMaps (stream \n. f). stream To convert a use of list concatMap to stream form we need a fusible list consumer c and \nfusible list producers p and f. For c, p and f to be fusible means that they must be de.ned in terms \nof stream or unstream and appropriate stream consumers and producers cs, ps and fs: c = cs . stream p \n= unstream . ps f = unstream . fs We now compose them, expanding their de.nitions to re\u00adveal the stream \nand unstream conversions, and then apply the stream /unstream fusion rule three times: c \u00b7 concatMap \nf \u00b7 p = cs \u00b7 stream \u00b7 unstream \u00b7 concatMaps (stream \u00b7 f) \u00b7 stream \u00b7 unstream \u00b7 ps = cs \u00b7 concatMaps (stream \n\u00b7 f) \u00b7 ps = cs \u00b7 concatMaps (stream \u00b7 unstream \u00b7 fs) \u00b7 ps = cs \u00b7 concatMaps fs \u00b7 ps Actually de.ning \nconcatMaps on streams is somewhat tricky. We need to get an element a from the outer stream, then fa \ngives us a new inner stream. We must drain this stream before moving onto the next outer a element. There \nare thus two modes: one where we are trying to obtain an element from the outer stream; and another mode \nin which we have the current inner stream and are pulling elements from it. We can represent these two \nmodes with the state type: (sa, Maybe (Stream b)) where sa is the state type of the outer stream. The \nfull concatMaps de.nition is given in Figure 2. 5. List comprehensions List comprehensions provide a \nvery concise way of expressing operations that select and combine lists. It is important to fuse them \nto help achieve our goal of ef.ciently compiling elegant, declarative programs. Recall our introductory \nexample: fn = sum [k * m | k . [1..n], m . [1..k]] There are two aspects to fusion of list comprehensions. \nOne is fusing with list generators. Obviously this is only possible when the generator expression is \nitself fusible. The other aspect is elimi\u00adnating any intermediate lists used internally in the comprehension, \nand allowing the comprehension to be fused with a fusible list con\u00adsumer. The build/foldr system tackles \nthis second aspect directly by using a translation of comprehensions into uses of build and foldr that, \nby construction, uses no intermediate lists. Furthermore, by using foldr to consume the list generators \nit allows fusion there too. Obviously the build/foldr translation, employing build , is not suitable \nfor streams. The other commonly used translation (Wadler 1987) directly generates recursive list functions. \nFor streams we either need a translation directly into a single stream (potentially with a very complex \nstate and stepper function) or a translation into fusible primitives. We opt for the second approach \nwhich makes the translation itself simpler but leaves us with the issue of ensuring that the expression \nwe build really does fuse. We use a translation very similar to the translation given in the Haskell \nlanguage speci.cation (Peyton Jones et al. 2003). However, there are a couple of important differences. \nThe .rst change is to always translate into list combinators, rather than concrete list syntax. This \nallows us to delay expansion of these functions and use compiler rewrite rules to turn them into their \nstream-fusible counterparts. The second change is to modify the translation so that condi\u00adtionals do \nnot get in the way of fusion. The Haskell 98 translations for expressions and generators are: T . [E \n| B,Q] . = if B then T . [E | Q] . else [] T . [E | P . L,Q] . = let ok P = T . [E | Q] . ok =[] in concatMap \nok L Note that for the generator case, P can be any pattern and as such pattern match failure is a possibility. \nThis is why the ok function has a catch-all clause. We cannot use this translation directly because in \nboth cases the resulting list is selected on the basis of a test. We cannot directly fuse when the stream \nproducer is not statically known, as is the case when we must make a dynamic choice between two streams. \nThe solution is to push the dynamic choice inside the stream. We use the function guard : guard :: Bool \n. [a]. [a] guard True xs = xs guard False xs =[] This function is quite trivial, but by using a named \ncombinator rather than primitive syntax it enables us to rewrite to a stream fusible implementation: \nguards :: Bool . Stream a . Stream a guards b (Stream next0 s0)= Stream next (b, s0) where next (False, \n)= Done next (True, s)= case next0 s of Done . Done ' Skip s . Skip (True,s ' ) ' Yield x s . Yield \nx (True,s ' ) The full translation is given in Figure 3. We can use guard directly for the case of .lter \nexpressions. For generators we build a function that uses guard with a predicate based on the generator \ns pattern. We can now use this translation on our example. For the sake of brevity we omit the guard \nfunctions which are trivial in this example since both generator patterns are simple variables. T . [k \n* m | k . [1..n],m . [1..k]] . = concatMap (.k . concatMap (.m . return (k * m)) (enumFromTo 1k)) (enumFromTo \n1n) Next we inline all the list functions to reveal the stream versions wrapped in stream / unstream \nand we apply the fusion rule three times: = unstream (concatMaps (.k . stream ( unstream (concatMaps \n(.m . stream ( unstream (returns (k * m)))) (stream (unstream (enumFromTos 1k)))))) (stream (unstream \n(enumFromTos 1n)))) = unstream (concatMaps (.k . concatMaps (.m . returns (k * m)) (enumFromTos 1k)) \n(enumFromTos 1n)) Finally, to get our full original example we apply sums (which is just foldls (+)0) \nand repeat the inline and fuse procedure one more time. This gives us a term with no lists left; the \nentire structure has been converted to stream form. = sums (concatMaps (.k . concatMaps (.m . returns \n(k * m)) (enumFromTos 1k)) (enumFromTos 1n))  6. Correctness Every fusion framework should come with \na rigorous correctness proof. Unfortunately, many do not and ours is not an exception. This might seem \nsurprising at .rst, as we introduce only one rather simple rewrite rule: . s. stream (unstream s ) . \ns Should it not be easy to show that applying this rule does not change the semantics of a program or, \nconversely, construct an ex\u00adample where the semantics is changed? In fact, a counterexample is easily \nfound for the system presented in this paper: with s =., we have: stream (unstream .)= Stream next ... \n= Depending on how we de.ne equivalence on streams, other counterexamples can be derived. In the rest \nof this section we discuss possible approaches to retaining semantic soundness of stream fusion. 6.1 \nStrictness of streams The above counter-example is particularly unfortunate as it implies that we can \nturn terminating programs into non-terminating ones. In our implementation, we circumvent this problem \nby not export\u00ading the Stream data type and ensuring that we never construct bot\u00adtom streams within our \nlibrary. Effectively, this means that we treat Stream as an unlifted type, even though Haskell does not \nprovide us with the means of saying so explicitly.1 Avoiding the creation of bottom streams is, in fact, \nfairly easy. It boils down to the requirement that all stream-constructing functions be non-strict in \nall arguments except those of type Stream which we can presume not to be bottom. This is always possible, \nas the 1 Launchbury and Paterson (1996) discuss how unlifted types can be inte\u00adgrated into a lazy language. \narguments can be evaluated in the stepper function. For instance, the combinator guard de.ned in the \nprevious section is lazy in the condition. The latter is not inspected until the stepper function has \nbeen called for the .rst time. In fact, we can easily change our framework such that the rewrite rule \nremoves bottoms instead of introducing them. For this, it is suf.cient to make stream strict in its argument. \nThen, we have stream (unstream .)= .. However, now we can derive a different counterexample: stream (unstream \n(Stream . s)) = ..Stream . s = This is much less problematic, though, as it only means that we turn some \nnon-terminating programs into terminating ones. Unfor\u00adtunately, with this de.nition of stream it becomes \nmuch harder to implement standard Haskell list functions such that they have the desired semantics. The \nHaskell 98 Report (Peyton Jones et al. 2003) requires that take 0xs =[], i.e., take must be lazy in its \nsecond argument. In our library, take is implemented as: take :: Int . [a]. [a] take n xs = unstream \n(takes n (stream xs )) takes :: Int . Stream a . Stream a takes n (Stream next s)= Stream next ' (n,s) \n where next ' (0,s) = Done next ' (n,s) = case next s of Done . Done ' ' Skip s . Skip (n, s ) ' ' Yield \nx s . Yield x (n - 1,s ) Note that since takes is strict in the stream argument, stream must be lazy \nif take is to have the required semantics. An alterna\u00adtive would be to make takes lazy in the stream: \ntakes ns = Stream next ' (n,s) where next ' (0, s)= Done next ' (n,Stream next s)= case next s of Done \n. Done ' Skip s . Skip (n, Stream next s ' ) ' Yield x s . Yield x (n - 1, Stream next s ' ) Here, we \nembed the entire argument stream in the seed of the newly constructed stream, thus ensuring that it is \nonly eval\u00aduated when necessary. Unfortunately, such code is currently less amenable to being fully optimised \nby GHC. Indeed, ef.ciency was why we preferred the less safe fusion framework presented in this paper \nto the one outlined here. We do hope, however, that improve\u00adments to GHC s optimiser will allow us to \nexperiment with alter\u00adnatives in the future.  6.2 Equivalence of streams Even in the absence of diverging \ncomputations, it is not entirely trivial to de.ne a useful equivalence relation on streams. This is mainly \ndue to the fact that a single list can be modeled by in.nitely many streams. Even if we restrict ourselves \nto streams producing different sequences of Step values, there is still no one-to-one correspondence \n two streams representing the same list can differ in the number and positions of Skip values they produce. \nThis suggests that equivalence on streams should be de.ned modulo Skip values. In fact, this is a requirement \nwe place on all stream\u00adprocessing functions: their semantics should not be affected by the presence or \nabsence of Skip values.  6.3 Testing Although we do not have a formal proof of correctness of our framework, \nwe have tested it quite extensively. It is easy to intro\u00adduce subtle strictness bugs when writing list \nfunctions, either di\u00adrectly on lists or on streams. Fortunately we have a precise speci.\u00adcation in the \nform of the Haskell 98 report. Comparative testing on total values is relatively straightforward, but \nto test strictness prop\u00aderties however we need to test on partial values. We were inspired by the approach \nin StrictCheck (Chitil 2006) of checking strict\u00adness properties by generating all partial values up to \na certain .nite depth. However, to be able to generate partial values at higher or\u00adder type we adapted \nSmallCheck (Runciman 2006) to generate all partial rather than total values up to any given depth. We \nused this and the Chasing Bottoms library (Danielsson and Jansson 2004) to compare our implementations \nagainst the Haskell 98 speci.cation and against the standard library used by many Haskell implemen\u00adtations. \n This identi.ed a number of subtle bugs in our implementation and a handful of cases where we can argue \nthat the speci.cation is unnecessarily strict. We also identi.ed cases where the standard library differs \nfrom the speci.cation. The tests document the strict\u00adness properties of list combinators and give us \ncon.dence that the stream versions do, in fact, have the desired strictness.  7. Compiling stream code \nUltimately, a fusion framework should eliminate temporary data structures. Stream fusion by itself does \nnot, however, reduce allo\u00adcation -it merely replaces intermediate lists by intermediate Step values. \nMoreover, when a stream is consumed, additional alloca\u00adtions are necessary to maintain its seed throughout \nthe loop. For instance, append allocates an Either node in each iteration. This behaviour is quite similar \nto programs produced by de\u00adstroy/unfoldr and like the latter, our approach relies on subsequent compiler \noptimisation passes to eliminate these intermediate val\u00adues. Since we consider more involved list operations \nthan Sven\u00adningsson (2002), in particular nested ones, we necessarily require more involved optimisation \ntechniques than the ones discussed in that work. Still, these techniques are generally useful and not \nspeci.cally tailored to programs produced by our fusion frame\u00adwork. In this section, we identify the \nkey optimisations necessary to produce good code for stream-based programs and explain why they are suf.cient \nto replace streams by nothing at all. 7.1 Flat pipelines Let us begin with a simple example: sum (xs \n++ ys). Our fusion framework rewrites this to: foldls (+)0(appends (stream xs )(stream ys )) Inlining \nthe de.nitions of the stream combinators, we get let nextstream xs = case xs of [] . Done '' x : xs \n. Yield x xs nextappend (Left xs)= case nextstream xs of Done . Skip (Right ys) ' Skip xs . Skip (Left \nxs ' ) ' Yield x xs . Yield x (Left xs ' ) nextappend (Right ys)= case nextstream ys of Done . Done \n ' Skip ys . Skip (Right ys ' ) Yield y ys ' . Yield y (Right ys ' ) go z s = case nextappend s of Done \n. z '' Skip s . goz s '' Yield x s . go (z + x)s in go 0(Left xs) Here, nextstream and nextappend are \nthe stepper functions of the corresponding stream combinators and go the stream consumer of foldls. While \nthis loop is rather inef.cient, it can be easily optimised using entirely standard techniques such as \nthose described by Pey\u00adton Jones and Santos (1998). By inlining nextstream into the .rst branch of nextappend \n, we get a nested case distinction: nextappend (Left xs)= case case xs of [] . Done ' ' x : xs . Yield \nx xs of Done . Skip (Right ys) ' Skip xs . Skip (Left xs ' ) ' Yield x xs . Yield x (Left xs ' ) This \nterm are easily improved by applying the case-of-case trans\u00adformation which pushes the outer case into \nthe alternatives of the inner case: nextappend (Left xs)= case xs of [] . case Done of Done . Skip (Right \nys) ' Skip xs . Skip (Left xs ' ) ' Yield x xs . Yield x (Left xs ' ) ' x : xs . case Yield x xs ' of \nDone . Skip (Right ys) ' Skip xs . Skip (Left xs ' ) ' Yield x xs . Yield x (Left xs ' ) This code trivially \nrewrites to: nextappend (Left xs)= case xs of [] . Skip (Right ys) ' x : xs . Yield x (Left xs ' ) The \nRight branch of nextappend is simpli.ed in a similar man\u00adner, resulting in nextappend (Right ys)= case \nys of [] . Done ' y : ys . Yield y (Right ys ' ) Note how by inlining, applying the case-of-case transforma\u00adtion \nand then simplifying we have eliminated the construction (in nextstream ) and inspection (in nextappend \n) of one Step value per it\u00aderation. The good news is that these techniques are an integral part of GHC \ns optimiser and are applied to our programs automatically. Indeed, the optimiser then inlines nextappend \ninto the body of go and reapplies the transformations described above to produce: let go z (Left xs)= \ncase xs of [] . go z (Right ys) ' x : xs . go (z + x)(Left xs ' ) go z (Right ys)= case ys of [] . z \n ' y : ys . go (z + y)(Right ys ' ) in go 0(Left xs) While this loop does not use any intermediate Step \nvalues, it still allocates Left and Right for maintaining the loop state. Eliminating these requires \nmore sophisticated techniques than we have used so far. Fortunately, constructor specialisation (Peyton \nJones 2007), an optimisation which has been implemented in GHC for some time, does precisely this. It \nanalyses the shapes of the arguments in recursive calls to go and produces two specialised versions of \nthe function, go1 and go2, which satisfy the following equivalences: . z xs. go z (Left xs)= go1 z xs \n. z ys. go z (Right ys)= go2 z ys The compiler then replaces calls to go by calls to a specialised version \nwhenever possible. The de.nitions of the two specialisa\u00adtions are obtained by expanding go once in each \nof the above two equations and simplifying, which ultimately results in the follow\u00ading program: let \ngo1 z xs = case xs of [] . go2 z ys '' x : xs . go1 (z + x)xs go2 z ys = case ys of [] . z '' y : ys \n. go2 (z + y)ys in go1 0xs Note that the original version of go is no longer needed. The loop has effectively \nbeen split in two parts one for each of the two concatenated lists. Indeed, this result is the best \nwe could have hoped for. Not only have all intermediate data structures been eliminated, the loop has \nalso been specialised for the algorithm at hand. By now, it becomes obvious that in order to compile \nstream pro\u00adgrams to ef.cient code, all stream combinators must be inlined and subsequently specialised. \nArguably, this is a weakness of our ap\u00adproach, as this sometimes results in excessive code duplication \nand a signi.cant increase in the size of the generated binary. However, as discussed in Section 8, our \nexperiments suggest that this increase is almost always negligible. 7.2 Nested computations So far, \nwe have only considered the steps necessary to optimise fused pipelines of .at operations on lists. For \nnested operations such as concatMap, the story is more complicated. Nevertheless, it is crucial that \nsuch operations are optimised well. Indeed, even our introductory example uses concatMap under the hood \nas described in Section 5. Although a detailed explanation of how GHC derives the ef.\u00adcient loop presented \nin the introduction would take up too much space, we can investigate a less complex example which, neverthe\u00adless, \ndemonstrates the basic principles underlying the simpli.cation of nested stream programs. In the following, \nwe consider the simple list comprehension sum [m * m | m . [1..n]]. After desugaring and stream fusion, \nthe term is transformed to (we omit the trivial guard ): foldls (+)0(concatMaps (.m. returns (m * m)) \n(enumFromTos 1n)) After inlining the de.nitions of the stream functions, we arrive at the following loop \n(nextenum , nextcm and nextret are the stepper functions of enumFromTos, concatMaps and returns, respec\u00adtively, \nas de.ned in Figures 1 and 2 ): let nextenum i | i > n = Done | otherwise = Yield i (i + 1) nextconcatMap \n(i, Nothing )= case nextenum i of Done . Done '' Skip i . Skip (i , Nothing ) ' Yield x i . let nextret \nTrue = Yield (x * x)False nextret False = Done in ' Skip (i , Just (Stream nextret True)) nextconcatMap \n(i, Just (Stream next s)) = case next s of Done . Skip (i, Nothing ) ' ' Skip s . Skip (i, Just (Stream \nnext s )) ' ' Yield y s . Yield y (i, Just (Stream next s )) go z s = case nextconcatMap s of Done . \nz '' Skip s . goz s '' Yield x s . go (z + x)s in go 0(1,Nothing ) As before, we now inline nextenum \nand nextconcatMap into the body of go and repeatedly apply the case-of-case transformation. Ultimately, \nthis produces the following loop: let go z (i, Nothing )| i > n = z | otherwise = let nextret True = \nYield (i * i)False nextret False = Done in go z (i +1, Just (Stream nextret True)) go z (i, Just (Stream \nnext s)) = case next s of Done . go z (i, Nothing ) ' Skip s . go z (i, Just (Stream next s ' )) ' Yield \nx s . go (z + x)(i, Just (Stream next s ' )) in go 0(1,Nothing ) Now we again employ constructor specialisation \nto split go into two mutually recursive functions go1 and go2 such that . zi. go z (i, Nothing )= go1 \nzi . z i next s. go z (i, Just (Stream next s)) = go2 z i next s The second specialisation is interesting \nin that it involves an exis\u00adtential component the state of the stream. Thus, go2 must have a polymorphic \ntype which, however, is easily deduced by the com\u00adpiler. After simplifying and rewriting calls to go, \nwe arrive at the following code: let go1 zi | i > n = z | otherwise = let nextret True = Yield (i * i)False \nnextret False = Done in go2 z (i + 1) nextret True go2 z i next s = case next s of Done . go1 zi ' \n' Skip s . go2 z i next s ' ' Yield x s . go2 (z + x)i next s in go1 01 The loop has now been split \ninto two mutually recursive func\u00adtions. The .rst, go1, computes the next element i of the enumer\u00adation \n[1..n]and then passes it to go2 which computes the product and adds it to the accumulator z. However, \nthe nested structure of the original loop obscures this simple algorithm. In particular, the stepper \nfunction nextret of the stream produced by returns has to be passed from go1, where it is de.ned, to \ngo2, where it is used. If we are to produce ef.cient code, we must remove this indirection and allow \nnextret to be inlined in the body of go2. In the following, we consider two approaches to solving this \nproblem: static argu\u00adment transformation and specialisation on partial applications. Static argument \ntransformation It is easy to see that next and i are static in the de.nition of go2, i.e., they do not \nchange between iterations. An optimising compiler can take advantage of this fact and eliminate the unnecessary \narguments: go2 z i next s = ' let go2 zs = case next s of Done . go1 zi ' ' ' Skip s . go2 zs '' ' Yield \nx s . go2 (z + x)s ' in go2 zs With this de.nition, go2 can be inlined in the body of go1. Subse\u00adquent \nsimpli.cation binds next to nextret and allows the latter to be inlined in go2' : go1 zi | i > n = z \n| otherwise = '' let go2 z True = go2 (z + i * i)False ' go2 z False = go1 z (i + 1) in ' go z True \n The above can now be easily rewritten to the optimal loop: go1 zi | i > n = z | otherwise = go1 (z + \ni * i)(i + 1) Note how the original nested loop has been transformed into a .at one. This is only possible \nbecause in this particular exam\u00adple, the function argument of concatMap was not itself recursive. More \ncomplex nesting structures, in particular nested list compre\u00adhensions, are translated into nested loops \nif the static argument transformation is employed. For instance, our introductory example would be compiled \nto let go1 zk | k > n = z | otherwise = let go2 zm | m > k = go1 z (k + 1) | otherwise = go2 (z + k * \nm)(m + 1) in go2 z 1 in go1 01 Specialisation An alternative approach to optimising the program is to \nlift the de.nition of nextret out of the body of go1 according to the algorithm of Johnsson (1985): nextret \ni True = Yield (i * i)False nextret i False = Done go1 zi | i > n = z | otherwise = go2 z (i + 1)(nextret \ni)True Now, we can once more specialise go2 for this call; but this time, in addition to constructors \nwe also specialise on the partial appli\u00adcation of the now free function nextret , producing a go3 such \nthat: . zij. go2 zj (nextret i)True = go3 zj i After expanding go2 once in the above equation, we arrive \nat the following unoptimised de.nition of go3: go3 zj i = case nextret i True of Done . go1 z j ' ' Skip \ns . go2 z j (nextret i)s ' ' Yield x s . go2 (z + x)j (nextret i)s Note that the stepper function is \nnow statically known and can be inlined which allows all case distinctions to be subsequently eliminated, \nleading to a quite simple de.nition: go3 zj i = go2 (z +(i * i)) j (nextret i)False The above call gives \nrise to yet another specialisation of go2: . zij. go2 zj (nextret i)False = go4 zj i Again, we rewrite \ngo4 by inlining nextret and simplifying, ulti\u00admately producing: go1 zi | i > n = z | otherwise = go3 \nz (i + 1) i go3 zj i = go4 (z +(i * i)) ji go4 zj i = go1 zj This is trivially rewritten to exactly the \nsame code as has been produced by the static argument transformation: go1 zi| i > n = z | otherwise = \ngo1 (z + i * i)(i + 1) This convergence of the two optimisation techniques is, how\u00adever, only due to \nthe simplicity of our example. For the more deeply nested program from the introduction, specialisation \nwould pro\u00adduce two mutually recursive functions: go1 zk | k > n = z | otherwise = go2 zk (k + 1)1 ' ' \n go2 zkk m| m > k = go1 zk | otherwise = go2 (z + k * m)kk ' (m + 1) This is essentially the code of \nf ' from the introduction; the only difference is that GHC s optimiser has unrolled go2 once and unboxed \nall loop variables. This demonstrates the differences be\u00adtween the two approaches nicely. The static \nargument transforma\u00adtion translates nested computations into nested recursive functions. Specialisation \non partial applications, on the other hand, produces .at loops with several mutually recursive functions. \nThe state of such a loop is maintained in explicit arguments. Unfortunately, GHC currently supports neither \nof the two ap\u00adproaches it only specialises on constructors but not on partial applications of free functions \nand does not perform the static argu\u00adment transformation. Although we have extended GHC s optimiser with \nboth techniques, our implementation is quite fragile and does not always produce the desired results. \nIndeed, missed optimisa\u00adtions are at least partly responsible for many of the performance problems discussed \nin Section 8. At this point, the implementation must be considered merely a proof of concept. We are, \nhowever, hopeful that GHC will be able to robustly optimise stream-based programs in the near future. \n  8. Results We have implemented the entire Haskell standard List library, in\u00adcluding enumerations and \nlist comprehensions, on top of our stream fusion framework. Stream fusion is implemented via equational \ntransformations embedded as rewrite rules in the library source code. We compare time, space, code size \nand fusion opportuni\u00adties for programs in the no.b benchmark suite (Partain 1992), when compared to the \nexisting build/foldr system. To ensure a fair comparison, both frameworks have been benchmarked with \nour extensions to GHC s optimiser (cf. Section 7) enabled. For the build/foldr framework, these extensions \ndo not signi.cantly affect the running time and allocation behaviour, usually improving them slightly, \nand without them, nested concatMap s under stream fu\u00adsion risk not being optimised. 8.1 Time Figure \n4 presents the relative speedups for Haskell programs from the no.b suite, when compared to the existing \nbuild/foldr system. On average, there is a 3% improvement when using stream fusion, with 6 of the test \nprograms more than 15% faster, and one, the in\u00adteger benchmark, more than 50% faster. One program, paraf.ns \n, ran 24% slower, due to remnant Stream data constructors not stat\u00adically removed by the compiler. In \ngeneral we can divide the results into three classes of pro\u00adgrams: 1. those for which there is plenty \nof opportunity for fusion which is under-exploited by build/foldr; 2. programs for which there is little \nfusion or for which the fusion is in a form handled by build/foldr; 3. and thirdly, programs such as \nparaf.ns with deeply nested list computations and comprehensions which overtax our ex\u00adtensions to GHC \ns optimiser.  For the .rst class or programs, those using critical left folds and zip, stream fusion \ncan be a big win. 10% (and sometimes much more) improvement is not uncommon. This corresponds to around \n15% of programs tested. In the second case, the majority of programs covered, there is either little \navailable fusion, or the fusion is in the form of right folds, and list comprehensions, already well \nhandled by build/foldr. Only small improvements can be expected here. Finally, the third class, corresponds \nto some 5% of programs tested. These programs have available fusion, but in deeply nested form, which \ncan lead to Step constructors left behind by limitations in current GHC optimisations, rather than being \nremoved statically. These programs currently will run dramatically, and obviously, worse. For large \nmulti-module programs, the results are less clear, with just as many programs speeding up as slowing \ndown. We .nd that for larger programs, GHC has a tendency to miss optimisation op\u00adportunities for stream \nfusible functions across module boundaries, which is the subject of further investigation. 8.2 Space \nFigure 5 presents the relative reduction in total heap allocations for stream fusion programs compared \nto the existing build/foldr system. The results can again be divided into the same three classes as for \nthe time benchmarks: those with under-exploited fusion opportunities, those for which build/foldr already \ndoes a good job, and those for which Step artifacts are not statically eliminated by the compiler. For \nprograms which correctly fuse, in the .rst class, with new fusion opportunities found by stream fusion, \nthere can be dramatic reductions in allocations (up to 30%). Currently, this is the minor\u00adity of programs. \nThe majority of programs have modest reductions, with an average decrease in allocations of 4.4%. Two \nprograms have far worse allocation performance, however, due to missed op\u00adportunities to remove Step \nconstructors in nested code. For large, multi-module programs, we .nd a small increase in allocations, \nfor similar reasons as for the time benchmarks. 8.3 Fusion opportunities In Figure 6 we compare the \nnumber of fusion sites identi.ed with stream fusion, compared to that with build/foldr. In the majority \nof cases, more fusion sites are identi.ed, corresponding to new fusion opportunities with zips and left \nfolds. Similar results are seen when compiling GHC itself, with around 1500 build/foldr fusion sites \nidenti.ed by the compiler, and more than 3000 found under stream fusion. 8.4 Code size Total compiled \nbinary size was measured, and we .nd that for single module programs, code size increases by a negligible \n2.5% on average. For multi-module programs, code size increases by 11%. 5% of programs increased by more \nthan 25% in size, again due to unremoved specialised functions and Step constructors.  9. Further work \n9.1 Improved optimisations The main direction for future work on stream fusion is to improve further \nthe compiler optimisations required to remove Step constructors statically, as described in Section 7. \nAnother possible approach to reliably fusing nested uses of concatMap is to de.ne a restricted version \nof it which assumes that the inner stream is constructed in a uniform way, i.e., using the same stepper \nfunction and the same function to construct initial inner-stream states in every iteration of the outer \nstream. This situation corresponds closely to the forms that we expect to be able to optimise with the \nstatic argument transformation. The aim would be to have a rule that matches the common situ\u00adation where \nthis restricted concatMap can be used. Unfortunately such rules cannot be expressed in the current GHC \nrules language. A more powerful rule matcher would allow us to write something like: concatMap (.x . \nunstream (Stream next .x. s.x.)) =concatMap ' (.y . next .y.)(.y . s.y.)  Figure 4: Percentage improvement \nin running time compared to build/foldr fusion Figure 5: Percent reduction in allocations compared to \nbuild/foldr fusion Figure 6: New fusion opportunities found when compared to build/foldr Here, T[x] matches \na term T abstracted over free occurrences of the variable x. In the right hand side the same syntax indicates \nsubstitution. The key point is that the stepper function of the inner stream is statically known in concatMap \n' . This is a much more favourable situation compared to embedding the entire inner stream in the seed \nof concatMap. Indeed, extending GHC s rule matching capabil\u00adities in this direction might be easier than \nrobustly implementing the optimisations outlined in Section 7.  9.2 Fusing general recursive de.nitions \nWriting stream stepper functions is not always easy. The represen\u00adtation of control .ow as state makes \nthem appear somewhat inside out. One technique we found useful when translating Haskell list functions \ninto stream fusible versions was to .rst transform the list version to very low level Haskell. From this \nform, where the pre\u00adcise control .ow is clear, there is a fairly direct translation into a stream version. \nFor example here is a list function written in a very low level style using three mutually tail-recursive \nfunctions. Each one has only simple patterns on the left hand side and on the right hand side: an empty \nlist; a call; or a cons and a call. intersperse :: a . [a]. [a] intersperse sep xs0 = init xs0 where \ninit xs = case xs of [] . [] (x : xs). gox xs gox xs = x : to xs to xs = case xs of [] . [] (x : xs). \nsep : gox xs We can translate this to an equivalent function on streams by making a data type with one \nconstructor per function. Each con\u00adstructor holds the arguments to that function except that arguments \nof type list are replaced by the stream state type. In the body of each function, case analysis on lists \nis replaced by calling next0 on the stream state. Consing an element onto the result is replaced by uses \nof Yield . Calls are replaced by Skips with the appropriate state data constructor: data State a s = \nInit s | Go a s | To s intersperses :: a . Stream a . Stream a intersperses sep (Stream next0 s0)= Stream \nnext (Init s0) where next (Init s )= case next0 s of Done . Done ' Skip s . Skip (Init s ' ) '' Yield \nx s . Skip (Go x s ) next (Gox s)= Yield x (To s) next (To s)= case next0 s of Done . Done ' Skip s \n. Skip (To s ' ) ' Yield x s . Yield sep (Go x s ' ) It would be interesting to investigate the precise \nrestrictions on the form which can be translated in this way and whether it can be automated. This might \nprovide a practical way to fuse general recursive de.nitions over lists: by checking if the list function \ncan be translated to the restricted form and then translating into a stream version. There is some precedent \nfor this approach: Launchbury and Sheard (1995) show that in many common cases it is possible to transform \ngeneral recursive de.nitions on lists into a form suitable for use with ordinary build/foldr short-cut \nfusion.  9.3 Fusing more general algebraic data types It seems straightforward to de.ne a co-structure \nfor any sum-of\u00adproducts data structure. Consider for example a binary tree type with information in both \nthe leaves and interior nodes: data Tree ab = Leaf a | Fork b (Tree ab)(Tree ab) The corresponding co-structure \nwould be: data Streama b = .s. Stream (s . Step ab s) s data Step ab s = Leafs a | Forks bss | Skip s \n Of course other short-cut fusion systems can also be generalised in this way but in practice they are \nnot because it requires de.ning a new infrastructure for each new data structure that we wish to fuse. \nAutomation would be required to make this practical. This problem is somewhat dependent on the ability \nto generate stream style code from ordinary recursive de.nitions.  10. Conclusion It is possible, via \nstream fusion, to automatically fuse a complete range of list functions, beyond that of previous short-cut \nfusion techniques. In particular, it is possible to fuse left and right folds, zips, concats and nested \nlists, including list comprehensions. For the .rst time, details are provided for the range of general \npurpose optimisations required to generate ef.cient code for unfoldr-based short-cut fusion. Stream fusion \nis certainly practical, with a full scale implemen\u00adtation for Haskell lists being implemented, utilising \nlibrary-based rewrite rules for fusion. Our results indicate there is a greater op\u00adportunity for fusion, \nthan under the existing build/foldr system, and also show moderate improvements in space and time performance. \nFurther improvements in the speci.c compiler optimisations re\u00adquired to remove fusion artifacts statically. \nThe source code for the stream fusion List library, and modi.ed standard Haskell library and compiler, \nare available online.2 Acknowledgments We are indebted to Simon Peyton Jones for clarifying and extending \nGHC s optimiser, which greatly assisted this effort. We are also grateful to Manuel Chakravarty and Spencer \nJanssen for feedback on drafts and to the four anonymous referees for their invaluable comments. References \nOlaf Chitil. Type inference builds a short cut to deforestation. In ICFP 99: Proceedings of the fourth \nACM SIGPLAN Interna\u00adtional Conference on Functional programming, pages 249 260, New York, NY, USA, 1999. \nACM Press. Olaf Chitil. Promoting non-strict programming. In Draft Pro\u00adceedings of the 18th International \nSymposium on Implementa\u00adtion and Application of Functional Languages, IFL 2006, pages 512 516, Budapest, \nHungary, September 2006. Eotvos Lorand University. Duncan Coutts, Don Stewart, and Roman Leshchinskiy. \nRewrit\u00ading Haskell strings. In Practical Aspects of Declarative Lan\u00adguages 8th International Symposium, \nPADL 2007, pages 50 64. Springer-Verlag, January 2007. Nils Anders Danielsson and Patrik Jansson. Chasing \nbottoms, a case study in program veri.cation in the presence of partial and in.nite values. In Dexter \nKozen, editor, Proceedings of the 7th International Conference on Mathematics of Program Construction, \nMPC 2004, volume 3125 of LNCS, pages 85 109. Springer-Verlag, July 2004. 2 This material can be found \non the website accompanying this paper at http://www.cse.unsw.edu.au/~dons/papers/CLS07.html. Jeremy \nGibbons. Streaming representation-changers. In D. Kozen, editor, Mathematics of Program Construction, \npages 142 168. Springer-Verlag, 2004. LNCS 523. Jeremy Gibbons and Geraint Jones. The under-appreciated \nunfold. In ICFP 98: Proceedings of the third ACM SIGPLAN Interna\u00adtional Conference on Functional programming, \npages 273 279, New York, NY, USA, 1998. ACM Press. Andrew Gill. Cheap Deforestation for Non-strict Functional \nLan\u00adguages. PhD thesis, University of Glasgow, January 1996. Andrew Gill, John Launchbury, and Simon \nPeyton Jones. A short cut to deforestation. In Conference on Functional Programming Languages and Computer \nArchitecture, pages 223 232, June 1993. Zhenjiang Hu, Hideya Iwasaki, and Masato Takeichi. Deriving structural \nhylomorphisms from recursive de.nitions. In Pro\u00adceedings 1st ACM SIGPLAN International Conference on \nFunc\u00adtional Programming, volume 31(6), pages 73 82. ACM Press, New York, 1996. Patricia Johann. Short \ncut fusion: Proved and improved. In SAIG 2001: Proceedings of the Second International Workshop on Se\u00admantics, \nApplications, and Implementation of Program Genera\u00adtion, pages 47 71, London, UK, 2001. Springer-Verlag. \nThomas Johnsson. Lambda lifting: transforming programs to recur\u00adsive equations. In Functional programming \nlanguages and com\u00adputer architecture. Proc. of a conference (Nancy, France, Sept. 1985), New York, NY, \nUSA, 1985. Springer-Verlag Inc. John Launchbury and Ross Paterson. Parametricity and unboxing with unpointed \ntypes. In European Symposium on Program\u00adming, pages 204 218, 1996. John Launchbury and Tim Sheard. Warm \nfusion: deriving build\u00adcatas from recursive de.nitions. In FPCA 95: Proceedings of the seventh international \nconference on Functional program\u00adming languages and computer architecture, pages 314 323, New York, NY, \nUSA, 1995. ACM Press. Erik Meijer, Maarten Fokkinga, and Ross Paterson. Functional programming with bananas, \nlenses, envelopes and barbed wire. In J. Hughes, editor, Proceedings 5th ACM Conf. on Func\u00adtional Programming \nLanguages and Computer Architecture, FPCA 91, Cambridge, MA, USA, 26 30 Aug 1991, volume 523, pages 124 \n144. Springer-Verlag, Berlin, 1991. Will Partain. The no.b benchmark suite of Haskell programs. In Functional \nProgramming, pages 195 202, 1992. Simon Peyton Jones. Constructor Specialisation for Haskell Pro\u00adgrams, \n2007. Submitted for publication. Simon Peyton Jones and Andr\u00b4A transformation\u00ad e L. M. Santos. based \noptimiser for Haskell. Sci. Comput. Program., 32(1-3): 3 47, 1998. ISSN 0167-6423. Simon Peyton Jones, \nAndrew Tolmach, and Tony Hoare. Playing by the rules: rewriting as a practical optimisation technique \nin GHC. In Ralf Hinze, editor, 2001 Haskell Workshop. ACM SIGPLAN, September 2001. Simon Peyton Jones \net al. The Haskell 98 language and libraries: The revised report. Journal of Functional Programming, \n13(1): 0 255, Jan 2003. Colin Runciman. SmallCheck 0.2: another lightweight testing library in Haskell. \nhttp://article.gmane.org/gmane. comp.lang.haskell.general/14461 , 2006. Josef Svenningsson. Shortcut \nfusion for accumulating parameters &#38; zip-like functions. In ICFP 02: Proceedings of the seventh ACM \nSIGPLAN International Conference on Functional pro\u00adgramming, pages 124 132, New York, NY, USA, 2002. \nACM Press. Akihiko Takano and Erik Meijer. Shortcut deforestation in calcu\u00adlational form. In Conf. Record \n7th ACM SIGPLAN/SIGARCH Int. Conf. on Functional Programming Languages and Com\u00adputer Architecture, FPCA \n95, pages 306 313. ACM Press, New York, 1995. The GHC Team. The Glasgow Haskell Compiler (GHC). http: \n//haskell.org/ghc, 2007. Philip Wadler. List comprehensions. In Simon Peyton Jones, ed\u00aditor, The implementation \nof functional programming languages. Prentice Hall, 1987. Chapter 15. Philip Wadler. Deforestation: transforming \nprograms to eliminate trees. Theoretical Computer Science, (Special issue of selected papers from 2nd \nEuropean Symposium on Programming), 73(2): 231 248, 1990.  \n\t\t\t", "proc_id": "1291151", "abstract": "<p>This paper presents an automatic deforestation system, <i>stream fusion</i>, based on equational transformations, that fuses a wider range of functions than existing short-cut fusion systems. In particular, stream fusion is able to fuse zips, left folds and functions over nested lists, including list comprehensions. A distinguishing feature of the framework is its simplicity: by transforming list functions to expose their structure, intermediate values are eliminated by general purpose compiler optimisations.</p> <p>We have reimplemented the Haskell standard List library on top of our framework, providing stream fusion for Haskell lists. By allowing a wider range of functions to fuse, we see an increase in the number of occurrences of fusion in typical Haskell programs. We present benchmarks documenting time and space improvements.</p>", "authors": [{"name": "Duncan Coutts", "author_profile_id": "81435592996", "affiliation": "Oxford University, Oxford, United Kingdom", "person_id": "P900677", "email_address": "", "orcid_id": ""}, {"name": "Roman Leshchinskiy", "author_profile_id": "81330494188", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "P869788", "email_address": "", "orcid_id": ""}, {"name": "Don Stewart", "author_profile_id": "81100264111", "affiliation": "University of New South Wales, Sydney, Australia", "person_id": "PP37044256", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291199", "year": "2007", "article_id": "1291199", "conference": "ICFP", "title": "Stream fusion: from lists to streams to nothing at all", "url": "http://dl.acm.org/citation.cfm?id=1291199"}