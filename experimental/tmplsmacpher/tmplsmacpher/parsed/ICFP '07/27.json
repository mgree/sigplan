{"article_publication_date": "10-01-2007", "fulltext": "\n Faster Laziness Using Dynamic Pointer Tagging Simon Marlow AlexeyRodriguez Yakushev Simon Peyton Jones \nMicrosoft Research University of Utrecht, The Netherlands Microsoft Research simonmar@microsoft.com alexey@cs.uu.nl \nsimonpj@microsoft.com Abstract In the light of evidence that Haskell programs compiled by GHC exhibit \nlarge numbers of mispredicted branches on modern proces\u00adsors, we re-examine the tagless aspect of the \nSTG-machine that GHC uses as its evaluation model. We propose two tagging strategies: a simple strategy \ncalled semi\u00adtaggingthatseekstoavoidone common sourceof unpredictablein\u00addirect jumps, and a more complex \nstrategy called dynamic pointer\u00adtagging that uses the spare low bits in a pointer to encode informa\u00adtion \nabout the pointed-to object. Both of these strategies have been implementedandexhaustively measuredinthe \ncontextofaproduc\u00adtion compiler, GHC, and the paper contains detailed descriptions of the implementations. \nOur measurements demonstrate signi.cant performance improvements (14% for dynamic pointer-tagging with \nonly a 2% increase in code size), and we further demonstrate that much of the improvement can be attributed \nto the elimination of mispredicted branch instructions. As part of our investigations we also discovered \nthat one optimisa\u00adtioninthe STG-machine,vectored-returns,isno longerworthwhile and we explain why. Categories \nand Subject Descriptors D.3.2[Programming Lan\u00adguages]: Language Classi.cations Applicative (functional) \nlan\u00adguages; D.3.4[Programming Languages]:Processors Code Gen\u00aderation; Compilers; Optimization General \nTerms Languages, Performance 1. Introduction The Glasgow Haskell Compiler (GHC) is the most widely-used \ncompiler for a lazy functional language. Since its inception, GHC has used the Spineless Tagless G-Machine \n(STG-machine) (Pey\u00adton Jones 1992) as its core execution model; for over 10 years this model remained \nlargely unchanged, before eval/apply was adopted as an alternative to push/enter as the mechanism for \nfunction calls (Marlow and Peyton Jones 2004). In this paper we re-examine an\u00adother aspect of the original \nSTG-machine, namely the concept of taglessness . Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 07, October 1 3, 2007, Freiburg, Germany. Copyright c &#38;#169; \n2007ACM 978-1-59593-815-2/07/0010...$5.00. The tagless aspect of the STG-machine refers to the way in \nwhich a heap closureisevaluated.Forexample, consider fxy=casexof (a,b)-> a+y In a lazy language, before \ntaking x apart the compiler must ensure that it is evaluated. So it generates code to push onto the stack \na continuation to compute a+y, and jumps to the entry code for x. The .rst .eld of every heap closure \nis its entry code, and jumping to this code is called entering the closure. The entry code for an unevaluated \nclosure will evaluate the closure and return its value to the continuation; an already-evaluated closure \nwill return immediately. In contrast, in a tag-ful approach, the closure to evaluate is entered onlyifitis \nfoundtobenotyetevaluated.Herethe generatedcode performs an extra test on the closure type the tag to \ndetermine its evaluatedness and to avoid entering it unnecessarily. The tagless scheme is attractive \nbecause the code to evaluate a clo\u00adsureissimpleand uniform:anyclosurecanbeevaluatedsimplyby entering \nit. But this uniformity comes at the expense of perform\u00ading indirectjumps, one to enter the closure and \nanother to return to the evaluation site. These indirect jumps are particularly expensive on a modern \nprocessor architecture, because theyfox the branch\u00adprediction hardware, leadingtoastallof10or morecycles \ndepend\u00ading on the length of the pipeline. If the closure is unevaluated, then we really do have to take \nan indi\u00adrect jump to its entry code. However, if it happens to be evaluated already, thena conditionaljumpmightexecute \nmuchfaster.In this paperwe describeanumberof approachesthatexploitthis possibil\u00adity.Wehaveimplementedthese \nschemesandshowthattheydeliver substantialperformanceimprovements (10-14%)inGHC,amature optimising compiler \nfor Haskell. Speci.cally, our contributions are these: We give the .rst accurate measurements for the \ndynamic be\u00adhaviour of case expressions in lazy programs, across a range of benchmark programs.First, \nwe measurehow oftena closure being scrutinised is already evaluated and, hence how often we can expect \nto avoid the enter-and-return sequence (Section 4). Second, we measure the distribution ofdata type sizes, \nwhich turns out to be important (Section 6).  We describe and implement semi-tagging, a simple approach \nthat avoids the enter-and-return associated with scrutinising an already-evaluated closure. Section 5 \ndescribes a full im\u00adplementation and gives measurements of its effectiveness. As well as bottom-line \nexecution times, we measure its effect on branch misprediction on a modern processor architecture. The \nimprovements are substantial: execution time improves by 7\u00ad9%, while the branch misprediction rateishalved. \n We describe and implement an improvement to semi-tagging, called dynamic pointer tagging. The idea \nis to use the spare low  bits of a pointer to encode (a safe approximation to) informa\u00adtion about the \npointed-to closure, even though that information can change dynamically. This technique appears to be \nnovel. Again we describe the details of the implementation, which are less straightforward than for semi-tagging. \nOur measurements of its effectiveness are promising: a further 4-6% increase in performance (depending \non processor architecture). We study the interaction of both these techniques with an exist\u00ading optimisation \ncalled vectored returns.Vectored returns is an attractive optimisationthathasbeen implementedinGHCfora \ndecade. However, it carries a signi.cant cost in terms of imple\u00admentation complexity, and our new tagging \nschemes reduce its potential bene.ts. In Section 7.2 we present measurements that show that vectored \nreturns are no longer attractive on today s processors, so the optimisation, and its attendant complexities, \ncan be safely retired. Amajor contribution of the paper is that our implementation and measurements are \nmade in the context of a widely-used, highly\u00adoptimising compiler for Haskell, namely GHC; and our measure\u00adments \naremadeagainstalarge suiteof benchmarks. BecauseGHC is a mature compiler, all the easy wins are part \nof the baseline, so a 10-15% improvement on real programs is a dramatic result. Our contributions are \nnot limited to the context of GHC. Semi\u00adtagging and dynamic pointer tagging have an immediate relevance \nforanyonebuildinga compilerforalazy functional language. The paper contains quite a bit of nitty-gritty \ndetail, but that too is part of our contribution. Real compilers use many optimisa\u00adtion techniques, whose \ninteraction is hard to foresee.We are able to report on concrete experience of implementing various tagging \nschemes, including their implementation costs as well as perfor\u00admance effects. 2. Compiling case expressions \nIn this section we review the relevant parts of the execution model and compilation scheme for the STG-machine \nimplementation as it is in GHC today. Our particular focus is the compilation scheme for case expressions, \nsuch as the one in the body of map: map :: (a->b) -> [a] -> [b] map f xs = case xs of [] ->[] (x:xs )->fx \n:mapf xs Informally, the semantics of case are as follows:  Thevariable xs isevaluatedto weak-head normal \nform(WHNF).  The appropriate alternativeis chosen according to the outermost constructor of the value \nof xs.  If the pattern has variables, then these are bound to the .elds of the constructor (in this \nexample, x and xs are bound in the second alternative).  The appropriate right hand side is evaluated \nin this extended environment.  The precise semantics aregiveninthe original STG-machine paper (Peyton \nJones 1992). Operationally, in the STG-machine implementation in GHC, the case expression is compiled \nto code that enters the variable being scrutinised, after pushing on the stack a return address, or contin\u00aduation, \nfor the alternatives. All heap objects have the uniform rep\u00adresentation shown in Figure 1. The .rst word \nof every object is an Figure 1. Aheap object info pointer, which points both to the entry code for the \nobject, and to the preceding info table for the object. The info table describes the layoutofthe objecttothegarbage \ncollector.It also containsa object type .eld which classi.es the object as: a data constructor, a thunk, \na function closure, or one of a variety of other object types. In the case of a data constructor, a further \n.eld in the info table gives the tag of the constructor; each constructorina data typehas a unique tag, \nstarting at 0. Because every heap object has associated entry code, we use the term closure for any heap \nobject. The entry code for a closure always has the following meaning: it is executed with the register \nR1 pointing to the closure, and it returns to the topmost stack frame having evaluated the closure pointed \nto by R1, returning the result in R1. GHC uses C--(Peyton Jones et al. 1999) as its back-end interme\u00addiate \nlanguage. C--can be thought of as a high-level assembly lan\u00adguage with nested expressions and named variables. \nGHC does not currently use the procedural abstractions of C--, instead it generates C--code that directly \nmanages an explicit stack. GHC s purely\u00adfunctional intermediate language is translated into C--before \nbeing turned into eitherC or native code.We will therefore present our generated code examples in C--. \nHere, then, is the C--code generated for the map example above, prior to the work described in this paper: \nmap_entry() // On entry, f is in R2, xs in R3 { // Save f on the stack, because it is // live in the \ncontinuation Sp[-4] = R2 // Set R1 to point to the closure to evaluate R1= R3; // Put the continuation \non the stack Sp[-8] = map_ret; // Adjust the stack pointer Sp= Sp-8; // Indirect jump to the entry code \n// for the closure in R1 jump R1[0]; } data { word32[] = {...} } // Info table for map_ret map_ret() \n{ // R1 now points to the value of xs info = R1[0]; tag = info[-8]; if (tag == 0) jump tag_0; // Extract \nthe fields: x = R1[4]; xs = R1[8]; // Recover the free variables from stack f = Sp[4]; // Code for (f \nx : map f xs ) follows ... tag_0: // Load [] into R1, adjust stack, and return R1 = Nil_closure; Sp = \nSp + 8; jump Sp[0]; } The C--procedure map_entry corresponds to the map function itself, and map_ret \nis the continuation generated for the case by the compiler. Note that map_ret is directly precededbysome \ndata; this is its info table. Return addresses have info tables just as heap closures do; this makes \nstack frames have almost exactly the same layoutasheap closures.Theinfotableforareturnaddress describes \nthe layout of the stack frame for thegarbage collector, just as the info table for a heap closure describes \nthe layout of the closure. The code of map_ret begins by scrutinising the value returned, in order to \ndistinguish between the two cases. Each constructor in a datatype has a distinct tag, which is stored \nin the info table of the constructor. In the case of lists, [] has tag zero, and (:) has tag one. R1,R2,andR3,likethe \nstack pointerSp,are registers of the STG\u00admachine. They may be implemented by a real machine register, \nor mapped to a memory location: the choice is left to the back\u00adend implementation. (In practice, since \nR1 is used so often by the abstract machine, it is essential for good performance that it is mapped to \na real machine register by the back-end). In C--a memory access always uses byte addressing, this differs \nfrom C, where the pointer type dictates the addressing granularity. In this paper, we assume that all \ncode examples are for a machine with a word size of four bytes. This means that most of the mem\u00adory accesses \nare done with offsets that are multiples of four (e.g. R1[8]), except for some of the examples in Section \n6. 3. Benchmark methodology Before embarking on the discussion of our optimisation techniques, we take \na small detour to explain our benchmark methodology. We benchmark Haskell code using the nofib benchmark \nsuite (Partain 1992), which contains 91 Haskell programs of various sizes, rangingfrom small micro-benchmarks(tak, \nrfib)to large programs solving real problems (e.g. LZH compression, hidden line removal, and a Prolog \ninterpreter). We make no apology for including the micro-benchmarks: in practice even large programs \noften have small inner loops, and the micro-benchmarks are useful for highlighting extreme cases in sharp \nrelief. This paper contains several tables of results taken from runs of the nofib benchmark suite, e.g. \nFigure 13. In these tables we do not attempt to list every single result; rather we present a selection \nof representativeprograms, and aggregate minimum/maximum and geometric means taken over the whole suite. \nFor each benchmark, we can take a range of measurements: Runtime: wall-clock running time averaged over \n10 runs on a quiet machine  Allocation: the amount of memory allocated by the program over its lifetime \n Code size:the size of the binary in bytes  We made most of our measurements on two separate machines: \n An Intel Xeon, 2.4GHz, using the IA32instruction set,  An AMD Opteron 250, using the x86-64 instruction \nset with 64-bit pointers.  Additionally, we have extended GHC with support for reading the CPU performance \ncounters provided by many modern CPUs. These enableustotakeaccurate measurementsofCPUeventssuch as raw \ncycles, instructions executed, cache misses, branch mispre\u00addictions, and so on.We performed our CPU-counter \nmeasurements on the AMD Opteron system only. We added hooks to the GHC runtime system that start and \nstop the CPU performance counters around the execution of Haskell code. We were thereby able to count \nevents that occur during the execution of the Haskell code (the mutator ) separately from those that \noccur in thegarbage collector, which is important since we re primarily interested in what is happening \nin the program itself. None of the techniques in this paper affect the amount of memory allocated by \nthe program, and hence the amount of work thegarbage collector has to do. 4. Measuring the dynamic behaviour \nof case expressions The target for our optimisation is this: the code path taken when a case expression \nthat scrutinises a single variable .nds that the closure referenced by the variable is already evaluated. \nFor these cases, we can hope to generate a fast path that avoids indirect jumps (and the associated branch \nmispredictions),byusing a more tag-ful approach. Note that not all case expressions scrutinise a single \nvariable. GHC s intermediate language allows case expressions that scru\u00adtinise an arbitrary expression; \nfor example: case (f x) of [] -> ... y:ys -> ... However, only case expressions that scrutinise a single \nvariable can possibly take advantage of the fast path, so we restrict our attention to this subclass \nof case expressions.For the remainderof the paper, we will use the term case expression to mean case \nexpression that scrutinises a single variable . The effectiveness of the optimisation depends crucially \non the frequency with which a case expression scrutinises an already\u00adevaluatedvariable.Weknowofnoprevious \nmeasurementsof this frequency, so we performed an experiment to .nd out. Our results are given in Figure \n2, which displays the dynamic proportion of all variable-scrutinising case expressions that found the \nclosure to be already evaluated. There was a wide variation (min 0.2%, max 99%), with a geometric mean \nof 62%. This .gure indicates that the already-evaluated scenariois common, andhenceworthy Program Evaluated \nscrutinee (%) anna 65.1 cacheprof 72.8 constraints 54.5 fulsom 41.5 integrate 67.6 mandel 73.9 simple \n78.6 sphere 72.8 typecheck 56.5 wang 41.9 (81 more) ... Min 0.20 Max 99.00 Average 61.86 Figure 2. Percentage \nof scrutinees already evaluated of attention;but the widevariation suggests that we shouldavoid imposing \nexcessive costs on the case where the closure is not evaluated. 5. Scheme 1: semi-tagging Inthis sectionweexamineastraightforward \nschemetoavoid enter\u00adand-return sequences in closure evaluation. This scheme produces code that does not \nenter closures that are already evaluated. The basic idea is to test the type of the closure before entering \nit: if the closure is a constructor, then we continue directly with the code to examine the constructor, \notherwise we enter the closure as normal to evaluate it. Version 1. The simplest modi.cation to the code \ngenerator is to add this test directly before the enter: map_entry() { // Set up continuation as before: \nSp[-4] = R2 R1 = R3; Sp[-8] = map_ret; Sp = Sp -8; // New code follows: // grab the info pointer from \nthe closure info = R1[0]; // grab the type field from the info table type = info[-12]; // if it s a constructor, \njump to the // continuation if (type <= MAX_CONSTR_TYPE) jump map_ret; jump info; } data { word32[] = \n{...} } // Info table for map_ret map_ret() { // exactly as before: info = R1[0]; tag = info[-8]; if \n(tag == 0) jump tag_0; ... code for (f x : map f xs ) ... tag_0: ... code for [] ... } Weread the info \npointer,and then read the .eld of the info table that contains the type of the closure (remember that \nthe info table is laid out backwards in memory directly before the info pointer, so this offsetisnegative).Thetypeofthe \nclosurecanthenbeexamined,to determine whetheritisa constructor.Forvarious reasons several different object \ntypes all represent constructors, but we arrange that constructor types occupy the lowest-numbered values, \nso a single comparison suf.ces (here MAX_CONSTR_TYPE is the highest\u00advalued constructor type). If the \nclosure is a constructor, then we jump directly to the continuation. The continuation code extracts the \ntag from the constructor and performs comparisons to select the appropriate branch code. Version 2. In \ntheevent that the closurewasaconstructor, the above scheme is slightly sub-optimal because we load the \ninfo pointer twice. An alternative scheme is as follows: map_entry() { Sp[-4] = R2 R1= R3; Sp[-8] = map_ret; \nSp= Sp-8; jump map_ret; } data { word32[] = {...} } // Info table for map_ret map_ret() { info = R1[0]; \ntype = info[-12] if (type > MAX_CONSTR_TYPE) jump info; tag = info[-8]; if (tag == 0) jump tag_0; ... \ncode for (f x : map f xs ) ... tag_0: ... code for [] ... } All the comparisons are done in the continuation. \nThis means an extra comparisonintheeventthatwehaveto enterand return,but the code sizeisoverall slightly \nsmaller.We found thisversiontobe marginally better. Version 3. There is one further improvement that \nwe could make: map_entry() { Sp[-4] = R2 R1= R3; Sp[-8] = map_ret; Sp= Sp-8; jump map_cont; } map_cont() \n{ info = R1[0]; type = info[-12] if (type > MAX_CONSTR_TYPE) jump info; tag = info[-8]; if (tag == 0) \njump tag_0; ... code for (f x : map f xs ) ... tag_0: ... code for [] ... } data { word32[] = {...} } \n// Info table for map_ret map_ret() With semi-tagging (.%) Program Code size Runtime anna +2.7 -5.3 cacheprof \n+1.7 -10.4 constraints +1.5 -11.6 fulsom +2.5 +4.9 integrate +1.6 -2.1 mandel +1.7 -17.9 simple +3.1 \n-9.8 sphere +2.2 0.18 typecheck +1.3 -18.1 wang +1.8 +9.3 (81 more) . . . . . . Min +0.9 -33.7 Max +3.1 \n+9.3 Geometric Mean +1.5 -7.2 Figure 3. Semi-tagging performance (AMD Opteron, 64-bit) With semi-tagging \n(.%) Program Code size Runtime anna +3.9 -10.4 cacheprof +2.4 -9.7 constraints +2.1 -3.1 fulsom +3.6 \n+1.3 integrate +2.4 -13.0 mandel +2.5 -17.0 simple +4.8 -23.4 sphere +3.2 -15.0 typecheck +1.8 -17.6 \nwang +2.6 -4.9 (81 more) . . . . . . Min +1.3 -37.2 Max +4.8 +5.8 Geometric Mean +2.2 -9.4 Figure 4. \nSemi-tagging performance (Intel P4, 32-bit) { jump map_cont; } Now map_entry canfall through directly \nto map_cont, because map_cont has no info table. This requiresafall-through optimisa\u00adtion that our back-end \ndoes not currently implement, so we have not tested this version. We expect it to be a marginal improve\u00adment, \nsince in total it eliminates one (direct) jump in the already\u00adevaluated case. Implementingversion2above \nrequired about 100 linesofmodi.\u00adcations to the STG-to-C--code generator inside GHC. 5.1 Results In Figures3and4we \nshow theeffectof semi-tagging on code size and run-time, on the AMD Opteron and Intel Xeon respectively. \nWe can see that while adding semi-tagging increases code size slightly,it hasa dramatic impact on performance: \n7.2%faster on the Opteron, and 9.4%faster on the Xeon. We conjectured that the difference is due mainly \nto the higher cost of branch mispredictions on the Xeon, namely 24 clock cycles on the Xeonversus 10-12 \non the Opteron(Fog 2006).To substantiate this guess, we used the CPU counters to measure the total num\u00adberof \nbranchesexecuted,andthe numberof mispredicted branches duringtheexecutionofHaskellcode(excludingthegarbage \ncollec\u00adtor). The results are shown in Figure 5. Baseline With semi-tagging Program Branch Branches Branch \nBranch miss rate executed misses miss rate (%) (.%) (.%) (%) anna 25.45 +20.5 -46.6 11.30 cacheprof 13.21 \n+10.7 -50.4 5.90 constraints 17.68 +17.6 -36.5 9.51 fulsom 23.29 +25.3 -21.4 14.61 integrate 18.20 +14.9 \n-35.9 10.14 mandel 22.50 +10.4 -47.0 10.80 simple 29.00 +13.3 -55.3 11.45 sphere 22.26 +13.2 -48.5 10.12 \ntypecheck 25.90 +20.9 -45.1 11.74 wang 19.20 +35.6 -34.8 9.23 (81 more) ... ... ... ... Min 0.20 +0.0 \n-95.0 0.20 Max 34.53 +39.1 +3.4 24.31 Average 17.43 +14.8 -46.3 8.80 Figure 5. Branch mispredictions \n(AMD Opteron, 64-bit) The .rst column shows the branch misprediction rate for the baseline compiler; \nthat is, what percentage of all branches are mispredicted, excluding the garbage collector. The next \ncolumn, Branches executed , shows the percentage increase in the total numberof branchesexecutedwhenmovingfromthe \nbaseline com\u00adpiler to the semi-tagging compiler. As we would expect, semi\u00adtagging increases the number \nof branches by around 15%, because of the extra conditional tests before entering a closure. The next \ncolumn branch misses shows the percentage increase in the total number of mispredicted branches. The \neffect is dramatic: even though there are more branches in total, the total number of mispredictions \nin a program run is almost halved! The .nal column shows the branch misprediction rate of the semi-tagging \ncompiler, which is also around half of the baseline value shown in the .rst column. (Note: the Average \n.gures for the branch misprediction rates are arithmetic means, whereas those for the percentage increase \nin branch instructions and mispredictions are, of course, still geometricmeans.) These results correlate \nwell with the changes in performance that we sawabove.Forexample, we can performaback-of-an-envelope \ncalculation for one of the example programs: cacheprof executed an average of 193.7M fewer cycles in \nthe mutator with semi\u00adtagging,andhadanaverageof 17.7Mfewer mispredicted branches. Amispredicted branch \ncosts 10-12cycles on this architecture(Fog 2006), so the difference in mispredicted branches accounts \nfor at least91%ofthedifferenceincyclesexecutedforthis program.The results are similar for other programs. \nThese measurements show that most of the speedup on the Opteron is due to the reduction of mispredicted \nbranches, and it is very likely that this is also the case for the Xeon. Hence the increased speedup \non the Xeon can probably be attributed to the higher misprediction penalty. 5.2 Indirections In the \nSTG-machine, a suspended computation is represented by a thunk, which is a combination of an info pointer \nand the free vari\u00adables of the suspended computation. When the thunk is evaluated, it overwrites itself \nwith an indirection to the value, and returns the valuetothe code that demandedit. Figure6givesa diagrammatic \nexplanation of the process. Indirectionshaveasubtleeffecton semi-tagging:athunkmayhave beenevaluated,butmay \nstillbe representedbyan indirectiontothe actual value. When a case expression examines the closure, it \nmay .nd an indirection: in our simple semi-tagging scheme, the test Figure 6. Update of a thunk Indirections/Cases \n(%) Program 0.5Mb 32Mb anna 24.1 47.1 cacheprof 5.0 9.0 constraints 22.3 24.4 fulsom 13.3 21.1 integrate \n4.0 4.4 mandel 11.9 16.9 simple 8.5 9.4 sphere 18.1 23.5 typecheck 27.1 29.2 wang 13.6 23.3 (81 more) \n... ... Min 0.00 0.00 Max 37.80 74.40 Average 10.43 18.04 Figure 7. Indirections encountered for different \nyoung generation sizes for isa constructor willfail, and the caseexpressionfalls back to entering the \nclosure. The entry code for an indirection simply returnsthevalueitpointsto,butthis sequencestill incursthe \nenter\u00adreturn penalty. Isit commonthata caseexpression encountersan indirection?The answertothis questiondependsonhow \noftenthegarbage collector runs, because thegarbage collector eliminates indirections, short\u00adcutting them \nas part of its heap traversal. So the more often the garbage collector runs, the fewer indirections will \nbe encountered during program execution. We measured the number of times an indirection was encountered \nbya caseexpressionasa percentageofthe total numberof caseex\u00adpressionsexecuted,fortwogarbage collector \nsettings.Inboth cases we used2generations,but the .rst setof resultsis with the default setting of a \n0.5Mb nursery (young generation), and the second set is for a 32Mb nursery. The size of the nurseryis \nmore signi.cant thantheoverallsizeoftheheap, becauseeachminorgarbagecol\u00adlection will remove all the indirections \nin the nursery, so increasing the size of the nursery will directly increase the length of time that \nan indirectionis visible.Weknow frompreviousexperiments that most thunks are updated soon after creation \n(Sansom and Peyton Jones 1993), and so most indirections will be in the nursery. The results, in Figure \n7, show that as the nursery size increases there is a de.nite increase in the number of indirections \nwe en\u00adcounter:onaverage8% morewhen changingfromthedefaultheap setting of an 0.5Mb nursery to a 32Mb nursery. \nSome programs show a more dramatic difference: for example one of our micro\u00adbenchmarks, wheel-sieve2 \n(not shown in the table), goes from 1.8%witha0.5Mb nurseryto74%witha32Mb nursery.Formost programs, the \ndifference is much less. In Section 4, when we counted the number of times a case expres\u00adsion encountered \nan evaluated closure, we did not count indirec\u00adtions in the class of evaluated closures. So combining \nthe results, we .nd that as we increase the nursery size from 0.5Mb to 32Mb, the numberofevaluated closures \nthat are encounteredbya caseex\u00adpressionfallsby8%,from62% (measuredin Section4)to54%. So although indirections \nseem to be highly signi.cant in at least a few programs, it is still safe to say that on the whole a \nmajority of case expressions directly scrutinise a constructor. One could recover this lost 8% by instead \ntreating an indirection as an evaluated closure, and adding an extra alternative in the case-analysis \ncode; in the case of an indirection, we would simply follow the indirection and start the case analysis \nagain. This would no doubt increase code size by a few percent and might give a modest improvement in \nperformance. We have not tried this variant, although it would also be straightforward to make this a \ncompile-time selectable option. 6. Scheme 2: pointer-tagging In the semi-tagging scheme, we determined \nthe evaluatedness and the tag of a closure by inspecting the closure s info table. Next we take this \nidea one step further, by encoding this information in the pointer to the closure. The clear win from \ndoing this is that we avoid dereferencing the info pointer and polluting the cache with the info table. \nIf the closure has no free variables think of True, False, and [], for example then we can even avoid \ndereferencing the pointer at all. How do we encode the type and tag in the pointer?We take ad\u00advantage \nof the unused least-signi.cant bits of a pointer: pointers to closures are always aligned to the word \nsize of the machine, so on a 32-bit architecture the two least-signi.cant bits of a pointer are always \nzero, and on a 64-bit architecture there are three zero-bits. We encode two kinds of information in these \nbits: Whether or not the pointer is to a constructor.  For constructors, the tag of the constructor. \n First some terminology: we call a datatype compact if the number of constructors is three or fewer \non a 32-bit platform, or seven or fewer on a 64-bit platform. The tag of a compact datatype will .t in \nthe tag bits of a pointer. For a compact datatype, the tag-bit encoding we use on a 32-bit architecture \nis: (extendedintheobviouswayfora64-bit architecture).Fora non\u00adcompact datatype, the encoding is: unevaluated \nor unknown evaluated 00 01 Family Distribution (%) Cumulative (%) Size 1 42.5 42.5 2 52.4 94.9 3 1.2 \n96.1 4 0.5 96.6 5 1.7 98.3 6 0.9 99.2  7 0.0 99.2 > 7 0.8 100.0 Figure 8. Constructors encounteredinacaseexpression, \nclassi.ed by datatypefamily size It is always safe to use zero for the tag bits, since that makes no \ncommitment about the closure type. As we shall see, we cannot guarantee to tag every pointer to a constructor, \nso this approximate representationis necessary.In contrast, though,anon-zerotag must never lie, because \nwe will compile code for a case expression that relies on its veracity. Wecanonlyencodetagsuptothreeona32-bit \narchitecture,which leads to the following question: what fraction of case-analysis acts on data types \nwith three or fewer constructors? To .nd out, we performed measurements across our benchmark suite of \nthe per\u00adcentage of case expressions executed that scrutinised datatypes of variousfamily sizes.The results \naregivenin Figure8.Aswe can see from the table, if we can encode tags up to 3, then we cover morethan95%of \ncaseexpressions,andtagsupto7givesus99%. These results are unsurprising: we know that datatypes like lists, \nBool, Int, and Char are very common, and these types all have just one or two constructors. However, \nit is goodto have solid data to back up these intuitive claims. Note that even if the datatype has more \nconstructors than we can encode in the tag bits, we can still encode evaluatedness in the tag bits, which \nresults in a cheaper test for evaluatedness than in the semi-tagging scheme. We have implemented this \ndynamic pointer-tagging scheme in GHC. The following sections describe the full details of the im\u00adplementation \nandgive performance measurements. 6.1 Implementation Compared to semi-tagging, pointer-tagging is a much \nmore inva\u00adsive change: it breaks a pervasive assumption, namely that a clo\u00adsure pointer can be de-referenced, \nand hence requires that we iden\u00adtify every place in the runtime system where closure pointers are de-referenced, \nand everyplace in the compiler that generates code to do so. There are four parts to the implementation: \n Initialising tag bits for constructor pointers  Using the tag bits in a case expression  Clearing \ntag bits when dereferencing closure pointers  Propagating tag bits: thegarbage collector  We deal with \nthese separately. 6.1.1 Initialising tag bits for constructor pointers Pointer-tagging of dynamic constructors \nDynamic constructors are allocated in the heap at run-time. With pointer-tagging, the expression length \n[x]1 compiles to 1[x] is Haskell s notation for a singleton list containing x, i.e. x:[]. f_entry { ... \nincrement Hp and check for heap overflow ... Hp[-8] = cons_info; Hp[-4] = x; Hp[ 0] = Nil_closure + 1; \nR2 = Hp -8 + 2; jump length_info; } Hp is the heap-allocation pointer, held in a global register. It \npoints to the last occupied word of heap; the free space begins at the next lower address. We construct \na cons cell starting at Hp -8 containing two .elds: x and Nil_closure respectively. The Nil_closure symbol \nrefers to []:since this is a nullary con\u00adstructor we need only a single instance of it at runtime, so \nGHC compiles the single instance of a nullary constructor statically into the module that de.nes the \ntype. When we refer to a nullary con\u00adstructor,we must of course tag the pointer; hence Nil_closure+1. \nNow, when passing the address of the newly constructed cons cell to length,we should tag the pointer. \nThis is done in the assignment to R2:thevalue2 is the tag we are adding(one plus the tag of cons, which \nis one). Note howthe tag assignment is merged with the heap offset, so noextra codeis generated: thisisa \ncommon pattern, asit turns out. Pointer-tagging of static constructors A static constructor in\u00adstance \nis one whose .eld values are known statically at compile\u00adtime. There is no code generated for these constructors, \nand they are not allocated in the heap; instead the constructor is generated at compile time and placed \nin the static data of the program. Refer\u00adringtoa static constructoris donebysymbol name.For instance, \nthe expression length [ a ] is compiled into the following: data lit1_closure { word32[] = { char_info, \n97 } } data lit2_closure { word32[] = { cons_info, lit1_closure+1 , Nil_closure+1 } } f_entry { R2 = \nlit2_closure+2; ... adjust Sp ... jump length_info; } There are two static constructor instances here: \nlit1_closure represents the literal a , which is a boxed value of type Char, and lit2_closure is the \nvalue [ a ], a single cons cell with lit1_closure as the head and [] as the tail. Just as in the dy\u00adnamic \ncase above, we ensure that the pointers to static constructors are tagged, by simple arithmetic on their \naddresses: we add 1 to the occurrence of lit1_closure and add2 to the occurrence of lit2_closure (cons \nhas tag 1). If the static constructor binding is in a separately-compiled mod\u00adule, correctly tagging \nits references requires the compiler to prop\u00adagate information about the constructor binding from the \nde.n\u00ading module to the usage site. GHC does not currently propagate this information between modules, \nso we are unable to tag ref\u00aderences to static constructors in other modules. If, for example, lit2_closure \nwas de.ned in a different module than f_entry, we would set R2 to lit2_closure instead of lit_closure+2; \nbut doing so is perfectly .ne because a tag of zero is always safe. 6.1.2 Using the tag bits in a case \nexpression Acase expression enters the closure for an inspected variable only if the tag bits are zero: \nmap_entry { ... Sp[-4] = R2 R1 = R3; Sp[-8] = map_ret; Sp = Sp -8; // if it s a constructor, jump to \nthe // continuation if (R1 &#38; TAG_MASK != 0) jump map_ret; jump R1[0]; } The continuation codevaries \ndepending on the datatype size.If the datatype is compact, then the code can test and branch using the \ntag in the pointer: data { word32[] = { ... } } map_ret { tag = R1 &#38; TAG_MASK; if (tag == 1) jump \nnil_alt; // else, tag == 2 // Code for (x:xs ) alternative // Extract the fields and free vars x = R1[4 \n-2]; xs = R1[8 -2]; f = Sp[4]; ... code for (f x : map f xs ) ... nil_alt: R1 = Nil_closure + 1; Sp = \nSp + 8; jump Sp[0]; } This code makes use of an invariant about return continuations, namely that the \npointer returned in R1 is always tagged. It is not dif.cult to ensure this invariant always holds: pointers \nto freshly\u00adbuilt constructors are always tagged (Section 6.1.1), and when returning an existing constructor \nwe simply tag the pointer before returning (the code after nil_alt is a good example). When unpacking \nthe .elds of the constructor in a case alternative, we must remember to account for the tag bits in the \npointer.For example, in the cons alternative above, we fetch x and xs using offsets that take account \nof the tag bits now known to be in R1. As in the case of allocation, this adjustment costs nothing we \nsimply adjust the .eld offset. It would be possible to apply the fall-through optimisation de\u00adscribedinVersion3of \nSection5, although currently wehave not implemented this. For a non-compact datatype the case continuation \nlooks like the semi-tagging one.Howevertheinfotableaddressmustbeextracted with care because nowthe closure \npointer is tagged. Remember that non-compact datatype closures are tagged with one, so a simple offset \nadjustment suf.ces: data { word32[] = { ... } } map_ret { info = R1[-1]; tag = info[-8]; ... do branch \nselection using tag ... } 6.1.3 Dereferencing closure pointers Accessing the closure from a closure \npointer requires clearing any tag bits in the pointer; we also call this operation untagging . In the \nruntime, this is done with a macro: ... ptr points to closure ... value = UNTAG(ptr)[offset]; the macro \nUNTAG(ptr) clears the tag bits of the pointer using bit\u00adwise conjunction: ptr &#38; ~TAG_MASK. There \nwere relatively few places where we had to add untagging in the runtime: thegarbage collector,  a macro \nENTER() for evaluating arbitrary closures,  the entry code for indirections,  pre-compiled code for \nsome common closure types,  the external API for inspecting heap objects,  the heap pro.ling subsystem, \n debugging and sanity checking code.  In the code generator, untagging of a pointer is necessary only \nin two places: the code for a case expression, which we have already covered, and when enteringavariableina \ntail-call position.For an example of the latter case, consider the expression case x of (a,b) -> a Here \nwe need to generate code to tail-call a. The simplest scheme is to untag and enter a. There are two ways \nin which this scheme can be optimised: If the type of a tells us that it is a function, then we have \nno untagging to do,but we still have to enter the closure.We say more about tagging pointers to functions \nin Section 6.3.  Instead of untagging and entering, we could test the tag bits, and if the closure is \nevaluated then we could return it immediately, and otherwise enter it as normal. This can be thought \nof as a variant of the tagged case expression, where we are evaluating avariablebutdon tstaticallyknowthe \naddressofthe continua\u00adtion.Wetriedthisvariant,andfounditto resultinasmallcode size increase (0.3%) with \nno measurable performance bene.t, so we rejected this option in favour of the simpler untag and enter \nsequence for now.  6.1.4 Propagating tag bits: the garbage collector Thegarbage collectorhasakeyroletoplayinthepointer-tagging \nscheme. It is the garbage collector s job to remove indirections in the heap, and as part of doing so \nit turns an untagged pointer (the pointer to the indirection) into a tagged pointer which points directly \nto the constructor. The more often the GC runs, the fewer Program Tagged/Evaluated (%) anna 98.4 cacheprof \n99.6 constraints 99.1 fulsom 93.2 integrate 76.6 mandel 80.3 simple 90.6 sphere 94.5 typecheck 100.0 \nwang 100.0 (81 more) ... Min 73.30 Max 100.00 Average 97.34 Figure 9. Percentage of evaluated scrutinees \nthat were tagged indirections there will be (as we saw in Section5.2) and the more tagged pointers will \nbe encountered. Is it possible that there can ever be an untagged pointer to a con\u00adstructor with no intervening \nindirection? Whenever we create a constructor in the heap, the code generator guarantees to only refer \nto it using a tagged pointer, so it is never the case that a pointer to a constructor on the heap can \nbe untagged. However, this leaves one way in which untagged pointers to constructors can arise: we don \nt guarantee to tag pointers to static constructors (see Sec\u00adtion 6.1.1). We have measured how often a \ncase expression en\u00adcounters a pointer to an (evaluated) constructor that isn t tagged. The results are \ngiven in Figure 9; on average, 97% of pointers to constructors were properly tagged. Thegarbage collector \ncould add the tag bits to pointers to static constructors, which would have the effect of making it even \nmore rareto encounter anuntaggedpointertoa constructor,butinthe version of the system we measured for \nthis paper it currently doesn t. Static constructors are most commonly encountered in the form of dictionaries, \nwhich are part of the encoding of Haskell s type\u00adclass overloading, so programs that make heavy use of \noverloading would be more likely to suffer from untagged pointers.  6.2 Results Pointer-tagging is more \nef.cient than the semi-tagging scheme. From Figures10 and 11 we can see that the runtimes have been reducedby \n13.7% and 12.8% on the AMD Opteron and Intel Xeon respectively, in exchange for an increase in binary \nsize of a little over 2%. Compared to semi-tagging, our pointer-tagging implementation is winning by \n5.5% on the AMD and 3.4% on the Intel, with code size increased by less than 1%. Pointer-tagging prevents \nroughly the same amount of enter-and\u00adreturn sequences that semi-tagging does. So we do not expect a changeinthenumberofmispredicted \nbranchesfromtaggingpoint\u00aders. Our measurements con.rm this, the rate of branch mispredic\u00adtions is similar \nto those obtained from the semi-tagging scheme. For this reason we have omitted branch-prediction measurements. \nThe enhanced performance of pointer-tagging over semi-tagging comes from reduced cache activity. In most \ncase expressions branch selection can be determined from the pointer tag alone. This is in contrast with \nthe semi-tagging situation, where the info table is loaded into the cache for the sole purpose of extracting \nthe constructor tag.We can see how pointer-tagging affects the cache With semi-tagging (.%) Program Code \nsize Runtime anna +3.8 -17.3 cacheprof +2.6 -18.0 constraints +2.4 -14.4 fulsom +3.5 -3.1 integrate +2.6 \n-8.7 mandel +2.7 -24.8 simple +3.6 -22.6 sphere +3.0 0.14 typecheck +2.2 -23.8 wang +2.7 +2.1 (81 more) \n. . . . . . Min +1.4 -37.3 Max +4.0 +10.6 Geometric Mean +2.4 -13.7 Figure 10. Pointer-tagging performance \n(AMD Opteron, 64-bit)  With semi-tagging(.%) Program Code size Runtime anna +4.2 -15.0 cacheprof +2.9 \n-14.9 constraints +2.6 -4.7 fulsom +3.9 -5.9 integrate +2.9 -16.7 mandel +2.9 -19.9 simple +4.4 -26.9 \nsphere +3.4 -22.8 typecheck +2.4 -31.4 wang +3.0 -11.1 (81 more) ... ... Min +1.5 -44.4 Max +4.4 +11.4 \nGeometric Mean +2.6 -12.8 Figure 11. Pointer-tagging performance (Intel P4,32-bit) L1 D-cache L2 D-cache \nProgram accesses misses misses anna -23.4 -13.3 -28.4 cacheprof -15.9 -5.8 -8.1 constraints -12.8 -4.8 \n-5.4 fulsom -8.9 -17.2 -53.3 integrate -7.7 -2.2 +18.6 mandel -8.4 -1.9 -29.4 simple -11.8 -3.9 -4.1 \nsphere -13.0 -19.5 -73.2 typecheck -20.4 -8.5 -30.8 wang -13.4 -2.0 -10.4 (81 more) . . . . . . Min -31.5 \n-19.5 -73.2 Max +0.6 +0.5 +81.6 Geometric Mean -13.9 -4.5 -20.1 Figure 12. Cache behaviour for pointer-tagging \nvs. the baseline  behaviour of the program in Figure 12. Accesses to both levels of cache and accesses \nto main memory all decrease, on average. 6.3 Should we tag pointers to functions? Thus far our attention \nhas focused on pointers to constructors, but any functional language has another important class of val\u00adues, \nnamely functions. If we tag constructor-value pointers to indi\u00adcate their evaluatedness (and perhaps \ntheir tag), could we also tag function-value pointersto indicate theirevaluatedness(and perhaps more \nbesides)? GHC already uses an eval/apply evaluation model for function ap\u00adplication (Marlow and Peyton \nJones 2004), which behaves rather like semi-tagging. When an unknown function is applied, the func\u00adtion \nand its arguments are passed to a pre-compiledcode sequence in the runtime system. This generic apply \ncode sequence .rst checks to see whether the function closure is evaluated. If not, it evaluates the \nthunk. If so, it checks whether the arity of the func\u00adtion (held in its info table) is correct for making \nthe call. Littlewouldbegained from encoding just theevaluatednessin the function-value pointer. But if \nwe could encode the arity of the func\u00adtion as well, then we could avoid ever accessing the info table, \nwhich would improve the cache behaviour (c.f. Section 6.2). The tag bits in a function pointer would \ntherefore have the following meanings:0 means unevaluated orevaluated with unknown arity, and 1 3 (or \n1 7 on a 64-bit machine) indicates an evaluated func\u00adtion with the given arity. Calls to evaluated functions \nwith the cor\u00adrect arity represent the majority (over 90%) of calls to unknown functions,andover99%of \ncallsto unknown functions pass3 or fewer arguments (Marlow and Peyton Jones 2004), so we have enough \ntag bits to cover the majority of cases. We could go further. Rather than always making an out-of-line \ncall to the generic apply sequence for an unknown call, we could compile an inline test of the tag bits: \nif the tag bits indicate an evaluated function with the correct arity, then jump immediately to the function, \notherwisefallbacktothe genericapply.Thiswould avoid a jump in the common case that the call is to a function \nwith the correct arity, in exchange for a little extra code. There might appear to be a dif.culty in \ntagging pointers to static function closures in separate modules, in the same way that we had dif.culty \nwith tagging pointers to static constructors in other modules (Section 6.1). However, GHC already passes \ninformation about the arity of functions between compilation units tofacilitate makingfast calls to statically-known \nfunctions,and we could use this same information to correctly tag pointers to static function closures2. \nWe have a preliminary implementation of the scheme described above, and initial measurements indicate \nthat it improves perfor\u00admancebya modest 1-2%over pointer-tagging.Weplanto further investigate this scheme \nin future work. 7. Constructor returns Sofarwehave focused attentionexclusivelyon entering the scruti\u00adnee \nofa case expression. But if the scrutinee is entered, it will take a second indirectjumpwhenitsevaluationiscomplete,to \nreturnto the case continuation. In this section we discuss two issues relating to this latter control \ntransfer. 2Note that the type of a function doesn t convey its arity: for example, a function with arity1might \nreturna functionof arity2,but its typewould suggest an arity of 3. 7.1 Using call/return instructions \nAs we mentioned in Section 2, GHC generates code that manages the Haskell stack entirely separately from \nthe system-supportedC stack. As a result, a case expression must explicitly push a return address, or \ncontinuation, onto the Haskell stack; and the return takestheformofan indirectjumptothis address.Thereisalostop\u00adportunity \nhere, becauseevery processor hasbuilt-in CALL and RET instructions that help the branch-prediction hardware \nmake good predictions:a RET instruction conveys much more information than an arbitrary indirect jump. \nNevertheless, for several tiresome rea\u00adsons, GHC cannot readily make use of these instructions: The \nHaskell stackis allocatedin the heap.GHC generates code to checkfor stackover.ow,and relocatesthe stackif \nnecessary. In this way GHC can support zillions of little stacks (one per thread), each of which may \nbe only a few hundred bytes long. However, operating systems typically take signals on the user stack, \nand do no limit checking. It is often possible to arrange that signals are executed on a separate stack, \nhowever.  The code for a case continuation is normally preceded by an info table that describes its \nstack frame layout. This arrange\u00adment is convenient because the stack frame looks just like a heap closure, \nwhich we described in Section 2. The garbage collector can now use the info table to distinguish the \npoint\u00aders from non-pointers in the stack frame closure. This changes if the scrutinee is evaluated using \na CALL instruction: when the called procedure is done, it RETurns to the instruction right after the \ncall. This means that the info table can no longerbe placed beforea continuation.Thusthe possible bene.tsofa \nCALL/RET scheme must outweighthe performance penalty of abandoning the current(ef.cient)info table layoutinfavourof \nsome hash\u00ading scheme.  A tentative solution would be to use RET for constructor returns, but do JMP \nand PUSH rather than doing CALL. This turns out to be unsatisfactory: RET prediction usesabuffer internaltothepro\u00adcessor, \nwhich is used to maintain return targets. CALL instructions push return addresses into thisbuffer,but \nthis isnot the case for JMP+PUSH. So this solution would still suffer from mispredicted RET instructions. \nIn conclusion, while it is possible to use CALL/RET instructions, it is not an easy change to make to \nGHC, and we we leave this experiment for future work. 7.2 Vectored returns Fifteen years ago, the original \npaper about the STG machine de\u00adscribed an attractive optimisation to the compilation of case ex\u00adpressions, \ncalled vectored returns (Peyton Jones 1992, Section 10.4). Consider a boolean case expression: case x \nof True -> e1 False -> e2 Aswehave describeditsofar,wepusha return addressand enter x;thecodeatthe return \naddress teststhe(tagofthe) returnedvalue, to choose between e1 and e2. But suppose instead we pushed \ntwo return addresses or, rather, a pointer to a vector of two return ad\u00addresses. Now the constructor \nTrue could return directly to the .rst, while the constructor False could return directly to the second. \nThe returnstill consistsofasingle indirectbranch,butnotestneed be performed on return, so there is a \nnet saving of one conditional branch. Novectored returns(.%) Code Run mutator L2 Program size time misses \nanna -6.4 -12.1 -51.5 cacheprof -3.7 -2.9 -37.9 constraints -2.7 +8.9 +111.5 fulsom -2.9 -6.8 -51.6 integrate \n-2.7 -0.8 -8.4 mandel -2.6 -3.2 -82.8 simple -2.2 -1.6 -3.2 sphere -2.6 -9.2 -42.3 typecheck -2.7 -2.7 \n-57.4 wang -2.8 -13.0 -1.2 (81 more) ... ... ... Min -6.4 -13.0 -89.0 Max -0.7 +8.9 +111.5 Geometric \nMean -2.3 -1.5 -22.7 Figure 13. Turning offvectored returns (AMD Opteron) GHC has embodied this idea \nfor a decade. The return address on the stack still points to an info table (so that thegarbage collector \ncan walk the stack), and the vector simply forms part of the info table. The choice between direct and \nvectored return is made on a type-by-type basis. For data types with many constructors, the vector table \nwould be large and might contain many duplicate entries, so GHC uses a threshold scheme: vectored returns \nwere used for data types with8constructors orfewer. The picture changes somewhat with our tagging schemes. \nNow x is tested before being entered, and if it is evaluated the appropriate alternative is selected. \nIn these cases the use of a vectored return is irrelevant, so its bene.ts (if any) would be reduced. \nThis obser\u00advation provoked us to re-evaluate the effectiveness of the whole vectored-return idea. In \nprinciple, vectored returns still soundlike a win: the total num\u00adberof instructionsexecuted shouldbe \nreducedbyvectored returns, whilethe numberof indirectbranches shouldbe unchanged.How\u00adever, vectored returns \nimpose several less obvious costs, on both code size and run time: Run time. Vectored returns are not \ncache-friendly.Avectored re\u00adturn makes a data access to the info table of a return continu\u00adation. During \nnormal execution (garbage collection aside), the info table of a return continuation would never otherwise \nenter the data cache. These vector-table accesses therefore increase the data-cache load, and this effect \nturns out to be signi.cant. Figure 13 shows the results of our measurements of mutator\u00adonlylevel-2data \ncachemiss ratesfor ourbenchmark programs. Switching off vectored returns improves the miss-rate by 20%! \nCode size. On 64-bit processors, each tableentryis8 bytes, and the corresponding test-and-branch code \nis typically smaller than the size of the vector. Even on a 32-bit machine this can some\u00adtimes be the \ncase. Code size. The info table for a return address has an optional .eld usedto store information about \nstatic referencesforthegarbage collector. In the absence of a vector table this .eld can often be omitted,butif \nthevector tableis present then this optional .eld must always be present so that the vector table is \nin a predictable location. This causes someextra code-sizeoverhead for vectored returns. Code size, run \ntime. If we want to generate position-independent code, which we do, the vector has to contain offsets \nrelative to the vector table rather than direct pointers to the alternatives. This increases the instruction \ncount for a vectored return (cur\u00adrently 4 instructions on x86 processors, compared to one in\u00adstruction \nfor a direct return). Complexity. Vectored returns signi.cantly complicate some other aspects of the \ncompiler. In particular, certain stack frames (e.g. the code for polymorphic seq)need to be able to handleany \nreturn convention, vectored or otherwise. These standard stack frames therefore need a fully-populated \nreturn vector in addi\u00adtion to the direct-return code. When we made our measurements we found, to our \nsurprise, that even before adding semi-tagging vectored returns had a net neg\u00adative impact on performance! \n(The previous measurements, made some years ago, showed a net bene.t of around 5%.) Figure 13 shows a \ncomparison between the performance of GHC-compiled programs with vectored returns for types with up to \n8 construc\u00adtors (the default), and the same programs compiled with a .xed direct-return convention.Turningoffvectored \nreturns both reduces code size and improves performance marginally. These results are for a 64-bit machine \nusing 8-byte vector table entries. On a 32-bit machine the code size reduction is -1.3% while the running \ntime difference is -1.7%. Our conclusion is simple: vectored returns carry no net bene.t, and can safely \nbe retired, with welcome savings in the complexity budget of both compiler and runtime system. All the \nother measurements in this paper are relative to the highest\u00adperforming baseline available, namely the \none with vectored re\u00adturns switched off. Thus, all the gains reported in this paper are genuinegains \ndue to tagging alone. 8. Related work Many programming language implementations (SML/NJ, Ocaml, Squeak) \nuse tag bits to differentiate pointers from non-pointers. The garbage collector uses the tags to .nd \nthe pointers when it is marking/copying the live heap. Although it uses the same low\u00adorder-bits encoding, \nthis technique is almost unrelated to ours. Our tag bits never indicate a pointer/non-pointer distinction, \nnor do we suffer from the loss of some bits of integer precision. Furthermore, our pointer-tagging scheme \ndescribes something about the object pointed to, even though the latter may change dynamically. That \nis why the zero tag always means unknown ; and it is why the garbage collector cooperates to propagate \ntags from objects into the pointers that reference them. CMUCL uses a 3-bit tagging scheme to distinguish \nvarious types of pointer (MacLachlan 2003). The tagging scheme is global (as opposed to type-dependent \nlike ours). Scheme 48 also uses a 2\u00adbit type tag on the pointer (Kelsey and Rees 1994). Neither of these \nsystems need to support lazy evaluation, so there is no need for tagging closure pointers with laziness \ninformation. However pointer tags could, for example, encode whether a cons or nil object is being pointed-to, \njust like the dynamic pointer-tagging schemedoes.Maybethisencodingisnotused becausetheavailable tag-bits \nare already reserved for dynamic type information. The lack of static type information leaves little \nroom for CMUCL and Scheme 48 to encode more information in the pointer tag. Apossible variant on pointer-tagging \nwould be to divide memory into segments where closures with the same constructor tag reside. Inthiswayhighbitsofpointerscanbeusedas \nconstructortags,and this frees up the lowest bits to encode other semantic information. The Big Bag of \nPages (BiBoP) (Steele 1977) gives this scheme additional .exibility: the tag bits do not directly encode \nclosure information,but rather,theyindexintoatable that provides it. Both of these variants introduce \ncomplexities into the storage manager and code generation however, because now constructors must be allocated \nin different heaps, using different heap pointers. The closest directly-related work is an unpublishedpaper \nby Ham\u00admond (1993), which describes several tagging schemes and mea\u00adsures the effectiveness of each using \nhand-written code to simulate the outputofa compiler. The main focusofHammond s paperis a scheme he calls \nsemi-tagging 3, in which the least-signi.cant bits of the info pointer in a closure are used to indicate \nevaluated vs. unevaluated closures. This doesn t correspond exactly to either our semi-tagging or dynamic-pointer-tagging \nschemes, but there are reasons to believe thatitwouldfall betweenthetwo, bothin terms of complexity and \nperformance: It is likely to be slightly more ef.cient to check the tag bits on the info pointer, as \nper Hammond s semi-tagging, than to inspect the info table as in our semi-tagging, since reading the \ninfo table is an extra memory operation (albeit one which is likely to be cached, because we only read \nthe info tables of constructors and there are relatively few ofthose).  It is not likely to be as ef.cient \nas dynamic-pointer-tagging, because loading the info pointer to check the tag bits may mean an extra \nmemory operation.  Tagging the least-signi.cant bits of the info pointer adds non\u00adlocalised complexity \nto the implementation,but perhaps not as much as dynamic pointer tagging, because thegarbage collector \nwould not need to propagate tag bits.  In the GRIN intermediate language (Boquist 1999) unevaluated \nclosures are encodedbyconstructors addedbythe compilation pro\u00adcess.Everythunkandfunctionis representedbyaconstructor(with \narguments if encoding an applied function). The code for forcing an unevaluated closure cannot just enter \nthe closure because now it has no code associated with it. Instead it matches the compiler\u00adintroduced \nconstructor and jumps to the thunk or function that is represented by it. This approach is somewhat tag-ful \nbecause it in\u00adspects the closurecontent forevaluation, and like dynamic pointer\u00adtagging it avoids the \nindirect jump incurred by a tagless approach. Nethercote and Mycroft (2003) were the .rst to do low level \nmea\u00adsurements of GHC-compiled Haskell programs. They used the CPU counters from the the AMD Athlon processor \nto collect cache miss and branch prediction information. Their work states that branch misprediction \nstalls account up to 32% of execution time. Nethercote and Mycroft attributed these stalls to the indirect \njumps that implement laziness in compiled code, and theysuggested tag\u00adging pointers with an evaluatedness \nbit. Our work takes this idea astepfurther:thetagnotonly indicates evaluatedness butalso caches information \nabout the pointed-to closure, such as construc\u00adtor tags and function arities. 9. Conclusion Our conclusion \nis that it is time to take tagless out of the Spine\u00adlessTagless G-Machine (STG-machine). Our performance \n.gures before and after the optimisations con.rm that the uniform enter\u00adto-evaluate strategy is at the \nsource of half of the branch mispre\u00addiction events. To solve this problem, we have proposed two schemes \nthat do a more tag-ful evaluation of closures: semi-tagging and dynamic 3We took the liberty of re-using \nthe name pointer tagging. Dynamic pointer-tagging wins on pure perfor\u00admance: a 13% performance improvement \ncompared to semi\u00adtagging s 9% on the 32-bit processor (14%/ 7% on the 64-bit processor). However, semi-tagging \nwins comprehensively if we consider complexity: the changes to the compiler were small and localised, \nwhereas dynamic pointer-tagging changesthe fundamen\u00adtal representation of pointers and hence requires \nnon-local changes to the compiler and runtime. On balance we plan to adopt dynamic pointer-tagging in \nfuture versions of GHC. Although the changes are non-local, they are still relatively small: approximately \n600 lines to the whole system, which consists of roughly 150,000 lines. Acknowledgements The second authordidpartofthisworkduringan \ninternshipatMi\u00adcrosoft Research Cambridge in late 2007. Thanks are due to three anonymous ICFP 2007 referees \nand MikeGunter for comments and suggestions, to Anoop Iyer for his clari.cations on branch predic\u00adtion,andtoJohnvanSchieforhis \nmodi.edversionofValgrindfor experiments. References Urban Boquist. Code OptimisationTechniques for Lazy \nFunctional Lan\u00adguages. PhD thesis, Chalmers University of Technology, April 1999. URL http://www.cs.chalmers.se/~boquist/phd/phd.ps. \nAgner Fog. The microarchitecture of Intel and AMD CPUs: An optimization guide for assembly programmers \nand compiler mak\u00aders. online manual, 2006. http://www.agner.org/optimize/ microarchitecture.pdf. Kevin \nHammond. The spineless tagless G-machine NOT. unpublished, 1993. URL citeseer.ist.psu.edu/hammond93spineless.html. \nRichard A. Kelsey and Jonathan A. Rees. A tractable scheme implementation. Lisp and Symbolic Computation, \n7(4):315 335, 1994. URL http://repository.readscheme.org/ftp/papers/ vlisp-lasc/scheme48.ps.gz. Robert \nA. MacLachlan. Design of CMU Common Lisp. online manual, 2003. http://common-lisp.net/project/cmucl/doc/ \nCMUCL-design.pdf. Simon Marlow and SimonPeyton Jones. Makingafast curry: Push/enter vs. eval/apply for \nhigher-order languages. In ACM SIGPLAN Interna\u00adtional Conference on Functional Programming (ICFP 04), \npages 4 15, Snowbird, Utah, September 2004.ACM. Nicholas Nethercote and Alan Mycroft. Redux:Adynamic \ndata.ow tracer. Electr. Notes Theor. Comput. Sci., 89(2), 2003. WillD.Partain. Thenofib benchmark suite \nof Haskell programs. In John Launchbury andPatrick M. Sansom, editors, Functional Programming, Glasgow \n1992,Workshopsin Computing, pages 195 202. SpringerVer\u00adlag, 1992. Simon Peyton Jones, Norman Ramsey,and \nFermin Reig. C--:aportable as\u00adsembly language that supportsgarbage collection. In Gopalan Nadathur, editor, \nInternational Conference on Principles and Practice of Declara\u00adtive Programming, number 1702 in Lecture \nNotes in Computer Science, pages 1 28, Berlin, September 1999. Springer. Simon L. Peyton Jones. Implementing \nlazy functional languages on stock hardware: The spineless tagless G-machine. Journal of Functional Programming, \n2(2):127 202, April 1992. Patrick M. Sansom and Simon L. Peyton Jones. Generational garbage collection \nfor haskell. In Functional Programming Languages and Computer Architecture, pages 106 116, 1993. URL \nciteseer.ist. psu.edu/sansom93generational.html. Guy Lewis Steele. Data representation in PDP-10 MACLISP. \nTechnical Report AI Lab Memo AIM-420, MIT AI Lab, 1977.   \n\t\t\t", "proc_id": "1291151", "abstract": "<p>In the light of evidence that Haskell programs compiled by GHC exhibit large numbers of mispredicted branches on modern processors, we re-examine the \"tagless\" aspect of the STG-machine that GHC uses as its evaluation model.</p> <p>We propose two tagging strategies: a simple strategy called semi-tagging that seeks to avoid one common source of unpredictable indirect jumps, and a more complex strategy called dynamic pointer-tagging that uses the spare low bits in a pointer to encode information about the pointed-to object. Both of these strategies have been implemented and exhaustively measured in the context of a production compiler, GHC, and the paper contains detailed descriptions of the implementations. Our measurements demonstrate significant performance improvements (14% for dynamic pointer-tagging with only a 2% increase in code size), and we further demonstrate that much of the improvement can be attributed to the elimination of mispredicted branch instructions.</p> <p>As part of our investigations we also discovered that one optimisation in the STG-machine, vectored-returns, is no longer worthwhile and we explain why.</p>", "authors": [{"name": "Simon Marlow", "author_profile_id": "81100515135", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P265492", "email_address": "", "orcid_id": ""}, {"name": "Alexey Rodriguez Yakushev", "author_profile_id": "81337495069", "affiliation": "Utrecht University, Utrecht, Netherlands", "person_id": "P900672", "email_address": "", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "PP40033275", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291194", "year": "2007", "article_id": "1291194", "conference": "ICFP", "title": "Faster laziness using dynamic pointer tagging", "url": "http://dl.acm.org/citation.cfm?id=1291194"}