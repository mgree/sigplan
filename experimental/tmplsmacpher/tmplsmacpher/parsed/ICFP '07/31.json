{"article_publication_date": "10-01-2007", "fulltext": "\n Call-pattern Specialisation for Haskell Programs Simon Peyton Jones Microsoft Research, UK simonpj@microsoft.com \nAbstract User-de.ned data types, pattern-matching, and recursion are ubiq\u00aduitous features of Haskell \nprograms. Sometimes a function is called with arguments that are statically known to be in constructor \nform, so that the work of pattern-matching is wasted. Even worse, the argument is sometimes freshly-allocated, \nonly to be immediately decomposed by the function. In this paper we describe a simple, modular transformation \nthat spe\u00adcialises recursive functions according to their argument shapes . We describe our implementation \nof this transformation in the Glas\u00adgow Haskell Compiler, and give measurements that demonstrate substantial \nperformance improvements: a worthwhile 10% on aver\u00adage, with a factor of 10 in particular cases. Categories \nand Subject Descriptors D.1.0 [Programming tech\u00adniques]: Functional programming; D.3.3 [Programming lan\u00adguages]: \nProcessors General Terms Algorithms, Languages, Performance. Keywords Haskell, compilers, optimisation, \nspecialisation. 1. Introduction Consider the following Haskell function de.nition: last :: [a] -> a last \n[] = error \"last\" last (x: []) =x last (x : xs) = last xs The .nal equation is the heavily-used path. \nAnnoyingly, though, this equation .rst establishes that xs is a cons, in order to exclude the second \nequation and then calls last recursively on xs.The .rst thing last will do is to analyse xs to see whether \nit is a cons or nil, even though that fact is already known. A programmer who worries about this might \nrewrite last as fol\u00adlows, so that there is no redundant pattern-matching: last [] = error \"last\" last \n(x:xs) = last x xs where last x[] =x last x (y:ys) = last y ys Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 07, October 1 3, 2007, Freiburg, Germany. \nCopyright c . 2007 ACM 978-1-59593-815-2/07/0010. . . $5.00 Here last is a specialised version of the \noriginal last,spe\u00adcialised for the case when the argument is a cons. The idea of specialising a function \nfor particular argument shapes is a very general one, and is the subject of this paper. In particular, \nour con\u00adtributions are these: We describe a simple, general program transformation that spe\u00adcialises \nfunctions based on the shapes of their arguments, or call patterns (Section 3). Since these shapes are \nconstructor trees, we call it the SpecConstr transformation.  The basic idea is simple enough, and easy \nto prove correct. However, to be effective, it must specialise the right func\u00adtions in the right way, \nsomething that is governed by a set of heuristics (Section 3.3). In the light of our experience of using \nSpecConstr in practice, we have developed a series of non\u00adobvious re.nements to the basic heuristics \n(Section 4).  We have implemented SpecConstr in GHC, a state-of-the art optimising compiler for Haskell. \nThe implementation is very modular, consisting simply of a Core-to-Core transformation, and does not \ninteract with any other part of the compiler. ( Core is GHC s main intermediate language.)  We give \nmeasurements of the effectiveness of SpecConstr, both for the full nofib suite, and for a few kernel \narray\u00adfusion benchmarks (Section 5). The results are encouraging: the nofib suite shows a 10% improvement \nin run time, and the array-fusion benchmarks run twice as fast or (sometimes much) better.  Good compilers \nhave a lot of bullets in their gun; each particular bullet may only be really effective on a few targets, \nbut few pro\u00adgrams evade all the bullets. SpecConstr is an excellent bullet. It is cheap to implement \n(less than 500 lines of code in a compiler of 100,000 lines), and the programs that it hits are well \nand truly knocked for six.  2. Motivation We begin by giving some further examples that motivate the \nneed for call-pattern specialisation of recursive functions. 2.1 Avoiding allocation The last example \nshows that specialising a recursive function can avoid redundant pattern matching, but it can also avoid \nredundant allocation. Consider this standard function: drop :: Int -> [a] -> [a] drop0xs =xs dropn[] \n=[] drop n (x:xs) = drop (n-1) xs GHC translates Haskell into a small intermediate language called Core, \nwhich is what the optimiser works on. Here is the translation of drop into Core1, after a bit of inlining: \ndrop = \\n. \\xs. case n of { I# un -> case un of{ 0->xs; _-> case xs of{ [] ->[]; (y:ys) -> drop (I# \n(un -# 1)) ys }}} The .rst case takes apart n, which has type Int. In GHC the Int type is not primitive; \nit is declared like this: data Int = I# Int# This is an ordinary algebraic data type declaration, saying \nthat Int has a single constructor I#, which has a single .eld of type Int# (Peyton Jones and Launchbury \n1991). The # has a mnemonic signi.cance only; the constructor I# is just an ordinary constructor with \na funny-looking name. The type Int# is a truly primitive type, however, built into GHC; it is the type \nof 32-bit .nite-precision integer values. So the second case expression does a perfectly ordinary pattern-match \non n to bind un (short for unboxed n ), of type Int# to the value of n. Haskell is a lazy language, so \nn may be a thunk in which case the case expression will evaluate it. The second case expression tests \nwhether or not un is zero, while the third scrutinises xs to see whether or not it is of form (y:ys); \nif so, there is a recursive call to drop, passing (n-1) as argument. This argument (which must have type \nInt) is built by decrementing un, using the built-in operator -# :: Int# -> Int# -> Int#, and constructing \nan Int value using the data constructor I#. Notice that in every iteration except the last, the newly-constructed \nInt is immediately de-constructed in the recursive call. In short, there is a redundant heap allocation \nof the Int value in every iteration, leading to increased memory traf.c and garbage-collector load. Especially \nfor tight loops, eliminating allocation is highly desirable. In the case of drop, we want to specialise \nthe function for the case when the .rst argument has shape (I# <something>).The specialised function \nlooks like this: drop :: Int# -> [a] -> [a] drop = \\un.\\xs. case un of { 0->xs; _-> case xs of { [] ->[]; \n (y:ys) -> drop (un -# 1) ys }} Now there is no allocation in the loop and removing allocation from \nthe inner loop of a program can be a very big win indeed.  2.2 Stream fusion These examples are suggestive, \nbut our recent interest in SpecConstr was provoked by the work on stream fusion by Coutts et al. (2007a,b). \nTheir goal is to eliminate intermediate data struc\u00adtures, such as lists or arrays. For example, consider \nthe following function: 1 NB: this display omits all type information, which Core includes. sum_append \n:: [Int] -> [Int] -> Int sum_append xs ys = sum (xs ++ ys) We would like to compute the result without \nconstructing the inter\u00admediate list xs++ys. The details of their work can be found else\u00adwhere in this \nproceedings, but the key point is this: a fold operation (sum in this case) is performed by a stream \nfold, looking something like this: data Stream a = forall s. Stream (s -> Step a s) s data Step as= \nDone |Yield as sumStream :: Stream Int -> Int {-# INLINE sumStream #-} sumStream (Stream next s) =go \n0s where go zs =case (next s)of Done -> z Yield x s -> go (z+x) s The intention is that sumStream will \nbe inlined at its call sites, which will instantiate its body with a (perhaps rather complicated) function \nnext and a (perhaps also complicated) initial state s, thereby producing a specialised, but still recursive, \nversion of go. In the case of sum_append, here it is the code that arises after inlining sum and (++), \nand simplifying a little (this example is taken from Coutts et al. (2007a)): sum_append xs ys = go 0 \n(Left xs) where go z (Left xs) = case xs of [] -> go z (Right ys) x : xs -> go (z+x) (Left xs ) go z \n(Right ys) = case ys of [] ->z y : ys -> go (z+y) (Right ys ) Notice the recursive calls to go with \nexplicit Left and Right constructors, and the pattern matching on that same parameter. If we specialised \ngo for these two cases we would get this: sum_append xs ys = go_left 0 xs where go_left z xs = case xs \nof [] -> go_right z ys x : xs -> go_left (z+x) xs go_right z ys = case ys of [] ->z y : ys -> go_right \n(z+y) ys Now the program stands revealed: go_left adds the elements of xs into an accumulating parameter, \nand then switches to go_right, which does the same for ys. Stream fusion entwines these two loops together \ninto one, driven by a state that distin\u00adguishes them. SpecConstr unravels the loop nest, improving per\u00adformance \nby avoiding the allocation of Left and Right construc\u00adtors. Indeed, without SpecConstr the performance \nis no better than the original list-ful program. (The alert reader may notice that the good performance \nof sum_append relies on the strictness analyser spotting that go_left  Code gen Figure 1: GHC compilation \npipeline and go_right are strict in z, else the accumulating parameter will build up a chain of thunks. \nThat is indeed true, but it is also true of the original sumStream function. Furthermore, we run the \nstrictness analyser before SpecConstr, so the latter transforms the program only after strictness analysis \nhas already done its work. Hence, one does not need to worry that SpecConstr might obfus\u00adcate the program, \nthereby defeating strictness analysis.)  3. Implementing SpecConstr It is easy enough to write down \nthe desired result of the transforma\u00adtion, but we also need a general algorithm that implements it. In \nthis case, we can leverage GHC s existing infrastructure to make the al\u00adgorithm rather simple. Before \ndiscussing the SpecConstr imple\u00admentation, we therefore digress brie.y to describe this infrastruc\u00adture. \nGHC s compilation pipeline looks like Figure 1. The program is parsed, renamed, typechecked, and desugared \ninto the Core language. Core is a small, explicitly-typed lambda-calculus lan\u00adguage in the style of System \nF. The Core program is processed by a succession of Core-to-Core optimisations, one of which is SpecConstr, \nafter which it is fed to the code generator. A particularly important Core-to-Core pass is the Simpli.er,which \nimplements a large set of simple, local optimising transformations, such as constant folding, beta reduction, \ninlining, and so on (Pey\u00adton Jones and Santos 1998). A common pattern is that a sophis\u00adticated optimisation, \nsuch as strictness analysis or SpecConstr, does the heavy lifting, but produces a result program that \nis littered with local inef.ciencies, of precisely the sort that the Simplifer can clean up. The assumption \nthat the Simpli.er will run later makes each optimisation much simpler to implement. Returning now to \nSpecConstr, the implementation proceeds in three steps: Step 1: Identify the call patterns for which \nwe want to specialise the function. Step 2: Specialise: create a specialised version of the function \nfor each distinct call pattern. Step 3: Propagate: replace calls to the original function with calls \nto the appropriate specialised version. A call pattern for a particular function is a pair vC p where \nv is a list of variables, which we call the pattern variables, and p is a list of argument expressions. \nIn the case of drop,the recursive call (drop (I# (un -# 1)) ys) gives rise to the call pattern [v, ys] \nC [I# v, ys] A call pattern describes the argument templates for which we want to generate a specialised \nvariant of the function. In the case of drop, the call pattern speci.es a specialised variant for calls \nof the form drop (I# (something)) (something else) The pattern variables v, ys stand for the (something) \nholes in this template. The order of pattern variables vs in a call pattern is unimportant, and a call \npattern is insensitive to consistent a\u00adrenaming of its pattern variables. In the case of drop, the number \nof pattern variables happens to be the same as the number of arguments, but that is not in general the \ncase. To illustrate, here are some further examples of call patterns: [x] C [True,x] First argument is \nTrue, second is anything [x, xs] C [(x:xs)] Sole argument is a cons (:) We now give the details of Steps \n1-3 identi.ed above, in order of increasing dif.culty, starting with Step 3. 3.1 Step 3: Propagation \nThe third step of the algorithm, propagation, replaces calls to the original function with calls to a \nspecialised version of the function, whenever such a version has been created by the earlier steps. The \npropagation step is particularly easy to implement. The Simpli.er already provides a general mechanism \ncalled extensible rewrite rules, that allows an optimisation pass (or indeed the programmer) to create \na rewrite rule that is subsequently applied by the Simpli.er as it traverses the program (Peyton Jones \net al. 2001). Knowing that the Simpli.er will run subsequently, all Step (3) need do is create one rewrite \nrule for each call pattern. For example, in the case of drop, from the call pattern [v, ys] C [I# v, \nys] Step (3) adds the rule: {-# RULES \"drop-spec\" forall v xs. drop (I# v) xs = drop v xs #-} (More precisely, \nthis is the concrete syntax that a programmer would use to express a rewrite rule, but Step (3) directly \ncreates the rule in its internal form.) The effect of this rule is that whenever the Simpli.er sees a \ncall matching the left-hand side of the rule, it replaces the call with the right-hand side. The rule \napplies when compiling the module Data.List where drop is de.ned, but it also survives across separate \ncompilation boundaries, so that any module that imports Data.List will also exploit the rule. Given a \nfunction f, a call pattern [v1,...,vn]C[p1,...,pm],and the corresponding specialised function f., the \nrule is trivial to generate: forall v1 ...vn.f p1 ...pm =f. v1 ...vn Incidentally, since Core is an explicitly-typed, \npolymorphic lan\u00adguage, the pattern variables vsmay, and often do, include type vari\u00adables. For example, \nin its internal form the explicitly-typed rule for drop looks like this: forall (a:*)(v:Int#)(xs:[a]). \ndrop a(I# v)xs = drop avxs The existing rewrite-rule mechanism therefore completely takes care of propagation. \n 3.2 Step 2: Specialisation The Specialise step looks rather harder, because specialisation can have \na radical effect. Whole chunks of code can disappear. For example, there is one fewer case expression \nin drop compared with drop, and the allocation has disappeared; and in the last example, the call to \nerror does not appear in last at all. For\u00adtunately, the Simpli.er makes it easy. Given a function de.nition \nf =.x1 ...xm.e, and call pattern vsCpsfor f, we can construct the specialised version f. thus: f. =.v1 \n....vn.e[p1/x1,...,pm/xm] where the notation e[p/x]means the result of substituting pfor x in e. In our \nnow-familiar drop example, with call pattern [v,ys]C [I# v, ys],we get drop = \\v.\\ys. <body>[I# v/n, \nys/xs] where <body> is the Core code given near the start of Section 2.1. Of course, this code is bigger \nthan the original de.nition, since we are substituting terms pi for variables vi. But the whole point \nis that we do this precisely when (we believe that) the Simpli.er will subsequently be able to simplify \nthe substituted body. In the case of drop, for example, (case n of ...) in the original drop becomes \n(case (I# v) of ...) in the substituted body, so the case expression can be eliminated, leaving drop \n:: Int# -> [a] -> [a] drop = \\v.\\xs. case v of { 0->xs; _-> case xs of { [] ->[]; (y:ys) -> drop (I# \n(v -# 1)) ys At this stage drop calls drop. However, the Simpli.er can apply the rewrite rule \"drop-spec\" \nthat we constructed in Step (3), and that ties the knot to give the self-recursive code for drop given \nat the end of Section 2.1. In short, Step (2) is extremely simple: just make a fresh copy of the right-hand \nside of the function, instantiated with the call patterns. The Simpli.er will do the rest.  3.3 Step \n1: Identifying call patterns Now we turn to the .rst step, that of identifying the call patterns for \nwhich we want to specialise each function. Here is the set of heuristics that we used initially: we treat \na call (fe1 ...en)as a specialisable call if all of the following conditions hold: H1 The function f \nis bound by a de.nition of the form f = .x1 ...xa.e (a>0) That is, the lambdas are explicit, and the \nfunction has arity a. H2 The right hand side eis suf.ciently small . In our implemen\u00adtation this size \nthreshold is controlled by a .ag. H3 The function f is recursive, and the specialisable call appears \nin its right-hand side. H4 All f s arguments are supplied in the call; that is n = a. H5 At least one \nof the arguments ei is a constructor application. H6 That argument is case-analysed somewhere in the \nbody of f. We can only specialise functions whose de.nitions are statically visible (H1). For example, \nif f is lambda-bound, then even if we .nd a call in which f is applied to structured arguments, we cannot \nspecialise f s de.nition. The further requirement that f s de.nition has explicit lambdas allows us to \nestablish whether or not (H4-6) hold. For example, the de.nition f = head ys is not specialisable. We \nspecialise only recursive functions (H3), because they represent loops. A non-recursive function is often \nspecialised by inlining. A large function will not be inlined, however, so it might be worth considering \nspecialising non-recursive functions called from within loops; but we have not yet done so. For such \nrecursive functions, we specialise only calls found in the body of the function itself (H3 again). Calls \nfrom outside the func\u00adtion start the loop; calls in the body are part of the loop. Moreover, to keep \nthings simple we require that the call is saturated; that is, if the function de.nition looks like f \n=.x1 ...xa.e then the call has at least aarguments (H4). The essence of call-pattern specialisation is \nthat one or more argu\u00adments of the call is a constructor application (H5). However, there is no point \nin specialising the function for a call pattern unless the specialised version can take advantage of \nthe knowledge about the argument s shape, and that is what (H6) is about. Take a unary func\u00adtion f = \n.x.e, for example. The body of the specialised function is e[p/x],where e is the body of the original \nfunction, and p is a constructor application. This is only going to be an improvement if some, or preferably \nall, of the occurrences of x in e are the scru\u00adtinee of a case expression, because then the case expression \ncan be eliminated, and the constructor application need never be con\u00adstructed. For example consider: \nf xy =(case x of {Just v -> v; Nothing ->0 }) }} : (f (Just y) (y+1)) For (H5) we note that the recursive \ncall has the constructor ap\u00adplication (Just y) as its argument; while for (H6) we note that f decomposes \nits .rst argument with a case expression. Notice, however, that the deconstruction of x need not lexically \nenclose the recursive call; indeed it need not be certain to be evaluated at all. Specialisation still \neliminates the case, regardless. In our current example we get (after some simpli.cation): {-# RULE \"fs1\" \nforall y w. f (Just y) w = f y w #-} f vy=v:f y(y+1) The call-pattern-identi.cation algorithm (Step 1) \ntherefore works in two phases, as follows: Step 1.1: It traverses the program, gathering two sorts of \ninforma\u00adtion: (a) Call instances; that is, the function together with its actual arguments (i.e. not \nyet turned into call patterns). (b) Argument usage; that is, information about which argu\u00adments are \nscrutinised by case expressions.  Step 1.2: At the de.nition site for a function f it combines (a) the \ncall-instance information for f with (b) the argument-usage information that describes how f uses its \narguments, to make the call patterns for which f will be specialised. For example, consider a recursive \nfunction f = .x1x2.e. Suppose that in e we .nd a call (f (Just v)(p,q)), where both arguments are constructor \napplications. Suppose further that the argument\u00adusage information from e indicates that only x1 is scrutinised \nby a case expression. Then it would be fruitless to specialise on x2, so we generate the call pattern \n[v, w] C [Just v, w],where w is a fresh pattern variable. Pattern variables are also used in call patterns \nin place of parts of the call that do not take the form of constructor applications. For example, consider \nthe call (f (Just x)(gx)). It would be perfectly correct to specialise f for the call pattern [g, x] \nC [Just x,g x] But it would be foolish to do so, because the specialised version of f cannot usefully \nexploit the knowledge that its second argument is a function application. Instead, when turning a call \ninstance into a call pattern, GHC abstracts each sub-expression of the call that is not a constructor \napplication by a pattern variable. In our example, the derived call pattern would be [x, w] C [Just x, \nw]. Lastly, in step 1.2, GHC eliminates duplicate call patterns, modulo a-conversion of course, since \nnothing is gained by making two identical specialisations of the same function.  3.4 Summary This concludes \nthe overview of the SpecConstr transformation. The transformation is implemented in GHC, which is itself \nwritten in Haskell. As a way to make the earlier discussion more concrete, here is the type signature \nof the main SpecConstr function in GHC s implementation: specExpr :: ScEnv -> CoreExpr -> (ScUsage, CoreExpr) \n data ScEnv = SCE { sc_size :: Int, sc_how_bound :: Map Id HowBound } data HowBound = SpecFun | SpecArg \ntype ScUsage = (Calls, ArgUsage) type Calls = Map Id [Call] type Call = [CoreExpr] type ArgUsage = Set \nId The function specExpr takes a CoreExpr and an environment that gives information about the context \nof the expression. It returns a transformed expression, along with usage information (of type ScUsage) \nthat describes how the expression uses its free variables. The environment ScEnv has two .elds: sc_size, \nthe (.xed) size threshold for specialisation (H2).  sc_how_bound, a .nite mapping that identi.es specialisable \nfunctions (SpecFun), and their arguments (SpecArg). This mapping is extended in the obvious way when \nthe transforma\u00adtion moves inside the body of a specialisable function (H1). Id is GHC s data type for \nidenti.ers.  The usage information, ScUsage has two components: call in\u00adstances (Calls) and argument \nusage (ArgUsage). The former is simply a .nite mapping from a specialisable function (a SpecFun) to a \nlist of its calls, each represented by a list of arguments. The latter is a set of the identi.ers (identi.ed \nas SpecArgs) that are scrutinised by a case expression.  4. Re.ning the basic scheme The alert reader \nwill have noticed that the particular choice of call patterns does not affect correctness. We can specialise \nfor too many call patterns (so that the specialisations go unused) or too few (so that worthwhile optimisation \nopportunities are missed), but in either case the program will still run correctly. Our goal is to select \ncall patterns for which useful optimisation opportunities will arise. So only Steps 2 and 3 affect correctness, \nand it is easy to prove that they are in fact correct. Step 2 simply adds a new function de.nition, which \nhas no effect on the meaning of the program, so the only question is whether the rewrite rule created \nin Step 3 is correct. For a function f = .x1 ...xm.e, and call pattern vCp,the rule looks like: forall \nv1 ...vn.f p1 ...pm = f. v1 ...vn Does the equality claimed by the rule hold? The left-hand side of theruleis, \nby \u00df-reduction, equal to e[p/x]. The de.nition of f. is f. = .v1 ...vm.e[p/x], so the right-hand side \nof the rule is also equal to e[p/x], and we are done. Matters are much less cut-and-dried when it comes \nto identifying call patterns (Step 1). We have spent considerable time tuning the choice of call patterns \nin the light of experience, and these re.nements are the subject of the rest of this section. Remember: \nthey are all optional! 4.1 Variables that have known structure Consider this function: f1 n x = ...(case \nx of (p,q) -> f1 p y)... where y = (n,True) Although the argument (n,True) does not appear literally \nin the call, it is obvious that we would like to record a call pattern [p, n] C [p, (n,True)], in which \nthe second argument of f is a pair whose second component is True. A very similar situation arises when \nthe recursive call occurs in a branch of a case expression, thus: f2 nx =case x of {(p,q) -> ...(f2 mx)... \n} Again, although the second argument of the recursive call is not literally a constructor application, \nx is known to be a pair (p,q) at the moment of the call. So it is desirable to record the call pattern \n[m, p, q] C [m, (p,q)]. A third situation is a call like this: ... f3 (let x= hy in(x,x)) ... Again, \nf3 is not literally applied to a pair, but despite the interven\u00ading let it is clear that f3 could usefully \nbe specialised for the call pattern [p, q] C (p,q). Incidentally, the reader might wonder why the Simpli.er \ndoes not instead eliminate the let in the .rst place. It cannot substitute (h y) for x, because that \nwould duplicate the call of h. Alterna\u00adtively, it could .oat the let outwards, to give this: ... let \nx =hy in f3(x,x) ... Indeed it will do so if f3 is strict, but not otherwise, because if f3 is lazy the \ntransformed program risks allocates two objects (the thunk for (h y) and the pair (x,x)) instead of one \n(the thunk for let x= hy in(x,x)). To summarise, here is the re.nement: R1: when collecting call patterns, \nSpecConstr should take ac\u00adcount of Variables that are let-bound to a constructor application (example: \nf1).  Variables that have been case-analysed by an enclosing case expression (example: f2).  Arguments \nthat are constructor applications disguised by enclosing lets (example: f3).  Exactly the same three \nre.nements must also be made to the Sim\u00adpli.er s rule matcher. For example, the call pattern for f2 will \ngen\u00aderate a rewrite rule looking like this: {-# RULES \"f2-spec\" forall m p q. f2 m (p,q) = f2 m p q #-} \nThe rule matcher embodied in the Simpli.er must spot that the call in the right hand side of f2 matches \nthis rule. Similarly, the rule matcher should spot that the call f2m (let x= hy in(x,x)) is also an instance \nof rule f2-spec, and rewrite it to letx =h yin f2 mx x Notice that these re.nements to the rule matcher \nare useful for all rules, not only for those generated by SpecConstr. A reader who is familiar with Haskell \nmay also notice that f2 is strict in x, and so GHC s strictness analyser will make f2 use call\u00adby-value \nand, furthermore, will pass two components of the pair separately to the function, thereby achieving \nthe same effect as call\u00adpattern specialisation. But the SpecConstr transformation deals with two cases \nthat leave the strictness analyser helpless. First, the function may not be strict: f4True nx=n f4 False \nnx= case xof (p,q) -> ...(f4 c m x)... Second, the argument may not be of a single-constructor type: \ndata Maybe a = Nothing | Just a f5 :: Int -> Maybe Int -> Int f5 nx =case x of Nothing -> n Just p -> \n...(f5 m x)... Although f5 is strict, GHC will still pass the argument x boxed. However, the SpecConstr \ntransformation can spot that, at the recursive call, x is always of form (Just p), and can make a specialised \nversion of f5 that passes p alone, eliminating the case expression altogether. In concrete terms, the \nSpecConstr data structures sketched in Section 3.4 are modi.ed as follows: The environment ScEnv is augmented \nwith a .eld that describes the shape of any known variables: data ScEnv = SCE { ...; sc_cons :: ShapeMap \n} type ShapeMap = Map Id (DataCon,[CoreExpr]) That is, ScEnv is extended with new .eld sc_cons of type \nShapeMap, which maps an identi.er to its shape (if known). The type DataCon is GHC s data type representing \na data constructor. The call-instance information must be augmented to capture the ShapeMap at the call \nsite: data Call = Call ShapeMap [CoreExpr] For example, in f5 above, the ShapeMap would be augmented \nin the Just branch of the case with the mapping [x p].The . Just Call record collected from the body \nof f5, will look like Call [x p] [m, x]. Just 4.2 Nested structure Consider this function: f x = ... \n(f (Just (x:xs))) ... Here, the argument to f is a nested constructor application. It is obviously attractive \nto specialise for the nested call pattern [x, xs] C [Just (x:xs)]. However, this apparently-simple re.ne\u00adment \ncomplicates (H6): we only want to specialise f for the nested call pattern if f not only case-analyses \nx,but also case-analyses the argument of the Just. In general, then, we must record all the evaluation \nthat f performs on its arguments, anywhere in its body. Here is a more complicated example: data T = \nA (Either Bool Bool) | B (Int,Int) data Either a b = Left a | Right b g :: T -> Int g (A (Left p)) = \n... g (A (Right True)) = ... g (A (Right False)) = ... g(B x) =... Here are some call patterns for g, \nalong with whether we would like to specialise g for that pattern: [p] C A (Right p) Yes [] C A (Right \nTrue) Yes [] C A (Left True) No: g does not decompose the Bool [x] C B x Yes [x, y] C B (x, y) No: g \ndoes not decompose the pair So we must extract usage information from g s body that says: g s argument \nis case-analysed  If it is of form A x,then x is case-analysed:  If x has form (Left p) then p is \nnot analysed further.  If x has form (Right q) then q is case-analysed.   If it is of form B x,then \nx is not further analysed.  In this example, all the pattern matching is done at the top of g, but that \nneed not be the case; we have seen earlier examples in which the case expressions are nested inside the \nfunction, and/or passed as arguments to other function calls in the body. R2: when collecting argument-usage \ninformation, SpecConstr should record nested pattern matches within the expression. In concrete terms, \nrather than simply accumulating a set of the variables that are case-analysed, SpecConstr must accumulate, \nfor each variable, evaluation information about each possible data constructor to which that variable \nmight evaluate. This is a case where writing the code is easier than describing it informally: type ArgUsage \n= Map Id ArgOcc data ArgOcc = ArgOcc (Map DataCon [Maybe ArgOcc]) For each variable x we accumulate a \n.nite map that gives informa\u00adtion of x s occurrences. If x is in the domain of the ArgUsage map, then \nx is scrutinised; otherwise it is not. If it is in the ArgUsage map, its ArgOcc is a .nite map that summarises, \nfor each data con\u00adstructor in x s type, how the pattern-bound arguments of that data constructor are \nused, using Nothing to indicate that the argument is not scrutinised. In our example g above, the ArgOcc \nfor g s ar\u00adgument would look like this2: 2 3 2 3 Left . [ Nothing ] \u00bb 6 7 A . 4 True . [] 5 6 7 Right \n. 4 False . [] 5 B . [ Nothing ] It is easy to de.ne functions that take the union of two ArgOcc values, \nso that the ArgUsage from different sub-expressions can be merged as we move back up the tree. 4.3 Specialisation \n.xpoints Consider this recursive function: f :: Either Int Int -> (Int,Int) -> Int f (Left n) (p,q) = \nf (Right n) (q,p) f (Right n) x = if n==0 then fst x else f (Left (n-1)) x The function is somewhat contrived, \nin the interests of brevity, and we have again taken the liberty of using pattern-matching de.nitions \ninstead of the case expressions that GHC really uses. From the right hand side of f we gettwo callpatterns: \n[n, p, q] C [Right n, (q, p)] [m, x] C [Left m, x] Specialising f for these patterns gives the functions: \n--Specialise f (Right n) (q,p) f1 n qp =if n==0 then fst (q,p) else f (Left (n-1)) (q,p) 2 We have left \nout a couple of Just constructors to reduce clutter --Specialise f (Left n) x f2 n (p,q) = f (Right n) \n(q,p) (We have dropped some dead code here, although in practice that would not be done until the simpli.er \nruns in Step 3.) A cursory examination shows that the specialised function f1 has a new call pattern \nfor f, namely [m, p, q]C[Left m, (q, p)]. On re.ection, it is unsurprising that specialising a function \nmay give rise to yet-more\u00adspecialised call patterns from its right-hand side. If we specialise f for \nthis pattern too, we get: --Specialise f (Left m) (q,p) f3 mq p= f(Right m)(p,q) The call pattern from \nthe right-hand side of this specialisation is [m, q, p] C [Right m, (p, q)]; and this one is the same \nas the call pattern for f1. So we have reached a .xed point. After the simpli.er runs, and propagates \nthe specialised versions to their call sites, we get this resulting program: f(Leftn) (p,q)=f1 nqp f \n(Right n) x = if n==0 then fst x else f2 (n-1) x f1 nq p= ifn==0 then fst (q,p) else f3 (n-1) q p f2 \nn(p,q) =f1 nq p f3 mq p= f1 m pq There is no construction and deconstruction of Left and Right inside \nthe loop; and once the pair is evaluated for the .rst time it is never evaluated again. The re.nement \nwe need is this: R3: when specialising a function de.nition, collect new call pat\u00adterns from its specialised \nright-hand side. This re.nement is easy to implement: we must simply call specExpr on each specialised \ncopy of the function. In practice, instead of a two-pass algorithm substitute and then specialise GHC \nextends ScEnv with one more .eld, a substitution. Then specExpr substitutes and specialises at the same \ntime, which turns out to be quite convenient. Is there any guarantee that this iterative process will \nreach a .xed point? Yes, there is. Remember that we never specialise a function for call patterns that \nare deeper than the ArgUsage information, and this information is invariant across all the specialisations. \n 4.4 Mutual recursion and non-recursive functions So far we have only considered self -recursive functions \n(H3). But mutual recursion is quite common, and gives rise to no real dif.\u00adculty. Consider a recursive \ngroup rec { f1=e1; ... ; ...fn =en } We simply look for call instances for f1, not only in e1 but also \nin e2 ...en. Then we derive call patterns from those calls, and specialise f1 for those call patterns, \njust as before. R4: when gathering call instances for a recursive function f, look in all de.nitions \nof the letrec in which f is de.ned. This optimisation turned out to be important in practice, because \nGHC sometimes splits a self-recursive function into two mutually\u00adrecursive parts. Here is an example: \nfoo :: Maybe Int -> Int foo Nothing = 0 foo (Just 0) = foo Nothing foo (Just n) = foo (Just (n-1)) This \ndoes not look mutually recursive, but GHC lifts the constant sub-expression (foo Nothing) out of the \nloop (Peyton Jones et al. 1996), to give this mutually-recursive pair: lvl = foo Nothing foo Nothing \n= 0 foo (Just 0) = lvl foo (Just n) = foo (Just (n-1)) In this case, nothing is gained by this transformation, \nsince (foo Nothing) is evaluated only once in the loop, but GHC is not clever enough to spot this.  \n5. Results We implemented SpecConstr, including all the re.nements de\u00adscribed above, in the Glasgow Haskell \nCompiler (as at March 2007). Our implementation comprises some 475 lines of Haskell. It is a straightforward \nCore-to-Core pass, which readily slots into GHC s compilation pipeline. We measured its effectiveness \nin two ways. First, we ran the en\u00adtire nofib benchmark suite with the SpecConstr transformation switched \noff, and then again with it switched on. All other opti\u00admisations were enabled (-O2). The results are \nshown in Figure 2. The minimum, maximum and geometric means are taken over all 91 programs in the suite, \nbut the table only shows programs whose allocation changed by more than 2% or whose runtime changed by \nmore than 5%. We place little credence in small changes in runtime, because they are hard to reproduce, \nwhereas the allocation .gures are repeatable. A runtime change of - therefore means that the runtime \nwas too short to report a meaningful change. We compiled the entire set of Haskell libraries in the same \nway as the benchmark program itself (i.e. with or without running SpecConstr, respec\u00adtively). These .gures \nshow a consistent increase in binary size of a few percent, which is not surprising since we are duplicating \ncode. Some programs, such as queens and mandel2 show a dramatic decrease in allocation. Most others show \nmuch smaller changes (remember that many are suppressed altogether from the .gure), but alas a handful \nshow a noticeable increase. We investigated one of these, fibheaps in detail, and found that the increase \nwas due to reboxing, which we discuss in Section 6.1. Nevertheless, run time almost invariably decreases, \nwith a geometric mean of 10%. The second way in which we evaluated SpecConstr was by apply\u00ading it to \nsome small array-fusion examples, taken from work in the Data Parallel Haskell project (Chakravarty et \nal. 2007). We took a set of .ve example pipelines of array operations that should fuse to make a single \nloop, and tried them with and without SpecConstr. The results are dramatic, and are shown in Figure 3. \nPerformance is at least doubled, and in one case is multiplied by 10. Here is one of the pipelines: pipe1 \n:: UArr Int -> UArr Int -> UArr Int pipe1 xs ys = mapU (+1) (xs +:+ ys) It could hardly by simpler: add \nthe two arrays xs and ys element\u00adwise, and then increment each element of the resulting array. For good \nperformance it is essential that we eliminate the intermediate array. A UArr Int is an array of unboxed \nintegers, so the fused loop will run along xs and ys adding corresponding elements, incrementing the \nresult, and writing it into the result array. In Program Binary size Allocation Run time % increase % \nincrease % increase anna +5.5% +0.5% -9.0% ansi +3.2% -10.1% -26.4% atom +3.1% -0.1% -12.8% bernouilli \n+3.3% -2.0% -15.1% boyer +3.2% +0.0% -16.4% boyer2 +3.2% +3.7% \u00adcalendar +3.2% +0.6% -17.9% cichelli \n+3.3% -2.2% -2.6% circsim +3.0% -0.7% -5.1% clausify +4.7% -6.8% -5.5% comp lab zift +3.8% +0.2% -16.4% \ncompress +3.3% +0.0% -19.0% compress2 +8.7% -2.2% -13.7% constraints +3.1% -0.6% -5.8% exp3 8 +3.1% +0.0% \n-18.6% fft +2.9% -1.5% -5.2% .bheaps +2.8% +2.0% -2.5% fulsom +2.8% -0.3% -10.8% gamteb +2.9% -8.9% \u00adgcd \n+3.3% -10.6% -10.2% gen regexps +3.2% +0.0% -6.4% genfft +3.0% -0.5% -17.0% gg +3.4% +11.0% \u00adida +3.5% \n-0.5% -27.9% integer +3.2% -2.3% -11.2% knights +3.2% +0.0% -8.5% lcss +3.3% -0.0% -9.2% life +3.2% +0.0% \n-14.3% lift +3.3% +2.7% 0.00 listcompr +3.0% +0.1% -20.4% listcopy +3.0% +0.2% -22.1% mandel2 +3.5% -67.3% \n\u00admultiplier +3.2% +0.0% -12.6% parstof +2.8% -2.3% 0.01 pic +3.0% -5.6% 0.01 power +3.6% -4.6% -25.0% \nprimes +3.1% +0.0% -8.7% primetest +3.7% -4.8% -0.4% puzzle +3.2% +0.0% -11.2% queens +3.1% -79.5% -38.5% \nrewrite +3.4% -0.0% -6.1% rsa +3.8% -8.5% \u00adscs +3.0% -3.6% -9.7% simple +2.9% -0.0% -10.5% solid +2.8% \n+0.4% -7.2% sphere +3.0% -2.0% -8.9% symalg +5.2% -2.9% 0.02 transform +3.8% +0.3% -13.0% typecheck +2.9% \n+0.4% -6.2% wang +3.0% +0.3% -20.3% wheel-sieve2 +3.1% +0.0% -9.5% ...and another 40 programs... Min \n+2.8% -79.5% -38.5% Max +8.7% +11.0% +3.7% Geometric Mean +3.4% -3.7% -10.5% Figure 2: Effects of SpecConstr \non no.b programs Runtime (ms) Runtime Program without with % increase pipe1 506 205 -60% pipe2 159 77 \n-51% pipe3 284 114 -60% pipe4 545 70 -87% pipe5 6,761 720 -89% Figure 3: Effects of SpecConstr on array-fusion \npipelines the fused loop, the increment is practically free, but there is a tremendous loss of performance \nif instead we allocate and .ll an intermediate array. Array fusion is carried out using the same stream \nparadigm as that described in Section 2.2, but successful fusion is much more important. (Indeed that \nis also the weakness of the fusion approach: failure to fuse can turn a good program into a bad one, \nyet that failure might be due to an obscure and hard-to-predict interaction of optimisation heuristics. \nIt remains to be seen whether array fusion can be made reliable and robust enough that this problem does \nnot show up in practice, but that is a challenge for another day.)  6. Further work In Section 4 we \ndescribed a series of re.nements to the basic scheme. However we have also encountered shortcomings in \nour heuristics that are not so easy to .x, and we collect that experience here. 6.1 Reboxing So far we \nhave presented the advantages of specialisation. But here is a function for which matters are more ambiguous: \nf :: (Int,Int) -> [Int] f x= hx :case x of (p,q) -> f(p+1,q) The recursive call to f is a constructor, \nand the argument x is indeed decomposed inside the function, so our heuristics say we should specialise \nit. Here is the specialised function: f1 :: Int -> Int -> [Int] f1 p q= h(p,q) : f1 (p+1) q We see good \nnews: f1 does not allocate the pair (p+1,q) at the recursive call to f,as f did. But we see countervailing \nbad news: f1 allocates the pair (p,q) at the call to h,which f does not. So, matters are no worse than \nbefore, and we have eliminated the case, but they are not as much better as we might have expected. The \nproblem is that the argument x is used in f both as a case scrutinee, and as a regular argument to an \nunrelated function h. Hence, if we pass the argument in its constituted parts, p and q, we may need to \nre-box them before passing it to h. We call this the reboxing problem . In the example above, allocation \nwas no worse, and the code was shorter, so specialisation is probably still a good idea. Alas, there \nare other examples where allocation is increased by SpecConstr due to reboxing, and so specialisation \nwould be a Bad Thing and it is not easy to predict exactly when the problem will occur. One possibility \nis to modify (H6) thus: H6 Specialise on an argument xonly if xis only scrutinised by a case, and is \nnot passed to an ordinary function, or returned as part of the result. Sadly, this is too conservative. \nHere is a slightly contrived example: f :: Maybe Int -> Int -> Int fxn=casexof Nothing -> 0 Just m -> \nif n==0 then f x (n-1) else m Here x occurs passed as a regular argument to f itself! So (H6 ) would \ndisable specialisation on the grounds that we do not want to risk reboxing x.But if we do specialise \nf for the .rst argument being Just m, the specialised function will not, in fact, rebox the argument. \nA simple-minded analysis risks throwing the baby out with the bathwater. Our current implementation of \nSpecConstr simply ignores the reboxing problem. Close inspection reveals that reboxing is the reason \nthat some programs allocate more heap with SpecConstr enabled, as we saw in Section 5. We need a more \nsophisticated analysis of argument usage, so that we can get (most of) the wins without risking losses \nfrom reboxing.  6.2 Function specialisation Here is an example that arose in a real program. the_fun \n:: [a] -> Step a [a] the_fun x = ...non-recursive... getA :: [[a]] -> [a] getA [] = [] getA (x:xs) = \ngetB the_fun x xs getB :: ([a] -> Step a [a]) -> [a] -> [[a]] -> [a] getB f xxs =case (fx) of Done -> \ngetA xs Yield y ys -> y :getB f ys xs Here, the_fun is a non-recursive function, and only getA was called \nelsewhere in the program. It would obviously be pro.table to specialise getB for the case when its .rst \nargument is the_fun,so that the_fun could be inlined in the specialised copy. This paper has focused \non specialising functions for particular constructor arguments, but here we want instead to specialise \non a particular functional argument. Doing so is of more than theoretical interest: the fragment above \narose when compiling the expression concatMap (map (+ 1)) xs using the stream-fusion techniques of Coutts \net al. (2007a). Specialising for function arguments is more slippery than for con\u00adstructor arguments. \nIn the example above the argument was a sim\u00adple variable but what if it was instead a lambda term? Should \nwe generate a RULE like this? f (.x....)=e The trouble is that lambda abstractions are much more fragile \nthan constructor applications, in the sense that simple transformations may make two abstractions look \ndifferent although they have the same value. So while we could generate such a rule, there is a good \nchance that it will never match anything! An alternative approach might be to specialise only on function\u00advalued \nvariables, and perhaps also partial applications thereof. In the example above, that amounts to treating \nthe_fun very similarly to a nullary constructor. We have not yet followed this idea through to its conclusion, \nbut it looks promising.  6.3 Join points Our heuristic (H3) also only considered recursive functions. \nHow\u00adever a recursive function may have a non-recursive function de.ned inside it. For example: f xy =let \ng p= ...(f Cp)... in case x of A p-> gTrue B q -> g False C -> if y then ... else ... Here, g is a local, \nnon-recursive function de.ned inside the recur\u00adsive function f. In fact, g is what GHC calls a join point \nbecause it is a single (albeit parameterised) piece of code that describes what to do in both the A and \nB branches of the case. Here we assume that g is large enough that GHC does not inline a copy at each \ncall site. Looking at f, the only call pattern for f has arguments [C, p]. However, if we specialised \ng, we would get call patterns for f with arguments [C, True] and [C, False], which is much better. There \nare two reasons that g is not specialised: .rst, it is non-recursive; and second, even if we were to \nspecialise non-recursive functions, g does not scrutinise its argument p. We do not yet have a good heuristic \nfor specialising f. Although the situation may look contrived, it is not uncommon in practice. GHC generates \nmany local join points as a result of the crucial case-swapping transformation, described in detail in \nPeyton Jones and Santos (1998).  7. Related work Automatic function specialisation is a well-known idea. \nHowever, specialising recursive functions based on statically-known infor\u00admation is largely the domain \nof the partial evaluation community (Jones et al. 1993). Early partial evaluators would only specialise \na function if one of its arguments was completely static (i.e. known to the compiler), but later work \ngeneralised this to partially-static values (Mogensen 1988). Even then, the partially-static nature of \nthe structure usually appears to mean that part of the structure is completely known and part is unknown, \nwhereas our focus is on structures whose shape is known. The partial-evaluation work that seems closest \nto ours is Bechet s Limix partial evaluator (Bechet 1994). In particular, he handles sum types as well \nas products, and he identi.es the reboxing problem (Section 6.1). Indeed, he devel\u00adops an analysis designed \nto identify functions where reboxing is not a problem. Although there are some similarities, partial \nevaluation has quite a different .avour than the work described here. The SpecConstr transformation has \nmodest goals and modest cost, whereas partial evaluation is a whole rich research area in its own right. \nNeverthe\u00adless, SpecConstr can certainly be regarded as a rather specialised partial evaluator. Much more \nclosely related is the work of Thiemann (1994) and its precursor (Thiemann 1993). Thiemann s goal is \nprecisely the same as ours, although his context is that of a strict language. He presents a relatively \nsophisticated abstract interpretation to determine both argument usage and call patterns. Our work is \nless technically complex, but perhaps more practical; Thiemann s paper never led to a full-scale implementation. \n 8. Conclusion The SpecConstr transformation is simple to describe, economical to implement, and devastatingly \neffective for certain programs. We are not ready to declare that it should be applied to every program, \nbecause it causes an increase in code size of 3-4%, even when it does not improve performance and, on \noccasion, can decrease performance. Nevertheless, an average runtime improvement of 10%, against the \nbaseline of an already well-optimised compiler, is an excellent result. We have also identi.ed avenues \nfor further work that may improve it further. Furthermore, and perhaps most important, we have advanced \nthe enterprise of domain-speci.c optimisation technology. The idea is this: that ordinary programmers \n(i.e. not compiler writers) should be able to build libraries that, through the medium of programmer-speci.ed \nrewrite rules, effectively extend the com\u00adpiler with domain-speci.c knowledge (Peyton Jones et al. 2001). \nThe streams library is an example of just such a library, and SpecConstr eliminates a road-block on its \nusefulness.  Acknowledgements We owe a particular debt of thanks to Roman Leshchinskiy for sup\u00adplying \na long succession of small, well-characterised test cases for SpecConstr, and for his array-fusion benchmarks. \nMy thanks also goes to Brandon Moore, Peter Thiemann, and the ICFP reviewers for their helpful feedback. \n References Denis Bechet. Limix: a partial evaluator for partially static struc\u00adtures. Technical report, \nINRIA, 1994. Manuel Chakravarty, Roman Leshchinskiy, Simon Peyton Jones, and Gabriele Keller. Data Parallel \nHaskell: a status report. In ACM Sigplan Workshop on Declarative Aspects of Multicore Programming, Nice, \nJanuary 2007. Duncan Coutts, Roman Leshchinskiy, and Don Stewart. Stream fu\u00adsion: from lists to streams \nto nothing at all. In ACM SIGPLAN In\u00adternational Conference on Functional Programming (ICFP 07), Freiburg, \nGermany, October 2007a. ACM. Duncan Coutts, Don Stewart, and Roman Leshchinskiy. Rewriting Haskell strings. \nIn Practical Aspects of Declarative Languages (PADL 07), pages 50 64. Springer-Verlag, January 2007b. \nNeil D. Jones, Carsten K. Gomard, and Peter Sestoft. Partial Evaluation and Automatic Program Generation. \nPrentice Hall, 1993. Torben Mogensen. Partially static structures in a self-applicable partial evaluator. \nIn Partial Evaluation and Mixed Computation, 1988. Simon L. Peyton Jones, Andrew Tolmach, and Tony Hoare. \nPlaying by the rules: rewriting as a practical optimisation technique in GHC. In Ralf Hinze, editor, \n2001 Haskell Workshop.ACM SIGPLAN, September 2001. SL Peyton Jones and J Launchbury. Unboxed values as \n.rst class citizens. In RJM Hughes, editor, ACM Conference on Functional Programming and Computer Architecture \n(FPCA 91), volume 523 of Lecture Notes in Computer Science, pages 636 666, Boston, 1991. Springer. SL \nPeyton Jones and A Santos. A transformation-based optimiser for Haskell. Science of Computer Programming, \n32(1-3):3 47, September 1998. SL Peyton Jones, WD Partain, and A Santos. Let-.oating: mov\u00ading bindings \nto give faster programs. In ACM SIGPLAN In\u00adternational Conference on Functional Programming (ICFP 96). \nACM Press, Philadelphia, May 1996. Peter Thiemann. Higher-order redundancy elimination. In ACM SIGPLAN \nSymposium on Partial Evaluation and Semantics-Based Program Manipulation (PEPM 94), pages 73 83, Or\u00adlando, \nFlorida, June 1994. ACM. Peter Thiemann. Avoiding repeated tests in pattern matching. In Gilberto Fil\u00b4e, \neditor, 3rd International Workshop on Static Analysis, number 724 in Lecture Notes in Computer Science, \npages 141 152, Padova, Italia, September 1993. Springer Ver\u00adlag. ISBN 3-540-57264-3.  \n\t\t\t", "proc_id": "1291151", "abstract": "<p>User-defined data types, pattern-matching, and recursion are ubiquitous features of Haskell programs. Sometimes a function is called with arguments that are statically known to be in constructor form, so that the work of pattern-matching is wasted. Even worse, the argument is sometimes freshly-allocated, only to be immediately decomposed by the function.</p> <p>In this paper we describe a simple, modular transformation that specialises recursive functions according to their argument \"shapes\". We describe our implementation of this transformation in the Glasgow Haskell Compiler, and give measurements that demonstrate substantial performance improvements: a worthwhile 10% on average, with a factor of 10 in particular cases.</p>", "authors": [{"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "PP40033275", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291200", "year": "2007", "article_id": "1291200", "conference": "ICFP", "title": "Call-pattern specialisation for Haskell programs", "url": "http://dl.acm.org/citation.cfm?id=1291200"}