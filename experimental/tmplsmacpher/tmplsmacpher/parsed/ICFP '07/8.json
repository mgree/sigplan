{"article_publication_date": "10-01-2007", "fulltext": "\n Relating Complexity and Precision in Control Flow Analysis DavidVan Horn HarryG.Mairson Brandeis University \n {dvanhorn,mairson}@cs.brandeis.edu Abstract We analyze the computational complexity ofkCFA, a hierarchyof \ncontrol .ow analyses that determine which functions may be ap\u00adplied at a given call-site. This hierarchyspeci.es \nrelated decision problems, quite apart from anyalgorithms that may implement their solutions.We identifya \nsimple decision problem answeredby this analysis and prove that in the 0CFAcase, the problem is complete \nfor polynomialtime.Theproofisbasedonanonstandard, symmet\u00adric implementation of Boolean logic within multiplicative \nlinear logic (MLL).We also identifya simplerversionof 0CFArelated to .-expansion, and prove that it is \ncomplete for logarithmic space, using arguments based on computing paths and permutations. For any.xedk> \n0, it is known that kCFA(and the analogous decision problem) can be computed in time exponential in the \nprogram size. For k =1, we show that the decision problem is NP-hard, and sketch why this remains true \nfor larger .xed values of k. The proof technique depends on using the approximation of CFA as an essentially \nnondeterministic computing mechanism, as distinct from the exactness of normalization. When k = n, so \nthat the depth of the control .ow analysis grows linearly in the program length, we show that the decision \nproblem is complete for exponential time. In addition, we sketch how the analysis presented here may \nbe extended naturally to languages with control operators. All of the insights presented give clear examples \nof how straightforward observations about linearity, and linear logic, may in turn be used to give a \ngreater understanding of functional programming and program analysis. Categories and Subject Descriptors \nF.3.2[Logics and Meanings of Programs]: Semantics of Programming Languages Program analysis; D.3.3[Programming \nLanguages]: Language Constructs and Features Control structures; F.4.1[Mathematical Logic and Formal \nLanguages]: Mathematical Logic Computability theory, Computational logic, Lambda calculus and related \nsystems General Terms Languages, Theory Keywords complexity, continuation, control .ow analysis, eta \nexpansion, geometry of interaction, linear logic, normalization, proofnet, static analysis Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page.To copyotherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 07, October 1 3, 2007, \nFreiburg, Germany. Copyright c . 2007ACM 978-1-59593-815-2/07/0010...$5.00 1. Introduction We investigate \nthe precision of static, compile-time analysis, and the necessary analytic tradeoff with the computational \nresources that go into the analysis. Control .ow analysis provides a fundamental and ubiquitous static \nanalysis of higher-order programs. Heintze and McAllester (1997b) point out that in fact, some form of \nCFA is used in most forms of analyses for higher-order languages. Control .ow analysis answers the following \nbasic questions (Palsberg1995): 1. For every application, which abstractions can be applied? 2. For \nevery abstraction, to which arguments can it be applied?  These questions specify well-de.ned, signi.cant \ndecision prob\u00adlems, quite apart from anyalgorithm proposed to solve them. What is the inherent computational \ndif.culty of solving these problems? What do they have to do with normalization (evaluation), as op\u00adposedto \napproximation? Whereisthe fulcrum located betweenex\u00adact computation and approximation to it, and howdoes \nthat balance make this compile-time program analysis tractable? Toanswer either of the enumerated questions \nabove, we must be ableto approximatethesetofvaluestowhichanygivensubexpres\u00adsionmayevaluate.This approximationcanbe \nformulatedasa .ows to relation between fragments of program text the set of values a subexpression may \nevaluate to, then, is the set of appropriately de\u00ad.ned (program text) values that .ow into that subexpression. \nThis analysis possibly includes false positives (values that may .ow toa call site,butinfactdo not), \nand thusa .ows to analysis might relate every program fragment. We focus attention on an accept\u00adably \nleast analysis, thatis, a solution to the constraints which has a minimum of false positives. More precision \nmeans fewer false positives. To ensure tractability of anystatic analysis, there has to be an approximation \nof something, where information is deliberately lost in the service of providing what s left in a reasonable \namount of time.Agoodexampleof whatis lost during static analysisis that the informationgathered for each \noccurrence ofa boundvariable is merged. When variable f occurs twice in function position with two different \narguments, and a substitution of a function is made for f, a monovariant CFAwill blur which copyof the \nfunction is applied to which argument. Further re.ning the relatively coarse approximation of the above \ncontrol.ow analysis,a hierarchyof increasinglypolyvariant analyseswasdevelopedbyOlinShiversinhisPh.D. \nthesis(Shivers 1991), the so-called kCFA analyses, of which the above analysis marks the base 0CFA.A \n1CFA, forexample, usesa contour to distinguisha more speci.cand dynamic calling contextinthe anal\u00adysis \nof each program point; kCFA, then, distinguishes between k levels of such contexts. Moving up the hierarchyincreases \nthe pre\u00adcision of this analysis, by constructing more elaborate contours. However, this increased precision \nis accompanied by an empiri\u00adcally observed increase in cost. As Shivers observed in his Best of PLDI \nretrospective on the kCFA work: It did not take long to discover that the basic analysis, for any k> \n0, was intractably slow for large programs. In the ensuing years, researchers have expended a great deal \nof effort deriving clever ways to tame the cost of the analysis. Technical contributions: We identify \na simple decision problem answered by control .ow analysis and prove that in the 0CFA case, the problem \nis complete for polynomial time. The proof is based on a nonstandard, symmetric implementation of Boolean \nlogic within multiplicative linear logic (MLL).We also identifya simpler version of 0CFA related to .-expansion, \nand prove that it is complete for logarithmic space, using arguments based on computing paths and permutations. \nMoreover, we show that the decision problem for 1CFA is NP-hard. Given that there is a naive exponential \nalgorithm, we conjecture thatitisinfact NP-complete, where prudent guessing in the computation of the \nnaive algorithm can answer speci.c questions about .ows. The proof generalizes to an NP-hardness proof \nfor all .xed k> 1, demonstrating that from a complexity point of view, the hierarchyis .at for all .xed \nk> 0. Like all good proofs, this NP-hardness result has simple in\u00adtuitions. 1CFA depends on approximation, \nwhere multiple values (here, closures) .ow to the same program point, including false positives. The \nbottleneck of the naive algorithm is its handling of closures with manyfreevariables.Forexample, .w.wx1x2 \n\u00b7\u00b7\u00b7 xn has n free variables, with an exponential number of possible as\u00adsociated environments mapping \nthese variables. Approximation al\u00adlows us to bind each xi, independently, to either closed .-terms for \ntrue or false . In turn, application to an n-ary Boolean function, as analyzed in CFA, must then evaluate \nthe function on each of the possible environments. Asking whether true can .ow out of the call site then \nbecomes a way of asking if the Boolean function is satis.able. No such pseudo parallelism would be possible \nin an exact normalization it is the existence of approximation that mashes these distinct closures together. \nSimilar NP-hardness of kCFAfor k> 1 results by suitably padding the construction for 1CFA so as to render \nthe added precision useless.We remark also that the NP-hardness construction cannotbe codedin 0CFAbecausein \nthat case, there are no environments and hence no closures. Despite being the fundamental analysis of \nhigher-order pro\u00adgrams, despite being the subject of investigation for over twenty\u00ad.ve years (Jones 1981), \nand the great deal of expended effort de\u00ad riving clever ways to tame the cost , there has remained a \npoverty of analytic knowledge on the complexity of control .ow analysis and the kCFAhierarchy, the essence \nof how this hierarchyis com\u00adputed, and where the sources of approximation occur that make the analysis \nwork. This paper is intended to repair such lacunae in our understanding of program analysis. 2. Preliminaries \nIn this section, we describe the programming language that is the subject of our control .ow analysis \nand provide the necessary mechanics of the graphical representation of programs employed in our algorithms. \nThe language on which the control .ow analysis will be performed is the untyped .-calculusextendedbyalabelling \nscheme servingto index subexpressionsofa program.Following Nielson et al. (1999), we de.ne the following \nsyntactic categories: e . Exp expressions (or labeled terms) t . Term terms (or unlabeled expressions) \nA countably in.nite set of labels(Lab)is assumed and for sim\u00adplicity we suppose the set of variable names \nare included in Lab. ll .x l [e0][e1] [e] [x l[(.x.e)l[(e0 e1)l ]] ] Figure 1. Graphical coding of expressions. \n The syntaxof the languageisgivenby the following grammar:1 \u00a3 e ::= texpressions (or labeled terms) t \n::= x | (ee) | (.x.e) terms (or unlabeled expressions) Expressions are represented graphically as folllows: \na graph consists of ternary apply(@), abstraction(.), and sharing nodes (v); unary weakening nodes(8); \nand wires between these nodes. Each node has a distinguished principal port;ternary node s other auxilary \nports are distinguished as the left and right ports. We call the principal port of an @-node the function, \nthe left port the continuation, and the right port the argument. Likewise, the principal port of a .-node \nis the root, the left port is the body, and the right port is the parameter. That is: root continuation \nargument .x body parameter function Wires are labeled to correspond with the labels on terms, i.e. the \ncontinuation wire for the graph of (ee)\u00a3 will be labeled e and the root wire for (.x.e)\u00a3 will be labeled \ne. Parameter wires will be labeled with the bound variable s name. Figure1givesthe graphical codingofexpressions.Inthe \n.x.e case, the .gure is drawn as though x appears twice in e, and thus the sharing node is used to duplicate \nthe wire, one going to each occurrence. The wire between the .-node and the sharing node is implicitly \nlabeled x. The two wires attached to the auxiliary ports of the sharing node will be labeled with the \ndistinct labels of each occurrence of x. If x occurred more than twice, more sharing nodes wouldbe attachedtofan \nout the binding the appropriate amount.If the bound variable occurred exactly once, the wire would connect \nthe . node directly to the variable occurrence; the label x and the label used at the occurrence of x \nwould both refer to the same wire. If the boundvariable x did not occur in e, the wire from the .-node \nwould attach to a weakening node. The dangling wire from the graphofthebodyofthe function denotesavariablefreein \ne. In the case of (e0 e1), the .gure is drawn with e0 and e1 both having a free variable in common, i.e. \nthere is an x in fv(e0) n fv(e1), and both e0 and e1 have another free variable not occuring in the other. \n3. 0CFA Control .ow analysis seeks to answer questions such as what functions can be applied at a give \napplication position? Because both functions and applications may be copied, such questions be\u00adcome ambiguous. \nWhen one points to an application in the program text, running the program may duplicate this point several \ntimes, so which of the copies are we really asking about? Likewise, if a lambda term is said to .ow into \nsome application, which copyof the termis going to be applied? The answer 0CFAgives is: all of them all \ncopies of a term are identi.ed. Later, we see how con\u00ad 1Unlike Nielson et al. (1999), constants, binary \noperators, and recursive\u00adand let-binding syntax are omitted. These language features add nothing interesting \nto the computational complexity of the analysis. Figure 2. CFAvirtual wire propagation rules. textual \ninformation can be used to distinguish the copies and give more precise answers to these questions. We \nfollow Nielson et al. (1999) and say the result of 0CFAis an abstract cache Cbassociating abstract values \nwith each labeled program point. More precisely: b. d= P(Term) abstract values v Val b. C . Cache = Lab \n. Val dabstract caches An abstract cache mapsaprogram labeltoan abstract value vb,aset of lambda expressions, \nwhich represents the set of (textual) values that may .ow into that label s subexpression during evaluation. \nSimilarly, the abstract enviroment maps a variable to an abstract value, which represents the set of \n(textual) values that may be bound to that variable during evaluation. An acceptable control .ow analysis \nfor an expression e is writ\u00adten Cb|= e. Recalling again from Nielson et al. (1999), the accept\u00ad ability \nrelationisgivenby the greatest .xed pointof the functional de.ned according to the following clauses: \nCb|= x \u00a3 iff Cb(x) . Cb(e) b C |=(.x.e)\u00a3 iff (.x.e) . Cb(e) b\u00a31 \u00a32 \u00a31 \u00a32 \u00a30 C |=(tt )\u00a3 iff Cb|= t . Cb|= \nt ..(.x.t) . Cb(e1): 12 120 Cb|= t\u00a30 . Cb(e2) . Cb(x) . Cb(e0) . Cb(e) 0 Wenowdescribe an algorithm for \nperforming control .owanal\u00adysis that is based on the graph coding of terms. The graphical for\u00admulation \nconsists of generating a set of virtual paths for a program graph.Virtualpathsdescribean approximationoftherealpathsthat \nwill arise during program execution. Figure2de.nes the virtual path propagation rules. The left hand \nrule states thata virtual wireis added from the continuation wireto the body wire and from the variable \nwire to the argument wire of each \u00df-redex. The right hand rule states analogous wires are added to each \nvirtual \u00df-redex an apply and lambda node connected by ' a virtual path. There is a virtual path between \ntwo wires e and e, written e . e' in a CFA-graph iff: 1) e = e' , 2) there is a virtual wire from e to \ne' , 3) e connects to an auxiliary port and e' connects '' . e' to the principal port of a sharing node, \nor 4) e . e'' and e. Some care must be taken to ensure leastness when propagat\u00ading virtual wires. In \nparticular, wires are added only when there is a virtual path between a reachable apply and a lambda. \nAn ap\u00adply node is reachable if it is on the spine of the program, i.e., if e =(\u00b7\u00b7\u00b7 ((e0e1)\u00a31 e2)\u00a32 \u00b7\u00b7\u00b7 \nen)\u00a3n then the apply nodes with con\u00adtinuation wires labeled e1,...,en are reachable,oritisonthespine \nof an expression with a virtual path from a reachable apply node. The graph-based analysis can now be \nperformed in the follow\u00ading way: construct the CFA graph according to the rules in Fig\u00ad ure 2, then de.ne \nCb(e) as {(.x.e)\u00a3' | e . e' }. It is easy to see that the algorithm constructs answers that satisfy the \nacceptabil\u00adity relation specifying the analysis. Moreover, this algorithm con\u00adstructs least solutions \naccording to the partial order CbLe Cb' iff .e . Labe : Cb(e) . Cb' (e), where Labe denotes the set of \nlabels restricted to those occurring in e, the program of interest. Figure 3. Graph coding and CFAgraph. \nCb' b Lemma 1. |= e implies CbLe C' for Cbconstructed for e as described above. We now consider an example \nof use of the algorithm. Consider the labeled program: 12345678910 ((.f.((ff )(.y.y)))(.x.x)) Figure \n3 shows the graph coding of the program and the corre\u00ad sponding CFAgraph. The CFAgraph is constructed \nby adding vir\u00adtual wires 10 . 6 and f . 9, induced by the actual \u00df-redex on wire 7. Adding the virtual \npath f . 9 to the graph creates a vir\u00adtual \u00df-redex via the route 1 . f (through the sharing node), and \nf . 9 (through the virtual wire). This induces 3 . 8 and 8 . 2. Thereis nowa virtual \u00df-redex via 3 . \n8 . 2 . f . 9, so wires 6 . 8 and 8 . 5 are added. This addition creates another virtual redex via 3 \n. 8 . 2 . 5, which induces virtual wires 6 . 4 and 4 . 5. No further wires can be added, so the CFA graph \nis complete. The resulting abstract cache gives: bb C(1) = {.x} C(6) = {.x, .y} bbb C(2) = {.x} C(7) \n= {.f} C(f)= {.x} bbb C(3) = {.x, .y} C(8) = {.x, .y} C(x)= {.x, .y} bbb C(4) = {.y} C(9) = {.x} C(y)= \n{.y} bb C(5) = {.y} C(10) = {.x, .y} We now describe a natural decision problem answered by this control \n.ow analysis. After describing the problem here, subse\u00adquent sections will consider variants of it for \nkCFA. 3.1 The 0CFAdecision problem Adecision problem a question that can be answered with a yes or a \nno makes the analysis insensitive to the output size of any control .ow analysis.Typically, this analysis \ncomputes the answer to questions like what functions can be applied at a particular call site? or whatargumentscana \nparticular functionbeappliedto? , so a natural decision problem based on these questions are is this \nparticular function applied at this particular call site? or does this function get applied to this argument? \n, where the function is denoted by some lambda expression or application in the program text. These questions \nprovide ways of answering the more general to what values can a subexpression evaluate? Control Flow \nProblem (0CFA): Given expressions e, (.x.e0), and label e, is (.x.e0) . Cb(e) in a least analysis of \ne? The graphical analogue of this problem point to a .-node and an application node, and ask if there \na virtual path (describing a \u00df\u00adredex from reductions to take place) from the function port of the apply \nto the root port of the .?Or likewise, point to two.-nodes: istherea virtualpathfromthevariableportofonetotherootofthe \nother? The graph-reduction characterization has the virtue of being free of a lot of notational clutter \n(like variable renaming). We now prove that this decision problem is complete for poly\u00adnomialtime.Agraph-basedargumentfor \ncontainmentin PTIME is straightforward: Theorem 1. 0CFAis contained in PTIME. Proof. 0CFAcomputes a binary \nrelation over a .xed structure (the graph description of a program). The computation of the relation \nis monotone: it begins as empty and is added to incrementally. Because the structure is .nite, a .xed \npoint must be reached by this incremental computation. The binary relation can be at most polynomial \nin size, and each increment is computed in polynomial time. We now turn to the more interesting hardness \nof the problem. As has been observed by Henglein and Mairson (1991); Mair\u00ad son (2004); Neergaard and \nMairson (2004), linearity is thekeyin\u00adgredient in understanding the lower-bound complexity of analyses. \nIn looking at the control .ow analysis algorithm of section 3, we may ask what is the source of approximation? \nThe answer is that every copyof a function is identi.ed, and every copyof an appli\u00adcation site is identi.ed. \nIn other words, a program with copying engenders an approximate analysis. More concretely, notice that \nan application site has multiple virtual paths to a value only through virtual paths that pass through \nsharing nodes. If there are no shar\u00ading nodes as in a linear term, a term where every bound variable \noccurs exactly once it is simple to see that there is only a sin\u00adgle virtual path. By the conservativity \nof the analysis, it follows immediately that if there is a single path from an apply node to a lambda \nnode, then these nodes will fuse during reduction. Thus, linearity subverts approximation and renders \ncontrol .ow analysis synonymous with normalization.Alower bound on the complexity ofthe analysis, then,istheexpressivityofthe \nlanguageit normal\u00adizes. Proving these lower bounds amounts to functional hacking within the constraints \nof linearity. Recall the canonical PTIME-complete decision problem (Ladner 1975): CircuitValue Problem: \nGiven a Boolean circuit C of n inputs and one output, and truth values .x = x1,...,xn, is .x accepted \nby C? A problem is PTIME-hard if any instance of it can be compiled using only O(ln |x|) space into an \ninstance of the circuit value problem.We now show how to program circuits using only linear terms,proving \n0CFAtobe PTIME-hard. As a calculational tool and lingua franca amongst functional programmers(and compilers),we \nusealinear fragmentof Standard ML to illustrate our constructions. The Boolean values True and False \narebuilt outof the constants TT and FF: -fun TT (x: a,y: a)= (x,y); val TT=fn : a * a-> a * a -fun FF \n(x: a,y: a)= (y,x); val FF=fn : a * a-> a * a -val True= (TT: ( a * a -> a * a), FF: ( a * a-> a* a)); \nval True = (fn,fn) : ( a * a -> a * a) *( a* a-> a* a) -val False= (FF: ( a * a -> a * a), TT: ( a * \na -> a* a)); val False = (fn,fn) : ( a * a -> a * a) *( a* a -> a* a) This little hack will print out \nwhat Boolean we are talking about: -fun Show (u,v)= (let val (x,y)= u(true,false) in x end, let val (x,y)= \nv(true,false) in x end); val Show =fn :(bool * bool -> a * b) *(bool * bool-> c * d)-> a* c -Show True; \nval it = (true,false) : bool * bool -Show False; val it = (false,true) : bool * bool The way we compute \nAnd isto usethefamous 1930s-era codingof conjunction (hacked by Alonzo Church?) in the .rst component, \nand the disjunction in the second component. That way, we are guaranteed that the junk in v and v is \nsymmetric: one is TT, and the other is FF. Then function composition can be used to erase the symmetricgarbage \nthesloganis, symmetricgarbageis self\u00adannihilating. -fun And (p,p ) (q,q )= let val ((u,v),(u ,v )) = \n(p (q,FF), p (TT,q )) in (u,Compose (Compose (u ,v),Compose (v ,FF))) end; val And = fn :( a* ( b * b-> \nb * b) -> c *( d-> e)) *(( f* f -> f* f) * g ->( e-> h)* ( i * i-> d)) -> a * g-> c *( i* i -> h) -Show \n(And True False); val it = (false,true) : bool * bool  Notice that since p is the complement of p, and \nq that of q, we know u is the complement of u. Composing v, v and FF is always the identity function \nTT, which can then be composed with u without changing the value of u . The construction of the Or term \nis symmetric to the And term, the Not termisjustaninversion2 allthisanobvious consequence of deMorgan \nduality and the construction here of the Booleans. The Copy gate uses thefact that eitherp or p will \ninvert its argument, so that either ((TT,FF),(TT,FF)) or ((FF,TT),(FF,TT)) is returned: -fun Copy (p,p \n)= (p (TT,FF), p (FF,TT)); val Copy = fn :(( a* a -> a* a) * ( b * b -> b* b) -> c) *(( d* d -> d* d) \n* ( e * e -> e* e) -> f) -> c* f -let val (p,q)= Copy True in (Show p, Show q) end; val it = ((true,false),(true,false)) \n: (bool * bool) * (bool * bool) By writing logicgatesin continuation-passing style, forexample: -fun \nAndgate p q k= k (And p q) we can then write circuits that look like straight-line code: -fun Circuit \ne1 e2 e3 e4 e5 e6= (Andgate e2 e3 (fn e7=> (Andgate e4 e5 (fn e8=> (Andgate e7 e8 (fn f=> (Copygate f \n(fn (e9,e10)=> (Orgate e1 e9 (fn e11=> (Orgate e10 e6 (fn e12=> (Orgate e11 e12 (fn Output=> Output)))))))))))))); \nval Circuit = fn : < big type... > 2Both are omitted for space reasons. The above code says: compute \nthe and of e2 and e3, putting the result in register e7,..., make two copies of register f, putting the \nvalues in registers e9 and e10,..., compute the or of e11 and e12, putting the result in the output register. \n-Circuit True False False False False True; val it=(fn,fn) :( a* a -> a * a) *( a * a-> a * a) -Show \n(Circuit True False False False False True); val it = (true,false) : bool * bool -let val (u,u )= Circuit \nTrue False False False False True in let val ((x,y),(x ,y ))= (u (f,g), u (f ,g )) in ((x a, y b),(x \na , y b )) end end; Now for the decision problem: the pair (x,y) is either the pair (f,g) or (g,f), depending \non the Boolean circuit computing output u. Thus, is f applied to a and g to b, or the other way around? \nBecause the computation is entirely linear, the set of pos\u00adsible binders to x is this must be emphasized \nnot {f, g}: it is precisely {f}, or {g}. The 0CFA approximation isinfact anex\u00adact normalization. Theorem \n2. 0CFAis complete for PTIME. 4. kCFA Increasing the precision of the coarse approximation given by the \nabove control .ow analysis, a hierarchyof more and more re.ned analyses was developed. These analyses \nare context-sensitive or polyvariant because they distinguish functions applied in distinct call sitesto \nincreasethe precisionofthe analysis.A1CFAanalysis, for example, uses a contour to distinguish one level \nof dynamic calling context in the analysis of each program point. Contours d are strings of labels of \nlength at most k;theyserveto record the last k dynamic call points and thus distinguish instances of \nvariables and program points. Contour environments map vari\u00adable names to contours. Contours describe \nthe context in which an instance of a term evaluates, and contour environments, similarly describe the \ncontext under which a variable was bound. Forexample,in theexpression(e0((.x.e1)e2)\u00a31 )\u00a32 , the subex\u00adpression \ne1 evaluates in contour e2e1. Likewise, x is bound in con\u00adtour e2e1. d . .= Lab=k contour ce . CEnv = \nVar . . contour environment Abstractvalues areextendedto pairsof(textual) programvalues and contour environments \nclosing the term, i.e. abstract closures. The environment maps variable names to the contours in place \nat the de.nition point for the free variables. The abstract cache now maps a label and a contour to an \nabstract (closure) value. That is: d v . Val = P(Term \u00d7 CEnv) abstract values Cb. Cache =(Lab \u00d7 .) . \nVal dabstract caches An acceptable k-level control .ow analysis for an expression e ce d is written \nCb|= e, which states that Cbis an acceptable analysis of e in the context of the current environment \nce and current contour d (for the top level analysis of a program, these will both be empty). Colloquially \nwe understand these judgments as follows: when running the program, fragments of the original program \ntext may be duplicated, such as when a function copies its argument: \u00a31 \u00a32 ((.f. \u00b7\u00b7\u00b7 (fe1)\u00b7\u00b7\u00b7 (fe2)\u00b7\u00b7\u00b7)(.x.e)) \nContours distinguish between these copies (for copies created via at most k application points, beyond \nthis the distinction in copies is blurred), so when judging an analysis correct for a program fragment, \nwe use d to tell us which copy of the text is being analyzed, and ce to tell us which copies of the free \nvariables in this copyofthe program arebeing analyzed.Sointheaboveexample, x .\u00a31 x .\u00a32 we have seperate \njudgements Cb|= e and Cb|= e as a \u00a31 \u00a32 way of talking about seperate run-time instances of the expression \ne that occurs statically only once. The acceptability relation is given by the greatest .xed point of \nthe functional de.ned accordingtothe following clauses(againwe are concerned only with least solutions):3 \nbce C |= d x \u00a3 iff Cb(x, ce(x)) . Cb(e, d) ce Cb|=(.x.e)\u00a3 iff ((.x.e), ce0). Cb(e, d) d where ce0 = ce|fv(.x.e0) \nbce \u00a31 \u00a32 ce \u00a31 ce \u00a32 d 12 d 1 d 2 C |=(tt )\u00a3 iff Cb|= t . Cb|= t . \u00a30 ce \u00a30 0 .((.x.t0 ), ce0). Cb(e1,d): \nCb|= ' t0 . d0 Cb(e2,d) . Cb(x, d0) . Cb(e0,d0) . Cb(e, d) where d0 = 1d, elk and ce ' 0 = ce0[x . d0] \n The notation 1d, elk denotes the string obtainedby appending e to the end of d and taking the rightmost \nk labels. Returning to the example given in section 3, the analysis is ableto distinguisheach occurrenceof \nf as distinct. The increased precision allows us to conclude that the program is approximated by the \n(singleton) set {.y.y}. Let us consider an acceptable least analysis for the program in Figure 3.We write \nE for the empty contour and [] for the empty contour environment. Since every .-term in the program is \nclosed, the contour environments in the results will always be empty and so we omit it from this table: \nbb C(1, 10) = {.x} C(7,E)= {.f} bbb C(2, 10) = {.x} C(8, 3) = {.x} C(f, 10) = {.x} bbb C(3, 10) = {.x} \nC(8, 6) = {.y} C(x, 3) = {.x} bbb C(5, 10) = {.y} C(9,E)= {.x} C(x, 6) = {.y} bb C(6, 10) = {.y} C(10,E)= \n{.y} And the following holds: b[]f 2)3)5)6)7)9)10 C |=1 ((.f.((f 1(.y.y4(.x.x8 4.1 The kCFAdecision \nproblem As we did in subsection 3.1, we now formulate a decision problem naturally answeredbythe analysisandask:Whatisthedif.cultyof \ncomputing within this hierarchy? What are the sources of approxi\u00admation that render such analysis tractable? \nControl Flow Problem(kCFA): Given an expression e, an ab\u00ad stract closure ((.x.e0), ce0), and a label-contour \npair (e, d) with |d|= k, is ((.x.e0), ce0). Cb(e, d) in a least analysis of e? The source of approximation \nin kCFA is the bounding of the length of contour strings. But suppose k is suf.ciently large that d is \nnever truncated during the analysis. What can be said about the precision of the result? If the contour \nis never truncated, the analysis is just normalization. The acceptability relation above can be read \nas specifying a non-standard interpreter, which is given an expression and constructs a table from which \nthe normalized program can be retrieved. 3To be precise, we take as our starting pointuniform kCFArather \nthan a kCFAin which .Val. The differences are Cache =(Lab \u00d7 CEnv) . d immaterial for our purposes. Let \ns rewrite the speci.cation to make this clear. Evaluation is paramaterized by an initially empty table \nand inclusion constraints are interpreted as destructiveupdates to the table. E[t\u00a3]ce evaluates d t and \nwrites the result into the table Cbat location (e, d). \u00a3ce b E x d = C(e, d) . Cb(x, ce(x)) ]ce b E[ \n(.x.e0)\u00a3]= C(e, d) .((.x.e0), ce0) d where ce0 = ce|fv(.x.e0) \u00a31 \u00a32 ce ce\u00a32 ce t)\u00a3= \u00a31 ; E[(t12 ]d E[t1 \n]d ; \u00a3E0[t2 ]d b let ((.x.t), ce0) = C(e1,d) in 0 Cb(x, de) . Cb(e2,d); \u00a30 ce0[x.d\u00a3]E[t0 ]d\u00a3 Cb(e, d) \n. Cb(e0, de) The contour environment plays much the same role as an environ\u00admentina typical interpreter,but \nrather than mappingavariableto itsvalue,itmapsavariabletoalocationinthetablewhereitsvalue is found. The \ncontour d is a history of call sites in place at the cur\u00adrent point of evaluation and serves to keep \nlocations in the table distinct (a simple induction proof shows that (e, d) is unique). In the application \ncase, the operator and operand are evaluated, updating the table at positions (e1,d) and (e2,d), respectively. \nThe closure in (e1,d) is retrieved from the table and the variable is bound by writing the value of the \nargument (found at position (e2,d))into position(x, de). The body of the closure is evaluated in an extended \nenvironment that maps x to the location in the table where its value is stored. After evaluating the \nbody, the table is updated to record the value of the application as being the value found in (e0, de). \nThis evaluation function can be seen as a variant of the exact collecting semantics from which the analysis \nwas originally ab\u00adstracted4, in other words, if k is big enough to that it is not trun\u00adcated, the analysis \nis simply normalization. [] Lemma 2. If e has an exact kCFA, then E[e]1 constructs it. A lower bound \non the hardness of the analysis, then, is the expressivity of the language that can be evaluated in the \nabove interpreter with a d of length at most k. When the analysis is inexact, the evaluator must be modi.ed \nto perform kCFAby truncating contours at length k. The relevant change to the evaluator is in the application \ncase: \u00a31 \u00a32 ce \u00a31 ce \u00a32 ce t)\u00a3= ; E[(t12 ]d E[t1 ]d ; E[t2 ]d .(.x.t\u00a30 , ce0). Cb(e1,d): 0 Cb(x, 1d, \nelk) . Cb(e2,d); \u00a30 ce0[x.rd,\u00a3hk]E[t0 ]rd,\u00a3h k ; Cb(e, d) . Cb(e0, 1d, elk) Notice that truncation destroys \nthe uniqueness of locations in the table, so in evaluating an application, anynumber of closures can \n.ow into the operator position and all of them must be applied. Furthermore, the evaluator has to be \niterated until a .xed point in the table Cbis reached. Lemma 3. The least kCFAanalysis of e is constructedbyiterating \nE[e][] 1 until Cbreaches a .xed point. In this case, the table Cbis .nite and has n k+1 entries. Each \nentry contains a set of values and the only values are closures 4However it varies in .avor being a big \nstep semantics rather than the structured operational semantics of Nielson and Nielson (1997). The moti\u00advation \nfor SOS in Nielson and Nielson (1997) was to prove correctness of the analysis for non-terminating programs. \nOur evaluator only works for .\u00adnite programs,but sincewe areinvestigatingthe complexityofthe analysis, \nthis is agreeable. ((.x.e0), ce0);the environment in a closure mapsp free variables to anyone of n k \ncontours. Because there are n possible .x.e0 and nkp such environments, there are sets of size at most \nn 1+kp in any table entry. Observe that the above evaluation is monotonic: each table entries is initialzed \nto the empty set, and built up incrementally. Thus in kCFA, there can be at most n1+(k+1)p updates to \nCb, and \u00a3ce E[t]d then has at most n O(p) program states during evaluation. Because p = n, we conclude \nwith the well-known observation see, for example, Nielson et al. (1999, page 193): Theorem 3. kCFAis \ncontained in EXPTIME. 4.1.1 kCFAis NP-hard Because CFA makes approximations, many closures can .ow to \na single program point and contour. In 1CFA, for example, .w.wx1x2 \u00b7\u00b7\u00b7 xn has n free variables, with \nan exponential num\u00adber of possible associated environments mapping these variables to program points \n(contours of length 1). Approximation allows us to bind each xi, independently, to either of the closed \n.-terms for True or False that we sawin the PTIME-completeness proof for 0CFA. In turn, application to \nan n-ary Boolean function necessi\u00adtates computation of all 2n such bindings in order to compute the .ow \nout from the application site. The term True can only .ow out if the Boolean function is satis.able by \nsome truth valuation. (.f1.(f1 True)(f1 False)) (.x1. (.f2.(f2 True)(f2 False)) (.x2. (.f3.(f3 True)(f3 \nFalse)) (.x3. \u00b7\u00b7\u00b7 (.fn.(fn True)(fn False)) (.xn. C[(.v.f v)(.w.wx1x2 \u00b7\u00b7\u00b7 xn)]) \u00b7\u00b7\u00b7)))) For an appropriately \nchosen program point (label) e, the cache location Cb(v, e) will contain the set of all possible closures \nwhich are approximated to .ow to v. This set is that of all closures ((.w.wx1x2 \u00b7\u00b7\u00b7 xn), ce) where ce \nrangesoverallassignmentsof True and False to the free variables (or more precisely assignments of locations \nin the table containing True and False to the free variables). The Boolean function f is completely linear, \nas in the PTIME-completeness proof; the context C uses the Boolean output(s) as in the conclusion to \nthat proof: mixing in some ML, the context is: -let val (u,u )= [---] in let val ((x,y),(x ,y ))= (u \n(f,g), u (f ,g )) in ((x a, y b),(x a , y b )) end end; Again,a can only .owas an argument to f if True \n.ows to (u,u ), leaving (f,g) unchanged, which can only happen if some closure ((.w.wx1x2 \u00b7\u00b7\u00b7 xn), ce) \nprovides a satisfying truth valuation for f.We have as a consequence: Theorem 4. 1CFAis NP-hard. We observe \nthat while the computation of the entire cache Cbrequires exponential time, the existence of a speci.c \n.ow in it may well be computable in NP.A nondeterministic polynomial might compute using the collection \nsemantics E[t\u00a3]ce ,but rather than d compute entire sets, choosethe element of the set that bears witness \nto the .ow. Increasing the precision to kCFA with k> 1 undermines the approximationthatallowsfor constructionofthe \nNP-hardness proof. We use the following program transformation to render a kCFAof the original synonymous \nwith a (k + 1)CFAof the trans\u00adformed program. The NP-hardnessof kCFAfor k> 0 falls out by iterating the \ntransformation k - 1 times on the 1CFAconstruction. Transform expressions as follows, where e* is a distinguished \nlabel and k a distinguished variable not appearing in the source expression: (x \u00a3) = x \u00a3 ((e1e2)\u00a3) =((e1)(e2))\u00a3 \n\u00a3* ((.x.e)\u00a3) = ((.k.(.x.(e)))(.y.y))\u00a3 The transformationworksbynestingevery .-abstractioninasingle application \ncontext (a K-redex), which consumes the added preci\u00adsion of a (k + 1)CFA. The following lemma states \nthat top-level .ows are analogous in the transformed program under a more pre\u00adcise analysis: [] [] Lemma \n4. If Cb|=1 t\u00a3 is a least kCFA, and Cb' |=1 (t\u00a3) is a least (k + 1)CFA, then ((.x.e), ce). Cb(e) iff \n((.x.(e)), ce ' ). Cb' (e), where ce and ce ' are isomorphic upto placement of *-labels. For any.xedk, \nthis constant-factorexpansion canbe computedby a logspace-computable reduction. From this, we conclude: \nTheorem 5. kCFAis NP-hard, for any k> 0.  4.1.2 nCFAis completefor EXPTIME We now examine the complexity \nof kCFA where k is allowed to vary linearly in the size of the program. In the previous section, we saw \n0CFA was suf.cient to eval\u00aduate linear .-terms.Following the construction of Neergaard and Mairson (2004), \nwe can codeaTuring machine transition function and machine IDs with linear terms. Suppose f is a linear \ntransition function that takes a tuple (q, L, R) consisting of the current ma\u00adchine state,thetapetotheleftoftheheadinreverseorder,andthe \ntape to the right of the head. Applying f returnsa tuple (q ' ,L ' ,R ' )consistingof the new state and \ntape.Let I be the initial con.gura\u00adtion, then fI simulates one step of the machine, f(fI) two steps, \nand fnI, n steps. If 0CFA is suf.cient to evaluate fI, then what is 1CFA suf.\u00adcient for? By introducing \nnon-linearity using the Church numeral 2, we can iterate the transition function twice, as follows: 2fI \n= (.s..z.(s(sz)1)2)fI Acontourof length1issuf.cientto distinguish betweenthe appli\u00adcation of f inthecalling \ncontextof4andthatof5,weareableto maintain an exact analysis. Scaling up, suppose we have: 2(2f)I = (.s..z.(s(sz)3)4)((.s..z.(s(sz)1)2)f)I \nA contour of length2 is suf.cient to distinguish between the ap\u00adplication of f in the calling context \n13, 14, 23 and 24. In general, nCFAis suf.cient for an exact analysis of 2 n fI all of the calling contexts \nin which f is applied are distinguished. Thus nCFAis syn\u00adonymous with normalization and the program normalizes \nto f2n I. So when k is linear in the size of the program n, we can simulate a Turing machine for an exponential \nnumber of steps. Theorem 6. nCFAis complete for EXPTIME. It should be remarked that in this case, the \ncontours are large enough that the computation is essentially by normalization, with\u00adout using the power \nof any approximation. Every location of the cache Cbcontains at most one value. Researchers have noted \nthat computing a more precise analysis is often cheaper than performinga less precise one.Aless precise \nanalysis yields coarser approximations, and thus induces more merging. More merging leads to more propagation, \nwhich in turn leads to more reevaluation (Wright and Jagannathan 1998). Har\u00ad nessing the computational \npower of this reevaluation is precisely whatmakesthe NP-hardness constructionworkandrelegateslower bounds \nusing exact analyses to limit cases such as k = n;an anal\u00adysis that is exact can only be polynomial in \nn k . On the other hand, these limiting cases shed analytic light on the nature of kCFA. Even when the \npolyvariance of the analysis is taken to an extreme as in k = n, the expressivity of the analysis is \nstill limited to EXPTIME. The intractible feature of kCFAis the approximation of closures, not the degree \nof polyvariance. Poly\u00advariant static analyses such as polymorphic splitting and poly-k CFAhave since \ndeveloped without the use of closures and enjoyed tractability. 5. LOGSPACE and .-expansion Inthis section,we \nidentifya restricted classof functional programs whose 0CFA decision problem may be simpler namely, com\u00adplete \nfor LOGSPACE. Consider programs that are simply typed, and whereavariableinthe function positionortheargument \npositionof an application is fully .-expanded. This case especially,but not only when the programs are \nlinear strongly resembles multiplica\u00adtive linear logic with atomic axioms. This distinction is highlighted \nin the discussion below. We remark that .-expansion changes control .ow analysis. If 0CFAinfers that \na call site . may call a function f in a program ., and we ask the same question of the residual . and \nf in the .-expanded version of ., the answer may vary. 5.1 MLL and (linear) functional programming The \nsequent rules of MLL are: G,A A. , .G, A, B G,A .,B AX . CUT . A, AG, . ? G,A'B G, .,A . B These rules \nhave an easy functional programming interpretation as the types of a linear programming language (eg. \nlinear ML), fol\u00adlowing the intuitions of the Curry-Howard correspondence (Girard et al. 1989, Chapter \n3).5 The AXIOM rule says that a variable can be viewed simultane\u00adously asa continuation(A.)or as anexpression(A) \none man s ceiling is another man s .oor. Thus we say input of type A and output of type A. interchangably, \nalong with similar dualisms. Wealso regard(A.). synonymous with A:forexample,Int is an integer, and Int. \nisa request(need)foraninteger,andifyouneed to need an integer (Int.). then you have an integer. TheCUT \nrulesaysthatifyouhavetwo computations,onewith an outputoftype A, another with an input of type A, you \ncan plug them together. The .-rule is about pairing: it says that if you have separate computations producing \noutputs of types A and B respectively,you can combine the computations to produce a paired output of \ntype A . B. Alternatively, given two computations with A an output in one, and B an input (equivalently, \ncontinuation B. an output) in the other, theyget paired as a call site waiting for a function which produces \nan output of type B with an input of type A. Thus . is both cons and function call (@). The '-rule is \nthe linear unpairing of this .-formation. When a computation uses inputs of types A and B, these can \nbe combined 5For a more detailed discussion of the correspondence between linear ML and MLL, see Mairson \n(2004). Figure 4. Expansion algorithm. as a single input pair, e.g., let (x,y)=p in.... Alternatively, \nwhen a computation has an input of type A (output of continuation of type A.)and an output of typeB, \nthese can be combined to construct a function which inputs a call site pair, and unpairs them appropriately. \nThus ' is both unpairing and ..  5.2 Atomic versus non-atomic axioms: PTIME versus LOGSPACE The aboveAXIOMrule \ndoes not make clear whether the formula A is an atomic typevariable ora more complex type formula. Whena \nlinear program only has atomic formulas in the axiom position, then we can evaluate (normalize) it in \nlogarithmic space. When the programisnot linear,wecansimilarly computea0CFAanalysisin LOGSPACE. Moreover,these \nproblems are complete for LOGSPACE. MLL proofs with non-atomic axioms can be easily converted to ones \nwith atomic axioms using the following transformation, analogous to .-expansion: a, a. \u00df, \u00df. a . \u00df, a.,\u00df. \n. . . .. a . \u00df,a a . \u00df, a '\u00df '\u00df This transformation can increase the sizeof the proof.Forex\u00adample, in \nthe circuit examples of the previous section (which are evidence for PTIME-completeness), .-expansion \ncauses an expo\u00adnential increase in the number of proof rules used.6 A LOGSPACE evaluation is then polynomial-time \nand -space in the original circuit description. The program transformation corresponding to the above \nproof expansion is a version of .-expansion: see Figure 4. The left hand expansion rule is simply ., \ndualized in the unusual right hand rule. The right rule is written with the@above the . only to emphasis \nits duality with the left rule. Although not shownin the graphs,but impliedbythe term rewriting rules, \nan axiom may pass through any number of sharing nodes. 6Itis linearinthe formulas used, whose length \nincreasesexponentially(not so if the formulas are represented by directed acyclic graphs).  5.3 Normalizationand0CFAfor \nlinearprogramsin LOGSPACE Anormalizedlinear program has no redexes. From the type of the program, one \ncan reconstruct in a totally syntax-directed way what the structure of the term is. It is only the position \nof the axioms thatis notrevealed.Forexample, both TT and FF from the above circuit example have type \na* a -> a* a.7 From this type, we can see that the term is a .-abstraction, the parameter is unpaired \nand then, are the two components of type a repaired asbefore,or twisted ?Totwistornottotwistiswhat distinguishes \nTT from FF. The geometry of interaction (GoI) the semantics of linear logic and the notion of paths provide \na way to calculate normal forms, and may be viewed as the logician s way of talking about static program \nanalysis.8 To understand how this analysis works, we need to have a graphical picture of what a linear \nfunctional program looks like. Without loss of generality, such a program has a typef. Nodes in its graphical \npicture are either . or linear unpairing(' in MLL), orapplication/call site or linear pairing(. in MLL).We \ndraw the graphical picture so that axioms are on top, and cuts (redexes, either \u00df-redexes or pair-unpair \nredexes) are on the bottom. ax a. a. ax a. aa a \u00b7\u00b7\u00b7 .. .. f. . cut cut Because the axioms all have \natomic type, the graph has the following nice property: Lemma 5. Begin at an axiom a and descend to a \ncut-link, saving in an (initially empty) stackwhether nodes are encountered ontheirleftorright auxiliaryport.Onceacutisreached, \nascend the accompanying structure, popping the stackand continuing left or right as speci.ed by the stacktoken. \nThen (1) the stackempties exactly when the next axiom a ' is reached, and (2) if the k-th node fromthe \nstarttraversedisa .,thek-th node from the end traversed is a ', and vice versa. The path traced in the \nLemma, using the stack, is geometry of interaction (GoI), also known as static analysis. The correspon\u00addence \nbetween the k-th node from the start and end of the traversal is precisely that between a call site (.)and \nacalled function ('), or between a cons (.)and a linear unpairing('). A sketch of the four .nger normalization \nalgorithm: The stack height maybe polynomial,but wedo not need the stack! Put .ngers a, \u00df on the axiom \nwhere the path begins, and iterate over all possible choices of another two .ngers a ' ,\u00df ' at another \naxiom. Now move \u00df and \u00df ' towards the cut link, where if \u00df encounters a node on the left (right), then \n\u00df ' must moveleft (right) also. If a ' ,\u00df ' were correctly placed initially, then when \u00df arrives at the \ncut link, it must be met by \u00df '. If \u00df ' isn t there, or got stuck somehow, then a ' ,\u00df ' were incorrectly \nplaced, and we iterate to another placement and try again. Lemma 6. Any path from axiom a to axiom a \n' tracedby the stack algorithm of the previous lemma is also traversed by the four .nger normalization \nalgorithm. 7The linear logic equivalent is (a.?a.)?(a . a). The . is represented by the outer ?, the \nunpairing by the inner ?, and the consing by the .. 8See Mairson (2002) for an introduction to context \nsemantics and normal\u00ad izationby static analysisin the geometryof interaction. Normalization by static \nanalysis is synonymous with traversing these paths. Because these .ngers can be stored in logarithmic \nspace, we conclude (Terui 2002; Mairson 2006): Theorem 7. Normalization of linear, simply-typed, and \nfully .\u00adexpanded functional programs is contained in LOGSPACE. That0CFAisthen containedin LOGSPACEisacasual \nbyproduct of this theorem, due to the following observation: if application site . calls function f, \nthen the . and ' (synonymously,@ and .) denoting call site and function are in distinct trees connected \nby a CUT link. As a consequence the 0CFAcomputation is a subcase of the four-.nger algorithm: traverse \nthe two paths from the nodes to the cut link, checking that the paths are isomorphic, as described above. \nThe full 0CFAcalculation then iterates over all suchpairs of nodes. Corollary 1. 0CFAof linear, simply-typed, \nand fully .-expanded functional programs is contained in LOGSPACE.  5.4 0CFAin LOGSPACE Now let us remove \nthe linearity constraint, while continuing to insist on full .-expansion as described above, and simple \ntyping. The normalization problem is no longer contained in LOGSPACE, but rather nonelementary recursive, \n(Statman 1979; Mairson 1992; Asperti and Mairson 1998). However, 0CFAremains contained in LOGSPACE,because \nit is nowanapproximation. This result follows from the following observation: l Lemma 7. Suppose (.x.e0)l \n' and (t1 e2) occurina simply typed, fully .-expanded program and .x.e0 . Cb(l). Then the correspond\u00ading \n. and ' occur in adjacent trees connected at their roots by a CUT-link and on dual, isomorphic paths \nmodulo placement of shar\u00ading nodes. Here modulo placement means: follow the paths to the cut then we \nencounter . (resp., ')on one path when we encounter ' (resp., .)on the other, on the same (left, right) \nauxiliary ports. We thusignore traversal of sharing nodes on each path in judging whether the paths are \nisomorphic. (Without sharing nodes, the . and ' would annihilate i.e., a \u00df-redex during normalization.) \nTheorem 8. 0CFAof a simply-typed, fully .-expanded program is contained in LOGSPACE. Observe that 0CFAde.nes \nan approximate form of normaliza\u00adtion which is suggested by simply ignoring where sharing occurs. Thus \nwe may de.ne the set of .-terms to which that a term might evaluate. Call this 0CFA-normalization. Theorem \n9. For fully .-expanded, simply-typed terms, 0CFA\u00adnormalization can be computed in nondeterministicLOGSPACE. \nConjecture 1. For fully .-expanded, simply-typed terms, 0CFA\u00adnormalization is complete for nondeterministicLOGSPACE. \nThe proof of the above conjecture likely depends on a coding of arbitrary directed graphs and the consideration \nof commensurate path problems. Conjecture 2. An algorithm for 0CFAnormalization can be real\u00adized by optimal \nreduction, where sharing nodes always duplicate, and never annihilate.  5.5 LOGSPACE-hardness of normalization \nand 0CFA: linear, simply-typed, fully .-expanded programs That the normalization and 0CFA problem for \nthis class of pro\u00adgrams is as hard as any LOGSPACE problem follows from the LOGSPACE-hardness of the \npermutation problem:given a permuta\u00adtion p on 1,...,n and integer 1 = i = n, are 1 and i on the same \ncycle in p?That is, is there ak where 1 = k = n and pk(1) = i? Brie.y, the LOGSPACE-hardness of the \npermutation problem is as follows.Given an arbitrary LOGSPACE Turing machineM and an input x toit, visualizeagraphwherethenodesare \nmachineIDs, with directed edges connecting successive con.gurations. Assume that M always accepts or \nrejects in unique con.gurations. Then the graph has two connected components: the accept component, and \nthe reject component. Each component is a directed tree with edges pointing towards the root (.nal con.guration).Take \nan Euler tour around each component (like tracing the .ngers on your hand) to derive two cycles,and thusa \npermutation on machine IDs. Eachcycleis polynomialsize, becausethe con.gurationsonlytake logarithmic \nspace. The equivalent permutation problem is then: does the initial con.guration and the accept con.guration \nsit on the same cycle? The following linear ML code describes the target code of a transformation of \nan instance of the permutation problem. For a permutation on n letters, we take here an example where \nn =3. Begin with a vector of length n set to False, and a permutation on n letters: -val V= (False,False,False); \nval V = ((fn,fn),(fn,fn),(fn,fn)) :(( a* a -> a* a) * ( a* a -> a* a)) *(( a* a -> a* a) * ( a* a -> \na* a)) *(( a* a -> a* a) * ( a* a -> a* a))  Denote as . the type of vector V. -fun Perm (P,Q,R)= \n(Q,R,P); val Perm =fn : . -> .  The function Insert linearly inserts True in the .rst vector com\u00adponent, \nusing all input exactly once: -fun Insert ((p,p ),Q,R)= ((TT,Compose(p,p )),Q,R); val Insert = fn : . \n-> . The function Select linearly selects the third vector component: -fun Select (P,Q,(r,r ))= (Compose \n(r,Compose (Compose P, Compose Q)),r ); val Select = fn : . ->(( a * a-> a * a) *( a* a-> a* a)) Because \nPerm and Insert have the same .at type, they can be composed iteratively in ML without changing the type. \n(This clearly is not true in our coding of circuits, where the size of the type increases with the circuit.Acareful \ncoding limits the type size to be polynomial in the circuit size, regardless of circuit depth.) Lemma \n8. Let p be coded as permutation Perm. De.ne Foo to be Compose(Insert,Perm) composed with itself n times. \nThen1 and i are on the same cycle of p iff Select (Foo V) normalizes to True. Because 0CFAof a linear \nprogram is identical with normaliza\u00adtion, we conclude: Theorem 10. 0CFAof a simply-typed, fully .-expanded \nprogram is complete for LOGSPACE. As a consequence of the hardness construction having a con\u00adstant type, \nwe may conclude the 0CFAof any bounded type pro\u00adgram(.-expanded or not)is LOGSPACE-hard: Theorem 11. \n0CFAofabounded, simply-typedprogramis LOGSPACE\u00adhard. 6. Languages with .rst-class control Shivers (2004) \nargues that CPS provide[s] a uniform representa\u00ad tion of control structure, allowing this machinery to \nbe employed to reason about context, as well, and that without CPS, seperate .k .f .f 2 .x 8 .v .v 1 \n88 [call/cc] contextual analyses and transforms must be also implemented redundantly, in his view. Although \nour formulation of kCFAisa direct-style formulation, a graph representation enjoys the same bene.ts of \na CPS representation, namely that control structures are made explicit in a graph a continuation is simply \na wire. Con\u00adtrol constructs such as call/cc can be expressed directly (Lawall and Mairson 2000) and our \ngraphical formulation of control .ow analysis carries over without modi.cation. Lawall and Mairson (2000) \nderive graph representations of pro\u00ad grams with control operators such as call/cc by .rst translating \nprograms into continuation passing style (CPS). Theyobserved that when edges in the CPS graphs carrying \nanswer values (of type .) are eliminated, the original (direct-style) graph is regained, modulo placement \nof boxes and croissants that control sharing. Compos\u00ading the two transformations results in a direct-style \ngraph coding for languages with call/cc (hereafter, .K). The approach applies equally well to languages \nsuch as Filinski s symmetric .-calculus (1989),Parigot s .\u00b5 calculus (1992), and most any language ex\u00ad \npressible in CPS.9 The left side of Figure5shows the graph coding of call/cc. Examining this graph, we \ncan read of an interpretation of call/cc, namely: call/cc is a function that when applied, copies the \ncur\u00adrent continuation(.)and applies thegiven functionf toafunction (.v ...)that when applied abandons \nthe continuation at that point (8)and gives its argumentv to a copyof the continuation where call/cc \nwas applied. If f never applies the function it is given, then control returns normally and the value \nf returns is given to the other copyof the continuation where call/cc was applied. The right sideof Figure5gives \ntheCFAgraph for the program: (call/cc (.k.(.x.1)(k2)))l From the CFA graph we see that Cb(l)= {1, 2}, \nre.ecting the fact that the program will return1 under a call-by-name reduction strategy and 2 under \ncall-by-value. Thus, the analysis is indifferent to the reduction strategy. Note that whereas before, \napproximation was introduced through non-linearity of bound variables, approxi\u00admation can now be introduced \nvia non-linear use of continuations, as seen in the example. In the same way that 0CFA considers all \noccurrences of a bound variable the same , 0CFA considers all continuations obtained with each instance \nof call/cc the same . Note that we can ask new kinds ofinteresting questions in this analysis.Forexample,in \nFigure5,we can compute which contin\u00ad uations are potentially discarded, by computing which continua\u00adtions \n.ow into the weakening node of the call/cc term. (The an\u00adswer is the continuation ((.x.1)[ ]).) Likewise, \nit is possible to ask 9Languages such as .., which contains the delimited control operators shift and \nreset (Danvy and Filinski 1990), are not immediately amenable to this approach since the direct-style \ntransformation requires all calls to functions or continuations be in tail position. Adapting this approach \nto such languages constitutes an active area of research for us. Figure 5. Graph coding of call/cc and \nexample CFAgraph. which continuations are potentially copied, by computing which continuations .ow into \nthe principal port of the sharing node in the call/cc term (in this case, the top-level empty continuation \n[]). Because continuations are used linearly in call/cc-free pro\u00adgrams, the questions were uninteresting \nbefore the answer is al\u00adways none. Our proofs for the PTIME-completeness of 0CFA for the un\u00adtyped .-calculus \nand likewise for the results on kCFA carry over without modi.cation languages such as .K, .\u00b5 and the \nsym\u00admetric .-calculus.In otherwords, .rst-class control operators such as call/cc increasetheexpressivityofthe \nlanguage,butadd noth\u00ading to the computational complexity of control .ow analysis. In the case of simply-typed, \nfully .-expanded programs, the same can be said.A suitable notion of simply-typed programs is needed, \nsuch as that provided by Grif.n (1990) for .K. The type-based expansion algorithm of Figure4applies without \nmodi.cation and Lemma 7 holds, allowing 0CFA for this class of programs to be done in LOGSPACE. Linear \nlogic provides a foundation for (clas\u00adsical) .-calculi with control; related logical insights allow control \n.ow analysis in this setting. 7. Related work ThePTIME-completeness of 0CFA is most closely related to \nthe PTIME-completenessof simply typingin the .-caclculus (Mairson 2004). Both results use linearity to \nsubvert the approximation of the analysis, and since both analyses rely on the same source of approximation, \nit is no suprise that theyshare the same lower bound on complexity. ML typing can be viewed as a bounded \nrunning of a program (reducing all let-redexes) followedbyasimple typingofthe resid\u00adual. The residual \nprogram can be exponentially larger, leading to EXPTIME-completeness results by using polymorphism to \niterate a linear TM transition function (Mairson 1990). The EXPTIME\u00adcompleteness of nCFA can be viewed \nin a similar light. Contours of length proportional to the program size provide a bounded run\u00adning of \nthe program by exact analysis of the non-linearity intro\u00adduced by iterative doubling of the transition \nfunction. The story is the same for k-rank bounded intesection typing a programis runbycomputing k successive \nminimal complete devel\u00adopments and the residual is simply typed. The resulting hardness of typing is \nelementary in k (Neergaard and Mairson 2004), and thus the complexity class of each .xed k is seperated. \nOn the other hand, for (k> 0)CFA, the complexity in the hierarchy remains the same as k grows. There \nshould be a natural way of developing an alternative control .ow hierarchy that relies on complete de\u00advelopments \nfor its notion of bounded running that will be strictly more expressive than the kCFA examined in this \npaper. The result is likely to be similiar in spirit to that of Mossin (1997b), although Mossin sanalysisissimplyevaluationbyvirtueofitsexactness.To \nremain useful, some information must be purposeless lost in order to compute an answer in less time than \nit takes to run the program. It also seems likely that the linear logic based investigation into CFA \npresented here can be coupled with that of Neergaard and Mairson (2004) to provide the foundation for \ncomplexity results for the control .ow analysis of rank-2 bounded intersection typed programs (Banerjee \nand Jensen 2003). Static program analysis has been recast as various kinds of graph reachability problems, \nand parenthesis languages have been usedtodescribepathsin these graphs;seeReps (2000)forexample. Words \nin these languages are thecontexts of the context semantics presentation (Mairson 2003) of the geometry \nof interaction (Girard 1989). The undecidability of decision problems for these special\u00ad ized parenthesis \nlanguages corresponds naturally to versions of the halting problem. The graph coding of terms in our \ndevelopment is based on the technology of sharing graphs in the untyped case, and proof nets in the typed \ncase (Lafont 1995). The graph codings, CFAgraphs, andvirtualwirepropogationrulesshareastrong resemblancetothe \npre-.ow graphs, .ow graphs, and graph closing rules , respec\u00adtively, of Mossin (1997a). Casting the analysis \nin this light leads to insights from linear logic and optimal reduction.Forexample, as Mossin (1997a, \npage 78) notes, the CFAvirtual paths computed by0CFAarean approximationofthe actual runtimepathsand cor\u00adrespond \nexactly to the well-balanced paths of Asperti and Lan\u00ad eve (1995) as an approximation to legal paths \n(L\u00b4 evy 1978) and results on proof normalization in linear logic (Mairson andTerui 2003) informed the \nnovel CFAalgorithms presented here. Other researchhas shown a correspondence between 0CFAand certain \ntype systems (Palsberg and O Keefe 1995; Heintze 1995) and a further connection has been made between \nintersection typ\u00ading and kCFA (Mossin 1997a; Palsberg and Pavlopoulou 1998). Work has also been done \non relating the various .avors of con\u00adtrol .ow analysis, such as 0CFA, kCFA, polymorphic splitting, and \nuniform kCFA(Nielson and Nielson 1997). Moreover, control .ow analysis can be computed under a number \nof different guises such as set-based analysis (Heintze 1994), closure analysis (Ses\u00ad toft 1988, 1989), \nabstract interpretation (Shivers 1991; Tang and Jouvelot1994),andtypeandeffect systems(Fax\u00b4en1995; Heintze \n1995;Fax\u00b4en 1997; Banerjee 1997). We believe a useful taxonomy of these and other static analyses can \nbe derived by investigating their computational complexity. In the preface to their textbook, Nielson \net al. (1999) make an analogy between their approach to the study of program analysis and that of complexity \ntheory, whereby seemingly unrelated problems are shown to be the same through notions such as logspace \nreduction. Webelievethere is more than an analogy here results on the com\u00adplexity of static analyses \nare a useful way of understanding when twoseeminglydifferent program analyses areinfact computingthe \nsame thing. 8. Conclusions and perspective The most obvious thing you can say about program analysis \nis this: if you want to know what a program is going to do, run it and .nd out. If you don t have time \nto run the program, run it for a while. If you don t have time to run it for a while, make a crude approximation.Ifyouwanttomakesureyouhaveallthe \nanswersin such an approximation, you ll end up computing false positives . Acommon approximation idea \nis to merge whatever is known for all occurrencesofavariable. Approximations are re.nedby gently distinguishing \nthese variable occurrences. Type inference follows this blueprint. Simple typing is the ap\u00adproximation: \nall occurrences of a variable have to have the same type. ML let-polymorphism computes a .nite development \nbe\u00adfore simple typing. Rank-bounded intersection systems iterate sev\u00aderal minimal complete developments \nbefore simple typing (with .-idempotency) or a linearity check (without .-idempotency): see (Neergaard \nand Mairson 2004). This is also the story of 0CFA: a truncated evaluation that merges all occurrences \nof a bound variable. The occurrences are then distinguished in kCFA(k> 0)bycontours that annotate ap\u00adplication \nsites, and an environment whosevalues are closures and it is the free variables of these closures that \nare the bottleneck of kCFA. The NP-hardnesslower bound, and likely NP-completeness given the known exponential \nupper bound, comes from computing with approximations rather than computing via exact normaliza\u00adtion. \nWhat other kind of static analysis could there possibly be?We would like to relate kCFAin a more precise \nway to the geometry of interaction, a complete static program analysis based on linear logic. It should \nbe possible to construct an exact correspondence when k is unbounded.Afurtherextensionwouldbea generaliza\u00adtion \nof kCFA to the precision of iterated exponentials, combining its technique with .nite developments ` \n a la ML. Finally, we hope to use the techniques described here to provide a useful taxonomy of other \nprogram analyses, viewed from the perspective of the com\u00adputational resources needed to realize them. \nAcknowledgments We are grateful to Olin Shivers and Matthew Might for patient deliberations on the nature \nof kCFA. We thank Ugo Dal Lago, Matthew Gold.eld, Julia Lawall, and David McAllester for pro\u00adviding comments \non this work. References Andrea Asperti and Cosimo Laneve. Paths, computations and labels in the .-calculus. \nTheor. Comput. Sci., 142(2):277 297, 1995. Andrea Asperti and Harry G. Mairson. Parallel beta reduction \nis not elementary recursive. In POPL 98: Proceedings of the 25th ACM SIGPLAN-SIGACT symposium on Principles \nof program\u00adming languages, pages 303 315.ACM Press, 1998. Anindya Banerjee. Amodular, polyvariant and \ntype-based closure analysis. In ICFP 97: Proceedings of the second ACM SIG-PLAN international conference \non Functional programming, pages 1 10.ACM Press, 1997. Anindya Banerjee and Thomas Jensen. Modular control-.ow \nanal\u00adysis with rank2intersection types. Mathematical Structures in Comp. Sci., 13(1):87 124, 2003. Olivier \nDanvy and Andrzej Filinski. Abstracting control. In LFP 90: Proceedings of the 1990 ACM conference on \nLISP and functional programming, pages 151 160.ACM Press, 1990. Karl-FilipFax \u00b4Polyvariance, polymorphism \nand .ow analysis. en. In Selected papersfromthe 5th LOMAPSWorkshop on Analysis andVeri.cation of Multiple-Agent \nLanguages, pages 260 278. Springer-Verlag, 1997. Karl-FilipFax\u00b4en. Optimizing lazy functional programs \nusing .ow inference. In SAS 95: Proceedings of the Second International Symposium on Static Analysis, \npages 136 153. Springer-Verlag, 1995. Andrzej Filinski. Declarative continuations: an investigation of \nduality in programming language semantics. In Category Theory and Computer Science, pages 224 249. Springer-Verlag, \n1989. Jean-Yves Girard. Geometry of interaction I: Interpretation of System F. In C. Bonotto, editor, \nLogic Colloquium 88, pages 221 260. North Holland, 1989. Jean-Yves Girard,PaulTaylor, andYves Lafont. \nProofs and types. Cambridge University Press,NewYork,NY, USA, 1989. Timothy G. Grif.n. A formulae-as-type \nnotion of control. In POPL 90: Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles of \nprogramming languages, pages 47 58.ACM Press, 1990. Nevin Heintze. Set-based analysis of ML programs. \nIn LFP 94: Proceedings of the 1994 ACM conference on LISP and functional programming, pages 306 317.ACM \nPress, 1994. Nevin Heintze. Control-.owanalysis and type systems.In SAS 95: Proceedings of the Second \nInternational Symposium on Static Analysis, pages 189 206. Springer-Verlag, 1995. Nevin Heintze and David \nMcAllester. On the cubic bottleneck in subtyping and .ow analysis. In LICS 97: Proceedings of the 12th \nAnnual IEEE Symposium on Logic in Computer Science, page 342. IEEE Computer Society, 1997a. Nevin Heintze \nand David McAllester. Linear-time subtransitive control .ow analysis. In PLDI 97: Proceedings of the \nACM SIGPLAN 1997 conference on Programming language design and implementation, pages 261 272.ACM Press, \n1997b. Fritz Henglein and Harry G. Mairson. The complexity of type infer\u00adence for higher-order lambda \ncalculi. In POPL 91: Proceedings ofthe18thACM SIGPLAN-SIGACT symposiumon Principlesof programming languages, \npages 119 130.ACM Press, 1991. Neil D. Jones. Flow analysis of lambda expressions (preliminary version). \nIn Proceedings of the 8th Colloquium onAutomata, Languages and Programming, pages 114 128. Springer-Verlag, \n1981. RichardE. Ladner. The circuitvalue problemislog space complete for P . SIGACT News, 7(1):18 20, \n1975. Yves Lafont. From proof-nets to interaction nets. In Proceedings of the workshop on Advances in \nlinear logic, pages 225 247. Cambridge University Press, 1995. Julia L. Lawall and Harry G. Mairson. \nSharing continuations: Proofnets for languages with explicit control. In ESOP 00: Proceedings of the \n9th European Symposium on Programming Languages and Systems,pages 245 259. Springer-Verlag, 2000. Jean-Jacques \nL\u00b4R\u00b4correctes et optimales dans le evy. eductions lambda-calcul. PhD thesis,Paris7, January 1978.th\u00b4ese \nd Etat. Harry G. Mairson. From Hilbert spaces to Dilbert spaces: Context semantics made simple. In FST \nTCS 02: Proceedings of the 22nd Conference Kanpur onFoundations of SoftwareTechnol\u00adogy and Theoretical \nComputer Science, pages 2 17. Springer-Verlag, 2002. Harry G. Mairson. From Hilbert space to Dilbert \nspace: context semantics asa language forgames and .ow analysis. In ICFP 03: Proceedings of the eighth \nACM SIGPLAN international conference on Functional programming, pages 125 125.ACM Press, 2003. Harry \nG. Mairson. Linear lambda calculus and PTIME\u00adcompleteness. Journal of Functional Programming, 14(6):623 \n633, 2004. Harry G. Mairson. Deciding ML typability is complete for deter\u00administicexponential time.In \nPOPL 90: Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles of program\u00adming languages, \npages 382 401.ACM Press, 1990. Harry G. Mairson. Axiom-sensitive normalization bounds for mul\u00adtiplicative \nlinear logic, 2006. Unpublished manuscript. Harry G. Mairson. A simple proof of a theorem of Statman. \nTheoretical Computer Science, 103(2):387 394, 1992. Harry G. Mairson and Kazushige Terui. On the computational \ncomplexity of cut-elimination in linear logic. In Carlo Blundo and Cosimo Laneve, editors, ICTCS, volume \n2841 of Lecture Notes in Computer Science, pages 23 36. Springer, 2003. Christian Mossin. Flow AnalysisofTyped \nHigher-OrderPrograms. PhD thesis, DIKU, University of Copenhagen, January (revised August) 1997a. Christian \nMossin. Exact .ow analysis. In SAS 97: Proceedings of the 4th International Symposium on Static Analysis, \npages 250 264. Springer-Verlag, 1997b. Peter M\u00f8llerNeergaardandHarryG. Mairson.Types,potency,and idempotency:whynonlinearityand \namnesiamakeatype system work. In ICFP 04:Proceedingsofthe ninthACM SIGPLANin\u00adternational conference on \nFunctional programming, pages 138 149.ACM Press, 2004. Flemming Nielson and Hanne Riis Nielson. In.nitary \ncontrol .ow analysis: a collecting semantics for closure analysis. In POPL 97: Proceedings of the 24th \nACM SIGPLAN-SIGACT sympo\u00adsium on Principles of programming languages, pages 332 345. ACM Press, 1997. \nFlemming Nielson, Hanne R. Nielson, and Chris Hankin. Prin\u00adciples of Program Analysis. Springer-Verlag \nNew York, Inc., Secaucus, NJ, USA, 1999. JensPalsberg. Closure analysis in constraint form. ACMTrans. \nProgram. Lang. Syst., 17(1):47 62, 1995. JensPalsbergandPatrickO Keefe. Atype systemequivalentto .ow \nanalysis. ACM Trans. Program. Lang. Syst., 17(4):576 599, 1995. JensPalsberg and ChristinaPavlopoulou. \nFrom polyvariant .ow information to intersection and union types. In POPL 98: Proceedings of the 25th \nACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 197 208.ACM Press, 1998. MichelParigot. \nLambda-mu-calculus: An algorithmic interpreta\u00adtion of classical natural deduction. In AndreiVoronkov, \neditor, LPAR,volume 624 of Lecture Notes in Computer Science, pages 190 201. Springer, 1992. Thomas Reps. \nUndecidability of context-sensitive data\u00adindependence analysis. ACMTrans. Program. Lang. Syst., 22 (1):162 \n186, 2000. Peter Sestoft. Replacing function parameters by global variables. In FPCA 89: Proceedings \nof the fourth international conference on Functional programming languages and computer architec\u00adture, \npages 39 53.ACM Press, 1989. Peter Sestoft. Replacing function parameters by global variables. Master \ns thesis, DIKU, University of Copenhagen, Denmark, Oct 1988. Master s thesis no. 254. Olin Shivers. Control-.ow \nanalysis of higher-order languages of taming lambda. PhD thesis, Carnegie Mellon University, Pittsburgh,PA, \nUSA, 1991. Olin Shivers. Higher-order control-.ow analysis in retrospect: lessons learned, lessons abandoned. \nSIGPLAN Not., 39(4):257 269, 2004. Richard Statman. The typed .-calculus is not elementary recursive. \nTheor. Comput. Sci., 9:73 81, 1979. Yan MeiTang and Pierre Jouvelot. Separate abstract interpretation \nfor control-.ow analysis. In TACS 94: Proceedings of the International Conference on Theoretical Aspects \nof Computer Software, pages 224 243. Springer-Verlag, 1994. Kazushige Terui. On the complexity of cut-elimination \nin linear logic, July 2002. Invited talk at LL2002 (LICS2002 af.liated workshop), Copenhagen. AndrewK. \nWright and Suresh Jagannathan. Polymorphic splitting: an effective polyvariant .ow analysis. ACM Trans. \nProgram. Lang. Syst., 20(1):166 207, 1998.   \n\t\t\t", "proc_id": "1291151", "abstract": "<p>We analyze the computational complexity of <i>k</i>CFA, a hierarchy of control flow analyses that determine which functions may be applied at a given call-site. This hierarchy specifies related decision problems, quite apart from any algorithms that may implement their solutions. We identify a simple decision problem answered by this analysis and prove that in the 0CFA case, the problem is complete for polynomial time. The proof is based on a nonstandard, symmetric implementation of Boolean logic within multiplicative linear logic (MLL). We also identify a simpler version of 0CFA related to &#951;-expansion, and prove that it is complete for logarithmic space, using arguments based on computing paths and permutations.</p> <p>For any fixed <i>k</i>&gt;0, it is known that <i>k</i>CFA (and the analogous decision problem) can be computed in time exponential in the program size. For <i>k</i>=1, we show that the decision problem is NP-hard, and sketch why this remains true for larger fixed values of <i>k</i>. The proof technique depends on using the <i>approximation</i> of CFA as an essentially nondeterministic computing mechanism, as distinct from the exactness of normalization. When <i>k</i>=<i>n</i>, so that the \"depth\" of the control flow analysis grows linearly in the program length, we show that the decision problem is complete for exponential time.</p> <p>In addition, we sketch how the analysis presented here may be extended naturally to languages with control operators. All of the insights presented give clear examples of how straightforward observations about linearity, and linear logic, may in turn be used to give a greater understanding of functional programming and program analysis.</p>", "authors": [{"name": "David Van Horn", "author_profile_id": "81337494657", "affiliation": "Brandeis University, Waltham, MA", "person_id": "P900676", "email_address": "", "orcid_id": ""}, {"name": "Harry G. Mairson", "author_profile_id": "81100061196", "affiliation": "Brandeis University, Waltham, MA", "person_id": "P107959", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291166", "year": "2007", "article_id": "1291166", "conference": "ICFP", "title": "Relating complexity and precision in control flow analysis", "url": "http://dl.acm.org/citation.cfm?id=1291166"}