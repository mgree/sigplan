{"article_publication_date": "10-01-2007", "fulltext": "\n Experience Report: Functional Programming in C-Rules Jeremy Wazny Constraint Technologies International \nLevel 7, 224 Queen Street Melbourne VIC 3000 Australia Jeremy.Wazny@constrainttechnologies.com Abstract \nC-Rules is a business rules management system developed by Con\u00adstraint Technologies International1 (CTI), \ndesigned for use in trans\u00adportation problems. Users de.ne rules describing various aspects of a problem, \nsuch as solution costs and legality, which are then queried from a host application, typically an optimising \nsolver. At its core, C-Rules provides a functional expression language which affords users both power \nand .exibility when formulating rules. In this paper we will describe our experiences of using functional \npro\u00adgramming both at the end-user level, as well as at the implementa\u00adtion level. We highlight some of \nthe bene.ts we, and the product s users, have enjoyed from the decision to base our rule system on features \nsuch as: higher-order functions, referential transparency, and static, polymorphic typing. We also outline \nsome of our expe\u00adriences in using Haskell to build an ef.cient compiler for the core language. Categories \nand Subject Descriptors D[3]:2; D[2]:3; J[7] General Terms Design, Languages 1. Introduction C-Rules \nis a business rules system which is used primarily to for\u00admulate rules that are used to solve scheduling \nproblems in the trans\u00adportation industry. Airline .ight schedules and staff work rosters must be assembled \naccording to many rules and regulations. These rules can be represented within C-Rules, and made available \nto a host application, such as a dedicated solver, which can invoke them as necessary. Typically, the \nrules required by such an application will be used to determine the legality and cost of a potential \nso\u00adlution, but in general can return any results that are required, e.g. tuning parameters for the solver. \nThe C-Rules system consists of the following three components, the .rst two of which we will consider \nin this article. 1. An editor for users to view, de.ne and edit rules. 2. An evaluation library (Libcrule) \nwhich host applications use to execute rule queries. 3. A server for storing and tracking rule repositories. \n  1 http://www.constrainttechnologies.com Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. ICFP 07, October 1 3, 2007, Freiburg, Germany. Copyright c &#38;#169; \n2007 ACM 978-1-59593-815-2/07/0010. . . $5.00 Working within the editor, users de.ne problem-speci.c \nrules as tables which produce conclusions (consequents) in terms of some conditions (antecedents.) In \nthis context, a rule or, equivalently, a rule table , can be thought of as a problem-speci.c function. \nWhen producing a table, the user must nominate a set of in\u00adputs, outputs, and their types. Each table \nconsists of a sequence of lines, with each line divided into an antecedent and a consequent component. \nThe antecedent acts as a guard: it contains a number of Boolean expressions which, for a given input, \ndetermine whether the rule line applies at all. The consequent part of a line contains an expression \nfor every output in the rule table. Rules are executed at the request of a host application, which entails \ndescribing input and output sources and evaluating the requested rules. As such, the C-Rules system can \nbe thought of as providing an embedded func\u00adtional language, and associated tools. EXAMPLE 1. Figure \n1 contains an image of the C-Rules edi\u00adtor. The main panel contains a rule table which we have called \nActivityQualification, with inputs Staff and Activity, and a pair of outputs Legal and Cost, which can \nbe seen at the tops of their respective rule table columns. This table encodes a rule for determining \ntwo things: 1) whether a staff member is capable of undertaking a particular activity (the Legal output, \nwhich is a Boolean) and 2) the cost of allowing this person to perform the activity (the Cost output, \nan integer.) Omitting the details for now, the rule can be understood as fol\u00adlows. The values CPT , FO \n, and INS represent the staff ranks captain , .rst-of.cer , and instructor , which we have used to distinguish \nmembers of an airline s .ight crew. The rule described by this table allows any person to undertake, \nat no cost, an activity which requires somebody of their rank. The second line allows a captain to act \nas a .rst-of.cer, with a cost proportional to the num\u00adber of legs .own. The .fth line of the table permits \nan instructor to perform as a captain if the activity takes place more than 72 hours after his last activity \nended, at a cost of 5000 units. No other com\u00adbination of ranks is allowed. This is a simpli.ed version \nof a rule that an airline might employ when generating a work schedule. The core expression language, \nwhich the user writes in when de.ning the contents of a rule table, is a statically-typed purely functional \nlanguage with call-by-value semantics. It has the following notable features. Higher order operators. \ne.g. foreach and summarise (which are the well-known map and foldl from Haskell, and other functional \nlanguages.)  The ability to invoke one rule table from another, as well as to generate thunks, which \nrepresent unevaluated tables.  A static, polymorphic type system based on an instantiation of the HM(X) \nframework (Sulzmann et al. 1997). The language   Figure 1. The C-Rules editor has built-in list and \nset types, plus user-de.ned record and enum types. Enum values and record .eld names may be freely overloaded, \nsaving users from having to write type-speci.c names. Basic types for dealing with dates, times and \ndurations with full support for time zones and time zone transitions (daylight savings.) Ef.cient date/time \ncomputations are a must for any transport-related problem.  Type-directed operator overloading. For \ninstance, the + oper\u00adator is de.ned for integer and .oating-point values as well as meaningful combinations \nof date, time, and duration argu\u00adments. e.g. 2007-01-21 18:35 + 12h 30m  In our example screenshot, \nuser-declared types can be seen in the left panel, and include an enumeration type called Rank, and record \ntypes called Activity and StaffMember. The type declarations can, of course, be viewed within the editor, \nbut to save space we describe them here. The Rank type represents staff ranks, and contains the values \nCPT, FO and INS, described earlier. An Activity record represents a unit of work which contains a number \nof legs, i.e. .ights, (a .eld named Legs), a departure time (DepTime), arrival time (ArrivalTime), and \na required rank (RankReqd). Values of the Staff type represent staff members and have .elds to represent \nan individual s rank (Rank) and a list of previously-undertaken activities (PastActs). A rule table s \noutputs may be evaluated independently, or in any combination, as required by the host application. Intuitively, \nevaluation of a single rule table output, o proceeds as follows. 1. Evaluate each rule line s antecedent \npart. 2. For every line whose antecedent was reduced to the Boolean true value: evaluate that line s \nexpression for o, producing a list of values, in the same order as the rule lines appear in the table. \n 3. Coalesce the results depending on o s coalescing mode. For example, if the coalescing mode is .rst, \nthen only the .rst result (corresponding to the .rst matching rule line) is returned. In the sum coalescing \nmode the result is the sum of all of the matching rule lines values.  Each of a rule table s outputs \nis evaluated in the same way though, in practice, re-evaluation of common sub-expressions is avoided \nby our implementation. EXAMPLE 2. In the antecedent portion of our example rule table there are two columns \n(plus an empty one which has been elided), the .rst of which is headed by Staff and Rank, and the second \nActivity and RankReqd. These represent the Rank .eld of the Staff input and the RankReqd .eld of the \nActivity input, re\u00adspectively. The values in these columns, like CPT and FO , are shorthand for equality \nrelations on their corresponding inputs. For example, the CPT value in the Staff/Rank column repre\u00adsents \nthe equality test expression Staff.Rank = CPT , where Staff.Rank is the projection of the .eld Rank from \nthe input Staff.A * in a column represents an always-true value. This is similar to the pattern matching \nof languages like Standard ML and Haskell, though we allow relations other than equality between in\u00adputs \nand pattern values . In general, additional arbitrary expres\u00adsions can be added to any line s antecedent. \nTo evaluate this rule, a value is bound to each of the Staff and Activity inputs, and the antecedent \npart of each line is evaluated. Assuming our staff member has a Rank of CPT and the activity has a RankReqd \nof FO, then both the second and last lines of the table will apply. To calculate the Legality output, \nwhich has coalescing mode .rst , we can simply evaluate the corresponding expression on the second line, \nin this case the value true. To determine the Cost output for the same inputs, we sum the values in the \nCost column of the second and last lines, since Cost has the sum coalescing mode. The expression on the \nsecond line can be read as: the number of legs in the input activity, multiplied by one thousand. This \nis the cost of allowing a staff member with rank CPT to ful.ll a role requiring someone of rank FO. In \nthe following sections, we will discuss the experiences we have had in supporting and using functional \nprogramming both at the user level, as a source language within the editor (Section 2), as well as at \nthe implementation level, in Libcrule (Section 3). We conclude in Section 4.  2. The C-Rules user experience \nThe C-Rules language and editor are intended to be relatively easy to use for people without programming \nexperience. The editor presents rule tables in a form similar to a spreadsheet2, making it more familiar \nto most prospective users than a text editor-style interface. An important goal of the system is to enable \nrule writers to make minor edits to tables, i.e. without changing the interfaces between tables, with \nthe con.dence that they are not inadvertently 2 Note that, unlike in a spreadsheet, individual cells \ndo not have identities and cannot be referenced. The basic, user-de.ned functional unit is the rule table. \nThe similarity to spreadsheets is merely visual. introducing errors in other rule tables which may depend \non the edited table. We have found that basing the language on a statically typed, pure, functional core, \nwith well-de.ned interfaces between rule tables, and no shared state, has enabled us to provide these \nsorts of guarantees. In our experience, catching errors early is especially important in the context \nof an embedded system, because rules will often be invoked many times from host applications which are \nstateful, and whose behaviours may be dif.cult to follow. The interleaving of host and rule system execution \nmakes bugs in applications ex\u00adtremely tricky to trace. It is also likely that the rules and host appli\u00adcation \nare developed separately, making testing dif.cult in the .rst place. The C-Rules type system has support \nfor parametric polymor\u00adphism and overloading at the expression level, but not between rule tables. This \ndesign decision, while limiting the expressiveness of the system, means that the implementation does \nnot suffer from the usual problems related to poor type error reporting where a mistake in one de.nition \ncan be propagated into another de.nition before it is reported, making its cause unclear (Wazny 2006). \nBy design, we require that every rule table input and output has a user-declared type, and that the expression \nin each cell must have a monomorphic type, while the types of local, lambda-bound variables are inferred \nautomatically and do not require any speci.c type annotations. In this way, type errors are always localised \nto the single cell contain\u00ading the expression that the user last edited. This makes type error diagnosis \nmuch more straightforward than in settings where type declarations are optional. Though such limitations \nmay seem burdensome to the more ex\u00adperienced programmer, we have found that rule writers do not feel \nencumbered by these restrictions. Within a single cell, users may freely make use of polymorphic, higher-order \nfunctions, such as foreach, which applies an anonymous function to every element of a list or set. We \nhave also found that rule writers appreciate be\u00ading able to freely overload the names of their own data \ntypes. It is common for our users to create a number of record types, many of which share generic .eld \nnames, like id, or kind. The user knows which type of record he has in mind when projecting out a speci.c \n.eld, and the system needs to be able to follow.3 In practice, there is little cause for confusion, since \nmonomorphic types must be de\u00adclared ahead of time for all input variables. Being required to give each \n.eld a type-speci.c name would be a noticeable hindrance from the user s perspective. The C-Rules editor \nis also designed to statically detect other kinds of non-local problems in rule tables and their expressions. \nAs well as editing the contents of rule tables, users may modify other aspects of the repository, such \nas inputs and outputs or the de.nitions of user-speci.ed types. These sorts of changes to table interfaces \nor global de.nitions may introduce errors. For example, removing a .eld from a record type de.nition \nwill invalidate all occurrences where such a record is constructed with this (now invalid) .eld. The \neditor is able to detect and present these sorts of errors to the rule writer, and even suggests likely \n.xes. These suggestions can be applied at the press of a button. We have found automatic and interactive \nproblem resolution to be a necessity since individual rule writers may not be familiar with an entire \nrepository. A simple interpreter is built into the editor allows users to to de.ne and run tests. Feedback \nhas shown that users both within the organisation, as well as external customers, have reacted positively \nto the C-Rules editor and have adopted a functional-style of rule writing without any problems. C-Rules \nis used by customers to describe 3 Because rule repositories are stand-alone entities we can adopt a \nclosed\u00adworld approach when disambiguating cases of overloading; if a solution to the typing problem exists, \nit can be found. The requirement that rule table interfaces are monomorphic reduces the search space \nconsiderably in practice. pairing (XBO) and rostering (CAM) problems for optimising air\u00adline schedules. \nThe company also has customers who are deploy\u00ading C-Rules to validate manually-created solutions to their \nown transportation problems. The collections of rules necessary to solve these sorts of problems are \noften large, containing hundreds of rule tables drawn from numerous sources, including legal and business \nrequirements. For complex problem domains, the bene.ts outlined in this section are especially welcome. \n 3. Implementation of the C-Rules compiler To evaluate rule tables, applications use the Libcrule library, \nwhich is capable of loading, compiling and executing rule repositories. The library itself is written \nin C++, but makes use of a separate executable for compiling repositories. This compiler is written in \nHaskell and produces plain ANSI C code from a rule repository and an input/output binding description. \nAn earlier version of Libcrule used a backend which produced and interpreted a purpose-built byte code \nlanguage. The byte code compiler and interpreter were both written by a single developer, on and off, \nover a period of about a year, in C++, and occupied approximately 25,000 lines of code. When we found \nthat the cost of interpretation was unacceptably high for our applications, we decided to adopt the approach \nof compiling to machine code via C, and then linking and directly executing the result. In 2006, the \nauthor started work on a design and, in parallel, be\u00adgan implementing a prototype in Haskell, to test \nits viability. By the time the design was done, the prototype was generating useful code for a number \nof small test cases, so we decided to simply refactor and extend it into a full implementation. It took \nabout a month to get the system into a form where it was passing all of our existing regression tests, \nand providing equivalent functionality to the sys\u00adtem it had replaced. We spent the following month identifying \nand .xing performance problems in the generated code exceeding the sophistication of the original C++ \nbackend. We have found the experience of iterative design and prototyp\u00ading to have been extremely bene.cial. \nThe drive to develop a con\u00adcise and accurate design speci.cation coupled with the additional foresight \ngained by having a working implementation greatly re\u00adinforced each other. The prototype implementation \nwas able to in\u00adform the design, while the design kept the implementation focused and manageable. Having \na strong, static type system in the source language has enabled us to use ef.cient data representations \nin the generated code. Because the type of all expressions is known at compile-time, run-time values \ncan remain untagged, and multiple record .elds can often be packed at the bit-level, when necessary. \nThe compiler is capable of specialising the implementation of overloaded operators depending on the user-declared \ntypes in use. The lack of side effects similarly makes feasible a large host of optimisations, such as: \ncommon subexpression elimination, partial evaluation (Augustsson 1997), data structure fusion, and other \ntechniques specialised for our particular rule structure. In general, we are able to produce space and \ntime ef.cient, straight-line code. One of the factors which made programming in Haskell so pro\u00adductive \nwas its ability to easily support embedded domain speci.c languages. In our code we make use of two such \npurpose-built lan\u00adguages. The compiler s intermediate target language is the untyped lambda calculus \nwith C language speci.c layout combinators, in\u00adspired by the pretty printing library of Hughes (1995). \nEvaluating, i.e. executing, a program in this intermediate language yields plain text C code. The advantage \nof using the lambda calculus as a tar\u00adget is that it allowed us to parameterise generated code in terms \nof other code fragments, enabling us to essentially build reusable code templates. Using a system of \nlayout combinators meant we could retain some structure in the generated code and permitted easy modi.cation \nof things like the target code s layout. We also designed and used a simple rule-based rewriting lan\u00adguage \nfor transforming operator expressions into their intermedi\u00adate language implementations. Having this \nsort of representation made it easy to specify compilation schemes for the large number of operators \nprovided by the user-level language. It also allowed us to mechanically generate rules for compiling \nmany operators from very concise speci.cations. Overall, the bene.ts of using Haskell to develop our \ncompiler were signi.cant: Less code: The compiler is implemented in about 9,300 lines of code as compared \nto the original backend s 25,000 lines of C++ (omitting comments and blank lines.) 4 Much of this contains \nimplementations of the many built-in operators available in the source language. The core of the compiler \nclosely matches its speci.cation.  Fast development: Most of the design and implementation were completed \nby one developer over the course of a single month. The system was subsequently extended, beyond the \ncapabilities of the C++ backend, and its results optimised for another month before being deployed. By \ncomparison, the C++ backend took an individual developer almost a year to complete.  Easier testing: \nBeing able to run the compiler s code within an interpreter allowed us to easily test and evaluate it \nduring de\u00advelopment. If we had chosen C++, testing would have required a great deal more effort, meaning \nthat, in practice, much of the code would have remained untested until the end of the pro\u00adgramming phase. \n  Despite all of the advantages of using Haskell, we have encoun\u00adtered a number of issues. Though they \nmay seem relatively minor, in reality any one of them could be enough to dissuade an organi\u00adsation from \nusing Haskell in their products. No Haskell implementation that we are aware of at the time is capable \nof generating relocatable code. This means that our compiler must be a separate executable which is \nunfortunate since, previously, the entire library could be shipped as a single shared object .le. Because \nof this, we settled on a design where the compiler s input is a serialised version of the source program, \nwhich it must then parse, adding unnecessary overhead.5  The number of skilled Haskell programmers available, \ni.e. those with a decent amount of practical experience working on non\u00adtrivial programs, is tiny. Efforts \nto mitigate this risk through cross-training have been made, but in a commercial environ\u00adment, a persistent \nskills shortage could ultimately necessitate a (reluctant) rewrite in another language.  Because of \nthe speed of its generated code, GHC (GHC) is the compiler of choice when developing applications in \nHaskell, but it can be dif.cult to install properly on a platform for which no binary is available. \n 4. Conclusion Functional programming languages and techniques have proven invaluable in the design \nand development of C-Rules, both at the user level and the implementation level. 4 The C++ backend contains \na separate byte-code compiler and interpreter while the Haskell backend contains only a compiler. We \nestimate, though, that an interpreter and compiler written in Haskell, in the style of the C++ backend \nwould not require a lot more code than the existing Haskell backend; it would probably be a few thousand \nlines bigger. 5 One upside of this is that if anything goes wrong during code generation, we have a perfect \nsnapshot of the program just before the failure, which can be easily replayed. Static typing, referential \ntransparency and decoupling of code allow the system to provide strong guarantees to users about the \nsafety of their rule de.nitions, while also encouraging testing, and facilitating debugging. These bene.ts \nhave allowed both develop\u00aders within the company, as well as customers, to de.ne problems of great complexity \nin clear, well-understood terms. Deciding to implement a compiler for our core language in Haskell has \nalso paid off greatly. The .nal design and implementa\u00adtion match each other closely, aiding maintenance \nand testing, and the fact that we were able to meet our requirements within about a month gave us time \nto spend optimising the compiler s results.  Acknowledgements Thanks to Karl Anderson, Michael Sperber, \nand the anonymous reviewers for their valuable comments and suggestions. References L. Augustsson. Partial \nevaluation in aircraft crew planning. In Partial Evaluation and Semantics-Based Program Manipula\u00adtion, \nAmsterdam, The Netherlands, June 1997, pages 127 136. New York: ACM, 1997. URL citeseer.ist.psu.edu/ \naugustsson97partial.html. CAM. CAM# Rostering Optimizer. http://www. constrainttechnologies.com/components/CAM_sharp_ \nRostering_Optimiser.html. GHC. Glasgow Haskell Compiler. http://www.haskell.org/ ghc/. John Hughes. The \nDesign of a Pretty-printing Library. In J. Jeur\u00ading and E. Meijer, editors, Advanced Functional Programming, \npages 53 96. Springer Verlag, LNCS 925, 1995. Martin Sulzmann, Martin Odersky, and Martin Wehr. Type \ninference with constrained types. In Fourth International Workshop on Foundations of Object-Oriented \nProgramming (FOOL 4), 1997. URL citeseer.ist.psu.edu/article/ odersky99type.html. Jeremy Wazny. Type \ninference and type error diagnosis for Hindley Milner with extensions. PhD thesis, University of Melbourne, \n2006. XBO. XBO Pairing Optimizer. http://www. constrainttechnologies.com/components/XBO_ Pairing_Optimiser.html. \n \n\t\t\t", "proc_id": "1291151", "abstract": "<p>C-Rules is a business rules management system developed by Constraint Technologies International<sup>1</sup> (CTI), designed for use in transportation problems. Users define rules describing various aspects of a problem, such as solution costs and legality, which are then queried from a host application, typically an optimising solver. At its core, C-Rules provides a functional expression language which affords users both power and flexibility when formulating rules. In this paper we will describe our experiences of using functional programming both at the end-user level, as well as at the implementation level. We highlight some of the benefits we, and the product's users, have enjoyed from the decision to base our rule system on features such as: higher-order functions, referential transparency, and static, polymorphic typing. We also outline some of our experiences in using Haskell to build an efficient compiler for the core language.</p>", "authors": [{"name": "Jeremy Wazny", "author_profile_id": "81100492845", "affiliation": "Constraint Technologies International, Melbourne, Australia", "person_id": "PP37044775", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291158", "year": "2007", "article_id": "1291158", "conference": "ICFP", "title": "Experience report: functional programming in c-rules", "url": "http://dl.acm.org/citation.cfm?id=1291158"}