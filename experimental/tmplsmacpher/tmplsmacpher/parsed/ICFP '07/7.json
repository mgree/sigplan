{"article_publication_date": "10-01-2007", "fulltext": "\n Termination Analysis and Call Graph Construction for Higher-Order Functional Programs Damien Sereni \nOxford University dsereni@comlab.ox.ac.uk Abstract The analysis and veri.cation of higher-order programs \nraises the is\u00adsue of control-.ow analysis for higher-order languages. The prob\u00adlem of constructing an \naccurate call graph for a higher-order pro\u00adgram has been the topic of extensive research, and numerous \nmeth\u00adods for .ow analysis, varying in complexity and precision, have been suggested. While termination \nanalysis of higher-order programs has been studied, there has been little examination of the impact of \ncall graph construction on the precision of termination checking. We examine the effect of various control-.ow \nanalysis techniques on a termination analysis for higher-order functional programs. We present a termination \nchecking framework and instantiate this with three call graph constructions varying in precision and \ncomplexity, and illustrate by example the impact of the choice of call graph construction. Our second \naim is to use the resulting analyses to shed light on the relationship between control-.ow analyses. \nWe prove precise inclusions between the classes of programs recognised as terminat\u00ading by the same termination \ncriterion over different call graph anal\u00adyses, giving one of the .rst characterisations of expressive \npower of .ow analyses for higher-order programs. Categories and Subject Descriptors F.3.2 [Logics and \nMeanings of Programs]: Semantics of Programming Languages Program Analysis General Terms Veri.cation, \nTheory Keywords Termination, Functional programs, Semantics, Pro\u00adgram Analysis 1. Introduction Termination \nanalysis is a fundamental problem in program veri.\u00adcation, and proofs of program correctness require \nproving termina\u00adtion of a program, often independently from the proof of functional correctness of the \nprogram. Furthermore, termination checking has been shown to address real issues in program correctness \n(Cook et al. 2006a,b; Berdine et al. 2006). Termination checking is often limited to .rst-order programs, \nor .rst-order term rewriting systems, though a number of extensions Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 07, October 1 3, 2007, Freiburg, Germany. \nCopyright c . 2007 ACM 978-1-59593-815-2/07/0010. . . $5.00 to higher-order programs have been made in \nrecent years (Jones and Bohr 2004; Sereni and Jones 2005; Giesl et al. 2005, 2006). Termination analysis \nfor higher-order programs introduces speci.c problems, in particular that of control-.ow analysis.The \n.owof control of a .rst-order program is apparent from the program text, however this is not the case \nfor higher-order programs. Indeed there has been a great deal of research on control-.ow analysis (CFA) \nof higher-order programs (Shivers 1991; Jones 1987; Shivers 1988; Malacaria and Hankin 1998; Might and \nShivers 2006), and several variants of higher-order CFA have been proposed. These present different characteristics, \nin particular in the aspects of precision and complexity. However, it has proved dif.cult to account \nfor the precise rela\u00adtionships between these .ow analyses. There is an intuitive sense in which we can \nview one method for CFA as more precise than another, and experimental evidence can justify such statements, \nbut it remains dif.cult to establish such properties formally. The ability to de.ne such statements precisely \nis nonetheless crucial to our understanding of control-.ow analysis. Indeed a precise hierarchy of expressiveness \nallows one to make a rational choice of one analysis over another. The aim of this work is to highlight \nthe importance of control\u00ad.ow analysis in achieving good termination analysis results, and to use the \neffectiveness of termination analysis to give a precise account of the precision of control-.ow analysis. \nThe termination problem gives us a unique handle on evaluating the precision of control-.ow analysis. \nUnlike, say, the veri.cation of safety properties, termination does not invite other issues such as the \nlogic used to express properties. A method for termination analysis (as long as it is sound) simply partitions \nthe set of ter\u00adminating programs into two sets: the set of programs accepted by the analysis, and the \nset of false negatives. It is thus in principle straightforward to compare the expressiveness of termination \nanal\u00adyses, as this reduces to comparing the respective sets of accepted programs. We shall use precisely \nthis measure to evaluate the effec\u00adtiveness of .ow analyses: one such analysis is strictly superior (in \nprecision) if it yields a strictly larger set of terminating programs. Our contributions are: 1. We de.ne \na framework for termination analysis of higher-order, purely functional programs that allows the underlying \ncontrol\u00ad.ow analysis to be varied at will. 2. We present three control-.ow analyses: the known 0CFA \nanal\u00adysis (Shivers 1991), a family of more precise analyses known as k-limited CFA, and .nally an analysis \nbased on tree automata with different precision characteristics. 3. Finally, we give a precise characterisation \nof the relationships between the family of analyses thus de.ned, giving the .rst systematic study of \nexpressiveness of termination analyses.  We shall not regard experimental evaluation as an aim of this \npaper. While it is doubtless important to obtain such validation, the task is made somewhat complicated \nby a lack of good canonical suites of higher-order purely functional programs. In its place, the relationships \nbetween the precision of our analyses serve to validate the use of more complex analyses to improve results. \nMuch detail is elided for space reasons, and the interested reader may .nd details and proofs in a companion \ntechnical report (Sereni 2006). 2. Size-Change Termination for Higher-Order Programs 2.1 Size-Change \nTermination Our basis for termination analysis is the size-change termination (SCT) method (Lee et al. \n2001). In this framework, we assume that each datatype in the language carries a natural well-founded \norder (though this restriction can be lifted (Avery 2006), we keep it for simplicity). The SCT method \nthen attempts to detect as terminating all those programs in which any potential in.nite sequence of \ncalls would give rise to an in.nitely decreasing value. As all datatypes are well-founded, in.nitely \ndecreasing value chains are impossible, and so termination is established. Let us illustrate the SCT \nmethod in more detail by example. Consider the following (somewhat archetypal) program: ack (m,n) = if \nm=0 then n+1 else if n=0 then 1 ack (m-1, 1) else 2 ack (m-1, 3 ack (m, n -1)) The ack function has three \nrecursive calls, labelled in the program text. For each of these calls, a simple analysis can produce \na size\u00adchange graph, shown below: > > = ddd m mm mm m d n nn nn n > 1:ack . ack 2:ack . ack 3:ack . ack \nTo wit, in calls 1 and 2 it is detected that the value of min the callee is strictly less than its value \nin the calling instance. Likewise, in the third call the value of mis guaranteed not to increase (in \nfact, mis passed unchanged), while the value of ndecreases. It is not dif.cult to see by inspection from \nthe above graphs that any in.nite sequence of calls would cause either the value of nto decrease in.nitely, \nor the value of mto decrease in.nitely. As a result, no in.nite sequence of calls is possible, as in.nitely \ndecreasing chains of natural numbers are impossible. The SCT framework formalises and automates the above \ndis\u00adcussion. First presented for .rst-order programs (Lee et al. 2001), and later extended for higher-order \nprograms (Jones and Bohr 2004; Sereni and Jones 2005), the SCT framework can be sum\u00admarised as follows: \n1. The call graph of the program is computed, 2. Each edge in the call graph is annotated with a size-change \ngraph describing data.ow and size changes in the call safely, and 3. The annotated call graph is checked \nfor in.nite paths which do not imply in.nite decrease in some value.  The third part of this process \nis decidable (if computationally ex\u00adpensive, as it is a PSPACE-complete problem) (Lee et al. 2001). However, \nthe .rst two phases of the procedure are not, and it is of particular interest to focus on the issues \nraised by call graph con\u00adstruction. We shall therefore not describe the algorithm due to Lee et al. (2001) \nfor proving termination given an annotated call graph, but rather focus on the .rst two points in the \nremainder of the paper. 2.2 Termination of Higher-Order Programs The SCT termination check was originally \nintroduced for .rst\u00adorder purely functional languages only. However, all major func\u00adtional languages \nsupport higher-order functions. These make the task of termination analysis substantially more complex, \nfor two essentially independent reasons. Comparing Functional Values The .rst issue in the analysis of \nhigher-order programs is the presence of functional values. The SCT framework relies on a well-founded \norder on values computed in the program s execution. In the .rst-order case it is straightfor\u00adward to \ncompare values: natural numbers may be compared using the numerical <order, while structures such as \nlists allow a range of natural well-founded orders, such as the sublist order. However, the appropriate \norder for functional values is not as obvious. We shall follow previous work on this problem (Jones and \nBohr 2004; Sereni and Jones 2005), and opt for a syntactic order on function values. The crucial observations \nis that in the execution of a program, a function value is represented as a closure, that is to say a \nfunction de.nition together with an environment assigning values to free variables. As a value may itself \nbe a closure, this gives rise to a tree structure, where the nodes are function names or constants. The \norder on such values is just given by the subtree order. As an example, consider the following program: \nmapf xs = match xs with [] . [] | y:: ys . fy :: map f ys compose fgx=f (g x) add x y=x+y map (compose \n(add 1) (add 2)) [0;1] Evaluation of this program will cause the following state, ssay, to be evaluated \n(where .f : .n represents a closure of f with environment .): . .add :{x. 1}n, {f f n, . .compose :{ \n.map : g . .add :{x . 2}n} n xs . 0::1::[]} The nesting structure of this closure gives rise to the order \nrelation, de.ned as the subtree ordering. For instance, .add :{x . 1}n <s This order is clearly well-founded, \nas closures are .nite trees. Fur\u00adthermore, it is useful in proving termination of nontrivial programs \nwe shall return to this point in the next section and further in\u00advite the reader to .nd a more complete \ndiscussion in the companion report (Sereni 2006). Call Graph Construction The second issue in the analysis \nof higher-order programs, and the main focus of this paper, is call graph construction. In the .rst-order \ncase, it is straightforward to de.ne the call graph of a program: this is a graph with a node for each \nfunction in the program, and an arrow f . g iff a call to g appears (textually) in the body of function \nf. This call graph, annotated with size-change graphs as shown above, is the structure on which the SCT \nalgorithm is de.ned. The setting of higher-order programs complicates both the de.nition and construction \nof the call graph, and we shall brie.y outline the issues in this section. While a formal de.nition of \nthe call graph of a program in our more general setting must wait until the next section, let us give \nthe intuition behind the de.nition. The execution of the program is represented by a call relation s \n. s' (between states in the execution). The intended meaning of the judgement s . s' is that the evaluation \nof state s depends on (and therefore triggers) the evaluation of state s'. This is a natural generalisation \nof the dynamic call relation for .rst-order programs, but this more general de.nition extends to arbitrary \nlanguages. The crucial property of this relation is that nontermination should be re.ected as an in.nite \ncall sequence, exactly as in the .rst-order case. The dynamic call graph of a program is just the graph \nof the . relation (restricted to reachable states). It is therefore the case that a program is terminating \niff its dynamic call graph is .nite and acyclic. Accordingly, the aim of our termination analysis will \nbe to prove that in.nite paths in the call graph are impossible. Naturally, the dynamic call graph of \na program cannot be com\u00adputed in general. We are therefore concerned with static approxima\u00adtions of the \ndynamic call graph. We use the framework of abstract interpretation (Cousot and Cousot 1977) to de.ne \nthis approxima\u00adtion. Informally, the static call graph is a .nite graph, where each node in the graph \n(henceforth called an abstract state) represents a set of nodes in the dynamic call graph. Nodes in the \nstatic call graph are drawn from a .nite set Statea of abstract states. Furthermore, the static call \ngraph should safely approximate the dynamic call graph, in the sense that each call in the dynamic call \ngraph should be re.ected in the static call graph. This condition is suf.cient to guarantee that any \nin.nite paths in the dynamic call graph is re.ected in the static call graph, and thus that termination \nmay be proved by showing that in.nite paths in the static call graph give rise to in.nite descent. To \nmake this general framework concrete, however, two aspects must be decided: 1. How the . relation is \nde.ned, and 2. What the set Statea and the abstraction function ashould be.  In the next section we \nshall settle the .rst question. However, no one answer can be given to the second question: many different \nchoices have been proposed, and these differ in the size of the set Statea, as well as the precision \nthey offer.  3. Termination Analysis In the previous section, we have introduced the main components \nof higher-order termination analyses in our framework: the SCT algorithm, the size order on values, the \ndynamic call graph (de.ned by the . relation) and the static call graph. In this section we make these \nideas more precise. 3.1 The language The language that we shall use throughout is a simple call-by-value \nlanguage of recursive, curried function de.nitions, often known as supercombinator form (Peyton-Jones \n1987). It is intended to be a small intermediate language, suitable for representing programs written \nin purely functional variants of ML or similar languages, and as such we shall refer to this language \nas the core language. The .-calculus can be embedded into this language via .-lifting (Johnsson 1985). \nAs the language is largely standard, for space reasons we shall not give a full de.nition, but merely \nillustrate the language with the example program in Figure 1. As this shows, the core language includes \npattern matching and arithmetic operations. The language allows several kinds of values: algebraic values \nsuch as lists, trees and other user-de.ned datatypes; function values and primitive constants such as \nintegers and booleans. The set of constants is largely arbitrary but must be well-founded, and we choose: \nConstant = N . B Nested function de.nitions are not permitted in the core language, and we eschew many \nof the complexities of ML-like languages, mapf xs = match xs with [] . [] | y:: ys . fy :: map f ys compose \nfgx=f (g x) succ n=n+1 map (compose succ succ) [1;2;3] Figure 1. The Core Language: Example Program but \nthe purely functional core of ML can be translated into our core language. The semantics of the language \nis de.ned in big-step operational style. The evaluation relation s . v denotes that a state sevaluates \nto a value v. Each value is a state, but not conversely values represent states that need no further \nevaluation. There are three kinds of values, namely primitive constants, al\u00adgebraic constants and closures. \nThis is described below in pseudo-BNF form: Value ::= Constant || Constructor(Value* ) f : .n where f \n. FunctionName . .. Environment and dom .S params(f) An algebraic constant is of the form C(v1,...,vn),where \nC is a constructor symbol and the vi are values. We write list constants using the usual abbreviations \n[] and v1 :: v2 (a list with head value v1 and tail value v2). Aclosure f : .n is a function name f together \nwith an environment ., where the domain of . is a strict subset of the parameters of f. The environment \nbinds all the parameters of f which have already been applied to values, and hence this closure represents \na function with parameters those parameters of f that are not in the domain of .. As an example, with \nthe usual map function, map : \u00d8n is a function of two arguments, while map : {f . id : \u00d8n}n is a function \nof one argument. States include all values, and in addition contain: complete closures of the form f \n: .n,where dom .is the set of parameters of f, and expression states of the form e: .: State ::= Value \n| f : .n where f . FunctionName . .. Environment and dom .= params(f) | Expression : Environment The \n.rst case represents a fully applied function, which must be evaluated, while the second represents an \nintermediate state in the evaluation of a function body. For instance, map : {f . id : \u00d8n,xs . []}n is \na state representing an application of map that requires evalu\u00adation. While complete closures use the \nsame notation as proper closures (values), the domain of the environment can be used to separate the \ntwo cases. The semantics is presented in Figure 2. We shall not comment on this further as it is largely \nstandard, in the style of the semantics of ML (Milner et al. 1997). For completeness we note that the \ngiven Values, Variables and Constants Function References and Closures body(f):..v v is a value dom \n.=params(f) v .vx:...(x) c:..cf :.. f :\u00d8n f :.n .v Primitive Operators Conditionals (.i)ei :..ci op(c1,...,cn)=ceg \n:..true et :..veg :..false ef :..v (.i)ci is a constant op(e1,...,en):..c if eg then et else ef :..v \nif eg then et else ef :..v Constructors Pattern Matching e:..Cl(v1,...,v) el :..{xjl .vj }nl .v (.i)ei \n:..vinl j=1 DEk C(e1,...,en):..C(v1,...,vn) match ewith Ci(x1i ,...,xni i ).ei :..v i=1 Function Application \ne1 :.. f :\u00b5n e2 :..wf :\u00b5n .w .v e1e2 :..v Notation:  body(f)is the body of function f, and params(f)is \nthe set of its parameters op is the evaluation function for an operator op (so +is the addition function) \n  f :\u00b5n .v = f :\u00b5.{x.v}n,where xis the .rst parameter of f not bound in \u00b5.  Figure 2. Semantics of \nthe Core Language semantics excludes rules for evaluating error states (for instance, pattern matching \nfailure), but that these are straightforward.  3.2 Dynamic and Static Call Graphs The semantics that \nwe have presented previously is useful as a con\u00adcise de.nition of the language, but it is not directly \npossible to use the SCT analysis on this semantics. This is because nontermination is not explicitly \nrepresented in the big-step style. In this semantics (augmented with error rules), evaluation of a state \nsis nonterminat\u00ading iff it is not the case that s .vfor some v. This implicit negative characterisation \nis inconvenient, and gives no handle on an in.nite series of events for nonterminating evaluations. As \nthe SCT method operates by proving that such in.nite execution sequences are im\u00adpossible, this is a serious \nobstacle. To remedy this issue, we introduce the dynamic call relation .. This is a relation between \nstates, and the intended meaning of the judgement s . s ' is that evaluation of state snecessarily depends \non evaluation of state s ' . This does not imply that .is a small-step reduction relation, as we shall \nsee below, as .is not related to the transitive closure of .. The de.nition of the .relation is given \nin Figure 3. Informally, s . s ' whenever the evaluation of s ' appears as a premiss of the rule that \napplies to the evaluation of s. For instance, the function application rules shown in Figure 3 reveal \nthat a function applica\u00adtion state e1e2 :..rst calls the operator state e1 :., and provided this evaluates \nto a closure, calls the operand state e2 : .. Finally, if both evaluations terminate without errors, \nthere is a .nal call to the callee function. This last call is the only proper function call, while the \n.rst two calls re.ect control dependencies. Furthermore, the calls to operator and operand states are \nnot reduction steps, as they do not preserve the values of states. The . relation can be systematically \nde.ned for any big-step operational semantics. We assume that two properties of the se\u00admantics hold: \nit should be deterministic, so that only one rule may apply to a state, and total, so that some rule \napplies to each state. Note that totality requires the addition of error rules to avoid stuck states, \nfor instance for ill-typed expressions. Given these two rules, the following holds: s . v for some v \niff there is no in.nite sequence of calls s.s1 .s2 . \u00b7\u00b7\u00b7 This relation is thus suf.cient to prove termination \nby the SCT prin\u00adciple: if we show that in.nite sequences of calls are impossible, then each state evaluates \nto a result (possibly an error value). The converse implication guarantees that no precision is lost \nby restrict\u00ading our attention to the .relation when proving termination. The Static Call Graph The dynamic \ncall graph, de.ned by the .relation is of course not computable, as otherwise the termina\u00adtion problem \nwould be decidable. We must hence approximate the dynamic call graph, leading to the static call graph. \nThe nature of this approximation is not .xed (unlike in Jones and Bohr (2004)), and the choice of approximation \nhas an important impact on the precision of the analysis, so we shall defer its de.nition to the next \nsection. However, we give the requirements for call graph construc\u00adtions here. We require a .nite set \nStatea of abstract states, where each ab\u00adstract state represents a set of concrete states. The correspondence \nbetween abstract and concrete states is de.ned by an abstraction function: a :State .Statea In addition, \nthe set Statea carries a partial order ., such that sc s ' whenever sis more de.ned than s ' , that is \nrepresents a strict subset of the states represented by s ' . For any state s, a(s)should be the most \nprecise (least in the .order) state representing s. As a result, the set of states represented by an \nabstract state can be deduced, and de.nes the concretisation function .(s)={t.State |a(t).s} The static \ncall graph approximates the dynamic call graph, and is de.ned by the abstract equivalent of the . relation, \nfollowing the usual abstract interpretation de.nition of safety. Namely, the call graph is sound provided \n Primitive Operators Closures (. j = i)ej :.. cj i<n dom .=params(f) (.j=i)cj is a constant op(e1,...,en):.. \nei+1 :.f :.n . body(f):. Constructors and Pattern Matching (. j = i)ej :.. vj i<n DEk C(e1,...,en):.. \nei+1 :. match ewith Ci(x1i ,...,x ni i ). ei :.. e:. i=1 e:.. Cl(v1,...,vnl ) DEk ii l match ewith Ci(x1,...,x \n). ei :.. el :..{xj . vj } nl ni j=1 i=1 Conditionals eg :.. true eg :.. false if eg then et else ef \n:.. eg :. if eg then et else ef :.. et :. if eg then et else ef :.. ef :. Function Application e1 :.. \nf :\u00b5n e1 :.. f :\u00b5n e2 :.. w e1e2 :.. e1 :.e1e2 :.. e2 :.e1e2 :.. f :\u00b5n . w Figure 3. Semantics: Dynamic \nCall Graph Whenever s . s ' in the dynamic call graph, then if t ; '' ' a(s), there exists t ; a(s )such \nthat t . a t in the static call graph. It is convenient to further require that the abstract interpretation \nde.ne an approximation . a of the evaluation judgement, obeying the equivalent axiom. In Section 4, we \nshall give three different instantiations of the set Statea, varying in complexity and precision, before \nevaluating the resulting analyses in Section 5.  3.3 Tracking Data.ow The static call graph alone is \nnot suf.cient to prove termination in the SCT framework. As we have described previously it is further \nnecessary to annotate each edge in the static call graph with a size-change graph describing the data.ow \nin a call, as well as the changes in sizes of values between the caller and the callee. In this section \nwe de.ne the order on values (more generally on arbitrary states), and de.ne a general procedure for \ncomputing size-change graphs independently of the static analysis used. The order on values is constructed \nfrom two sources: a given order <c say on primitive constants, and a structural order on all other values. \nMore precisely, we de.ne the order as follows, by recursion on the right-hand side value or state: ' \nc '' c<c .. c<c for constants c,c w<C(v1,...,vn) .. (. i)w = vi w< f :.n .. (. x)w = .(x) w<e:. .. (. \nx)w = .(x) This de.nition treats algebraic values and closures in the same way, which certainly has the \nadvantage of simplicity. We shall later see how the order on closures turns out to be useful in proving \ntermination. Environment Paths In the .rst-order case, size-change graphs track data.ow and size relationships \nbetween function parameters. It is possible to restrict oneself to the same abstraction in the higher\u00adorder \ncase also, but this proves limiting. Indeed, in the presence of higher-order functions environments are \nno longer .at, and values stored in the environment may themselves be functions with param\u00adeters1. We \nare therefore moved to give a more general de.nition of the subparts of the state, which we represent \nas environment paths. An environment path is a string of variable names representing a path from the \nroot of the tree representing a state sto a substate of s. Formally, De.nition 1. The set of environment \npaths of a state s, called the graph basis of s, isde.nedasfollows: gb(c)= { e} for a constant c n [ \ngb(C(v1,...,vn)) = { e}. { xip| p. gb(vi)} i=1 [ gb( f :.n)= { e}. { xp| p. gb(.(x))} x.dom . [ gb(e:.)= \n{ e}. { xp| p. gb(.(x))} x.dom . where concatenation of strings is denoted by juxtaposition, so that \nxpis the path ppreceded by the variable x. In the constructor case, dummy variable names x1, ..., xn \nare used to identify constructor parameters. For a complete closure state f :.n, the parameters of f \ncan be found as strings of length one, so this is a (strict) generalisation of the use of function parameters \nin the .rst-order case. De.nition 2. The substate of a state s at environment path p is denoted by s. \np, and is the subtree of sat path pfrom the root: s. e = s C(v1,...,vn). xip = vi. p f :.n. xp = .(x). \np e:.. xp = .(x). p 1 Environments need not be .at in the .rst order case in the presence of algebraic \nvalues, and this limitation also applies in the .rst-order case, but was not previously noted. Data.ow \nin the Dynamic Call Graph As a useful stepping stone to de.ning size-change graphs for the static call \ngraph, we de.ne an instrumentation of the dynamic call graph to record data.ow infor\u00admation alongside \nevaluations and calls. We shall not be concerned with size changes yet, but merely to account for the \neffect of each evaluation step on data. We shall then use this to de.ne size-change graphs in the static \ncall graph. Note that while we are chie.y con\u00adcerned with data.ow along calls (the .relation), it is \nconvenient to de.ne data.ow instrumentations for both calls and evaluations (the .relation). De.nition \n3. Let s and s ' be states. A data.ow graph G for (s,s ' ) is a relation G .gb(s) \u00d7gb(s ' ). The graph \nG is safe for (s,s ' ) if whenever (p,q) .G, s.p = s ' .q. G Whenever (p,q) . G, we write p -. q. The \ninstrumentation of the dynamic call graph de.nes new judgements s . s ' ,G and s . v,G. The intended \nmeaning is that G is safe, and thus accurately describes data.ow, for (s,s ' ) (resp. (s,v)). Inspection \nof the semantics in Figures 2 and 3 reveals that there are four rules giving rise to nontrivial data.ow: \nvariable lookup, function application, pattern matching and constructor application. Furthermore, the \nlast two operations are direct equivalents of the .rst two, as closures and algebraic values share the \nsame structure. It thus suf.ces to describe data.ow for variable lookup and function application. To \nillustrate these cases, we shall use the following running example. We consider the apply function: apply \nfx=fx and describe two stages of its evaluation in the following environ\u00adment: s = {f . apply : {f . \nid : \u00d8n}n,x .0} The .rst step of its evaluation is to .nd the value of f in the environ\u00adment, that is \nto say to evaluate the state sf = f : s. Evaluation of sf will illustrate variable lookup. To illustrate \nfunction application, we consider evaluation of the state sapp = fx : s, which occurs after both the \noperator f and the operand x have been evaluated. Variable Lookup Consider a state s = x : ..Then s . \nv = .(x). The data.ow in this state transition is straightforward: the value bound to x (that is, at \nenvironment path x)in s is just v. Therefore: Lemma 4. The graph {x : p .p |p .gb(v)}is safe for (s,v). \nFor instance, consider the state sf = f : s from our running example. Then sf .vfun = s(f)= apply : {f \n. id : \u00d8n}n and the the data.ow graph for this evaluation judgement is shown below: e f ff e f d d x \n Function Application Let s be a state of the form s = e1e2 : .,and let s1 = e1 : . and s2 = e2 : .. \nSuppose further that s1 .sb,where sb = f : \u00b5n with G1 safe for (s1,sb);and that s2 .w with G2 safe for \n(s2,w). Evaluation of s then requires evaluation of the state s ' = f : \u00b5 .{x .w}n,where x is the .rst \nparameter of f not in dom \u00b5. The data.ow in the state transition (s,s ' ) is given by the graph de.ned \nbelow. Lemma 5. The graph: G2 {p .q .G1 |p,q - = e}.{p .x : q |p .q .q = e} is safe for (s,s ' ). The \ninterpretation of this graph is as follows: the .rst compo\u00adnent re.ects data.ow to parameters of the \ncallee that have already been bound, while the second component describes data.ow to the newly bound \nvalue of x. The exclusion of the epath re.ects the fact that G1 and G2 are data.ow graphs from states \ns1 and s2 (resp.), and that while the environments of these states are the same as that of s,the s1 and \ns2 are not equal to s. Let us illustrate the function application case with our running example, this \ntime considering the state sapp = fx : s. Evaluation of this state proceeds as follows. The operand is \nevaluated .rst, which is just the judgement sf . vfun presented in the variable lookup case. Then, the \noperand sx = x : s is evaluated to 1. Finally, sapp .s ' ,where s ' is the state for the callee: s ' \n= apply : {f . id : \u00d8n,x .1}n The data.ow graphs for these three operations (operator evaluation, operand \nevaluation and transition to the callee function) are shown below: e e e e f ed f f ff fd ff ff fd x \nx ed x xd  ' sf .sfun sx .1 sapp .s Abstract Interpretation The data.ow graphs provide all the nec\u00adessary \ninformation to de.ne size-change graphs in the static call graph, and thus apply the SCT method to prove \ntermination. The one exception is the construction of size-change graphs for primi\u00adtive operators, such \nas operators on numbers, which may handled as in .rst-order SCT analysis (Lee et al. 2001; Frederiksen \n2001). In this section we de.ne size-change graphs in the static call graph, and show how these may be \ndeduced from data.ow graphs. We .rst observe that as an abstract state can represent several concrete \nstates, the graph bases of these states may vary and there\u00adfore may not be known. We must therefore approximate \nthe graph basis of an abstract state: De.nition 6. Let t be an abstract state. The graph basis gba(t) \nof t is a set of environment paths such that gba(t) .gb(s) whenever s ..(t). A size-change graph is then \nde.ned as annotating a pair of abstract states, giving both data.ow and size-change information: De.nition \n7. A size-change graph for abstract states (t,t ' ) is a labelled relation G .gba(t) \u00d7{=,>}\u00d7gba(t ' ).This \nis safe for >= (t,t ' ) iff whenever p -.q (resp. p -.q)in G, then for any states s ..(t), s ' ..(t ' \n), s.p>s ' .q (resp. s.p =s ' .q). In other words, the information encoded in the size-change graph should \naccurately describe any states that the abstract states might represent. Apart from the move to abstract \nstates, the crucial difference with data.ow graphs is the presence of size change information (edges \nlabelled >). However, this is not as substantial a difference as might appear at .rst sight, as these \nmay be deduced from data.ow graphs. The key is the observation that data.ow between nested environment \npaths interacts with the subtree order for values. For instance, consider the graph basis A ={x, xy}and \nlet G be the graph on A with the single arrow (xy, =,x). This graph con\u00adtains no decrease labels. However, \nconsider any (s, s ' )for which G is safe. Then s ' .x = s.xy.Now v = s.xy =(s.x).y, so v is a strict \nsubstate of w =s.x. As our order on states is the subtree order, we deduce that v<w. Putting all of this \ntogether, s ' .x =s.xy =v<w =s.x. We may conclude that the graph G ' is also safe for (s, s ' ),where \nG and G ' are shown below: xx >d xx xy xyxy xy G ' G Generalising this, we may complete a data.ow graph \nwith size\u00adchange information as follows: De.nition 8. Let G be a size-change graph. Then the completion \nof G is: G = G .{(p, >, q)|(.l)(pr, l, q).G .r=e}.{(p, >, qr)|r =e .qr .B .(.l)(p, l, q).G} where as \nbefore pq denotes the concatenation of p and q. The completion operator on graphs allows us to recover \ninfor\u00admation about sizes from data.ow graphs. To complete the picture, we must account for the fact that \nthe full graph basis of an abstract state is not known (and is not well-de.ned, as an abstract state \nen\u00adcodes several distinct concrete states). To attack this problem, we assume that for each abstract \ndomain there is a depth k such that whenever s ..(t),thengba(t)is just the set of environment paths of \nlength at most k in gb(s). In particular, all states represented by t have the same graph basis up to \na certain depth. A natural choice is k =1, giving states with the same function parameters, but we shall \ngive analyses with arbitrarily large values of k. Given this as\u00adsumption, we de.ne: De.nition 9. Let \nG be a data.ow or size-change graph. The k\u00adrestriction of G is the set of tuples in G such that both \nenvironment paths have length at most k. It is now straightforward to de.ne the size-change graph we \nwish to construct for each edge in the static call graph. Each edge in the static call graph is constructed \nwith an abstract equivalent of one of the rules in the concrete semantics (for instance, the function \napplication rule). For each such abstract rule, the graph annotating the edge is obtained by the same \nconstruction as in the exact semantics, but applying both completion to recover size\u00adchange information, \nand k-restriction to ensure soundness. To illustrate k-restricted size-change graphs, let us return to \nthe running example illustrating data.ow graphs shown in the previous section. We have shown the size-change \ngraphs generated in the evaluation of the state sapp =fx :s in the exact semantics. Below, we give the \nequivalent 1-limited graphs, after completion: e e e e f ed f f f >d x x d e x xd ' sf .vfun sx .1 sapp \n.s > The key difference with the exact graphs is the arrow f -. f in the last graph. This is obtained \nby completion from the previous arrow ff . f:the value of f in the callee state is a proper substate \nof the caller s value for f, and hence is smaller under the subtree order. This example further illustrates \nthe point of the subtree order for closures: it registers decreases in value when self\u00adloops are generated \nby the application of a function parameter, as in this case. Indeed, in the call from apply to itself, \nthe depth of the closure passed to apply decreases, so even though this is a recursive call, it is guaranteed \nto terminate. With this our overview of the generation of size-change graphs is complete. It is important \nto note that as size-change graphs are constructed (by rules such as the application rule illustrated \nabove) from k-restricted graphs, some precision is lost with respect to the annotated dynamic semantics. \nThis is natural in the context of static analysis, however, and is necessary to maintain a computable \napproximation.  4. Control-.ow Analysis The choice of static analysis (essentially determined by the \nset Statea of abstract states) is a crucial factor in the precision of the resulting termination analysis. \nIn this section, we de.ne three call graph constructions, and show how they .t in the higher-order SCT \nmethod: the .rst is the well-known 0CFA analysis (Shivers 1991), the second is a family of analyses which \nwe refer to as k-limited CFA (distinct from k-CFA (Shivers 1991)) and .nally we introduce a new method \nfor call graph construction based on the use of tree automata. 4.1 Common Framework The three analyses \nwe describe share many common aspects, and making these aspects explicit clari.es the issues underlying \ncontrol-.ow analysis in our setting. Each analysis aims to restrict the set of abstract states to a .nite \nset. In order to understand the means of achieving this, one must note that the set of concrete states \nis in.nite for two essentially distinct reasons: .rst, states may con\u00adtain constants, and there are in.nitely \nmany of these; second, the environments of states may have unbounded depth. In keeping with our focus \non higher-order issues we shall concentrate on the second problem. As such, the problem of approximating \nstates reduces to approximating environments by a .nite set. For this reason, in ev\u00adery abstract interpretation \neach abstract state will be represented as apair p/. of a program point and an abstract environment: \nStatea =ProgramPoint \u00d7Environmenta A program point is one of: 1. an expression e, 2. a constructor C,or \n 3. a closure representation f : Sn,where S is a subset of the parameters of f (the variables that are \nalready bound).  In closure program points f : Sn, the set of bound variables is included to distinguish \npartial function applications: map :\u00d8n is a function of two arguments, while map : {f}n is a function \nof one argument. Note that thanks to this choice, for each program point any abstract state with this \nprogram point is unambiguously a value or a proper state: e is always a state, C always a value, and \nf :Sn is a value iff S S params(f). This goes further, and in fact the 1-limited graph basis of a state \n(for a function, this is just the set of its parameters) is entirely .xed by its program point. There \nare only two operations on environments, given below: lookup :Environment . Variable . Value lookup .x=.(x) \nbind :Environment . Variable . Value . Environment bind .xv=..{x. v} These two operations encode all \nfundamental manipulations of en\u00advironments: adding values to the environment and retrieving them. It \nnow suf.ces to de.ne the abstract equivalent of each of these to de.ne the abstract interpretation (call \ngraph construction) for any choice of abstract domain. The types of the abstract operations are as follows: \nlookupa : Environmenta . Variable . P(Valuea) binda : Environmenta . Valuea . Environmenta We will also \nuse an operation derived from binda to simultaneously bind several variables without comment. The types \nof these operations re.ect the fact that each abstract value has a single program point, so that only \nthe environment may be partially unknown. The lookupa operator may therefore return a set of values with \ndistinct program points. Formally, the abstract interpretation is de.ned on the Hoare powerdomain of \nStatea . Given these operations only, it is possible to de.ne the abstract interpretation. This is shown \nin Figure 4, where we use the notation . rather than .a for the abstract evaluation relation. The call \nrelation is left to the companion report, as its derivation from the exact call relation of Figure 3 \nis similar to that of the abstract evaluation relation. With this framework in place, it now suf.ces \nto de.ne our instantiations for Environmenta, to de.ne the analyses that we discuss. As shown previously, \nthe size-change graph constructions can be derived from data.ow annotations of the dynamic call graph, \nand so these are not included in the analyses. 4.2 0CFA 0CFA (Shivers 1991) is the simplest possible \ninstantiation of our framework: no information at all is stored about environments, so the set Environmenta \nis a dummy one-point set { }. 0CFA is thus a context-insensitive analysis: each program point (such as \nthe body of a function) is given only one node in the static call graph, and no distinction is made between \ndifferent occurrences of the same program point. The abstract graph basis of a state is just its 1\u00adlimited \ngraph basis, which is given by the program point. The de.nition of the binda function is trivial, as \nthere is only one abstract environment. De.ning the lookupa function is more involved, however, as no \ninformation is available from the environ\u00adment to give the value of a variable. This is solved using \nclosure analysis, which relies on the assumption that the whole program is being analysed. Given this \nassumption, a value vmay only appear bound to a variable xin an environment if there is some appropri\u00adate \nbinding site in the program P. Suppose that xis a parameter of function f, and that the set of parameters \nof f that appear be\u00adfore xis S. Then the only binding sites for xare those of the form e1e2,where e1 \nevaluates (in 0CFA) to a value with program point f :Sn. We therefore de.ne: lookupa .x= {w| (.e1e2 . \nP) e1/ .a f :Sn/ . e2/ .a w} 0CFA is well-documented, and was used in the .rst higher-order SCT analysis \n(Jones and Bohr 2004). As such, we shall not describe it further, and in particular do not present the \nstandard proof of soundness of closure analysis, but rather give an example showing that it is frequently \ninsuf.cient. Consider the following program: id x=x kus =map id (1 :: us) hvs =map k vs h * where * \nrepresents any arbitrary value. This arti.cial example is a terminating program, and only involves uses \nof the map function, so should be straightforwardly size-change terminating. However, this is not the \ncase in 0CFA. For, the kfunction calls map;in turn map calls k(as kis a parameter to map). The call graph \nis shown in Figure 5. However, there is no size-change information in the k . k call (this is the purpose \nof calling map with argument 1::us in k, as this prevents what is otherwise a decrease in the list value). \nThe imprecision of the 0CFA call graph thus prevents a successful termination proof for even a very simple \nprogram, due to the confusion between unrelated instances of map. While it took a contrived example to \nshow this in a tiny program, as programs grow larger, precision drops through the many uses of functions \nsuch as map. This is the motivation for introducing the more precise analyses that follow.  4.3 k-limited \nCFA In 0CFA, no information at all was kept about environments, lead\u00ading to the above problem with uses \nof the map function. In the ex\u00adample we have shown, however, it is easy to separate the two uses of the \nmap function, by the value bound to the f parameter of map one call binds f to k, the other to id. This \nleads to the idea of k-limited CFA: keep a .xed, limited amount of information about environments in \nthe call graph. In k-limited CFA, we keep the environment (a tree) up to a certain depth k. In particular, \nin 1-limited CFA each variable is mapped to the program point of its value, but no information is kept \nabout the environments of these values. In general, this leads to a family of abstract values and environments \n(one set of environ\u00adments for each k): Valueak = ProgramPoint \u00d7 Environmentak Environmenta 0 = { } Environmentak+1 \n= Variable . Valueak This makes Valueak into a set of trees of depth at most k,where some of the leaf \nnodes may have environment . This special en\u00advironment (the 0CFA environment) denotes that some informa\u00adtion \nis lost in this state. The abstraction map ak : State . Stateak is readily de.ned, and as an example, \nif s = map : {f . apply :{f . id :\u00d8n}n,xs . []}n,then a0(s)= map :{f,xs}n/ a1(s)= map :{f,xs}n/{f . apply \n:{f}n/ ,xs . []} a2(s)= s The k-limited CFA has a useful characteristic in our setting: a k\u00adlimited state \nde.nes the environment up to depth k, so the graph basis of a state is known for the .rst klevels of \nthe environment. Furthermore, a leaf node (for instance, f : Sn) de.nes the set of parameters in the \nenvironment for this node (in this case, this set is S). The graph basis of a k-limited state is therefore \nknown up to depth (k +1), and the abstract graph basis function for k-limited CFA is the (k +1)-limited \ngraph basis function. This analysis therefore allows us to track data.ow between values that occur nested \nin the environment, increasing precision. The bindak operation (for depth k) is once more easy to de.ne: \nbinda .xvsimply binds vto xin the environment, and applies the above ak function to restrict the depth \nof the resulting tree to k. The lookup function uses ideas similar to 0CFA. Given an abstract k-limited \nenvironment .,the value .(x)of a variable x in .is a (k- 1)-limited value and as such may be less precise \nthan is required. The lookupak therefore proceeds in two steps: Values, Variables and Constants Function \nReferences and Closures v .lookupa .x body(f)/. .v v is a value S=params(f) v .v x/..v c/. .c f/. . \nf : \u00d8n/\u00d8 f : Sn/. .v Primitive Operators (.i)ei/. .ci op a(c1,...,cn)= c Conditionals eg/. .c ;true \net/. .v eg/. .c;false ef /. .v op(e1,...,en)/. .c if eg then et else ef /. .v if eg then et else ef /. \n.v Constructors (.i)ei/. .vi C(e1,...,en)/. .C/binda \u00d8{xi .v1,...,xn .vn} Pattern Matching e/..Cl/\u00b5 (.j) \nvj .lookupa \u00b5xj el/binda .{x l j .vj }nl j=1 .v match ewith D Ci(x i 1,...,x i ni ) .ei Ek i=1 /. .v \n Function Application e1/. . f : Sn/\u00b5 e2/. .wf : S.{x}n/binda \u00b5xw .v x is the .rst parameter of f not \nin S e1e2/. .v Figure 4. Abstract Interpretation k id  Figure 5. 0CFA Counterexample Program: Call \nGraph 1. Look up a a value in the environment, yielding a (k-1)-limited value, then 2. Find the set \nof k-limited values that are compatible with this (k-1)-limited value.  This second step once more uses \nclosure analysis: closure analysis is applied exactly as in the 0CFA case to the leaves of the tree, \nconverting each leaf into a set of possible 1-limited trees. Each possible way of substituting these \ntrees for leaves gives rise to a potential k-limited abstract value. To illustrate this, consider the \nfollowing 1-limited state: f : {h,k}n/{h . h1 : {g1}n/ ,k . h2 : {g2}n/ } Suppose further that by closure \nanalysis, there are two potential values for the parameter g1 and three potential values for g2.Then \nthere are in total six 2-limited trees that are compatible with the above 1-limited tree. Each of these \ntrees is obtained by replacing the environments of h1 and h2 with each possible pair of values of g1 \nand g2. The k-limited CFA analysis is considerably more expensive than 0CFA, as the number of abstract \nstates increases (doubly) exponentially with the limit k. However, this increased precision is indeed \nuseful in distinguishing between otherwise equivalent nodes in the call graph. As an example, let us \nconsider the program given as a failure of 0CFA. The 1-limited call graph for this program is given in \nFigure 6. The difference between this call graph and the 0CFA call graph (Figure 5) lies in the fact \nthat the 1-limited call graph ends up with two nodes for the map function, due to the different values \nof the f parameter. This transforms the call graph into one that is size-change terminating: the only \nself-loops left are those around the map function, which are trivially size-change terminating as map \nis de.ned by primitive recursion. start h: {vs .*} map : {f . k : \u00d8n} k : {us .*} id : {x .*} map : \n{f . id : \u00d8n} Figure 6. Example Program: 1-limited CFA Call Graph  4.4 Tree Automata The k-limited \nCFA analysis seeks to resolve the problem posed by uses of functions such as map and fold, where a single \nhigher-order function is called with numerous parameter values, and is a natural way of controlling precision \nin this context. However, k-limited CFA is not a natural way of increasing precision for programs that \ncreate unbounded environments. As an example, consider the following function: hn f g= if n=0 then g \nelse compose f (h (n -1)fg) An application hnf g evaluates to the composition fn .g,rep\u00adresented in the \nsemantics as a closure of n nested applications of the compose function, with g appearing as the second \nargument of the innermost occurrence of compose.The value of gtherefore ap\u00adpears at depth n in the environment. \nAs n is in general unknown in static analysis, the k-limited CFA analysis is incapable of distin\u00adguishing \nbetween two values hnfg1 and hnfg2 whenever the value of n is not fully known. Increasing the value of \nk does not in this case improve precision, and a program similar to the 0CFA counterexample can be constructed \nto take advantage of this. It should be noted that while the above example is contrived, the phenomenon \nof arbitrarily deep closure values is not rare, in particular in the analysis of lazy programs lazy \nprograms can involve arbitrarily long chains of suspended computations, represented as unbounded closures. \nTo remedy this issue with k-limited CFA, we introduce a new analysis, based on the idea of using a restricted \nset of tree automata to represent environments. The idea is to represent a set of trees as g  f g \n Figure 7. Tree Automata Analysis: Example Value a possibly cyclic graph with a root by identifying all \nnodes with the same label. In our context, the label of a node is its program point, so this amounts \nto identifying all nodes in an environment tree with the same program point. For instance, Figure 7 represents \nthe representation of the value of hnfg,for n>0, showing the fact that the value of gis recorded in the \nabstract state. More generally, a value in this analysis is a labelled relation ProgramPoint \u00d7Variable \n\u00d7ProgramPoint, together with a distin\u00adguished root node. This expands to a set of trees by following \nall paths from the root node. The abstraction function is readily de.ned by collapsing a tree to the \nminimal directed graph representing it. The graph basis function just gives the 1-limited graph basis \nof a state, as no de.nite information is known beyond the .rst level in the environment. The de.nition \nof the binda and lookupa operations is of more interest. Let us .rst consider variable lookup. The possible \nvalues of a variable xmay be found by .nding all the successors of the root node of a state along an \nedge x: x lookupa .x = {.[v/root]|root(.)-.v..} where .[v/root]is the graph obtained from .by setting \nnode vto be the root of the graph. For ef.ciency it is desirable to remove unreachable nodes from the \nresult, but this is not necessary for soundness. To compute the value binda .xv, three steps must be \ntaken: 1. Add a new root to the graph ., to record the fact that xis bound. If the previous root was \nf :Sn, the new root is f :S.{x}n. 2. Take the union of the two graphs .and v. This merges corre\u00adsponding \nnodes, and so naturally can cause a loss of precision. 3. Add an edge labelled xfrom the root of the \nnew graph to the root of v, to record the binding of x.  The soundness proof of these operations is \nstraightforward but lengthy, and the interested reader is referred to the companion re\u00adport (Sereni 2006) \nfor details. It is however interesting to com\u00adpare the k-limited CFA approaches and this graph approach. \nIn k-limited CFA, the environment is kept exactly (up to constants) for a certain depth, but no contextual \ninformation is stored beyond this depth. In contrast, this analysis stores information at arbitrary depth, \nbut no information is stored exactly as any two nodes with the same label are merged. While we have argued \nthat k-limited CFA is more natural for programs with bounded environments and that the tree automata \nanalysis is better suited for unbounded envi\u00adronments, this further suggests that these analyses may \nbe incom\u00adparable, and that each may offer better precision in certain contexts. The purpose of the next \nsection is to present these claims in a more precise light.  5. Precision of the Analysis We have in \nthe previous sections introduced the framework for higher-order SCT analysis, and given three instantiations \nof this framework with control-.ow analyses, namely the inexpensive context-insensitive 0CFA analysis, \nthe bounded context k-limited analyses, and the unbounded depth tree automata analysis. These vary wildly \nin cost: the 0CFA is invariably cheapest, the worst-case cost of k-limited analyses increases doubly \nexponentially with k (but for .xed kis polynomial in the size of the program), while the tree automata \nanalysis has an exponential worst-case complexity, rarely attained. It is thus crucial to evaluate what \nis achieved by more precise analyses. Our aim in this section is to give exact relationships between \nthe sets of programs accepted by each analysis. For each CFA, this is the set of programs such that the \nconstructed call graph is size-change terminating. This set is always a subset of the set T of terminating \nprograms, but can never equal it by undecidability. Any terminating program not in this set represents \na failure of the analysis, and so the aim is that this set should be as large as possible. In this section, \nwe compare the sets 0CFA, kCFA and TA of programs accepted by the 0CFA, k-limited CFA and tree automata \nanalyses (resp.). The .nal result is the precise hierarchy shown in Figure 8. 5.1 The k-limited CFA Hierarchy \nWe have argued that k-limited CFA increases precision as k in\u00adcreases. This is indeed the case, and the \n.rst part of this result is the following k-limited CFA hierarchy theorem: 0CFA .1CFA .\u00b7\u00b7\u00b7.kCFA .(k+1)CFA \n.\u00b7\u00b7\u00b7 The key to establish this result is to show that the (k+1)-limited call graph is more precise than \nthe k-limited call graph, in the sense de.ned below: De.nition 10. Let A and B be static call graphs \nfor a program P, de.ning relations .A , .A and .B , .B respectively, and with abstract state sets Aand \nB. Amap f : A . Bis a simulation if whenever s .A v(resp. s .A s ' )and t ; f(s), then there exists w \n; f(v) (resp. '' ' t ; f(s)) such that t .B w (resp. t .B t). In addition the size-change graph of the \nedge in Bis a subset of that of the edge in A. Informally, such a map f takes an edge s . s ' in A to \nan edge f(s) . f(s ' ) in B, but as some precision may be lost in this embedding it is permitted for \nthe target edge t. t ' to be less precise (in the ;order) than f(s).f(s ' ). If there is a simulation \nfrom A to B, then each edge in A is re.ected in B,and so B cannot be more precise than A.In particular, \nwe have the following: Lemma 11. Suppose that f is a simulation from Ato B, and that Bis size-change \nterminating. Then Ais size-change terminating. Proof sketch. Suppose that Ais not size-change terminating. \nThen there must exist a cycle s0 . s1 . \u00b7\u00b7\u00b7 . sn = s0 in A such that the sequence of size-change graphs \n.i annotating each edge does not give rise to in.nite descent. The simulation f allows us to construct \na corresponding sequence t0 . \u00b7\u00b7\u00b7 . tn in B,where ti f(si). Furthermore, each size-change graph in this \nsequence is a subset of the corresponding graph in A, so this sequence does not give rise to in.nite \ndescent and Bis nonterminating. This result allows us to compare static analyses for the SCT framework. \nCorollary 12. Suppose that G and G ' are control-.ow analyses, and that for each P there is a simulation \nfrom the G-call graph PG ' of P to the PG. . Then any program accepted by G is accepted by G. Proof. \nIf P is accepted by G ' ,then P G. is size-change terminating. As there is a simulation from P G to P \nG. ,sois P G. Hence G accepts P . We are now able to show that (k +1)CFA . kCFA. By the corollary it \nsuf.ces to show that there is a simulation from the (k +1)-limited call graph to the k-limited call graph. \nProposition 13. The map ak (restricting a state to a tree of depth k) is a simulation from a (k +1)-limited \ncall graph to a k-limited call graph. The proof of this result is essentially the same as the proof of \nsoundness of k-limited CFA, and is omitted. As a result, we have that kCFA . (k +1)CFA for each k, as \nrequired. This result shows that (k +1)-limited CFA is at least as precise as k-limited CFA. Naturally, \nwe want more we can certainly hope that is the case that (k +1)-limited CFA is strictly more expressive. \nWe shall show this in the next section, via a detour through the simply-typed .-calculus. 5.2 Simple \nTypes and k-limited CFA The simply-typed .-calculus represents the best-known set of terminating functional \nprograms. By strong normalisation, each simply-typed .-expression terminates under call-by-value. It \nis therefore natural to ask whether the SCT analysis, in one of its forms, can recognise all simply-typed \n.-expressions as terminat\u00ading. Indeed, it was conjectured by Jones (Jones and Bohr 2004) that this was \nthe case under 0CFA. As we shall show, this is not the case, even under k-limited CFA. Consider .rst \nthe following (simply-typable) .-expression: `\u00b4`\u00b4 .0 = .a.a(.b.a(.cd.d)).e.e(.f.f) We claim that .0 lies \nin 1CFA but not 0CFA, so that it is not 0CFA-terminating. To justify this, observe that evaluation of \n.0 proceeds as follows (where we use an equivalent formulation of our semantics for .-terms, which may \nbe translated into our own framework by .-lifting (Johnsson 1985)): start (1) . .a : {a . .e : \u00d8n}n (2) \n . .e : {e . .b : {a . .e : \u00d8n}n}n (3)  . .b : {a . .e : \u00d8n,b . .f : \u00d8n}n (4) . .e : {e . .c : \u00d8n}n \n(5) . .c : {c . .f : \u00d8n}n (6)  . .d : \u00d8n (7) We note that states (3) and (5) are identical under 0CFA, \nas they are both closures of the .e expression, that is if si is the ith state in this sequence, a0(s3)= \na0(s5). Furthermore, the only safe 1\u00adlimited size-change graph for (s3,s5) is empty, so no size-change \ninformation can be obtained. As a result, the 0CFA call graph for .0 contains a cycle around a0(s3)= \na0(s5), which cannot be proved to terminate. However, the 1CFA call graph for .0 is acyclic, as the 1-limited \nabstraction of each state in the above call sequence is distinct. As a result, .0 lies in 1CFA \\ 0CFA. \nNote that this is not an artefact of our formulation of the 0CFA analysis the above argument shows that \n.0 cannot be recognised as size-change terminating under any CFA with the same abstract domain as our \n0CFA. This result extends beyond 1CFA, and indeed for each k there is a .-expression in (k +1)CFA \\ kCFA. \nThese can be derived from .0 by noting a general technique for manufacturing pro\u00adgrams that defeat k-limited \nCFA. De.ne the application function apply = .xy.xy. Then given any application e1e2 in a program, transforming \nthis to apply e1 e2 has the effect of binding the value of e1 in the environment of apply before applying \nit. This increases the depth of intermediate states by one, as the value of e1 would otherwise have been \nfound at the top level of the environment. By nesting applications of apply it is therefore possible \nto increase the depth of intermediate states arbitrarily, which may be used to con\u00adfuse k-limited CFA. \nThe result of applying this technique to our example is given below: `kk\u00b4`\u00b4 .k = .ga.a(g (.b.a(g (.cd.d))))apply.e.e(.f.f) \nk 1 n+1 where e1 e2 is syntactically de.ned by e1e2 = e1e2 and e1 e2 = n e1(e1 e2). Once more .k ./kCFA, \nas the nested uses of apply add k levels to the environment, but .k . (k +1)CFA. Before recapitulating \nour results, it is interesting to consider the entire k-limited CFA hierarchy. De.ne [ .kCFA = kCFA k \nto be the set of programs recognised by some k-limited CFA analy\u00adsis. This is not obviously computable, \nbut we may wonder whether some analysis might subsume all k-limited CFA analyses. In fact, this is not \nthe case, as a consequence of this result: Proposition 14. For each k = 0, kCFA S (k +1)CFA. Further\u00admore, \nno kCFA contains the simply-typed .-calculus. Finally, the set .kCFA contains all terminating closed \n.-expressions. In particular, each terminating .-expression lies in some kCFA, and the set .kCFA is not \ndecidable. Proof. We have outlined the proofs of much of the result. To show that any terminating .-expression \nlies in .kCFA, note that such an expression has a .nite call graph. There is hence a bound d to the depth \nof states in its call graph. Analysis of this expression in (d + 1)-limited constructs the exact call \ngraph, which is acyclic as the expression terminates, and so is trivially size-change terminating. 5.3 \nTree Automata and k-limited CFA We shall conclude our description of the relationships between our analyses \nby outlining the results relating the tree automata analysis to k-limited CFA analyses. It is not entirely \nunexpected that this analysis performs at least as well as the context-insensitive 0CFA: Lemma 15. TA \n. 0CFA. Proof outline. The map taking each environment to the dummy environment is a simulation from \na tree automata call graph to a 0CFA call graph, so we may appeal to Corollary 12. However, as we have \ninformally argued before, the tree au\u00adtomata analysis does not keep precise environment information at \nany depth, and so should not be expected to be strictly more ex\u00adpressive than any k-limited CFA for k> \n0: Lemma 16. TA . 1CFA. Proof outline. The program: id x=x omega x = omega x fxyz=yz f (fid omega)id \n1 is terminating under 1-limited CFA, but not the tree automata analysis. The key is that in the state \nresulting from evaluating f (f id omega) id, the second argument of f appears bound to omega and id, \nand thus as these two uses are not distinguished, f is found to potentially apply omega. .kCFA TA Figure \n8. Precision of Static Analyses Thus all k-limited analyses for k=1 accept programs that the tree automata \nanalysis does not. The two analyses are essentially orthogonal, however, and the tree automata analysis \naccepts pro\u00adgrams that require arbitrarily large depth values in k-limited CFA: Lemma 17. For each k,TA \nn((k+1)CFA \\kCFA) is nonempty. Proof outline. The expression .k is accepted by the tree automata analysis \nfor each k(proof omitted), whence the result follows. The tree automata analysis therefore intersects \nthe k-limited CFA hierarchy at each level, but does not perform better than any .xed k-limited CFA for \nk>0. To conclude this section, we describe the relationship between the tree automata analysis and the \ncumulative .kCFA criterion. As we have shown previously, TA cannot be a superset of .kCFA. It is in fact \nincomparable with .kCFA, though it intersects each kCFA nontrivially: Lemma 18. TA ..kCFA. The tree automata \nanalysis thus accepts programs that cannot be handled by any k-limited CFA analysis. The proof of this \nresult is omitted as the corresponding program is lengthy, but may be found in a companion report (Sereni \n2006). This completes our discussion of relationships between static analyses, the results of which are \nsummarised in Figure 8.  6. Related Work The size-change termination criterion was introduced by Lee, \nJones, and Ben-Amram (2001), for .rst-order purely functional (strict) programs, and focusing on the \nalgorithm for deciding ter\u00admination rather than program analysis. Several extensions have been proposed, \nfor instance to handle non-well founded datatypes (Avery 2006). The extension to higher-order functions \nwas .rst studied in the context of the pure untyped .-calculus (Jones and Bohr 2004), implicitly using \n0CFA. Sereni and Jones (2005) ex\u00adtend this to a larger higher-order language and introduce the use of \nk-limited CFA. Our framework is a substantial generalisation of this work, and we further introduce the \ntree automata CFA for programs with unbounded environments. Many other termination provers have been \nproposed, with a strong emphasis on logic programming (for instance, Termilog (Lindenstrauss and Sagiv \n1996, 1997) which appears to be closely related to the size-change termination analysis) and term rewriting \nsystems (one of the most recent tools being the AProVE system (Giesl et al. 2005, 2004)). While these \nsettings naturally empha\u00adsize .rst-order programs, and indeed most TRS termination provers are restricted \nto .rst-order systems, there have been a number of studies of termination in a higher-order context. \nIn particular, Giesl et al. (2006) show that termination tools for term rewriting systems, and in particular \nthe AProVE system, can fruitfully be applied to termination of (lazy) functional programs, in particular \nhandling higher-order functions. While it is dif.cult to compare such tools to our framework, in particular \nbecause of their reliance on theorem proving, it appears likely that techniques for improving call graph \nprecision would be applicable, and would improve results, in such settings. A different approach to termination \nanalysis is the transi\u00adtion predicate abstraction (Podelski and Rybalchenko 2005) tech\u00adnique used by \nthe Terminator tool (Cook et al. 2006a,b). While this has only been used for .rst-order (imperative) \nprograms, the ter\u00admination criterion used by Terminator, based on disjunctive well\u00adfounded relations, \nappears similar to size-change termination, sug\u00adgesting that our results may be applicable. Control-.ow \nanalysis for functional programs has been exten\u00adsively studied, starting with 0CFA (Shivers 1991, 1988). \nThe kCFA analyses, also due to Shivers, are different from our k-limited CFA analyses in that they use \napproximations of the call stack, rather than environments, to improve precision. Approximating environ\u00adments \nhas the bene.t of allowing better data.ow information to be recorded, improving precision at the cost \nof increased complexity. Many other settings for control-.ow analysis have been proposed, from the use \nof game semantics (Malacaria and Hankin 1998) to the approximation of function values, rather than closures, \nin an abstract interpretation context (Cousot and Cousot 1994, 1991). Our tree automata analysis is related \nto the .ow analysis of Jones (Jones 1987), in which regular tree grammars are used to describe reachable \nprogram states in lazy higher-order functional programs. GCFA (Might and Shivers 2006) is a promising \nnew technique for improving .ow analyses of higher-order programs, but we shall leave the study of this \ntechnique in our framework to future work. Finally, there has been little investigation into the relationship \nbetween static analysis and typing. A notable exception is the work of Palsberg and Schwartzbach (1995) \non safety analysis of the .\u00adcalculus, where it is shown that 0CFA safety analysis, surprisingly, is strictly \nmore expressive than simple types: any simply-typed term will be accepted by the 0CFA safety analysis. \nIn constrast, no k-limited CFA analysis achieves this for termination in the SCT framework. 7. Conclusion \nWe have presented a framework for size-change termination analy\u00adsis of higher-order functional programs, \nbuilding on previous work adapting this framework to the higher-order setting. This analysis proceeds \nin two steps, as the call graph of the program is computed .rst, together with data.ow and size change \ninformation, before an existing algorithm can be applied to prove termination. We set out requirements \nfor the static call graph construction, independently of the gathering of termination information, and \nhave shown three distinct instantiations of this procedure with control-.ow analyses: the (known) context-insensitive \n0CFA anal\u00adysis, the family of k-limited CFA analyses, and an analysis based on the use of simple tree \nautomata to represent environments. The precision of the call graph constructed in the .rst phase of \nthe analysis emerges as a crucial factor in.uencing the expressive\u00adness of the resulting termination \ncriterion. In particular, proving ter\u00admination of programs using the inexpensive 0CFA analysis is likely \nto fail due to the imprecision of the call graph whenever common functions such as map are used repeatedly. \nWe proposed a characterisation of precision of control-.ow analyses, through the study of the expressiveness \nof the size-change termination analysis obtained with each control-.ow analysis. This gives precise relationships \nbetween the sets of programs that each analysis accepts. Settling a previous conjecture, we have further \nclari.ed the relationship between the simply-typed .-calculus and SCT analyses. While the use of termination \nanalysis results to evaluate call graph precision may appear limiting, it seems that this corresponds \nclosely to our intuitive notion of precision, mainly through the simplicity of the SCT criterion. It \nis hence likely that this measure will be a good indicator of precision in other settings. Avenues of \nfuture work include .rst the study of recently pro\u00adposed methods for CFA such as G-CFA (Might and Shivers \n2006) in our framework. This techniques are promising ways to improve precision of CFA, and the question \nof their relationship to our hierarchy is unknown. A separate area of future work is a more systematic \nstudy of the relationship between the simply-typed .\u00adcalculus and SCT or related termination analyses, \nand in particular determining whether some control-.ow analysis can recognise all simply-typed .-expressions \nas size-change terminating.  References James Avery. Size-change termination and bound analysis. In \nProceedings of FLOPS 06, volume 3945 of LNCS, pages 192 207. Springer, 2006. Josh Berdine, Byron Cook, \nDino Distefano, and Peter W. O Hearn. Automatic termination proofs for programs with shape-shifting heaps. \nIn Proceedings of CAV 06, volume 4144 of LNCS, pages 386 400. Springer, 2006. Byron Cook, Andreas Podelski, \nand Andrey Rybalchenko. Termi\u00adnation proofs for systems code. In Proceedings of PLDI 06, pages 415 426. \nACM Press, 2006a. Byron Cook, Andreas Podelski, and Andrey Rybalchenko. Termi\u00adnator: Beyond safety. In \nProceedings of CAV 06, volume 4144 of LNCS, pages 415 418. Springer, 2006b. Patrick Cousot and Radhia \nCousot. Abstract interpretation: a uni\u00ad.ed lattice model for static analysis of programs by construc\u00adtion \nor approximation of .xpoints. In Proceedings of POPL 77, pages 238 252. ACM Press, 1977. Patrick Cousot \nand Radhia Cousot. Relational abstract interpreta\u00adtion of higher-order functional programs. In Actes \nJTASPEFL 91, volume 74 of Bigre, 1991. Patrick Cousot and Radhia Cousot. Higher-order abstract inter\u00adpretation \n(and application to comportment analysis generaliz\u00ading strictness, termination, projection and PER analysis \nof func\u00adtional languages), invited paper. In Proceedings of the IEEE International Conference on Computer \nLanguages (ICLL 94), pages 95 112. IEEE Computer Society Press, 1994. Carl Christian Frederiksen. A simple \nimplementation of the size-change termination principle. Working paper (DIKU, D-442). Available online \nat http://www.diku.dk/topps/ bibliography/2001.html, 2001. J\u00a8urgen Giesl, Ren\u00b4e Thiemann, Peter Schneider-Kamp, \nand Stephan Falke. Automated termination proofs with AProVE.In Pro\u00adceedings of RTA 04, volume 3091 of \nLNCS, pages 210 220. Springer, 2004. J\u00a8urgen Giesl, Ren\u00b4e Thiemann, and Peter Schneider-Kamp. Prov\u00ading \nand disproving termination of higher-order functions. In Proceedings of the 5th International Workshop \non Frontiers of Combining Systems (FroCoS 05), volume 3717 of LNAI, pages 216 231. Springer, 2005. J\u00a8urgen \nGiesl, Stephan Swiderski, Peter Schneider-Kamp, and Ren\u00b4e Thiemann. Automated termination analysis for \nhaskell: From term rewriting to programming languages. In Proceedings of RTA 06, volume 4098 of LNCS, \npages 297 312. Springer, 2006. Thomas Johnsson. Lambda lifting: Transforming programs to recursive equations. \nIn Proceedings of FPCA 85, volume 201 of LNCS, pages 190 203. Springer, 1985. Neil D. Jones. Flow analysis \nof lazy higher-order functional pro\u00adgrams. In Samson Abramsky and Chris Hankin, editors, Ab\u00adstract Interpretation \nof Declarative Languages, pages 103 122. Ellis Horwood, 1987. Neil D. Jones and Nina Bohr. Termination \nanalysis of the untyped .-calculus. In Proceedings of RTA 04, volume 3091 of LNCS. Springer, 2004. Chin \nSoon Lee, Neil D. Jones, and Amir M. Ben-Amram. The size-change principle for program termination. In \nProceedings of POPL 01, pages 81 92. ACM Press, 2001. Naomi Lindenstrauss and Yehoshua Sagiv. Checking \ntermination of queries to logic programs. Available online at http://www. cs.huji.ac.il/~naomil/, 1996. \nNaomi Lindenstrauss and Yehoshua Sagiv. Automatic termination analysis of logic programs. In Proceedings \nof ICLP 97, pages 63 77. MIT Press, 1997. Pasquale Malacaria and Chris Hankin. A new approach to control \n.ow analysis. In Proceedings of CC 98, volume 1383 of LNCS, pages 95 108. Springer, 1998. Matthew Might \nand Olin Shivers. Improving .ow analyses via GCFA: Abstract garbage collection and counting. In Proceed\u00adings \nof ICFP 06, pages 13 25, Portland, Oregon, September 2006. Robin Milner, Mads Tofte, Robert Harper, and \nDavid MacQueen. The De.nition of Standard ML (Revised). MIT Press, May 1997. Jens Palsberg and Michael \nI. Schwartzbach. Safety analysis versus type inference. Information and Computation, 118(1):128 141, \n1995. Simon Peyton-Jones. The Implementation of Functional Program\u00adming Languages. Prentice-Hall, 1987. \nOut of print. Online ver\u00adsion available at http://research.microsoft.com/users/ simonpj/papers/slpj-book-1987/. \nAndreas Podelski and Andrey Rybalchenko. Transition predicate abstraction and fair termination. In Proceedings \nof POPL 05, pages 132 144. ACM Press, 2005. Damien Sereni. Termination Analysis of Higher-Order Func\u00adtional \nPrograms. PhD thesis, Oxford University, 2006. Online version at http://metacomp.comlab.ox.ac.uk/Members/ \ndamien/publications/thesis.pdf. Damien Sereni and Neil D. Jones. Termination analysis of higher\u00adorder \nfunctional programs. In Proceedings of APLAS 05,vol\u00adume 3780 of LNCS, pages 281 297. Springer, 2005. \nOlin Shivers. Control-.ow analysis in Scheme. In Proceedings of PLDI 88, pages 164 174. ACM Press, June \n1988. Olin Shivers. Control-Flow Analysis of Higher-Order Languages. PhD thesis, Carnegie Mellon University, \nMay 1991. \n\t\t\t", "proc_id": "1291151", "abstract": "<p>The analysis and verification of higher-order programs raises the issue of control-flow analysis for higher-order languages. The problem of constructing an accurate call graph for a higher-order program has been the topic of extensive research, and numerous methods for flow analysis, varying in complexity and precision, have been suggested.</p> <p>While termination analysis of higher-order programs has been studied, there has been little examination of the impact of call graph construction on the precision of termination checking. We examine the effect of various control-flow analysis techniques on a termination analysis for higher-order functional programs. We present a termination checking framework and instantiate this with three call graph constructions varying in precision and complexity, and illustrate by example the impact of the choice of call graph construction.</p> <p>Our second aim is to use the resulting analyses to shed light on the relationship between control-flow analyses. We prove precise inclusions between the classes of programs recognised as terminating by the same termination criterion over different call graph analyses, giving one of the first characterisations of expressive power of flow analyses for higher-order programs.</p>", "authors": [{"name": "Damien Sereni", "author_profile_id": "81100584039", "affiliation": "Oxford University, Oxford, United Kingdom", "person_id": "P439237", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1291151.1291165", "year": "2007", "article_id": "1291165", "conference": "ICFP", "title": "Termination analysis and call graph construction for higher-order functional programs", "url": "http://dl.acm.org/citation.cfm?id=1291165"}