{"article_publication_date": "10-01-1973", "fulltext": "\n A Parallel Approach Mary Zosel Lawrence Livermore Introduction Design of programs to run on computers \nwith vector processing capability requires redevelop\u00adment of many long-established programming tech\u00adniques. \nThese vector oriented solutions are not limited to those few computers with special vector hardware. \nCurrent experiments indicate that the performance of a scalar computer with a high speed instruction \ncache and overlap, such as the CDC 7600, can be significantly improved when a program consists of a series \nof vector-like loops. (1) In the programming languages area, work in IIvectorizingll problems has included \nincorporation of vector extensions into languages and also the recognition of FORTRAN DO-loops which \ncan be collapsed into vector instructions. Another area for attention is vector izing the compiler itself. \nThis project was undertaken in 1972 by the lan\u00ad guage group at Lawrence Livermore Laboratory. The APL-$ \nTAR (2) compiLer project must actually be considered experimental in several different, but related, \naspects. This paper will concentrate on the compilation of a program using vector instructions, but the \nother aspects of the project also enter into the discussion, and deserve a brief introduction. The high \nlevel of the CDC-STAR (3) instruc\u00adtion set poses a real challenge of providing a high level language \nwhich can offer a competitive al\u00ad ternative to programming directly in machine lan\u00ad59 Permission to make \ndigital or hard copies of part or all of this work or personal or classroom use is granted without fee \nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear \nthis notice and the full citation on the first page. To copy otherwise, to republish, to post on servers, \nor to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; 1973 ACM 0-12345-678-9 \n$5.00 to Compilation Laboratory guage. The similarity of APL (4) and the STAR instruction set made APL \na natural candidate. However, the dynamic nature of APL, requiring a great deal of run-time overhead, \nwas considered unacceptable. Instead of designing a new language, the choice was made to eliminate those \naspects of APL which required the run-time overhead. This required limiting the domain of the operations \nto vector and scalar operands and adding a set of declarations to bind the scope, type, and maximum size \nof the variables in a program. There are really two problems: first to compile any language with the \nSTAR as a target machine, and second to compile an APL subset, regardless of the target machine. Care \nwas taken in selecting the APL subset to ensure that the resulting language was compatible with the APL-7600 \ninterpreter (5, 6) so that the interpreter could be used to develop APL-STAR programs. Since the compiler \nwas intended to run on the STAR, the next question to be answered was what language to use to develop \nthe compiler. Again, APL seemed like the best choice, because of the availability of the interpreter. \nThe APL-STAR compiler is written in APL-STAR so that an even\u00adtual bootstrap to the STAR would be possible. \nThe choice of APL posed yet another question which was explored as the project progressed: is APL a suitable \nlanguage for a large non-numeric problem such as compiler writing? vectorized compiler, the general problem \nis to find successive transformations which can be applied to large portions of the input program, translating \nand reducing it, step by step to a machine code representation. Compilation is no longer the proc\u00ad ess \nof recognizing an occurrence of a condition in a given state and taking some appropriate action. Instead \nthe compiler needs to recognize all occur\u00ad rences of some condition in a given state and take the appropriate \naction. This approach to compiling was quite success\u00ad ful for lexical analysis and symbol table organiza\u00ad \ntion. The problems involved in parsing and code generation met with varying degrees of success and frustration. \nThese problems are much more dependent on the structure of APL-STAR itself, so the characteristics which \nfacilitated or frustrated solution efforts do not necessarily apply to other languages or machines. The \nremaining sections of this paper, describe in order, the lexical analysis, the syntactic analysis, of \nboth declarations and executable statements, and code generation. Code generation which is heavily dependent \non the STAR instruction format is discussed only briefly. LEXICAL ANALYSIS Lexical analysis is the easiest \npart of a com\u00adpiler to visualize as a sequence of transformations. The steps listed below characterize \nthe process for most languages. Most of these steps consist of locating the beginning and end of all \nthe items in some lexical class, forming a bit mask to extract the elements, and marking their places \nwith single representative tokens. The extracted elements are placed in a separate buffer where they \ncan be further processed in a homogeneous manner. Figure 1 follows the steps through as they occur in \na segment of an APL-STAR program. A description of the process for FORTRAN is given in (7). Lexical Analysis \nTransformations 1. Change the source from physical records llcontinuation marks, sequencing information, \nII blanks and other input format compressed , restrictions. 2. Mask out program comments. 3. Extract \nany other input elements which are not subject to syntactic analysis (such as text  strings or format \nspecifications). A single descriptor is left to mark each position. 4. Locate and extract constants from \nthe source. Converted values are stored in a separate vector. A pointer to the value is left in the source \nstring. 5. Alphanumeric tokens are identified. These may include syntactic words of the program as well \nas identifiers. These tokens are moved to another buffer for identification, and a single marker holds \neach place. 6. All blanks are removed from the source. At this point the size of the input has been greatly \nreduced. One characteristic of this re\u00ad duced form is that there are no remaining variable length tokens. \nEach string, identifier, constant, and operator has been reduced to a sing Ie entry. This is an important \ndetail when looking for the patterns of tokens which can be identified as syn\u00adtactic elements of the \nlanguage. One further source reduction which may be classified as lexical is the identification of the \nalphanumeric tokens. The list of identifiers and syntactic elements is reduced to a list where each name \noccurs exactly once. Tokens which may appear as either syntactic symbols or identifiers occur first in \nthe collapsed list so that syntactic symbols have a known index in the list, (See step 7 of figure 1. \n) An algorithm similar to an associ\u00adative memory is used to find all of the different tokens. During \nthis process additional information about the positions of a token relative to all other occurrences \nof the same token might be recorded. Such information could be useful in later optimiza\u00ad tion passes \nof the compiler. Source representation of program segment Step 1.. . Sequencing information is removed \n[11 R1 BUWIDTH HALF ; BLW.ER CHAIR[ 1201 nSOkE COMMENT and separate lines are concatenated [21 BUFWIDTH \n+ 2. 54x PBUFFER + BEGIN , LINE2 , END. into one long source string. Step 2.. . Comments are located \nby finding o o ] BUFWIDTH HALF ; BUFFER CHAR[1201 FISOME COMMEN not followed by ], and forming a ooooooooooooooooooooooooooooooooooooooolllllllllllll \n%?) 1 mask from that point to an end of BUFWIDTH 4 2.54x PBUFFER + BEGIN , LINE2, END. line marker. \noooooooooooooooooooooooooooooooooooooooooooooooooooo Q 2 Step 3.. . Character strings are masked out \nII] BUF W.TDTH HALF ; BUFFER CE4R[120] BVFWIDTH + 2.54x and put into a special buffer. 00000000000000000000000000000000000000 \nR 00000000000000000 PBUFFER 4 BEGIN , LINE2, END.T STRINGS: 0000000000111111110000000001111110 Q BEGIN \nkEND. # Step 4.. . Constants are located, extracted, n] BUFWID2 H HALF ; BUFFER CHAR[1201 L BUFWIDTH \n+ 2.54x converted and replaced by a 00000000000000000000000000000000011100 $3 00000000000111100 pointer \nto their value. PBVFFER + S1, LINE2, s ~ ~ CONSTANTS: 120 2.54 00000000000 0000000000 Q0 Step 5.. . All \nalphanumeric tokens are extracted n] BUFWIDTH HALF ; BUFFER CHAR[C 1 BUFWIDTH + C x p replaced by a marker \nand stored for 0000111111110111100011111100111100 100Q lIIIIIIIOOOO?IOO further processing. BUFFER+ S \n, LINE2, S8 z 111111000010011111000 D ALPHANUMERIC:BUFWIDTH HALF BUFFER CHAR BUFWID2 H BUFFE LINE2 Step \n6.. . Blanks are compressed out. o]AA;AA[C1~C2xP A+S1,A,S8 z b Step 7.. . Finally, the alphanumeric buffer \nis dA7A3;A8A2[Cl ~+C2XPA8+S1,A9,S8 matched against the list of known @ Q keywords and itself to come \nup with STRINGS: BEGIN# END.# CONSTANTS: 120 2.54 unique indexes to replace in the ALPHANUMERIC: DISCARDED \nsource. UNK2UE WORD LIST: BIT CHAR HALF FULL EXT GLOBAL BUFWIDTH BUFFER LINE2 ***************************Ark******** \nLEXICAL AIVALYSLS Figure 1 *******~***~***************~**~**** . .. .... .. . . .. SYNTACTIC ANALYSIS \n The process of parsing a program involves numeric token. recognizing a syntactic gruupofsyrnbols, replacing \nBLock structuring ofALGOL compounds the it by a singie representative symbol, and invoking parsing probIems. \nThe context of a construct can the appropriate semantic action. One approach to encompass hundreds of \nlines of source program. parallel parsing might be informally described as Classification of statement \ntypes in ALGOL-68 recognizing all occurrences (or as many as pos-provides an extreme examDle of this \nsort ofprnb\u00adsible) of a given syntactic construct. This is done lem. A left-parenthesis (open-symbol) \ncan begin by detecting special symboIs or context which signal a closed (serial) clause, a conditional \nclause, a a unique construct. For ad-hoc parsers, this proc-case clause, or a collateral clause. The \ncorrect ess is very much dependent on characteristics of classification cannot be made until the other \ncon\u00adthe source language. In FORTRAN, for example, trol punctuation (semicolon, cOmma, periOd, Or there \nare no reserved words so the presence of a vertical bar) of the clause is located. If all keyword at \nthe beginning ofa statement cannotbe of the open-symbol, close-symbol pairs of the used to classify statement \ntypes. The keyword rnaY program have been correctly matched, this is not well be a variable name. The \nproblem is compli-a difficult decision. The problem becomes a serial cated further if blanks are not \nsignificant delimiters. nightmare, however, if the open-close pairs of the The lexical analysis may have \nrecorded a combi-source program contain errors. Matching block nation of keywords and identifiers as \na single alpha-Open and close symbols is not the only kind Of problem encountered in a block structured \nlanguage. The nesting of declarations also presents a context problem. It is not difficult to find all \nof the decla\u00adrations and applied occurrences of a given identifier, but the problem of matching the applied \noccurrences with the correct defining occurrences without re\u00adsorting to serial processing is non-trivial. \nAPL-STAR is not free of these problems. There is no block-structuring to worry about, but the parentheses \nand square brackets must still be correctly balanced on a statement by statement basis. The keywords \nused in the declarations are not reserved, so additional care must be taken to separate them from variables. \nSince the declara\u00adtions do not share any significant amount of syntax with the executable statements, \nthey are handled separately, as described below. All declarations start with a special symbol pair which \nappears to be a comment to the interpreter, so the unique context necessary to split the declarations \nfrom the remainder of the program is immediately avail\u00ad able. Parsing of the executable statements is \ndiscussed later. Analysis of APL-STAR Declarations and Symbol Table Organization Specific details of \nparsing APL-STAR declara\u00adtions are, for the most part, unique to the APL-STAR syntax. However the analysis \nof the declara\u00adtions is illustrative of the processing of recursively defined lists of notions which \noccur in many lan\u00adguages (e. g. REAL X, Y, Z from ALGOL and FORTRAN) and also of the factoring of attributes \n(very similar to that of PL/1: PL/1. . . DECLARE ((A FIXED, B FLOAT )STATIC, C CONTROLLED) EXTERNAL APL-STAR \n. . . ~ ] ((A [so] ; ~[MAXBUFX81 ~cHAR; C HAL? ) GLOBAL ). The steps below describe how the declared \nnames are identified, and the attributes are col\u00adlected. With the analysis of the declaration syntax \ncomplete, the information is organized into a symbol table form for use in processing the ex\u00adecutable \nstatements. Figure 2 illustrates these steps for a small example. Analysis of Declarative Syntax step \n1. (Lexical ana~ysis is complete and declara\u00ad tions have been separated from executable statements. ) \nAll of the separate declar\u00ad ative statements are put together as one long statement by replacing the \ndeclaration symbol-pair ( ]) with ; . step 2. Parenthesis anubracket struct,~~e is cnecked to make sure \nthat they are properly matched. This is very important for all of the following steps. step 3. The declared \nidentifiers are located by finding the first non-( past each semi\u00ad colon. These identifiers are removed \nand form the basis for the symbol table. step 4. Constant expressions must be removed because they may \ncontain identifier names which conflict with attribute names. (There are no reserved keywords). The expres\u00ad \nsions also may contain parenthesis struc\u00ad ture which should be removed before the succeeding steps. These \nexpressions are stored in a separate vector for later proc\u00ad essing and the positions are marked by a \npainter. lAPL-STAR DECLARATIONS APL-STAR identifiers are required to be de\u00adclared exactly once. Each \ndeclarative statement has the form: <declaration list> ::= <declaration I -declaration>; <declaration \nlist> <declaration> ::= <identifier> <attributes> ( -declaration list> ) < attributes> where <attributes> \nis a possible empty list defining type, size, and scope of the identifier. Types are bit, character (8-bits), \nhalfword (32-bits) and fullword (64-bits), constant value, or function. The size attribute specifies \neither scalar or vector ( [ constant express ion,] ). The scope attributes specify local, global (known \nto other APL-STAR functions), or external (known to all programs which are resolved by the loader). Additional \nattributes allow the user to exercise control over the memory allocation of his variables, since it may \nbe im\u00adportant that some variables do not cross page boundaries. The default attributes are fullword local \nscalar. Source form of declarative statement Token representation after lexical anaylsis: Step 3.. . \nlocation of declared names: Step 4.. . names are removed and expressions are located: Step 5.. . expressions \nare removed and program profile is built: Step 6.. . Par enthesis information: Step 7.. . The attributes \nare located and their profile numbers recorded. Step 8.. . START and STOP vectors describing attributes \nare built: Attributes START STOP [El 11 [E2 22 A2 (CHAR) 1 2 A3 (HALF) 3 3 ( GLOBAL) 1 3 6 Attributes \nrecorded in symbol table and defaults resolved: n] (( A[801 ; B [MAXBUFX81 ) CHAR ; C HALF ) GLOBAL ~1 \n((A7[C1];A8[A9X C21)A2;A10A3)A6 CONSTANTS: 80 8 UNIQUE WORD LIST: BIT CHAR HALF FULL EXT GLOBAL A B MAXBUF \nC ;((A7[C1];A8[A9 XC2])A2;A10A3)A6 . ..t .t -t DECLARED NAMES: A 7A ~A ~0 (A, B, and C in UNIQUE WORD \nLIST) :(([CI]:[A9 xC21)A2;A3)A6 ~-$f--t EXPRESSIONS: C ~$A9x C2$ ;(([E1;[EZ)A2;A3)A6 01111 122 22 23 \n33 . . . running sum of semicolons 01222222 11 11 00 . . . parenthesis depth RPAREN +2.1 3.0 . . . profile \nnumbers of right parentheses LPAREN + 1 1 . . . semicolon count of corresponding left parentheses. ;(([E1;[E2)A2:A3)A6 \nI rttr . . . attribute locations ATTR+l.2 2.2 2,1 3.1 3.0 (Profile numbers from step 5) MATCH +(.1.2 \n2.2 2.1 3.1 3.01t(2.l 3.0) =33132 . . . attributes compared to right parentheses STOP +1.2 2.2 2.1 3.1 \n3.0 12233 . . . attribute stops START +(1 10)[3 3132] +(1 2233)X(3 3132)>p2 00101 + -attribute tarts \n factored starts non-factored starts . 12131 IDENTIFIERS A7 A AIO 8 BIT 000 CHAR 110 HALF 001 FULL \n000 EXT 000 GLOBAL 111 LOCAL 000 VECTOR 110 (Sizes : El E2) SCALAR 001 UNIQUE WORD LIST : BIT CHAR HALF \nFULL EXT GLOBAL A B MAXBUF C CONSTANTS 80 8 EXPRESSIONS C1$A9 x C2$ of DECLARATION SYNTAX Figure 2 63 \nANALYSIS ing tokens. These numbers are a composite number built from the running sum of the semicolon \ncount and the parenthesis depth count of each item. step 6. The parenthesis structure is recorded. \nTwo vectors are built. RPAREN is a vector of the profile numbers of all the right parentheses. LPAREN \nis a vector of the semicolon count numbers of the left parentheses. This vector is reordered so that \nthe entries correspond to the entries in R PAR EN. (i. e. The open parentheses are matched with the closed \nparentheses. ) step 7. The attributes in the declarative string are located. All of the alphanumeric \ntokens remaining in the string are attributes. There are also a few special symbols such as [ which mark \nthe presense of attributes. The vector ATTR is a vector of the profile numbers of these attributes. A \nvector of semicolon counts for the attributes is also needed for step 8, but this is easily found by \ntaking the floor (integer part) of the ATTR vector. step 8. An attribute is factored if and only if \nits profile number matches the profile number of a closing parenthesis~ If the attribute is not factored, \nthen its semicolon count tells which entry it applies to. If the attribute is factored, the semicolon \ncount of the open\u00ad parenthesis which corresponds to the matched closing parenthesis tells where the factoring \nstarts. The semicolon count of the attribute (or the closing parenthesis) tells where the factoring stops. \nUsing the vectors constructed in steps 6 and 7, the following APL state\u00ad 1 ments produce the desired \nvalues. 7 ~lY yields the index of the first occurrence of each element of the X vector in the Y vector. \nIf the element does not occur, the corresponding element of the re\u00adsult is one greater than the length \nof the Y vector. LX truncates X to integer. PX yields the length of the vector X. MATCH -+ ATTR \\ ??P.4R?7!! \nSTOP + 1-ATTR START + ( LPAREN ,0 ) [MATCH 1+STOPXMATCH>pLPAREN ~~ factored non-factored attributes attributes \nAt this point all of the useful information in the declarative statements has been recorded and the \nsymbol table can be constructed. Using the START and STOP vectors, bit vectors are built for each attribute. \nDefault attributes can be recorded by complementing the bit vectors for the declared attributes (e. g. \nLOCAL = GLOBAL VEXT). Sim \u00adilarly conflicting attributes can be detected by in\u00ad tersecting bit vectors \nwhich should be unique (e. g. CHAR A BIT). In addition to the declared attribute informa\u00adtion, the symbol \ntables must provide addressing information. The constant expressions extracted in step 4 above are evaluated \nso that the size of the vectors is known. The relative addresses of all the local variables can then \neasily be assigned. Syntactic Analysis of Executable Statements Best results can be expected from parallel \nprocessing when the data can be processed in a uniform manner. On the surface, the executable statements \nof an APL-STAR program fit this re \u00ad quirement nicely. There are no keywords to process and no special \nstatement types to recog \u00ad nize. Each statement is an expression to be eval\u00aduated from right to left \nwith no operator precedence involved. Despite these advantages, there are still many details to be resolved \nbefore analysis for code generation can begin. The infix state\u00adments are revised and reordered until \nthey consist entirely of operands and dyadic operators. This infix is then transformed into inverted \npostfix Polish form (X op Y becomes YX op) before further semantic analysis precedes. The steps involved \nin reordering the infix are listed below, followed by comments on the Polish analysis. Source representation \nof two statements XIII+ (( B* C)-D)+.E to be processed: W+(p(-x) + Y)<z Step 3.. , Brackets are replaced \nby sub-(XII)+((B*C)-D)+E L W-(P(-X)+Y)<Z L script operator ( [ ). Step 5.. .Null operands (a ) are inserted \n(X[l)+((f?*C)-~)tE LW(ctP(a-X)+Y)<Z L for monadic operators. Step 6... Parenthesis depth for each (XII)-+((B*C)-D)+E \nI-/V+(aP(a-X)+Y)<Z L operator is found and then 10 210 01210 parentheses are dropped. X [1 1+0 B *2 \nC -1 D +OE L-lW+O Qpf ry,-2 X+l Y <0 Z L -1 Step 7... Operands and operators operand list , . . XIBCDEVactXYZ \n[+*-+L+O-+<L operator list . . . are separated. 10210 -101210-1 .!,.,..,,.!..,,.,>.,..!,.,..!,.,,.!,>!,.,,.!,.!,.,,.!,.,..!,...!,.!,...!..!,......!.!..!..!.4..$.!......!..!,......,-.......!..!....!..!..,-......!..!....,,....,,:,:>:>:>~,~>:,~>:* \n>~>~::,** >:>*>:* :.,x* >:>:>:>*:?.::>!>::>::+>!>:>:* .,... .. ............. .. .. ,,.,.,,.,,..,,.......,........ \n.. ..,,._,,... >..,.,,....... ..,,... ,,.,,... .. .... .. .. .. ............. ,,... .. .,..... , PREPARATION \nOF I.NFIX FORM Figure 3a **:k:l<,k>f: >*** *>:::,~::<:~>~>k>k* *>x:k>g*:$ :y>k>t>~:~*>~,~**>~:~>~* >~*:; \n>~*:~>~*,~>:::g,~,~>:::~>~*:~,k:F:z:x:k**:? :k*:k:%:%:F:t:x:?* :?>ti*:?:t:z,ti :ti:*:t *:k:$:*:$:x:ti>%:k:t:k \n(Use operand list and operator list from step 7 above.) Generation steps Polish generated 1. Consider \nE,L-l and z,L -1 2a. Move. 2b. Consider D,+ and Y,< E -l z -1 o 0 2b. Consider C,-l and X,+ +L <L 1 \nf)-1 y o -1 2a. Move. ED 2a. Move. 2b. Consider B,*2 and a,- EDC ZYX + < L-l -l+oL. 1 2 10 2a. Move. \n2b. Consider r,+ and ~,P *-+.L EDCB zYxa o 1 210-1 -2+I<OL-1 2c. Push both operator stacks. zYxa-2+1 \n<L Dc~*2-l+o -1 o -1 2a. Move. 2b. Consider x,[ and w,+ EDCB*2-1+01 + L zYxa-2+1!2 p<L 1 o o-1 10-1 2c. \nPush second operator stack. EZICB*2-1+01 + o L _l zYxa-2+1.upl<o -1 2a. Move, 2b. Drop both pointers. \nEDCB*2-l+OIX[l+0 L-1 ZYXa-t u(I< W+ L 21 10 0-1 Polish is complete. EDCB* - t-IX[ L ZYXct-+ ap < W L \n :F:*:k:%:k:K:k:%*:k:x*:k :8:** :x~?:?*:k:k:?:k:t:K::::x::::t:x:k:?:k:x:i:%:%:k:ti**:k :k*:k:t* :?:k:?:*>t:%*>?:x:k:k:x:x:x \n:8:?:k:%ok:*:*:::*:x:k>t**:$* >?:;::k*:K:k:$:8:$:*:F:x:k:k:k:F:zPOLISH Generation (Serial Adaptation) \nFigure 3b :*:!:x:!.::* *:!:!:<::> 6:k*::<:::Z* ~k:::::k:k:!.:k:::k:?::::<:%ix-+:k,X::>!::::*::,x,::k:::x,:>Z* \n::* >:>:::>:::>:>K>X* >:>:>K>*>*>:::>:>*>K::>*>** >X,::k>k-+>:::::>!-::>::::k::>:>k:k::::>k>k:: . .-- \n.. . -..-. .-. . . ... -.- . -.. Preparation of Infix Form a descriptor marking their length. step 3. \nSubscript brackets may be considered as step 1. (The statements have been through the lex\u00ad a dyadic operator \nwhich has higher priority ical analysis and all of the identifiers have than the surrounding operators. \nThis is been matched with symbol table entries. ) handled by inserting parentheses to force Labels are \neliminated from the source and the proper order of evaluation e.g. XII] all references to labels are \nreplaced by becomes (X sub I ). Subscripted operators constants representing the line numbers are handled \nin a similar fashion. where they were defined. step 4. It is not always possible to determine from step \n2. Constant vectors are extracted from the context whether a function is monadic or source and placed \nin the constant list with Step 1.. . Select end-of-line markers in the operator list and move them to \n:,:o:,:,:ogo;l:,:,:o~ Polish with their corresponding operands. Polish . . . *:Z .+ Step 2.. . Select \nall of the operators on level O. ;,~:,:lg.i%~l Step 3.. . Find lower priority operator to the right (-1) \nand locate corre-~~:. ~&#38;:L sponding operand in Polish. Step 4.. . Insert pairs selected in 2 into \nthe locations found in 3 in reverse order. Step 2.. . Select all of operators on level 1. Step 3.. . \nFind lower priority operator to g@-ioi 2&#38;, -; the right (0) and locate corre\u00adsponding operand in \nPolish. yJ ~ ~%-~;~ Step 4.. . Insert pairs selected in 2 into EDC-l to I X[l +0 L-l ZYX+l apl <0W+.L_l \nlocations found in 3 in reverse order, / Step 2.. . Select all of operators on level 2. ,i:o&#38;;:o:.,:o:,g~ \n:o:l Step 3.. . Find level 1 operators to right and locate operand in Polish. J Step 4.. . Insert the \npairs into the selected J \\ J@c ~EDCB* places. z 1 o ~ X[l+o L-lz Y X a\u00ad 2 1 PI o niladic. FUNG -X \ncan be parsed either way. end-of-line markers are considered to be Since functions are declared, specifying \nthe Operators Of the lowest priority. number and type of the arguments and result> step 7. The infix \nform now consists operands and parentheses can be placed around the func-operators. These are split into \ntwo sep\u00adtion name and the proper number of argu-arate vectors for subsequent reordering ments to resolve \nthis question. during Polish analysis. step 5. All monadic operators are made to look Two different approaches \nfor generating Polish dyadic by inserting a null operand on their are presented. The first described \nin an adapta\u00adleft. tion of the serial method of Polish generation. step 6. The parenthesis pairs which \nspecify order The algorithm is carefully set up, so that Polish of evaluation are removed from the infix \nsequences for all statements are processed from by introducing precedence to the operators right to left \nin a synchronized fashion which loops based on their parenthesis depth. The through the generation process \nas many times as there are operators in the longest statement. The ing them into the correct position \nin the Polish. second approach outlined later loops only as many Minor variations in the scheme will \nproduce times as the deepest set of parentheses in the regular postfix Polish as well as prefix forms. \nsource program. On each pass all of the operators The algorithm works off the operator and operand on \nthat parentheses level are handled. lists produced after processing the infix as de\u00ad scribed above, so \nthe successive parenthesisAdaptation of Serial Polish Generation levels are represented by the precedence \nnumbers The Polish created for each statement will of the operators. Figure 3C illustrates the work\u00adhave \nthe same length as the infix representation. ing of the fol!owing steps:While the Polish form is built \nup at the left end step 1. Select all of the end-of-line markers in the of each statement, the right \nend can be used as opero operator list. Copy the correspondingthe operator stack for the statement. The \ntwo operand followed by the end-of-line marker stacks meet and fill the statement exactly when to form \nthe Polish basis. (Recall that the the Polish for that statement is complete. An end-of-line markers \nare represented as the example of the process is followed in figure 3b. lowest priority operators. ) \nstep I. Start by considering each end-of-line step 2. If all of the operators have been processed,marker \nin the operator list (produced by the Polish is compIete. Otherwise select step 7 above). (Recall these \nare the low \u00adall of the operators in the operator list of est priority operators). the next higher precedence \nlevel. step 2. Repeat the following steps until there are step 3. For each selected operator, find the \nfirst no more entries to be considered: operator of Lower priority to the right. (2a) Move the considered \noperators to the tops Locate the current position in the Po~ish of the corresponding operator stacks. \nof the operand associated with this lower Move the corresponding operands to the priority operator. This \nis the Polish lo\u00ad tops of the Polish stacks. cation for the selected operator. (2b) Decrement the pointers \nin the operator list step 4. Insert the operands, corresponding to each by one. Drop any of the pointers \nif the new selected operator, followed by the selected operatOr considered is an end-of.lke marker operators \ninto the Polish in the chosen lo\u00ad (the Polish for that statement is completed). cations. If more than \none pair of operand\u00ad (2c) Compare the precedence of the considered operators is to move to the same location, \noperators with those on the tops of the cor \u00adthey are copied in the reverse order of their responding \noperator stacks. Move any appearance in the operator list. Go to step Z. operators from the operator \nstacks to the The necessity for the reversal in step 4 comes Polish whose precedences are less than or \nfrom the fact that the APL statement is being equal to that of the considered operators. turned around \nso that it can be processed from Polish Generation by Parenthesis Level left to right (hence the inverted \npostfix Polish). The required inverted postfix Polish for the This reversal could be eliminated from \nthe Polish statements can be built up by moving all of the Loop if the preprocessing effort were made \nto re\u00ad operands and operators which are not inside pa\u00adverse all the operands and operators within state\u00ad \nrentheses to the Polish (in inverted order). The ments. The parallel scheme described does ap\u00ad algorithm \nloops through each successive paren\u00adpear to exploit the parallel processing idea more thesis level, selecting \nall operators and their than the serial scheme, but at the time of this corresponding operands on that \nlevel, and insert\u00ad writing ii has not been coded in APL, so it is not known if a simple APL representation \ncan be found to perform the rather complex operations described. SEMANTIC ANALYSIS AND CODE GENERATION \nThe APL-STAR operator set and the STAR in\u00adstruction set are both very complex, requiring an intricate \nprocess to find the correct STAR repre\u00adsentation for an APL-STAR operator. This process is only outlined \nbriefly here. A set of code skele\u00adtons are defined which map the APL-STAR instruc\u00adtion set into actual \nSTAR instructions. Before these are applied, constant expressions are iden\u00ad tified and evaluated and \nspecial operator pairs such as +/ are located and replaced by single operators. Finally the operand types \nof each operator are analyzed so that the correct code skeletons can be applied. The expression is analyzed \nfor all operators for which the required operands are available. This defines a level of the expression \ntree. The compila\u00ad tion precedes iteratively through tree levels. At each level the operators which have \ntheir operands available are selected by comparing the Polish positions of all operators with the positions \nof the previous operators. Where there are two or more operands between operators, the right operator \ncan be processed. Figure 4 gives a simple example of this selection. The triples selected from the Polish \nare reduced to single entries with the re\u00adcorded result types and the process is repeated for the next \nlevel of the tree. Polish DF/BA+E-t C+ position 1234567891011 operator positions . . . 368911 previous \noperator . . . 03689 positions (shift left by one) distance between . . . 33212 operators (only the fir \nst two have the operands available. ) OPERATOR SELECTION Figure 4 With the large number of operators \nin APL and the variety of data types in APL-STAR, it is not feasible to provide code skeletons for every \npossible combination of operand types with every operator. About 300 code skeletons in total are defined. \nAppropriate coercion operators are se-Iected where necessary to make the operand types match defined \ncode skeletons. Where possible the replacement and subscripting operators are com\u00adbined with other operators \nto avoid unnecessary temporary results. The actual structure of the code skeletons is complex and will \nnot be defined in detail in this paper. The STAR instruction formats are not simple. For some vector \noperations, there are seven fields in addition to the op-code to be fiiled in. The code skeletons, some \nof which describe many STAR instructions, are organized such that these STAR instructions fields can \nall be treated uniformly at the same time. The entries for each field are coded to indicate what operand \nis to be used, whether the registers involved are fullword, halfword, or double-word pairs. The entries \nare further coded to indicate any of the following possibilities: a. Issue code to fetch a variable of \ndescriptor described by a given symbol table entry. b. Issue code to load a given constant value into \na register. c. Allocate a result register. d. Allocate result space for a temporary vec\u00adtor result \nand loade the descriptor into a register. e. Use a value directly in the instruction field (e. g. the \nop-code). f. Copy the entry from a previously defined field (necessary to match up registers for temporary \nresults).  Figure 5 shows the encoding for a simple single\u00ad instruction skeleton, After all of the \ninstruction fields, have been processed, the resulting code is packed into the instruction format and \nput into the correct order for output tc) the loader. BA+ ~ 62 332 331 :::is:. . code ,keleton use fullword \nregisters issue fetch code w allocate result register Code Generated: 3E, x , address of A 7E, base, \nx, 31 . . . value of A to register 31 3E ,x, address of B 7E, base ,x,32 . . . value of B to register \n32 62 ,31, 32,41 . . . sum to register 41 CODE SKELETON DECODING Figure 5 CONCLUSIONS It is difficult \nto give a final assessment of the success of the APL-STAR compilation experiment. Final judgment will \nhave to wait for actual imple\u00ad mentation on the STAR. There are, however, a few comments that can be \nmade about the overall approach. Use of Vectors for Compilation: A traditional one-pass compiler handles \nthe source of the program one symbol at a time, per\u00ad forming a wide range of compilation functions to \ncompletely process the symbol. The process described in this paper is completely opposite in nature. \nA wide window of the input source is examined while the code execution is held to a narrow locality. \nThere are tradeoffs to be con\u00ad sidered in these two approaches. The speed-up expected with vector instructions \non the STAR is unknown but expected to be on the order of 16 to 1 over register instructions, assuming \nthe vectors are sufficiently long. (Specific timing rates are dependent on the operator, types of the \noperands, and memory conflicts encountered. ) This speed\u00ad up comes at a definite cost. Some of the tokens \nare accessed many more times than would be the case in serial compilation. It is difficult to know where \nthe break point between a complex parallel algorithm and a looping serial algorithm occurs. The actual \nlength of the vectors used in the compiler will be a critical factor in the perfor\u00admance of the compiler. \nIt is clear that the com\u00adpiler cannot expect to handle the entire source of all programs submitted to \nit, so it is designed to break the program into pieces if necessary. If these pieces are too small, many \nof the vectors may be too short, especially those which process semantic end-cases and error checking. \nWith such vectors the overhead to set up the descriptors and the start-up time of the vector instruction \nmay dominate any advantage gained by handling the elements in parallel. On the other hand, if the program \npieces are large, the vectors may be\u00adcome too long, creating another set of problems. The storage required \nfor temporary vector results may become costly. If bit vectors are used to extract elements from a vector, \nthe entire vector passes through the stream unit from memory, even if only a few elements are selected. \nAdditional care must be taken with long vectors to ensure that they do not cause repeated page faults, \nwhich would completely destroy the efficiency of the compiler. The best size of pieces to handle will \nhave to be determined experimentally. APL and Compilation: The facilities of the APL interpreter provided \nan effectual tool for development and debugging of the compiler. However, as the project progressed, \nproblems with APL as a compiler writing tool and as a natural language for the STAR became more apparent. \nPacking and unpacking bit fields is a common compiler operation which is very costly in APL. The only \nway to shift a number is to divide or multiply by powers of two. APL maintains com\u00adplete control over \nthe data types of the variables. This lead to some debugging surprises when a carefully constructed bit \npattern would suddenly be converted to an unusable floating point repre\u00adsentation during a simple shift \n(division) operation. As the code skeletons for the APL-STAR operators were developed it also became \napparent that maintaining compatibility with the APL inter\u00adpreter would be more costly than expected. \nCorn. plex code sequences had to be developed to handle all of the special cases of some operators which \nhad very simple counterparts in the STAR instruc\u00adtion set. The take operator ( +) is a good example of \nthis. If the left operand is positive, the elements are selected from the beginning of the right oper. \nand. If negative, the end of the right operand vector is used. In addition the result vector must be \nextended with zeros if the magnitude of the left operand is greater than the length of the vector on \nthe right. The STAR instruction sequences for the different cases are simple, but the code skeleton for \nthe take operator must emit all four different possibilities, as well as the run-time check to select \nthe right one. In this case it would be ad\u00advantageous to define different operators, especially for the \npositive-negative cases. In other cases, it was discovered that APL had no operator to describe very \npowerful STAR in\u00adstructions. APL provides the compress operator (/) to extract selected elements from \na vector under control of a bit vector, but there is no in \u00adverse for this operation. In order to store \na sequence of elements into positions selected by a bit vector (an important STAR instruction) a com\u00adplex \nAPL sequence is required: A [ BITVECTOR / 1p A ]+C , APL-STAR would be a more effective tool for the \nSTAR if the interpreter compatibility require\u00adment were dropped, or alternatively, if the inter\u00ad preter \ncould be modified to reflect the STAR in\u00adstruction repertoire. Parallel Processing Algorithms A number \nof unique algorithms for processing a Program in Parallel were developed for the APL-STAR compiler, some \nof which have been described. Others have been. omitted because of the complexity of an illustrative \nexample. Doubtless these algo\u00adrithms can be improved and many new variations can be invented. Many of \nthese algorithms have been developed in an ad-hoc fashion. There is much work to be done in formalizing \nsuch tech\u00ad niques (especially parsing and register allocation) and proving that the formalized technique \nis the best, or even proving that it works in all cases. The features of language design (e. g. reserved \nwords, significant blanks, minimal recursive syn\u00adtactic constructs) which facilitate vector processing \nand minimize semantic end-cases should be char\u00adacterized. The vector approach to compilation also offers \ninteresting new possibilities for global optimization algorithms. ACKNOWLEDGMENTS The design and implementation \neffort of the APL-STAR compiler is the work of Ned Dairiki, John Engle, Jeanne Martin, C. A. Wilgus, \nand project leader Richard Zwakenberg, in addition to the author. This work was performed under the auspices \nof the United States Atomic Energy Commission. References 1. F. H. McMahon, L. J. Sloan, G. A. Long, \nSTACKLIB: A Vector Function Library of Optimum Stack-Loops for the CDC 76oo, (to be published), Lawrence \nLivermore Laboratory, University of California, Livermore, Ca. , 1973. 2. R. G. Zwakenberg, APL>* Language \nSpecification and Compiler Progress, UC ID-30052, Lawrence Livermore Laboratory, University of California \nLivermore, Ca. , 1972.  3. Control Data STAR-100 Computer System, Publication number 60256oo, Control \nData Corporation, Arden Hills, Minnesota. 4. K. E. Iverson, A Programming Language, John Wiley &#38; \nSons, Inc. , New York, 1962. 5. R. G. Zwakenberg, et. al. , APL Interpreter, UC ID-30024, Lawrence Livermore \nLaboratory, University of California, Livermore, CaO , 1971. 6. C. A. Wilgus, et. al. , Preliminary \nUser!s Manual for the LRL-APL, UCID-30026, Lawrence Livermore Laboratory, University of California, Liver \nmore, Ca. , 1971.  7, N. Lincoln, Parallel Programming Techniques for Compilers, SIGPLAN Notices, Vol. \n5, No. 10, Oct. 1970. \n\t\t\t", "proc_id": "512927", "abstract": "", "authors": [{"name": "Mary Zosel", "author_profile_id": "81100025564", "affiliation": "Lawrence Livermore Laboratory", "person_id": "PP14021780", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512927.512933", "year": "1973", "article_id": "512933", "conference": "POPL", "title": "A parallel approach to compilation", "url": "http://dl.acm.org/citation.cfm?id=512933"}