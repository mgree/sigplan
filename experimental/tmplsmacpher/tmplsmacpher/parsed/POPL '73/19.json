{"article_publication_date": "10-01-1973", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1973 ACM 0-12345-678-9 $5.00 TRANSITIONS IN EXTENDIBLE ARRAYS* Arnold L. Rosenberg Mathematical Sciences \nDepartment IBM Watson Research Center Yorktown Heighta, New York ARSTRACT Arrays are among the best understood \nand most widely used data structures. Yet even now, there are no satisfactory techniques for handling \nalgor ithme involving extendible arrays (where, e.g. , rows and/or columns can be added dynamically). \nIn this paper, the problem of allocating storage for extendible arrays is examined in the light of our \nearlier work on data graphs and addressing schemes. A formal analog of the assertion that simplicity \nof array extension precludes simplicity of trans\u00adition (marching along rows/columns) is proved. 1. INTRODUCTION \nFew classes of data structures are as well understood or as widely used as are arrays. The most primitive \nof high level languages offer some array processing facilities; certain languages such as AFL have been \ndesigned with arrays as the basic data structure. One striking deficiency in current techniques for handling \narrays is the poor handling of extendible arrays --arrays whose sizes in vari\u00adous dimensions may change \nin the course of execut\u00ading an algorithm. Consider, for instance, a situ\u00adation in which a two-dimensional \narray is stored by rows, Should one wish to add a new row to this array, one could do so with no material \nchange to the functions which access elements of the array or which are used to traverse rows or columns \nof the array. In contrast, adding a column to this array would create two undesirable options: One could \nstore the column willy-nilly and thereby destroy the simplicity of the element-access and transition \nfunctions. Or, one could opt to pre\u00adserve the simplicity of these functions at the coat of reallocating \nstorage for the whole array; only elements of the first row of the old array would escape being moved. \n(Note that we are ignoring all problems that accrue because of other material stored near our array.) \nIs such asyrmnetry in the handling of rows and columns inevitable? Do there exist schemes for allocating \nstorage for arrays which are at once easily extendible and easily accessible and traversable? Of course \na formal framework is required before one can even state these questions with precision. In Section 2, \nwe discuss the problem of find\u00ading a model for arrays which is suitable for study\u00ading problems of implementation \nand extendibility. We settle on two models, each of which is approp\u00ad riate in certain contexts: The _ \narray, which is oriented towards languages such as FORTRAN and PL/1, models a situation where no attempt \nto add both rows and columns dynamically to an array is 1 the ~rth~t array, which is compatible envisaged; \nwith the APL view of arrays, models situations where no constraints on dynamic extensions to an array \nare assumed. Section 3 is devoted to study\u00ad ing array realizations in which row and/or column transitions \nare very efficient, being realized by additive displacements. Using the auxiliary notion of a shift chain, \nsharp distinctions between the . prism and orthant models are demonstrated. Specifi\u00adcally we prove in \na sense made precise in the text that the goals of easy extension of arrays (adding rows or columns at \nwill) and of efficient transition along both rows and columns are incompatible. How\u00adever, we demonstrate \nalso that these goals can almost be attained simultaneously at the cost of inefficient storage utilization. \nThe material in this paper is developed more fully in the first half of [3]. 2. BASIC NOTIONS A. Arrays \nand Their Realizations A definition of array suitable for studying array implementation should (a) expose \nthe structure in array coordinates which facilitates implementation and (b) specify those moves in the \narray which are to be implemented efficiently. Let N denote the positive integers, and let Nn= {I,.. \n,n} for ncN. (2.1) A k-dimensional ~ (scheme) of size <nl, . . ..~> (k,nl, . . ..nkcN) is an ordered \npair A = (P,M) where (d ~ = NnlX N x---xN is the aet of 2 k positions of the array; (b) M is a finite \nset of -, each move ~ being a partial transformation lJ:p + P of the set of positions. 1 In this and \nsubsequent discussions we concentrate on two dimensional arrays for ease of exposition. The formal sections \nof the paper do not share this restriction. * This research was supported in part by ONR Contract NQO014-69-C-0023. \n218 Xi+l if i=j A = (P,M) ia a standard array i.f former being defined the latter only on only multiples \non multiples of 3. of Figure 2 and 2 illus- M = {uj, njljcNk} where for arbitrary trates schematically \nthe layout of storage under this realization. x = xl .\u00ad .,x >6P,k Our notion of realization suggests \nimmediately if xjcNn _l (Xlsj)i = if i+j I j i I ~ undefined otherwise. [x. -l if i=j) 1 J. if x d j \nnj-{l}i+j } (Xmj)i = i f ~ un~fined otherwise, a is the j _axis successor, j th the j sxie predecessor. \nIn this paper we study only standard arrays; however, our proof technique are sufficiently general to \novercome this restriction. Figure 1 portrays a standard array of size <3,4>, The symbology of (2.1) should \nbe materially clarified by thie picture. Our notion of realization or implementation of an array is \npurposely as general as possible. Our notion is geared to establishing a correspon\u00addence between posit \nione/moves in th~ array be~ng realized and positions/moves in an idealized infinite linear space of addresses. \nSee [1] for a discussion (in a more general setting) of the rationale behind our definition. (2.2) A \nrealization of the array A = (P, M) ia a pair of total, one-to-one functions R = <~,p> where (a) &#38;:P \n+ N is the storage ~; (b) P:M+ {partial transformations of N} is the access ~; subject to the conditions \n (c) peP ia in the domain of ueM precisely when (P@(!.lP)d f; (d) if pc domain(v), then (Pu)~= (P@(!-lP). \nThese conditions assert that transitions in the array can be effected within the realization and that \nany transition in the realization reflects one in the array. Definition (2.2) is easily illustrated by \nthe following realization of the array of Figure 1. i-13j-l(2.3) (a) For <i,j>cP, <i,,>; = 2 (b) Using \nChurch s familiar lambda notation for functions, u 1P = An[2n], mlp = An[n+2], U2Q = An[3n], n2p = An[n+3]. \nNote that ITlp and r2p are both nontotal, the three criteria for assessing the quality of a real\u00ad .. \nization, and the precealng example suggests a fourth: (a) Complexity of element access: the compu\u00adtational \ncomplexity of the storage map ~. (b) Complexity of transition: the computa\u00adtional complexity of the \ntransition functions MP = {!.IPIucM}. (c) Complexity of extension: the complexity of obtaining, from \na realization of an array A of size <nl, . . ..nk>. a real\u00ad  ization of a superarray of A of size <n \n,...where n~c{ni, ni+l} for 1 i each i (i.e., the extent to which ~ and p must be changed). (d) Inefficiency \nof Storage Utilization: the extent to which a storage map leaves gaps (i.e., is not onto a prefix Np \nof N). The main thrust of the investigation of which this paper is the first output is to determine tradeoffs \nbetween criterion (c) and the other three criteria. Here we concentrate on criterion (b) vs. criterion \n(c). In [3,4] we study all four crite~a as well as other concomitants of easy extension. We do not, at \nthis point, wish to pin our\u00adselves down to precise interpretations of (a) -(d). [This is at least partly \nbecause we feel that these notions do not admit absolute definitions; the pro\u00ad priety of a given notion \nof efficiency may depend on the intended computing environment.] Even at an intuitive level, however, \nwe can remark that realization (2.3) has the following characteristics: (a) Element access is not simple, \nsince ~ is exponential in i and j. (c) Extension is very simple, singe F and P needn t be changed at \nall.z v (d) Storage utilization is not efficient since only twelve of the 108 affected locations are \nused; i.e. , only 11% of N108 is used for storing the array.  (b) Transition is rather simple, since \neach step involves just one multiplication/ division. We contrast realization (2.3) with the follow\u00ading \nstore-by-row scheme. 2 Strictly speaking, the domain of ~ would have to be enlarged, but we ignore this \nformal point since the required extension of ~ is straight\u00adforward and requires no change to ~ on P. \n(2.5) (a) For <i,j>cP, <i,j>~ = 4(i-l)+j, (b) IJlp = An[n+4]; TIO = An[n-4]; n(02p) = n+l if n ZO (mod \n4) { undefined otherwise; n(n2p) = n-1 if n ~1 (mod 4) { undefined otherwise. Figure 3 illustrates schematically \nthe layout of storage under realization (2.5). As before, even an intuitive notion of simplicity suffices \nto evaluate this realization relative to the criteria of (2.4): (a) Element access is simple, since \n~ is linear in i and j. (b) Transition is simple, since each step involves a single addition/subtraction \n(plus, possibly, a test for a residue class modulo 4). (c) Ease of extension is more complicated to \nevaluate. Adding a row is very simple,  equiring 0 change n k r p dd\u00ading a column is rather difficult \nsince II ~~ ~~ ~~anged (in fact, o*ly remain fixed) and so,, must p. (d) Storage utilization is very \nefficient, the 12 cells of the array being assigned to locations 1-12 for 100% utilization. IS it inevitable \nthat realization (2.3), which is easily extended, admit more complicated trans\u00aditions than the not easily \nextended realization (2.5)? We pave the way for answering this question. B. Two Models for Arrays We \nneed a model for arrays which allows us to discuss extendibility, Now, easy extension is epitomized by \na situation where the storage map for the extended array is just a (functional) extension of the storage \nmap of the original array. Therefore, easy extension along a certain direction is assured if one constructs \na paper realization not of the array in question but rather of a superarray which is infinite along the \ndesired direction. One then realizes the array in hand by a (functional) restric\u00adtion of the storage \nmap of the infinite array. Unbridled extendibility is thus embodied in an array scheme which is infinite \nin every direction, one whose positions are the lattice points of the positive orthant of k-space. These \ninfinite array schemes afford us the desired models of arrays? models which ensure simplicity of extension \nof an allocation scheme as we study simplicity of trans\u00adition. Realizations of infinite arrays are not \nunfam\u00adiliar to anyone who haa considered realizations of finite arrays, One would be hard put to specify \na realization of an array which waa not really the restriction of a realization of an infinite array \nscheme which had the given array as a prefix. For instance, realization (.2.3) of our aemple array of \nsize <3,4> is a restriction of a realization of the 2-dimensional orthant ~. (2.6) The k-dimensional \northant ~ (kcN) is the pair = (Pk,~) where ik (a) Pk = Nk = NxNx.--xN (k times); (b) ~= {uj,mjljENk}; \nfor x= <x ,.. .,x>ep and jcNk,  1 kk (Xuj)i = Xi+l if i=j if i+j{ i (xlTj)i = p xi-l xi if if i=j i#j \nif xjcN-{l] undefined otherwise. ia the axis j successor and T. is j 3 the axis j predecessor. Similarly, \nrealization (2.5) is a restriction of a realization of the following (2-dimensional 4-column) prism array, \nas can be seen in our comment about the ease of adding rows in that realization. (2.7) The 2-dimensional \nc-column ~rism array is the pair IIc = (P,M) where (a) P= NxN . c (b) M = {IJ1,1J2,~l,~2};  for ci,j>cP, \n<i.,j>u = <i+l,j>;1 <i, j>n = ~ J<i-l,j> if i>l ~ undefined otherwise; <i, j>u ~= \\<i,j+l> if j.sNc_l \n\\ undefined otherwise; <i,j>n ~= j<i,j-l> if jeNc-{l} ~undefined otherwise. IIc is obviously a prefix \nof which is bounded 2 in the second coordinate. By considering generally prefixes of which are bounded \nin k-1 coordin\u00ad ik ates, one obtains the family of prism arrays. (Hybrid arrays obtained by bounding \nfewer than k-1 coord\u00adinates can, of course, be defined; the reader can easily extend our results to such \nhybrids.) By viewing array realizations, henceforth, as realizations of infinite array schemes, we shall \nbe better able to recognize the type of extension which they admit gracefully. Conversely, by con\u00ad 220 \nstrutting realizations for an infinite array scheme which models the kind of array extensions one wishes \nto facilitate, one can attempt to optimize our other quality criteria, all the while being assured of \nease of extension. 3. ALLOCATION SCHEMES WITH ADDITIVE TRANSITIONS Many computational procedures call \nfor repea\u00adted traversal of the axes (e.g., rows or columns) of arrays. We now consider how compatible \ncriter\u00adion (b), ease of transition, is with our main topic of study, criterion (c) , simplicity of extension. \nWe have, to this point, purposely avoided pin\u00adning down any notion of simplicity of transition. At this \npoint, we can no longer afford such aloof\u00ad ness. However, we shall restrict our attention in the main \nto two types of access maps which are so obviously efficient that we have no reluctance in labelling \nthem so, namely, access maps which assoc\u00ad iate with an axis successor a (possibly piecewise) 3 Relative \nto this rather additive displacement. strong notion of efficient transition, we uee the prism and orthant \nmodels to establish formal analogs of the following results. (I) No array realization which enjoye simple \ntransitions along both rows and columns admits simple ad junction of both rows and columns. (II) No \neasily extendible array realization affords simple transition along both rows and columns.  The negative \nimpact of these results is mitigated by exhibiting a readily extendible realization with relatively eimple \ntransitions (but, unfortunat\u00ad ely, with inefficient storage utilization). This last example will serve \nalso to hint at the diffi\u00adculty of establishing formal analogs of (I, 11) which are materially stronger \nthan ours. A. Shift Chains4 The development in this section is facilitated by a slight digreaaion into \na more abstract frsme\u00adwork. Specifically, we introduce the notion of a shift of a set and the related \nnotions of shift relationa and shift chains. . Let A be a transformation of a set S, i.e. , a (possibly \nnontotal) function from s into s. ~k For any nonnegative integer k, define by: 1\u00b0 = 1s (the identity \nfunction on S); and gener\u00ad~i+l ~i ally, = lli (the composition of with X), For arbitrary a,teS, we say \nthat a A-precedes k t -- written s<At --if sa = t for some kcN. 3 That is, successive steps along, \nsay, a row are effected by successively adding a given constant which, in the piecewise case, depends \non the row. 4 The material in this section presupposes some elementary concepts of modern algebra. The \nreader can consult a beginning text like 15] for back\u00adground. Clearly i is a transitive relation. (3.1) \nThe transformation A is a shift (of S) if the following three conditions hold: (a) k is injective (= \none-to-one); (b) ~ is cycle free --for no kcN doee ~k  have a fixed point; (c) each scS~ (the range \nof 1) has a A-predecessor which is not in SA. For any shift A, the relation <A (which we now refer to \ngenerically as a shift relation) is a strong partial order; that is, it is asymmetric, and irreflexive \nas well as transitive. Even stron\u00adger, the order ie the union of disjoint well\u00ad i orders. The relation \ncan, thus, be viewed as a collection of parallel shift chains, each chain . with a minimum element meS \nwhich is connected to 2 mX, which in turn is connected to ml = (mk)A, and so on, possibly forever. Our \ninterest in shift chains and relations stems from the fact that the axis successors of array schemes \nare shifts, as indeed, are all func\u00adtions obtained as finite compositions of the suc ceseors (e. g., \ndiagonal successors like U102). No axis predecessor is a shift in orthant arrays since the predecessors \nrelations lack minimal elements. Each predecessor save one is a shift in a prism array. In finite arraye \nthe notione of predecessor and successor are interchangeable at the whim of the beholder, so both types \nof transformations are shifts. Thus, information about realizations of shift relations can be translated \ninto information about array realizations with emphasis on the access maps. We consider first t-he problem \nof realizing a single shift relation. (3.2) Let S be a set, let A be a shift of S, and let MA s S be \nthe set of minimal elements of the shift relation Let A ~:S + N be a total one-one function. we say \nthat T has the (a) additive property for k if, for each m~MA there is a constant kmeN such that, for \nall ncNu{O}, if mc domain(in) , then (mln). = (m.) + n-km; uniform additive property for i if there is \na constant k6N such that, (b) for all SCS, if SE domain(l), then (SI)T = (ST) + k. The term additive \nderives from the fact that any l-chain can be traversed by successive additions of a constant which depends \nonly on the chain. The term uniformly additive indicates that all chains are eerved by the same constant. \nWhen can we find a T with one of these pro\u00adperties? Some terminology will help us expose the answer. \nThe shift relation is finitary if A each shift chain is of finite length. The relation is thin if the \nset of minimal elements is A finite, so that there are but a finite number of shift chains. Finally, \n<A is limited if all but finitely many of its shift chains are of finite length. Theorem 1. Let S be \na countable set with a shift A. (a) There is a total one-one map Ta:S + N with the additive property \nfor 1. (b) When is limited5 --and only A when is limited --there is a total one\u00ad A one map TU:S + N with \nthe uniform additive property for l., In general we are interested in realizing more than one shift in \nan efficient way. For example, one would usually wish both row and column successors to be easy transitions \nin a matrix. It appears that ~ositive results about two or more shifts can be sought fruitfully only \nin a concrete environment, where information about the relation\u00adships among the shifts can be brought \nto bear. Hence, we defer consideration of injections with the (uniform) additive property for several \nshifts to the next subsection where we consider only arrays. However, an important negative result with \nimplica\u00ad tions for orthant arrays can be formulated in the present framework. (3.3) Let 1 and p be shifts \nof the set S. We say that i and p are independent if there do not exist Ses and L,meN for which S1l = \nSpin. [Of course, we assume both are defined.] 5 In particular, <A is limited if it is either thin or \nfinitary. 6 In the context of arrays, independence is essen tially linear independence of vector displacements. \nWhen i and u are total, independence is precisely linear independence. As an example, any two axis successors \nin any of our array models are independent. Theorem 2. Let S be an infinite set, and let 1 and u be independent \ntotal shifts of S. There is no total one-one map 7:S + N which has the additive property for both A \nand U. Theorem 2 can be generalized along the following lines. Say that the (not necessarily total) independent \nshifts A and v are both to be realized with the additive property. Then, intuitively, if the i-chain \nemanating from eeS is long, then the P-chain em~ating from s must be short, and vice versa. B. Applications \nto Arrays Rather than continue in the abstract vein of Section A, we return to our study of allocation \nschemes for arrays, but we do so in the light of Theorems 1 and 2. Transitions in Prism. In any prism \narray, one axis successor is a thin and each of the others is a finitary shift of the set of cells, 7 \nHence, by Theorem 1, any single successor can be realized with the uniform additive property. In fact, \nthe orthogonality of the axis successors can be exploi\u00ad ted to realize all successors with the uniform \nadditive proper-Such totally additive schemes can be easily derived from the familiar schemes used in \ncompilers for languages like FORTRAN and PL/l; a derivation of such schemes appears in [6, p.296]. Note, \nhowever, that these familiar schemes must be modified somewhat to comply with the restriction, (c) P6P \nis in the domain PCM precisely when (P@(PP)cp~ in the definition (2.2) of realization. To be more precise, \nthe schemes alluded to are perfectly good realizations of prism arrays, but they are not totally additive. \nNote, for example, our ~lization (2.5) which implements the 2-dimensional 4-column prism array using \na scheme like those in [6] (cf. Figure 3.). Note in (2.5) that u2p was defined as follows: for all neN, \nn+l if n #O(mod 4) n(02p) = { undefined otherwise. We could not opt here to let CJZP be the successor \nfunction (kn[n+l]) or else we would have (<i,4>@(u2P) = <i+l,l>~, while <i,4>u2 # <i+l,l>. This problem \nis overcome by adding gaps to the scheme of realization (2.5); for instance, 7 strictly sPeaking the \nterms thin and finitarY have been defined only for shift relations; how ever, the reader should have \nlittle problem in deciphering our loose usage, 222 (3.4) (a) For <i,j~cP, <i,j>~ = 5(i-1) + j. (b) Olo \n= ~n[n+5] ; Tlp = kn[n-5]; J20 = ~n[n+l] ; n2p = An[n-l]. The modified storage layout is depicted in \nFigure 4. For the loss of less than 20% storage utilization (in the limit),8 one thus obtains a totally \naddi\u00adtive realization of this prism array. Using (3.4) as a model, the reader can most assuredly derive \nthe appropriate modifications of the general schemes in [6, p.296]. Thus, prism arrays admit transition \nwhich are very efficient, albeit at the cost of ease of extension. What happens to ease of transition \nwhen ease of extension is emphasized? Transitions in Orthant Arrays. In an orthant array, all axis successors \nare total, and none is limited. Any two distinct successors are independent shifts of the set of cells. \nFrom these facts one can deduce Fact 1. (Theorem 1) Any single axis succes\u00adsor can be realized with the \nadditive property, but none can be realized with the uniform additive property. Fact 2. (Theorem 2) No \nrealization can have the additive property for more than one axis successor. Fact 2 is the formal version \nof our assertion that, in some sense, ease of extension (which is embodied in an orthant realization) \nis. incompatible with ease of transition (which we are equating with the additive property). Together \nwith our earlier discussion, Fact 2 points out why the prism model is to be preferred when extension \nis of little concern. HOW well can one do with the additivity guar\u00ad anteed by Fact 1? At the cost of \nquite inefficient storage utilization, one can do rather well - at least in two dimensions. Consider \nthe following orthant realization which is depicted in Figure 5. (3.5) (a) For ci,j>cP , <i,j>~ = 2 -1(2j-1). \n2 (b) alp = ln[2n] ; TIP = kn[n+2]; for <i,j>6P , 2 (<i,j>~) (02p) = (<i,j>~) +2i ; (zi,j>;)(n2p) = (zi,j>,@ \n-2i if j>l, { undefined otherwise. In this realization, the row euccessor U2 is additive. While the \ncolumn successor al is not additive, it is just multiplication by 2, an easily implemented operation \non most computers. Thus , this realization affords quite efficient transitions ae well as easy extension. \nHowever, l_t is a rather poor realization in its utilization of storage; and it quickly deteriorates \nin this respect 8 Observe that 4k elementS of N5k_1 are used for a k row prefix. as the prefix implemented \ngrows by adding rows: An m row, n column prefix uses only mn of the 2mT1(2n-1) locations affected by \nthe allocation, so that, in the extreme, a column vector use exponentially fewer of the affected locations \nas It grows. Such sparseness is an -inevitable concomitant of any realization with a multiplicative transition. \nRealization (3.4) is valuable, not necessarily as a technique for allocating storage for arrays, but \nas an illustration of the complicated inter\u00adplay among the quality criteria of (2.4). It indi\u00adcates also \nthat a materially stronger statement than Fact 2 of the incompatibility of ease of extension and ease \nof transition will require delicate formulation. We have thus far left unanswered the question of whether \nor not there is a good realization of the orthant arrays. In [3,4] we demonstrate that, at least in special \nsituations, such good real\u00adizations do exist. Much study is needed before we really under\u00adstand these \nbest understood of all data-structures. ACKNOWLEDGMENT It is a pleasure to acknowledge the encourage\u00adment \nand assistance of several of mv colleawes. notably John S. Lew and Shmuel Winograd. -- REFERENCES 1. \nA. L. Rosenberg, Data grapha and addressing schemes. J.CSS 5 (1971) 193-238. .  2. A. L. Rosenberg, \nExploiting address ability in data graphs. In Computational Complexity: Courant Computer Science Symposium \n7. (R, Rustin, cd.), Algorithmic Press, New York, 1973. 3. A. L. Rosenberg, Allocating storage for extendible \narrays. IBM Report RC-4306, 1973. Also submitted for publication. 4. A. L. Rosenberg, Inherent limitations \nof  extendible array realizations. In pre\u00adparation. BACKGROUND 5. G. Birkhoff and S. Mac Lane, A Survey \nof Modern Algebra, Macmillan, New York, 1961. 6. D. E. Knuth, The Art of Computer Program\u00adming I: Fundamental \nAlgorithms, Addison-Wesley, Reading, Mass. 1968.  Figure 1. The Array Scheme A of Size <3,4>. Figure \n2. Realization (2.3): A Sparse Realization of A. Figure 3. Realization (2.5): The Array A Stored by \nRows. 224 Figure 4. Realization (3.4): A Totally Additive Realization of the 4-Column, 2-Dimensional \nPrism Array. Figure 5. Realization (3.5): A Realization of with Simple Transitions. 2 \n\t\t\t", "proc_id": "512927", "abstract": "Arrays are among the best understood and most widely used data structures. Yet even now, there are no satisfactory techniques for handling algorithms involving extendible arrays (where, e.g., rows and/or columns can be added dynamically). In this paper, the problem of allocating storage for extendible arrays is examined in the light of our earlier work on data graphs and addressing schemes. A formal analog of the assertion that simplicity of array extension precludes simplicity of transition (marching along rows/columns) is proved.", "authors": [{"name": "Arnold L. Rosenberg", "author_profile_id": "81100493202", "affiliation": "IBM Watson Research Center, Yorktown Heights, New York", "person_id": "PP14172572", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512927.512947", "year": "1973", "article_id": "512947", "conference": "POPL", "title": "Transitions in extendible arrays", "url": "http://dl.acm.org/citation.cfm?id=512947"}