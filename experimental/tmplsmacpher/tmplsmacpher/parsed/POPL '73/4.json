{"article_publication_date": "10-01-1973", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1973 ACM 0-12345-678-9 $5.00 PRACTICALSYNTACTICERRORRECOVERYIN COMPILERS* ( EXTENDEDABSTRACT) by Susan \nL. Graham and Steven P. Rhodes t University of California at Berkeley INTRODUCTION A substantial portion \nof any programmer s time is spent in debugging. One of the major services of any compi 1er ought to be \nto provide as much information as possible about compile\u00adtime errors in order to minimize the time required \nfor debugging. A good error detection and recovery scheme should maximize the number of errors detected \nbut minimize the number of times it reports an error when there is none. These spurious error detections \nand their associated error messages are usual ly engendered by an inappropriate recovery action. In this \npaper we describe a recovery scheme for syntax errors which provides high quality recovery with good \ndiagnostic information at relatively low cost. In addition, implementation of the recovery scheme can \nbe automated -that is, the recovery routine can be created by a parser\u00ad generator. Therefore, the compiler \ndesigner need not be burdened with the difficulties of error recovery and the programming effort necessary \n to design and debug a myriad of ad hoc recovery routines. The syntax error recovery scheme we propose \ndoes not necessitate any overhead for parsing correct programs or correct portions of programs with errors. \nOnce an error has been detected, the parsing stack and/or the input stream are modified in order to get \nthe syntax analyzer back on the track , i.e., to get back to parsing as if the program were syntactically \nvalid. As is true of any recovery scheme, the recovery action taken does not necessarily corres\u00adpond \nwith the programmer s intentions. Any given piece of syntactically incorrect source text may mean different \nthings for different computations. * Research supported by the National Science Foundation under grant \nGJ 474 + Present address: Bell Laboratories, P.O.Box 21447, Greensboro, North Carolina, 27420. For example, \nconsider the incorrect Algo statement I: = 1 -(((N*M) -(I*J)/2). Among the many equally plausible correct \nversions of the above statement are the following two (different) statements. I: =1 -(((N*M) -(I*J))i2) \n I: = 1 -((N*M) -(I* J)/2)  Furthermore, different classes of programmers (for instance, novice programmers \nvs. experienced programmers) may make different kinds of errors. Consider the syntactically invalid Fortran \nIV statement READ532ABLE given by E. James and Partridge [6]. A former Fortran II programmer would probably \nhave meant READ532, ABLE , whereas a Fortran IV programmer would probably have meant READ (5,32) ABLE \n. Some people try to dichotomize syntactic error handling systems into two categories-\u00adrecovery and correction. \nThey define the former to be systems which do not allow the execution of programs having syntax errors, \nwhile defining the latter as those systems which permit the execution of such programs. The previous \nexample shows why this terminology is very misleading. Whether a system allows syntactically incorrect \nprograms to be run has little to do with whether it has made the right correction of the syntax errors \ninvolved. As shown above, there is often no single right correction. Hence, we will not make this distinction. \nThe recovery method to be presented is embedded in a simple precedence parser [15]. Although the techniques \nwe used are particularly well suited to this parsing method, manyof the ideas are easily incorporated \nin other methods of parsing. The results presented in this abstract are contained in the Ph.D. dissertation \nof the second author [14], which contains a more complete discussion of many of the issues which can \nonly be touched on here. Summary of Previous Methods In the last decade there has been increasing interest \nin the problem of syntactic error recovery and a variety of techniques have been discovered. The simplest \nand crudest automatic recovery technique was discovered independent y by several people. This strategy \nis frequently referred to as the go forward to a safe place and then back up technique or the panic mode. \nIn this scheme, when an error is detected, the input is advanced until one of a class of special symbols, \nsuch as a ; or an END is located. The parsing stack is then erased until the special symbol can legitimately \nfollow the top of the parsing stack. This method is fast and requires a small amount of code, but the \nerrors contained in that portion of the text which is skipped are not detected, thus possibly necessit\u00adating \nmany additional computer runs to detect all of the errors in the user s program. In addition, little \ninformation is available about the nature of the error. If the parsing method being used is predictive, \nthat is, it is possible to determine easily all the possible valid continuation symbols for the input \nread so far, then the input can be advanced until one of these symbols is encountered. The recovery scheme \nfor LR-parsing described by Leinius [10] and implemented by L. James [7] is a more sophisticated version \nof this technique. In the most parsing methods, it is possible to find very local changes to the input \nat the point of error detection (for example insertion or deletion of a single symbol or token) which \nmake the input locally correct . However, these changes might not be correct with respect to larger portions \nof the input. One approach (used by Levy [11] and La France [9], among others) is to try out all the \npossible local modifications by continuing the parse for each. If, as in the Levy method, one continues \nmultiple parsing for an unbounded number of steps, the ensuing combinatorial explosion in space and time \nmakes the technique very impractical . Therefore, La France bounds the amount of multiplicity. Another \napproach is to try only a small list.of possible local modifications and use the first one that works \n(see Wirth [16]). The approach taken by Gries [5] and the PL/C implementers [4] is to build into the \nparsing table error actions based on the implementers knowl edge of comnon programming errors and appropriate \nparticular recovery actions. Such a system handles the expected errors reasonably well but can fai 1 \nbadly on unanticipated errors. Augmenting the grammar by error productions (Wirth [16], Leinius [10]), \nis another form of this approach One can combine the local correction approach and the special knowledge \ntechnique by having an implementer-designed table of possible local modifications for each symbol or \npair of symbols in error. (See, for example, Bauer, et al [1], and Peterson [13].) Definitions and Notation \nA ~context-free) grammar is a 4-tuple G = (V,Z,P,S), where z~V is a finite set of terminal symbols, N \n= V -Z is a finite nonempty set of nontermlnal symbols, P is a finite set of rules or productions, x+x, \nwhere X e N and x e ~(1) and S e N is the initial symbol. For any rule X+x, X is termed the left hand \nside abbreviated LHS) and x is the right hand side I RHS). As usual, with respect to a grammar G = (V,Z,P,S) \nwe define the relation * on V*xV* suchthat for any aeV*, beV*, a +-b if and only if there exist U e N, \nu, m,ueV* and U+ u in P suchthat a = d-h and b = OUT. We represent by ~ (~) the transitive closure (reflexive-transitive \nclosure)of +. For any n~O, aie V*where O<i< n, we call the sequence a. -al * . *an a derivation of an \n~ a. of length ~ fi Q. . If ao~al~ -an where for O<i<n, ai = a.u.Tr. and ai+l = Uiuini for some 111 \nOi eV*, and Ui +u. in P, the i e T derivation is a rightmost deriv~tion. A sequence of symbols u is a \nsentential form if S au. The sentential form is riqhtmost if there is a rightmost derivation such that \nS ~u. If u e X* then u is a sentence. The language L(G) defined (or ge ~~ Q IS th e set of all es~ Thus \nL(G) = {u e Z* I S$u} We refer to the process of reconstructing a derivation, given a sequence of terminal \nsymbols and a grammar, as parsing. Given a sentential form uuTr, cr,u,n e V*, anda rule U+u, the transition \nfrom uum to OUT obtained by substituting U for u is called a reduction. For any grammar G = (V,Z,P,S), \nsimple precedence relations <O, ~, + are defined for all A, Be V by: A~B if for some u, n e V*, P contains \na rule U + oABm A<,B if for some u, 7, a e V*, YeN, P contains a rule U + uAYn and y$Ba A>B if for some \no, v, a, ye V*, X,YeN; P contains a rule U + UXYT and y : Ba. X~yA and A grammar grammar G = (V,Z,P,S) \n[15] if: is a simple precedence (a) For relation all A,BeV, is satisfied. at most one preced ence (1) \nFor any set of symbols V, V* denotes the set of all sequences of symbols from V and v+ = V* . {a} where \nA is the empty sequence. 53 (b) P contains no rule with RHS 1. (c) No two rules in P have the same RHS. \n (d) With respect to G, there is no rightmost  derivation S~S. Simple Precedence Parsing and Error Detection \nWe briefly review simple precedence parsing for the reader. The parser for a simple precedence grammar \nG = (V, X,P,S) uses a pushdown store, called the parsing stack which we represent as a sequence of symbols \nwith the base of the stack at the left and the top of the stack at the right. There is a base\u00adof-the \nstack symbol designated d and an end-of\u00adfile symbol designated $ , where t, $ are not in V. The last \ninput symbol is always $. We extend the precedence relations to $ and $ by the rules that for every X \ne V, if there is some u e V* such that S ~Xo, then @<X and if there is some o e V* such that S ~oX, then \nx> $. Initially, the parsing stack contains only $ . For an input string which is contained in L(G), \nthe parser works in the following way (excluding the output steps, semantic routines, etc.): Step 1. \nRead the next input symbol Step 2. If the precedence relation between the symbol at the top of the parsing \nstack and the input symbol is <. or = then stack the input symbol and go to Step 1. Step 3. If the input \nsymbol is $ and the contents of the stack is dS then exit. Step 4. (Otherwise the precedence relation \nbetween the top stack symbol and the input symbol is ,>). Scan the stack from right to left until the \nfirst instance in which a symbol (call it A) and the symbol to the right of it in the stack have the \nprecedence relation < Step 5. Find the rule having as RHS the sequence of symbols to the right of A on \nthe parsing stack. Replace the symbols to the right of A by the LHS of that rule and go to Step 2. In \nthe usual precedence parser, errors are detected in one of two ways. The first occurs when there is no \nprecedence relation between the top of the parsing stack and the incoming symbol, (Step 2); this situation \nis usually referred to as a character pair error. In a typical Algol grammar, for instance. the strina \nA:=I J: =K; would have a-character ~air error between the I and the J since an identifier can never be \nfollowed by an identifier. The second type of error is found when a potential RHS is detected using the \nprecedence relations (Step 4), but it does not match the RHS of the grammar (Step 5). This type of error \nis normally referred to as a reduction error. A reduction error can arise in the folmg way. The simple \nprecedence grammar s+ N; N + D.D D+l generates the sentence 1.1 ; If the parser for this grammar is given \nas input 1.1.1. eventually Step 4 finds a potential RHS D.6.D. , yet no rule of the grammar has this \nRHS. The error detection capabilityof a simple precedence parser can be significantly improved without \nany increase in the running time of the parser. First, as suggested by Leinius [10], when a reduction \nis performed, a check can be made (Step 5) to see that A and the LHS to be stacked have precedence relation \n< or ~, otherwise a stackability error occurs. The second error detection extension is an improvement \nin the detection of reduction errors. In this second extension, which is original as far as we know, \nthe system continually checks the top of the stack for prefixes of RHS S of rules of the grammar before \nit puts a symbol onto the stack. This can be done, for example, by having the production table sorted \nlexicographically by RHS S and having a pointer into this table which is advanced before each symbol \nof a RHS is stacked. All the RHS S with a common prefix will then be grouped together. When a new RHS \nis begun (i. e. when the top symbol of the stack and the symbol to be stacked have precedence relation \n< ), the previous pointer value is saved and the pointer is set to the first production such that the \nleftmost symbol of the RHS is the symbol to be stacked. When the prefix at the top of the stack is to \nbe continued (i.e. when the top symbol of the stack and the symbol to be stacked have precedence relation \n~ ), the pointer is set to the first RHS having that prefix fol lowed by the symbol to be stacked. When \nthe prefix at the top of the stack should be a RHS (i.e. when the top stack symbol and the input symbol \nhave precedence relation >), the pointer should be pointing to the rule with that RHS. In the latter \ntwo cases, if there is no such RHS, a non-val I d RHS error is said to have oc\u00adcurred. Since the traditional \nparsing method must also search through the RHSS, (Step 5) our method entails no increase in parsing \ntime. The only difference in our method is that it does the searching incrementally, whereas the usual \nmethod performs it all at one time. Consider again the example 1.1.1; given in the discussion of the \nusual precedence parser. In the system described in this paper, the parser detects a non-valid RHS error \non the second  since there is no production whose RHS beg~ns with D.D Notice that a character pair error \nis j~st another kind of stackability error and a reduction error is one kind of non-valid RHS error. \nIn certain other parsing methods, such as LR parsing [8], errors are detected as soon as the part of \nthe source program that has been seen thus far no longer forms a prefix of a sentence in the language \nbeing parsed. In this class of methods, which will be referred to, following Levy [11], as methods having \nthe correct prefix property, the error is recognized at the earliest possible point in a single deterministic \nlet-to-right scan of the input text. This does not mean, however, that the error is necessarily detected \nexactly at the point of its occurrence. The error and its detection may, in fact, be an unbounded distance \napart. Levy, for example, suggests considering the regular language {$O*$$]U {#O*##} with the input $Ok##. \nThe error is detected at the first #. If one assumes that a correction involving the minimum number of \nchanges is the most appropriate change, then the symbol in error is most probably the $ that is k+l symbols \nback in the input. Notice, however, that it suffices to change both # s im $ s in order to recover from \nthe error. It is possible that even with our modifications, the simple precedence parser may detect an \nerror an unbounded number of symbols after a correct prefix parser for the same grammar. However, in \npractice our modified simple precedence parser detects most errors at the same place they would be detected \nby parsers having the correct prefix property. Furthermore, while early detection of errors may be useful \nfor providing error messages, the accompanying necessity to correct the error at that early stage (in \norder to preserve the correct prefix property) can be a decided disadvantage. For example, given the \nlanguage {abc} U {(bf)n I n > O} and the input abfbf . . . bf, a correct prefix parser will detect an \nerror at the first f, change the f to a c, and repeatedly indicate that each succeeding symbol is in \nerror. A grammar is easily constructed for this language such that the modified simple precedence parser \nfor that grammar will not detect an error until the end of the input. However, at that point our recovery \nscheme will simply delete the a . The Condensation Phase After an error is detected, a recovery scheme \nmust somehow recover from the error and continue parsing so as to be able to detect and report subsequent \nerrors to the programmer. The recovery strategy may involve simple or complex manipulations of the input \nstream and/or the parsing stack. Immediately after error detection, most recovery strategies begin to \nconsider what correction--to the input stream and/or the parsing stack--would recover from the error. \nThe method described herein interposes a phase between the error detection and the correction phase. \nThe phase thus interposed is called the condensation phase. When an error is detected, the stack may \nbe in an unstable configuration in the sense that reductions may be possible if the error is temporarily \nignored. In addition, parsing of the input stream past the point of the error detection may yield important \ninformation as to the cause of the error. The two ideas above are the motivation for the condensation \nphase. The condensation phase attempts to condense information about the point of the error detection \nby perform\u00ading as many reductions as possible in the vicinity of the detection point. The point at which \nthe error is detected wil be designated ?1 This is the point immediately preceding a RHS if the corresponding \nLHS lead: to a stackability error; the point at the top of the stack if the corresponding LHS leads to \na non-valid RHS error. Similarly, if there is a character-pair error, ?, is at the top of the stack, \nbut if the incoming s.wnbol causes a non\u00advalid RHS error, the i~co~ing symbol is stacked and followed \nby and the input is advanced. -?1 The condensation phase is divided into two (sometimes three) parts. \nThe backward move consists of assuming that there is a pr~ence relation ~> between the symbol to the \nleft of ?1 and the symbol to the right (which may be the input symbol) and returning control to the parser. \nThis causes all possible reductions at the top of the stack to be made. Once the backward move is done \n( the state of the parser perhaps being unchanged) the forward move is carried out. It is assumed that \n~s a precedence relation <. between the symbol to the left of and the symbol to the ?1 right and control \nis again returned to the parser. The forward move terminates when a second error is detected. The second \nerror is either another real error in the source text or another manifestation of the first error (i.e., \nbecause of the top of the stack does not contain the ?1 prefix of a RHS)(?) We designate by the second \nerror point. ?2 (The;rwdisTa]ways at least one symbol between ?1 2 Finally, a second backward move may \nbe done from the point This completes the condensation ?2 phase. Consider the following Algol example: \nM:=Q-3; I: = 2* (M-P) then K: =1 else M:=1; (The most probable error is that there is a missing if preceding \nI). For a typical Algol grammar,~he state of the parser after the error is detected will be that the \nstack has the form d <blockbody> ?1 <variable> , 2) A diagnostic message is issued only if there appears \nto be a second error in the source text. 55 the input symbol is =, and the rule with RHS <variable> has \na LHS which cannot follow <blockbody> (for example, the rule may be <factor> + <variable> ). No reductions \nare made by the backward move. The forward move reduces I = 2 * (M -P) to <expression> and then (assuming \nthe grammar contains a rule <if clause> + if <expression> then) detects a second erro~because there is \nno RHS with prefix <expression> ~. The stack then has the form: ~ <bl~ckb(jdy> <expression> then ?1 ?2 \nand the incoming symbol is K. The second backward move does not alter the configuration. This example \nillustrates the usefulness of a forward lookahead without a fixed a priori bound. Since the forward move \nreaches the ~, the correction phase has the information to be able to insert an if. We are unaware of \nany other practical =rategy that would be able to recover in this manner. We suspect most other schemes \nwould change the = to a := or initiate some other equally unsatisfactory recovery action. Of course, \nthe backward and forward moves may not make the correct reductions. For example, the Algol fragment X: \n= I J; with a missing operator between I and J wi 11 be parsed to something like a statement (X: = I) \nfollowed by an expression (J). However, our empirical tests indicate that the condensation phase rarely \nprovides inappropriate information. For instance, in the previous example, the condensation wi 11 1ead \nthe correction phase to delete the expression J . This is as correct an action as inserting a + . \nThe Correction Phase The forward move provides, in effect, an unbounded lookahead beyond the point of \nerror detection. After the condensation phase, the right context of the error provided by the forward \nmove is contained in the parsing stack (plus the current input symbol ). The purpose of the correction \nphase is to change the contents of the condensed parsing stack so that the error situation is corrected \nand the parsing stack is restored to the prefix of a rightmost sentential form. (In fact, the correction \nphase insures only that in the vicinity of the error, the corrected parsing stack appears to be the prefix \nof a rightmost sentential form; that is, the contents of the parsing stack could be obtained by the simple \nprecedence parser on some input without having discovered an error). Since we wish to use as much of \nthe context of the error as can be efficiently exploited, the correction phase considers changes to sequences \nof symbols rather than isolated changes to single symbol S. For the sake of efficiency only three sequences \nof symbols are candidates for correction and only a fixed number of possible corrections are considered. \nThe choice among the possibilities is determined by a probabilistic pattern match. The candidates for \nmodification are the sequence of symbols from the nearest < to the left of ?1 up to ?2, from that <L \nup to ?2, to ?2. ?1 can be changed only be deleting the entire candidate sequence or by replacing the \ncandidate sequence by the RHSof some rule of the grammer. The restriction on possible replacements is \nnot unreasonable, since it corresponds to the possib\u00adilities first that the precedence relation at point \nis I>, second that the precedence and from Furthermore, these candidates ?1 . relation at point and third \nthat the ?1 s = precedence relation at point is <o. (There ?1 is also an implicit assumption that the \nprecedence relation at point In practice, this ?2 s > is almost always the case. One can modify the correction \nphase so that prefixes of RHS S are also possible replacements, but the increase in computation is significant \nwhen measured against the empirical percentage of instances when such replacements are necessary). Thus \nif the number of rules in the grammar is n, we consider only 3(n+l) possibilities. We choose from among \nthese, the change which is the closest fit and appears to correct the error. The correction phase is \ndivided into two parts. The first part reduces the set of possibilities to those for which the result \nof the replacement or deletion is locally correct, Essentially, a modification is locally correct if \nit does not create a stackability error or a non-valid RHS error. More precisely, Definition Let G = \n(V,Z,P,S) be a simple precedence grammar. A reduction X+x is locally correct in the context(~), y e V* \n; -V if P1) A<~Xor AgX P2) X<.Bor X2Bor X,>B P3) if A ~ X and XS>B, then yAX is the RHS of some rule \nin P; if A <.X and X> B then X is the RHS of some rule in P; otherwise, if A ~ X then YAX is a proper \nprefix of the RHS of some rule in P. The deletion of x is locally correct in the context( 3=) y e V*; \nA, Be V 01) A<,Bor A~Bor AoB D2) if A ~ B then YAB is the prefix of the RHSof some rule in P; if A.> \nB then yA is the RHSof some rule in P. (3) In considering a candidate sequence for replacement or deletion, \nthe left context is always the sequence of symbols starting with the nearest + to the left. The tests \nfor local correctness can be done rapidly, since it is the set of LHS S or non\u00adterminals which are tested \nand the error checks are those done by the parser. In most cases, these tests eliminate a substantial \nportion of the possibilities. (90-95% in our experiments. ) The second part of the correction phase determines \nwhich of the remaining possibilities has the closest fit ; that is, which of the potential corrections \nrequires a minimum of symbol \u00adby-symbol modifi cation. A weighted minimum\u00addistance measure is used. In \norder to compute how close a given RHS is to one of the candidates for change, two vectors I and D are \nused. For each symbol in the grammar, the I vector contains the cost of inserting that symbol any\u00adwhere \nin the stack and the O vector gives the cost of deleting that symbol anywhere in the stack. The closest \nfit is then defined to be the match with the minimum cost. As an example of the cost computation, consider \nthe following two lines: IM3 IMP := IMP+l; (The apparent error here is that the symbol IM3 was inadvertently \nleft in the source text.) For a typical Algol grammar, the condensation phase leaves the stack in the \nfollowing state: i# <blockbody> <variable> <statement>; ?2 ?1 The cost of changing the <,-to-?2 candidate \nusing the production <blockbody> + <blockbody> <statement> ; is I(<blockbody>) + D(<variable>). If the \n?1 -to-?2 candidate is changed using the same production, the cost is I(<blockbody>). The most viable \nalternative for our recovery system in this situation is probably the deletion of the <, -to-?1 candidate. \nThe cost of deleting a sequence of symbols is simply the sum of the cost of deleting each symbol. Hence, \nthe cost of deleting the <* -to-?, candidate is D(<variable>). After the pattern matching process has \ndetermin\u00aded the cost of the changes by using the cost vectors, the minimum cost change is made. Control \nthen returns to the parser. In the unlikely event that the minimum cost is greater than a fixed ~ -maximumY \na form of the panic mode is used. The assumption in that case 1s that the change, although locally correct, \nis so bizarre that it is probably wrong. The cost vectors I and D can be generated mechanically or can \nbe provided by the implementer. There are a variety of heuristics which can be used in selecting the \ncosts in order to improve the quality of the recovery. For example, brackets (w, end, (, ), etc.) and \nthe non\u00adterminals generatfi them (<blockhead>, <blockbody>, etc.) should have relatively high I and D \nvalues and long reserved words should have high deletion costs. The form of the correction phase we have \njust described is only one of many variations that can be used. For example, one can have a cost function \nR which assigns costs to replacing one symbol by another (these values are normally lower than the corresponding \nI + D costs). One can permit a greater variety of replacements for the three candidate phrases. A more \nstringent definition of locally correct can be used. Spelling corrections [12] can be incorporated. All \nof these additions have their uses, although at a certain additional cost in the amount of recovery code \nand the maximum time to recover from a single error. One can also reduce the amount of computation in \nthe correction phase and get recovery that is almost as good. All of these variations are discussed further \nin [14]. Comparisons With Other Recovery Methods We implemented the recovery method (complete with a \nvariety of features which could be independently activated or deactivated for comparative purposes) for \nful 1 PASCAL[17] and for an Al gol -1 ike syntax. We then compared the recovery actions taken by our \nsystem on a body of test data with the performance of the PASCAL [17,18], PL/C [3,4], and Algol W [1 \n,2], compilers. Although such comparisons are necessarily somewhat subjective (each compiler does some \nkind of recovery for every error it discovers), our system appears to discover more of the errors when \nthe errors are dense and in general to give fewer spurious error indications. In addition, the errors \nthat one or more of the other compilers handles well are almost always dealt with properly by our system. \nIt is hoped that more widespread use of these techniques will provide additional comparative information. \nThe sorts of errors for which our techniques seem particularly powerful are such things as errors in \nthe middle of a list (other methods end the list at that point and either disregard or misinterpret the \nremainder), misuse of reserved words (hand-tailored systems rarely anticipate that), errors that show \nup later (correct-prefix parsers tend to treat everything already parsed as correct), and errors involving \nbracket\u00ading. Concluding Remarks We have omitted from this paper any discussion of error messages, language \ndesign to reduce errors, grammar design to facilitate recovery, and incorporation of these recovery techniques \nin other parsing methods. All of these topics are discussed in [14]. The aspects of this approach to \nerror recovery which give it its power are the ability to deviate from left-to-right parsing in order \nto analyze the look-ahead beyond the point of error, the systematic approach which reduces implementer \nbias in the treatment of errors, the structural constraints on the grammar which reduce the number of \ncorrections to consider,the cost vectors which can reflect the expected behaviour of the programmer, \nand the relatively easy way in which the implementer can tune the error recovery. 57 REFERENCES 1. Bauer, \nH.R., Becker, S. and Graham, S.L. Algol W Implementation, Technical Report CS 98, Computer Science Department, \nStanford University, Stanford, Ca., May 1968. 2. Bauer, H.R., Becker, S., Graham, S.L., and Satterthwai \nte, E. Algol W Language Description, Technical Report CS 110 Computer Science Department, Stanford University, \nStanford, Ca., Sept. 1969. 3. Conway, R.W., Morgan, H.L., Wagner, R.A., and Wilcox, T.R. PL/C. A High \nPerformance Subset of PL/1, Technical Report 70-55, Computer Science Department, Cornell University, \nIthaca, N,Y., February, 1970. 4. Conway, R.W. , and Wilcox, T.R. Design and Implementation of a Diagnostic \nCompiler for PL/1, Communications of the Association for Computing Machinery, Vol. 16, March 1973, pp. \n169-179. 5. Gries, D. The Use of Transition Matrices in Compiling, ~, Vol. 11, January, 1968, pp. 26-34. \n 6. James, E.B. and Partridge, D.P. Adaptive Correction of Proaram Statements. Communications of-the \nAssociation for ~, Vol. 16, January 1973, pp. 27-37. 7. James, L.R. A Syntax Directed Error Recovery \nMethod, Master s thesis, Technical Report CSRG-13, COmpUter Systems Research Group, University of Toronto, \nToronto, Canada, Nay 1972. 8. Knuth, D.E. On the Translation of Languages from Left to Right , Information \nand Control , Vol . 8, November 1965, pp. 607\u00ad 639. 9. La France, J.E. Syntax-directed Error Recovery \nfor Compilers, Ph.D. thesis, Illiac IV Document 249, Computer Science Department, University of Illinois, \nUrbana, Illinois, June 1971. 10. Leinius, R.P. Error Detection and Recovery for Syntax Directed Compiler \nSystems , Ph.D. thesis, Computer Science Department, University of Wisconsin, Madison, Wisconsin, 1970. \n 11. Levy, J.P. Automatic Correction of Syntax Errors in Programming Languages, Ph.D. thesis, Technical \nReport TR71-116, Computer Science Department, Cornell University, Ithaca, N.Y., December, 1971. 12. \nMorgan, H.L. Spelling Correction in System Programs, Communications of the Assoc\u00adiation for Computing \nMachinery, Vol. 13, February 1970, pp. 90-94.  13. Peterson, T.G. Syntax Error Detection, Correction \nand Recovery in Parsers, Ph.D. thesis, Stevens Institute of Technology, Hoboken, N.J. 1972. 14. Rhodes, \nS.P. Practical Syntactic Error Recovery for Programming Languages, Ph.D. thesis, Technical Report 15, \nDepartment of Computer Science, University of California, Berkeley, Ca., June 1973. 15. Wirth, N. and \nWeber, H. Euler, A Generaliz\u00adation of Algol and Its Formal Definition , Communications of the Association \nfor Computing Machinery, Vol. 9, January and February 1966, pp. 13-23 and pp. 89-99. 16. Wirth, N. A \nProgramming Language for the 360 Computers, Journal of the Assoc\u00adiation for Computing Machinery, Vol. \n15, January 1968, pp. 37-74. 17. Wirth, N. The Programming Language Pascal, Acts Informatica, Vol. 1, \nJanuary 1971, pp. 35-63. 18. Wirth, N. The Design of a Pascal Compiler, Proceedings of the International \nSummer School on Program Structures and Fundamental Concepts of Programming, !lunich, Germany, July 1971. \n \n\t\t\t", "proc_id": "512927", "abstract": "A substantial portion of any programmer's time is spent in debugging. One of the major services of any compiler ought to be to provide as much information as possible about compile-time errors in order to minimize the time required for debugging. A good error detection and recovery scheme should maximize the number of errors detected but minimize the number of times it reports an error when there is none. These spurious error detections and their associated error messages are usually engendered by an inappropriate recovery action.In this paper we describe a recovery scheme for syntax errors which provides high quality recovery with good diagnostic information at relatively low cost. In addition, implementation of the recovery scheme can be automated - that is, the recovery routine can be created by a parser-generator. Therefore, the compiler designer need not be burdened with the difficulties of error recovery and the programming effort necessary to design and debug a myriad of ad hoc recovery routines.", "authors": [{"name": "Susan L. Graham", "author_profile_id": "81452606376", "affiliation": "University of California, Berkeley", "person_id": "PP14173434", "email_address": "", "orcid_id": ""}, {"name": "Steven P. Rhodes", "author_profile_id": "81332523446", "affiliation": "University of California, Berkeley and Bell Laboratories, Greensboro, North Carolina", "person_id": "PP31036680", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512927.512932", "year": "1973", "article_id": "512932", "conference": "POPL", "title": "Practical syntactic error recovery in compilers", "url": "http://dl.acm.org/citation.cfm?id=512932"}