{"article_publication_date": "10-01-1973", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1973 ACM 0-12345-678-9 $5.00 Labelleci PrecedenceParsing Mario Schkolnick Carnegie-Mellon University \n Abstract Precendece techniques have been widely used in the past in the construction of parsers. However, \nthe restrictions imposed by them on the grammars were hard to meet. Thus, alteration of the rules of \nthe grammar was necessary in order to make them acceptable to the parser. We have shown that, by keeping \ntrack of the possible set of rules that could be applied at any one time, one can enlarge the class of \ngrammars considered. The possible set of rules to be considered is obtained directly from the information \ngiven by a Iabelled set of precedence relations. Thus, the parsers are easily obtained. Compared to the \nprecedence parsers, this new method gives a considerable increase in the class of parsable grammars, \nas well as an improvement in error detection. An interesting consequence of this approach is a new decomposition \ntechnique for LR parsers. ~ Introduction Among the large variety of techniques used for parsing, one \ncan distinguish the bottom-up parsers, as those which attempt to make succesive reductions on a given \nstring so as to eventually get to the starting symbol of the grammar. These parsers can be thought of \noperating in two modes (or phases). On the detection phase, the parser attempts to determine the portion \nof a right hand side of aphrasewtthin the string which is being considered. Once this boundary is detected, \nthe parser goes into a reduction phase, consisting of selecting a production which is a handle at the \ndetermined position. If we classify different types of bottom-up parsers according to the amount of information \nthey carry while in the detection phase, we can distinguish two extremes. On one hand we have the precedence \nparsers, which are characterized by the fact that they carry no information while Iooklng for the righthand \nside of a phrase and by making its decisions in the reduction phase by using local context only. The \nparsers obtained are relatively simple but the classes of grammars they can parse is restricted by the \nexistance of local ambiguities. By varying the amount of context examined one can define different families \nof precedence grammars. Among the most popular ones, we have the Wirth-Weber precedence [1], the simple \nweak precedence [2,3], and the simpte mixed strategy precedence [2]. On the other side of the spectrum \nlie the LR(k) parsers [4]. While in the detection phase, they carry enough information so that the decision \nto reduce can be made immediately after a right hand side is detected. The number of states an LR(k) \nparser has can become Imimense. Part of this high number of states is due to the fact that different \ninformation that is carried forwarcl has to be further distinugished for the same local context. An intermediate \nsituation is obtained if one separates what is to be considered information which has to be carried forward \nand information that can be obtained from local context. A parser thus constructed will consist of two \nmachines: a forward machine J and a decision machine D. The parser will work as follows: Initially the \ncontrol is given to the 7 machine. While on T, the parser behaves like a precedence parser but every \ntime it shifts an input, it stores in the stack the input symbol together with a symbol denoting the \nstate It is currently in. The decision to shift, which is accompanied by a transition to a new state, \nis done by examining local context. The T machme can also cletermine acceptance, an error condition or \na call on the D machine for a decision. The D machine determines whether a shift or a reduce has to be \nperformed, by examining local context together with the state information that exists on the pushdown. \nA shift is performed like the T machina. If a reduce is called for, the right hand side of the production \nused is removed from the st?ck, the T machine is initialized to the state denoted by the topmost symbol, \nand the left hand side of the production used is given as input to it (this is like an LR(k) parser). \nA parser of this type is given in Example 1. Example J_ Let G be given by: S + cAlbB A -ad~a,~. B A adlaB \n G is not a member of any of the classes of precedence grammars mentioned above. An LR(I ) (or an LR(0)) \nparser for G has 10 states. We can see that we really need 2 states to carry information forward (i.e. \nwhether a c or a b was first seen). The rest of the information can be determined from local context. \nA diagram for the 7 machine could be: Definition ~ Let X,Y < V. Let al, az, a3, aL s l.. Then, The D \nmachine would check the contents of the stack to match a right hand side of a subset of the productions, \ndetermined by the state of T from which it was called and it would give a decision on which reduction \nto make. A diagram for D can be given as a forest: Called from: 1 2 reduce reduce reduce reduce reduce \nreduce S CA A ~aA A-ad S -bB B -aB B-~ad In this paper we examine parsers built using this approach. \nDifferent classes of parsable grammars can be obtained by applying different cnterla for the construction \nof the T and 32 machines. We will see that any class of precedence grammars can be extended this way, \nwithout a significant complication of the parsers and wih the blg advantage of not having to accommodate \nthe rules of the grammar to satisfy the requirements of the particular precedence method used. Although \nthe intent of this study was to extend precedence parsers, we get as a side effect a decomposition method \nfor LR(k) parsers. This approach is a matter of further study. 2. Labeled Precedence Parsing In this \nsection we examine the construction of different parsers and the classes of grammars they can parse. \nWe assume the reader is familiar with the terminology for context free grammars [7,8]. Since our original \nattempt was in the direction of extending precedence techniques, all the grammars considered here will \nbe proper. Extensions to non A-free grammars can be studied along the same lines. Definition ~ A~ context \n&#38; grammar G= (V,VT,P,S) is a reduced, A-free, cycle-free context free grammar. V denotes the vocabulary, \nVT is the set of terminals, VN is the set of nontermmals. We assume the productions in P are indexed. \nThe set I of indices will consist of symbols of the form Ak where A < VN. An index i=A~ < I will denote \nthe k-th production whose left hand side is A. If this production is A..&#38; we will write I: A -. S \n(or AK: A -6). If there is only one production for nonterminal A we will use A instead of Al as Its index. \nThere will be an index @ to denote an augmented production of the form S -.1S1 (S < V). (This is just \na convenience to make definitions simpler. ) Except where otherwise noted, the following conventions \napply throughout the paper: A,B,C,D < VN ; a,b,c,d,e,g,r < VT; (3,1J,&#38;,c, p,tf,P,0-,T ( V*; X,Y,Z \nc V We will now define certain relations between pairs of symbols in V. These relations will be defined \nin a similar way as was done in [ 1 ] but there will be a label attached to them. The labels will provide \ninformation about the way the relation between the symbols was obtained. 1 ) X is less than Y * al, az, \nwhich we will write as [ml; az] :X <Y, if Vi ~ 3 AE?XPti i:A -i PXBti and al, >29>> such that w={jl B&#38; \nCm, j: C-*y T). 2) X is equal than Y -a3, which we will write as [a3]:X* Y,ifa3= {ili:A-, pXYv} 3) X \n~ greater ~ Y m ai, which we will write as[a4]: X> Y,if Y< VT,3i <I, i: A-. pBDv, D&#38;YP and ~L={jl \nB&#38;o-C, j: C+ TX} Notice that, ignoring the labeling, the relations are ciefined as in [ 1]. Example \n2 shows a grammar together with a matrix of Iabelled relations. Exanmle &#38; Let G be defined by the \nproductions Sl: SAbZg Y: Y-. ag S2: S -~ crY Z:Zra S3: S -. brX X:Xa The Iabelled precedence relations \ncan be displayed in matrix form: sb c g s [;:+ Y [s2]:> z [s,]:~ x [s3]:> a [Y]:+ [x]) [z]:) [Y,s@$3 \n.L [0]:+ [0;S1,S3]:< [0;S2]:< [s3]:+ c [s+ r [s2]:= [s3]:4 [sZ;Y]:< [S3;X]:< [Z]:A (We have listed the \nelements of the sets a, instead of using the usual set notation. ) The matrix of Iabelled precedence \nrelations will be denoted by M. Note that for two symbols X and Y there may be more than one parr of \nlabels al, a~ such that [cxl;a~]:x<Y. We will later perform reductions on this matrix. These will amount \nto merging some indices into one. We can think of the set of labels as coming from a set L and having \na mapping 9 :1-~L. The or!ginal matr{x is defined with L= I and y 1-1. In general though, we will have \na Iabelled precedence matrix M with labels from a set L. Given a Iabelled matrix of precedence relations \nwe now define a parser for the grammar. The (forward) states of the parser will be subsets of L. Informally, \nthe parser can be defined as follows: Define SF (s,(X,Y)) = (sfla3) U U a2 a directed graph whose nodes \nare the members of V (plus srtal#+ two other nodes, denoted by 1, one of them will be the unique source \nnode, the other, the unique sink node in the The D machine can be defined in different ways, giving graph). \nAn arc exists between nodes X and Y if the X-Y rise to different classes of parsable grammars. We will \ngive entry of the M matrix is not empty. The initial state will be some definitions here. For simplicity, \nwe will restrict to local the set consisting of the label for production 0, and we will contexts of one \nsymbol, but these constructions can be say it is incident to the source node 1. Now we perform extended \nto other contexts. We will need some definitions the following operation at every node: Let state s be \nwhich we now give: incident to node X and let there be an arc from X into Y. Let [a~a2]:X <Y and [aS]:X*Y. \n(There may be more than Definition &#38; Let SW+. We denote by ft an operator one label of the form [a1;a2] \nfor the < relation.) We then such that fkS is the longest prefix of &#38; of length <k. We define a state \nt incident to node Y as s (l a3 together with denote by fk* an operator such that fk*&#38; = {fk PI &#38;&#38;P]. \nthe set of all indices of productions in az such that s (1 al#&#38; Sirniltn4y we define lk&#38; for \nsuffix strings. The state t will be referred to as the successor of state s. When no new states are \ncreated the process stops. Note Let (Z,s) be an interior symbol of a 2-channel stack (i.e.t that the \ncomputation of the states is done using only the stack is W=(Y1,WZ),IVII=IVZI> 1, and for some n > 1J \nboolean operations on sets and that checking if a state has fllnvl=z, fllnvz=s). already been created \nis straightforward. (The whole process can be viewed as a parallel operation at all nodes.) Let i:A +S \nbe the production whose index is i. If [al; a2] : Z < flS, snal+% ACa2 we say that Ohe The set of states \nso created constitutes the set QF of distinguished occurrence of) Z w !&#38;! production ~. states of \nthe IF machine. The underlying fsa will be called the unrestrictedTmachine. The parsing of a word proceeds \n, If 3n>l, Invl=fnzs=zs and (the distinguished as follows: Initially the T machine is in the initial \nstate so, occurrence of) Z leads into production i then (the incident to node 1. There is a stack which \nwill have two distinguished occurrence of) S is a VYMCJ excmnsion d channels, subsequently referred as \nWI and Y2. b lC(V U {1})*, production ~. V2<QF*. Initially V1=1,V2=S0. Let Y1=lVX for some WV*, V2={k3]crs \nfor O-CQF*, IYI=Io-1, be the contents of the stack If [al;uzl:X KY or [as]:X~Y and for some state s, \nat some point in the computation. (Thus the T machine is in sfl(a1Ua3)#@ then we will say that X !!z&#38; \nti y ~. -state s incident to node X.) Let Y be the next input symbol We will write [s]:XJY. (normally \nthis is the next symbol in the input string). Let [U4]:X >Y. If SMX4=$, a shift is performed. This consists \nin If i<a and [a]:X~Y we will sometimes write (i):X+Y. A changing state to the successor state t of s \nand pushing in similar convention holds for the other labels. the stack the symbols Y on the first channel \nand t on the second. If s(ta4#$ we say that a potential conflict occurs. Now we can give a definition \nfor the D machine. The D The set of all productions whose indices are in machine is specified as follows: \nsfl(a4Ua3Ua1), for all al, is made available to the D machine which (hopefully) will give a unique decision \nof what to do. a: ~ 3i, ~icsna4, i: A-@X, n=l(3Xl+l, lnYl=Zf3X and Z leads into i then reduce i ; 2 The \nD machine will either determine a shift, by examining productions in sft(a3Ua1), or a reduce to one of \nb: {$ il~i6t=(slta3) U U a2 , i:A--@XC&#38; Y<fl*C, the productions in s(tai. If a shift is determined, \ncontrol is sUaj#$ transferred to the succesor state of s in the machine 7. If n=l(3Xl+ 1, lnlfl=Z~X and \nZ leads into i} a reduce is determined, the right hand side of the production being reduced is popped \nup from the stack, control is (when D is called, the parser has Y as input and transfered to the topmost \nstate now appearing on channel Ifl=o-x) 2, and the input symbol fed to machine T is the left hand side \nof the production used. The parser accepts If the input This D machine works as follows: For each production \nsymbol is 1, T is in its final state and YJ=lS. i:A ?S in sfla4 it checks that S appears as a valid expansion \nof i. If so, machine D outputs reduce i . AIso, it my We will now define the 7 machine. output a state \nconsisting of the set of all labels of productions i:A-(3XC&#38; such that Y< fl*C, [s]:X -Y and such \nT is a finite state machine, that (3X appears as a valid expansion of i. Thus, the D T=(QF,VXV,SF,Y-l( \n0),{ Y-l(O)]), where QF is a subset of the machine could produce more than one output. We are set of \nall subsets of L, VXV is the input alphabet, the initial interested in deterministic behavior so we will \nsay that a (and final) state is the set containing ~-1(0) and &#38;F is parser is well defined if the \nD machine has at most one defined as follows: Let s<QF,(X,Y)6VXV. The (X,Y) entrY of output. (An empty \noutput from D is an indication of error.) M contains labels [al;az], [a3],[a4] (there may be many labels \nof type [a1,a2]). The class of grammars which have deterministic parsers whose D machine are defined \nas above and whose Y machines have n states will be called the class of _ Iabelled precedence wammars \nw~ independent la @ right context (n-LPI grammars). (~ in the range of &#38;r is interpreted as a call \nto machine Let us compute the machines $ and D for the grammar D). The empty state is interpreted as \nan error indication. in Example 2: The transition function for the unrestricted T machine is T machine \nStates (X,Y) {a} {s1,s31 {s2} {s1} U,S3) w-] {y} {s3} 1S {0] lb {s1,s3} lC {s2] bZ {s1] br {LS31 cr \n{s21 Zg {s1] rY {s2} rX {s3] ra {Y] {X,z} gl D(sl) D(Y) Y1 3)(s2) xl D(S3) D(z) {Y] D(x) Whenever a \ncall to the D machine is given, the set of all i such that ~i<sft(a4Ua1Ua3) is given. The D machine can \nbe represented as a forest where the root of each tree is Iabelled by an element I of L and the corresponding \ntree represents all right hand sides of productions i such that ~i=l. In this case, L= I and !f is 1-1 \nso there is one tree for each production. Jj+JifJj  reduce S1 reduce S2 reduce &#38; reduce Y reduce \nZ reduce X The parsers constructed as above will be such that their T machines usually have more states \nthan it is necessary. We can get minimal machines T as follows: Assume we have a definition for the class \nof 3) machines. We then define an incompatibility relation on the set of productions I. We will say that \ntwo productions II, iz, are incompatible if when a call to 32 occurs with state s=~il=~iz, D will produce \nmore than one output. Once we have determi,~ed all incompatible pairs of productions we will define a \nnew set L and a new function Y such that if il and iz are incompatible then ~il+ !J i2. (In other words \nwe are defining an equwalence relation on I.) Note that a call to D occurs whenever there is an entry \nin the matrix M containing a relation >. The incompatibilities are defined below. Let # denote incompatibility \nbetween productions. 1) Ai#Ck if 3X,Y such that (Ck;B,):X<Y, (A,):X~y, A i ,AAPXY@Z , Bj:B sY(3ZV and \n(Al ):Z>W for some Wtfl v or U=A and 3W such that (A,, Bj):Z>W. 2) Ck $D. if there are productions A, \n:A-,Y(3Zti, 13j :B-Y(3Z, there is V such that (Ck;A, ):V<Y and (Dm;Bj):V~y and (6 I )G!)w for some Wcfl \nu or TJ=A and 3W such that (A,, Bj):Z>W. Given the set of incompatible productiorw, we can define a partition \nn on the set of productions such that if il,iz are incompatible productions they belong to different \nclasses. For each class we define a symbol. Let L be the set of all these symbols and define the natural \nmap ~:1 -L such that $ i=~j if i and j belong to the same class of n. We can now define the T and D machines \nas before. For some partitions rt it may happen that Z) will not be well defined. But if the parser defined \non the identity partition was well defined, there exist a partition for which the parser is well defined \nand for which the number of states of the machine T is minimal. This number gives an indication on the \namount of information that has to be carried forward in order to successfully parse the sentences of \nthe language generated by the grammar. It is clear that, for each n, we can define grammars for which \nthe T machine will have at least n states, so this gives a measure of the complexity of the grammar. \nAs the following result shows, even the simple class of grammars in this hierarchy, i.e., those for which \nthe number of states of the T machine is 1, IS an extension of the largest class of grammars defined \nusing precedence relations over VXV, i.e., the class of simple mixed strategy precedence. Theorem J The \nclass of SMSP grammars is contained in the class of 1 -LPI grammars. =: Let G be a SMSP grammar. Assume \nthere are two productions A I :A -~PXYQZ, B, :B -Y(3ZV. Let !JfA and kf/<fl*~nVT. Since Z<W or Z+W we \ncannot have Z>W. In particular, we cannot have (AI ):Z>W. If ti=A we cannot have X+6 or X <B so, in particular, \nthere is no index Ck such that (Ck ;8, ):X <Y. SO no incompatibilities of type 1 can occur. If there \nare two productions A, :A-sYfiZv, Bj :Y(3Z then again, if U*A there can be no W<fl*~flVT such that (Bj \n):Z~W. If V=A then A, and Bj have identical right hand sides. So, there is no V such that (V,A)t <U* \nand (V, B){ <U% In particular, there are no Ck, Dm such that (Ck;A, ):V<Y and (Dtn;Bj):V<Y. Thus no incompatibilities \nof type 2 occur. Thus, we can define T with one state. It is easy to see the D is deterministic. 1 The \nclass of 1 state Iabelled grammars with independent left and right context has been presented in the \nliterature under another name as indicated by the following result. Theorem 2: The class of 1 state Iabelled \ngrammars with independent left and right context coincides with the class of overlap resolvable (OR) \ngrammars [5]. w The reader is referred to [5] for the definition of OR grammars. A case analysis shows \nthat D has a deterministic behavior iff every conflict is left or right resolvable. ~ Thus we get the \nfollowing corolary, which answers a conjecture of Wise: Corolary J_: The class of OR languages coincides \nwith the class of deterministic languages. Proof. Follows from the fact that every deterministic langu~~as \nan SMSP grammar. # Example 2 presented a grammar which failed to be OR. There are two entries in M which \ncan cause incompatibilities, namely M(a,g) and M(g,l). For the latter we have that productions Y and \nS1 are not of the form occuring in case 1 or 2 for the definition of incompatibility. For the former, \nwe do have that Sz#Z. Thus, at least 2 states are required for the ? machine. It turns out that 2 states \nare sufficient to get a parser for this grammar. Because we have defined the D machine as one which \nchecks left and right context indepen dently we have the following result. Theorem 3: For any n, the \nclass of n-state Iabelled grammars with independent left and right context is properly included in the \nclass of SLR( 1 ) grammars [6]. W Given the set Qo of sets of LR(0) items for a grammar and the set QF \nof states of the unrestricted T machine, we can define a mapping h from Q. to QF as follows: h(So)={k3]. \nLet S, be a set of LR(0) items. For each symbol Y<V we can partition Si in 5 sets, Si=Sll U Si2 U S,3 \nU S14 U Sis, Sil={A-aX.Y~}, S, Z={ A-aX.Z~lZ+Y}, S i3={A+cxX.], S i6~{A-.Y(3], Sls={A *. Z(31Z#Y~. If \nh(Si)=qj then h(S(S 1,Y))=S (q i ,(X,Y)), where S is the transition function of the unrestricted T machine \nand S(S ,,Y)=S j is the set of LR(0) items obtained as the GOTO(S, ,Y) (see [7] for undefined terms). \nNow we make the following claim. CJa&#38;l: If S, is a set of LR(0) iterns partitioned as above, then \nh(S 1) contains the indices of all productions in Sil U S12 US,3. The claim is certainly true for So \nbecause S01=$02=S03=$. Now, assumming the claim holds for S i, we note that GOTO(S i ,Y) is obtained \nby taking all productions in S 11 U S 14 with the dot shifted over the symbol Y (which becomes the set \nS, 1 U S,2 U S,3), and applying a closure operator to get the set S J4 U S js. But, for every index i \nof a production in S I J we have (i):X+Y, and for every index j of a production in S, d, there is an \nindex i of a production in S i 1 IJ S, 2 such that (i;j):X<Y. Thus, all indices of productions in Sjl \nU S,2 U Sj3 appear in state h(Sj) and the claim holds. It is now straightforward to verify that if G \nis not SLR( 1 ), i.e., if there are two conflicting items in some set S I of LR(0) items, then the corresponding \nstate of the T machine will produce a call of the D machine which will in turn, give more than one output. \nThus the parser will not be a deterministic one and the grammar will not be an n\u00adstate LPI grammar. 1 \nWe note that to generate the T machine we do not distinguish positions within a production, as an LR(or \nSLR) parser does. Thus, we are able to get the T machine faster, but we restrict the class of grammars \nwhich can be parsed, excluding those which have productions in which a repeated occurrence of a symbol \nmay cause problems, as suggested by the following example: Example ~. Let G have productions S-~abcabA \nI abB A-ad B-+d Since [~;sl,s~]:l~a, [SJ3~]:a+b and [Sl;A]:b<d, [Sz;B]:b<d and [A, B]:d>l. we have that \nthe T machinecalls the 33 machine when in state {A,B} and reading symbol (d,l), The 3) machine gives \nas output both reduce A and reduce B . This behavior will occur even if the D machine checks the left \nand right context simultaneously as is done later. On the other hand, it is easily seen that G is an \nSLR(l ) grammar. Example 3 leads us to the following definition: Definition ~ Let A~XIX2...XlX,,,, be \na production. We will say that this production is free of repwt~o~ (FOR) if for all 1 Si,j<n we have \ni+j implies X ,#X, (i.e., there is no repeated occurrence of a symbol among the first n-1 symbols). A \ngrammar will be free of repetitions (FOR) if all of its rules are FOR. FOR grammars and FOR productions \noccur very often. Any grammar in normal 2 form is a FOR grammar and every CF language can be given a \ntrivial FOR grammar. Among the grammars used in programming languages, a quick glance at some reveals \nthat: PL360 as defined in [9, pages 39-53] is FOR; SNOBOL4,as defined in [7, pages 595-507], has only \none non FORrule; ALGOL 60, as defined in [ 10], has only one non FOR rule (which happens to be a production \nfor the <for list element>!); PAL, as defined in [7, pages 512-5 14], is FOR. If we are dealing with \nFOR grammars, we can strengthen the result of Theorem 3: Theorem ~: If G is FOR and SLR( I ), then it \nis n-LPI. Proof. Define the 7 machine using the identity map .. ~: I+L=I. If G is FOR, the claim stated \nin the proof of Theorem 3 becomes the following: CM If S i is a set of LR(0) items partitioned as before,then \nh(S i ) coincides with the set of indices of all productions in Sil U Si2 U Si3. To prove the claim, \nit suffices to show that there are no indices of productions in h(S, ) which are not in S, 1 (J S i 2 \nU S, 3. This follows from the fact that, if (i):X*Y or (i;j):X <Y then, since G is FOR, there is only \none occurrence of X in the production whose index is i. Since an LR(0) item is identified by this symbol, \nthe map h is I -1. It is easy to see that the parser constructed is isomorphic to the SLR(1) parser. \n1 Thus, if we restrict our attention to FOR grammars, both classes coincide. Moreover, the SLR( 1) parser \ncan be obtained very easily from the T machine so that a fast procedure for constructing SLR( 1) parsers \nis obtained. AS mentioned above, FOR productions and grammars occur frequently in programming languages. \nThus, we should take advantage of this fact when constructing parsers for them. We will now modify the \ndefinition of the D Imachine so as to make it check for simultaneous left and right context. We need \nto introduce the following definition. Definition 5: A symbol ~ ~ adiacent &#38; SW.!@k ~ id ~ within \nthe context of q production QJ if either 1 ) (C, ):X~Y and either (Cj):Y>Z or (CJ):Y>Z or 2) (CJ;Dk):X<y \nand (Dk):Y->Z for some production Dk. Let A, :A-% be a production and G (A)={BIB&#38;.A}. We say that \nA ~ a valid reduction @.ES within symbols X and ~ and state s if 1) (Cj;Al):X<flS for some C,(S 2) 3Y~(P(A) \nsuch that Y is adjacent to symbols X and Z within the context of production C,. Note that we can check \nthe condition of valid reduction by inspecting the matrix M. As the following Ieimma shows, we get information \nabout possible simultaneous left and right context in which a nonterminal may appear. !-emma ~ Let c, \n:C--vxc WV*,COJ+. S%aC@~aVXcfi&#38;u8 XYc fi&#38;aVXYZc , with a,(3,c ,c OJ* et  (but Z~fl*(c (3)) for \nsome Y<F(A) such that P(Y)=+. Then A is a valid reduction for S within svmbols X and Z and some state \ns such that Cj<s. m We know C=+ YXC%YXYC . There are two cases: c=Yc or c+Yc ,c +A (since F(Y)=@). In \nthe first case, (Cj):X~Y. Also, either Z<fl*(c ) or c =A and Z<fl*(~). Then, either (C, ):Y--~Z or (Cj):Y>Z. \nIf c#Yc then 3D,:D-.YP such that c&#38; Dp ~YPP =Yc with p#A. Then Zffl*(V) so (C, ;D, ):X<Y and (D, \n):Y-:Z. In either case, Y is adjacent to symbols X and Z within the context of Cj. Since Y&#38;A*&#38; \nwe have (C j ;A i ):X <fl&#38; where A, :A JS. Thus we have that conditions 1 ) and 2) of definition \n5 are satisfied. # We are now in a position to specify another class of parsers, by changing the D machine. \nThe change will only affect the instruction label led a. This instruction is changed to: a: ~ ai, ~iMia4, \ni:A--(3X, n=l(3Xl+l, lnY1=Z(3X, Z leads into i and A is a valid reduction for (IX within symbols Z and \nY and state s, where s= flln V2 (i.e., the state which appears next to Z) @ reduce i . We will now construct \na parser for a grammar using this machine D. Example: Let G be Sl: S--~Aa S3: S-Bb A: A-c S2: $,dAb \nS4: S -,dBa B: B-AC The matrix M is: sABa bc dl s [0]:+ [s1]:+ [s2]:4 [s~]:~ [s3]:+. [s1,s4]:) ; [S*,S3]:) \n[A,B]:> [A,B]:> ; [s2]:+ [s4]:= [S4;B]:<,[S2A]:< 1 a]:~ [0;s1]:< [43;s3]:< [O;A,B]:< [0; S2,S4]:< The \nmachine T is: {@] {s1} {s3] {W} {SZ<S/I} {S2} {s4] 1s] {g} 1A lB lC -1-d s-l Aa Ab {s2] Ba {s4} E3b \n{s3} ca 3X{ A, B)) cb D({A,B}) dA {s2) c~El {s4) dc {A,B] al 3X{S1}) D({S4)) bl D({S3]) D({S2)) The forest \nfor machine D is as follows: {s11 {s2} {s31 {s4} {A,B] {A,B} T T T 1 al bf :1RI :1 :1 .1 .1 dd reduce \nSlreduce S2 reduce S3 reduce S4 d:reduce B reduce A l:reduce A reduce B When 33 is called with {A,B} \nit knows its lookahead symbol. Assume it is an a . Then it checks that the stack contains c and looks \nat the left context. If it is a (d,{ S2,S4]) it checks to see if A or B are valid reductions of c within \nd and a and state {S2,S4). From the matrix M we see that B is valid while A is not Thus the output reduce \nB is given. We could proceed as before and give a criteria for incompatible productions. We will not \ndo this here, but k clear we again get a hierarchy depending on the number of states the 7 machine has. \nIn the above example we really didn t need the states in the T machine in order to decide the output \nfor the D machine. Thus, we could have built a parser with 1 state in the T machine. Actually, we have \nTheorem @ The class of 1-state Iabelled precedence grammars with simultaneous left and right context \nis properly included in the class of (1 -1 )BRC. If the grammars are restricted to be FOR, these classes \ncoincide. M: Because the D machine can check for context of at most one to both left and right of the \nright hand side of a production we have that we are within the (1 -1 )BRC. The following grammar is (1 \n-1 )BRC but not in the class of Iabelled precedence grammars considered: S-+aAbAc\\aBc A-d B~d It thus \nremain to be shown that any FOR grammar which is (1 -1 )BRC is in this class. This follows from the fact \nthat for a FOR grammar, the converse of lemma 1 holds, i.e., if A is a valid reduction for &#38; within \nsvmbols X and Z then XAZ is a substring of some sentential form. Thus, if the D machine gwes more than \none output, it means that knowledge of the left and right context of a handle of a sentential form does \nnot uniquely determines it. Thus, G is not (1 -l )BRC. ~ ~ decomposition ~ LR parsers SO far, we have \nconsidered parsers which operate as precedence parsers, in the sense that, once a reduction could occur \n(as determined by the 7 machine) we would check the contents of the stack to either determine the production \nto use in the reduction, or to continue the forward scan. This sequentiality of actions is clearly not \nnecessary. Since the D machine, when called, only inspects a bounded amount of tape (not more than one \nplus the length of the longest right hand side of any production), we can construct a (definite) machine \nwhich can operate in parallel with the T machine and which performs the checking that D does. (We will \nalso refer to this new machine as the T) machine.) In this way, the decisions are already taken when \nthe T machine requests them. Now the parser is behaving exactly as an LR parser, but since we have separated \nthe functions in the Y and D machines, the total number of states is reduced. As an example of these \nideas, consider the following grammar: s: s-->DADB 131: B-c D: D+aC B2: B-+d Al: A-+b cl: C-.Ce Az : \nA-c C2: C-e From the M matrix we can determine the incompatibilities. We find there are none. Thus one \nstate is sufficient for the T machine. (In fact, G is an OR grammar,though not an SMSP). The Y machine \nis obtained directly from the matrix of (unlabeled) precedence relations. It has only one state, which \nisdenoted by a. Acail toDis denoted by D. Input Action Input Action Input Action 1Sa aea CAD lDa ADa \nCbD laa Aaa ccD S1e-BlD CdD DAa bDD Cea DBa baD eA0 Dba CDD ebD Dc a ca1) ecD Dda clD edD aCu dlD eeD \nTo obtain D we reduce (using standard techniques of finite stale machines) the machine which checks all \nproductions. Since there is only one state in T, the only information 3J has, to determine its output, \nis the input from which it is called from T. The following is the transition table for 33. It has 5 states. \nNotice that the input to D is taken as the second component of the input to T (i.e., the new input symbol, \nnot the one already on top of the stack). The output depends on both. string. Since T has 1 state we \ndo not show it on the stack. The state of 3) appears as a second component. Stack Input Action of machine \nTD 1 aebaecal 1 shift la ebaecal 11 shift lae baecal 112 D reduce C2 laC baecal 112 T) reduce D lD baecal \n13 shift lDb aecal 131 D reduce Al lDA aecal 134 shift lD Aa ecal 1341 Shift lDAae cal 13412 D reduce \nCz lDAa C cal 13412 D reduce D lDAD cal 1345 shift lDADc al 13451 D reduce AZ lD ADA al 1345X error \n Next state, under new symbol. Stat ABC Da bcde Output -i .\u00ad 231 ---2 (B,-):S (b,-):A1 (e,-):CZ (c,a):A2 \n(C,l):B1 (d,-):B2 2 --\u00ad --\u00ad .\u00ad 1 (C,-):D (e,-):Cz 34X ---11x\u00ad 4 51 ---\u00ad 5 x l---xii\u00ad (A don t care entry \nis shown as -. An error entry is shown as x.) The following example shows a sequence of configuration \ntaken by the parser when given an input Had the last symbol a not been there, the last two configurations \nwould have been changed to: 1 DADc 1 13451 D reduce B1 lDADB 1 13451 D reduce S 1s1 @ It is interesting \nto note that this grammar has an 18\u00ad state LR(l) parser (constructed a la Knuth)~ a 14-state parser (using \nKorenjak s method [11]), and a 10-state SLR(l) parser. By allowing the parser to postpone error detection \n(as the one above does), Aho and Unman constructed a 7-state parser [7]. We have shown that using decomposition \ntechniques one can get a 1+5-5tate parser for this grammar. Because of the simple way the T and D machines \nare determined, this decomposition technique appears quite useful. We should point out here that, although \nnot explicitly mentioned, a similar decomposition technique appears in [12]. ~ Conclusions Keeping track \nof the possible productions which can be in use at any one time during the operation of a precedence \nparser can significantly enlarge the class of grammars to which it applies. We have shown how to obtain \nsuch parsers and given some ideas about their relative power. An additional feature over conventional \nprecedence parsers is the improved error detection capability. The fact that we have more than one state \nduring the detection phase allows the parser to discover errors before they are detected by conventional \nprecedence parsers. In fact, these parsers Iook very much like LR parsers, but areeasier toobtain, and \nthey are considerably smaller than these. By reversing the machine which decides which reduction to perform \nwe were able to get parsers which are equivalent to LR parsers obtained using error postponement techniques \n[7] but, again, at a substantial savings in the number of states. More work is needed concerning this \nmethod of LR decomposition. References 1. Wirth, N. and H. Weber [1 966], EULER -a generalization of \nALGOL and its formal definition, Parts 1 and 2, Comm. ACM 9:1, 13-23 and 9:2, 89-99. 2. Ichbiah, J. \nD., and S. P. Morse [1970], A technique for generating almost optimal Floyd-E~ans productions for precedence \ngrammars, Comm ACM 13:8, 5!21 -508.  - 3. Aho, A. V., P. J. Denning, and J, D. Uliman [ 1972], sWeak \nand mixed strategy precedence parsing, w 19:2,225-243 4. Knuth, D. E, [ 1965], On the translation of \nlanguages from left to right, Information and Control 8:6, 607-639. 5. Wise, D. S. [1971], Domolki s \nalgorithm applied to generalized overlap resolvable grammars, Proc. Third Annual ACM Symo. on Theory \nof Computin&#38; 171-184. 6. DeRemer, F. L. [ 1971], Simple LR(k) grammars, Comm ACM 14:7, 453-460. \n - 7. Aho, A. V. and UIlman, J. D. [1 972-3], The Theory ~ ParsinR, Translation @ Compiling, Prentice-Hall. \n 8. Ginsburg, S. [ 1966], ~ Mathematical Theory of Context-Free Languapes, McGraw-Hill, New York. 9. \nWirth, N. [ 1968], PL360 -a programming language for the 360 computers, _ 15:1, 37-74. 10. Naur, P. \n(cd.) [ 1963], Revised report on the algorithmic language ALGOL 6!3, Comm. ACM 6:1, 1-17. 11. Korenjak, \nA. J. [ 1969], A practical method for constructing LR(k) processors, Comm. ACM 12:1 1, 613\u00ad  623. 12. \nHarrison, M. A. and Havel 1. M., On the parsing of deterministic languages, to be published. \n\t\t\t", "proc_id": "512927", "abstract": "Precendece techniques have been widely used in the past in the construction of parsers. However, the restrictions imposed by them on the grammars were hard to meet. Thus, alteration of the rules of the grammar was necessary in order to make them acceptable to the parser. We have shown that, by keeping track of the possible set of rules that could be applied at any one time, one can enlarge the class of grammars considered. The possible set of rules to be considered is obtained directly from the information given by a labelled set of precedence relations. Thus, the parsers are easily obtained. Compared to the precedence parsers, this new method gives a considerable increase in the class of parsable grammars, as well as an improvement in error detection. An interesting consequence of this approach is a new decomposition technique for LR parsers.", "authors": [{"name": "Mario Schkolnick", "author_profile_id": "81100615043", "affiliation": "Carnegie-Mellon University", "person_id": "PP39050902", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512927.512930", "year": "1973", "article_id": "512930", "conference": "POPL", "title": "Labelled precedence parsing", "url": "http://dl.acm.org/citation.cfm?id=512930"}