{"article_publication_date": "10-01-1973", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1973 ACM 0-12345-678-9 $5.00 A UNIFIED APPROACH TO GLOBAL PROGRAM OPTIMIZATION Gary A. Kildall Computer \nScience Group Naval Postgraduate School Monterey, California Abstract A technique is presented for global \nanalysie of program structure in order to perform compile time optimization of object code generated \nfor expressions. The global expression optimization presented includes constant propagation, common subexpression \nelimination, elimination of redundant register load operations, and live expression analysis. A general \npurpose program flow analysis algorithm is developed which depends upon the existence of an optimizing \nfunction. The algorithm is defined formally using a directed graph model of program flow structure, and \nis shown to be correct, Several optimizing functions are defined which, when used in conjunction with \nthe flow analysis algorithm, provide the various forms of code optimization. The flow analysis algorithm \nis sufficiently general that additional functions can easily be defined for other forms of globa~ cod: \noptimization. 1. INTRODUCTION of the graph represent program control flow Dossi\u00adbilities between the \nnodes at execution time. A number of techniques have evolved for the compile-time analysis of program \nstructure in order to locate redundant computations, perform constant computations, and reduce the number \nof store load sequences between memory and high-speed registers. Some of these techniques provide analysis \nof only entry o~:=j straight-line sequences of instructions [5,6,9,14, 17,18,19,20,27,29,34,36,38,39,43,45 \n,46], while B others take the program branching structure into &#38;() account [2,3,4,10,11,12,13,15,23,30, \n32,33,35]. The purpose here is to describe a single program c flow analysis algorithm which extends all \nof b=2 these straight-line optimizing techniques to in- D clude branching structure. The algorithm is \npre\u00ad d= a+b sented formally and is shown to be correct. Im\u00adplementation of the flow analysis algorithm \nin a practical compiler is also discussed. e=btc The methods used here are motivated in the F section \nwhich follows. C:=4 iii 2. CONSTANT PROPAGATION A fairly simple case of program analysis and optimization \noccurs when constant computations Figure 1. A program graph corresponding to are evaluated at compile-time. \nThis process is an ALGOL 60 program containing one loop. referred to as constant propagation, or folding. \nConsider the following skeletal ALGOL 60 program: For purposes of constant propagation, it is convenient \nto associate a pool of propagated = integer i,a, b,c, d,e; constants with each node in the graph, The \npool a:=l; c:=O; . . . is a set of ordered pairs which indicate variables ~ i:=l steu 1 until 10 @ which \nhave constant values when the node is encoun\u00ad-bb:=z; . tered. Thus, the pool of constants at node B, \nde\u00ad :\u00ad;... noted by PB, consists of the single element (a,l) e:=b+c; . . . since the assignment a:=l \nat node A must occur c:=4; ... before node B is encountered during execution of end the program. end \nThe fundamental global analysis problem is This program is represented by the directed graph that of \ndetermining the pool of propagated constantsshown in Figure 1 (ignoring calculations which con\u00adfor each \nnode in an arbitrary program graph. Bytrol the for-loop). The nodes of the directed inspection of the \ngraph of Figure 1, the pool ofgraph represent sequences of instructions contain\u00adconstants at each node \nising no alternate program branches, while the edges pA=@ PD = {(a,l), (b,2)} Using P;, the constant \npool resulting from node F PB = {(a,l)} PE = {(a,l), (b,2), (d,3)} entering node C is Pc = {(a,l)} PF \n= {(a,l),(b,2),(d,3)} . P = {(a,l),(b,2),(d,3),(e,2),(c ,4)}. In the general case, PN could be determined \nfor Note, however, that since each node N in the graph as follows. Consider each path (A,p ,p2,. ..,Pn, \nN) from the entry node A to then.de~. Apply constant propagation throughout this path to obtain a set \nof propagated constants at node N for this path only. The intersection of the propagated constants determined \nfor each path to N is then the set of constants which can be assumed for optimization purpoees, since \nit is not known which of the paths will be taken at execution-time. The pool of propagated constants \nat node D of Figure 1, for example, can be determined as follows. A path from the entry node A to the \nnode II is (A, B,c,D). Considering only this path, the first approximation to PD is P1 = {(a,l), (b,2), \n(c,0)} D A longer path from A to D is (A,B,C,D,E,F,C,D) which results in the pool P; = {(a,l),(b,2), \n(c,4), (cl,3), (e,2) } corresponding to this particular path to D. suc\u00adcessively longer patha from A \nto D can be evalu\u00adated, resulting in Pa, pi, . . ..pfi for arbitrarily large n. The pool of propagated \nconstants which can be assumed no matter which flow of control, occurs is the set of constants common \nto all P%; that is, ni PD = D Vi This procedure, however, is not effective since the number of such paths \nmay have no finite bound in the case of an arbitrary directed graph. Hence, the procedure would not \nnecessarily halt. The purpose of the algorithm of the following section is to compute this intersection \nin a finite number of steps. 3. A GLOBAL ANALYSIS ALGORITHM The analysis of the program graph of Figure \n1 suggests a solution to the global constant prop\u00adagation problem. Considering node C, the first approximation \nto P is given by propagating constants along th~ path (A, B, C), resulting in 1 = {(a,l), (c, O)}. c \nBased upon this approximate pool, the first approximations to subsequent nodes can be determined: P1 \n= {(a,l),(c,o) >(bsz)}~ D P1 = {(a,l),(c,0),(b,2),(d,3)} E d = {(a,l),(c,0),(b,2),(d,3),(e,2) }. F \nc = n i c vi it follows that Pc ~ P: n P: . Thus, rather than assuming 2Pc = P, the second approximation \nto Pc is taken as P: =P~nP=P~n {(a,l),(b,2), (d,3),(e,2),(c,4)} = {(a,l)}. Using P;, the circuit through \nthe looP Past C is traced once again. The next approximation at sub\u00ad s~quent nodes can then be determined \nbased upon P; n {(a,l), (b,2)} = {(a,l), (b,2)}, P; n {(a,l),(b,2),(d,3)} {(a,l),(b,2),(d,3)} , p; n \n{(a,l),(b,2), (d,3)} {(a,l), (b,2), (d,3)}. Continuing around the loop once again from node F to node \nC, the third approximate pool P; ia determined as p: = p: n {(a,l),(b,2), (d,3)} = {(a,l)}. Clearly, \nno changes to subsequent approximate pools W-ll o cur if the circuit is2traversed again since 35 Pc = \nPc, and the effect of Pc on the pools in the circuit has already been investigated. Thus, the analysis \nstops, and the last approximate pools at each node are taken as the final constant pools. Note that these \nlast approximations correspond to the constant pools determined earlier by inspection. Based upon these \nobservations, it is possible to informally state a global analysis algorithm. a. Start with an entry \nnode in the program graph, along with a given entry pool corresponding to this entry node. Normally, \nthere is only one entry node, and the entry pool is empty. b. Process the entry node, and produce optimizing \ninformation (in this case, a set of propagated constants) which is sent to all immediate successors of \nthe entry node. c. Intersect the incoming optimizing pools with that already established at the successor \nnodes (if this is the first time the node is encoun\u00adtered, assume the incoming pool as the first  approximation \nand continue processing), d. Considering each successor node, if the amount of optimizing information \nis reduced by this intersection (or if the node has been encoun tered for the first time) then process \nthe successor in the same manner as the initial entry node (the order in which the successor nodes are \nprocessed is unimportant). A finite directed graph G = <~, E> is an arbitrary finite set of nodes N an~ \nedges EcNxN. A path from A to B-in G, for A,B c ~,In order to generalize the above notions, it ~s-a seq~ence \nof nodes (p1,P2, . ..Pk)3 PI = A andis useful to define an optimizing function f which maps an input \npool, along with a particular Pk= B, where (pi,p +1)6 ~ Vi, 1 < i . k. The node, to a new output pool. \nGiven a particular length of a path t p1,p2,. ..,Pk) is k-1. set of propagated constants, for example, \nit is possible to examine the operation at a particular A program graph is a finite directed graph node \nand determine the set of propagated constants G along with a non-empty set of entry nodes which can be \nassumed after the node is executed, c SY such that given N c YH a path (Pi,...,Pn)In the case of constant \npropagation, the function 3 p1c8 and pn = N (i.e., there is a path to every can be informally stated \nas follows. Let V be a set of variables, let C be a set of constants, node in the graph from an entry \nnode), and let ~ be the set of nodes in the graph being analyzed, The set U = V X C represents ordered \nThe set of immediate successors of a node N pairs which may appear in any constant pool. In is given \nby fact, all constant pools are elements of the power I(N) = {N elil 3 (N,N ) c~}. set of U (i.e., the \nset of all subsets of U), denoted by P(U). Thus, Similarly, the set of immediate predecessors of f: Ix \nP(u) + P(u), N is given by where (v,c) c f(N,P) <=> I-l(N) = {N E ~lLI(N ,N) e E}. a. (v, c) c P and \nthe operation at node N does not assign a new value to the variable v, or Let the finite set ~ be the \nset of all possi ble optimizing pools for a given application (e.g., b. the operation at node N assigns \nan expression ~ = P(u) in the constant propagation case, where to the variable v, and the expression \nevaluates U = V x C), and A be a meet operation with the to the constant c, based upon the constants \nin properties P. A: P XP+~, . Consider the graph of Figure 1, for example. x A y = y A x (commutative), \n The optimizing function can be applied to node A with an empty constant pool resulting in x A (y A Z) \n= (x A y)A z (associative), f(A,@) = {(a,l)}. where x, y, z c ~. The set P and the A operation define \na finite meet-semilat~ice. Similarly, the function f can be applied to node B with {(a,l)} as a constant \npool yielding The A operation defines a partial ordering on ~ given by f(B, {(a,l)}) = {(a,l), (c, O)}. \nx < y <=> x A y= xVX,Y EP. Note that given a particular path from the entry node A to an arbitrary node \nN e ~, the optimizing Similarly, pool which can be assumed for this path is deter\u00ad mined by composing \nthe function f up to the last x < y <=> x s y S,nd X# y. node of the path. Given the path (A,B,c,D), \nfor example, A Given X c P the generalized meet operation x ~ ~ , is defined simply as the pairwise application \nof f(C,f(B,f(A,@))) = {(a,l),(c,O), (b,2)} A to the elements of X. ~ is assumed to contain a zefo element \nQ3 ~ s x Vx e P. An augmented is the constant pool at D for this path. set ~ is constructed from P by \nadding a unit element ~ with the propert~es ~ ~ ~ and ~ A x = The pool of optimizing information which \ncan be assumed at an arbitrary node N in the graph xvxc~;~ =gu {l}. It follows that x <lVX cP,being analyzed, \nindependent of the path taken at execution time, can now be stated formally as An optimizing function \nf is defined n PN= x f: NxP+P XEI? N where and must have the homomorphism property: FN = {f(pn,f(Pn_la \n..o,f(P1,p)). .,)1 f(N,x A y) = f(N,x) A f(N,y), N 6 ~, X,y c ~. N) is a path from an entry node PI (P1>P2, \n. ..>Pn) Note that f(N,x) <~ VN e~ and x ~~. with corresponding entry pool P to the node N}. Before formally \nstating the global analysis The global analysis algorithm is now stated: algorithm, it is necessary to \nclarify the funda- Algorithm A. Analysis of each particular program mental notions. graph G depends \nupon an entry pool set ~~~ x ~, where (e, x) 6~ if ec C is an entry node with corresponding entry optimizing \npool x e ~. Al[initialize] L+6 A2[terminate ?] If L = @ then halt. A3[select node] Let L ~ L, L = (N$Pj-) \nfor some N <N and Pi c~, L-+L-{~t} A4[traverse?] I,et PN be the current approxi\u00ad mate pool of optimizing \ninfor\u00admation associated with the node N (initially, PN = ~). If PN s Pi then go to step A2. A5[set pool] \np+p Ap., L+Lu {~N ,f~N,~j)lN E I(N)}. A6[loop] Go to step A2. For purposes of constant propagation, ~ \n= P(u), where U = V x C, as before. The meet operation is n, and the less-than-or-equal rela\u00adtion ia \nc. Note that the zero element in this case is T e P(U), The unit element in P(U) is U itself. The algorithm \nrequires a new unit element, however, which is not in P(u). The new unit element is constructed as follows: \nlet 6 be a symbol not in U, and let ~= U u {6}. It follows that~n x= XVX c P(u) and~~ P(U). Thus, ~ .1 \nu {y} is obtained from ~ by adding a unit element ~. Aa demonstrated in the proof in Theorem 2, the addition \nof the symbol 6 to U causes the algorithm A to consider each node in the program graph at least once. \nAppendix A shows the analysis of the program graph of Figure 1 using the entry pool aet ~= {(A,@)}. Theorem \n1. The algorithm A is finite. Proof. The algorithm A terminates when L = 0. Each evaluation of step A3 \nremoves an element from L, and elements are added to L only in step A5 . Thus, A is finite if the number \nof evalua\u00ad tions of step A5 is finite. Informally, each evaluation of step A5 reduces the size of the \npool PN at some node N. Since the size cannot be less than ~, the process must be finite. Formally, step \nA5 ia performed only when PN~pN Api. Ut pNAp$)p~%~Np~~ip;~~. PN APi s PN, and PN Api Thus, the approximate \npool PN at node N can be reduced at most to ~ since P + P A P.. N y _ Further, since the first approximation \nto isll and the lattice is finite, it follows thatNstep A5 can be performed only a finite number of times. \nThus A is finite. An upper bound on the number of steps in the algorithm A can easily be determined. \nLet n be the cardinality of ~ and h(~ ) be a function of P (which, in turn, may be a function of n) pro\u00adviding \nthe maximum length of any chain between ~ and ~ in ~ . Step A5 can be executed a maximum of h(~ ) times \nfor any given node. Since there are n nodes in the program graph, step A5 can be per\u00adformed no more than \nn E h(~ ) times. In the case of constant propagation, for example, let u be the cardinality of U. The \nsize of U varies directly with the number of nodes n. In addition, the maximum length of any chain lY \n2Y-. .,uk such that u = U and Uk = fl, where 1 ul~ U2 ~ U3 . ..~llkisll. Thus, h@(U)) = U; and the theoretical \nbound is n . u. Since u varies directly with n, it follows that the order of the algorithm A is no worse \nthan n2. The correctness of the algorithm A is guar\u00adanteed by the following theorem. Theorem 2, Let FN \n= {f(Pn,f(Pn_l ,...$ f(Pl,p)). .)l(Pl, . . ..Pn.N) is a path from an entry node pl with corresponding \nentry pool P to the node N}. Further, let A XN=X,F; corresponding to a particular program graph G, set \nP , and optimizing function f, which satisfy the ~onditions of the algorithm A, If PN is the final approximate \npool associated with node N when A halts, then PN =~VNcN. Theorem 2 thus relates the final output of \nthe algorithm to the intuitive results which were de\u00adveloped earlier. The proof of Theorem 2 is given \nin Appendix B. An interesting side-effect of Theorem 2 is that the order of choice of elements from L \nin step A3 is arbitrary, as given in the following corol\u00ad lary. Corollary 1. The final pool PN associated \nwith each node N c ~ upon termination of the algo\u00adrithm A ia uniquely determined, independent of the \norder of choice of L from L in step A3. Proof. This corollary follows immediately, since the proof of \nTheorem 2 in Appendix B is independent of the choice of L @ Since the choice of L from L in step A3 is \narbitrary, it is interesting to investigate the effects of the selection criteria upon the algo\u00adrithm. \nThe number of steps to the final solution is clearly affected by this choice, No selection method has \nbeen established, however, to maximize this convergence rate. One might also notice that by treating \naccesses to L as critical sections in steps A3 and A5, the elements of L can be processed in parallel. \nThat is, independent processes can be started in step A3 to analyze all elements of L. It is important \nto note at this point that the algorithm A allows one to ignore the global analy\u00ad sis, and concentrate \nupon development of straight\u00ad line code optimizing functions. That is, if an optimizing function f can \nbe constructed for opti\u00ad mizing a sequence of code containing no alternative branches, then the algorithm \nA can be invoked to perform the branch analysis, as long aa f satisfies the conditions of the algorithm. \n4. COMMONSUBEXPRESSION ELIMINATION Global common subexpression elimination in\u00ad volves the analysis of \na program s structure in order to detect and eliminate calculations of re dundant expressions. A fundamental \nassumption is that it requires less execution time to store the result of a previous computation and \nload this value when the redundant expression is encountered. As an example, consider the simple sequence \nof expressions: ... r:=a+b; ... r+x ... (a+b)+x ... which could occur as part of an ALCOL 60 program. \nFigure 2 shows this sequence written as a directed graph. Note that the redundant expression (a+b) at \nnode v is easily recognized. The entire expres sion (a+b)+x at node v is redundant, however, since r \nhas the same value as a+b at node U, and rtx is computed at node U ahead of node V. It is only necessary \nto describe an optimizing function f which detects this situation for straight-line code; the algorithm \nA will make the function glob\u00adally applicable. entry:T r:.a+b u r+x v kl+b)+x w B Figure 2. An acyclic \nprogram graph representing a simple computation sequence. A convenient representation for the optimizing \npool in the case of common subexpression elimina\u00adtion is a partition of a set of expressions. The expressions \nin the partition at a particular node are those which occur before the node is encoun tered at execution \ntime. The optimizing function for common sub\u00adexpression elimination manipulates the equiva\u00adlence classes \nof the partition. Two expressions are placed into the same class of the partition if they are known to \nhave equivalent values, Con\u00adsidering Figure 2, for example, the set of expres\u00adsions which are evaluated \nbefore node T is encoun\u00adtered is empty; thus, PT = 0. The expressions evaluated before node U are exactly \nthose which occur at node T, including partial computations. The set of (partial) computations at node \nT is {a,b,a+b,r}. Since r takes the value of a+b at node T, r is known to be equivalent to a+b. Thus, \nP ={alb/a+b,r}, where / separates the equival~nce classes of the pool. Similarly, Pv = {a/b/a+b,rlxlr+x} \nand PW = {a/bla+b,rlx/r+x/(a+b)+x}. The expression a+b at node V is redundant since a+b is in the pool \nPv. Note, however, that the redundant expression (a+b)+x at node V is not readily detected. This is due \nto the fact that r+x was computed at node U and, as noted above, the evaluation of r+x is the same as \nevaluation of (a+b)+x at node U. In order to account for this in the output optimizing pool, (a+b)+x \nis added to the same class as r+x. Thus, PV becomes {albla+b,rlxlr+x, (a+b)+x}. This process is called \nstructuring an optimizing pool . Structuring consists of adding any expres\u00ad sions to the partition which \nhave operands equiva\u00ad lent to the one which occurs at the node being considered. The entire expression \n(a+b)+x at node V is then found to be redundant since the struc\u00ad tured pool Pv contains a class with \n(a+b)+x. An optimizing function fl(N,P) for common sub\u00ad expression elimination can now be informally \nstated. 1. Consider each partial computation e in the ex\u00adpression at node N < ~. 2. If the computation \ne is in a class of P then e is redundant; otherwise 3. create a new class in P containing e and add \nall (partial) computations which occur in the program graph and which have operands equiva\u00ad  lent to \ne (i.e., structure the pool P). 4. If N contains an assignment d:=e, remove from P all expressions containing \nd as a subexpres\u00adsion. For each expression e in P containing e as a subexpression, create e with d sub\u00adstituted \nfor e, and place e in the class of e . The meet operation A of the algorithm A must be defined for common \nsubexpression elimination. Since the optimizing pools in ~ are partitions of expressions, the natural \ninterpretation is as intersection by classes, denoted by ?!. That is, given Pl,P2 c ~ , P = PI h P2 is \ndefined as follows. Let and P(c) = PI(c) n P2(c) Vc E C. C is the set of expressions common to both PI \nand P2, while PI(c) and P2(c) are the classes of c in PI and P2, respectively. Thus , the class of each \nc c C in the new partition P is derived from PI and P2 by intersecting the classes P (c) and P2(c). For \nexample, if P ~ ~ {a, bld,e,f} and1p2 = {a,cld,f,g} then -{a,d,f} andP1 fip2 = {a\\d,f}. It is easily \nshown that H has the properties required of the meet operation; hence, a refine\u00adment relation is defined: \nThat is, PI ~ p2 if and only if PI is a refinement of P2. The refinement relation provides the order\u00ading \nrequired on the set ~ for the algorithm A. The function fl can be stated formally, and shown to have \nthe homomorphism property required by the global analysis algorithm [33]: fl(N,pl * P2) = fl(N,P1) K \nf1(N,P2). Before considering an example of the use of fl with the algorithm A, the function fl is extend\u00aded \nto combine constant propagation with common sub\u00adexpression elimination, 5. CONSTANT PROPAGATION AND COMMON \nSUBEXPRRSSION ELIMINATION The common subexpression elimination optimizing function fl of Section 4 can \neasily be extended to include constant propagation. Consider, for example, the following segment of an \nALGOL 60 pro\u00adgram: ... U:=20; .,. V:=30 , ... U+v ... X:=lo; ... y:=40; ... X+y ... y-x ... Figure 3 \nshows a program graph representing this segment. Assume the entry pool is empty; i.e., PB = 0. The analysis \nproceeds up to node E as before, resulting in PE = {u,201v,30}. Note that u and v are both propagated \nconstants in PE since they are both in classes containing con\u00adstants. If the expression u-i-v at node \nE is pro\u00adcessed as in fl, the output pool is {u,201v,301u+v}. Noting that u and v are in classes with \nconstants, then U+V must be the propagated constant 20+30 = 50. Hence, the constant 50 is placed into \nthe class of U+V in the resulting partition. Thus , = {u,201v,301u+v,50}. F The analysis continues as \nbefore up to node H, resulting in PH= {u,2OIV,3OIU+V,5OIX,1OIY,4O} . In the case of the f optimizing \nfunction, the expression x+y at no i e H is placed into a distinct class. The operands x and y, however, \nare propa\u00adgated constants since they are equivalent to 10 and 40, respectively. The expression x+y is \nequivalent to 50 which is already in the par\u00adtition. Thus, x+y is added to the class of 50, resulting \nin PI = {u,2OIV,3OIU+V,5O,X+YIX,1OIY,4O} . Similarly, the output pool from node I is {u,2OIV,3O,Y-XIU+V,5O,X+YIX,1O \nly,40}. The analysis above depends upon the ability to recognize certain expressions as constants and \nthe ability to compute the constant value of an expression when the operands are all propagated constants. \nIt is also implicit that no two differing constants are in the same class. An optimizing function f2 \nwhich combines constant propagation with common subexpression elimination can be constructed from fl \nby altering step (3) as follows: 3a. create a new class in P containing e and add all (partial) computations \nwhich occur in the program graph and which have operands equiva\u00adlent to those of e (structure the pool \nas before). 3b . If e does not evaluate to a constant value . based upon propagated constant operands, \nthen no further processing is required (same as step (3) of fl); otherwise let z be the constant value \nof e. If z is already in the paKti\u00adtion P then combine the class of z with the class of e in the resulting \npartition. If z is not in the partition P, then add z to the class of e. The expression e becomes a propagated \nconstant in either case. The function f2 is stated formally and its properties are investigated elsewhere \n[33]. ~= {u,201v,3~lu+v,50) FaX=lo ~= {u,201v~l~+v,501x,10} yz4 0 E= {u,201v,301u+~,501x,101y,40} HOX+y \n~ = {u,201v,301u+vj50,x+ylx,101y,40} I y-x 6 Figure 3. A program graph demonstrating the effects of constant \npropagation. 6. EXPRESSION OPTIMIZATION Expression optimization, as defined earlier, includes common \nsubexpression elimination, constant propagation, and register optimization. The first two forms of optimization \nare covered by the f2 optimizing function; only register optimization needs to be considered. It will \nbe shown below that f2 also provides a simple form of register optimization. In general, global register \noptimization in\u00advolves the assignment of high speed registers (accumulators and index registers) throughout \na program in such a manner that the number of store\u00adfetch sequences between the high-speed registers \nand central memory is minimized. Ths store-fetch sequences arise in two ways. The first form in\u00advolves \nredundant fetches from memory. Consider the sequence of expressions a:=b+c; d:=a+e. . for example. A \nstraight-forward translation of these statements for a machine with multiple general purpose registers \nmight be r :=b; r :=c; r :=r +r ; a:=r . 1 2 112 1 r :=a; r :=e; r :=r +r . d:=r 1 2 112 1 Note, however, \nthat the operation r :=a is not 1 necessary since r contains the value of the vari\u00ad 1 able a hefore the \noperation. McKeeman [38] dis\u00adcusses a technique called peephole optimization which eliminates these redundant \nfetches within a basic block. Figure 4 shows a program corresponding to the register operations above. \nThe f2 optimizing func\u00adtion is applied to each successive node in the graph, resulting in the optimizing \npools shown in the Figure. In particular, note that PE -{a,r11blr2,c}. The operation at node E assigns \nthe variable a to the register rl. Since a is already in the class of r however, the operation is redundant \nand can be e it lminated. Hence, the f2 optimizing function can be used to generalize peephole optimization. \nFurther, the algorithm A extends f to allow global elimination of redundant register load operations. \n Figure 4. Elimination of redundant register load operations. The second source of store-fetch sequences \narises when registers are in use and must be released temporarily for another purpose. The contents of \nthe busy register is stored into a central memory location and restored again at a later point in the \nprogram. An optimal register allocation scheme would minimize the number of temporary stores. This form \nof register optimiza\u00adtion has been treated on a local basis, including algorithms which arrange arithmetic \ncomputations in order to reduce the total number of registers required in the evaluation [5,27,36,39,43,45,46]. \nGlobal register allocation has also been formulated as an integer programming problem by Day [14], given \nthat register interference and cost of data is known by the author at this time. A solution to the global \nregister allocation problem will be aided by the analysis of live and dead variables at each node in \nthe program graph. A variable v is live at a node N if v could possibly be referenced in an expression \nsubsequent to node N. The variable v is dead otherwise. Re\u00adcent work has been done by Kennedy [32] using \nin\u00adterval analysis techniques to detect live and dead variables on a global basis. An optimizing function \nf3 can be constructed which produces a set of live expressions at each node in the graph. The detection \nof live expres\u00adsions requires the analyais to proceed from the end of the program toward the beginning. \nFigure 5 shows the graph of Figure 4 with the direction of the edges reversed. The live expressions at \nthe beginning of the graph correspond to the live expressions at the end of program execution; hence, \nPH = @ (there are no live expressions at the end of execution) , The expression d:=rl at node H refers \nto the expression r . Thus, rl is live ahead of node H. This fact is recorded by including rl in PG, \n PG = {rl}. Since rl is assigned a new value at node G, it be\u00adcomes a dead expression, but, since r is \nalso 1 involved in the expression r1+r2, it mediately becomes a live expression again. Thus, PF = {r1,r2,r1+r2}. \nThe analysis continues , producing the optimizing pools associated with each node in Figure 5. The expressions \nwhich are live at node C, for example, are pB = {e,r~!r~,r~+r21. ~ Lg H ~.r( o ~= {r,} G :=f+~ 6 displacement \nfrom registers is known. No complete Figure 5. Detection of live expressions in a solution to the global \nregister allocation problem reversed program graph. The optimizing function f3(N,P) which provides marked \nL, represents the set of nodes remaining li,ve expression analysis can be informally stated to be prncessed \n(the set L of the algorithm A). as follows: Paraphrasing the algorithm A, the tabular form 1. If the \nexpression at node N involves an assign is processed as follows. ment to a variable, let d be the destination \nof the assignment; setP+P -{eld is ~ sub-1. List all entry nodes and entry pools vertically expression \nin e}(d and all expressions con-in the right-hand columns, with entry node ei taining d become dead expressions). \nin column L, and associated entry pool xi in column f(N,PN). Normally, there is only one 2. Consider \neach partial computation e at node N. entry node, with the null set as an entry pool, Set P + P u {e} \n(e becomes a live expression). The value of f3(N,P) is the altered value of P. 2. Select an L from L \nas follbws, Choose any node from column L, say node N. If there are The algorithm A can then be applied \nto the no elements remaining in L then the algorithm reversed program graph using the optimizing func \nhalts. The line where N was added to L con\u00adtion f . The exit nodes of the original graph be-tains the \nassociated output pool Pi in the come t ; e entry nodes of the reversed graph. In column f(N,PN). Eliminate \nL from L by cross\u00adaddition, the meet operation of the algorithm A is ing out N from column L. the set \nunion operation u. The union operation 3. Using L = (N, Pi) from step 2, scan the table induces the partial \nordering given by from the bottom upward to the first occurrence of node N in column N. The current \napproximatePI <P2 <=> p uP21 = 1 => 1 ~ 2 1 2 ~ pOOl PN is adjacent in the column PN + P A Pi. If node \nN has not appeared in column N, ~hen where P is the set of (partial) computations which aesume the first \napproximation to PN = ~ (and occur ~n the program graph. Note that ~ = P and hence, pN i-1 APi = pi). \n~ = @ in this case. Thus, all initial appr~ximate pools in the algorithm A are set to 0. 4. If PN S Pi \nthen go to step 2. Otherwise, write the node name N in column N and the value of There is a simple generalization \nof detection the new approximate pool determined by PN A Pi of live expressions to minimum distance analysis \nin the column marked PN + PN A Pi. Compute where each live expression is accompanied by the the output \npool based upon the new approximate minimum distance to an occurrence of the expres-pOOl PN in the column \nf(N,PN), and write the sion. The optimizing pools in this case are sets names of the immediate successors \nof N in of ordered pairs (e,d), where e is a live expres-column L. Go back to step 2. sion and d is the \nminimum distance (in program steps) to an occurrence of e, The optimizing Upon termination of this algorithm, \nthe table function extends live expression analysis by is scanned from bottom to top; the first occurrence \ntabulating a distance measure as the live expres of each node N 6 N is circled. The pool associated sion \nanalysis proceeds. In addition, the meet with each circled node in column PN ~ PN A Pi is operation consists \nof both set union and a com-the final DOO1 for that node. Anv nodes of N which parison of the distances \ncorresponding to each do not app~ar in column N cannot be reached ~rom live expression. This minimum \ndistance infor an entry node, and can be eliminated from the pro\u00ad mation can then be used in the register \nreplace-gram graph, ment decision: whenever all registers are busy and contain live expressions, the \nregister con-Table I shows the analysis of the program taining the live expression with the largest graph \ngiven in Figure 1, using the f 2 ptimizin~ distance to its occurrence is displaced. function. The entry \nnode set for th~s analysis is L= {(A,@)}, as before. L is treated as a stack; Examples are given in the \nsection which elements are removed from the lower right position follows demonstrating the fz and f3 \noptimizing of column L in step 2. After processing the graph, functions when used in conjunction with \nthe the final pools at each node are listed in the algorithm A. table opposite the circled nodes. The \nfinal pool at node E, for example, is 7. A TABULAR FORM FOR THE ALGORITHM A PE = {a,llb,21d,a+b,3}. \nThe processing of the algorithm A can be The final pools determined by the algorithm corre\u00ad .. expressed \nin a tabular form. The tabular form spend to those determined previously in Section 2, allows presentation \nof a number of examples, and provides an intuitive basis for implementing the TABLE I optimizing techniques. \nIn particular, this form step N f[N, PN) N-PNAPI allows representation of the approximate optimizing \ni0 pools at each node, the elements of L, and the node a,i traversing decision. As shown in Table I, \nthe 2@ ~ 3 @ a,l a,llc, O column labeled N contains the current node being 4 c a,l[c, O a,llc,01b,2 processed \n(i. e., the N in L = (N, P.) in step A5). 5 D a,llc,01b,2 a,llc,01b,21d, a+b,3 The column labeled PN \n+ PN A P. S*OWS the change 6 E a,l]c,01b,2 [d, a+b,3 a,llc,0[b,2, e, btcld, a+b, 3 in the approximate \npool at node \\ when the node is 7 F a.11c,01b,2, e, b+cld. a+b, 3 .a,llb,2, eld, Mb,31c,4 traversed in \nstep A5. The column marked f(N, PN) 8 @ a,l a,llb,2 contains the output optimizing pool produced by a,l\\b,2 \na,l/b,2 traversing the node N (the set bracee are omitted a,llb,21d, a+ b,31b+c, 90 10 @ a+b,3 a,llb,21d, \ne for convenience of notation). The last column, u @ a,l]b,2 a,llb,21c,4 Figure 6 shows a program graph \nwith two paral\u00ad lel feedback loops. The analysis of this program graph is given in Table II, using the \nf2 optimizing function. Note that in step (8), 3 Gx X,Y!X.Y PF = {lo]ylx,5,u}. 4 @ X,y, x.y X,y, x.y \n 5 G X, Y, X-Y x,Y.~,Y Applying f2@,PF), the resulting output pool is 6 @ X, Y,X. Y X,Y,X. Y,U,U. Y 7 \n@ X, Y, X. Y, U,U. Y X, YIX.Y {1OIY]X,5,U]U Y,X Y}. 8 @ X, Y,X-Y Y 9 @ X. Y.X, Y X.y,x.y The expression \nX*Y is placed into the class of U*Y 10 x8Y8~.Y when the partition is structured. That is, x-y ia an expression \nwhich occurs in the program, and x-y This tabular form can be used for processing is operand equivalent \nto u y. Thus, x,y must be any program graph using an optimizing function added to the class of U*Y in \nthe output pool. The which satisfies the conditions of the algorithm A. redundant expression x y is detected \nat node G since the final pool PG contains X*Y. 8. IMPLEMENTATION NOTES J1----- Implementation of the \nabove optimizing tech\u00adentry x:=KI niques in a practical compiler is considered below. In particular, \nthe optimizer operates upon an in-B A termediate form of the program, such as tree struc-X Y F tures \nor Polish [24], augmented by branching infor-U Y mation. The control flow analyzer accepts the in- Cx \n termediate form and calls the various optimizing E functions to process each basic block, roughly U=x \nDG paralleling the tabular form given previously. A X:=5 x-y &#38; single stack can be used to list \nuninvestigated basic blocks, corresponding to L of the tabular Figure 6. A program graph with two parallel \nform. Pool information must be maintained for feedback loops. each basic block corresponding to the P \n+ P A Pi column, but may be discarded and replace ! if ~he node is encountered again in the analysis \n(i.e. , TABLE 11 the node reappears in column N ). The output tep N f(N, PN) N- NAPI optimizing pools \nfound in column f(N,PN), however, 1 @ can be intersected with all immediate successors X,lo 2 @ @ \nas they are produced, and thus need not be main\u00ad 3 @ X,lo X,lo[ylx. y tained during analysis. The final \noptimizing pools 4 c X,lolylx.y X,lolylx. y (determined by scanning the tabular form) are 5 G X,lolylx. \ny X,lolylx. y simply the current pools attached to each basic 6 D X,lolylx. y 10jylx. ylx,5 block. \n7 @ lo/ylx,5 lolylx,5, u 8 @ lolylx,,,u 101YIX,5, UIU-Y, X.Y The optimizing functions and corresponding \nXllolylx.y 9 @ Xllolylx y meet operations are generally simple to implement X[lolylx. 10 @ XI1OIYIX.Y \ny using bit strings for sets, and lists for ordered 11 @ XI1OIYIX.Y x,5110/y pairs. Common subexpression \nelimination, however, requires further consideration since direct repre- Global live expression analysis \ncan be per sentation and manipulation of structured partitions formed on the program graph of Figure \n6 by re\u00ad is particularly unwieldy. versing the gr~ph~ as ~ho&#38; in Figure 7. Given that node C is the \nexit node of the original graph, One approach to handling structured partitionsnode C becomes the entry \nnode of the reversed allows direct representation of the classes, but graph. Thus, ~ = {(C,@)} in the \nanalysis shOwn in limits the number of expressions which appear. A Table 111, using the f optimizing \nfunction. For list of all (sub)expressions is constructed byexample, the final poo 3 prescanning the \nprogram (an optimizing function which always returns @ ia useful for this scan). = {X,y,x y} A When \na partition is structured, only those expres\u00adsions which occur in the expression list are in indicates \nthat the expressions x, y, and x.y are eluded. The set of eligible expressions can be live immediately \nfollowing node A in the original further reduced by first performing live expressiongraph. G D X:.5 \nE analysis. The expressions which appear in a parti- X Y tion are limited to the live expressions at \nthe U.x c point the partition is generated. The use of live entry: x expression analysis before common \nsubexpression F elimination will generally reduce partition size U YB and improve the convergence rate \nof the analysis w algorithm. X.y A X:=lo A second approach to representation of struc\u00ad b Fieure 7. \nThe reversed graph corresponding to tured partitions involves the assignment of value the program graph \nof Fig~re 6. numbers to the expressions in the optimizing pools [13,24,33,34]. A value number is a unique \ninteger assigned to all elements of the same class. The sequence of statements a:=b+c; d:=b; s:=a. results \nin the structured partiti~n PI ={b,d I c I b+c,d-tc,a,e}. Next, assign the value numbers 1, 2, and 3 \nto the three classes, and replace the expressions b+c and d+c by (l)+(2), representing the addition of \nelements of class (1) and class (2). PI can now be written Similarly, the sequence of assignments a:=d; \nb:=c; e:=b+c; produces the structured partition represented by P2 = {a,dlb,c\\(5)+(5),e }. (4) (5) (6) \n which expands to P2 = {a,dlb,clb+c,b+b,c+b,c+c,e} Thus , the assignment of value numbers provides a \ndata structure whose size is linear in the number of expressions in the basic block. In addition, the \nvalue number representation is particularly easy to construct and use in the detection of common subexpresaions. \nGiven two partitions P1 and P2 in value number form, the meet operation P = P H P2 can be itera\u00adtively \ncomputed. The computat i on proceeds as follows . Construct a list C consisting of the number of occurrences \nof each value number in P 1 The elements of C thus provide a count of the number of elements in each \nclass of P . This count is decremented whenever an elem~nt of the class is processed, until the count \ngoes to zero indicating the entire class is exhausted, A list R is also maintained which gives a mapping \nof the class numbers in P and P2 to the resulting class numbers in P. Th~ elements of R are of the form \nr(rl,r2), indicating that value number rl from Pl and value number r from P2 map to value number r in \nthe resulting p?irtition P. R is built during the construction of P. The elements of PI are scanned and \nprocessed until the classes of P are exhausted. Suppose q 1 is an identifier in PI with value number \nv . The 1 count corresponding to VI in the list C is first decremented. If q does not occur in P2 then \nthe next element of PI is selected. Otherwise, let V2 be the value number corresponding to q in P . R \nis scanned for an element V(V1, V2) ; if not $ound, a new value number v is assigned, and V(V1,V2) is \nadded to R. The identifier q is placed into P with value number v. If the element selected from PI is \nnot an identifier, then it ia an expression of the form (nl) 9 (ml) with value number VI, where nl and \nml are value numbers in PI (assuming all operations +3 are binary). If the count of either class (nl) \nor (ml) is non-zero in C, defray the processing of this expression; otherwise, decrement the count for \nclass (v ) in C, as abOve. Examine R for pairs of 1 elements n(nl,n2) and m(m ISm2) where n and m are \nvalue numbers in P2, For each such pa $ r, sea~ch p2 for an entry (n ) 9 (m2). If found, let V2 be thevalue \nnumber o!this matched expression. Scan R for an element of the form V(vl,v ), and make a new entry if \nnot found, as above. ?he expression (n) Q (m) with value number v is then placed into the intersection \nP. As an example, consider the class intersection of the partitions PI and P2 given previously. These \npartitions are represented by the value number tables d (1) d (4) b (5) (1):(2) ::1 a (3) (5):(5) {21 \ne (3) e (6) The class count list C for the partition PI is initially val# _count m2 (2) 1 (3) 3  \nThe identifiers b, d, and c are processed first, reducing the class counts for (1) and (2) to zero in \nC, The class mapping list at this point is R= {7(1,5), 8(1,4), 9(2,5)} The identifiers b, d, and c are \nplaced into P with value numbers 7, 8, and 9, respectively. The ex\u00adpression (l)+(2) with value number \n(3) is then processed from PI) since the class counts for both (1) and (2) are zero. Based upon the mappings \nin R, P is searched for an occurrence of (5)+(5) or (4)+?5) . Since (5)+(5) occurs in P2 with value number \n(6) , R is scanned for an element of the form v(3,6), and, since no such element is found, 10(3,6) is \nadded to R. The expression (7)+(9) with value number (10) is included in P. The identifier a is then \nprocessed, resulting in another mapping 11(3,4) in R; a is added to P with value number (11). Fi\u00ad nally, \nthe identifier e from PI with value number (3) is processed. A match is found in P2 with value number \n(6). Since the element 10(3,6) is already in R, e is added to P with value number (lo) . The final value \nof the class list is R= {7(1,5), 8(1,4), 9(2,5), 10(3,6), 11(3,4)} which can now be discarded. The value \nof the re suiting partition P is d (8) (9) (7):(9) (lo) a (11) e (lo) which represents the structured \npartition {bld\\clb+c,ela} Note that the predicate P2 2 PI is easily computed during this process. The \ncontrol flow analysis algorithm has been implemented as a general-pugpose optimizing module, including \nseveral optimizing functions. The imple\u00admentation is described in some detail elsewhere [33]. 9. CONCLUSIONS \nAn algorithm haa been presented which, in conjunction with various optimizing functions, provides global \nprogram optimization, Optimizing functions have been described which provide con\u00adstant propagation, common \nsubexpression elimina\u00ad tion, and a degree of register optimization. The functions which have been given \nby no means exhaust those which are useful for optimiza\u00adtion. Simplifying formal identities such as \nO+x = O-I-X = x can be incorporated to further coalesce equivalence classes at each application of the \nf2 optimizing function. In addition, it may be pos\u00adsible to develop functions which extend live ex pression \nanalysis to completely solve the global register allocation problem. REFERENCES 1. Aho, A., Sethi, R., \nand Unman, J. A formal approach to code optimization. Proceedings of a Symposium on Compiler Optimization. \nUniversity of Illinois at Urbana-Champaign, July, 1970. 2. Allen, F. Program optimization. In Annual \nReview in Automatic Programming, Pergamon Press, 5(1969), 239-307.  3. ---A basis for program optimization. \nIFIP Congress 71, Ljubljana, August, 1971, 64 68. 4. ---Control flow analysis. Proceedings of a Symposium \non Compiler Optimization, Univer\u00adsity of Illinois at Urbana-Champaign, July, 1970. 5. Aoderson, J. A \nnote on some compiling algo\u00adri thins. Comm. ACM 7, 3 (March 1964), 149-150.  6. Arden, B. Galler, B., \nand Graham, R. An algorithm for translating boolean expres\u00adsions. Jour. ACM 9, 2(April 1962)$ 222-239. \n 7. Bachmann, P. A contribution to the problem of the optimization of programs. IFIP Congress 71, Ljubljana, \nAugust, 1971, 74-78. 8. Ballard, A., and Tsichritzis, D. Transforma tions of programs. IFIP Congress \n71, Ljubljana, August, 1971, 89-93.  9. Breuer, M. Generation of optimal code for ex\u00adpressions via factorization. \nComm. ACM 12, 6(June 1970), 333-340. 10. Busam, V., and Englund, D. Optimization of expressions in FORTMN. \nComm. ACM 12, ~2(Dec. 1969), 666-674.  11. Cocke, J. Global common aubexpre.ssion elimi\u00adnation. Proceedings \nof a Sybposium on Com\u00adpiler Optimization. University of Illinois  at Urbana Champaign, July, 1970. 12. \n---, and Miller, R. Some analysis techniques for optimizin~ computer programs. Proc~ Second International \nConference of System Sciences, Hawaii, January, 1969, 143-146. 13. --, and Schwartz, J. Programming \nLanguages and Their Compilers: Preliminary Notes.  Courant Institute of Mathematical Sciences, New \nYork University, 1970. 14, Day, W. Compiler assignment of data items to registers. IBM Systems Journal, \n8, 4(1970), 281-317. 15. Earnest, C., Balke, K,, and Anderson, J. Analysis of graphs by ordering nodes. \nJour. ACM 19, l(Jan. 1972), 23-42. 16, Elaon, M., and Rake, S. Code generation technique for large language \ncompilers. IBM Systems Journal 3(1970), 166-188. 17. Fateman, R. Optimal code for serial and parallel \ncomputation. Comm. ACM 12, 12(Dec. 1969), 694-695. 18. Finkelstein, M. A compiler optimization technique. \nThe Computer Review (Feb. 1968), 22-25. 19. Floyd, R. An algorithm for coding efficient arithmetic \noperations. Comm. ACM 4, l(Jan. 1961), 42-51. 20. Frailey, D. Expression Optimization using unary complement \noperators. Proceedings of a Symposium on Compiler Optimization, Uni\u00adversity of Illinois at Urbana-Champaign, \nJuly, 1970. 21. ---, A study of optimization using a general purpose optimizer. (PhD Thesis) Purdue \nUniversity, Lafayette, Ind., January 1971. 22. Freiburghouse, R. The MULTICS PL/I compiler. AFIPS Conf. \nProc. FJCC (1969), 187-199. 23. Gear, C. High speed compilation of efficient object code. Comm. ACM \n8, 8(Aug. 1965), 483-488.  24. Gries, D. Compiler Construction for Dipital Computers. John Wiley and \nSons ~nc., New York, 1971. 25. Hill, V., Langmaack, H., Schwartz, H., and Seegumuller, G. Efficient \nhandling of sub\u00ad scripted variables in ALGOL-60 compilers. Proc. Symbolic Languages in Data Processing, \nGordon and Breach, New York, 1962, 331-340. 26. Hopkins, M. An optimizing compiler deaign. IFIP Congress \n71, Ljubljana, August, 1971,  27. Horowitz, L., Karp, R., Miller, R., and Winograd, S. Index register \nallocation. Jour. ACM 13, l(Jan. 1966), 43-61. 28. Huskey, H., and Wattenberg, W. Compiling techniques \nfor boolean expressions and conditional statements in ALGOL-60. Comm. ACM 4, l(Jan. 1961), 70-75. 29. \nHuskey, H. Compiling techniques for algebraic expressions. Computer Journal 4, 4(April 1961), 10-19. \n 69-73. 30. 31. 32. 33. 34. 35. 36. 37. 38. 39, 40. 41. 42. 43. 44. 45. 46. 47.  Huxtable, \nD. On writing an optimizing trans\u00ad lator for ALGOL-60. In Introduction~ System Programming, Academic \nPress, Inc., New York, 1964. IBM System/360 Operating System, FORTRAN IV (G and H) Programmer s Guide. \nc28-6817-1, International Business Machines, 1967, 174-179. Kennedy, K. A global flow analysis algorithm. \nIntern. J. of Computer Mathematics, Section A, Vol. 3, 1971, 5-15. Kildall, G. Global expression optimization \nduring compilation. Technical Report No. TR# 72-06-02, University of Washington Com\u00adputer Science Group, \nUniversity of Washington, Seattle, Washington, June, 1972. A code synthesis filter for basic block optimization. \nTechnical Report No. TR# 72\u00ad 01-01, University of Washington Computer Science Group, University of Washington, \nSeattle, Washington, January, 1972. Lowry, E., and Medlock, C. Object code opti\u00ad mization. Comm. ACM \n12, l(Jan. 1969), 13-22. Luccio, F. A comment on index register allo\u00ad cation. Comm. ACM 10,9 (Sept. 1967), \n572\u00ad 572-574. Maurer, W. Programming-An Introduction to Computer Language Techn~ue. Holden-DaZ San Francisco, \n1968, 202-203. McKeeman, W. Peephole optimization. Comm. ACM 8, 7(July 1965), 443-444. Nakata, 1, On \ncompiling algorithms for arithmetic expressions. COmm. ACM19, 8(Aug, 1967), 492-494. Nievergelt, J. On \nthe automatic simpli\u00adfication of computer programs. Comm. ACM 8, 6(June 1965), 366-370. Painter, J. \nCompiler effectiveness. Pro\u00adceedings of a Symposium on Compiler Optimization, University of Illinois \nat Urbana-Champaign, July, 1970. Randell, B., and Russell, L. ALGOL 60 . Implementation. Academic Press, \nInc., New York, 1964. Redziejowski, R. On arithmetic expressions and trees. COmm. ACM 12, 2(Feb. 1969), \n 81-84. Ryan, J. A direction-independent algorithm for determining the forward and backward compute points \nfor a term or subscript during com\u00adpilation. Computer Journal 9, 2(Aug. 1966), 157-160. Schnieder, V. \nOn the number of registers needed to evaluate arithmetic expressions. BIT 11(1971), 84-93. Sethi, R., \nand Unman, J. The generation of optimal code for arithmetic expressions. Jour. ACM17, 4(Oct. 1970), 715-728. \nWagner, R. Some techniques for algebraic optimization with application to matrix arithmetic expressions. \nThesis, Carnegie- Mellon University, June, 1968. 48. Yershov, A. On programming of arithmetic operationa. \nComm. ACM 1, 8(Aug. 1958), 3-6. 49. ---ALPHA-an automatic programming system of high efficiency. Jour. \nACM 13, l(Jan. 1966), L7-24 .  APPENDIX A 1 Al: L = {(A,PI)} ~1 = 2 A3: 3 A4 : PA=~,Pi=O, PAi4Pi, N \n= pA + pAApi=pi=@  4 A5: PA = 0, L= {(B,{(a,l) })} 5 A3: L = (B, {(ail)}), L = ~ 6 A5: PB = {(a,l)}, \nL= {( C,{(a,l), (c, O)})} L! = 7 A3 : (C, {(a,l), (c, O)}), L = @ 8 A5 : [(a,l),(c,O)}, c = L = {(D,{(a,l),(c,0),(b,2)})} \n 9 A3 : L = (D,{(a,l),(c,0),(b,2)}), L = @ 10 A5 : = {(a,l),(c,0),(b,2)}, D L = {(E,{(a,l),(c,0),(b,2),(d,3) \n})}  11 A3 : L = (E,{(a,l),(c,0),(b,2),(d,3) }), L = @ 12 A5 : = {(a,l),(c,0),(b,2),(d,3)}, E L = {(F,{(a,l),(c,O), \n(b,2),(d,3) ,(e,2)})}  13 A3 : L = (F,{(a,l),(c,O), (b,2),(d,3) ,(e,2) }), L=@ 14 A5 : PF = {(a,l),(c,0),(b,2),(d,3), \n(e,2)}, L = {(C,{(a,l),(c,4),(b,2), (d,3) ,(e,2)})} 15 A3 : L = (C, {(a,l), (c,4), (b,2), (d,3), (e,2) \n}), L=fl 16 A5 : = {(a,l)}, L= {(D,{(a,l), (b,2)})} c 17 A3 : L = (D,{(a,l),(b,2)}), L = @ 18 A5: = \n{(a,l),(b,2)}, D L = {(E,{(a,l),(b,2),(d,3)})}  19 A3: L = (E,{(a,l),(b,2),(d,3)}), L = @ 20 A5: PE \n= {(a,l), (b,2), (d,3)}, L = {(F,{(a,l),(b,2),(d,3)})} 21 A3: L = (F,{ (a,l),(b,2),(d,3)}), L = @ 22 \nA5: PF = {(a,l),(b,2), (d,3)}, L = {(C,{ (a,l),(b,2),(d,3),(c,4) })} 23 A3: L = (C,{(a,l),(b,2), (d,3),(c,4) \n}), halt. APPENDIX B The proof of Theorem 2 is given below. First note that given a program graph G \nwith multiple entry nodes, an augmented graph G can be con\u00ad structed with only one entry node with entry \npool The construction is as follows. Let ~= f~1,e2, . . . ek} be the entry node set and ~ = } be the \nen~ry pool (el$x~), (e2,x~), . ..$ (ek,x~) set corresponding to a particular analysis. COn\u00adsider the \naugmented graph G = <~ ,~ > where E1=E {(v, vi), (v, v2),. ... (v,vk), (vi, el), (vk,ek)}k)}. The augmented \ngraph G has a single entry mode v and entry node set~ = {v}. The functional value of f is defined for \nthese nodes as f(v,P) =~ VP c~, and f(vi,P) = xiVP c ~, l<i<k. Hence, the analysis proceeds as if there \nis only a Fingle entry node with entry pool O_; i.e., ~ = {(v,g)}. Lemma 1. If f(N,P AP ) = f(N,P ) \nA f(N,P ) then => f@, P1)sf?N,l,), VN c i, P~,P, 2P . 1SP2 Proof. The proof is immediate since P SP2 \n=> f(N, P1AP2) = f(N, P1) =( f(N, P1) Af(N, P2j)=> f(N,pl) s f(N,P2)o Lemma 2. Let X ~ ~, if f(N,P1AP2) \n= f(N,P1)Af(N,P2) VN < ~, P1,P2 e ~ then f(N, ~xx) =x~Xf(N,x). Proof. The proof proceeds by induction \non the cardinalit of X, denoted by C(X). If C(X) = 1 then f (N, &#38;xcx ) = f(N, x) and the lemma is \ntrivially true. If G(X) = k, k>l, assume lema is true for all X ~(X ) i k. Let y~x and X! = x -{y}. \n f(N,&#38;x) = f(N,yA(~x,)) = f(N,y)Af(N,@x,) . f(N, y) A(x~x, f(N, x)) = ~~xf(N, x)O Proof of Theorem \n2. It will first be shown by induction on the path length that PN5~vNe~. Consider the following proposition \non n: pN s f(pn,f(pn_l,. ..,f(p1,9) )...) fOr all final POOIS PN and paths of length n from the entry \nnode pl with entry pool ~ to node N, QN E N.  The trivial case is easily proved. The only node which \ncan be reached by a path of length O from the entry node PI is PI itself. Hence, it is only necessary \nto show that P s o. This is P1 immediate, however, since (p ,0) is initially placed into L in step Al, \nan~ ~xtracted in step A3 as L = (P~,~). But, Ppl is initially~, and hence P~1 * Pi =~in step A4. Thus, \nPpl +P AO=O Pl in step AS. Thus , it follows that P =os0 P1 Suppose the proposition is true for all \nn<k, for k>O. That is, PN s f(p ,,. .,f(p ,0)) ..,) ~\u00adfor all paths of length less than k rom PI to node \nN, for each node N c N.  Let K c ~~3a path (Pi,...,pk,K) of length k. It will be shown that P~ s f(pk,f(pk_l,. \n..,f(pl ,g)). ..). Consider each immediate predecessor in I-l(K). Let p denote one such predecessor, \nand let T = f ? pk_~,. ... f(pl,g)) ...). By inductive hypothesis, PPk s T. It will be shown that PK \ns f(pk,T). Since P is the final approximation to the Pool at pk, [~,f(pk,pp )) must have been added \nto L in step A5. But, Ppk~T=> f(pk,pp ) ~ f(Pk,T) by Lemma 1. The pair ( ,f(pk, PPk)) mu!$t be processed \nin step A3 before the algorithm halts, Thus, either pK<f (pk, ppk) in Step A4, or PKi-PK A f(pk,ppk). \nIn either case, p~ ~ f(pk,ppk). But , pK s (pk,ppk) S f(pk,T) > PK S f(pk,T) => PK-< (pk,f(pk_l, ..., \nf(P1,Q)) ...). Thus , since the proposition holds for paths of length k, it follows by induction that \nthe proposi\u00adtion is true for all paths from the entry node to node N, for all NcN, The following claim \nwill be proved in Order to show that XN<P for all N<N: ,. the processing .! G by the alg~ri;~ ~g~~ ; \nhas not been encountered in step A5, or % < Plja where P is the current approximate pool associated \nwith no! e N, for all N E N. The proof proceeds by  induction on the number of times step A5 has been \n executed. Suppose step A5 has been executed only once. Then L = (P1,Q) and the only n~de encountered \nin step A5 is the entry node PI. The entry pool ~ corresponds to a path of length zero from PI tO PI. \nThus, ~ < F = ~ and the proposition is PI P1 trivially true since X PI o~pPl=Q Suppose that either \nN has not been encountered in step A5, or ~spN VN6Nwhen step A5 has been executed n<k times, k>l. Con=ider \nthe kth execution of step A5. Let L = (N,T) where T = f(N ,PN,) for some N s I-l(N). The pair (N,T) was \nadded to L when the node N was processed in the nth execution of step A5, for n<k. Hence, t ~ pNt by \ninductive hypothesis. But, using Lemma ? , A %s v paths f(N >f(pts , f(P1,~)) ...) = (Pi>...,pt,,N)N) \nf(N , A f(Ptyf(Pt_l, . . ..f(P1.Q)) . ..) V Paths (P; , . . ..pt.N ) = f(N ,XNT). ~, ~pN, andthus~~f(N \n,~, ) => ~ s f(N ,P N,) = T, using Lemma 1. If this step is the first occurrence of node N inA5, then \nPN+~AT = T since f(Nf,p) + I for any N c ~, P E p. In this case, ~ <PN-=T after step A5. Othe %ise, suppose \nthis is not the first occurrence of node N in step A5. ~<PN and~<T=> ~sPNAT=> XN<PN+PNATafter step A5 \nis executed. Hence, the proposition holds for each execution of step A5. In particular, XN ~ PN Q N c \n_N upon termination of the algorithm A. Hence, the theorem is proved since PN<~and~SPN=~XN=PNVN<~e \n\t\t\t", "proc_id": "512927", "abstract": "A technique is presented for global analysis of program structure in order to perform compile time optimization of object code generated for expressions. The global expression optimization presented includes constant propagation, common subexpression elimination, elimination of redundant register load operations, and live expression analysis. A general purpose program flow analysis algorithm is developed which depends upon the existence of an \"optimizing function.\" The algorithm is defined formally using a directed graph model of program flow structure, and is shown to be correct. Several optimizing functions are defined which, when used in conjunction with the flow analysis algorithm, provide the various forms of code optimization. The flow analysis algorithm is sufficiently general that additional functions can easily be defined for other forms of global code optimization.", "authors": [{"name": "Gary A. Kildall", "author_profile_id": "81100148856", "affiliation": "Naval Postgraduate School, Monterey, California", "person_id": "P330425", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512927.512945", "year": "1973", "article_id": "512945", "conference": "POPL", "title": "A unified approach to global program optimization", "url": "http://dl.acm.org/citation.cfm?id=512945"}