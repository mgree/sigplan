{"article_publication_date": "10-01-1973", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1973 ACM 0-12345-678-9 $5.00 Top Down Operator Precedence Vaughan R. Pratt Massachusetts Institute of \nTechnology 1. Survey of the Problem Domain. of this kind of oversight is our universal preoccupation \nwith BNF grammars and their There is little agreement on the extent various offspring: type 1 [Chomsky \n1959], to which syntax should be a consideration indexed [Aho 1968], macro [Fischer 1968], in the design \nand implementation of program-LR(k) [Knuth 1965], and LL(k) [Lewis 1968] ming languages. At one extreme, \nit is con-grammars, to name a few of the more prominent sidered vitat, and one may go to any lengths \nones , together with their related automata [Van Wijngaarden 1969, McKeeman 1970] to and a large body \nof theorems. I am person\u00adprovide adequate syntactic capabilities. ally enamored of automata theory per \nse, The other extreme is the spartan denial of but I am not impressed with the extent a need for a rich \nsyntax [Minsky 1970]. In to which it has so far been successfully between, we find some language implementers \napplied to the writing of compilers or willing to incorporate as much syntax as interpreters. Nor do \nI see a particularly possible provided they do not have to work promising future in this direction. Rather, \nhard at it [Wirth 1971]. I see automata theory as holding back the development of ideas valuable to language \nIn this paper we present what should design that are not visibly in the domain be a satisfactory compromise \nfor a res~ect-of automata theory. ably large propo~tion-of language designers Users of BNF grammars encounter \ndiffi\u00ad and implementers. We have in mind particularly culties when tryi~g to reconcile the con\u00ad (i) \u00ad \nthose who want to write translators flitting goals of practical generality and interpreters (soft, firm \nor hardwired) (coping simultaneously with symbol tables, for new or extant languages without having data \ntypes and their inter-relations, reso\u00adto acquire a large system to reduce the lution of ambiguity, unpredictable \ndemands labor, and by the BNF user, top-down semantics, etc.) (ii) those who need a convenient yet and \ntheoretical efficiency (the guarantee efficient language extension mechanism that any translator using \na given technique accessible to the language user. will run in linear time and reasonable space, regardless \nof the particular grammar used). The approach described below is very BNF grammars alone do not deal \nadequately simple to understand, trivial to implement, with either of these issues, and so they easy \nto use, extremely efficient in prac-are stretched in some directions to increase tice if not in theory, \nyet flexible generality and shrunk in others to improve enough to meet most reasonable syntactic efficiency. \n1 Both of these operations tend needs of users in both categories (i) and to increase the size of the \nimplementation (ii) above. (What is reasonable is life-support system, that is, the soft\u00adaddressed in \nmore detail below) , More-ware needed to pre-proc.ess grammars and to over, it deals nicely with error \ndetec-supervise the execution of the resulting tion. translator. This makes these methods correspondingly \nless accessible and less One may wonder why such an obviougly pleasant to use. Also , the stretching \nutopian approach has not been generally operation is invariably done gingerly, adopted already. I suspect \nthe root cause dealing only with those issues that have Work reported herein was supported in part at \nStanford by the National Science Foundation under grant no GJ 992, and the Office of Naval Research under \ngrant number N-OOO14-67-A\u00ad0112-0057 NR 044-402; by IBM under a post-doctoral fellowship at Stanford; \nby the IBM T.J. Watson Research Center, Yorktown Heights, N.Y,; and by Project MAC, an MIT research Program \nsponsored by the Advanced Research Projects Agency, Department of Defense, under Office of Naval Research \nContract Number NOO014-70-0362-OO06 and the National Science Foundation under contract number GJOO-4327. \nReproduction in whole or in part is permitted for any purpose of the United States Government. been anticipated, \nleaving no room for unexpect-lines on how to write modular. efficient. ed needs. I am thinking here particularly \nof the work of Lewis and Stearns and their colleames on LL(k) grammars, table grammars, and att~i\u00adbuted \ntranslations. Their approach, while retaining the precision characteristic of the mathematical sciences \n(which is unusual in what is really a computer-engineering and human-engineering problem) , is tempered \nwith a sensitivity to the needs of transla\u00adtor writers that makes it perhaps the most promising of the \nautomata-theoretic approaches . To demonstrate its practicality, they have embodied their theory in an \nefficient Algol compiler. A number of down-to-earth issues are not satisfactorily addressed by their \nsystem deficiencies which we propose to make up in the approach below; they are as follows. (i) From \nthe point of view of the lan\u00adguage designer, implementer or extender, writing an LL(k) grammar, and keeping \nit LL(k) after extending it, seems to be a black art, whose main redeeming feature is that the life-support \nsystem can at least localize the problems with a given grammar. It would seem preferable, where possible, \nto make it easier for the user to write acceptable grammars on the first try, a property of the approach \nto be presented here. (ii) There is no escape clause for dealing with non-standard syntactic prob\u00adlems \n(e.g. Fortrafi formqt statements), The procedural approach of this paper makes it possible for the user \nto deal with difficult problems in the same language he uses for routine tasks.  (iii) The life-support \nsystem must be up, running and debugged on the user s compu\u00adter before he can start to take advantage \nof the technique. This may take more effort than is justifiable for one-shot applications. lie suggest \nan approach that requires only a few lines of code for supporting soft\u00adware. (it ) Lewis and Stearns \nconsider only translators, in the context of their LL(k) system; it remains to be determined how effectively \nthey can deal with interpreters. The approach below is ideally suited for interpreters, whether written \nin software, firmware or hardware. 2. Three Syntactic Issues. To cope with unanticipated syntactic needs, \nwe adopt the simple expedient of allowing the language implementer to write arbitrary programs. By itself, \nthis would represent a long step backwards; instead, we offer in place of the rigid structure of a BNF-oriented \nmeta-language a modicum of supporting software, and a set of guide\u00ad compact and comprehensible translators \nand interpreters while preserving the impression that one is really writing a grammar rather than a program. \nThe guidelines are based on some ele\u00admentary assumptions about the primary syn\u00adtactic needs of the average \nprogrammer. First, the programmer already under\u00adstands the semantics of both the problem and the solution \ndomains, so that it would seem appropriate to tailor the syntax to fit the semantics. Current practice \nentails the reverse. Second, it is convenient if the pro= grammer can avoid having to make up a -special \nname for every object his program computes. The usual way to do this is to let the computation itself \nname the result \u00ad e.g. the object which is the second argu\u00adment of + in the computation a+b*c is the \nresult of the computation b*c . We may regard the relation is an argument of as defining a class of trees \nover computa\u00adtions ; the program then contains such trees, which need conventions for express\u00ading linearly. \nThird, semantic objects may require varying degrees[of annotation at each invo\u00adcation, depending on how \nfar the particular invocation differs in intent from the norm (e.g. for loops that don t start from 1, \nor don~step by 1). The programmer needs to be able to formulate these annotations within the programming \nlanguage. There are clearly many more issues than these in the design of programming languages. However, \nthese seem to be the ones that have a significant impact on the syntax aspects. Let us now draw inferences \nfrom the above assumptions. 2.1 Lexical Semantics versus Syntactig Semantic? The traditional mechanism \nfor assign\u00ading meanings to programs is to associate semantic rules with phrase-structure rules, or equivalently, \nwith classes of phrases. This is inconsistent with the following reasonable model of a programmer. The \nprogrammer has in mind a set of semantic objects. His natural inclination is to talk about them by assigning \nthem names , or tokens. He then makes up pro\u00adgrams using these tokens, tog,ether with other tokens useful \nfor program control, and some purely syntactic tokens. (No clear-cut boundary separates these classes,) \nThis suggests that it is more natural to associate semantics with tokens thanwith classes of phrases. \nThis argument is independent of whether we specify program control expli\u00adcitly, as in Algol-like languages, \nor implicitly, as in Planner-Conniver-like languages. In either case, the programmer wants to express \nhis instructions or inten\u00adtions concerning certain objects. When a given class of phrases is character \nized unambi~usly by the pr~sence of a parti\u00adcular token, the effect is the same, but this is not always \nthe case in a BNF-style semantic specification, and I conjecture that the difficulty of learning and \nusing a given language specified with a BNF gratirnati incteases in proportion to the num\u00adber of rules \nnot identifiable by a single token. The existence of an operator grammar [Floyd 1963] for Algol 60 provides \na plausi\u00adble account of why people succeed in learn\u00ading Algol, a process known not to be strongly correlated \nwith whether they have seen the BNF of Algol. The~e are two advantages of separating semantics from syntax \nin this way. First, phrase-structure rules interact more strong\u00adly than individual tokens because rules \ncan share non-terminals whereas tokens have nothing to share. So our assignment of semantics to tokens \nhas a much better chance of being modular than an assignment to rules. Thus one can tailor the language \nto one s needs by selecting from a library, or writing, the semantics of just those objects that one \nneeds for the task in hand, without having to worry about preordained interactions between two semantic \nobjects at the syntactic level. Second, the lan\u00adguage designer is free to develop the syntax of his language \nwithout concern for how it will affect the semantics; instead, the semantics will affect decisions about \nthe syntax. The next two issues ~linear\u00adizing trees and annotating tokens) illustrate this point well. \nThus syntax is the servant of semantics, an appro\u00adpriate relationship since the substance of the message \nis conveyed with the semantics, variations in syntax being an inessential trimming added on human-engineering \ngrounds. The idea of lexical semantics is implicit in the usual approach to macro generation, although \nthe point usually goes unmentioned. I suspect many people find syntax macros [Leavenworth 1966] appealing \nfor reasons related to the above discussion. 2.2 Conventions for Linearizing Trees . We argued at the \nbeginning of section 2 that in order to economize on names the programmer resorted to the use of trees. \nThe precedent is a long history of use of the same trick in natural language. Of necessity (for one-dimensional \nchannels) the trees are mapped into strings for trans\u00admission and decoded at the other end. We are concerned \nwith both the human and computer engineering aspects of the coding. We may as sume thetrees look like, \ne.g. apply / \\+ ./ \\; rea( \\, / >,int / \\, j Y I j\\ XY1 That is, every node is labelled with a token \nwhose arguments if any are its sub\u00adtrees. Without further debate we shall adopt the following conventions \nfor encod\u00ading trees as strings. (i) The string contains every Occurrence of the tokens in the tree. [which \nwe cdl the semantic tokens, whic~ include proced\u00adural Items such as if , ; ) together with some additional \nsyntactic tokens where necessary. (ii) Subtrees map to contiguous sub\u00adstrings containing no semantic \ntoken out\u00adside that subtree. (iii) The order of arguments in the tree is preserved. (Naturally these \nare orient\u00aded trees in general.) (iv) A given semantic token in the lang\u00aduage, together with any related \nsyntactic tokens , always appear in the same place within the arguments; e.g. if we settle for Tt+a,b \n, we may not use a+b as well. (This convention is not as strongly motiva\u00adted as (i) -(iii); without it, \nhowever, we must be overly restrictive in other areas more important than this one.) If we insist that \nevery semantic token take a fixed number of arguments, and that it always precede all of its arguments \n(prefix notation) we may unambiguously re\u00adcover the tree from the string (and similarly for postfix) \nas is well known. For a variable number cf arguments, the LISP solution of having.syntactic tokens (parentheses) \nat the beginning and end of a subtree s string will suffice. Many people find neither solution particularly \neasy to read. They prefer 22 ab +cd = 4sin (a+b) to II=~*a+b 2*C +d 2*4sin+ ab p or to (= (+ (* a(+b2)) \n(* c(+d2))) (* 4 (sin (+ a b)))) , although they will settle for 1! ahb+j + c*d+2 = 4*sin(a+b) in lieu \nof the first if necessary. (But I have recently encountered some LISP users claiming the reverse, so \nI may be biased.) An unambiguous compromise is to require parentheses but move the tokens, as in ~$(~ab~]}~,,+ \n2)).+ (c * (d ~ 2))) = (4 * (sin This is actually quite readable. if not ve;y writable, but it ~s-diffi~ul~~o \ntell if the parentheses balance, and it nearly doubles the number of symbols. Thu S we seem forced inescapably \ninto having to solve the problem that operator precedence was designed for, namely the association problem. \nGiven a substring AEB where A takes a right argument, B a left, and E is an expression, does E associate \nwith A or B? A simple convention would be to say E always associates to the left. However, in print a \n+ b , it is clear that a is meant to associate with + , not print . The reason is that (print a) + b \ndoes not make any conventional sense, print being a procedure not normally returning an arithmetic value. \nThe choice of print (a + b) was made by taking into account the data types of print s right argument, \n+ s left argument, and the types returned by each. Thus the association is a function of these four types \n(call them aA,rA,aB,rB for the argument and result respectively of and B) that also takes into account \nthe legal coercions (implicit type conversions) Of course, sometimes both associations make sense,and \nsometimes neither. Also or r ~ may depend on the type of E, A further complicating matters. One way to \nresolve the issue is simply to announce the outcome in advance for each pair A and B, basing the choices \non some reasonable heuristics. Floyd [1963] suggested this approach, called operator precedence. The \noutcome was stored in a table. Floyd also suggested a way of en\u00adcoding this table that would work in \na small number of cases, namely that a number should be associated with each argument position by means \nof precedence functions over tokens; these numbers are sometimes called binding powers . Then E is associated \nwith the argument position having the higher number. Ties need never occur if the numbers are assigned \ncare\u00adfully; alternatively, ties may be broken by associating to the left, say. Floyd showed that Algol \n60 could be so treated. One objection to this approach is that there seems to be little guarantee that \none will always be able to find a set of numbers consistent with one s needs. Another ob@ection is that \nthe programmer has to learn as many numbers as there are argument positions, which for a respectable \nlanguage may be the order of a hundred. lie present an approach to language design which simultaneously \nsolves both these problems, without unduly restricting normal usage, yet allows us to retain the numeric \napproach to operator precedence. The idea is to assign data types to classes and then to totally order \nthe classes. An example might be, in ascending order, Outcomes (e.g., the pseudo-result of print ), Booleans, \nGraphs (e.g. trees, lists. ulexes), Strings. Algebraic [e.g. ; tnteger;, complex nos~ poly~omials, reai \narrays) and References (as on the left side of an assignment.) We write Strings c References , etc. We \nnow insist that the class of the type at any argument that might participate in an association problem \nnot be less than the class of the data type of the result of the function taking that argument. This \nrule applies to coercions as well. Thu S we may use < since its argument types (Algebraic) are each greater \nthan its result type (Boolean.) We may not write length XI (where x is a string or a graph) since the \nargument type is less than the result type. However, lx] would be an acceptable substitute for Tlength \nx as its argument cannot participate in an associa\u00adtion problem. Finally, we adopt the convention that \nwhen all four data types in an association are in the same class, the association isA to the left. These \nrestrictions on the language, while slightly irksome, are certainly not as demanding as the LISP restriction \nthat every expression have parentheses around it. Thus the following theorem should be a little surprising, \nsince it implies that the programmer never need learn any associations! Theorem 1. Given the above restrictions, \nevery association problem has at most one solution consistent with the data types of the associated operators. \nProof. Let . ..AEB. . . be such a problem, ~ppose E may associate with both A and B. Hence because E \nassociates with A, [aA]~ [rA]~ [aB]&#38; [rB] (type x is in class[x]) since coercion is non-increasing, \nand the type class of the result of ...AE is not greater than [r ], by an obvious inductive proof. Also \nfo$ E with B, [aB]~ [rB]~ [aA]~ [rA] similarly. Thus [aA]=[aB], [rA]=[rB]> and [aA]=[rB] , that is,all \nfour are in the same class. But the convention in this case is that E must associate with A, contradicting \nour assumption that E could associate with B as well. a This theorem implies that the program\u00admer need \nnot even think about association except in the homogeneous case (all four types in the same class), and \nthen he just remembers the left-associativity rule. More simply, the rule is always associate to the \nleft unless it doesn t make sense . What he does have to remember is how to write expressions containing \na given token (e.g. he must know that one writes x , not length x ) and which coercions are allowed. \nThese sorts of facts are quite modular, being contained in the description of the token itself independently \nof the properties of any other token, and should certainly be easier to remember than numbers associated \nwith each argument. Given all of the above, the obvious way to parse strings (i.e. recover their trees) \nis, for each association problem, to associate to the left unless this yields semantic nonsense. Unfortunately, \nnonsense testing requires looking up the types rA and a and verifying the existence of a coerc!on from \nr to a. For translation this is not ser 4 OUS, But for interpretation it might slow things down significantly. \nFortunately, there is an efficient solution that uses operator precedence functions. Theorem 2. Given \nthe above restrictions on a language, there exists an assignment of integers to the argument positions \nof each token in the language such that the correct association, if any, is always in the direc\u00adtion \nof the argument position with the larger number, with ties being broken to the left. Proof. First assign \neven integers (to make room for the followin~terpolations) to the data type classes. Then to each argument \nposition assign an integer lying strictly (where possible) between the integers corresponding to the \nclasses of the argument and result types. To see that this assign\u00adment has the desired property, consider \nthe homogeneous and non-homogeneous cases in the problem .. .AE .. . as before. In the homogeneous case \nall four types are in the same class and so the two numbers must be equal, resulting in left association \nas desired. If two of the data types are in different classes, then one of the inequalities in [aA]~[rA]2 \n[aB]L[rB] (assuming E associates with A) must be strict. If it is the first or third inequality, then \nA s number must be strictly greater than B s because of the strictness condition for lying between different \nargument and result type class numbers. If it is the second inequality then A s number is greater than \nB s because ATS result type class number is greater than B s argument one. A similar argument holds if \nE associates with B, completing the proof. u Thus Theorem 1 takes care of what the programmer needs to \nknow, and Theorem 2 what the computer needs to know. In the former case we are relying on the programmer \ns familiarity with the syntax of each of his tokens; in Lhe latter, on the computer s agility with numbers. \nTheorem 2 establishes that the two methods are equivalent. Exceptions to the left association rule for \nthe homogeneous case may be made for classes as a whole without upsetting theorem 2. This can be done \nby decrementing by the numbers for argument positions to the right of all semantic tokens in that class, \nthat is, the right binding powers. Then the programmer must remember the classes for which the exception \nholds. Applying this trick to some tokens in a class but not to others gives messy results, and so does \nnot seem worth the extra effort required to remember the affected tokens, The non-semantically motivated \ncon\u00ad* ventions about and and f may be implemented ~f~rt%r s~b~ividing the appropriate classes (here \nthe Booleans and Algebraic) into pseudo-classes, e.g. terms < factors < primaries, as in the BNF for \nAlgol 60. Then + is defined over terms, * over factors and + over primaries, with coercions allowed from \nprimaries to factors to terms. To be consistent with Algol, the primaries should be a right associative \nclass. While these remarks are not essential to the basic approach, they do provide a sense in which \noperator precedence is more than just an ad hoc solution to the associa\u00adtion problem. Even if the language \ndesigners find these guidelines too restrictive, it would not contradict the fact that operator precedence \nis in practice a quite satis\u00adfactory solution, and we shall use it in the approach below regardless of \nwhether the theoretical justification is reasonable. Nevertheless we would be interested to see a less \nrestrictive set of conventions that offer a degree of modularity comparable with the above while retaining \nthe use of precedence functions. The approach of recomputing the precedence functions for every operator \nafter one change to the grammar is not modular, and does not allow flexible access to individual items \nin a library of semantic tokens. An attractive alternative to precedence functions would be to dispose \nof the ordering and rely purely on the data types and legal coercions to resolve associations. Cases \nwhich did not have a unique answer would be referred back to the programmer, which would be acceptable \nin an on-line environment, but undesirable in batch mode. Our concern about efficiency for interpreters \ncould be dealt with by having the outcome of each associa\u00adtion problem marked at its occurrence, to speed \nthings up on subsequent encounters. Pending such developments, operator precedence seems to offer the \nbest overall compromise in terms of modularity, ease of use and memorizing, and efficiency. The theorems \nof this section may be interpreted as theorems about BNF grammars, with the non-terminals playing the \nrole of data type classes. However, this is really a draw-back of BNF; the non-terminals tempt one to \ntry to say everything with just context\u00adfree rules, which brings on the difficulties mentioned in Section \n1. It would seem preferable to refer to the semantic objects directly rather than to their abstraction \nin an inadequate language. 2.3 Annotation When a token has more than two argu\u00adments, we lose the property \nof infix nota\u00adtion that the arguments are delimited. This is a nice property to retain, partly for readability, \npartly be\u00adcause complications arise, e.g. , if ,,_ It is to be used as both an infix and a prefix operator~ \n ( also has this property; as an infix it denotes applica\u00adtion, as a prefix? a no-op. Accordingly we \nrequire that all arguments be de\u00adlimited by at least one token: such a grammar Floyd [1963] calls an \noperator grammar. Provided the number of argu\u00adments remains fixed it should be clear that no violenceis \ndone by the extra arguments to theorems 1 and 2P since the string of tokens and arguments including the \ntwo arguments at each end plays the same syntactic role as the single semantic token in the two\u00adargument \ncase. We shall call the seman\u00adtic tokens associated with a delimiter its parents. An obvious choice of \ndelimiters is commas. However, this iS nQt as valuable as a syntactic token that documents the role of \nthe argument following it. For example, if a then b else c is more readable [by a human) then if a, b, \nC . Other examples are print x format f , for i from s to f by dwhile cdo b , log x base b , solve e \nusing m , x between y and Z t, etc. Sometimes arguments may be fre\u00adquently used constants, e.g.? for \ni from 1 to n by 1 while true do b , If an argument is uniquely identified by its preceding delimiter, \nan obvious trick is to permit the omission of that argument and its token to denote that a default value \nshould be used. Thus, we may abbreviate the previous example to fpr i to n do b~ , as in extended Algol \n68. Other obvious defaults are log x for log x base 2 , if x then y l far (if x then y else nil , and \nsa on. Note that various arguments now may be involved in associations, depending on which ones are absent. \nAnother situation is that of the variable length parameter list, e.g. , clear at b, Cf d . Commas are \nmore although again we may appropriate here, need more varietyP as in Iturn on a Qn b off g on m off \np off t (in which the unamed switches or bits are left as they are) . All of these examples show that \nwe want to be able to handle quite a variety of situations with default para\u00admeters and variable-length \nparameter lists. No claim is made that the above examples exhaust the possibilities, so our language \ndesign should make provision not only for the above, but for the unexpected as well. This is one reason \nfor preferring a procedural embedding of semantics; we can write arbitrary code to find all the arguments \nwhen the language designer feels the need to complicate things. 3. Implementation In the preceding section \nwe argued for lexical semantics, operator prece\u00addence and a variety of ways of supplying arguments. Tn \nthis sectionwe reduce this to practice. To combine lexical semantics with a procedural approach, we assign \nto each semantic token a program called its semantic code, which contains almost all the information \nabout the token. To translate or interpret a string of tokens, execute the code of each token in turn \nfrom left to right. Many tokens will expect arguments, which may occur before or after the token. If \nthe argument always comes before, as with unary postfix operators such as 1!t 11 ., we may parse expressions \nusing the following one-state parser. q~ left + run code; advance t) This parser is initially positioned \nat the beginninq of the input. It runs the code of the current token, stares the result in a variable \ncalled left , advances the input, and repeats the pro\u00adcess. If the input is exhausted, then by default \nthe parser halts and returns the value of left! . The variable leftt may be consulted by the code of \nthe next tokenr which will use the value of left! as either the translation Or value of the left-hand \nargument, depending on whether it is translating or interpre. ting. Alternatively, all arguments may \nappear on the right, as with unary pre\u00adfix operqtors such as log and sin~, In this case the code of a \nprefix operatQr can get its argument by calling the code of the following token. This pro\u00adcess will continue \nrecursively until a token is encountered (e.g., a variable or a constant) that does not require an argument. \nThe code of this token returns the appropriate translation and then so does the code of each of the other \ntokens, in the reverse of the order in which they were called. Clearly we want to be able to deal with \na mixture of these two types of tokens, together with tokens having both kinds of arguments (infi operators). \nThis is where the problem of association arises, for which we recommended operator precedence. We add \na state to the parser, %hus : ~o c -+ code; advance; I left + run c ql rbp < lbp/ B  Starting in state \nqo, the parser inter\u00adprets a token after advancing past that token, and then enters state ql. If a certain \ncondition is satisfied, the parser returns to qO to process the next token: otherwise it halts and returns \nthe value of left by default. We shall also change our strategy when asking for a right-hand argument, \nmaking a recursive call of the parser it\u00adself rather than of the code of the next token. In making this \ncall we supply the binding power associated with the desired argument, which we call the rbp (right binding \npower), wh~se value remains fixed as this incarnation of the parser runs. The lbp (left binding power~ \nis a Property of the current token in the input stream, and in general will change each time state q \nis entered. left binding powe~ ts tFieenlyp~&#38;rty of the token not in its semantic code. To return \nto q we require rbp ~ Ibp. Ig this test fail 8 , then by default the parser returns the last value of \n~left to whoever called it, which corresponds to A getting E in AEB~ if A had called the parser tiiat \nread E . If the test succeeds, the parser enters state qo, in which case B[ gets ~E~ instead. Because \nof the possibility of there being several recursive calls of the parser running simultaneously, a stack \nof return addresses and right binding powers must be used. This stack plays essentially the same role \nas the stacks described explicitly in other parsing schemes. lie can embellish the pa,rser a little by \nhaving the edge leaving ql return to A rather than qo. This may appear w steful since we have to repeat \nthe q -ql code on the q -q edge as Well. H8wever, this chang &#38; allows us to take advantage of the \ndistinction between c1 and ql, namely that left is unde\u00adf!?ned in state q. and defined inq that is, some \nexpression precedes 4 token interpreted during the q -q transition but not a token ink LJrp eted during \nthe q -q transition. We will call th~ c~de denoted by a token with (without) a Precedinq expression i,ts \nleft (nuli) d~notatio~ or-led (nud). The machin~comes or by split\u00adting trans-nudc+nud; itions andadvance; \nusing aleft+run c led stack instead c q of variables (the state = advance; rbp<,lbp/ the variable run \nc+~ed; on the stack) : advance; left+run c rbp<lb / left  illLL It now makes sense for a token to denote \ntwo different codes. For example, the nud of -! denotes unary minus, and its led, binary minus. We may \ndo the same for / (in\u00adteger-to-semaphore conversion as fn Algol 68, versus division], ( (syntactic grouping, \nas in a+(bxc), versus applications of variables or constants whose value is a function, as in Y(F) , \n(IX.X2] (3), etc.), and E (the empty stxing versus the membership relationl . A possibly more important \nrole for nuds and leds is in errcm detec\u00adtion. If a token only has a nud and is given a left argument, \nor only has a led and is not given a left argument, or has neither, therm on-existent semantic code is \ninvoked, which can be arranged to result $n the calling of an error routine. 50 far we have assumed that \nsemantic code optionally calls the parser once, and then retumxs the appropriate translatiem. One is \nat liberty to have more elaborate code, however, when the code can read the input (but not-backspace \nit) , request and use arbitrary amounts of storage, and carry out arbitrary computations in whatever \nlanguage is available (for which an ideal choice is the language being defined). These capa\u00adbilities \ngive the apprQach the pQwer of a Turing machine, to be used and abused by the language implementer as \nhe sees fit. While one may object to all this power on the ground that obscure language descriptions \ncan then be written, for practical purposes the same objection holds for BNF grammars, of which some \nquite obscure yet brief examples exist. In fact, the argument really runs the other way; the cooperative \nlanguage implementer can use the extra power to produce more comprehensible implementations, as we shall \nsee in section 4. One use for this procedural capability is for the semantic code to read the delimiters \nand the arguments following them if any. Clearly any delimiter that might come directly after an argument \nshould have a left binding power no greater than the binding power for that argument. For example, the \nnud of if , when encountered in the context if a then b else C( ~ may call the parser for a~ verify that \nthen is present, advance, call the parser for b , test if else! is pre\u00adsent and if so then advance and \ncall the parser a third time. (.This resolves the dangling else in the usual way.] The nud of ( will \ncall the parser, and then simply check that ) is pre= sent and advance the input. Delimiters of course \nmay have multiple parents, and even semantic code, such as 1 , which might have a nud ( absolute Value \nof as in IX ~~~ )r~nd two parents, it\u00adself and !+ !a+blc~ is shorthand for if a then b else c ]. The \nease with which mandatory and optional delimiters are dealt with constitutes one of the advantages of \nthe top down approach over the conventional methods for implementing operator precedence The parser s \noperation may perhaps be better understood graphically. Consider the example if 3*a + b!+-3 = O then \nprint a + (b 1) else rewind . We may exhibit the tree recovered by the parser from this expression as \nin the diagram below. The tokens encountered during one incarnation of the parser are enclosed in a dotted \ncircle, and are connected via down-and-left links, while calls on the parser are connected to their caller \nby down-and-right links. Delimiters label the links of the expression they precede, if any. The no-op \n( is included, although it is not really a semantic object. The major difference between the approach \ndescribed here and the usual operator precedence scheme is that we have modified the Flovd o~erator precedence \nparser to work top-down, _implementing the stack by means of recursion, a technique known as recursive \ndescent. This would appear to be of no value if it is necessary to imple\u00adment a stack anyway in order \nto deal with the recursion. However, the crucial pro\u00adperty of recursive descent is that the stack entries \nare no longer just operators or operands, but the environments of the pro\u00adgrams that called the parser \nrecursively. When the programs are very simple, and only call the parser once, this environment gives \nus no more information than if we had semantic tokens themselves on the stack. When we consider more \ncomplicated sorts of constructions such as operators with various default parameters the technique becomes \nmore interesting. While the above account of the al\u00adgorithm should be more or less self-explana\u00adtory, \nit may be worth while summarizing the properties of the algorithm a little more urecisel~. befiniti;n. \nAn expression is a string S such that there exists a token t and an environment E in which if the parser \nis started with the input at the beginning of St, it will stop with the input at t, and return the interpretation \nof S relative to E. Properties. (i) When the semantic code of a token t is run, it begins with the input \npositioned just to the right of that token, and it returns the interpretation of an expression ending \njust b;fore the final position of the input, and starting either at tif tis anud, or if tis aled then \nat the beginning of the expression of which left was the interpretation when the code of t started. (ii) \nWhen the parser returns the interpre\u00adtation of an expression S relative to en\u00advironment E, S is immediately \nfollowed by a token with lbp~rbp in E. (iii) The led of a token is called only if it immediately follows \nan expression whose interpretation the parser has assigned to left . (iv) The lbp of a token whose led \nhas just been called is greater than the rbp of the current environment, (v) Every expression is either \nreturned by the Parser or given to the following l~d via- left . \u00ad[vi>. . A token used only as a nud \ndoes noc need a left binding power.  These proverties are the ones that make the algorithm useful. They \nare all straight\u00adforward to verify. Property (i) says that a semantic token pushes the input pointer \noff the right end of the expression whose tree it is the root, Properties (ii), (iv) and (v) together \ncompletely account for the two possible fates of the ntents of left . Property (iii) guarantees that \nwhen the code of a led runs, it has its left hand argument interpreted for it in left , There (ii) boole(m,x,y): \nforms the bitwise is no guarantee that a nud is never preceded by boolean combination of strings x and \nY, an expression; instead, property (v) guards against losing an expression in left by calling a nud \nwhich does not know the expres\u00adsion is there. Property (vi) says that binding powers are only relevant \nwhen an argument is involved. 4 %%%% e~am~les we shall assume that lbp,nud-and led ar~ reallv the functions \nIbp(token), nud(token) and led(token). To call the parser and simultaneously establish a value for rbp \nin the environment of the parser, we write parse (rbp), passing rbp as a parameter. when a led runs, \nits left hand arguments interpretation is the value of the variable left, which is local to the parser \ncallfig that led. Tokens without an explicit nud are assumed to have for their nud the value of the variable \nnonud , and for their led, noled . Also the variable self will have as value the token whose code is \nmissing when the error occurs. In the language used for the semantic code, we use a + b to define the \nvalue of expression a to be the value of expression b (not b itself); also, the value of a + b is that \nof b. The value of an expression is itself unless it has been defined ex\u00adplicitly by assignment or implicitly \nby procedure definition; e.g., the value of 3 is 3, of 1+1, 2. We write a to mean the expression a whose \nvalue is a itself, as distinct from the value of a, e.g. II+l!must be evaluated twice to yield 2. A string \nx is written x ; this differs from x only in that x is now assumed to be a token, so that the value of \n1+1 is the token 1+1, which does not evaluate to 2 in general. To evaluate a, then b, re\u00adturning the \nvalue of b, write a;b. If the value of a is wanted instead, write aGb. (These are for side-effects.) \nWe write (check X) for (if token = xthen advance else (print missing ; print x ; halt)). Every\u00adthing \nelse should be self-explanatory. (Since this language is the one implemented in the second example, it \nwill not hurt to see it defined and used during the first.) We give specifications, using this approach, \nof an on-line theorem prover, and a fragment of a small general-purpose programming language. The theorem \nprover is to demonstrate that this approach is useful for other applications than just programming languages. \nThe translator demonstrates the flexibility of the approach. For the theorem prover s semantics, we assume \nthat we have the following primitives available: (i) generate; this returns the bit string ~klk and also \n doubles k, assumed 1 initially. where m is a string of four bits that specifies the combination in \nthe obvious way (1000 = Q, 1110 = or, 1001 = eqv etc)o If one string 1s exhaust= before t=other, boole \ncontinues from the beginning of the exhausted string, cycling until both strings are exhausted simultaneously. \nBoole is not defined for strings of other than O s and 1 s. (iii) x isvalid: a predicate that holds only \nwhen x is a string of all ones. We shall use these primitives to write a prog~arn which will read a zero-th \norder proposltlon, parse it, determine the truth\u00adtable column for each subtree in the parse, and print \ntheorem or non-theorem when ? is encountered at the end of the proposi\u00adtion, depending on whether the \nwhole tree returns all ones. The theorem prover is defined by evaluating the following expression. nonud \n+ if null led(self) then nud(self) -+ generate else (print self; print has no argument ) ; led( ? ) \n+ if left isvalid then print theorem else print non-theorem ; parse 1 ; lbp( ? ) + 1; nud( ( ) + parse \nO 6 check ) ; lbp( ) ) + O; led( -+ ) i- boole( llOl , left, parse 1) ; lbp(!!.+!!) +-2; led( v ) -+ \nboole( lllO , left, parse 3) ; lbp( v ) +-3; led( A ) + boole( 1000 , left, parse 4) ; lbp( A ) + 4; \nnud(!!-!t) + boole( OlOl , parse 5, O ) . To run the theorem prover, evaluate k+l; parse O . For example, \nwe might have the following exchange: (a+b)A(b+c)+(a+c)? theorem a? non-theorem av-a? theorem until \nwe turn the machine off somehow. The first definition of the program deals with new variables; which \nis anything without a prior meaning that needs a nud. The first new variable will get the constant 01 \nfor its nud,the next 0011, then 00001111, etc. Next, ? is defined to work as a delimiter; it responds \nto the value of its left argument (the truth-table column for parses a list of expressions delimited \nby the whole proposition), processes the next a s, parsing each one by calling parse b, proposition by \ncalling the parser, and and it returns a LISP list of the results. returns the result to the next level \nparser. This parser then passes it to the next ? The object is to translate, for as its left argument, \nand the process example, a+b into (PLUS a b) , a;b into con=ues, without building up a stack of (PROG2 \na b), a&#38;b into (PROG2 nil a b), IT?f?!s since  ? is left associative. -a into (MINUS a) , Ax,y, \n. . ..z.a into (LAMBDA (x y . . . z) a) , etc. These target Next, ( is defined to interpret and objects \nare LISP lists, so we will use [ return an expression, skipping the follow-to build them; [a,b, . . ..c \n] translates ing ) . The remaining definitions should into (LIST a b . . . c) . be self-explanatory. \nThe reader interested A fragment of the definition of L: in how this approach to theorem-provers works \nis on his own as we mainly concerned nilfix right [ PARSE , bp] $ here with the way in which the definitions \ninfixr ; 1 [ PROG2 , left, right] $ specify the syntax and semantics of the infixr 6 1 [ PROG2 , nil, \nleft, right] $ language. prefix is 1 [ LIST , right, left , [ PARSE , bp]] $ The overhead of this approach \nis infix $ 1 (print eval left; right) $ almost negligible. The parser spends prefix delim 99 [ DELIM \n, token G advance] $ possibly four machine cycles or so per prefix 0 [ QUOTE , right 6 check ] $ token \n(not counting lexical analysis), and delim $ the semantics can be seen to do almost prefix [ 0 ( LIST \n getlist bp nothing; only when the strings get longer ~ check l ) $ than a computer word need we expect \nany j:;;: 1 $ significant time to be spent by the logical $ operations. For this particular interpreter, \nprefix ~ O (right G check ) ) $ this efficiency is irrelevant; however, for ::;;; ~ $ a general-purpose \ninterpreter, if we prepro-2S (left . f token # ) then cess the program so that the lexical items (  \nget ist O) 6 check ) become pointers into a symbol table, then elle nil $, the efficiency of interpreting \nthe resulting infix getlist 25 is GETLIST $ string would be no worse than interpreting prefix if 2 [ \nCOND , [right, a tree using a tree-traversing algorithm check then ; right]] as in LISP interpreters. \n@ (if token = else then (advance; [[right]])) $ For the next example we describe a delim then $ translator \nfrom the language used in the delim else $ above to trees whose format is that of the nilfix advance \n[ ADVANCE ] $ internal representation of LISP s-expressions, prefix check 25 [ CHECK , right] $ an ideal \nintermediate language for most infix -+ 25 [t SETQ , left, parse(l)] $ compilers. prefix 1 0 [ LAMBDA \n,  getlist 25 G check ; ; right] $ In this example we focus on the prefix + 20 right $ versatility \nthe procedural approach gives infix + 20 is PLUS $ us , and the power to improve the descrip-prefix -20 \n[ MINUS , right] $ tive capacity of the metalanguage that we infix -20 is DIFFERENCE $ get from bootstrapping. \nSome of the infix X 21 is TIMES $ verbosity of the theorem prover can be infix + 21 is QUOTIENT $ done \naway with in this way. infixr + 22 is EXPT $ infixr + 22 is LOG We present a subset of the definitions \nprefix I O [ ABS ri~ht G check l ] $ > of tokens of the language L; all of them delim ~ infixr $ are \ndefined in L, although in practice one 14 is APPEND $ would begin with a host language H (say infixr \n. 14 is CONS $ the target language, here LISP) and write prefix a 14 [ CAR , right] $ as many definitions \nin H as are sufficient prefix 6 14 [ CDR , right] $ to define the rest in L, We do not give infix E 12 \nis MEMBER $ Che definitions of nilfix, prefix, infix infix = 10 is EOUAL $ or infixr here; however, they \nperform infix # 10 [ NOTti,[ EQUAL ,left,right] 1 $ assignments to the appropriate objects; infix ~ \n10 is LESSP $ (nilfix a b) performs nud(a)+ b , infix > 10 is GREATERP (prefix a b c) sets bp+b before \nperforming nud(a)+- c , (infix a b c) does the same as (prefix a b c) except that the led is and so on, \ndefined instead and also lbp(a)+b is done, and infixr is like infix except that The reader may find some \nof the boot\u00adbp+b-1 replaces bp+b. The variable bp is strapping a little confusing. Let us available for \nuse for calling the parser consider the definitions of right and + . when reading c. Also (delim x) does \nThe former is equivalent to lbp(x)+-O. The function (a getlist b) nud(right) + [ PARSE , bp] . e.g. \n The latter is equivalent to nud(+) + parse(20) and led(+) + [ PLUS , left, parse(20)] , because when \nthe nud of right is encountered while reading the definitions of + , it is evaluated by the parser in \nan environment where bp is 20 (assigned by prefixlinfix). It is worth noting how effectively we made \nuse of the bootstrapping capability in defining is , which saved a considerable amount of typing. With \nmore work, one could define even more exotic facilities. A useful one would be the ability to describe \nthe argument structure of operators using regular expressions. The is facility is more declarative than \nimperative in flavor, even though it is a program. This is an instance of the boundary between declarative \nand imperatives becoming fuzzy. There do not appear to be any reliable ways of distinguishing the two \nin general. 5. Conclusions We argued that BNF-oriented approaches to the writing of translators and interpreters \nwere not enjoying the success one might wish for. We recommended lexical semantics, operator precedence \nand a flexible approach to dealing with arguments. We presented a trivial parsing algorithm for realizing \nthis approach, and gave examples of an interpretive theorem prover and a trans\u00adlator based on this approach. \nIt is clear how this approach can be used by translator writers. The modularity of the approach also \nmakes it ideal for implementing extensible languages. The triviality of the parser makes it easy to implement \neither in software or hardware, and efficient to operate. Attention was paid to some aspects of error \ndetection, and it is clear that type checking and the like, though not exemplified in the above, can \nbe handled in the semantic code. And there is no doubt that the procedural approach will allow us to \ndo anything any other system could do, although conceivably not always as conveniently. The system has \nso far found two practical applications. One is as the front-end for the SCRATCH-PAD system of Greismer \nand Jenks at IBM Yorktown Heights. The implementation was carried out by Fred Blair. The other application \nis the syntactic component of Project MAC s Mathlab system at MIT, MACSYMA, where this approach added \nto MACSYMA extension facilities not possible with the previous precedence parser used in MACSYMA. The \nimplementer was Michael Genesreth. 6. Acknowledgments I am indebted to a large number of people who have \ndiscussed some of the ideas in this paper with me. In particular I must thank Michael Fischer for supplying \nmany valuable ideas relevant to the implementation, and for much programming help in defining and implementing \nCGOL, a pilot language initially used to break in and improve the system, but which we hope to develop \nfurther in the future as a desirable programming language for a large number of classes of users. 7. \nReferences Aho, A.V. 1968. Indexed Grammars. JACM ~, 4, 647-671 Chomsky, N. 1959. On certain formal properties \nof grammar. Information and Control, ~, ~, 137-167. Fischer, M.J. 1968. Macros with Grammar\u00adlike Productions. \nPh. D. T hesls, Harvard University. Floyd, R.W. 1963. Syntactic Analysis and Operator Precedence. JACM \n10, 3, 316-333. Knuth, D.E. 1965. On the transl~ion of lanwages from left to right, Informa\u00adtio~ a~d \nControl, 8, 6, 607-639 Leavenworth, B.N. SyntaF macros and extended translation. CACM, ~, 11, 790-793. \n1966. Lewis, P.M., and R.E. Stearns. 1968. SYntax\u00addirected transduction, JACPI ~, 3, 465-488. McKeeman, \nW.M., J.J. Horning and D.B. Wort\u00admanl 1970~ A Compiler Generator. Prentice-Hall Inc. Englewood Cliffs, \nN.J. Minsky. M.L. 1970. Form and Content in bomputer Science. Turing Lecture, JACM ~, 2, 197-215. Van \nWIJngaarden, A., B.J. Mailloux, J.E.L. Peck and C.H.A. Koster. 1969. ReDort on the Algorithmic Language \nALG- Mathematisch Centrum, Amsterdam, MR 101. Wirth, N. 1971. The programming language PASCAL . Acts \nInformatica, ~, 35-68. \n\t\t\t", "proc_id": "512927", "abstract": "", "authors": [{"name": "Vaughan R. Pratt", "author_profile_id": "81100298352", "affiliation": "Massachusetts Institute of Technology", "person_id": "PP39036582", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512927.512931", "year": "1973", "article_id": "512931", "conference": "POPL", "title": "Top down operator precedence", "url": "http://dl.acm.org/citation.cfm?id=512931"}