{"article_publication_date": "10-01-1973", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee.&#38;#169; \n1973 ACM 0-12345-678-9 $5.00 \\ DETERMINISTIC PARSING OF AMBIGUOUS GRAMMARS A. V. Aho an~ S. C. Johnson \nBell Laboratories Murray Hillj New Jersey 07974 J. D. Unman* Princeton University Princeton, New Jersey \n08P40 ABSTRACT We consider methods of describing the syntax of programming languages in ways that are \nmore flexible and natural than conventional BNF descriptions. These methods in\u00advolve the use OT ambiguous \ncontext-free grammars together with rules to resolve syntactic ambiguities. We show how efficient LL \nand LR parsers can be constructed directly from certain classes of these specifications. I. INTRODUCTION \nThere has been much work on developing efficient parsing methods (see [AU1]) such as the LL, LR and precedence \nbased techniques. This work has focused on mechanical ways of ~roducing deterministic (no backtracking) \nwarsers for certain classes of context\u00ad \\ -,fr;e graiunars. &#38;ce one has a grammar in the requ~red \nclass~an efficient parser can be constructed automatically. Unfortunately, the most natural grammar describing \na language is frequently not in the class of grammars for which we have known how to mechanically construct \nefficient parsers. Consequently, the grammar must be rewritten by hand to make it fall into the required \nclass. While the necessary manipulations are not hard to do after some practice, they often expand the \nsize of the grammar and introduce seemingly ext~aneous nonterminal symbols . For example, the natural \ngrammar for simple arithmetic expressions in + and * is: Gl: <exp> +<exp> + <exp>l<exp> * <=P>I (<=w>) \nI Q However, G1 is ambiguous. The most natural LR(l) grammar for this language is: G2: <exp> + <exp> \n+ <term> l<term> <term> + <term> * <factor> I< factor> <factor> + (<exp>) I id The introduction of the \nnonterminals <term> and <factor> enforces the (here unstated) assumption that * takes precedence over \n+ and that + and * are each left asso\u00adciative. In e.o doing, the grammar becomes unambiguous but larger. \nThe resulting grammar GP can be parsed efficiently by various means such as the simple LR or weak precedence \nt~chniques; *Work of this author ~artially supported by NSF Grants GJ 474 and GJ 35570. Despite various \nfacilities which have been created for the automatic generation of parsers from grammars (e.g., [MHW,DI), \nwe are told by McClure [M] that virtually all compilers in existence use either operator precedence [F%] \nor recursive descent [c] tech\u00ad niques. The latter two methods are not really gramar based, so it is often \ndifficult to be sure of what language is being parsed. (See [BU], e.g., for some undecidability results \nregarding recursive descent parsers. ) Nevertheless, in skilled hands recursive descent and operator \nprecedence methods perform satisfactorily and can be quite efficient. For example, operator precedence \nparsers can be made to behave as though they are parsing according to the grammar G1 above, while a pure \nLR parser would have to parse according to G2, necessitating reductions by productions such as <exp> \n+ <term> which are wasteful of time, (However, optimization techniques such as those in [AU2] will eliminate \nsuch re\u00ad ductions in this and many other situations.) It is likely, therefore, that mechanical parser \ngenerating techniques will never be widely accepted until they are capable of producing parsers that \nare as simple and effi\u00adcient as those produced by skilled hands using ad hoc techniques. In this paper \nwe would like to set down some of the ideas which we feel will be useful in the development of auto\u00admatic \nparser constructors that meet this criterion. Principally, we feel that many of the parsing techniques \nwhich have heretofore been applied only to unambiguous grammars apply equally well to certain ambiguous \nones, especially to the natural grammars describing pro\u00adgramming language constructs. We shall do three \nthings in this paper. 1. We shall give some examples of how ambiguities in grammars can be naturally \nresolved using operator precedence and recursive descent techniques. 2. We shall consider how LL or \nLR-like parsers can be mechanically built from the same grammars by using rules analogous to (1) to resolve \nambiguities. 3. We shall define a generalization of the class of LL grammars that permits mechani\u00adcal \ngeneration of parsers. This class includes some ambiguous grammars (including the dangling elsell grammar \ndescribed below, which gave [LR] problems in their LL(l) based ALGOL compiler).  This paper is intended \nto be an informal exploration of what might be done to treat ambiguous grammars, rather than a rigorous \ntheoretical treatment of the subject. II. Context-Free Grammars We assume that the reader is familiar \nwith BNF notation. The more formal context-free grammar notation is explained in [HU and AU1]. We shall \nreview only the salient aspects of context-free grammars here. Definition: A context-free grammar (CFG \nfor short) consists of: 1. a finite set of nonterminals (which we denote either in BNF style, e.g., < \nexp> , or as upper case letters, e.g., A,B,... )3 2. a finite set of terminals (for which we use a,b, \n. . . or certain special symbols such as +, id, etc.),  3. a finite set of productions of the form \nA 4 a, where A is a nonterminal and a is a  (possibly empty) string of terminal and nonterminal symbols. \nWe use the BlyF nota\u00adt ion to stand for the set of productions A dal, A +a2, . . . , A -+an. 4. One nonterminal \nis distinguished and called the start symbol. The nonterminals and terminals of a grammar will be called \nthe grammar symbols . Example 1: G2 mentioned previously has nonterminals <exp>, <term> and <factor> \nand terminals +, *, (,) and id The productions are: <exp> + <exp> + <term> <exp> + <term> 2 <term> + \n<term> * <factor) < term> + < factor> <factor> ~ ( <exp> ) < factor> + id  Presumably, < exp> is the \nstart symbol, although this was not previously indicated. CFGIS define languages by means of derivations. \nAny nonterminal A in a string of grammar symbols may be replaced by a right side of one of its productions. \nThat is, if czA~ is any string of terminal and nonterminal symbols (a and @ themselves stand for arbi\u00ad \ntrary strings, possibly empty) and A -+ y is a production, then we write aA~ ==> ayp. If A is the leftmost \nnonterminal (a contains no nonterminals), then we call this a leftmost step and write aA@ =~> ayp. Similarly, \nif A is the rightmost nonterminal, the~te aA~ ===> aye and call this a rightmost step. We use a star \nabove ==>, ==> and ==> to indicate repeated application of pro\u00adlg rm ductions. For example, we write \na ==> at if there is a sequence @l,p2, . . ..@r of strings such that i. @l=a ii. @r =a iii. for each \ni, Pi => ~i+~ The case r = 1, in which a = at (and (iii) is vacuous) is not ruled out. Example 2: A derivation \nin G2 is <exp> ==> <exp> + <term> ==> <term> + <term> ==> <factor> + <term> ==> id + <term> ==> id + \n<factor> ==> id +id . Thus, we may write <exp> ==> id + id. Also, at each step above, the leftmost non\u00adterminal \nis replaced, so we coul~rep~ce ==> by ==> at each step and conclude lm <exp> ;~}g+~. Hclwever, certain \nsteps in this derivation, such as <exp> + 41,, term>,==> <term> + <term>, are not rightmost. Thus, we \ncannot immediately conclude { exp > ==> id + id, although this relation happens to be true, as one can \nsee from the rm rightmost derivation <exp> ==> <exp> + <exp> ===> <exp> + <factor> ==> <exp> + id ==> \n<term> + id ==> <factor> + id ==> id +id Definition: If S is the start symbol of a grammar and S =5> \nU, then a is called a sentential form. If S =3> a or S =g> a, then a is a left (right) sentential =, \nres\u00ad lm m pectively. The sequence of steps leading to a from S is called a leftmost (rightmost) derivation. \nEvery sentential form has a leftmost and a rightmost derivation (among others) IAUIJ . A string is in \nthe language defined by a CFG if and only if it is a sentential form that contains no nonterminals. \nDefinition: A grammar is unambiguous if and only if no string in the language generated by the gramar \nhas more than one leftmost derivation, or equivalently has more than one rightmost derivation. Otherwise, \nthe grammar is said to be ambiguous. Example 3: For example, the string id + id + id has Gramar G1 is \nambiguous. . _ the two leftmost derivations <exp> ==> <exp> + <exp> <exp> ==> <exp> + <exp> ==> <exp> \n+ <exp> + <exp> ==> id + <exp) ==> id + <exp> + <exp> ==> id + <exp> + <exp> ==> id +id + <exp> ==> \nid+id+ <exp> ==> id +id +id ==> id +id +id _ _ We can make an ambiguous grammar unambiguous by defining \na function that accepts or rejects leftmost (rightmost) derivations of the grammar in such a manner that \neach string of terminals has at most one acceptable leftmost (rightmost) derivation. We shall call such \na function a disambiguating rule. In this paper we shall specify disambiguating rules rather informally. \nFor example, for G~j we can specify a disambiguating rule that -1-and * are to be left associ\u00adative and \nthat * is to have higher precedence than +. Some of these ideas were developed independently in [Es]. \nDefining a language in terms of an ambiguous CFG plus a set of disambiguating rules offers certain advantages. \nIt is possible to define a CFG plus a disambiguating rule that will define a language that is not context-free \n[GS]. However, this is not our intent here. We shall use ambiguous CFGIS and disambiguating rules to \nspecify traditional programming language constructs more economically and more clearly than would be \npossible with an equivalent unambiguous CFG. In addition, we shall show how we can construct, directly \nfrom certain ambiguous grammars and disambiguating rules, parsers that are more efficient than those \nconstructed from the equivalent unambiguous CFGIS. One other application of the use of ambiguous grammars \nis in code optimization. We might include productions for certain special subexpressions and statements \nfor which we can construct highly efficient object code. Our disambiguating rule here might be to use \nthese special productions wherever possible. For example, if there is a special ob\u00adject language instruction \nto add a constant to an expression, we might write the expres\u00adsion gramar: 1. <exp> -> <exp> + constant \n 2. <exp> -><exp> + <exp> 3. <exp> -><exp> * <exp> 4. <exp> -> (<exp>) 5. <eXp> -) variable 6. <exp> \n-> constant  Our disambiguating rule in addition to that given for G1 above might also specify that \nproduction (1) is to be used in preference to productions (2) and (6) wherever possible, consistent with \nthe rule that * takes precedence over -1-. The semantic action associated with production (1) would generate \nthe more efficient object language instructions. 4 III . LL(l) Grammars and Parsers For certain grsmmars \nwe can find a leftmost derivation (left parse) of any string in the language by a simple deterministic \nautomaton, shown schemally in Fig. 1, called a predictive parser. input .. e:Put point~r,. , Predictive \n: Parsing Table . . ... -s\u00ad tack pointer -~ stack Fig. 1 Predictive Parser There is an input tape upon \nwhich is placed the string to be parsed. An input pointer indicates the current input symbol. Initially \nthe input pointer is at the left\u00admost symbol of the input string. There is a stack which initially contains \nonly the start symbol. The unique symbol $ marks the end of the input and of the stack. At every point \nin time, if the input (xay in Fig. 1) is in the language, then x, the portion of the input over which \nthe input pointer has moved, followed by Aa, the stack contents, will be called the left sentential form \nrepresented by the parser. In Fig. 1 xAa is the left sentent~al form represe~d~the parser. Note that \nx is a string of terminals, A is the leftmost nonterminal and a is a string of terminals and nonterminals. \nThe moves of the parser are of three types. 1. If the current input symbol matches the top stack symbol, \npop the stack and move the input pointer one symbol right. This move is made only with a terminal on \ntop of the stack and does not change the left sentential form represented by the parser. 2. If there \nis a nonterminal, say A, on top of the stack, and a is the current input symbol, consult a parsing table \nto determine a particular production A + p associ\u00adated with A and a. Replace A on top of the stack by \nthe string p. The input pointer is not advanced. This move causes the parser to represent the next left \nsentential fem. 3. If the current input symbol is $ and the stack contains only $, then the parser halts \nand accepts the original input.  Example 4: Consider the CFG A+aAlbB A-a B+b with the start symbol S. \nThe appropriate parsing table is ab $ S S->~-S-> bB error error error With input aa the parser would \nmake the following moves. Here indicates the location of the input and stack pointers. The symbol $ \nis used to mark the end of the input and stack. left sentential form input stack represented by the parser \n;a$2$ s Za$ ~A$ aA aZ$ x$ aA a;$ ;$ aa aa$ aa $ Definition: Let X be a nonterminal or terminal in a CFG. \nFOLLOW(X) is the set of terminals that can appear immediately to the right of X in a sentential form. \nIf X can be the rightmost symbol in a sentential form, then we shall include $ in FOLLOW(X). Definition: \nA gramar is LL(l) if whenever A +a and A + @ are distinct pro\u00adductions, the following two conditions \nhold: 1. There is no terminal a such that both a and p derive a string beginning with a. 2. At most \none of a and @ is or derives e, the empty string. If, say, a ==> e, then there is no terminal a in FOLLOW(A) \nsuch that p derives a string beginning with a.  For an LL(l) gramar, we can construct a predictive parser \neasily. The entry for (A,a) in the parsing table is the unique production A + y, if it exists, such that \ny =+> aij for some 5 or Y ==> e and a is in FOLLOW(A). If no such y exists (and there can\u00adnot be more \nthan one by the LL(l) definition), then the entry for (A,a) is error. Example 5: The (S,a) entry in Example \n4 is S + aA, since aA ==> aA (in zero steps), but bB, the other right side for A, does not derive any \nstring beginning with a. LL(l) grammars are a natural class of unambiguous grammars for which recursive \ndescent parsers which never backtrack can be fabricated, and they have been so studied by [K, LS, w]. \nHowever, there are certain ambiguous gramars for which predictive parsing algorithms (nonbacktracking \nrecursive descent parsers) exist and which, like all ambiguous grammars, are not LL(l). We now consider \none example in detail. Suppose we have a language such as PL/I with permissible statement constructions \nof the forms if. ..theelseelse and also simply if...=. Part of the grammar for such a _ language migh~be: \n<stat> +~ <boolean exp> then <stat> else <stat>/ if <boolean exp> then <stat> I c Here c stands for \nother productions for <stat> . Such a grammar must be ambiguous. To see this, let us instead consider \nthe CFG S+s,sbslasle 3: which can be thought of as modeling the above gramar with S standing for <stat> \n, a standing for if <boolean exp> ~, and b standing for else. For example, aacbc models a statement o~the \nform then if B2 then S1 else S2 ~%-. 6 where B1 and B2 are expressions, and S~ and S2 are statements \nThe else in the above statement is dangling , in the sense that it is not immediately clear whether S2 \nis to be executed when Bl is false or when B2 is false. The string aacbc has the following two leftmost \nderivations: S ==> aSbS ==> aaSbS ==> aacbS ==> aacbc S ==> aS ==> aaSbS ==> aacbS ==> aacbc The usual \nsolution to this ambiguity is to specify a disambiguating rule that says an else is to be associated \nwith the previous unelsetd then. In other terms, S2 is executed when B2 is false, and only the second \nof the~ve two derivations is correct. A skilled person writing a recursive descent parser would have \nno trouble implementing this rule; in fact, it is probably reasonable to claim that only the customary \nresolution of the dangling else ambiguity is easy to implement by any parsing method. The recursive descent \nparser would, if the implementer was clever enough to left factor the grammar [S], wcmk as follows: 1. \nGiven that we need to find a <stat> on the input, search for if <boolean exp>  then <stat>.  2. If \n(1) succeeds, check if the next input symbol is else. If so, succeed if and only if the else is followed \nby a <stat>. If else~not the next input symbol,  however, succeed without looking further. The above \nstrategy not only works, it never causes backtracking. Thus, it should not be surprising that we can \ndesign a predictive parser for the left factored form of G3, even though that grammar may easily be shown \nnot to be LL. Indeed, the language generated by G3 is not LL. The left factored version of G3 is: G4: \ns+a,s S /C s) +b SIe The predictive parser for G4 is: a bc$ ,5 S -> aSS error S->c error ;~, error S \n-> bS error S) -> e For example, the predictive parser would process aacbc as follows: input stack ~acbc \n3$ agcbc :ss!$ agcb c Ss $ aa~bc &#38;WWf$ aa~bc :s s $ aac~ c ~sfst$ aac~ c ~ s aacb~ pis~ 1 aacbg ~s \n$ aacb c :s1$ aacbc aacb c ! $ A curious question is how the compiler designer is to know that his attempt \nto resolve the dangling ~lse or any other ambiguity works, in the sense that he is not changing the language \nrecognized by the compiler when he restricts his parser to throw away certain legal parses. In general, \none cannot effectively decide such matters, but we shall see that G4 is in a class where the usual resolution \ncan be shown not to change the language defined by the grammar. Perhaps the best answer to the question \nis that in reality, languages were always defined with the parsing method in mind. What a recursive descent \nparser would recognize became the standard, and gramars were later written to (hopefully) fit the language \nrecog\u00adnized. Of course, any philosophical notions ignore the real problem, which is that it is in general \nimpossible to tell whether an ad hoc parser agrees with a language definition. IV. An Extension to the \nLL Class of Grammars We shall now consider two classes of gramars which include the LL(l) grammars and \nalso some ambiguous ones. The LL(k) grammars for k > 1 can be extended in the same way, and the details \nshould be obvious to the reader who is acquainted with LL(k) grammars. Suppose we are in a situation \nwhere we have derived the left sentential form wAa and there are two productions A * a and A -~ such \nthat ay and @y each derive strings be\u00adginning with the terminal a. Then clearly we do not have an LL(l) \ngrammar. However, it is conceivable that in every situation where we are faced with a choice of expanding \nA by a or f3 one choice will always enable us to continue the parse to completion. Definition: A grammar \nis left recursive if it has a nonterminal A, used in t,he derivation of some terminal string, ~ th at \nA =3> Aa for some a in a derivation of one or more steps. Definition: We say a grammar G is in Class \n1 if: 1. G is not left recursive. 2. Let wAy be a left sentential form. Suppose A *a and A + ~ are \ntwo productions in G such that a st$ing beginning with a terminal a can be derived from both ~y and @y. \n~hen either~y ==> ax implies ay =~> ax for all terminal strings x or ay ==> ax Implies j3y ==> ax for \nall terminal strings x.  Condition (2) incorporates the disambiguating rule. Informally, (2) says that \na predictive parser may always expand A by the more inclusive production on lookahead a. Condition (1) \nprevents the parser from getting into a loop and assures that it will trace out a leftmost derivation \nof its input. Clearly, every LL(l) gramar is in Class 1, since condition (2) is trivially satisfied and \nno LL(l) grammar is left recursive. However, certain ambiguous grammars such as G4 are also in Class \n1, as we shall see. We should observe that it is not immediately apparent if we can effectively determine \nwhether a gramar is in Class 1. In fact, we shall show that if it is decidable whether a CFG is in Class \n1, then we can solve the inclusion problem for the languages generated by Class 1 grammars. We shall \ntherefore progress to a smaller class of grammars which is effective but contains at least G4 in addition \nto the LL(l) grammars. Definition: Given an CFG and a nonterminal A, define F(A) to be the set of symbols \nX no~termmal or terminal) such that there is some left sentential form wAyX6, such that ~ ==> e. (The \ncase where y is itself the empty string is not ruled out.) Definition: We say a CFG is in Class 2 if \nwhenever A + a and A -p are distinct . productions, the following two conditions hold: 1. There is no \nterminal a such that both a and @ derive a string beginning with a. 2. At most one of a and p is or \nderives e. Suppose p =2> e and a =%> ax for some terminal a such that a is in FOLLOW(A). Then for all \nX in F(A) such that X # A, X does not derive a string beginning with a. (This rules out the possibility \nthat X=a.)  We can easily show that (1) and (2) imply that there is no left recursion. It can also be \nshown that condition (2) of the Class 1 definition is satisfied. In particu\u00adlar, when condition (2) of \nthe Class 2 definition prevails and we are presented with a choice of expanding A by a or @ with a as \nthe next lookahead symbol, we always choose a. It is easy to compute F(A) for each nonterminal A and \nthus decide whether a grammar is in Class 2. Example 6: The grammar G4, the left factored version of \nthe dangling else grammar, has productions 8 We shall show that G4 is in Class 2, and hence in Class \n1. The productions for S satisfy the LL(l) definition, so we certainly get no contradiction of the Class \n2 condition. For the S1 productions, we observe that S1 + bS derives a string beginning with b, and b \nis also in FOLLOW(S~). We must therefore check that S is the only symbol in F(S)) that de\u00ad rives strings \nbeginning with b. We note that S is in F(S ), since S =%> aaSSrS , for example, However, there lm are \nno other members of F(S~), so condition (2) of the Class 2 definition is satisfied. Thus , on lookahead \nb, we must expand S! to bS rather than e. The predictive parser gen\u00aderated is the one given in Section \n3. Example 7: Let us consider a block structured language in which an end will terminate all open blocks. \nThe following gramar abstracts this situation. <stat> -> begin <list> end I 5: begin <list> ] c <list> \n-> <stat>; <list> I < stat> Here c stands for the productions for simple statements. The left factored \nform of G5,in which the productions are numbered is: G(;: (1) <stat> -> begin <list> < stail> (2) <stat> \n-> c (3) <stail> -> end (4) <stail> -> e (5) <list> -> <stat> <ltail> (6) <Itail> ->; <list> (7) \n<ltail) -> e  GG is not LI(l). For example, <stail> + end obviously derives a string begin\u00adning with \nend. But end is in FOLLOW( <stat> ), since~stat> =~> begin <list><s tail> ~, e.g. ~is observation, together \nwith the fact that <stail> end + e is a production violates the LL(l) definition. However, GG is in \nClass 2. It is eas to check that no violation of condition (1) occurs. To check condition (2) we find \nF( ?stail> ) = F( < ~tai~> ) =(< stai~> , <ltail ). we ItIUSt Chf3Ckj because of the pair of productions \n<stail> + end / e, that . 1 < ltail does not derive a string beginning with end. not GG de~ive is ifi \nAlso, because of a string beginning Class 2. - <ltail> with ;. + ; <list> Clearly, / e, neither we must \ncheck that of these derivations <stail> exist, does so The predictive parser for @ is: < stat> begin \n1 end _ c; $ < stail> 3 4 < list> < ltail > 5 J\u00ad5 6 7 Here the numbers refer to the productions in GG. \nWe would now like to show that every Class 2 grammar is in Class 1. The absence of left recursion is \neasy to demonstrate and we omit this part of the proof. We must show for Class 2 grammars that whenever \nA +a and A + P are two productions such that a is in FOLLOW(A), P =% e, a derives strings beginning with \na, and S =~> wAy ==> w13Y =%> wax m lm lm then ay =~> ax. Since the gramar is in Class 2, we may assume \nthat @ does not derive a string beginning with a, and that no symbol but A in F(A) derives strings beginning \nwith a. Then the derivation @y =~> ax can be written By =:> y =5> Xy =~> aYy ==> ax. That lm lmlm Im \nis, y = y Xyt, yhere y =%> e and X derives a string beginning with a (or X is ~. Then X is in F(A)l so \nX 1s A. Since X derives a string beginning with a, the first step in :he deri\u00advation X ==> ay must be \nX ==> a. Thus, there exists another leftmost derivation ay ==> ayy. lm lm Sin:e y = y Xy , y =5> e, X \n==> ~ =~> e, and y derives the string z such that yz = x, we have y ==> z. Thus, way =~> wax, aswas tobe \nproved. We now have the following theorem. I-m Theorem 1: Every Class 2 grmar is in Class 1. At this \npoint we invite the reader who is not interested in the theory of LL grammars to skip directly to Section \nVII, v. Properties of Class 1 and 2 Grammars Let us first observe that every Class 1 grammar can be put \ninto a modified Greibach normal form where each production is of the form A + acz or A -e and no two \nproductions are of the form A +aal I aa2. The production A + e will be used by the predictive parser \non look\u00adahead a only when there is no production of the form A + aa. To put a grammar in this form, we \nobserve that for each nonterminal A on top of the stack the predictive parser on lookahead a will either \nhalt, or cause A to be replaced, in one or more steps, by some string of the form aa, or by the empty \nstring. To construct the new grammar, for each such situation we create the productions A + aa or A + \ne, respectively. The new gramar clearly generates the same language as the old and is in the desired \nform. Example 8: Let us consider G from Example 7. The only production whose right side is not empty \nand does not begin with a $ erminal is <list + <Stat> <Itail> . With <list> on top of the stack and begin \nas the next input symbol, ? list> is replaced in two steps by begin <list> <stail> <ltail> according \nto the parser of Example 7. With c on top of the stack, <list> is instead replaced by <list> + c Other \ninputs cause the parser to halt. We thus give our new grammar the productions $ :%> + begin <list> <stail> \n<ltail> I c <ltail> for the nonterminal <list> and the pro-ons of GG for nonterminals other than <list> \n. We shall state without proof the following theorem. Theorem 2: Every Class 1 (Class 2) grmar has an \nequivalent Class 1 (Class 2) gram\u00admar in which every production is either of the form A ~ aa or A -e. \nMoreover, the predictive parser for the grammar does not on lookahead a expand A by A + e, unless there \nis no production of the form A +aa. Using Theorem 2, we can show that certain rather primitive languages \ndo not have a Class 1 gramar. Theorem 3: The language L= (anbnln~l] v(ancnln~ 1] has no Class 1 grammar. \nProof: The proof parallels the one given in [LS] to show that L is not LL(l) and will be omitted. A \none state pushdown automaton (one state PDA) is a deterministic pushdown automaton which makes ~emed \non only the current input symbol and the symbol on top of the stack. In a move, a one state PDA can 1. \nreplace the symbol on top of the stack by some string of symbols, and 2. shift its input head one symbol \nright or keep it stationary,  10 One state PDA!s have been studied by [Fi] and they are equivalent to \nthe DeBakker-Scott schemas [DS,MNV] . Every language that has a predictive parser is also recognizable \nby a one state PDA. The language I, from Theorem 3 is recognizable by a one state PDA. Thus, the languages \ngenerated by Class 1 granunars are a proper subset of the one state PDA languages. VI. Effectiveness \nof Class 1 Languages lie mentioned earlier that it is not clear if one can decide whether a gr&#38;ar \nis in Class 1, i.e., whether the class if effective. However, one can show the following. Theorem 4: \nIi it is decidable whether a CFG is a Class 1 grammar, then it is decida\u00adble for two Class 1 grammars \nG~ and G2, whether L(G1) is included in L(G2). Proof: Assume that we have an algorithm to determine \nwhether a grmar is in Class 1. Using this~ritb we shall construct an algorithm to decide for Class l-grmars \nG~ and G2 whether L(G1) is included in L(G2). First, one may test [AU1] for arbitrary CFGIS G1 and G2, \nwhether a. L(G~) contains e,, but L(G2) does not. b. L(G2) is empty, but L(G~) is not. c. For some \nterminal a, G1 generates a string beginning with a but G2 generates no such string.  We may begin our \ntest for inclusion with tests for the above three conditions. If any of them hold, then L(G1) is not \nincluded in L(G2) and our test ends. Now assume that none of (a), (b), or (c) hold. Let S1 and S2 be \nthe start symbols of G1 and G9, and,,assume that G1 and G9 have no nonterminals in common. Let S and \nS be new non\u00adterminal= and # be a new sym~ol. Construct a new grammar G with the productions of G1 and \nG2 plus S -S1 I S2 S and S -+#1 e. Now if L(G ) is included in L(+), then the choice of S +S2 S over \nS + S1 satis\u00adfies condition (2) of c~e definition of Class 1 grammar when the nonterminal is S and the \nterminal is arbitrary. Also, no other violation of the Class 1 definition can occur for G, since G1 and \nG2 are in Class 1 by hypothesis. , suppose L(G ) is not included in L(G2). Then since conditions (a) \n-(c) do not hold, weH%~~~nd aw in L(G1) but not in L(G2) and also find ax in L(@) for some x. Then consideration \nof aw tells us that S -S2 S) does not take priority over S +S1 on lookahead a, and consideration of ax# \ntells us neither does S -+ S take priority over S + S2 S~. This situatLon violates the Class 1 definition. \nThus, G is in $ lass 1 if and only if L(G~) is included in L(G2). On the hypothesis that we may test \nwhether G is in Class 1, we would have an algorithm to test set inclusion for Class 1 grammars. To put \nTheorem .4 in perspective, we should observe that decidability of inclusion for Class 1, or even Class \n2, grammars would be a powerful result. Recall that it ts not known even for the simple grmars of [KHI, \na seemingly trivial subset of the LL(l) grammars, whether inclusion is decidable. Moreover, if we could \ntest inclusion of languages, we could clearly test equality of languages, since L1 = IQ if and only if \nL1 is included in L2 and conversely. Thus , if inclu\u00adsion were decidable, even for Class 2 grammars, \nwe would have an extension of the decidability of equivalence for LL grammars [RS]. It should be observed \nthat the proof of decidability of equivalence for LL grammars (see [AU1] or [RS]) breaks down even for \nClass 2 grammars, since the latter may have arbitrarily long strings of lnullab~erf s~bols (those that \ncan derive the empty string) in their left sentential forms. VII . ~mnars and Parsers We have seen that \nthe controlled use of ambiguous constructs can give rise to faster predictive parsers, allow more natural \ngrammars, and extend the LL language class. With an LR parser we cannot expect to extend the language \nclass, but we can obtain faster parsers for more natural grammars. Consider the rightmost derivation \n<exp> ==> <exp> + <exp> ==> <exp> + <factor> ==> <exp> -t id . ==> <term> + id . 11 ==> <factor> + id \n ==> id i-id in the grammar G2. An LR parser would trace out this derivation in reverse. We call the \nsequence of productions used in the reverse of a reverse of a rightmost derivation a right parse. For \ncertain grammars we can find a right parse of any string in the language by a machine shown in Fig. 2, \ncalled an LR. parser. input xay$ input pointer Parsing Table 1-, 3 stack pointer r\u00ad -J-l /azJ stack \nFig. 2 LR Parser An LR parser is similar to a predictive parser in that it has an input tape and a stack. \nHowever, unlike the predictive parser the top of the stack is at the right instead of the left. Moreover, \nat every point in time if the input (xay in Fig. 1) is in the language, then aZ, the sequence of grammar \nsymbols on the stack, followed by ay, the portion of the input not yet used, will be called the right \nsentential form represented by the parser. In Fig. 2, aZay is the right sentential form represented by \nthe parser. However, in addition to putting grammar symbols on the stack an LR parser places special \nsymbols called states on the stack. In fact, at all times the contents of the stack will be a sequence \nof the form SOX1S1X2S2 . . . Xmsm where each s. is a state and each Xi a gramar symbol. The next move \nof the parser is dictated by ~m, the state on top of the stack, and a, the current input symbol. This \nmove is found in the parsing table which consists of two parts, an action table and a goto table. The \nmove is determined by the entry for Sm and a in the action table. The move 65ii75e one of four types: \n1. Shift s. The current input symbol a is shifted on to the stack and the state numbered s is then placed \non top of the stack. This move has the effect of ad\u00ad vancing the input pointer one symbol to the right. \n 2. Reduce by production A + p. Suppose the right side @ is of length r > 0. Then Tr symb~s are popped \noff the stack. That is, Xm-r+lsm.r+l . . . XmSm ~S removed from the stack, leaving sOXls~ . . . Xm_rsm_r. \nThe nonterminal A is placed on top of the stack. The goto table is then consulted and the state in the \nentry for Sm and A is placed on top of the stack. When a reduction by the production 12 A + @ is called \nfor, Xm-r+l . . . Xm, the string of grammar symbols removed from the stack will match p, the right side \nof the production. Accept. The parser halts and announces successful completion of parsing. 3. 4. Error. \nThe input string is not in the language being parsed; the parser trans\u00adfers to an error recovery routine. \n If a shift move the right sentential form represented by the parser does not c hang e. In a reduce \nmove it does. Suppose S =g> aAay ==> spay =%> xay m rm rm An LR parser would represent the After is a \nrightmost derivation in the grammar at hand. right sentential form a~ay with af3 on the stack and ay \nas the unexpended input. the reduction by the production A + @, the parser would represent the right \nsentential form aAay with aA on the stack. Example 9: Consider the following expression grammar G7 which \nis G2 with certain grammar symbols abbreviated. G7 : (1) E+E+T (2) E+T (3) T+T*F (4) T+F (5) F +(E) \n (6) F+a  As before, $ represents the right end- A parsing action table for G7 is shown below. I marker. \nTerminal State + a (, T o err err sh 4 err sh5 err 1 sh6 err err err err accept2 red 2 Sh7 err red 2 \nerr red 2 red4 red4 err red.4 err red4 err err sh err 4 sh5 err red 6 red6 err red6 err red 6 err err \nsh4 err sh5 err err err sh&#38; err sh5 er? 8 sh 6 err err sh 11 err err 9 red 1 sh7 err red 1 err red \n1 10 red3 red3 err red3 err red 3 11 red5 red5 err red5 err red5 IL. Action Table . In this table, err \nstands for error, sh i stands for shift i, red i stands for reduce by production numbered (i). The goto \ntable is Nonterminal ETF State o 12 3 1 2 3 82 3 z9 1: z 9 10 11 Goto Table . referenced. An LR parser \nusing The blank entries in the goto table are never the above action and goto tables wo~ld parse the \nstring a + a * a in the following marine r: right sentential form stack input represented by the parser \no a +a* a a +a++a Oa5 +a* a a+a* a OF3 F+a* a 0T2 +a* a:; T+a* a OE1 +a* a:; E+a* a OE1% asac E-t-as \na 0Elt6a5 *a E+a* a oEl+6F3 *a E+F* a 0E1+5T9 *a E+T* a 0E1+5T9*7 a E+T* a OEl+6Tg*7a5 E+T* a OE1+5T9*7F1O \nE+T*F 0E1+6T9 E-l-T OE1 E VIII . Construction of Simple LR(l) Parsers We shall sketch a technique that \nwill produce LR(l) parsers for a class of grammars called the simple LR(l) grammars [D]. Let G be a CFG \nfor which we wish to construct an LR(l) parser. First we augment the grammar with a new starting production \nof the form To determine the states of the parser we construct the collection of sets of items for G. \nAn item is a production in G with a dot somewhere in the right side. The mndicates how much of the right \nside will have been recognized when the parser is in the state associated with that item. We define a \nfunction CLOSURE on sets of items. If I is a set of items, CLOSURE(I) is the smallest set of items that \ncontains I and has the property: if A + a.By is in CLOSURE(I) and B + @ is a production in G, then B \n+ .@ is in CLOSURE(I) 14 An easy way to compute CLOSURE(I) is to begin with I and continue adding new \nitems of the form B ~ .~ to I for each production B +~ in G and item A +a.B@ currently in I. We continue \nadding items to I until no more new items can be added. The resulting set is CLOSURE(I). We need to introduce \none other function, the GOTO function on sets of items. If I is a set of items and X a grammar symbol, \nthen GOTO(I,X) = J where J is the clo\u00adsure of the set of items (A -+aX.f? I A +a.X@ is in I). Computing \nGOTO(I,X) is straightforward. We take each item in I with an X immediately to the right of the dot. We \nshift the dot past the X and add the resulting item to J. We then compute the clo\u00adsure of J. To construct \nthe collection of sets of items for the given grammar G we begin with C, a collection of sets of items \nthat initially contains 10 = CLOSURE({S~ + .S} . Then for each set of items I in C and for each grammar \nsymbol X, we compute GOTO(I,X 1 and add the resulting set of items to C if it is not already there. Once \nwe cannot add any more sets of items to C we have computed the collection of sets of items for the gi;en \ngrammar. Example 10: The collections of sets of items for G7 is given below: 10: E E-E T\u00ad !-.E . E+T \n-.T . T*F 15: 16: F-> E-T\u00ad a. E+.T . T*F T- T-.F F\u00ad :~E) F\u00ad .(E) F 2- .a F-a 1 11: E! E\u00ad 1-E. E.+T 17: \nT-F- T*.F .(E) F-a } 12: E T\u00ad 1-T. T.*F 18: F\u00ad (E.) E\u00ad 1 E.+T T -> F. 13: E- E+T . 14: F\u00ad ( .E) 19: T\u00ad \n1 T.*F E\u00ad . E+T E T\u00ad - .T . T*F 110: T -> T*F. T F\u00ad - .F .(E) 111: F-> (E). F /- ,a Given C, the collection \nof sets of items for the given grammar G, we can con\u00adstruct the action and goto tables of an LR(l) parser \nfor G as follows. For each set of items in C, there will be a state. Let state i correspond to set of \nitems Ii. The parsing action table entries for state i are determined as follows: 1. If Ii contains an \nitem of the form A +a.a~ where a is a terminal, then the action of state i is shift j, where j is the \nstate associated with the set of items GOTO(Ii,a). 2. If Ii contains the item A +u. and a is in FOLLOW(A), \nthen the action of state i on input a is reduce by production A + a .  If Ii contains the item S + S., \nthen the action of state i on the right end\u00admarker $ is accept. 3. 4. Otherwise, the. action of state \ns on inpuk a is error.  The goto table entry for state i on nonterminal A is simply the state associa\u00adted \nwith the set of items GOTO(Ii,A). If at most one move is defined for each entry of the parsing action \ntable, then the given grammar G is said to be simple LR(l). It can be shown that the LR(l) parser so \nconstructed is a valid parser for G. Moreover, the parser will announce error on input a after having \nscanned x, provided there is no string in L(G) of the form xaz, for any z. Example 11: The action and \ngoto tables for the collection of sets of items for G7 given in Example 10 are those in Example 9. IX. \nResolving Parsing Action Conflicts Let us now examine what happens when we try to build an LR(l) parser \nfor an ambiguous grammar. Let us first consider the following grammar G~ (which is G1 with new nonterminal \nnames): (1) E+E+E (2) E+E+E (3) E+(E) (4) E+a  The collection of sets of items for G8 is: 10: g E-E-E \n-!- E . E+E . E*E .(E) -.a 15 ; : E-E\u00adE-a ! E*.E . E+E .~+~ .(E) 11: y - E.-I-E E\u00ad -[ E.%E 12: E E\u00ad \n- (.E) . E-I-E 17: : : E+E . E.+E E-E\u00ad .E*E .(E) E\u00ad } E.*E E }- .a 18: E - E*E. E- E.+E 13 E -> a E\u00ad \ni E.%E 14: E E\u00ad - E+.E . E+E 19: E-> (E). E\u00ad .E+E E\u00ad .(E) E I- .a As before we can try to construct \nthe parsing action and goto tables from these sets of items. However, sets of items I 7 and 18 give rise \nto entries in the action table with more than one action. These multiply defined entries are represented \nby 1?, 2?, 3?, and 4? in the action table shown below. Action Table Goto Table * +a () $ . .- .. ._ \n._._.... .... --. err err sh2 err sh3 err sh 4 sh 5 err err err accept err err sh2 erx sh3 err red4 red4 \nerr red4 err red4 err err sh2 err sh3 err err err sh2 err sh 3 err sh4 sh5 err sh9 err err 1? I 2? err \nred 1!err red 1 3? 4? err red 2~err red 3 red3 red3 err red3 err red3 Each of the multiply defined entries \nin this action table happens to represent a shift-reduce conflict: ,,shift 4!! and reduce 1 1? : conflict \nbetween 2? : conflict between shift 5 and reduce 1 3?,: conflict between shift 4 and reduce 2!! 4? : \nconflict between shift 5 and reduce 2!! In effect the grammar G% will permit many different parses for \ncertain input strings. The multiply defined entries in the action table represent choices which, when \nuniquely re\u00adsolved, will result in the selection of one of the possible parses for a given input string. \nAs an example of these choices, we may observe that the input string a + a causes the parser to have \nstate 7 on top of the stack. If we replace the entry 1? in the action table by reduce 1 , then the input \nstring a + a + a will be parsed: E==> E-I-E ==>E+a ==>E+E+a ==>E+a+a ==>a+a+a That is + would be treated \nas being left associative. On the other hand if we replace 1? by shift 4 , then the same string would \nbe parsed: E==>E+ E ==> E+E+E ==> E+E+a ==> E+a+a ==> a+a+a Here, + is treated as being right associative. \nWe could also re~lace 1? by error. Doing so would make + be a binarv v. o~erator like .LT. in FORTRAN. \nThe inpu~ string a + a + a would b~come illegal. Thus, the question of how we should resolve the parsing \naction conflicts present in 1? is directly connected with the question of the desired associativity of \nthe + operator. Now, examine the entry 2?. Similar reasoning shows that, if we wish to associate a +a \n* a as a + (a * a) (i.e., * has higher precedence) then 2? should be shift 5 ; if we wish a + a * a to \nbe treated as (a + a) * a, then 2? should be reduce 1 . Again, we also have the option of forbidding \nthis input string by replacing 2? by error . In the same fashion, we may observe that the input string \na * a causes the parser to have state 8 on top of the stack. The entry 4? dictates the associativity \nof the * operator, and the entry 3? reflec:ts the relative precedence of + following *. 17 The following \ntable gives several different resolutions of the ambiguous entries along with a brief description of \nthe disambiguating rule used. 1? 27 3? 4? Disambiguating Rule red 1 sh 5 red 2 red 2 The usual interpretation; \n+ and * are left associative and * has higher precedence red 1 red 1 sh 4 sh 5 + left associative, + \nright associative, + has higher precedence.  red 1 red 2 red 2 + and * evaluated left to rightred 1 \nsh 4 sh 5 sh 4 Sh 5 + and * evaluated right to left (as inAPL) error error error error precedence must \nbe fully specified by parentheses. Thus for a grammar like G~ we can give a clear meaning to the resolutions \nof the multiply defined entries in the action table. Suppose we resolve the entries according to the \ntop row of the above table. A parse of the input string a + a * a goes as: stack input o a+a* a Oa3 +a* \na OE1 +a* a OE1+4 a* a 0El+4as *a OE1+4E7 *a OE1+4E7*5 a I 0El+4E7*~a3 oElt4E7*5E8 OE1+4E7 OE1 There \nis a close correspondence between this parser and the one given for G7 in Example 9. G7 uses the first \nfour productions to enforce the left associativity of the + and * operators. G7 also uses two nonterminals \nE and T to make * of higher precedence than +. However, introducing these nonterminals requires the use \nof the productions E+T T-)F to allow a factor to be trivially reduced to a term, and a term to be trivially \nreduced to an expression. These productions are called single productions, and they serve no other useful \nsyntactic purpose. A programing language may have operators on ten or more different prece\u00addence levels, \nso that in parsing expressions a considerable fraction of the time can be spent in making reductions \nby these single productions. In Example 9, the LR parser for G7 made 13 moves parsing a+a*a. The parser \nabove makes only ten moves. When we are parsing we can skip the reductions by these single productions \nrovided we take into account the associativities and precedence levels of the operators. In YAU2] an \nalgorithmic technique for eliminating reductions by single productions of this nature was pre\u00adsented. \nIf we apply this technique to the simple LR(l) parser for G we obtain the parser for Gg given above. \nThus , by using the ambiguous grammar G8 and introdu z lng precedence informa\u00adtion to specify the associativity \nand relative precedence of + and % we can obtain the reduced parser directly. This direct procedure has \nseveral advantages. First of all, we have a smaller and easier to read grammar for our language. Secondly, \na more efficient parser with fewer states can be directly obtained from the ambiguous grammar and the \nprecedence information, than from the equivalent unambiguous grammar. Moreover, it can be shown that \nthis more efficient parser is equivalent in a useful sense to the simple LR parser constructed from the \nunambiguous grammar [AU2]. The work of [El] should also be mentioned. There, given any operator precedence \ngrammar, one obtains a bottom-up parser that works in an Lll(l) style on a skeletal grammar (see [AU1]). \nThe method does not (unfortunately) guarantee that the language parsed is the same as that defined by \nthe original grammar. It does, however, produce the parser of Example 9 for G7, so in at leas-t some \ncases the language is not changed. In fact, our analysis of G7 can be extended to the following class \nof grmars. Consider a sequence *1,* ,...,* of n left associative operators and assume *i has lower precedence \nthan *i+l. We ca $ writenthe ambiguous grammar: E->E*lE E->E*2E 9** E->E*nE E -> (E) E->a When we construct \nthe parser for this grammar, we find n2 ambiguous entries in the action table. Each is of the form: h \nstate s on input *i, we may either shift or reduce by E * E * . E . To reflect the given associativities \nand precedence levels of the operators, we may useJthe simple resolution rule: If i > j, shift; otherwise, \nreduce. It can be shown that when the ambiguities are resolved in this way, the resulting parser analyzes \nexpressions giving the proper associativities and precedences to *~, . ..*n. The resolution rule can \nbe easily extended to deal with right associative and binary operators, as well as several operators \nat each precedence level. For example, we can list the operators in order of increasing precedence. On \nline 1 we list all operators that are on precedence level 1. Beside each operator we can specify whether \nit is left associative, binary, or right associative. On the next line we list all operators on precedence \nlevel 2, together with their associativities, and so on. These ideas have been implemented in a compiler-compiler \ncalled YACC at Bell Laboratories. Notice that in a scheme of this nature the precedence information is \nconsulted only where the grammar is ambiguous. Thus, there is no need to specify awkward precedences \nfor terminals such as reserved words or parentheses, as one would have to do in a purely operator precedence \nscheme. An analogous technique can be applied to the abstract if-then-else grammar G3. 3: (1) S -> aSbs \n(2) s -> as (3) s-> c The collection of sets of items for G is: 3 St-.s10: s . aSbS s-.aS -! s-.C s \n-> s.11: a.SbS12: s- S -a.S s-. aSbS s-.aS S-c i s -> c. 13: I:s aS .bS 4 S -aS. 3 aSb. S15: :: . \naSbS S -.aS S-c 1 16: S -> asbs. 19 The resulting action and goto table are shown below. Terminal Nonterminal \nstateabc E $ o sh2 err err err 1 1 err err err accept 2 Sh2 err sh3 err 4 err red3 err red3 ? 2 err \nerr red 2 sh2 err sh3 err 6 z err red 1 err red 1 [ Action Table . Goto Table _ Since FOLLOW(S) = (b,$), \nthe entry for state 4 on input b is a shift-reduce conflict between shift 5 and  reduce by S + aS . \nIf we choose to shift on input b, then we shall always asso\u00ad ciate this b with the last available a. \nThis would correspond to associating an else with the last unelseld then. . Let us review the main \nideas of this section. We have seen that an ambiguous gram\u00admar gives rise to an LR parser with multiply \ndefined entries in its action table. If we make each multiply defined entry single-valued by choosing \nonly one of the possible actions or error in accordance with some disambiguating rule, then we obtain \nan LR parser that will parse a subset of the language defined by the original grammar. The resulting \nlanguage is a deter\u00administic context-free language and there exists an LR(l) grammar for it. (An LR parser \ncanbe simulated by a deterministic pushdown automaton (DPDA for short). Moreover, for every DPDA we can \nfind an LR(l) gramar that defines the same language as that recognized by the DPDA [AU1].) Thus , several \ndefinitions are possible for the same language. We have seen several practical examples where the ambiguous \nCFG and a disambiguating rule provides the more natural and eco\u00adnomical description. x. Summary We have \nseen that by using ambiguous context-free grammars with some rather simple disambiguating rules we can \nobtain definitions of languages that are shorter and easier to comprehend than an equivalent definition \nin terms of an unambiguous grammar. Moreover, if the disambiguating rules are carefully chosen, we can \noften resolve the parsing action conflicts in a straightforward way and obtain a parser equivalent to \nthat constructed from an equivalent unambiguous grammar. A number of open questions remain. In this paper \nwe have used rather informal dis\u00adambiguating rules that reflect current practices in progrannning language \ndesign. A more gen\u00aderal but still easy-to-use formalism for specifying disambiguating rules would be \nthe next step. We would also be interested in knowing to what extent ambiguous granunars and disambi\u00adguating \nrules should be used to obtain the clearest description of a language. REFERENCES [AU11 A. V. Aho and \nJ. D. Unman, The Theory of Parsing, Translation and Compiling. Volume 1: Parsing. Prentice-Hall,=g~ miffs, \nN.J., 1972. Volume 2: Compiling, 1973. [AU21 A. V. Aho and J. D. Unman, A technique for speeding up LR(k) \nparsers. SIAM J. Com\u00adputing 2:2 (June, 1972). [AMPI E. Ashcroft, Z. Manna, and A. Pnueli, Decidable properties \nof monadic functional schemas. J. ACM 20:3 (1973) 489-499. [BU] A. Birman and J. D. Unman, Parsing algorithms \nwith backtrack. Conference Record of IEEE llth Annual Symposium on Switching and Automata Theory, 1970, \npp. 153-174. [cl M. E. Conway, Design of a, separable transition-diagram compiler. Comm. ACM 6:7 (1963) \n396-408. [D] F. L. DeRemer, Simple LR(k) gramars. Comm. ACM 14:7 (1971) 453-46o. [DS] J. W. de Bakker \nand D. Scott, A theory of programs. Unpublished manuscript. 1969. 20 Technical Report 13, [Es] J. Ea,rley, \nAmbiguity and precedence in syntax desyript~on. Berkeley, 1973. Department of Computer Science, University \nof Callfornlaj to some non-LR grammars. TR 121, [El] N. El Djabri, Extending the LR parsing technique \ncorppute~ ScienGe Labora~ofY, De rtment of Electrical Engineering, PrinCetOn Unlverslty, Pr2.nceton, \n. ., 19?9. [Fi] M. J. Fischer, Some properties of precedence languages. Proc. ACM Symposium on Theory \nof Computing, May 1969, 181-19o. [Fl] R. W. Floyd, Syntactic analysis and operator precedence. J. ACM \n10:3 (1963) 316\u00ad 333. [GS] S. Ginsburg and E. H. Spanier, Control sets on Grammars. Mathematical Systems \nTheory 2:2 (1968) 159-178. [HU] J. E. Hopcroft and J. D. Unman, Formal Languages and their Relation \n&#38; Automata.  Addison-Wesley, Reading, Mass., l= [K] D. E. Knuth, Top-down syntax analysis. Acts \nInformatica 1:2 (1971) 79-110. [ml A. J. Korenjak and J. E. Hopcroft, Simple deterministic languages. \nIEEE Conference Record of 7th Annual Symposium on Switching and Automata Theory, 1966, pp. 36-46. [LRI \nP. M. Lewis and D. J. Rosenkrantz, An ALGOL Compiler designed using automata the ory. Proc. Symposium \non Computers and Automata, Polytechnic Institute of Brooklyn, N.Y., 1971, pp. 75-88. [LS] P. M. Lew5s \nand. R. E. Stearns, Syntax directed transduction. J. ACM 15:3 (1968) 464-488. . [M] R. M. McClure, An \nappraisal of compiler technology. Proc. AFIPS 1972 Spring Joint Computer Conference, AFIPS Press, Montvale, \nN.J., 1-9. [MHWI w. M. McKeeman, J. J. Horning, and D. B. Wortman, ~ ComPiler Generator. prentice- Hallj \nEnglewood Cliffs, N.J.~-1970. &#38; [RS] D. J. Rosenkrantz and R. E. Stearns, Properties of deterministic \ntop-down grammars. Information and Control 14:5 (1969) 226-256. [s1 R. E. Stearns, Deterministic top-down \nparsing. Proc. Fifth Annual Princeton Conference on Information Sciences and Systems, 1971, pp. 182-188. \n [w] D. Wood, The theory of left factored languages. Computer J. 12:4 (1969) 349-356 and 13:1 (1970) \n55-62. \n\t\t\t", "proc_id": "512927", "abstract": "We consider methods of describing the syntax of programming languages in ways that are more flexible and natural than conventional BNF descriptions. These methods involve the use of ambiguous context-free grammars together with rules to resolve syntactic ambiguities. We show how efficient LL and LR parsers can be constructed directly from certain classes of these specifications.", "authors": [{"name": "A. V. Aho", "author_profile_id": "81100024612", "affiliation": "Bell Laboratories, Murray Hill, New Jersey", "person_id": "PP43126072", "email_address": "", "orcid_id": ""}, {"name": "S. C. Johnson", "author_profile_id": "81332506933", "affiliation": "Bell Laboratories, Murray Hill, New Jersey", "person_id": "PP43126474", "email_address": "", "orcid_id": ""}, {"name": "J. D. Ullman", "author_profile_id": "81100314798", "affiliation": "Princeton University, Princeton, New Jersey", "person_id": "PP43144682", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/512927.512928", "year": "1973", "article_id": "512928", "conference": "POPL", "title": "Deterministic parsing of ambiguous grammars", "url": "http://dl.acm.org/citation.cfm?id=512928"}