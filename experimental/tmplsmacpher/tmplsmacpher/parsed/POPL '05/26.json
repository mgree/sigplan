{"article_publication_date": "01-12-2005", "fulltext": "\n Precise Interprocedural Analysis using Random Interpretation Sumit Gulwani George C. Necula gulwani@cs.berkeley.edu \nnecula@cs.berkeley.edu Department of Electrical Engineering and Computer Science University of California, \nBerkeley Berkeley, CA 94720-1776  ABSTRACT We describe a uni.ed framework for random interpretation \nthat generalizes previous randomized intraprocedural anal\u00adyses, and also extends naturally to e.cient \ninterprocedural analyses. There is no such natural extension known for de\u00adterministic algorithms. We \npresent a general technique for extending any intraprocedural random interpreter to per\u00adform a context-sensitive \ninterprocedural analysis with only polynomial increase in running time. This technique in\u00advolves computing \nrandom summaries of procedures, which are complete and probabilistically sound. As an instantiation of \nthis general technique, we obtain the .rst polynomial-time randomized algorithm that discov\u00aders all linear \nrelationships interprocedurally in a linear pro\u00adgram. We also obtain the .rst polynomial-time randomized \nalgorithm for precise interprocedural value numbering over a program with unary uninterpreted functions. \nWe present experimental evidence that quanti.es the pre\u00adcision and relative speed of the analysis for \ndiscovering linear relationships along two dimensions: intraprocedural vs. in\u00adterprocedural, and deterministic \nvs. randomized. We also present results that show the variation of the error proba\u00adbility in the randomized \nanalysis with changes in algorithm parameters. These results suggest that the error probabil\u00adity is much \nlower than the existing conservative theoretical bounds. Categories and Subject Descriptors D.2.4 [Software \nEngineering]: Software/Program Veri.\u00adcation; F.3.1 [Logics and Meanings of Programs]: Spec\u00adifying and \nVerifying and Reasoning about Programs; F.3.2 This research was supported by NSF Grants CCR-0326577, \nCCR-0081588, CCR-0085949, CCR-00225610, CCR-0234689, NASA Grant NNA04CI57A, Microsoft Research Fellowship \nfor the .rst author, and Sloan Fellowship for the second author. The information presented here does \nnot necessarily re.ect the posi\u00adtion or the policy of the Government and no o.cial endorsement should \nbe inferred. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n05, January 12 14, 2005, Long Beach, California, USA. Copyright 2005 ACM 1-58113-830-X/05/0001 ...$5.00. \n[Logics and Meanings of Programs]: Semantics of Pro\u00adgramming Languages Program analysis General Terms \nAlgorithms, Theory, Veri.cation  Keywords Interprocedural Analysis, Random Interpretation, Random\u00adized \nAlgorithm, Linear Relationships, Uninterpreted Func\u00adtions, Interprocedural Value Numbering 1. INTRODUCTION \nA sound and complete program analysis is undecidable [11]. A simple alternative is random testing, which \nis complete but unsound, in the sense that it cannot prove absence of bugs. At the other extreme, we \nhave sound abstract interpreta\u00adtions [2], wherein we pay a price for the hardness of program analysis \nin terms of having an incomplete (i.e., conservative) analysis, or by having algorithms that are complicated \nand have long running-time. Random interpretation is a prob\u00adabilistically sound program analysis technique \nthat can be simpler, more e.cient and more complete than its determin\u00adistic counterparts [5, 6], at the \nprice of degrading soundness from absolute certainty to guarantee with arbitrarily high probability. \nUntil now, random interpretation has been applied only to intraprocedural analysis. Precise interprocedural \nanal\u00adysis is provably harder than intraprocedural analysis [15]. There is no general recipe for constructing \na precise and e.cient interprocedural analysis from just the correspond\u00ading intraprocedural analysis. \nThe functional approach pro\u00adposed by Sharir and Pnueli [20] is limited to .nite lattices of data.ow facts. \nSagiv, Reps and Horwitz have generalized the Sharir-Pnueli framework to build context-sensitive anal\u00adyses, \nusing graph reachability [16], even for some kind of in.nite domains. They successfully applied their \ntechnique to detect linear constants interprocedurally [19]. However, their generalized framework requires \nappropriate distribu\u00adtive transfer functions as input. There seems to be no obvi\u00adous way to automatically \nconstruct context-sensitive trans\u00adfer functions from just the corresponding intraprocedural analysis. \nWe show in this paper that if the analysis is based on random interpretation, then there is a general \nprocedure for lifting it to perform a precise and e.cient interprocedu\u00adral analysis. Our technique is \nbased on the standard procedure summa\u00adrization approach to interprocedural analysis. However, we compute \nrandomized procedure summaries that are prob\u00adabilistically sound. We show that such summaries can be \ncomputed e.ciently, and we prove that the error probabil\u00adity, which is over the random choices made by \nthe algorithm, can be made as small as desired by controlling various pa\u00adrameters of the algorithm. We \ninstantiate our general technique to two abstractions, linear arithmetic (Section 7) and unary uninterpreted \nfunc\u00adtions (Section 8), for which there exist intraprocedural ran\u00addom interpretation analyses. For the \ncase of linear arith\u00admetic, our technique yields a more e.cient algorithm than the existing algorithms \nfor solving the same problem. For the case of unary uninterpreted functions, we obtain the .rst polynomial-time \nand precise algorithm that performs inter\u00adprocedural value numbering [1] over a program with unary uninterpreted \nfunction symbols. In the process of describing the interprocedural random\u00adized algorithms, we develop \na generic framework for describ\u00ading both intraprocedural and interprocedural randomized analyses. This \nframework generalizes previously published random interpreters [5, 6], guides the development of ran\u00addomized \ninterpreters for new domains, and provides a large part of the analysis of the resulting algorithms. \nAs a novel feature, the framework emphasizes the discovery of relation\u00adships, as opposed to their veri.cation, \nand provides generic probabilistic soundness results for this problem. Unlike previous presentations \nof random interpretation, we discuss in this paper our experience with implement\u00ading and using an interprocedural \nrandom interpreter on a number of C programs. In Section 10, we show that the er\u00adror probability of such \nalgorithms is much lower in practice than predicted by the theoretical analysis. This suggests that tighter \nprobability bounds could be obtained. We also compare experimentally the randomized interprocedural lin\u00adear \nrelationship analysis with an intraprocedural version [5] and with a deterministic algorithm [19] for \nthe related but simpler problem of detecting constant variables. This paper is organized as follows. \nIn Section 2, we present a generic framework for describing intraprocedural random\u00adized analyses. In \nSection 3, we explain the two main ideas behind our general technique of computing random proce\u00addure \nsummaries. In Section 4, we formally describe our generic algorithm for performing an interprocedural \nran\u00addomized analysis. We prove the correctness of this algorithm in Section 5 and discuss .xed-point \ncomputation and com\u00adplexity of this algorithm in Section 6. We instantiate this generic algorithm to \nobtain an interprocedural algorithm for discovering linear relationships, and for value numbering in \nSection 7 and Section 8 respectively. In Section 9, we de\u00adscribe how to avoid large numbers that may \narise during computation, so that arithmetic can be performed using .\u00adnite number of bits. Section 10 \ndescribes our experiments. 2. RANDOM INTERPRETATION In this section, we formalize the intraprocedural \nrandom interpretation technique, using a novel framework that gen\u00aderalizes existing random interpreters. \n2.1 Preliminaries We .rst describe our program model. We assume that the .owchart representation of a \nprogram consists of nodes Figure 1: Flowchart nodes of the kind shown in Figure 1(a), (b) and (c). In \nthe assign\u00adment node, x denotes a program variable, and e is either some expression or ?. Non-deterministic \nassignments x :=? represent a safe abstraction of statements (in the original source program) that our \nabstraction cannot handle pre\u00adcisely. We also abstract the conditional guards by treating them as non-deterministic. \nA random interpreter executes a program on random in\u00adputs in a non-standard manner. An execution of a \nrandom interpreter computes a state . at each program point p.A state is a mapping from program variables \nto values v over some .eld F. A random interpreter processes ordinary assignments (x := e,where e is \nan expression) by updating the state with the value of the expression being assigned. Expressions are \neval\u00aduated using an Eval function, which depends on the under\u00adlying domain of the analysis. We give some \nexamples of Eval functions in Section 2.3, where we also describe the proper\u00adties that an Eval function \nmust satisfy. Non-deterministic assignments x :=? are processed by assigning a fresh ran\u00addom value to \nvariable x. A random interpreter executes both branches of a conditional. At join points, it performs \na random a.ne combination of the joining states using the a.ne join operator (fw). We describe this operator \nand its properties in Section 2.2. In presence of loops, a random interpreter goes around loops until \na .xed point is reached. The number of iterations nrIters required to reach a .xed point is abstraction \nspeci.c. In Section 2.4, we discuss how to verify or discover pro\u00adgram equivalences from such random \nexecutions of a pro\u00adgram. We also give a bound on the error probability in such an analysis. We give \nboth generic and abstraction spe\u00adci.c results. Finally, in Section 2.5, we give an example of the random \ninterpretation technique for linear arithmetic to verify assertions in a program. We use the notation \nPr(E) to denote the probability of event E over the random choices made by a random in\u00adterpreter. Whenever \nthe interpreter chooses some random value, it does so independently of the previous choices and uniformly \nat random from some .nite subset F of F.Let q = |F |. (In Section 9, we argue the need to perform arith\u00admetic \nover a .nite .eld, and hence choose F = Zq , the .eld of integers modulo q, for some randomly chosen \nprime q.In that case F = F.) 2.2 Af.ne Join Operator Thea.nejoin operator fw takes as input two values \nv1 and v2 and returns their a.ne join with respect to the weight w as follows: def fw(v1,v2)= w\u00d7 v1 +(1 \n- w) \u00d7 v2 The a.ne join operator can be thought of as a selector be\u00adtween v1 and v2, similar to the f \nfunctions used in static single assignment (SSA) form [3]. If w =1 then fw(v1,v2) evaluates to v1,and \nif w =0 then fw(v1,v2)evaluates to v2. The power of the fw operator comes from the fact that a non-boolean \n(random) choice for w captures the e.ect of both the values v1 and v2. The a.ne join operator can be \nextended to states . in which case the a.ne join is performed with the same weight for each variable. \nLet .1 and .2 be two states, and x be a program variable. Then, def fw(.1,.2)(x)= fw(.1(x),.2(x)) For \nany polynomial P and any state ., we use the notation [[P]]. to denote the result of evaluation of polynomial \nP in state .. The a.ne join operator has the following useful properties. Let P and P. be two polynomials \nthat are linear over program variables (and possibly non-linear over other variables). Let .w = fw(.1,.2) \nfor some randomly chosen w from a set of size q. Then, A1. Completeness: If P and P. are equivalent in \nstate .1 as well as in state .2, then they are also equivalent in state .w. ([[P]].1 =[[P.]].1) . ([[P]].2 \n=[[P.]].2) . [[P]].w =[[P.]].w A2. Soundness: If P and P. are not equivalent in either state .1 or state \n.2, then it is unlikely that they will be equivalent in state .w. ([[P]].1 .]].1) . ([[P]].2 =[[P.]].2) \n. =[[P 1 Pr([ P]].w =[[P.]].w ) = q 2.3 Eval Function A random interpreter is equipped with an Eval \nfunction that takes an expression eand a state .and computes a .eld value. The Eval function plays the \nsame role as an abstract interpreter s transfer function for an assignment operation. Eval is de.ned \nin terms of a symbolic counterpart SEval that translates an expression into a polynomial over the chosen \n.eld F. This polynomial is linear in program variables, and may contain random variables as well, which \nstand for ran\u00addom .eld values chosen during the analysis. Eval(e,.)is computed by replacing program variables \nin SEval(e)with their values in state ., replacing the random variables with some random values, and \nthen evaluating the result over the .eld F. Eval function for Linear Arithmetic. The random inter\u00adpretation \nfor linear arithmetic was described in a previous paper [5]. The following language describes the expressions \nin this domain. Here x refers to a variable and c refers to an arithmetic constant. e ::= x | e1 \u00b1 e2 \n| c\u00d7 e The SEval function for this domain simply translates the linear arithmetic syntactic constructors \nto the corresponding .eld operations. In essence, Eval simply evaluates the linear expression over the \n.eld F. Eval function for Unary Uninterpreted Functions. The random interpretation for uninterpreted \nfunctions was de\u00adscribed in a previous paper [6]. We show here a simpler SEval function, for the case \nof unary uninterpreted func\u00adtions. The following language describes the expressions in this domain. Here \nx refers to a variable and F refers to a unary uninterpreted function. e::= x | F(e) The SEval function \nfor this domain is as follows. SEval(x)= x SEval(F(e)) = aF \u00d7 SEval(e)+ bF Here aF and bF are random \nvariables, unique for each unary uninterpreted function F. Note that in this case, SEval pro\u00adduces polynomials \nthat have degree more than 1, although still linear in the program variables. 2.3.1 Properties of the \nSEval Function The SEval function should have the following properties. Let x be any variable and e1 \nand e2 be any expressions. Then, B1. Soundness: The SEval function should not introduce any new equivalences. \nSEval(e1)= SEval(e2) . e1 = e2 Note that the .rst equality is over polynomials, while the second equality \nis in the analysis domain. B2. Completeness: The SEval function should preserve all equivalences. e1 \n= e2 . SEval(e1)= SEval(e2) B3. Referential transparency: SEval(e1[e2/x]) = SEval(e1)[SEval(e2)/x] B4. \nLinearity: The SEval function should be a polynomial that is linear in program variables. Property B2 \nneed not be satis.ed if completeness is not an issue. This is of signi.cance if the underlying theory \nis di.cult to reason about, yet we are interested in a sound and partially complete reasoning for that \ntheory. For example, the following SEval function for bitwise or operator (||) satis.es all the above \nproperties except property B2. SEval(e1||e2)= SEval(e1)+ SEval(e2) This SEval function models commutativity \nand associativity of the || operator. However, it does not model the fact that x||x = x. In this paper, \nwe assume that the SEval function satis.es all the properties mentioned above. However, the results of \nour paper can also be extended to prove relative completeness for a theory if the SEval function does \nnot satisfy property B2. 2.4 Analysis and Error Probability A random interpreter performs multiple executions \nof a program as described above, the result of which is a collec\u00adtion of multiple, say t, states at each \nprogram point. Let Sp denote the collection of tstates at program point p.We can use these states to \nverify and discover equivalences. The process of verifying equivalences and the bound on the error probability \ncan be stated for a generic random interpreter. The process of discovering equivalences is ab\u00adstraction \nspeci.c; however a generic error probability bound can be stated for this process too. 2.4.1 Verifying \nEquivalences The random interpreter declares two expressions e1 and e2 to be equivalent at program point \np i. for all states ..Sp , Eval(e1,.)= Eval(e2,.). We denote this by Sp |= e1 = e2. Two expressions e1 \nand e2 are equivalent at program point p i. for all program states .that may arise at program point p \nin any execution of the program, [ e1]]. =[[e2]]..We denote this by p |= e1 = e2. The following properties \nhold: C1. The random interpreter discovers all equivalences. p |= e1 = e2 .Sp |= e1 = e2 C2. With high \nprobability, any equivalence discovered by the random interpreter is correct. \u00abt d p |= e1 = e2 . Pr(Sp \n|= e1 = e2) = q d =(nj +.) \u00d7nrIters Here nj denotes the number of join points in the program, and q denotes \nthe size of the set from which the random interpreter chooses random values. For de.ning ., consider \nevery sequence of assignments x1 := e1; ...; xm := em along acyclic paths in the program. . is the maximum \nde\u00adgree of any polynomial SEval(ei[ei-1/xi-1] \u00b7\u00b7\u00b7[e1/x1]), where 1 = i = m. The arguments to SEval are \nexpressions com\u00adputed on the path, expressed symbolically in terms of the input program variables. For \nexample, . is 1 for the linear arithmetic abstraction. For the abstraction of uninterpreted functions, \n. is bounded above by the size of program (mea\u00adsured in terms of number of function applications). Intu\u00aditively, \nwe add nj to . to account for the multiplications with the random weights at join points. Note that the \nerror probability in property C2 can be made arbitrarily small by choosing q to be greater than dand \nchoosing tappropriately. The proof of property C1 follows from a more general property (property D1 in \nSection 5) that states the com\u00adpleteness of an interprocedural random interpreter. The proof of property \nC2 is by induction on the number of steps performed by the random interpreter, and is similar to the \nproof of Theorem 2 (in Section 5) that states the soundness of an interprocedural random interpreter. \n 2.4.2 Discovering Equivalences We now move our attention to the issue of discovering equivalences. Although, \nthis usage mode was apparent from previous presentations of random interpretation, we have not considered \nuntil now the probability of erroneous judg\u00adment for this case. The process of discovering equivalences \nat a program point is abstraction speci.c. However, we do present a generic up\u00adper bound on the probability \nof error. For the case of linear arithmetic, the set of equivalences at a program point p can be described \nby a basis (in the linear algebraic sense) of the set of linear relationships that are true at point \np. Such a basis can be mined from Sp by solving a set of simultaneous linear equations. For the case \nof uninterpreted functions, the set of all equivalences among program expressions can be described succinctly \nusing an equivalence value graph [7]. This equivalence value graph can be built by .rst construct\u00ading \nthe value .ow graph [13] G of the program and then assigning t values V1(m),...,Vt(m)to each node m in \nG as follows. A node m in a value .ow graph G is either a variable x,or an F-node F(m1), or a f-node \nf(m1,m2)for some nodes m1,m2 in G.We use Si to denote the ith state in sample S. Vi(x)= Si(x) Vi(F(m)) \n= Eval(F(Vi(m))),Si) Vi(fj(m1,m2)) = f wj (Vi(m1),Vi(m2)) i Here wij denotes the ith random weight chosen \nfor the jth phi function fj in the value .ow graph G. The nodes with equal values are merged, and the \nresulting graph re.ects all equivalences among program expressions. The following theorem states a generic \nupper bound on the probability that the random interpreter discovers any false equivalence at a given \nprogram point. Theorem 1. For any program point p, \u00ab t/kv dkv Pr(.e1,e2 : p |= e1 = e2 . Sp |= e1 = \ne2) = q where d is as described in Section 2.4.1 and kv is the max\u00adimum number of program variables visible \nat any program point. The proof of Theorem 1 follows from a more general theo\u00adrem (Theorem 2 in Section \n5) that we prove for analyzing the correctness of an interprocedural random interpreter. Note that we \nmust choose t greater than kv,and q greater than dkv. For speci.c abstractions, it may be possible to \nprove bet\u00adter bounds on the error probability. For example, for the theory of linear arithmetic, it can \nbe shown that the error t-kv d\u00df te probability is bounded above by ,where \u00df =, qt-kv ebeing the Euler \nconstant 2.718 \u00b7\u00b7\u00b7. For the theory of unary uninterpreted functions, it can be shown that the error prob\u00ad \nt-2 ability is bounded above by kv2t2 dq . This latter bound implies that a small error probability can \nbe achieved with a value of tslightly greater than 2, as opposed to the generic bound, which requires \nt to be greater than kv for achieving a low error probability. The proofs of these speci.c bounds follow \nfrom more general theorems that we state in Section 7 (Theorem 8) and Section 8 (Theorem 10) for analyzing \nthe error probability of the interprocedural analysis for these abstractions. This completes the description \nof the generic framework for random interpretation. We show an example next, and then we proceed to extend \nthe framework to describe inter\u00adprocedural analyses.   Figure 2: A code fragment with four paths. Of \nthe two equations asserted at the end the .rst one holds on all paths but the second one holds only on \nthree paths. The numbers shown next to each edge repre\u00adsent values of variables in a random interpretation. \n 2.5 Example In this section, we illustrate the random interpretation scheme for discovering linear \nrelationships among variables in a program by means of an example. Consider the procedure shown in Figure \n2 (ignoring for the moment the states shown on the side). We .rst con\u00adsider this procedure in isolation \nof the places it is used (i.e., intraprocedural analysis). Of the two assertions at the end of the procedure, \nthe .rst is true on all four paths, and the second is true only on three of them (it is false when the \n.rst conditional is false and the second is true). Regular testing would have to exercise that precise \npath to avoid inferring that the second equality holds. Random interpre\u00adtation is able to invalidate \nthe second assertion in just one (non-standard) execution of the procedure. The random interpreter starts \nwith a random value 3 for the input variable i and then executes the assignment state\u00adments on both sides \nof the conditional using the Eval func\u00adtion for linear arithmetic, which matches with the standard interpretation \nof linear arithmetic. In the example, we show the values of all live variables at each program point. \nThe two program states before the .rst join point are combined with the a.ne join operator using the \nrandom weight w1 =5. Note that the resulting program state after the .rst join point can never arise \nin any real execution of the program. However, this state captures the invariant that a + b = i, which \nis necessary to prove the .rst assertion in the proce\u00addure. The random interpreter then executes both \nsides of the second conditional and computes an a.ne join of the two states before the second join point \nusing the random weight w2 = 2. We can then verify easily that the resulting state at the end of the \nprocedure satis.es the .rst asser\u00adtion but does not satisfy the second. Thus, in one run of the procedure \nwe have noticed that one of the (potentially) exponentially many paths breaks the invariant. Note that \nchoosing w to be either 0 or 1 at a join point corresponds to executing either the true branch or the \nfalse branch of its corresponding conditional; this is what naive testing accom\u00adplishes. However, by \nchoosing w (randomly) from a set that also contains non-boolean values, we are able to capture the e.ect \nof both branches of a conditional in just one interpre\u00adtation of the program. In fact, there is a very \nsmall chance that the random interpreter will choose such values for i, w1 and w2 that will make it conclude \nthat both assertions hold (e.g., i =2, or w1 =1). In an interprocedural setting, the situation is more \ncom\u00adplicated. If this procedure is called only with input argu\u00adment i = 2, then both assertions hold, \nand the analysis is expected to infer that. One can also check that if the ran\u00addom interpreter chooses \ni = 2, then it is able to verify both the assertions, for any choice of w1 and w2.We look next at what \nchanges are necessary to extend the analysis to be interprocedural.  3. INFORMAL DESCRIPTION OF THE \nALGORITHM Our algorithm is based on the standard summary-based approach to interprocedural analysis. \nProcedure summaries are computed in the .rst phase, and actual results are com\u00adputed in the second phase. \nThe real challenge is in comput\u00ading context-sensitive summaries, i.e., summaries that can be instantiated \nwith any context to yield the most precise behavior of the procedures under that context. In this section, \nwe brie.y explain the two main ideas be\u00adhind our summary computation technique that can be used to perform \na precise interprocedural analysis using a precise intraprocedural random interpreter. 3.1 Random Symbolic \nRun Intraprocedural random interpretation involves interpret\u00ading a program using random values for the \ninput variables. The state at the end of the procedure can be used as a sum\u00admary for that procedure. \nHowever, such a summary will not be context-sensitive. For example, consider the code frag\u00adment in Figure \n2. The second assertion at the end of the code fragment is true in the context i = 2, but this condi\u00adtional \nfact is not captured by the random state at the end of the code fragment. The .rst main idea of the algorithm \nis that in order to make the random interpretation scheme context-sensitive, we can simply delay choosing \nrandom values for the input variables. Instead of using states that map variables to .eld values, we \nuse states that map variables to linear polyno\u00admials in terms of input variables. This allows the .exibility \nto replace the input variables later depending on the con\u00adtext. However, we continue to choose random \nweights at join points and perform a random a.ne join operation. As an example, consider again the code \nfragment from be\u00adfore, shown now in Figure 3. Note that the symbolic random state at the end of the code \nfragment (correctly) does not satisfy the second assertion. However, in a context where i = 2, the state \ndoes satisfy c = a + i since c evaluates to 2 and a to 0. This scheme of computing partly symbolic summaries \nis surprisingly e.ective and guarantees context-sensitivity, i.e., it entails all valid equivalences \nin all contexts. In contrast, there seems to be no obvious way to extend an arbitrary Figure 3: An illustration \nof symbolic random inter\u00adpretation on the code fragment shown in Figure 2. Note that the second assertion \nis true in the context i =2, and the symbolic random interpreter veri.es it. intraprocedural abstract \ninterpretation scheme to perform a context-sensitive interprocedural analysis. This is the case, for \nexample, for the intraprocedural abstract interpretation for linear arithmetic that was .rst described \nby Karr [9] in 1976. An interprocedural abstract interpretation for linear arithmetic was described by \nM\u00a8uller-Olm and Seidl [14] only recently in 2004. 3.2 Multiple Runs Consider the program shown in Figure \n4. The .rst asser\u00adtion in procedure B is true. However, the second assertion is false since the non-deterministic \nconditional (which arises as a result of our conservative abstraction of not modeling the conditional \nguards) in procedure A can evaluate di.erently in the two calls to procedure A, even with the same input. \nIf we use the same random symbolic run for procedure A at di.erent call sites in procedure B, then we \nincorrectly con\u00adclude that the second assertion holds. This happens because use of the same run at di.erent \ncall sites assumes that the non-deterministic conditionals in the called procedure are resolved in the \nsame manner in di.erent calls. This prob\u00adlem can be avoided if a fresh or independent run is used at \neach call point. By fresh run, we mean a run computed with a fresh choice of random weights at the join \npoints. One naive way to generate n fresh runs for a procedure P is to execute n times the random interpretation \nscheme for procedure P , each time with a fresh set of random weights. However, this may require computing \nan exponential num\u00adber of runs for other procedures. For example, consider a program in which each procedure \nFi calls procedure Fi+1 two times. To generate a run for F0, we need 2 fresh runs for F1, which are obtained \nusing 4 fresh runs for F2,and so on. The second main idea in our algorithm is that we can generate the \nequivalent of t fresh runs for a procedure P if Figure 4: A program with 2 procedures. The .rst assertion \nat the end of procedure B is true, while the second assertion is not true, because procedure A may run \ndi.erent branches in di.erent runs. This .gure demonstrates that use of just one random symbolic run \nis unsound. t fresh runs are available for each of the procedures that P calls, for some parameter t \nthat depends on the underlying abstraction. This idea relies on the fact that an a.ne com\u00adbination of \nt runs of a procedure yields the equivalent of a fresh run for that procedure. For an informal geometric \nintu\u00adition, note that we can obtain any number of fresh points in a 2-dimensional plane by taking independent \nrandom a.ne combinations of three points that span the plane. In Figure 5, we revisit the program shown \nin Figure 4 and illustrate this random interpretation technique of using a fresh run of a procedure at \neach call site. Note that we have chosen t =2. The t runs of the procedure are shown in parallel by assigning \na tuple of t values to each variable in the program. Note that procedure B calls procedure A three times. \nHence, to generate 2 fresh runs for procedure B, we need 6 fresh runs for procedure A. The .gure shows \ncomputation of 6 fresh runs from the 2 runs for procedure A. The .rst call to procedure A uses the .rst \ntwo of these 6 runs, and so on. Note that the resulting program states at the end of procedure B satisfy \nthe .rst assertion, but not the second assertion thereby correctly invalidating it.  4. INTERPROCEDURAL \nRANDOM INTERPRETATION We now describe the precise interprocedural randomized algorithm, as a standard \ntwo-phase computation. The .rst phase, or the bottom-up phase, computes procedure sum\u00admaries by starting \nwith leaf procedures. The second phase, or top-down phase, computes the actual results of the analy\u00adsis \nat each program point by using the summaries computed in the .rst phase. In presence of loops in the \ncall graph and inside procedures, both phases require .xed-point computa\u00adtion, which we address in Section \n6. We .rst describe our program model. 4.1 Preliminaries A program is a set of procedures, each with \none entry and one exit node. We assume that the .owchart representation of a procedure consists of nodes \nof the kind shown in Fig\u00adure 1. For simplicity, we assume that the inputs and outputs  Figure 5: A program \nwith two procedures, which is also shown in Figure 4. This .gure illustrates the random interpretation \ntechnique of computing multiple random symbolic runs (in this example, 2) for a procedure, and using \nthem to generate a fresh random symbolic run for every call to that procedure. Run j and Run j. are used \nat the jth call site of procedure A while computing the two runs for procedure B. Note that this technique \nis able to correctly validate the .rst assertion and falsify the second one. of a procedure are passed \nas global variables. We use the following notation: IP : Set of input variables for procedure P.  OP \n: Set of output variables for procedure P.  kI: Maximum number of input variables for any pro\u00adcedure. \n ko: Maximum number of output variables for any pro\u00adcedure.  kv: Maximum number of visible variables \nat any pro\u00adgram point.  n: Number of program points.  f: Fraction of procedure calls to the number \nof pro\u00adgram points.  r: Ratio of number of f assignments (in the SSA ver\u00adsion of the program) to the \nnumber of program points.  Note that the set IP includes the set of all global variables read by procedure \nP directly as well as the set IP. for any procedure P. called by P. Similarly for set OP .  4.2 Phase \n1 A summary for a procedure P, denoted by YP ,iseither . (denoting that the procedure has not yet been \nanalyzed, or on all paths it transitively calls procedures that have not yet been analyzed), or is a \ncollection of t runs {YP,i}ti=1.A run is a mapping from output variables in OP to random symbolic values, \nwhich are linear expressions in terms of the input variables IP of procedure P.The value of t is abstraction \ndependent. (For linear arithmetic, the value of t= kI + kv + csu.ces, for some small constant c. For \nunary uninterpreted functions, the value of t=4+ c su.ces.) To compute a procedure summary, the random \ninterpreter computes a sample S at each program point, as shown in Figure 1. A sample is either . or \na sequence of t states. A state at a program point pis a mapping of program variables (visible at point \np) to symbolic (random) linear expressions in terms of the input variables of the enclosing procedure. \nWe use the notation Si to denote the ith state in sample S. The random interpreter starts by initializing \nto . the summaries of all procedures, and the samples at all pro\u00adgram points except at procedure entry \npoints. The samples at entry points are initialized to Si(x)= x for every input variable x and 1 = i \n= t. The random interpreter com\u00adputes a sample S at each program point from the samples at the immediately \npreceding program points, and using the summaries computed so far for the called procedures. The transfer \nfunctions for the four kinds of .owchart nodes are described below. After the random interpreter is done \ninter\u00adpreting a procedure, the summary is simply the projection of the sample (or the t states) at the \nend of the procedure to the output variables of the procedure. The random inter\u00adpreter interprets each \n.owchart node as follows. Assignment Node. See Figure 1(a). Let S be the sam\u00adple before an assignment \nnode x := e.If S is ., then the sample S. is de.ned to be .. Otherwise, the random inter\u00adpreter computes \nS. by executing the assignment statement in sample S as follows. ( . rand() if e is ? Si(x)= Eval(e,Si) \notherwise Si.(y)= Si(y),for all variables y other than x. Here rand() denotes a fresh random value. Conditional \nNode. See Figure 1(b). Let S be the sample before a conditional node. The samples S1 and S2 after the \nconditional node are simply de.ned to be S. This re.ects the fact that we abstract away the conditional \nguards. Join Node. See Figure 1(c). Let S1 and S2 be the two sam\u00adples immediately before a join point. \nIf S1 is ., then the sample S after the join point is de.ned to be S2 . Similarly, if S2 is .,then S \n= S1 . Otherwise, the random interpreter selects t random weights w1,...,wt to perform a join oper\u00adation. \nThe join operation involves taking a random a.ne combination of the corresponding states in the two samples \nS1 and S2 immediately before the join point. Si = fwi (Si 1,Si 2) Procedure Call. See Figure 1(d). Let \nS be the sample before a call to procedure P. , whose input (global) vari\u00adables are i1,...,ik.If S is \n., or if the summary YP . is ., then the sample S. after the procedure call is de.ned to be .. Otherwise \nthe random interpreter executes the proce\u00addure call as follows. The random interpreter .rst generates \nt fresh random runs Y1,...,Yt for procedure P. using the current summary (t runs) for procedure P. . \nEach fresh run Yi for procedure P. is generated by taking a random a.ne combination of the t runs in \nthe summary of procedure P. . This involves choosing random weights wi,1,...,wi,t with the constraint \nthat wi,1 + \u00b7\u00b7\u00b7+ wi,t = 1, and then doing the following computation. t X Yi(x)= wi,j \u00d7YP .,j(x) j=1 The \ne.ect of a call to procedure P. is to update the values of the variables OP . that are written to by \nprocedure P. . The random interpreter models this e.ect by updating the values of the variables OP using \nthe fresh random runs Yi (computed above) as follows. ( Yi(x)[Si(i1)/i1,...,Si(ik)/ik]if x.OP . Si.(x)= \nSi(x) otherwise 4.3 Phase 2 For the second phase, the random interpreter also main\u00adtains a sample S \n(which is a sequence of t states)ateach program point, as in phase 1. The samples are computed for each \nprogram point from the samples at the preceding program points in exactly the same manner as described \nin Section 4.2. However, there are two main di.erences. First, the states are mapping of program variables \nto constants, i.e., elements of the .eld F. Second, the sample S at the entry point of a procedure P \nis obtained as an a.ne com\u00adbination of all the non-. samples at the call sites to P.Let these samples \nbe S1,...,Sk . Then for any variable x, k X jSi(x)= wi,j \u00d7Si (x) j=1 where wi,1,...,wi,k are random weights \nwith the constraint that wi,1 + \u00b7\u00b7\u00b7 + wi,k = 1, for all 1 = i = t.This a.ne combination encodes all the \nrelationships (involving input variables of procedure P) that hold in all calls to procedure P. 4.4 \nOptimization Maintaining a sample explicitly at each program point is expensive (in terms of time and \nspace complexity) and redundant. For example, consider the samples before and after an assignment node \nx:= e. They di.er (at most) only in the values of variable x. An e.cient way to represent samples at \neach program point is to convert all procedures into minimal SSA form [3] and to maintain one global \nsample for each procedure in\u00adstead of maintaining a sample at each program point. The values of a variable \nxin the sample at a program point p are represented by the values of the variable vx,p in the global \nsample, where vx,p is the renamed version of variable x at program point p after the SSA conversion. \nUnder such a representation, interpreting an assignment node or a proce\u00addure call simply involves updating \nthe values of the modi.ed variables in the global sample. Interpreting a join node in\u00advolves updating \nthe values of f variables at that join point in the global sample. This completes the description of \nthe interprocedural ran\u00addom interpretation. We sketch next the proof of correctness.  5. CORRECTNESS \nOF THE ALGORITHM Asample Sp computed by the random interpreter at a program point p has the following \nproperties. We use the term input context, or simply, context, to denote any set of equivalences between \nprogram expressions involving only the input variables.) Sp D1. Completeness: In all input contexts, \nentails all equivalences that hold at p along the paths analyzed by the random interpreter. D2. Soundness: \nWith high probability, in all input con\u00adtexts, Sp entails only the equivalences that hold at p along \nthe paths analyzed by the random interpreter. The error probability .(t) (assuming that the preced\u00ading \nsamples satisfy property D2) is bounded above by \u00ab t/kv .(t) = q kI dkv q where d= nj +nc +.. Here nj \nrefers to the number of join points and nc refers to the number of procedure calls along any path analyzed \nby the random inter\u00adpreter. . refers to the maximum degree of SEval(e) for any expression e computed \nby the random inter\u00adpreter (and expressed in terms of the input variables of the program) along any path \nanalyzed by it. 1 We brie.y describe the proof technique used to prove these properties. Detailed proof \ncan be found in the full version of the paper [8]. First we hypothetically extend the random interpreter \nto compute a fully-symbolic state at each program point, i.e., a state in which variables are mapped \nto polynomials in terms of the input variables and random weight variables corresponding to join points \nand procedure calls. A key part of the proof strategy is to prove that the fully-symbolic state at each \npoint captures exactly the set of equivalences at that point in any context along the paths analyzed \nby the random interpreter. In essence, a fully-symbolic inter\u00adpreter is both sound and complete, even \nthough it might be computationally expensive. The proof of this fact is by induction on the number of \nsteps performed by the random interpreter. Property D1 follows directly from the fact that the tstates \nthat make up a sample in the random interpreter can all be obtained by instantiating the random variables \nin the fully\u00adsymbolic state. 1. is 1 for the linear arithmetic abstraction. For the ab\u00adstraction of uninterpreted \nfunctions, . is bounded above by the maximum number of function applications processed by the random \ninterpreter along any path analyzed by it. We now prove property D2 in two steps. (For practical reasons, \nwe perform arithmetic over the .nite .eld Zq,which is the .eld of integers modulo q, for some randomly \nchosen prime q. This is described in more detail in Section 9. Hence, we compute and show that the value \nof .(t) is small under this assumption. It is possible to do the proof without this assumption, but the \nproof is more complicated.) Step 1. We .rst bound the error probability that a sam\u00adple S with t states \ndoes not entail exactly the same set of equivalences as the corresponding fully-symbolic state . of degree \nd in a given context. The following theorem speci\u00ad.es a bound on this error probability, which we refer \nto as the discovery factor D(d, t). It is possible to prove better bounds for speci.c abstractions. t/kv \nTheorem 2. D(d, t) = dkq v . The proof of this theorem is in the full version of the pa\u00adper [8]. Note \nthat this theorem also provides a bound on the error probability in the process of discovering equiva\u00adlences \nfor the intraprocedural analysis (Theorem 1). Step 2. Next we observe that it is su.cient to analyze \nthe soundness of a sample in a smaller number of contexts (com\u00adpared to the total number of all possible \ncontexts), which we refer to as a basic set of contexts. If a sample entails exactly the same set of \nequivalences as the corresponding fully-symbolic state for all contexts in a basic set, then it has the \nsame property for all contexts. Let N denote the number of contexts in any smallest basic set of contexts. \nThe following theorem speci.es a bound on N. It is possible to prove better bounds for speci.c abstrac\u00adtions. \nTheorem 3. N = q kI . The proof of this theorem is in the full version of the pa\u00adper [8]. The probability \nthat a sample Sp is not sound in any of the contexts is bounded above by the probability that Sp is not \nsound in any one given context multiplied by the size of any basic set of contexts. Thus, the error probability \n.(t) mentioned in property D2 is bounded above by D(d, t) \u00d7 N. 6. FIXED POINT COMPUTATION AND COMPLEXITY \nThe notion of loop that we consider for .xed point compu\u00adtation is that of a strongly connected component \n(SCC). For de.ning SCCs in a program in an interprocedural setting, we consider the directed graph representation \nof a program that has been referred to as supergraph in the literature [16]. This directed graph representation \nconsists of a collection of .owgraphs, one for each procedure in the program, with the addition of some \nnew edges. For every edge to a call node, say from node n1 to call node n2 with the call being to pro\u00adcedure \nP , we add two new edges: one from node n1 to start node of procedure P , and the other from exit node \nof pro\u00adcedure P to node n2.Now consider the DAG D of SCCs of this directed graph representation of the \nprogram. Note that an SCC in DAG D may contain nodes of more than one procedure 2 (in which case it contains \nall nodes of those procedures). In both phase 1 and phase 2, a random interpreter pro\u00adcesses all SCCs \nin the DAG D in a top-down manner. It goes around each SCC until a .xed point is reached. In phase 1, \na sample computed by the random interpreter rep\u00adresents sets of equivalences, one for each context. A \n.xed point is reached for an SCC in phase 1, if for all points p in the SCC and for all contexts C (for \nthe procedure enclos\u00ading point p), the set of equivalences at p in context C has stabilized. In phase \n2, a sample computed by the random interpreter represents a set of equivalences; and .xed point is reached \nfor an SCC, if for all points p in the SCC, the set of equivalences at p has stabilized. Let H1 and H2 \nbe the upper bounds on the number of iterations required to reach a .xed point across any SCC in phase \n1 and 2 respectively. To prove a bound on H1, we .rst observe that there exists a concise deterministic \nrepresentation for the sets of equiv\u00adalences, one for each context, that can arise at any program point. \nThe set of all equivalences in one given context can usually be succinctly described. For example, the \nset of all linear equivalences can be represented by a basis [9]. For the domain of uninterpreted functions, \nthe set of all Herbrand equivalences can be described by a value graph [7]. How\u00adever, it it challenging \nto represent these sets concisely for all contexts, which are potentially in.nite in number. We illustrate \nsuch a representation by means of an example. Consider the following program fragment. if (*) then {x \n:=a; } else {x :=3; } if (*) then {y :=b; z := c; } else {y :=2; z := 2; } The sets of equivalences for \nall contexts can be represented by the pair (E, L),where E is a set of relationships involving program \nvariables and a set of join variables ., x - 3+ .1(3 - a)=0 y - 2+ .2(2 - b)=0 z - 2+ .3(2 - c)=0 and \nL is the following set of relationships among . s. .2 = .3 The above set of equations capture all equivalences \namong the program expressions in all contexts. For example, note that these equations imply that x = \n3 in the context {a = 3},or y = z in the context {b = c}. Theorem 4 stated below describes more precisely \nthis rep\u00adresentation. Theorem 4. At any program point p, the sets of equiv\u00adalences for all contexts can \nbe described by a pair (E, L), where E isa set of f = kv equations of the form: ai X pi + .ji pji =0 \n1 = i = f j=1 with the following properties: 2This happens when the call graph of the program contains \na strongly connected component of more than one node. E1. For all 1 = i =f, pi 1 =0,...,p iai =0 are \nindependent equations that are linear over the input variables of the enclosing procedure, and ai =kI \n. E2. p1 =0,...,p. =0 are independent equations that are linear over the program variables visible at \np. and L is a set of linear relationships among .ij s. The proof of Theorem 4 is in the full version \nof the paper [8]. It must be pointed out that the representation described by the above theorem is only \nof academic interest; it has been introduced for proving a bound on H1.It is not clear how to construct \nsuch a representation e.ciently. We now state and prove the theorem that bounds H1. Theorem 5. H1 = (2kI \n+1)kv +1. Proof. Consider the representation (E,L) of the sets of equivalences for all contexts (as described \nin Theorem 4) that can arise at a program point p. For any pair (E,L),we de.ne measure M((E,L)) to be \nthe sum of the size of L and ai P j the sum of integers in the set {1+kI -ai | pi + p.E}. i j=1 Now suppose \nthat at some program point in an SCC, the sets of equivalences for all contexts are described by (E1,L1) \nin some iteration and by (E2,L2) in a later iter\u00adation. Then it must be the case that for all contexts \nC, the set of equivalences described by (E2,L2) in context C is implied by the set of equivalences described \nby (E1,L1)in context C. Using this, it can be shown that if (E1,L1)and (E2,L2) do not represent the same \nsets of equivalences, then M((E2,L2)) <M((E1,L1)). Note that M((E,L))is non-negative with a maximum value \nof (2kI +1)kv .Hence, H =(2kI +1)kv +1. The following theorem states a bound on H2. Theorem 6. H2 = kv \n+1. The proof of the above theorem follows from the observation that the set of equivalences at any program \npoint in phase 2 can be represented by a set of at most kv equations that are linear over the program \nvariables visible at that point. We have described a worst-case bound on the number of iterations required \nto reach a .xed point. However, we do not know if there is an e.cient way to detect a .xed point since \nthe random interpreter works with randomized data-structures. Hence, the random interpreter blindly goes \naround each SCC as many times as is su.cient for reach\u00ading .xed-point. Note that the edges in an SCC \nS can be decomposed intoaDAG DS and a set of back-edges. If a random interpreter processes the nodes \ninside an SCC S in a top-down manner according to their ordering in the DAG DS , then it needs to process \neach SCC for H1(b+1) steps in phase 1 and for H2(b+ 1) steps in phase 2, where b de\u00adnotes the number \nof back-edges in that SCC. Note that this guarantees that a .xed point has been reached. Since b is typically \nvery small, as experiments also suggest, we ignore it in our further discussion. We now state the time \ncomplexity of the random inter\u00adpreter as a function of the error probability. We assume a unit time complexity \nfor our arithmetic operations since we perform arithmetic over a small .nite .eld. However, performing \narithmetic over a .nite .eld may lead to some additional error probability, which we analyze in Section \n9. Theorem 7. For any constant c, if we choose t= kI kv + c, then the random interpreter runs in time \nO(nkv kI 2t(1 + r+ ftko)) and has an error probability of O(( 1 q )c). Proof. The total number of samples \ncomputed by the random interpreter is ns = n(H1 + H2). We assume the use of optimization described in \nSection 4.4. The computation of a sample across an assignment node takes O(kI t)time (assuming that the \nexpression assigned is of constant size, and the Eval function takes O(kI ) time). The processing of \na join node takes O(pkI t) time, where p is the number of f assignments at the join node after SSA conversion. \nThe cost of executing a procedure call is O(kI t2ko). Hence, the total cost of computing all samples \nis O(nskI t(1 + r+ ftko)). Each sample can be unsound with an error probability of .(t), whichisde.nedinpropertyD2 \ninSection 5. Hence, the total error probability is bounded above by ns.(t). Assum\u00ading that q is signi.cantly \ngreater than the various parame\u00adters d,kI ,kv,n, the error probability decreases exponentially with c,where \nc = t-kI kv. If we perform arithmetic using 32 bit numbers, then q 232 and the error probability can \nbe made arbitrarily small with even a small value of c.The ratio r is bounded above by kv. However, it \nhas been reported [3] that r typically varies between 0.3to2.8 irrespective of program size. If we re\u00adgard \nkI and ko to be constants, since they denote the size of the interface between procedure boundaries and \nare sup\u00adposedly small, then the complexity of our algorithm reduces to O(nkv 2), which is linear in the \nsize of the program and quadratic in the maximum number of visible program vari\u00adables at any program \npoint.  7. APPLICATION TO DISCOVERING LINEAR RELATIONSHIPS In this section, we apply the generic interprocedural \nanal\u00adysis developed in the earlier sections to the abstraction of linear arithmetic. For this abstraction, \nit is possible to prove a better bound on one of the parameters D(d,t), which con\u00adtrols the complexity \nof the generic algorithm. t-kv Theorem 8. D(d,t) = d te .Here e denotes qt-kv the Euler constant 2.718 \n\u00b7\u00b7\u00b7. The proof of Theorem 8 is in the full version of the paper [8]. Theorem 8 implies the following \ncomplexity bound, which is better than the generic bound. Theorem 9. For any constant c, if we choose \nt = kv + kI +c, then the random interpreter runs in time O(nkvkI 2t(1+ r+ ftko)) and has an error probability \nof O(( 1 q )c). It must be mentioned that the above bound is a conser\u00advative bound in terms of the constraint \non t. Experiments discussed in Section 10 suggest that even t = 3 does not yield any error in practice. \nRelated Work. Recently, Muller-Olm and Seidl gave a de\u00adterministic algorithm (MOS) that discovers all \nlinear rela\u00adtionships in programs that have been abstracted using non\u00addeterministic conditionals [14]. \nThe MOS algorithm is also based on computing summaries of procedures. However, their summaries are deterministic \nand consist of linearly  Figure 6: A code fragment that illustrates the dif\u00adference between the deterministic \nsummaries com\u00adputed by MOS algorithm and the randomized sum\u00admaries computed by our algorithm. independent \nruns of the program. The program shown in Figure 6 illustrates the di.erence between the deterministic \nsummaries computed by MOS algorithm and the random\u00adized summaries computed by our algorithm. The MOS \nal\u00adgorithm maintains the (linearly independent) real runs of the program, and it may have to maintain \nas many as kv 2 runs. The runs maintained by our algorithm are .ctitious as they do not arise in any \nconcrete execution of the program; however they have the property that (with high probability over the \nrandom choices made by the algorithm) they entail exactly the same set of equivalences in all contexts \nas do the real runs. Our algorithm needs to maintain only a few runs. The conservative theoretical bounds \nshow that kv + kI runs are required while experiments suggest that even 3 runs are good enough. The authors \nhave proved a complexity of O(nkv 8)for the MOS algorithm assuming a unit cost measure for arithmetic \noperations. However, it turns out that the arithmetic con\u00adstants that arise in MOS algorithm may be so \nhuge that .(2n) bits for required for representing constants, and hence .(2n) time is required for performing \na single arithmetic operation. Thus, the complexity of MOS algorithm is expo\u00adnential in n. Program Pm \nshown in Figure 7 in Section 9 illustrates such an exponential behavior of MOS algorithm. The MOS algorithm \ncan also use the technique of avoiding large arithmetic constants by performing arithmetic modulo a randomly \nchosen prime, as described in Section 9. How\u00adever this makes MOS a randomized algorithm; and the com\u00adplexity \nof our randomized algorithm remains better than that of MOS. It is not clear if there exists a polynomial \ntime deterministic algorithm for this problem. Sagiv, Reps and Horwitz gave an e.cient algorithm (SRH) \nto discover linear constants interprocedurally in a program [19]. Their analysis only considers those \na.ne assignments whose right hand sides contain at most one occurrence of a vari\u00adable. However, our analysis \nis more precise as it treats all a.ne assignments in a precise manner, and also it discovers all linear \nrelationships (not just constants). The .rst intraprocedural analysis for discovering all linear relationships \nwas given by Karr much earlier in 1976 [9]. The fact that it took several years to obtain an interprocedural \nanalysis to discover all linear relationships in linear programs demonstrates the complexity of interprocedural \nanalysis.  8. APPLICATION TO VALUE NUMBERING In this section, we discuss the application of our tech\u00adnique \nto discovering equivalences among expressions built from unary uninterpreted functions. This abstraction \nis very useful in modeling .elds of data-structures and can be used to compute must-alias information. \nFor this abstraction, it is possible to prove better bounds on the parameters D(d, t) and N, which control \nthe complexity of the generic algo\u00adrithm. t-2 The proof of Theorem 10 is similar to the proof of Theo\u00adrem \n8 and is based on the observation that any equivalence in the theory of unary uninterpreted functions \ninvolves only 2 variables, rather than kv variables. k22 Theorem 11. N = vq . The proof of Theorem 11 \nis given in the full version of the paper [8]. Theorem 10 and Theorem 11 imply the following com\u00adplexity \nbound, which is better than the generic bound. Theorem 12. For any constant c, if we choose t =4+ c, \nthen the random interpreter runs in time O(nkvkI 2t(1 + r + ftko)) and has an error probability of O( \nq1 c ). Related Work. There have been number of attempts at de\u00adveloping intraprocedural algorithms for \nglobal value num\u00adbering, which is the problem of discovering equivalences in a program with non-deterministic \nconditionals and uninter\u00adpreted functions. Until recently, all the known algorithms were either exponential \nor incomplete [10, 17, 18]. Recently, we presented a randomized [6] as well as a deterministic [7] polynomial \ntime complete algorithm for this problem. We are not aware of any complete interprocedural algo\u00adrithm \nfor value numbering. Gil and Itai have characterized the complexity of a similar problem, that of type \nanalysis of object oriented programs in an interprocedural setting [4]. 9. ARITHMETIC IN A FINITE FIELD \nWe .rst illustrate the problem that arises from perform\u00ading arithmetic over an in.nite .eld. Consider \nthe program shown in Figure 7. Any summary-based approach for dis\u00adcovering linear relationships will \nessentially compute the fol\u00adlowing summary for procedures Fm and Gm: Fm(x)=22m x Gm(x)=22m x Note that \nrepresenting the arithmetic constant 22m using standard decimal notation requires .(2m) digits. F0(x) \n= { return 2x; } Fi(x) = { t:= Fi-1(x); return Fi-1(t); } G0(x) = { return 2x; } Gi(x) = { t:= Gi-1(x); \nreturn Gi-1(t); } Main() = { t1 := Fm(0); t2 := Gm(0); Assert(t1 =t2); } Figure 7: A program Pm for which \nany determin\u00adistic summary based interprocedural analysis would require .(2m) space and time for manipulating \narith\u00admetic constants (assuming standard binary repre\u00adsentation). The program Pm contains 2m +3 proce\u00addures \nMain, Fi and Gi for every integer i .{0,...,m}. A randomized analysis does not have this problem. The \nproblem of big numbers can be avoided by performing computations over the .nite .eld Zq , which is the \n.eld of in\u00adtegers modulo q, for some randomly chosen prime q,instead of working over the in.nite .eld \n[12]. However, this may result in an additional error probability in the algorithm, if the soundness \nproperty (property B1) of the SEval func\u00adtion described in Section 2.3.1 breaks down. The soundness of \nSEval function for the abstraction of uninterpreted func\u00adtions does not rely on the underlying .eld; \nhence performing arithmetic over a .nite .eld in that case does not introduce any additional error probability. \nHowever, the soundness of SEval function for linear arithmetic abstraction breaks if arithmetic is performed \nover a .nite .eld. We next de\u00adscribe the resulting additional error probability in that case for both \ndiscovering and verifying equivalences. For the purpose of discovering equivalences, we need to run the \nalgorithm described in Section 4 two times, each time with an independently chosen random prime, say \nq1 and q2. The equivalences discovered in the .rst run of the algorithm are veri.ed in the second run \nof the algorithm. This is required because if there is an equivalence involv\u00ading large constants, say \ny = bx for some large constant b and variables x and y, then the .rst run of the algorithm will print \nout an spurious equivalence y =(b mod q1)x. The second run of the algorithm will (very likely) invali\u00addate \nthis spurious equivalence since the probability that b mod q1 = b mod q2 for randomly chosen large enough \nq1 and q2 is very small. However, note that this technique does not discover the equivalence y = bx; \nin general it does not discover equivalences that involve constants that are larger than the randomly \nchosen prime numbers. Perhaps this is the best that can be hoped from a polynomial time algo\u00adrithm since \nthe size of the constants in equivalences may be exponential in the size of the program, as illustrated \nby the example in Figure 7. The resulting additional error proba\u00adbility for discovering equivalences \nis given by the product of the number of independent equivalences discovered and the additional error \nprobability for verifying an equivalence. The additional error probability for verifying an equiva\u00adlence \nis simply the error probability that two given distinct integers are equal modulo a random prime q. The \nfollowing property states this error probability. Prime Used t 983 65003 268435399 E1 a E2 E3 E1 E2 E3 \nE1 E2 E3 2 1.7 0.2 95.5 0.1 0 95.5 0 0 95.5 3 0 0 64.3 0 0 3.2 0 0 0 4 0 0 0.2 0 0 0 0 0 0 5 0 0 0 0 \n0 0 0 0 0 6 0 0 0 0 0 0 0 0 0 aEi =((m . i -mi)/m. i) \u00d7100 m1 . = # of reported variable constants x=c \nm2 . = # of reported variable equivalences x=y m3 . = # of reported dependent induction variables mi \n= # of correct relationships of the corresponding kind Table 1: The percentage Ei of incorrect relationships \nof di.erent kinds discovered by the interprocedural random interpreter as a function of the number of \nruns and the randomly chosen primes on a collection of programs, which are listed in Table 2. The to\u00adtal \nnumber of correct relationships discovered were m1 = 3442 constants, m2 = 4302 variable equivalences, \nand m3 =50 dependent induction variables. Property 13. The probability that two distinct m bit in\u00adtegers \nare equal modulo a randomly chosen prime less than U is at most u 1 for U> 2um log (um). Theoretically, \nthe size of constants that can arise in the equivalences can be O(2n), as illustrated by the example \nin Figure 7. Hence, theoretically, to make the error proba\u00adbility arbitrarily small, we need to work \nwith primes whose size is O(n). However, in practice, the size of all equiva\u00adlences is bounded by a small \nconstant, and hence working with a small constant sized prime yields a low error proba\u00adbility. Our experiments \nsuggest that a 32 bit prime is good enough in practice, as it did not yield any false equivalence. 10. \nEXPERIMENTS In the earlier part of this paper we have expanded the body of theoretical evidence that \nrandomized algorithms may have certain advantages, such as simpler implementa\u00adtions and better computational \ncomplexity, over determin\u00adistic ones. In this paper, for the .rst time, we investigate experimentally \nthe behavior of these algorithms. The goal of these experiments are threefold: (1) attempt to measure \nexperimentally the soundness probability and how it varies with certain parameters of the algorithm, \n(2) measure the running time and e.ectiveness of the interprocedural ver\u00adsion of the algorithm, as compared \nwith the intraprocedural version, and (3) perform a similar comparison with a deter\u00administic interprocedural \nalgorithm. We ran all experiments on a Pentium 1.7GHz machine with 1Gb of memory. We used a number of \nprograms, up to 28,000 lines long, some from the SPEC95 benchmark suite, and others from similar measurements \nin previous work [19]. We measured running time using enough repetitions to avoid timer resolution errors. \nWe have implemented the interprocedural algorithm de\u00adscribed in this paper, in the context of the linear \nequalities domain. The probability of error grows with the complex\u00adity of the relationships we try to \n.nd, and shrinks with the Program go ijpeg li gzip bj linpackc sim whets .ops Lines 28622 28102 22677 \n7760 2109 1850 1555 1275 1151 in 63 31 53 49 0 14 3 9 0 x=c 1700 825 392 525 117 86 117 80 52 Random \nInter. x=y dep Time2 796 6 47.3 851 12 3.8 2283 9 34.0 372 2 2.0 9 0 1.2 16 1 0.07 296 0 0.35 2 6 0.03 \n4 4 0.02 Time3 70.4 5.7 51.4 3.05 1.8 0.11 0.54 0.05 0.03 .x=c 170 34 160 200 0 17 3 17 0 Random Intra. \n.x=y .dep Speedup3 260 3 107 1 9 24 1764 6 756 12 1 39 0 0 11 1 1 9 11 0 22 1 0 9 0 0 22 Determ. Inter. \n.in Speedup2 17 1.9 3 2.3 20 1.3 6 2.0 0 2.3 0 1.8 0 1.7 0 1.5 0 2.0 Table 2: The column in shows the \nnumber of linear relationships involving the input variables at the start of procedures. The column x=c \nshows the number of variables equal to constant values. The column x=y shows the number of variable equivalences. \nThe column dep shows dependent loop induction variables. The Timet columns show running time (in seconds) \nfor t=2 and t=3. The columns labeled . show how many fewer relationships of a certain kind are found \nwith respect to the interprocedural randomized algorithm. The columns labeled Speedupt columns show \nhow many times faster is each algorithm with respect to the interprocedural randomized one with a given \nvalue of t. increase in number of runs and the size of the prime num\u00adber we use for modular arithmetic. \nThe last two parameters have a direct impact on the running time.3 We .rst ran the interprocedural randomized \nanalysis on our suite of programs, using a variable number of runs, and three prime numbers of various \nsizes. We consider here equalities with constants (x=c), variable equalities (x=y), and linear induction \nvariable dependencies among variables used and modi.ed in a loop (dep).4 Table 1 shows the num\u00adber of \nerroneous relationships reported in each case, as a percentage of the total relationships found for the \ncorre\u00adsponding kind. These results are for programs with hundreds of variables, for which the theory \nrequires t>kv, yet in practice we do not notice any errors for t= 4. Similarly, our theoretical probability \nof error when using small primes, are also pes\u00adsimistic. With the largest prime shown in Table 1 we did \nnot .nd any errors if we use at least 3 runs. 5 In fact, for the simpler problem of discovering equalities, \nwe do not ob\u00adserve any errors for t= 2. This is in fact the setup that we used for the experiments described \nbelow that compare the precision and cost (in terms of time) of the randomized interprocedural analysis \nwith that of randomized intrapro\u00adcedural analysis and deterministic interprocedural analysis. The .rst \nset of columns in Table 2 show the results of the interprocedural randomized analysis for a few benchmarks \nwith more than 1000 lines of code each. The column head\u00adings are explained in the caption. We ran the \nalgorithm with both t=2 and t= 3, since the smaller value is faster and suf\u00ad.cient for discovering equalities \nbetween variables and con\u00adstants. As expected, the running time increases linearly with t. The noteworthy \npoint here is the number of relationships found between the input variables of a procedure. In the second \nset of columns in Table 2 we show how many 3For larger primes, the arithmetic operations cannot be in\u00adlined \nanymore. 4We found many more linear dependencies, but report only the induction variable ones because \nthose have a clear use in compiler optimization. 5With only 2 runs, we .nd a linear relationship between \nany pair of variables, as expected. fewer relationships of each kind are found by the intraproce\u00addural \nrandomized analysis, and how much faster that anal\u00adysis is, when compared to the interprocedural one. \nThe intraprocedural analysis obviously misses all of the input re\u00adlationships and consequently misses \nsome internal relation\u00adships as well, but it is much faster. The loss of e.ective\u00adness results are similar \nto those reported in [19]. Whether the additional information collected by the interprocedural analysis \nis worth the extra implementation and compile-time cost will depend on how the relationships are used. \nFor com\u00adpiler optimization it is likely that intraprocedural results are good enough, but perhaps for \napplications such as program veri.cation the extra cost might be worth paying. Finally, we wanted to \ncompare our interprocedural algo\u00adrithm with similar deterministic algorithms. We have imple\u00admented and \nexperimented with the SRH [19] algorithm, and the results are shown in the third set of columns in Table \n2. SRH is weaker than our algorithm, in that it searches only for equalities with constants. It does \nindeed .nd all such equalities that we also .nd. In theory, there are equalities with constants that \nwe can .nd but SRH cannot, because they are consequences of more complex linear relationships. However, \nthe set of benchmarks that we have looked at does not seem to have any such hard-to-.nd equalities. For \ncom\u00adparison with this algorithm, we used t=2, which is suf\u00ad.cient for .nding equalities of the form x= \ncand x= y. However, we .nd a few more equalities between the input variables (.in), and numerous equalities \nbetween local vari\u00adables, which SRH does not attempt to .nd. On average, SRH is 1.5 to 2.3 times faster \nthan our algorithm. Again, the cost may be justi.ed by the expanded set of relationships that we discover. \nA fairer comparison would have been with the MOS [14] algorithm, which is the most powerful deterministic \nalgo\u00adrithm known for this problem. However, implementing this algorithm seems quite a bit more complicated \nthan either of our algorithm or SRH. We also could not obtain an im\u00adplementation from anywhere else. \nFurthermore, we spec\u00adulate that due to the fact that MOS requires data struc\u00adtures whose size is O(kv \n4) at every program point, it will not fare well on the larger examples that we have tried, which have \nhundreds of variables and tens of thousands of program points. Another source of bottleneck may be the \ncomplexity of manipulating large constants that may arise during the analysis. 11. CONCLUSION We described \na uni.ed framework for random interpre\u00adtation, along with generic completeness and probabilistic soundness \ntheorems, both for verifying and for discovering relationships among variables in a program. These results \ncan be instantiated directly to the domain of linear relation\u00adships and, separately, of Herbrand equivalences, \nto derive ex\u00adisting algorithms and their properties. This framework also provides guidance for instantiating \nthe algorithms to other domains. It is, however, an open problem if a complete al\u00adgorithm can be obtained \nfor a combination of domains, such as linear arithmetic and Herbrand equivalences. The most important \nresult of this paper is to show that random interpreters can be extended in a fairly natural way to an \ninterprocedural analysis. This extension is based on the observation that a summary of a procedure can \nbe stored concisely as the results of a number of intraprocedural ran\u00addom interpretations with symbolic \nvalues for input variables. Using this observation, we have obtained interprocedural randomized algorithms \nfor linear relationships (with better complexity than similar deterministic algorithms) and for Herbrand \nequivalences (for which there is no deterministic algorithm). Previously published random interpretation \nalgorithms re\u00adsemble random testing procedures, from which they inherit trivial data structures and low \ncomplexity. The algorithms described in this paper start to mix randomization with sym\u00adbolic analysis. \nThe data structures become somewhat more involved, essentially consisting of random instances of oth\u00aderwise \nsymbolic data structures. Even the implementation of the algorithms starts to resemble that of symbolic \ndeter\u00administic algorithms. This change of style re.ects our belief that the true future of randomization \nin program analysis is not in the form of a world parallel to traditional symbolic analysis algorithms, \nbut in an organic mixture that exploits the bene.ts of both worlds. Acknowledgments We thank the anonymous \nreviewers for their useful feedback on a draft of this paper. 12. REFERENCES [1] P. Briggs, K. D. Cooper, \nand L. T. Simpson. Value numbering. Software Practice and Experience, 27(6):701 724, June 1997. [2] \nP. Cousot and R. Cousot. Abstract interpretation: A uni.ed lattice model for static analysis of programs \nby construction or approximation of .xpoints. In Proceedings of the 4th ACM Symposium on Principles of \nProgramming Languages, pages 234 252, 1977. [3] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Wegman, and \nF. K. Zadeck. E.ciently computing static single assignment form and the control dependence graph. ACM \nTransactions on Programming Languages and Systems, 13(4):451 490, Oct. 1990. [4] J. Y. Gil and A. Itai. \nThe complexity of type analysis of object oriented programs. Lecture Notes in Computer Science, 1445:601 \n634, 1998. [5] S. Gulwani and G. C. Necula. Discovering a.ne equalities using random interpretation. \nIn 30th ACM Symposium on Principles of Programming Languages, pages 74 84. ACM, Jan. 2003. [6] S. Gulwani \nand G. C. Necula. Global value numbering using random interpretation. In 31st ACM Symposium on Principles \nof Programming Languages, pages 342 352, Jan. 2004. [7] S. Gulwani and G. C. Necula. A polynomial-time \nalgorithm for global value numbering. In 11th Static Analysis Symposium, volume 3148 of Lecture Notes \nin Computer Science. Springer, 2004. [8] S. Gulwani and G. C. Necula. Precise interprocedural analysis \nusing random interpretation. Technical Report UCB//CSD-04-1353, University of California, Berkeley, 2004. \n[9] M. Karr. A.ne relationships among variables of a program. In Acta Informatica, pages 133 151. Springer, \n1976. [10] G. A. Kildall. A uni.ed approach to global program optimization. In 1st ACM Symposium on \nPrinciples of Programming Language, pages 194 206. ACM, Oct. 1973. [11] W. Landi. Undecidability of static \nanalysis. ACM Letters on Programming Languages and Systems, 1(4):323 337, Dec. 1992. [12] R. Motwani \nand P. Raghavan. Randomized Algorithms. Cambridge University Press, 1995. [13] S. S. Muchnick. Advanced \nCompiler Design and Implementation. Morgan Kaufmann, San Francisco, 2000. [14] M. M\u00a8uller-Olm and H. \nSeidl. Precise interprocedural analysis through linear algebra. In 31st Annual ACM Symposium on Principles \nof Programming Languages, pages 330 341. ACM, Jan. 2004. [15] T. Reps. On the sequential nature of interprocedural \nprogram-analysis problems. Acta Informatica, 33(8):739 757, Nov. 1996. [16] T. Reps, S. Horwitz, and \nM. Sagiv. Precise interprocedural data.ow analysis via graph reachability. In 22nd ACM Symposium on POPL, \npages 49 61. ACM, 1995. [17] B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and redundant \ncomputations. In 15th ACM Symposium on Principles of Programming Languages, pages 12 27, 1988. [18] O. \nR\u00a8uthing, J. Knoop, and B. Ste.en. Detecting equalities of variables: Combining e.ciency with precision. \nIn Static Analysis Symposium, volume 1694 of Lecture Notes in Computer Science, pages 232 247. Springer, \n1999. [19] M. Sagiv, T. Reps, and S. Horwitz. Precise interprocedural data.ow analysis with applications \nto constant propagation. Theoretical Computer Science, 167(1 2):131 170, 30 Oct. 1996. [20] M. Sharir \nand A. Pnueli. Two approaches to interprocedural data .ow analysis. In S.S. Muchnick and N.D. Jones, \neditors,. Program Flow Analysis: Theory and Applications, pages 189 234, 1981.   \n\t\t\t", "proc_id": "1040305", "abstract": "We describe a unified framework for random interpretation that generalizes previous randomized intraprocedural analyses, and also extends naturally to efficient interprocedural analyses. There is no such natural extension known for deterministic algorithms. We present a general technique for extending any intraprocedural random interpreter to perform a context-sensitive interprocedural analysis with only polynomial increase in running time. This technique involves computing <i>random</i> summaries of procedures, which are complete and probabilistically sound.As an instantiation of this general technique, we obtain the first polynomial-time randomized algorithm that discovers all linear relationships interprocedurally in a linear program. We also obtain the first polynomial-time randomized algorithm for precise interprocedural value numbering over a program with unary uninterpreted functions.We present experimental evidence that quantifies the precision and relative speed of the analysis for discovering linear relationships along two dimensions: intraprocedural vs. interprocedural, and deterministic vs. randomized. We also present results that show the variation of the error probability in the randomized analysis with changes in algorithm parameters. These results suggest that the error probability is much lower than the existing conservative theoretical bounds.", "authors": [{"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "PP14115174", "email_address": "", "orcid_id": ""}, {"name": "George C. Necula", "author_profile_id": "81100295630", "affiliation": "University of California, Berkeley, Berkeley, CA", "person_id": "PP14109324", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1040305.1040332", "year": "2005", "article_id": "1040332", "conference": "POPL", "title": "Precise interprocedural analysis using random interpretation", "url": "http://dl.acm.org/citation.cfm?id=1040332"}