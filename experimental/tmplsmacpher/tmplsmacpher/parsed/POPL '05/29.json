{"article_publication_date": "01-12-2005", "fulltext": "\n Automated Soundness Proofs for Data.ow Analyses and Transformations via Local Rules Sorin Lerner Univ. \nof Washington lerns@cs.washington.edu  Erika Rice Univ. of Washington erice@cs.washington.edu ABSTRACT \nWe present Rhodium, a new language for writing compiler optimizations that can be automatically proved \nsound. Un\u00adlike our previous work on Cobalt, Rhodium expresses opti\u00admizations using explicit data.ow facts \nmanipulated by lo\u00adcal propagation and transformation rules. This new style allows Rhodium optimizations \nto be mutually recursively de.ned, to be automatically composed, to be interpreted in both .ow-sensitive \nand -insensitive ways, and to be ap\u00adplied interprocedurally given a separate context-sensitivity strategy, \nall while retaining soundness. Rhodium also sup\u00adports in.nite analysis domains while guaranteeing termina\u00adtion \nof analysis. We have implemented a soundness checker for Rhodium and have speci.ed and automatically \nproven the soundness of all of Cobalt s optimizations plus a variety of optimizations not expressible \nin Cobalt, including An\u00addersen s points-to analysis, arithmetic-invariant detection, loop-induction-variable \nstrength reduction, and redundant array load elimination. Categories and Subject Descriptors: D.2.4 [Software \nEngineering]: Software/Program Veri.cation correctness proofs, reliability, validation; D.3.4 [Programming \nLan\u00adguages]: Processors compilers, optimization; F.3.1 [Logics and Meanings of Programs]: Specifying \nand Verifying and Reasoning about Programs mechanical veri.cation General Terms: Reliability, languages, \nveri.cation. Keywords: Compiler optimization, automated correctness proofs. 1. INTRODUCTION Compilers \nare an important part of a programmer s com\u00adputing infrastructure. If the compiler doesn t generate cor\u00adrect \ncode, the whole application being compiled is compro\u00admised. As a result, much work has been directed \ntoward Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n05, January 12 14, 2005, Long Beach, California, USA. Copyright 2005 ACM 1-58113-830-X/05/0001 ...$5.00. \nTodd Millstein UCLA todd@cs.ucla.edu  Craig Chambers Univ. of Washington chambers@cs.washington.edu \n making compilers trustworthy, including testing, transla\u00adtion validation [25, 23], credible compilation \n[26], and man\u00adual proof techniques [8, 9, 37, 14, 17, 10]. In previous work [19], we presented a system \nin which optimizations could be checked for soundness automatically. An optimiza\u00adtion is sound if it \nis guaranteed to preserve the semantics of any program it optimizes. Our solution was centered on a domain-speci.c \nlanguage for writing optimizations, called Cobalt. An optimization written in Cobalt was checked for \nsoundness by asking an automatic theorem prover to dis\u00adcharge a small set of simple proof obligations. \nWe proved by hand, once and for all, that if a Cobalt optimization satis.es these obligations, then the \noptimization is indeed sound. Unlike testing, credible compilation, and translation validation, this \nchecking is done once when the compiler is developed, separately from any particular programs being optimized. \nCobalt thus enables a key component of modern optimizing compilers to become trusted, and it opens the \ndoor for users to extend their compilers with application\u00adspeci.c optimizations without compromising \nthe correctness of the compiler. Cobalt is expressive enough to allow a range of .ow\u00adsensitive intraprocedural \noptimizations to be de.ned and proved correct. Using Cobalt, we were able to write and automatically \ncheck the soundness of constant propagation, copy propagation, dead-assignment elimination, common subexpression \nelimination, partial redundancy elimination, partial dead-code elimination, and simple kinds of pointer \nanalyses. However, Cobalt s design, where optimizing trans\u00adformations are triggered based on a restricted \ntemporal-logic predicate over the entire control .ow graph (CFG), imposes limits that make it di.cult \nto extend to a wider range of optimizations. In this paper we present Rhodium, a new domain-speci.c language \nfor optimizations that can express a much greater range of optimizations while still proving them sound \nauto\u00admatically. The key technical change from Cobalt is to make data.ow facts explicit (rather than implicit \nin a temporal\u00adlogic predicate) and to use a separate and extensible set of local propagation and transformation \nrules to generate new data.ow facts from old data.ow facts and to specify when statements are optimized \nbased on in.owing data.ow facts. Each data.ow fact is given a semantic meaning, in the form of a predicate \nover program states. To prove a Rhodium optimization correct, our system asks an automatic theo\u00adrem prover \nto discharge a local soundness lemma for each propagation and transformation rule, using the meanings \nof the facts manipulated by the rules and the concrete seman\u00adtics of the program s statements. We proved, \nonce by hand, that these lemmas imply that the optimization is globally sound. Because Rhodium s local \npropagation model is fun\u00addamentally di.erent from Cobalt s, this hand proof is also fundamentally di.erent, \nand couched in terms of abstract interpretation [8]. Rhodium s use of explicit data.ow facts with local \nprop\u00adagation and transformation rules enables several important advances over Cobalt s use of global \ntemporal-logic predi\u00adcates: Traditional form. A local propagation rule is a kind of .ow-or transfer \nfunction, which may be a more comfort\u00adable and understandable model for an optimization writer than Cobalt \ns global model.  Extensibility. Rhodium allows new propagation rules to be added without modifying any \nexisting rules or fact de.nitions, enabling optimizations to be enhanced more easily.  Recursively de.ned \nanalyses. When deciding whether to generate a particular data.ow fact on a state\u00adment s successor edge, \na Rhodium propagation rule can examine any other data.ow facts on the statement s predecessor edges. \nCobalt was in e.ect only able to propagate the same data.ow fact through a statement unchanged. Rhodium \nallows the propagation rules of data.ow facts to be de.ned mutually recursively, signi.\u00adcantly increasing \ntheir expressiveness and clarity.  Composed analyses and transformations. By us\u00ading a model based on \nlocal propagation and transforma\u00adtion rules, we can exploit previous work on automatically composing \nanalyses and transformations [18] to enable Rhodium optimizations to be automatically composed.  Flow-insensitive \nanalyses. We show how to interpret Rhodium propagation rules in a .ow-insensitive manner, soundly, yielding \nmore-e.cient analyses with no extra optimization-writer work. In contrast, Cobalt s global model was \ninherently .ow-sensitive.  Interprocedural analyses. We show how to de\u00ad.ne a context-sensitive interprocedural \nanalysis from a Rhodium intraprocedural analysis and a speci.cation of a context-sensitivity strategy. \nRhodium s local propaga\u00adtion model allows the local propagation rule for call state\u00adments to be derived \nautomatically. If the intraprocedural analysis is sound, then the interprocedural one is sound, too. \n In addition to moving to a local propagation model, we have also enriched Rhodium s expressiveness \nin the following orthogonal ways: Dynamic semantics extensions. Rhodium allows the optimization-writer \nto de.ne virtual extensions to the intermediate language s dynamic semantics which can compute properties \nof program execution traces. For ex\u00adample, the statement at which each memory location was allocated \ncan be computed via a dynamic semantics ex\u00adtension. These extensions can then be referenced in the meanings \nof data.ow facts, for instance in a points-to analysis with allocation-site heap summaries, enabling \na wider class of optimizations to be proved sound automat\u00adically. In.nite analysis domains. Rhodium allows \ndata.ow\u00adfact domains to be in.nite, leading to increased expres\u00adsiveness over Cobalt which only allowed \n.nite domains (such as the set of constants, variables, and expressions that appeared in the program \nbeing optimized). We present su.cient conditions, including some adapted from the database community, \nfor automatically guaranteeing that analyses terminate even in the face of such in.nite domains. Rhodium \nanalyses can also specify widening operators [8], without a.ecting soundness. The end result is a language \nthat is signi.cantly more expressive than Cobalt but nonetheless provides the same strong soundness guarantees. \nWe have implemented our strategy for automatically proving Rhodium analyses and optimizations sound using \nSimplify, the automatic theorem prover from ESC/Java [12]. We de.ned and automatically proved sound all \nof Cobalt s optimizations plus the follow\u00ading new optimizations and analyses that were not express\u00adible \nin Cobalt: loop-induction-variable strength reduction, a .ow-sensitive version of Andersen s points-to \nanalysis [3] with heap summaries, arithmetic invariant detection, con\u00adstant propagation through array \nelements, redundant array load elimination, and integer range analysis. Our Rhodium code de.nes 24 data.ow \nfacts, 105 propagation rules, and 14 transformation rules. Moreover, all these analyses can be interpreted \nas .ow-insensitive analyses and/or context\u00adsensitive or -insensitive interprocedural analyses, and they \ncan be automatically composed together to yield more\u00adprecise solutions, soundly. Section 2 introduces \nthe new .ow-function-oriented way of writing optimizations in Rhodium and describes the as\u00adsociated automated \nproof strategy based on abstract inter\u00adpretation. Section 3 presents our technique for reducing the complexity \nof proof obligations using extensions to the dy\u00adnamic semantics and shows how our technique can be used \nto reason automatically about heap summaries. Section 4 describes how we support in.nite analysis domains \nwhile still being able to guarantee termination. Sections 5 and 6 present our frameworks for building \nprovably sound .ow\u00adinsensitive and interprocedural optimizations. Section 7 dis\u00adcusses our execution \nengine for Rhodium in the Whirlwind compiler. Finally, sections 8 and 9 discuss future work and related \nwork, respectively.  2. Rhodium Rhodium optimizations run over a C-like intermediate language (IL) with \nfunctions, recursion, pointers to dy\u00adnamically allocated memory and to local variables, and arrays. This \nsection describes how intraprocedural, .ow\u00adsensitive analyses are expressed and automatically proven \nsound in Rhodium; sections 5 and 6 respectively discuss .ow-insensitive and interprocedural analyses. \nRhodium op\u00adtimizations operate over a CFG representation of the IL pro\u00adgram, with each node representing \na simple register-transfer\u00adlevel statement. Data.ow information is encoded in Rhodium by means of data.ow \nfacts, which are user-de.ned function symbols applied to a set of terms, for example hasConstValue(x, \n5) or exprIsAvailable(x, a + b). A Rhodium analysis uses prop\u00adagation rules, which are a stylized way \nof writing .ow func\u00adtions, to specify how data.ow facts propagate across CFG 1. decl X:Var, Y :Var, Z:Var \n 2. de.ne edge fact mustNotPointTo(X:Var, Y :Var) 3. with meaning C(X)=.C(&#38;Y ) 4. if .  stmt(X \n:= &#38;Z) . Y = Z 5. then mustNotPointTo(X, Y )@out 6. if mustNotPointTo(X, Y )@in . mustNotDef (X) \n 7. then mustNotPointTo(X, Y )@out  Figure 1: Simple pointer analysis in Rhodium. nodes. These user-de.ned \n.ow functions implicitly de.ne a data.ow analysis, whose solution is the least .xed point of the standard \nequations induced by the .ow functions. Once an analysis has reached a .xed point, the computed infor\u00admation \ncan be used by Rhodium transformation rules to rewrite some of the CFG s nodes. We wish to automatically \nprove Rhodium analyses and transformations sound. An analysis is sound if, for all IL procedures P , \nthe data.ow information computed for P is consistent with the procedure s concrete semantics. A trans\u00adformation \nis sound if, for all IL procedures P , the transfor\u00admation preserves P s semantics. Section 2.1 illustrates \nRhodium s propagation rules, and section 2.2 describes how such rules are automatically proven sound \nusing abstract interpretation. In section 2.3 we compare Rhodium s design and proof strategy with those \nof Cobalt and show the expressiveness bene.ts of our new design in Rhodium. Section 2.4 discusses Rhodium \ntrans\u00adformations and how they are automatically proven sound. Section 2.5 shows how to incorporate pro.tability \ninforma\u00adtion into Rhodium optimizations. 2.1 Propagation Rules We illustrate Rhodium s propagation rules \nwith a simple pointer analysis, shown in .gure 1. The analysis determines that a variable x de.nitely \ndoes not point to another variable y if x was assigned the address of a variable di.erent from y, and \nthen x was not modi.ed afterwards. Because our strategy for automated soundness checking is geared toward \nmust analyses, we encode our pointer information using the must-not-point-to relation instead of the \nmay-point-to re\u00adlation. Each edge in the CFG will therefore be annotated with facts of the form mustNotPointTo(X, \nY ), where X and Y range over variables in the associated IL procedure. The declaration of the mustNotPointTo \nedge fact is shown on line 2 of the Rhodium code (for now the meaning on line 3 can be ignored). Propagation \nrules in Rhodium indicate how edge facts are propagated across CFG nodes. For example, the rule on lines \n6-7 of .gure 1 de.nes a condition for preserv\u00ading a mustNotPointTo fact across a node: if the fact mustNotPointTo(X, \nY ) appears on the incoming CFG edge of a node n and n does not modify X, then the data.ow fact mustNotPointTo(X, \nY ) should appear on the outgoing edge of n. The left-hand side of a rule is called the antecedent and \nthe right-hand side the consequent. Each propagation rule is interpreted within the context of a CFG \nnode. Edge facts are followed by @ signs, with the name after the @ sign indicating the edge on which \nthe fact appears. For example, mustNotPointTo(X, Y )@in is true if the in\u00adcoming CFG edge of the current \nnode is annotated with mustNotPointTo(X, Y ). Facts without @ signs are node facts, and they represent \ninformation about the current node. For example, the user-de.ned mustNotDef (X) fact holds at a node \nif the node does not modify X. An accom\u00adpanying technical report [20] shows how users can de.ne these \nnode facts. The semantics of a propagation rule on a CFG is as fol\u00adlows: for each substitution of the \nrule s free variables that make the antecedent valid at some node in the CFG, the fact in the consequent \nis propagated. For the rule described above, the mustNotPointTo(X, Y ) fact will be propagated on the \noutgoing edge of a node for each substitution of X and Y with variables that makes the antecedent valid. \nWhile the rule in lines 6-7 of .gure 1 speci.es how to preserve mustNotPointTo facts, the rule in lines \n4-5 speci.es how to introduce them in the .rst place. That rule says that the outgoing CFG edge of a \nstatement X := &#38;Z should be annotated with all facts of the form mustNotPointTo(X, Y ), where Y and \nZ are distinct variables. All rules in .gure 1 are forward: the antecedent only refers to a node s incoming \nCFG edge and the consequent only refers to a node s outgoing CFG edge. Rhodium also sup\u00adports backward \nrules, where the antecedent only refers to out and the consequent only refers to in. The primary fo\u00adcus \nof our Rhodium work so far has been on forward analyses and transformations, and so we do not present \nany backward rules here. Also, for brevity and clarity, we only present def\u00adinitions and theorems for \nthe forward case, with the back\u00adward case covered in the accompanying technical report [20]. Section \n8 discusses the state of backward analyses and trans\u00adformations in Rhodium. A set of propagation rules \ntogether implicitly de.ne a data.ow analysis A whose domain D is the powerset lattice (2Facts of all \ndata.ow facts: (D, U, ., ., T, .)= , ., ., \u00d8, =, Facts), where Facts is the set of all data.ow facts. \nEach edge in the CFG is therefore annotated with a set of data.ow facts, where bigger sets are more precise \nthan smaller sets.1 The .ow function F of the analysis is de.ned by the prop\u00adagation rules: given a node \nand a set of incoming data.ow facts, F returns the set of all data.ow facts propagated by any of the \nindividual rules. Formally, the .ow function F is de.ned in terms of the meaning of an antecedent a, \nwhich is given by the function [a] : Node \u00d7 D \u00d7 Subst n bool (where Node is the set of all CFG nodes \nand Subst is the set of all substitutions). Given a node n, a set of facts d, and a substitution e, [a](n,d, \ne) is true i. e(a) holds at node n with incoming facts d (where e(\u00b7) represents substitution application). \nThe de.nition of [a] is straightforward, with the interesting case being: n [f( -n t )@in](n, d, e)= \nf(e( -t )) = d - n (where t denotes a sequence of terms) A complete de.nition of [a] is given in the \naccompanying technical report [20]. The .ow function F : Node \u00d7 D n D induced by a set R of forward propagation \nrules is then: -n - n F (n, d)= {e(f( t )) | [if a then f( t )@out] = R . [a](n,d, e)} The solution of \nthe induced analysis A is the least .xed 1We use the abstract interpretation convention that . is the \nmost optimistic information, and T is the most conservative information. point of the standard set of \ndata.ow equations generated from F . Although the two rules in .gure 1 propagate the same data.ow fact, \ndi.erent rules can propagate di.erent data.ow facts, and the .xed point is computed over all data.ow \nfacts simultaneously. In addition to edge facts and node facts, Rhodium also provides virtual data.ow \nfacts, which can be used to de\u00ad.ne shorthands for boolean combinations of other facts. This facility \nallows a may-point-to fact to be de.ned and referred to in analyses and transformations if desired: mayPointTo(X, \nY ) . \u00acmustNotPointTo(X, Y ). Such vir\u00adtual facts get replaced with the boolean expression they stand \nfor as a preprocessing step. Negation is provided in Rhodium only as a convenience. After all the virtual \nfacts have been expanded out, and nega\u00adtion has been pushed to the inside through conjunctions, disjunctions \nand quanti.ers, we require all negation on edge facts to cancel out. The absence of negated edge facts \nguar\u00adantees the monotonicity of F , as shown in the accompanying technical report [20]. Although disallowing \nnegated edge facts sounds restrictive, it actually corresponds to a com\u00admon usage pattern. Because Rhodium \nfacts are all must facts, the absence of a fact does not provide any informa\u00adtion only its presence \ndoes. As a result, we never found the need to use any negated edge facts, except as a nota\u00adtional convenience. \nFor example, in our analyses that use mayPointTo(X, Y ), it is always the lack of possible points-to \ninformation, i.e., \u00acmayPointTo(X, Y ), that enables more\u00adprecise analysis or transformation, which when \nexpanded yields mustNotPointTo(X, Y ). 2.2 Proving soundness automatically Our goal is to ensure automatically \nthat the data.ow in\u00adformation computed by the analysis A is sound with respect to the concrete collecting \nsemantics of the IL. Our automatic proof strategy separates the proof that A is sound into two parts: \nthe .rst part is analysis dependent and it is discharged by an automatic theorem prover; the second part \nis anal\u00adysis independent and it was shown by hand once and for all. For the analysis-dependent part, \nwe de.ne a su.cient soundness property that must be satis.ed by each propa\u00adgation rule in isolation, \nand we ask an automatic theorem prover to discharge this property for each rule. Separately, we have \nshown manually that if all propagation rules sat\u00adisfy the soundness property, then the data.ow information \ncomputed by the analysis A is sound. The formalization of Rhodium, including this manual proof, employs \nour pre\u00advious abstract-interpretation-based framework for compos\u00ading data.ow analyses and transformations \n[18]. As a result, all Rhodium analyses and transformations can be composed soundly, while allowing them \nto interact in mutually bene\u00ad.cial ways. The de.nition of soundness of a propagation rule depends on \nmeaning declarations that describe the concrete seman\u00adtics of edge facts. The meaning of a fact f is \na predicate on concrete execution states, C, with the intent that whenever f appears on an edge, the \nmeaning of f should hold in all con\u00adcrete execution states C that can appear on that edge. For example, \nthe meaning of mustNotPointTo(X, Y ), shown on line 3 of the Rhodium code, is C(X) . = C(&#38;Y ), where \nC(E) represents the result of evaluating expression E in execution state C. The meaning of mustNotPointTo \ntherefore says that the value of X in the execution state C should not be equal to the address of Y . \nWe denote the meaning of a fact f by [f], so that for example [mustNotPointTo](X, Y, C) . C(X) . = C(&#38;Y \n). To be sound, a propagation rule must preserve meanings: if a rule .res at a CFG node n, and the meanings \nof all facts .owing into n hold of execution states right before n, then the meaning of the propagated \nfact must hold for execution states right after n. To de.ne this formally, we denote by State the set \nof concrete execution states C, and we use n C n C' to say that the execution of n from state C yields \nstate C'. We also use allMeaningsHold(d, C) to say that the meanings of all facts in d hold of a program \nstate C: n allMeaningsHold(d, C) . Vf( -n t ) = d. [f]( -t ,C) The soundness of a propagation rule can \nthen be stated as follows: - n Def 1. A propagation rule if a then f( t )@out is said to be sound i. \nit satis.es the following property: V(n,C, C', d, e) = Node \u00d7 State2 \u00d7 D \u00d7 Subst . \u00bb nn [a](n, d, e) \n. C n C' .-(prop-ok) . [f](e( t ),C') allMeaningsHold(d, C) For each propagation rule, we use an automatic \ntheorem prover to discharge (prop-ok). The allMeaningsHold as\u00adsumption provides a one-way link between \n[a](n, d, e) and meanings of facts: it allows the theorem prover to derive -n - n [f]( t ,C) from f( \nt )@in, but not the other way around. For example, consider the rule in lines 6-7 of .gure 1. We e.ectively \nask the theorem prover to show that if a state\u00adment satisfying mustNotDef (X) is executed from a state \nC in which C(X) .C(&#38;Y ), then C'(X) .C'(&#38;Y ) in the re\u00ad == sulting state C'. The truth of this \nformula follows easily from the user-provided de.nition of mustNotDef and the system-provided concrete \nsemantics of our IL. If all propagation rules are sound, then it can be shown by hand, once and for all, \nthat the .ow function F is sound. The de.nition of soundness of F is the one from our frame\u00adwork on composing \ndata.ow analyses [18]. This de.nition depends on an abstraction function a : Dc n D, which formalizes \nthe notion of approximation. The concrete se\u00admantics of our IL is a collecting semantics, so that elements \nof Dc are sets of concrete stores. Meaning declarations nat\u00adurally induce an abstraction function a: \ngiven a set c = Dc of concrete stores, a(c) returns the set of all data.ow facts whose meanings hold \nof all stores in c. An element d = D approximates an element c = Dc if a(c) . d, or equiva\u00adlently if \nthe meanings of all facts in d hold of all stores in c. The de.nition of soundness of F , taken from \n[18], is then as follows (where Fc is the concrete collecting semantics .ow function): Def 2. A .ow function \nF is said to be sound i. it satis\u00ad.es the following property: V (n, c,d) = Node \u00d7 Dc \u00d7 D. a(c) . d . \na(Fc(n, c)) . F (n, d) The following lemma, which is proved in the accompa\u00adnying technical report [20], \nformalizes the link between the soundness of local propagation rules and the soundness of F . Lemma 1. \nIf all propagation rules are sound, then the induced .ow function F is sound. Once we know that the .ow \nfunction F is sound, we can use the following de.nition and lemma from our framework on composing data.ow \nanalyses to show that the analysis A is sound (where we denote by EP the set of edges in IL procedure \nP ): Def 3. An analysis A is said be sound i. for any IL program P , the concrete solution Sc : EP n \nDc and the ab\u00adstract solution SA : EP n D satisfy the following property: Ve = EP .a(Sc(e)) . SA(e). \nLemma 2. If the .ow function F is sound, then the anal\u00adysis A induced by the standard data.ow equations \nof F is sound. A proof of lemma 2 can be found in the accompanying technical report [20]. The following \ntheorem is immediate from lemmas 1 and 2: Theorem 1. If all propagation rules are sound, then the analysis \nA induced by the propagation rules is sound. Theorem 1 summarizes the part of the soundness proof of \nA that was done by hand once and for all. The automatic theorem prover is only used to discharge (prop-ok) \nfor each propagation rule, thus establishing the premise of theorem 1 that all propagation rules are \nsound. This way of factoring the proof is critical to automation. The proof of theorem 1 (which includes \nproofs of lemmas 1 and 2) is relatively complex. It requires reasoning about F , a and the .xed point \ncomputation, each one adding extra complexity. The proof also requires induction, which would be di.cult \nto fully automate. In contrast, (prop-ok) is a non-inductive local property that requires reasoning only \nabout a single state transition at a time. We have found that the heuristics used in automatic theorem \nprovers are well suited for these kinds of simple proof obligations. 2.3 Comparison with Cobalt To better \nexplain the additional expressive power of Rhodium, we show the Cobalt version of the pointer analysis \nfrom .gure 1: decl X:Var, Y :Var, Z:Var stmt(X := &#38;Z) . Y . = Z followed by mustNotDef (X) de.nes \nmustNotPointTo(X, Y ) with witness C(X) . = C(&#38;Y ) The Cobalt version says that an edge e should \nbe annotated with the mustNotPointTo(X, Y ) fact if on all CFG paths reaching e, there exists a statement \nX := &#38;Z where Y . = Z, which is followed by zero or more statements that do not modify X until the \nedge e is reached. The region between the statement X := &#38;Z and the edge e is called the witness\u00ading \nregion, and the key property of this region is that the witness, in this case C(X) .C(&#38;Y ), holds \nof all program = states C in the region. As shown above, the condition for triggering a Cobalt transformation \nis expressed as a global temporal-logic pred\u00adicate over the entire control .ow graph (CFG). This styl\u00adized \nglobal condition codi.es a scenario common to many data.ow analyses: an enabling statement establishes \na data.ow fact, and then a sequence of zero or more innocu\u00adous statements preserve it. The Cobalt proof \nstrategy was tailored toward such analyses: we asked the theorem prover to show that the witness was \nestablished by the enabling statement and preserved by any innocuous statements. In the pointer-analysis \nexample, the theorem prover would be asked to show that C(X) .C(&#38;Y ) holds after a statement = X \n:= &#38;Z, where Y .Z, and that C(X)= C(&#38;Y ) is pre\u00ad = .served by statements that don t modify X. \nWhile Cobalt can express this analysis and prove it sound automatically, Cobalt s global condition for \nexpressing op\u00adtimizations has drawbacks. First, Cobalt s proof strategy only allows each data.ow fact \nto have one associated global condition. This requirement makes it di.cult to extend an existing Cobalt \nanalysis. In contrast, a Rhodium analysis can be easily and modularly extended simply by writing new \npropagation rules. Second, Cobalt s global condition requires the same data.ow fact to hold throughout \nthe entire witnessing re\u00adgion. In contrast, the Rhodium abstract interpretation strategy allows .ne-grained \ncontrol over how facts are propa\u00adgated. Programmers can write propagation rules that string di.erent \ndata.ow facts together in .exible ways. This al\u00adlows Rhodium to express many kinds of global conditions \nnot supported by Cobalt. Third, Cobalt s metatheory did not allow an analysis to refer to itself, either \ndirectly or indirectly. One consequence of this restriction is that the mustNotDef fact used in our pointer \nanalysis had to be overly conservative because it could not make use of the pointer information currently \nbeing computed. In contrast, the antecedents of Rhodium rules can refer to arbitrary facts, even those \nthat are being propagated in the consequent. The .xed-point semantics of Rhodium and the accompanying \nabstract interpretation theory ensure that such recursion is well-de.ned. To illustrate some of the additional \n.exibility of Rhodium, we extend our simple pointer analysis from .gure 1 with ad\u00additional rules, slowly \nbuilding up toward a .ow-sensitive ver\u00adsion of Andersen s points-to analysis [3]. This analysis was not \nexpressible in Cobalt. We start with a rule for propa\u00adgating pointer information through simple assignments: \ndecl X:Var , Y :Var , A:Var if stmt (X := A) . mustNotPointTo(A, Y )@in then mustNotPointTo(X, Y )@out \nThe outgoing information, mustNotPointTo(X, Y ), isa di.erent instantiation of the mustNotPointTo fact \nthan the incoming information, mustNotPointTo(A, Y ). This way of stringing together mustNotPointTo(X, \nY ) and mustNotPointTo(A, Y ) was impossible to achieve in Cobalt. Next we extend our Rhodium analysis \nwith a rule for prop\u00adagating pointer information through pointer stores: decl X:Var , Y :Var , A:Var \n, B:Var if stmt (*A := B) . mustPointTo(A, X)@in . mustNotPointTo(B, Y )@in then mustNotPointTo(X, Y \n)@out The mustPointTo(A, X) fact, computed by rules not shown here, says that A de.nitely points to \nX, and its meaning is C(A)= C(&#38;X). The above rule for pointer stores performs a strong update in \nwhich we know exactly what A points to. We can also write a weak-update rule for pointer stores: decl \nX:Var , Y :Var , A:Var , B:Var if stmt(*A := B) . mustNotPointTo(X, Y )@in . mustNotPointTo(B, Y )@in \nthen mustNotPointTo(X, Y )@out Finally, we add a rule for propagating pointer information through pointer \nloads: decl X:Var , Y :Var , A:Var if stmt(X := *A) . mustNotPointToHeap(A)@in . V B:Var . mayPointTo(A, \nB)@in . mustNotPointTo(B, Y )@in then mustNotPointTo(X, Y )@out The mustNotPointToHeap(A) fact, whose \nrules are not shown here, says that A does not point to the heap (or equiv\u00adalently, that A points to \nsome variable), and its meaning is *Z : Var .C(A)= C(&#38;Z). The mayPointTo fact is a vir\u00ad tual data.ow \nfact as de.ned earlier: mayPointTo(X, Y ) \u00acmustNotPointTo(X, Y ). The rule as a whole says that X does \nnot point to Y after a statement X := *A if all the variables in the may-point-to set of A do not point \nto Y . Starting with a simple pointer analysis and extending it step by step with additional rules, we \nhave now expressed in Rhodium a .ow-sensitive intraprocedural version of An\u00addersen s pointer analysis. \nRhodium s propagation rules are the key enablers of this expressiveness leap over Cobalt. Propagation \nrules allow us to de.ne mustNotPointTo re\u00adcursively, and they allow us to string together instances of \nthe mustNotPointTo fact, and other facts, in .exible ways. Rhodium s new proof strategy allows us to \nautomatically prove this analysis sound, despite the extra expressiveness over Cobalt. In section 3 we \nwill show how to extend our Rhodium pointer analysis even further by adding heap sum\u00admaries, and in sections \n5 and 6 we will show how to make it .ow-insensitive and/or interprocedural, all while retaining automated \nsoundness reasoning. 2.4 Transformation Rules Rhodium propagation rules are used to de.ne data.ow analyses. \nThe information computed by these analyses can then be used in transformation rules to optimize IL pro\u00adgrams. \nA transformation rule describes the conditions un\u00adder which a node in the CFG can be replaced by a new \nnode without changing the behavior of the program. To illustrate transformations, .gure 2 shows an arithmetic \nsimpli.cation optimization. The optimization is driven by an arithmetic invariant analysis that keeps \ntrack of invari\u00adants of the form E1 = E2 * E3, represented in Rhodium with the equalsTimes data.ow fact. \nSome of the rules for this analysis are shown in .gure 2. The optimization per se is performed by a single \ntransformation rule on lines 27-28, which says that a statement Y := I * C can be transformed to Y := \nX if we know that X = I * C holds before the statement. We want to automatically show that a Rhodium \noptimiza\u00adtion is sound, according to the following de.nition: Def 4. A Rhodium optimization O, which \nincludes any number of propagation rules and transformation rules, is sound i. for all IL procedures \nP , the optimized version P ' of P , produced by performing some subset of the transfor\u00admations suggested \nby O, has the same semantics as P . 8. decl E1:Expr, E2:Expr, E3:Expr 9. decl X:Var, Y :Var, I:Var \n10. decl C:Int, C1:Int, C2:Int, C3:Int 11. de.ne edge fact equalsTimes(E1:Expr, E2:Expr, 12. E3:Expr \n 13. with meaning C(E1)= C(E2) * C(E3) 14. if equalsTimes(E1,E2 ,E3)@in . 15. unchanged (E1) . unchanged \n(E2) . 16. unchanged (E3) 17. then equalsTimes(E1,E2,E3)@out 18. if .  stmt(X := I * C) . X = I 19. \nthen equalsTimes(X, I, C)@out 20. if .  stmt(I := I + C1) . X = I . 21. equalsTimes(X, I, C2)@in 22. \nthen equalsTimes(X, I - C1 ,C2)@out 23. if .  stmt(X := X + C1) . X = I . 24. equalsTimes(X, I - C2,C3 \n)@in . 25. C1 = applyBinaryOp(*,C2 ,C3) 26. then equalsTimes(X, I, C3)@out 27. if stmt(Y := I * C) \n. equalsTimes(X, I, C)@in 28. then transform Y := X  Figure 2: Arithmetic simpli.cation optimization \nin Rhodium. Due to space limitations, only a few rep\u00adresentative rules are shown here. As with propagation \nrules, our automatic proof strategy requires an automatic theorem prover to discharge a local soundness \nproperty for each transformation rule. This prop\u00aderty is given in the following de.nition of soundness \nfor a transformation rule. Def 5. A transformation rule if a then transform n ' is said to be sound i. \nit satis.es the following property: V(n, C, C ' , d, e) = Node \u00d7 State2 \u00d7 D \u00d7 Subst . \u00bb n C ' . [a](n, \nd, e) . C n. n . C n C ' allMeaningsHold(d, C) The following theorem, which is proven in the accompany\u00ading \ntechnical report [20], summarizes the part of the proof of soundness of an optimization O that is performed \nby hand: Theorem 2. If all the propagation rules and transforma\u00adtion rules of a Rhodium optimization \nO are sound, then O is sound. As described earlier, the fact that each propagation rule is sound is su.cient \nto ensure that the induced analysis A is sound. This fact, along with the fact that each transfor\u00admation \nrule is sound, is su.cient to show that any subset of the suggested transformations can be performed \nwithout changing the semantics of any IL procedure. 2.5 Pro.tability heuristics In many optimizations, \nthe condition that speci.es when a transformation is legal can be separated from the condition that speci.es \nwhen a transformation is pro.table. Rhodium provides pro.tability edge facts for implementing pro.tabil\u00adity \ndecisions. Because they are not meant to be used for justifying soundness, these facts have an implicit \nmeaning of i:=0; i:=0; while(...){ x:=i*20; . inserted ... while (...) { i:=i+1; ... ... i:=i+1; if(...){ \nx:=x+20; . inserted i:=i+1; ... } if (...) { ... i:=i+1; y:=i*20; x:=x+20; . inserted }} ... y := x; \n. transformed } (a) (b) Figure 3: Code snippet before and after loop\u00adinduction-variable strength reduction. \ntrue, and as a result, they can always be safely added to the CFG. We can therefore give programmers \na lot of freedom in computing these facts. In particular, we allow programmers to write regular compiler \npasses called pro.tability analy\u00adses, which are given a read-only view of the compiler s data structures, \nexcept for the ability to add pro.tability facts to the CFG. In this way, one can for example use standard \nalgorithms to annotate the CFG with facts indicating where the loop heads are, what the loop-nest is, \nor how many times a variable is accessed inside of a loop these algorithms do not have to be expressed \nusing propagation rules. Transfor\u00admation rules can then directly use these facts to select only those \ntransformations that are pro.table. To illustrate the use of pro.tability facts, we show how to write \nloop-induction-variable strength reduction in Rhodium. The idea of this optimization is that if all de.\u00adnitions \nof a variable I inside of a loop are increments, and some expression I * C is used in the loop, then \nwe can (1) insert X := I * C before the loop (2) insert X := X + C right after every increment of I in \nthe body of the loop and (3) replace I * C with X in the body of the loop. Con\u00adsider for instance the \ncode snippet in .gure 3(a). The result of performing loop-induction-variable strength reduction is shown \nin .gure 3(b). This optimization was not expressible in Cobalt. The e.ect of this optimization can be \nachieved in Rhodium in two passes. The .rst pass inserts assignments to the newly created induction variable \nx. The second pass propagates arithmetic invariants in order to determine that x =i *20 holds just before \nthe statement y:= i *20, thereby justifying the strength-reduction transformation. A dead-assignment \nelimination pass can also be run afterwards in order to clean up the dead assignments to i. For the .rst \npass, determining when it is safe to insert an assignment is simple: an assignment X := E can be in\u00adserted \nif X is dead after the insertion point. The tricky part of this .rst pass lies in determining which of \nthe many legal insertions should be performed so that the later arithmetic\u00adinvariant pass can justify \nthe desired strength reduction. This decision of what assignments to insert can be guided by pro.tability \nfacts. A pro.tability analysis running stan\u00addard algorithms can insert the following three pro.tability \nfacts: indVar (I, X, C) is inserted on the edges of a loop (includ\u00ading the incoming edge into the loop) \nto indicate that I is a induction variable in the loop, X is a fresh induction variable that would be \npro.table to insert, and C is the anticipated multiplication factor between I and X. afterIncr (I) is \ninserted on the immediate edge following a statement I := I + 1.  afterLoopInit (I) is inserted on the \nimmediate edge follow\u00ading a statement I := E that is at the head of a loop.  In the example of .gure \n3, indVar (i, x, 20) would be in\u00adserted throughout the loop, afterIncr (i) would be inserted after the \nincrements of i and afterLoopInit(i) would be in\u00adserted after the assignment i :=0. The following two \ntrans\u00adformation rules then indicate which assignments should be inserted: decl X:Var , I:Var, C:Const \nif stmt (skip) . dead(X)@out . afterIncr (I)@in . indVar (I,X, C)@in then transform X := X + C if stmt \n(skip) . dead(X)@out . afterLoopInit (I)@in . indVar(I, X, C)@in then transform X := I * C Following \nour previous work on Cobalt, we express inser\u00adtion as replacement of a skip statement. These skip state\u00adments \nare only virtual, and the compiler implicitly inserts an in.nite supply of them in between any two nodes \nin the CFG. The above transformations are sound because of the dead (X) fact. The other facts are simply \nthere to guide which dead assignments to insert. Since their meaning is true and they are used in a conjunction, \nthey do not have any impact on soundness checking.2 Rhodium s way of incorporating pro.tability information \nis superior to Cobalt s approach. Cobalt allowed pro.tabil\u00adity decisions to be made in a choose function \nthat did not af\u00adfect soundness: after the set of all legal transformations was generated, the choose \nfunction would select a subset of these transformations to actually perform. The generate-and-test approach \nof the choose function is not always well-suited in practice because there may be in.nitely many legal \ntrans\u00adformations to generate. The above example is such a case: there are in.nitely many expressions \nE for which we can insert an assignments X := E when X is dead. Rhodium solves this problem by allowing \nprogrammers to write ar\u00adbitrarily complex compiler passes for inserting pro.tability facts that can then \nbe used to prune out the transformations at the point where they are generated. For the second pass that \nruns after the dead assign\u00adments have been inserted, we can use the arithmetic\u00adinvariant analysis from \n.gure 2. The rules in .gure 2 are su.cient to trigger the strength-reduction transforma\u00adtion in .gure \n3(b). The statement x :=i *20 establishes the data.ow fact equalsTimes(x, i, 20). Every sequence of i \n:=i +1 followed by x:= x+ 20 propagates .rst equalsTimes(x, i-1, 20) and then equalsTimes(x, i, 20). \nAsa result, equalsTimes(x, i, 20) is propagated to y:= i* 20, thereby triggering the transformation to \ny := x. 2This example uses the backward data.ow fact dead(X). Section 8 describes the state of backward \nanalyses and trans\u00adformations in Rhodium.  3. DYNAMIC SEMANTICS EXTENSIONS The meaning of data.ow facts \nwe have seen so far all talked about the concrete program states occurring on edges annotated with the \nfact. Unfortunately, the natural way to express the meaning of certain data.ow facts is to talk about \ncomplete traces of program states rather than single program states. As a motivating example, consider \nextending our pointer analysis from section 2.1 with heap summaries, where each allocation statement \nS represents all the memory blocks al\u00adlocated at S. The meaning of mustNotPointTo(X, S), where X is a \nvariable and S is an allocation site, is that X does not point to any of the memory blocks allocated \nat S. This property, however, cannot be expressed by just looking at the current program state, because \nthere is no way to deter\u00admine which memory blocks were allocated at site S. We could try to .x this problem \nby enriching our meanings so that they talk about execution traces. From the execution trace one can \neasily extract the memory blocks that were allocated at site S (by evaluating, for each statement S : \nX := new T in the trace, the value of X in the successor state). However, in order to extract this information, \none has to use quanti.ers that range over indices of unbounded\u00adlength traces. Unfortunately, we have \nfound the heuristics used in automatic theorem provers for managing quanti.ers to be easily confounded \nby these kinds of quanti.ed formulas that arise when using unbounded-length traces. In order to solve \nthis problem Rhodium allows the pro\u00adgram state to be extended with user-de.ned components called state \nextensions. These components are meant to gather the information from a trace that is relevant for a \nparticular data.ow fact. Instead of referring to the trace, the meaning can then refer to the state extension. \nFor the above heap summary example, the state would be extended with a map describing which heap locations \nwere allocated at which sites, and the meaning of mustNotPointTo could then use this map instead of referring \nto the trace. To update the user-de.ned components of the state, pro\u00adgrammers also extend the dynamic \nsemantics of the interme\u00addiate language. Because of the way these extensions to the semantics are declared, \nthey are guaranteed to be conser\u00advative, meaning that the trace of a program in the original semantics \nand the corresponding trace in the extended se\u00admantics agree on all the components of the program state \nfrom the original semantics. As a result, if we preserve the extended semantics using our regular Rhodium \nproof strat\u00adegy, we are guaranteed to also preserve the original seman\u00adtics. User de.ned state extensions \nare just a formal tool for proving soundness: they can be erased without having any impact on how analyses \nor IL programs are executed. We present state extensions in more detail by show\u00ading how they can be used \nto extend our pointer analysis with heap summaries. In order to de.ne the meaning of mustNotPointTo over \nsummaries, we de.ne an additional component of the program state called summary of , which maps each \nheap location to the heap summary that repre\u00adsents it. We start by considering allocation site summaries, \nwhere the locations created at the same site are summarized together by the node that created them. The \ndeclaration of summary of then looks as follows: type HeapSummary = Node de.ne state extension summary \nof : Loc n HeapSummary  The summary of map gets updated according to the follow\u00ading dynamic semantics \nextension: decl X:Var , T :Type if stmt (X := new T ) then (C@out).summary of = (C@in).summary of [C@out(X) \nn currNode] The terms C@in and C@out refer respectively to the pro\u00adgram states before and after the current \nstatement, while the special term currNode refers to the current CFG node. The rule as a whole says that \nan allocation site X := new T updates the summary of component of the state by mak\u00ading the newly created \nlocation, obtained by evaluating X in C@out, map to the CFG node that was just executed. In all other \ncases the summary of component implicitly remains unchanged. We can easily modify the above declarations \nto achieve other kinds of summaries. In particular, table 1 shows how to modify the HeapSummary de.nition \nand change what C@out(X) maps to in the dynamic semantics extension in order to specify di.erent summarization \nstrategies. The rest of our treatment of heap summaries applies to all of the strategies, except when \nexplicitly stated. The next step is to de.ne the domain of abstract locations: type AbsLoc = Var | HeapSummary \nAn abstract memory location AL is either a variable or a heap summary. The intuition is that AL represents \na set of concrete memory locations: if AL is a variable, it represents the address of the variable; if \nAL is a heap summary, it represents the set of summarized heap locations. We can now modify our mustNotPointTo \nfact to take ab\u00adstract locations, instead of just variables (the meaning is explained below): de.ne edge \nfact mustNotPointTo(AL1 :AbsLoc, AL2 :AbsLoc) with meaning VL : Loc . belongsTo(L, AL1,C) . isLoc(C(*L)) \n. \u00acbelongsTo(C(*L), AL2,C) de.ne belongsTo(L:Loc, AL:AbsLoc,C:State) isVar(AL) . [L = C(&#38;AL)] . \nisHeapSummary(AL) . [C.summary of [L]= AL] The meaning of mustNotPointTo says that none of the lo\u00adcations \nbelonging to AL1 point to any of the locations be\u00adlonging to AL2. The locations belonging to AL1 are \nthose locations L for which belongsTo(L, AL1 ,C) holds. For all these locations L, we look up the memory \ncontent of L us\u00ading C(*L). If the memory content C(*L) is a location (as opposed to a scalar value, which \ncannot hold pointers), then we want C(*L) to not belong to AL2 . The auxiliary function belongsTo(L, \nAL,C) returns whether or not a location L belongs to an abstract loca\u00adtion AL in state C. The de.nition \nof belongsTo is split into two cases, based on the type of AL. If AL is a variable, then L belongs to \nAL if L is exactly the address of AL. If AL is a heap summary, then L belongs toAL if C.summary of maps \nL to AL. The rules for our pointer analysis must now be modi.ed to take summaries into account. Because \nof space limitations, HeapSummary C@out(X) maps to this in the dynamic semantics extension Allocation \nsite summaries Node currNode Type based summaries Type T Variable based summaries Var X Single heap summary \nunit () Table 1: Various kinds of heap summarization strategies achievable by varying the de.nition \nof HeapSummary and the dynamic semantics extension. we only present some representative rules here. The \ncom\u00adplete set of rules can be found in the accompanying technical report [20]. The following rule, which \nonly works for allocation site summaries, says that after an allocation site X := new T , X does not \npoint to any heap summary that is di.erent from the current node: decl Summary:HeapSummary, X:Var , T \n:Type if stmt(X := new . T ) . Summary = currNode then mustNotPointTo(X, Summary)@out To prove this \nrule sound, the theorem prover must show that the meaning of mustNotPointTo(X, Summary) holds after X \n:= new T . Since X is a variable and Summary is a heap summary, the meaning expands to isLoc(C(X)) . \nC.summary of [C(X)] .Summary. Since the = theorem prover knows that new T returns a location, it determines \nthat isLoc(C(X)) holds, and then the remaining obligation is C.summary of [C(X)] .Summary. To prove this, \nthe = theorem prover makes use of the user-de.ned extension to the dynamic semantics. Indeed, if we let \nC be the program state right after executing the allocation, then the dynamic semantics extension tells \nus that C.summary of [C(X)] = currNode. In conjunction with Summary .= currNode, this implies C.summary \nof [C(X)] .Summary, = which is what needed to be shown. The above rule for stmt(X := new T ) only works \nfor al\u00adlocation site summaries. Of all the pointer analysis rules, it is the only one that depends on \nthe heap summarization strategy. In order to modify it for other kinds of heap sum\u00admaries, the antecedent \nof the rule should compare Summary with the third column of table 1, rather than with currNode. Finally, \nwe now show the rule of our pointer analysis that requires the most complicated reasoning from the theorem \nprover: decl X:Var , Y :Var , AL2 :AbsLoc if stmt(X := *Y ) . VAL1 : AbsLoc . mayP ointT o(Y, AL1 )@in \n. mustNotPointTo(AL1 , AL2 )@in then mustNotPointTo(X, AL2)@out In the above rule, we again de.ne mayPointTo \nas before: mayPointTo(AL1, AL2) \u00acmustNotPointTo(AL1 , AL2 ). The rule as a whole says that X does not \npoint to AL2 after X := *Y if for all abstract locations AL1 that Y may point to, we have that AL1 does \nnot point to AL2 .  4. INFINITE ANALYSIS DOMAINS The domains of data.ow fact parameters in Cobalt were \n.nite for a particular intermediate language pro\u00adgram. For example, the Const and Expr domains did not \nrepresent all possible constants and expressions, but rather only those constants and expressions that \nap\u00adpeared in the intermediate-language program being ana\u00adlyzed. Rhodium improves on Cobalt by introducing \nin.nite domains. The Expr and Const domains in Rhodium now re\u00adfer to the in.nite unrestricted versions \nwhereas ExprInProg and ConstInProg refer to the .nite versions restricted to constants and expressions \nin the source program.3 The addition of in.nite domains increases the expressive\u00adness of Rhodium. For \nexample, being able to refer to expres\u00adsions that are not in the analyzed program is crucial for ex\u00adpressing \nthe arithmetic invariant analysis equalsTimes from section 2.4. Rhodium can also perform range analysis \nwhere the end points of the range are not restricted to constants in the program. Finally, Rhodium can \nexpress a better version of constant propagation because it can construct and then propagate constants \nthat are not in the source code. However, with this extra .exibility comes a challenge: whereas Cobalt \nanalyses were trivially guaranteed to ter\u00adminate, because all domains were .nite, Rhodium analyses may \nnow run forever. There are two ways in which a Rhodium analysis might run forever. The .rst one is that \na particular rule might not terminate. The second is that the .xed-point computation might not terminate. \nWe deal with each one of these in the next two subsections. 4.1 Termination of a single rule In order \nto guarantee that execution of each rule termi\u00adnates, we must guarantee that the rule has only a .nite \nnumber of instantiations (i.e., substitutions for its free vari\u00adables), and that each instantiation can \nbe evaluated in .nite time. For the latter, we restrict the logic of each rule s an\u00adtecedent to the decidable \nsubset of .rst-order logic in which quanti.ers only range over .nite domains.4 For the former, we wish \nto ensure a .nite-in-.nite-out property: if a rule is invoked on a node where all incoming edges have \n.nite sets of facts, then the rule will have only a .nite number of instantiations and will generate \nonly a .nite set of facts on outgoing edges. Unfortunately, unrestricted propagation rules do not have \nthat property: it is possible for a sound rule to propagate in.nitely many data.ow facts, even when the \ninput facts are .nite. For example, consider the following sound range-analysis rule: 3When we say .nite \nhere, we mean .nite once a given intermediate-language program has been singled out. 4Here again, the \ndomain must be .nite for a particular pro\u00adgram, not necessarily for all programs. de.ne edge fact inRange(X \n: Var , lo : Const, hi : Const) with meaning lo . C(X) . C(X) . hi if stmt(X := C) . C1 . C . C2 . C \nthen inRange(X, C1,C2 )@out There are in.nitely many instantiations of C1 and C2 that will make this \nrule .re, even if the input contains no data.ow facts. In order to guarantee that such a situation does \nnot oc\u00adcur, we make use of a notion from database community called safety [33], adapting it to the context \nof Rhodium. A Rhodium propagation rule is said to be .nite-safe if ev\u00adery free variable of in.nite domain \nin the consequent is .nite-safe. A variable is .nite-safe if it appears (after ex\u00adpanding virtual facts \nand folding away all negations) in the antecedent either in a data.ow fact, or on one side of an equality \nwhere the other side contains only .nite-safe vari\u00adables; .nite-safe variables thus are constrained to \nhave a .nite number of instantiations if the input fact set is .nite. The range-analysis rule above is \nnot .nite-safe, since neither C1 nor C2 is .nite-safe. Even if all rules are .nite-safe, a rule can still \nbe invoked on an in.nite input set: .. This case can happen at the start of analysis, since all edges \n(aside from the entry edge) are initialized with .. Since it is sound to propagate . as the result of \nany rule invoked with . on its input, we treat . specially and directly propagate . to the output without \ninvoking the rule explicitly. Thus, if all rules are .nite-safe, either they will be invoked on . and \nimmediately propagate ., or they will be invoked on a .nite set of input facts and propagate another \n.nite set of output facts in .nite time.  4.2 Termination of the .xed-point computa\u00adtion As discussed \nin section 2.1, the .ow function F is guaran\u00adteed to be monotonic, and so the data.ow values computed \nby iterative analysis form an ascending chain. To guarantee termination, all that is left is to ensure \nthat all ascending chains in the lattice have .nite length. In order to do this, we recall from section \n4.1 that we al\u00adready imposed the .nite-safe requirement, which led to all propagated sets being either \n.nite or .. We can therefore shrink our lattice to only include these .nite sets and .. The original \nunderlying lattice was the power-set lattice, in which the ordering was the superset relation. The shrunken \nlattice uses this same ordering, which means that all ascend\u00ading chains in the shrunken lattice must \nhave a .nite length, since the longest chain of decreasing-sized .nite sets is .nite. Notice that the \nlattice does not have a .nite height, because there can still be in.nite descending chains. Our technique \nfor guaranteeing termination is e.ective even in the face of data.ow facts with in.nite-domain pa\u00adrameters. \nFor example, the equalsTimes data.ow fact has all three of its parameters ranging over in.nite domains, \nand yet we are still able to guarantee that the analysis ter\u00adminates. In this case, the shrunken lattice \nis in.nitely wide and in.nitely tall, but its ascending chains are nonetheless guaranteed to be .nite. \n 4.3 Custom merges The range-analysis propagation rule in section 4.1 was sound but not .nite-safe: it \ncould produce an in.nite (and non-.) set of output inRange facts. However, the meaning of one of the \npropagated inRange facts, inRange(X, C, C), implies all the others meanings. So an alternative sound \nand .nite-safe propagation rule could be the following: if stmt(X := C) then inRange(X, C, C)@out Unfortunately, \nthis propagation rule interacts poorly with the powerset lattice s join function, intersection. If we \nuse intersection to join the fact set {inRange(x, 1, 1)} with {inRange(x, 2, 2)}, we get {}. We would \nprefer instead to get the fact set {inRange(x, 1, 2)}: this fact set is sound (and precise) since its \nmeaning is exactly the disjunction of the meanings of the two merging fact sets. Rhodium avoids this \ninformation-loss problem while re\u00adtaining .nite-safe propagation rules by allowing program\u00admers to de.ne \ntheir own merges. Rather than provide spe\u00adcial syntax for de.ning merge functions, we simply intro\u00adduce \na merge statement for which users can write ordinary Rhodium propagation rules: decl X:Var , C1:Int, \nC2:Int, C3:Int, C4:Int if stmt(merge) . inRange(X, C1 ,C2)@in[0] . inRange(X, C3 ,C4)@in[1] then inRange(X, \nmin(C1,C3 ), max(C2,C4))@out This example introduce edge indices: in[i] refers to the ith CFG input edge. \nThe previously used in was just syntactic sugar for in[0]. Similarly, out can also be indexed to re\u00adfer \nto the true and false successor edges of a branch node. When a rule refers to multiple input or output \nedges, there is one proof obligation sent to the theorem prover for each input-output-edge pair. The \ngeneral version of (prop-ok) that handles an arbitrary number of input and output edges is given in the \naccompanying technical report [20]. In the above case, there would be two proof obligations, one for \ninput edge 0 and one for input edge 1. For input edge 0, we would ask the theorem prover to show that \nif the meaning of inRange(X, C1 ,C2) holds of some program state C, and C on edge 0 steps to C ' through \nthe merge node, then the meaning of inRange(X, min(C1,C3), max(C2,C4 )) holds of C ' . A similar proof \nobligation would be generated for input edge 1. From a formal point of view, the lattice of the implicitly \nde.ned data.ow analysis A must be modi\u00ad.ed in order to take into account custom merge func\u00adtions. Consider \nthe example above, where the merge of S = {inRange(x, 1, 1)} and T = {inRange(x, 2, 2)} gives merge(S, \nT )= {inRange(x, 1, 2)}. In the powerset lattice of all data.ow facts, the expressions S, T and merge(S, \nT ) are unrelated. To prove the soundness of the custom merge, we instead need a lattice in which S U \nT . merge(S, T ) holds, meaning that the user s merge function returns an approxi\u00admation of the best \npossible merge (which is U). To address this problem, when a user-de.ned merge func\u00adtion is speci.ed, \nwe make use of the more general lattice of predicates: (D, U, ., ., T, .)=(Pred , ., ., ., true, false). \nThis lattice subsumes the powerset lattice since a set of data.ow facts can be interpreted as a predicate \nby taking the conjunction of the meanings of all the data.ow facts in the set. The view shown to the \nprogrammer is still that sets of data.ow facts are being stored on edges, but from a formal point of \nview, we interpret these sets as predi\u00adcates. In the example above, S becomes 1 . x . 1, T becomes 2 \n. x . 2, and merge(S, T ) becomes 1 . x . 2. Therefore S U T = (1 . x . 1) . (2 . x . 2), and since (1 \n. x . 1) . (2 . x . 2) . 1 . x . 2, we now have S U T . merge(S, T ) as desired. More generally, if a \nmerge rule passes property (prop-ok), we are guaranteed that if S and T are the two incoming predicates \nto the merge node, then the outgoing predicate merge(S, T ) will satisfy S . T . merge(S, T ), or S U \nT . merge(S, T ) in the lattice of predicates. Unfortunately, the lattice of predicates, even when shrunken \nto the meanings of only .nite sets of facts plus ., does not have the .nite-ascending-chain property. \nCon\u00adsider for example the inRange fact, and the in.nite sequence S0 ,S1,S2,..., where Si = {inRange(x, \n0,i)}. Each one of the sets Si is .nite and therefore belongs to the shrunken lattice; furthermore the \nsequence is an ascending chain, be\u00adcause each Si implies Si+1. Consequently, termination of the .xed-point \ncomputation is not guaranteed of analyses using custom merges, and indeed the kind of range analysis \ndiscussed here does not terminate.5 To allow the optimization writer to achieve termination in such cases, \nas well as allowing the optimization writer to make terminating analyses converge faster, Rhodium pro\u00advides \nwidening operators [8]. A Rhodium widening opera\u00adtor is a function, written in the underlying language \nof the compiler, that takes a node, an incoming data.ow fact set, and an unwidened outgoing data.ow fact \nset, and pro\u00adduces the widened outgoing fact set. After the Rhodium evaluation engine runs the propagation \nrules on a node n, given an input set din to produce an unwidened output set dout, the widening operator \nis run on n, din , and dout to produce the widened output set dwide . Finally, we com\u00adpute merge(dout,dwide) \n(using either the default merge or a custom merge if one is speci.ed) as the .nal outgoing set to propagate. \nFrom the soundness of F we know that the fact dout is sound, and since merge(dout ,dwide ) is more con\u00adservative \nthan dout , merge(dout ,dwide ) must also be sound, which means that the value dwide returned by the \nwidening operator does not a.ect soundness it only makes the result more conservative, thus helping \nthe iterative analysis reach a .xed point faster.  5. FLOW-INSENSITIVE ANALYSES An additional bene.t \nthat falls out from Rhodium s new .ow-function model is that Rhodium can easily support provably sound \n.ow-insensitive analyses. In particular, we can interpret propagation rules in a .ow-insensitive man\u00adner. \nInstead of keeping a separate set of data.ow facts at each edge, we keep a single set I for the whole \nprocedure. Iterative analysis proceeds as usual, except that each time a .ow function is run, it takes \nI as input, and its result is merged into I. In this way one can produce a sound .ow\u00adinsensitive analysis \nfrom a sound .ow-sensitive version. We have shown once by hand that if all the propagation rules are \nsound, then the result of running the analysis in .ow\u00adinsensitive mode is also sound. A proof can be \nfound in the accompanying technical report [20].  6. INTERPROCEDURAL ANALYSES Yet another bene.t of \nusing .ow functions is that we can adapt a previous .ow-function-based framework [7] from the 5 Or, if \nusing bounded-sized integers, it takes a long time. Vortex compiler [11] in order to automatically build \nprov\u00adably sound interprocedural analyses in Rhodium. The pre\u00advious Vortex framework has been used to \nwrite realistic in\u00adterprocedural analyses, such as various kinds of class anal\u00adysis [13], constant propagation, \nside-e.ect analysis, escape analysis, and various synchronization-related analyses [2]. The contribution \nof the new Rhodium framework is a rigor\u00adous formal description combined with a proof of soundness. These \nare stand-alone contributions whose applications are broader than just the Rhodium system. Our approach \nrevolves around a framework for creating a provably sound interprocedural analysis from a sound intraprocedural \nversion. The framework is parameter\u00adized by a context-sensitivity strategy that describes what context \na function should be analyzed in at a particu\u00adlar call site. The context-sensitivity strategy is embod\u00adied \nin a function selectCalleeContext . Given a call site n, the context c = Context in which the caller \nis being an\u00adalyzed, and the data.ow information d at the call site, selectCalleeContext (n, c,d) returns \nthe context for analyz\u00ading the callee at this call site. We have instantiated our framework with two \ncommonly used context-sensitivity strategies: the transfer function strategy (also known as Sharir and \nPnueli s functional approach [27]), and Shivers s k-CFA algorithm [28] (also known as the k-deep call-strings \nstrategy of Sharir and Pnueli [27]). Table 2 shows the de.nition of Context and selectCalleeContext for \nthese two strategies. The context\u00adinsensitive strategy can be achieved using 0-CFA. Our key insight is \nthat these instantiations of the frame\u00adwork can be proven sound by hand once and for all, in\u00addependent \nof any user-de.ned analysis. As a result, any interprocedural analysis generated by one of these instanti\u00adations \nis guaranteed to be sound provided the intraproce\u00addural version is. To build a provably sound interprocedural \nanalysis, the programmer writes the intraprocedural version in Rhodium, making sure that it passes all \nthe soundness checks, and then picks one of the prede.ned context sensitiv\u00adity strategies. Our framework \nthen automatically generates an interprocedural version of the analysis that is guaranteed to be sound. \nThe Rhodium framework operates by creating an inter\u00adprocedural .ow function Fi from an intraprocedural \nversion F . Due to space limitations, we only give an informal de\u00adscription of Fi here a formal description \nof the framework, accompanied by proofs of soundness, can be found in the accompanying technical report \n[20]. Instead of propagating facts d = D, the interprocedu\u00adral analysis propagates partial maps cd = \nContext .D which map a calling context c to the data.ow information d that holds in that context. For \nnodes that are not func\u00adtion calls or returns, Fi simply evaluates F pointwise on each range d element. \nFor a call node n, for each (c n d) pair .owing into the call, Fi merges (pointwise) the pair (selectCalleeContext \n(n,c, d) n d) into the map on the en\u00adtry edge of the callee s CFG, which will cause the callee to be \nfurther analyzed if the edge information changes. For a return node, for each (c ' n d ' ) pair .owing \ninto the return, for each call site n and in.owing pair (c n d) such that c ' = selectCalleeContext (n, \nc,d), the pair (c n d ' ) is merged into the map on n s successor edge. The accompanying tech\u00adnical report \n[20] describes how data.ow facts are translated from callers to callees and vice versa. Analogously to \nwidening operators as discussed in sec\u00ad Strategy Context selectCalleeContext Transfer function D selectCalleeContext \n(n, c, d) = d k-CFA list[string] selectCalleeContext (n, c, d) = last(concat(c, [fnOf (n)]), k) where: \nconcat (l1, l2 ) concatenates lists l1 and l2 fnOf (n) returns the name of the enclosing function containing \nn last(l, k) returns the sublist containing the last k elements of l (or l if l contains fewer than k \nelements) Table 2: De.nition of Context and selectCalleeContext for two common context-sensitivity strategies. \ntion 4.3, we could enrich Rhodium by allowing optimization writers to specify a context widening operator \nto control the amount of context-sensitivity. For example, after k di.erent contexts have been selected \nfor a function, all future con\u00adtexts could be widened to T, bounding the number of times the function \nis analyzed.  7. EXECUTION ENGINE Rhodium analyses and transformations are meant to be directly executable; \nthey do not have to be reimplemented in a di.erent language to be run. Using Whirlwind s frame\u00adwork for \ncomposable optimizations [18], we have imple\u00admented a forward intraprocedural execution engine for the \ncore of the Rhodium language. Rhodium optimizations in Whirlwind peacefully co-exist with Cobalt optimizations \nand with hand-written optimizations. By supporting such incremental adoption, it is possible to provide \nbene.ts to compiler-writers even if the whole optimizer is not written in Rhodium. The Rhodium execution \nengine stores at each edge in the CFG an element of D (each element of D is a set of facts), and propagates \nfacts across statements by interpreting the Rhodium rules. The engine s .ow function Fexec : Node \u00d7 D \nn D operates as follows (where R is the set of forward propagation rules that the engine is executing): \nFexec(n, d)= .rER apply rule(r, n, d) - n apply rule(if a then f( t )@out, n,d)= - n let e= sat(a,n, \nd, []) in .8E8 {f(e( t ))} The .ow function applies each rule separately and returns the union of the \nindividual results. The apply rule func\u00adtion computes all the facts propagated by a given rule. To do \nthis, apply rule .rst uses the sat function to compute all the satisfying substitutions that make the \nantecedent a hold. For each returned substitution e, apply rule adds the - n propagated fact, f(e( t \n)), to the result set. n 2Subst The sat : Pred \u00d7 Node \u00d7 D \u00d7 Subst function (where we denote by Pred the \nset of all Rhodium predi\u00adcates, and by Subst the set of all substitutions) .nds sat\u00adisfying substitutions: \ngiven a predicate a, a node n, a set of facts d, and a substitution e, sat(a, n, d, e) returns the set \nof all substitutions e ' that have the following properties: (1) e ' makes a hold at node n when d .ows \ninto n, or more formally, [a](n, d, e ' ) holds (2) e ' is an extension of e and (3) the additional \nmappings in e ' are only for free variables of a. The original call to sat passes the empty substitution \n[] for e, and in this case sat(a,n, d, []) computes the set of all substitutions over the free free variables \nof a that make a hold at node n. Here are some representative cases from the implementation of sat: \n sat (true,n, d,e)= {e} sat (false,n, d,e)= = sat (a1 . a2 ,n, d,e)= sat (a1,n, d,e) . sat(a2 , n, d, \ne) sat (a1 . a2 ,n, d,e)= let e= sat(a1,n, d,e) in .8 E8 sat(a2,n, d,e ' ) sat (t1 = t2, n,d,e)= unify(n, \nt1,t2 ,e) -n - n n s sat (f ( t )@in, n, d, e)= .f (-s )Ed unify terms(n, t, -s ,e) sat (*x.a, n, d, \ne)= sat (a,n, d,e \\ x)[x n e(x)] In the above de.nition, we use e \\ x to denote e with any mapping of \nx removed. We also use e[x n e(x)] to denote .8 E8{e ' [x n e(x)]}, where e ' [x n e(x)] stands for the \nsubstitution e ' updated so that it maps x in the same way that e does: if e maps x to a value, then \ne ' [x n e(x)] maps x to the same value, and if e does not have a mapping for x, then neither does e \n' [x n e(x)]. The sat function above makes use of a uni.cation routine: the call unify(n, t1,t2 ,e) attempts \nto unify e(t1) and e(t2). If the uni.cation fails, then the empty set is returned. If the uni.cation \nsucceeds with substitution e ' , then e ' is aug\u00admented with all the mappings from e to produce e '' \n, and the singleton set {e '' } is returned. The unify terms function works like unify, except that it \nuni.es a sequence of terms - n - n t with another sequence s . The uni.cation procedure also tries to \nevaluate terms such as applyBinaryOp(*,C2,C3 ) from .gure 2. If such a term can be evaluated, unify re\u00adplaces \nthe term with what it evaluates to, and then proceeds as usual. If such a term cannot be evaluated (because \nfor example either C2 or C3 is not bound yet), then uni.cation fails. Universal quanti.ers are handled \nby expanding them into conjunctions over the domain of the quanti.er. This expan\u00adsion is possible because \nthe domain of quanti.ed variables is .nite for any particular intermediate-language program. For existential \nquanti.ers, the sat function locally skolemizes the quanti.ed variable, and then proceeds with the body \nof the quanti.er. Any mapping of the quanti.ed variable intro\u00adduced for satisfying the body of the quanti.er \nis discarded in the resulting substitutions. 8. CURRENT AND FUTURE WORK We have so far focused our attention \nprimarily on forward analyses and transformations in Rhodium. We have im\u00adplemented a fully automated \nchecker and execution engine for Rhodium forward analyses and transformations, and we have .nished the \nhand proofs for the forward case. We are now extending our work to backward optimiza\u00adtions. We already \nhave a proof strategy for backward Rhodium analyses and transformations, but have not yet implemented \nthe checker nor completed the hand proofs. We have written in Rhodium the two backward optimiza\u00adtions \nwe had in Cobalt (dead assignment elimination and code hoisting), and simulated our proof strategy by \nhand on these optimizations. The proof obligations for these two optimizations in Rhodium end up being \nexactly the same as the proof obligations for their Cobalt counterparts. We are currently working on \ngenerating these proof obligations mechanically, and we are also in the process of .nishing the hand \nproofs for the backward case. In future work, we would like to extend our execution engine to handle \nthe full language design, including back\u00adward analyses and transformations, interprocedural and .ow-insensitive \nanalyses, pro.tability heuristics and user\u00adde.ned widenings. We also plan to explore more e.cient implementation \nstrategies for our execution engine, such as generating spe\u00adcialized code to run each optimization [30]. \nFor example, consider a rule whose antecedent is a conjunction where one of the conjuncts is stmt (X \n:= &#38;Z). We statically know that this rule will only .re on statements of the form X := &#38;Z, but \nbecause our current engine does not make use of this information, the rule is repeatedly considered on \nstatements of the wrong form. By partially evaluating the rules with respect to each statement kind, \nwe can produce a specialized set of rules that will be smaller than the whole set (because some rules \nwill not apply) and in which each rule will be simpler (because the antecedent can be simpli.ed based \non the statement kind). The generated .ow function would dispatch on the form of the statement being \nanalyzed, and would directly jump to specialized code that runs the sim\u00adpli.ed rules. Furthermore, we \nwould also like to investigate more ef\u00ad.cient representations of the data.ow information. For example, \nstoring the does-not-point-to relation using ex\u00adplicit pairs can incur a signi.cant memory overhead. \nWe would like to investigate ways of automatically converting to more space-e.cient representations, \nfor instance the in\u00adverted may-point-to relation, or a bit-vector representation of the relation. Also, \nmotivated by recent advances in the use of BDDs to represent pointer information [5, 34], we would like \nto explore ways of inferring when it would be bene.cial to use BDDs for encoding our sets of facts. Finally, \nwe want to continue on our path of pushing more and more of the burden of compiler-writing onto the com\u00adputer. \nBy automating more and more of the tedious, di.\u00adcult and error-prone parts of compiler-writing, we can \nallow the human to concentrate on the creative and interesting parts. One such direction is to automatically \ninfer propaga\u00adtion rules given only the facts and their meanings. Another direction would be to generate \nthe facts, meanings and prop\u00adagation rules for supporting a given CFG rewrite rule. 9. RELATED WORK The \nidea of analyzing optimizations written in a domain\u00adspeci.c language was introduced by Whit.eld and So.a \nwith the Gospel language [35]. The di.erences between our work and the Gospel work stem from the di.erence \nin focus: we explore soundness whereas Whit.eld and So.a explore op\u00adtimization dependencies. Many other \nframeworks and languages have been pro\u00adposed for specifying data.ow analyses and transformations, including \nSharlit [32], System-Z [36], languages based on reg\u00adular path queries [29], and temporal logic [30, 17]. \nNone of these approaches, however, addresses automated soundness checking of the speci.ed transformations. \nA signi.cant amount of work has been done on manually proving data.ow analyses and transformations correct, \nin\u00adcluding abstract interpretation [8, 9, 10], the work on the VLISP compiler [14], Kleene algebra with \ntests [16], manual proofs of correctness for optimizations expressed in temporal logic [30, 17], and \nmanual proofs of correctness based on par\u00adtial equivalence relations [4]. Analyses and transformations \nhave also been proven correct mechanically, but not auto\u00admatically: the soundness proof is performed \nwith an inter\u00adactive theorem prover that requires guidance from the user. For example, Young [37] has \nproven a code generator cor\u00adrect using the Boyer-Moore theorem prover enhanced with an interactive interface \n[15]. As another example, Cachera et. al. [6] show how to specify static analyses and prove them correct \nin constructive logic using the Coq proof assistant. Via the Curry-Howard isomorphism, an implementation \nof the static analysis algorithm can then be extracted from the proof of correctness. Aboul-Hosn and \nKozen present KAT-ML [1], an interactive theorem prover for Kleene Algebra with Tests, which can be used \nto interactively prove prop\u00aderties of programs. In all these cases, however, the proof requires help \nfrom the user. In contrast, Rhodium s proof strategy is fully automated. Instead of proving that the \ncompiler is always correct, translation validation [25, 23] and credible compilation [26] both attack \nthe problem of checking the correctness of a given compilation run. Therefore, a bug in an optimiza\u00adtion \nonly appears when the compiler is run on a program that triggers the bug. Our work allows optimizations \nto be proven correct before the compiler is even run once. How\u00adever, to do so we require optimizations \nto be written in a special-purpose language. Our approach also requires the Rhodium execution engine \nto be part of the trusted com\u00adputing base, while translation validation and credible com\u00adpilation do \nnot require trust in any part of the optimizer. Proof-carrying code [22], certi.ed compilation [24], \ntyped intermediate languages [31], and typed assembly lan\u00adguages [21] have all been used to prove properties \nof pro\u00adgrams generated by a compiler. However, the kinds of prop\u00aderties that these approaches have typically \nguaranteed are type safety and memory safety. In our work, we prove the stronger property of semantic \nequivalence between the orig\u00adinal and resulting programs. 10. CONCLUSION We presented a new language \ncalled Rhodium for express\u00ading data.ow analyses and transformations that is signi.\u00adcantly more expressive \nthan previous work while retaining automated soundness checking. The key to Rhodium s ex\u00adpressiveness \nlies in its use of local propagation rules, which can be used by programmers to implement .ow functions \nthat are checked automatically for soundness, and from which can be derived .ow-insensitive, .ow-sensitive, \nand in\u00adterprocedural analyses. Acknowledgments This research is supported in part by NSF grants CCR\u00ad0073379, \nACI-0203908 and ITR-0326590, a Microsoft Grad\u00aduate Fellowship, an IBM Faculty Development Award, and \nby gifts from Sun Microsystems. We would also like to thank Dan Grossman, Alexandru S.alcianu, and the \nanonymous re\u00adviewers for their useful suggestions on how to improve the paper. 11. REFERENCES [1] Kamal \nAboul-Hosn and Dexter Kozen. KAT-ML: An interactive theorem prover for kleene algebra with tests. In \nProceedings of the 4th International Workshop on the Implementation of Log\u00adics (WIL 03), University of \nManchester, September 2003. [2] Jonathan Aldrich, Craig Chambers, Emin Gun Sirer, and Susan Eggers. Static \nanalyses for eliminating unnecessary synchroniza\u00adtion from java programs. In Proceedings of the sixth \nInterna\u00adtional Static Analysis Symposium, pages 19 38, Venice Italy, 1999. [3] L. O. Andersen. Program \nAnalysis and Specialization for the C Programming Languge. PhD thesis, DIKU, University of Copen\u00adhagen, \nMay 1994 (available as DIKU technical report 94-19). [4] Nick Benton. Simple relational correctness proofs \nfor static anal\u00adyses and and program transformations. In Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Languages, Venice Itally, January 2004. [5] Marc Berndl, Ond.rej Lhot\u00b4ak, \nFeng Qian, Laurie Hendren, and Navindra Umanee. Points-to analysis using BDDs. In Proceed\u00adings of the \nACM SIGPLAN 2003 Conference on Programming Language Design and Implementation, pages 103 114. ACM Press, \n2003. [6] David Cachera, Thomas Jensen, David Pichardie, and Vlad Rusu. Extracting a data .ow analyser \nin constructive logic. In Proceedings of the 13th European Symposium on Programming (ESOP 2004), volume \n2986 of Lecture Notes in Computer Sci\u00adence. Springer-Verlag, 2004. [7] Craig Chambers, Je.rey Dean, and \nDavid Grove. Frameworks for intra-and interprocedural data.ow analysis. Technical Report UW-CSE-96-11-02, \nUniversity of Washington, November 1996. [8] Patrick Cousot and Radhia Cousot. Abstract interpretation: \nA uni.ed lattice model for static analysis of programs by construc\u00adtion or approximation of .xpoints. \nIn Proceedings of the Fourth ACM Symposium on Principles of Programming Languages, pages 238 252, Los \nAngeles CA, January 1977. [9] Patrick Cousot and Radhia Cousot. Systematic design of pro\u00adgram analysis \nframeworks. In Proceedings of the Sixth ACM Symposium on Principles of Programming Languages, pages 269 \n282, San Antonio TX, January 1979. [10] Patrick Cousot and Radhia Cousot. Systematic design of pro\u00adgram \ntransformation frameworks by abstract interpretation. In Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium \non Principles of Programming Languages, Portland OR, Jan\u00aduary 2002. [11] Je.rey Dean, Greg DeFouw, Dave \nGrove, Vassily Litvinov, and Craig Chambers. Vortex: An optimizing compiler for object\u00adoriented languages. \nIn Proceedings of the 1996 ACM Confer\u00adence on Object-Oriented Programming Systems, Languages, and Applications, \npages 83 100, San Jose CA, October 1996. [12] Cormac Flanagan, K. Rustan M. Leino, Mark Lillibridge, \nGreg Nelson, James B. Saxe, and Raymie Stata. Extended static checking for Java. In Proceedings of the \nACM SIGPLAN 02 Conference on Programming Language Design and Implemen\u00adtation, June 2002. [13] David Grove \nand Craig Chambers. A framework for call graph construction algorithms. ACM Transactions on Programming \nLanguages and Systems, 23(6):685 746, 2001. [14] J. Guttman, J. Ramsdell, and M. Wand. VLISP: a veri.ed \nim\u00adplementation of Scheme. Lisp and Symbolic Compucation, 8(1\u00ad2):33 110, 1995. [15] M. Kau.mann and R.S. \nBoyer. The Boyer-Moore theorem prover and its interactive enhancement. Computers and Mathematics with \nApplications, 29(2):27 62, 1995. [16] Dexter Kozen. Kleene algebra with tests. ACM Transactions on Programming \nLangauges and Systems, 19(3):427 443, Septem\u00adber 1997. [17] David Lacey, Neil D. Jones, Eric Van Wyk, \nand Carl Christian Frederiksen. Proving correctness of compiler optimizations by temporal logic. In Proceedings \nof the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan\u00adguages, Portland OR, January \n2002. [18] Sorin Lerner, David Grove, and Craig Chambers. Composing data.ow analyses and transformations. \nIn Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, Portland \nOR, January 2002. [19] Sorin Lerner, Todd Millstein, and Craig Chambers. Automati\u00adcally proving the correctness \nof compiler optimizations. In Pro\u00ad ceedings of the ACM SIGPLAN 2003 conference on Program\u00adming language \ndesign and implementation, pages 220 231. ACM Press, 2003. [20] Sorin Lerner, Todd Millstein, Erika Rice, \nand Craig Chambers. Automated soundness proofs for data.ow analyses and transfor\u00admations via local rules. \nTechnical Report UW-CSE-2004-07-04, University of Washington, July 2004. [21] Greg Morrisett, Karl Crary, \nNeal Glew, Dan Grossman, Richard Samuels, Frederick Smith, David Walker, Stephanie Weirich, and Steve \nZdancewic. TALx86: A realistic typed assembly language. In 1999 ACM SIGPLAN Workshop on Compiler Support \nfor System Software, pages 25 35, Atlanta GA, May 1999. [22] George C. Necula. Proof-carrying code. In \nProceedings of the 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, Paris, France, \nJanuary 1997. [23] George C. Necula. Translation validation for an optimizing com\u00adpiler. In Proceedings \nof the ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 83 95, Vancouver, \nCanada, June 2000. [24] George C. Necula and Peter Lee. The design and implementation of a certifying \ncompiler. In Proceedings of the ACM SIGPLAN 98 Conference on Programming Language Design and Imple\u00admentation, \nMontreal, Canada, June 1998. [25] A. Pnueli, M. Siegel, and E. Singerman. Translation validation. In \nTools and Algorithms for Construction and Analysis of Sys\u00adtems, TACAS 98, volume 1384 of Lecture Notes \nin Computer Science, pages 151 166, 1998. [26] Martin Rinard and Darko Marinov. Credible compilation. \nIn Proceedings of the FLoC Workshop Run-Time Result Veri.\u00adcation, July 1999. [27] Micha Sharir and Amir \nPnueli. Two approaches to interproce\u00addural data .ow analysis. In Steven S. Muchnick and Neil D. Jones, \neditors, Program Flow Analysis: Theory and Applica\u00adtions, chapter 7, pages 189 233. Prentice-hall, 1981. \n[28] Olin Shivers. Control-.ow analysis in Scheme. In Proceedings of the SIGPLAN 88 Conference on Programming \nLanguage Design and Implementation, pages 164 174, Atlanta GA, June 1988. [29] Ganesh Sittampalam, Oege \nde Moor, and Ken Friis Larsen. In\u00adcremental execution of transformation speci.cations. In Pro\u00adceedings \nof the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, Venice Italy, January \n2004. [30] Bernhard Ste.en. Data .ow analysis as model checking. In T. Ito and A.R. Meyer, editors, Theoretical \nAspects of Computer Sci\u00adence (TACS), Sendai (Japan), volume 526 of Lecture Notes in Computer Science \n(LNCS), pages 346 364. Springer-Verlag, September 1991. [31] David Tarditi, Greg Morrisett, Perry Cheng, \nChris Stone, Robert Harper, and Peter Lee. TIL: A type-directed optimizing com\u00adpiler for ML. In Proceedings \nof the ACM SIGPLAN 96 Confer\u00adence on Programming Language Design and Implementation, Philadelphia PA, \nMay 1996. [32] Steven W. K. Tjiang and John L. Hennessy. Sharlit a tool for building optimizers. In \nProceedings of the 5th ACM SIGPLAN Conference on Programming Language Design and Implemen\u00adtation, pages \n82 93, July 1992. [33] Je.rey D. Ullman. Principles of Database and Knowledge-base Systems, Volume I. \nComputer Science Press, 1988. [34] John Whaley and Monica S. Lam. Cloning-based context\u00adsensitive pointer \nalias analysis using binary decision diagrams. In Proceedings of the Conference on Programming Language \nDesign and Implementation. ACM Press, June 2004. [35] Deborah L. Whit.eld and Mary Lou So.a. An approach \nfor exploring code improving transformations. ACM Transactions on Programming Languages and Systems, \n19(6):1053 1084, November 1997. [36] Kwangkeun Yi and Williams Ludwell Harrison III. Automatic generation \nand management of interprocedural program analy\u00adses. In Proceedings of the Twentieth ACM SIGPLAN-SIGACT \nSymposium on Principles of Programming Languages, pages 246 259, January 1993. [37] William D. Young. \nA mechanically veri.ed code generator. Jour\u00adnal of Automated Reasoning, 5(4):493 518, December 1989. \n  \n\t\t\t", "proc_id": "1040305", "abstract": "We present Rhodium, a new language for writing compiler optimizations that can be automatically proved sound. Unlike our previous work on Cobalt, Rhodium expresses optimizations using explicit dataflow facts manipulated by local propagation and transformation rules. This new style allows Rhodium optimizations to be mutually recursively defined, to be automatically composed, to be interpreted in both flow-sensitive and -insensitive ways, and to be applied interprocedurally given a separate context-sensitivity strategy, all while retaining soundness. Rhodium also supports infinite analysis domains while guaranteeing termination of analysis. We have implemented a soundness checker for Rhodium and have specified and automatically proven the soundness of all of Cobalt's optimizations plus a variety of optimizations not expressible in Cobalt, including Andersen's points-to analysis, arithmetic-invariant detection, loop-induction-variable strength reduction, and redundant array load elimination.", "authors": [{"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "University of Washington", "person_id": "PP43119616", "email_address": "", "orcid_id": ""}, {"name": "Todd Millstein", "author_profile_id": "81100018064", "affiliation": "UCLA", "person_id": "PP14019523", "email_address": "", "orcid_id": ""}, {"name": "Erika Rice", "author_profile_id": "81100249884", "affiliation": "University of Washington", "person_id": "P707738", "email_address": "", "orcid_id": ""}, {"name": "Craig Chambers", "author_profile_id": "81100528252", "affiliation": "University of Washington", "person_id": "PP39047060", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1040305.1040335", "year": "2005", "article_id": "1040335", "conference": "POPL", "title": "Automated soundness proofs for dataflow analyses and transformations via local rules", "url": "http://dl.acm.org/citation.cfm?id=1040335"}