{"article_publication_date": "01-12-2005", "fulltext": "\n Dynamic Partial-Order Reduction for Model Checking Software Cormac Flanagan Patrice Godefroid University \nof California at Santa Cruz Bell Laboratories, Lucent Technologies cormac@cs.ucsc.edu god@bell-labs.com \nABSTRACT We present a new approach to partial-order reduction for model checking software. This approach \nis based on ini\u00adtially exploring an arbitrary interleaving of the various con\u00adcurrent processes/threads, \nand dynamically tracking inter\u00adactions between these to identify backtracking points where alternative \npaths in the state space need to be explored. We present examples of multi-threaded programs where our \nnew dynamic partial-order reduction technique signi.cantly re\u00adduces the search space, even though traditional \npartial-order algorithms are helpless.  Categories and Subject Descriptors D.2.4 [Software Engineering]: \nSoftware/Program Veri.\u00adcation; F.3.1 [Logics and Meanings of Programs]: Spec\u00adifying and Verifying and \nReasoning about Programs General Terms Algorithms, Veri.cation, Reliability  Keywords Partial-order \nreduction, software model checking 1. INTRODUCTION Over the last few years, we have seen the birth of \nthe .rst software model checkers for programming languages such as C, C++ and Java. Roughly speaking, \ntwo broad approaches have emerged. The .rst approach consists of automatically extracting a model out \nof a software application by statically analyzing its code and abstracting away details, applying traditional \nmodel checking to analyze this abstract model, and then mapping abstract counter-examples back to the \ncode or re.ning the abstraction (e.g., [1, 14, 4]). The sec\u00adond approach consists of systematically exploring \nthe state space of a concurrent software system by driving its exe\u00adcutions via a run-time scheduler (e.g., \n[10, 27, 5]). Both of these approaches to software model checking have their advantages and limitations \n(e.g., [11]). Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n05, January 12 14, 2005, Long Beach, California, USA. Copyright 2005 ACM 1-58113-830-X/05/0001 ...$5.00. \nIn the context of the second approach, partial-order re\u00adduction seems (so far) to be the most e.ective \ntechnique for reducing the size of the state space of concurrent soft\u00adware systems at the implementation \nlevel. Two main core partial-order reduction techniques are usually considered: persistent/stubborn sets \nand sleep sets. In a nutshell, the persistent/stubborn set technique [25, 9] computes a provably-su.cient \nsubset of the set of enabled transitions in each visited state such that unselected enabled transitions \nare guaranteed not to interfere with the execu\u00adtion of those being selected. The selected subset is called \na persistent set, while the most advanced algorithms for com\u00adputing such sets are based on the notion \nof stubborn sets [25, 9]. These algorithms exploit information about which oper\u00adations on which communication \nobjects each process might execute in the future . This information is typically ob\u00adtained from a static \nanalysis of the code. Minimally, such a static analysis can simply attempt to identify objects acces\u00adsible \nby a single process only, and then classify operations on such objects as local (e.g., [5]). In contrast, \nthe sleep set technique (see [9]) exploits infor\u00admation on dependencies exclusively among the transitions \nenabled in the current state, as well as information recorded about the past of the search. Both techniques \ncan be used simultaneously and are complementary [9]. In the presence of cycles in the state space, these \ntech\u00adniques must be combined with additional conditions to make sure the transition selection is fair \nwith respect to all pro\u00adcesses in order to verify properties more elaborate than dead\u00adlock detection, \nsuch as checking arbitrary safety and liveness properties. For instance, an ample set (see [2]) is a \nper\u00adsistent set that satis.es additional conditions su.cient for LTL model checking. Techniques for dealing \nwith cycles are mostly orthogonal to the two core techniques mentioned above, which are su.cient for \ndetecting deadlocks. In what follows, we will assume that the state spaces we consider do not contain \nany cycles, and focus the discussion on detecting deadlocks and safety-property violations such as assertion \nfailures (e.g., speci.ed with assert() in C). Note that acyclic state spaces are quite common in the \ncon\u00adtext of model checking of software implementations: the ex\u00adecution of most software applications \neventually terminates, either because the application is input driven and reacts to external events speci.ed \nin a test driver encoding only .\u00adnite sequences of inputs, or because the execution length is bounded \nat run/test time and hence forced to terminate. Unfortunately, existing persistent/stubborn set techniques \nsu.er from a severe fundamental limitation: in the context Figure 1: Indexer Program. Thread-global (shared) \nvariables: const int size = 128; const int max = 4; int[size] table; Thread-local variables: intm =0, \nw, h; Code for thread tid: while (true) { w := getmsg(); h := hash(w); while (cas(table[h],0,w) == false) \n{ h := (h+1) % size; } } int getmsg() { if (m< max ) { return (++m) * 11 + tid; }else { exit(); // terminate \n}} int hash(int w) { return (w * 7) % size; } of concurrent software systems executing arbitrary C, C++ \nor Java code, determining which operations on which com\u00admunication objects each process might execute \nin the fu\u00adture with acceptable precision is often di.cult or impossi\u00adble to compute precisely. If this \ninformation is too imprecise, persistent/stubborn sets techniques cannot prune the state space very e.ectively. \nSleep sets can still be used, but used alone, they can only reduce the number of explored tran\u00adsitions, \nnot the number of explored states [9], and hence cannot avoid state explosion. To illustrate the nature \nof this problem, consider the pro\u00adgram Indexer shown in Figure 1, where multiple concur\u00adrent threads \nmanipulate a shared hash table. Each thread has a a thread identi.er tid .{1,...,n}and receives a number \nof incoming messages w and inserts each message into the hash table at corresponding index h=hash(w).Ifa \nhash table collision occurs, the next free entry in the table is used. All hash table entries are initially \n0. The atomic compare-and-swap instruction cas(table[h],0,w) checks if table[h] is initially 0; if so, \nthen it updates table[h] to w and returns true, and otherwise returns false (without changing table[h]). \nA static alias analysis for determining when two di.erent threads can access the same memory location \nwould need to know all the possible messages received by all the threads as well as predict the hash \nvalues computed for all such messages, for all execution paths. Since this is clearly not realistic, \nstatic analyses conservatively assume that every access to the hash table may access the same entry. \nThe latter is equivalent to treating the entire hash table as a single shared variable, and prevents \npartial-order reduction techniques from signi.cantly pruning the state space of this program. Instead, \nall possible interleavings of accesses to the hash table are still explored, resulting in state explosion \nand making model checking intractable for all but a small number of threads. We propose in this paper \na new approach to partial-order reduction that avoids the inherent imprecisions of static alias analyses. \nOur new algorithm starts by executing the program until completion, resolving nondeterminism com\u00adpletely \narbitrarily, and it dynamically collects information about how threads have communicated during this \nspeci.c execution trace, such as which shared memory locations were read or written by which threads \nand in what order. This data is then analyzed to add backtracking points along the trace that identify \nalternative transitions that need to be explored because they might lead to other execution traces that \nare not equivalent to the current one (i.e., are not linearizations of the same partial-order execution). \nThe pro\u00adcedure is repeated until all alternative executions have been explored and no new backtracking \npoints need be added. When the search stops, all deadlocks and assertion failures of the system are guaranteed \nto have been detected. For the Indexer example, if it is detected dynamically during the .rst execution \ntrace of the program that the var\u00adious threads access disjoint memory location, then no back\u00adtracking \npoints are added along that trace, and the reduced state space with our dynamic partial-order reduction \nis a single path. This turns out to be the case for this program with up to 11 threads, as we will show \nin Section 5. The paper is organized as follows. After some background de.nitions, we present in Section \n3 a general dynamic partial\u00adorder reduction algorithm for detecting deadlocks in acyclic state spaces. \nIn section 4, we discuss how to optimize this algorithm to the case of multithreaded programs. In Sec\u00adtion \n5, we present preliminary experimental results on some small examples. Section 6 discusses other related \nwork, and we conclude with Section 7. 2. BACKGROUND DEFINITIONS 2.1 Concurrent Software Systems We consider \na concurrent system composed of a .nite set Pof threads or processes, and de.ne its state space using \na dynamic semantics in the style of [10]. Each process exe\u00adcutes a sequence of operations described in \na deterministic sequential program written in a language such as C, C++ or Java. The processes communicate \nby performing atomic op\u00aderations on communication objects, such as shared variables, semaphores, locks, \nand FIFO bu.ers. In what follows, pro\u00adcesses that share the same heap are called threads. Threads are \nthus a particular type of processes. Unless otherwise speci.ed, the algorithms discussed in this paper \napply to both processes and threads. Operations on communication objects are called visible operations, \nwhile other operations are invisible. The execu\u00adtion of an operation is said to block if it cannot currently \nbe completed; for instance, an operation acquire(l) may block until the lock is released by an operation \nrelease(l) . We assume that only executions of visible operations may block. A state of the concurrent \nsystem consists of the local state LocalState of each process, and of the shared state SharedState of \nall communication objects: State = SharedState \u00d7LocalStates LocalStates = P.LocalState For ls .LocalStates,wewrite \nls[p := l]to denotethe map that is identical to ls except that it maps p to local state l. A transition \nmoves the system from one state to a subse\u00adquent state, by performing one visible operation of a chosen \nprocess, followed by a .nite sequence of invisible operations of the same process, ending just before \nthe next visible op\u00aderation of that process. The transition tp,l of process p for local state l .LocalState \nis de.ned via a partial function: tp,l : SharedState -LocalState \u00d7SharedState Let T denote the set of \nall transitions of the system. A transition tp,l .T is enabled in a state s = (g, ls)(where g .SharedState \nand ls .LocalStates)if l = ls(p)and tp,l(g) ' is de.ned. If t is enabled in s and tp,l(g)= (g,l'),thenwe \nsay the execution of t from s produces a unique1 successor tw '''' state s= (g, ls[p := l')] , written \ns .s.We write s .sto mean that the execution of the .nite sequence w .T * ' leads from s to s. We de.ne \nthe behavior of the concurrent system as a tran\u00adsition system AG =(State, .,s0), where . .State \u00d7State \nis the transition relation de.ned by 't' (s, s) ..i. .t .T : s .s and s0 is the initial state of the \nsystem. In any given state s = (g, ls),let next(s, p)= tp,ls(p) de\u00adnote the (unique) next transition \nto be executed by process p. For any transition tp,l,let proc(tp,l)= p denote the pro\u00adcess executing \nthe transition (we thus assume all processes have disjoint sets of transitions). A state in which no \ntran\u00adsition is enabled is called a deadlock,or a terminating state. The state transformation resulting \nfrom the execution of a transition may vary depending on the current state. For instance, if the next \nvisible operation of thread p in a state s is read(x) in the program: {if (read(x)) then i=0 else i=2}; \nwrite(x); where x is a shared variable and i is a local variable, the invisible operation(s) following \nread(x) will depend on the value of x. However, the transition t = next(s, p) is still unique and can \nthus be viewed as the entire block between the braces. Note that next(s, p) does not change even if other \nprocesses execute other transitions changing the value w '' of x from s: for all ssuch that s .swhere \nw does not con\u00ad ' tain any transition from p,wehave next(s,p)= next(s, p). Consider again the Indexer \nexample of Figure 1. Its state space AG (for two threads and max=1) is shown in Figure 2. Transitions \nin AG are labeled with the visible operation of the corresponding thread transition being executed. Nonde\u00adterminism \n(branching) in AG is caused only by concurrency. This state space contains a single terminating state \n(where both threads are blocked on their exit() statement) since the two threads access distinct hash \ntable entries. Observe how the above de.nition of state space collapses purely-local computations into \nsingle transitions, by com\u00adbining invisible operations with the last visible one. This 1To simplify the \npresentation, we do not consider operations that are nondeterministic [10] or that create dynamically \nnew processes, although both features are compatible with the algorithms and techniques discussed in \nthe paper. Figure 2: State space for Indexer example for two threads T 1 and T 2 and max=1. T1: cas(table[84],0,12) \n T2: cas(table[91],0,13) s0 terminating state T2: cas(table[91],0,13) T1: cas(table[84],0,12) de.nition \navoids including the (unnecessary) interleavings of invisible operations as part of the state space (hence \nal\u00adready reducing state explosion), while still being provably su.cient for detecting deadlocks and assertion \nviolations as shown in [10]. Also, a model checker for exploring such state spaces needs only control \nand observe the execution of visi\u00adble operations, as is done in the tool VeriSoft [10, 11]. 2.2 De.nitions \nfor Partial-Order Reduction We brie.y recall some basic principles of partial-order re\u00adduction methods. \nThe basic observation exploited by these techniques is that AG typically contains many paths that correspond \nsimply to di.erent execution orders of the same uninteracting transitions. When concurrent transitions \nare independent , meaning that their execution does not in\u00adterfere with each other, changing their order \nof execution will not modify their combined e.ect. This notion of inde\u00adpendency between transitions and \nits complementary notion, the notion of dependency, can be formalized by the following de.nition (adapted \nfrom [15]). Definition 1. Let T be the set of transitions of a con\u00adcurrent system and D .T \u00d7T be a binary, \nre.exive, and symmetric relation. The relation D is a valid dependency relation for the system i. for \nall t1,t2 .T, (t1,t2). D (t1 and t2 are independent) implies that the two following properties hold for \nall states s in the state space AG of the system: t1' 1. if t1 is enabled in s and s .s,then t2 is enabled \nin ' s i. t2 is enabled in s;and 2. if t1 and t2 are enabled in s, then there is a unique t1t2t2t1' state \ns' such that s . s' and s . s. Thus, independent transitions can neither disable nor en\u00adable each other, \nand enabled independent transitions com\u00admute. This de.nition characterizes the properties of possible \nvalid dependency relations for the transitions of a given system. In practice, it is possible to give \neasily-checkable conditions that are su.cient for transitions to be indepen\u00addent (see [9]). Dependency \ncan arise between transitions of di.erent processes that perform visible operations on the same shared \nobject. For instance, two acquire operations on the same lock are dependent, and so are two write op\u00aderations \non the same variable; in contrast, two read opera\u00adtions on the same variable are independent, and so \nare two write or compare-and-swap operations on di.erent variables (such as cas(table[84],0,12) and cas(table[91],0,13) \nin the program of Figure 1). To simplify the presentation, we assume in this paper that the dependency \nrelation is not conditional [15, 9] and that all the transitions of a particular process are dependent. \nTraditional partial-order algorithms operate as classical state-space searches except that, at each state \ns reached during the search, they compute a subset T of the set of transitions enabled at s, and explore \nonly the transitions in T. Such a search is called a selective search and may explore only a subset of \nAG. Two main techniques for com\u00adputing such sets T have been proposed in the literature: the persistent/stubborn \nset and sleep set techniques. The .rst technique actually corresponds to a whole family of al\u00adgorithms \n[25, 12, 13, 20], which can be shown to compute persistent sets [9]. Intuitively, a subset T of the set \nof tran\u00adsitions enabled in a state s of AG is called persistent in s if whatever one does from s, while \nremaining outside of T,does notinteractwith T. Formally, we have the following [12]. Definition 2. Aset \nT .T of transitions enabled in a state s is persistent in s i., for all nonempty sequences of transitions \nt1t2tn-1tn s1 .s2 .s3 ... . sn .sn+1 from s in AG and including only transitions ti .T, 1 =i= n, tn is \nindependent with all the transitions in T. It is beyond the scope of this paper to review stubborn-set\u00adlike \nalgorithms for computing persistent sets. In a nutshell, these algorithms can exploit information on \nthe static struc\u00adture of the system being veri.ed, such as from its current local state, process x could \nperform operation y on shared object z in some of its executions , and inferred (approxi\u00admated) by a \nstatic analysis of the system code. For instance, see [9] for several such algorithms and a comparison \nof their complexity. The new partial-order reduction algorithm introduced in the next section also explores \nat each visited state sthe tran\u00adsitions in a persistent set in s. But unlike previously known algorithms, \nthese persistent sets are computed dynamically. Before presenting this new algorithm, we brie.y recall \nsome properties of persistent sets that will be used later. First, a selective search of AG using persistent \nsets is guar\u00adanteed to visit all the deadlocks in AG (see Theorem 4.3 in [9]). Moreover, if AG is acyclic, \na selective search using persistent sets is also guaranteed to visit all the reachable local states of \nevery process in the system (see Theorem 6.14 in [9]), and hence can be used to detect violations of \nany property reducible to local state reachability, including violations of local assertions and of safety \nproperties.  3. DYNAMIC PARTIAL-ORDER REDUCTION We present in this section a new partial-order reduction \nalgorithm that dynamically tracks interactions between pro\u00adcesses and then exploits this information \nto identify back\u00adtracking points where alternative paths in the state space AG need to be explored. The \nalgorithm is based on a tradi\u00adtional depth-.rst search in the (reduced) state space of the system. The \nalgorithm maintains the traditional depth-.rst search stack as a transition sequence executed from the \ninitial state s0 of AG. Speci.cally, a transition sequence S .T * is a (.nite) sequence of transitions \nt1t2 ...tn where there exist states s1,...,sn+1 such that s1 is the initial state s0 and t1tn s1 .s2 \n... .sn+1 Given a transition sequence S, we use the following notation: Si refers to transition ti; \n S.t denotes extending S with an additional transition t;  dom(S) means the set {1,...,n};  pre(S,i)for \ni .dom(S) refers to state si;and  last(S) refers to sn+1.  A transition t.T can appear multiple times \nin a transition sequence S.We write ti = tj to denote that transitions ti and tj are occurrences of a \nsame transition in T. We say a transition t1 may be co-enabled with a transition t2 if there may exist \nsome state in which both t1 and t2 are both enabled. For example, an acquire and release on the same \nlock are never co-enabled, but two write operations on the same variable may be co-enabled. If two adjacent \ntransitions in a transition sequence are independent, then they can be swapped without changing the overall \nbehavior of the transition sequence. A transi\u00adtion sequence thus represents an equivalence class of sim\u00adilar \nsequences that can be obtained by swapping adjacent independent transitions. To help reason about the \nequiva\u00adlence class represented by a particular transition sequence, we maintain a happens-before ordering \nrelation on these transitions. The happens-before relation .S for a transition sequence S = t1 ...tn \nis the smallest relation on {1,...,n}such that 1. if i =j and Si is dependent with Sj then i.S j; 2. \n.S is transitively closed.  By construction, the happens-before relation .S is a partial\u00adorder relation, \noften called a Mazurkiewicz s trace [18, 9], and the sequence of transitions in S is one of the lineariza\u00adtions \nof this partial order. Other linearizations of this par\u00adtial order yield equivalent transition sequences \nthat can be obtained by swapping adjacent independent transitions. We also use a variant of the happens-before \nrelation to identify backtracking points. Speci.cally, the relation i .S p holds for i.dom(S) and process \np if either 1. proc(Si)= p or 2. there exists k .{i+1,...,n}such that i .S k and proc(Sk)= p.  Intuitively, \nif i .S p, then the next transition of process p from the state last(S) is not the next transition of \nprocess p in the state right before transition Si in either this tran\u00adsition sequence or in any equivalent \nsequence obtained by swapping adjacent independent transitions. The new partial-order reduction algorithm \nis presented in Figure 3. In addition to maintaining the current transition sequence or search stack \nS, each state sin the stack S is also associated with a backtracking set , denoted backtrack(s), which \nrepresents processes with a transition enabled in s that needs to be explored from s. Whenever a new \nstate s is reached during the search, the procedure Explore is called with the stack S with which the \nstate is reached. Initially (line 0), the procedure Explore is called with the empty stack as argument. \nIn line 2, last(S) Figure 3: Dynamic Partial-Order Reduction Algorithm. 0 Initially: Explore(\u00d8); 1 \nExplore(S) { 2 let s = last(S); 3 for all processes p { 4 if .i = max({i . dom(S) | Si is dependent and \nmay be co-enabled with next(s, p)and i .S p}) { 5 let E = {q . enabled(pre(S, i)) | q = p or .j . dom(S): \nj>i and q = proc(Sj)and j .S p}; 6 if(E = \u00d8) then add any q . E to backtrack(pre(S, i)); 7 else add all \nq . enabled(pre(S, i)) to backtrack(pre(S, i)); 8 9 10 11 12 }}if (.p . enabled(s)) {backtrack(s):= \n{p}; let done = \u00d8; 13 14 while (. p . (backtrack(s) \\ done)) {add p to done; 15 Explore(S.next(s, p)); \n16 17 18 } } } is the state reached by executing S from the initial state s0. Then, for all processes \np, the next transition next(s, p) of each process p in state s is considered (line 3). For each such \ntransition next(s, p) (which may be enabled or disabled in s), one then computes (line 4) the last transition \ni in S such that Si and next(s, p) are dependent (cf. De.nition 1) and may be co-enabled, and such that \ni .S p. If there exists such a transition i, there might be a race condition or dependency between i \nand next(s, p), and hence we might need to introduce a backtracking point in the state pre(S, i), i.e., \nin the state just before executing the transition i. 2 This is determined in line 5 by computing the \nset E of processes q with an enabled transition in pre(S, i) that happens-before next(s, p) in the current \npartial or\u00adder .S .Intuitively, if E is nonempty, the execution of all the processes in E is necessary \n(although perhaps not suf\u00ad.cient) to reach transition next(s, p) and to make it enable in the current \npartial order .S; in that case, it is therefore su.cient to add any single one of the processes in E \nto the backtracking set associated with pre(S, i). In contrast, if E is empty, the algorithm was not \nable to identify a process whose execution is necessary for next(s, p)to becomeen\u00adabled from pre(S, i); \nby default (line 7), the algorithm then adds all enabled processes in backtrack(pre(S, i)). Once the \ncomputation of possibly new backtracking points of lines 3 9 is completed, the search can proceed from \nthe current state s. If there are enabled processes in s (line 10), any one of those is selected to be \nexplored by being added to the backtracking set of s (line 11). As long as there are enabled processes \nin the backtracking set associated with the current state s that have not been explored yet, those processes \nwill be executed one by one by the code of lines 13 15. When all the processes in backtrack(s) have been \n2Only the last transition i in S satisfying the constraints of line 4 needs be considered: if there are \nother such transitions j<i before i in S that require adding other backtracking points, these are added \nlater through the recursion of the algorithm. explored this way, the search from s is over and state \ns is said to be backtracked . Note that the algorithm of Figure 3 is stateless [10]: it does not store \npreviously visited states in memory since e.ciently computing a canonical representation for states of \nlarge con\u00adcurrent (possibly distributed) software applications is prob\u00adlematic and prohibitively expensive. \nBacktracking can be performed without storing visited states in memory, for in\u00adstance by re-executing \nthe program from its initial state, or forking new processes at each backtracking point, or stor\u00ading \nonly backtracking states using checkpointing techniques, or a combination of these [11]. To illustrate \nthe recursive nature of our dynamic par\u00adtial order reduction algorithm, consider two concurrent pro\u00adcesses \np1 and p2 sharing two variables x and y, and executing the two programs: p1: x=1; x=2; p2: y=1; x=3; \nAssume the .rst (arbitrary and maximal) execution of this concurrent program is: p1:x=1; p1:x=2; p2:y=1; \np2:x=3; Before executing the last transition p2:x=3 of process p2,the algorithm will add a backtracking \npoint for process p2 just before the last transition of process p1 that is dependent with it (the transition \np1:x=2), forcing the subsequent exploration of: p1:x=1; p2:y=1; p2:x=3; p1:x=2; Similarly, before executing \nthe transition p2:x=3 in that sec\u00adond sequence, the algorithm will add a backtracking point for process \np2 just before p1:x=1, which in turn will force the exploration of: p2:y=1; p2:x=3; p1:x=1; p1:x=2; Note \nthat the two possible terminating states (deadlocks) and three possible partial-order executions of this \nconcurrent program are eventually explored. This example illustrates why it is su.cient to consider only \nthe last transition in S in line 4 of our algorithm of Figure 3. The correctness of the above algorithm \nis established via the following theorem. Theorem 1. Whenever a state s is backtracked during the search \nperformed by the algorithm of Figure 3 in an acyclic state space, the set T of transitions that have \nbeen explored from s is a persistent set in s. Proof: See Appendix. Since the algorithm of Figure 3 \nexplores a persistent set in every visited state, it is guaranteed to detect every deadlock and safety-property \nviolation in any acyclic state space (see Section 2). The complexity of the algorithm depends on how \nthe happens-before relation .S is implemented and is discussed further in the next section. Theorem 1 \nspeci.es the type of reduction performed by our new algorithm, as well as its complementarity and com\u00adpatibility \nwith other partial-order reduction techniques. In particular, any algorithm for computing statically \npersis\u00adtent sets, such as stubborn-set-like algorithms, can be used in conjunction of the algorithm in \nFigure 3: in lines 5 and 7, replace enabled(pre(S, i)) by PersistentSet(pre(S, i))), and in line 10, \nreplace enabled(s)by PersistentSet(s), where the function PersistentSet(s) computes statically a per\u00adsistent \nset T in state s and returns {proc(t) |t .T }.These modi.cations will restrict the search space to transitions \ncontained in the statically-computed persistent sets for each visited state s, while using dynamic partial-order \nreduction to further re.ne these statically-computed persistent sets. Moreover, sleep sets can also be \nused in conjunction with the new dynamic technique, combined or not with statically\u00adcomputed persistent \nsets. In our context (i.e., for acyclic state spaces), sleep sets can be added exactly as described in \n[10]. The known bene.ts and limitations of sleep sets compared to persistent sets remain unchanged: used \nalone, they can only reduce the number of explored transitions, but used in conjunction with (dynamic \nor static) persistent set techniques, they can further reduce the number of states as well [9].3 We conclude \nthis section by brie.y discussing two opti\u00admizations of the algorithm of Figure 3. 1. Since any process \nq in E can be added to the set backtrack(pre(S, i)) in line 6, it is clearly preferable to pick a process \nq that is already in backtrack(pre(S, i)), whenever possible, in order to minimize the size of backtrack(pre(S, \ni)). 2. A more subtle optimization consists of not adding all enabled processes to backtrack(pre(S, \ni)) in line 7 when  3There is a nice complementarity between sleep sets and our dynamic partial-order \nreduction algorithm: when a process q is introduced in backtrack(pre(S, i)) in line 6 or 7 be\u00adcause of \na potential con.ict between i and next(s, p), there is no point in executing Si following next(pre(S, \ni),q)before next(s, p) is executed; this optimization is captured exactly by sleep sets. E is empty, \nbut instead of selecting a single other process q enabled in pre(S, i) and not previously exe\u00adcuted from \npre(S, i), and to re-start a new persistent\u00adset computation in pre(S, i)with q as the initial pro\u00adcess. \nHowever, to avoid circularity in this reasoning and to ensure the correctness of this variant algorithm, \nit is then necessary to mark process proc(ti) in state pre(S, i)so that, if proc(ti) is ever selected \nto be back\u00adtracked in pre(S, i) during this new persistent-set com\u00adputation starting with q, yet another \nfresh persistent\u00adset computation may be needed in pre(S, i), and so on.  4. IMPLEMENTATION In this section, \nwe discuss how to implement the previous general algorithm. We assume that the system has m pro\u00adcesses \np1,...,pm;that d is the maximum size of the search stack; and that r is the number of transitions explored \nin the reduced search space. 4.1 Clock Vectors The implementation of the algorithm of Figure 3 is mostly \nstraightforward, apart from identifying the necessary back\u00adtracking points in lines 3 9, which requires \ndeciding the happens-before relation i .S p. A natural representation strategy for the happens-before \nrelation is to use clock vec\u00adtors [17]. A clock vector is a map from process identi.ers to indices in \nthe current transition sequence S: CV = P.N We maintain a clock vector C(p) .CV for each process p. If \nprocess pi has clock vector C(pi)= (c1,...,cm),then cj is the index of the last transition by process \npj such that cj .S pi. More generally: i .S p if and only if i =C(p)(proc(Si)) Thus clock vectors allow \nus to decide the happens-before relation i .S p in constant time. We use max(\u00b7, \u00b7) to denote the pointwise \nmaximum of two clock vectors; C[pi := ci' ]to update the clock vector C so that the clock for process \npi is ci' ;and . to denote the minimal clock vector: '' '' max((c1, .., cm), (c1, .., c m))= (max(c1,c1), \n.., max(cm,cm))(c1,...,cm)[pi := c ' i]= (c1,...,ci-1,c ' i,ci+1,...,cm).= (0,..., 0) Whenever we explore \na transition of a process p, we need to update the clock vector C(p) to be the maximum of the clock vectors \nof all preceding dependent transitions. For this purpose,we alsokeep aclock vector C(i) for each index \ni in the current transition sequence S. Clock vectors can also be used to compute the set E as in line \n5 of Figure 3. However, this requires O(m 2.d)time per explored transition. Instead, for simplicity, \nour modi.ed algorithm just backtracks on all enabled processes in the case where p is not enabled in \npre(S, i) (line 7). Note that this last modi.cation is independent of the use of clock vectors. Figure \n4 presents a modi.ed algorithm that maintains and uses these per-process and per-transition clock vectors. \nThe code at lines 14.1 14.5 that updates these clock vectors re\u00adquires O(m.d) time per explored transition. \nLine 4 of the algorithm searches for an appropriate backtracking point, Figure 4: DPOR using Clock Vectors. \nFigure 5: DPOR without Stack Traversals. 0 Initially: Explore(\u00d8, .x..); 1Explore(S, C) { 2 let s = last(S); \n3 for all processes p { 4 if .i = max({i .dom(S) |Si is dependent and may be co-enabled with next(s, \np) and i =C(p)(proc(Si))}) { 5 if(p .enabled(pre(S, i))) 6 then add p to backtrack(pre(S, i)); 7 else \nadd enabled(pre(S, i)) to backtrack(pre(S, i)); 8 } 9 } 10 if(.p .enabled(s)) { 11 backtrack(s):= {p}; \n12 let done = \u00d8; 13 while (.p .(backtrack(s) \\done)) { 14 add p to done;  14.1 let t = next(s, p); 14.2 \nlet S ' = S.t; 14.3 let cv = max{C(i) |i .1..|S|and Si dependent with t}; 14.4 let cv2= cv[p := |S ' \n|]; 14.5 let C ' = C[p := cv2, |S ' |:= cv2]; 15 Explore(S ' , C ' ); 16 } 17 } 18 } and can be implemented \nas a sequential search through the transition stack S. The worst-case time complexity of this algorithm \nis O(m.d.r). The following invariants hold on each call to Explore: for all i, j .dom(S)and for all p \n.P: i .S p i. i =C(p)(proc(Si)) i .S j i. i =C(j)(proc(Si)) Using these invariants, we can show that \nthe algorithm of Figure 4 is a specialized version of the algorithm of Fig\u00adure 3 (although it may conservatively \nadd more backtracking points in line 7). 4.2 Avoiding Stack Traversals This section re.nes the previous \nalgorithm to avoid travers\u00ading the entire transition stack S. Instead, we assume that each transition \nt operates on exactly one shared object, which we denote by a(t) . Object. In addition, we as\u00adsume that \ntwo transitions t1 and t2 are dependent if and only if they access the same communication object, that \nis, if a(t1)= a(t2). Under these assumptions, the dependence relation is an equivalence relation and \nall accesses to an ob\u00adject o are totally ordered by the happens-before relation. In this case, it is \nsu.cient to only keep a clock vector C(o)for the last access to each object o. The use of per\u00adobject \ninstead of per-transition clock vectors signi.cantly reduces the time and space requirements of our algorithm. \nMaintaining these per-object clock vectors requires O(m) time per explored transition, as shown in lines \n14.3 14.4 of Figure 5. 0 Initially: Explore(\u00d8, .x.., .x.0); 1Explore(S, C, L) { 2 let s = last(S); 3 \nfor all processes p { 4 let i = L(a(next(s, p))); if i =0 and i =C(p)(proc(Si)) { 5 if(p .enabled(pre(S, \ni))) 6 then add p to backtrack(pre(S, i)); 7 else add enabled(pre(S, i)) to backtrack(pre(S, i)); 8 } \n9 } 10 if(.p .enabled(s)) { 11 backtrack(s):= {p}; 12 let done = \u00d8; 13 while (.p .(backtrack(s) \\done)) \n{ 14 add p to done; 14.1 let S ' = S.next(s, p); 14.2 let o = a(next(s, p)); 14.3 let cv = max(C(p),C(o))[p \n:= |S ' |]; 14.4 let C ' = C[p := cv, o := cv]; 14.5 let L ' =if next(s, p)is a release then L else L[o \n:= |S ' |]; 15 Explore(S ' , C ' , L ' ); 16 } 17 } 18 } To avoid a stack traversal for identifying \nbacktracking points, we also make some assumptions about the co-enabled relation. Speci.cally, for any \nobject o that is not a lock, we assume that any two transitions that access o may be co\u00adenabled. (Even \nif two operations are never co-enabed, it is still safe to assume that they may be co-enabled this may \nlimit the amount of reduction obtained, but will not a.ect correctness.) We use an auxiliary variable \nL(o)to track the index of the last transition that accessed o.When we con\u00adsider a subsequent access to \no by a transition next(s, p), we need to .nd the last dependent, co-enabled transition that does not \nhappen-before p. By our assumptions, the last ac\u00adcess L(o) must be co-enabled and dependent with next(s, \np), as they both access the same object o, which is not a lock. Therefore, L(o) is the appropriate backtracking \npoint, pro\u00advided L(o) does not happen-before p.In the case where L(o) happens-before p, since the accesses \nto o are totally\u00adordered, there cannot be any previous access to o that does not happen-before p, and \ntherefore no backtracking point is necessary. For a lock acquire, the appropriate backtracking point \nis not the preceding release, since an acquire and a release on the same lock are never co-enabled. Instead, \nthe appropriate backtracking point for a lock acquire is actually the preced\u00ading lock acquire. Hence, \nfor any lock o,we use L(o)to record the index of the last acquire (if any) on that lock. Figure 5 contains \na model checking algorithm based on these ideas. On each call to Explore, .nding backtrack\u00ading points \nrequires constant time per process, or O(m)time per explored transition. The clock vector operations \non lines 14.3 14.4 also require O(m) time per explored tran\u00adsition. Thus, the overall time complexity \nof this algorithm is O(m.r). In the following section, we evaluate the perfor\u00admance of this optimized \nalgorithm. We can show that this algorithm implements Figure 3, based on the following invariants that \nhold on each call to Explore: for all i .dom(S), for all p .P,and for all o .Object: i .S p i. i =C(p)(proc(Si)) \nL(o)= max{i .dom(S) |a(Si)= o and Si is not a release} Note that this implementation supports arbitrary \ncom\u00admunication objects such as shared variables, but it requires that all operations on shared variables \nare dependent. In particular, it does not exploit the fact that two concurrent reads on a shared variable \ncan commute. We could improve the algorithm along these lines by recording two clock vec\u00adtors for each \nshared variable, one for read accesses and for write accesses, and by using additional data structures \nto correctly identify backtracking points. The time complexity of the resulting algorithm is O(m 2 .r). \nDue to space con\u00adstraints we do not present this algorithm.  5. EXPERIMENTAL EVALUATION In this section, \nwe present a preliminary performance com\u00adparison of three di.erent partial-order reduction algorithms: \n No POR: A straightforward model-checking algorithm with no partial-order reduction.  Static POR: A \nhigh-precision stubborn-set-like algo\u00adrithm for statically computing persistent sets based on a precise \nstatic analysis of the program.  Dynamic POR: Our new dynamic partial-order reduc\u00adtion algorithm for \nmulti-threaded programs shown in Figure 5.  We describe the impact of using sleep sets in conjunction \nwith each algorithm. We also show the bene.t of extending No POR and Static POR to perform a stateful \nsearch,where visited states are stored in memory and the model checking algorithm backtracks whenever \nit visits a previously-visited state. We do not have a Dynamic POR implementation that supports a stateful \nsearch, since it is not obvious how to combine these ideas. Thus, we have ten model checking con.gurations, \nand we evaluate each model checking con.g\u00aduration on two benchmark programs. 5.1 Indexer Benchmark Our \n.rst benchmark is the Indexer program of Figure 1, where each threads inserts 4 messages into the shared \nhash table. For this benchmark, since a static analysis cannot reasonably predict with su.cient accuracy \nthe conditions under which hash table collisions would occur, Static POR yields the same performance \nas No POR. For clarity, we do not show the results for No POR, since they are identical to those for \nStatic POR. Our experimental results are pre\u00adsented in Figure 6. The key for this .gure is the same as \nin Figure 8: we use triangles for Dynamic POR,circles for Static POR,squares for No POR, dotted lines \nto indicate a stateful search, and hollow objects to indicate the use of sleep sets. Run-time is directly \nproportional to the number of explored transitions in all these experiments. For con.gurations with up \nto 11 threads, since there are no con.icts in the hash table and each thread accesses dif\u00adferent memory \nlocations, the reduced state space with Dy\u00adnamic POR is a single path. In comparison, the Static POR \nquickly su.ers from state explosion. When combined with sleep sets, Static POR performs better, but still \ncannot avoid state explosion. Using a stateful search in addition to sleep sets does not signi.cantly \nfurther reduce the number of ex\u00adplored transitions. 5.2 File System Benchmark Our second example in \nFigure 7 is derived from a syn\u00adchronization idiom found in the Frangipani .le system [24], and illustrates \nthe statically-hard-to-predict use of shared memory that motivates this work. For each .le, this ex\u00adample \nkeeps a data structure called an inode that contains a pointer to a disk block that holds the .le data. \nEach disk block b has a busy bit indicating whether the block has been allocated to an inode. Since the \n.le system is multi-threaded, these data structures are guarded by mu\u00adtual exclusive locks. In particular, \ndistinct locks locki[i] and lockb[b] protect each inode inode[i] and block busy bit busy[b], respectively. \nThe code for each thread picks an inode i and, if that inode does not already have an associ\u00adated block, \nthe thread searches for a free block to allocate to that inode. This search starts at an arbitrary block \nindex, to avoid excessive lock contention. Figure 8 shows the number of transitions executed when model \nchecking this benchmark for 1 to 26 threads, using each of the ten model checking algorithms. For No \nPOR, the search space quickly explodes, although sleep sets and a stateful search provide some bene.t. \nStatic POR identi.es that all accesses to the inode and busy arrays are protected by the appropriate \nlocks, thus reducing the number of in\u00adterleavings explored. Again, sleep sets help, but a stateful search \ndoes not provide noticable additional bene.t once sleep sets are used. Indeed, the two lines in Figure \n8 are essentially identical. Static POR must conservatively consider that acquires of the locks locki[i] \nmay con.ict, and similarly for lockb[b]. In contrast, Dynamic POR dynamically detects that such con.icts \ndo not occur for up to 13 threads, thus reducing the search space to a single path. For larger numbers \nof threads, since con.icts do occur, sleep sets provide additional bene.ts in combination with Dynamic \nPOR.  5.3 Discussion The results obtained with the two previous benchmarks clearly demonstrate that \nour dynamic partial-order reduc\u00adtion approach can sometimes signi.cantly outperform prior partial-order \nreduction techniques. However, note that Dynamic POR is not always strictly better than Static POR,since \nDynamic POR arbitrarily picks the initial transition t from each state, and then dy\u00adnamically computes \na persistent set that includes t.In con\u00adtrast, Static POR may be able to compute a smaller persis\u00adtent \nset that need not include t.Since Static POR and Dy\u00adnamic POR are compatible, they can be used simultaneously \nand bene.t from each other s strengths these experiments simply show that Dynamic POR can go much beyond \nStatic POR in cases where the latter is helpless. Figure 8: Number of transitions explored for the File \nSystem Benchmarks. File System Benchmark 1e+06  Number of Transitions 100000 10000 1000 100 10 Number \nof Threads Figure 6: Indexer Benchmark. (See Fig. 8 for key.) Indexer Benchmark 1e+06 100000 Figure \n7: File System Example. Global variables: const int NUMBLOCKS = 26; const int NUMINODE = 32; boolean[NUMINODE] \nlocki; int[NUMINODE] inode; boolean[NUMBLOCKS] lockb; boolean[NUMBLOCKS] busy; Thread-local variables: \nint i, b; Number of Transitions Code for thread tid: 10000i := tid % NUMINODE; acquire(locki[i]); if \n(inode[i] == 0) {b := (i*2) % NUMBLOCKS; while (true) {acquire(lockb[b]); if (!busy[b]) { busy[b] := \ntrue; inode[i] := b+1; 1000 100 release(lockb[b]); break; } release(lockb[b]); 10 b := (b+1)%NUMBLOCKS; \n}} Number of Threads release(locki[i]); exit();  6. RELATED WORK Our dynamic partial-order reduction \ntechnique has some general similarities with the least-commitment search strat\u00adegy used in non-linear \nplanners (e.g., see [3]) which origi\u00adnally inspired the work on partial-order reduction via net\u00adunfoldings \n[19], later extended from deadlock detection to full model checking [6, 7]. Loosely speaking, the term \nleast\u00adcommitment strategy means that every enabled transition is assumed by default not to interfere \nwith any other concur\u00adrent transition, unless this assumption is proved wrong later during the search. \nThe net-unfolding technique uses an elab\u00adorate data structure, called net-unfolding, for representing \nexplicitly all the partial-order executions explored so far plus all the nondeterminism (branching) to \ngo from one to the other. In contrast, our technique only uses a partial-order representation .S of a \nsingle execution trace S.Another key di.erence is that detecting deadlocks in a net-unfolding is itself \nNP-hard in the size of the net-unfolding in gen\u00aderal [19], while checking whether the current state is \na dead\u00adlock is immediate with an explicit state-space exploration, as in our approach. We are not aware \nof any implementation of the net-unfolding technique for languages (like C or Java) more expressive than \nPetri-net-like formalisms. It would be worth further comparing both approaches. A number of recent techniques \nhave considered various kinds of exclusive access predicates for shared variables that specify synchronization \ndisciplines such as this variable is only accessed when holding its protecting lock [21, 22, 8, 5]. These \nexclusive access predicates can be leveraged to re\u00adduce the search space, while simultaneously being \nveri.ed or inferred during reduced state-space exploration. However, these techniques do not work well \nwhen the synchronization discipline changes during program execution, such as when an object is initialized \nby its allocating thread without syn\u00adchronization, and subsequently shared in a lock-protected manner \nby multiple threads. Also, these techniques would not help in the case of the examples considered in \nthe pre\u00advious section. Note that dynamic partial-order reduction is also compatible and complementary \nwith these techniques. Partial-order representations of execution traces [16] have also been used for \ndetecting invariant violations in distributed systems (e.g., see [23]). In contrast with this prior work, \nwe exploit partial-order information to determine the possible existence of execution traces that are \nnot part of the cur\u00adrent partial-order execution, and to introduce backtracking points accordingly in \norder to prune the state space safely for veri.cation purposes. 7. CONCLUSIONS We have presented a new \napproach to partial-order reduc\u00adtion for model checking software. This approach is based on dynamically \ntracking interactions between concurrent pro\u00adcesses/threads at run time, and then exploiting this informa\u00adtion \nusing a new partial-order reduction algorithm to iden\u00adtify backtracking points where alternative paths \nin the state space need to be explored. In comparison to static partial-order methods, our algo\u00adrithm \nis easy to implement and does not require a compli\u00adcated and approximate static analysis of the program. \nIn addition, our dynamic POR approach can easily accommo\u00addate programming constructs that dynamically \nchange the structure of the program, such as the dynamic creation of additional processes or threads, \ndynamic memory allocation, or the dynamic creation of new communication channels be\u00adtween processes. \nIn contrast, static analysis of such con\u00adstructs is often di.cult or overly-approximate. We therefore \nbelieve that the idea of dynamic partial\u00adorder reduction is signi.cant since it provides an attractive \nand complementary alternative to the three known fam\u00adilies of partial-order reduction techniques, namely \npersis\u00adtent/stubborn sets, sleep sets and net unfoldings. The algorithms presented in this paper can \nbe used to prune acyclic state spaces while detecting deadlocks and safety-property violations without \nany risk of incomplete\u00adness. In practice, these algorithms can be used for system\u00adatically and e.ciently \ntesting the correctness of any concur\u00adrent software system, whether its state space is acyclic or not. \nHowever, in the presence of cycles, the depth of the search has to be bounded somehow, by simply using \nsome arbitrary bound [10] for instance. For application domains and sizes where computing canon\u00adical \nrepresentations for visited system states is tractable, such representations could be stored in memory \nand used both to avoid the re-exploration of previously visited states and to detect cycles. It would \nbe worth studying how to combine the type of dynamic partial-order reduction in\u00adtroduced in this paper \nwith techniques for storing states in memory and with existing partial-order reduction tech\u00adniques for \ndealing with cycles, liveness properties, and full temporal-logic model checking (e.g., [26, 9, 2]). \nAcknowledgements: We thank the anonymous review\u00aders for their helpful comments. This work was funded \nin part by NSF CCR-0341658 and NSF CCR-0341179. 8. REFERENCES [1] T. Ball and S. Rajamani. The SLAM \nToolkit. In Proceedings of CAV 2001 (13th Conference on Computer Aided Veri.cation), volume 2102 of Lecture \nNotes in Computer Science, pages 260 264, Paris, July 2001. Springer-Verlag. [2] E. M. Clarke, O. Grumberg, \nand D. A. Peled. Model Checking. MIT Press, 1999. [3] P.R. Cohen and E.A.Feigenbaum. Handbook of Arti.cial \nIntelligence. Pitman, London, 1982. [4] J. C.Corbett, M. B. Dwyer, J. Hatcli., S.Laubach, C.S. Pasareanu, \nRobby, and H. Zheng. Bandera: Extracting Finite-State Models from Java Source Code. In Proceedings of \nthe 22nd International Conference on Software Engineering, 2000. [5] M.B.Dwyer,J. Hatcli.,V.R.Prasad, \nand Robby. ExploitingObject Escape and LockingInformation in Partial Order Reduction for Concurrent Object-Oriented \nPrograms. To appear in Formal Methods in System Design, 2004. [6] J. Esparza. Model CheckingUsingNet \nUnfoldings. Science of Computer Programming, 23:151 195, 1994. [7] J. Esparza and K. Heljanko. ImplementingLTL \nmodel checkingwith net unfoldings. In Proceedings of the 8th SPIN Workshop (SPIN 2001), volume 2057 of \nLecture Notes in Computer Science, pages 37 56, Toronto, May 2001. Springer-Verlag. [8] C. Flanagan \nand S. Qadeer. Transactions for Software Model Checking. In Proceedings of the Workshop on Software Model \nChecking, pages 338 349, June 2003. [9] P. Godefroid. Partial-Order Methods for the Veri.cation of Concurrent \nSystems An Approach to the State-Explosion Problem, volume 1032 of Lecture Notes in Computer Science. \nSpringer-Verlag, January 1996. [10] P. Godefroid. Model Checking for Programming Languages usingVeriSoft. \nIn Proceedings ofPOPL 97(24thACM Symposium on Principles of Programming Languages), pages 174 186, Paris, \nJanuary 1997. [11] P. Godefroid. Software Model Checking: The VeriSoft Approach. To appear in Formal \nMethods in System Design, 2005. Also available as Bell Labs Technical Memorandum ITD-03-44189G. [12] \nP. Godefroid and D. Pirottin. Re.ningdependencies improves partial-order veri.cation methods. In Proceedings \nof CAV 93 (5th Conference on Computer Aided Veri.cation), volume 697 of Lecture Notes in Computer Science, \npages 438 449, Elounda, June 1993. Springer-Verlag. [13] P. Godefroid and P. Wolper. Usingpartial orders \nfor the e.cient veri.cation of deadlock freedom and safety properties. Formal Methods in System Design, \n2(2):149 164, April 1993. [14] T. Henzinger, R. Jhala, R. Majumdar, and G. Sutre. Lazy Abstraction. In \nProceedings of the 29th ACM Symposium on Principles of Programming Languages, pages 58 70, Portland, \nJanuary 2002. [15] S. Katz and D. Peled. De.ningconditional independence usingcollapses. Theoretical \nComputer Science, 101:337 359, 1992. [16] L. Lamport. Time, clocks, and the orderingof events in a distributed \nsystem. Communications of the ACM, 21(7):558 564, 1978. [17] F. Mattern. Virtual Time and Global States \nof Distributed Systems. In Proc. Workshop on Parallel and Distributed Algorithms, pages 215 226. North-Holland \n/ Elsevier, 1989. [18] A. Mazurkiewicz. Trace theory. In Petri Nets: Applications and Relationships to \nOther Models of Concurrency, Advances in Petri Nets 1986, Part II; Proceedings of an Advanced Course, \nvolume 255 of Lecture Notes in Computer Science, pages 279 324. Springer-Verlag, 1986. [19] K. McMillan. \nUsingunfoldingto avoid the state explosion problem in the veri.cation of asynchronous circuits. In Proc. \n4th Workshop on Computer Aided Veri.cation, volume 663 of Lecture Notes in Computer Science,pages 164 \n177, Montreal, June 1992. Springer-Verlag. [20] D. Peled. All from one, one for all: on model checkingusing \nrepresentatives. In Proc. 5th Conference on Computer Aided Veri.cation, volume 697 of Lecture Notes in \nComputer Science, pages 409 423, Elounda, June 1993. Springer-Verlag. [21] S. D. Stoller. Model-CheckingMulti-Threaded \nDistributed Java Programs. International Journal on Software Tools for Technology Transfer, 4(1):71 91, \nOctober 2002. [22] S. D. Stoller and E. Cohen. Optimistic Synchronization-Based State-Space Reduction. \nIn H. Garavel and J. Hatcli., editors, Proceedings of the 9th International Conference on Tools and Algorithms \nfor the Construction andAnalysisofSystems(TACAS),volume 2619 of Lecture Notes in Computer Science, pages \n489 504. Springer-Verlag, April 2003. [23] S. D. Stoller, L. Unnikrishnan, and Y. A. Liu. E.cient Detection \nof Global Properties in Distributed Systems UsingPartial-Order Methods. In Proceedings of the 12th Conference \non Computer Aided Veri.cation, volume 1855 of Lecture Notes in Computer Science, pages 264 279, Chicago, \nJuly 2000. Springer-Verlag. [24] C.A. Thekkath, T. Mann, and E.K. Lee. Frangipani: A scalable distributed \n.le system. In Proceedings of the 16th ACM Symposium on Operating Systems Principles,pages 224 237, October \n1997. [25] A. Valmari. Stubborn sets for reduced state space generation. In Advances in Petri Nets 1990, \nvolume 483 of Lecture Notes in Computer Science, pages 491 515. Springer-Verlag, 1991. [26] A. Valmari. \nOn-the-.y veri.cation with stubborn sets. In Proc. 5th Conference on Computer Aided Veri.cation, volume \n697 of Lecture Notes in Computer Science,pages 397 408, Elounda, June 1993. Springer-Verlag. [27] W. \nVisser, K. Havelund, G. Brat, and S. Park. Model CheckingPrograms. In Proceedings of ASE 2000 (15th International \nConference on Automated Software Engineering), Grenoble, September 2000.  APPENDIX: Proof of Theorem \n1 Let AG denote the state space of the system being analyzed, and let s0 denote its unique initial state. \nDe.ne E(S, i, p)as: { q . enabled(pre(S, i)) |q = p or .j . dom(S): j>i and q = proc(Sj)and j .S p} De.ne \nPC(S, j, p)as: if S is a transition sequence from s0 in AG and i = max({i . dom(S) | Si is dependent \nand co-enabled with next(last(S),p)and i .S p}) and i = j then if E(S, i, p)= \u00d8 then backtrack(pre(S, \ni)) n E(S, i, p)= \u00d8 else backtrack(pre(S, i)) = enabled(pre(S, i)) De.ne the postcondition PC for Explore(S)as: \n.p .w : PC(S.w, |S|,p) We .rst show that the set of transition explored from each reached state is a \npersistent set, provided the postcondition holds for each recursive call to Explore(\u00b7). Lemma 1. Whenever \na state s reached after a transition sequence S is backtracked during the search performed by the algorithm \nof Figure 3, the set T of transitions that have been explored from s is a persistent set in s, provided \nthe postcondition PC holds for every recursive call Explore(S.t) for all t . T . Proof. Let s = last(S) \nT = {next(s, p) | p . backtrack(s)} We proceed by contradiction, and assume that there exist t1,...,tn \n. T such that: 1. S.t1 ...tn is a transition sequence from s0 in AG and 2. t1,...,tn-1 are all independent \nwith T and 3. tn is dependent with some t . T .  By property of independence, this implies that t is \nenabled in the state last(S.t1 ...tn-1) and hence co-enabled with tn. Without loss of generality, assume \nthat t1 ...tn is the shortest such sequence. We thus have that .1 = i<n : i .t1...tn-1 n (If this was \nnot true for some i, the same transition se\u00adquence without i would also satisfy our assumptions and be \nshorter.) Let w denote the resulting (possibly empty) transition sequence produced by removing from t1 \n...tn-1 all the transitions ti (if any) such that i .t1...tn-1 proc(tn) By de.nition, S.w is itself a \ntransition sequence from s0 in AG and we have next(last(S.w),proc(tn)) = next(last(S.t1 ...tn),proc(tn)) \n= tn (Although tn is enabled in last(S.t1 ...tn-1), tn may no longer be enabled in last(S.w), but this \ndoes not matter for the proof.) If proc(t)= proc(tn)then t = next(last(S),proc(t)) = next(last(S.w),proc(t)) \n= tn since t is independent with all the transitions in w, contra\u00addicting that tn .T .Hence proc(t)= \nproc(tn). Since t is in a di.erent process than tn and since t is independent with all the transitions \nin w,wehave tn = next(last(S.w),proc(tn)) = next(last(S.w.t),proc(tn)) = next(last(S.t.w),proc(tn)) Let \ni = |S|+ 1. Consider the postcondition PC(S.t.w, i, proc(tn)) for the recursive call Explore(S.t). Clearly, \ni .S.t.w proc(tn) (since t is in a di.erent process than tn and since t is inde\u00adpendent with t1,...,tn-1). \nIn addition, we have (by de.ni\u00adtion of E): E(S.t.w, i, proc(tn)) . {proc(t1),...,proc(tn-1),proc(tn)}nenabled(s) \n Moreover, we have by construction: .j .dom(S.w): j>i .j .S.t.w proc(tn) Hence, by the postcondition \nPC for the recursive call Explore(S.t), either E(S.t.w, i, proc(tn)) is nonempty and at least one pro\u00ad \n(e) .i .dom(S): S.w p i. i .S p. cess in E(S.t.w, i, proc(tn)) is in backtrack(s), or E(S.t.w, i, proc(tn)) \n.p : PC(S, |S|,p), and is directly established by lines 3 9 of the algorithm of Figure 3. (Inductive \ncase) We assume that each recursive call to Explore(S.t) satis.es its postcondition. That T is a per\u00adsistent \nset in s then follows by Lemma 1. We show that Explore(S) ensures its postcondition for any p and w such \nthat S.w is a transition sequence from s0 in AG. 1. Suppose some transition in w is dependent with some \ntransition in T . In this case, we split w into X.t.Y , where all the transitions in X are independent \nwith all the transitions in T and t is the .rst transition in w that is dependent with some transition \nin T .Since T is a persistent set in s, t must be in T (otherwise, T would not be persistent in s). Therefore, \nt is inde\u00adpendent with all the transitions in X. By property of independence, this implies that the transition \nsequence t.X.Y is executable from s. By applying the inductive hypothesis to the recursive call Explore(S.t), \nwe know .p : PC(S.t.X.Y, |S|+1,p) which implies (by the de.nition of PC)that .p : PC(S.t.X.Y, |S|,p) \nSince t is independent with all the transitions in X,we also have that .i .dom(S.t.X.Y ): i .S.t.X.Y \np i. i .S.X.t.Y p Therefore, by de.nition, PC(S.t.X.Y, |S|,p)i. PC(S.X.t.Y, |S|,p) We can thus conclude \nthat .p : PC(S.X.t.Y, |S|,p) 2. Suppose that all the transitions in w are independent with all the transitions \nin T and p . backtrack(s). Then (a) next(s, p) .T ; (b) next(s, p) is independent with w; (c) p is a \ndi.erent process from any transition in w;  (d) next(last(S.w),p)= next(last(S),p); i . is empty and \nall the processes enabled in s are in backtrack(s). In either cases, at least one transition among {t1,...,tn}is \nin T . This contradicts the assumption that t1,...,tn .T . We now turn to the proof of Theorem 1. Theorem \n1. Whenever a state s reached after a transi\u00adtion sequence S is backtracked during the search performed \nby the algorithm of Figure 3 in an acyclic state space, the postcondition PC for Explore(S) is satis.ed, \nand the set T of transitions that have been explored from s is a persistent set in s. Proof. Let s = \nlast(S) T = {next(s, p) |p .backtrack(s)} The proof is by induction on the order in which states are \nbacktracked. (Base case) Since the state space AG is acyclic and since the search is performed in depth-.rst \norder, the .rst back\u00adtracked state must be a deadlock where no transition is en\u00adabled. Therefore, the \npostcondition for that state becomes Thus, we have PC(S.w, |S|,p)i. PC(S, |S|,p), and the latter is directly \nestablished by the lines 3 9 of the al\u00adgorithm for all p. 3. Suppose that all the transitions in w are \nindependent with all the transitions in T and p .backtrack(s). Pick any t .T . Wethen havethat (a) proc(t)= \np; (b) t independent with all the transitions in w;  (c) next(last(S.w),p)= next(last(S.t.w),p); (d) \n.i .dom(S): i .S.w p i. i .S.t.w p. Thus, we have PC(S.w, |S|,p)i. PC(S.t.w, |S|,p). By applying the \ninductive hypothesis to the recursive call Explore(S.t), we know .p : PC(S.t.w, |S|+1,p) which implies \n(by the de.nition of PC)that .p : PC(S.t.w, |S|,p) which in turn implies .p : PC(S.w, |S|,p) as required. \n  \n\t\t\t", "proc_id": "1040305", "abstract": "We present a new approach to partial-order reduction for model checking software. This approach is based on initially exploring an arbitrary interleaving of the various concurrent processes/threads, and <i>dynamically</i> tracking interactions between these to identify backtracking points where alternative paths in the state space need to be explored. We present examples of multi-threaded programs where our new dynamic partial-order reduction technique significantly reduces the search space, even though traditional partial-order algorithms are helpless.", "authors": [{"name": "Cormac Flanagan", "author_profile_id": "81100538763", "affiliation": "University of California at Santa Cruz, CA", "person_id": "PP14187273", "email_address": "", "orcid_id": ""}, {"name": "Patrice Godefroid", "author_profile_id": "81100504535", "affiliation": "Bell Laboratories, Lucent Technologies", "person_id": "PP40027996", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1040305.1040315", "year": "2005", "article_id": "1040315", "conference": "POPL", "title": "Dynamic partial-order reduction for model checking software", "url": "http://dl.acm.org/citation.cfm?id=1040315"}