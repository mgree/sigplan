{"article_publication_date": "01-12-2005", "fulltext": "\n The Java Memory Model* Jeremy Manson and William Pugh Department of Computer Science University of Maryland, \nCollege Park College Park, MD {jmanson, pugh}@cs.umd.edu ABSTRACT This paper describes the new Java \nmemory model, which has been revised as part of Java 5.0. The model speci.es the legal behaviors for \na multithreaded program; it de.nes the semantics of multithreaded Java programs and partially determines \nlegal implementations of Java virtual machines and compilers. The new Java model provides a simple interface \nfor cor\u00adrectly synchronized programs it guarantees sequential con\u00adsistency to data-race-free programs. \nIts novel contribution is requiring that the behavior of incorrectly synchronized programs be bounded \nby a well de.ned notion of causality. The causality requirement is strong enough to respect the safety \nand security properties of Java and weak enough to allow standard compiler and hardware optimizations. \nTo our knowledge, other models are either too weak because they do not provide for su.cient safety/security, \nor are too strong because they rely on a strong notion of data and control dependences that precludes \nsome standard compiler transformations. Although the majority of what is currently done in compil\u00aders \nis legal, the new model introduces signi.cant di.erences, and clearly de.nes the boundaries of legal \ntransformations. For example, the commonly accepted de.nition for control dependence is incorrect for \nJava, and transformations based on it may be invalid. In addition to providing the o.cial memory model \nfor Java, we believe the model described here could prove to be a useful basis for other programming \nlanguages that currently lack well-de.ned models, such as C++ and C#. Categories and Subject Descriptors: \nD.3.1 [Program\u00adming Languages]: Formal De.nitions and Theory; D.3.0 [Programming Languages]: Standards; \nF.3.2 [Logics * Jeremy Manson and William Pugh were supported by National Science Foundation grants CCR-9619808, \nACI\u00ad9720199 and CCR-0098162, and a gift from Sun Microsys\u00adtems. Sarita Adve was supported in part by \nan Alfred P. Sloan research fellowship. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 05, January 12 14, 2005, Long Beach, California, USA. Copyright \n2005 ACM 1-58113-830-X/05/0001 ...$5.00. Sarita V. Adve Department of Computer Science University of \nIllinois at Urbana-Champaign Urbana-Champaign, IL sadve@cs.uiuc.edu and Meanings of Programs]: Operational \nSemantics General Terms: Design, Languages, Standardization Keywords: Concurrency, Java, Multithreading, \nMemory Model  1. INTRODUCTION The memory model for a multithreaded system speci.es how memory actions \n(e.g., reads and writes) in a program will appear to execute to the programmer, and speci.cally, which \nvalue each read of a memory location may return. Ev\u00adery hardware and software interface of a system that \nadmits multithreaded access to shared memory requires a memory model. The model determines the transformations \nthat the system (compiler, virtual machine, or hardware) can apply to a program written at that interface. \nFor example, given a program in machine language, the memory model for the machine language / hardware \ninterface will determine the optimizations the hardware can perform. For a high-level programming language \nsuch as Java, the memory model determines the transformations the compiler may apply to a program when \nproducing bytecode, the trans\u00adformations that a virtual machine may apply to bytecode when producing \nnative code, and the optimizations that hardware may perform on the native code. The model also impacts \nthe programmer; the transfor\u00admations it allows (or disallows) determine the possible out\u00adcomes of a program, \nwhich in turn determines which de\u00adsign patterns for communicating between threads are legal. Without \na well-speci.ed memory model for a programming language, it is impossible to know what the legal results \nare for a program in that language. For example, a mem\u00adory model is required to determine what, if any, \nvariants of double checked locking [37] are valid within a particular language. 1.1 Background Over the \nlast two decades, there has been a tremendous amount of work in the area of memory models at the hard\u00adware \ninterface [2, 3, 4, 12, 14, 17, 31, 40, 43], but little at the programming language level. Programming \nlanguages present two challenges that do not occur in the hardware world. First, many programming languages, \nsuch as Java, have strong safety and security properties that must be re\u00adspected. Second, the ability \nof a compiler to perform global and subtle analysis in order to transform a program is es\u00adsentially unlimited, \nwhile hardware systems tend to use a more circumscribed scope in performing optimizations. Previous work \n[33, 34] has shown that the original seman\u00adtics for multithreaded Java [18, \u00a717] had serious problems. \nTo address these issues, the Java memory model has recently undergone a major revision. The new model \nnow provides greater .exibility for implementors, a clear notion of what it means to write a correct \nprogram, and clear semantics for both correct and incorrect programs. Despite this funda\u00admental change, \nit is largely, although not entirely, consistent with previous good programming practice and existing \nJVM implementations and hardware. The memory model must strike a balance between ease\u00adof-use for programmers \nand implementation .exibility for system designers. The model that is easiest to understand, sequential \nconsistency [26], speci.es that memory actions must appear to execute one at a time in a single total \norder; actions of a given thread must appear in this total order in the same order in which they appear \nin the program (called program order). Whether sequential consistency by itself is an appropriate model \nfor high-level language programmers is a subject of debate: it makes it easier for the programmer to \nwrite subtle and complicated concurrent algorithms that do not use ex\u00adplicit synchronization. Unfortunately, \nsuch algorithms are often designed or implemented incorrectly, and some would argue that almost all programmers \nshould be discouraged from writing subtle and complicated synchronization-free concurrent algorithms. \nSequential consistency substantially restricts the use of many compiler and hardware transformations. \nIn general, sequential consistency restricts the reordering of any pair of memory statements within a \nthread, even if there are no data or control dependences between the statements (see Figure 1). This \ncan be a serious limitation since many important optimizations involve reordering program state\u00adments. \nEven optimizations as ubiquitous as common subex\u00adpression elimination and redundant read elimination \ncan be seen as reorderings: each evaluation of the common expres\u00adsion is conceptually moved to the point \nat which it is eval\u00aduated for the .rst time. In a sequentially consistent system, no compiler or processor \nreorderings should become visible to the programmer. Recently, hardware designers have developed techniques \nthat alleviate some of the limitations of sequential consis\u00adtency by reordering accesses speculatively \n[16, 35]. The re\u00adordered accesses are committed only when they are known to not be visible to the programmer. \nCompiler techniques to determine when reorderings are safe have also been pro\u00adposed [38, 42], but are \nnot yet comprehensively evaluated or implemented in commercial compilers. A common method to overcome \nthe limitations of sequen\u00adtial consistency is the use of relaxed memory models, which allow more optimizations \n[12, 17, 19, 31, 40, 43]. Many of these models were originally motivated by hardware op\u00adtimizations and \nare described in terms of low-level system attributes such as bu.ers and caches. Generally, these mod\u00adels \nhave been hard to reason with and, as we show later, do not allow enough implementation .exibility either. \nTo achieve both programming simplicity and implementa\u00adtion .exibility, alternative models, referred to \nas data-race\u00adfree models [2, 4, 6] or properly-labeled models [14, 15], have been proposed. This approach \nexploits the observation that good programming practice dictates that programs be cor\u00adrectly synchronized \n(data-race-free); a data race is often a Initially, x == y == 0 Thread 1 Thread 2 1: r2 =x; 3: r1 =y \n2: y =1; 4: x=2 r2 == 2, r1 == 1 violates sequential consistency. With sequential consistency, the result \nr2==2, r1==1 is impossible. Such a result would imply that statement 4 came before 1 (because r2 == 2), \n1 came before 2 (due to program order), 2 came before 3 (because r1 ==1), 3 came before 4 (due to program \norder), and so 4 came before 4. Such a cycle is prohibited by sequential consistency. How\u00adever, if the \nhardware or the compiler reorder the instructions in each thread, then statement 3 may not come before \n4, and/or 1 may not come before 2, making the above result possible. Thus, sequential consistency restricts \nreordering instructions in a thread, even if they are not data or control dependent. Figure 1: A Violation \nof Sequential Consistency. symptom of a bug. The data-race-free models formalize cor\u00adrect programs as \nthose that are data-race-free, and guaran\u00adtee the simple semantics of sequential consistency for such \nprograms. For full implementation .exibility, these models do not provide any guarantees for programs \nthat contain data races. This approach allows standard hardware and compiler optimizations, while providing \na simple model for programs written according to widely accepted practice. 1.2 The Java Memory Model \nThe new Java model adopts the data-race-free approach for correct programs correct programs are those \nthat are data-race-free; such programs are guaranteed sequential con\u00adsistency. Unfortunately, if a model \nleaves the semantics for incorrect programs unspeci.ed, it allows violations of safety and security properties \nthat are necessary for Java programs. A key contribution of the revision e.ort has been to show that \nthe safety and security properties of Java require pro\u00adhibiting a class of executions that contain data \nraces and have not been previously characterized. The challenge is to characterize and prohibit this \nclass of executions with\u00adout prohibiting standard compiler and hardware transforma\u00adtions or unduly complicating \nthe model. In earlier relaxed models, such executions were either allowed, or were only prohibited by \nenforcing traditional control and data depen\u00addences. We show that the latter approach restricts standard \ncompiler optimizations and is not viable for Java. The revised Java model is based on a new technique \nthat prohibits the necessary executions while allowing standard optimizations. Our technique builds legal \nexecutions itera\u00adtively. In each iteration, it commits a set of memory actions; actions can be committed \nif they occur in some well-behaved execution that also contains the actions committed in previ\u00adous iterations. \nA careful de.nition of well-behaved execu\u00adtions ensures that the appropriate executions are prohib\u00adited \nand standard compiler transformations are allowed. To our knowledge, this is the only model with this \nproperty. This paper focuses on the semantics for ordinary reads and writes, locks, and volatile variables \nin the Java memory model. It also covers the memory semantics for interaction with the outside world, \nand in.nite executions. For space reasons, we omit discussion of two important issues in the Java memory \nmodel: the treatment of .nal .elds, and .nal\u00adization / garbage collection. We discuss the rest of the \nmodel in detail; the full speci.cation is available elsewhere [20]. The revision process for the Java \nmodel involved continual feedback and participation from the broader Java commu\u00adnity; we refer the reader \nto the Java memory model mailing list archives for more information on the evolution of the model [21]. \nWhile this model was developed for Java, many of the issues that are addressed apply to every multithreaded \nlan\u00adguage. It is our hope that the work presented here can be leveraged to provide the right balance \nof safety and e.ciency for future programming languages.  2. REQUIREMENTS FOR THE JAVA MEM-ORY MODEL \nOur major design goal was to provide a balance between (1) su.cient ease of use and (2) transformations \nand opti\u00admizations used in current (and ideally future) compilers and hardware. Given that current hardware \nand compilers employ trans\u00adformations that violate sequential consistency, it is currently not possible \nto provide a sequentially consistent program\u00adming model for Java. The memory model for Java is there\u00adfore \na relaxed model. A detailed description of the requirements for the Java model and their evolution can \nbe found elsewhere [30]. Here we provide a brief overview. 2.1 Correctly Synchronized Programs It is \ndi.cult for programmers to reason about speci.c hardware and compiler transformations. For ease of use, \nwe therefore specify the model so that programmers do not have to reason about hardware or compiler transformations \nor even our formal semantics for correctly synchronized code. We follow the data-race-free approach to \nde.ne a correctly synchronized program (data-race-free) and correct seman\u00adtics for such programs (sequential \nconsistency). The follow\u00ading de.nitions formalize these notions. Con.icting Accesses: A read of or write \nto a vari\u00adable is an access to that variable. Two accesses to the same shared .eld or array element are \nsaid to be con.icting if at least one of the accesses is a write.  Synchronization Actions: Synchronization \nactions  include locks, unlocks, reads of volatile variables, and writes to volatile variables. Synchronization \nOrder: Each execution of a pro\u00adgram is associated with a synchronization order, which is a total order \nover all synchronization actions. To de\u00ad.ne data-race-free programs correctly, we are allowed to consider \nonly those synchronization orders that are also consistent with program order and where a read to a volatile \nvariable v returns the value of the write to v that is ordered last before the read by the syn\u00adchronization \norder. These latter requirements on the synchronization order are explicitly imposed as a syn\u00adchronization \norder consistency requirement in the rest of the paper.  Synchronizes-With Order: For two actions x \nand  sw y, we use x . y to mean that x synchronizes-with y. An unlock action on monitor m synchronizes-with \nall subsequent lock actions on m that were performed by any thread, where subsequent is de.ned accord\u00ading \nto the synchronization order. Similarly, a write to a volatile variable v synchronizes-with all subse\u00adquent \nreads of v by any thread. There are additional synchronized-with edges in Java [20] that are not dis\u00adcussed \nhere for brevity. Happens-Before Order: For two actions x and y, hb we use x . y to mean that x happens-before \ny [25]. Happens-before is the transitive closure of program or\u00adder and the synchronizes-with order. \nData Race: Two accesses x and y form a data race in an execution of a program if they are from di.erent \nthreads, they con.ict, and they are not ordered by happens-before.  Correctly Synchronized or Data-Race-Free \nPro\u00adgram [4, 6]: A program is said to be correctly syn\u00adchronized or data-race-free if and only if all \nsequentially consistent executions of the program are free of data races.  The .rst requirement for \nthe Java model is to ensure se\u00adquential consistency for correctly synchronized or data-race\u00adfree programs \nas de.ned above. Programmers then need only worry about code transformations having an impact on their \nprograms results if those programs contain data races. As an example, the code in Figure 1 is incorrectly \nsyn\u00adchronized. Any sequentially consistent execution of the code contains con.icting accesses to x and \ny and these con.icting accesses are not ordered by happens-before (since there is no synchronization \nin the program). A way to make this pro\u00adgram correctly synchronized is to declare x and y as volatile \nvariables; then the program is assured of sequentially con\u00adsistent results with the Java model. 2.2 \nOut-of-Thin-Air Guarantees for Incorrect Programs The bulk of the e.ort for the revision, and our focus \nhere, was on understanding the requirements for incorrectly syn\u00adchronized code. The previous strategy \nof leaving the seman\u00adtics for incorrect programs unspeci.ed is inconsistent with Java s security and \nsafety guarantees. Such a strategy has been used in some prior languages. For example, Ada simply de.nes \nunsynchronized code as erroneous [1]. The reason\u00ading behind this is that since such code is incorrect \n(on some level), no guarantees should be made when it occurs. This is similar to the strategy that some \nlanguages take with array bounds over.ow unpredictable results may occur, and it is the programmer s \nresponsibility to avoid these scenarios. The above approach does not lend itself to the writing of secure \nand safe code. In an ideal world, every program\u00admer would write correct code all of the time. In our \nworld, programs frequently contain errors; not only does this cause code to misbehave, but it can also \nallows attackers to vio\u00adlate safety assumptions in a program (as is true with bu.er over.ows). Our earlier \nwork has described the dangers of such scenarios in more detail [28]. Program semantics must be completely \nde.ned: if pro\u00adgrammers don t know what their code is doing, they won t Initially, x == y == 0 Thread \n1 r1 =x; y = r1; x = r2; Incorrectly synchronized, but we want to disallow r1== r2==42. Figure 2: An \nOut Of Thin Air Result be able to know what their code is doing wrong. The second broad requirement of \nthe Java model is to provide a clear and de.nitive semantics for how code should behave when it is not \ncorrectly written, but without substantially a.ecting current compilers and hardware. Figure 2 contains \na common example of an program for which careful semantics are required. It is incorrectly syn\u00adchronized; \nall sequentially consistent executions of this pro\u00adgram exhibit data races between Threads 1 and 2. However, \nwe need to provide a strong guarantee for this code: we need to ensure that a value such as 42 will not \nappear out of thin air in r1 and r2. There are no current optimizations that would permit such an out-of-thin-air \nresult. However, in a future aggres\u00adsive system, Thread 1 could speculatively write the value 42 to y, \nwhich would allow Thread 2 to read 42 for y and write it out to x, which would allow Thread 1 to read \n42 for x, and justify its original speculative write of 42 for y. A self-justifying write speculation \nlike that one can create serious security violations and needs to be disallowed. For example, this would \nbe a serious problem if the value that was produced out of thin air was a reference to an object that \nthe thread was not supposed to have (because of, for example, security guarantees). Characterizing precisely \nwhat constitutes an out-of-thin\u00adair violation is complicated. Not all speculations that appear to be \nself-justifying are security violations, and some can even arise out of current compiler transformations. \nHow we characterize the appropriate set of violations is at the core of the new Java model, and discussed \nfurther in Section 4.2.  3. HAPPENS-BEFORE MEMORY MODEL Before presenting the Java model in full, we \npresent a simpler memory model, called the happens-before memory model. Each legal execution under this \nmodel has several proper\u00adties/requirements (Section 5 provides a more formal version of the requirements): \n There is a synchronization order over synchronization actions, synchronization actions induce synchronizes\u00adwith \nedges between matched actions, and the tran\u00adsitive closure of the synchronizes-with edges and pro\u00adgram \norder gives an order known as the happens-before order (as described in Section 2.1).  For each thread \nt, the actions t performs in the exe\u00adcution are the same as those that t would generate in program order \nin isolation, given the values seen by the reads of t in the execution.  A rule known as happens-before \nconsistency (see be\u00adlow) determines the values a non-volatile read can see.  A rule known as synchronization \norder consistency (see below) determines the value a volatile read can see.  Initially, x == 0, ready \n== false. ready is a volatile variable. Thread 1 Thread 2 x =1; if (ready) ready = true r1 =x; If r1 \n= x; executes, it will read 1. Figure 3: Use of Happens-Before Memory Model with Volatiles Happens-before \nconsistency says that a read r of a variable v is allowed to observe a write w to v if, in the happens-before \npartial order of the execution: r does not happen-before w (i.e., it is not the case that hb r . w) \na read cannot see a write that happens-after it, and there is no intervening write w . to v, ordered \nby happens\u00ad hbhb before (i.e., no write w . to v such that w . w . . r) the write w is not overwritten \nalong a happens-before path. Synchronization order consistency says that (i) synchro\u00adnization order is \nconsistent with program order and (ii) each read r of a volatile variable v sees the last write to v \nto come before it in the synchronization order. The happens-before memory model is a simple approxima\u00adtion \nto the semantics we want for the Java memory model. For example, the happens-before memory model allows \nthe behavior shown in Figure 1. There are no synchronizes-with or happens-before edges between threads, \nand each read is allowed to return the value of the write by the other thread. Figure 3 provides an example \nof how to use the happens\u00adbefore memory model to restrict the values a read can re\u00adturn. Notice that \nready is a volatile boolean variable. There is therefore a happens-before edge from the write ready = \ntrue to any read that sees that write. Assume Thread 2 s read of ready sees the write to ready. Thread \n1 s write of x must happen-before Thread 2 s read of x (because of program order), and Thread 2 s read \nmust return the new value of x. In more detail: the initial value of x is assumed to happen-before all \nactions. There is a path of happens-before edges that leads from the initial write, through the write \nto x, to the read of x. On this path, the initial write is seen to be overwritten. Thus, if the read \nof ready in Thread 2 sees true, then the read of x must see the value 1. If ready were not volatile, \none could imagine a compiler transformation that reordered the two writes in Thread 1, resulting in a \nread of true for ready, but a read of 0 for x.  4. CAUSALITY Although it is simple, the happens-before \nmemory model (as discussed in Section 3) allows unacceptable behaviors. Notice that the behavior we want \nto disallow in Figure 2 is consistent with the happens-before memory model. If both write actions write \nout the value 42, and both reads see those writes, then both reads see values that they are allowed to \nsee. The happens-before memory model also does not ful.ll our requirement that correctly synchronized \nprograms have sequentially consistent semantics (.rst observed in [6, 7]). Before compiler transformation \nInitially, a = 0, b = 1 Thread 1 Thread 2 1: r1=a; 2: r2=a; 3: if (r1 == r2) 4: b=2; Is r1 == r2 == r3 \n5: r3=b; 6: a=r3; == 2 possible? Thread 1 4: b=2; 1: r1=a; 2: r2=r1; 3: if (true) ; r1== r2==r3 == consistent \n Figure 5: E.ects of Redundant Read Elimination Initially, x==y ==0 Thread 1 Thread 2 r1 = x; r2= y; \nif(r1 !=0) if (r2 != 0) y = 42; x = 42; Correctly synchronized, so we must disallow r1==r2 == 42. Figure \n4: Correctly Synchronized Program For example, the code shown in Figure 4 [2] is correctly synchronized. \nThis may seem surprising, since it doesn t perform any synchronization actions. Remember, however, that \na program is correctly synchronized if, when it is exe\u00adcuted in a sequentially consistent manner, there \nare no data races. If this code is executed in a sequentially consistent way, each action will occur \nin program order, and neither of the writes will occur. Since no writes occur, there can be no data races: \nthe program is correctly synchronized. For the above program, the only sequentially consistent result \nis r1 == r2 == 0; this is therefore the only result that we want to allow under the Java model. However, \nas the result in Figure 2, the result of r1 == r2 == 42 for Figure 4 is consistent with the happens-before \nmemory model. If both writes occur, and both reads see them, then the if guards are both true, and both \nreads see writes that they are allowed to see. As mentioned, this violates the guarantee that we wish \nto make of sequential consistency for correctly synchronized programs Regardless of these weaknesses, \nthe happens-before mem\u00adory model provides a good outer bound for our model; all of our executions must \nbe consistent with it. 4.1 Data and Control Dependencies The missing link between our desired semantics \nand the happens-before memory model is one of causality. In Fig\u00adures 2 and 4, actions which caused the \nillegal writes to occur were, themselves, caused by the writes. This kind of circular causality seems \nundesirable in general, and these examples are prohibited in the Java memory model. In both of those \nexamples, a write that is data or control dependent on a read appears to occur before that read, and \nthen causes the read to return a value that justi.es that write. In Figure 2, the value written by the \nwrite is used to justify the value that it does write. In Figure 4, the occurrence of the write is used \nto justify the fact that the write will execute. Initially, x=y=0 Thread 1 Thread 2 1: r1 = x; 4: r3 \n= y; 2: r2 = r1 | 1; 5: x = r3; 3: y = r2; Is r1==r2 ==r3== 1 possible? Figure 6: Using Global Analysis \nUnfortunately, the notion of cause is a tricky one, and cannot employ the notion of data and control \ndependencies. For example, the Java memory model allows the behavior shown in Figure 5, even though that \nexample also seems to involve a case of circular causality. This behavior must be allowed, because a \ncompiler can eliminate the redundant read of a, replacing r2 =a with r2= r1, then  determine that the \nexpression r1 ==r2 is always true, eliminating the conditional branch 3, and .nally  move the write \n4: b=2 early.  After the compiler does the redundant read elimination, the assignment 4: b=2 is guaranteed \nto happen; the sec\u00adond read of a will always return the same value as the .rst. Without this information, \nthe assignment seems to cause itself to happen. With this information, there is no depen\u00addency between \nthe reads and the write. For this transformed program (illustrated in Figure 5(b)), the outcome r1 == \nr2 == r3 == 2 is sequentially consistent and could be produced on many commercial systems. Thus, dependence-breaking \noptimizations can also result in executions containing ap\u00adparent causal cycles . Figure 6 shows another \nexample in which the compiler should be allowed to perform a global analysis that elimi\u00adnates a data \ndependence. For the behavior in Figure 6 to occur, it would seem that the write to y must occur before \nthe read of x, which would seem to violate the data depen\u00addence of the write to y on the read of x. However, \na compiler could use an interthread analysis to determine that the only possible values for x and y are \n0 or 1. Knowing this, the compiler can determine that r2 is always 1. This breaks the data dependence, \nallowing a write of the constant value 1 to y before the read of x by thread 1. The Java model allows \nboth this analysis and the behavior in Figure 6. After compiler transformation Initially, a =0,b= 1 \nThread 2 5: r3=b; 6: a=r3; 2 is sequentially 4.2 Causality and the Java Memory Model The key question \nnow becomes how to formalize the no\u00adtion of causality so we restrict causal violations of the type in \nFigures 2 and 4, but allow outcomes of the type in Figure 5. We address these issues by considering when \nactions can occur in an execution. Both Figures 4 and 5 involve mov\u00ading actions (speci.cally, writes) \nearlier than they otherwise would have occurred under pure sequential consistency. One di.erence between \nthe acceptable and unacceptable results is that in Figure 5, the write that we perform early 4: b=2 would \nalso have occurred if we had carried on the execution in a sequentially consistent way, accessing the \nsame location and writing the same value. In Figure 4, the early write could never write the same value \n(42) in any se\u00adquentially consistent execution. These observations provide an intuition as to when an \naction should be allowed to occur in a legal Java execution. To justify performing the writes early in \nFigures 5 and 6, we .nd a well-behaved execution in which those writes took place, and use that execution \nto perform the justi.cation. Our model therefore builds an execution iteratively; it al\u00adlows an action \n(or a group of actions) to be committed if it occurs in some well-behaved execution that also contains \nthe actions committed so far. Identifying what constitutes a well-behaved execution is the key to our \nnotion of causality. We distinguished the early writes in Figures 2 and 4 from the early writes in Fig\u00adures \n5 and 6 by considering whether those writes could occur in a sequentially consistent execution. While \nit may appear intuitive to use the notion of a sequentially consistent exe\u00adcution to justify the occurrence \nof early writes, this notion turns out to be too relaxed in some subtle cases. After trying several possibilities \nfor formalizing the well\u00adbehaved executions that could justify early writes, we con\u00adverged on the following. \nWe observed that early execution of an action does not result in an undesirable causal cy\u00adcle if its \noccurrence is not dependent on a read returning a value from a data race. This insight led to our notion \nof a well-behaved execution : a read that is not yet committed must return the value of a write that \nis ordered before it by happens-before. We can use a well-behaved execution to justify further executions \nwhere writes occur early. Given a well-behaved execution, we may commit any of the uncommitted writes \nthat occur in it (with the same address and value). We also may commit any uncommitted reads that occur \nin such an execution, but additionally require that the read return the value of a previously committed \nwrite in both the justifying execution and the execution being justi.ed (we allow these writes to be \ndi.erent). To keep consistency among the successive justifying ex\u00adecutions, we also require that across \nall justifying execu\u00adtions, the happens-before and synchronization order rela\u00adtions among committed accesses \nremains the same, and the values returned by committed reads remain the same. Our choice of justifying \nexecutions ensures that the oc\u00adcurrence of a committed action and its value does not de\u00adpend on an uncommitted \ndata race. We build up a notion of causality, where actions that are committed earlier may cause actions \nthat are committed later to occur. This ap\u00adproach ensures that later commits of accesses involved in \ndata races will not result in an undesirable cycle. Consider again the executions with undesirable causal \nbe\u00adhaviors in Figure 2 and 4. To get the undesirable behavior, the actions in those executions must all \nread and write the value 42. This implies that some action must read or write the value 42 in an execution \nwhere reads only return val\u00adues of writes that happen-before them. However, there is no such execution, \nso we are unable to commit any of the actions in those executions. On the other hand, given the code \nin Figure 5, there is an execution where all reads return values from writes that happen-before them \nand where the write 4: b=2 occurs. This is any execution where both r1 and r2 read the value 0 for a. \nThus, we can commit 4: b=2. This allows us to commit a read of b by Thread 2 that returns the value 2. \nWe use the same justifying execution for this the read returns the value 1 in the justifying execution \n(since this does not involve a race), but can return the value 2 in the execution being justi.ed (since \nthe write of b to 2 is already committed). Using a similar procedure, we can commit the rest of the accesses. \nThus, the model allows the execution in Figure 5, but not those in Figure 4 and 2. The next section presents \nthe model, as we have discussed it, using a formal notation. The causality constraints are presented \nin Section 5.4.  5. FORMAL SPECIFICATION This section provides the formal speci.cation of the Java \nmemory model. 5.1 Actions and Executions An action a is described by a tuple (t, k, v, u), comprising: \nt -the thread performing the action k -the kind of action: volatile read, volatile write, (non\u00advolatile) \nread, (non-volatile) write, lock, unlock, spe\u00adcial synchronization action, thread divergence actions \nand external actions. Volatile reads, volatile writes, locks and unlocks are synchronization actions, \nas are special synchronization actions such as the start of a thread, the synthetic .rst or last action \nof a thread, and actions that detect that a thread has terminated, as described in Section 2.1. v -the \n(runtime) variable or monitor involved in the ac\u00adtion u -an arbitrary unique identi.er for the action \nAn execution E is described by a tuple po soswhb (P, A, ., ., W, V, . , .) comprising: P -a program A \n-a set of actions po . -program order, which for each thread t, is a total order over all actions performed \nby t in A so . -synchronization order, which is a total order over all synchronization actions in A W \n-a write-seen function, which for each read r in A, gives W (r), the write action seen by r in E. V -a \nvalue-written function, which for each write w in A, gives V (w), the value written by w in E. sw . -synchronizes-with, \na partial order over synchroniza\u00adtion actions. hb . -happens-before, a partial order over actions Note \nthat the synchronizes-with and happens-before are uniquely determined by the other components of an execu\u00ad \ntion and the rules for well-formed executions. Two of the action types require special descriptions, \nand are detailed further in Section 7. These actions are intro\u00ad duced so that we can explain why such \na thread may cause all other threads to stall and fail to make progress. external actions -An external \naction is an action that may be observable outside of an execution, and may have a result based on an \nenvironment external to the execution. An external action tuple contains an ad\u00additional component, which \ncontains the results of the external action as perceived by the thread performing the action. This may \nbe information as to the success or failure of the action, and any values read by the action. Parameters \nto the external action (e.g., which bytes are written to which socket) are not part of the exter\u00adnal \naction tuple. These parameters are set up by other actions within the thread and can be determined by \nexamining the intra-thread semantics. They are not explicitly discussed in the memory model. In non-terminating \nexecutions, not all external actions are observable. Non-terminating executions and ob\u00adservable actions \nare discussed in Section 7. thread divergence action -A thread divergence action is only performed by \na thread that is in an in.nite loop in which no memory, synchronization or external actions are performed. \nIf a thread performs a thread diver\u00adgence action, that action is followed in program order by an in.nite \nsequence of additional thread divergence actions. 5.2 De.nitions 1. De.nition of synchronizes-with. \nThe synchronizes\u00adwith order is described in Section 2.1. The source of a synchronizes-with edge is called \na release, and the destination is called an acquire. 2. De.nition of happens-before. The happens-before \norder is given by the transitive closure of the program order and the synchronizes-with order. This is \ndis\u00adcussed in detail in Section 2.1. 3. De.nition of su.cient synchronization edges. A set of synchronization \nedges is su.cient if it is the minimal set such that if the transitive closure of those edges with program \norder edges is taken, all of the happens-before edges in the execution can be deter\u00admined. This set is \nunique. 4. Restrictions of partial orders and functions. We use f|d to denote the function given by \nrestricting the domain of f to d: for all x . d, f(x)= f |d(x) and for  e all x .. d, f|d(x) is unde.ned. \nSimilarly, we use .|d e to represent the restriction of the partial order . to e the elements in d: for \nall x, y . d, x . y if and only e if x .|d y. If either x .. d or y .. d, then it is not the e case that \nx .|d y. 5.3 Well-Formed Executions We only consider well-formed executions. An execution po soswhb \nE = (P, A, ., ., W, V, ., .) is well formed if the following conditions are true: 1. Each read of a variable \nx sees a write to x. All reads and writes of volatile variables are volatile actions. For all reads r \n. A, we have W (r) . A and W (r).v = r.v. The variable r.v is volatile if and only if r is a volatile \nread, and the variable w.v is volatile if and only if w is a volatile write. 2. Synchronization order \nis consistent with pro\u00adgram order and mutual exclusion. Having syn\u00adchronization order consistent with \nprogram order im\u00adplies that the happens-before order, given by the tran\u00adsitive closure of synchronizes-with \nedges and program order, is a valid partial order: re.exive, transitive and antisymmetric. Having synchronization \norder consis\u00adtent with mutual exclusion means that on each moni\u00adtor, the lock and unlock actions are \ncorrectly nested. 3. The execution obeys intra-thread consistency. For each thread t, the actions performed \nby t in A are the same as would be generated by that thread in program-order in isolation, with each \nwrite w writing the value V (w), given that each read r sees / returns the value V (W (r)). Values seen \nby each read are de\u00adtermined by the memory model. The program order given must re.ect the program order \nin which the ac\u00adtions would be performed according to the intrathread semantics of P , as speci.ed by \nthe parts of the JLS that do not deal with the memory model. 4. The execution obeys synchronization-order \ncon\u00adsistency. Consider all volatile reads r . A. It is not  so the case that r . W (r). Additionally, \nthere must be soso no write w such that w.v = r.v and W (r) . w . r. 5. The execution obeys happens-before \nconsist\u00adency. Consider all reads r . A. It is not the case hb that r . W (r). Additionally, there must \nbe no write hbhb w such that w.v = r.v and W (r) . w . r. 5.4 Causality Requirements for Executions \nA well-formed execution po soswhb E = (P, A, ., ., W, V, ., .) is validated by committing actions from \nA. If all of the actions in A can be committed, then the execution satis.es the causality requirements \nof the Java memory model. Starting with the empty set as C0, we perform a sequence of steps where we \ntake actions from the set of actions A and add them to a set of committed actions Ci to get a new set \nof committed actions Ci+1. To demonstrate that this is reasonable, for each Ci we need to demonstrate \nan execution Ei containing Ci that meets certain conditions. Formally, an execution E satis.es the causality \nrequire\u00adments of the Java memory model if and only if there exist Sets of actions C0,C1,... such that \n C0 = \u00d8  Ci . Ci+1  A = .(C0,C1,C2,...)  such that E and (C0,C1,C2,...) obey the restrictions listed \nbelow. The sequence C0,C1,... may be .nite, ending in a set Cn = A. However, if A is in.nite, then the \nsequence C0,C1,... may be in.nite, and it must be the case that the union of all elements of this in.nite \nsequence is equal to A. poi Well-formed executions E1,..., where Ei = (P, Ai, . soiswihbi , .,Wi,Vi, \n. , .). Given these sets of actions C0,... and executions E1,..., every action in Ci must be one of the \nactions in Ei. All actions in Ci must share the same relative happens-before order and synchronization \norder in both Ei and E. Formally, 1. Ci . Ai hbihb 2. .|Ci = .|Ci soiso 3. .|Ci = .|Ci The values written \nby the writes in Ci must be the same in both Ei and E. The reads in Ci-1 need to see the same writes \nin Ei as in E (but not the reads in Ci-Ci-1) Formally, 4. Vi|Ci = V |Ci 5. Wi|Ci-1 = W |Ci-1  All reads \nin Ei that are not in Ci-1 must see writes that happen-before them. Each read r in Ci - Ci-1 must see \nwrites in Ci-1 in both Ei and E, but may see a di.erent write in Ei from the one it sees in E. Formally, \nhbi 6. For any read r . Ai - Ci-1, we have Wi(r) . r 7. For any read r . Ci -Ci-1, we have Wi(r) . Ci-1 \nand W (r) . Ci-1  Given a set of su.cient synchronizes-with edges for Ei, if there is a release-acquire \npair that happens-before an action in Ci -Ci-1, then that pair must be present in all Ej , where j = \ni. Formally, sswiswi 8. Let . be the . edges that are in the transitive hbipoisswi reduction of . but \nnot in .. We call . the su.\u00ad sswihbi cient synchronizes-with edges for Ei. If x . y . z swj and z . Ci \n- Ci-1, then x . y for all j = i. If an action y is committed, all external actions that happen\u00adbefore \ny are also committed. hbi 9. If y . Ci, x is an external action and x . y, then x . Ci. Initially, x=y=0 \nThread 1 Thread 2 r1 =x; r2 = y; y = 1; x = r2; r1 ==r2== 1 is a legal behavior Figure 7: A Standard \nReordering Final First First Sees Action Value Committed In Final Value In x = 0 0 C1 E1 y = 0 0 C1 E1 \ny = 1 1 C1 E1 r2 = y 1 C2 E3 x = r2 1 C3 E3 r1 = x 1 C4 E Figure 8: Table of commit sets for Figure \n7  6. EXAMPLE As a simple example of how the memory model works, consider Figure 7. Note that there \nare initially writes of the default value 0 to x and y. We wish to get the result r1 == r2 == 1, which \ncan occur if a compiler reorders the statements in Thread 1. This result is consistent with the happens-before \nmemory model, so we only have to ensure that it complies with the causality rules in Section 5.4. The \nset of actions C0 is the empty set. Therefore, accord\u00ading to Rule 6, in execution E1, all reads must \nsee values of writes that happen-before them. That is, in E1, both reads must see the value 0. We .rst \ncommit the initial writes of 0 to x and y, as well as the write of 1 to y by Thread 1; these writes are \ncontained in the set C1. We wish the action r2 = y to see the value 1. C1 cannot contain this action \nseeing this value: neither write to y had been committed. C2 may contain this action; however, the read \nof y must return 0 in E2, because of Rule 6. Execution E2 is therefore identical to E1. In E3, by Rule \n7, r2 = y can see any con.icting write that occurs in C2 (as long as that write is happens-before consistent). \nThis action can now see the write of 1 to y in Thread 1, which was committed in C1. We commit one additional \naction in C3: a write of 1 to x by x = r2. C4, as part of E4, contains the read r1 = x; it still sees \n0, because of Rule 6. In our .nal execution E = E5, however, Rule 7 allows r1 =x to see the write of \n1 to x that was committed in C3.  7. OBSERVABLE BEHAVIOR AND NONTERMINATING EXECUTIONS For programs \nthat always terminate in some bounded .\u00adnite period of time, their behavior can be understood (in\u00adformally) \nsimply in terms of their allowable executions. For programs that can fail to terminate in a bounded amount \nof time, more subtle issues arise. The observable behavior of a program is de.ned by the .nite sets of \nexternal actions that the program may perform. A program that, for example, simply prints Hello forever \nis described by a set of behaviors that for any non-negative integer i, includes the behavior of printing \nHello i times. Termination is not explicitly modeled as a behavior, but Initially, v is volatile and \nv = false Thread 1 Thread 2 while (!v); v = true; System.out.println(\"Thread 1 done\"); System.out.println(\"Thread \n2 done\"); If we observe print message by thread 2, Thread 1 must see write to v, print its message and \nterminate. But program can also be observed to hang and not print any messages. Figure 9: Fairness Example \na program can easily be extended to generate an additional external action executionTermination that \noccurs when all threads have terminated. We also de.ne a special hang action. If a behavior is de\u00adscribed \nby a set of external actions including a hang action, it indicates a behavior where after the (non-hang) \nexternal actions are observed, the program can run for an unbounded amount of time without performing \nany additional external actions or terminating. Programs can hang: if all non-terminated threads are \nblocked, and at least one such blocked thread exists, or  if the program can perform an unbounded number \nof actions without performing any external actions.  A thread can be blocked in a variety of circumstances, \nsuch as when it is attempting to acquire a lock or perform an external action (such as a read) that depends \non an ex\u00adternal data. If a thread is in such a state, Thread.getState will return BLOCKED or WAITING. \nAn execution may result in a thread being blocked inde.nitely and the execution not terminating. In such \ncases, the actions generated by the blocked thread must consist of all actions generated by that thread \nup to and including the action that caused the thread to be blocked inde.nitely, and no actions that \nwould be gen\u00aderated by the thread after that action. To reason about observable behaviors, we need to \ntalk about sets of observable action. If O is a set of observable actions for E, then set O must be a \nsubset of A, and must contain only a .nite number of actions, even if A contains an in.nite number of \nactions. Furthermore, if an action y . O, hbso and either x . y or x . y, then x . O. Note that a set \nof observable actions is not restricted to containing external actions. Rather, only external actions \nthat are in a set of observable actions are deemed to be observable external actions. A behavior B is \nan allowable behavior of a program P if and only if B is a .nite set of external actions and either \nThere exists an execution E of P , and a set O of ob\u00adservable actions for E, and B is the set of external \nactions in O (if any threads in E end in a blocked state and O contains all actions in E, then B may \nalso contain a hang action), or  There exists a set O of actions such that  B consists of a hang action \nplus all the external actions in O and  for all K =|O|, there exists an execution E of P and a set of \nactions O. such that:   * Both O and O. are subsets of A that ful.ll the requirements for sets of observable \nactions. * O . O. . A *|O.|= K * O. - O contains no external actions Note that a behavior B does not \ndescribe the order in which the external actions in B are observed, but other (im\u00adplicit and unstated) \nconstraints on how the external actions are generated and performed may impose such constraints. 7.1 \nDiscussion The Java language speci.cation does not guarantee pre\u00ademptive multithreading or any kind of \nfairness: there is no hard guarantee that any thread will surrender the CPU and allow other threads to \nbe scheduled. The lack of such a guarantee is partially due to the fact that any such guaran\u00adtee would \nbe complicated by issues such as thread priorities and real-time threads (in Real-time Java virtual machine \nim\u00adplementations). Most Java virtual machine implementations will provide some sort of fairness guarantee, \nbut the details are implementation speci.c and are treated as a quality of service issue, rather than \na rigid requirement. However, there are some limitations on compiler transfor\u00admations that reduce fairness. \nFor example, in Figure 9, if we observe the print message from Thread 2, and no threads other than Threads \n1 and 2 are running, then Thread 1 must see the write to v, print its message and terminate. This pre\u00advents \nthe compiler from hoisting the volatile read of v out of the loop in Thread 1. The fact that Thread 1 \nmust terminate if the print by Thread 2 is observed follows from the rules on observable actions. If \nthe print by Thread 2 is in a set of observable actions O, then the write to v and all reads of v that \nsee the value 0 must also be in O. Additionally, the program cannot perform an unbounded amount of additional \nactions that are not in O. Therefore, the only observable behavior of this program in which the program \nhangs (runs forever without performing additional external actions) is one in which it performs no observable \nexternal actions other than hanging. This includes the print action. It follows from this that in Figure \n9, the instructions in Thread 2 cannot be arbitrarily reordered. If this reordering were performed, it \nwould be possible for Thread 2 to print, after which there could be a context switch to Thread 1; Thread \n1 would never terminate. Note that the program in Figure 9 has only .nite execu\u00adtions. Since there are \nno blocking operations in Thread 2, it must run to completion in any execution. However, we want to allow \nthis program to be executed such that it is not observed to terminate (which might happen if Thread 1 \nwere run and never performed a context switch). One of the things we needed to accomplish in the formalization \nof observable behavior was that no distinction should be made Initially, x == y == z == 0  Must not \nallow r1 == r2 == r3 == 1 Figure 11: A variant bait-and-switch behavior between a program that can run \nfor a .nite, but unbounded, number of steps and a program that can run for an in.nite number of steps. \nFor example, if accesses to volatile .elds in Figure 9 were implemented by surrounding them with locks, \nThread 2 could block and never acquire the lock. In this situation, the program would execute in.nitely. \n  8. SURPRISING AND CONTROVERSIAL BEHAVIORS Many of the requirements and goals for the Java memory model \nwere straightforward and non-controversial (e.g., cor\u00adrectly synchronized programs exhibit only sequentially \ncon\u00adsistent behavior). Other decisions about the requirements generated quite a bit of discussion; the \n.nal decision often came down to a matter of taste and preference rather than any concrete requirement. \nOne of the most subtle of these is that in programs with data races, certain kinds of behaviors were \ndeemed to display an unacceptable bait-and-switch circular reasoning. One such example is shown in Figure \n10, in which it was decided that the behavior r1==r2==r3== 1 was unacceptable. In an execution that exhibits \nthis behavior, only Threads 3 and 4 read and write to x and y. Thus, this seems to be an out-of-thin-air \nexample, as in Figure 2. What di.erenti\u00adates this example from Figure 2 is that in Figure 10, Thread \n2 might have, but didn t write 1 to x. After much discussion within the Java community, it was decided \nthat allowing such behaviors would make reasoning about the safety properties of programs too di.cult. \nRea\u00adsoning about the behaviors of Threads 3 and 4 in Figure 10 requires reasoning about Threads 1 and \n2, even when trying to reason about executions in which Threads 1 and 2 did not interact with Threads \n3 and 4. Given that the behavior in Figure 10 is unacceptable, the behavior in Figure 11 is very similar, \nand it seems reasonable to prohibit it as well (the behaviors in Figures 10 and 11 are prohibited by \nthe Java memory model). Figure 12 shows a code fragment very similar to that of Figure 11. However, for \nthe code in Figure 12, we must allow the behavior that was prohibited in Figure 11. We do this because \nthat behavior can result from well understood and reasonable compiler transformations. The compiler \ncan deduce that the only legal values for x and y are 0 and 1.  The compiler can then replace the read \nof x on line 4  Initially, x == y == 0 Thread 1 Thread 2 1: r1 = x 6: r3 = y 2: if (r1 == 0) 7: x = \nr3 3: x = 1 4: r2 = x 5: y = r2 Compiler transformations can result in r1 == r2 == r3 == 1 Figure 12: \nBehavior that must be allowed with a read of 1, because either 1 was read from x on line 1 and there \nwas no intervening write, or  0 was read from x on line 1, 1 was assigned to x on line 3, and there \nwas no intervening write.  Via forward substitution, the compiler is allowed to transform line 5 to \ny=1. Because there are no de\u00adpendencies, this line can be made the .rst action per\u00adformed by Thread 1. \nAfter these transformations are performed, a sequentially consistent execution of the program will result \nin the behav\u00adior in question. The fact that the behavior in Figure 11 is prohibited and the behavior \nin Figure 12 is allowed is, perhaps, surpris\u00ading. We could derive Figure 12 from Figure 11 by inlining \nThreads 1 and 2 into a single thread. This shows that, in general, thread inlining is not legal under \nthe Java memory model. This is an example where adding happens-before relation\u00adships can increase the \nnumber of allowed behaviors. This property can be seen as an extension of the way in which causality \nis handled in the Java memory model (as discussed in Section 4.2). The happens-before relationship is \nused to express causation between two actions; if an additional happens-before relationship is inserted, \nthe causal relation\u00adships change. Other approaches may exist to de.ning an acceptable memory model for \na programming language. However, much energy was expended on the Java memory model, and more than a dozen \ndi.erent approaches were considered. The ap\u00adproach presented in this paper, part of the Java 5.0 standard, \nwas found to be the only acceptable approach. 9. FORMALIZING THE IMPACT OF THE MODEL Section 2 discussed \nsome of the properties we wanted the model to re.ect. This section discusses how those properties are \nrealized. 9.1 Considerations for Implementors This section discusses some of the key ways in which the \nnew Java memory model impacts the decisions that must be taken by Java platform implementors. Remember \nfrom Section 2, that one of our key informal requirements was that the memory model allow as many optimizations \nas it can. 9.1.1 Control Dependence The new Java memory model makes changes that are sub\u00adtle, but deep, \nto the way in which implementors must reason about Java programs. For example, the standard de.nition \nof control dependence assumes that execution always pro\u00adceeds to exit. This must not be casually assumed \nin multi\u00adthreaded programs. Consider the program in Figure 13. Under the traditional de.nitions of control \ndependence, neither of the writes in either thread are control dependent on the loop guards. This might \nlead a compiler to decide that the writes could be reordered to before the loops. However, this would \nbe illegal in Java. This program is, in fact, correctly synchronized: in all sequentially consistent \nexecutions, neither thread writes to shared variables and there are no data races (this .gure is very \nsimilar to Figure 4). A compiler must not reorder the writes in that example. The notion of control dependence \nthat correctly encapsu\u00adlates this is called weak control dependence [32] in the con\u00adtext of program veri.cation. \nThis property has also been restated as loop control dependence [10] in the context of program analysis \nand transformation. 9.1.2 Semantics Allow Reordering We mentioned earlier that a key notion for program \nop\u00adtimization was that of reordering. We demonstrated in Fig\u00adure 1 that standard compiler reorderings, \nunobservable in single threaded programs, can have observable e.ects in mul\u00adtithreaded programs. However, \nreorderings are crucial in many common code optimizations; e.g., instruction schedul\u00ading, register allocation, \ncommon sub-expression elimination and redundant read elimination. In this section, we demonstrate that \nmany of the reorder\u00adings necessary for these optimizations are legal. This is not a complete list of \nlegal reorderings; others can be de\u00adrived from the model. However, this demonstrates a sample of common \nreorderings, used for many common optimiza\u00adtions. Speci.cally, we demonstrate the legality of reordering \ntwo independent actions when doing so does not change the happens-before relationship for any other actions. \nTheorem 1. Consider a program P and the program P . that is obtained from P by reordering two adjacent \nstate\u00adments sx and sy. Let sx be the statement that comes before sy in P , and after sy in P . . The \nstatements sx and sy may be any two statements such that reordering sx and sy doesn t eliminate any \ntransitive happens-before edges in any valid execution (it will re\u00adverse the direct happens-before edge \nbetween the actions generated by sx and sy)  Reordering sx and sy does not hoist an action above an \nin.nite loop  sx and sy are not con.icting accesses to the same vari\u00adable,  sx and sy are not both \nsynchronization actions or ex\u00adternal actions, and  the intra-thread semantics of sx and sy allow reorder\u00ading \n(e.g., sx doesn t store into a register that is read by sy).  Transforming P into P . is a legal program \ntransformation. Proof: Assume that we have a valid execution E. of program P . . It has a legal set of \nbehaviors B. . To show that the trans\u00adformation of P into P . is legal, we need to show that there is \na valid execution E of P that has the same observable behaviors as E. . Let x be the action generated \nby sx and y be the action generated by sy. If x and y are executed multiple times, we repeat this analysis \nfor each repetition. The execution E. has a set of observable actions O., and the execution E has a set \nof observable actions O. If O hb includes x, O must also include y because x . y in E. If O. does not \ninclude y (and therefore y is not an external action, as it must not take place after an in.nite series \nof actions), we can use O as the set of observable actions for E. instead: they induce the same behavior. \nSince E. is legal, we have E0. ,E1 . ..., a sequence of ex\u00adecutions that eventually justi.es E. doesn \nt have any E0 . committed actions and .i, 0 = i, Ei . is used to justify the additional actions that \nare committed to give Ei. +1. We will show that we can use Ei = Ei . to show that E = E. is a legal execution \nof P . If x and y are both uncommitted in Ei. , the happens\u00adbefore ordering between x and y doesn t change \nthe possible behaviors of actions in Ei and Ei. . Any action that happens\u00adbefore x or y happens-before \nboth of them. If either x or y happens-before an action, both of them do (excepting, of course, x and \ny themselves). Thus, the reordering of x and y can t a.ect the write seen by any uncommitted read. Similarly, \nthe reordering doesn t a.ect which (if any) in\u00adcorrectly synchronized write a read can be made to see \nwhen the read is committed. If Ei . is used to justify committing x in Ei. +1, then Ei may be used to \njustify committing x in Ei+1. Similarly for y. If one or both of x or y is committed in Ei., it can also \nbe committed in Ei, without behaving any di.erently, with one caveat. If y is a lock or a volatile read, \nit is possible that committing x in Ei . will force some synchronization actions that happen-before y \nto be committed in Ei. . However, we are allowed to commit those actions in Ei, so this does not a.ect \nthe existence of Ei. Thus, the sequences of executions used to justify E. will also justify E, and the \nprogram transformation is legal. 9.1.3 Other Code Transformations Although the Java memory model is \nnot de.ned in terms of which transformations are legal, it is possible to derive the legality of a transformation \nbased on the model. For example, synchronization on thread local objects can be ignored or removed altogether \n(the caveat to this is the fact that invocations of methods like wait and notify have to obey the correct \nsemantics for example, even if the lock is thread local, it must be acquired when perform\u00ading a wait), \n redundant synchronization (e.g., when a synchronized method is called from another synchronized method \non the same object) can be ignored or removed,  volatile .elds of thread local objects can be treated \nas normal .elds.  Initially, x==y ==0 Thread 1 do { r1 = x; } while (r1 == 0); } while (r2 == 0); y \n= 42; x = 42; Correctly synchronized, so non-termination is the only legal behavior Figure 13: Correctly \nSynchronized Program  9.2 Considerations for Programmers The most important property of the memory model \nthat is provided for programmers is the notion that if a program is correctly synchronized, it is unnecessary \nto worry about re\u00adorderings. In this section, we prove this property holds of the Java memory model. \nFirst, we prove a lemma that shows that when each read sees a write that happens-before it, the resulting \nexecution behaves in a sequentially consistent way. We then show that reads in executions of correctly \nsyn\u00adchronized programs can only see writes that happen-before them. Thus, by the lemma, the resulting \nbehavior of such programs is sequentially consistent. 9.2.1 Correctly synchronized programs exhibit only \nsequentially consistent behaviors Lemma 2. Consider an execution E of a correctly syn\u00adchronized program \nP that is legal under the Java memory model. If, in E, each read sees a write that happens-before it, \nE has sequentially consistent behavior. Proof: Since the execution is legal under the memory model, the \nexecution is happens-before consistent and synchronization order consistent. A topological sort on the \nhappens-before edges of the ac\u00adtions in an execution gives a total order consistent with pro\u00adgram order \nand synchronization order. Let r be the .rst read in E that doesn t see the most recent con.icting write \nw in the sorted order but instead sees w . . Let the topological sort of E be aw.\u00dfw.rd. Let aw.\u00dfw.r.d. \nbe the topological sort of an execution E. E. . is obtained exactly as E, except that instead of r, it \nperforms the action r ., which is the same as r except that it sees w; d. is any sequentially consistent \ncompletion of the program such that each read sees the previous con.icting write. The execution E. is \nsequentially consistent, and it is not hbhb the case that w . . w . r, so P is not correctly synchro\u00adnized. \nThus, no such r exists and the program has sequentially consistent behavior. Theorem 3. If an execution \nE of a correctly synchro\u00adnized program is legal under the Java memory model, it is also sequentially \nconsistent. Proof: By Lemma 2, if an execution E is not sequen\u00adtially consistent, there must be a read \nr that sees a write w such that w does not happen-before r. The read must be committed, because otherwise \nit would not be able to see a write that does not happen-before it. There may be multi\u00adple reads of this \nsort; if so, let r be the .rst such read that was committed. Let Ei be the execution that was used to \njustify committing r. The relative happens-before order of committed actions and actions being committed \nmust remain the same in all executions considering the resulting set of committed ac\u00ad hb tions. Thus, \nif we don t have w . r in E, then we didn t hb have w . r in Ei when we committed r. Since r was the \n.rst read to be committed that doesn t see a write that happens-before it, each committed read in Ei \nmust see a write that happens-before it. Non-committed reads always sees writes that happens-before them. \nThus, each read in Ei sees a write that happens-before it, and there is a write w in Ei that is not ordered \nwith respect to r by happens-before ordering. A topological sort of the actions in Ei according to their \nhappens-before edges gives a total order consistent with pro\u00adgram order and synchronization order. This \ngives a total order for a sequentially consistent execution in which the con.icting accesses w and r \nare not ordered by happens\u00adbefore edges. However, Lemma 2 shows that executions of correctly synchronized \nprograms in which each read sees a write that happens-before it must be sequentially consistent. Therefore, \nthis program is not correctly synchronized. This is a contradiction. 10. RELATED WORK Hardware architectures \nhave motivated most prior work on memory models. Lamport provides one of the earliest dis\u00adcussions of \nmemory models [25], which provides the widely used de.nition for sequential consistency. Several relaxed \nmodels have been proposed in academia and for commercial hardware [4, 6, 12, 17, 19, 31, 40, 43]. Adve \nand Ghara\u00adchorloo provide a primer for this work [3]. Adve and Hill [4, 6] and Gharachorloo et al. [17] \n.rst formalized the prop\u00aderty of sequential consistency for data-race-free (or properly\u00adlabeled [17]) \nprograms for memory models. The data-race\u00adfree work also observed that models such as happens-before \nare insu.cient to provide this property [6, 7]. There has also been substantial work on relaxed models \nin the area of runtime systems for software distributed shared\u00admemory [9, 22]. Much of this work has \nbuilt on the hardware models. Our work is primarily motivated by requirements for pro\u00adgramming languages, \nwhich di.er signi.cantly from those for hardware-driven memory models. Speci.cally, the out\u00adof-thin-air \nrequirement stems from security and safety prop\u00aderties of programming languages. Formalizing this require\u00adment \nin a way that would not prohibit compiler transforma\u00adtions was particularly challenging. In particular, \nthe amount of analysis and transformation that a compiler might per\u00adform is essentially unlimited. Thus, \ndevising a memory model that doesn t interfere with desirable compiler trans\u00adformations is signi.cantly \nharder; it is also undesirable, as it would require a major restructuring of current compilers and incur \na signi.cant performance penalty. Not surprisingly, all of the above hardware-driven mem\u00adory models either \nallow unacceptable out-of-thin-air behav\u00adior for incorrectly synchronized programs (e.g., allow the be\u00adhavior \nshown in Figure 2), or they do not allow dependence\u00adbreaking transformations as shown in Figures 5 and \n6. In programming languages such as C [23] and C++ [41], threads are not part of the language speci.cation. \nInstead, libraries such as POSIX threads (pthreads) [27] support multi-threading. Since the C/C++ spec \ndoesn t talk about threads and the pthreads speci.cation doesn t talk about the C/C++ language features, \nvarious elements of the se\u00admantics can be left unspeci.ed. For example, it is unclear if double-checked \nlocking [37] works in C as described in vari\u00adous publications or if there is a way of modifying it to \nwork. In contrast, the Java speci.cation makes it clear that stan\u00addard double-checked locking idiom only \nworks if the checked .eld is volatile. After the .aws in the original Java memory model were pointed \nout [33], a number of proposals for replacement memory models for Java emerged. Most of these are based \naround techniques originally used for hardware memory mod\u00adels. The various proposals for revising the \nJava memory model di.ered in a number of details, such as whether they al\u00adlowed removal of synchronization \non thread-local objects and whether they de.ned special semantics for .nal .elds. Due to space limitations, \nwe are unable to enumerate all of the di.erences between this work and previous proposals. However, none \nof the previous proposals for revising the Java memory model adequately handled cases involving the apparent \nreordering of data and control dependences, such as shown in Figures 5 and 6. This de.ciency was even \nin previous work [28] by two of the authors of this paper. The SC-model [5] handled Figures 5 and 6, \nbut not Figure 10. Maessen, Arvind and Shen [8] present an operational se\u00admantics for Java threads based \non the Commit / Reconcile / Fence protocol [39]. The CRF semantics prohibits reorder\u00ading of control and \ndata dependences, and thus prohibits the behaviors in Figures 5 and 6. Yang, Gopalakrishnan and Lindstrom \n[44, 45] attempt to characterize the approaches taken in [8, 28, 29] using a sin\u00adgle, uni.ed memory model \n(called UMM). They also [44] use the same notation to propose their own memory model for Java. In their \nsemantics, an action can only be performed early if the action is guaranteed to occur in all executions. \nThis disallows a variety of possible optimizations, including the transformation seen in 5 and 6. Kotrajaras \n[24] proposes a memory model for Java that is based on the original, .awed model. It su.ers not only \nfrom the complexity of that model, but from its reliance on a single, global shared memory. Saraswat \n[36] presents a memory model for Java based on solving a system of constraints between actions for a \nunique .xed point, rather than depending on control and data de\u00adpendence. Thus, Saraswat s model allows \nthe behavior in Figure 5; given which writes are seen by each read, there is a unique .xed point solution. \nHowever, Saraswat s model does not allow the behavior in Figure 6; in that example, given the binding \nof reads to writes, there are multiple .xed-point solutions. The ECMA speci.cation for the Common Language \nIn\u00adfrastructure (CLI) provides a memory model [13]. However, it is vague and informal; as a result it \nseems impossible to determine whether that model allows or disallows behaviors such as shown in Figures \n2, 4 6 and 10 13. It is also worth noting that some of the Microsoft engineers have published articles \n[11] in which they claim that the CLI speci.cation is too relaxed, and that they have written code as \npart of Microsoft s core libraries that won t work according to the ECMA spec. 11. CONCLUSION In this \npaper, we have outlined the necessary properties for a programming language memory model, and outlined \nhow those properties can be achieved. The resulting model balances two crucial needs: it allows implementors \n.exibility in their ability to perform code transformations and opti\u00admizations, and it also provides \na clear and simple program\u00adming model for those writing concurrent code. The model meets the needs of \nprogrammers in several key ways: For data-race-free programs, it allows programmers to reason about \ntheir programs using the simple seman\u00adtics of sequential consistency, oblivious to any compiler and hardware \ntransformations.  It provides a clear de.nition for the behavior of pro\u00adgrams in the presence of data \nraces, including the most comprehensive treatment to date of the dangers of causality and how they can \nbe avoided.  This paper clari.es and formalizes these needs, balanc\u00ading them carefully with a wide variety \nof optimizations and program transformations commonly performed by compil\u00aders and processor architectures. \nIt also provides proof tech\u00adniques to ensure that the model re.ects this balancing act accurately and \ncarefully. It is this balance that has given us the necessary con.dence to use this model as the foundation \nfor concurrency in the Java programming language. Acknowledgments Many other people have made signi.cant \ncontributions to this work, all those who participated in discussions on the JMM mailing list. The total \nset of people that have con\u00adtributed is far too large to enumerate, but certain people made particularly \nsigni.cant contributions, including Doug Lea, Victor Luchangco, Jan-Willem Maessen, Hans Boehm, Joseph \nBowbeer, and David Holmes. We also thank Mark Hill for comments on this paper. 12. REFERENCES [1] Ada \nJoint Program O.ce. Ada 95 Rationale. Intermetrics, Inc., Cambridge, Massachusetts, 1995. [2] Sarita \nAdve. Designing Memory Consistency Models for Shared-Memory Multiprocessors. PhD thesis, University of \nWisconsin, Madison, December 1993. Ph.D. Thesis. [3] Sarita Adve and Kourosh Gharachorloo. Shared memory \nconsistency models: A tutorial. IEEE Computer, 29(12):66 76, 1996. [4] Sarita Adve and Mark Hill. Weak \nordering A new de.nition. In Proc. of the 17th Annual Int l Symp. on Computer Architecture (ISCA 90), \npages 2 14, 1990. [5] Sarita V. Adve. The SC-memory model for Java, 2004. http://www.cs.uiuc.edu/~sadve/jmm. \n [6] Sarita V. Adve and Mark D. Hill. A uni.ed formalization of four shared-memory models. IEEE Trans. \non Parallel and Distributed Systems, 4(6):613 624, June 1993. [7] Sarita V. Adve and Mark D. Hill. Su.cient \nconditions for implementing the data-race-free-1 memory model. Technical Report #1107, Computer Sciences \nDepartment, University of Wisconsin-Madison, September 1992. [8] Arvind, Jan-Willem Maessen, and Xiaowei \nShen. Improving the Java memory model using CRF. In OOPSLA, pages 1 12, October 2000. [9] John K. Bennett, \nJohn B. Carter, and Willy Zwanaepoel. Adaptive software cache management for distributed shared memory \narchitectures. In Proc. 17th Annual Intl. Symp. on Computer Architecture, May 1990. [10] Gianfranco Bilardi \nand Keshav Pingali. A Framework for Generalized Control Dependence. In ACM SIGPLAN 1996 Conference on \nProgramming Language Design and Implementation, Philadelphia, Pennsylvania, United States, June 1996. \n[11] Christopher Brumme. C# memory model. http://blogs.msdn.com/cbrumme/archive/2003/ 05/17/51445.apx. \n[12] Michel Dubois, Christoph Scheurich, and Faye A. Briggs. Memory access bu.ering in multiprocessors. \nIn Proc. 13th Ann. Intl. Symp. on Computer Architecture, pages 434 442, June 1986. [13] ECMA. Common \nLanguage Infrastructure (CLI), December 2002. http://www.ecma\u00adinternational.org/publications/standards/Ecma-335.htm. \n[14] Kourosh Gharachorloo. Memory consistency models for shared-memory multiprocessors. PhD thesis, Stanford \nUniversity, 1996. [15] Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hennessy, and Mark \nD. Hill. Programming for di.erent memory consistency models. Journal of Parallel and Distributed Computing, \n15(4):399 407, August 1992. [16] Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. Two techniques \nto enhance the performance of memory consistency models. In Proc. Intl. Conf. on Parallel Processing, \npages I355 I364, 1991. [17] Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop \nGupta, and John Hennessy. Memory consistency and event ordering in scalable shared-memory multiprocessors. \nIn Proc. 17th Ann. Intl. Symp. on Computer Architecture, pages 15 26, May 1990. [18] James Gosling, Bill \nJoy, and Guy Steele. The Java Language Speci.cation. Addison Wesley, 1996. [19] IBM System/370 Principles \nof Operation, May 1983. Publication Number GA22-7000-9, File Number S370-01. [20] Java Speci.cation Request \n(JSR) 133. Java Memory Model and Thread Speci.cation Revision, 2004. http: //jcp.org/jsr/detail/133.jsp. \n[21] The Java memory model. Mailing list and web page. http://www.cs.umd.edu/ users/ pugh/ java/ memoryModel. \n[22] Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. Lazy release consistency for software distributed \nshared memory. In Proc. 19th Ann. Intl. Symp. on Computer Architecture, pages 13 21, 1992. [23] B. W. \nKernighan and D. M. Ritchie. The C Programming Language. Prentice Hall, second edition, 1988. [24] Vishnu \nKotrajaras. Towards an Improved Memory Model for Java. PhD thesis, Department of Computing, Imperial \nCollege, August 2001. [25] Leslie Lamport. Time, clocks, and the ordering of events in a distributed \nsystem. CACM, 21(7):558 564, 1978. [26] Leslie Lamport. How to make a multiprocessor computer that correctly \nexecutes multiprocess programs. IEEE Transactions on Computers, 9(29):690 691, 1979. [27] Bil Lewis and \nDaniel J. Berg. Multithreaded programming with pthreads. Sun Microsystems, 2550 Garcia Avenue, Mountain \nView, CA 94043, USA, 1998. [28] Jeremy Manson and William Pugh. Core semantics of multithreaded Java. \nIn ACM Java Grande Conference, June 2001. [29] Jeremy Manson and William Pugh. Semantics of Multithreaded \nJava. Technical Report CS-TR-4215, Dept. of Computer Science, University of Maryland, College Park, March \n2001. [30] Jeremy Manson and William Pugh. Requirements for Programming Language Memory Models. In PODC \nWorkshop on Concurrency and Synchronization in Java Programs, St. John s, Newfoundland, Canada, July \n2004. [31] C. May et al., editors. The PowerPC Architecture: A Speci.cation for a New Family of RISC \nProcessors. Morgan Kaufmann, San Francisco, 1994. [32] Andy Podgurski and Lori Clarke. A formal model \nof program dependences and its implications for software testing debugging and maintenance. IEEE Transactions \non Software Engineering, 1990. [33] William Pugh. Fixing the Java memory model. In ACM Java Grande Conference, \nJune 1999. [34] William Pugh. The Java memory model is fatally .awed. Concurrency: Practice and Experience, \n12(1):1 11, 2000. [35] Parthasarathy Ranganathan, Vijay S. Pai, and Sarita V. Adve. Using speculative \nretirement and larger instruction windows to narrow the performance gap between memory consistency models. \nIn Proceedings of the 9th Annual ACM Symposium on Parallel Algorithms and Architectures, pages 199 210, \n1997. [36] Vijay Saraswat. Concurrent Constraint-based Memory Machines: A framework for Java Memory Models. \nTechnical report, IBM TJ Watson Research Center, March 2004. [37] Douglas Schmidt and Tim Harrison. Double-checked \nlocking: An optimization pattern for e.ciently initializing and accessing thread-safe objects. In 3rd \nAnnual Pattern Languages of Program Design Conference, 1996. [38] Dennis Shasha and Marc Snir. E.cient \nand correct execution of parallel programs that share memory. ACM Trans. on Programming Languages and \nSystems, 10(2):282 312, April 1988. [39] Xiaowei Shen, Arvind, and Larry Rudolph. Commit-reconcile &#38; \nfences (CRF): A new memory model for architects and compiler writers. Proceedings of the 26th International \nSymposium on Computer Archite cture, 1999. [40] R. L. Sites and R. T. Witek, editors. Alpha AXP Architecture \nReference Manual. Digital Press, Boston, 1995. 2nd edition. [41] Bjarne Stroustrup. The C++ Programming \nLanguage. Addison-Wesley Longman, Reading Mass. USA, 3rd edition, 1997. [42] Z. Sura, C.-L. Wong, X. \nFang, J. Lee, S. Midki., and D. Padua. Automatic implementation of programming language consistency models. \nIn Proc. of the 15th International Workshop on Languages and Compilers for Parallel Computing (LCPC 02), \n2002. To appear Lecture Notes in Computer Science, Springer-Verlag. [43] David Weaver and Tom Germond. \nThe SPARC Architecture Manual, version 9. Prentice-Hall, 1994. [44] Yue Yang, Ganesh Gopalakrishnan, \nand Gary Lindstrom. Specifying Java thread semantics using a uniform memory model. In ACM Java Grande \nConference, November 2001. [45] Yue Yang, Ganesh Gopalakrishnan, and Gary Lindstrom. Formalizing the \nJava memory model for multithreaded program correctness and optimization. Technical Report UUCS-02-011, \nUniversity of Utah, April 2002.   \n\t\t\t", "proc_id": "1040305", "abstract": "This paper describes the new Java memory model, which has been revised as part of Java 5.0. The model specifies the legal behaviors for a multithreaded program; it defines the semantics of multithreaded Java programs and partially determines legal implementations of Java virtual machines and compilers.The new Java model provides a simple interface for correctly synchronized programs -- it guarantees sequential consistency to data-race-free programs. Its novel contribution is requiring that the behavior of incorrectly synchronized programs be bounded by a well defined notion of causality. The causality requirement is strong enough to respect the safety and security properties of Java and weak enough to allow standard compiler and hardware optimizations. To our knowledge, other models are either too weak because they do not provide for sufficient safety/security, or are too strong because they rely on a strong notion of data and control dependences that precludes some standard compiler transformations.Although the majority of what is currently done in compilers is legal, the new model introduces significant differences, and clearly defines the boundaries of legal transformations. For example, the commonly accepted definition for control dependence is incorrect for Java, and transformations based on it may be invalid.In addition to providing the official memory model for Java, we believe the model described here could prove to be a useful basis for other programming languages that currently lack well-defined models, such as C++ and C#.", "authors": [{"name": "Jeremy Manson", "author_profile_id": "81100102332", "affiliation": "University of Maryland, College Park, MD", "person_id": "PP14045623", "email_address": "", "orcid_id": ""}, {"name": "William Pugh", "author_profile_id": "81100057068", "affiliation": "University of Maryland, College Park, MD", "person_id": "PP15020758", "email_address": "", "orcid_id": ""}, {"name": "Sarita V. Adve", "author_profile_id": "81100524186", "affiliation": "University of Illinois at Urbana-Champaign, Urbana-Champaign, IL", "person_id": "PP14181820", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1040305.1040336", "year": "2005", "article_id": "1040336", "conference": "POPL", "title": "The Java memory model", "url": "http://dl.acm.org/citation.cfm?id=1040336"}