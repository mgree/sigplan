{"article_publication_date": "10-29-2013", "fulltext": "\n Detecting API Documentation Errors Hao Zhong1,2 Zhendong Su2 1Institute of Software, Chinese Academy \nof Sciences, China 2University of California, Davis, USA zhonghao@nfs.iscas.ac.cn, su@cs.ucdavis.edu \nAbstract When programmers encounter an unfamiliar API library, they often need to refer to its documentations, \ntutorials, or discussions on de\u00advelopment forums to learn its proper usage. These API documents contain \nvaluable information, but may also mislead programmers as they may contain errors (e.g., broken code \nnames and obsolete code samples). Although most API documents are actively main\u00adtained and updated, studies \nshow that many new and latent errors do exist. It is tedious and error-prone to .nd such errors manually \nas API documents can be enormous with thousands of pages. Existing tools are ineffective in locating \ndocumentation errors because tra\u00additional natural language (NL) tools do not understand code names and \ncode samples, and traditional code analysis tools do not under\u00adstand NL sentences. In this paper, we \npropose the .rst approach, DOCREF, speci.cally designed and developed to detect API doc\u00adumentation errors. \nWe formulate a class of inconsistencies to in\u00addicate potential documentation errors, and combine NL and \ncode analysis techniques to detect and report such inconsistencies. We have implemented DOCREF and evaluated \nits effectiveness on the latest documentations of .ve widely-used API libraries. D OCREF has detected \nmore than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have \nalready been con.rmed and .xed, after we reported them. Categories and Subject Descriptors D.2.2 [Design \nTools and Techniques]: Software libraries; D.2.7 [Distribution, Maintenance, and Enhancement]: Documentation; \nI.7.2 [Document Prepara\u00adtion]: Hypertext/hypermedia General Terms Documentation, Experimentation, Reliability \nKeywords API documentation error, Outdated documentation 1. Introduction Programmers increasingly rely \non Application Programming Inter\u00adface (API) libraries to speed up development [29]. However, stud\u00adies \n[12, 31] show that it is dif.cult to learn and use unfamiliar APIs, partly due to poorly designed or \npoorly documented API libraries. To learn unfamiliar APIs, programmers often read various API doc\u00adumentations \nsuch as API references, tutorials, wikis, and forum dis\u00adcussions [26]. API documentations are useful \nfor programmers to Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, \nIndianapolis, Indiana, USA. Copyright c . 2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509523 \n understand API usage, but may mislead programmers, because they can also contain various documentation \nerrors [22, 35]. There are a few typical types of documentation errors. First, API documentations may \nhave grammatical errors, placing unnecessary barriers on programmers to learn API usage [35]. Second, \nAPI doc\u00adumentations may describe out-of-date API usage, likely confusing and misleading programmers. \nFor example, the API reference for Lucene 3.5 includes the following code snippet: This can be done with \nsomething like: public TokenStream tokenStream(...) {final TokenStream ts = someAnalyzer.tokenStream(fieldName, \nreader); TokenStream res = new TokenStream(){TermAttribute termAtt = addAttribute(...); ... } This code \nsample uses a TermAttribute class, which is dep\u00adrecated. A programmer reported this error and submitted \na patch for the code sample.1 The patch was accepted, and the correct code sample from Lucene 3.6 is: \nThis can be done with something like the following (note, however, that StopFilter natively includes \nthis capability by subclassing FilteringTokenFilter): public TokenStream tokenStream(...) {final TokenStream \nts = someAnalyzer.tokenStream(fieldName, reader); TokenStream res = new TokenStream(){CharTermAttribute \ntermAtt = addAttribute(...); ... } In particular, the deprecated class is replaced with an up-to-date \nCharTermAttribute class. Third, API documentations can even describe illegal API usage. For example, \nJ2SE s latest API reference2 contains the following code sample: if (ss.isComplete() &#38;&#38; status \n== SUCCESS){... ldap.in = new SecureInputStream(ss, ldap.in); ldap.out = new SecureOutputStream(ss, ldap.out); \n... } The above code sample uses SecureInputStream and Secure-OutputStream. We examined all the releases \nof J2SE, but did not .nd the two classes. Such illegal code samples can easily confuse and frustrate \nprogrammers. For this example, on the discussion of a 1https://issues.apache.org/jira/browse/ LUCENE-3666 \n2http://docs.oracle.com/javase/7/docs/api/javax/ security/sasl/SaslServer.html reported bug,3 a programmer, \nPhilip Zeyliger, complained to other programmers about the above code sample: Isn t it totally bizarre \nthat the SaslServer javaDoc talks about SecureInputStream , and there doesn t seem to be such a thin(g)? \nI think they must have meant com.sun.jndi.ldap.sasl.SaslInputStream, which seems to be part of OpenJDK... \n 1.1 The Problem To describe API usage, an API documentation consists of (1) natu\u00adral language sentences, \n(2) sentences with code names (Er), and (3) blocks of code samples. Here, code names refer to names of \ncode elements such as classes, methods, variables, and .elds. Code sam\u00adples refer to code names (Er) \nand declare new code names (Ed). For example, Figure 1 shows two paragraphs from Lucene s API reference.4 \nThe two paragraphs have natural language sentences, sentences with code names, and a code sample. Typically, \nan API document can have two types of documentation errors: Natural language errors. In API documentations, \nnatural lan\u00adguage sentences and sentences with code names can contain syn\u00adtax errors. Existing natural \nlanguage tools can produce false errors for these sentences, since they do not understand code names. \nFor example, the .rst sentence in Figure 1 has a code name, Filter-AtomicReader. Existing document checkers \ntypically identify it as a typo, since it is unlikely that the class name exists in the dic\u00adtionary of \na spell checker. Broken code names. In API documentations, sentences with code names and code samples \ncan have broken code names. If we use EAPI to denote all the API elements of the latest API library, \nan API documentation should satisfy the following criterion: . Er EAPI Ed (1) The sample page of API \nreference in Figure 1 violates this criterion. In particular, the code sample in Figure 1 includes an \ninvocation to the DirectoryTaxonomyWriter.addTaxonomies() method. After checking the method, we have \nfound that the API method is a broken code name, since it does not appear in Ed of the page or EAPI of \nthe Lucene s latest API reference. In an API document, documentation errors confuse program\u00admers and \ncan even lead to defects in developed code. It is thus desirable to detect documentation errors and .x \nthem. To help .x documentation errors, programmers continually submit found issues as defects to authors \nof those documents. On the other hand, the authors also take much effort to improve document qual\u00adity \n[10, 33, 35]. However, in their daily programming tasks, pro\u00adgrammers still frequently encounter API \ndocumentation errors, due to the following challenges: Challenge 1. It takes great effort to detect API \ndocumentation errors manually. For example, to determine whether the API refer\u00adence in Figure 1 contains \nout-of-date code names, authors should examine all the code names in the sentences and the code sample. \nThey should refer to the latest API reference of lucene constantly to determine whether the code name \nis outdated or not. To make things more dif.cult, authors often have to maintain a large num\u00adber of documents. \nFor example, the API reference of J2SE contains thousands of pages. It is challenging to detect documentation \nerrors for all the pages manually. 3https://issues.apache.org/jira/browse/ HADOOP-6419 4http://lucene.apache.org/core/4 \n1 0/ facet/org/apache/lucene/facet/index/ OrdinalMappingAtomicReader.html Challenge 2. To describe API \nusage, API documentations typ\u00adically consist of both sentences in NLs and code names/samples in programming \nlanguages. To detect API documentation errors, a tool needs to distinguish and understand NL words, code \nnames, and code samples. State-of-the-art natural-language tools such as Standford NLP Parser [19] do \nnot understand code names and code samples, while code analysis tools such as Polyglot [27] do not un\u00adderstand \nNL sentences. Although work exists to analyze API doc\u00adumentations (see Section 6 for details), the proposed \napproaches address different research problems and cannot effectively detect API documentation errors. \n 1.2 Contributions In this paper, we propose the .rst automatic approach, called DOCREF, for detecting \nerrors in API documentations. To address the aforementioned challenges, DOCREF combines natural lan\u00adguage \nprocessing (NLP) [25] with island parsing [7]. This paper makes the following main contributions: The \n.rst approach, called D OCREF, that combines NLP tech\u00adniques with island parsing to automatically detect \nerrors in API documentations.  A tool implementation and an extensive evaluation on API references of \n.ve real-world API libraries (e.g., J2SE). Our results show that D OCREF detects more than one thousand \ndetected real bugs that are previously unknown. In particular, 48 reported bugs were con.rmed and .xed \nby developers of API libraries within two weeks after we reported.  The rest of the paper is structured \nas follows. Section 2 illus\u00adtrates our high-level approach with an example, while Section 3 presents \nour detailed approach. We present our evaluation in Sec\u00adtion 4, and discuss limitations of our approach \nin Section 5. Sec\u00adtion 6 surveys related work, and Section 7 concludes. 2. Example This section illustrates \nmajor steps of our approach with the API reference in Figure 1, and explains the technical dif.culties \nto detect API documentation errors. Step 1. Extracting code samples. NL sentences and code samples follow \ndifferent grammars, and different techniques are needed to extract Er and Ed from code samples and NL \nsentences. Thus, we need to extract code samples from documents .rst. It is relatively simple to extract \ncode samples from NL sentences, since code sam\u00adples have many code names while NL sentences do not. However, \nsince API documentations have many sentences with code names, the difference between code samples and \nNL sentences sometime may not be apparent. In particular, Bacchelli et al. [3] show that it is inef.cient \nto classify source code from other email contents by traditional text classi.cation techniques [34]. \nIn addition, some code samples have introductory sentences in natural languages. For example, the code \nsample in Figure 1 consists of the introductory sentence, For re-mapping the ordinals during index merge, \ndo the following: . It is relatively straightforward to distinguish the in\u00adtroductory sentence from the \ncode sample, since they are tagged differently. However, there is no guarantee that authors do tag them \ndifferently, and Table 1 illustrates some exceptions. A code pars\u00ader cannot understand NL sentences, \nand can extract incorrect Ed and Er from introductory sentences. These introductory sentences need to \nbe removed from code samples. To address these dif.cul\u00adties, D OCREF extracts code samples according \nto tags and charac\u00adteristic differences between NLs and programming languages (see Section 3.1 for details). \nStep 2. Extracting Er and Ed. Next, we need to extract Er and Ed from documents. For sentences with code \nnames, the dif.cul\u00ad  Figure 1. A page of API reference for Lucene. ty lies in extracting and identifying \ncode names from NL words correctly. Typically, spell checkers report code names in sentences as typos, \nsince code names unlikely appear in their dictionaries. We classify reported typos into packages, types, \nmethods, and vari\u00adables, according to their context words and the naming convention of Java (see Section \n3.2 for details). For example, based on the .rst paragraph in Figure 1, we add to Er two types FilterAtomic-Reader \nand DirectoryTaxonomyWriter.OrdinalMap. For code samples, the main dif.culty is in constructing the code \nto be parsed. In API documentations, most code samples are code fragments that do not have any enclosure \nstatements or import statements. As a result, even an island parser (e.g., PPA [7]) cannot parse many \ncode samples, and we cannot accurately extract Er and Ed from these code samples. For example, the code \nsample in Figure 1 does not have enclosure statements such as method or class bodies. The code sample \nneither has import statements. We fed the code sample to PPA, but it failed to parse the code. To address \nthis dif.culty, we add enclosure statements and synthesize import statements for code samples, before \nwe leverage island parsing to build Abstract Syntax Trees (see Section 3.3 for details). From a code \nsample, we extract both Er and Ed. For example, based on the second statement of the example in Figure \n1, we add map to Ed, and add OrdinalMap and DirectoryTaxonomyWri\u00adter.addTaxonomies to Er. Step 3. Detecting \nmismatches. Finally, we add all API classes, interfaces, enums, methods, .elds, and parameters of the \nlatest Lucene to EAP I , and examine code names with Equation 1. There are two dif.culties in this step. \nFirst, there are typically many code names. For example, the API reference of J2SE has thousands of pages, \nand each page can have hundreds of sentences with code names and complicated code samples. Second, API \nlibraries pro\u00advide a large number of API elements. For example, J2SE alone provides thousands of classes, \nand even more methods and .elds. As a result, Er and EAP I can both be large, and it can take effort \nto examine the two sets. To speed up the process, we localize search scopes and cache already found code \nnames (see Section 3.4 for de\u00adtails). A mismatch may indicate a real typo or a broken code name. For \nexample, in Figure 1, we examined the latest API reference of lucene, and did .nd the DirectoryTaxonomyWriter.addTaxo\u00adnomies() \nmethod. We reported this issue as an out-of-date error in API reference, and the developers of Lucene \nhave already .xed the reported error. 3. Approach API documentations, such as API references, wiki pages, \ntutorials, and forum discussions, are typically in the form of HTML pages. DOCREF takes HTML documentations \nand the latest API reference as inputs, and detects mismatches that indicate errors or smells. Furthermore, \nour approach can be easily adapted to analyze other forms of documentations such as PDF, since D OCREF \nuses only one HTML tag and our approach includes an alternative technique when such a tag is not available. \n3.1 Extracting Code Samples First, as code samples and sentences need to be treated different\u00adly, DOCREF \nextracts code samples from API documentations. Al\u00adthough code samples in different documents have different \nstyles, we .nd that many of them follow the W3C guideline5 to place each block of code inside a pre element. \nFor example, the source .le of Figures 1 is: ... <p>For re-mapping the ordinals...</p> <pre...>...// \nmerge the old taxonomy...</pre> Table 1 shows additional examples of API documentations. Col\u00adumn API \ndocumentation lists .ve typical types of API docu\u00admentations, such as API references, wiki pages, tutorials, \nforum discussions, and development emails. Column Code lists their source code in HTML. We .nd that the \ntop four examples all fol\u00adlow the W3C guideline. For API documentations that follow the W3C guideline, \nDOCREF extracts code samples by matching the pre tags in HTML .les. In these HTML documents, introducto\u00adry \nsentences and code samples are under different tags. When ex\u00adtracting code samples by tags, D OCREF separates \nintroductory sen\u00adtences from code samples by their tags. API documentations such as development emails \nare informal. These documentations often do not follow the W3C guideline, and tags of code samples are \nquite ad hoc. For example, the develop\u00ad 5http://www.w3.org/TR/2011/WD-html5-author-20110809/ the-code-element.html \n API documentation Code (a) An API reference of J2SE <p>As another example, this code... </p>... <pre> \nScanner sc = new Scanner(new File(\"myNumbers\")); ...</pre> (b) A page of Wikipedia <p>Here is an example \nin...</p>... <pre...> <span...>import</span> <span...>java.util.HashMap</span> ...</pre> (c) A tutorial \nof Eclipse <p>The <code...>Viewer</code> also allow ...</p>... <pre...> viewer.addDoubleClickListener(...</pre> \n(d) A discussion of StackOver.ow <p>Server connection handler code:</p> <pre><code>public void run() \n...</code></pre> (e) A development email of AspectJ <p><font...>If I write the advice as follows:</font></p> \n<p><b><font...>public</font></b><b><font...>aspect...</p> Table 1. Additional API documentation examples. \nment email in Table 1 does not follow the W3C guideline. Fur\u00adthermore, API documentations may be in other \nforms than HTML (e.g., PDF), and these documents may not have any tags for code samples. As a result, \nwe cannot rely on tags to extract code sam\u00adples from these documents. Existing approaches use island \nparsing to extract code samples from plain texts (see Section 6 for details). These approaches need to \nimplement an island parser for each pro\u00adgramming language and may fail to extract code samples, if their \nisland parsers fail to process some code elements correctly [30]. Instead of island parsing, D OCREF \nrelies on certain characteristics of plain texts to extract code samples. The insight of DOCREF to extract \ncode samples is: NL sentences follow grammars of NL, and code samples follow grammars of programming \nlanguages. As the two types of gram\u00admars are different, code samples have different punctuation fre\u00adquencies \nfrom NL sentences, and code samples are ungrammatical with respect to standard NL grammars. For each \nparagraph, DOCREF identi.es it as a code sample, if it satis.es the following two criteria: 1. Punctuation \ncriterion. A code sample should have all the punctuation marks such as ; , ( , ) and = . An existing \nem\u00adpirical study [6] shows that the frequencies of the preceding punc\u00adtuation marks are typically quite \nlow in NL corpora (e.g., English newspapers). In programming languages, such as Java, C#, and C++, the \npreceding punctuation marks are quite common. As a re\u00adsult, we use these punctuation marks for extracting \ncode samples. 2. NL-error criterion. In linguistics, Fraser et al. [14] de.ne the grammar of a language \nas the systematic ways of the lan\u00adguage through which words and sentences are assembled to convey meaning. \nCode samples follow the grammars of programming lan\u00adguages, which are typically different from NL grammars. \nAs a re\u00adsult, code samples should have more NL errors than NL sentences. An NL checker [25] examines \nfour categories of NL errors, such as spelling errors, grammar errors, style errors, and semantic errors. \nBased on the reported NL errors, D OCREF de.nes an NL-error ra\u00adtio as follows: |N L errors| error ratio \n= (2) |words| When calculating the number of NL errors, DOCREF does not count typos. The NL checker \nreports many code names as typos, since code names are unlikely to appear in its dictionary. If we calculate \ntypos as NL errors, we cannot distinguish code samples from sentences with code names. An alternative \nway to extract code samples is to calculate the number of compilation errors that are reported by an \nisland parser. We .nd that island parsers (e.g., PPA [7]) typically stop reporting further compilation \nerrors, after it encounters a critical error. As a result, an island parser often reports few compilation \nerrors for NL sentences, since NL sentences are different from code samples in their grammars, and these \ndifferences are often critical errors to an island parser. On the other hand, an island parser can report \nmany compilation errors for code samples, since code samples often do not have a complete compilation \nunit and corresponding import statements. It may still be feasible to adapt an island parser for code \nextraction, but it is more general to calculate the number of documentation errors that are reported \nby an NL checker, since we do not need to adapt individual parsers to extract code samples in different \nprogramming languages. As Table 1 shows, some paragraphs may include introducto\u00adry sentences before \ncode samples. It is desirable to remove these sentences, but we cannot .nd reliable identi.ers for the \nbeginning of code samples, since code samples are typically code fragments. To remove these introductory \nsentences, DOCREF .rst splits each code sample into two parts by : . If the .rst part does not sat\u00adisfy \nour criteria for code samples, and the second part satis.es, DOCREF removes the .rst part from the code \nsample. For example, DOCREF splits the development email in Table 1 into two parts: Part 1: If I write \nthe advice as follows Part 2: public aspect SysOutLoggAspect{... DOCREF removes the .rst part, since \nit does not satisfy our punctuation criterion and the second part satis.es our criteria for code samples. \n 3.2 Extracting Er from Sentences W3C recommends the use of code tags to represent code names.6 However, \nwe .nd that many documents do not follow this recom\u00admendation. For example, a sentence of Batik s reference7 \nis Cre\u00adates a new CombinatorCondition object. , and its HTML source .le is as follows: <span...>Creates \na new CombinatorCondition object.</span> Here, CombinatorCondition is a code name, but it does not have \nany code tags. As a result, it is unreliable to extract code names using tags. As code names are typically \nabbreviations and camel-case terms that are unlikely to appear in existing dictionaries, Our underlying \nNL checker [25] reports them as typos. However, in practice, we notice that dictionaries in existing \nNL checkers are often limited, and do not cover many computer science abbreviations and terms (e.g., \nAPI, IO, and localhost). To address this limitation, we add a customized dictionary to our underlying \nNL checker, so it does not report those computer science terms and abbreviations as typos. DOCREF further \nclassi.es typos into variables, methods, types, or packages, with the following two strategies: 1. Context \nstrategy. We call the two words before and after a code name as the context words of the code name. Context \nwords pro\u00advide valuable hints to identify types of code names. For example, a sentence from the documentation \nof J2SE is Retrieves whether or not a visible row insert can be detected by calling the method Re\u00adsultSet.rowInserted. \nFrom the context word, method , we under\u00adstand that ResultSet.rowInserted is a method. DOCREF uses context \nwords to identify types of code names, and the general s\u00adtrategy is as follows: .eld, value, parameter, \nor variable -variable method, function, or operation -method class, interface, enum, exception, or object \n-type package or subpackage -package 6http://dev.w3.org/html5/html-author/ 7http://xmlgraphics.apache.org/batik/ \njavadoc/org/apache/batik/css/engine/sac/ AbstractCombinatorCondition.html Algorithm 1: parseCodeReference \n(Recursive) Input:c is a code name Output:t is the type of the code name 1 begin 2 if c.indexOf( ( ) \n> 0 and c.indexOf( ) ) > 0then 3 t+ method 4 else if c.indexOf( . ) > 0then 5 if c.toLowerCase() = c \nthen 6 t+ package 7 else 8 index + c.indexOf( . ) 9 c + c.subString(index+1) 10 t+ parseCodeReference(c) \n11 else if c.indexOf( ) > 0or c.toUpperCase() = c or c.toLowerCase() = c then 12 t+ variable 13 else \nif c.charAt(0).isUpperCase() then 14 t+ type 15 else 16 words + split c by upper case characters 17 if \nwords start with a verb then 18 t+ method 19 else 20 t+ variable 21 return t  2. Naming conventions. \nDOCREF then classi.es the remain\u00ading code names by their naming conventions [15]. In particular, DOCREF \nuses Algorithm 1 to identify types of code names, and on Line 17, it leverages LingPipe [5] as the underlying \npart-of-speech (POS) tagger to determine whether split words start with a verb. Although it identi.es \nreal typos as code names, D OCREF re\u00adports most of the real typos as mismatches, since Ed and EAPI unlikely \ncontain those typos. Authors of API documentations of\u00adten ignore reported typos, since many reported \ntypos are actually valid code names. DOCREF does not report valid code names as mismatches, thus reducing \nthe manual effort to identify real typos. 3.3 Extracting Er and Ed from code samples DOCREF leverages \nisland parsing to build Abstract Syntax Trees (ASTs) for code samples. In API documentations, code samples \nare often code fragments, and do not have a complete compilation unit or import statements. As a result, \neven an island parser [7] fails to build ASTs for many code samples. Before DOCREF leverages island parsing, \nit constructs code for parsing from code samples. 1. Constructing code for parsing. DOCREF constructs \ncode based on the compilation result of island parsers. In particular, if is\u00adland parsers fail to build \nan AST for a code sample, D OCREF adds type-declaration statements to the code sample. If island parsers \nstill fail to build a valid AST, D OCREF then adds both method\u00addeclaration and type-declaration statements \nto the code sample. If a valid AST is built, DOCREF visits all SimpleName nodes and queues EAPI for their \nfully quali.ed names (see Section 3.4 for details of extracting EAPI ). Based on these names, DOCREF \nadds import statements to the corresponding code sample. For instance, DOCREF adds type-declaration and \nmethod-declaration statements to the code sample in Table 1a, and adds two import statements, based on \nthe queried fully quali.ed names for Scanner and File. The constructed code of the code sample is: import \njava.util.Scanner; import java.io.File; public class EnclosureClass{public void enclosureMethod(){Scanner \nsc = new Scanner(new File(\"myNumbers\")); while (sc.hasNextLong()){long aLong = sc.nextLong(); }}} 2. \nAnalyzing parsed ASTs for Er and Ed. Code samples have references to existing code elements (Er), and \ndeclare new code elements (Ed). For example, the Scanner sc expression has a code reference to the Scanner \nclass and declares a new sc vari\u00adable. Latter sentences may refer to sc, when explaining its usage. To \nbe consistent with the code names in Section 3.2, DOCREF ex\u00adtracts three types of code names such as, \ntypes (classes, interfaces, enums, and exceptions), methods (constructors and methods), and variables \n(parameters, local variables, and .elds), from code sam\u00adples. D OCREF does not extract packages from \nimport statements, since these statements are added by itself. In particular, D OCREF uses the following \nrules to extract Er: 1. For each a.f expression where a is a variable and f is a .eld, DOCREF adds both \na and f to Er. 2. For each T.f expression where T is a type and f is a .eld, DOCREF adds f to Er. Here, \nDOCREF does not add T to Er, since island parsers add T to the fully quali.ed name of f. 3. For each \na.m(p, ...) expression where a is a variable, m is a method, and p is a parameter, DOCREF adds a,m, and \n{p,...}to Er. 4. For each T.m(p, ...) expression where T is a type, m is a method, and p is a parameter, \nDOCREF adds m and {p,...} to Er. 5. For each m(p,...) expression where m is a method and p is a parameter, \nD OCREF adds m and {p,...} to Er. 6. For each (T)a cast expression where T is a type and a is a variable, \nDOCREF adds both T and a to Er. 7. For each T a declaration expression where T is a type and a is a \nvariable, DOCREF adds T to Er. 8. For each T m(T1 p1,...) throws E1... expression where T is a return \ntype, m is a method, p1 is a parameter, T1 is the type of p1, and E1 is a thrown exception, DOCREF adds \nT, {T1,...} and {E1,...} to Er. 9. For each class|interface|enum T extends T1... imp\u00adlements I1 ... \nexpression where T is a declared class, in\u00adterface, or enum, T1 is a type, and I1 is an interface, DOCREF \nadds {T1,...} and {I1,...} to Er.  DOCREF uses the following rules to extract Ed: 1. For each T a declaration \nexpression where T is a type and a is a variable, DOCREF adds a to Ed. 2. For each T m(T1 p1,...) throws \nE1... expression where T is a return type, m is a method, p1 is a parameter, T1 is the type of p1, and \nE1 is a thrown exception, DOCREF adds m and {p1,...} to Ed. 3. For each class|interface|enum T extends \nT1... imp\u00adlements I1 ... expression where T is a declared class, in\u00adterface, or enum, T1 is a type, and \nI1 is an interface, DOCREF adds T to Ed.  For example, Er extracted from the code sample in Table 1a \ncontains the following code names: type -java.util.Scanner method -java.util.Scanner.Scanner java.io.File.File \njava.util.Scanner.hasNextLong java.util.Scanner.nextLong Ed extracted from the code sample in Table 1a \ncontains the follow\u00ading code names: Figure 2. A paragraph of Lucene s API reference. variable -sc, \nmyNumbers, and aLong Here, when extracting fully quali.ed names, D OCREF ignores the names of enclosure \nmethods and types that are added by itself. In addition, some resolved fully quali.ed names may be incorrect. \nFor example, the API reference8 of Lucene contains the following code sample: public TokenStream tokenStream(...){final \nTokenStream ts = someAnalyzer.tokenStream(...); ... } In this example, someAnalyzer refers to analyzers \nthat are implemented by programmers, but an island parser considers it as a class name. As a result, \nthe resolved fully quali.ed name of the method in the second line is someAnalyzer.tokenStream. For extracted \nfully quali.ed names of variables and methods, DOCREF uses Algorithm 1 to check whether the extracted \nclass names are valid and removes those invalid names. As someAnalyzer is not a valid class name, DOCREF \nremoves the name, and the extracted method name becomes tokenStream. 3.4 Checking code names DOCREF \nextracts EAPI from API references, which de.ne API el\u00adements of an API library in a semi-structured form. \nIn particular, DOCREF extracts four types of API elements, such as types (class\u00ades, enums, interfaces, \nand exceptions), variables (parameters and .elds), packages, and methods (constructors and methods). \nAfter that, D OCREF checks code names of each document with the fol\u00adlowing steps. Step 1. Removing invalid \ncode names from Er. Some code names refer to code elements that should be implemented by pro\u00adgrammers. \nFor example, the API reference of J2SE9 contains the following code sample: EventListenerList listenerList \n= new EventListenerList(); FooEvent fooEvent = null; public void addFooListener(FooListener l){ listenerList.add(FooListener.class, \nl); }... Here, FooEvent and FooListener are two classes that are sup\u00adposed to be implemented by programmers. \nD OCREF uses the char\u00adacteristics of names to .lter these code names. For each code name in Er, DOCREF \nsplits the name into words delimited by uppercase letters. If the split words of a name contain words \nsuch as foo , my , something , somewhere , or some , D OCREF removes the code name from Er. Some code \nnames refer to primitive types (e.g., int). DOCREF .lters these types, since API libraries do not provide \nde.nitions for primitive types. Step 2. Resolving attached URLs. Code names can have at\u00adtached URLs to \ntheir API references. For example, Figure 2 shows some sentences with code names. In these sentences, \nPosition\u00adIncrementAttribute.setPositionIncrement(int) has an attached URL, while StopFilter does not. \nFor those code names with attached URLs, we could resolve whether such URLs are valid 8http://lucene.apache.org/core/4 \n1 0/core/org/ apache/lucene/analysis/package-summary.html 9http://docs.oracle.com/javase/7/docs/api/javax/ \nswing/event/EventListenerList.html Name Version P N R C Lucene 4.1.0 2,157 Batik 1.7 1,876 Hadoop 2.0.3alpha \n734 Uimaj 2.4.0 433 J2SE 7.0 4,723 24,354 26,531 12,132 7,970 157,045 19,632 22,409 7,359 7,246 112,503 \n365 75 93 4 2,214 Total 9,923 228,032 169,149 2,751 Table 2. API references. to save the matching effort. \nIf an attached URL is valid, DOCRE-F puts its corresponding code name into the matched category to reduce \nthe matching effort. Step 3. Matching code names. For the remaining code names, DOCREF searches Ed of \nthe document. After that, for those code names that are not found, D OCREF further searches EAPI in a \nconservative manner. First, if a method or variable does not con\u00adtain the name of its declared type, \nD OCREF searches all the types for the method or variable. Second, document authors can use plu\u00adral forms, \nsince code names are nouns in sentences. If DOCREF cannot .nd a code name, it resolves the stem of the \ncode name and searches the stem. Finally, document authors may use sim\u00adple regular expressions to denote \nmultiple code names. For ex\u00adample, the J2SE s API reference10 contains the sentence: Extra buttons have \nno assigned BUTTONx constants as well as their button masks have no assigned BUTTONx DOWN MASK con\u00adstants. \nIn this sentence, BUTTONx DOWN MASK denotes the three .elds, such as BUTTON1 DOWN MASK, BUTTON2 DOWN \nMASK, and BUTTON1 DOWN MASK, declared in InputEvent. 11 If DOCREF can\u00adnot .nd a code name, it tries to \nsearch it as a regular expression. For this example, DOCREF searches variables that start with BUT-TON \nand end with DOWN MASK , and thus .nds the preced\u00ading three API .elds. When describing API usage, a \ndocument typically does not refer to Ed in other documents, so that programmers do not need to read multiple \ndocuments to understand API usage. This characteristic allows D OCREF to check documents in parallel. \nAmong all the search threads, D OCREF maintains a found list and a not found list. When locating a code \nname, each thread checks the two lists. If found, the code name is put into the corresponding category. \nIf not found, D OCREFs tries to locate the code name, and updates the two lists accordingly. 4. Evaluation \nWe implemented DOCREF and conducted an extensive evaluation using the tool. In our evaluation, we address \nthe following two main research questions: (RQ1) How effectively does DOCREF detect unknown errors in \nAPI documentations (Section 4.1)? (RQ2) How accurate is D OCREF (Section 4.2)? RQ1 mainly concerns the \neffectiveness of DOCREF for detect\u00ading real errors, while RQ2 mainly concerns the effectiveness of DOCREF \ns internal techniques. Our result shows that D OCREF detects more than one thousand documentation errors \nand bad smells with reasonable high preci\u00adsion and recall. In particular, 48 reported errors were .xed \nwithin two weeks after we reported them. 10 http://docs.oracle.com/javase/7/docs/api/java/ awt/event/MouseEvent.html \n11 http://docs.oracle.com/javase/7/docs/api/java/ awt/event/InputEvent.html 4.1 RQ1: Detecting Unknown \nErrors 4.1.1 Study Setup We choose API references as evaluation subjects to investigate the .rst research \nquestion for the following considerations. First, ex\u00adisting studies [35] show that developers of API \nlibraries constantly .x errors in API references, and programmers who read API ref\u00aderences also often \nreport found errors through bug-report systems. As other types of API documentations, such as forum discussions \nand tutorials, are not revised as frequently as API references are, it is more challenging to detect \nunknown errors in API references than in other types of API documentations. Second, API references typically \ncontain thousands of pages, while other types of API doc\u00adumentations may not have as many pages. For \nexample, an API li\u00adbrary may have only several pages of tutorials. As a result, it could take much more \nefforts to detect errors in API references. Final\u00adly, other studies [26] also show that programmers of \nclient code frequently refer to API references when they encounter unfamiliar API usage. Thus, it is \nimportant to detect errors in API references. Table 2 shows the API references used as subjects in our \nevalua\u00adtion. Column Name lists the names of API libraries, and column Version lists their versions. All \nthe API libraries are the latest re\u00adleases at the time when we conducted the evaluation. Column P lists \nthe number of pages in these API references. In an API refer\u00adence, each type (class, interface, enum, \nand exception) has a page, and each package also has one. In total, our subject API references contain \nthousands of pages. D OCREF classi.ed contents of pages into NL sentences, sentences with code names, \nand code samples. Column N lists the number of NL sentences. Column R lists the number of sentences with \ncode names. The results highlight the importance of our approach, since about half of the sentences have \ncode names. Column C lists the number of code samples. Com\u00adpared with number of pages, we .nd that API \nreferences do not contain many code samples. However, as code samples are quite important for programmers \nto understand API usage [18], it is es\u00adsential to present correct code samples. 4.1.2 Empirical Results \nTable 3 shows our overall results. Column Mismatch shows the number of detected unique mismatches. Different \npages may have the same mismatch, and we count them as a single one. For ex\u00adample, D OCREF detected that \nthe API references of UIManager, 12 MetalLookAndFeel, 13 and DefaultMetalTheme14 have the same typo, \nsytem . Table 3 counts it as a single mismatch. We inspect\u00aded the mismatches and classi.ed them into \nthree major categories: errors, smells, and false alarms. Documentation errors. Column Error lists the \ndetected docu\u00admentation errors. Subcolumn T lists the number of NL words with spelling errors. We see \nthat all the API references have no\u00adtable numbers of typos. Their authors may have ignored warnings from \nspell checkers, because too many reported warnings are actu\u00adally valid code names. Subcolumn C lists \nthe number of broken code names. To better present the broken code names, we break them down into four \ncategories, and Table 4 shows the results. 1. Outdated code names. In Table 4, column Outdated lists \nthe number of outdated code names. In earlier versions, developers re\u00adfer to some API elements in sentences \nor code samples, but later forget to revise these sentences or code samples accordingly when 12 http://docs.oracle.com/javase/7/docs/api/javax/ \nswing/UIManager.html 13 http://docs.oracle.com/javase/7/docs/api/javax/ swing/plaf/metal/MetalLookAndFeel.html \n14 http://docs.oracle.com/javase/7/docs/api/javax/ swing/plaf/metal/DefaultMetalTheme.html Name Mismatch \nError Smell False alarm T C % I W % Er Ed Dic EAPI % Lucene 512 58 96 30.1% 42 18 11.7% 37 23 3 235 58.2% \nBatik 460 198 52 54.3% 33 7 8.7% 18 5 144 3 37.0% Hadoop 284 59 144 71.5% 4 0 1.4% 21 13 3 40 27.1% Uimaj \n222 104 49 68.9% 9 7 7.2% 18 6 2 27 23.9% J2SE 2086 463 432 42.9% 92 240 15.9% 226 134 127 372 41.2% \nTotal 3564 882 773 46.4% 180 272 12.7% 320 181 279 677 40.9% T: typos. C: broken code names; I: private \ncode names; W: incorrectly classi.ed code names; Er: false alarms that are related to false Er;Ed, Dic, \nEAP I : false alarms that are related to incomplete Ed, dictionary, or EAP I , respectively. Table 3. \nOverall result. they delete these API elements. For example, Figure 1 shows a de\u00adtected outdated code \nname that was con.rmed by the developers of Lucene. In Lucene s reference of the same version, our tool \ndetect\u00aded that the API references of DateTools15 and QueryParser-Base16 talk about RangeQuery. According \nto Lucene s change log,17 RangeQuery is already deleted in its latest version. As an\u00adother example, the \nAPI reference of DocumentAnnotation18 con\u00adtains the sentence: created and made accessable via a call \nto the JCasImpl.getDocumentAnnotationFs() method. Although an earlier version has the JCasImpl.getDocumentAnnotationFs \nmethod,19 it was deleted from the latest version. Sometimes, an API element is not deleted, but moved \nto other libraries in the latest version. For example, the API reference of Configuration20 contains \nthe code sample: Login { com.sun.security.auth.module.UnixLoginModule required; com.sun.security.auth.module.Krb5LoginModule \noptional ... }; The API reference of J2SE 6.0 contains the two classes Unix-LoginModule and Krb5LoginModule. \nIn J2SE 7.0, both classes are moved from J2SE to an external module. When Lethbridge et al. [22] enumerate \nthe characteristics of bad software documentations, the authors list out of date as the .rst characteristic. \nOur results con.rm that poor API documentations also have such characteristics. Developers of API libraries \nmay forget to update corresponding contents after they have deleted API elements. These contents describe \nout-of-date API usage that can mislead programmers. Using our tool, API library developers can revise \nsuch out-of-date contents to improve documentation quality. 2. Bizarre code names. We have noticed that \nsome code names cannot be found in any releases of API libraries. In Table 4, column Bizarre lists the \nnumber of these bizarre code names. One such example is the SecureInputStream class that we discussed \nin Section 1. For another example, the API reference of TextAttribute21 refers to a bizarre .eld, An \nintermediate 15 http://lucene.apache.org/core/4 1 0/core/org/ apache/lucene/document/DateTools.html 16 \nhttp://lucene.apache.org/core/4 1 0/queryparser/ org/apache/lucene/queryparser/classic/ QueryParserBase.html \n17 http://lucene.apache.org/core/4 1 0/changes/ Changes.html 18 http://uima.apache.org/d/uimaj-2.4.0/apidocs/ \norg/apache/uima/jcas/tcas/DocumentAnnotation.html 19 http://javasourcecode.org/html/open-source/uima/ \nuima-2.3.1/org/apache/uima/jcas/impl/JCasImpl. html 20 http://docs.oracle.com/javase/7/docs/api/javax/ \nsecurity/auth/login/Configuration.html 21 http://docs.oracle.com/javase/7/docs/api/java/ awt/font/TextAttribute.html \n weight between WEIGHT LIGHT and WEIGHT STANDARD. We examined all the released versions of J2SE, but \ndid not .nd WEIGHT STANDARD. When programmers encounter bizarre code names, they have to guest what these \nnames actually refer to. For example, in Section 1, the programmer guessed that SecureInputStream actually \nrefers to SaslInputStream. However, if the guess is wrong, program\u00admers can easily introduce defects \ninto their developed code. In some cases, we can .nd an API element with a similar name. For example, \nthe API reference of ReentrantLock22 contains the sentence: this class de.nes methods isLocked and getLockQueue-Length... \n. We examined all the released versions of J2SE, but did not .nd the latter method. Instead, we found \na method with a similar name, getQueueLength. When developers wrote the above sentence, there may have \nexisted the getLockQueueLength method. However, later, the method might have been renamed as getQueueLength, \nbut the above sentence was not updated accord\u00adingly. Existing tools provide strong support for code refactoring \n(e.g., renaming method names in this example), but these refactor\u00ading tools typically do not update code \nnames in NL documents [24]. We suggest that tool designers and developers should take code names in NL \ndocuments into consideration to complement existing refactoring tools. 3. Incorrect camel names. Most \ncode names are camel names, and we .nd that some camel code names have incorrect uppercase characters. \nIn Table 4, column Camel lists the number of camel code names with incorrect uppercase characters. For \nexample, the API reference of SQLPermission23 contains the sentence: The permission for which...Connection.setNetworktimeout \nmethod... The Connection interface does not have such a method, but a setNetworkTimeout method instead. \nWe classi.ed this type of mismatches as broken code names since Java is case-sensitive. 4. Code names \nwith typos. Code names can have typos. In Table 4, column Typo lists the number of typos. For example, \nthe API ref\u00aderence of CancelablePrintJob24 contains the sentence: Service implementors are encouraged \nto implement this optional interface and to deliver a javax.print.event.PrintJobEvent.JOB CANCELLED. \nThe correct name of the above .eld is JOB CANCELED.  From a programmer s viewpoint, broken code names \nare harm\u00adful, since they describe obsolete or even incorrect API usage. Pro\u00adgrammers can get frustrated, \nwhen they try to check those code names, but fail to .nd them. Subcolumn % lists the ratio of de\u00adtected \ndocumentation errors to mismatches. In total, about half of the detected mismatches are documentation \nerrors. We reported the 22 http://docs.oracle.com/javase/7/docs/api/java/ util/concurrent/locks/ReentrantLock.html \n23 http://docs.oracle.com/javase/7/docs/api/java/ sql/SQLPermission.html 24 http://docs.oracle.com/javase/7/docs/api/javax/ \nprint/CancelablePrintJob.html detected documentation errors to API library developers, and 48 reported \nerrors have already been .xed by their developers. Smells. In Table 3, column Smell lists detected smells. \nIn par\u00adticular, subcolumn I lists the number of private code names in API implementations. These code \nnames are not API elements, and should be invisible to programmers. For example, the API reference of \nthe KEY PAGE HEIGHT .eld25 is The pageHeight key and does not explain what pageHeight is. We examined \nthe source .les of Batik and found the following related lines of code: public static final String KEY \nPAGE HEIGHT STR = \"pageHeight\"; ... setTranscoderFloatHint(transcoder, KEY_PAGE_HEIGHT_STR, KEY_PAGE_HEIGHT); \n... public static void setTranscoderFloatHint(Transcoder transcoder, String property, TranscodingHints.Key \nkey){... In the above code, pageHeight is a the value of KEY PAGE- HEIGHT STR, and KEY PAGE HEIGHT is \nthe key of the correspond\u00ading transcoder. We understand that it is straightforward for API library developers \nto use private code names of their API imple\u00admentations to explain API usage. However, programmers typically \nhave no or little knowledge of API library implementations, and may not understand what these code names \nare. For this reason, we consider such practices as bad smells. Subcolumn W lists the number of code \nnames that are incor\u00adrectly classi.ed, and thus cannot be found. For example, the API reference of CharStream \ncontains the sentence: ...being matched after the last call to BeginTOken. Here, BeginTOken is a method, \nbut Algorithm 1 classi.es it as a type. The number from J2SE is much higher than the other API libraries. \nWe checked the J2SE s API reference, and we found that some packages follow quite dif\u00adferent naming conventions. \nFor example, we .nd that many types of the org.omg packages follow the C naming convention, and in these \npackages, ARG IN is an interface, instead of a .eld. H\u00f8st and \u00d8stvold [17] show that naming conventions \ncan be used to detected bugs in method names. We consider code names that violate nam\u00ading conventions \nas bad smells in API libraries, since these names are misleading. False alarms. In Table 3, column False \nalarm lists false alarms. Subcolumn Er lists the number of false alarms that are related to false Er. \nThese code names contain the following types: 1. User code elements. When describing API usage, developers \nof API libraries may introduce code elements that should be im\u00adplemented by programmers. As described \nin Section 3.4, D OCREF .lters this type of code elements by speci.c words of their names. However, in \nsome cases, such code names do not contain our de\u00ad.ned words, and thus are not .ltered. For example, \nthe API refer\u00adence of RecursiveAction26 contains the following code sample: class SortTask extends RecursiveAction{ \n... protected void compute() { sequentiallySort(array, lo, hi); ... }} In the above code, sequentiallySort \nis a method that should be implemented by programmers. Our approach fails to .lter the these code names \nwith the technique described in Section 3.4. 25 http://xmlgraphics.apache.org/batik/javadoc/org/ apache/batik/transcoder/print/PrintTranscoder. \nhtml 26 http://docs.oracle.com/javase/7/docs/api/java/ util/concurrent/RecursiveAction.html Name Outdated \nBizarre Camel Typo Lucene 54 22 15 5 Batik 18 14 16 4 Hadoop 118 5 15 6 Uimaj 12 14 12 11 J2SE 160 119 \n91 62 Total 362 174 149 88 Table 4. Broken code names. 2. Unavailable code names. To reminder themselves, \ndevelopers of API libraries may mention API elements that are unavailable or under implementation in \nAPI references. For example, the API ref\u00aderence of FacetRequest27 contains the sentence: TODO (Facet): \nadd AUTO EXPAND option. Here, AUTO EXPAND is not imple\u00admented yet, and thus cannot be found. Both types \nof code names should be .ltered from Er, since programmers do not need any references for them. Subcolumn \nEd list the number of false alarms that are related to incomplete Ed. We .nd that some code samples are \nin languages other than Java: 1. Mathematic equations. Developers of API libraries may use equations \nto explain their algorithms. For example, the API refer\u00adence of NumericRangeQuery28 contains the mathematic \nequation: indexedTermsPerValue = ceil(bitsPerValue / precisionStep) 2. Regular expressions. Developers \nof API libraries may use regu\u00adlar expressions to explains syntaxes or grammars. For example, the API \nreference of RegExp29 contains the following regular expres\u00adsions to explain the Automaton: regexp ::= \nunionexp | unionexp ::= interexp | unionexp (union) ... 3. SQL. Some API libraries are related to databases, \nand their de\u00advelopers may use SQL to explain database related usage. For exam\u00adple, the API reference \nof CachedRowSet30 contains the sentence: Column numbers are generally used when the RowSet object s command \nis of the form SELECT * FROM TABLENAME... 4. XML. Developers may use XML to present sample .les. For \nexample, the API reference of WebRowSet31 contains the following code sample in XML:  <properties> <command>select \nco1, col2 from test_table</command> <concurrency>1</concurrency> <datasource/> ... 5. C++. Some API libraries \nsupport multiprogramming, and their API references may contain code samples in multiple programming languages. \nFor example, we .nd that the API reference of the org. apache.hadoop.record package32 contains the following \ncode sample in C++: 27 http://lucene.apache.org/core/4 1 0/facet/org/ apache/lucene/facet/search/params/FacetRequest. \nhtml 28 http://lucene.apache.org/core/4 1 0/core/org/ apache/lucene/search/NumericRangeQuery.html 29 \nhttp://lucene.apache.org/core/4 1 0/core/org/ apache/lucene/util/automaton/RegExp.html 30 http://docs.oracle.com/javase/7/docs/api/javax/ \nsql/rowset/CachedRowSet.html 31 http://docs.oracle.com/javase/7/docs/api/javax/ sql/rowset/WebRowSet.html \n32 http://hadoop.apache.org/docs/current/api/org/ apache/hadoop/record/package-summary.html namespace \nhadoop { enum RecFormat{ kBinary, kXML, kCSV }; class InStream { public: virtual ssize_t read(void *buf, \nsize_t n) = 0; ... Our underlying island parser analyzes only Java code. As a result, our tool fails \nto extract Ed from code samples in the above languages and incorrectly reports the corresponding code \nnames as mismatches. Subcolumn Dic lists the number of false alarms that are relat\u00aded to incomplete dictionaries. \nThe corresponding code names con\u00adtain the following types: 1. Existing standards or speci.cations. Developers \nof API li\u00adbraries may refer to speci.cations. For example, the API reference of LinearTransfer33 contains \nthe sentence: This class de.nes the Linear type transfer function for the feComponentTransfer .l\u00adter, \nas de.ned in chapter 15, section 11 of the SVG speci.cation. As explained in the sentence, feComponentTransfer \nis de.ned in an external speci.cation. Developers of API libraries may also refer to existing inter\u00adnational \nstandards. For example, the API reference of ICC Pro\u00adfileGray34 contains a sentence, ...and the pro.le \ncontains the grayTRCTag and mediaWhitePointTag tags . Here, grayTRCTag and mediaWhitePointTag are de.ned \nby the International Color Consortium (ICC).35 2. Tool or company names. Developers may describe an \nexternal tool or a company in API references. For example, the API refer\u00adence of RecordStore contains \nthe sentence: A Wmf .le can be produced using the GConvert utility... Here, GConvert is the name of an \nexternal tool.  We did not add the above names to the customized dictionary of our tool. As a result, \nour tool adds these names to Er, and fails to .nd them in Ed or EAPI . Subcolumn EAPI lists the number \nof false alarms that are re\u00adlated to incomplete EAPI . Our tool adds packages, types, variables, and \nmethods to EAPI . However, API libraries provide more names that should be added to EAPI : 1. De.nitions \nof .les or objects. Developers of API libraries may de.ne formats of .les in API references. The de.nitions \nof .les can be formal. For example, the API reference of BlockTreeTerms-Writer36 de.nes .tim .les as \nfollows: TermsDict(.tim)--> Header, Postings Metadata... Block--> SuffixBlock, StatsBlock, MetadataBlock \n... Here, the preceding names (e.g., Header)explain the structure of .tim .les. The de.nitions of .les \ncan also be informal. For example, the API reference of Marshaller37 enumerates its supported system \nproperties (e.g., jaxb.encoding). API library Developers may de.ne the formats of objects in API references. \nFor example, the API reference of Configuration38 de.nes the following object: 33 http://xmlgraphics.apache.org/batik/javadoc/org/ \napache/batik/ext/awt/image/LinearTransfer.html 34 http://docs.oracle.com/javase/7/docs/api/java/ awt/color/ICC \nProfileGray.html 35 http://www.color.org/index.xalter 36 http://lucene.apache.org/core/4 1 0/core/org/ \napache/lucene/codecs/BlockTreeTermsWriter.html 37 http://docs.oracle.com/javase/7/docs/api/javax/ xml/bind/Marshaller.html \n38 http://docs.oracle.com/javase/7/docs/api/javax/ security/auth/login/Configuration.html Name{ModuleClass \nFlag ModuleOptions; ModuleClass Flag ModuleOptions; ModuleClass Flag ModuleOptions; }; Sometimes, such \nde.nitions are quite short. For example, the API reference of Date39 de.nes a Date object: Converts this \nDate object to a String of the form: dow mon dd hh:mm:ss zzz yyyy... The latter sentences explain the \nde.nition: dow is the day of the week (Sun, Mon, Tue, Wed, Thu, Fri, Sat)... API library developers may \neven de.ne valid values for parame\u00adters or properties in API references. For example, the API reference \nof AWTPermission40 de.nes all the valid inputs for its constructor (e.g., accessClipboard). 2. File names. \nDevelopers of API libraries may introduce related .les names. For example, the API reference of SystemFlavorMap41 \ncontains the sentence: ...the default SystemFlavorMap is initial\u00adized by the .le jre/lib/.avormap.properties... \nHere, flavormap. properties is a .le shipped with J2SE. 3. Incomplete or abbreviation names. Developers \nof API libraries may use incomplete code names, if they do not introduce any ambiguities. For example, \nthe API reference of the Rendering-Hints .eld42 contains the sentence: ALPHA INTERPOLATION hint is a \ngeneral hint that provides... The following sentence explains the code name: The allowable values for \nthis hint are VALUE ALPHA INTERPOLATION SPEED, ...  Developers may also use abbreviated names of API \nelements. For example, the API reference of RenderableImageOp43 con\u00adtains the sentence: ...then the CRIF.create() \nmethod is called... . We found that another sentence44 explains CRIF: The name Con\u00adtextualRenderedImageFactory \nis commonly shortened to CRIF. Our tool did not add these names to EAPI , so it reported their references \nas mismatches. 4.1.3 Summary In summary, our evaluation results show that DOCREF detects more than 1,000 \ndocumentation errors and smells from the latest API references of .ve popular API libraries. The results \ndemonstrate the effectiveness of our approach, since all the detected errors and smells are previously \nunknown, and some of them have already been con.rmed and .xed immediately after we reported them. Due \nto the complexity of its research problem, DOCREF also reports some false alarms, which we discuss in \nmore details in Section 5.  4.2 RQ2: Accuracies of D OCREF 4.2.1 Study Setup We selected the API reference \nof the analysis45 package as the subject to investigate the second research question. In total, the subject \ncontains one package page, one interface page, and 16 39 http://docs.oracle.com/javase/7/docs/api/java/ \nutil/Date.html 40 http://docs.oracle.com/javase/7/docs/api/java/ awt/AWTPermission.html 41 http://docs.oracle.com/javase/7/docs/api/java/ \nawt/datatransfer/SystemFlavorMap.html 42 http://docs.oracle.com/javase/7/docs/api/java/ awt/RenderingHints.html \n43 http://docs.oracle.com/javase/7/docs/api/java/ awt/image/renderable/RenderableImageOp.html 44 http://docs.oracle.com/javase/7/ \ndocs/api/java/awt/image/renderable/ ContextualRenderedImageFactory.html 45 http://lucene.apache.org/core/4 \n1 0/core/org/ apache/lucene/analysis/package-summary.html Name Code sample Code name Mismatch P R F \nP R F P R F analysis 78.9% 100.0% 88.2% 99.7% 89.1% 94.1% 50.0% 100.0% 66.7% Analyzer 100.0% 100.0% 100.0% \n100.0% 80.5% 89.2% 66.7% 100.0% 80.0% GlobalReuseStrategy n/a n/a n/a 100.0% 93.8% 96.8% n/a n/a n/a \nPerFieldReuseStrategy n/a n/a n/a 100.0% 88.9% 94.1% 100.0% 100.0% 100.0% ReuseStrategy n/a n/a n/a 100.0% \n95.2% 97.6% n/a n/a n/a TokenStreamComponents n/a n/a n/a 100.0% 95.0% 97.4% 100.0% 100.0% 100.0% AnalyzerWrapper \nn/a n/a n/a 100.0% 65.6% 79.2% 100.0% 100.0% 100.0% CachingTokenFilter n/a n/a n/a 100.0% 89.7% 94.5% \n100.0% 100.0% 100.0% CharFilter n/a n/a n/a 100.0% 44.0% 61.1% n/a 0.0% n/a NumericTokenStream 100.0% \n100.0% 100.0% 100.0% 88.0% 93.6% n/a n/a n/a NumericTermAttributeImpl 100.0% 100.0% 100.0% 100.0% 95.2% \n97.5% n/a n/a n/a Token 100.0% 100.0% 100.0% 100.0% 73.6% 84.8% 100.0% 100.0% 100.0% TokenAttributeFactory \nn/a n/a n/a 100.0% 66.7% 80.0% n/a n/a n/a TokenFilter n/a n/a n/a 100.0% 83.3% 90.9% n/a n/a n/a Tokenizer \nn/a n/a n/a 100.0% 91.3% 95.5% 100.0% 100.0% 100.0% TokenStream n/a n/a n/a 100.0% 77.9% 87.6% n/a n/a \nn/a TokenStreamToAutomaton n/a n/a n/a 100.0% 87.5% 93.3% n/a n/a n/a NumericTermAttribute n/a n/a n/a \nn/a 0.0% n/a n/a n/a n/a Total 86.7% 100.0% 92.9% 99.9% 82.9% 90.6% 85.7% 96.0% 90.6% Table 5. Precision \n(P), recall (R), and F-score (F) of DOCREF. class pages. We manually examined these pages and compared \nthe manual results with D OCREF s results with the following metrics: 1. True positive (TP). An item \nthat is correctly identi.ed by DOC-REF. 2. False positive (FP). An item that is not a code name/code \nsample/mismatch, but is misidenti.ed by D OCREF. 3. False negative (FN). An item that is a code name/code \nsam\u00adple/mismatch, but was not identi.ed by D OCREF.  Based on these measures, we calculate standard \nprecision, re\u00adcall, and F-score of D OCREF: T P Precision = T P + F P (3) Recall = T P T P + F N (4) \nF -score = 2 \u00d7 P recision \u00d7 Recall P recision + Recall (5) 4.2.2 Results Table 5 shows our results. \nColumn name lists the short names of the package, the interface, and the classes. Column Code sample \nlists the result of identifying code samples. D OCREF achieves high recall, but relatively low precision, \nsince it may incorrectly identify outputs as code samples. For example, an output is as follows: Here \nis the new output: This: Noun demo: Unknown ... DOCREF identi.es it as a code sample, since it is within \na pre tag. If we treat the above output as a plain text, D OCREF will not identify the output as a code \nsample, since the output does not satisfy our punctuation criterion for code samples. Column Code name \nlists the result of identifying code names. DOCREF achieves high precision, but relatively low recall. \nNames of some API elements are natural words, and our spell checker does not identify them as typos. \nFor example, the API reference of the analysis package contains the sentence: Covers Analyzer and related \nclasses. Here, D OCREF fails to identify Analyzer as a class name. The recall can be improved, if we \nde.ne more strict rules for spell checking. For example, it is odd in NL to capitalize the .rst character \nof a noun in the middle of a sentence. If we de.ne a rule for the above odd usage, DOCREF can identify \nAnalyzer as a code name correctly. Column Mismatch list the result of identifying mismatches. In the \nsubject package, D OCREF identi.es code elements that should be implemented by programmers as mismatches, \nwhich reduces the precisions. D OCREF fails to identify code elements whose names are natural words. \nAs a result, it fails to identify some mismatches, which reduces its recall. We further discuss this \nissue in Section 5. Our approach achieves similar precision and recall for most doc\u00aduments. However, \nwe also observe that a few documents are quite different from the others, and our approach achieves much \nlower precision and recall on these documents. For example, as shown in Maalej and Robillard [23], only \nabout 1% API documents describe the concepts of API libraries. In these documents, authors can in\u00adtroduce \nde.nitions of .les or objects, and our approach cannot cor\u00adrectly analyze these de.nitions as we discussed \nin Section 4.1.2. There are also a few documents that contain code samples in lan\u00adguages other than Java, \nand our approach cannot extract Ed for these code samples. Thus, most of the false alarms in Table 3 \nwere introduced by a small set of documents. In summary, the results show that our approach achieves \nrea\u00adsonably high precision, recall, and F-score.  4.3 Threats to Validity The threat to external validity \nconcerns the representativeness of the evaluation subjects. Although we applied our approach on .ve popular \nlibraries, our approach is evaluated only on their API ref\u00aderences. The threat could be mitigated in \nfuture work by adddi\u00adtoinal evaluations on more subjects such as tutorials, wiki pages, and forum discussions. \nThe threat to internal validity concerns hu\u00adman factors for determining the types of detected mismatches. \nTo reduce this threat, we inspected mismatches carefully and contact\u00aded the developers to con.rm bugs. \nThe threat could be reduced by con.rming more bugs with developers in future evaluations. 5. Discussions \nand Future Work In this section, we discuss limitations of our approach and avenues for future work. \n Reducing false alarms. The false alarm rate of our approach is about 40%. It is reasonable, but it can \nbe further reduced. Sec\u00adtion 4.1.2 provides many concrete examples of false alarms, which provide valuable \nhints to reduce false alarms. For example, Sec\u00adtion 4.1.2 shows that many code samples are in languages \nother than Java. Synytskyy et al. [36] propose an island parser for multi\u00adple programming languages. \nIn future work, we plan to adapt their parser to analyze more languages, to help reduce false alarms. \nAs another example, Section 4.1.2 shows that many terms are de.ned in existing standards or speci.cations. \nIn future work, we also plan to extract and add these terms to our underlying dictionary, to help further \nreduce false alarms. Classifying reported mismatches. Our current implementation does not classify mismatches, \nand we have to manually classify them. It is desirable to classify them automatically. Our evaluation \nresults provide valuable hints for classi.cation. For example, if a mismatch can be found in EAPI of \nprevious versions, it should be an out-of-date code name. As another example, if a mismatch can be found \nin private code names of API implementations, it should be a bad smell. In future work, we plan to work \ntowards this direction, so that we can reduce the manual effort to identify them. Analyzing API documentations \nin other formats and program\u00adming languages. Some API documentations are using formats other than HTML \n(e.g., PDF and CHM). To analyze such docu\u00admentations, we can extract and feed plain text to D OCREF, \nsince DOCREF includes a code-extraction technique for plain text (Sec\u00adtion 3.1). Many API documentations \nare for programming lan\u00adguages other than Java. To analyze such documentations, we need to extend DOCREF \nin three aspects. First, we need to revise Algo\u00adrithm 1, since other programming languages may follow \nnaming conventions that are different from Java. Second, we need different strategies to construct complete \ncode from code fragments, since other programming languages may have different code construc\u00adtors. Third, \nwe need corresponding island parsers to extract Er and Ed from the constructed code. 6. Related Work \nThis section discusses related work and how they differ from our approach. Analyzing API documentations. \nDagenais and Robillard [8] an\u00adalyze the production model of writing open source API documenta\u00adtions. \nMaalej and Robillard [23] analyze natures of API references. Buse and Weimer [4] propose an approach \nto generate comments for exception clauses via code analysis. Dekel and Herbsleb [11] propose eMoose \nthat pushes and highlights those rule-containing sentences from API documentation for developers. Kim \net al. [18] propose an approach that enriches API documents with code sam\u00adples mined from code repository. \nZhong et al. [40] mine API usage as patterns, and use patterns as documentations to aid program\u00adming. \nOur approach addresses a different research question from the previous work. Zhong et al. [41] propose \nan approach that infers resource spec\u00adi.cations from API references. Pandita et al. [28] propose an ap\u00adproach \nthat infers pre-conditions and post-conditions from code comments. Tan et al. [37] propose an approach \nthat infers rules (e.g., call sequences between two methods) from code comments. Tan et al. [38] propose \nan approach that infers pre-conditions of exceptions from code comments. The inferred rules are effective \nto detect defects in client code, but are not as effective to detect errors in documentations, since \nmost documents describe correct rules. Our approach is effective to detect many errors in API documenta\u00adtions, \ncomplementing the previous work. Extracting code samples from informal documents. Dagenais and Robillard \n[9] propose an approach that recovers links between an API and its learning resources. One step of their \napproach ex\u00adtracts code from learning resources. Bacchelli et al. [1, 2] propose approaches that extract \ncode from emails and recover links between emails and source code artifacts. Bacchelli et al. [3] propose \nan ap\u00adproach that classi.es development emails into source code, junk, patch, stack trace, and natural \nlanguage text. Rigby and Robil\u00adlard [30] propose an approach that extracts code samples from in\u00adformal \ndocuments. Our approach also includes an underlying tech\u00adnique that extracts code samples and code names \nfrom informal documents. The main difference between our approach and previ\u00adous approaches lies in that \nour approach relies on the punctuation frequency and the NL-error ratio, whereas the previous approaches \nrely on island parsing. Our approach is more general, since it does not need to implement multiple island \nparsers to extract code in different programming languages. Analyzing requirement documents. Kof [20] \npropose an ap\u00adproach that uses part-of-speech (POS) tagging to identify missing objects and actions in \nrequirement documents. Sawyer et al. [32] propose an approach that uses POS and semantic tagging to support \nrequirement synthesis. Fantechi et al. [13] propose an approach that extracts uses cases from requirement \ndocuments. Xiao et al. [39] infers security policies from functional requirements. Le et al. [21] propose \nan approach that infers mobile scripts from natural lan\u00adguage descriptions. Hindle et al. [16] conducted \nan empirical study that uses statistical language models to analyze the naturalness of software. Our \napproach uses NLP techniques to detect out-of-date code name in API usage documents, and our documents \nare quite different in contents and structures from documents analyzed in previous work (e.g., requirement \ndocuments). 7. Conclusion API documentations such as API references, tutorials, forum dis\u00adcussions, and \ndevelopment emails are an essential channel for pro\u00adgrammers to learn unfamiliar API usage. However, \nthese API doc\u00adumentations can also contain errors or smells that may mislead or frustrate programmers. \nIn this paper, we have proposed the .rst approach that detects errors for API documentations. We conduct\u00aded \nan extensive evaluation on Javadocs of .ve widely used API li\u00adbraries. The results show that our approach \ndetects more than 1,000 previously unknown errors with relatively high precision and recall. We have \nreported detected errors to the developers of API libraries. Some of the reported errors were con.rmed \nand .xed shortly after we reported them. Acknowledgments We appreciate the anonymous reviewers for their \nconstructive com\u00adments. Hao Zhong s work is supported by the National Natural Sci\u00adence Foundation of \nChina No. 61100071. Zhendong Su s work is supported by the United States NSF grants 0917392 and1117603. \nReferences [1] A. Bacchelli, M. D Ambros, and M. Lanza. Extracting source code from e-mails. In Proc. \n18th ICPC, pages 24 33, 2010. [2] A. Bacchelli, M. Lanza, and R. Robbes. Linking e-mails and source code \nartifacts. In Proc. 32nd ICSE, pages 375 384, 2010. [3] A. Bacchelli, T. Dal Sasso, M. D Ambros, and \nM. Lanza. Content classi.cation of development emails. In Proc. 34th ICSE, pages 375 385, 2012. [4] R. \nBuse and W. Weimer. Automatic documentation inference for exceptions. In Proc. ISSTA, pages 273 282, \n2008.  [5] B. Carpenter and B. Baldwin. Text analysis with LingPipe 4. LingPipe Inc, 2011. [6] C. E. \nChaski. Empirical evaluations of language-based author identi\u00ad.cation techniques. Forensic Linguistics, \n8:1 65, 2001. [7] B. Dagenais and L. J. Hendren. Enabling static analysis for partial Java programs. \nIn Proc. 23rd OOPSLA, pages 313 328, 2008. [8] B. Dagenais and M. P. Robillard. Creating and evolving \ndeveloper documentation: understanding the decisions of open source contribu\u00adtors. In Proc. 18th FSE, \npages 127 136, 2010. [9] B. Dagenais and M. P. Robillard. Recovering traceability links be\u00adtween an API \nand its learning resources. In Proc. 34rd ICSE, pages 47 57, 2012. [10] S. de Souza, N. Anquetil, and \nK. de Oliveira. A study of the docu\u00admentation essential to software maintenance. In Proc. 23rd SIGDOC, \npages 68 75, 2005. [11] U. Dekel and J. D. Herbsleb. Improving API documentation usability with knowledge \npushing. In Proc. 31st ICSE, pages 320 330, 2009. [12] E. Duala-Ekoko and M. P. Robillard. Asking and \nanswering questions about unfamiliar APIs: An exploratory study. In Proc. 34rd ICSE, pages 266 276, June \n2012. [13] A. Fantechi, S. Gnesi, G. Lami, and A. Maccari. Applications of linguistic techniques for \nuse case analysis. Requirement Engineering, 8(3):161 170, 2003. [14] I. S. Fraser and L. M. Hodson. Twenty-one \nkicks at the grammar horse: Close-up: Grammar and composition. English journal, 67(9):49 54, 1978. [15] \nJ. Gosling, B. Joy, G. Steele, and G. Bracha. The Java Language Speci.cation, Java SE 7 Edition. 2012. \n[16] A. Hindle, E. T. Barr, Z. Su, M. Gabel, and P. Devanbu. On the naturalness of software. In Proc. \n34th ICSE, pages 837 847, 2012. [17] E. W. H\u00f8st and B. M. \u00d8stvold. Debugging method names. In Proc. 23rd \nECOOP, pages 294 317, 2009. [18] J. Kim, S. Lee, S.-W. Hwang, and S. Kim. Enriching documents with examples: \nA corpus mining approach. ACM Transactions on Information Systems, 31(1):1, 2013. [19] D. Klein and C. \nD. Manning. Accurate unlexicalized parsing. In Proc. 41st ACL, pages 423 430, 2003. [20] L. Kof. Scenarios: \nIdentifying missing objects and actions by means of computational linguistics. In Proc. 15th RE, pages \n121 130, 2007. [21] V. Le, S. Gulwani, and Z. Su. SmartSynth: Synthesizing smartphone automation scripts \nfrom natural language. In MobiSys, to appear, 2013. [22] T. C. Lethbridge, J. Singer, and A. Forward. \nHow software engineers use documentation: The state of the practice. Software, IEEE, 20(6): 35 39, 2003. \n[23] W. Maalej and M. P. Robillard. Patterns of knowledge in API refer\u00adence documentation. IEEE Transactions \non Software Engineering, to appear. [24] T. Mens and T. Tourw \u00b4e. A survey of software refactoring. \nIEEE Transactions on Software Engineering, 30(2):126 139, 2004. [25] M. Milkowski. Developing an open-source, \nrule-based proofreading tool. Software: Practice and Experience, 40(7):543 566, 2010. [26] J. Nykaza, \nR. Messinger, F. Boehme, C. L. Norman, M. Mace, and M. Gordon. What programmers really want: results \nof a needs assess\u00adment for SDK documentation. In Proc. 20th SIGDOC, pages 133 141, 2002. [27] N. Nystrom, \nM. Clarkson, and A. Myers. Polyglot: An extensible compiler framework for Java. Compiler Construction, \n2622:138 152, 2003. [28] R. Pandita, X. Xiao, H. Zhong, T. Xie, S. Oney, and A. Paradkar. In\u00adferring \nmethod speci.cations from natural language API descriptions. In Proc. 34th ICSE, pages 815 825, 2012. \n[29] R. Prieto-D\u00b4iaz. Status report: Software reusability. Software, IEEE, 10(3):61 66, 1993. [30] P. \nC. Rigby and M. P. Robillard. Discovering essential code elements in informal documentation. In Proc. \n35th ICSE, page 11, 2013. [31] M. P. Robillard and R. DeLine. A .eld study of API learning obstacles. \nEmpirical Software Engineering, 16(6):703 732, 2011. [32] P. Sawyer, P. Rayson, and R. Garside. REVERE: \nSupport for require\u00adments synthesis from documents. Information Systems Frontiers, 4(3): 343 353, 2002. \n[33] D. Schreck, V. Dallmeier, and T. Zimmermann. How documentation evolves over time. In Proc. IWPSE, \npages 4 10, 2007. [34] F. Sebastiani. Machine learning in automated text categorization. ACM computing \nsurveys, 34(1):1 47, 2002. [35] L. Shi, H. Zhong, T. Xie, and M. Li. An empirical study on evolution \nof API documentation. In Proc. FASE, pages 416 431, 2011. [36] N. Synytskyy, J. R. Cordy, and T. R. Dean. \nRobust multilingual parsing using island grammars. In Proc. CASCON, pages 266 278, 2003. [37] L. Tan, \nD. Yuan, G. Krishna, and Y. Zhou. /* iComment: Bugs or Bad Comments?*/. In Proc. 21st SOSP, pages 145 \n158, 2007. [38] S. H. Tan, D. Marinov, L. Tan, and G. T. Leavens. @tComment: Testing Javadoc comments \nto detect comment-code inconsistencies. In Proc. 5th ICST, pages 260 269, 2012. [39] X. Xiao, A. Paradkar, \nS. Thummalapenta, and T. Xie. Automated extraction of security policies from natural-language software \ndocu\u00adments. In Proc. 20th FSE, pages 12:1 12:11, 2012. [40] H. Zhong, T. Xie, L. Zhang, J. Pei, and H. \nMei. MAPO: Mining and recommending API usage patterns. In Proc. 23rd ECOOP, pages 318 343, 2009. [41] \nH. Zhong, L. Zhang, T. Xie, and H. Mei. Inferring resource speci.ca\u00adtions from natural language API documentation. \nIn Proc. 24th ASE, pages 307 318, 2009.   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>When programmers encounter an unfamiliar API library, they often need to refer to its documentations, tutorials, or discussions on development forums to learn its proper usage. These API documents contain valuable information, but may also mislead programmers as they may contain errors (<i>e.g.</i>, broken code names and obsolete code samples). Although most API documents are actively maintained and updated, studies show that many new and latent errors do exist. It is tedious and error-prone to find such errors manually as API documents can be enormous with thousands of pages. Existing tools are ineffective in locating documentation errors because traditional natural language (NL) tools do not understand code names and code samples, and traditional code analysis tools do not understand NL sentences. In this paper, we propose the first approach, DOCREF, specifically designed and developed to detect API documentation errors. We formulate a class of inconsistencies to indicate potential documentation errors, and combine NL and code analysis techniques to detect and report such inconsistencies. We have implemented DOCREF and evaluated its effectiveness on the latest documentations of five widely-used API libraries. DOCREF has detected more than 1,000 new documentation errors, which we have reported to the authors. Many of the errors have already been confirmed and fixed, after we reported them.</p>", "authors": [{"name": "Hao Zhong", "author_profile_id": "83358867957", "affiliation": "Institute of Software, Chinese Academy of Sciences, Beijing, China &#38; University of California, Davis, Davis, CA, USA", "person_id": "P4290466", "email_address": "zhonghao@nfs.iscas.ac.cn", "orcid_id": ""}, {"name": "Zhendong Su", "author_profile_id": "81100108298", "affiliation": "University of California, Davis, Davis, CA, USA", "person_id": "P4290467", "email_address": "su@cs.ucdavis.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509523", "year": "2013", "article_id": "2509523", "conference": "OOPSLA", "title": "Detecting API documentation errors", "url": "http://dl.acm.org/citation.cfm?id=2509523"}