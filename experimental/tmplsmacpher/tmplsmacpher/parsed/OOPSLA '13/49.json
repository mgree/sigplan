{"article_publication_date": "10-29-2013", "fulltext": "\n Relaxed Separation Logic: A Program Logic for C11 Concurrency Viktor Vafeiadis Max Planck Institute \nfor Software Systems (MPI-SWS) viktor@mpi-sws.org Abstract We introduce relaxed separation logic (RSL), \nthe .rst pro\u00adgram logic for reasoning about concurrent programs running under the C11 relaxed memory \nmodel. From a user s per\u00adspective, RSL is an extension of concurrent separation logic (CSL) with proof \nrules for the various kinds of C11 atomic accesses. As in CSL, individual threads are allowed to access \nnon-atomically only the memory that they own, thus pre\u00adventing data races. Ownership can, however, be \ntransferred via certain atomic accesses. For SC-atomic accesses, we per\u00admit arbitrary ownership transfer; \nfor acquire/release atomic accesses, we allow ownership transfer only in one direction; whereas for relaxed \natomic accesses, we rule out ownership transfer completely. We illustrate RSL with a few simple ex\u00adamples \nand prove its soundness directly over the axiomatic C11 weak memory model. Categories and Subject Descriptors \nD.3.1 [Programming Languages]: Formal De.nitions and Theory; F.3.1 [Logics and Meanings of Programs]: \nSpecifying and Verifying and Reasoning about Programs Keywords Concurrency; Weak memory models; C/C++; \nProof system; Separation logic 1. Introduction Wanting to enable many hardware and software optimiza\u00adtions, \nmodern programming language de.nitions provide rather weak guarantees on the semantics of concurrent \nmem\u00adory accesses allowing, for example, different threads to ob\u00adserve shared operations happening in \ndifferent orders. One such case is the concurrency model adopted by the 2011 re\u00advisions of the C and \nC++ standards (ISO/IEC 9899:2011; Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright is held by the owner/author(s). Publication \nrights licensed to ACM. ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509532 \nChinmay Narayan Indian Institute of Technology, Delhi chinmay@cse.iitd.ac.in ISO/IEC 14882:2011), which \nwe will study in this paper and refer to as the C11 model. C11 provides several kinds of memory accesses \nnon\u00adatomic, relaxed atomic, acquire atomic, release atomic, and sequentially consistent (SC) atomic each \nproviding differ\u00adent consistency guarantees. On the one end of the spectrum, races on non-atomic accesses \nresult in completely unde.ned behaviour (they are treated as programming errors); on the other end, SC-atomic \naccesses are globally synchronized. The guarantees provided by relaxed, acquire, and release ac\u00adcesses \nlie somewhere in between: different threads can ob\u00adserve them happening in different orders. The reason \nfor having all these kinds of accesses is that they map differently to the various common architectures, \nand have very different implementation costs. Non-atomic and relaxed atomic accesses are generally rather \ncheap as they correspond to vanilla machine loads and stores, and may be reordered by the compiler and/or \nby an out-of-order execution unit. At the other end of the spectrum, SC accesses are very expensive because \ntheir implementation involves a full memory barrier. The cost of acquire and release accesses depends \na lot on the architecture. On x86, they are compiled down to plain reads and writes (Batty et al. 2011) \nand are therefore cheap. On PowerPC and ARM, the cost is some\u00adwhat higher as they induce a memory barrier, \nbut of a weaker kind than full memory barriers (Sarkar et al. 2012). Our goal is to help C11 programmers \nby providing them with sound reasoning principles for concurrent programs. We show that C11 concurrency \nsupports resource reasoning in the style of separation logic (O Hearn 2007); in particular, ownership \ncan be transferred along acquire/release atomic memory accesses and does not require SC-accesses. We \ndevelop relaxed separation logic (RSL), a program logic that follows the resourceful reading of separation \nlogic  triples. When we assert the Hoare triplePCmdQ, we say that the command Cmd will not access any \nmemory other than that given by its precondition, P , or subsequently acquired during its execution. \nWe thus support the parallel composition rule of separation logic, P1Cmd1Q1P2Cmd2Q2 (PA R) P1 * P2Cmd1!Cmd2Q1 \n* Q2  which ensures that the two threads do not have any races on non-atomic memory accesses, a condition \nrequired by C11. To handle acquire/release atomics, we introduce two new assertion kinds, Rel(\u00a3, Q1) \nand Acq(\u00a3, Q2). These denote respectively the permissions to perform a release-write of some value v \nat location \u00a3 and give away ownership of the resource described by Q1(v), or an acquire-read and gain \nownership of Q2(v). With these assertion forms we provide simple proof rules for release writes and acquire \nreads, similar to those for releasing and acquiring mutual exclusion locks in concurrent separation logic. \nBesides RSL itself, the main contribution of this work was to de.ne the meaning of Hoare triples in a \nrelaxed memory model setting, so as to prove the soundness of RSL. This was rather challenging for three \nmain reasons. No global state/time: Traditionally, P Cmd Q asserts that if we execute Cmd in an initial \nstate satisfying P and it terminates, then the .nal state will satisfy Q. In C11 concurrency, however, \nthe terms initial state and .nal state are ill-de.ned, because there exist no global notions of time \nor state. To interpret triples, we thus resort to logical local notions of time and state. We de.ne a \nlogical notion of local state at each event of a program execution, and thread the logical state through \nC11 happens-before edges. Assertions in heaps: Our assertions for dealing with ac\u00adquire and release atomics \nrequire that the logical heaps used to interpret them contain assertions. This results in a circularity \nin the model of assertions, which for sim\u00adplicity we resolve by storing syntactic assertions. No operational \nsemantics: Concurrent program logics are typically proved sound over an operational or a trace se\u00admantics. \nIn either case, the meaning of Hoare triples can be de.ned in terms of an auxiliary predicate by induction \nover the length of an execution trace. These de.nitions cannot directly be extended to the C11 model \nas there is no obvious total order for the induction. Our solution is to order the C11 events according \nto the total number of events that happen before them. As a secondary contribution, we observed that \nthe seman\u00adtics of relaxed atomic memory accesses in C11 is too weak to permit even the most basic reasoning \nprinciples about them, which in turn renders basic compiler optimizations unsound. In order to allow \nsuch reasoning principles, we proposed a crude .x to C11, which we discuss in Section 6. In the remainder \nof this paper, we de.ne a minimal con\u00adcurrent programming language (\u00a72), review the C11 concur\u00ad rency \nmodel (\u00a73), describe the assertions and proofs rules of RSL (\u00a74), verify a few examples using RSL (\u00a75), \nexplain the problems caused by relaxed accesses and their resolution (\u00a76), present the semantics of assertions \nand Hoare triples and sketch the main parts of the soundness proof (\u00a77). We conclude with a discussion \nof related and future work (\u00a78). A Coq formalization of the soundness proof of RSL can be found at the \nfollowing URL. http://www.mpi-sws.org/~viktor/rsl/  2. Programming Language In order to focus on the \nconcurrency aspects of C11 and to avoid the inherent complexity of a large language like C, we introduce \na minimal concurrent programming language featuring the various kinds of memory accesses supported by \nC11. Following Batty et al. (2012), we omit consume reads from the model, because they are relevant only \nfor a few architectures (PowerPC and ARM) and substantially complicate the model. For simplicity, we \nalso omit memory fences. To make the local sequential execution order explicit, we present the grammar \nof expressions in A-normal form (cf. Flanagan et al. 1993). Atomic expressions, e . AExp, consist of \nvariables and values (locations and numbers). Program expressions, E . Exp, consist of atomic expres\u00adsions, \nlet-bound computations, conditionals, loops, parallel composition, memory allocation, loads, stores, \nand atomic compare-and-swap (CAS) instructions. v . Val ::= \u00a3 | n where \u00a3 . Loc, n . N e . AExp ::= x \n| v where x . Var E . Exp ::= e | let x = E in E' | if e then E else E' | repeat E end | E1!E2 | alloc() \n| [e]X | [e]Y := e' | CASZ,W (e, e', e'') where X . { sc, acq, rlx, na}, Y . {sc, rel, rlx, na}, Z . \n{ sc, rel_acq, acq, rel, rlx}, W . {sc, acq, rlx} As in C, in conditional expressions we treat zero \nas false and non-zero values as true. The construct repeat E end executes E repeatedly until it returns \na non-zero value. Memory accesses are annotated by their mode: sequen\u00adtially consistent (sc), acquire \n(acq), release (rel), combined release-acquire (rel_acq), relaxed (rlx), or non-atomic (na). According \nto the C standard, not all modes are available for all accesses: reads cannot be releases, writes cannot \nbe ac\u00adquires, CASs cannot be non-atomic. These restrictions are to avoid redundancy in the language. \nFor example, an ac\u00adquire write, if such a thing were allowed, would behave ex\u00adactly the same as a relaxed \nwrite. CAS is an atomic operation used to heavily in lock-free concurrent algorithms. It takes a location, \n\u00a3, and two values, v' and v'', as arguments. It atomically checks if the value at location is v' or not. \nIf the value is same as v', then CAS succeeds: it atomically writes v'' to \u00a3 and returns the old value. \nIf the value is different, CAS fails: it returns that value and does not modify the location. CAS is \nannotated with two access modes: one to be used for the successful case, and one for the unsuccessful \ncase. For conciseness in examples, we will often write expres\u00adsions such as [E]na instead of let x = \nE in [x]na. We also write E1; E2 instead of let x = E1 in E2 when x /. fv(E2). new_lock () = let x = \nalloc() in [x]rel := 1; x spin(x) = repeat [x]rlx end lock (x) = repeat spin(x); CASacq,rlx(x, 1, 0) \nend unlock (x)= [x]rel := 1 Figure 1. Simple spinlock implementation. Spinlock Example There are two \nimportant uses of ac\u00adquire/release accesses: in implementing locks, and in mes\u00adsage passing. Relaxed \naccesses are useful in cases of opti\u00admistic reads, where the value read, if it is of interest to the \nalgorithm, will be read again by an acquire read, or an ac\u00adquire fence will be issued. For example, in \nthe simple CAS\u00adbased spinlock implementation shown in Figure 1, lock (x) performs an acquire-on-success \nCAS and unlock (x) does a release write. The optimistic spin(x) loop that waits for the lock to become \nfree, in contrast, does relaxed reads. The combined release-acquire CAS is supposed to be used for operations \nthat atomically release one lock and acquire another this is possible, for example, if the locks are \nrepre\u00adsented as different bits of the same word. Further examples can be found in McKenney and Garst \n(2011). 3. The C11 Memory Model The C11 memory model is de.ned axiomatically in terms of program executions. \nA program execution consists of a set of actions and several binary relations over them. Actions describe \nthe memory operations performed by the program, and are labelled with information about the memory order \nof the operation, the address accessed and the values read and/or written. Act ::= skip | W(sc|rel|rlx|na)(\u00a3, \nv) | R(sc|acq|rlx|na)(\u00a3, v) | RMW(sc|rel_acq|acq|rel|rlx)(\u00a3, v, v ' ) | A(\u00a3) In summary, we have a no-op \naction; SC, release, relaxed and non-atomic writes; SC, acquire, relaxed, and non-atomic reads; atomic \nread-modify-write actions; and allocations. The no-op (skip) action represents local computations, thread \nforks and joins. For the subset of C11 we consider, an execution contains the following relations:1 \nSequenced-before (sb) relates actions of the same thread that follow one another in control .ow. We have \nsb(a, b) if a and b belong to the same thread and a immediately precedes b in the thread s control .ow, \nor a is a fork action and b the .rst action of the forked thread, or b is a join action and a the last \naction of the joined thread.  The reads-from map (rf) maps every read action r to the write action w \nthat wrote the value read by r.  1 The full model includes two additional relations, dd (data dependency) \nand dob (dependency ordered before), used to de.ne the happens-before relation for consume reads. The \nmemory-order relation (mo) is a total order on the store actions writing to the same atomic location. \n The sequential-consistency order (sc) is a total order over all SC-atomic actions.  Formally, let \nAName be a countably in.nite of action names. Then, an execution, X , is represented as a tuple, (A, \nlab, sb, rf, mo, sc), where A ..n AName is the set of action names included in the execution, lab . A \n. Act maps every action identi.er to its label, rf . A -A is the reads-from map, and sb, mo, sc . P(A \n\u00d7 A) are the sequenced-before relation, the memory order, and the sequential consistency order respectively. \nFrom these relations, C11 de.nes a number of derived relations, the most important of which are: the \nsynchronizes\u00adwith relation and the happens-before order. Synchronizes-with (sw) relates acquire reads \nwith the re\u00adlease writes that precede in mo order the write whose value was read by the acquire read \nprovided that all the writes between these two writes belong to the same thread or are RMW operations. \n Happens-before (hb) is a partial order on actions formal\u00adizing the intuition that one action was completed \nbefore the other. In the C11 subset we consider, hb = (sb.sw)+ .  The semantics of a program is given \nby the set of con\u00adsistent executions. An execution is said to be consistent if it satis.es the axioms \nof the memory model, which will be presented shortly. If, however, any of these consistent execu\u00adtions \ncontains a data race on non-atomic actions, i.e. events generated from two con.icting operations on the \nsame non\u00adatomic location not ordered by hb in either direction, then the program is deemed to have arbitrary \nsemantics. Thus, any sound program logic for C11 concurrency must ensure its speci.cations imply race-freedom \nfor non-atomic actions. Expression Semantics Let CExp denote closed expres\u00adsions (i.e., ones with no \nfree variables). The semantics of such closed expressions, [E], is given in Figure 2 as a set of tuples \n(res , A, lab, sb, fst, lst). These tuples represent .nite complete executions as well as .nite incomplete \nexecution pre.xes (used to model in.nite executions), where: (1) res is the result of evaluating the \nexpression or . if the execution is incomplete; (2) A is the set of all actions contained in the execution; \n (3) lab labels the actions with the corresponding operations; (4) sb represents the sequenced-before \nrelation; and (5) fst and lst are the .rst and last actions in the sb-order. For uniformity, we record \nthe last action even in incomplete executions. In the parallel composition case, the auxiliary function \ncombine(res 1, res 2) returns res 1 if res 2 . and  = . otherwise. In the [E] semantics, allocations \ncan return an arbitrary new location, and reads can read an arbitrary value. These will later be constrained \nby the consistency axioms. [- [v [alloc() ' [[v]Z := v [[v]Z ] [CASX,Y (v, vo, vn)] [let x = E1 in E2] \n[repeat E end][E1!E2] .x. hb(x, x) .\u00a3. totalorder({a . A | iswritec(a)}, mo) . hbc . mo totalorder({a \n. A | isSeqCst(a)}, sc) . hbSeqCst . sc . moSeqCst . sc .b. rf(b) = . .. .\u00a3, a. iswritec(a) . isreadc(b) \n. hb(a, b) .a, b. rf(b) = a =. .\u00a3, v. iswritec,v(a) . isreadc,v(b) . \u00achb(b, a) .a, b. rf(b) = a . (mode(a) \n= na . mode(b) = na) =. hb(a, b) .a, b. rf(b) = a . isSeqCst(b) =. isc(a, b) . \u00acisSeqCst(a) . (.x. isc(x, \nb) . \u00achb(a, x)) .a, b. hb(a, b) . mo(rf(b), rf(a)) . locs(a) = locs(b) .a, b. hb(a, b) . mo(rf(b), a) \n. iswrite(a) . locs(a) = locs(b) .a, b. hb(a, b) . mo(b, rf(a)) . iswrite(b) . locs(a) = locs(b) .a. \nisrmw(a) . rf(a) = . =. mo(rf(a), a) . .c. mo(rf(a), c) . mo(c, a) (res1, A1, sb1, fst1, lst1) . [E1] \n. (res2, A2, sb2, fst2, lst2) . [E2] . afork, ajoin . AName} Figure 2. Semantics of closed program expressions. \ndef where iswritec,v(a) = def isreadc,v(a) = def rsElem(a, b) = def rseq(a) = def sw = def hb = def hbc \n= def XSeqCst = def isc(a, b) = : CExp . P((res : Val . {.}, A : P(AName), lab : A . Act, sb : P(A \u00d7 \nA), fst : A, lst : A)) def = {(v, {a}, lab, \u00d8, a, a) | a . AName . lab(a) = skip} def = {(\u00a3, {a}, lab, \n\u00d8, a, a) | a . AName . \u00a3 . Loc . lab(a) = A(\u00a3)} def ' = {(v , {a}, lab, \u00d8, a, a) | a . AName . lab(a) \n= WZ (v, v ' )} def ' ' = {(v , {a}, lab, \u00d8, a, a) | a . AName . v . Val . lab(a) = RZ (v, v ' )} def \n ' '' = {(v , {a}, lab, \u00d8, a, a) | a . AName . v . Val . v = vo . lab(a) = RY (v, v ' )}. {(vo, {a}, \nlab, \u00d8, a, a) | a . AName . lab(a) = RMWX (v, vo, vn)} def = {(., A1, lab1, sb1, fst1, lst1) | (., A1, \nlab1, sb1, fst1, lst1) . [E1]}. {(res2, A1 1 A2, lab1 . lab2, sb1 . sb2 . {(lst1, fst2)}, fst1, lst2) \n|(v1, A1, lab1, sb1, fst1, lst1) . [E1] . (res2, A2, lab2, sb2, fst2, lst2) . [E2[v1/x]]} def = {(resN \n, Ai, labi, sbi . {(lst1, fst2), . . . , (lstN-1, fstN )}, fst1, lstN ) | i.[1..N] i.[1..N] i.[1..N] \n.i. (resi, Ai, labi, sbi, fsti, lsti) . [E] . (i = N =. resi = 0) . resN = 0} def = {(combine(res1, \nres2), A1 1 A2 1 {afork, ajoin}, lab1 . lab2 . {afork . skip, ajoin . skip}, sb1 . sb2 . {(afork, fst1), \n(afork, fst2), (lst1, ajoin), (lst2, ajoin)}, afork, ajoin) | .a, b, \u00a3. lab(a) = lab(b) = A(\u00a3) =. a = \nb def (Irre.exiveHB) (ConsistentMO) (ConsistentSC) (ConsistentRFdom) (ConsistentRF) (ConsistentRFna) \n(RestrSCReads) (CoherentRR) (CoherentWR) (CoherentRW) (AtomicRMW) (ConsistentAlloc) .X, vold. lab(a) \n. {WX (\u00a3, v), RMWX (\u00a3, vold, v)} iswritec(a) = .v. iswritec,v(a) .X, vnew. lab(a) . {RX (\u00a3, v), RMWX \n(\u00a3, v, vnew)} etc. sameThread(a, b) . isrmw(b) {a} . {b | rsElem(a, b) . mo(a, b) . (.c. mo(a, c) . \nmo(c, b) . rsElem(a, c))} {(a, b) | mode(a) . {rel, rel_acq, sc} . mode(b) . {acq, rel_acq, sc} . rf(b) \n. rseq(a)} (sb . sw)+ {(a, b) . hb | iswritec(a) . iswritec(b)} {(a, b) . X | isSeqCst(a) . isSeqCst(b)} \niswritelocs(b)(a) . sc(a, b) . .c. sc(a, c) . sc(c, b) . iswritelocs(b)(c) Figure 3. Axioms satis.ed \nby consistent C11 executions, Consistent(A, lab, sb, rf, mo, sc). -o c : W(\u00a3, 1) a : R(\u00a3, 1) rf mohb \nt -o d : W(\u00a3, 2) b : R(\u00a3, 2) rf violates CoherentRR c : W(\u00a3, 2) -o a : W(\u00a3, 1) mohb t rf b : R(\u00a3, 2) \nviolates CoherentWR -o c : W(\u00a3, 1) a : R(\u00a3, 1) rf hb t mo b : W(\u00a3, 2) violates CoherentRW a rf-. \nb means a = rf(b) a mo- . b means mo(a, b) a hb-. b means hb(a, b)  Figure 4. Sample executions violating \ncoherency conditions (Batty et al. 2011). Consistent Executions According to the C11 model, an execution \nis consistent, Consistent(A, lab, sb, rf, mo, sc), if all of the properties shown in Figure 3 hold. (Irre.exiveHB) \nThe happens-before order, hb, must be ir\u00adre.exive: an action cannot happen before itself. (ConsistentMO) \nAll write actions on an atomic location \u00a3 must be totally ordered by mo, and be consistently or\u00addered \nby hb (restricted to the location \u00a3). (ConsistentSC) The sc relation must be a total order and include \nboth hb and mo restricted to SC actions. This in effect means that SC actions are globally synchronized. \n(ConsistentRFdom) The reads-from map, rf, is de.ned for those read (or RMW) actions for which the execution \ncontains an earlier write (or RMW) to the same location. (ConsistentRF) Each entry in the reads-from \nmap, rf, should map a read to an earlier or concurrent write to the same location and with the same value. \n(ConsistentRFna) Further, if a read reads from a write and either the read or write are non-atomic, then \nthe write must have happened before the read. Batty et al. (2011) also require the write to be visible: \ni.e. not to have been overwritten by another write that happened before the read. This extra condition \nis unnecessary, as it follows from CoherentWR. (RestrSCReads) SC reads are further restricted to read \nonly from the immediately preceding SC write to the same location in sc order or from a non-SC write \nthat has not happened before that immediately preceding SC write. (CoherentRR,CoherentWR,CoherentRW) \nNext, we have three per-location coherence properties relating mo, hb, and rf. These properties require \nthat mo never contra\u00addicts hb or the observed read order, and that rf never reads values that have been \noverwritten by more recent actions that happened before the read. These coherence properties are depicted \nin Figure 4. (AtomicRMW) Each read-modify-write action should exe\u00adcute atomically: it should read from \nthe immediately pre\u00adceding write in mo. (ConsistentAlloc) Finally, the same location cannot be allo\u00adcated \ntwice by different allocation actions.2 Remark Our model differs in a few minor ways from that of Batty \net al. (2011, 2012). First, we have incorporated the C standard s additional synchronized with (asw) \nrelation in sb rather than in sw, because it describes synchronization induced by control .ow rather \nthan by data .ow. Second, our sw-relation also relates acquire reads with release writes (whenever the \nread returns a value written by or after the release write), even if the two actions belong 2 This axiom \nsuf.ces, because we do not support deallocation. Had we in\u00adcluded deallocation, we would instead require \nthere to be a deallocation actions between any two allocation actions of the same location. The for\u00admalized \nC11 model by Batty et al. (2011, 2012) does not model allocation. let a = alloc() in let c = alloc() \nin [c]rlx := 0; [a]na := 7; repeat [c]acq end; [c]rel := 1 [a]na := [a]na + 1 Figure 5. Message passing \nexample showing transfer of ownership of the non-atomic location a. to the same thread (and are thus \nsb-related), whereas Batty et al. (2012) do not add any sb-related actions to the sw\u00adrelation. Since \nrelating such actions also by sw does not affect execution consistency, we do so for uniformity, which \neases the de.nition of validity of Hoare triples in \u00a77. Finally, in the standard, the sb and sw relations \nare taken to be strict partial orders, corresponding to the transitive closure of our relations. Conversely, \nour sb relation can be de.ned in terms of the sb order from the C and C++ standards as follows, sbour \n= {(a, b) . sbstd . aswstd | c. (a, c) . sbstd . aswstd . (c, b) . sbstd . aswstd}. Again, we found the \nnon-transitive versions slightly more convenient when de.ning the meaning of Hoare triples. 4. Relaxed \nSeparation Logic To motivate RSL, consider the message passing program shown in Figure 5. The thread \non the left updates some data structure using non-atomic memory accesses (here, the location a), and \nthen signals to other threads that the data structure has been updated by performing a release write \nto c. The thread on the right repeatedly performs acquire reads until it notices that [c] = 0. Then, \nit can conclude that the thread on the left has .nished its work, and so may safely access the data structure \nwithout interfering with it. This message passing idiom is correct (i.e., race-free) because whenever \nan acquire read sees the value written by a release write, the write synchronizes with the acquire read. \nThus, as hb is transitive, any event that happened before the write (e.g., by being sequenced before \nit), also happens before the read. This, in turn, justi.es the ownership transfer from the writing thread \nto the reading thread. To model such ownership transfers, RSL extends the grammar of separation logic \nassertions, P , with three new assertion forms, Rel(e, Q), Acq(e, Q), and RMWAcq(e, Q), where Q ranges \nover functions from values to assertions. Formally, RSL assertions are given by following grammar: k \n' P, Q ::= false | P . Q | .x. P | emp | e . e | P * Q | Rel(e, Q) | Acq(e, Q) | RMWAcq(e, Q) | Init(e) \n| Uninit(e) where k ranges over fractional permissions (Perm = (0, 1], see Boyland 2003). We have the \nusual classical .rst order logic constructs (the three primitive ones and the derived: true, ., ., \u00ac, \n.), the three assertions forms pertinent to separation logic (empty heap, a single memory cell with \n Pey. P . y = ePEy. Q P * REy. Q * RPE1x. Q.x.QE2y. R Plet x = E1 in E2y. RPEy. Q P ' . P .y. Q . \nQ' P . bE1y. Q P 'Ey. Q'P . \u00acbE2y. Q Pif b then E1 else E2y. Q P E y. Q P 'Ey. Q' PEy. QQ[0/y] . P \nP . P 'Ey. Q . Q' Prepeat E endy. Q . y 0 = PEy. QP1E1y. Q1P2E2Q2  .x. PEy. .x. Q P1 * P2E1dE2y. Q1 \n* Q2 Figure 6. Standard proof rules supported by RSL. fractional permission k, and separating conjunction), \nand .ve new forms, which we will explain shortly. RSL judgements are of the form P E y. Q , where P and \nQ are assertions respectively denoting the precondition and the postcondition of the expression E. The \npostcondi\u00adtion, Q, also describes the return value of the expression E, which is bound by the variable \ny. In cases where the post\u00adcondition does not describe the return value, we often omit the y binder. \nWith this setup, we support all the standard rules from Hoare and separation logic (see Figure 6) includ\u00ad \ning the so-called structural rules: the frame, consequence, disjunction, and existential rules. Another \ngeneric rule we support is the R E L A X rule be\u00adlow. Generally, when reasoning about a program E, we \nare always allowed to reason about a relaxation of the program E ' [ E, which is identical to E except \non the atomic access annotations, which may be weaker than those of E accord\u00ading to the partial order: \nrlx [ rel [ sc, rlx [ acq [ sc. P E ' y. Q P E E ' [ E y. Q (R E L A X) Atomic Writes We return to the \ntreatment of atomic mem\u00ad ory accesses and the new assertion forms. The .rst one, Rel(\u00a3, Q), represents \na permission to write any value v to location \u00a3, provided the assertion Q(v) holds separately. Per\u00adforming \nthe write consumes the Q(v) assertion so that it can be transferred to the reader(s). Q(v) * Init(\u00a3) \n* [\u00a3]rel := v(W-R E L) Rel(\u00a3, Q)Rel(\u00a3, Q) In order for the ownership transfer to be valid, the writer \nmust synchronize with the reader(s), which means that the write must be at least of release kind (or \nstronger, namely SC). Besides the ownership transfer, the write also initializes the location \u00a3. Keeping \ntrack of initialized locations is neces\u00adsary for subsequent proof rules. In the special case when no \nownership transfer occurs (i.e., when P = emp), intuitively we can also use a relaxed write as in the \nfollowing rule. Rel(\u00a3, v, emp) [\u00a3]rlx := v Init(\u00a3) (W-R L X *) In this rule, we used the following shorthand \nnotation def Rel(\u00a3, v, P ) = Rel(\u00a3, .x. if x = v then P else false) for representing the permission \nto write only the value v and release ownership of P (in this case emp). Intuitive though this rule is, \nit is unfortunately unsound in C11, as we will explain in Section 6, where we also show that we can restore \nits soundness by mildly strengthening the model. In RSL, we allow multiple concurrent writes to the same \natomic location by making the permission to perform an atomic write splittable as follows: Rel(\u00a3, Q1) \n* Rel(\u00a3, Q2) (R E L -S P L I T) .. Rel(\u00a3, .v. Q1(v) . Q2(v)) Of course, programs that perform multiple \nconcurrent writes to same location and transfer away ownership may leak memory, as some of the writes \nmay be overwritten and thus never read. In this paper, however, we do not regard such memory leaks as \nan error. If desired, the programmer may explicitly count the number of allocations and deallocations \nin order to prove that the program has no memory leaks. Similar to write permissions, the fact that a \nlocation has been initialized captured by Init(\u00a3) can be freely dupli\u00adcated. Once a location is initialized, \nit remains initialized: it cannot be de-initialized. Init(\u00a3) .. Init(\u00a3) * Init(\u00a3) (I N I T-S P L I T) \n Atomic Reads The second assertion form, Acq(\u00a3, Q), de\u00adnotes a permission to perform an acquire read \nof location \u00a3 and obtain ownership of Q(v), where v is the value read. .x. precise(Q(x)) (R-AC Q) Init(\u00a3) \n* v. Q(v) * [\u00a3]acq Acq(\u00a3, Q)Acq(\u00a3, Q[v:=emp]) The premise of the rule (that Q should be precise) is \na tech\u00adnical requirement that will be explained in Section 7 and may be ignored for the time being. As \na precondition, we require not only the permission to perform an acquire read from \u00a3, but also the knowledge \nthat the location has been initialized. The latter is needed because reading from unini\u00adtialized locations \nmay return any arbitrary value and thus we cannot ensure that Q(v) was ever established. When reading \na value, we acquire Q(v) and give up the permission to read the same value again with ownership transfer, \nbecause oth\u00aderwise it would have been possible to acquire the same Q(v) multiple times. Therefore, in \nthe postcondition the assertion attached to the acquire predicate becomes def Q[v:=emp] = .y. if y=v \nthen emp else Q(y) . This allows further reads of the same value, but consequent reads will simply not \ngain any ownership. At any point, it is also possible to do a relaxed read and acquire no ownership. \nInit(\u00a3) * Acq(\u00a3, Q) [\u00a3]rlx Acq(\u00a3, Q) (R-R L X)  Note that this rule does not assert anything about the \nvalue read. A more useful rule is the following, which asserts that the value read must be one that may \nhave been written. Init(\u00a3) * v. Acq(\u00a3, Q) . [\u00a3]rlx (R-R L X *) Acq(\u00a3, Q) (Q(v) = false) Similar to W-R \nL X *, this latter rule is not sound in C11, but is so in the strengthened model of Section 6. In RSL, \nwe permit multiple readers to read the value writ\u00adten by a single release write. Concretely, consider \nthe sce\u00adnario where thread A initializes two data structures and sig\u00adnals by a release write that it \nhas .nished its work. Then thread B can do an acquire read and notice that A has .n\u00adished its initialization \nand then access the .rst data structure non-atomically. Likewise, thread C can do an acquire read and \naccess the second data structure non-atomically. Such an execution does not have data races and should \ntherefore be permitted. In terms of our program logic, this means that acquire read permissions should \nbe splittable and joinable as follows: Acq(\u00a3, Q1) * Acq(\u00a3, Q2) (AC Q -S P L I T) .. Acq(\u00a3, .v. Q1(v) \n* Q2(v)) Read-Modify-Write Instructions The next new assertion form, RMWAcq(\u00a3, Q), is used in the following \nproof rule for atomic compare-and-swaps. P . Init(\u00a3) * RMWAcq(\u00a3, Q) * true P * Q(v) . Rel(\u00a3, Q ' ) * \nQ ' (v ' ) * R[v/y] X . {rel, rlx} . Q(v) = emp X . {acq, rlx} . Q' (v ' ) = emp P [\u00a3]Y y. y = v . R \n(C A S*) P CASX,Y (\u00a3, v, v ' ) y. R The rule has .ve premises. First, the precondition must ensure that \nwe have permission to do a RMW-read from \u00a3 and acquire ownership of Q(v). Second, we require the up\u00addate \nperformed by the successful CAS to be valid: that is, to have the necessary release permission, to satisfy \nQ ' (v ' ), the assertion that is to be transferred away, and to separately also satisfy the postcondition. \nAs a precondition for this up\u00addate, we get to assume not only that the initial precondition holds, but \nalso that we have access to the state acquired by ownership transfer, Q(v). The next two premises take \nthe access modes into ac\u00adcount, suitably restricting the ownership that can be acquired or released. \nIf the successful CAS is of release or relaxed kind, then it does not synchronize with the write whose \nvalue it read, so it should not acquire any ownership. This is en\u00adsured by demanding that Q(v) = emp. \nSymmetrically, if the successful CAS is of acquire or relaxed kind, it does not synchronize with the \nreads seeing the value it produced, so it should not release any ownership. This is ensured by demanding \nthat Q ' (v ' ) = emp. Finally, we require that failed CASs also satisfy the postcondition, R, under \nthe assumption that a value different from the expected one was read. In its general form, the C A S* \nrule is sound in the strength\u00ad ened model of Section 6. In the standard model, it is sound only when \nX . {rel_acq, sc}. Unlike multiple normal reads, multiple successful CAS instructions cannot all read \nfrom (and therefore potentially synchronize with) the same write. This follows from the AtomicRMW axiom, \nwhich requires RMW actions to read from the immediately preceding write in mo-order. There\u00adfore, it is \nsound to duplicate the RMW-acquire permission, RMWAcq(\u00a3, Q) (R M W-S P L I T) .. RMWAcq(\u00a3, Q) * RMWAcq(\u00a3, \nQ) because the semantics ensures that at most one process will effectively be able to use this permission \nat any given instant. In order to be able to prove the last premise of the C A S* rule, we also support \nthe following rule, allowing us to carve out a plain acquire permission from an RMW-acquire one. .v. \nQ ' (v) = emp . Q(v) = Q ' (v) = false RMWAcq(\u00a3, Q) .. RMWAcq(\u00a3, Q) * Acq(\u00a3, Q ' ) (R M W-AC Q -S P \nL I T) The premise of R M W-AC Q -S P L I T ensures that the assertion that we have carved out for plain \nreads is empty, except perhaps for the values where Q(v) is false, in which case Q ' (v) may also be \nfalse. Allocation of Atomic Locations Whenever a new atomic location is allocated, the veri.er is free \nto choose a suitable ownership assertion Q and attach it to the newly allocated location, and moreover \nto choose whether the ownership of Q will be acquired using plain reads or using successful CASs. We \nthus have the following two rules. emp alloc() \u00a3. Rel(\u00a3, Q) * Acq(\u00a3, Q) (A-R) emp alloc() \u00a3. Rel(\u00a3, Q) \n* RMWAcq(\u00a3, Q) (A-M) Following the C standard, newly allocated locations are not initialized, and thus \ndo not generate the Init(\u00a3) permission required for reading them. To enable reading from these lo\u00adcations, \nthe programmer must .rst initialize them by per\u00adforming a plain write as we have already seen. Non-Atomic \nLocations Finally, the rules for non-atomic accesses are exactly as in concurrent separation logic. Al\u00adlocation \nreturns an uninitialized new cell with full permis\u00adsion; writing requires full permission of a location \n(whether initialized or not), whereas reading works also with partial permission but requires the location \nto be initialized. emp alloc() x. Uninit(x) (A-NA) 11 \u00a3 . _ . Uninit(\u00a3)[\u00a3]na := v\u00a3 . v(W-NA) kk \u00a3 . \nv[\u00a3]nax. x = v . \u00a3 . v(R-NA)  def Let QJ (v)= (v = 0 . emp) . (v = 1 . J) def Lock(x, J )= Rel(x, QJ \n) * RMWAcq(x, QJ ) * Init(x) def def new_lock () = lock (x) = J Lock(x, J ) let x = alloc() in repeat \nJ * Rel(x, QJ ) * Lock(x, J ) RMWAcq(x, QJ ) spin(x); [x]rel := 1 Lock(x, J ) Lock(x, J ) CASacq,rlx(x, \n1, 0) . . . . def y. Lock(x, J ) * unlock (x) = any invariant J may be attached to a lock so that we \nget the same speci.cations as in concurrent separation logic: J new_lock () x. Lock(x, J ) Lock(x, J \n) lock (x) J * Lock(x, J ) J * Lock(x, J ) unlock (x) Lock(x, J ) Lock(x, J ) .. Lock(x, J ) * Lock(x, \nJ ) As expected, creating a lock requires the invariant J to hold initially and returns a token con.rming \nthat the lock exists and protects the invariant J. Acquiring the lock requires this J * Lock(x, J ) y \n= 1 . J . .. token and obtains ownership of the invariant. Conversely, y = 0 . emp [x]rel := 1 releasing \nthe lock requires the invariant to hold and transfers end Init(x) * Lock(x, J ) J * Lock(x, J ) Lock(x, \nJ ) Figure 7. Veri.cation of the lock module. it away. Finally, the token saying that x is a lock protecting \n resource J can be freely duplicated. To derive this speci.cation, we de.ne the predicates: def Let \nQ(x) = if x = 0 then emp else a . 7 emp let a = alloc() in Uninit(a) let c = alloc() in Uninit(a) * Rel(c, \nQ) * Acq(c, Q) [c]rlx := 0; Uninit(a) * Rel(c, Q) * Acq(c, Q) * Init(c) Uninit(a) * Rel(c, Q) Acq(c, \nQ) * Init(c) [a]na := 7 repeat [c]acq end a . 7 * Rel(c, Q) true * a . 7 [c]rel := 1 [a]na := [a]na + \n1 Rel(c, Q) true * a . 8 a . 8 * true Figure 8. Veri.cation of the message passing example. These rules \nensure that all accessed location have been al\u00adlocated and there are no races on non-atomic memory loca\u00adtions, \nand moreover that only initialized locations are read. 5. Examples We now illustrate RSL by proving simple \nrace-free programs involving release-acquire synchronization patterns. Owner\u00adship transfer along those \nrelease-acquire synchronizations is necessary to prove them correct, that is, to show that they are memory \nsafe and do not contain data races. To make the proof outlines more concise, we de.ne the following short\u00adhand \nnotations. def Emp = .v. emp def IAcq(\u00a3, v, P ) = Init(\u00a3) * Acq(\u00a3, Emp[v := P ]) def IRMWAcq(\u00a3, v, P \n) = Init(\u00a3) * RMWAcq(\u00a3, Emp[v := P ]) Figure 7: Lock Module As our .rst example, we consider the lock \nmodule introduced in Figure 1. Here we show that def QJ (v) = (v = 0 . emp) . (v = 1 . J) def Lock(x, \nJ ) = Rel(x, QJ ) * RMWAcq(x, QJ ) * Init(x) The QJ (v) predicate describes the invariant associated \nwith the location x implementing the lock. It assigns empty own\u00adership when the lock is held (v = 0 . \nemp), and ownership of the invariant, J, when the lock is free (v = 1 . J). The Lock(x, J ) predicate \ncontains permissions to access the lock by performing release-writes and acquire-RMWs. It also contains \nthe knowledge that the lock is initialized. In new_lock (), we use the A-M and W-R E L rules to ini\u00adtialize \nthe location and transfer away the ownership of J. Similarly, in unlock (x), we use the W-R E L to transfer \naway the ownership of J and then the I N I T-S P L I T rule to remove the duplicate Init(x) fact. In \nlock (x), we use the R-R L X rule for the relaxed optimistic read in the spin(x) loop, and then the C \nA S* and the R-R L X * rules to deal with the CAS. Fi\u00adnally, the fact that the Lock(x, J ) predicate \ncan be freely du\u00adplicated follows immediately from R E L -S P L I T, R M W-S P L I T, and I N I T-S P \nL I T. Figure 8: Message Passing As our second example, we consider the message passing idiom of Figure \n5. Here, by constructing a proof, we conclude that the program has no data races and moreover, when both \nthreads terminate, we have [a] = 8. The proof illustrates the use of the Acq(-, -) predicate, and the \nrules A-NA, A-R, W-R L X *, W-NA, W-R E L, R-AC Q, and R-NA. Figure 9: Partial Ownership Transfer Our \nnext exam\u00adple is a variant of the message passing program we have just seen, where after the synchronization \nbetween the two threads, both threads read from a. This is valid because two concurrent read accesses \ndo not count as a data race. In order to verify this program, we use fractional permis\u00adsions and transfer \nthe partial ownership of the non-atomic lo\u00adcation a from the .rst to the second thread. The .rst thread \nwrites to a, and then performs a release write to x, giving  11111 a . _ * Rel(lock, 0, a . 2 . a . \n3) * Rel(lock, 1, emp) * IRMWAcq(lock, 0, a . 2 . a . 3) 111 a . _ * Rel(lock, 0, a . 2 . a . 3) [a]na \n:= 2; 111 a . 2 * Rel(lock, 0, a . 2 . a . 3) [lock]rel := 0; emp ( Rel(lock, 1, emp) * 1 1 IRMWAcq(lock, \n0, a . 2 . a . 3) if (CASacq,rlx(lock, 0, 1) = 0) then ( 11 (a . 2 . a . 3) * IRMWAcq(...) 11 * Rel(lock, \n1, a . 2 . a . 3) [a]na := 3 ( 1 a . 3 * IRMWAcq(lock, 0, ...) 11 * Rel(lock, 1, a . 2 . a . 3) [lock]rel \n:= 0; IRMWAcq(lock, 0, ...) endif ( Rel(lock, 1, emp) * 1 1 IRMWAcq(lock, 0, a . 2 . a . 3) if (CASacq,rlx(lock, \n0, 1) = 0) then ( 11 (a . 2 . a . 3) * IRMWAcq(...) 11 * Rel(lock, 1, a . 2 . a . 3) [a]na := 2 ( 1 a \n. 2 * IRMWAcq(lock, 0, ...) 11 * Rel(lock, 1, a . 2 . a . 3) [lock]rel := 0; IRMWAcq(lock, 0, ...) endif \n Figure 10. Example illustrating the use of CAS to implement a lock. 10.60.6 a . _ * Rel(x, 1, a . 2) \n* IAcq(x, 1, a . 2) a 1. _ * Rel(x, 1, a 0.6. 2) IAcq(x, 1, a 0.6. 2) [a]na := 2; let y = [x]acq in a \n1. 2 * Rel(x, 1, a 0.6. 2) (y=1) ? a 0.6. 2 : emp [x]rel := 1; assume(y = 1); a 0.4. 2 a 0.6. 2 [a]na \n[a]na r. r = 2 . a 0.4. 2 r. r = 2 . a 0.6. 2 Figure 9. Example illustrating fractional ownership and \ntransfer thereof. 0.6 away the partial permission a . 2 (using the W-R E L rule). 0.4 With its remaining \na . 2 permission, it then reads a us\u00ading the R-NA rule. The second thread synchronizes with the 0.6 write \nto x and gets the a . 2 permission (using the R-AC Q rule), after which it reads a and also gets the \nvalue 2 (using the R-NA rule). Figure 10: Transfer of Permission in Both Directions Our next example \ndemonstrates the use of CAS directly to implement a simple mutual exclusion lock. (We could of course \nuse the lock module veri.ed previously, but we in\u00adclude this example in order to demonstrate the C A \nS* rule again.) Here, for a change, we implement a non-blocking tryLock command using a conditional, \nrather than a blocking locking command using a loop. The lock is implemented by a single location, lock, \nstor\u00ading 0 if the lock is free and 1 if it is held. (This is oppo\u00adsite to the covention of the earlier \nlock module.) The lock protects a resource invariant describing the memory cell a. Initially, the .rst \nthread starts with the lock acquired and owning a: it establishes the resource invariant and releases \nthe lock. The other two threads start without knowing that the lock was initially held; they both have \nthe permission to write the value 1 to the lock without releasing any owner\u00ad let a = alloc() in [a]rlx \n:= 0; let b = alloc() in [b]rlx := 0; if 1 = [a]rlx then if 1 = [b]rlx then [b]rlx := 1 [a]rlx := 1 Figure \n11. Program with a possible dependency cycle. [Initialization actions not shown] Rrlx(a, 1) Rrlx(b, 1) \nrf sb sb t t rf Wrlx(b, 1) Wrlx(a, 1) Figure 12. Execution exhibiting the dependency cycle. ship, Rel(lock, \n1, emp). By itself, this permission is pretty useless: the threads can do a blind relaxed-atomic write \nto lock setting its value to 1 (acquired) but without gaining any information. What makes this permission \nuseful, is its com\u00adbination with the other permission they have, namely to read the state of an unacquired \nlock with a CAS and get owner\u00adship of the resource invariant. Successfully performing the CAS enables \nthem to later release the lock with the same resource invariant. 6. Dealing with Relaxed Memory Accesses \nA serious de.ciency of the C11 memory model is that it allows out of thin air reads, as illustrated by \nthe program in Figure 11, adapted from Batty et al. (2013). In this program, two locations are initialized \nwith the value 0, and then two threads are forked, each writing 1 to the one location provided the other \nalready has the value 1. Intuitively, one would expect that the writes would never be executed, but actually \nthe C11 concurrency model permits this outcome. The questionable execution, depicted in Fig\u00adure 12, is \nconsistent as the two reads can get the value 1 by reading from the corresponding conditional stores. \nThis counterintuitive behaviour is extremely problematic for formal reasoning as it inhibits even the \nsimplest form let a = alloc() in [a]rlx := 0; let b = alloc() in [b]rlx := 1; ; [b]rlx := [a]rlx [a]rlx \n:= [b]rlx ; if [a]rlx < 20 then print [a]rlx Figure 13. Program showing that range analysis is unsound \nunder C11. of thread-local reasoning, that of non-relational conjunctive invariants (i.e., invariants \nwhere each conjunct describes a property of only one variable). Intuitively, in the previous program, \none would expect the invariant [a]rlx = 0 . [b]rlx = 0 to hold throughout the parallel composition since \nit holds ini\u00adtially and is preserved by every reachable atomic statement of the program, arguing that \nthe conditional stores are not reachable because the conditions are unsatis.able accord\u00ading to the invariant. \nThis kind of reasoning is performed by standard compiler optimizations such as sparse conditional constant \npropagation (Wegman and Zadeck 1991). Note that with the W-R L X * and R-R L X * rules, we can easily \nprove that if we were to read [a] at the end of the program, we would get 0. (To do so, pick Q := (.x. \nx = 0) in the allocation rule for both locations.) This shows that these two rules are unsound under \nC11. Observe that the same problematic execution remains consistent even if we strengthen either the \nrelaxed reads to acquire/SC reads or (exclusively) the relaxed writes to release/SC writes. To make this \nexecution inconsistent, we have to strengthen both the reads and the writes except at most one access. \nThis means that even adding one of the W-R L X * and R-R L X * rules is unsound. Global Range Analysis \nA concrete optimization that is un\u00adsound under C11 is global range analysis. Consider the pro\u00adgram in \nFigure 13. An optimizing compiler may argue that the test [a]rlx < 20 will always succeed because [a] \nand [b] store either 0 or 1, and therefore replace the conditional ex\u00adpression by the then branch. Somewhat \nsurprisingly, un\u00adder C11, this transformation introduces new behaviour and is therefore unsound. Because \nof the causal dependency cy\u00adcle, the [a]rlx read can return any arbitrary value. Therefore, the transformed \nprogram can print any arbitrary value, while the original one could only print values less than 20. A \nCrude Fix to the Model Since even this very basic reasoning is unsound for relaxed accesses, we decided \nto strengthen the C11 concurrency model with the following axiom stating that hb . rf must be acyclic \n(i.e., its transitive closure must be irre.exive). acyclic(hb . {(rf(a), a) | a . A}) (StrongAcyclicHB) \ndef where acyclic(R) = x . A. R+(x, x). let a = alloc() in [a]rlx := 0; let b = alloc() in [b]rlx := \n0; let x = [a]rlx in let y = [b]rlx in [b]rlx := 1 [a]rlx := 1 Figure 14. Program without a dependency \ncycle. With this additional axiom, we can also show the sound\u00adness of the starred rules for relaxed \nmemory accesses pre\u00adsented in the previous section. In contrast, the soundness of the other rules does \nnot depend on this axiom. Notice that when adding this strong acyclicity condition, we can drop the strictly \nweaker Irre.exiveHB axiom, as well as the \u00achb(b, a) conjunct from the ConsistentRF axiom. We can further \ndrop the slightly awkward ConsistentRFna ax\u00adiom, and still have the soundness proof go through, because \nall the proof really needs to know is that the write precedes the read in some well-founded order. In \nthe absence of causal cycles, this order need not be hb: we can instead take it to be hb . rf. Simple \nthough our proposed .x might seem, it is not perfect. Alas, the StrongAcyclicHB consistency axiom pre\u00adcludes \nthe reordering of independent instructions, a trans\u00adformation that compilers and processors with out-of-order \nexecution units frequently perform. To illustrate the prob\u00adlem, consider the program in Figure 14, a \nslight variant of the program in Figure 11, where the writes to [b] and [a] are now independent of the \nearlier reads from [a] and [b] re\u00adspectively. The problem is that when operating at the level of single \nexecutions, one cannot distinguish whether the hb.rf cycle in the execution shown in Figure 12 constitutes \na de\u00ad pendency cycle or not. If the execution comes from the pro\u00adgram in Figure 11, the cycle should \nclearly be outlawed, but if it comes from the program of Figure 14, the cycle is harm\u00ad less and should \nbe allowed. Distinguishing these two cases is not easy and seems to require a radical change to the C11 \nmodel. Clearly, this lies beyond the scope of this paper. 7. Semantics and Soundness In this section, \nwe de.ne the semantics of assertions and Hoare triples, and prove that our logic is sound with respect \nto the C11 memory model. 7.1 Semantics of Assertions To de.ne the meaning of separation logic assertions, \nwe need an underlying separation algebra, i.e. a commutative partial monoid. To interpret the Acq and \nRel assertions, our model of heaps will have to store assertions, which in turn represent sets of heaps. \nIf we naively write down the domain equation, we will get an equation of the form, ? ~ Heap= Loc -(... \n+ (... \u00d7 P(Heap))) , spec spec which does not have a solution in Set. Therefore, we either have to move \nto a more advanced category such as bounded  def rval(b, Q) = if b then Q else .v. emp def adef(b1, \nQ1, b2, Q2) = Q1 .b1,b2 acq Q2 def = . .. .. Q1 if b1 . b2 and Q1 = Q2 .v. Q1(v) * Q2(v) if (\u00acb1 . \n\u00acb2) and adef(b1, Q1, b2, Q2) undef if \u00acadef(b1, Q1, b2, Q2) .v. rval(b2, Q1)(v) = rval(b1, Q2)(v) . \nQ1(v) = Q2(v) = false . . . . . . . . . h1(e) if e . dom(h1) \\ dom(h2) h2(e) if e . dom(h2) \\ dom(h1) \ndef h1 . ' h2 = .e. NA[v, k1 + k2] if hi(e) = NA[v, ki] for i = 1, 2 and k1 + k2 = 1 Atom[.v. R1(v) \n. R2(v), Q1 .b1,b2 Q2, b1 . b2, b ' 1 . b ' if hi(e) = Atom[Ri, Qi, bi, bi ' ] for i = 1, 2 acq 2] undef \notherwise def h1 . h2 = ( h1 . ' h2 if dom(h1 . ' h2) = dom(h1) . dom(h2) undef otherwise Figure 15. \nDe.nition of heap composition, h1 . h2. ultrametric spaces (Birkedal et al. 2010), or change the equa\u00adtion \nto avoid the problematic recursion. Here, for simplicity, we do the latter and cut the cycle by storing \nsyntactic assertions, Assn, instead of semantic assertions, P(Heap), within heaps. Simply storing syn\u00ad \nspec tactic assertions is, however, insuf.cient because we want the heap model to form a separation algebra \nand to support the conversions rules REL-SPLIT and ACQ-SPLIT. To allow these conversions, we therefore \nhave to store syntactic as\u00adsertions up to associativity and commutativity of * and . and their units. \nFurthermore, to support RMW-ACQ-SPLIT, we also need to equate false * false and false. Formally, we de.ne \n~ to be the smallest equivalence relation on syntactic assertions, equating the following assertions: \n(S1) .P, Q . Assn. P * Q ~ Q * P , (S2) .P, Q, R . Assn. P * (Q * R) ~ (P * Q) * R, (S3) .P . Assn. P \n* emp ~ P , (S4) false * false ~ false, (S5) .P, Q . Assn. P . Q ~ Q . P , (S6) .P, Q, R . Assn. P . \n(Q . R) ~ (P . Q) . R, and (S7) .P . Assn. (P . false) ~ (P . P ) ~ P . where, for convenience, we have \nalso included idempotence for disjunction. The model of heaps, Heap, therefore is: spec def def Perm \n= (0, 1] B = {true, false} def M = Val . Assn/~ def NA[U+ (Val \u00d7 Perm)] Heapspec = Loc \u00ad +Atom[M \u00d7 M \n\u00d7 B\u00d7 B] Each allocated location is either non-atomic or atomic. Non\u00adatomic locations can either be uninitialized \n(represented by special symbol U) or contain a value and a permission. Atomic locations contain two maps \nfrom values to syntac\u00adtic assertions modulo ~ and two Boolean .ags. The two maps represent the release \nand the acquire maps used to in\u00adterpret the three assertion forms pertinent to RSL: Rel(\u00a3, Q), Acq(\u00a3, \nQ), and RMWAcq(\u00a3, Q), with the .rst Boolean .ag indicating whether the second map acts as a plain acquire \nmap or as an RMW-acquire map. The second Boolean .ag records whether the location has been initialized \nor not. Figure 15 de.nes the composition of two logically dis\u00adjoint heaps, h1 . h2. Note that two logically \ndisjoint heaps can share some locations, provided that they store com\u00adpatible information about them. \nFor non-atomic locations, they should be initialized and have compatible permis\u00adsions (i.e., whose sum \ndoes not exceed the full permis\u00adsion, 1). For atomic locations, the two heaps must con\u00adtain compatible \nacquire maps, represented by the predicate adef(b1, Q1, b2, Q2). This predicate is somewhat complex because \nacquire maps represent plain acquire or RMW\u00adacquire permissions depending on the relevant Boolean .ag. \nThe cases are: (Case b1 . b2) we must have Q1 = Q2; (Case b1 . \u00acb2) we require that for all v, either \nQ2(v) = emp or Q1(v) = Q2(v) = false; (Case \u00acb1 . b2) symmetrically to the previous case; and (Case \n\u00acb1 . \u00acb2) no conditions. Given these de.nitions, we can show that (Heap, ., \u00d8) spec forms a separation \nalgebra, which in turn means that it is a good model for separation logic assertions. Lemma 1. (Heap, \n., \u00d8) forms a separation algebra. spec That is, . is associative, commutative, and has \u00d8 as its identity \nelement. In the proof of this lemma, property S4 is required to show associativity; replacing S4 with \nthe more general prop\u00aderty .P . Assn. P * false ~ false breaks associativity. We remark that in contrast \nto most models for separation logic, our . is not cancellative. For example, consider the heap hI = {\u00a3 \n. Atom[False, Emp, false, true]}. Clearly, hI = \u00d8 and yet hI . hI = hI = hI . \u00d8. In practice, the lack \nof cancellativity does not affect reasoning about RSL assertions. It also does not mean that the heap \nmodel contains junk information. Indeed, the heap hI is used to model the assertion Init(\u00a3), and we want \nhI . hI = hI to validate INIT-SPLIT. De.nition 1 (Assertion Semantics). Let [-] : Assn . P(Heap) be:spec \n[false] def \u00d8 = [P . Q] def = =. {h | h . [P ] h . [Q]}[.x. P ] def {h | .v. h . [P [v/x]]} = [emp] def \n= {\u00d8} [P * Q] def = {h1 . h2 | h1 . [P ] . h2 . [Q]} [Uninit(e)] def {{e . NA[U]}} = . v] def [e k= {{e \n. NA[v, k]}} [Init(e)] def {{e . Atom[False, Emp, false, true]}} = [Rel(e, Q)] def = {{e . Atom[Q, Emp, \nfalse, false]}}[Acq(e, Q)] def = {{e . Atom[False, Q, false, false]}} [RMWAcq(e, Q)] def {{e . Atom[False, \nQ, true, false]}} = def def where False = .v. false and Emp = .v. emp. Figure 16. De.nition of the semantics \nof assertions. Equipped with speci.cation heaps, Heap, we proceed spec to the semantics of assertions. \nThese are given as a function [-] : Assn . P(Heap) in Figure 16. spec A basic property of the assertion \nsemantics, that justi.es treating stored assertions up to ~, is that ~-related assertions have the same \nsemantics. Lemma 2. If P ~ Q, then [P ] = [Q]. Moreover, we can easily show that our model validates \nthe logical entailments of Section 4. Lemma 3. The properties R E L -S P L I T, AC Q -S P L I T, R M \nW-S P L I T, R M W-AC Q -S P L I T, and I N I T-S P L I T hold universally. Finally, we say that an assertion \nis precise if and only if it uniquely determines a subheap where it holds. The de.nition is standard \n(O Hearn 2007), but due of the lack of cancella\u00ad tivity of . we require both the heaps satisfying the \nassertion to be equal (h1 = h ' ) as well as their remainders (h2 = h ' ). 1 2 If . were cancellative, \nthen either of the equalities would suf.ce as it would imply the other. De.nition 2 (Precision). An assertion \nis precise, denoted precise(P ), if and only if for all h1, h ' 1, h2, h ' , if h1 . [P ] 2 and h2 . \n[P ] and h1 . h ' = h2 . h ' = undef, then 1 2 = h ' and h2 = h ' h11 2.  7.2 Semantics of Hoare triples \nWe move on to the meaning of RSL triples, P E y. Q . To handle both models the C11 standard one and the \nstrengthened one of Section 6 we parametrize the de.ni\u00ad tions of the semantics of triples and all auxiliary \nde.nitions with respect to the model. For notational simplicity, how\u00adever, we will present the de.nitions \nonly for the strength\u00adened model and we will note in text any differences for the standard C11 model. \nGiven an execution X = (A, lab, sb, rf, mo, sc), we de\u00ad.ne the helper functions: SBinX (a), SBoutX (a), \nSWinX (a), and SWoutX (a), to get the set of sb/sw incoming/outgoing edges of an action a . A. Given \nalso a set of actions, V . A, we denote the set of its hb and rf predecessors as PreX (V ). def PreX \n(V ) = {a | .b . V . hb(a, b) . a = rf(b)} This de.nition is very useful because we will generally be \nconsidering sets of actions V that are pre.x-closed, namely PreX (V ) . V , and we will be growing such \nsets by adding one action at a time while maintaining pre.x-closure. Doing so is always possible for \nconsistent executions because of the StrongAcyclicHB axiom. In the standard C11 model, we have to resort \nto a stronger de.nition of PreX (V ) that includes only the hb edges, not arbitrary rf edges as well. \nPrestandard_C11 def (V ) = {a | .b . V. hb(a, b)} X To de.ne the meaning of RSL triples, we will generally \nbe annotating hb-edges with appropriate heaps. When do\u00ading so, however, it will be important to distinguish \nbetween happens-before edges that occur because of an sb-edge and those that occur because of an sw-edge. \nWe therefore intro\u00adduce the following de.nition that tags them accordingly. De.nition 3 (Tagged Happens \nBefore). Given an execution X , let thbX be a tagged union of sbX and swX , constructed as follows def \nthbX = {( sb , a, b) | (a, b) . sbX } . {( sw , a, b) | (a, b) . swX } For a program expression, E, \nwe denote C[E] as the set of its consistent contextual executions. These executions are obtained by plugging \nin an execution of E in some arbitrary execution context, such that the whole execution is consistent, \nas follows. def = { (res , Actx, Aprg, X , fst, lst) | .labctx, labprg. C[E] .sb. .sbprg = sb n (Aprg\u00d7Aprg). \nX = (Actx 1 Aprg, labctx . labprg, sb, _, _, _). (res , Aprg, labprg, sbprg, fst, lst) . [E]. (.!a. sb(a, \nfst)) . (.!b. sb(lst, b)) . Consistent(X ) } In the de.nition of C[E], we require that (1) the part \nof the execution corresponding to the expression matches its se\u00admantics, (2) fst has a unique sb-predecessor, \n(3) lst has a unique sb-successor, and (4) the entire execution is consis\u00adtent. The requirements about \nthe unique predecessor of fst and the unique successor of lst will be used for selecting unique edges \nresponsible for carrying the expression s pre\u00adcondition and postcondition. To de.ne the meaning of RSL \ntriples, we will annotate the thb-edges of consistent contextual executions with heaps. We will call \nsuch functions, hmap : thbX -Heap, spec  De.nition 4 (Local annotation validity). Given an execution, \nX = (A, lab, sb, rf, mo, sc), a heap map, hmap : thbX -Heap, and a set of actions V . A, the predicate \nValid(hmap, V ) holds if and only if for all actions a . V , there exist spec \u00a3, v, Q, Q ' , Q '' , Z, \nh1, h ' 1, h2, hF, hsink such that . . lab(a) = skip lab(a) = A(e). hmap(SBinX (a)) = hmap(SBoutX (a)) \n. hsink .. . . hmap(SBoutX (a)) = hmap(SBinX (a)) . . . lab(a) = WZ (e, v) . Z . {rlx, rel, sc}. h1 \n{e . Atom[Q, Q, _, false]} ......... ......... lab(a) = A(e) = {e . Atom[Q, Emp, Emp, _])}. h ' 1 = \n{e . Atom[Q, Emp, Emp, true])} . . . hmap(SBoutX (a)) = hmap(SBinX (a)) . {e . NA[U]}lab(a) = Wna(e, \nv) . . h2 . = hmap(SWoutX (a)) . hsink . h2 . [Q(v)]. hmap(SBinX (a)) = h1 . h2 . hF 1 . hF . hmap(SBoutX \n(a)) = h ' .. . hmap(SBinX (a))(e) . {NA[U], NA[_, 1]}. hmap(SBoutX (a)) = hmap(SBinX (a))[e.NA[v, 1]] \n. . (Z = rlx =. Q(v) = emp) lab(a) = RMWZ (e, v, v ' ) . Z = na . . . . lab(a) = Rna(e, v) .. ............. \n. hmap(SBinX (a))(e) = Atom[_, Q, true, true] . hmap(SBinX (a)) . hmap(SWinX (a)) . . hmap(SBinX (a))(e) \n= NA[v, _] . hmap(SBinX (a)) = hmap(SBoutX (a)) . ..... . = {e . Atom[Q ' , Emp, false, false]} . hmap(SBoutX \n(a)) . hmap(SWoutX (a)) . hsink ............. lab(a) . {Rrlx(e, v), Racq(e, v), Rsc(e, v)} . ..... \n. hmap(SWinX (a)) . [Q(v)]. (hmap(SWoutX (a)) . hsink) . [Q ' (v . (Z . {rlx, rel} =. Q(v) = emp) ' \n. ' )] . hmap(SWinX (a)) . [Q(v)] . precise(Q(v)) . hmap(SBinX (a)) = {e . Atom[False, Q, Emp, true]} \n. hF . hmap(SBoutX (a)) = hmap(SWinX (a)) . hF . {e . Atom[False, Q[v := emp], Emp, true]} . (Z . {rlx, \nacq} =. Q ' (v ) = emp) De.nition 5 (Con.guration safety). Given sets of actions, Actx and Aprg, an \nexecution X = (Aprg 1 Actx, lab, sb, rf, sc), a natural number, n . N, a set of actions, V , a heap map, \nhmap : RespX (V ) . Heap, a distinguished .nal action, spec lst . Aprg, and a set of heaps, Q, we de.ne \nsafen X (V, hmap, Actx, Aprg, lst, Q) by structural recursion on n as follows: safe0 X (V , hmap, Actx, \nAprg, lst, Q) holds always. safen+1(V , hmap, Actx, Aprg, lst, Q) holds if and only if the following \nconditions all hold: X  If lst . V , then hmap(SBoutX (lst)) . Q; and  For all a . Aprg \\ V such that \nPreX ({a}) . V , there exists hmap ' : RespX ({a}) . Heapsuch that  spec Valid(hmap . hmap ' , V . \n{a}) and safen , Actx, Aprg, lst, Q); and X (V . {a}, hmap . hmap ' For all a . Actx \\ V such that PreX \n({a}) . V , and all hmap ' : RespX ({a}) . Heap, spec if Valid(hmap . hmap ' , V . {a}), then safen \n, Actx, Aprg, lst, Q). X (V . {a}, hmap . hmap ' Figure 17. De.nitions of annotation validity and con.guration \nsafety. heap annotations or heap maps. For each thb-edge, it is important to decide who is responsible \nfor choosing a heap to annotate that edge with: is it the program itself or is it its environment? Therefore, \ngiven a set of program actions A . A, we de.ne the set, RespX (A) of edges whose annotation is the responsibility \nof the program.  def RespX (A) = (SBoutX (a) . SWinX (a)) a.A This de.nition deserves some explanation. \nFirst, as expected, the program is responsible for cor\u00adrectly annotating its outgoing sequenced-before \nedges. Con\u00adversely, it can assume that incoming sb-edges are correctly annotated. This part is consistent \nwith the usual semantics of Hoare triples: the program may assume the precondition holds when starting \nits execution, and must establish the postcondition when returning. What is perhaps a bit unusual is \nthat the program is also responsible for the annotations on the incoming synchro\u00adnization edges, and \nnot the outgoing ones. This is because when an acquire read synchronizes with a release write, it is \nthe reader that knows how much state ownership is to be transferred along the sw-edge. The writer simply \nknows how much total ownership is to be transferred away from itself, but not how this is to be distributed \nto the various readers that synchronize with the write. In a slight abuse of notation, given a heap \nannotation, hmap, and a set of context edges, S . thbX , we will let def hmap(S) =hmap(x). x.Sndom(hmap) \n Annotation Validity and Con.guration Safety Figure 17 contains two important auxiliary de.nitions. First, \nwe have annotation validity (De.nition 4). A heap map, hmap is valid up to a set of actions V , if and \nonly if for every action a . V , the annotation is locally valid around that action: basically the sum \nof the annotated heaps on the incoming edges should equal the sum of the annotated heaps on the outgoing \nedges, modulo the effect of action a. Second, we have con.guration safety (De.nition 5), de\u00ad .ned in \nthe style of Vafeiadis (2011). Here, a con.guration is a set of visited actions, V . (Actx . Aprg), and \nheap annota\u00adtion, hmap, annotating precisely the thb-edges for which V is responsible. safen X (V, hmap, \n. . .) asserts that such a con\u00ad.guration is safe for at least n further actions. Unless n = 0, a safe \ncon.guration must: Annotate the (unique) sb-outgoing edge from the com\u00admand with a heap satisfying the \npostcondition in case the last action of the command is in V ;  For any ready-to-execute action a of \nthe command, it must be possible to extend the heap map so that it is safe also up to a for n - 1 actions; \nand  For any ready-to-execute action of the context, any valid extension of the heap map should be safe \nfor n - 1 actions.  The informal notion of action a being ready-to-execute is captured by the constraint \nthat a has not yet been visited whereas all its predecessors have: a /. V . Pre({a}) . V . With these \nauxiliary de.nitions, we de.ne the meaning of RSL triples as follows: De.nition 6 (Meaning of RSL triples). \nThe Hoare triple, P E y. Q , holds if and only if for all (res , Actx, Aprg, X , fst, lst) . C [E], for \nall V . Actx such that PreX (V ) . V , for all hmap . RespX (V ) . Heap, for all R . Assn, spec if hmap(SBinX \n(fst)) . [P * R] and Valid(hmap, V ), then for all n . N, safen X (V, hmap, Actx, Aprg, lst, Post), [Q[v/y] \n* R] if res = v where Post = Heapif res = . . spec The de.nition says that for any consistent contextual \nexecu\u00adtion of E, all valid con.gurations annotating only the context edges and satisfying the precondition \non the (unique) incom\u00ading sb-edge to the program, are safe for any number of steps. As is common in the \nde.nitions of the meaning of separation logic triples, the de.nition bakes in the frame rule that is, \nit quanti.es over all assertions R and star-conjoins R to the precondition and the postcondition.  7.3 \nMemory Safety and Race Freedom The soundness proof of RSL consists of two parts. First, we have to show \nthat every proof rule of \u00a74 is a valid entailment according to the semantics of Hoare triples in De.nition \n6. Second, we have to show that RSL triples denote something useful for program executions, for example \nthat they do not contain any data races nor any dangling reads. We start with the second task as it is \nsomewhat simpler. More speci.cally, we shall show that any consistent execu\u00adtion of a veri.ed program \nunder the true precondition is (a) memory safe, (b) has no uninitialized reads, (c) has no data races, \nand (d) if the program terminates, its postcondition is satis.able. By memory safety, we mean that allocation \nof a location must happen before any action reading or writing that location. De.nition 7. An execution \nis memory safe if and only if .a . A. isaccessc(a) =. .b. hb(b, a) . lab(b) = A(\u00a3). Given a validly annotated \nexecution by the heap map hmap, observe the following: (1) any action, a, accessing the location \u00a3 must \nhave \u00a3 . dom(hmap(SBin(a))); and (2) whenever a location is in the domain of the annota\u00adtion of an edge \nleading to some action b, (i.e., when \u00a3 . dom(hmap(_, _, b))), then there must be an hb-earlier allo\u00adcation \naction for that location. Putting these two together, we get memory safety for validly annotated executions. \nAbsence of reads from uninitialized locations follows by a similar argument. First, we say that a read \naction, a, reads from an uninitialized location if rf(a) = ., which from (ConsistentRFdom) means that \nthere must be no pre\u00advious write to that location. We can, however, observe that the annotation validity \nfor read actions, a, requires that hmap(SBin(a))(\u00a3) = Atom[_, _, _, true] (for atomic loca\u00adtions) or \nhmap(SBin(a))(\u00a3) = NA[v, _] (for non-atomic locations). But, in order to get one of these heaps in a \nvalid annotation, it must be the case that there was an hb-earlier write to the same location. Proving \nrace-freedom is slightly more involved. First, let us formalize exactly what race-freedom is. We say \nthat two actions are con.icting if both access the same location, at least one of them is a write, and \nat least one of the accesses is non-atomic (i.e., atomic accesses do not con.ict with one another). An \nexecution is race-free is all con.icting actions are ordered by hb. De.nition 8. Two actions a=b are \ncon.icting if there exists a location \u00a3 such that isaccessc(a) and isaccessc(b) and iswrite(a) . iswrite(b), \nand mode(a) = na . mode(b) = na. De.nition 9. An execution is race-free if and only if for all con.icting \nactions a, b . A, we have hb(a, b) . hb(b, a). To prove race-freedom, we need the notion of a set of \ntransitions, T , being pairwise independent. We say that T is pairwise independent, if there exists no \npair of transitions in T such that one happens before the other. De.nition 10 (Independent Edges). In \nan execution, X , a set of transitions T . thbX is pairwise independent, de\u00adnoted PairIndep(T ), if and \nonly if for all (_, a, a ' ), (_, b, b ' ) . T , we have \u00achbX (a ' , b). The crux of the race freedom \nproof is the following in\u00addependent heap compatibility lemma, which states that in every validly annotated \nexecution, the heaps annotated at in\u00addependent edges are .-compatible. Lemma 4 (Independent Heap Compatibility). \nFor every consistent execution, X , heap map, hmap : RespX (AX ) . Heap, and pairwise independent set \nof transitions, T , if spec Valid(hmap, AX ) holds, then hmap(x) is de.ned. x.T  To prove this lemma, \nwe need the notion of the depth of a set of actions, which we take to be the number of its elements and \nits predecessors. De.nition 11 (Action Depth). Given an execution, X , the depth of a set of actions, \nA . AX , which we denote as DX (A), is the number of actions in the set or that have happened before \nit, | PreX (\u00b7 \u00b7 \u00b7 PreX (A) \u00b7 \u00b7 \u00b7)|. n=0 n The depth of actions satis.es this important property: Lemma \n5. If hbX (a, b), then DX ({a}) < DX ({b}). Lemma 4 is then proved by induction using the metric DX ({a \n| (_, a, _) . T }), followed by case analysis on the action in T with largest DX ({-}) value. Putting \neverything together, our main soundness theorem is stated in terms of complete consistent executions: \ndef C C[E] = { (res , X ) | .a, b. a = b . labX (a) = labX (b) = skip . (res , {a, b}, _, X , _, _) . \nC [E] } where the program expression is put inside the trivial context providing it with an incoming \nsb-edge from a skip action and an outgoing sb-edge to a skip action. Here it is: Theorem 1 (Adequacy). \nLet true E y. Q . For every execution (res , X ) . C C [E], X is memory safe, has no reads from uninitialized \nlocations and no races. Moreover, if the execution is terminating, then Q holds of the result.  7.4 \nSoundness of the Proof Rules We move on to the proofs of soundness of the individual rules. For each \nrule, we have to prove that it is a valid entailment given the meaning of RSL triples (De.nition 6). \nWith the exception of R-AC Q and C A S*, these proofs are relatively straightforward because the conditions \nimposed by local validity are almost directly enforced by the proof rules. The proofs of R-AC Q and C \nA S* are more complex be\u00ad cause we also have to annotate the incoming sw-edges cor\u00adrectly and show that \nthe annotation is valid not only for the program action under consideration, but also for the context \nactions at the other end that is, for the write or RMW ac\u00adtion with which the read or CAS synchronizes. \nWe start with the R-AC Q rule. Consider a consistent con\u00adtextual execution where Aprg = {a} and lab(a) \n= Racq(\u00a3, v). We proceed with a case split. If Q(v) = emp, we can simply annotate any incoming sw-edges \nwith the empty heap and set hmap ' (SBoutX (a)) = hmap(SBinX (a)), which trivially preserves validity. \nWhen, however, Q(v) = emp, the situ\u00adation is much more dif.cult, because it is not immediately obvious \nthat there is an incoming sw-edge that can be anno\u00adtated in a way that satis.es the local validity conditions \nof both the acquire-read and the release-write (or RMW) at the other end. For this case, our proof works \nas follows. First, as the precondition includes Init(\u00a3), we know that there exists a write to \u00a3 that \nhappens before the acquire read. Lemma 6 (Init). If Valid(V, hmap) and PreX (V ) . V and hmap(_, _, \na)(\u00a3) = Atom[_, _, _, true], then there exists c . V such that labX (c) = W_(\u00a3, _) and (c, a) . hbX . \nTherefore, from the consistency axiom ConsistentRF, we get that .w. rf(a) = w. As PreX ({a}) . V , we \nalso know w . V . Next, we will show that w must be a plain atomic write that synchronizes with a. To \nsee why this holds, observe that \u00a3 . dom(hmap(SBinX (w))) holds as hmap is locally valid at w . V . Now, \ninformally, we can trace back through the thbX edges to the point where for some node c . V such that \nhbX (c, w), we have \u00a3 ./dom(hmap(SBinX (c))) and yet \u00a3 . dom(hmap(SBoutX (c))). Since hmap is locally \nvalid, the only way for this to happen is if labX (c) = A(\u00a3). Similarly, we can follow thbX edges backwards \nfrom a and .nd a node d . V such that hbX (d, a) and \u00a3 ./dom(hmap(SBinX (d))) and \u00a3 . dom(hmap(SBoutX \n(d))). Again, since hmap is locally valid, labX (d) = A(\u00a3), and so from the consistency axiom ConsistentAlloc, \nwe obtain that c = d. When tracing back from a, at each step we can show that there exist Q ' and b such \nthat hmap(tn+1)(\u00a3) = Atom[_, Q ' , b, _] and either b = false . Q ' (v) = emp or Q ' (v) = false. So, \nin total, we get hmap(SBoutX (c))(\u00a3) = Atom[Q ' , Q ' , b, _] and either b = false . Q ' (v) = emp or \nQ ' (v) = false. Similarly, when tracing back from b, at each step we can show that whenever hmap(tn+1)(\u00a3) \n= Atom[Q ' , _, b, _], then there exist Q '' and b ' such that hmap(tn)(\u00a3) = Atom[Q '' , _, b ' , _] \nand Q '' (v) . Q(v) and b ' . b. So, in total we get that there exist Q '' and b ' such that hmap(SBinX \n(w))(\u00a3) = Atom[Q '' , _, b ' , _], and Q '' (v) . Q ' (v) and b ' . b. Since hmap is locally valid at \nw, .h . [Q '' (v)]; thus, b ' = b = false and Q '' (v) = emp, which means that w is a write that synchronizes \nwith a. We have the following picture: w synchronizes with a, but possibly also with some other reads \nr1, . . . , rn . V and perhaps even some reads not in V . a : Racq(\u00a3, v) . . . From the local validity \nof hmap at w . V , we know that (h1 .\u00b7 \u00b7 \u00b7.hn .hsink) . [Q '' (v)], where each hi is the heap annotated \non the sw-edge from w to ri. What remains to be shown is that we can split hsink further; that is, we \ncan .nd h ' , h ' such that hsink = h ' .h ' and h ' . [Q(v)]. Then, sink sink we annotate the ( sw \n, w, a) edge with h ' and SBoutX (a) with h ' .SBinX (a), thereby ensuring local validity at both a and \nw. To .nd such a split, we rely on the following lemma. Lemma 7 (Well-formedness). Given a consistent \nexecution X , a pre.x-closed set of actions, V . AX with PreX (V ) . V , a heap map, hmap . RespX (V \n) . Heap, that is spec locally valid with respect to V , Valid(hmap, V ), a pairwise independent set \nof transitions T , such that {a | (_, a, _) . T } . V and hmap(T )(\u00a3) = Atom[_, Q, _, _], an action, \nw, such that lab(w) = W(\u00a3, v) and hmap(SBinX (w))(\u00a3) = Atom[Q '' , _, _, _], and a partial map R : V \n-Assn, such that for all a . dom(R), a is a read action that synchro\u00adnizes with w and acquires ownership \nof R(a), if moreover, {a | .(_, _, b) . T . (a = b . hb(a, b))} n dom(R) = \u00d8, then, Q '' (v) . Q(v) * \n.r.dom(R)R(r) * true. The proof of this lemma is rather technical and can be found in the Coq formalization. \nAt a high level, however, it is similar to the proofs already described, using the depth metric to trace \nback the T . {( sw , w, r) | r . dom(R)}edges until we reach c, the action that allocated \u00a3. Applying \nthis lemma, we get that (h1.\u00b7 \u00b7 \u00b7.hn.hsink) . [Q(v) * R(r1) * . . . * R(rn) * true], and since for all \ni, we also know that precise(R(ri)) and hi . [R(ri)], we obtain hsink . [Q(v) * true], as required. The \nproof of CAS is actually much simpler because there cannot be any resource-acquiring reads that synchro\u00adnize \nwith the write/RMW whence the CAS reads from. De\u00adtails of this proof can be found in the Coq formalization. \n 7.5 The Coq Formalization Our Coq development covers the entire soundness proof out\u00adlined in this section \nand follows the LATEX presentation very closely. To avoid excessive proof duplication, the de.nitions \nof con.guration safety and triple validity are parametrized with respect to the memory model; that is, \neither the stan\u00addard model or the one with the StrongAcyclicHB condition. One notable difference is that \nin Coq we represent .nite sets of actions, A, as lists, and domain-restricted functions as functions \nover the full domain. For example, instead of lab . A . Act, in Coq we have lab : AName . Act, and add \na consistency axiom stating that .x /. A. lab(x) = skip. Sim\u00adilarly, we de.ne hmap : thb(AName \u00d7 AName, \nAName \u00d7 AName) . Heap, and in the de.nition of con.guration spec safety, instead of saying that there \nexists a hmap ' such that the con.guration with hmap 1 hmap ' is safe, we say that there exists hmap \n'' such that .e . RespX (V ). hmap '' (e) = hmap(e) holds and the con.guration with hmap '' is safe. \nAnother difference is that in Coq the treatment of as\u00adsertions up to ~ is achieved by de.ning a syntactic \nasser\u00adtion normalization function, norm, with the property that P ~ Q .. norm(P ) = norm(Q). Then, we \nrepresent Assn/~ as {P . Assn | norm(P ) = P }. Finally, following Nanevski et al. (2010), we represent \nheaps as the option type Heap. {.}, with . represent\u00ad spec ing unde.ned heaps. This removes the de.nedness \nside\u00adconditions from the statements of commutativity and asso\u00adciativity of heap composition. In effect, \nwe move the de\u00ad.nedness checks to the semantics of assertions, where we ensure that . ./[P ] for any \nassertion P . The formal development excluding standard libraries consists of about 3000 lines of de.nitions \nand statements of lemmas and theorems, 5500 lines of proof, 500 lines of comments, and took the .rst \nauthor about two months to complete. It is worth pointing out that the formal proof re\u00advealed that a \nbug that we had missed in our earlier paper proofs: namely, the requirement that the ownership transfer \ngoverned by the R-AC Q rule to be precise. While we do not think that this side condition is strictly \nnecessary for sound\u00adness in the absence of the conjunction rule, the current proof style fundamentally \nrequires it. 8. Related Work and Conclusion This paper introduced relaxed separation logic, a moder\u00adate \nextension of concurrent separation logic (O Hearn 2007) with special primitives for handling C11 s acquire \nand re\u00adlease atomic accesses. 8.1 Related Work About the C11 Model The C11 concurrency model is part \nof the C and C++ 2011 standards (ISO/IEC 9899:2011; ISO/IEC 14882:2011), and has been formalized by Batty \net al. (2011). In a subsequent paper, Batty et al. (2012) simpli.ed the C11 model in the absence of consume \nreads. It is this simpli.ed model that we used in this paper. In a recent paper, Batty et al. (2013) \nconsidered the no\u00ad tion of library atomicity in the context of the C11 memory model. While this work \nis largely orthogonal to our de.ning a program logic about C11, we expect that combining the two approaches \nwill be fruitful as program logics are often the means for proving atomicity, at least in the SC setting. \nLogics for Other Weak Memory Models We are aware of only three lines of work that de.ne program logics \nover a relaxed memory model, none of which handles the C11 memory model. Ferreira et al. (2010) proved \nthe soundness of concur\u00ad rent separation logic (CSL) over a class of relaxed mem\u00adory models, all satisfying \nthe DRF-guarantee. In hind\u00adsight, their result is not surprising as the soundness of CSL over SC (sequential \nconsistency) ensures that CSL\u00adveri.ed programs do not contain any data races, and hence whether the soundness \nproof is done over SC or over the relaxed memory model is irrelevant.  Ridge (2010) developed a rely-guarantee \nproof system over x86-TSO and used it to verify a x86-TSO version of Simpson s four slot algorithm, with \nall the results mechanized in the HOL theorem prover.  Wehrman and Berdine (2011) proposed a variant \nof sep\u00ad aration logic for x86-TSO featuring primitive assertions for modelling the state of the TSO buffers \nand both tem\u00adporal and spatial separating conjunctions.  Besides obviously handling different memory \nmodels and being quite different program logics, there is a fundamen\u00adtal difference between the current \nwork and these earlier pa\u00adpers on program logics for relaxed memory models. In this work, we de.ne the \nmeaning of Hoare triples directly over an axiomatic partial order semantics for concurrent programs, \nwhereas the earlier works used an operational or an opera\u00adtionally .avoured trace semantics, very much \nlike the tradi\u00adtional soundness proofs over SC. As a result, our soundness proof is completely different \nfrom the soundness proofs of the aforementioned papers.  8.2 Possible Future Research Directions Being \nthe .rst program logic for C11 concurrency, there are numerous opportunities for extending RSL, for example \nto deal with more advanced features of the C11 memory model, such as consume reads and memory fences. \nSimilarly, one can also try to adapt to C11 setting more advanced program logics, such as RGSep (Vafeiadis \nand Parkinson 2007), con\u00ad current abstract predicates (Dinsdale-Young et al. 2010), or CaReSL (Turon \net al. 2013). Initialization of Atomics For simplicity, RSL tags loca\u00adtions as atomic or non-atomic and \npermits atomic accesses only on atomic locations and non-atomic accesses only on non-atomic locations. \nAs a result of this choice, initializa\u00adtion writes to atomic accesses in our model also have to be atomic, \nwhereas the C11 standard also allows non-atomic initialization writes to atomic location. To enable the \nver\u00adi.cation of such programs, we should somehow allow the following conversion rule Q(v) = emp \u00a3 . v \n0 Rel(\u00a3, Q) * Acq(\u00a3, Q) * Init(\u00a3) Automation Another important research direction would be to develop \ntechniques and tools for automating RSL proofs by adapting some of the work that has been done in automating \nstandard separation logic (Distefano et al. 2006; Calcagno et al. 2009; Dudka et al. 2011). Acknowledgments \nWe would like to thank Lars Birkedal, Arthur Chargu\u00e9raud, Mike Dodds, Alexey Gotsman, Matthew Parkinson, \nAaron Turon, John Wickerson, and the anonymous OOPSLA 2013 reviewers for their very useful comments that \nimproved the content of this paper. The research was supported by the EC FP7 FET Young Explorers scheme \nvia the project ADVENT. References M. Batty, S. Owens, S. Sarkar, P. Sewell, and T. Weber. Mathe\u00admatizing \nC++ concurrency. In POPL 2011, pages 55 66. ACM, 2011. M. Batty, K. Memarian, S. Owens, S. Sarkar, and \nP. Sewell. Clarifying and compiling C/C++ concurrency: From C++11 to POWER. In POPL 2012, pages 509 520. \nACM, 2012. M. Batty, M. Dodds, and A. Gotsman. Library abstraction for C/C++ concurrency. In POPL 2013, \npages 235 248. ACM, 2013. L. Birkedal, K. St\u00f8vring, and J. Thamsborg. The category-theoretic solution \nof recursive metric-space equations. Theoretical Com\u00adputer Science, 411(47):4102 4122, 2010. J. Boyland. \nChecking interference with fractional permissions. In SAS 2003, volume 2694 of LNCS, pages 55 72. Springer, \n2003. C. Calcagno, D. Distefano, and V. Vafeiadis. Bi-abductive resource invariant synthesis. In APLAS, \nvolume 5904 of LNCS, pages 259 274. Springer, 2009. T. Dinsdale-Young, M. Dodds, P. Gardner, M. Parkinson, \nand V. Vafeiadis. Concurrent abstract predicates. In ECOOP 2010, volume 6183 of LNCS, pages 504 528. \nSpringer, 2010. D. Distefano, P. W. O Hearn, and H. Yang. A local shape analysis based on separation \nlogic. In TACAS, volume 3920 of LNCS, pages 287 302. Springer, 2006. K. Dudka, P. Peringer, and T. Vojnar. \nPredator: A practical tool for checking manipulation of dynamic data structures using sep\u00adaration logic. \nIn CAV, volume 6806 of LNCS, pages 372 378. Springer, 2011. R. Ferreira, X. Feng, and Z. Shao. Parameterized \nmemory models and concurrent separation logic. In ESOP 2010, volume 6012 of LNCS, pages 267 286. Springer, \n2010. C. Flanagan, A. Sabry, B. F. Duba, and M. Felleisen. The essence of compiling with continuations. \nIn PLDI 1993, pages 237 247. ACM, 1993. ISO/IEC 14882:2011. Programming language C++, 2011. ISO/IEC 9899:2011. \nProgramming language C, 2011. P. E. McKenney and B. Garst. N1525: Memory-order rationale, 2011. Available \nat http://www.open-std.org/jtc1/sc22/ wg14/www/docs/n1525.htm. A. Nanevski, V. Vafeiadis, and J. Berdine. \nStructuring the veri.ca\u00adtion of heap-manipulating programs. In POPL, pages 261 274. ACM, 2010. P. O Hearn. \nResources, concurrency, and local reasoning. Theoret\u00adical Computer Science, 375(1):271 307, 2007. T. \nRidge. A rely-guarantee proof system for x86-TSO. In VSTTE 2010, volume 6217 of LNCS, pages 55 70. Springer, \n2010. S. Sarkar, K. Memarian, S. Owens, M. Batty, P. Sewell, L. Maranget, J. Alglave, and D. Williams. \nSynchronising C/C++ and POWER. In PLDI 2012, pages 311 322. ACM, 2012. A. Turon, D. Dreyer, and L. Birkedal. \nUnifying re.nement and Hoare-style reasoning in a logic for higher-order concurrency. In ICFP 2013. ACM, \n2013. V. Vafeiadis. Concurrent separation logic and operational seman\u00adtics. In MFPS 2011, volume 276 \nof ENTCS, pages 335 351. Elsevier, 2011. V. Vafeiadis and M. Parkinson. A marriage of rely/guarantee \nand separation logic. In CONCUR 2007, volume 4703 of LNCS, pages 256 271. Springer, 2007. M. N. Wegman \nand F. K. Zadeck. Constant propagation with conditional branches. ACM Trans. Program. Lang. Syst., 13(2): \n181 210, Apr. 1991. I. Wehrman and J. Berdine. A proposal for weak-memory local reasoning. In LOLA 2011, \n2011.  \n\t\t\t", "proc_id": "2509136", "abstract": "<p>We introduce <i>relaxed separation logic</i> (RSL), the first program logic for reasoning about concurrent programs running under the C11 relaxed memory model. From a user's perspective, RSL is an extension of concurrent separation logic (CSL) with proof rules for the various kinds of C11 atomic accesses. As in CSL, individual threads are allowed to access non-atomically only the memory that they own, thus preventing data races. Ownership can, however, be transferred via certain atomic accesses. For SC-atomic accesses, we permit arbitrary ownership transfer; for acquire/release atomic accesses, we allow ownership transfer only in one direction; whereas for relaxed atomic accesses, we rule out ownership transfer completely. We illustrate RSL with a few simple examples and prove its soundness directly over the axiomatic C11 weak memory model.</p>", "authors": [{"name": "Viktor Vafeiadis", "author_profile_id": "81100493655", "affiliation": "Max Planck Institute for Software Systems (MPI-SWS), Kaiserslautern, Germany", "person_id": "P4290476", "email_address": "viktor@mpi-sws.org", "orcid_id": ""}, {"name": "Chinmay Narayan", "author_profile_id": "81508690984", "affiliation": "Indian Institute of Technology, Delhi, Delhi, India", "person_id": "P4290477", "email_address": "chinmay@cse.iitd.ac.in", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509532", "year": "2013", "article_id": "2509532", "conference": "OOPSLA", "title": "Relaxed separation logic: a program logic for C11 concurrency", "url": "http://dl.acm.org/citation.cfm?id=2509532"}