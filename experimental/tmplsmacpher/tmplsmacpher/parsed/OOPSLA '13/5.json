{"article_publication_date": "10-29-2013", "fulltext": "\n Taking Off the Gloves with Reference Counting Immix Rifat Shahriyar, Stephen M. Blackburn, Xi Yang \nKathryn S. McKinley Australian National University Microsoft Research <First.Last>@anu.edu.au mckinley@microsoft.com \n Abstract Despite some clear advantages and recent advances, refer\u00adence counting remains a poor cousin \nto high-performance tracing garbage collectors. The advantages of reference counting include a) immediacy \nof reclamation, b) incremen\u00adtality, and c) local scope of its operations. After decades of languishing \nwith hopelessly bad performance, recent work narrowed the gap between reference counting and the fastest \ntracing collectors to within 10%. Though a major ad\u00advance, this gap remains a substantial barrier to \nadoption in performance-conscious application domains. Our work identi.es heap organization as the principal \nsource of the remaining performance gap. We present the design, implementation, and analysis of a new \ncollector, RC Immix, that replaces reference counting s traditional free-list heap organization with \nthe line and block heap structure introduced by the Immix collector. The key in\u00adnovations of RC Immix \nare 1) to combine traditional ref\u00aderence counts with per-line live object counts to identify reusable \nmemory and 2) to eliminate fragmentation by in\u00adtegrating copying with reference counting of new objects \nand with backup tracing cycle collection. In RC Immix, ref\u00aderence counting offers ef.cient collection \nand the line and block heap organization delivers excellent mutator local\u00adity and ef.cient allocation. \nWith these advances, RC Immix closes the 10% performance gap, outperforming a highly tuned production \ngenerational collector. By removing the performance barrier, this work transforms reference count\u00ading \ninto a serious alternative for meeting high performance objectives for garbage collected languages. Categories \nand Subject Descriptors Software, Virtual Machines, Memory management, Garbage collection Keywords Reference \nCounting, Immix, Mark-Region, Defragmenta\u00adtion Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. \n. . $15.00. http://dx.doi.org/10.1145/2509136.2509527 1. Introduction In 1960, researchers introduced \nthe two main branches of automatic garbage collection: tracing and reference count\u00ading [14, 24]. Reference \ncounting directly identi.es dead ob\u00adjects by counting the number of incoming references. When the count \ngoes to zero, the object is unreachable and the collector may reclaim it. Tracing takes the opposite \ntack. It identi.es live objects by performing a transitive closure over the object graph, implicitly \nidentifying dead objects. It then reclaims all untraced objects. Reference counting has advantages. 1) \nIt may reclaim ob\u00adjects as soon as they are no longer referenced. 2) It is inher\u00adently incremental. 3) \nIts operations are object-local, rather than global in scope. Its major disadvantage is that it can\u00adnot \nreclaim cycles and therefore it requires a backup trac\u00ading collector [2, 18]. This limitation has the \npractical con\u00ad sequence that any reference counter that guarantees com\u00adpleteness (i.e., it will eventually \nreclaim all garbage) es\u00adsentially requires two collector implementations. Further\u00admore, the performance \nof reference counting implementa\u00adtions lagged high performance tracing collectors by 30% or more until \nrecently [21, 22, 27]. In 2012, Shahriyar et al. solved two problems responsible for much of the perfor\u00admance \noverhead of reference counting. This paper identi.es and solves the remaining problems, completely eliminating \nperformance degradation as a barrier to adoption. Shahriyar et al. identify the following characteristics \nof programs and use them to optimize reference counting. (We call their collector RC for simplicity.) \n1. The vast majority of reference counts are low, less than .ve. The RC collector uses only a few bits \nfor the ref\u00aderence count. It sticks counts at a maximum before they over.ow and then corrects stuck counts \nwhen it traces the heap during cycle collection. 2. Many reference count increments and decrements are \nto newly allocated objects. RC elides reference counting of new objects and allocates them as dead, which \neliminates a lot of useless work.  RC performs deferred reference counting and occasional backup cycle \ntracing. Deferral trades vastly fewer reference counting increments and decrements for less immediacy \nof reclamation. RC divides execution into three distinct phases: mutation, reference counting collection, \nand cycle collec\u00adtion. The result is a reference counting collector with the same performance as a whole-heap \ntracing collector, and within 10% of the best high performance generational col\u00adlector in MMTk [6, 8, \n27]. This paper identi.es that the major source of this 10% gap is that RC s free-list heap layout has \npoor cache local\u00adity and imposes instruction overhead. Poor locality occurs because free-list allocators \ntypically disperse contemporane\u00adously allocated objects in memory, which degrades locality compared to \nallocating them together in space [6, 8]. Instruc\u00ad tion overheads are greater in free lists, particularly \nwhen pro\u00adgramming languages require objects to be pre-initialized to zero. While a contiguous allocator \ncan do bulk zeroing very ef.ciently, a free-list allocator must zero object-by-object, which is inef.cient \n[33]. To solve these problems, we introduce Reference Count\u00ading Immix (RC Immix). RC Immix uses the allocation \nstrat\u00adegy and the line and block heap organization introduced by Immix mark-region garbage collection \n[6]. Immix places ob\u00ad jects created consecutively in time consecutively in space in free lines within \nblocks. Immix allocates into partially free blocks by ef.ciently skipping over occupied lines. Objects \nmay span lines, but not blocks. Immix reclaims memory at a line and block granularity. The granularity \nof reclamation is the key mismatch be\u00adtween reference counting and Immix that RC Immix re\u00adsolves. Reference \ncounting reclaims objects, whereas Im\u00admix reclaims lines and blocks. The design contributions of RC Immix \nare as follows. RC Immix extends the reference counter to count live objects on a line. When the live \nobject count of a line is zero, RC Immix reclaims the free line.  RC Immix extends opportunistic copying \n[6], which mixes copying with leaving objects in place. RC Im\u00admix adds proactive copying, which combines \nreference counting and copying to compact newly allocated live objects. RC Immix on occasion reactively \ncopies old ob\u00adjects during cycle detection to eliminate fragmentation.  Combining copying and reference \ncounting is novel and sur\u00adprising. Unlike tracing, reference counting is inherently lo\u00adcal, and therefore \nin general the set of incoming references to a live object is not known. However, we observe two impor\u00adtant \nopportunities. First, in a reference counter that coalesces increments and decrements [21, 22], since \neach new object starts with no references to it, the .rst collection must enu\u00admerate all references to \nthat new object, presenting an op\u00adportunity to move that object proactively. We .nd that when new objects \nhave a low survival rate, the remaining live ob\u00adjects are likely to cause fragmentation. We therefore \ncopy new objects, which is very effective in small heaps. Second, since completeness requires a tracing \ncycle collection phase, RC Immix seizes upon this opportunity to incorporate reac\u00adtive defragmentation \nof older objects. In both cases, we use opportunistic copying, which mixes copying and leaving ob\u00adjects \nin place, and thus can stop copying when it exhausts available memory. Two engineering contributions \nof RC Immix are im\u00adproved handling of roots and sharing the limited header bits to serve triple duty \nfor reference counting, backup cy\u00adcle collection with tracing, and opportunistic copying. The combination \nof these innovations results in a collector that attains great locality for the mutator and very low \noverhead for reference counting. Measurements on a large set of Java benchmarks show that for all but \nthe smallest of heap sizes RC Immix outper\u00adforms the best high performance collector in the literature. \nIn some cases RC Immix can perform substantially better. In summary, we make the following contributions \ncompared to the previous state of the art [27]. 1. We identify heap organization as the remaining perfor\u00admance \nbottleneck for reference counting. 2. We merge reference counting with the heap structure of Immix by \nmarrying per-line live object counts with object reference counts for reclamation. 3. We identify two \nopportunities for copying objects one for young objects and one that leverages the required cy\u00adcle collector \n further improving locality and mitigating fragmentation both proactively and reactively. 4. RC Immix \nimproves performance by 12% on average compared to RC and sometimes much more, outperform\u00ading the fastest \nproduction and eliminating the perfor\u00admance barrier to using reference counting.  Because the memory \nmanager determines performance for managed languages and consequently application capabili\u00adties, these \nresults open up new ways to meet the needs of applications that depend on performance and prompt recla\u00admation. \n2. Motivation and Related Work This section motivates our approach and overviews the nec\u00adessary garbage \ncollection background on which we build. We start with a critical analysis of the performance of Shahriyar \net al s reference counter [27], which we refer to simply as RC. This analysis shows that inef.ciencies \nderive from 1) remaining reference counting overheads and 2) poor locality and instruction overhead due \nto the free-list heap structure. We then review existing high performance collec\u00adtors, reference counting, \nand the Immix [6] garbage collector upon which we build. 2.1 Motivating Performance Analysis All previous \nreference counting implementations in the lit\u00aderature use a free-list allocator because when the collector \ndetermines that an object s count is zero, it may then imme\u00addiately place the freed memory on a free \nlist. We start our analysis by understanding the performance impact of this  Figure 1. Immix Heap Organization \nchoice, using hardware performance counters. We then an\u00adalyze RC further to establish its problems and \nopportunities for performance improvements. Free-List and Contiguous Allocation The allocator plays a \nkey role in mutator performance since it determines the placement and thus locality of objects. Contiguous \nmem\u00adory allocation appends new objects by incrementing a bump pointer by the size of the new object [13]. \nOn the other hand, modern free-list allocators organize memory into k size-segregated free lists [4, \n8]. Each free list is unique to a size class and is composed from blocks of contiguous mem\u00adory. It allocates \nan object into a free cell in the smallest size class that accommodates the object. Whereas a contiguous \nallocator places objects in memory based on allocation or\u00adder, a free list places objects in memory based \non their size and free memory availability. Blackburn et al. [8] show that contiguous allocation in a \ncopying collector delivers signi.cantly better locality than free-list allocation in a mark-sweep collector. \nFeng and Berger [17] show similar locality bene.ts from initial con\u00ad tiguous allocation in a free list \nfor C applications, but only when allocation to live ratios are very low since with high ratios the allocator \nreverts to free-list allocation. We con.rm the locality bene.ts of contiguous on contemporary hard\u00adware, \nworkloads, and allocator implementations below. When contiguous allocation is coupled with copying col\u00adlection, \nthe collector must update all references to each moved object [13], a requirement that is at odds with \nrefer\u00ad ence counting s local scope of operation. Because reference counting does not perform a closure \nover the live objects, in general, a reference counting collector does not know of and therefore cannot \nupdate all pointers to an object it might otherwise move. Thus far, this prevented reference counting \nfrom copying and using a contiguous allocator. On the other hand, the Immix mark-region heap layout offers \nlargely contiguous heap layout, line and block recla\u00admation, and copying [6]. Figure 1 shows how Immix \nallo\u00ad cates objects contiguously in empty lines and blocks (see Section 2.3 for more details). In partially \nfull blocks, Immix skips over occupied lines. First to explore the performance impact of free-list allo\u00adcation, \nwe compare the mutator time, which is the total time Table 1. The mutator characteristics of mark-sweep \nrelative to Immix using the geometric mean of the benchmarks. GC time is excluded. Free-list allocation \nincreases the number of instructions retired and L1 data cache misses. Semi-space serves as an additional \npoint of comparison. Mutator Immix Mark-Sweep Semi-Space Time 1.000 1.087 1.007 Instructions Retired \n1.000 1.071 1.000 L1 Data Cache Misses 1.000 1.266 0.966 minus the collector time, in Table 1. We measure \nImmix, mark-sweep using a free list, and semi-space [13], across a suite of benchmarks. (See Section \n4 for methodology de\u00ad tails.) We compare mutator time of Immix to mark-sweep to cleanly isolate the performance \nimpact of the free-list allo\u00adcator versus the Immix allocator. Mark-sweep uses the same free-list implementation \nas RC, and neither Immix nor mark\u00adsweep use barriers in the mutator. We also compare to semi\u00adspace. Semi-space \nis the canonical example of a contiguous allocator and thus an interesting limit point, but it is incom\u00adpatible \nwith reference counting. The semi-space data con\u00ad.rms that Immix is very close to the ideal for a contiguous \nallocator. The contiguous bump allocator has two advantages over the free list, both of which are borne \nout in Table 1. The combined effect is almost a 9% performance advantage. The .rst advantage of a contiguous \nallocator is that it improves the cache locality of contemporaneously allocated objects by placing them \non the same or nearby cache lines, and interacts well with modern memory systems. Our measurements in \nTable 1 con.rm this intuition, showing that a free list adds 26% more L1 data cache misses to the mutator, \ncompared to the Immix contiguous allocator. This degradation of locality has two related sources. 1) \nContemporaneously allocated objects are much less likely to share a cache line when using a free list. \n2) A contiguous allocator touches memory sequentially, priming the prefetcher to fetch lines before the \nallocator writes new objects to them. On the other hand, a free-list allocator disperses new objects, \ndefeating hardware prefetching prediction mechanisms. Measurements by Yang et al. show these effects \n[33]. Mutator Sticky Immix RC Immix Time 1.000 1.093 0.975 Instructions Retired 1.000 1.092 0.972 L1 \nData Cache Misses 1.000 1.329 1.018 Table 2. The mutator characteristics of RC and Sticky Immix, which \nexcept for heap layout have simi\u00adlar features. GC time is excluded. RC s free list allocator increases \ninstructions retired and L1 cache misses. Immix serves as a point of comparison. The second advantage \nof contiguous allocation is that it uses fewer instructions per allocation, principally because it zeros \nfree memory in bulk using substantially more ef.\u00adcient code [33]. The allocation itself is also simpler \nbecause it only needs to check whether there is suf.cient memory to accommodate the new object and increase \nthe bump pointer, while the free-list allocator has to look up and update the metadata to decide where \nto allocate. However, we inspect generated code and con.rm the result of Blackburn et al. [8] that in \nthe context of a Java optimizing compiler, where the size of most objects is statically known, the free-list \nal\u00adlocation sequence is only slightly more complex than for the bump pointer. The overhead in additional \ninstructions shown in Table 1 is therefore solely attributable to the substantially less ef.cient cell-by-cell \nzeroing required by a free-list al\u00adlocator. We measure a 7% increase in the number of retired instructions \ndue to the free list compared to Immix s con\u00adtiguous allocator. Analyzing RC Overheads We use a similar \nanalysis to examine mutator overheads in RC [27] by comparing to Sticky Immix [6, 15], a generational \nvariant of Immix. We choose Sticky Immix for its similarities to RC. Both collec\u00adtors a) are mostly non-moving, \nb) have generational behav\u00adior, and c) use similar write barriers. This comparison holds as much as possible \nconstant but varies the heap layout be\u00adtween free list and contiguous. Table 2 compares mutator time, \nretired instructions, and L1 data cache misses of RC and Sticky Immix. The mutator time of RC is on average \n9.3% slower than Sticky Immix, which is re.ected by the two performance counters we re\u00adport. 1) RC has \non average 9.2% more mutator retired in\u00adstructions than Sticky Immix. 2) RC has on average 33% more mutator \nL1 data cache misses than Sticky Immix. These results are consistent with the hypothesis that RC s use \nof a free list is the principal source of overhead com\u00adpared to Sticky Immix, and motivates our design \nthat com\u00adbines reference counting with the Immix heap structure.  2.2 High Performance Reference Counting \nThe .rst account of reference counting was published by George Collins in 1960 [14], just months after \nJohn Mc-Carthy .rst described tracing garbage collection [24]. The two approaches are duals. Reference \ncounting directly iden\u00adti.es dead objects by keeping a count of the number of ref\u00aderences to each object, \nfreeing the object when its count reaches zero. Tracing algorithms, such as McCarthy s, do not directly \nidentify dead objects, but rather, they identify live objects, and the remaining objects are implicitly \ndead. Most high performance tracing algorithms are exact, which means that they precisely identify all \nlive objects in the heap. To identify live objects, they must enumerate all live refer\u00adences from the \nrunning program s stacks, which means that the runtime must maintain accurate stack maps. Maintaining \nstack maps is a formidable engineering burden, and is a rea\u00adson why some language developers use reference \ncounting rather than tracing [19]. To build stack maps, the compiler (or interpreter) must be able to \ndetermine for every register and stack location, at every point in the program s execution where a GC \nis legal, whether that location contains a valid heap reference or not. Collins .rst reference counting \nalgorithm suffered from signi.cant drawbacks including: a) an inability to collect cycles of garbage, \nb) overheads due to tracking very frequent pointer mutations, c) overheads due to storing the reference \ncount, and d) overheads due to maintaining counts for short lived objects. The following paragraphs brie.y \noutline .ve important optimizations developed over the past .fty years to improve over Collins original \npaper. Shahriyar et al. show that together these optimizations deliver competitive performance [27]. \nDeferral To mitigate the high cost of maintaining counts for rapidly mutated references, Deutsch and \nBobrow intro\u00adduced deferred reference counting [16]. Deferred reference counting ignores mutations to \nfrequently modi.ed variables, such as those stored in registers and on the stack. Deferral re\u00adquires \na two phase approach, dividing execution into distinct mutation and collection phases. This tradeoff \nreduces refer\u00adence counting work signi.cantly, but delays reclamation. Since deferred references are \nnot accounted for during the mutator phase, the collector counts other references and places zero count \nobjects in a zero count table (ZCT) defer\u00adring their reclamation. Periodically in a GC reference count\u00ading \nphase, the collector enumerates all deferred references into a root set and then reclaims any object \nin the ZCT that is not in the root set. Bacon et al. [3] eliminate the zero count table by buffer\u00ad ing \ndecrements between collections. At collection time, the collector temporarily increments a reference \ncount to each object in the root set and then processes all of the buffered decrements. Although much \nfaster than na\u00a8ive immediate ref\u00aderence counting, these schemes typically require stack maps to enumerate \nall live pointers from the stacks. Stack maps are an engineering impediment, which discourages many refer\u00adence \ncounting implementations from including deferral [19]. Coalescing Levanoni and Petrank observed that \nall but the .rst and last in any chain of mutations to a reference within a given window can be coalesced \n[21, 22]. Only the initial and .nal states of the reference are necessary to calculate correct reference \ncounts. Intervening mutations generate increments and decrements that cancel each other out. This observation \nis exploited by remembering (logging) only the initial value of a reference .eld when the program mutates \nit between periodic reference counting collections. At each collection, the collector need only apply \na decrement to the initial value of any over-written reference (the value that was logged), and an increment \nto the latest value of the reference (the current value of the reference). Levanoni and Petrank implemented \ncoalescing using ob\u00adject remembering. The .rst time the program mutates an object reference after a collection \nphase a) a write barrier logs all of the outgoing references of the mutated object and marks the object \nas logged; b) all subsequent reference mutations in this mutator phase to the (now logged) object are \nignored; and c) during the next collection, the collector scans the remembered object, increments all \nof its outgoing pointers, decrements all of its remembered outgoing refer\u00adences, and clears the logged \n.ag. This optimization uses two buffers called the mod-buf and dec-buf. The allocator logs all new objects, \nensuring that outgoing references are incre\u00admented at the next collection. The allocator does not record \nold values for new objects because all outgoing references start as null. Limited Bit Counts Each object \nhas a reference count. A dedicated word for the count guarantees that it will never over.ow since a word \nis large enough to count a pointer from every address in the address space. However, reference counting \nmay use fewer bits [20]. Shahriyar et al. show that using a full word adds non-negligible overhead. They \ninstead use just four bits that are available in the object s header word in many systems [27]. The reference \ncounter leaves any count that is about to over.ow in a stuck state, protecting the integrity of the remainder \nof the header word, but introducing a potential garbage leak. Each time the cycle collector runs it resets \neach reference count, which has the effect of bounding the impact of stuck reference counts. Shahriyar \net al. show that this strategy performs well. Cycle Collection Reference counting suffers from the problem \nthat cycles of objects will sustain non-zero refer\u00adence counts, and therefore cannot be collected. To \nattain completeness, a separate backup tracing collector executes from time to time to eliminate cyclic \ngarbage [31]. Backup tracing must enumerate live root references from the stack and registers, which \nrequires stack maps. For this reason, na\u00a8ive reference counting implementations usually do not perform \ncycle collection. A backup tracing collector typically collects cycles by performing a mark-sweep trace \nof the entire heap. Re\u00adsearchers tried limiting tracing to mutated objects [2], but subsequently Frampton \nshowed backup tracing, starting from the roots, performs better [18]. Both RC and RC Immix use backup \ntracing [18, 27]. Per\u00ad forming the trace requires a mark bit in each object header. During the trace, \nRC takes the opportunity to recompute all reference counts and thus may .x stuck counts. RC then sweeps \nall dead objects to the free list. RC Immix uses the same approach, except that it also recomputes object \ncounts on lines and then reclaims free lines and blocks. Young Objects As the weak generational hypothesis \nstates, most objects die young [23, 30], and as a consequence, young objects are a very important optimization \ntarget. All high performance collectors today exploit this observation, typically via a copying generational \nnursery [30]. Prior work applies the weak generational hypothesis to reference count\u00ading by combining \nreference counting with a copying nurs\u00adery [5] and by in-place mark-sweep tracing [25]. Shahriyar et \nal. applied two optimizations to deferred, co\u00adalescing reference counting to exploit short lived young \nob\u00adjects: 1) lazy mod-buf insertion and 2) allocate as dead. Lazy mod-buf insertion avoids adding new \nobjects to the mod\u00adbuf. Instead, it sets a new bit in object headers during allo\u00adcation and the collector \nonly adds new objects to the mod\u00adbuf lazily when it processes increments. During collection whenever \nthe subject of an increment has its new bit set, the collector .rst clears the new bit and then pushes \nthe ob\u00adject into the mod-buf. Because in a coalescing deferred ref\u00aderence counter, all references from \nroots and old objects will increment all objects they reach, this approach will only re\u00adtain new objects \ndirectly reachable from old objects and the roots. For each object in the mod-buf, the collector will \nin\u00adcrement each of its children, which makes this scheme tran\u00adsitive. Thus new objects are effectively \ntraced. The allocate as dead optimization is a simple extension of the above strategy. Instead of allocating \nobjects live with a reference count of one and enqueueing a compensating decrement, this strategy allocates \nnew objects as dead and does not enqueue a decrement. This optimization inverts the presumption: the \nreference counter does not need to identify new objects that are dead, but it must rather identify live \nob\u00adjects. This inversion means that the collector performs work in the infrequent case when a new object \nsurvives a col\u00adlection, rather that in the common case when it dies. New objects become live when they \nreceive their .rst increment when the collector processes the mod-buf. This strategy re\u00admoves the need \nfor creating compensating decrements and avoids explicitly freeing short lived objects. Modern widely \nused implementations of reference counting employ few, if any, of the above optimizations. The likely \nexplanation for this phenomena is two-fold. First, reference counting lagged the best generational garbage \ncollectors by 40% until 2012 when Shahriyar et al. closed the gap to 10%. Consequently, reference counting \nis not currently used in performance critical settings. Second, one attraction of sim\u00adple reference counting \nimplementations is that they do not require sophisticated runtime system support, such as pre\u00adcise stack \nmaps. Many of the optimizations we describe here require the same runtime support as a tracing collector, \nun\u00addermining a principal advantage of a simple implementation. The reference counting implementations \nin widely used lan\u00adguages such as PHP and Objective-C are na\u00a8ive. Because they lack these optimizations, \nthey are inef.cient. Shahriyar et al. s collector implements all these optimizations and is our RC baseline. \n We have outlined the state of the art in reference counting. RC Immix builds upon this foundation and \nthen extends it by a) changing the underlying heap structure, and b) performing proactive and reactive \ncopying to mitigate fragmentation and improve locality. The result is that RC Immix entirely eliminates \nthe 10% performance overhead suffered by the fastest previous reference counting implementation.  2.3 \nHeap Organization and Immix Blackburn and McKinley outline three heap organizations: a) free lists, b) \ncontiguous, and c) regions [6]. Until now, ref\u00ad erence counting used a free-list heap structure. In this \npaper, we adapt reference counting to use regions. In particular, we combine object reference counting \nwith the line and block reclamation strategy used by Immix. Free List A free-list allocator uses a heap \nstructure that di\u00advides memory into cells of various .xed sizes [32]. When space is required for an object, \nthe allocator searches a data structure called a free list to .nd a cell of suf.cient size to accommodate \nthe object. When an object becomes free, the allocator returns the cell containing the object to the \nfree list for reuse. Free lists are used by explicit memory manage\u00adment systems and by mark-sweep and \nreference counting garbage collectors. Importantly, free-list allocators do not require copying of objects, \nwhich makes them particularly amenable to systems that use reference counting and to sys\u00adtems that require \nsupport for pinning of objects (i.e. objects that cannot be moved). Free lists support immediate and \nfast reclamation of in\u00addividual objects, which makes them particularly suitable for reference counting. \nOther systems, such as evacuation and compaction, must identify and move live objects before they may \nreclaim any memory. Also, free lists are a good .t to backup tracing used by many reference counters. \nFree lists are easy to sweep because they encode free and occupied memory in separate metadata. The sweep \nidenti.es and re\u00adtains live objects and returns memory occupied by dead ob\u00adjects to the free list. Free \nlists suffer two notable shortcom\u00adings. First, they are vulnerable to fragmentation of two kinds. They \nsuffer from internal fragmentation when objects are not perfectly matched to the size of their containing \ncell, and they suffer external fragmentation when free cells of particu\u00adlar sizes exist, but the allocator \nrequires cells of another size. Second, they suffer from poor locality because they often po\u00adsition contemporaneously \nallocated objects in spatially dis\u00adjoint memory, as discussed in Section 2.1. Mark-Region Mark-region \nmemory managers use a sim\u00adple bump pointer to allocate objects into regions of contigu\u00adous memory [6]. \nA tracing collection marks each object and marks its containing region. Once all live objects have been \ntraced, it reclaims unmarked regions. This design addresses the locality problem in free-list allocators. \nA mark-region memory manager can choose whether to move surviving ob\u00adjects or not. By contrast, evacuating \nand compacting col\u00adlectors must copy, leading them to have expensive space or time collection overheads \ncompared to mark-sweep collec\u00adtors. Mark-region collectors are vulnerable to fragmentation because a \nsingle live object may keep an entire region alive and unavailable for reuse, and thus must copy some \nobjects to attain good performance. Immix: Lines, Blocks, and Opportunistic Copying Immix is a mark-region \ncollector that uses a region hierarchy with two sizes: lines, which target cache line locality, and blocks, \nwhich target page level locality [6]. Each block is composed of lines, as shown in Figure 1. The allocator \nplaces new ob\u00ad jects contiguously into empty lines and skips over occupied lines. Objects may span lines, \nbut not blocks. Immix uses a bit in the header to indicate whether an object straddles lines, for ef.cient \nline marking. Immix recycles partially free blocks, allocating into them .rst. Immix tackles fragmentation \nusing opportunistic defrag\u00admentation, which mixes marking with copying. At the be\u00adginning of a collection, \nImmix identi.es fragmentation as follows. Blocks with available memory indicate fragmenta\u00adtion because \nalthough available, the memory was not usable by the mutator. Furthermore, the live/free status for these \nblocks is up-to-date from the prior collection. In this case, Immix performs what we call here, a reactive \ndefragment\u00ading collection. To mix marking and copying, Immix uses two bits in the object header to differentiate \nbetween marked and forwarded objects. At the beginning of a defragmenting col\u00adlection, Immix identi.es \nsource and target blocks. During the mark trace, when Immix .rst encounters an object that resides on \na source block and there is still available mem\u00adory for it on a target block, Immix copies the object \nto a target block, leaving a forwarding pointer. Otherwise Immix simply marks the object as usual. When \nImmix encounters forwarded objects while tracing, it updates the reference ac\u00adcordingly. This process \nis opportunistic, since it performs copying until it exhausts memory to defragment the heap. The result \nis a collector that combines the locality of a copy\u00ading collector and the collection ef.ciency of a mark-sweep \ncollector with resilience to fragmentation. The best performing production collector in Jikes RVM is \ngenerational Immix (GenImmix) [6], which consists of a copying young space and an Immix old space. 3. \nDesign of RC Immix This section presents the design of RC Immix, which com\u00adbines the RC and Immix collectors \ndescribed in the previous section. This combination requires solving two problems. 1) We need to adapt \nthe Immix line/block reclamation strat\u00adegy to a reference counting context. 2) We need to share the limited \nnumber of bits in the object header to satisfy the de\u00admands of both Immix and reference counting. In \naddition, RC Immix seizes two opportunities for de\u00adfragmentation using proactive and reactive opportunistic \ncopying. When identifying new objects for the .rst time, it opportunistically copies them, proactively \ndefragmenting. When it, on occasion, performs cyclic garbage collection, RC Immix performs reactive defragmentation. \nSimilar to RC, RC Immix has frequent reference counting phases and occasional backup cycle tracing phases. \nThis structure divides execution into discrete mutation, reference counting collection, and cycle collection \nphases. 3.1 RC and the Immix Heap Until now, reference counting algorithms have always used free-list \nallocators. When the reference count for an object falls to zero, the reference counter frees the space \noccupied by the object, placing it on a free list for subsequent reuse by an allocator. Immix is a mark-region \ncollector, which reclaims memory regions when they are completely free, rather than reclaiming memory \non a per-object basis. Since Immix uses a line and block hierarchy, it reclaims free lines and if all \nthe lines in a block are free, it reclaims the free block. Lines and block cannot be reclaimed until \nall objects within them are dead. RC Immix Line and Block Reclamation RC Immix de\u00adtects free lines by \ntracking the number of live objects on a line. RC Immix replaces Immix s line mark with a per-line live \nobject count, which counts the number of live objects on the line. (It does not count incoming references \nto the line.) As mentioned in Section 2.2, each object is born dead in RC, with a zero reference count \nto elide all reference count\u00ading work for short lived objects. In RC Immix, each line is also born dead \nwith a zero live object count to similarly elide all line counting work when a newly allocated line only \ncon\u00adtains short lived objects. RC only increments an object s ref\u00aderence count when it encounters it \nduring the .rst GC af\u00adter the object is born, either directly from a root or due to an increment from \na live mutated object. We propagate this laziness to per-line live object counts in RC Immix. A newly \nallocated line will contain only newly born objects. During a reference counting collection, before RC \nImmix increments an object s reference count, it .rst checks the new bit. If the object is new, RC Immix \nclears the new object bit, indicating the object is now old. It then increments the object reference \ncount and the live object count for the line. When all new objects on a line die before Figure 2. How \nRC, Immix, and the different phases of RC Immix use the eight header bits.  the collection, RC Immix \nwill never encounter a reference to an object on the line, will never increment the live object count, \nand will trivially collect the line at the end of the .rst GC cycle. Because Immix s line marks are bytes \n(stored in the metadata for the block) and the number of objects on a line is limited by the 256 byte \nline size, live object counts do not incur any space penalty in RC Immix compared to the original Immix \nalgorithm. Limited Bit Count In Jikes RVM, one byte (eight bits) is available in the object header for \nuse by the garbage collec\u00adtor. RC uses all eight bits. It uses two bits to log mutated objects for the \npurposes of coalescing increments and decre\u00adments, one bit for the mark state for backup cycle tracing, \none bit for identifying new objects, and the remaining four bits to store the reference count. Figure \n2(a) illustrates how RC fully uses all its eight header bits. Table 3 shows that four bits for the reference \ncount is suf.cient to correctly count references to more than 99.8% of objects. To integrate RC and Immix, \nwe need some header bits in objects for Immix-speci.c functionality as well. The base Immix implementation \nrequires four header bits, fewer header bits than RC, but three bits store different informa\u00adtion than \nRC. Both Immix and RC share the requirement for one mark bit during a tracing collection. Immix however \nrequires one bit to identify objects that span multiple lines and two bits when it forwards objects during \ndefragmenta\u00adtion. (Copying collectors, including Immix and RC Immix, .rst copy the object and then store \na forwarding pointer in the original object s header.) Figure 2(b) shows the Immix header bits. Immix \nand RC Immix both require a bit to identify ob\u00adjects that may span lines to ensure that all affected \nlines are kept live. Immix and RC Immix both use an optimization called conservative marking which means \nthis bit is only set for objects that are larger than one line, which empirically is relatively uncommon \n[6]. Immix stores its line marks in per-block metadata and RC Immix does the same.  1 47.65 33.93 57.31 \n9.37 66.77 7.29 20.23 44.76 34.74 49.54 47.08 86.36 49.68 96.89 66.31 75.83 38.23 59.62 13.54 47.83 \n2 6.75 0.08 0.16 0.80 4.96 0.16 0.95 0.95 15.74 2.54 1.83 9.38 4.73 47.38 20.75 5.57 4.67 5.15 0.01 2.47 \n 3 0.65 0 0.08 0.68 0.59 0 0.16 0.01 6.69 0.10 0.02 1.15 0.31 0.21 0.16 0.01 0.14 1.53 0.01 0.59 4 0.11 \n0 0.06 0.68 0.28 0 0.08 0 0.06 0.05 0.01 0.24 0.10 0.01 0.01 0.01 0.02 0.26 0.01 0.17 5 0.06 0 0.03 \n0.49 0.12 0 0.03 0 0.06 0.03 0.01 0.07 0.05 0.01 0.01 0.01 0.01 0.14 0.01 0.06  Table 3. Percentage \nof objects that over.ow for a given number of reference counting bits. RC Immix and RC use three and \nfour bits, respectively. Data from Shahriyar et al. [27]. On average, 0.65% of objects over.ow with three \nbits. Immix and RC Immix both need to forward objects dur\u00ading defragmentation. Forwarding uses two bits \nduring a col\u00adlection to record the forwarding state (not forwarded, being forwarded, forwarded). At .rst \ncut, it seems that there are not enough bits since adding Immix functionality to RC requires three bits \nand would thus reduce the bits for the reference count to just one. However, we observe that RC Immix \nonly needs the logged bits for an object to coalesce increments and decrements dur\u00ading reference counting, \nand it only needs forwarding bits when tracing new objects and during backup cycle collec\u00adtion. These \nactivities are mutually exclusive in time, so they are complementary requirements. We therefore put the \ntwo bits to use as follows. 1) Dur\u00ading mutation RC Immix follows RC, using the logged bits to mark modi.ed \nobjects that it has remembered for coalesc\u00ading. 2) During a reference counting collection, RC Immix follows \nRC. For old objects, RC Immix performs increments and decrements as speci.ed by coalescing and then clears \nthe two bits. 3) For new objects and during cycle collec\u00adtion, RC Immix follows Immix. It sets the now \ncleared bits to indicate that it has forwarded an object and at the end of the collection, reclaims the \nmemory. RC Immix thus over\u00adloads the two bits for coalescing and forwarding. Figure 2(c) shows how RC \nImmix uses the header bits during mutation and reference counting. Figure 2(d) shows how RC Immix repurposes \nthe logged bits for forwarding during a collec\u00adtion. All the other bits remain the same in both phases. \nConsequently, we reduce the number of reference count\u00ading bits to three. Three bits will lead to over.ow \nin just 0.65% of objects on average, as shown in Table 3. When a reference count is about to over.ow, \nit remains stuck un\u00adtil a cycle collection occurs, at which time it is reset to the correct value or \nleft stuck if the correct count is higher. Several optimizations and languages such as C# require pinning. \nPinned objects are usually identi.ed by a bit in the header. The simplest way to add pinning is to steal \nanother bit from the reference count, reducing it to two bits. A slightly more complex design adds pinning \nto the logged and forwarded bits, since each of logged and forwarding only require three states. When \nwe evaluated stealing a reference count bit for pinning, it worked well (see Section 5.3), so we did \nnot explore the more complex implementation. Our default RC Immix con.guration does not use pinning. \n 3.2 Cycle Collection and Defragmentation Cycle Collection Reference counting suffers from the problem \nthat cycles of objects will sustain non-zero refer\u00adence counts and therefore cannot be collected. The \nsame problem affects RC Immix, since line counts follow object liveness. RC Immix relies on a backup \ntracing cycle collec\u00adtor to correct incorrect line counts and stuck object counts. It uses a mark bit \nfor each object and each line. It takes one bit from the line count for the mark bit and uses the remaining \nbits for the line count. The cycle collector starts by setting all the line marks and counts to zero. \nDuring cycle collection, the collector marks each live object, marks its corresponding line, and increments \nthe live object count for the line when it .rst encounters the object. At the end of marking, the cycle \ncollector reclaims all unmarked lines. Whenever any reference counting implementation .nds that an object \nis dead, it decrements the reference counts of all the children of the dead object, which may recursively \nresult in more dead objects. This rule applies to reference counting in RC and RC Immix. RC and RC Immix \ns cycle collection is tasked with explicitly resetting all reference counts. In addition, RC Immix restores \ncorrect line counts. This feature eliminates the need to sweep dead objects alto\u00adgether and RC Immix \ninstead sweeps dead lines. RC Immix performs cycle collection on occasion. How often to perform cycle \ncollection is an empirical question that trades off responsiveness with immediacy of cycle recla\u00admation \nthat we explore below. Defragmentation with Opportunistic Copying Reference counting is a local operation, \nmeaning that the collector is only aware of the number of references to an object, not their origin. \nTherefore it is generally not possible to move objects during reference counting. However, RC Immix seizes \nupon two important opportunities to copy objects and thus mit\u00adigate fragmentation. First, we observe \nthat when an object is subject to its .rst reference counting collection, all ref\u00aderences to that object \nwill be traversed, giving us a unique opportunity to move the object during a reference counting collection. \nBecause each object is unreferenced at birth, at its .rst GC, the set of all increments to a new object \nmust be the set of all references to that object. Second, we exploit the fact that cycle collection involves \na global trace, and thus presents another opportunity to copy objects. In both cases, we use opportunistic \ncopying. Opportunistic copying mixes copying with in-place reference counting and marking such that it \ncan stop copying when it exhausts the available space. Proactive Defragmentation RC Immix s proactive \ndefrag\u00admentation copies as many surviving new objects as possible given a particular copy reserve. During \nthe mutator phase, the allocator dynamically sets aside a portion of memory as a copy reserve, which \nstrictly bounds the amount of copy\u00ading that may occur in the next collection phase. In a classic semi-space \ncopying collector, the copy reserve must be large enough to accommodate all objects surviving because \nit is dictated by the worst case survival scenario. Therefore, every new block of allocation requires \na block for the copy reserve. Because RC Immix is a mark-region collector, which can reuse partially \noccupied blocks, copying is optional. Copy\u00ading is an optimization rather than required for correctness. \nConsequently, we size the copy reserve according to perfor\u00admance criteria. Choosing the copy reserve \nsize re.ects a tradeoff. A large copy reserve eats into memory otherwise available for allo\u00adcation and \ninvites a large amount of copying. Although copy\u00ading mitigates fragmentation, copying is considerably \nmore expensive than marking and should be used judiciously. On the other hand, if the copy reserve is \ntoo small, it may not compact objects that will induce fragmentation later. Our heuristic seeks to mimic \nthe behavior of a genera\u00adtional collector, while making the copy reserve as small as possible. Ideally, \nan oracle would tell us the survival rate of the next collection (e.g., 10%) and the collector would \nsize the copy reserve accordingly. We seek to emulate this policy by using past survival rate to predict \nthe future. Computing .ne-gain byte or object survival in production requires look\u00ading up every object \ns size, which is too expensive. Instead, we use line survival rate as an estimate of byte survival rate. \nWe compute line survival rates of partially full blocks when we scan the line marks in a block to recycle \nits lines. This computation adds no measurable overhead. Table 4 shows the average byte, object, line, \nand block percentage survival rates. Block survival rates signi.cantly over predict actual byte survival \nrates. Line survival rates over predict as well, but much less. The difference between line and block \nsurvival rate is an indication of fragmentation. The larger the difference between the two, the more \nlive objects are spread out over the blocks and the less likely a fresh allocation of a multi-line object \nwill .t in the holes (contiguous free lines). We experimented with a number of heuristics and choose \ntwo effective ones. We call our default copy reserve heuristic Table 4. Benchmark characteristics. Bytes \nallocated into the Immix heap and minimum heap, in MB. The average sur\u00advival rate as a percentage of \nbytes, objects, lines, and blocks measured in an instrumentation run at 1.5\u00d7 the minimum heap size. Block \nsurvival rate is too coarse to predict byte survival rates. Line survival rate is fairly accurate and \nadds no measurable overhead. Immix Min Immix Survival Alloc Heap Byte Object Line Block Benchmark MB \nMB % % % % compress 0.3 21 6 5 7 11 jess 262 20 1 1 7 53 db 53 19 8 6 8 10 javac 174 30 17 19 32 66 mpegaudio \n0.2 13 41 37 44 100 mtrt 97 18 3 3 6 11 jack 248 19 3 2 6 32 avrora 53 30 1 4 8 9 bloat 1091 40 1 1 5 \n32 chart 628 50 4 5 17 67 eclipse 2237 84 6 6 7 36 fop 47 35 14 13 29 69 hsqldb 112 115 23 23 26 56 jython \n1349 90 0 0 0 0 luindex 9 30 8 11 11 15 lusearch 1009 30 3 2 4 22 lusearch-.x 997 30 1 1 2 8 pmd 364 \n55 9 11 14 26 sun.ow 1820 30 1 2 5 99 xalan 507 40 12 5 24 51 pjbb2005 1955 355 11 12 24 87 Heap Size \nHeuristic 1.2\u00d7 1.5\u00d7 2\u00d7 MAX 1.031 0.984 0.976 EXP 1.036 0.990 0.982  Table 5. Two proactive copying heuristics \nand their perfor\u00admance at 1.2, 1.5 and 2 times the minimum heap size, av\u00aderaged over all benchmarks. \nTime is normalized relative to GenImmix. Lower is better. MAX. MAX simply takes the maximum survival \nrate of the last N collections (4 in our experiments). Also good, but more complex, is a heuristic we \ncall EXP. EXP computes a moving window of survival rates in buckets of N bytes of allocation (32 MB in \nour experiments) and then weights each bucket by an exponential decay function (1 for the current bucket, \n1/2 for the next oldest, 1/4, and so on). Table 5 shows that the simple MAX heuristic performs well. \nWe believe better heuristics are possible. Reactive Defragmentation RC Immix also performs reac\u00adtive \ndefragmentation, during cycle collection. At the start of each cycle collection, the collector determines \nwhether Table 6. Sensitivity to frequency of cycle detection and re\u00adactive defragmentation at 1.2, 1.5 \nand 2 times the minimum heap size, averaged over all benchmarks. Time is normalized relative to GenImmix. \nLower is better. Threshold Cycle Defrag Heap Size 1.2\u00d7 1.5\u00d7 2\u00d7 1% 5% 10% 1% 5% 10% 1.030 1.041 1.096 \n0.983 0.983 0.993 0.975 0.976 0.980 to defragment based on fragmentation levels, any available free \nblocks, and any available partially .lled blocks contain\u00ading free lines, using statistics it gathers \nin the previous col\u00adlection. RC Immix uses these statistics to select defragmen\u00adtation sources and targets. \nIf an object is unmovable when the collector .rst encounters it, the collector marks the ob\u00adject and \nline live, increments the object and line counts, and leaves the object in place. When the collector \n.rst encoun\u00adters a movable live object on a source block, and there is still suf.cient space for it on \na target block, it opportunisti\u00adcally evacuates the object, copying it to the target block, and leaves \na forwarding pointer that records the address of the new location. If the collector encounters subsequent \nrefer\u00adences to a forwarded object, it replaces them with the value of the object s forwarding pointer. \nA key empirical question for cycle detection and defrag\u00admentation is how often to perform them. If we \nperform them too often, the system loses its incrementality and pays both reference counting and tracing \noverheads. If we perform them too infrequently, it takes a long time to reclaim objects kept alive by \ndead cycles and the heap may suffer a lot of fragmentation. Both waste memory. This threshold is nec\u00adessarily \na heuristic. We explore thresholds as a function of heap size. We use the following principle for our \nheuristic. If at the end of a collection, the amount of free memory available for allocation falls below \na given threshold, then we mark the next collection for cycle collection. We can always in\u00adclude defragmentation \nwith cycle detection, or we can per\u00adform it less frequently. Triggering cycle collection and de\u00adfragmentation \nmore often enables applications to execute in smaller minimum heap sizes, but will degrade performance. \nDepending on the scenario, this choice might be desirable. We focus on performance and use a free memory \nthreshold which is a fraction of the total heap size. We experiment with a variety of thresholds to pick \nthe best values for both and show the results for three heap sizes in Table 6. (See Sec\u00ad tion 4 for our \nmethodology.) Based on the results in Table 6, we use 1% for both.  3.3 Optimized Root Scanning The \nexisting implementation of the RC algorithm treats Jikes RVM s boot image as part of the root set [27], \nenumerating each reference in the boot image at each collection. We identi.ed this as a signi.cant bottleneck \nin small heaps and instead treat the boot image as a non-collected part of the heap, rather than part \nof the root set. This very simple change delivers a signi.cant performance boost to RC in modest heaps \nand is critical to RC Immix s performance in small heaps (Figure 4(a)). 4. Methodology This section \npresents software, hardware, and measurement methodologies that we use to evaluate RC Immix. Benchmarks. \nWe draw 21 benchmarks from DaCapo [10], SPECjvm98 [28], and pjbb2005 [9]. The pjbb2005 bench\u00admark is \na .xed workload version of SPECjbb2005 [29] with 8 warehouses that executes 10,000 transactions per ware\u00adhouse. \nWe do not use SPECjvm2008 because that suite does not hold workload constant, so is unsuitable for GC \nevalua\u00adtions unless modi.ed. Since a few DaCapo 9.12 benchmarks do not execute on our virtual machine, \nwe use benchmarks from both 2006-10-MR2 and 9.12 Bach releases of DaCapo to enlarge our suite. We omit \ntwo outliers, mpegaudio and lusearch, from our .gures and averages, but include them grayed-out in tables, \nfor completeness. The mpegaudio benchmark is a very small benchmark that performs almost zero allocation. \nThe luse\u00adarch benchmark allocates at three times the rate of any other. The lusearch benchmark derives \nfrom the 2.4.1 stable release of Apache Lucene. Yang et al. [33] found a performance bug in the method \nQueryParser.getFieldQuery(), which revision r803664 of Lucene .xes [26]. The heavily executed getFieldQuery() \nmethod unconditionally allocated a large data structure. The .xed version only allocates a large data \nstructure if it is unable to reuse an existing one. This .x cuts total allocation by a factor of eight, \nspeeds the benchmark up considerably, and reduces the allocation rate by over a factor of three. We patched \nthe DaCapo lusearch benchmark with just this .x and we call the .xed benchmark lusearch-.x. The presence \nof this anomaly for over a year in public releases of a widely used package suggests that the behavior \nof lusearch is of some interest. Compared with GenImmix, RC Immix improves the performance of lusearch \nby 34% on i7-2600, but we use lusearch-.x in our results. Jikes RVM &#38; MMTk. We use Jikes RVM and \nMMTk for all of our experiments. Jikes RVM [1] is an open source high performance Java virtual machine \n(VM) written in a slightly extended version of Java. We use Jikes RVM re\u00adlease 3.1.2+hg r10475 to build \nRC Immix and compare it with different GCs. MMTk is Jikes RVM s memory man\u00adagement sub-system. It is \na programmable memory manage\u00adment toolkit that implements a wide variety of collectors that reuse shared \ncomponents [8]. All of the garbage collectors we evaluate are paral\u00adlel [7]. They use thread local allocation \nfor each applica\u00ad tion thread to minimize synchronization. During collection,  the collectors exploit \navailable software and hardware par-5.1 RC Immix Performance Overview allelism [12]. To compare collectors, \nwe vary the heap size to understand how well collectors respond to the time space tradeoff. In our experiments, \nno collector consistently ran in smaller heaps than the other collectors. Therefore we se\u00adlected for \nour minimum heap size the smallest heap size in which all of the collectors execute, and thus have complete \nresults at all heap sizes for all collectors. Jikes RVM does not have a bytecode interpreter. Instead, \na fast template-driven baseline compiler produces machine code when the VM .rst encounters each Java \nmethod. The adaptive compilation system then judiciously optimizes the most frequently executed methods. \nUsing a timer-based ap\u00adproach, it schedules periodic interrupts. At each interrupt, the adaptive system \nrecords the currently executing method. Using a cost model, it then selects frequently executing methods \nit predicts will bene.t from optimization. The opti\u00admizing compiler compiles these methods at increasing \nlevels of optimizations. To reduce perturbation due to dynamic optimization and to maximize the performance \nof the underlying system that we improve, we use a warmup replay methodology. Be\u00adfore executing any experiments, \nwe gathered compiler opti\u00admization pro.les from the 10th iteration of each benchmark. When we perform \nan experiment, we execute one complete iteration of each benchmark without any compiler optimiza\u00adtions, \nwhich loads all the classes and resolves methods. We next apply the benchmark-speci.c optimization pro.le \nand perform no subsequent compilation. We then measure and report the subsequent iteration. This methodology \ngreatly re\u00adduces non-determinism due to the adaptive optimizing com\u00adpiler and improves underlying performance \nby about 5% compared to the prior replay methodology [11]. We run each benchmark 20 times (20 invocations) \nand report the average. We also report 95% con.dence intervals for the average us\u00ading Student s t-distribution. \nOperating System. We use Ubuntu 10.04.01 LTS server distribution and a 64-bit (x86 64) 2.6.32-24 Linux \nkernel. Hardware Platform. We report performance, performance counter, and detailed results on a 32nm \nCore i7-2600 Sandy Bridge with 4 cores and 2-way SMT running at 3.4GHz. The two hardware threads on each \ncore share a 32KB L1 in\u00adstruction cache, 32KB L1 data cache, and 256KB L2 cache. All four cores share \na single 8MB last level cache. A dual\u00adchannel memory controller is integrated into the CPU. The system \nhas 4GB of DDR3-1066 memory installed. 5. Results We .rst compare RC Immix with other collectors at a \nmoder\u00adate 2\u00d7 heap size, then consider sensitivity to available mem\u00adory, and perform additional in depth \nanalysis. Table 7 and Figure 3 compare total time, mutator time, and garbage collection time of RC Immix \nand RC Immix without proactive copying ( no PC ) against a number of collectors. The .gure illustrates \nthe data and the table includes raw per\u00adformance as well as relative measurements of the same data. This \nanalysis uses a moderate heap size of 2\u00d7 the minimum in which all collectors can execute each benchmark. \nPro\u00adduction systems often use this heap size because it strikes a balance in the space-time tradeoff \nexposed by garbage col\u00adlected languages between memory consumption and garbage collection overheads. \nWe explore the space-time tradeoff in more detail in Section 5.2. In Figure 3(c) and 3(d), results are \nmissing for some con.gurations on some benchmarks. In each of these cases, either the numerator or denominator \nor both performed no GC (see Table 7). The table and .gure compare six collectors. 1. GenImmix, which \nuses a copying nursery and an Immix mature space. 2. Sticky Immix, which uses Immix with an in-place \ngener\u00adational adaptation [6, 15]. 3. Full heap Immix. 4. RC from Shahriyar et al. 5. RC Immix (no \nPC) which excludes proactive copying and performs well in moderate to large heaps due to very low collection \ntimes. 6. RC Immix as described in the previous section, which performs well at all heap sizes.  We \nnormalize to GenImmix since it is the best performing in the literature [6] across all heap sizes and \nconsequently is the default production collector in Jikes RVM. All of the col\u00adlectors, except RC, defragment \nwhen there is an opportunity, i.e., there are partially .lled blocks without fresh allocation and fragmentation \nis high, as described in Section 3.2. These results show that RC Immix outperforms the best performing \ngarbage collector at this moderate heap size and completely eliminates the reference counting performance \ngap. The timegc show that, not surprisingly, Immix, the only full heap collector that does not exploit \nany generational be\u00adhaviors, has the worst collector performance, degrading by on average 34%. Since \ngarbage collection time is a relatively smaller in.uence on total time in a moderate heap, all but RC \nperform similarly on total time. At this heap size RC Immix performs the same as RC Immix (no PC), but \nits worse-case degradation is just 5% while its best case improvement is 22%. By comparison, RC Immix \n(no PC) has a worst case degradation of 12% and best case improvement of 24%. Ta\u00adble 7 and Figure 3(c) \nshow that RC Immix (no PC) has the best garbage collection time, outperforming GenImmix by 48%. As we \nshow later, RC Immix has an advantage over RC Immix (no PC) when memory is tight and fragmentation is \na bigger issue.  (a) Total slowdown compared to GenImmix (b) Mutator slowdown compared to GenImmix \n (c) GC slowdown compared to GenImmix (d) Percentage of total execution time spent in GC   Figure \n3. RC Immix performs 3% better than GenImmix, the highest performance generational collector, at a moderate \nheap size of 2 times the minimum. The .rst three graphs compare total, mutator, and GC slowdowns relative \nto GenImmix; lower is better. The fourth graph indicates the GC load seen by each con.guration. RC Immix \neliminates all the mutator time overheads of RC. Error bars are not shown, but 95% con.dence intervals \nare given in Table 7.  Benchmark GenImmix StickyImmix Immix RC RC Immix (no PC) RC Immix milliseconds \n    Normalized to GenImmix time timemu timegc time timemu timegc time timemu timegc time timemu timegc \ntime timemu timegc time timemu timegc compress 2256 \u00b10.2 2237 \u00b10.2 20 \u00b14.6 1.00 \u00b10.2 1.00 \u00b10.2 1.37 \u00b15.9 \n0.99 \u00b10.2 0.99 \u00b10.2 1.09 \u00b14.1 1.00 \u00b10.1 1.00 \u00b10.2 0.76 \u00b13.2 0.97 \u00b10.2 0.98 \u00b10.2 0.25 \u00b12.9 0.97 \u00b10.2 0.97 \n\u00b10.2 0.28 \u00b11.6 jess 485 \u00b10.7 453 \u00b10.7 32 \u00b14.3 0.98 \u00b10.6 0.99 \u00b10.6 0.77 \u00b13.2 1.09 \u00b10.8 1.00 \u00b10.8 2.42 \n\u00b18.0 1.33 \u00b11.1 1.28 \u00b11.2 2.08 \u00b17.4 1.02 \u00b10.9 1.03 \u00b11.0 0.85 \u00b16.7 1.01 \u00b10.7 1.01 \u00b10.6 0.98 \u00b16.8 db 1491 \n\u00b10.4 1460 \u00b10.4 31 \u00b17.1 1.06 \u00b10.5 1.01 \u00b10.5 3.29 \u00b117.8 0.96 \u00b11.0 0.96 \u00b11.0 0.92 \u00b15.6 1.09 \u00b10.8 1.10 \u00b10.8 \n0.68 \u00b14.0 0.96 \u00b10.9 0.97 \u00b10.8 0.51 \u00b17.0 0.97 \u00b10.7 0.97 \u00b10.8 0.85 \u00b18.0 javac 1048 \u00b10.7 911 \u00b10.4 137 \u00b14.5 \n1.02 \u00b10.6 1.01 \u00b10.3 1.10 \u00b14.6 0.86 \u00b10.4 0.95 \u00b10.3 0.25 \u00b10.9 0.97 \u00b10.5 1.08 \u00b10.3 0.20 \u00b10.7 0.89 \u00b10.5 1.01 \n\u00b10.3 0.08 \u00b11.5 1.05 \u00b12.3 1.03 \u00b10.5 1.17 \u00b115.2 mpegaudio 1406 \u00b10.1 1406 \u00b10.1 0 \u00b10.0 1.01 \u00b10.2 1.01 \u00b10.2 \n0.00 \u00b10.0 1.01 \u00b10.1 1.01 \u00b10.1 0.00 \u00b10.0 1.00 \u00b10.1 1.00 \u00b10.1 0.00 \u00b10.0 0.97 \u00b10.1 0.97 \u00b10.1 0.00 \u00b10.0 0.97 \n\u00b10.1 0.97 \u00b10.1 0.00 \u00b10.0 mtrt 340 \u00b13.5 302 \u00b13.8 38 \u00b12.8 1.00 \u00b13.7 1.01 \u00b14.2 0.92 \u00b17.1 1.06 \u00b12.6 0.98 \n\u00b12.7 1.72 \u00b15.7 1.06 \u00b12.6 1.07 \u00b12.9 1.00 \u00b13.9 0.96 \u00b13.6 1.01 \u00b14.2 0.58 \u00b13.0 0.98 \u00b13.3 0.99 \u00b13.4 0.89 \u00b17.0 \njack 715 \u00b10.7 665 \u00b10.7 50 \u00b17.3 0.94 \u00b10.6 0.97 \u00b10.6 0.57 \u00b13.4 1.00 \u00b10.8 0.97 \u00b10.7 1.40 \u00b17.4 1.18 \u00b10.8 \n1.13 \u00b10.7 1.75 \u00b19.2 0.97 \u00b10.8 0.98 \u00b10.7 0.72 \u00b16.6 0.97 \u00b10.7 0.99 \u00b10.7 0.74 \u00b14.6 mean 1056 \u00b10.9 1005 \u00b10.9 \n51 \u00b14.4 geomean 1.00 1.00 1.12 0.99 0.97 1.06 1.10 1.11 0.85 0.96 1.00 0.39 0.99 0.99 0.75 avrora 3154 \n\u00b11.2 3134 \u00b11.2 20 \u00b19.6 0.99 \u00b11.3 1.00 \u00b11.3 0.21 \u00b11.7 0.98 \u00b11.1 0.98 \u00b11.1 0.58 \u00b15.1 0.97 \u00b11.3 0.98 \u00b11.3 \n0.35 \u00b12.6 0.98 \u00b11.2 0.98 \u00b11.2 0.12 \u00b11.2 0.98 \u00b11.1 0.99 \u00b11.1 0.46 \u00b116.4 bloat 3164 \u00b10.4 3018 \u00b10.5 145 \n\u00b11.7 1.04 \u00b10.5 1.03 \u00b10.6 1.09 \u00b12.1 1.07 \u00b10.8 0.99 \u00b10.8 2.71 \u00b16.4 1.20 \u00b10.5 1.19 \u00b10.6 1.51 \u00b12.6 1.02 \u00b10.8 \n1.02 \u00b10.7 0.90 \u00b14.0 0.99 \u00b10.5 1.00 \u00b10.5 0.68 \u00b13.1 chart 3750 \u00b10.2 3473 \u00b10.1 276 \u00b11.6 1.02 \u00b10.2 1.02 \u00b10.2 \n1.09 \u00b11.4 0.98 \u00b10.5 1.01 \u00b10.5 0.60 \u00b10.9 1.08 \u00b10.5 1.13 \u00b10.5 0.48 \u00b10.9 0.99 \u00b10.8 1.04 \u00b10.8 0.35 \u00b11.0 0.99 \n\u00b10.5 1.03 \u00b10.7 0.52 \u00b13.2 eclipse 16203 \u00b14.0 15382 \u00b14.2 821 \u00b11.1 1.07 \u00b15.7 1.04 \u00b15.9 1.51 \u00b11.5 1.06 \u00b113.1 \n1.01 \u00b18.5 2.06 \u00b1170.2 1.12 \u00b15.7 1.13 \u00b16.1 0.99 \u00b11.0 0.99 \u00b14.9 1.02 \u00b15.2 0.57 \u00b10.7 1.03 \u00b15.2 1.04 \u00b15.5 \n0.79 \u00b14.5 fop 868 \u00b10.8 848 \u00b10.8 20 \u00b12.0 1.05 \u00b10.9 1.04 \u00b10.9 1.11 \u00b11.9 0.99 \u00b10.9 0.99 \u00b10.9 0.98 \u00b14.1 1.02 \n\u00b10.9 1.02 \u00b10.9 0.92 \u00b11.8 0.97 \u00b10.9 0.98 \u00b10.9 0.59 \u00b11.2 0.99 \u00b11.0 0.99 \u00b11.0 1.16 \u00b112.4 hsqldb 970 \u00b10.8 \n783 \u00b10.1 188 \u00b14.3 1.13 \u00b11.9 0.98 \u00b10.2 1.72 \u00b111.2 1.41 \u00b12.4 0.96 \u00b12.8 3.25 \u00b110.0 1.11 \u00b10.7 1.16 \u00b10.2 0.88 \n\u00b12.7 0.92 \u00b10.6 0.98 \u00b10.5 0.66 \u00b12.3 1.03 \u00b10.6 0.98 \u00b10.1 1.26 \u00b13.9 jython 3581 \u00b10.5 3493 \u00b10.5 88 \u00b11.8 1.03 \n\u00b10.5 1.01 \u00b10.5 1.66 \u00b12.8 1.02 \u00b10.4 0.95 \u00b10.4 3.71 \u00b19.1 1.15 \u00b10.5 1.12 \u00b10.5 2.36 \u00b13.4 0.99 \u00b10.4 1.00 \u00b10.4 \n0.58 \u00b14.2 0.98 \u00b10.6 0.99 \u00b10.6 0.61 \u00b11.5 luindex 626 \u00b10.3 620 \u00b10.3 7 \u00b14.4 1.02 \u00b10.3 1.01 \u00b10.3 1.50 \u00b16.8 \n0.99 \u00b10.3 1.00 \u00b10.3 0.00 \u00b10.0 1.02 \u00b10.3 1.02 \u00b10.3 1.10 \u00b14.7 1.02 \u00b10.3 1.03 \u00b10.3 0.50 \u00b12.9 1.04 \u00b10.4 1.04 \n\u00b10.4 0.73 \u00b15.1 lusearch 3154 \u00b10.3 2147 \u00b10.3 1007 \u00b10.9 1.01 \u00b10.7 0.77 \u00b10.5 1.52 \u00b12.4 0.91 \u00b10.4 0.72 \u00b10.5 \n1.30 \u00b11.1 1.12 \u00b10.7 0.89 \u00b10.6 1.61 \u00b11.4 0.66 \u00b10.4 0.75 \u00b10.6 0.46 \u00b10.5 0.66 \u00b10.5 0.75 \u00b10.8 0.46 \u00b10.5 lusearch.x \n887 \u00b13.2 767 \u00b13.7 120 \u00b12.9 0.92 \u00b12.8 0.96 \u00b13.2 0.66 \u00b11.7 1.03 \u00b13.0 0.89 \u00b13.1 1.90 \u00b14.2 1.23 \u00b14.1 1.11 \n\u00b14.5 2.05 \u00b14.3 0.92 \u00b12.7 0.94 \u00b13.2 0.75 \u00b11.7 0.89 \u00b12.2 0.92 \u00b12.6 0.68 \u00b11.7 pmd 934 \u00b11.2 790 \u00b11.2 144 \n\u00b14.5 0.96 \u00b11.0 0.98 \u00b11.2 0.82 \u00b14.5 1.00 \u00b11.4 0.98 \u00b11.0 1.07 \u00b17.8 1.09 \u00b11.5 1.15 \u00b11.2 0.78 \u00b16.0 0.98 \u00b11.2 \n1.03 \u00b11.2 0.73 \u00b15.3 0.94 \u00b11.1 0.98 \u00b11.1 0.72 \u00b13.5 sun.ow 2482 \u00b11.1 2175 \u00b11.3 307 \u00b11.5 0.95 \u00b11.1 0.98 \n\u00b11.2 0.72 \u00b11.4 1.11 \u00b11.1 0.95 \u00b11.1 2.23 \u00b13.8 1.18 \u00b11.1 1.06 \u00b11.1 2.03 \u00b12.9 1.12 \u00b11.3 0.98 \u00b11.1 2.12 \u00b15.5 \n0.95 \u00b11.0 0.98 \u00b11.2 0.73 \u00b12.5 xalan 1393 \u00b18.5 1008 \u00b111.6 385 \u00b11.1 1.09 \u00b16.7 0.90 \u00b17.5 1.58 \u00b13.6 0.93 \n\u00b111.1 0.91 \u00b110.5 0.97 \u00b122.9 0.98 \u00b16.0 0.98 \u00b18.1 0.99 \u00b11.9 0.76 \u00b15.8 0.92 \u00b19.1 0.34 \u00b10.6 0.78 \u00b14.8 0.90 \n\u00b17.7 0.45 \u00b11.2 mean 3168 \u00b11.8 2957 \u00b12.1 210 \u00b12.9 geomean 1.02 1.00 1.01 1.04 0.97 0.00 1.09 1.08 1.05 \n0.97 0.99 0.56 0.96 0.99 0.70 pjbb2005 3775 \u00b11.0 3363 \u00b11.1 412 \u00b12.1 1.07 \u00b11.0 1.00 \u00b11.1 1.61 \u00b13.7 1.11 \n\u00b120.4 1.07 \u00b123.7 1.37 \u00b18.1 1.07 \u00b11.1 1.11 \u00b11.2 0.80 \u00b14.7 1.05 \u00b11.2 1.01 \u00b11.3 1.40 \u00b14.5 0.97 \u00b11.3 1.00 \n\u00b11.3 0.74 \u00b16.5 min 340 302 7 0.92 0.90 0.21 0.86 0.89 0.00 0.97 0.98 0.20 0.76 0.92 0.08 0.78 0.90 0.28 \nmax 16203 15382 821 1.13 1.04 3.29 1.41 1.07 3.71 1.33 1.28 2.36 1.12 1.04 2.12 1.05 1.04 1.26 mean 2533 \n\u00b11.4 2362 \u00b11.6 171 \u00b13.4 geomean 1.02 1.00 1.07 1.03 0.98 1.34 1.09 1.09 0.97 0.97 1.00 0.52 0.97 0.99 \n0.72 Table 7. RC Immix performs 3% better than GenImmix at a moderate heap size of 2\u00d7 the minimum. We \nshow at left total, mutator, and GC time for GenImmix in milliseconds and performance of RC, Immix, Sticky \nImmix, RC Immix (no PC), and RC Immix normalized to GenImmix. Lower is better. We grey-out and exclude \nfrom aggregates lusearch and mpegaudio because of their pathological behaviors, although both perform \nvery well with our systems. The numbers in grey beneath the corresponding arithmetic mean report 95% \ncon.dence intervals, expressed as percentages. \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Despite some clear advantages and recent advances, reference counting remains a poor cousin to high-performance tracing garbage collectors. The advantages of reference counting include a) immediacy of reclamation, b) incrementality, and c) local scope of its operations. After decades of languishing with hopelessly bad performance, recent work narrowed the gap between reference counting and the fastest tracing collectors to within 10%. Though a major advance, this gap remains a substantial barrier to adoption in performance-conscious application domains. </p> <p>Our work identifies heap organization as the principal source of the remaining performance gap. We present the design, implementation, and analysis of a new collector, RC Immix, that replaces reference counting's traditional free-list heap organization with the line and block heap structure introduced by the Immix collector. The key innovations of RC Immix are 1) to combine traditional reference counts with per-line live object counts to identify reusable memory and 2) to eliminate fragmentation by integrating copying with reference counting of new objects and with backup tracing cycle collection. In RC Immix, reference counting offers efficient collection and the line and block heap organization delivers excellent mutator locality and efficient allocation. With these advances, RC Immix closes the 10% performance gap, matching the performance of a highly tuned production generational collector. By removing the performance barrier, this work transforms reference counting into a serious alternative for meeting high performance objectives for garbage collected languages.</p>", "authors": [{"name": "Rifat Shahriyar", "author_profile_id": "81350603030", "affiliation": "Australian National University, Canberra, Australia", "person_id": "P4290317", "email_address": "Rifat.Shahriyar@anu.edu.au", "orcid_id": ""}, {"name": "Stephen Michael Blackburn", "author_profile_id": "81100547435", "affiliation": "Australian National University, Canberra, Australia", "person_id": "P4290318", "email_address": "Steve.Blackburn@anu.edu.au", "orcid_id": ""}, {"name": "Xi Yang", "author_profile_id": "81490653895", "affiliation": "Australian National University, Canberra, Australia", "person_id": "P4290319", "email_address": "Xi.Yang@anu.edu.au", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P4290320", "email_address": "mckinley@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509527", "year": "2013", "article_id": "2509527", "conference": "OOPSLA", "title": "Taking off the gloves with reference counting Immix", "url": "http://dl.acm.org/citation.cfm?id=2509527"}