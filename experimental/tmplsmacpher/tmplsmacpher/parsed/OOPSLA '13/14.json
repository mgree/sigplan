{"article_publication_date": "10-29-2013", "fulltext": "\n Set-Based Pre-Processing for Points-To Analysis Yannis Smaragdakis George Balatsouras George Kastrinis \nDepartment of Informatics University of Athens, 15784, Greece {smaragd,gbalats,gkastrinis}@di.uoa.gr \nAbstract We present set-based pre-analysis: a virtually universal op\u00adtimization technique for .ow-insensitive \npoints-to analysis. Points-to analysis computes a static abstraction of how ob\u00adject values .ow through \na program s variables. Set-based pre-analysis relies on the observation that much of this rea\u00adsoning \ncan take place at the set level rather than the value level. Computing constraints at the set level results \nin sig\u00adni.cant optimization opportunities: we can rewrite the in\u00adput program into a simpli.ed form with \nthe same essential points-to properties. This rewrite results in removing both local variables and instructions, \nthus simplifying the sub\u00adsequent value-based points-to computation. E.ectively, set\u00adbased pre-analysis \nputs the program in a normal form opti\u00admized for points-to analysis. Compared to other techniques for \no.-line optimization of points-to analyses in the literature, the new elements of our approach are the \nability to eliminate statements, and not just variables, as well as its modularity: set-based pre-analysis \ncan be performed on the input just once, e.g., allowing the pre-optimization of libraries that are subsequently \nreused many times and for di.erent analyses. In experiments with Java programs, set-based pre-analysis \neliminates 30% of the program s local variables and 30% or more of computed context-sensitive points-to \nfacts, over a wide set of bench\u00admarks and analyses, resulting in a ~20% average speedup (max: 110%, median: \n18%). Categories and Subject Descriptors F.3.2 [Logics and Meanings of Programs]: Semantics of Programming \nLanguages Program Analysis; D.3.4 [Programming Lan\u00adguages]: Processors Compilers Keywords points-to analysis; \noptimization; o.-line Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than the author(s) must be honored. Abstracting with credit \nis permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October \n29 31, 2013, Indianapolis, Indiana, USA. Copyright is held by the owner/author(s). Publication rights \nlicensed to ACM. ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509524 1. \nIntroduction Points-to analysis consists of computing a static abstraction of all the data that a pointer \nvariable (and, by extension, any pointer expression) can point to during program execution. In modern \nlanguages, points-to analysis forms the substrate of practically any other static analysis: any use of \nstatic anal\u00adysis (e.g., for optimization, bug detection, program compre\u00adhension, online programming assistance) \nneeds to discover the true value of an expression involving pointers (or refer\u00adences , in Java and C#). \nIn a points-to analysis, objects are represented by their allocation sites, possibly quali.ed with a \ncontext for more precision. These object representations are the values of the analysis. Analysis algorithms \noperate at the value level, dis\u00adtinguishing between di.erent values as the program requires. For instance, \nin a simple Java code fragment, such as the one below, the analysis needs to distinguish the di.erent \nal\u00adlocation sites and to reason about the .ow of such values throughout the program. 1 p = new A(); \n2 p.f = new B(); 3 q = new A(); 4 q.f = new C(); 5 r = p.f; 6 r.foo(); Even though lines 1 and 3 allocate \nan object of the same type, the allocations are distinguished (even in a .ow\u00adinsensitive analysis, which \nconsiders all statements in any order). Such value-based reasoning is key: since the two objects are \nused in di.erent ways throughout the fragment (e.g., their f .eld receives objects of di.erent dynamic \ntype) the distinction is kept and leads to higher analysis precision. Eventually, the fact that r can \nonly be assigned the B object allocated in line 2 is essential for deciding what method will be called \nin line 6, due to dynamic dispatch. Our set-based reasoning/pre-analysis technique is based on the observation \nthat value-based reasoning is not always essential in the course of executing a points-to analysis algo\u00adrithm. \nInstead, some reasoning can be performed entirely at the set level, i.e., by considering the entire set \nof values that a variable may hold as a black-box. This is a very general observation, practically applicable \nto any .ow-insensitive points-to analysis algorithm. For a simple example, con\u00adsider the three-statement \npattern below1 and their e.ects on a points-to analysis:  1 p = q; 2 r = p; 3 r = q; // redundant Regardless \nof what values .ow into variables p, q, and r, the assignment in line 3 can be eliminated without a.ecting \nthe results of the points-to analysis. The reasoning for this is entirely at the set level: the set of \nvalues .owing from q to r (due to the assignment of line 3) is a subset of the set of values .owing from \np to r via the assignment of line 2, because the set of values .owing into q is a subset of those .owing \ninto p (due to line 1). As we show later in detail, similar reasoning also applies to several cases of \nvalue .ow through .elds, establishing the redundancy of .eld-read, .eld-write, or pointer assignment \ninstructions, e.g., in the two patterns below. (Again, statements may actually appear in any order, but \nthey have to be in the same procedure.) r = q; p.f = q; p.f = r;  r = p.f; p.f = q; // redundant r \n= q; // redundant Contributions. The main contributions of our work are as follows: We introduce set-based \npre-analysis as the idea of set\u00adbased reasoning and program transformation for points-to analysis, and \ndemonstrate its potential via numerous op\u00adtimizations. Set-based pre-analysis introduces a set-based \n(abstract) reasoning phase, to complement existing value\u00adbased (concrete) reasoning in points-to analysis. \n We implement set-based points-to reasoning as a pre\u00adanalysis and pre-transformation step over the Doop \nframe\u00adwork for Java points-to analysis by Bravenboer et al. [4]. This allows us to transparently apply \nset-based transfor\u00admation to more than 20 di.erent .ow-insensitive points\u00adto analysis algorithms and \ntwo di.erent intermediate representations the default Jimple representation of the Soot framework [21, \n22] and a Static Single Assignment (SSA) version. None of the closest comparable past tech\u00adniques have \nhad such wide applicability.  We evaluate the impact of set-based pre-analysis and pre\u00adtransformation \nover several large Java programs and the standard library. (Notably, no past work on pre-processing constraints \nhas been applied and evaluated in the con\u00adtext of OO languages our technique is intraprocedural and fully \ncompatible with dynamic dispatch/on-the-.y\u00adcallgraph discovery.) In all, 30% of the program s local variables \nand the same amount or more of the context\u00adsensitive points-to facts can be safely eliminated. This re\u00ad \n 1 For all such statement patterns throughout the paper, we consider the state\u00adments to appear in any \norder and amidst any other program instructions, as long as they are in the same procedure. sults in \nspace savings and an average speedup of about 20% over all analyses, with signi.cantly higher numbers \n(up to 110%) for speci.c analyses and input programs. The rest of the paper places our approach amongst \nits closest relatives in the literature (Section 2), introduces the base reasoning for points-to analysis \nand connects it to our optimization approach (Section 3), describes our patterns (Section 4), details \nour implementation (Section 5), presents experimental results (Section 6), and concludes (Section 7). \n2. Placement of the Work The literature on points-to analysis and its optimizations is extensive and \ncovers intriguing breadth and depth. There\u00adfore, it is useful to place our approach in relation to others \nearly on, to make clear its similarities to the closest past work and its novelty. The closest past work \nto our approach consists of tech\u00adniques to establish that two variables are clones, i.e., that a variable \ns points-to set is identical to that of another. Such clone detection has been explored in the context \nof .ow-insensitive C-language analyses, by techniques that are based on the concept of the constraint \ngraph: a graph with nodes denoting pointer variables and an edge between nodes p and q denoting .ow (e.g., \na direct assignment) from vari\u00adable p to variable q. Online cycle elimination by F\u00a8andrich et al. [5] \ndetects cycles in the constraint graph and collapses all nodes in a cycle into a representative node, \nsince such nodes will have identical points-to information. (An enhancement, which does not change the \nessence of online cycle elimina\u00adtion, is o.ered by the projection merging technique of Su et al. [20].) \nThe technique of Nasre [14] extends such con\u00adstraint graph reasoning based on the observation that if \ntwo nodes have the same dominator in the constraint graph, then they are clones: the values .owing to \nthem are (only) those of the dominator node. Even more closely related to our approach are constraint-graph-based \ntechniques that are ap\u00adplied o.-line (i.e., before the points-to analysis runs). Prime examples of such \ntechniques are Rountev and Chandra s [16] and Hardekopf and Lin s [8]. (Hardekopf and Lin have also applied \nsimilar ideas in a hybrid online/o.ine setting [7], but for the purposes of our work the o.ine technique \nis a closer comparable.) Both of these techniques perform an o.-line detection of equivalent points-to \nsets and use this knowl\u00adedge to eliminate redundant work in subsequent points-to computations. Hardekopf \nand Lin s approach is impressively general, computing hash codes that encode all the logical processing \nof a points-to set that is induced by the current program and, thus, detecting equivalent points-to sets \neven through complex program patterns. Set-based pre-analysis has two bene.ts compared to all such past \nwork: Generality: Our reasoning is not centered around points\u00adto sets but around instructions, o.ering \nmore opportunities for optimization. In constraint-graph terms, our approach can also eliminate edges \nof the graph, whereas past ap\u00adproaches could only eliminate entire nodes (which was the only way to eliminate \ntheir incident edges).  Modularity: Past approaches applied such optimizations as an integral part of \nthe analysis. The optimization was tied to the representation structure (constraint-graph) used for the \nanalysis implementation itself. In contrast, our ap\u00adproach can be applied as a local pre-processing step. \nSpeci.cally, set-based pre-analysis generalizes past ap\u00adproaches because it does not need to establish \nequivalence (i.e., that two variables are clones) in order to reap optimiza\u00adtion bene.ts. All our earlier \nexamples are applicable even when the variables involved are not clones of each other. For instance, \nconsider the program pattern: r = q; p.f = r; p.f = q; // redundant  Past approaches could avoid the \ncomputation of the last line but only if points-to sets r and q 2 (or q and p.f) were shown equivalent. \nOur observation is that merely knowing that points-to set r is a superset of points-to set q is su.cient \nfor establishing the redundancy of the last instruction, when taken in conjunction with the second line. \nThus, the above pattern is applicable even when the program contains other assignments to r and to p.f. \nAt the same time, set-based pre-analysis is more mod\u00adular and orthogonal. Speci.cally, we apply set-based \npre\u00adanalysis entirely intraprocedurally, i.e., without considering the subset relationships between points-to \nsets of local vari\u00adables that occur in di.erent methods. This allows set-based pre-analysis to be expressed \nas a local program transforma\u00adtion: every set-based pre-analysis optimization is a rewrite pattern that, \nonce triggered, performs a simpli.cation of the program used as input to the subsequent points-to analysis. \nSuch simpli.cations consist of eliminating instructions or variables from the input program (together \nwith renamings of use-sites of eliminated variables). E.g., in all our exam\u00adples, the instruction labeled \nredundant is removed. This produces a reduced input program that condenses program behavior before the \nreal points-to analysis starts. The result is tantamount to introducing a new intermediate language, \noptimized for the subsequent value-based points-to analysis. Di.erent points-to analysis algorithms can \nthen run on the reduced program and will yield results equivalent to apply\u00ading them on the original input \nprogram. In contrast, it is not always possible to express the closest comparable techniques in past \nliterature [5, 8, 14, 16] as lo\u00adcal program transformations: if two points-to sets are equiv\u00adalent but \nthe sets correspond to local variables at di.erent scopes (e.g., a formal argument in a callee function \nand an 2 More accurately: the points-to sets for variables r and q , but we overload the terminology \nthroughout, when no confusion should arise. actual in a caller), there is no local program transformation \nto express the elimination of one variable and its replace\u00adment by the other at every use site: each \nof the variables is out of scope at the use-site of the other. Our application of set-based reasoning \nonly intra-procedurally means that the rewrite approach is always possible. This yields important modularity \nbene.ts: 1. When the program-under-analysis does not vary (but multiple analyses need to be performed) \nthe program can be reduced once-and-for-all via set-based reasoning. Then, all points-to analyses can \nbe performed over the reduced program. 2. Even when the program-under-analysis varies, much of the analysis \ncomplexity is due to combining the pro\u00adgram with a large standard library. By using our purely\u00adintraprocedural \ntechnique, we can pre-process large li\u00adbraries once-and-for-all, and subsequently analyze them with any \ninput program. 3. Our optimizations are easy to illustrate and understand, since they are pattern-based \nprogram transformations. The optimizations are also orthogonal to other complex reasoning in a points-to \nanalysis (e.g., past o.-line tech\u00adniques do not work with online call-graph construction).  3. Set-Based \nPre-Analysis and Points-To Analysis Via Subset Constraints We next give background on points-to analysis \nfrom an an\u00adgle that demonstrates the applicability of our set-based pre\u00adanalysis idea. As outlined in \nthe Introduction, set-based pre\u00adanalysis is based on the observation that many points-to analysis inferences \ncan be performed at the set level and not the value level. This insight is very general and applies to \nessentially any analysis (although di.erent transformations may be valid for di.erent kinds of analyses, \nas we discuss in Section 4). The generality of the idea of set-based pre-analysis can be seen by .rst \nconsidering how di.erent points-to anal\u00adyses can be expressed in a uni.ed setting. One of the most popular \nways to express points-to analysis algorithms [4, 6, 11, 15, 23, 24] is via subset constraints. Subset \ncon\u00adstraints e.ectively state which value set has to be a subset of which other, with these value sets \nbeing sets of constants, the points-to sets of local variables, the .eld-points-to sets of object expressions, \nand more. Finding minimal sets that satisfy all the subset constraints produces the output of the points-to \nanalysis. Such analyses often leverage the Datalog programming language for their implementation. Datalog \ndirectly encodes recursive subset constraints. This means that the program under analysis is .rst encoded \nas data tables that represent all the program information. For instance, there is typically an input \ntable representing each kind of program instruction in an intermediate language (e.g., tables Move, Alloc, \nStore, etc.). Then, the analysis is performed via rules that encode the subset constraints. For instance, \nthe typical handling of Alloc and Move instructions (i.e., direct assignment of a newly allocated object \nto a variable and assignment between local variables, respectively) is via the following rules:  VarPointsTo(var, \nheap) <-Alloc(var,heap). VarPointsTo(to, heap) <-Move(to, from), VarPointsTo(from, heap). The .rst of \nthese rules states that the set of data in the Alloc table (which is an input table, whose rows encode \nthe corresponding program instructions) is a subset of the VarPointsTo data for the entities (a variable \nand a heap ob\u00adject, i.e., a unique identi.er of an allocation site) participat\u00ading in the Alloc instruction. \nThe second rule states that the points-to set for variable from is a subset of the points-to set for \nvariable to if the program contains a Move instruction between from and to. Other rules can be used to \nintroduce more subset constraints and eventually implement arbitrarily complex analyses. The computation \nperformed by the anal\u00adysis consists of successively enlarging the sets in order to satisfy all the subset \nconstraints. An interesting observation is that the variability between points-to analyses is usually \nnot a.ecting the structure of the main rules for handling program instructions. Virtually all points-to \nanalyses expressed in Datalog will have rules much like the above for handling Alloc and Move instructions. \nFor instance, the GateKeeper analysis of Guarnieri and Livshits [6] has very analogous rules: PTSTO(v, \nh) <-Alloc(v, h). PTSTO(v1, h) <-PTSTO(v2, h), Move(v1, v2). Our own context-sensitive analysis framework \nhas rules that just add context variables to the above. (The context variables are used to vary the precision \nand performance of the analysis, but the mechanism for doing so is not important for our current discussion. \nA concise 9-rule model su.cient to express a large variety of points-to analyses can be found in Kastrinis \net al. [10].) VarPointsTo(var, ctx, heap, hctx) <\u00ad Alloc(var,heap). VarPointsTo(to, ctx, heap, hctx) \n<\u00ad Move(to, from), VarPointsTo(from, ctx, heap, htcx). The common structure of the rules in all these \ndi.erent analyses means that it is possible to make simpli.cations of the input program in a way that \nthese simpli.cations apply to a multitude of analyses. Set-based pre-analysis is based on the observation \nthat some subset constraints are always implied by others and can therefore be eliminated. In other words, \nall set-based pre-analysis patterns that we are go\u00ading to examine in this paper are instances of the \nimplica\u00adtion S . T . T . U . S . U. This simple pattern can be applied to the constraints induced by \ndi.erent rules of existing points-to analyses, e.g., rules handling local assign\u00adments, .eld loads and \nstores, static .elds, etc. For illustra\u00adtion, consider one of our earlier examples of a high-level program: \np = q; r = p; r = q; // redundant This example consists entirely of Move instructions at the intermediate \nlanguage level. To show that the third in\u00adstruction is redundant, consider that its use inside a subset\u00adconstraint-based \npoints-to analysis is in a rule such as: VarPointsTo(to, heap) <-Move(to, from), VarPointsTo(from, heap). \nFor the third instruction, the rule is instantiated with to equal to program variable r and from equal \nto program variable q . Thus, the rule s e.ect is to state that the points\u00adto set of variable q is a \nsubset of the points-to set of variable r a constraint already inferrable by applying the same Dat\u00adalog \nrule to the .rst two instructions. In summary, the idea of set-based pre-analysis is highly general. \nSince virtually all points-to analysis algorithms can be expressed via subset constraints, applying common \nset\u00adbased reasoning on these constraints can determine that sev\u00aderal of them are redundant. A key element \nis that such rea\u00adsoning can be applied to simplify the program alone, inde\u00adpendently of the analysis, \nunder the expectation that all anal\u00adyses of the same general family treat program features via similar \nrules. 4. Set-Based Pre-Analysis and Optimizations We next present a collection of set-based pre-analysis \nin\u00adstances as well as a general discussion on how these trans\u00adformations are applied. 4.1 Set-Based \nPre-Analysis Patterns Our set-based pre-analysis instances are expressed as intra\u00adprocedural program \ntransformations. Although each trans\u00adformation may have speci.c pre-conditions of applicabil\u00adity, all \nof the transformations share a general structure: they consist of multiple program statements whose presence \nen\u00adables removing one or more other (redundant) statements. Therefore all transformations share their \nmain applicability pre-condition: the transformations are applicable to .ow\u00adinsensitive points-to analyses \n(for which the statements of a procedure are considered to execute in any order) as long as the enabling \nstatements appear anywhere in the same proce\u00addure. (Some of our ideas can be adapted to apply to a .ow\u00adsensitive \nsetting, but, in that setting, points-to sets are kept per-instruction, so set-based reasoning is likely \nto be super\u00adseded by normal updates of points-to sets per-statement. Our implementation setting the Doop \nanalysis framework only contains .ow-insensitive points-to analyses.) The optimizations can apply up \nto .xpoint, since apply\u00ading one of them may enable others. Several of the patterns below do not often \nappear verbatim in practice but arise once variables start getting merged, other instructions eliminated, \netc. We discuss this topic further in Section 4.2. Addition\u00adally, application of transformations should \nbe done in a way that the enabling statements are not themselves eliminated by application of two transformations \nat the same time an issue discussed in detail in Section 5.  Store statement elimination. Our .rst transformations \neliminate store statements, i.e., assignments to pointer\u00adindexed memory. r = q; p.f = r; p.f = q; // \nredundant  Via standard subset-based reasoning, the third statement is redundant if the .rst two are \npresent. The same applies to static store statements: r = q; C.f = r; // C is a class C.f = q; // redundant \nLoad statement elimination. The next two transforma\u00adtions eliminate load statements, i.e., reads from \npointer\u00adindexed memory. r = q; q = p.f; r = p.f; // redundant Again, any value .owing to r through the \nload from p.f is redundant, since it also .ows through the move from q (given that q also loads p.f). \nThe same applies to static loads: r = q; q = C.f; // C is a class r = C.f; // redundant Move statement \nelimination. The next patterns eliminate move statements, i.e., copies between local variables. The .rst \nis our earlier example: p = q; r = p; r = q; // redundant However, the same .ow of values can occur \nthrough assign\u00adments to pointer-indexed memory: p.f = q; r = p.f; r = q; // redundant And similarly \nfor static .elds: C.f = q; // C is a class r = C.f; r = q; // redundant Handling of array accesses. \nAll of the earlier patterns also apply to load and store statements involving arrays instead of local \nobjects. This is doubly interesting since points-to analyses often have very approximate handling of \narrays, e.g., considering all array locations arr[i] to be the same abstract location arr[*] an approach \noften called array insensitive. Thus, for instance, we can eliminate array loads: r = q; q = arr[*]; \nr = arr[*]; // redundant Similarly, we can use array loads to eliminate move state\u00adments (and in general \ncan adapt all earlier patterns to array statements): arr[*] = q; r = arr[*]; r = q; // redundant Method \ncall elimination. An interesting observation is that method call statements can also be eliminated using \nset\u00adbased reasoning: r = q; q = p.m(); r = p.m(); // redundant The above is not limited to no-argument \nmethods, but the arguments need to be identical local variables for the trans\u00adformation to apply.3 As \nusual, analogous transformations apply to static meth\u00adods: r = q; q = C.m(); // C is a class r = C.m(); \n// redundant It is somewhat surprising that method calls can be elimi\u00adnated, as above. After all, the \nusual semantics of impera\u00adtive languages dictate that identical method calls cannot be merged since they \ncan have di.erent e.ects on state. A .ow\u00adinsensitive points-to analysis, however, computes an over\u00adapproximation \nof all executions of a program, assuming that every reachable method is executed an unbounded number \nof times. That is, upon encountering a method, the analysis takes into account not just a single execution \nof the method but the maximal e.ects that any number of executions might have on the points-to information. \nThus, points-to analyses do not model state changes performed by a method in a way that repeated equivalent \nactions make a di.erence. Similarly, the above transformation is valid even when the analysis adds context, \nof any usual kind. For instance, as can be seen in models of various kinds of context-sensitivity [10, \n19], new contexts are created at method call sites and the context remains the same throughout the method \nbody. This means that local variables of the same calling method have the same context throughout the \nmethod body, i.e., hold the same values as far as the analysis is concerned. Therefore, two calls to \na method m may be analyzed in di.erent contexts but if their arguments (and receiver object) are lexically \niden\u00adtical then the two calls receive the same information from the 3 Since methods can cause exceptions \nto be thrown, in the case of precise exception handling [3] an extra requirement is that no exception \nhandler starts or ends between the two equivalent method calls.  outside world. Therefore, the information \ncomputed for the body of the called method, m, will be identical under both contexts. Thus, analyzing \nthe function twice has no e.ect on its callers or on the heap model the only di.erence is the (undesirable) \nreplication of identical information in the analysis of the method itself. Duplicate statement elimination. \nAn obvious use of set\u00adbased reasoning is to eliminate duplicate statements (e.g., two instances of p \n= q; , q = p.f; or q = p.m(); ). For many kinds of points-to analyses, duplicate statements are not even \nrepresented in the analysis input. (For our im\u00adplementation setting the Doop framework the only dupli\u00adcate \nstatements represented in the input are method calls. Thus, this transformation has been largely applied \nto the in\u00adput implicitly even before our work.) Interestingly, program transformation patterns can allow \nsome variability when detecting duplicate statements. For instance, consider: q = p.m(); p.m(); // redundant \n The second call to p.m() is redundant even though its form is not identical to the .rst call the return \nvalue is ig\u00adnored. In practice, this detection pattern needs to be speci.ed separately, for most intermediate \nlanguages. Note also that the .rst statement is not made redundant by the existence of the second: it \nde.nes variable q, while the second does not. Duplicate variable elimination. An important optimiza\u00adtion \nin set-based reasoning imitates the variable elimination logic of past work that builds on the constraint \ngraph ab\u00adstraction [5, 8, 14, 16]. The constraint graph is a graph with nodes denoting pointer variables \nand expressions, and edges between them denoting value .ow. The graph encodes all known subset relations \nbetween points-to sets. For instance, an assignment p = q; implies an edge from points-to set q to points-to \nset p in the constraint graph, as well as an edge from set q.f to set p.f, etc. Similarly, an assignment \nq = p.f; implies an edge from set p.f to set q in the con\u00adstraint graph. Similar edges are induced by \nother program constructs (e.g., store statements) and the transitivity prop\u00aderty is applied. On the resulting \ngraph, two local variables are clones of each other whenever any of the following con\u00additions apply:4 \n The variables belong in the same strongly-connected\u00adcomponent of the constraint graph.  The variables \nhave identical in-.ows. E.g.,  q = p.f; r = p.f; q,r receive no other assignments 4 Note that this \nreasoning only applies to local variables. Field expressions cannot be eliminated with such local examination. \nRecall that all our opti\u00admizations are entirely intraprocedural and are expressed as local program transformations. \nor: q = p.m(); r = p.m(); q,r receive no other assignments The above are the most pro.table special cases \nof the general approach of Hardekopf and Lin [8] for identifying equivalent .ows. The variables have \nthe same dominator in the constraint graph: the values .owing to them are the same since they are (only) \nthose of the dominator node. This is Nasre s insight [14], which is also handled by the reasoning in \nHardekopf and Lin s approach [8]. When clone variables are detected, one of them can be eliminated. All \ndef-sites of the eliminated variable are re\u00admoved from the program and all use-sites are renamed to use \nthe other clone variable.  4.2 How Transformations Are Applied Set-based pre-analysis separates set-based \n(abstract) from value-based (concrete) reasoning in points-to analysis. Ef\u00adfectively the approach normalizes \nprograms into a normal form suitable for quick points-to analysis execution. It is important that this \nnormalization is performed in an intra\u00adprocedural setting, so that the results of the transformation \ncan be reused independently of other changes to the code. For instance, large libraries can be transformed \nonce and the result of the transformation can be reused for any program using the library. The transformations \nwe just saw can be applied up to .x\u00adpoint. The reason is that there is a synergy between the two kinds \nof transformations: statement-elimination can enable variable-elimination and vice-versa. For an example \nof the former direction, consider (in the program fragment below, together with its associated constraint \nsubgraph) the rule that two variables are equivalent when they have the same domi\u00adnator [14]: p = new \nObject(); // or any other in-flow, e.g., calls q = new Object(); // or any other in-flow, e.g., calls \nq = p; r = q; r = p; Due to the external .ow to p and q, there is no dom\u00adinance relationship among any \nof the three nodes in this (sub)graph. Yet the assignment r = p; is redundant, as established via set-based \nreasoning. Consequently, if we re\u00admove the p-to-r edge, the resulting constraint graph has q as the dominator \nof r: any value .owing to r has to go through q. Indeed, this is an instance where set-based pre\u00adanalysis \ngeneralizes past approaches. The above example is handled by a special algorithm (called the HU algorithm) \nin Hardekopf and Lin s approach [8]. The HU algorithm aims precisely at exploiting subset relationships \nin the course of determining the equivalence of other points-to sets. (Recall, however, that Hardekopf \nand Lin s approach, just like other past constraint-graph-based approaches, only yields bene.t when it \ndiscovers equivalent variables to eliminate, whereas our approach can also eliminate redundant constraint \ngraph edges/instructions regardless of whether the points-to sets involved end up being equivalent or \nnot.)  Examples of the converse direction are even more com\u00admon: eliminating a variable can trigger \nany of the statement\u00adeliminating optimizations. For instance, the program may contain statements: r = \nq; q = p.m(a); r = s.m(b); If earlier steps establish that p and s are clones, and that a and b are \nclones, then normalizing the use-sites of all clone variables reveals that the last statement is redundant. \n 4.3 Illustration To see the simpli.cation that set-based pre-processing can introduce, consider its \napplication to an example method from the JDK, java.util.TreeMap.rotateRight. Figure 1 shows the full \nbody of the original method in the Jimple in\u00adtermediate language of the Soot framework [21, 22]. Run\u00adning \nour analysis determines that several of the local vari\u00adables are redundant and the method body can be \nsigni.cantly simpli.ed. (r0 is cloning @this; r1 is cloning @param0; r4 and r5 are cloning r3; r7, r8, \nr10, and r11 are cloning r6.) The result of the transformation can be seen in Figure 2. 5 As can be seen, \nthe reduced form is much shorter than the original and eliminates internal complexity. It also allows \nus to illustrate some important points. First, the reduced form of the bytecode is obtained under the \nassumption of .ow-insensitive points-to analysis and, thus, is not guaranteed to be equivalent to the \noriginal, or even legal (e.g., may violate conventions of the intermediate language). For instance, the \nreduced program may be using variables that are only assigned in di.erent .ows of con\u00adtrol. Nevertheless, \nthe simpli.ed method body is a faithful substitute of the original as far as .ow-insensitive points-to \nanalysis is concerned. Even if we were to apply the trans\u00adformations with a .ow-sensitive analysis in \nmind (e.g., only 5 This result was produced manually, since our implementation does not a.ect the textual \nform of the intermediate language. However, the man\u00adual mapping depicts the actual simpli.cations of \nthe input program as per\u00adformed by our implementation. apply transformations that eliminate duplicate \nactions), the result would not be guaranteed equivalent. For instance, in normal execution, reading the \nsame .eld twice or calling the same method twice is not the same as reading the .eld or calling the method \njust once. Also note that the reduction of the program may be af\u00adfecting externally visible elements. \nFor most client analyses this is not the case e.g., analyses .nding reachable methods or computing a \nconnectivity graph of heap objects are unaf\u00adfected by set-based pre-processing. Yet other analyses may \nhave a concept of internal elements, such as exact calling instructions or temporary local variables. \nFor instance, the user of the analysis may request the points-to set of elim\u00adinated local variable r1, \nor the target methods of an elimi\u00adnated call-site. Computing this information is a mere matter of post-processing \nand does not a.ect the inherent precision or correctness of the analysis. That is, the analysis on the \nreduced program is fully equivalent to that on the original, yet the output information can be viewed \nas being in a con\u00addensed form. Similar no-loss condensed representations are common in points-to analysis \nalgorithms (e.g., for excep\u00adtion object merging [9]). It is straightforward to reproduce the original \noutput, if so desired by the client analysis, and this can also be done lazily, upon request. For instance, \nin\u00adstead of querying the points-to set of variable r1, a client analysis will .nd the variable that replaced \nr1 and query its points-to set. To simulate the full original points-to set for all variables in the \nprogram, we only need to combine the con\u00addensed points-to information with the information of which local \nvariables were replaced by which others. As we discuss in Section 6, this per-analysis post-processing \nhas virtually zero cost. 5. Implementation Our implementation of set-based pre-analysis is in the con\u00adtext \nof the Doop framework for Java points-to analysis by Bravenboer et al. [4]. Doop expresses a large variety \nof .ow\u00adinsensitive points-to analyses declaratively, using the Data\u00adlog language. Our set-based pre-analysis \nimplementation ap\u00adplies transparently to all Doop analyses as a pre-processing step over their normal \ninput. Although this step could be im\u00adplemented in some other language, we chose to use Datalog in our \nimplementation for reasons of convenience and engi\u00adneering uniformity. Speci.cally, set-based pre-analysis \nis a pre-computation over all input tables of the regular points-to analysis. As mentioned earlier, such \ntables represent all syntactic con\u00adstructs, i.e., instruction types, of the intermediate language. Since \nour set-based pre-analysis implementation is declara\u00adtive, it cannot alter the input tables in-place. \nInstead, the op\u00adtimization is performed as a sequence of alternating phases. First, a detection phase \ndiscovers all opportunities for opti\u00ad  private void rotateRight(java.util.TreeMap$Entry) java.util.TreeMap \nr0; java.util.TreeMap$Entry r1, r2, $r3, $r4, $r5, $r6, $r7, $r8, $r9, $r10, $r11; r0 := @this: java.util.TreeMap; \nr1 := @param0: java.util.TreeMap$Entry; if r1 == null goto label4;  r2 = r1.<java.util.TreeMap$Entry: \njava.util.TreeMap$Entry left>; $r3 = r2.<java.util.TreeMap$Entry: java.util.TreeMap$Entry right>; r1.<java.util.TreeMap$Entry: \njava.util.TreeMap$Entry left> = $r3; $r4 = r2.<java.util.TreeMap$Entry: java.util.TreeMap$Entry right>; \nif $r4 == null goto label0; $r5 = r2.<java.util.TreeMap$Entry: java.util.TreeMap$Entry right>; $r5.<java.util.TreeMap$Entry: \njava.util.TreeMap$Entry parent> = r1; label0: $r6 = r1.<java.util.TreeMap$Entry: java.util.TreeMap$Entry \nparent>; r2.<java.util.TreeMap$Entry: java.util.TreeMap$Entry parent> = $r6; $r7 = r1.<java.util.TreeMap$Entry: \njava.util.TreeMap$Entry parent>; if $r7 != null goto label1; r0.<java.util.TreeMap: java.util.TreeMap$Entry \nroot> = r2; goto label3; label1: $r8 = r1.<java.util.TreeMap$Entry: java.util.TreeMap$Entry parent>; \n$r9 = $r8.<java.util.TreeMap$Entry: java.util.TreeMap$Entry right>; if $r9 != r1 goto label2; $r10 = \nr1.<java.util.TreeMap$Entry: java.util.TreeMap$Entry parent>; $r10.<java.util.TreeMap$Entry: java.util.TreeMap$Entry \nright> = r2; goto label3; label2: $r11 = r1.<java.util.TreeMap$Entry: java.util.TreeMap$Entry parent>; \n$r11.<java.util.TreeMap$Entry: java.util.TreeMap$Entry left> = r2; label3: r2.<java.util.TreeMap$Entry: \njava.util.TreeMap$Entry right> = r1; r1.<java.util.TreeMap$Entry: java.util.TreeMap$Entry parent> = r2; \nlabel4: return; Figure 1. Original JDK method in Jimple form (manually reformatted for space). private \nvoid rotateRight(java.util.TreeMap$Entry) java.util.TreeMap$Entry r2, $r3, $r6, $r9; if @param0 == null \ngoto label4; r2 = @param0.<java.util.TreeMap$Entry: java.util.TreeMap$Entry left>; $r3 = r2.<java.util.TreeMap$Entry: \njava.util.TreeMap$Entry right>; @param0.<java.util.TreeMap$Entry: java.util.TreeMap$Entry left> = $r3; \nif $r3 == null goto label0; $r3.<java.util.TreeMap$Entry: java.util.TreeMap$Entry parent> = @param0; \nlabel0: $r6 = @param0.<java.util.TreeMap$Entry: java.util.TreeMap$Entry parent>; r2.<java.util.TreeMap$Entry: \njava.util.TreeMap$Entry parent> = $r6; if $r6 != null goto label1; @this.<java.util.TreeMap: java.util.TreeMap$Entry \nroot> = r2; goto label3; label1: $r9 = $r6.<java.util.TreeMap$Entry: java.util.TreeMap$Entry right>; \nif $r9 != @param0 goto label2; $r6.<java.util.TreeMap$Entry: java.util.TreeMap$Entry right> = r2; goto \nlabel3; label2: $r6.<java.util.TreeMap$Entry: java.util.TreeMap$Entry left> = r2; label3: r2.<java.util.TreeMap$Entry: \njava.util.TreeMap$Entry right> = @param0; @param0.<java.util.TreeMap$Entry: java.util.TreeMap$Entry parent> \n= r2; label4: return; Figure 2. Reduced JDK method. Labels remain and should help in matching with the \ncorresponding regions of Figure 1. mization over the entire program (as if) in parallel.6 Then, a transformation \nphase produces simpli.ed new input tables by computing the result of the optimizations over the origi\u00adnal \ntables. These two steps repeat until no more optimization is pro.table. The above scheme introduces some \nsubtleties. The .rst concerns the detection phase: Discovering the potential for all optimizations in \nparallel means that we have to manually ensure to never optimize away an instruction that may enable \nanother optimization. This requires all of our declarative rules to have disabling conditions. For instance, \nconsider our usual pattern for eliminating load instructions: r = q; q = p.f; r = p.f; // redundant This \npattern is implemented by the Datalog rule below (sim\u00adpli.ed, with variables renamed for easy correspondence \nwith the example): RedundantLoad(_r, _p, _f) <\u00ad Load(_r, _p, _f), Load(_q, _p, _f), Move(_r, _q), _r \n!= _q, !TransitiveFlow(_r, _q). The last two conditions of the rule ensure that logical vari\u00adables r \nand q represent di.erent program variables (which is a necessary precondition in any setting) and that \nthere is no .ow (i.e., no direct or indirect assignments) from r to q. The latter condition is included \nonly because of the declar\u00adative evaluation of the rules, which evaluates them as-if-in\u00adparallel. Its \nresult is to guard against removing one of the .rst two lines of the pattern. Without it, the same pattern \nwould match twice in code such as: z = y; y = z; // could be more complex flow z = x.f; // redundant \ny = x.f; // also redundant but not both Matching the pattern twice would remove both load instruc\u00adtions \n(i.e., both of the last two lines) which is erroneous. Some of our rules have numerous disabling conditions \nas a result of similar reasoning for instance, the rule to com\u00adpute redundant Move instructions has six \nextra conditions, to avoid accidental cycles as well as overlap with other patterns that would invalidate \nthe rewrite. A second subtlety concerns the transformation phase: Al\u00adthough multiple optimizations may \nbe legitimately enabled, there may be overlap in their application. For a declarative implementation, \nwhere the rules can be applied in any order, we need to explicitly disambiguate what happens in case \nof such overlap. For instance, consider the program statement p.f = q;. Our rules distinguish the following \ncases of en\u00adabled transformations: 6 Currently there is no real parallel computation, although this is \nentirely up to the Datalog evaluation engine. The entire statement is redundant and, thus, eliminated. \n The statement is not redundant, but variable p alone is a redundant copy of some other variable s. \nThe above use of variable p should be replaced by s.  The statement is not redundant, but variable q \nalone is a redundant copy of some other variable r (with a similar transformation as before).  The statement \nis not redundant, but both variables p and q are redundant copies of variables s and r, respectively. \nThus, both variable uses should be replaced.  The above complications are typical of rule-based rewrite \nsystems and should also apply to other implementations that employ pattern-based program transformation \nfor set-based pre-processing. Note that the correctness of the above trans\u00adformations (especially due \nto the potential of removing state\u00adments that enable other transformations performed in par\u00adallel) is \nleft to the implementor of the transformations. For our implementation, we have followed a highly stylized \npat\u00adtern that orders optimizations for every statement kind from stronger to weaker, as in the example \nof the bullet points above. Additionally, every optimization transformation in\u00adcludes extra conditions \nto ensure that its enabling statements are not themselves eliminated. Of course, human error can always \ncreep in. In practice we found that it is quite easy to debug our rewrite rules, since they produce semantically \nequivalent input programs from the perspective of subse\u00adquent points-to analysis. Observing a single \ndetailed met\u00adric that remains invariant by our optimizations is typically enough to detect even rare \nerrors in our logic. (An excellent such metric has been the context-sensitive instance .eld points-to \nvalue, which sums together the sizes of points\u00adto sets of all heap objects. This value typically has \n6 or more signi.cant digits and is very sensitive to any seman\u00adtic change in the input program.) Our \nimplementation of set-based analysis and transfor\u00admation currently consists of approximately 150 Datalog \nrules, some of which are fairly involved.7 Recall that, in addition to transformation patterns, these \nrules implement algorithms for strongly-connected components, dominance, etc., over the constraint-graph \nabstraction. This logic usually takes well under a minute to apply to the programs of our benchmark set \ntogether with the full JDK library. The run\u00adning time can be signi.cantly reduced further e.g., we can \nenable only the most pro.table transformations for roughly half the cost. Furthermore, the only inevitable \ntime cost is that of the detection phase, i.e., of identifying all the sites where the optimization will \ntake place. This typically takes 10secs or less. The rest of the time consists of program transformation \nand is bloated due to low-level engineering considerations. (E.g., on every transformation our declara\u00adtive \nengine incrementally adjusts the results of the analysis 7 Our implementation can be found in http://doop.program-analysis.org/ \nand mainly in .les logic/transform.logic and logic/transform-delta.logic.  even though they will not \nbe needed.) We did not try to im\u00adprove the speed of detection and transformation, at the cost of complicating \nour implementation, because this speed is largely irrelevant. As discussed earlier, our analysis cost \nis one-o.: the program is transformed once and for all, and can be reused in its reduced form for any \nnumber of further analyses or queries. Additionally, the vast majority of the cost is not concerning \nthe program but the library. The above (sub-minute average) times include the set-based analysis and \ntransformation of the entire JDK 1.6 (not just its parts reachable by the current program under analysis). \nTherefore, even in a setting where one wants to re-analyze the program regularly (e.g., because it is \nunder current development) the library can be analyzed and transformed only once, with the resulting \nreduced library re-used for every program. 6. Experiments We evaluated the impact of set-based pre-analysis \non 7 rep\u00adresentative analyses from the Doop framework. The analy\u00adses span a wide range of precision and \nperformance, com\u00adprising a context-insensitive Andersen-style points-to anal\u00adysis (insens), and context-sensitive \nvariants both with and without a context-sensitive heap abstraction (a.k.a. heap cloning) for di.erent \nkinds of context-sensitivity: call-site\u00adsensitive [17, 18] (1call, 1call+H), object-sensitive [12, 13] \n(1obj, 1obj+H, 2obj+H), and type-sensitive [19] (2type+H). (Comparing the precision of these known algorithms \nis out\u00adside the scope of this work, but such measurements for our exact setting can be found in past \nliterature [10].) We used two di.erent intermediate representations (IRs) in our evaluation. The .rst \nis the default form of the Jimple intermediate language of the Soot framework [21, 22]. The second is \na Static Single Assignment (SSA) version of an otherwise similar intermediate language, also supported \nby the Soot framework. The reason for trying both intermediate languages was to see whether the impact \nof set-based pre\u00adanalysis would be signi.cantly greater on a representation that is pro.igate with local \nvariables (SSA) vs. a representa\u00adtion that was designed merely as a convenient intermediate language \nof a major compiler framework (Jimple). Our evaluation setting uses the LogicBlox Datalog en\u00adgine, v.3.9.0, \non a Xeon X5650 2.67GHz machine with only one thread running at a time and 24GB of RAM (i.e., ample for \nthe analyses studied). We analyze the DaCapo bench\u00admark programs under JDK 1.6.0 37. We use the same \nset\u00adtings as earlier published work [1, 19]: jython and hsqldb are analyzed with re.ection disabled and \nhsqldb has its en\u00adtry point set manually in a special harness. Our set-based pre-processing runs in an \naverage of 49sec (max: 96, min: 31) over all benchmarks and the standard library in the Jimple representation, \nwith an average of 47sec (max: 93, min: 30) for the SSA representation. As discussed in the previous \nsection, this time is incurred only once per program or library and is in.uenced heavily by low-level \nengineering considerations that we did not try to optimize. Tables 1-8 show the results of our experiments. \nMiss\u00ading entries correspond to analyses that did not terminate in 90mins. Running time. Tables 1 and \n2 (for the Jimple and SSA IR, respectively) present the running times of all analyses and compute the \nspeedup a.orded by set-based optimization. (All running time numbers given are medians of three runs.) \nSet-based pre-analysis has a signi.cant impact on the run\u00adning time of almost every analysis, with its \nhighest impact on call-site-sensitive analyses. There is virtually no program that does not consistently \nbene.t from set-based optimiza\u00adtion and we see overall speedups that are as high as 110%, with averages \naround 20%. Although some programs clearly bene.t more than others, the result is not particularly pro\u00adnounced: \nnote how the maximum and minimum speedup en\u00adtries (in bold) are distributed over several columns and \nare often not far from numbers for other benchmarks over the same analysis. The choice of intermediate \nrepresentation does not a.ect the e.ectiveness of our technique much, either. Although the SSA form introduces \nmore local variables, the di.er\u00adence in speedup is small and mostly within noise levels. This shows that \neven a human-designed intermediate representa\u00adtion (Jimple) o.ers enough opportunities for set-based \nop\u00adtimization. Recall that the elimination of variables and IR statements by set-based optimization is \nnot something that a regular intermediate language could replicate: the optimiza\u00adtion is valid only for \nthe purposes of points-to analysis, not for the purposes of program execution. Variables eliminated. \nTables 3 and 4 (for the Jimple and SSA IR, respectively) show the numbers of reachable local variables \n(as computed by the points-to analysis itself) both with and without set-based optimization. This is \na useful metric for seeing how much of the program (together with reachable code in the JDK libraries) \nis distilled away when applying set-based optimization. Importantly, this measure is static: it counts \neliminated variables in the program text. As we can see, this does not correlate well with the speedup \nnumbers from the earlier tables. Indeed, the reduction per\u00adcentage for local variables is remarkably \nsteady over all benchmarks, at roughly 30%. The number also does not vary much over analyses, but this \nis quite expected: the only im\u00adpact the analysis has on the number of variables eliminated is because \nof changes in reachable code. (More precise anal\u00adyses, e.g., 2obj+H, have fewer reachable variables than \nim\u00adprecise ones, e.g., insens. Still, this variance hardly a.ects the average reduction in reachable \nvariables due to set-based pre-analysis.) On these tables we can see a little more clearly the e.ect \nof IR choice. The SSA form has consistently higher variable counts than the Jimple form. The reduction \npercentages are also consistently (but very slightly) higher, but not nearly as Table 1. Execution time \n(in seconds) for a variety of analyses on various benchmarks using the Jimple intermediate language. \nMaximum and minimum speedups per row are shown in bold.  antlr bloat chart eclipse hsqldb jython luindex \nlusearch pmd xalan AVG insens original set-based speedup 69.30 63.09 9.84% 59.59 52.02 14.55% 123.44 \n106.01 16.44% 47.74 43.55 9.62% 57.05 50.93 12.01% 55.96 50.12 11.65% 40.97 37.23 10.04% 42.07 38.14 \n10.30% 59.57 55.11 8.09% 64.85 57.46 12.86% 11.54% 1obj original set-based speedup 166.26 150.76 10.28% \n373.93 316.90 17.99% 1240.59 1192.19 4.05% 117.66 107.30 9.65% 218.33 178.24 22.49% 119.96 109.18 9.87% \n76.61 70.35 8.89% 89.18 83.05 7.38% 135.84 124.11 9.45% 189.40 171.10 10.69% 11.07% 1obj+H original set-based \nspeedup 810.71 638.28 27.01% 1593.31 1226.35 29.92% --- 555.22 479.37 15.82% 4335.92 3249.96 33.41% 832.50 \n759.40 9.62% 240.08 206.51 16.25% 262.64 228.29 15.04% 332.01 284.70 16.61% 803.66 672.34 19.53% 20.35% \n2obj+H original set-based speedup 217.53 183.43 18.59% 5060.39 3670.67 37.86% 896.04 739.40 21.18% 532.55 \n435.69 22.23% --- --- 131.01 109.46 19.68% 183.31 159.54 14.89% 167.71 140.53 19.34% 4521.01 4145.91 \n9.04% 20.35% 2type+H original set-based speedup 108.13 94.45 14.48% 142.85 118.09 20.96% 211.91 179.94 \n17.76% 152.45 125.17 21.79% 194.73 155.92 24.89% 731.41 621.14 17.75% 75.22 66.16 13.69% 76.29 66.19 \n15.25% 114.48 99.20 15.40% 168.16 142.26 18.20% 18.01% 1call original set-based speedup 110.09 86.52 \n27.24% 186.30 120.17 55.03% 288.43 231.98 24.33% 81.42 65.71 23.90% 90.49 70.25 28.81% 88.29 74.33 18.78% \n59.36 50.65 17.19% 63.62 54.27 17.22% 89.97 75.29 19.49% 108.70 89.07 22.03% 25.40% 1call+H original \nset-based speedup 366.16 243.94 50.10% 1351.58 643.61 109.99% 957.15 766.31 24.90% 478.31 324.06 47.59% \n332.63 197.56 68.36% 401.10 271.76 47.59% 171.59 127.75 34.31% 186.63 135.49 37.74% 245.95 181.33 35.63% \n470.74 391.31 20.29% 47.65% antlr bloat chart eclipse hsqldb jython luindex lusearch pmd xalan AVG insens \noriginal set-based speedup 69.23 63.84 8.44% 59.49 52.53 13.24% 124.16 106.31 16.79% 47.49 43.30 9.67% \n56.70 50.44 12.41% 55.33 50.77 8.98% 41.16 38.00 8.31% 41.95 38.66 8.51% 60.12 55.04 9.22% 65.90 58.65 \n12.36% 10.79% 1obj original set-based speedup 166.39 149.45 11.33% 358.83 307.02 16.87% 1256.02 1123.30 \n11.81% 118.34 105.04 12.66% 223.60 182.26 22.68% 119.45 108.37 10.22% 76.27 69.91 9.09% 89.28 81.92 8.98% \n134.19 123.84 8.35% 190.18 171.61 10.82% 12.28% 1obj+H original set-based speedup 815.78 656.89 24.18% \n1460.25 1194.86 22.21% --- 625.52 515.53 21.33% 4349.26 3122.87 39.27% 843.57 748.94 12.63% 245.65 209.86 \n17.05% 261.08 225.76 15.64% 340.82 290.02 17.51% 793.59 694.81 14.21% 20.44% 2obj+H original set-based \nspeedup 223.09 185.10 20.52% 4621.55 3479.07 32.83% 920.44 685.19 34.33% 535.00 436.68 22.51% --- --- \n131.41 109.90 19.57% 139.95 114.85 21.85% 168.02 142.35 18.03% 4621.02 4256.24 8.57% 22.27% 2type+H original \nset-based speedup 108.17 95.33 13.46% 138.99 116.01 19.80% 226.27 182.78 23.79% 151.12 125.31 20.59% \n197.54 156.04 26.59% 726.93 605.48 20.05% 76.37 66.64 14.60% 76.76 66.78 14.94% 115.65 100.29 15.31% \n172.52 144.48 19.40% 18.85% 1call original set-based speedup 108.97 85.36 27.65% 186.63 120.62 54.72% \n281.24 224.66 25.18% 81.88 65.48 25.04% 90.54 69.64 30.01% 88.24 74.24 18.85% 59.68 50.74 17.61% 64.30 \n53.75 19.62% 90.39 75.74 19.34% 110.65 88.18 25.48% 26.35% 1call+H original set-based speedup 360.12 \n240.62 49.66% 1419.06 680.60 108.50% 896.61 694.66 29.07% 442.65 317.40 39.46% 308.10 188.66 63.30% 377.58 \n264.51 42.74% 171.58 125.71 36.48% 183.68 133.46 37.62% 244.42 180.70 35.26% 461.27 344.56 33.87% 47.59% \n Table 2. Execution time (in seconds) for a variety of analyses on various benchmarks using an SSA version \nof the intermediate language. Maximum and minimum speedups per row are shown in bold. much as the di.erence \nin absolute variable counts between struction types are renamed to be mostly self-explanatory.) Jimple \nand SSA. This shows more vividly that the speedup As expected, local assignments (Move instructions) \nare dras\u00adof set-based optimization is not due to eliminating variables tically reduced: close to 90% \nof them for the SSA IR and that would be redundant in the IR anyway. about 97% for the Jimple IR are \neliminated! Nevertheless, this does not impact performance signi.cantly in our setting: Instructions \neliminated. Tables 5 and 6 (for the Jimple and Move instructions are handled very e.ciently in the DoopSSA \nIR, respectively) show di.erent instruction types and implementation. Other kinds of instructions that \nare reduced the impact of set-based pre-processing on them. (The in\u00adTable 3. Number of reachable (local) \nvariables for the Jimple intermediate language representation. Maximum and minimum reduction percentages \nper row are shown in bold.  insensoriginal set-based reduction 1objoriginal set-based reduction 1obj+Horiginal \nset-based reduction 2obj+Horiginal set-based reduction 2type+Horiginal set-based reduction 1calloriginal \nset-based reduction 1call+Horiginal set-based reduction antlr 87,906 63,050 28.28% 86,409 62,019 28.23% \n86,154 61,826 28.24% 84,605 60,699 28.26% 84,805 60,853 28.24% 86,684 62,204 28.24% 86,684 62,204 28.24% \nbloat 91,859 64,377 29.92% 90,302 63,318 29.88% 90,012 63,106 29.89% 88,387 61,941 29.92% 88,725 62,172 \n29.93% 90,567 63,497 29.89% 90,567 63,497 29.89% chart 138,286 97,760 29.31% 135,458 95,893 29.21% ---108,722 \n78,004 28.25% 114,663 81,913 28.56% 135,361 95,852 29.88% 135,230 95,748 29.20% eclipse 83,269 59,149 \n28.97% 81,228 57,751 28.90% 80,613 57,331 28.88% 78,545 55,833 28.92% 78,872 56,073 28.91% 81,372 57,846 \n28.91% 81,372 57,846 28.91% hsqldb 88,809 61,214 31.07% 87,891 60,580 31.07% 87,076 59,970 31.13% ---85,672 \n58,972 31.17% 88,181 60,779 31.07% 88,181 60,779 31.07% jython 76,164 52,982 30.44% 75,383 52,438 30.44% \n74,738 51,949 30.49% ---72,354 50,140 30.70% 75,637 52,608 30.45% 75,637 52,608 30.45% luindex 66,649 \n47,173 29.22% 65,032 46,047 29.19% 64,777 45,854 29.21% 63,203 44,706 29.27% 63,403 44,860 29.25% 65,427 \n46,327 29.19% 65,427 46,327 29.19% lusearch 71,330 50,063 29.81% 69,716 48,937 29.81% 69,306 48,631 29.83% \n67,689 47,450 29.90% 67,889 47,604 29.88% 70,111 49,217 29.80% 70,111 49,217 29.80% pmd 77,653 54,228 \n30.17% 76,101 53,166 30.14% 75,666 52,816 30.20% 73,886 51,509 30.29% 74,128 51,701 30.25% 76,396 53,363 \n30.15% 76,396 53,363 30.15% xalan AVG 87,983 61,308 30.32% 29.75% 86,403 60,224 30.30% 29.71% 86,042 \n59,965 30.31% 29.80% 83,879 58,332 30.46% 29.41% 84,175 58,553 30.44% 29.73% 86,678 60,410 30.31% 29.79% \n86,678 60,410 30.31% 29.72% insensoriginal set-based reduction 1objoriginal set-based reduction 1obj+Horiginal \nset-based reduction 2obj+Horiginal set-based reduction 2type+Horiginal set-based reduction 1calloriginal \nset-based reduction 1call+Horiginal set-based reduction antlr 91,642 65,407 28.63% 90,029 64,315 28.56% \n89,774 64,122 28.57% 88,201 62,978 28.60% 88,401 63,132 28.58% 90,264 64,470 28.58% 90,264 64,470 28.58% \nbloat 95,196 66,558 30.08% 93,523 65,438 30.03% 93,233 65,226 30.04% 91,588 64,048 30.07% 91,928 64,281 \n30.07% 93,748 65,587 30.04% 93,748 65,587 30.04% chart 144,580 101,850 29.55% 141,628 99,889 29.47% ---113,387 \n80,997 28.57% 119,598 85,079 28.86% 141,415 99,791 29.43% 141,251 99,665 29.44% eclipse hsqldb jython \nluindex 86,577 92,942 79,450 69,513 61,196 63,698 54,995 48,973 29.32% 31.46% 30.78% 29.55% 84,780 92,035 \n78,710 67,764 59,982 63,072 54,484 47,771 29.25% 31.47% 30.78% 29.50% 84,145 91,210 78,048 67,509 59,545 \n62,454 53,983 47,578 29.24% 31.53% 30.83% 29.52% 82,038 --65,911 58,020 --46,413 29.28% --29.58% 82,392 \n89,782 75,631 66,111 58,283 61,438 52,149 46,567 29.26% 31.57% 31.05% 29.56% 84,851 92,283 78,921 68,135 \n60,022 63,240 54,621 48,036 29.26% 31.47% 30.79% 29.50% 84,851 92,283 78,921 68,135 60,022 63,240 54,621 \n48,036 29.26% 31.47% 30.79% 29.50% lusearch 74,513 52,060 30.13% 72,771 50,859 30.11% 72,361 50,553 30.14% \n70,718 49,353 30.21% 70,918 49,507 30.19% 73,142 51,124 30.10% 73,142 51,124 30.10% pmd 80,615 56,114 \n30.39% 78,947 54,991 30.34% 78,503 54,633 30.41% 76,680 53,291 30.50% 76,927 53,488 30.47% 79,202 55,158 \n30.36% 79,202 55,158 30.36% xalan 92,463 64,203 30.56% 90,775 63,064 30.53% 90,408 62,800 30.54% 88,196 \n61,135 30.68% 88,496 61,358 30.67% 91,012 63,221 30.54% 91,012 63,221 30.53% AVG 30.05% 30.04% 30.09% \n29.69% 30.03% 30.07% 30.07% Table 4. Number of reachable (local) variables for an SSA version of the \nintermediate language. Maximum and minimum reduction percentages per row are shown in bold. in signi.cant \nnumbers include both Load instructions (Load-Context-sensitive facts eliminated. Tables 7 and 8 (for \nthe Array, LoadField, LoadSField the latter for static .eld Jimple and SSA IR, respectively) show the \ne.ect of set\u00adloads) and Call instructions (VirtMethCall, StatMethCall). based optimization on perhaps \nthe most important internal Load instructions are reduced by more than 40% and the complexity metric \nof a points-to analysis: the cumulative very common virtual method calls by roughly 10%. These size of \ncontext-sensitive points-to sets. This is a metric that two numbers show quite well the source of time \nsavings correlates very well with the memory requirements of the from our approach, at least in the Doop \nsetting. analysis and has the advantage of being impervious to plat\u00adTable 5. Number of instructions (per \ninstruction type) for the Jimple intermediate language representation. Maximum and minimum reduction \npercentages per row are shown in bold. antlr bloat chart eclipse hsqldb jython luindex lusearch pmd \nxalan Move original removed reduction 97342 94300 96.87% 50499 49012 97.05% 112656 109104 96.84% 49593 \n48099 96.98% 105074 101775 96.86% 116649 113265 97.09% 49373 47930 97.07% 49373 47930 97.07% 111388 107967 \n96.92% 105636 102208 96.75% Return original removed reduction 76662 5290 6.90% 42708 3103 7.26% 79794 \n5655 7.08% 39199 2194 5.59% 82965 5276 6.35% 86623 5087 5.87% 36849 1925 5.22% 36849 1925 5.22% 82812 \n5747 6.93% 75835 4850 6.39% Cast original removed reduction 11908 17 0.14% 6159 10 0.16% 13849 23 0.16% \n5381 9 0.16% 12653 17 0.13% 14108 17 0.12% 5060 12 0.23% 5060 12 0.23% 13715 23 0.16% 12545 27 0.21% \nLoadArray original removed reduction 4249 1775 41.77% 2861 1379 48.19% 4882 2067 42.33% 3009 1309 43.50% \n5372 2221 41.34% 5104 2144 42.00% 2383 978 41.04% 2383 978 41.04% 4643 1979 42.62% 4575 1887 41.24% LoadField \noriginal removed reduction 56357 26190 46.47% 26562 11297 42.53% 61493 27184 44.20% 25780 10767 41.76% \n61839 28558 46.18% 62033 28027 45.18% 26202 11058 42.20% 26202 11058 42.20% 62342 29630 47.52% 57407 \n26088 45.44% LoadSField original removed reduction 17509 6239 35.63% 8758 3694 42.17% 21450 7411 34.55% \n8159 3058 37.48% 19064 7037 36.91% 20810 6988 33.58% 8399 3388 40.33% 8399 3388 40.33% 20002 7765 38.82% \n18243 6581 36.07% StoreArray original removed reduction 11648 217 1.86% 3714 90 2.42% 13778 226 1.64% \n3528 104 2.94% 13470 288 2.13% 12818 261 2.03% 3568 95 2.66% 3568 95 2.66% 13180 258 1.95% 13111 230 \n1.75% StoreField original removed reduction 13581 90 0.66% 7126 44 0.61% 15737 98 0.62% 7048 41 0.58% \n14679 106 0.72% 15191 109 0.71% 7260 49 0.67% 7260 49 0.67% 15587 155 0.99% 14364 96 0.66% StoreSField \noriginal removed reduction 4674 3 0.06% 1825 1 0.05% 5574 3 0.05% 1943 1 0.05% 4927 4 0.08% 5727 3 0.05% \n2025 1 0.04% 2025 1 0.04% 5091 3 0.05% 4820 3 0.06% VirtMethCall original removed reduction 136362 14447 \n10.59% 71755 7670 10.68% 147917 15394 10.40% 64302 6005 9.33% 144532 12406 8.58% 144718 12380 8.55% 61360 \n5725 9.33% 61360 5725 9.33% 144805 13149 9.08% 134451 11858 8.81% StatMethCall original removed reduction \n26680 3645 13.66% 14421 1818 12.60% 30266 4341 14.34% 15188 1705 11.22% 30237 4070 13.46% 32616 4294 \n13.16% 14738 1712 11.61% 14738 1712 11.61% 29736 4638 15.59% 28070 3756 13.38% SpecMethCall original \nremoved reduction 52727 1262 2.39% 29750 795 2.67% 59778 1478 2.47% 29039 807 2.77% 56784 1424 2.50% \n60228 1764 2.92% 29411 1164 3.95% 29411 1164 3.95% 60426 2698 4.46% 54618 1312 2.40% Total original removed \nreduction 599547 153475 25.59% 313278 78913 25.18% 664975 172984 26.01% 298922 74099 24.78% 649383 163182 \n25.12% 677861 174339 25.71% 292799 74037 25.28% 292799 74037 25.28% 661095 174012 26.32% 615111 158896 \n25.83% form and implementation .uctuations: an optimization that speeds up execution could do so by \ntaking advantage of the speci.cs of the environment e.g., peculiarities of our Datalog execution engine. \nImprovement in context-sensitive points-to set sizes, however, is a change that transfers well to completely \ndi.erent implementation settings.8 As seen on the tables, set-based optimization has signif\u00adicant impact \non the sizes of context-sensitive points-to sets. More than 30% of points-to facts on average never need \nto be inferred in the optimized version of the input. This dif\u00adference a.ects the complexity of the analysis \nitself but not its outcome: the .nal, context-insensitive points-to sets for 8 It is telling that analysis \nimplementations that use binary decision dia\u00adgrams (BDDs) try hard to minimize this metric in order to \nachieve peak performance [2]. the same variable or object .eld will be identical, since our transformation \nis semantics-preserving relative to the points\u00adto analysis. Post-processing. Set-based pre-analysis leaves \nthe output of points-to analysis in a condensed form, as far as certain further analyses are concerned. \nThis is the case for client analyses that have knowledge of program internals, such as temporary variables \nor call-sites, which may have been op\u00adtimized away. As mentioned in Section 4.3, this information can \nbe retrieved via post-processing. Such post-processing is typically speci.c to the client analysis: the \nanalysis will post-process the information it cares about to add back miss\u00ading elements. To enable post-processing, \nour implementa\u00adtion o.ers maps from eliminated variables and call-sites to Table 6. Number of instructions \n(per instruction type) for an SSA version of intermediate language representation. Maximum and minimum \nreduction percentages per row are shown in bold.  antlr bloat chart eclipse hsqldb jython luindex lusearch \npmd xalan Move original removed reduction 117905 104245 88.41% 59909 53586 89.44% 135342 120005 88.66% \n59797 53170 88.91% 128252 112849 87.99% 141059 124026 87.92% 59108 52772 89.28% 59108 52772 89.28% 134639 \n118791 88.22% 128690 113154 87.92% Return original removed reduction 76662 5415 7.06% 42708 3103 7.26% \n79794 5768 7.22% 39199 2209 5.63% 82965 5414 6.52% 86623 5216 6.02% 36849 1939 5.26% 36849 1939 5.26% \n82812 5875 7.09% 75835 4977 6.56% LoadArray original removed reduction 4261 1813 42.54% 2867 1410 49.18% \n4894 2108 43.07% 3017 1329 44.05% 5384 2283 42.40% 5125 2193 42.79% 2389 997 41.73% 2389 997 41.73% 4655 \n2018 43.35% 4587 1932 42.11% LoadField original removed reduction 56377 26415 46.85% 26575 11403 42.90% \n61513 27403 44.54% 25794 10879 42.17% 61870 28809 46.56% 62057 28275 45.56% 26215 11178 42.63% 26215 \n11178 42.63% 62412 29938 47.96% 57428 26312 45.81% LoadSField original removed reduction 17526 6411 36.57% \n8760 3770 43.03% 21480 7811 36.36% 8161 3144 38.52% 19081 7390 38.72% 20832 7991 38.35% 8401 3504 41.70% \n8401 3504 41.70% 20022 8286 41.38% 18262 6909 37.83% StoreArray original removed reduction 11650 212 \n1.81% 3715 89 2.39% 13781 221 1.60% 3528 103 2.91% 13472 283 2.10% 12824 257 2.00% 3568 94 2.63% 3568 \n94 2.63% 13183 252 1.91% 13113 224 1.70% StoreField original removed reduction 13588 94 0.69% 7130 47 \n0.65% 15744 102 0.64% 7052 43 0.60% 14688 110 0.74% 15198 113 0.74% 7266 54 0.74% 7266 54 0.74% 15599 \n159 1.01% 14371 100 0.69% StoreSField original removed reduction 4674 3 0.06% 1825 1 0.05% 5574 3 0.05% \n1945 2 0.10% 4927 4 0.08% 5727 3 0.05% 2025 1 0.04% 2025 1 0.04% 5095 3 0.05% 4822 3 0.06% VirtMethCall \noriginal removed reduction 136362 14400 10.56% 71755 7584 10.56% 147917 15289 10.33% 64302 5937 9.23% \n144532 12302 8.51% 144718 12313 8.50% 61360 5656 9.21% 61360 5656 9.21% 144805 13094 9.04% 134451 11786 \n8.76% SpecMethCall original removed reduction 52727 1269 2.40% 29750 795 2.67% 59778 1483 2.48% 29039 \n807 2.77% 56784 1437 2.53% 60228 1772 2.94% 29411 1169 3.97% 29411 1169 3.97% 60426 2694 4.45% 54618 \n1317 2.41% StatMethCall original removed reduction 26680 3657 13.70% 14421 1806 12.52% 30266 4359 14.40% \n15188 1693 11.14% 30237 4098 13.55% 32616 4311 13.21% 14738 1701 11.54% 14738 1701 11.54% 29736 4649 \n15.63% 28070 3773 13.44% Total original removed reduction 620181 163934 26.43% 322721 83594 25.90% 687747 \n184552 26.83% 309164 79316 25.65% 672651 174979 26.01% 702366 186470 26.54% 302566 79065 26.13% 302566 \n79065 26.13% 684479 185759 27.13% 638250 170487 26.71% equivalent ones. For instance, it is easy to \npost-process the tables that depict our .nal points-to information for every program variable, by augmenting \nthe existing logic with an extra case: InsensVarPointsTo(var, heap) <\u00ad VarPointsTo(var2, _, heap, _), \nDupCopies(var, var2). The DupCopies table, above, stores the fact that var is replaced by the equivalent \nvariable var2. The rule causes the .nal output of the analysis, table InsensVarPointsTo, to also integrate \nfacts for eliminated variables, thus replicating exactly the analysis results without set-based pre-analysis. \nSuch post-processing incurs virtually zero cost. For example, for a 2obj+H analysis, the above post\u00adprocessing \nadds roughly 1sec to the query reporting the InsensVarPointsTo results.9 The reason that post-processing \nis virtually cost-free is dual. First, post-processing only adjusts information in the .nal output table, \nand not in all other tables involved in in\u00adtermediate computations. Second, post-processing can avoid \nchanging the context-sensitive facts computed by an analy\u00adsis and instead only a.ect the .nal context-insensitive \nfacts, as in the above example query. Summary. In all, we see that set-based reasoning has a signi.cant \nimpact on points-to analysis, and that this applies 9 Although in theory the output of a points-to analysis \nis the points-to information for local variables, most of the time points-to analysis is not performed \nwith the purpose of producing results for all local variables. Instead, points-to analysis may be required \nin order to compute reachable methods, points-to information for heap objects, object type connectivity \ngraphs, etc. Thus, post-processing is relatively rarely required in practice.  insensoriginal set-based \nreduction 1objoriginal set-based reduction 1obj+Horiginal set-based reduction 2obj+Horiginal set-based \nreduction 2type+Horiginal set-based reduction 1calloriginal set-based reduction 1call+Horiginal set-based \nreduction antlr 10,988 10,038 8.65% 14,301 10,003 30.05% 82,899 56,858 31.41% 19,917 13,642 31.51% 5,354 \n3,846 28.17% 16,093 10,337 35.77% 54,844 29,697 45.85% bloat 9,819 8,412 14.33% 21,927 14,462 34.04% \n81,797 57,255 30% 153,469 99,222 35.35% 11,446 7,510 34.39% 32,946 18,111 45.03% 150,516 73,972 50.84% \nchart 15,340 13,503 11.98% 62,502 48,332 22.67% ---67,608 48,102 28.85% 13,319 9,328 29.96% 49,649 34,245 \n31.03% 120,865 78,952 34.68% eclipse 6,275 5,628 10.31% 9,353 6,718 28.17% 58,271 41,433 28.90% 44,638 \n30,823 30.95% 13,552 9,290 31.45% 12,264 8,251 32.72% 61,524 38,998 36.61% hsqldb 5,398 4,739 12.21% \n13,955 9,249 33.72% 193,882 126,778 34.61% ---13,660 8,586 37.14% 9,601 6,238 35.03% 39,783 24,348 38.80% \njython 4,706 4,139 12.05% 8,671 6,177 28.76% 101,621 69,461 31.65% ---52,015 34,514 33.65% 10,430 6,988 \n33% 50,633 31,251 38.28% luindex 4,792 4,382 8.56% 5,435 3,978 26.81% 25,707 18,359 28.58% 11,143 7,487 \n32.81% 4,108 2,783 32.25% 7,839 5,380 31.37% 26,107 16,937 35.12% lusearch 5,073 4,614 9.05% 6,218 4,560 \n26.66% 26,885 19,194 28.61% 13,182 9,176 30.39% 4,204 2,858 32.02% 8,763 6,002 31.51% 28,525 18,200 36.20% \npmd 5,544 5,024 9.38% 7,987 5,807 27.29% 30,558 21,754 28.81% 13,202 9,001 31.82% 4,550 3,075 32.42% \n11,369 7,812 31.29% 35,945 23,372 34.98% xalan AVG 6,553 5,832 11% 10.75% 15,449 11,573 25.09% 28.33% \n97,004 72,472 25.29% 29.76% 166,641 120,744 27.54% 31.15% 10,205 6,912 32.27% 32.37% 14,499 10,129 30.14% \n33.69% 59,872 38,553 35.61% 38.70% Table 7. Number of context-sensitive VarPointsTo entries (measured \nin thousands) for the Jimple representation. Maximum and minimum reduction percentages per row are shown \nin bold. insensoriginal set-based reduction 1objoriginal set-based reduction 1obj+Horiginal set-based \nreduction 2obj+Horiginal set-based reduction 2type+Horiginal set-based reduction 1calloriginal set-based \nreduction 1call+Horiginal set-based reduction antlr 11,032 10,028 9.1% 15,319 10,583 30.92% 89,220 60,233 \n32.49% 21,435 14,521 32.26% 5,607 3,998 28.70% 16,256 10,294 36.68% 55,508 29,599 46.68% bloat 9,750 \n8,323 14.64% 22,299 14,650 34.30% 84,823 58,887 30.58% 149,871 98,137 34.52% 11,170 7,332 34.36% 33,020 \n18,007 45.47% 150,685 75,764 49.72% chart 15,209 13,328 12.37% 63,017 48,453 23.11% ---73,953 51,984 \n29.71% 13,942 9,676 30.60% 48,119 33,174 31.06% 118,214 77,077 34.80% eclipse hsqldb jython luindex 6,226 \n5,402 4,674 4,756 5,560 4,707 4,088 4,335 10.70% 12.87% 12.54% 8.85% 9,649 15,328 8,875 5,630 6,848 9,880 \n6,266 4,082 29.03% 35.54% 29.40% 27.50% 61,463 219,442 103,896 27,138 42,807 138,527 70,326 19,040 30.35% \n36.87% 32.31% 29.84% 48,425 --11,702 32,919 --7,829 32.02% --33.10% 14,556 14,737 54,314 4,318 9,858 \n9,139 35,787 2,913 32.28% 37.99% 34.11% 32.54% 12,373 9,743 10,427 7,840 8,223 6,187 6,913 5,317 33.54% \n36.50% 33.70% 32.18% 62,299 40,667 50,873 26,151 38,886 24,213 30,952 16,742 37.58% 40.46% 39.16% 35.98% \nlusearch 5,033 4,564 9.32% 6,367 4,624 27.38% 28,032 19,647 29.91% 12,480 8,380 32.85% 4,410 2,978 32.47% \n8,724 5,908 32.28% 28,526 17,975 36.99% pmd 5,493 4,962 9.67% 8,202 5,908 27.97% 32,259 22,527 30.17% \n13,907 9,472 31.89% 4,773 3,213 32.68% 11,364 7,730 31.98% 36,075 23,185 35.73% xalan 6,523 5,771 11.53% \n16,155 11,951 26.02% 106,986 78,634 26.50% 182,049 130,912 28.09% 10,826 7,272 32.83% 14,642 10,097 31.04% \n61,156 38,846 36.48% AVG 11.16% 29.12% 31% 31.80% 32.86% 34.44% 39.36% Table 8. Number of context-sensitive \nVarPointsTo entries (measured in thousands) for the SSA representation. Maximum and minimum reduction \npercentages per row are shown in bold. transparently to a very wide variety of analyses, without any \n7. Conclusions need to change the analysis at all. For practical usability In 2000, Rountev and Chandra \nwrote, regarding their o.-line it is also important to recall that this impact is modular: optimization \ntechnique [16]: a library (or other invariant code) can be optimized once While we have concentrated \non reducing the cost of and the results reused in conjunction with any other client Andersen s analysis, \nwe conjecture that such precom\u00ad program, and for di.erent points-to analyses. putation can be helpful \nin other points-to analyses as well.  Although subsequent work advanced the area of o.-line optimization \nof points-to analysis, it has not achieved this conjectured generality and independence from the analysis \nalgorithm. In the work we presented here, we ful.ll this promise by expressing the optimization as a \npre-processing step that is largely orthogonal to the subsequent points-to analysis. We applied our approach \nto 7 di.erent points\u00adto analysis algorithms for demonstration purposes, and it transparently applies \nto any other points-to analysis in the Doop framework. There is virtually no other comparable op\u00adtimization \nmechanism of such wide applicability to di.erent points-to analyses in the literature algorithmic improve\u00adments \nin this area are usually analysis-speci.c. Furthermore, the intraprocedural nature of our approach means \nthat it can be applied once-and-for-all to libraries and have the results be reused, and that it works \nwell with points\u00adto analysis algorithms employing on-the-.y call-graph con\u00adstruction (in languages with \ndynamic dispatch). Finally, our approach is also more general than past techniques for o.\u00adline optimization, \nbecause it allows removing individual re\u00addundant program statements instead of just collapsing vari\u00adables. \nWe believe that the generality and orthogonality of our set-based pre-analysis will render it a valuable \nweapon in the arsenal of the static analysis programmer for years to come. Acknowledgments We gratefully \nacknowledge funding by the European Union under a Marie Curie International Reintegration Grant and a \nEuropean Research Council Starting/Consolidator grant; and by the Greek Secretariat for Research and \nTechnology under an Excellence (Aristeia) award. References [1] K. Ali and O. Lhot\u00b4 ak. Application-only \ncall graph construc\u00adtion. In J. Noble, editor, European Conf. on Object-Oriented Programming (ECOOP), \nvolume 7313 of Lecture Notes in Computer Science, pages 688 712. Springer Berlin Heidel\u00adberg, 2012. [2] \nM. Berndl, O. Lhot\u00b4ak, F. Qian, L. J. Hendren, and N. Umanee. Points-to analysis using BDDs. In Conf. \non Programming Language Design and Implementation (PLDI), pages 103 114. ACM, 2003. [3] M. Bravenboer \nand Y. Smaragdakis. Exception analysis and points-to analysis: Better together. In L. Dillon, editor, \nInt. Symp. on Software testing and analysis (ISSTA), New York, NY, USA, July 2009. [4] M. Bravenboer \nand Y. Smaragdakis. Strictly declarative speci.cation of sophisticated points-to analyses. In Conf. on \nObject Oriented Programming, Systems, Languages, and Applications (OOPSLA), New York, NY, USA, 2009. \nACM. [5] M. F\u00a8 ahndrich, J. S. Foster, Z. Su, and A. Aiken. Partial online cycle elimination in inclusion \nconstraint graphs. In Conf. on Programming Language Design and Implementation (PLDI), pages 85 96, New \nYork, NY, USA, 1998. ACM. [6] S. Guarnieri and B. Livshits. GateKeeper: mostly static en\u00adforcement of \nsecurity and reliability policies for Javascript code. In Proceedings of the 18th USENIX Security Sympo\u00adsium, \nSSYM 09, pages 151 168, Berkeley, CA, USA, 2009. USENIX Association. [7] B. Hardekopf and C. Lin. The \nant and the grasshopper: fast and accurate pointer analysis for millions of lines of code. In Conf. on \nProgramming Language Design and Implementation (PLDI), pages 290 299, New York, NY, USA, 2007. ACM. [8] \nB. Hardekopf and C. Lin. Exploiting pointer and location equivalence to optimize pointer analysis. In \nIn International Static Analysis Symposium (SAS), pages 265 280, 2007. [9] G. Kastrinis and Y. Smaragdakis. \nE.cient and e.ective handling of exceptions in Java points-to analysis. In Int. Conf. on Compiler Construction \n(CC), Mar. 2013. [10] G. Kastrinis and Y. Smaragdakis. Hybrid context-sensitivity for points-to analysis. \nIn Conf. on Programming Language Design and Implementation (PLDI). ACM, June 2013. [11] M. S. Lam, J. \nWhaley, V. B. Livshits, M. C. Martin, D. Avots, M. Carbin, and C. Unkel. Context-sensitive program analysis \nas database queries. In PODS 05: Proc. of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles \nof database systems, pages 1 12, New York, NY, USA, 2005. ACM. [12] A. Milanova, A. Rountev, and B. G. \nRyder. Parameterized object sensitivity for points-to and side-e.ect analyses for Java. In Int. Symp. \non Software testing and analysis (ISSTA), pages 1 11, New York, NY, USA, 2002. ACM. [13] A. Milanova, \nA. Rountev, and B. G. Ryder. Parameterized object sensitivity for points-to analysis for Java. ACM Trans. \nSoftw. Eng. Methodol., 14(1):1 41, 2005. [14] R. Nasre. Exploiting the structure of the constraint graph \nfor e.cient points-to analysis. In Int. Symp. on Memory Management (ISMM), pages 121 132, New York, NY, \nUSA, 2012. ACM. [15] T. Reps. Demand interprocedural program analysis using logic databases. In R. Ramakrishnan, \neditor, Applications of Logic Databases, pages 163 196. Kluwer Academic Publishers, 1994. [16] A. Rountev \nand S. Chandra. O.-line variable substitution for scaling points-to analysis. In Conf. on Programming \nLanguage Design and Implementation (PLDI), pages 47 56, New York, NY, USA, 2000. ACM. [17] M. Sharir \nand A. Pnueli. Two approaches to interprocedural data .ow analysis. In S. S. Muchnick and N. D. Jones, \neditors, Program Flow Analysis, pages 189 233, Englewood Cli.s, NJ, 1981. Prentice-Hall, Inc. [18] O. \nShivers. Control-Flow Analysis of Higher-Order Lan\u00adguages. PhD thesis, Carnegie Mellon University, May \n1991. [19] Y. Smaragdakis, M. Bravenboer, and O. Lhot\u00b4ak. Pick your contexts well: Understanding object-sensitivity \n(the making of a precise and scalable pointer analysis). In Symp. on Principles of Programming Languages \n(POPL), pages 17 30. ACM Press, Jan. 2011. [20] Z. Su, M. F \u00a8ahndrich, and A. Aiken. Projection merging: \nreducing redundancies in inclusion constraint graphs. In  Symp. on Principles of Programming Languages \n(POPL), pages 81 95, New York, NY, USA, 2000. ACM. [21] R. Vall\u00b4ee-Rai, E. Gagnon, L. J. Hendren, P. \nLam, P. Pom\u00adinville, and V. Sundaresan. Optimizing Java bytecode using the Soot framework: Is it feasible? \nIn Int. Conf. on Compiler Construction (CC), pages 18 34, 2000. [22] R. Vall\u00b4ee-Rai, L. Hendren, V. Sundaresan, \nP. Lam, E. Gagnon, and P. Co. Soot -a Java optimization framework. In Proceedings of CASCON 1999, pages \n125 135, 1999. [23] J. Whaley, D. Avots, M. Carbin, and M. S. Lam. Using Datalog with binary decision \ndiagrams for program analysis. In K. Yi, editor, APLAS, volume 3780 of Lecture Notes in Computer Science, \npages 97 118. Springer, 2005. [24] J. Whaley and M. S. Lam. Cloning-based context-sensitive pointer alias \nanalysis using binary decision diagrams. In Conf. on Programming Language Design and Implementation (PLDI), \npages 131 144, New York, NY, USA, 2004. ACM.  \n\t\t\t", "proc_id": "2509136", "abstract": "<p>We present set-based pre-analysis: a virtually universal optimization technique for flow-insensitive points-to analysis. Points-to analysis computes a static abstraction of how object values flow through a program's variables. Set-based pre-analysis relies on the observation that much of this reasoning can take place at the set level rather than the value level. Computing constraints at the set level results in significant optimization opportunities: we can rewrite the input program into a simplified form with the same essential points-to properties. This rewrite results in removing both local variables and instructions, thus simplifying the subsequent value-based points-to computation. Effectively, set-based pre-analysis puts the program in a normal form optimized for points-to analysis. </p> <p>Compared to other techniques for off-line optimization of points-to analyses in the literature, the new elements of our approach are the ability to eliminate statements, and not just variables, as well as its modularity: set-based pre-analysis can be performed on the input just once, e.g., allowing the pre-optimization of libraries that are subsequently reused many times and for different analyses. In experiments with Java programs, set-based pre-analysis eliminates 30% of the program's local variables and 30% or more of computed context-sensitive points-to facts, over a wide set of benchmarks and analyses, resulting in a ~20% average speedup (max: 110%, median: 18%).</p>", "authors": [{"name": "Yannis Smaragdakis", "author_profile_id": "81100614708", "affiliation": "University of Athens, Athens, Greece", "person_id": "P4290350", "email_address": "smaragd@di.uoa.gr", "orcid_id": ""}, {"name": "George Balatsouras", "author_profile_id": "83358952957", "affiliation": "University of Athens, Athens, Greece", "person_id": "P4290351", "email_address": "gbalats@di.uoa.gr", "orcid_id": ""}, {"name": "George Kastrinis", "author_profile_id": "81554797356", "affiliation": "University of Athens, Athens, Greece", "person_id": "P4290352", "email_address": "gkastrinis@di.uoa.gr", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509524", "year": "2013", "article_id": "2509524", "conference": "OOPSLA", "title": "Set-based pre-processing for points-to analysis", "url": "http://dl.acm.org/citation.cfm?id=2509524"}