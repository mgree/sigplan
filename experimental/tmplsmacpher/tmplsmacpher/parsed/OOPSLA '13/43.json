{"article_publication_date": "10-29-2013", "fulltext": "\n Injecting Mechanical Faults to Localize Developer Faults for Evolving Software Lingming Zhang1 Lu Zhang2 \nSarfraz Khurshid1 1Department of Electrical and Computer Engineering, University of Texas, Austin, 78712, \nUSA Email: zhanglm@utexas.edu, khurshid@ece.utexas.edu 2Key Laboratory of High Con.dence Software Technologies \n(Peking University), MoE, Beijing, 100871, China Email: zhanglu@sei.pku.edu.cn Abstract This paper presents \na novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing \nedits made by the developer can be cap\u00adtured using mechanical program transformations (e.g., mu\u00adtation \nchanges). Based on the insight, we present the FIFL framework, which uses both the spectrum information \nof edits (obtained using the existing FAULTTRACER approach) as well as the potential impacts of edits \n(simulated by mu\u00adtation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world \nrepositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experi\u00admental results show that \nFIFL is able to outperform the state\u00adof-the-art FAULTTRACER technique for localizing failure\u00adinducing \nprogram edits signi.cantly. For example, all 19 FIFL strategies that use both the spectrum information \nand simulated impact information for each edit outperform the existing FAULTTRACER approach statistically \nat the signi.\u00adcance level of 0.01. In addition, FIFL with its default set\u00adtings outperforms FAULTTRACER \nby 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than FAULTTRACER on one \nversion pair. Categories and Subject Descriptors D2.5 [Software Engi\u00adneering]: Testing and Debugging \nGeneral Terms Algorithms, Experimentation Keywords Software Evolution; Regression Testing; Fault Localization; \nMutation Testing Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. Copyrights for components \nof this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy \notherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, Indianapolis, \nIndiana, USA. Copyright c &#38;#169;2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509551 \n1. Introduction The problem of fault localization, i.e., identifying the lo\u00adcations of faulty lines of \ncode, remains challenging, often requiring much manual effort. This paper presents a novel solution to \nthis problem in the context of code that evolves. Our insight is that the essence of failure-inducing \nedits made by the developer can be simulated using mechanical pro\u00adgram transformations. Speci.cally, \nsome transformations are likely to share the same locations with failure-inducing edits if those transformations \ntransform the old program ver\u00adsion (i.e., the version right before the faults were introduced) to have \nsimilar test pass/fail results as the new version with real developer edits. To simulate developer edits, \nwe use program transforma\u00adtions based on mutation testing [4, 9, 12, 14, 18, 20, 34, 42, 43, 56], which \nis a methodology originally designed for mea\u00adsuring test-suite quality based on injected faults. Mutation \ntesting generates variants (termed mutants)for the original program under test using mechanical transformation \nrules (termed mutation operators). Each mutant is the same with the original program except for the mutated \nstatement. A mu\u00adtant is termed killed by a test suite if some test from the suite produces different \nresults on the mutant and the original pro\u00adgram. Empirical studies have shown that test suites that kill \na high percentage of mutants are likely to reveal more faults and mutation testing is often viewed as \nthe strongest test cri\u00adteria [3, 11]. It has been used to evaluate the quality of exist\u00ading test suites \n[9, 34, 42, 47] and to generate test suites with high quality [8, 12, 18, 35, 56]. To our knowledge, \nmutation changes have not been uti\u00adlized to simulate developer edits to achieve precise fault lo\u00adcalization. \nThe existing approaches for fault localization dur\u00ading software evolution mainly use sole coverage information \nof developer edits. Change impact analysis [40] is a well\u00adknown methodology for determining affecting \nchanges, i.e., a subset of program edits that might have caused the test fail\u00adure, based on edit coverage \ninformation in regression test\u00ading [19, 40, 57]. It has been shown that the number of af\u00adfecting changes \nfor each failed test can still be large [57]. Therefore, various techniques have been proposed to local\u00adize \nfailure-inducing changes more precisely [39, 45, 57]. The recently developed FAULTTRACER approach [57] \nintroduces spectrum-based fault localization [1, 23, 28, 52] to localiza\u00adtion of failure-inducing edits; \nexperimental results show that FAULTTRACER signi.cantly outperforms Ren et al. s ranking heuristic [39] \nbased on test call graph structures. However, FAULTTRACER still suffers from lack of accuracy, because \nthe spectrum information (i.e., the edits that are mainly executed by failed tests are considered more \nsuspicious) is still based on only coverage information and the suspicious edits may not be responsible \nfor test failures. For instance, some ed\u00adits are ranked at the top just because they are accidentally \nexecuted by failed tests.  A straightforward idea to re.ne the fault localization re\u00adsults is to automatically \napply various subsets of program edits according to their ranking to localize failure-inducing edits \nmore precisely. However, there are three basic rea\u00adsons that make this idea impractical. First, program \ned\u00adits might have complex compilation dependences between them, which does not allow them to be applied \nindepen\u00addently. Second, iteratively applying various combination of edits may cost extra test execution \ntime. Third, iteratively applying program edits does not work for concurrent pro\u00adgrams, since some faulty \nedits may be missed just because they accidentally passed the test suite once. As a result, exist\u00ading \ntechniques for localizing failure-inducing edits usually recommend manually applying and inspecting edits \nafter ranking them [5, 39, 57]. Although real program edits by developers and mechan\u00adical mutation changes \nby mutation testing are both changes to the original program, they are traditionally treated as two separate \ndimensions. This paper uni.es these two dimen\u00adsions of changes. We use both the spectra of edits (obtained \nusing FAULTTRACER)as well as the potential impacts of ed\u00adits (simulated by mutation changes) to achieve \nmore accurate fault localization. This paper presents our methodology of fault injection for fault localization \n(FIFL)and our framework that embodies it for achieving more precise fault localization during soft\u00adware \nevolution based on mutation testing. To localize failure\u00adinducing edits, FIFL .rst utilizes the mutation \ntesting results on the old version1 and gets the test execution results for each mutant. Second, following \nFAULTTRACER, FIFL uses spectrum-based techniques [1, 23, 28, 52] to calculate the suspiciousness of program \nedits between the old and new versions. Third, FIFL builds a mapping between program ed\u00adits with mutants \nof the old version that can potentially simu\u00adlate the corresponding edits based on a set of inference \nrules. Fourth, FIFL determines the impacts of mutation changes by calculating the similarity between \ntest execution results 1 For evolving software systems, the mutation testing results for the old version \nmay be already available in the repository, and ready to use. (Pass/Fail) of the mutants for the old \nversion with the test execution results of the new version, and treats the similarity as the suspiciousness \nof mutants. Finally, for every program edit, FIFL re.nes its suspiciousness based on the suspicious\u00adness \nof its mapped mutants, because those mutants can sim\u00adulate the potential impact of the edit. We believe \nour basic insight into unifying mutation changes with developer edits is also applicable to other key \nsoftware testing realms. For example, mutation testing re\u00adsults for the old program version can optimize \ntest selec\u00adtion [19] and prioritization [41] for the new version, because the potential impact of program \nedits can be simulated by existing mutants. We plan to establish these connections in future work. This \npaper makes the following contributions: We unify two widely used dimensions of software changes: mechanical \nmutation changes and developer edits. This paper leverages this uni.ed view to calculate the spectra \nas well as impacts of program edits to local\u00adize faults for evolving software. Furthermore, this uni.ed \nview can also impact other realms of software testing.  We present the FIFL fault localization framework \nto improve the accuracy of state-of-the-art techniques for localizing failure-inducing edits using the \nexisting mu\u00adtation testing results on the old program version. This framework creates a new dimension \nof possibilities to im\u00adprove fault localization during software evolution.  We present an empirical \nstudy on the code repositories of nine real-world Java programs. The experimental re\u00adsults show that \nFIFL (using its default settings) is able to outperform the state-of-the-art FAULTTRACER technique signi.cantly \n(e.g., by more than 80% for some subjects) in localizing failure-inducing edits, indicating a promis\u00ading \nfuture for localizing faulty edits by injecting mechan\u00adical faults.  2. Example In this section, we \nuse the example in Figure 1 to illustrate the FAULTTRACER approach for localizing failure-inducing edits \nand to motivate our FIFL approach. Figure 1 (a) shows the edited program, which manages the basic bank \naccount functionality of two account types, i.e., BankAcnt (ba\u00adsic account type), SuperAcnt (super account \ntype). Fig\u00adure 1 (b) presents the regression test suite for validating the edits made on the example \nprogram. Assuming that the developer made a failure-inducing edit when adding SuperAcnt.deposit()2 (shown \nin gray), test4 then fails and detects the fault. The goal is to identify the failure\u00adinducing edit precisely. \nWe .rst show the steps applied by FAULTTRACER, then we show the limitations of FAULT-TRACER and the intuition \nof FIFL. 2Please note that in this paper we omit the parameters and return types for methods to make \nthe method names shorter.   1 public class BankAcnt{ 2 public static String bank=\"ABank\" 3 public \nString account; 1 public class TestSuite{ 4 public double saving=100; 2 void test1() { 5 public double \niRate=0.01; 3 BankAcnt acnt=new BankAcnt(\"acnt1\"); 6 public double iRate2=0.02; 4 acnt.withdraw(20); \n7 public Acnt(String a){account=a;} 5 double rate=acnt.getRate(); 8 public double getBalance() 6 assertEquals(acnt.getBalance(), \n80); 9 {return saving;} 7 } 10 public double withdraw (double v) { 8 void test2() { 11 if(saving>=v) \n{ 9 SuperAcnt acnt=new SuperAcnt(\"acnt1\"); 12 saving = saving-v; 10 acnt.withdraw(40); 13 return v; 11 \nassertEquals(acnt.getBalance(), 60); 14 }else return 0; 12 } 13 void test3() { 15 } 14 SuperAcnt acnt=new \nSuperAcnt(\"acnt1\"); 16 public void deposit (double v) 15 acnt.deposit(0); 17 {saving = saving+v;} 16 \nassertEquals(acnt.getBalance(), 100); 18 double getRate(){return iRate;} 17 } 19 double getRate2(){return \niRate2;} 18 void test4() { 20 } 19 BankAcnt acnt1=new BankAcnt(\"acnt1\"); 21 public class SuperAcnt extends \nBankAcnt { 20 SuperAcnt acnt2=new SuperAcnt(\"acnt2\"); 21 double amount=acnt1.withdraw(80); 23 public \nSuperAcnt(String a){super(a);} 22 23 double rate1=acnt1.getRate(); acnt2.deposit(amount); { 24 double \nrate2=acnt2.getRate(); 25 //fault, \"0\" should be \"v\" 25 assertEquals(acnt2.getBalance(), 180); 26 saving=saving+0; \n26 } 27 if(v>=50){saving=saving+1.0;} 27 } 28 } 28 (b) 29 } 30 (a) Figure 1. (a) Example in evolution. \nNote that methods/.elds in box are added, methods/.elds with line-through are deleted, and methods/.elds \nwith underlines are changed. The statements with underlines inside changed methods are added. (b) Tests \nfor the example. Following traditional change impact analysis [40, 57], FAULTTRACER .rst extracts the \nedits between program ver\u00adsions as atomic changes, denoted as .. Atomic changes are categorized as added \nmethods (AM), deleted methods (DM), changed methods (CM), added .elds (AF), deleted .elds (DF), changed \ninstance .elds (CFI), changed static .elds (CSFI), .eld lookup changes (LCf) due to the .eld hiding hierarchy \nchanges, and method lookup (i.e., dynamic dis\u00adpatch) changes (LCm)due to the method overriding hierar\u00adchy \nchanges. Note that FAULTTRACER splits all higher-level changes (e.g., class changes) into atomic changes. \nFAULT-TRACER also infers dependences between atomic changes. For example, a method/.eld lookup change \nis dependent on the method/.eld addition or deletion that causes the lookup change. A non-lookup change \nc1 (e.g., CM or AM)is depen\u00addent on another atomic change c2 iff applying c1 to the original program \nversion without applying c2 results in a syntactically invalid program. FAULTTRACER extracts atomic changes \nAM(SuperAcnt.deposit()), LCm(SuperAcnt, SuperAcnt.deposit())3 , DF(BankAcnt.iRate2), etc, for the example \nin Figure 1. For the change dependences, DF(BankAcnt.iRate2) is determined to be dependent on DM(BankAcnt.getRate2()) \n(DM(BankAcnt.getRate2()) 3An LCm change LCm(R,S.m()) models the fact that an invocation to method S.m() \non an object with run-time type R results in a different target method due to method additions or deletions \nduring evolution. .DF(BankAcnt.iRate2)), as deleting BankAcnt.iRate2 without deleting method BankAcnt.getRate2() \ncan cause BankAcnt.getRate2() to access a .eld without de.\u00adnition. FAULTTRACER also infers that LCm(SuperAcnt, \nSuperAcnt.deposit()) depends on method addition AM(SuperAcnt.deposit()) (AM(SuperAcnt.deposit()) . LCm(SuperAcnt, \nSuperAcnt.deposit())), since the AM change causes the LCm change. Second, FAULTTRACER determines the \nset of affected tests in the regression suites that have been in.uenced by the program edits based on \nthe precise Extended Call Graph (ECG) analysis [57]. For each affected test, FAULTTRACER further identi.es \nthe set of atomic changes that might have changed the test s behavior, and denotes them as affecting \nchanges for the test. For the example program, all the four tests are affected tests, and their affecting \nchanges are shown in Columns 3-6 in the upper part of Table 1. A checkmark denotes that an atomic change \nis an affecting change of a affected test. Third, FAULTTRACER uses the correlation between tests and \naffecting changes to determine the potential failure\u00adinducing edits. The basic intuition is that an affecting \nchange that is mainly executed by failed tests rather than passed tests is more suspicious. Therefore, \nFAULTTRACER utilizes the cor\u00adrelation between affecting changes and the failed tests to calculate the \nsuspiciousness score for each affecting change.  Affected Tests Suspiciousness Score Edits Mapping Mutants \n  Out P P P F Table 1. Suspiciousness Calculation for Developer Edits and Mutation Changes. Columns \n7-10 of Table 1 show the suspiciousness score for each affecting change calculated by four well-known \nsus\u00adpicious calculation formulae, i.e., Tarantula [23], Statistical Bug Isolation (SBI) [28], Jaccard \n[1], and Ochiai [1, 52]. However, the localization results are not ideal for this exam\u00adple: all the four \nformulas rank the real failure-inducing edit AM(SuperAcnt.deposit()) as the tied third suspicious edit. \nThe reason is that AM(SuperAcnt.deposit()) is ex\u00adecuted by test3, which passed, and AF(SuperAcnt.iRate) \nhappens to be executed by the only failed test, thus mis\u00adtakenly lowering the rank of AM(SuperAcnt.deposit()) \nand lifting the rank of AF(SuperAcnt.iRate). The basic intuition of FIFL is that the mutation changes \nmade by mutants of the old program version are able to simulate the impacts of developers edits, and \nmake the test execution results on some suspicious mutants (which share the same locations with real \nfailure-inducing edits) similar to the test execution results on the new program version (with program \nedits). Therefore, we can directly use the existing mutation testing results of the old program version \nto boost the fault localization results while avoiding the drawbacks of iteratively applying subsets \nof program edits. For exam\u00adple, we can use the mutants occurring on the statements inside the same code \nelement with CFI(BankAcnt.iRate) or CM(BankAcnt.withdraw()) to simulate the effect of these two edits. \nFor AM(SuperAcnt.deposit()) and AF(SuperAcnt.iRate), we cannot .nd the statements that share the same \ncode elements with them because they do not exist on the old version. After analyzing the program, we \n.nd that a mutant occurring in BankAcnt.deposit() has a similar impact with adding SuperAcnt.deposit(), \nbecause the invocation to SuperAcnt.deposit() results in the target method of BankAcnt.deposit() in the \nold version. Therefore, adding SuperAcnt.deposit() may have a similar impact with changing BankAcnt.deposit() \n(using mutation). Similarly, we .nd that a mutant occur\u00adring in BankAcnt.iRate has a similar effect with \nedit\u00ading a SuperAcnt.iRate, since SuperAcnt.iRate hides BankAcnt.iRate in the new version (detailed change \nmap\u00adping inference is shown in Section 3.1). For each edit, Col\u00adumn 2 of the lower part of Table 1 shows \nsome example mapping mutants that may simulate each edit. Columns 3-6 show the test execution results \nfor each mutant of the old program version. A checkmark denotes a mutant is killed by a test, i.e., the \ntest fails on the mutant. Intuitively, we can .nd that any mapping mutants of the .rst three edits cannot \nfail the real failed test, test4, while a mapping mutant (in BankAcnt.deposit()) of the real failure-inducing \nedit, AM(SuperAcnt.deposit()), has exactly the same test execution results with the test execution results \nafter evolu\u00adtion, indicating the bene.ts of improving edit suspiciousness calculation based on mutation \ntesting. The detailed fault lo\u00adcalization for this example will be further illustrated in Sec\u00adtion 3.3. \n3. Approach Figure 2 shows the general framework of FIFL. Assume we have two program versions during \nsoftware evolution, P and P ' , together with their regression test suite T, which passes on P but failed \non P ' . First, traditional mutation testing pro\u00adcess is applied on P: generating all the mutants M for \nP and recording the mutant execution results, i.e., the correla\u00adtion between mutants and the tests that \nkill them (denoted as MT ). As FIFL only requires mutation testing results on the old version, FIFL recommends \nthat this step is performed in the background in parallel with developing the new version, and thus the \nmutation testing results are directly available be\u00adfore applying FIFL. Second, FIFL extracts edits between \nP  Figure 2. FIFL architecture. ' and P and calculates the suspiciousness of each edit using FAULTTRACER \n[57], which utilizes the spectrum information of edits and assigns a higher suspiciousness value to a \nedit if it is mainly executed by failed tests. Third, FIFL infers the mapping between developer edits \nand mutants based on a set of inference rules. Fourth, FIFL calculates the suspi\u00adciousness value for \neach mutant. A mutant is assigned with a higher suspiciousness value if it has a similar impact to re\u00adgression \ntests with the program after edits, P ' . Finally, FIFL re.nes the suspiciousness values of program edits \n(based on spectrum information)using the suspiciousness of their map\u00adping mutants (based on impact information), \nand returns the edits with .nal suspiciousness values, . '' , as the .nal re\u00adsult. As the .rst two steps \n(i.e., mutation testing and FAULT-TRACER)are based on existing techniques, the following sub\u00adsections \nshow the change mapping inference (Section 3.1), mutant suspicious calculation (Section 3.2), and suspicious\u00adness \ncombination (Section 3.3) in detail. 3.1 Change Mapping Inference ' In order to bridge the developer \nedits between P and P with the mechanical changes to P via mutation, FIFL de.nes a set of inference rules \nfor inferring the mapping between them. Figure 3 shows the inference rules. In the .gure, mc denotes \nthe corresponding method for method-level change c, fc denotes the corresponding .eld for .eld-level \nchange c, and s\u00b5 denotes the mutated statement of a mutant \u00b5. s.Px denotes that statement s is within \nthe scope of method or ' .eld x in the old program version P. f--+P' f denotes that ' ' .eld f hides \n.eld f in the new version P ' , and m.P' m ' ' denotes that method m overrides method m in P . c . ' \n' c denotes change c depends on change c. Finally, c . \u00b5 denotes that edit c is mapped with mutant \u00b5. \nTo better motivate and illustrate the rules, we present simple arti.cial examples as well as real-world \ncode snippets to show how the change mapping can help increase the suspiciousness of public StringBuffer \nformat(Calendar calendar, StringBuffer buf) { 866: if (mTimeZoneForced) { 867: calendar.getTimeInMillis(); \n868: calendar = (Calendar) calendar.clone(); 869: calendar.setTimeZone(mTimeZone); 870: } 871: return \napplyRules(calendar, buf); } Figure 4. Code snippet of Commons-Lang V3.03 to illus\u00adtrate change mapping \nfor CM edits failure-inducing edits and/or decrease the suspiciousness of fault-free edits. 3.1.1 Inference \nfor Changed/Deleted Elements Shown by the .rst two rules, for modi.cations and deletions of methods and \n.elds, the mapping is trivial: FIFL just maps a change with a mutant if the change and the mutant occur \nin the same method or .eld (since the method or .eld exists in the old version). The mapped developer \nedits and mutation changes occur at the same functional point and thus these two dimensions of changes \nmay have similar impacts to the program under test. For example, when Commons-Lang evolves from V3.02 \nto V3.03, the developer changed the format() method of class FastDateFormat and removed lines 866 to \n870. As the changed method is executed by 35 tests with only one failed, making traditional approaches \nbased on spectrum in\u00adformation not able to localize the fault precisely. In con\u00adtrast, FIFL directly \nmaps the CM change to the 5 mutants occurred inside the format() method in V3.02. Within the mapped mutants, \n3 mutants, which remove method invoca\u00adtions for line 867 to line 869 respectively, have exactly the same \nfailed test as V3.03, demonstrating that the mapped mutants can be used to simulate the effect of real \nmethod changes. This mapping signi.cantly increase the ranking of the failure-inducing edit (details \nshown in Section 5).  3.1.2 Inference for Added Elements Overridding/Hiding Existing Elements The mutant \nmapping inference rules for additions of .elds and methods are more complex because they have no cor\u00adresponding \ncode elements in the old version. We illustrate those rules with examples in Figure 6. In the .gure, \nwe con\u00adnect an added element (AM or AF)with another non-added el\u00adement using a twin line if and only \nif the added element can be mapped to the mutant within the scope of the non-added element. The basic \nintuition for Rules 3-6 is that the mu\u00ad ' tants occurring in a method/.eld c that is close to the added \nmethod/.eld c in the overriding/hiding hierarchy (such that ' an invocation/access to c actually executes \nc in the old ver\u00adsion) can be used to simulate the impact of adding c, because ' both adding c and mutating \nc may change the execution of c ' . Rules 3 and 4 de.ne the mapping inference for method additions that \noverride some methods: If the added method  c . CM .DM \u00b5 . M s\u00b5.Pmc (1) c . \u00b5 c . CF I .CSF I .DF \u00b5 \n. M s\u00b5.Pfc (2) c . \u00b5 c . AM \u00b5 . M s\u00b5.Pm mc.P' m c, c ' . AM \u00b5 . M c ' . \u00b5 mc.P' mc ' (3) (4) c . \u00b5 c \n. \u00b5 c . AF \u00b5 . M s\u00b5.Pf fc--+P' f c, c ' . AF \u00b5 . M c ' . \u00b5 fc--+P' fc ' (5) (6) c . \u00b5 c . \u00b5 '' ' '' \nc . AM c ' . DM c '' . LCm \u00b5 . M s\u00b5.Pmc ' c . c c . c (7) c . \u00b5 '' ' '' c . AF c ' . DF c '' . LCf \n\u00b5 . M s\u00b5.Pfc ' c . c c . c (8) c . \u00b5 c . AM .AF c ' . . \u00b5 . M c ' . \u00b5 c . c ' (9) c . \u00b5 Figure 3. Rules \nfor inferring change mapping. class org.joda.time.chrono.AssembledChronology: public long getDateTimeMillis(int \nyear, int monthOfYear, int dayOfMonth, int hourOfDay, int minuteOfHour, int secondOfMinute, int millisOfSecond) \n{...} .class org.joda.time.chrono.BasicChronology: public long getDateTimeMillis(int year, int monthOfYear, \nint dayOfMonth, int hourOfDay, int minuteOfHour, int secondOfMinute, int millisOfSecond) {...} Figure \n5. Code snippet of Joda-Time V1.20 to illustrate change mapping for AM edits with overridden methods \noverrides some existing methods that are not newly added, Rule 3 maps the addition change to all mutants \nthat occur in\u00adside the existing method; if the atomic change of the method overridden by the added method \nis already mapped with a set of mutants, Rule 4 also maps the added method with those mutants. Similarly, \nRules 5 and 6 infer the change mapping for .eld additions that hide other .elds: If the added .eld hides \nsome existing .elds that are not newly added, Rule 5 maps the addition change to all mutants that occur \ninside the existing .eld; otherwise, if the change on the .eld hidden by the added .eld is already mapped \nwith a set of mutants, Rule 6 also maps the added .eld to those mutants. Note that Rules 4 and 6 should \nbe iteratively applied until they reach a .x point. To illustrate, the change mapping inference steps \nfor Figure 6(a) are shown as follows: Applied Rules AM(m3 ) AM(m4 ) Rule 3 Mm2 - Rule 4 Mm2 Mm2 where \nMm2 denotes the mutants occurring in the body of method m2. Similarly, the two AF changes in Figure 6(b) \nare mapped with mutants inside f1 and f3. For example, when Joda-Time evolves from V1.10 to V1.20, the \nfault-free edit AM(getDateTimeMillis()) in class BasicChronology was ranked high because it was executed \nby some failed tests accidentally. As the method was newly added, Rule 1 cannot map the edit with any \nmu\u00adtants of the old version. Figure 5 shows the class inheritance hierarchy for the class containing \nthe added method. In the .gure, . denotes the below class is a subclass of the above class. As the added \nmethod overrides an existing method in class AssembledChronology (denoting that the two meth\u00adods have \nsimilar functionalities), the invocation to the spe\u00adci.c functionality of the new method may be resolved \nto the overridden method in the old version. Thus, some mutation changes to the old overridden method \nmay have similar im\u00adpacts with the edit of adding the faulty method, and thus the mutants in the overridden \nmethod can be mapped to the AM edit to simulate its impact. In fact, using this mapping, FIFL successfully \ndecreases the suspiciousness of the fault\u00adfree edit. 3.1.3 Inference for Added Elements Sharing Overriding/Hiding \nHierarchy with Deleted Elements There may also be some added method/.eld c that shares the same overriding/hiding \nhierarchy with some deleted method\u00ad/.eld c ' , i.e., although they never co-exist in one version, they \nmay implement the same functionality. Therefore, the mutants within mc ' in the old version may also \nbe able to simulate the impact of c. As added and deleted elements do not exist in the same version, \nFIFL cannot use the ordi\u00adnary overriding/hiding hierarchy analysis to infer the change mapping. Instead, \nFIFL utilizes the fact that both addition changes and deletion changes would cause method or .eld lookup \nchanges, and uses those lookup changes to bridge the mapping between addition changes and mutants in \ndeleted elements. For any method/.eld addition c, if some method/\u00ad ' .eld deletion c causes the same \nmethod/.eld lookup change with c, Rule 7/8 maps the mutants inside the corresponding ' deleted element \nof c with c. Figures 6(c) and 6(d) illustrate that the mutants within the deleted methods/.elds can be \nmapped with addition changes. In the same revision with the last example code snip\u00adpet (i.e., when Joda-Time \nevolves from V1.10 to V1.20), the fault-free program edit AM(getAverageMillisPerMonth()) in class BasicFixedMonthChronology \nwas ranked high by the existing FAULTTRACER approach, because it was exe\u00adcuted by the failed tests accidentally. \nAs the added method does not override any existing method, Rules 3 and 4 can\u00ad    (a) (b) Figure 6. \nIllustration for mutant mapping. class org.joda.time.chrono.AssembledChronology public XStream(ReflectionProvider...) \n{ .class org.joda.time.chrono.BaseGJChronology .class org.joda.time.chrono.CopticChronology: long getAverageMillisPerMonth() \n{...} (a) Joda-Time V1.10 class org.joda.time.chrono.AssembledChronology .class org.joda.time.chrono.BasicChronology \n.class org.joda.time.chrono.BasicFixedMonthChronology: long getAverageMillisPerMonth() {...} .class org.joda.time.chrono.CopticChronology \n(b) Joda-Time V1.20 Figure 7. Code snippets of Joda-Time V1.10 and V1.20 to illustrate change mapping \nof AM edits with deleted methods sharing the same overriding hierarchy not map the edit with any mutant. \nFigure 7(b) shows the inheritance hierarchy for the class of the added method (BasicFixedMonthChronology). \nThe containing class of the edit has a superclass named AssembledChronology and a subclass named CopticChronology. \nShown in Fig\u00adure 7(a), the old version has a method (which was deleted in the new version) with the same \nsignature and under the same class inheritance hierarchy as the added method. The actual change logic \nis that the developer pulled up the deleted method to a new superclass in the new ver\u00adsion. In this way, \nthe old deleted method and the new added method implement the same functionality and thus the mutation \nchanges to the deleted method and the edit for adding the new method may have the same impact to the \nprogram. Therefore, using Rule 7, FIFL identi.es that both the deleted method and the newly added method \ncause the same LC change: LCm(CopticChronology, *.getAverageMillisPerMonth()) (i.e., invocation of method \ngetAverageMillisPerMonth() on run-time ob\u00adject of type CopticChronology resolves to a different method), \nand thus maps the mutants occurring on the old method with the method addition. In fact, the mutants \nfor the old method have different test execution results with the new version, making the ranking of \nthe fault\u00adfree AM(getAverageMillisPerMonth()) decreased, and thus the ranking of actual failure-inducing \nedits increased. ... 367: this.mapper=mapper==null?buildMapper():mapper; ... } private Mapper buildMapper() \n{ 379: Mapper mapper = new DefaultMapper( classLoaderReference); 380: if ( useXStream11XmlFriendlyMapper() \n){ 381: mapper = new XStream11XmlFMapper(mapper); 382: } ... } Figure 8. Code snippet of XStream \nV1.21 to illustrate change mapping for AM edits without methods sharing the same method overriding hierarchy \n 3.1.4 Inference for Other Added Elements For the other added method/.eld c that neither overrides/hides \nany existing method/.eld nor shares the overriding/hiding hierarchy with any deleted method/.eld, if \nc is executed by the regression test suite, c must be invoked/accessed by some changed or added method \nc ' . Then, a mutant \u00b5 occurring in ' c may be used to simulate the impacts of c, because mutant ' \u00b5in \nc may have the same impacts with adding invocation to c in c ' . In this situation, the change impact \nanalysis compo\u00ad ' nent of FIFL would have detected that change c depends on ' change c (i.e., c . c ' \n), because c would invoke/access unde\u00adclared method/.eld without applying change c. Rule 9 then ' directly \nmaps any mutant that has been mapped with c to c. Note that Rule 9 should be iteratively applied until \nit arrives a .x point. To illustrate, the change mapping inference for Figure 6(e) is shown as follows: \nApplied Rules AM(m2 ) AF(f1 ) Rule 9 Map[CM(m1)] - Rule 9 Map[CM(m1)] Map[CM(m1 )] where Map[CM(m1)] \ndenotes the mutants that have been mapped to change CM(m1) (Rule 1). The .rst inference us\u00ading Rule 9 \ndoes not .nd mapped mutants for AF(f1), because AM(m2) has no mapped mutants yet. On the contrary, the \nsec\u00adond application of Rule 9 successfully .nds mapped mutants for AM(f1), because AM(m2) is mapped to \nmutants at the .rst step. Among the studied subjects, when XStream evolves from V1.20 to V1.21, the developer \nadded the faulty method buildMapper(). Shown in Figure 8, the added faulty method buildMapper() was invoked \nby a changed method XStream() at line 367 (which is the only changed line for the method, and invoked \nan old method for building map\u00adpers in the old version). The faulty AM change is executed by every tests \nbecause it is used for initializing mappers, and thus cannot be distinguished from other edits. Because \nthe functionality of building mappers in the old version was also invoked by the changed method, some \nmutation changes (especially mutation changes on the speci.c line for invok\u00ading the mapper builder) may \nhave similar impacts as adding a new method for building mappers. Thus, FIFL maps the AM edit with the \nmutants that occurred in the old version of XStream() based on Rules 1 and 9. In fact, some mu\u00adtants \nthat occurred at the changed line of the old method XStream() exactly fail the same tests with the new \nversion because the statement for constructing the old map builder was changed, demonstrating that mapped \nmutants can be used to simulate the effect of method additions when the added methods do not override \nany existing method or share overriding hierarchy with any deleted method.  Finally, Figure 6(f) illustrates \na slightly more complex situation: an added method m2 overrides an existing method m1, and also invokes \nanother added method m3, which in turns accesses an added .eld f1. FIFL .rst maps AM(m2) with mutants \nin m1 based on Rule 3, then maps AF(f1)and AM(m3)with mutants in m1 based on Rule 9. The detailed inference \nis shown as follows: Applied Rules AM(m2) AM(m3 ) AF(f1 ) Rule 3 Mm1 - - Rule 9 Mm1 Mm1 - Rule 9 Mm1 \nMm1 Mm1 Note that each program edits will be applied with any ap\u00adplicable rules and may be mapped to \nmutants from vari\u00adous methods/.elds. FIFL uses all those mutants to select the most suitable one (details \nare shown in the next section).  3.2 Mutant Suspiciousness Calculation The calculation of mutant suspiciousness \nis based on the in\u00adtuition that a mutant that has a similar test pass/fail results with the program after \nedits might share same positions with the real failure-inducing program edits. Therefore, the suspi\u00adciousness \nof a mutant can be calculated based on the similar\u00adity between its test execution results and the test \nexecution results after edits. As mutant suspiciousness will be used to re.ne edit suspi\u00adciousness, we \nuse the same suspicious calculation formulae used by FAULTTRACER, for calculating the suspiciousness \nof edits [57]. The difference is that FIFL additionally uses the correlation between tests and mutant \nkilling as input, while FAULTTRACER only uses the correlation between tests and ed\u00adits as input. FIFL \nadapts four representative suspiciousness computations for mutants as follows. (1) Statistical Bug Isolation. \nLiblit et al. [28] .rst pro\u00adposed Statistical Bug Isolation (SBI) to rank faulty predi\u00adcates. Yu et al. \n[52] adapted SBI to rank potential faulty state\u00adments. FAULTTRACER adapted the SBI formula to calculate \nthe suspiciousness for program edits [57]. We further adapt the formula to de.ne a suspiciousness score \nfor a mutant \u00b5 as Ss(\u00b5): '' '' '' (|K(\u00b5, T ) nTf|)/(|K(\u00b5, T ) nT p|+ |K(\u00b5, T ) nTf|) f ailed(\u00b5) passed(\u00b5) \nf ailed(\u00b5) ' ' In this formula, T denotes all affected tests, Tp denotes ' the passed affected tests, \nTf denotes the failed affected tests, K(\u00b5, T ' ) denotes the affected tests that kill \u00b5, failed(\u00b5) denotes \nthe number of failed affected tests that kill mutant \u00b5, and passed(\u00b5)denotes the number of passed affected \ntests that kill \u00b5. (2) Tarantula. Jones et al. [23] proposed Tarantula, which assigns higher suspiciousness \nscores to statements primarily executed by failed tests than statements primarily executed by passed \ntests. FAULTTRACER then adapted the Tarantula for\u00admula to calculate the suspiciousness for program edits \n[57]. Similarly, we adapt the formula to de.ne a suspiciousness score for a mutant \u00b5as St(\u00b5): ''' ''' \n''' (|K(\u00b5, T ) nT |/|T |)/(|K(\u00b5, T ) nT |/|T |+ |K(\u00b5, T ) nT |/|T |) ff ff p p -  -  - %f ailed(\u00b5) \n%passed(\u00b5) %f ailed(\u00b5) In this formula, %failed(\u00b5)denotes the ratio of failed affected tests that can \nalso kill mutant \u00b5to all failed affected tests, while %passed(\u00b5)denotes the ratio of passed affected \ntests that can kill mutant \u00b5to all passed affected tests. (3) Ochiai. Yu et al. [52] and Abreu et al. \n[1] used the Ochiai formula, which originated from the molecular biol\u00adogy domain, to rank faulty statements \nin one speci.c pro\u00adgram version. FAULTTRACER then adapted the Ochiai for\u00admula to calculate the suspiciousness \nfor program edits [57]. Similarly, we adapt the formula to de.ne a suspiciousness score for a mutant \n\u00b5as So(\u00b5): ''' '' '' (|K(\u00b5, T ) nT |)/ |T | *(|K(\u00b5, T ) nT |+ |K(\u00b5, T ) nT |) ff pf  -- f ailed(\u00b5) \nall f ailed passed(\u00b5) f ailed(\u00b5) In this formula, all failed denotes the number of all failed affected \ntests. (4) Jaccard. Abreu et al. [1] used the Jaccard formula, which was used for measuring the statistical \nsimilarity and diversity between sample sets, to rank faulty statements. FAULTTRACER then adapted the \nJaccard formula to calculate the suspiciousness for program edits [57]. We further adapt the formula \nto de.ne a suspiciousness score for a mutant \u00b5 as Sj(\u00b5): ''' '' (|K(\u00b5, T ) nT |)/( |T | + |K(\u00b5, T ) nT \n|) ff p f ailed(\u00b5) all f ailed passed(\u00b5)  In this way, the suspiciousness values for all mutants are \nbetween 0.00 and 1.00. If a mutant failed exactly the same set of tests with the program after edits, \nits suspiciousness would be calculated as 1.00 by all formulae. To illustrate, we show the suspiciousness \nscore calculated for each mutant of the example program in Columns 7-10 of the lower part of Table 1. \nShown in the gray row, FIFL directly determines a mapping mutant of the real failure-inducing edits as \nthe most suspicious. Note that when a real program has multiple faulty edits, the suspiciousness of mapped \nmutants for all the faulty edits can be determined as suspicious because the injection of each mutant \nmay make the program fail a subset of the real failed tests. Therefore, all the four formulae can calculate \nthose mutants mapped with faulty edits as suspicious.  3.3 Suspiciousness Combination In this section, \nwe present suspiciousness combination based on the suspiciousness for mutants, the suspiciousness for \ned\u00adits, and the mapping between edits and mutants. FIFL inte\u00adgrates three general combination strategies \nshown as follows. Note that FIFL uses the maximum suspiciousness value for the set of mapping mutants \n(i.e., using the most suitable mu\u00adtant) to re.ne the suspiciousness of an edit, because a large ratio \nof mutants might not be effective in simulating the im\u00adpacts of program edits. (1) Min-Max. This strategy \nre.nes an edits s suspiciousness by using the minimum value between the edit s initial suspi\u00adciousness \nvalue and the maximum suspiciousness value for its mapping mutants: Sref ined (c)= Min(S(c), Max\u00b5.Map[c]S(\u00b5)) \nTo illustrate, when we use Ochiai for both the edit and mu\u00adtant suspiciousness calculation, the re.ned \nsuspiciousness value for AM(SuperAcnt.deposit()) is calculated as fol\u00adlows. Sref ined (AM(SuperAcnt.deposit())) \n= Min(0.71, Max(1.00, 0.71, 0.71, 0.71)) = 0.71 The suspiciousness value for other three edits are all \nre.ned to 0.00, making AM(SuperAcnt.deposit()) as the top one suspicious edit. (2) Max-Max. This strategy \nre.nes an edits s suspiciousness by FAULTTRACER using the maximum value between the edit s initial suspiciousness \nvalue and the maximum suspi\u00adciousness value for its mapping mutants: Sref ined (c)= Max(S(c), Max\u00b5.Map[c] \nS(\u00b5)) in which Map[c]denotes all the mapping mutants for edit c. To illustrate, when we use Ochiai for \nboth the edit and mu\u00adtant suspiciousness calculation, the re.ned suspiciousness value for AM(SuperAcnt.deposit()) \nis calculated as fol\u00adlows. Sref ined (AM(SuperAcnt.deposit())) = Max(0.71, Max(1.00, 0.71, 0.71, 0.71)) \n= 1.00 The suspiciousness value for other three edits are re.ned as 0.71, 0.58, and 1.00, making AM(SuperAcnt.deposit()) \ntied as the top ranking edit. (3) Ratio-Max. This strategy re.nes an edit s suspiciousness by assigning \ndifferent weights to the edit s initial suspicious\u00adness value and the maximum suspiciousness value of \nits map\u00adping mutants. The combination is shown as follows. Sref ined (c)= a* S(c) + (1 -a)* Max\u00b5.Map[c]S(\u00b5) \nwhere a . [0.0,1.0) denotes the weight for an edit s initial suspicious4 . Note that when a = 0.0, the \nstrategy ranks edits only based on the suspiciousness of their map\u00adping mutants. To illustrate, when \nwe use Ochiai for both the edit and mutant suspiciousness calculation and the de\u00adfault a value of 0.5, \nthe re.ned suspiciousness value for AM(SuperAcnt.deposit()) is calculated as follows. Sref ined (AM(SuperAcnt.deposit())) \n= a* 0.71 + a* Max(1.00, 0.71, 0.71, 0.71) 0.5* 0.71 + 0.5* 1.00 = 0.86 The suspiciousness value for \nother three edits are re.ned as 0.36, 0.29, and 0.50, making AM(SuperAcnt.deposit()) as the top one suspicious \nedit. Note that when the number of mapping mutants for an edit is smaller than a threshold (2 in this \npaper), those mu\u00adtants may not be suf.cient to simulate the impact of the edit. Therefore, for this circumstance, \nFIFL simply keeps the ini\u00adtial suspiciousness for that edit.  3.4 Tackling the Cost of FIFL: Edit-Oriented \nMutation Testing Compared with the existing FAULTTRACER technique, FIFL additionally utilizes the available \nmutation testing results of the old program version from the repository to further re.ne the fault localization \nresults. Mutation testing results can already be available due to its other applications, e.g., generating \n[8, 12, 18, 35, 56] or evaluating test suites [4, 42, 43]. In addition, since FIFL depends on the mutation \ntesting results of the old version, the mutation testing process can be conducted at the same time with \ndeveloping the new version, thus improving the fault localization results with no overhead. However, \nthe mutant testing results for the old program version might still be absent when doing fault localization, \nwe further show how to tackle the cost of mutation testing at that situation. We propose the concept \nof Edit-Oriented Mu\u00adtation Testing, which only collects mutation testing results of the mutants mapping \nwith program edits, since the execution results of other mutants are not used by FIFL. Formally, the \nsubset of mutants executed by edit-oriented mutation testing can be represented as follows. Medit = {\u00b5|.c \n. ., \u00b5 . Map[c]} 4We exclude the a value of 1.0, because the edit s suspiciousness is not re.ned at all \nin such a case.  Projects Description License LoC(Src/Test) Time&#38;Money Time and money library MIT \n2.7K/3.0K Barbecue Bar-code creator BSD 5.4K/3.3K Mime4J Message stream parser Apache2.0 7.0K/3.8K Jaxen \nJava XPath library Apache-style 14.0K/8.8K Xml-Security XML security standards MIT 19.8K/4.0K XStream \nObject serialization library BSD 18.4K/20.1K JMeter Performance testing Apache2.0 44.6K * Commons-Lang \nJava helper utilities Apache2.0 23.3K/32.5K Joda-Time Time library Apache2.0 32.9K/55.9K * indicates \nsource and test code are written together, and cannot be measured separately Table 2. Subjects overview. \nwhere .denotes all the edits between two program versions, and Map[c]denotes the mapping mutants for \nprogram edit c. As the mutant generation is much more ef.cient than mutant execution [42], our implementation \nsimply generates all the mutants, and only executes the mutants mapped with edits. 4. Implementation \nWe built our FIFL technique on top of Javalanche5 [42]and FAULTTRACER [57]. FIFL uses Javalanche for \nthe .rst step mutant generation and execution. Javalanche is a state-of\u00adthe-art mutation testing tool \nfor Java programs. Javalanche allows ef.cient mutant generation, as well as mutant execu\u00adtion. More precisely, \nJavalanche uses a small set of suf.cient mutant operators, and manipulates Java bytecode directly us\u00ading \nmutant schemata to enable ef.cient mutant generation. In addition, Javalanche only executes the set of \nin.uenced tests for each mutant based on coverage checking, and al\u00adlows parallel execution to enable \nef.cient mutant execution. FIFL uses FAULTTRACER for the second step edit detec\u00adtion and suspiciousness \ncalculation. FAULTTRACER is a state\u00adof-the-art technique for localizing failure-inducing program edits \nduring software evolution. FAULTTRACER calculates the suspiciousness of each program edit based on their \ncorrela\u00adtion with failed tests. FAULTTRACER has been shown to out\u00adperform the previous ranking technique \n[39] by more than 50% [57]. FIFL s third step change mapping inference requires the method-overriding \nhierarchy, .eld-hiding hierarchy, as well as source code scope for given changed entities, etc. We implemented \nthis step based on the Eclipse JDT toolkit6 . The fourth and the .fth steps mainly involve data compu\u00adtation \nand transformation, and are directly implemented with the Java language. Although FIFL is currently imple\u00admented \nfor Java programs, the FIFL methodology of localiz\u00ading faulty edits based on fault injection is generalizable \nfor other object-oriented languages. 5. Experimental Study FIFL aims to make suspiciousness calculation \nmore pre\u00adcise for program edits. To evaluate the effectiveness of 5http://www.st.cs.uni-saarland.de/mutation/. \nAccessed in July 2013. 6http://www.eclipse.org/jdt/. Accessed in July 2013. FIFL, the experimental study \ncompares FIFL against FAULT-TRACER [57], a state-of-the-art approach for localizing failure\u00adinducing \nedits on real-world code repositories. 5.1 Independent Variables According to the theory of experimentation, \nwe used the following independent variables (IVs) to investigate their in.uences on the .nal experimental \nresults: IV1: Different Localization Approaches. We considered the following choices of approaches as \nthe .rst indepen\u00addent variable: (1) FAULTTRACER, which is a state-of-the-art approach for localizing \nfailure-inducing program edits; (2) FIFL, which is proposed in this paper and embodies the idea of injecting \nfaults to localize failure-inducing edits. IV2: Different Calculation Formulae for Edit Suspicious\u00adness. \nWe considered all the four formulae used by FAULT-TRACER [57]to calculate the suspiciousness of program \ned\u00adits: (1) SBI; (2) Tarantula; (3) Ochiai;and (4) Jaccard. IV3: Different Calculation Formulae for Mutant \nSuspi\u00adciousness. Similarly, we considered the same set of formu\u00adlae for calculating the suspiciousness \nof mutants (shown in Section 3.2): (1) SBI; (2) Tarantula; (3) Ochiai;and (4) Jac\u00adcard. IV4: Different \nCombination Strategies. We considered all the three combination strategies shown in Section 3.3 for re\u00ad.ning \nthe suspiciousness of edits based on the suspicious\u00adness of mutants: (1)Min-Max; (2)Max-Max; and (3)Ratio-Max. \nFor the Ratio-Max strategy, we use values of aranging from 0.00 to 0.95 with increments of 0.05, i.e., \n20 values of a. Note that when a=0.0, the strategy ranks edits based on pure mutant suspiciousness. \n5.2 Dependent Variables Since we are concerned with the effectiveness as well as ef\u00ad.ciency achieved \nby our FIFL approach, we used the follow\u00ading dependent variable (DV): DV: Rank of Failure-Inducing Edits. \nThis variable denotes the total number of edits that developers need to inspect before .nding the real \nfailure-inducing edits when using the compared techniques.  5.3 Subjects and Experimental Setup We obtained \nversions of the source code of nine open-source projects in various application domains, which have been \nwidely used for regression testing and mutation testing re\u00adsearch [10, 42, 43, 58]. Table 2 depicts brief \ninformation about the latest release of each studied project. The sizes of the studied projects range \nfrom 5,675 lines of code (LoC) (Time&#38;Money, including 2,678 LoC source code and 2,997 LoC test code) \nto 88,835 LoC (Joda-Time, with 32,932 LoC source code and 55,903 LoC test code). We obtained Xml-Security \nand JMeter from the well-known Software\u00adartifact Infrastructure Repository (SIR) [10], and all the other \nprojects from their host repositories. For each project, we obtained all the available releases in its \nrepository, and   All Mutants Mapped Mutants No Project Version Pair #Tests #FTests #Edits #FEdits \nNumber Execution Time Number Execution Time P1 Time&#38;Money 3.0, 4.0 143 1 215 1 1737 9min24s 792 7min38s \nP2 Time&#38;Money 4.0, 5.0 159 1 246 1 1984 6min43s 325 1min5s P3 Barbecue 1.5a1, 1.5a2 160 2 23 1 41310 \n15min37s 96 57s P4 Mime4J 0.50, 0.60 120 8 2862 3 19111 181min20s 8086 37min50s P5 Mime4J 0.61, 0.70 \n348 3 3160 4 27654 227min14s 24443 49min15s P6 Jaxen 1.0b7, 1.0b9 24 2 204 3 3820 100min25s 964 28min38s \nP7 Jaxen 1.1b2, 1.1b5 69 2 419 1 5489 19min8s 1493 7min42s P8 Jaxen 1.1b6, 1.1b7 243 2 473 5 9704 44min49s \n6037 24min8s P9 Jaxen 1.1b9, 1.1b11 645 1 92 1 10045 61min59s 157 2min9s P10 Xml-Security 1.0, 2.0 91 \n5 329 2 10599 16min6s 1134 2min3s P11 XStream 1.20, 1.21 637 3 209 1 10956 49min51s 547 5min31s P12 XStream \n1.21, 1.22 698 1 222 2 11516 54min24s 847 6min56s P13 XStream 1.22, 1.30 768 24 540 11 12536 64min22s \n1870 8min25s P14 XStream 1.30, 1.31 885 12 416 3 14140 96min43s 2206 13min0s P15 XStream 1.31, 1.40 924 \n13 1225 7 15006 99min26s 3462 22min15s P16 XStream 1.41, 1.42 1200 6 136 5 18046 132min2s 1817 10min50s \nP17 JMeter 0.0, 1.0 51 1 1714 1 5363 29min56s 1779 8min40s P18 JMeter 1.0, 2.0 60 2 1056 1 21896 57min32s \n3604 13min22s P19 JMeter 2.0, 3.0 72 11 2809 4 8067 43min27s 4347 34min44s P20 JMeter 3.0, 4.0 76 1 764 \n1 7116 34min36s 703 10min50s P21 Commons-Lang 3.02, 3.03 1698 1 221 1 20792 90min26s 803 4min7s P22 Commons-Lang \n3.03, 3.04 1703 2 172 2 20792 90min29s 1441 3min33s P23 Joda-Time 0.90, 0.95 219 4 5976 2 6581 6min29s \n5365 3min37s P24 Joda-Time 0.98, 0.99 1932 6 1254 2 16208 31min36s 3631 4min45s P25 Joda-Time 1.10, 1.20 \n2420 1 793 1 19012 53min10s 1997 12min32s P26 Joda-Time 1.20, 1.30 2516 11 571 3 19566 106min10s 718 \n31min32s    Table 3. Version pairs with test failures. treated every two continuous releases as a \nversion pair. For each version pair, we applied the regression test suite of the old version on the new \nversion, and treated the edits that cause the regression suite to fail on the new version as regression \nfaults. We studied all those version pairs with re\u00adgression test failures to evaluate FIFL s performance. \nThe experimental study was performed on a Dell desktop with Intel i7 8-Core Processor (2.8G Hz), 8G RAM, \nand Win7 Enterprise 64-bit version. For all the projects, we were able to .nd version pairs with regression \nfaults except JMeter. However, JMeter comes with seeded faults in SIR, thus we use seeded faults for \nJMeter. In total, we have 26 version pairs with regression faults, and the details are shown in Table \n3. In Table 3, Col\u00adumn 1 shows the abbreviations for all the version pairs with regression faults. Columns \n2 and 3 show the project name and corresponding versions for each version pair. Columns 4 and 5 show \nthe number of tests and failed tests for each version pair. Columns 6 and 7 show the number of edits \nand failure-inducing edits for each version pair. The table also presents the mutation testing statistics \nus\u00ading Javalanche. Columns 8 and 9 show the number and ex\u00adecution time for all mutants. Similarly, Columns \n10 and 11 show the number and execution time for the mutants that are actually needed by FIFL (i.e., \nmutants mapped with edits). We observe that overall mutation testing time by Javalanche for each studied \nversion pair is acceptable for the studied subjects, ranging from 6 minutes 43 seconds to 227 min\u00adutes \n14 seconds. The reason is that Javalanche embodies a set of optimization strategies for improving ef.ciency \n(Sec\u00adtion 4). We also .nd that the mutation testing time for only mapped mutants is much more ef.cient, \nand is less than 1 hour for all subjects. Recall that FIFL never costs the devel\u00adoper the entire mutation \ntesting time: (1) mutation testing results can already be in the repository before applying FIFL due \nto its other applications; (2) the mutation testing results can be collected at the same time as developing \nthe new ver\u00adsion, because FIFL uses the mutation testing results on the old version; (3) even when mutation \ntesting results are not available before applying FIFL, developers can only collect the mutation testing \nresults for the mutants mapped with ed\u00adits (Section 3.4).  5.4 Results and Analysis In this section, \nwe .rst compare all strategies of FIFL with FAULTTRACER (Section 5.4). Then we compare the de\u00adfault strategies \nof FIFL with FAULTTRACER in detail (Sec\u00adtion 5.4.2). Finally, we discuss about the scope and limita\u00adtions \nof the FIFL approach and its evaluation (Section 5.4.3). 5.4.1 Overall comparison between FAULTTRACER \nand various strategies of FIFL Figures 9(a) to 9(d) show the comparison of FIFL with FAULTTRACER for \ndifferent suspicious calculation formulae. We denote FAULTTRACER (the ranking based on pure edit suspiciousness) \nas Ft., FIFL s Min-Max strategy as Min, and FIFL s Max-Max strategy as Max. For FIFL s Ratio-Max strategies, \nwe use R and the value of a to represent each strategy. For example, we use R95 to denote the Ratio-Max \nstrategy with a=0.95. In each .gure, the horizontal axis shows compared techniques, and the vertical \naxis shows the rank of failure-inducing edits by each technique across all the version pairs with regression \nfaults. Each box plot shows  30 25 Edits 25 Edits 20 nducing 20 nducing 15 Failure-I 15 Failure-I 10 \nRanking of 10 Ranking of 5 5 0 0 Ft. Min Max R95 R90 R85 R80 R75 R70 R65 R60 R55 R50 R45 R40 R35 R30 \nR25 R20 R15 R10 R05 R00 Ft. Min Max R95 R90 R85 R80 R75 R70 R65 R60 R55 R50 R45 R40 R35 R30 R25 R20 R15 \nR10 R05 R00 (a) Ranking failure-inducing edits using the SBI formula. (b) Ranking failure-inducing edits \nusing the Tarantula formula. 30 30 25 Edits 25 Edits 20 ducing 20 ducing 15 Failure-In 15 Failure-In \n10 Rankin g of 10 Rankin g of 5 5 0 0 Ft. Min Max R95 R90 R85 R80 R75 R70 R65 R60 R55 R50 R45 R40 R35 \nR30 R25 R20 R15 R10 R05 R00 Ft. Min Max R95 R90 R85 R80 R75 R70 R65 R60 R55 R50 R45 R40 R35 R30 R25 R20 \nR15 R10 R05 R00 (c) Ranking failure-inducing edits using the Ochiai formula. (d) Ranking failure-inducing \nedits using the Jaccard formula. Figure 9. Ranking failure-inducing edits using various techniques with \nvarious formulae. the average (a dot in the box), median (a line in the box), and upper/lower quartile \nvalues for the ranking of failure\u00adinducing edits across various version pairs7 . We mark all the techniques \nthat outperform the original FAULTTRACER technique (which is based on the pure edit suspiciousness) in \nterms of both average and median values as shadowed box plots. The key .ndings from the experimental \nresults are as follows. First, in terms of median effectiveness across all ver\u00adsion pairs, all ranking \ntechniques of FIFL outperform FAULT-TRACER. When using the SBI formula (the median case for the Tarantula \nformula is similar), in the median case, FAULT-TRACER localizes failure-inducing edits within 9.5 edits. \nIn contrast, the Min-Max and Max-Max strategies of FIFL lo\u00adcalize failure-inducing edits within 8.5 and \n6 edits, respec\u00adtively. The Ratio-Max strategies of FIFL with all a values within [0.00,0.95] localize \nfailure-inducing edits within 6 edits. When using the Ochiai formula (the median case for the Jaccard \nformula is similar), in the median case, FAULT-TRACER localizes failure-inducing edits within 9.5 edits. \nIn contrast, the Min-Max and Max-Max strategies of FIFL local\u00adize failure-inducing edits within 8.5 and \n5.05 edits, respec\u00adtively. Furthermore, the Ratio-Max strategies of FIFL with a . [0.30,0.55] localize \nfailure-inducing edits within 4 ed\u00adits, an improvement of 57.89% over FAULTTRACER. Second, in terms of \naverage effectiveness across all ver\u00adsion pairs, the Max-Max strategy and the Ratio-Max strate\u00ad 7Note \nthat for each version pair with multiple faulty edits, we use the average ranking of all its faulty edits. \ngies with a . [0.05,0.95] still outperform FAULTTRACER. For example, using the SBI formula, FAULTTRACER \nlocalizes failure-inducing edits within 15.40 edits, the Max-Max strat\u00adegy of FIFL localizes failure-inducing \nedits within 12.42 ed\u00adits, and all strategies of FIFL with a . [0.05,0.95] are able to localize faulty \nedits within 11.08 edits. Furthermore, the Ratio-Max strategy of FIFL with a = 0.35 localizes failure\u00adinducing \nedits within 9.68 edits, indicating an average im\u00adprovement of 37.14% over FAULTTRACER. However, the \nMin-Max strategy and the Ratio-Max strategy with a = 0.00 (the ranking based on pure mutant suspiciousness) \ncannot outper\u00adform FAULTTRACER. For example, when using the SBI for\u00admula, the Min-Max strategy and the \nRatio-Max strategy with a = 0.00 (the ranking based on pure mutant suspiciousness) localize failure-inducing \nedits within 19.81 and 17.43 edits, respectively. The reason is that for some failure-inducing ed\u00adits, \naccidentally none mapped mutant can simulate their real impacts. Then the suspiciousness values for their \nmapped mutants can be quite low (even 0.00), making the Min-Max strategy and the strategy based on pure \nmutant suspicious\u00adness perform extremely worse at those cases. Third, in terms of stability, the Max-Max \nstrategy and the Ratio-Max strategies with a . [0.05,0.95] outperform FAULTTRACER. As Figures 9(a) to \n9(d) show, the box plots representing Max-Max and Ratio-Max strategies with a . [0.05,0.95] are consistently \nmore condensed than that of FAULTTRACER based on pure edit suspiciousness. To vali\u00addate this observation, \nwe also compute the Standard Devi\u00adations (SD) for each compared technique. The SDs for the Max-Max strategy \nand all Ratio-Max strategies with a from 0.05 to 0.95 are also consistently smaller than that of FAULT-TRACER \nacross all the four formulae. Fourth, for different formulae, different avalues have dif\u00adferent impacts \nfor the Ratio-Max strategy. For example, all a values perform similarly for both the Ochiai and Jaccard \nformulae. On the contrary, a values between 0.35 and 0.85 perform better than other values for the SBI \nformula, and a values between 0.15 and 0.40 perform better than other values for the Tarantula formula. \nAn interesting .nding is that even adding a little .avor of mutant suspiciousness to the edit suspiciousness \n(i.e., a=0.95) would boost the rank\u00ading based on pure edit suspiciousness (i.e., FAULTTRACER) signi.cantly. \nFor example, when using the Jaccard formula, in the median case, FAULTTRACER is able to localize faults \nwithin 9.5 edits, while the Ratio-Max strategy with a=0.95 is able to localize faults within 5.25 edits, \nthus signi.cantly reducing the burden on developers to localize faults. Finally, we also perform statistical \ntests to compare FAULTTRACER with various FIFL strategies8. For each FIFL strategy, we use its ranking \nof failure-inducing edits on dif\u00adferent version pairs as a sample data set, and compare it against the \ncorresponding sample set for FAULTTRACER. Be\u00adfore applying paired signi.cance test, we .rst apply the \nShapiro-Wilk Normality Test [44] to check the normality assumption. The results show that the differences \nbetween any FIFL strategy and FAULTTRACER do not follow normal distribution even at the 0.01 signi.cance \nlevel. Therefore, we choose to use the Wilcoxon Signed-Rank Test [49] to compare FIFL and FAULTTRACER, \nbecause it is suitable for the case that the sample differences may not be normally distributed [29]. \nTable 4 shows the detailed Wilcoxon test results. In Table 4, Column 1 shows the various FIFL strategies \ncompared against FAULTTRACER. Columns 2-5 show the p values for comparing the corresponding FIFL strategies \nwith FAULTTRACER when using the four different formulae. The Null hypothesis was rejected at the 0.01 \nsigni.cance level (i.e., p < 0.01) when comparing all FIFL Ratio-Max strate\u00adgies with a . [0.05,0.95] \nagainst FAULTTRACER, indicating that the vast majority of FIFL strategies are able to statisti\u00adcally \n(i.e., not likely to be accidentally) outperform FAULT-TRACER in localizing failure-inducing edits. The \ntable also shows that the Null hypothesis was not rejected at the 0.01 signi.cance level when comparing \nFAULTTRACER against FIFL s Min-Max strategy, Max-Max strategy, and the strat\u00adegy based on pure mutant \nsuspiciousness (i.e., Ratio-Max strategy with a value of 0.00), indicating that these three FIFL strategies \nmay not outperform FAULTTRACER consis\u00adtently. One interesting .nding is that although the FIFL Max-Max \nstrategy is able to outperform some FIFL Ratio-Max strategy with a . [0.05,0.95] in terms of average/median \n8All the statistical tests used in this paper were performed using the R language [21]. FIFL SBI (p) \nTarantula (p) Ochiai (p) Jaccard (p) Min. 0.1179 0.1179 0.6602 0.2679 Max. 0.1487 0.1147 0.1089 0.1207 \nR95 0.0006** 0.0011** 0.0011** 0.0006** R90 0.0006 ** 0.0007 ** 0.0007 ** 0.0007 ** R85 0.0006 ** 0.0011 \n** 0.0004 ** 0.0007 ** R80 0.0006** 0.0009** 0.0012** 0.0006** R75 0.0006 ** 0.0008 ** 0.0006 ** 0.0010 \n** R70 0.0007 ** 0.0008 ** 0.0006 ** 0.0011 ** R65 0.0006** 0.0008** 0.0006** 0.0013** R60 0.0006 ** \n0.0012 ** 0.0007 ** 0.0013 ** R55 0.0004 ** 0.0006 ** 0.0004 ** 0.0008 ** R50 0.0004** 0.0004** 0.0005** \n0.0009** R45 0.0004 ** 0.0004 ** 0.0006 ** 0.0008 ** R40 0.0004 ** 0.0004 ** 0.0005 ** 0.0017 ** R35 \n0.0004** 0.0004** 0.0008** 0.0010** R30 0.0013 ** 0.0004 ** 0.0009 ** 0.0035 ** R25 0.0013 ** 0.0004 \n** 0.0021 ** 0.0035 ** R20 0.0013** 0.0004** 0.0024** 0.0035** R15 0.0014 ** 0.0004 ** 0.0038 ** 0.0040 \n** R10 0.0014 ** 0.0013 ** 0.0040 ** 0.0040 ** R05 0.0014** 0.0012** 0.0045** 0.0040** R00 0.4665 0.4444 \n0.4079 0.2762 * indicates signi.cance at the 0.05 level (p<0.05) ** indicates signi.cance at the 0.01 \nlevel (p<0.01) Table 4. Wilcoxon tests for comparing FIFL techniques with FAULTTRACER performance, the \nMax-Max strategy is not able to outper\u00adform FAULTTRACER in terms of signi.cance tests while all FIFL \nRatio-Max strategies with a . [0.05,0.95] outperform FAULTTRACER. The reason is that the performance \nof the Max-Max strategy is not stable for different subjects it out\u00adperforms FAULTTRACER substantially \nfor some subjects but also performs worse than FAULTTRACER for some subjects. On the contrary, although \nsome Ratio-Max strategies with a . [0.05,0.95] cannot outperform FAULTTRACER substan\u00adtially, it outperforms \nFAULTTRACER consistently across dif\u00adferent subjects. The results demonstrate that using both edit suspiciousness \nand mutant suspiciousness for ranking each edit (i.e., FIFL s Ratio-Max strategies with a . [0.05,0.95]) \nperforms better than using either edit suspiciousness or mu\u00adtant suspiciousness for ranking each edit \n(i.e., FAULTTRACER that uses pure edit suspiciousness, FIFL Min-Max that uses the lower suspiciousness \nvalues, FIFL Max-Max that uses the higher suspiciousness values, and FIFL Ratio-Max with a = 0.00 that \nuses pure mutant suspiciousness). The reason is that both the spectrum information and the impact infor\u00admation \n(simulated by mutation testing) are useful for local\u00adizing failure-inducing edits, and thus using any \none of them for one edit may not be both precise and stable. In summary, both the descriptive statistics \nand the signif\u00adicance tests show that a vast majority of FIFL strategies are able to outperform FAULTTRACER \nsigni.cantly. Furthermore, the signi.cance tests show that using both edit suspicious\u00adness and mutant \nsuspiciousness for ranking each edit per\u00adforms better than using either edit suspiciousness or mutant \nsuspiciousness for ranking each edit, further demonstrating the motivation of the paper combining edit \nspectrum infor\u00admation and edit impact information (simulated by mutation testing) can achieve better \nfault localization results.  5.4.2 Detailed comparison between FAULTTRACER and FIFL with the default \nsettings We present the detailed comparison between FAULTTRACER and FIFL s Ratio-Max strategy with the \ndefault a of 0.50 on all the version pairs. In Table 5, Column 1 lists all the version pairs with regression \nfaults. Columns 2-4 present the average rank of faulty edits by FAULTTRACER, the average rank of faulty \nedits by FIFL, and improvement by FIFL over FAULTTRACER(%) using the SBI formula for each subject. Note \nthat we also show the improvement achieved by FIFL without mapping approximations for addition edits \n(Rules 3-9 in Figure 3) in parentheses. Similarly, Columns 5-13 present the comparison between FAULTTRACER \nand FIFL using the Tarantula, Ochiai, and Jaccard formulae. In general, using all the formulae, all FIFL \ntechniques in Table 5 are able to achieve improvements over FAULT-TRACER for the majority of the version \npairs. Also, the statis\u00adtical test in Table 4 also con.rms that all FIFL techniques in Table 5 can statistically \noutperform FAULTTRACER. For example, using the default SBI formula, FIFL outperforms FAULTTRACER by 2.33% \nto 86.26% for 16 of 26 version pairs and is only slight inferior than FAULTTRACER on one version pair \n(with an average improvement of 36.46%). The reason FIFL techniques in Table 5 outperform FAULTTRACER \nover the vast majority of the studied subjects is that those FIFL techniques use both the edit suspiciousness \nand mutant suspi\u00adciousness for ranking each edit, which provide both coverage and impact information \nfor precise fault localization. The reason FIFL techniques do not outperform FAULTTRACER on every case \nis that FAULTTRACER techniques already rank the failure-inducing edits precisely using only edit suspicious\u00adness \nfor some version pairs, and the use of mutant suspi\u00adciousness may bring some noises to the ranked list. \nThe ex\u00adperimental data supports this reasoning: for example, among the 10 version pairs where FIFL cannot \noutperform FAULT-TRACER using the SBI formula, FAULTTRACER is already able to localize failure-inducing \nedits within 5 edits for 8 version pairs, leaving little room for FIFL to improve. We also observe that \neven FIFL without mapping approx\u00adimations is also able to outperform FAULTTRACER signi.\u00adcantly. For example, \nusing the SBI formula, FIFL without mapping approximations can outperform FAULTTRACER for 14 of 26 version \npairs with an average improvement of 30.72%. For some version pairs (e.g., P14 ), FIFL without mapping \napproximations even slightly outperforms FIFL with mapping approximations. The reason is that FIFL without \nmapping approximations aggressively ignores the chance to increase the suspiciousness for addition edits \nus\u00ading mutant suspiciousness, and thus performing better for some version pairs with only faults in non-addition \nedits. However, for the majority of the version pairs, FIFL with mapping approximations performs better. \nTo further understand the performance of FIFL, we also manually analyzed why FIFL outperform or cannot \noutper\u00adform FAULTTRACER for each subject using the SBI formula. We describe the following interesting \ncases: Case 1. When XStream evolved from V1.20 to V1.21 (P11 ), 3 tests failed because the developers \nadded faulty method XStream.buildMapper() (shown in Figure 8), which is used to initialize XStream object \nand is executed by every test. Therefore, FAULTTRACER, which treats edits mainly executed by failed tests \nas more suspicious, can\u00adnot rank this AM edit high. FIFL without mapping approxi\u00admations cannot improve \nover FAULTTRACER because it can\u00adnot map the addition edit to any mutant. In contrast, FIFL with mapping \napproximations maps the edit to mutants in\u00adside changed method XStream.XStream() using Rules 1 and 9. \nSome mapped mutants failed exactly the same set of tests with the new program version because those mu\u00adtants \nmutate the old statements for building mappers inside XStream.XStream(), and therefore boost the ranking \nof the failure-inducing edit by 76.92%. Case 2. When Commons-Lang evolved from V3.02 to V3.03 (P14 ), \ntest FastDateFormatTest.testLang538 failed because the developer removed a conditional block for updating \ntime zone in method FastDateFormat.format() (shown in Figure 4). The changed method is used by 35 tests \nthat involve date format transformation. However, only 1 of them failed because the other 34 tests do \nnot check the de\u00adtailed time zone, making CM(FastDateFormat.format()) have a suspiciousness value of \nonly 0.0286 using FAULT-TRACER, and not able to be ranked high. In contrast, us\u00ading Rule 1, FIFL maps \nCM(FastDateFormat.format()) with 5 mutants (each mapped with a line in the method in V3.02), three of \nwhich are killed exactly by the failed test and thus have suspiciousness values of 1.0. In this way, \nFIFL precisely localizes the failure-inducing program edit within top 2 edits, outperforming FAULTTRACER \nby 80%. In this case, FIFL without mapping approximation can also local\u00adize the fault precisely because \nthe failure-inducing edits is not addition change. Case 3. When JMeter evolved from V1.0 to V2.0, test \ntestArgumentCreation in class ArgumentsPanel.Test and test testTreeConversion in class Save.Test failed \nbecause of one faulty edit, AM(NamePanel.updateName()). Although the suspiciousness value of the edit \nis already 1.0 using FAULTTRACER, 12 other edits also have the suspi\u00adciousness value of 1.0, making FAULTTRACER \nonly rank the failure-inducing edit within top 13 edits. In contrast, FIFL is able to localize the failure-inducing \nedit within 9 edits because FIFL re.nes the suspiciousness of each edit based  Rev. SBI Tarantula \n Ochiai Jaccard  Ft. Fi. Improvement(%) Ft. Fi. Improvement(%) Ft. Fi. Improvement(%) Ft. Fi. Improvement(%) \n P10 5.50 4.50 18.18 (0.00) 5.50 4.00 27.27 (0.00) 5.00 4.00 20.00 (0.00) 5.00 4.50 10.00 (0.00) P11 \n13.00 3.00 76.92 (0.00) 13.00 4.00 69.23 (15.38) 13.00 3.00 76.92 (0.00) 13.00 3.00 76.92 (0.00) P12 \n7.00 4.00 42.86 (42.86) 7.00 4.50 35.71 (35.71) 7.00 4.00 42.86 (42.86) 7.00 4.00 42.86 (42.86) P13 59.45 \n37.36 37.16 (22.94) 59.45 42.45 28.59 (14.37) 78.73 63.36 19.52 (18.24) 73.64 59.45 19.26 (14.44) P14 \n43.67 6.00 86.26 (93.13) 43.67 10.00 77.10 (83.21) 42.00 5.67 86.51 (92.06) 44.00 4.33 90.15 (94.70) \nP15 62.86 39.43 37.27 (43.64) 62.86 43.43 30.91 (33.64) 66.00 62.14 5.84 (7.58) 64.86 48.00 25.99 (29.52) \nP16 15.40 6.60 57.14 (36.36) 15.40 6.40 58.44 (37.66) 12.20 4.40 63.93 (34.43) 15.40 6.60 57.14 (36.36) \nP17 4.00 4.00 0.00 (0.00) 4.00 4.00 0.00 (0.00) 4.00 4.00 0.00 (0.00) 4.00 4.00 0.00 (0.00) P18 13.00 \n9.00 30.77 (15.38) 13.00 9.00 30.77 (15.38) 13.00 9.00 30.77 (15.38) 13.00 9.00 30.77 (15.38) P19 32.25 \n31.50 2.33 (2.33) 32.25 31.00 3.88 (3.10) 48.50 47.00 3.09 (2.58) 51.00 49.75 2.45 (4.41) P20 9.00 9.00 \n0.00 (0.00) 9.00 9.00 0.00 (0.00) 9.00 9.00 0.00 (0.00) 9.00 9.00 0.00 (0.00) P21 10.00 2.00 80.00 (80.00) \n10.00 2.00 80.00 (80.00) 10.00 2.00 80.00 (80.00) 10.00 2.00 80.00 (80.00) P22 2.50 2.50 0.00 (0.00) \n2.50 3.00 -20.00 (-20.00) 2.50 2.50 0.00 (0.00) 2.50 2.50 0.00 (0.00) P23 1.50 1.50 0.00 (0.00) 1.50 \n1.50 0.00 (0.00) 1.50 1.50 0.00 (0.00) 1.50 1.50 0.00 (0.00) P24 3.00 3.00 0.00 (0.00) 3.00 3.00 0.00 \n(0.00) 2.50 2.50 0.00 (0.00) 2.50 2.50 0.00 (0.00) P25 10.00 6.00 40.00 (30.00) 10.00 6.00 40.00 (30.00) \n10.00 6.00 40.00 (30.00) 10.00 6.00 40.00 (30.00) P26 3.67 4.00 -9.09 (-18.18) 3.67 3.67 0.00 (-9.09) \n2.00 2.67 -33.33 (-33.33) 3.67 3.67 0.00 (-9.09) P1 4.00 3.00 25.00 (25.00) 4.00 3.00  25.00 (25.00) \n4.00 3.00  25.00 (25.00) 4.00 3.00 25.00 (25.00) P2 1.00 1.00 0.00 (0.00) 1.00 1.00 0.00 (0.00) 1.00 \n1.00 0.00 (0.00) 1.00 1.00 0.00 (0.00) P3 1.00 1.00 0.00 (0.00) 1.00 1.00 0.00 (0.00) 1.00 1.00 0.00 \n(0.00) 1.00 1.00 0.00 (0.00)      P4 6.33 6.33 0.00 (0.00) 6.33 6.33 0.00 (0.00) 2.33 2.33 0.00 \n(0.00) 2.33 2.33 0.00 (0.00) P5 25.00 14.25 43.00 (23.00) 25.00 13.50 46.00 (23.00) 12.25 12.00 2.04 \n(4.08) 11.00 12.25 -11.36 (-9.09) P6 16.00 14.00 12.50 (25.00) 16.00 14.67 8.33 (20.83) 16.00 14.33 \n10.42 (22.92) 16.00 14.33 10.42 (22.92) P7 34.00 32.00 5.88 (2.94) 34.00 32.00 5.88 (2.94) 30.00 16.00 \n46.67 (23.33) 30.00 16.00 46.67 (23.33) P8 16.20 8.40 48.15 (48.15) 16.20 12.20 24.69 (24.69) 14.60 \n8.40 42.47 (42.47) 16.20 8.40 48.15 (48.15) P9 1.00 1.00 0.00 (0.00) 1.00 1.00 0.00 (0.00) 1.00 1.00 \n0.00 (0.00) 1.00 1.00 0.00 (0.00)         Avg 15.40 9.78 36.46 (30.72) 15.40 10.45 32.14 (26.12) \n15.74 11.22 28.67 (23.75) 15.87 10.74 32.35 (27.49) Table 5. Comparison between FAULTTRACER and default \nsettings of FIFL. on their mapping mutants, and decreases the suspiciousness values of 4 top-ranked fault-free \nedits. Case 4. When Mime4J evolved from V0.61 to V0.70, 3 tests failed because of 4 failure-inducing \nprogram ed\u00adits. FAULTTRACER and FIFL perform similarly on 2 failure\u00adinducing edits. The other 2 failure-inducing \nedits are CM and AM edits on constructors of MimeBoundaryInputStream class. Because the 2 failure-inducing \nedits are constructor edits, they are executed by almost all tests, making FAULT-TRACER only localize \nthem within 25 edits. In contrast, FIFL maps the two constructor edits with mutants using Rule 1 and \nRule 3, respectively. Some mapped mutants have the suspiciousness value of 1.0 because they are only \nkilled by one failed test, making FIFL able to localize the two failure\u00adinducing edits within top 2 edits. \nCase 5. When Xml-Security evolved from V1.0 to V2.0 (P10 ), there are two failure-inducing edits: AM \non method engineCanonicalizeXPathNodeSet() inside the class CanonicalizerBase, and CM on circumventBug2650() \nin class XMLUtils. Using Rule 7, FIFL .nds that mutants in the deleted method engineCanonicalizeXPathNodeSet() \nof class Canonicalizer20010315 are able to simulate the impacts of the newly added method. The mapped \nmutants increase the rank of the failure-inducing AM edit by 1. In addition, AF(CanonicalizerBase. includeComments) \ncan also be mapped with mutants inside a deleted method by applying Rules 7 and 9. Then, AF edit s rank \nwas lowered correspondingly, making the rank of the failure-inducing CM edit, CM(XMLUtils.circumventBug2650()), \nincreased by 1. Therefore, the average ranking for two failure-inducing edits is improved by 18.18%. \n Case 6. The evolution from Joda-Time1.20 to 1.30 (P26 ) is the only case where FAULTTRACER outperforms \nFIFL using SBI. The reason is that one failure-inducing edit, CSFI(GregorianChronology.MAX YEAR), does \nnot have mapped mutants, and another fault-free edit, CM(BasicChro\u00adnology.getYear()), has mapped mutants \nthat acciden\u00adtally share some failed tests with the new version. In summary, the Ratio-Max strategy of \nFIFL with default setting is able to outperform FAULTTRACER signi.cantly. For example, even the default \nsetting of FIFL with SBI formula outperforms FAULTTRACER by 2.33% to 86.26% on 16 of 26 studied version \npairs, and is only inferior than FAULTTRACER on one version pair.    5.4.3 Discussions Although automated \nfault localization approaches [1, 16, 23, 28, 38, 52 54, 57, 65] have been intensively studied for more \nthan a decade, there are common limitations for them. For example, Parnin and Orso [37] recently argued \nthat exist\u00ading fault localization approaches rely on a strong assump\u00adtion that examining a potential \nfaulty statement in isolation is enough to localize and .x a fault. They performed a case study showing \nthat a traditional fault localization technique (which ranks all program statements to localize faults) \ndoes not help the developer much with localizing faults. The study shows that traditional fault localization \nat the statement gran\u00adularity can be painful because (1) it may cause the developer to inspect a extremely \nlong ranked list for large program and (2) developers tend to also inspect the context of each state\u00adment \nother than the statement itself. Therefore, the study sug\u00adgested that fault localization at the method \nor .le granulari\u00ad  R50 Rank all edits Rank edits per test Formula Ft. Fi. Impr. Ft. Fi. Impr. SBI 15.40 \n9.78 36.46% 10.12 5.83 42.44% Tarantula 15.40 10.45 32.14% 10.12 6.10 39.70% Ochiai 15.74 11.22 28.67% \n10.43 6.21 40.48% Jaccard 15.87 10.74 32.35% 10.80 6.00 44.47% Table 6. Summary results when using the \ndefault R50 strat\u00adegy to rank all edits and rank edits for each failed test ties may be a promising direction \nfor fault localization, be\u00adcause those granularities provide a shorter candidate list and provide enough \ncontext information for each ranked entity. Although our FIFL approach may share the same limitations \nwith traditional fault localization, FIFL makes attempts to ad\u00address the limitations of traditional fault \nlocalization. For ex\u00adample, FIFL focuses on program edits during software evolu\u00adtion, which provides \na much shorter ranked list than ranking all program statements. In addition, FIFL extract program edits \nat the method/.eld level, which provides the developer enough context information for reasoning each \nranked entity. There is also another intrinsic limitation for the spectrum\u00adbased fault localization approaches \nthat FIFL is based on [1, 23, 28, 52, 57] they only use the correlation between the coverage of program \nelements with test pass/fail results to localize faults. However, there is a gap between the cover\u00adage \nand the actual impact of program elements to the test pass/fail results. In this paper, we make an attempt \nto bridge the gap by using the simulated impact information via muta\u00adtion testing. However, the impact \ninformation simulated by mutation testing may be still not precise enough. In addi\u00adtion, some edits may \nnot even have mapped mutants due to various reasons, e.g., the mutation operators do not support the \nspeci.c statement pattern. In the future, we hope more research efforts can be put into this area to \nbridge the gap between program coverage information and actual impact in\u00adformation using more advanced \ntechniques. Our experimental evaluation also has limitations. In this paper, we used FAULTTRACER and \nFIFL to directly rank all the edits once for all failed tests. This corresponds to the de\u00adbugging process \nthat the developer iterates over all the edits to .nd all the potential faults for the failed program \nversion. However, some deveopers may prefer to inspect related edits for each failed test to .x failed \ntests one by one. Therefore, we also used FAULTTRACER and FIFL to rank edits related to each failed test \n(i.e., only ranking the edits that are affecting changes of each failed test based on their suspiciousness). \nIn this case, we would have a ranked list of edits for each failed test. For each subject, we collected \nthe average rank of each failure-inducing edit on each failed test. Table 6 shows the average summary \nresults across all subjects for FAULT-TRACER and FIFL s default strategy when ranking all edits together \nand when ranking edits for each failed test. In the ta\u00adble, Column 1 presents the four formulae. Columns \n2-4 show the average rank of failure-inducing edits by FAULTTRACER and FIFL when ranking all edits together, \nand the improve\u00adment of FIFL over FAULTTRACER. Similarly, Columns 5-7 present the comparison between \nFIFL and FAULTTRACER when ranking edits for each failed test. We can observe that FIFL outperforms FAULTTRACER \neven more when ranking edits for each failed test. For example, when using the de\u00adfault strategy with \nthe SBI formula, FIFL is able to localize failure-inducing edits within 5.83 edits for each failed test, \nindicating an improvement of more than 40% over FAULT-TRACER. Last but not least, Murphy-Hill et al. \n[33] recently pre\u00adsented an interesting study showing a suite of factors that can cause a program fault \nto be .xed in different ways at different circumstances or time points. One such factor is the development \nphase of the project. For example , when .xing a fault at an earlier phase, developers may choose to \n.x the root cause of the fault so that if a risk raises, they would have a longer period to compensate. \nOn the contrary, when .xing a fault at a later phase, developers may choose to make a walk-around which \nwould be least disruptive . The .ndings in this study raises serious questions for tra\u00additional bug prediction \n[24, 66] or fault localization tech\u00adniques [1, 23, 28, 52, 57], because faults can actually be .xed in \ndifferent ways and locations rather than the root causes. The effectiveness evaluation of FIFL may also \nbe in.uenced, because we only evaluate FIFL in localizing the root cause of failure-inducing edits, whereas \nthe developer may choose to .x the faults in different ways at different program locations. However, \nwe believe that FIFL may still be useful for devel\u00adopers even when they .nally decide not to .x the faults \nat the root-cause locations, because fully understanding of the fault root cause is still preferred no \nmatter where the faults are .nally .xed (also con.rmed by the same study [33]).  5.5 Threats to Validity \nThreats to internal validity are concerned with uncontrolled factors that may also be responsible for \nthe results. In this paper, the main threat to internal validity is the possible faults in the implementation \nof the compared techniques. To reduce this threat, we built FIFL on top of state-of-the\u00adart tools [42, \n57], and implemented FIFL using well-known libraries such as Eclipse JDT toolkit and ASM bytecode manipulation \nframework. We also reviewed all the code that we produced to assure its correctness. The .rst author, \nwith Java programming experience for eight years, isolated the failure-inducing edits manually. We have \nalso reviewed all outputs produced by FIFL manually to ensure correctness. However, because this inspection \nwas done manually, there is still a risk of introducing subjectivity and errors. Threats to external \nvalidity are concerned with whether the .ndings in our study are generalizable for other situa\u00adtions. \nTo mitigate threats to external validity, we used all re\u00adleased versions of nine medium sized open source \nprojects from various application areas. In addition, as our work is re\u00adlated to both regression testing \nand mutation testing, we also ensure that the selected subjects have been used for regres\u00adsion testing \nor mutation testing research [6, 7, 10, 31, 42, 43, 57 59, 61, 63]. However, they still might be not \nrepresenta\u00adtive for all the possible subject programs.  Threats to construct validity are concerned \nwith whether the measurement in our study re.ects real-world situations. To mitigate threats to construct \nvalidity, we measured the ranking of failure-inducing edits, which denotes the num\u00adber of edits that \nthe developer need to manually inspect be\u00adfore .nding the fault. Furthermore, we also compare FIFL with \nthe existing technique for localizing failure-inducing edits (FAULTTRACER [57]) in the same experimental \nsetting. The ranking of failure-inducing program elements has been widely used in the fault localization \nresearch area [1, 23, 37, 39, 40, 52, 57]. However, the ranking of failure-inducing pro\u00adgram elements \nmay still not correlate with the actual costs in inspecting those elements. To further reduce this threat, \nin\u00adspired by user case studies in other areas [25, 30], rigorous and well-designed studies for investigating \nthe correlation between the ranking of faulty elements and actual fault local\u00adization costs should be \nperformed in future work. In addition, as recent work has shown that it is suitable to use program re\u00adpair \ntechniques to evaluate fault localization techniques fully automatically [38], we also plan to use automated \nprogram repair techniques [27, 48] to further demonstrate the effec\u00adtiveness of FIFL. 6. Related Work \nAs our approach aims to localize failure-inducing program edits precisely, we mainly discuss the existing \nefforts for this purpose in this section. We also show the related work in tra\u00additional fault localization, \nwhich aims to localize faulty pro\u00adgram elements for a speci.c program version. In addition, as our approach \nutilizes the mutation testing methodology for improving failure-inducing edit localization, we also brie.y \ndiscuss recent trends on mutation testing and its applications. 6.1 Localizing Failure-Inducing Edits \nMany research efforts have been dedicated to localizing failure-inducing program edits. Change impact \nanalysis is a well-known methodology for determining affecting changes, i.e., a subset of program edits \nthat might have caused test fail\u00adures, for each failed test. It has been shown that the number of affecting \nchanges for each failed test can still be large in number [57]. Therefore, various techniques have been \npro\u00adposed to localize failure-inducing changes more precisely. Crisp provided tool supports for manually \nisolating failure\u00adinducing program edits [5]. Stoerzer et al. [45] proposed to use change classi.cation \ntechniques to .nd failure-inducing changes. However, those techniques do not rank changes and the number \nof classi.ed changes can still be large in number. Ren et al. [39] proposed the .rst ranking heuristic \nfor local\u00adizing failure-inducing edits. However, the ranking heuristic is only based on calling structures \nof failed tests, and can only rank method-level edits. Recent work [2, 57, 62] (e.g. FAULTTRACER) introduced \nspectrum-based fault localiza\u00adtion methodology to localize failure-inducing edits. FAULT-TRACER can rank \nall types of edits, and has been shown to outperform Ren et al. s ranking heuristic by more than 50%. \nHowever, some edits ranked as top by FAULTTRACER might just because they are accidentally executed by \nthe failed tests. In fact, they might not be responsible for test failures. A natural idea to further \nre.ne the fault localization re\u00adsults would be iteratively applying subsets of program edits to localize \nthe failure-inducing edits precisely. However, due to the limitations for applying edits automatically \n(details are shown in Section 2), existing techniques for localizing failure-inducing edits usually recommend \nmanually apply\u00ading changes after ranking the edits [5, 39, 57]. In contrast, FIFL directly uses the existing \nmutation testing results of the old program version to boost the fault localization re\u00adsults while avoiding \nthe limitations of iteratively applying subsets of program edits. Note that similar with existing tech\u00adniques \n[5, 39, 57], FIFL can also be combined with manually applying program edits to further facilitate fault \nlocalization. Delta Debugging [32, 51, 53, 54] is another methodol\u00adogy for localizing failure-inducing \nedits. Delta Debugging iteratively applies a subset of all changes to construct in\u00adtermediate versions \nto .nd a minimum set of changes that lead to a test failure. However, Delta Debugging considers all changes \nas the candidate set without considering compila\u00adtion dependences among those changes, and needs costly \ntest execution for validating various combinations of program changes. Also, Delta Debugging does not \nwork for multiple thread programs as shown by previous work [46] because it always assumes that tests \ngive deterministic results. Fur\u00adthermore, Delta Debugging does not rank edits, leaving it to a programmer \nto sort out a real culprit of a regression test failure. To the best of our knowledge, our FIFL approach \nis the .rst approach to localizing failure-inducing edits based on mutation testing. FIFL uni.es two \ndimensions of changes to consider the potential impact of each program edit. The consideration of both \nspectra and impacts of program edits makes FIFL more precise than the existing techniques.  6.2 Traditional \nFault Localization Traditional fault localization techniques [1, 15, 17, 22, 23, 28, 36, 52, 64] aims \nto localize faults among all the elements of a speci.c program version, and may suffer from scalabil\u00adity \nissues when applied to large evolving software systems, since they do not leverage edits between the \nold and new versions. A number of spectrum-based techniques [1, 15, 23, 28, 52] have been proposed to \nlocalize faults by analyzing the correlation between test fail/pass results and test cover\u00adage information. \nDue to the imprecision of the spectrum\u00adbased techniques, Zhang et al. [64] proposed the predicate switching \napproach, which forces critical predicates to take an alternate branch at runtime. By examining the switched \npredicate that caused a failed test to pass, the cause of the bug then may be identi.ed. Jeffrey et al. \n[22] further pro\u00adposed the value replacement approach, which subsumes the predicate switching approach \nand forces variables at each program location to take different values at runtime to local\u00adize potential \nfaulty statements. Zhang et al. [63] then pro\u00adposed to replace test objects using randomly constructed \nob\u00adject pool for program test code to explain the potential causes of test failures. A recent work by \nPapadakis et al. [36] is closely related to our work. Their work to our knowledge is the .rst to utilize \nmutation testing to facilitate traditional fault localization. Our FIFL differs from their work in three \nkey ways. First, their work only considers mutation changes and aims to localize faults for one speci.c \nversion, whereas FIFL considers two dimensions of changes, including mu\u00adtation changes and programmer \nedits, and aims to localize faulty edits during software evolution. Second, our approach is different: \ntheir approach uses mutation testing as a cov\u00aderage criterion, whereas FIFL uses mutation testing to \nsim\u00adulate the impact of program edits. Third, their technique is not applicable to tests with assertions, \nwhich are widely used in real-world systems. The reason is that they directly apply mutation testing \non the faulty program with failed tests and the set of mutants killed by already failed tests cannot \nbe de\u00adtermined. In contrast, FIFL applies to more general forms of tests because FIFL applies mutation \ntesting on the old ver\u00adsion where all tests pass.  6.3 Mutation Testing Mutation testing, .rst proposed \nby DeMillo [9] and Ham\u00adlet [14], is a fault-based testing methodology. Mutation test\u00ading has been shown \nto be effective in indicating the quality of test suites, and is raising more and more attention from \nboth the industry and the academia. As mutation testing is widely recognized as a heavy-weight testing \nmethodology, many researchers aim to reduce the cost of mutation testing. Selective mutation testing \ntechniques [13, 34, 55] select a representative subset of all mutants and ensure that the se\u00adlected set \nof mutants achieves almost the same results as the whole mutant set. Howden [20] proposed weak mutation, \nwhich checks whether the test could result in a different in\u00adternal state in the mutant from that in \nthe original program rather than check the .nal results. Untch et al. [47] proposed schema-based mutation, \nwhich transforms all mutants into one meta-mutant that can be compiled by a standard com\u00adpiler. Researchers \nhave also investigated the use of parallel processing (e.g., SIMD [26]) to speed up mutation testing. \nRecently, Zhang et al. [58, 60] proposed techniques inspired by regression testing to further reduce \nmutation testing cost. Mutation testing has also been traditionally used for gen\u00aderating high-quality \ntest suites. DeMillo and Offutt [8] pro\u00adposed constraint based testing (CBT), which uses control\u00ad.ow \nanalysis and symbolic evaluation to generate tests each killing one mutant. Offutt et al. [35] further \nproposed dy\u00adnamic domain reduction (DDR) to address some limitations of CBT. Fraser and Zeller [12] proposed \nto use search based software testing (SBST) to generate tests each killing one mutant. Zhang et al. [56] \nproposed to use dynamic symbolic execution (DSE) to generate tests each killing one mutant. Harman et \nal. [18] proposed to combine DSE and SBST to generate tests each killing multiple mutants. Different \nfrom traditional applications of mutation test\u00ading, our FIFL uni.es two dimensions of changes mutation \nchanges and developer edits to localize failure-inducing program edits more precisely. 7. Conclusion \nThis paper uni.es two widely studied dimensions of soft\u00adware changes mutation changes [4, 9, 12, 14, \n18, 20, 34, 42, 43, 56] and developer edits [5, 39, 40, 45, 50, 51, 57] and presents our methodology \nof fault injection for fault localization (FIFL). Our insight is that the essence of failure\u00adinducing \nedits made by the developer can be captured using mechanical program transformations (i.e., mutation \nchanges in this paper). Speci.cally, mutants of the old program ver\u00adsion, which have similar test execution \nresults as the new ver\u00adsion, are likely to share the same locations with some failure\u00adinducing edits. \nAs an optimization to reduce the cost of FIFL, we also present edit-oriented mutation testing, which \nonly executes the subset of mutants mapped with program edits when the mutation testing results for the \nold version are not available when applying FIFL. We performed an empirical study on the real-world versions \nof nine open-source Java projects. The empirical results show that the FIFL approach achieves considerable \nimprovement (i.e., even up to more than 80% for some subjects under test) over state-of-the-art FAULTTRACER \napproach. The uni.ed view of mutation changes and developer edits is not limited to the fault localization \narea. We believe it is also applicable to other key software testing areas. For example, mutation testing \nresults for the old program version can optimize test selection [19] and prioritization [41] for the \nnew version, because the potential impact of program edits can be simulated by existing mutants. We plan \nto establish these connections in future work. Acknowledgments We thank the anonymous reviewers for the \nvaluable reviews and suggestions that helped improving the paper a lot dur\u00ading the second-phase revision. \nThis material is based upon work partially supported by the US National Science Foun\u00addation under Grant \nNo. CCF-0845628. Lu Zhang s work is sponsored by the National 973 Program of China No. 2011CB302604, \nthe National 863 Program of China No. 2012AA011202, the Science Fund for Creative Research Groups of \nChina No. 61121063, and the National Natural Science Foundation of China under Grant Nos. 91118004, 61225007, \n61228203.  References [1] R. Abreu, P. Zoeteweij, and A. J. Van Gemund. On the accuracy of spectrum-based \nfault localization. In Testing: Academic and Industrial Conference Practice and Research Techniques-MUTATION, \npages 89 98, 2007. [2] E. Alves, M. Gligoric, V. Jagannath, and M. d Amorim. Fault\u00adlocalization using \ndynamic slicing and change impact analysis. In Proc. of ASE, pages 520 523, 2011. [3] P. Ammann and J. \nOffutt. Introduction to software testing. Cambridge University Press, 2008. [4] T. A. Budd, R. A. DeMillo, \nR. J. Lipton, and F. G. Sayward. Theoretical and empirical studies on using program mutation to test \nthe functional correctness of programs. In Proc. of POPL, pages 220 233, 1980. [5] O. Chesley, X. Ren, \nB. Ryder, and F. Tip. Crisp A Fault Localization Tool for Java Programs. In Proc. of ICSE, pages 775 \n779, 2007. [6] B. Daniel, V. Jagannath, D. Dig, and D. Marinov. Reassert: Suggesting repairs for broken \nunit tests. In Proc. of ASE, pages 433 444, 2009. [7] B. Daniel, T. Gvero, and D. Marinov. On test repair \nusing symbolic execution. In Proc. of ISSTA, pages 207 218, 2010. [8] R. DeMillo and A. Offutt. Constraint-based \nautomatic test data generation. IEEE Transactions on Software Engineering, 17(9):900 910, 1991. [9] R. \nDeMillo, R. Lipton, and F. Sayward. Hints on test data selection: Help for the practicing programmer. \nComputer, 11 (4):34 41, 1978. [10] H. Do, S. Elbaum, and G. Rothermel. Supporting controlled experimentation \nwith testing techniques: An infrastructure and its potential impact. Empirical Software Engineering, \n10 (4):405 435, 2005. ISSN 1382-3256. [11] P. G. Frankl, S. N. Weiss, and C. Hu. All-uses vs mutation \ntesting: an experimental comparison of effectiveness. Journal of Systems and Software, 38(3):235 253, \n1997. [12] G. Fraser and A. Zeller. Mutation-driven generation of unit tests and oracles. In Proc. of \nISSTA, pages 147 158, 2010. [13] M. Gligoric, L. Zhang, C. Pereira, and G. Pokam. Selective mutation \ntesting for concurrent code. In Proc. of ISSTA, pages 224 234, 2013. [14] R. Hamlet. Testing programs \nwith the aid of a compiler. IEEE Transactions on Software Engineering, (4):279 290, 1977. [15] D. Hao, \nL. Zhang, Y. Pan, H. Mei, and J. Sun. On similarity\u00adawareness in testing-based fault localization. Automated \nSoft\u00adware Engineering, 15(2):207 249, 2008. [16] D. Hao, L. Zhang, T. Xie, H. Mei, and J.-S. Sun. Interactive \nfault localization using test information. Journal of Computer Science and Technology, 24(5):962 974, \n2009. [17] D. Hao, T. Xie, L. Zhang, X. Wang, J. Sun, and H. Mei. Test input reduction for result inspection \nto facilitate fault localization. Automated Software Engineering, 17(1):5 31, 2010. [18] M. Harman, Y. \nJia, and W. Langdon. Strong higher order mutation-based test data generation. In Proc. of FSE, pages \n212 222, 2011. [19] M. J. Harrold, J. A. Jones, T. Li, D. Liang, A. Orso, M. Pen\u00adnings, S. Sinha, S. \nA. Spoon, and A. Gujarathi. Regression test selection for Java software. In Proc. of OOPSLA, pages 312 \n326, 2001. [20] W. Howden. Weak mutation testing and completeness of test sets. IEEE Transactions on \nSoftware Engineering, (4):371 379, 1982. [21] R. Ihaka and R. Gentleman. R: A language for data analysis \nand graphics. Journal of computational and graphical statis\u00adtics, 5(3):299 314, 1996. [22] D. Jeffrey, \nN. Gupta, and R. Gupta. Fault localization using value replacement. In Proc. of ISSTA, pages 167 178, \n2008. [23] J. Jones, M. Harrold, and J. Stasko. Visualization of test information to assist fault localization. \nIn Proc. of ICSE, page 477, 2002. [24] S. Kim, E. J. Whitehead, and Y. Zhang. Classifying software changes: \nClean or buggy? IEEE TSE, 34(2):181 196, 2008. [25] A. J. Ko, B. A. Myers, M. J. Coblenz, and H. H. Aung. \nAn exploratory study of how developers seek, relate, and collect relevant information during software \nmaintenance tasks. IEEE TSE, 32(12):971 987, 2006. [26] E. Krauser, A. Mathur, and V. Rego. High performance \nsoft\u00adware testing on simd machines. IEEE Transactions on Soft\u00adware Engineering, 17(5):403 423, 1991. \n[27] C. Le Goues, M. Dewey-Vogt, S. Forrest, and W. Weimer. A systematic study of automated program repair: \nFixing 55 out of 105 bugs for $8 each. In Proc. of ICSE, pages 3 13, 2012. [28] B. Liblit, M. Naik, A. \nZheng, A. Aiken, and M. Jordan. Scal\u00adable statistical bug isolation. In Proc. of PLDI, pages 15 26, 2005. \nISBN 1595930566. [29] R. Lowry. Concepts and applications of inferential statistics. R. Lowry, 1998. \n\u00b4 An empirical study of the in.uence of static type systems on the usability of undocumented software. \nIn Proc. of OOPSLA, pages 683 702, 2012. [30] C. Mayer, S. Hanenberg, R. Robbes, E. Tanter, and A. Ste.k. \n[31] H. Mei, D. Hao, L. Zhang, L. Zhang, J. Zhou, and G. Rother\u00admel. A static approach to prioritizing \njunit test cases. TSE, 38 (6):1258 1275, 2012. [32] G. Misherghi and Z. Su. Hdd: hierarchical delta debugging. \nIn Proc. of ICSE, pages 142 151, 2006. [33] E. Murphy-Hill, T. Zimmermann, C. Bird, and N. Nagappan. \nThe design of bug .xes. In Proc. of ICSE, pages 332 341, 2013. [34] A. Offutt, A. Lee, G. Rothermel, \nR. Untch, and C. Zapf. An experimental determination of suf.cient mutant operators. ACM Transactions \non Software Engineering and Methodology (TOSEM), 5(2):99 118, 1996. [35] A. Offutt, Z. Jin, and J. Pan. \nThe dynamic domain reduction procedure for test data generation. Software-Practice and Experience, 29(2):167 \n194, 1999. [36] M. Papadakis and Y. L. Traon. Using mutants to locate unknown faults. In Proc. of ICST \nWorkshop on Mutation Analysis, pages 691 700, 2012.  [37] C. Parnin and A. Orso. Are automated debugging \ntechniques actually helping programmers? In Proc. of ISSTA, pages 199 209, 2011. [38] Y. Qi, X. Mao, \nY. Lei, and C. Wang. Using automated program repair for evaluating the effectiveness of fault localization \ntechniques. In Proc. of ISSTA, pages 191 201, 2013. [39] X. Ren and B. Ryder. Heuristic ranking of Java \nprogram edits for fault localization. In Proc. of ISSTA, pages 239 249, 2007. [40] X. Ren, F. Shah, F. \nTip, B. Ryder, and O. Chesley. Chianti: A tool for change impact analysis of Java programs. In Proc. \nof OOPSLA, 2004. [41] G. Rothermel, R. H. Untch, C. Chu, and M. J. Harrold. Prior\u00aditizing test cases \nfor regression testing. IEEE Transactions on Software Engineering, 27(10):929 948, 2001. [42] D. Schuler \nand A. Zeller. Javalanche: Ef.cient mutation testing for Java. In Proc. of FSE, pages 297 298, 2009. \n[43] D. Schuler, V. Dallmeier, and A. Zeller. Ef.cient mutation testing by checking invariant violations. \nIn Proc. of ISSTA, pages 69 80, 2009. [44] S. S. Shapiro and M. B. Wilk. An analysis of variance test \nfor normality (complete samples). Biometrika, 52(3/4):591 611, 1965. [45] M. Stoerzer, B. Ryder, X. Ren, \nand F. Tip. Finding failure\u00adinducing changes in Java programs using change classi.ca\u00adtion. In Proc. of \nFSE, pages 57 68, 2006. [46] R. Tzoref, S. Ur, and E. Yom-Tov. Instrumenting where it hurts: an automatic \nconcurrent debugging technique. In Proc. of ISSTA, pages 27 38, 2007. [47] R. Untch, A. Offutt, and M. \nHarrold. Mutation analysis using mutant schemata. In ACM SIGSOFT Software Engineering Notes, volume 18, \npages 139 148, 1993. [48] W. Weimer, T. Nguyen, C. Le Goues, and S. Forrest. Automat\u00adically .nding patches \nusing genetic programming. In Proc. of ICSE, pages 364 374, 2009. [49] F. Wilcoxon. Individual comparisons \nby ranking methods. Biometrics bulletin, 1(6):80 83, 1945. [50] G. Xu and A. Rountev. Regression test \nselection for aspectj software. In Proc. of ICSE, pages 65 74, 2007. [51] K. Yu, M. Lin, J. Chen, and \nX. Zhang. Practical isolation of failure-inducing changes for debugging regression faults. In Proc. of \nASE, pages 20 29, 2012. [52] Y. Yu, J. Jones, and M. Harrold. An empirical study of the effects of test-suite \nreduction on fault localization. In Proc. of ICSE, pages 201 210, 2008. [53] A. Zeller. Yesterday, my \nprogram worked. today, it does not. why? In Proc. of FSE, pages 253 267, 1999. [54] A. Zeller. Automated \ndebugging: Are we close? Computer, 34(11):26 31, 2001. [55] L. Zhang, S.-S. Hou, J.-J. Hu, T. Xie, and \nH. Mei. Is operator\u00adbased mutant selection superior to random mutant selection? In Proc. of ICSE, pages \n435 444, 2010. [56] L. Zhang, T. Xie, L. Zhang, N. Tillmann, J. De Halleux, and H. Mei. Test generation \nvia dynamic symbolic execution for mutation testing. In Proc. of ICSM, pages 1 10, 2010. [57] L. Zhang, \nM. Kim, and S. Khurshid. Localizing failure\u00adinducing program edits based on spectrum information. In \nProc. of ICSM, pages 23 32, 2011. [58] L. Zhang, D. Marinov, L. Zhang, and S. Khurshid. Regression mutation \ntesting. In Proc. of ISSTA, pages 331 341, 2012. [59] L. Zhang, D. Hao, L. Zhang, G. Rothermel, and H. \nMei. Bridging the gap between the total and additional test-case prioritization strategies. In Proc. \nICSE, pages 192 201, 2013. [60] L. Zhang, D. Marinov, and S. Khurshid. Faster mutation testing inspired \nby test prioritization and reduction. In Proc. of ISSTA, pages 235 245, 2013. [61] S. Zhang. Practical \nsemantic test simpli.cation. In Proc. of ICSE, pages 1173 1176, 2013. [62] S. Zhang, Y. Lin, Z. Gu, and \nJ. Zhao. Effective identi.cation of failure-inducing changes: a hybrid approach. In Proc. of PASTE, pages \n77 83, 2008. [63] S. Zhang, C. Zhang, and M. D. Ernst. Automated documenta\u00adtion inference to explain \nfailed tests. In Proc. of ASE, pages 63 72, 2011. [64] X. Zhang, N. Gupta, and R. Gupta. Locating faults \nthrough automated predicate switching. In Proc. of ICSE, pages 272 281, 2006. [65] X. Zhang, N. Gupta, \nand R. Gupta. Pruning dynamic slices with con.dence. In Proc. of PLDI, pages 169 180, 2006. [66] T. Zimmermann, \nN. Nagappan, P. J. Guo, and B. Murphy. Characterizing and predicting which bugs get reopened. In Proc. \nof ICSE, pages 1074 1083. IEEE, 2012.   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>This paper presents a novel methodology for localizing faults in code as it evolves. Our insight is that the essence of failure-inducing edits made by the developer can be captured using mechanical program transformations (e.g., mutation changes). Based on the insight, we present the FIFL framework, which uses both the spectrum information of edits (obtained using the existing <i>FaultTracer</i> approach) as well as the potential impacts of edits (simulated by mutation changes) to achieve more accurate fault localization. We evaluate FIFL on real-world repositories of nine Java projects ranging from 5.7KLoC to 88.8KLoC. The experimental results show that FIFL is able to outperform the state-of-the-art <i>FaultTracer</i> technique for localizing failure-inducing program edits significantly. For example, all 19 FIFL strategies that use both the spectrum information and simulated impact information for each edit outperform the existing <i>FaultTracer</i> approach statistically at the significance level of 0.01. In addition, FIFL with its default settings outperforms <i>FaultTracer</i> by 2.33% to 86.26% on 16 of the 26 studied version pairs, and is only inferior than <i>FaultTracer</i> on one version pair.</p>", "authors": [{"name": "Lingming Zhang", "author_profile_id": "81435592964", "affiliation": "University of Texas, Austin, Austin, TX, USA", "person_id": "P4290460", "email_address": "zhanglm@utexas.edu", "orcid_id": ""}, {"name": "Lu Zhang", "author_profile_id": "81100121502", "affiliation": "Peking University, MoE, Beijing, China", "person_id": "P4290461", "email_address": "zhanglu@sei.pku.edu.cn", "orcid_id": ""}, {"name": "Sarfraz Khurshid", "author_profile_id": "81100052115", "affiliation": "University of Texas, Austin, Austin, TX, USA", "person_id": "P4290462", "email_address": "khurshid@ece.utexas.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509551", "year": "2013", "article_id": "2509551", "conference": "OOPSLA", "title": "Injecting mechanical faults to localize developer faults for evolving software", "url": "http://dl.acm.org/citation.cfm?id=2509551"}