{"article_publication_date": "10-29-2013", "fulltext": "\n Storage Strategies for Collections in Dynamically Typed Languages Carl Friedrich Bolz Lukas Diekmann \nLaurence Tratt University of D \u00a8University of D \u00a8King s College London usseldorf usseldorf Hasso Plattner \nInstitute Potsdam King s College London http://tratt.net/laurie/ King s College London http://lukasdiekmann.com/ \nhttp://cfbolz.de/ Abstract Dynamically typed language implementations often use more memory and execute \nslower than their statically typed cousins, in part because operations on collections of elements are \nunoptimised. This paper describes storage strategies, which dynamically optimise collections whose elements \nare instances of the same primitive type. We implement storage strategies in the PyPy virtual machine, \ngiving a performance increase of 18% on wide-ranging benchmarks of real Python programs. We show that \nstorage strategies are simple to imple\u00adment, needing only 1500LoC in PyPy, and have applicability to \na wide range of virtual machines. Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors \nrun-time environments, code gen\u00aderation, incremental compilers, interpreters General Terms Algorithms, \nLanguages, Performance Keywords Implementation, collection types, memory opti\u00admization, dynamic typing \n1. Introduction Implemented naively, dynamically typed languages tend to have poor performance relative \nto statically typed lan\u00adguages [35]. The .exibility and dynamism of dynamically typed languages frustrates \nmost traditional static optimisa\u00adtions. Just-In-Time (JIT) compilers defer optimisations until run-time, \nwhen the types of objects at speci.c points in a pro\u00adgram can be identi.ed, and specialised code can \nbe generated. In particular, variables which reference common types such Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. Copyrights for components of this work owned by others than the \nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to \npost on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request \npermissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright \nis held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2374-1/13/10. . . \n$15.00. http://dx.doi.org/10.1145/2509136.2509531 as integers can be unboxed [8, 24]: rather than being \nrefer\u00ad ences to an object in the heap, they are stored directly where they are used. This lowers memory \nconsumption, improves cache locality, and reduces the overhead on the garbage col\u00adlector. Unboxing is \nan important technique in optimising such languages. However, JITs do not directly in.uence how objects \nare laid out in the heap. Therefore as soon as objects are added to a collection (e.g. lists, dictionaries), \nvirtual machines (VMs) such as PyPy have to revert to boxing objects. This is necessary since collections \nin dynamically typed languages can store objects of multiple types. We call collections which do so heterogeneously \ntyped. Pointer tagging [21] where the spare bits in an aligned pointer are used to represent primitive \ndata-types such as integers can partially alleviate this issue. However, tagging has few bits to play \nwith and some primitive types (e.g. strings) are not easily tagged. Furthermore, tagging is complex to \nimplement, and has hard to predict performance trade-offs (e.g. due to branch prediction). For reasons \nsuch as this, VMs such as HotSpot and PyPy forego tagging. Dynamically typed languages therefore pay \na signi.cant performance penalty for the possibility that collections may store heterogeneously typed \nelements, even for programs which create no such collections. Statically typed languages can determine \nef.cient storage representations of collections storing elements of a primitive type based on a collection \ns static types. The underlying aim of this work is to .nd a simple means of achieving a similar effect \nin dynamically typed languages. Ideally, we want an approach which is simpler to implement, understand, \nand debug than tagging, and which, unlike tagging, is generalisable to an arbitrary number of primitive \ntypes. This paper presents storage strategies, a novel approach for optimising the representation of \nhomogeneously typed collections (e.g. a list whose elements are all strings) in VMs for dynamically typed \nlanguages, even for those types that cannot sensibly be represented by pointer tagging. In essence, each \ncollection has a single storage strategy, though the storage strategy may change throughout the collection \ns lifetime. When elements are added to an empty collection, and when those elements are homogeneously \ntyped, an optimised storage strategy is used for the collection. When collections are heterogeneously \ntyped, or when an optimised storage strategy is not available, a fallback storage strategy is used, which \nworks as a traditional dynamically typed VM does. Similarly, if an element of a different type is added \nto a previously homogeneously typed collection forcing the collection to type dehomogenise the collection \ns storage strategy is changed to a generic one, and its elements representation changed. Fortunately, \nand as is common in dynamically typed languages [9, 23, 31], this wide-ranging dynamicity is used infrequently \nin practise. Similarly, the storage strategy is reset when a collection is emptied of elements. Though \noptimised storage strategies are free to operate as they please, they customarily unbox elements and \nstore them alongside one another. Importantly, all this happens without any input from the end programmer \nand without observably changing behaviour.  In order to validate our approach, we modi.ed the PyPy VM \n[32] with support for storage strategies. PyPy is a fully feature-compatible Python JITing VM [7], whose \nperfor\u00ad mance is amongst the best for dynamically typed VMs [6]. It therefore makes a good test platform \nfor storage strategies since it is already fairly well optimised: as with any reason\u00adably mature VM, \nmeaningful optimisations become harder to .nd over time. As our experiments on real-world Python systems \nshow, storage strategies are a powerful optimisation, increasing performance in our benchmarks by 18% \nand low\u00adering peak memory usage by 6%. Because of this, storage strategies were merged into the PyPy \ndevelopment stream and have been part of shipping releases since version 1.9, released in June 2012. \nAll the strategies we describe in this paper have been included in PyPy releases since then. As our implementation \nshows, storage strategies also have the advantage of being lightweight: PyPy s storage strategies are \napproximately 1500 LoC in total. In comparison to tagging, which must be spread throughout a VM, storage \nstrategies are implemented in a single place and are more easily comprehended. Storage strategies can \ntherefore be retro.tted to existing VMs, or used within execution engines running atop an existing VM \n(e.g. HotSpot). Furthermore, our experiments demonstrate that storage strategies are an orthogonal concept \nto tagging: we still get useful speedups of 12% on primitive types that are not amenable to tagging. \nThe paper is structured as follows. We .rst describe the general design of storage strategies (Section \n3), before de\u00ad tailing the speci.c implementation of storage strategies in PyPy (Section 4). Using a \nwide range of benchmarks, we then evaluate the speed and memory utilisation of storage strategies in \nPyPy (Section 5). Our experimental system is fully repeatable, and has been accepted by the OOPSLA arte\u00adfact \nevaluation committee. It can be downloaded from http: //soft-dev.org/pubs/files/storage_strategies/ \n Figure 1. The list [1, 2, 3] as laid out in memory in PyPy. 2. Background 2.1 Memory usage in dynamically \ntyped languages An implicit effect of dynamic typing is that every variable, object slot, and so on, \ncan reference objects of arbitrary types. The most natural way to implement this is to box all objects: \nthat is, to allocate them on the heap with a common header. Although objects of different types may have \ndifferent (and differently sized) bodies, this allows a simple, common representation for all objects, \nwhich simpli.es VM implementation considerably. However, while universal boxing simpli.es an implemen\u00adtation, \nit is inef.cient. An integer in C typically occupies one word in memory; in PyPy, in contrast, 3 words \nare needed (1 word is reserved for the garbage collector; 1 word speci.es the particular type of the \nobject; and the .nal word stores the actual integer value). This problem is exacerbated when integers \nare stored in a list where the pointer to the boxed object adds further overhead. In PyPy, the seemingly \nsimple list [1, 2, 3] is laid out in memory as in Figure 1.1 The raw memory statistics tell part of the \nstory: a list of 1,000,000 integers takes 3.8MiB of memory in C but 15.3MiB in PyPy. However, other factors \nare important. In particular, boxed objects require memory on the heap to be allocated and later garbage \ncollected, placing extra pressure on the garbage col\u00adlector. Harder to quantify is the poor cache locality \ncaused by the extra indirection of boxing, which may mean that sequentially accessed objects are spread \naround the heap. One partial solution to this problem is pointer tagging [21]. This technique makes use \nof the fact that pointers to objects in a heap are aligned to multiples of 4 or 8 (depending on the combination \nof hardware and memory allocation library used). This means that at least 2 bits in every object pointer \nare unused, since they are always set to 0. Pointer tagging utilises these spare bits to differentiate \ngenuine pointers from other sorts of information. Most commonly, this technique is used to store .xed-width \ndatatypes most commonly integers into a single word. For example, any tagged pointer which has its \nleast signi.cant bit set may be de.ned to be a pointer a simple OR can recover the real pointer address \n otherwise it is an integer. In so doing, integers that .t into a machine word do not need to be allocated \non the heap. Pointer tagging can allow a dynamically typed language to represent a list of 1 We have \nelided details related to over-allocation to simplify the explanation.  integers as memory ef.ciently \nas C, substantially improving performance. Pointer tagging is not without costs, however. Most obvi\u00adously, \nevery access of a tagged pointer requires a check of its tag type, and, in general, a modi.cation to \nremove the tag and recover the pure value, hurting performance (not to men\u00adtion occasional poor interaction \nwith modern CPU s branch prediction). As pointer accesses are extremely frequent, and spread throughout \na VM, it is impossible to localise pointer tagging to a single portion of a VM: the complexity must be \nspread throughout the VM. Retrospective changes to pointer tagging involve widespread disruption to a \nVM; similarly, retrospectively introducing tagging into a previously non\u00adtagging VM is likely to be a \ngargantuan effort. There are also only a .nite number of spare bits in a pointer, putting a .xed, low, \nbound on the number of types that can be tagged. Finally, variable length data such as strings can not, \nin general, be tagged. Perhaps because of these reasons, pointer tagging is far from universal, and notable \nVMs such as the JVM s HotSpot do not use this technique, as far as we are aware.  2.2 PyPy and tracing \nJITs PyPy2 is a Python VM that is fully compatible with the standard C Python VM3 (known as CPython ). \nPyPy is written as an interpreter in RPython, a statically typed subset of Python that allows translation \nto (ef.cient) C. RPython is not a thin skin over C: it is fully garbage collected and contains several \nhigh-level data-types. More interestingly, RPython automatically generates a tracing JIT customised to \nthe interpreter. Tracing JITs were initially explored by the Dynamo Project [3] and shown to be a relatively \neasy way to implement JITs [18]. The assumption underlying tracing JITs is that programs typically spend \nmost of their time in loops and that these loops are very likely to take the same branches on each execution. \nWhen a hot loop is detected at run-time, RPython starts tracing the interpreter ( meta\u00adtracing ), logging \nits actions [7]. When the loop is .nished, the trace is heavily optimised and converted into machine \ncode. Subsequent executions of the loop then use the machine code version. An important property of tracing \nsystems is that traces naturally lead to type specialisation, and escape analysis on the traces allows \nmany actions on primitive types to be unboxed [8]. Speci.cally, most interpreters are able to be written \nin a style that makes unboxing primitive types stored in variables possible [6]. Meta-tracing allows \nfairly high performance VMs to be created with signi.cantly less effort than traditional ap\u00adproaches \n[6]. PyPy is, on average, around 5 6 times faster on real-world benchmarks compared to CPython; it is \neven faster when measured against Jython, the Python implementation running atop the JVM [6]. In short, \nmeta-tracing evens out 2 http://pypy.org/ 3 http://python.org/  Figure 2. Strategy transition diagram \nthe performance playing .eld for hard to optimise languages like Python. 3. Storage strategies In Section \n2.1, we saw how memory is typically used in dynamically typed languages. Pointer tagging is one partial \nsolution but, as was noted, has limitations and costs as well. The obvious question is: can one come \nup with an alternative scheme which works well in practice? Our starting assumption is that a signi.cant \nweakness of VMs such as PyPy is their treatment of collections. By optimising the use of primitive types \nin collections, we have a realistic hope of broadly matching and, in some cases, perhaps exceeding \npointer tagging, without having to deal with its complexity. As is common in high-performance VMs, it \nis impossible to optimise all use cases all of the time. Instead, we need to determine what bottlenecks \nreal-world programs cause and focus our attention there. Our hypotheses of how real-world dynamically \ntyped programs use collections are as follows: H1 It is uncommon for a homogeneously typed collection \nstoring objects of primitive types to later type deho\u00admogenise. H2 When a previously homogeneously typed \ncollection type dehomogenises, the transition happens after only a small number of elements have been \nadded. We validate these hypotheses in Section 5.6; for the time being, we take them as given. These \nhypotheses give a clear target when it comes to optimising collections: collections storing elements \nof a single primitive type. We must also bear in mind that the second hypothesis, in particular, is one \nof probabilities, not absolutes: even if type dehomogenisation is rare, it can happen, and the possibility \nmust be catered for. The design of our solution is straightforward. Each col\u00adlection references a storage \nstrategy and a storage area. All operations on the collection are handed over to the storage strategy, \nwhich also controls how data is laid out in the stor\u00adage area. An arbitrary number of strategies can \nbe provided to ef.ciently store objects of different types. Although a col\u00adlection can only have one \nstrategy at any point in time, its strategy can change over time. As the name may suggest, this idea \nis an implementation of the Strategy design pattern [19].  Figure 2 shows how a collection s strategy \ncan evolve over its lifetime. Collections start with the EmptyStrategy, which is a placeholder while \nthe collection is empty; as soon as an object is put into the collection, a more speci.c strategy will \nbe used. If an object of primitive type, such as an int, is added to the collection, and if an optimised \nstrategy is available (e.g. IntegerStrategy), the collection switches to it; the strategy will unbox \nthe element and store it. As more elements of that primitive type are added, the collection will grow \naccordingly. If an object of a different type is then added to the collection, the strategy will be changed \nto the ObjectStrategy, which has the same performance characteristics as a typical VM without storage \nstrategies; any existing objects in the collection will have to be reboxed. Finally, if the collection \nis emptied of all objects, its strategy is returned to the EmptyStrategy. Storage strategies have low \noverheads. Each collection needs only a single extra pointer to a strategy object. Each strategy can \nbe a singleton or static class, allowing it to be shared between multiple collections, requiring a small, \n.xed overhead per process. As many strategies as desired can be implemented, and collections can easily \nmove between strategies over their lifetime. Operations on collections need one extra method call on \nthe strategy to be implemented, the cost of which should be offset by the time saved by not boxing as \nmany elements. 4. Storage strategies in PyPy 4.1 Basic design We have implemented storage strategies \nin PyPy for the three major forms of collections in Python lists, sets, and dictionaries (known in different \nlanguages as maps or hashtables) and three major primitive types integers, .oats, and strings. List \nand set storage strategies are relatively obvious; dictionaries, having both keys and values, less so. \nAlthough we could implement strategies for each pairwise (key, value) primitive type combination we believe \nthis is unnecessary as the main bottleneck in dictionary performance is the hashing and comparing of \nkeys. Therefore dictionary strategies dictate the storage of keys, but leave values boxed. For each collection \ntype X, we have implemented an EmptyXStrategy (for when the collection has no ob\u00adjects), an ObjectXStrategy \n(for storing objects of arbi\u00adtrary types), and IntegerXStrategy, FloatXStrategy, and StringXStrategy \n(for the primitive types). Collections return to the EmptyXStrategy when their clear method is called. \nSince Python lists do not have such a method, they can not currently return to the EmptyListStrategy. \nEach ObjectXStrategy largely reuses the relevant por\u00adtion of pre-storage strategy code (with minor modi.cations), \nwhereas the other strategies contain all-new code. In addition to the normal collection operations, we \nalso added shortcuts to allow collections with optimised storage strategies to inter\u00adact ef.ciently with \neach other (see Section 4.4 for further de-Figure 3. Storage strategies on the two lists [1, 2, 3] and \n[4, 5, 6].  Figure 4. Adding other types: If an object of a different type is added to a list using \nan integer strategy, all elements need to be boxed and the collection will use the object strategy. tails). \nWith the normal caution to readers about the dangers of over-interpreting Lines of Code (LoC) .gures, \nthe following details give a good idea of the simplicity of storage strategies. In total, storage strategies \nadded 1500LoC4 to PyPy when merged in. Of that total, lists contribute 750LoC, sets 550, and dictionaries \n200. The relatively small LoC count for dic\u00adtionaries is due to the existing implementation of dictionaries \nalready having storage strategy-like behaviour, which is hard to factor out from the newer, full-blown, \nstorage strategies. Figure 3 shows the memory layout of two lists using the IntegerListStrategy, one \nstoring [1, 2, 3] and the other [4, 5, 6]. Comparing them to the conventional layout shown earlier in \nFigure 1, we can see that storage strategies unbox the integers, saving three words per element. Figure \n3 also shows that the use of singleton strategies keeps the overhead of storage strategies to a .xed \nminimum. As Figure 3 suggests, strategies add one extra word to each collection. If a program used a \nlarge number of small, almost exclusively, heterogeneously typed collections, the overhead of the extra \nword may outweigh the savings on homoge\u00adneously typed collections. Such behaviour seems unlikely, and \nnone of our benchmarks exhibits it (see Section 5.6). As motivated in Section 3, users of dynamically \ntyped languages always have the possibility of changing an ho\u00admogeneously typed collection into a heterogeneously \ntyped collection: it is vital that storage strategies can cope with this. Figure 4 shows what happens \nwhen an object of a different type (in this case a .oat) is added to the previously homo\u00adgeneously typed \nlist of ints [4, 5, 6]. The integers in the list are (re)boxed, the list s strategy is changed to the \ngeneric ObjectListStrategy, and the data section rewritten ap\u00adpropriately. Clearly, such reboxings are \ncostly when applied 4 We exclude blank lines and comments from the count, but otherwise leave formatting \nwhich causes line breaks in.  1 class W_ListObject(W_Object): 2 def __init__(self): 3 self.strategy \n= EmptyListStrategy() 4 self.lstorage = None 5 6 def append(self, w_item): 7 self.strategy.append(self, \nw_item) 8 9 @singleton 10 class ListStrategy(object): 11 def append(self, w_list, w_item): 12 raise NotImplementedError(\"abstract\") \n13 14 @singleton 15 class EmptyListStrategy(ListStrategy): 16 def append(self, w_list, w_item): 17 if \nis_boxed_int(w_item): 18 w_list.strategy = IntegerListStrategy() 19 w_list.lstorage = new_empty_int_list() \n20 elif ...: 21 ... 22 else: 23 w_list.strategy = ObjectListStrategy() 24 w_list.lstorage = new_empty_object_list() \n25 w_list.append(w_item) 26 27 @singleton 28 class IntegerListStrategy(ListStrategy): 29 def append(self, \nw_list, w_item): 30 if is_boxed_int(w_item): 31 w_list.lstorage.append_int(unbox_int(w_item)) 32 return \n33 self.switch_to_object_strategy(w_list) 34 w_list.append(w_item) 35 36 def switch_to_object_strategy(self, \nw_list): 37 lstorage = new_empty_object_list() 38 for i in w_list.lstorage: 39 lstorage.append_obj(box_int(i)) \n40 w_list.strategy = ObjectListStrategy() 41 w_list.lstorage = lstorage 42 43 @singleton 44 class ObjectListStrategy(ListStrategy): \n45 def append(self, w_list, w_item): 46 w_list.lstorage.append_obj(w_item) Figure 5. A simpli.ed view \nof the various list strategies, with each showing its part in the user-visible append method. to collections \nwith large numbers of elements. Fortunately, this occurs relatively rarely in practise and when it does, \ncollections contain only a single element on average (see Section 5.6). The disadvantages, therefore, \nare signi.cantly outweighed by the advantages.  4.2 Implementation To make our explanation of PyPy s \nimplementation of storage strategies somewhat more concrete, Figure 5 shows the relevant sections of \ncode for the append method on lists. Although Figure 5 necessarily elides and simpli.es various details, \nit gives a good .avour of the relative simplicity of storage strategies. PyPy s root object class is \nW Object from which the list class W ListObject inherits. Each instance of W ListOb\u00adject has a storage \nstrategy and a storage area (which is controlled by the strategy). Calling append on a list object transfers \ncontrol over to the list s strategy (line 7): note that, as the .rst parameter, the storage strategy \nis passed a pointer to the list object so that it can, if necessary, change the storage strategy and \n/ or storage area. All list storage strategies are singletons and subclasses of ListStrategy, which is \nan abstract class that can not be directly instantiated. Empty list objects therefore use a reference \nto the EmptyListStrategy class for their storage strategy. When an object is appended to an empty list, \nthe Empty-ListStrategy .rst sees if an optimised storage strategy is available for that type of object \n(lines 17 21). For example, if the incoming object is an integer (line 17), IntegerList-Strategy is used. \nIf no optimised storage strategy is avail\u00adable, ObjectListStrategy is used as the fallback (lines 23 \n24). Once the new strategy is decided upon, its append method is then called (line 25). The .nal interesting \ncase is when appending an item to a list using an optimised storage strategy. For example, if the IntegerListStrategy \nis being used, then append has two choices. If the object being appended is also an integer (line 30), \nit is unboxed, added to the storage area, and the function returns (lines 31 32). If, however, an object \nof a different type is appended, the list must be switched to the ObjectListStrategy. First a temporary \nstorage area is created (line 37), and all the list s integers boxed and put into it (38 39). The temporary \nstorage area then replaces the original and the strategy is updated (lines 40 41).  4.3 Exotic storage \nstrategies PyPy, as of writing, has 17 storage strategies, some of which implement less obvious optimisations. \nFor example, consider Python s range(i, j, s) func\u00adtion which generates a list of integers from i to \nj in steps of s. Its most common uses are in for statements and func\u00adtional idioms such as map. Conceptually, \nrange creates a full list of integers, but this is highly inef.cient in practise: its most common idioms \nof use require neither random access nor, often, do they even read the full list (e.g. when a for loop \ncontains a break). In order to optimise this common idiom, Python 3 s range operator5 returns a special \nrange type which is not a proper list (for example, items can not be written to it). If the programmer \ndoes want to use it as a normal list, it must be manually converted. Storage strategies provide a natural \nsolution to this prob\u00adlem, optimising the common case, while making the general case transparent to the \nprogrammer. Internally, PyPy provides a RangeListStrategy, which the range function uses. A 5 Effectively, \nPython 2 s xrange function has subsumed the original range.  list using this strategy stores only three \nwords of informa\u00adtion, no matter the size of the range created: start, stop, and step (all as integers). \nTypical operations on such a collection (e.g. contains) do not need to search all elements, instead using \na simple mathematical calculation. If the user does add or remove an element from the range, it changes \nto an appro\u00adpriate strategy (e.g. IntegerListStrategy upon element removal). The beauty of storage strategies \nis that, outside the RangeListStrategy itself, only the range function needs to be aware of its existence. \nAll other code operates on such collections in blissful ignorance. The implementation of RangeListStrategy \nis straightforward, being well under 200LoC, but leads to a highly effective optimisation. Dictionaries \nhave a special storage strategy Identity-DictStrategy which optimises the case where key equality and \nhashing are based on object identity. This happens when user-de.ned classes which inherit from Python \ns root object class do not override equality and hashing. This allows operations on dictionaries storing \nsuch objects to be optimised more effectively. Similarly, storage strategies can be used to provide ver\u00adsions \nof normal datatypes customised to speci.c use cases. Python modules, classes, and objects can all have \ntheir con\u00adtents accessed as dictionaries. Modules and class dictio\u00adnaries are rarely written to after \ninitialization. Objects dic\u00adtionaries, on the other hand, are often written to, but each class s instances \ntends to have a uniform set of keys. Us\u00ading maps [12] as inspiration, optimised storage strategies for \nobjects, modules, and classes optimise these use cases [6]. A similar strategy is used for the dictionaries \nthat hold the variadic arguments of functions that take arbitrary (keyword) arguments.  4.4 Optimising \ncollection creation and initialisation Conceptually, collections are always created with an EmptyX-Strategy, \nwith elements then added to them. In practice, PyPy has fast paths to optimise common cases which do \nnot need the generality, and consequent overhead, associated with the simple route. We now give two examples. \nFirst, collections are sometimes created where the type of all the elements is known in advance. A simple \nexample is the split(d) method on strings, which separates a string (using d as a delimiter) into a list \nof sub-strings. Rather than create a collection which starts in the EmptyListStrategy before immediately \ntransitioning to the StringListStrategy, a fast path allows lists to be created which are initialised \nto the StringListStrategy. This technique is particularly useful when PyPy knows that a collection will \nbe homogeneously typed and can use an optimized strategy immediately. This bypasses type-checking every \nelement of the new collection. Second, collections are often initialised from another collection: for \nexample, cloning a list, or initialising sets with lists. In Python, for example, it is common to initialise \nsets with lists i.e. set([1,3,2,2]) creates a set {1, 2, 3}. The naive approach to creating the set is \nto iterate over the list, boxing each of its elements if necessary, and then putting them into the set \nwhich if it uses storage strategies may then unbox them again. Since, in our implementation, the list \nwill be stored using the IntegerListStrategy, a few LoC in the set initialization code can detect the \nuse of the other collection s strategy and access the integers directly. Not only does this bypass boxing, \ntype-checking, and unboxing, it allows the set storage strategy to perform hash operations directly on \nthe primitive types rather than requiring an expensive type dispatch. 4.5 Optimising type-based operations \nOptimised storage strategies such as IntegerListStrat\u00adegy can make use of the type of data to improve \nthe ef\u00ad.ciency of common operations. A simple example is the contains(o) method which returns true if \no is found in a collection. The IntegerListStrategy has a specialised contains method which, if passed \nan integer, performs a fast machine word comparison with the collection s contents. If passed a user \nobject with a custom de.nition of equality, standard expensive method dispatch occurs. Since the most \nlikely test of equality on a collection is to see if an object of the same type exists in it, this can \nbe a substantial optimisation, and is only a few LoC long. Furthermore, it is a bene.t even when the \nVM is running as a pure interpreter (i.e. when the JIT is not running). This technique can be used for \nmany other collection methods, such as a list s sort or a set s difference or issubset methods.  4.6 \nInteraction with RPython s Tracing JIT Storage strategies give PyPy extra information about the types \nof objects in a collection. Most obviously, when a collection uses an optimised storage strategy such \nas IntegerListStrategy, we implicitly know that every ele\u00adment in the collection must be an integer. \nThis is of particular interest to RPython s tracing JIT, which can use this knowl\u00adedge to prove additional \nproperties about a trace, leading to more extensive optimisation. Consider the following code fragment: \ni = 0 for x in c: i += x Let us assume that c stores only integers. In a normal implementation, every \nread of an element from c would be followed by a type-check before the addition is performed, constituting \na sizeable overhead in such a simple loop. With storage strategies, the .rst iteration of the list will \ndeduce that the list uses the IntegerListStrategy. The resulting trace will naturally establish that \nthe contents of c are not changed, and that since i is also an integer, no further type checks need to \nbe performed on the elements coming out of c at all. No matter the size of the list, only a simple check \non c s strategy is needed before the loop is executed. Assuming this condition is met, the integers extracted \nfrom c can be accessed unboxed, leading to highly ef.cient machine code.  This same basic idea works \nequally well on more complex collection operations. Consider code which writes an object into a dictionary. \nPotentially, the object may provide custom hashing and comparison operations which may have arbitrary \nside effects, including mutating other objects. Even though few hash methods do such mutations, the possibility \nprevents the JIT from proving several seemingly obvious properties of the system before and after the \ndictionary write, hindering optimisations. However, when the dictionary uses an opti\u00admised storage strategy, \nthe JIT can trivially prove that such mutation is impossible, leading to signi.cant optimisations in \nthe resulting traces. 5. Evaluation In this section we evaluate the effectiveness of storage strate\u00adgies \non two axes: execution speed and memory usage. We .rst detail the sets of benchmarks we use, before then \ndescrib\u00ading the PyPy variants with various strategies turned on/off. Having done this, we are then in \na position to present ex\u00adecution speed and memory usage measurements. We then explore why storage strategies \nwork as they do, validating our earlier hypotheses. 5.1 Benchmarks To perform our evaluation we use two \nsets of benchmarks: PyPy s standard reference benchmarks [28]; and a set of benchmarks of real-world \nprograms we have compiled for this paper, shown in Table 1. PyPy s benchmarks derive from those used \nfor the (now defunct) Unladen Swallow Python VM, and have a fairly long heritage, being a mix of synthetic \nbenchmarks and (parts of) real-world programs. However, while PyPy s benchmarks measure performance \ninclud\u00ading several dif.cult corner-cases reasonably well, few of the benchmarks allocate signi.cant \nmemory. In other words, while differences to their execution speed is relevant for stor\u00adage strategies, \ntheir peak memory usage is, generally, insignif\u00adicant. The real-world programs in Table 1 use programs \nthat use much larger heaps (ranging, approximately, from 10 to 1000MiB). In most cases, the actual benchmarks \nconsist of a few LoC, which then exercise library code. The precise details are available in our downloadable \nand repeatable ex\u00adperiment (see page 2). 5.2 PyPy variants Table 2 shows the PyPy variants with different \nstrategies turned on/off we created to measure their effect. From the point of view of an end-user, the \nmost signi.cant variants are pypy-none (which turns off nearly all strategies) and pypy-all (which turns \non nearly all strategies). The other variants allow us to determine which strategies play the biggest \nrole in optimising performance. All the PyPy variants (including pypy-none) have object, method, and \nclass strategies turned on, because these are fundamental to PyPy s optimisations and, in various forms, \nhave been present long before the current work. The strategies themselves have all been described previ\u00adously \nin the paper, with the exception of KW args . This is a special strategy for optimising Python s keyword \narguments, which are dictionaries mapping strings to objects, and which are typically constant at a given \ncall location.  5.3 Methodology For the execution speed benchmarks, we are most interested in the steady-state \nperformance once the JIT has warmed up, since the resulting numbers are stable. Determining when the \nsteady-state has been reached is impractical since the branching possibilities of a program are vast: \nthere is always the possibility that a previously unvisited branch may later be involved in a hot loop. \nHowever, by running a program for a large number of iterations, we increase the chances that we are at \n or at least near the steady-state. Using the PyPy reference benchmarks and those of Table 1, we execute \neach benchmark 35 times within a single process, discarding the .rst 5 runs, which are likely to contain \nmost, if not all, of the warm-up process. We report con.dence intervals with a 95% con.dence level [20]. \nFor ratios of speeds we give the con.dence interval assuming that the measurements are normally distributed. \nSpeed ratios are calculated using the geometric mean, which is better suited to these kinds of benchmarks \nthan the arithmetic mean [17]. While execution speed is the most obvious motivation for storage strategies, \nmemory usage can be as important for many users. However, while execution speed has an obvious de.nition, \nmemory usage is somewhat trickier. We use what we think is probably the most important factor from a \nuser s perspective: peak memory. In simple terms, peak memory is the point of greatest memory use after \ngarbage collection. This measure is relevant because RAM is a .nite resource: a program that needs 10GiB \nof memory allocated to it at its peak will run very slowly on an 8GiB RAM machine, as it thrashes in \nand out of swap. To benchmark peak memory, we use benchmarks from Table 1; we exclude the PyPy reference \nbenchmarks from our measurements, as they are unusually low consumers of memory (most were speci.cally \ndesigned to be useful measures of execution speed only). We manually determined the point in the program \nwhen memory usage is at, or near, its peak. We then altered the programs to force a garbage collection \nat that point, and measured the resulting heap size. This approach minimises the effects of possible \nnon-determinism in the memory management subsystem. All benchmarks were run on an otherwise idle Intel \nCore i7-2600S 2.8GHz CPU with 8GB RAM, running 64-bit Linux 3.5.0 with GCC 4.7.2 as the compiler. The \nmeasurement system is fully repeatable and downloadable (see page 2).  Name Version Description disaster \n1.1 Disambiguator and statistical chunker [30] Feedparser 5.1.3 RSS library [16] invindex n/a Calculates \ninverted index [33] multiwords n/a LocalMax algorithm [15] NetworkX 1.7 Graph Network analysis [26] \n nltk-wordassoc 2.0b9 Real-world text analysis using the Natural Language Toolkit [5] orm n/a Object-relational \nmapping benchmark [4] PyDbLite 2.7 In-memory database [29] PyExcelerator 0.6.4.1 Excel .le manipulation \n[27] Scapy 2.1.0 Network packet manipulation [25] slowsets n/a Builds combinations from letters whoosh \n2.4.1 Text indexing and searching library [36]  Table 1. The set of memory-intensive real-world libraries \nwe use as benchmarks in this paper. Data types Collection types Other strategies Ints Floats Strings \nLists Dicts Sets Range KW args pypy-none ... ... .. pypy-list  .. . pypy-dict  . . . pypy-set  .. \n.. pypy-ints ..  .. pypy-.oats . .  .. pypy-strings ..  . pypy-all Table 2. The PyPy variants (along \nthe left) and the aspects they contain (along the top). pypy-list, for example, contains IntListStrategy, \nFloatListStrategy, StringListStrategy, and RangeListStrategy. 5.4 Execution speed Table 3 shows the \nspeed ratios of the results of running several of our PyPy variants over the full set of benchmarks (i.e. \nthe PyPy reference benchmarks and those of Table 1). The unnormalized results can be found in Table 5 \nin Appendix A. While there is inevitable variation amongst the benchmarks, the relative speedup of pypy-all \ncompared to pypy-none shows a clear picture: storage strategies improve performance by 18%. Some benchmarks \nreceive much larger performance boosts (invindex runs over seven times faster); some such as Feedparser \nare only imperceptibly sped up. We explain the reason for the latter in Section 5.6. One expectation \nof Table 3 might be that the .gure for pypy-all should equal both the combination of pypy-list * pypy-set \n* pypy-dict or pypy-ints * pypy-strings * pypy-.oats. Both are true within the margin of error, though \nthe latter is suf.ciently different that a more detailed explanation is worth considering. pypy-ints \n* pypy-strings * pypy-.oats (0.783 \u00b1 0.057) is faster than pypy-all (0.816 \u00b1 0.034), albeit within the \nmargin of error of the two ratios. If we break the comparison down on a benchmark-by-benchmark basis, \nthen 12 benchmarks are unequal when pypy-dict or pypy-ints * pypy-strings * pypy\u00ad.oats is considered \nvs. pypy-all (taking con.dence intervals into account). Of those, the .gures of 9 are faster than pypy\u00adall \nand 3 slower. We believe the faster benchmarks are likely to be due to the overlap between storage strategies: \neach has the same overall optimisations for lists, dictionaries, and sets (e.g. the initialisation optimisations \nof Section 4.4), but sim\u00ad ply turns off two of the primitive types. The more commonly used a primitive \ntype, the more likely that such optimisations have a cumulative effect. The slower benchmarks are harder \nto explain de.nitively. We suspect that several factors play a part. First, turning off parts of PyPy \ns code may interact in hard-to-examine ways with various tracing-related thresholds  (e.g. when to trace). \nSecond, the removal of some code is likely to make the trace optimiser s job arti.cially harder in some \ncases (see Section 4.6), frustrating some optimisa\u00ad tions which may initially seem unrelated to storage \nstrategies. Dif.ng traces from two different PyPy variants is currently a manual task, and is unfeasible \nwhen two interpreters have changed suf.ciently. Table 6 in the appendix provides the de\u00ad   Benchmark \npypy-all pypy-list pypy-set pypy-dict pypy-ints pypy-strings pypy-.oats disaster 0.939 \u00b1 0.021 1.030 \n\u00b1 0.021 0.949 \u00b1 0.019 0.967 \u00b1 0.020 0.943 \u00b1 0.019 0.931 \u00b1 0.019 0.932 \u00b1 0.016 Feedparser 0.999 \u00b1 0.461 \n0.997 \u00b1 0.459 0.999 \u00b1 0.431 0.995 \u00b1 0.453 1.003 \u00b1 0.462 1.003 \u00b1 0.454 0.996 \u00b1 0.441 invindex 0.136 \u00b1 \n0.002 0.136 \u00b1 0.001 1.152 \u00b1 0.011 1.172 \u00b1 0.010 1.144 \u00b1 0.010 0.137 \u00b1 0.001 1.203 \u00b1 0.012 multiwords \n0.958 \u00b1 0.016 0.953 \u00b1 0.016 0.972 \u00b1 0.015 0.994 \u00b1 0.017 0.989 \u00b1 0.030 1.036 \u00b1 0.037 1.014 \u00b1 0.044 NetworkX \n0.582 \u00b1 0.015 0.954 \u00b1 0.022 1.044 \u00b1 0.025 0.623 \u00b1 0.015 0.561 \u00b1 0.014 0.738 \u00b1 0.018 0.782 \u00b1 0.021 nltk-wordassoc \n0.928 \u00b1 0.017 0.948 \u00b1 0.016 1.009 \u00b1 0.018 0.984 \u00b1 0.016 1.014 \u00b1 0.017 0.915 \u00b1 0.017 1.017 \u00b1 0.018 orm \n0.990 \u00b1 0.115 0.998 \u00b1 0.118 0.998 \u00b1 0.116 0.977 \u00b1 0.117 0.987 \u00b1 0.121 0.971 \u00b1 0.119 1.005 \u00b1 0.120 PyDbLite \n0.888 \u00b1 0.017 1.029 \u00b1 0.015 1.042 \u00b1 0.013 0.876 \u00b1 0.011 1.147 \u00b1 0.014 0.890 \u00b1 0.011 1.144 \u00b1 0.014 PyExcelerator \n0.986 \u00b1 0.005 1.057 \u00b1 0.006 0.980 \u00b1 0.005 0.944 \u00b1 0.004 1.015 \u00b1 0.005 0.956 \u00b1 0.005 0.994 \u00b1 0.004 Scapy \n0.671 \u00b1 0.164 0.999 \u00b1 0.213 1.005 \u00b1 0.217 0.677 \u00b1 0.163 0.839 \u00b1 0.217 0.697 \u00b1 0.167 0.860 \u00b1 0.222 slowsets \n0.881 \u00b1 0.022 1.018 \u00b1 0.028 0.907 \u00b1 0.042 1.009 \u00b1 0.004 0.985 \u00b1 0.007 0.893 \u00b1 0.022 0.987 \u00b1 0.004 whoosh \n0.915 \u00b1 0.014 0.972 \u00b1 0.013 0.987 \u00b1 0.014 0.900 \u00b1 0.017 0.966 \u00b1 0.014 0.891 \u00b1 0.013 0.976 \u00b1 0.013 ai \n0.748 \u00b1 0.068 0.859 \u00b1 0.063 0.853 \u00b1 0.065 0.948 \u00b1 0.072 0.741 \u00b1 0.058 1.044 \u00b1 0.075 0.886 \u00b1 0.065 bm \nchameleon 0.893 \u00b1 0.009 0.967 \u00b1 0.008 0.925 \u00b1 0.004 0.851 \u00b1 0.003 0.971 \u00b1 0.004 0.911 \u00b1 0.004 0.946 \u00b1 \n0.005 bm mako 0.956 \u00b1 0.615 1.032 \u00b1 0.665 1.042 \u00b1 0.663 1.020 \u00b1 0.644 1.009 \u00b1 0.651 1.051 \u00b1 0.651 1.025 \n\u00b1 0.667 chaos 0.403 \u00b1 0.011 0.402 \u00b1 0.011 0.783 \u00b1 0.023 0.781 \u00b1 0.023 0.407 \u00b1 0.011 1.022 \u00b1 0.028 1.033 \n\u00b1 0.029 crypto pyaes 0.985 \u00b1 0.050 0.983 \u00b1 0.050 0.991 \u00b1 0.049 0.969 \u00b1 0.049 0.972 \u00b1 0.049 0.983 \u00b1 0.050 \n0.984 \u00b1 0.050 django 0.721 \u00b1 0.011 0.931 \u00b1 0.012 1.010 \u00b1 0.012 0.794 \u00b1 0.011 0.874 \u00b1 0.013 0.710 \u00b1 0.011 \n0.873 \u00b1 0.012 fannkuch 0.956 \u00b1 0.005 0.966 \u00b1 0.006 1.155 \u00b1 0.007 1.134 \u00b1 0.006 0.967 \u00b1 0.005 1.213 \u00b1 \n0.007 1.067 \u00b1 0.006 genshi text 0.871 \u00b1 0.086 0.995 \u00b1 0.097 1.077 \u00b1 0.102 0.870 \u00b1 0.087 1.041 \u00b1 0.099 \n0.884 \u00b1 0.088 1.013 \u00b1 0.099 go 0.940 \u00b1 0.517 0.968 \u00b1 0.530 1.000 \u00b1 0.531 0.993 \u00b1 0.529 0.938 \u00b1 0.513 \n1.014 \u00b1 0.534 1.008 \u00b1 0.533 html5lib 0.924 \u00b1 0.026 0.939 \u00b1 0.029 1.021 \u00b1 0.031 0.955 \u00b1 0.030 0.933 \u00b1 \n0.025 0.916 \u00b1 0.025 0.934 \u00b1 0.025 meteor-contest 0.645 \u00b1 0.017 1.016 \u00b1 0.020 0.635 \u00b1 0.014 1.010 \u00b1 0.018 \n0.648 \u00b1 0.017 0.944 \u00b1 0.019 0.679 \u00b1 0.015 nbody modi.ed 0.986 \u00b1 0.007 0.920 \u00b1 0.006 0.947 \u00b1 0.071 0.930 \n\u00b1 0.008 0.933 \u00b1 0.007 0.939 \u00b1 0.007 0.938 \u00b1 0.007 py.ate-fast 0.850 \u00b1 0.027 0.868 \u00b1 0.031 1.017 \u00b1 0.050 \n1.025 \u00b1 0.052 0.940 \u00b1 0.027 0.912 \u00b1 0.040 0.999 \u00b1 0.030 raytrace-simple 0.990 \u00b1 0.291 1.050 \u00b1 0.283 1.073 \n\u00b1 0.292 1.031 \u00b1 0.291 1.012 \u00b1 0.251 1.007 \u00b1 0.252 1.030 \u00b1 0.361 richards 0.882 \u00b1 0.066 0.889 \u00b1 0.066 \n1.022 \u00b1 0.071 0.896 \u00b1 0.067 0.897 \u00b1 0.067 0.990 \u00b1 0.299 0.940 \u00b1 0.068 slowspit.re 0.908 \u00b1 0.014 0.825 \n\u00b1 0.036 0.981 \u00b1 0.029 0.985 \u00b1 0.032 1.001 \u00b1 0.046 0.909 \u00b1 0.074 1.032 \u00b1 0.050 spambayes 0.956 \u00b1 0.743 \n0.981 \u00b1 0.770 1.012 \u00b1 0.771 0.974 \u00b1 0.744 0.970 \u00b1 0.765 0.971 \u00b1 0.742 0.981 \u00b1 0.769 spectral-norm 0.993 \n\u00b1 0.137 0.994 \u00b1 0.137 0.995 \u00b1 0.138 1.003 \u00b1 0.142 0.991 \u00b1 0.137 0.994 \u00b1 0.135 0.994 \u00b1 0.138 sympy integrate \n0.935 \u00b1 0.291 1.018 \u00b1 0.354 0.996 \u00b1 0.356 0.974 \u00b1 0.341 1.014 \u00b1 0.345 0.962 \u00b1 0.294 1.005 \u00b1 0.346 telco \n0.858 \u00b1 0.185 0.900 \u00b1 0.181 0.947 \u00b1 0.196 0.870 \u00b1 0.201 0.858 \u00b1 0.161 0.855 \u00b1 0.196 0.914 \u00b1 0.166 twisted \nnames 0.921 \u00b1 0.032 0.998 \u00b1 0.034 1.004 \u00b1 0.032 0.913 \u00b1 0.031 0.983 \u00b1 0.035 0.931 \u00b1 0.030 0.981 \u00b1 0.032 \nRatio 0.816 \u00b1 0.034 0.888 \u00b1 0.037 0.981 \u00b1 0.040 0.934 \u00b1 0.039 0.915 \u00b1 0.038 0.882 \u00b1 0.037 0.970 \u00b1 0.041 \n Table 3. Benchmark speed ratios. All .gures are given relative to pypy-none: lower is better. Benchmarks \nwhere storage strategies perform worse are highlighted in bold. tailed comparison .gures for those readers \nwho are interested in exploring the individual datapoints further. An interesting question is how close \nthe .gures of Table 3 come to the theoretical maximum. Unfortunately, we do not believe that there is \na meaningful way to calculate this. One possibility might seem to be to create a statically typed variant \nof each benchmark, optimise its collections statically, and use the resulting .gures as the theoretical \nmaximum. However, a type inferencer (human or automatic) would inevitably have to conclude that many \npoints in a program which create collec\u00adtions must have the most general type (ObjectXStrategy) even \nif, at run-time, all of its instances were type homoge\u00adneous over their lifetime. For such programs, \nit is quite proba\u00adble that the statically optimised version would execute slower than that using storage \nstrategies. An alternative approach is V8 s kinds, which we discuss in Section 7.  5.5 Peak memory The \npeak memory results are shown in Table 4. As the results clearly show, storage strategies lead to a useful \nsaving in peak memory usage because, in essence, storage strategies cause objects to be shorter-lived \non average. As with execution speed, pypy-all is not necessarily a simple composition of a seemingly \ncomplete sub-set of interpreters, for the same reasons given in Section 5.4. One of the more surprising \n.gures is for pypy-list, which causes memory usage to increase. The main reason for this is that some \nitems that are unboxed inside a list are later used outside and reboxed, potentially multiple times. \nWithout storage strategies, a single object is allocated once on the heap, with multiple pointers to \nit from collections and elsewhere. With storage strategies, the same element can be unboxed in the storage \nstrategy, and then, when pulled out of the list, reboxed to multiple distinct objects in the heap. This \nis exacerbated by list storage strategies relative inability to save memory relative to sets and dictionaries, \nas Table 4 shows. Fortunately, such objects are typically short lived.  5.6 Validating the hypotheses \nIn Section 3, we stated two hypotheses about real-world programs, which led us to create storage strategies \nas they are. We can also slightly narrow the scope of interest for the hypotheses to ints, .oats, and \nstrings, as these are the only cases which affect storage strategies. Usefully, storage strategies themselves \ngive us a simple way of validating the hypotheses. We created a PyPy variant with two counters for each \nstorage strategy, recording the number of times a transition from one storage strategy to another is \ntaken, and the size of the collection at that point, allowing us to trivially calculate the average size \nof a collection s elements at switch. Figure 6 shows the resulting cumulative transition diagram for \nthe real-world benchmarks from Table 1. Hypothesis H1 postulates that it is uncommon for a homogeneously \ntyped collection storing objects of primitive types to later have an element of a different type added \nto it. The basis for this hypothesis is our expectation that, even in dynamically typed languages, programmers \noften use collections in a type homogeneous fashion. Figure 6 clearly shows this. The transitions of \ninterest are those where elements are either created or reboxed (shown in bold), which is where the hypothesis \nis violated. Consider StringList-Strategy, which is the biggest example. Around 21 million lists are \ncreated with that storage strategy, but only a little over 15% type dehomogenise. On average, across \nthe whole of Figure 6, a little under 10% of collections using optimised storage strategies later type \ndehomogenise. Some benchmarks (e.g. Feedparser and orm) type dehomogenise more than double this amount \nof objects: both see little gain from storage strategies. As this suggests, programs which violate hypothesis \nH1 may perform worse with storage strategies than without. Hypothesis H2 postulates that When a previously \nhomo\u00adgeneously typed collection type dehomogenises, the transi\u00adtion happens after only a small number \nof elements have been added. The basis for this hypothesis follows from H1, and is our expectation that \nwhen a collection does type deho\u00admogenise, it contains few elements. Therefore, we are really interested \nin how often the worst case scenario extremely large collections type dehomogenising happens. A sensible \nmeasure is thus the average number of elements a collection contains when type dehomogenisation occurs. \nAs Figure 6 shows, when collections using optimised storage strategies type dehomogenise, they typically \ncontain only a single ele\u00adment. The costs of such a switch are, thus, minimal. We consider the evidence \nfor hypothesis H1 strong and that for H2 conclusive. The validation of these hypotheses also goes some \nway to explain why storage strategies are effective: although the potential worst case is deeply unpleas\u00adant, \nprogrammers do not write programs which run in a way that triggers the worst case.  5.7 Threats to validity \nFor the speed benchmarks, the main threat to validity is that we report performance in PyPy s steady-state, \nlargely excluding start-up times. While this is representative of a large class of programs e.g. server \nprograms, or those which perform long calculations it does not tend to represent short-lived batch-like \nprograms very well. Balancing these two factors is a perennial problem when measuring the performance \nof programs in JIT VMs, because the best answer is highly dependent on how an individual user uses the \nJIT. The virtue of using steady-state measurements for this paper is that the numbers are more stable: \nmeasuring numbers before the steady-state is reached can be highly volatile, as the impact of the JIT \ncan be signi.cant. For the memory benchmarks, a signi.cant threat to validity is that we manually chose \nthe locations in the program to measure peak memory usage. We may unwittingly have chosen locations which \nfavour storage strategies. However, by using large number of benchmarks, we have reduced the chances \nof this potential bias having a signi.cant effect. 6. Relevance to other languages The evaluation in \nSection 5 shows that enough real-world Python programs collections contain elements of a single primitive \ntype to make storage strategies a substantial optimi\u00adsation. We now consider an important question: are \nstorage strategies only relevant to Python (the language) or PyPy (the language implementation)? Perhaps \nthe most important results in the evaluation are the statistic surrounding type dehomogenisation (Section \n5.6) which show that: collections of primitive types dehomogenise in only 10% of cases; when type dehomogenisation \noccurs, collections only contain a single element on average. These two statistics go a considerable \nway to explain why storage strategies are so effective. We believe that programs in most dynamically \ntyped languages will follow a similar pattern, for the simple reason that this is the most predictable, \nsafe way to develop programs. We therefore expect that most dynamically typed language implementations \nhave the same underlying potential for improvement that we have shown with PyPy. Importantly, we do not \nbelieve that storage strategies only make sense when used in the lowest-level VM: language implementations \nrunning atop an existing VM can easily use storage strategies. For example, an implementation such as \nJython (Python running on the JVM) could make use of storage strategies. We do, however, note that JIT-based \nimplementations seem likely to see larger relative bene.ts from storage strategies than non-JIT-based \nimplementations because of the ability of the JIT to more frequently unbox el\u00adTable 4. Peak memory at \npoints of interest during execution (MiB). The ratios are relative to pypy-none: lower values indicate \nless memory usage.  benchmark pypy-none pypy-all pypy-list pypy-set pypy-dict pypy-ints pypy-strings \npypy-.oats disaster 243.4 239.7 242.9 241.3 235.0 257.6 208.0 216.6 Feedparser invindex 69.9 45.4 72.6 \n40.8 70.6 45.1 69.2 42.2 69.2 42.4 74.2 42.7 71.9 40.6 73.0 42.7 multiwords 72.7 86.5 87.0 86.7 85.8 \n88.3 88.7 88.7 NetworkX 247.8 163.1 253.9 261.4 132.7 126.0 156.5 173.0 nltk-wordassoc 87.7 87.7 90.0 \n87.8 86.5 93.4 86.4 93.4 orm 136.6 133.3 139.8 133.9 127.5 137.2 128.7 135.4 PyDbLite PyExcelerator Scapy \nslowsets 101.9 308.9 67.1 1812.8 101.2 295.6 68.7 1297.6 106.9 310.0 66.5 1812.8 101.9 309.2 67.5 1297.9 \n101.2 295.0 65.3 1557.0 120.7 325.8 67.4 1556.9 102.6 294.6 64.6 1297.5 122.5 335.3 68.8 1556.9 whoosh \n72.8 71.2 72.1 72.9 70.6 71.6 71.3 72.3 Ratio 0.94 1.024 0.983 0.926 0.975 0.919 0.991 ements. However, \nthis should apply equally well to language implementations running atop another JIT-based VM. 7. Related \nWork Pointer tagging is the obvious alternative to storage strate\u00adgies (see Section 2.1). There is currently \nno direct comparison between pointer tagging and storage strategies. It is therefore possible that pointer \ntagging could provide a more ef.cient solution to storing easily tagged items such as integers or .oats. \nHowever, as Table 3 clearly shows, a signi.cant part of the speed-up of storage strategies comes from \nstrings. On a 64-bit machine, 7 bytes in a tagged pointer could be used for strings; strings longer than \nthat would have to be allocated on the heap. To get an idea of how often large strings are used, we created \na PyPy variant (included in our downloadable ex\u00adperiment) to count the length of every string allocated \nin the benchmark suite from Section 5. 64% of the strings created by our benchmark suite from Section \n5 are larger than 7 bytes (85% are bigger than 3 bytes, the cut-off point for 32-bit ma\u00adchines). We caution \nagainst over-interpreting these numbers. First, we can not easily separate out strings used internally \nby PyPy s interpreter. However such strings are, by design, small, so the numbers we obtained are lower-bounds \nfor the real numbers of strings too large to be used with tagging. Sec\u00adond, we suspect that the length \nof strings used by different programs varies more than many other factors. Nevertheless, these numbers \nindicate that storage strategies can outperform pointer tagging. The inspiration behind storage strategies \nis the map con\u00adcept, which originated in the Self project as a way to ef.\u00adciently represent prototype \nobjects with varying sets of in\u00adstance .eld names [11]. This is done by splitting objects into a (mostly \nconstant) map and a (continually changing) storage area. The map describes where within the storage area \nslot names can be looked up. Maps have two bene.ts. When us\u00ading an interpreter or a JIT, they lower memory \nconsumption (since maps are shared between objects, with only the storage area being unique to each object). \nWhen the JIT is used, they also signi.cantly speed-up slot lookups because the constant nature of maps \nallows JITs to fold away many computations based upon them. Storage strategies similarly split apart \na collection and its data; some optimised storage strategies also identify some data as being constant \nin a similar way to maps, though not all strategies are able to do so. The only other VM we are aware \nof which uses a mecha\u00adnism comparable to storage strategies is V8 [13, 14]. Arrays are specialised with \nan element kind which determines the memory layout, in similar fashion to storage strategies. Unlike \nPyPy, in V8 an array s default storage strategy is equivalent to what we might call IntegerArrayStrategy. \nIf a .oat is added to an array with IntegerArrayStrategy, the storage strategy is changed to FloatArrayStrategy \nand the storage area revised accordingly. Non-numeric objects added to arrays with IntegerArrayStrategy \nor FloatArrayStrategy cause a transition to ObjectArray-Strategy, forcing existing elements to be reboxed. \nEmpty\u00ading a V8 array does not reset its element kind, which thus moves monotonically towards ObjectArrayStrategy. \nTo avoid strategy changes, array creation sites in a program can create arrays with different initial \nstrategies. To do this, arrays can track from which part of the program they were created. When an array \ns storage strategy changes, the site which created it is informed. Subsequent arrays created from that \nsite are initialised with the revised storage strategy (though existing arrays are left as-is), even \nif those subsequent arrays will never change strategy. V8 s element kinds thus aim to minimise type dehomogenisation, \nwhile PyPy s storage strategies attempt to optimise every possible collection. To understand how these \ntwo schemes work in practise, we created a PyPy variant (included in our Figure 6. The number (upper \n.gure on transitions), and average size (lower .gure, left off for transitions involving empty strategies), \nof collections when they switch storage strategies. Bold transitions indicate the most potentially expensive \ntransitions, when type dehomogenisation occurs.   downloadable experiment) to see how often collections \nof one speci.c creation site type dehomogenise. 98.8% of the collection creation sites do not type dehomogenise \nat all. Of those which do type dehomogenise, around one third always create collections that all dehomogenise. \nThe remaining two thirds of sites create some collections which dehomogenise and some which do not; the \ndistribution has no clear pattern and we are currently unable to determine the size of the collections \ninvolved. V8 has several mechanisms to mitigate the likelihood of array creation sites unnecessarily \nbeing set to Object-ArrayStrategy (which is particularly likely for e.g. factory functions). Notably, \nwhen a function is compiled by the JIT compiler, array call sites become .xed and any changes of strategy \nto arrays created from those sites are ignored. A deoptimisation has to occur before those call sites \nelement kinds can be reconsidered. Overall, it is unclear what the performance implications of V8 and \nPyPy s approaches are: both have troubling worst\u00adcase performance, but neither situation appears to happen \noften in real programs. We suspect that each approach will perform similarly due to the typical nature \nof dynamically typed programs (see Section 5.6). While storage strategies and V8 s kinds are fully dynamic, \nHackett and Guo show how a type inferencer for Java can optimise many arrays [22]. The unsound type inferencer \nis combined with dynamic type checks to make a reliable system: type assumptions are adjusted as and \nwhen the program violates them. Similarly to V8, array types are referenced from each creation site. \nWhile, in Javascript, this may well result in similar, or better, performance to V8 s kinds, its relevance \nto Python is unclear. Python programs frequently frustrate type inferencers [10], which may only allow \nsmall improvements in collections performance. There has been substantial work on memory optimisation \nin the JVM, though most of it is orthogonal to storage strate\u00adgies, perhaps re.ecting the work s origins \nin optimising em\u00adbedded systems. [34] details various memory inef.ciencies found from a series of Java \nbenchmarks, before describing how previously published approaches (e.g. object sharing [2] and data compression \n[1]) may be able to address them. Such techniques can potentially be applied alongside storage strate\u00adgies, \nthough it is unclear if the real-world bene.ts would justify the effort involved in doing so. 8. Conclusions \nStorage strategies are a simple technique for improving the performance of real-world dynamically typed \nlanguages and which we believe has wide applicability. We suspect that the core storage strategies presented \nin this paper (particularly lists and dictionaries, integers and strings) will be applica\u00adble to virtually \nevery dynamically typed language and its implementations. However, different language s semantics and \nidioms of use may mean that different exotic storage strategies are needed relative to those in PyPy. \nStorage strategies also make it feasible to experiment with more exotic techniques such as data compression, \nand could also allow user-level code to determine storage layout (in similar spirit to CLOS). It may \nalso be possible to reuse parts of existing strategies when implementing new collection types. An important \navenue for future research will be to reduce storage strategies worst case behaviour (when type deho\u00admogenising \ncollections using specialised storage strategies). It is possible that a hybrid of V8 s kinds (which \ntake into account a collection s creation site) and storage strategies may allow a scheme to be created \nwhich avoids the worst case performance of each. Acknowledgments We thank: Michael Leuschel, David Schneider, \nSamuele Pe\u00addroni, Sven Hager, and Michael Perscheid for insightful com\u00adments on various drafts of the \npaper; and Daniel Clifford, Vyacheslav Egorov, and Michael Stanton for providing in\u00adsight on V8 s mechanism. \nThis research was partly funded by the EPSRC Cooler grant EP/K01790X/1. References [1] C. S. Ananian \nand M. Rinard. Data size optimizations for Java programs. In Proc. LCTES, pages 59 68, 2003. [2] A. W. \nAppel and M. J. Gonc\u00b8alves. Hash-consing garbage col\u00adlection. Technical Report CS-TR-412-93, Princeton \nUniversity, Dept. of Computer Science, 1993. [3] V. Bala, E. Duesterwald, and S. Banerjia. Dynamo: a \ntranspar\u00adent dynamic optimization system. In Proc. PLDI, pages 1 12, 2000. [4] M. Bayer. Orm2010, 2012. \nURL http://techspot.zzzeek. org. [Online; accessed 26-March-2013]. [5] S. Bird, E. Klein, and E. Loper. \nNatural Language Processing with Python. O Reilly Media, July 2009. [6] C. F. Bolz and L. Tratt. The \nimpact of meta-tracing on VM design and implementation. To appear, Science of Computer Programming, 2013. \n[7] C. F. Bolz, A. Cuni, M. Fijalkowski, and A. Rigo. Tracing the meta-level: PyPy s tracing JIT compiler. \nIn Proc. ICOOOLPS, pages 18 25, 2009. [8] C. F. Bolz, A. Cuni, M. Fijalkowski, M. Leuschel, S. Pedroni, \nand A. Rigo. Allocation removal by partial evaluation in a tracing JIT. Proc. PEPM, Jan. 2011. [9] O. \nCalla \u00b4othlisberger. How de\u00ad u, R. Robbes, E. Tanter, and D. R \u00a8velopers use the dynamic features of \nprogramming languages: the case of smalltalk. In Proc. MSR, page 23 32, 2011. [10] B. Cannon. Localized \ntype inference of atomic types in Python. Master thesis, California Polytechnic State University, 2005. \n[11] C. Chambers, D. Ungar, and E. Lee. An ef.cient implemen\u00adtation of SELF a dynamically-typed object-oriented \nlanguage based on prototypes. In OOPSLA, volume 24, 1989.  [12] C. Chambers, D. Ungar, and E. Lee. An \nef.cient implemen\u00adtation of self a dynamically-typed object-oriented language based on prototypes. SIGPLAN \nNot., 24:49 70, Sept. 1989. [13] D. Clifford. URL http://v8-io12.appspot.com/. Talk at IO12 [Online; \naccessed 27-June-2013]. [14] D. Clifford, V. Egorov, and M. Stanton. Personal communica\u00adtion, July 2013. \n[15] J. da Silva and G. Lopes. A local maxima method and a fair dispersion normalization for extracting \nmulti-word units from corpora. In Meeting on Mathematics of Language, 1999. [16] FeedParser Developers. \nFeedparser, 2012. URL http:// code.google.com/p/feedparser/. [Online; accessed 26\u00adMarch-2013]. [17] P. \nFleming and J. Wallace. How not to lie with statistics: the correct way to summarize benchmark results. \nCommun. ACM, 29(3):218 221, Mar. 1986. [18] A. Gal, B. Eich, M. Shaver, D. Anderson, D. Mandelin, M. \nR. Haghighat, B. Kaplan, G. Hoare, B. Zbarsky, J. Oren\u00addorff, J. Ruderman, E. W. Smith, R. Reitmaier, \nM. Bebenita, M. Chang, and M. Franz. Trace-based Just-In-Time type spe\u00adcialization for dynamic languages. \nIn Proc. PLDI, 2009. [19] E. Gamma, R. Helm, R. E. Johnson, and J. Vlissides. Design Patterns: Elements \nof Reusable Object-Oriented Software. Addison-Wesley Longman, Amsterdam, Oct. 1994. [20] A. Georges, \nD. Buytaert, and L. Eeckhout. Statistically rigorous Java performance evaluation. SIGPLAN Notices, 42 \n(10):57 76, 2007. [21] D. Gudeman. Representing type information in Dynamically-Typed languages. Technical \nReport TR93-27, University of Arizona at Tucson, 1993. [22] B. Hackett and S.-y. Guo. Fast and precise \nhybrid type inference for JavaScript. In Proc. PLDI, pages 239 250, 2012. [23] A. Holkner and J. Harland. \nEvaluating the dynamic behaviour of python applications. In Proc. ACSC, pages 19 28, 2009. [24] T. Kotzmann \nand H. M \u00a8ock. Escape analysis in the con\u00ad ossenb\u00a8text of dynamic compilation and deoptimization. In \nProc. VEE, page 111 120, 2005. [25] Logilab. Pylint, 2012. URL http://www.logilab.org/ project/pylint. \n[Online; accessed 26-March-2013]. [26] NetworkX Developers. Networkx, 2012. URL http:// networkx.lanl.gov. \n[Online; accessed 26-March-2013]. [27] pyExcelerator Developers. pyexcelerator, 2012. URL http: //sourceforge.net/projects/pyexcelerator. \n[Online; accessed 26-March-2013]. [28] PyPy Team. Pypy speed benchmarks, 2013. URL http: //speed.pypy.org/. \n[Online; accessed 27-June-2013]. [29] P. Quentel. Pydblite, 2012. URL http://www.pydblite. net/en/index.html. \n[Online; accessed 26-March-2013]. [30] A. Radziszewski and M. Piasecki. A preliminary Noun Phrase Chunker \nfor Polish. In Proc. IOS, pages 169 180. Springer, 2010. Chunker available at http://nlp.pwr.wroc.pl/trac/private/disaster/. \n[31] G. Richards, S. Lebresne, B. Burg, and J. Vitek. An analysis of the dynamic behavior of JavaScript \nprograms. In Proc. PLDI, pages 1 12, 2010. [32] A. Rigo and S. Pedroni. PyPy s approach to virtual machine \nconstruction. In Proc. DLS, 2006. [33] Rosetta Code. Inverted index, 2012. URL http:// rosettacode.org/wiki/Inverted_index#Python. \n[On\u00adline; accessed 26-March-2013]. [34] J. B. Sartor, M. Hirzel, and K. S. McKinley. No bit left behind: \nthe limits of heap data compression. In Proc. ISSM, pages 111 120. ACM, 2008. [35] L. Tratt. Dynamically \ntyped languages. Advances in Comput\u00aders, 77:149 184, July 2009. [36] Whoosh Developers. Whoosh, 2012. \nURL https:// bitbucket.org/mchaput/whoosh. [Online; accessed 26\u00adMarch-2013].  A. Appendix  Table 5. \nExecution speed for benchmarks (in seconds). Benchmark pypy-none pypy-all pypy-list pypy-set pypy-dict \npypy-ints pypy-strings pypy-.oats disaster 14.219 \u00b1 0.206 13.357 \u00b1 0.223 14.651 \u00b1 0.201 13.496 \u00b1 0.186 \n13.743 \u00b1 0.210 13.403 \u00b1 0.181 13.241 \u00b1 0.200 13.254 \u00b1 0.134 Feedparser 0.801 \u00b1 0.258 0.800 \u00b1 0.265 0.799 \n\u00b1 0.263 0.800 \u00b1 0.231 0.797 \u00b1 0.257 0.803 \u00b1 0.265 0.803 \u00b1 0.256 0.798 \u00b1 0.243 invindex 20.643 \u00b1 0.167 \n2.808 \u00b1 0.042 2.797 \u00b1 0.010 23.774 \u00b1 0.112 24.199 \u00b1 0.056 23.607 \u00b1 0.067 2.826 \u00b1 0.015 24.842 \u00b1 0.131 \nmultiwords 1.293 \u00b1 0.015 1.238 \u00b1 0.016 1.233 \u00b1 0.015 1.256 \u00b1 0.013 1.285 \u00b1 0.016 1.279 \u00b1 0.037 1.339 \n\u00b1 0.046 1.311 \u00b1 0.054 NetworkX 2.383 \u00b1 0.054 1.387 \u00b1 0.015 2.274 \u00b1 0.009 2.489 \u00b1 0.016 1.486 \u00b1 0.010 \n1.338 \u00b1 0.014 1.759 \u00b1 0.017 1.865 \u00b1 0.029 nltk-wordassoc 1.160 \u00b1 0.015 1.077 \u00b1 0.014 1.100 \u00b1 0.012 1.171 \n\u00b1 0.014 1.142 \u00b1 0.011 1.177 \u00b1 0.013 1.062 \u00b1 0.014 1.180 \u00b1 0.015 orm 24.584 \u00b1 2.057 24.330 \u00b1 1.953 24.542 \n\u00b1 2.066 24.531 \u00b1 1.986 24.006 \u00b1 2.059 24.265 \u00b1 2.179 23.866 \u00b1 2.134 24.712 \u00b1 2.099 PyDbLite 0.221 \u00b1 0.003 \n0.196 \u00b1 0.003 0.227 \u00b1 0.002 0.230 \u00b1 0.001 0.193 \u00b1 0.001 0.253 \u00b1 0.001 0.196 \u00b1 0.001 0.252 \u00b1 0.001 PyExcelerator \n12.106 \u00b1 0.048 11.932 \u00b1 0.029 12.797 \u00b1 0.045 11.862 \u00b1 0.046 11.427 \u00b1 0.027 12.293 \u00b1 0.033 11.571 \u00b1 0.030 \n12.037 \u00b1 0.025 Scapy 0.202 \u00b1 0.031 0.136 \u00b1 0.026 0.202 \u00b1 0.030 0.204 \u00b1 0.031 0.137 \u00b1 0.026 0.170 \u00b1 0.035 \n0.141 \u00b1 0.026 0.174 \u00b1 0.036 slowsets 38.333 \u00b1 0.122 33.777 \u00b1 0.838 39.027 \u00b1 1.073 34.767 \u00b1 1.607 38.669 \n\u00b1 0.091 37.750 \u00b1 0.231 34.249 \u00b1 0.844 37.831 \u00b1 0.096 whoosh 1.435 \u00b1 0.014 1.313 \u00b1 0.015 1.395 \u00b1 0.014 \n1.416 \u00b1 0.015 1.291 \u00b1 0.020 1.387 \u00b1 0.015 1.278 \u00b1 0.014 1.401 \u00b1 0.012 ai 0.064 \u00b1 0.003 0.048 \u00b1 0.004 \n0.055 \u00b1 0.003 0.054 \u00b1 0.003 0.060 \u00b1 0.003 0.047 \u00b1 0.003 0.067 \u00b1 0.003 0.056 \u00b1 0.003 bm chameleon 0.017 \n\u00b1 0.000 0.015 \u00b1 0.000 0.017 \u00b1 0.000 0.016 \u00b1 0.000 0.015 \u00b1 0.000 0.017 \u00b1 0.000 0.016 \u00b1 0.000 0.016 \u00b1 0.000 \nbm mako 0.030 \u00b1 0.014 0.029 \u00b1 0.013 0.031 \u00b1 0.014 0.031 \u00b1 0.014 0.030 \u00b1 0.013 0.030 \u00b1 0.013 0.031 \u00b1 0.013 \n0.031 \u00b1 0.014 chaos 0.020 \u00b1 0.001 0.008 \u00b1 0.000 0.008 \u00b1 0.000 0.016 \u00b1 0.000 0.016 \u00b1 0.000 0.008 \u00b1 0.000 \n0.020 \u00b1 0.000 0.021 \u00b1 0.000 crypto pyaes 0.055 \u00b1 0.002 0.054 \u00b1 0.002 0.054 \u00b1 0.002 0.054 \u00b1 0.002 0.053 \n\u00b1 0.002 0.053 \u00b1 0.002 0.054 \u00b1 0.002 0.054 \u00b1 0.002 django 0.051 \u00b1 0.000 0.037 \u00b1 0.000 0.047 \u00b1 0.000 0.051 \n\u00b1 0.000 0.040 \u00b1 0.000 0.044 \u00b1 0.000 0.036 \u00b1 0.000 0.044 \u00b1 0.000 fannkuch 0.148 \u00b1 0.001 0.142 \u00b1 0.000 \n0.143 \u00b1 0.001 0.171 \u00b1 0.001 0.168 \u00b1 0.001 0.143 \u00b1 0.001 0.179 \u00b1 0.001 0.158 \u00b1 0.001 genshi text 0.022 \n\u00b1 0.002 0.019 \u00b1 0.001 0.022 \u00b1 0.001 0.024 \u00b1 0.002 0.019 \u00b1 0.001 0.023 \u00b1 0.001 0.019 \u00b1 0.001 0.022 \u00b1 0.002 \ngo 0.098 \u00b1 0.037 0.092 \u00b1 0.037 0.095 \u00b1 0.038 0.098 \u00b1 0.037 0.098 \u00b1 0.037 0.092 \u00b1 0.037 0.100 \u00b1 0.037 \n0.099 \u00b1 0.037 html5lib 2.626 \u00b1 0.058 2.426 \u00b1 0.042 2.466 \u00b1 0.054 2.682 \u00b1 0.055 2.508 \u00b1 0.056 2.449 \u00b1 \n0.039 2.405 \u00b1 0.041 2.454 \u00b1 0.038 meteor-contest 0.154 \u00b1 0.002 0.099 \u00b1 0.002 0.156 \u00b1 0.002 0.097 \u00b1 0.001 \n0.155 \u00b1 0.001 0.100 \u00b1 0.002 0.145 \u00b1 0.002 0.104 \u00b1 0.002 nbody modi.ed 0.035 \u00b1 0.000 0.035 \u00b1 0.000 0.032 \n\u00b1 0.000 0.033 \u00b1 0.002 0.033 \u00b1 0.000 0.033 \u00b1 0.000 0.033 \u00b1 0.000 0.033 \u00b1 0.000 py.ate-fast 0.381 \u00b1 0.008 \n0.324 \u00b1 0.008 0.331 \u00b1 0.009 0.388 \u00b1 0.017 0.391 \u00b1 0.018 0.358 \u00b1 0.007 0.347 \u00b1 0.013 0.381 \u00b1 0.008 raytrace-simple \n0.024 \u00b1 0.004 0.024 \u00b1 0.006 0.025 \u00b1 0.005 0.026 \u00b1 0.005 0.025 \u00b1 0.005 0.024 \u00b1 0.004 0.024 \u00b1 0.004 0.025 \n\u00b1 0.007 richards 0.004 \u00b1 0.000 0.004 \u00b1 0.000 0.004 \u00b1 0.000 0.004 \u00b1 0.000 0.004 \u00b1 0.000 0.004 \u00b1 0.000 \n0.004 \u00b1 0.001 0.004 \u00b1 0.000 slowspit.re 0.275 \u00b1 0.004 0.250 \u00b1 0.002 0.227 \u00b1 0.009 0.270 \u00b1 0.007 0.271 \n\u00b1 0.008 0.275 \u00b1 0.012 0.250 \u00b1 0.020 0.284 \u00b1 0.013 spambayes 0.085 \u00b1 0.047 0.081 \u00b1 0.044 0.083 \u00b1 0.046 \n0.086 \u00b1 0.045 0.082 \u00b1 0.043 0.082 \u00b1 0.046 0.082 \u00b1 0.043 0.083 \u00b1 0.046 spectral-norm 0.014 \u00b1 0.001 0.014 \n\u00b1 0.001 0.014 \u00b1 0.001 0.014 \u00b1 0.001 0.014 \u00b1 0.001 0.014 \u00b1 0.001 0.014 \u00b1 0.001 0.014 \u00b1 0.001 sympy integrate \n0.872 \u00b1 0.218 0.815 \u00b1 0.152 0.887 \u00b1 0.214 0.868 \u00b1 0.222 0.849 \u00b1 0.208 0.884 \u00b1 0.204 0.839 \u00b1 0.148 0.876 \n\u00b1 0.208 telco 0.045 \u00b1 0.006 0.039 \u00b1 0.007 0.041 \u00b1 0.007 0.043 \u00b1 0.007 0.039 \u00b1 0.008 0.039 \u00b1 0.006 0.039 \n\u00b1 0.008 0.041 \u00b1 0.006 twisted names 0.002 \u00b1 0.000 0.002 \u00b1 0.000 0.002 \u00b1 0.000 0.002 \u00b1 0.000 0.002 \u00b1 0.000 \n0.002 \u00b1 0.000 0.002 \u00b1 0.000 0.002 \u00b1 0.000 Benchmark pypy-all pypy-list pypy-ints * pypy-dict * pypy-strings \n* pypy-set * pypy-.oats disaster 0.939 \u00b1 0.021 0.945 \u00b1 0.033 = 0.818 \u00b1 0.028 < Feedparser 0.999 \u00b1 0.461 \n0.991 \u00b1 0.771 = 1.002 \u00b1 0.785 = invindex 0.136 \u00b1 0.002 0.183 \u00b1 0.003 > 0.188 \u00b1 0.003 > multiwords 0.958 \n\u00b1 0.016 0.921 \u00b1 0.026 = 1.038 \u00b1 0.066 = NetworkX 0.582 \u00b1 0.015 0.621 \u00b1 0.025 = 0.324 \u00b1 0.014 < nltk-wordassoc \n0.928 \u00b1 0.017 0.942 \u00b1 0.028 = 0.944 \u00b1 0.029 = orm 0.990 \u00b1 0.115 0.973 \u00b1 0.199 = 0.963 \u00b1 0.203 = PyDbLite \n0.888 \u00b1 0.017 0.939 \u00b1 0.022 > 1.169 \u00b1 0.025 > PyExcelerator 0.986 \u00b1 0.005 0.978 \u00b1 0.009 = 0.965 \u00b1 0.008 \n< Scapy 0.671 \u00b1 0.164 0.680 \u00b1 0.264 = 0.503 \u00b1 0.220 = slowsets 0.881 \u00b1 0.022 0.931 \u00b1 0.050 = 0.868 \u00b1 \n0.023 = whoosh 0.915 \u00b1 0.014 0.863 \u00b1 0.023 < 0.840 \u00b1 0.020 < ai 0.748 \u00b1 0.068 0.694 \u00b1 0.090 = 0.686 \u00b1 \n0.088 = bm chameleon 0.893 \u00b1 0.009 0.761 \u00b1 0.008 < 0.838 \u00b1 0.007 < bm mako 0.956 \u00b1 0.615 1.097 \u00b1 1.211 \n= 1.087 \u00b1 1.202 = chaos 0.403 \u00b1 0.011 0.246 \u00b1 0.012 < 0.430 \u00b1 0.021 = crypto pyaes 0.985 \u00b1 0.050 0.944 \n\u00b1 0.082 = 0.940 \u00b1 0.083 = django 0.721 \u00b1 0.011 0.747 \u00b1 0.017 = 0.541 \u00b1 0.014 < fannkuch 0.956 \u00b1 0.005 \n1.266 \u00b1 0.013 > 1.252 \u00b1 0.012 > genshi text 0.871 \u00b1 0.086 0.933 \u00b1 0.157 = 0.933 \u00b1 0.158 = go 0.940 \u00b1 \n0.517 0.961 \u00b1 0.894 = 0.959 \u00b1 0.887 = html5lib 0.924 \u00b1 0.026 0.915 \u00b1 0.049 = 0.798 \u00b1 0.038 < meteor-contest \n0.645 \u00b1 0.017 0.651 \u00b1 0.022 = 0.415 \u00b1 0.017 < nbody modi.ed 0.986 \u00b1 0.007 0.810 \u00b1 0.061 < 0.821 \u00b1 0.011 \n< py.ate-fast 0.850 \u00b1 0.027 0.906 \u00b1 0.072 = 0.856 \u00b1 0.052 = raytrace-simple 0.990 \u00b1 0.291 1.161 \u00b1 0.552 \n= 1.050 \u00b1 0.522 = richards 0.882 \u00b1 0.066 0.815 \u00b1 0.103 = 0.834 \u00b1 0.266 = slowspit.re 0.908 \u00b1 0.014 0.797 \n\u00b1 0.049 < 0.938 \u00b1 0.099 = spambayes 0.956 \u00b1 0.743 0.967 \u00b1 1.290 = 0.924 \u00b1 1.247 = spectral-norm 0.993 \n\u00b1 0.137 0.993 \u00b1 0.239 = 0.980 \u00b1 0.234 = sympy integrate 0.935 \u00b1 0.291 0.987 \u00b1 0.601 = 0.981 \u00b1 0.561 = \ntelco 0.858 \u00b1 0.185 0.741 \u00b1 0.274 = 0.672 \u00b1 0.233 = twisted names 0.921 \u00b1 0.032 0.915 \u00b1 0.052 = 0.897 \n\u00b1 0.052 = Combined Ratios 0.816 \u00b1 0.034 0.813 \u00b1 0.058 = 0.783 \u00b1 0.057 = Table 6. Ratios of PyPy variants \nmultiplied together (see Section 5.4). The symbols <, = or > indicate whether, after taking the con.dence \nintervals into account, the result is smaller than, equal to, or larger than pypy-all.   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Dynamically typed language implementations often use more memory and execute slower than their statically typed cousins, in part because operations on collections of elements are unoptimised. This paper describes storage strategies, which dynamically optimise collections whose elements are instances of the same primitive type. We implement storage strategies in the PyPy virtual machine, giving a performance increase of 18% on wide-ranging benchmarks of real Python programs. We show that storage strategies are simple to implement, needing only 1500LoC in PyPy, and have applicability to a wide range of virtual machines.</p>", "authors": [{"name": "Carl Friedrich Bolz", "author_profile_id": "81436599727", "affiliation": "University of D&#252;sseldorf, D&#252;sseldorf, Germany", "person_id": "P4290328", "email_address": "cfbolz@gmx.de", "orcid_id": ""}, {"name": "Lukas Diekmann", "author_profile_id": "83358810957", "affiliation": "University of D&#252;sseldorf, D&#252;sseldorf, Germany", "person_id": "P4290329", "email_address": "lukas.diekmann@uni-duesseldorf.de", "orcid_id": ""}, {"name": "Laurence Tratt", "author_profile_id": "81316491200", "affiliation": "King's College London, London, United Kingdom", "person_id": "P4290330", "email_address": "laurie@tratt.net", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509531", "year": "2013", "article_id": "2509531", "conference": "OOPSLA", "title": "Storage strategies for collections in dynamically typed languages", "url": "http://dl.acm.org/citation.cfm?id=2509531"}