{"article_publication_date": "10-29-2013", "fulltext": "\n Bounded Partial-Order Reduction Katherine E. Coons* Madanlal Musuvathi Kathryn S. McKinley* The University \nof Texas at Austin* Microsoft Research Abstract Eliminating concurrency errors is increasingly important \nas systems rely more on parallelism for performance. Exhaus\u00adtively exploring the state-space of a program \ns thread in\u00adterleavings .nds concurrency errors and provides coverage guarantees, but suffers from exponential \nstate-space explo\u00adsion. Two prior approaches alleviate state-space explosion. (1) Dynamic partial-order \nreduction (DPOR) provides full coverage and explores only one interleaving of independent transitions. \n(2) Bounded search provides bounded coverage by enumerating interleavings that do not exceed a bound. \nIn particular, we focus on preemption-bounding. Combin\u00ading partial-order reduction with preemption-bounding \nhad remained an open problem. We show that preemption-bounded search explores the same partial orders \nrepeatedly and consequently explores more executions than unbounded DPOR, even for small bounds. We further \nshow that if DPOR simply uses the pre\u00ademption bound to prune the state space as it explores new partial \norders, it misses parts of the state space reachable in the bound and is therefore unsound. The bound \nessen\u00adtially induces dependences between otherwise independent transitions in the DPOR state space. We \nintroduce Bounded Partial Order Reduction (BPOR), a modi.cation of DPOR that compensates for bound dependences. \nWe identify prop\u00aderties that determine how well bounds combine with partial\u00adorder reduction. We prove \nsound coverage and empirically evaluate BPOR with preemption and fairness bounds. We show that by eliminating \nredundancies, BPOR signi.cantly reduces testing time compared to bounded search. BPOR s faster incremental \nguarantees will help testers verify larger concurrent programs. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. Copyrights for components of this work owned by others than ACM must \nbe honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. Request permissions from \npermissions@acm.org. OOPSLA 13, October 29 31, 2013, Indianapolis, IN, USA. Copyright &#38;#169; 2013 \nACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/http://dx.doi.org/10.1145/2509136.2509556 \nCategories and Subject Descriptors D.2.4 [Software Engi\u00adneering]: Software/Program Veri.cation; D.2.5 \n[Software Engineering]: Testing and Debugging General Terms Algoirthms, Veri.cation Keywords bounded \npartial-order reduction, dynamic partial\u00adorder reduction, concurrency, fairness, liveness, model check\u00ading, \nshared-memory programs, software testing 1. Introduction Concurrency errors are notoriously dif.cult \nto debug be\u00adcause they often occur only under unexpected thread inter\u00adleavings. One approach to identify \nand reproduce concur\u00adrency errors is stateless model checking [8], which system\u00adatically drives the program \nalong possible thread interleav\u00adings. This approach is limited in practice, however, by state\u00adspace explosion. \nThe number of possible thread interleav\u00adings grows exponentially with the size of the program and number \nof threads. Two existing approaches alleviate state\u00adspace explosion: partial-order reduction and bounded \nsearch. Partial order methods provide full coverage by exploring all unique reachable states, and they \nalleviate state space ex\u00adplosion by exploring only one interleaving of independent events [6, 7]. Independent \nevents commute their inter\u00adleavings do not change program behavior. Dynamic partial\u00adorder reduction \n(DPOR) observes the program s actual de\u00adpendences at runtime, rather than a static conservative esti\u00admate \nof dependences, to identify independent events [6]. In practice, the reduced state space is much smaller \nthan the full state space. Searching this state space is suf.cient to verify safety properties such as \nabsence of deadlocks and adherence to local assertions [7]. The size of this reduced space still grows \nexponentially, but with the number of de\u00adpendent events in the program, rather than the total num\u00adber \nof events. Nevertheless, for larger programs DPOR often runs for longer than developers are willing to \nwait. Bounded search, in contrast, alleviates state-space ex\u00adplosion by pruning executions that exceed \na bound [5, 12, 14]. For example, Musuvathi and Qadeer use preemption\u00adbounded search [12] to explore \nexecutions involving a small number of preemptions. Their insight is that many concur\u00adrency bugs require \na small number of preemptions to man\u00adifest. Therefore, by focusing on a much smaller portion of Figure \n1. Preemption-bounded search with DPOR. Al\u00adthough s' is reachable within the bound, the search never \nreaches it.  the state space, preemption-bounded search often .nds bugs faster than unbounded search \ndoes. At the same time, such a search provides a useful coverage metric to the user when a search with \nk preemptions has .nished, any remaining bug requires at least k + 1 preemptions, and as such is more \ncom\u00adplex to .nd and less likely to happen in practice [12]. Our experiments show that preemption-bounded \nsearch is useful only for very small bounds. For example, searches that allow more than two preemptions \nfor our benchmarks explore so many redundant executions of the same partial or\u00adder that they explore \nmany more executions than unbounded DPOR. Thus, preemption-bounded search fails to improve performance \nprecisely on those programs where it holds promise -for large programs where DPOR either runs longer \nthan developers are willing to wait, or does not terminate. This result motivates combining preemption-bounded \nsearch with DPOR. Figure 1 illustrates why combining these two approaches has remained an open problem \nuntil now. The .gure shows the state space for a program with two threads u and v and three instructions. \nEach transition in the state space corresponds to a thread executing an instruction. Starting from the \ninitial state s0, the transitions t1 of thread u and t3 of thread v are independent as they access different \nmemory locations. Thus, executing the transitions in either order reaches the same state. However, transitions \nt2 and t3 are dependent, leading to two different .nal states, s3 and ' s. Note, both .nal states are \nreachable with no preemp\u00adtions, respectively through executions t1, t2, t3 and t3, t1, t2. However, a \nna\u00a8ive combination of DPOR with preemption ' bounding can miss exploring s. Consider combining DPOR with \npreemption-bounded search with bound 0. Assume the search .rst explores ex\u00adecution t1, t2, t3, as illustrated \nin the .gure. The DPOR al\u00adgorithm achieves state-space reduction by carefully insert\u00ading backtrack points \nonly where necessary to explore both orderings of dependent transitions. In the example, because t3 and \nt2 are dependent, DPOR schedules t3 at s1, prior to dependent transition t2, but not at s0 since t3 and \nt1 are inde\u00adpendent. However, executing t3 in thread v from s1 requires preempting thread u since it \nis still enabled in s1. Since the preemption-bound is 0, the algorithm will not explore this execution \n it exceeds the bound. This na\u00a8ive combination is therefore unsound. This paper proposes an ef.cient \nalgorithm called Bounded Partial Order Reduction (BPOR) to soundly combine DPOR with preemption-bounded \nsearch. The key idea behind BPOR is to conservatively add more backtrack points be\u00adyond what DPOR requires. \nIf a backtrack point required by DPOR requires a preemption, then BPOR conservatively backtracks to a \nprevious point in the execution where the backtracked transition does not require a preemption. In the \nexample in Figure 1, when DPOR schedules t3 at s1, BPOR additionally schedules t3 at s0. Subsequently, \nBPOR will prune the execution that exceeds the bound at s1, but not at s0, eventually exploring both \n.nal states. In this paper, we prove that BPOR with the preemption\u00adbound is sound. In particular, given \na preemption-bound, when BPOR terminates it guarantees that it explored all states reachable within the \nbound in the state space. As the bound increases, BPOR gradually approaches the coverage and time of \nfull DPOR. Although adding conservative backtrack points decreases the amount of partial-order reduction, \nwe empirically show that BPOR vastly reduces the exploration of redundant exe\u00adcutions of the same partial \norder. We also identify properties of bound functions that do not require conservative back\u00adtrack points. \nUnfortunately, the bounds studied in this paper do not satisfy these properties. In addition, we recast \nfair stateless model checking as bounded search [14] and show how to combine BPOR with fair-bounded search. \nWhereas DPOR cannot explore cyclic state spaces, BPOR with fair-bounded search soundly ex\u00adplores the \nfair-bounded state space when the state space is cyclic. Most of our test programs have cyclic state \nspaces that result from the use of lock-free constructs such as spin loops. Fair exploration [14] is \nessential to scale BPOR to these benchmarks. Our empirical evaluation shows that BPOR provides bounded \ncoverage far more quickly than preemption-bounded search does. We also show that BPOR .nds more bugs \nfaster than DPOR or preemption-bounded search alone. Section 2 formalizes our representation of concurrent \nprograms, and reviews partial-order reduction and bounded search. Section 3 compares DPOR with bounded \nsearch and shows why na\u00a8ively combining them sacri.ces bounded cov\u00aderage. The algorithms in Section 4 \ncompensate for bound dependences, computing a suf.cient set of transitions to ex\u00adplore in each state. \nSection 5 introduces bounded partial\u00adorder reduction (BPOR) and computes suf.cient sets. For brevity \nhere within, we state the theorems and put the proofs in companion materials available from the ACM Digital \nLi\u00adbrary [3]. Coons dissertation contains a fuller treatment [2].  Section 6 evaluates BPOR performance \non concurrent unit tests and Section 7 concludes. 2. Background and related work This section provides \nnecessary background and describes our formalism for concurrent programs, partial-order reduc\u00adtion, and \nbounded search. 2.1 Stateless model checking We use stateless model checking to systematically explore \nthe state space of multithreaded programs [8]. In such an exploration, a runtime scheduler systematically \nforces con\u00adcurrent programs down different thread interleavings start\u00ading from the same initial state. \nThere are two useful exploration modes for an implemen\u00adtation of stateless model checking. In synchronization \nmode, the model checker interleaves threads only at synchroniza\u00adtion primitives such as locks, events, \nand volatile variables. In this mode, the checker .nds all concurrency errors that do not require a data \nrace to trigger, and it .nds all data races. In data-race mode, the checker additionally interleaves \nthreads at every shared memory location. This mode is use\u00adful only if the programmer intentionally introduced \ndata races in the program. The model checker determines whether these data races are indeed benign that \nis, do not trigger concurrency errors. Usually, data-race mode is more expen\u00adsive because it requires \nadditional overhead to instrument shared-memory accesses, and an (exponential) increase in state space \nresulting from .ne-grained interleaving. BPOR is applicable to both modes.  2.2 Multithreaded programs \nand semantics A concurrent program contains a .xed set Tid of thread identi.ers and a set T of transitions. \nA transition t . T is a tuple (tid, var, op) that represents an operation t.op on variable(s) t.var performed \nby a thread t.tid . Tid. Example operations include fork, join, lock acquire, lock release, and load/store \noperations. In stateless model checking, a program state is conserva\u00adtively identi.ed by the sequence \nof transitions executed to reach that state from the initial state. Note that it is possi\u00adble for different \nsequences of transitions to lead to the same program state represented by the contents of the program \nvariables (including registers, stack, and heap). One such possibility is when the two sequences differ \nonly in the or\u00adder of independent transitions, such as two threads accessing different memory locations. \nPartial-order reduction seeks to eliminate such redundancies. Accordingly, we de.ne a state as the directed \nacyclic graph (T , H ) where T . T is a set of transitions and H (for happens-before ) is an irre.exive \npartial order on T . H captures all the dependences between the transitions such that reordering transitions \nnot related by H behaviorally leads to the same program state. In other words, two tran\u00adsition sequences \ncontaining the same set of transitions T Figure 2. Program representation: S is a sequence of tran\u00adsitions \nsuch that state s = .nal(S). Threads u, v . enabled(s). but different linearizations of the partial-order \nH lead to the same program state (T , H ). In this paper, we assume that all transitions of a single \nthread are dependent on each other. Accordingly, we will assume that for each thread u, H is a total \norder on the set {t . T | t.tid = u}. There exists a unique initial state (T0, H0) where T0 is the empty \nset and H0 is the empty relation. A transition t transfers the program from a state (T , H ) to a successor \n' ' state, (T ' , H '), where T = T . {t}, and H is the partial order H with any additional orderings \nrequired by t. The local state for a thread u in a state s = (T , H ), denoted by local(s, u), is de.ned \nas the subgraph of s that contains only those transitions that happen-before the most recent transition \nby u. Essentially, these are the transitions that are guaranteed to drive the thread to said local state. \nFormally, if u has no transitions in T then local(s, u) = (\u00d8, \u00d8). Otherwise, if t is the most recent \ntransition by u, then local(s, u) = (TL, HL), where ' ' TL = {t . T | (t , t) . H} . {t} '' '' ' HL = \n{(t , t ' ) . H | t . TL and t . TL} Figure 2 illustrates the following terms. The term next(s, u) . \nT denotes the transition that thread u will execute next from state s. We assume that the next transition \nfor each thread from a given state is unique. A transition is enabled in s if it can execute from s. \nA thread u is enabled in s if next(s, u) is enabled in s. The function enabled(s) returns the set of \nall threads enabled in s. A state s in which enabled(s) = \u00d8 is a deadlock state. t ' The expression s \n-s. indicates that transition t leads ' from state s to state s . Using Flanagan and Godefroid s notation \n[6], a transition sequence S is a .nite sequence of transitions t1.t2 . . . tn such that there exist \nstates s1, . . . sn+1 t1tn where s1 is the initial state s0, and s1 -. . . . -. sn+1. The function dom(S) \nreturns the domain of S, the set {1 . . . n}, and the length of S is len(S) = n. The term .nal(S) refers \nto sn+1, the .nal state reached after execut\u00ading all transitions in S. Transition Si is the ith transition \nin S, i . dom(S), and last(S) refers to Sn, the last transi\u00adtion in S. Greek symbols a, \u00df, ., . represent \nany arbitrary\u00adlength sequences of transitions. The term S.t denotes the se\u00adquence of transitions that \nresults when transition t executes from .nal(S), and S.a is the sequence that results when se\u00adquence \nof transitions a executes from .nal(S). An execu\u00adtion is a sequence of transitions where s0 = (T0, H0) \nand enabled(sn+1) = \u00d8.  The rest of our de.nitions exactly follow Flanagan and Godefroid [6]. The behavior \nof a concurrent system is a transition system AG = (State, ., s0) where State is the set of all possible \nstates, . . State\u00d7State is the transition relation de.ned by t ' (s, s ' ) . . iff .t .T - : s . s and \ns0 is the initial state of the system. Bounded search and DPOR each explore only a subset of AG [5, 6, \n12]. A Mazurkiewicz trace is an equivalence class of se\u00adquences of transitions that can be obtained from \none an\u00adother by permuting adjacent independent transitions [11]. A Mazurkiewicz trace, or trace for concision, \nis uniquely de\u00ad.ned by one of its members. We use [.] to denote the trace that contains .. Formally, \nusing Godefroid s de.nition of traces [7], the concurrent alphabet for a system is the pair . = (T , \nD) where T is the .nite set of transitions in the system, and D is the dependence relation. The relation \nI. = T 2 \\ D is independence in .. Let E denote the empty word. The relation =. is the least congruence \nin the monoid [T * ; ., E] such that ' ' (t, t ' ) . I. =. t.t =. t .t We de.ne a trace as follows, De.nition \n2.1. Traces [7]. Equivalence classes of =. are traces over .. The term [.] denotes the trace that contains \nthe sequence of transitions .. Intuitively, a trace is a set of sequences of transitions where each \nsequence in the trace can be derived from each other sequence in the trace by permuting independent transitions. \n 2.3 Dependence relation A dependence relation, D, identi.es transitions whose in\u00adterleavings may lead \nto new states. The dependence relation is critical for partial-order methods because it determines which \ninterleavings may be pruned. The following de.nition characterizes valid dependence relations for the \ntransitions of a concurrent system. De.nition 2.2. Valid dependence relation [6, 10]. Let T be the set \nof transitions in a concurrent system and let D . T \u00d7 T be a binary, re.exive, and symmetric relation. \nThe relation D is a valid dependence relation for the system iff for all t, t ' . T , (t, t ' ) ./D (t \nand t ' are independent) implies that the following properties hold for all states s of the system: \nt 1. if t . enabled(s) and s -' , then t ' . s . enabled(s) iff t ' . enabled(s ' ) ' 2. if t, t ' . \nenabled(s), then there is a unique state s such ee t.tt.t ' ' that s - . s and s - . s We implement the \nvalid dependence relation in De.ni\u00adtion 2.3 and use t . t ' to denote that t is independent with t ' \n, and t -t ' to denote that t is dependent with t ' . These operators apply to sequences of transitions, \nas well. De.nition 2.3. Dependence relation. Transitions t, t ' . T are dependent, t -t ' , iff 1. t.tid \n= t ' .tid, or 2. t.var n t ' .var = \u00d8 . (IsWrite(t.op) . IsWrite(t ' .op))  This relation differentiates \nread operations from write op\u00aderations because multiple read operations to the same vari\u00adable commute. \nWe consider only constant dependence rela\u00adtions in this work and assume that the dependence relation \nis not conditional [9]. Next, we use these de.nitions to provide background for partial-order reduction \nand bounded search.  2.4 Partial-order reduction A search using partial-order reduction provides full \ncover\u00adage: it explores all relevant, reachable states [7]. A state is relevant if it must be explored \nto provide the required safety guarantee. We focus on local state reachability, which guar\u00adantees absence \nof deadlocks and user-speci.ed local asser\u00adtion failures. A state is a partial order on a program s dependent \ntransi\u00adtions. A sequence of transitions is a total order on those tran\u00adsitions. Many total orders may \ncorrespond to a single par\u00adtial order. Whenever possible, partial-order methods explore only one total \norder per partial order. The dependence rela\u00adtion in De.nition 2.3 identi.es transitions whose interleav\u00adings \ndo not affect the partial order on transitions. We de.ne two classes of partial-order reduction algorithms, \npersistent sets and sleep sets, then describe DPOR. 2.4.1 Persistent sets A persistent set in a state \ns is a suf.cient set of transitions to explore from s while maintaining local state reachability for \nacyclic state spaces [9]. A selective search using persistent sets explores a persistent set of transitions \nfrom each state s where enabled(s) \u00d8 and prunes enabled transitions that = are not persistent in s. Godefroid \nde.nes a persistent set as follows. De.nition 2.4. Persistent sets [9]. A set T . T of transitions enabled \nin a state s is persistent in s iff for all nonempty sequences a of transitions from s  ' (a) T is \npersistent in s. (b) t1 is in the sleep set in s. Figure 3. Persistent sets and sleep sets. Transitions \nin gray may be pruned. in AG such that .i . dom(a) : ai . T and for all t . T , t . last(a). Intuitively, \nany transitions reachable via transitions not in T are independent with respect to all transitions in \nT . Fig\u00adure 3(a) illustrates a transition t in the persistent set T in a state s, and a sequence of transitions \na, none of which are in T . The interleaving in gray need not be explored because it is equivalent to \nthe interleaving in black.  2.4.2 Sleep sets Sleep sets prohibit visited transitions from executing \nagain until the search explores a dependent transition. Assume that the search explores transition t \nfrom state s, backtracks t, then explores t ' from s instead. Unless the search explores a transition \nthat is dependent with t, no states are reachable via t ' that were not already reachable via t from \ns. Thus, t sleeps unless a dependent transition is explored. Figure 3(b) illustrates sleep sets. After \nthe search explores t1 and all states reachable via t1 from s, it places t1 in the sleep set for s. No \nnew states become reachable via t1 until the search performs a transition that is dependent with t1. \nThus, t1 propagates to the sleep set in state s ', because t1 . t2. When the search explores t3, however, \nit cannot '' propagate t1 to the sleep set in s because t1 -t3. New states may be reachable via t1 from \ns '', so the search must '' explore t1 from s . Many algorithms reduce the state space with persistent \nsets and sleep sets [7, 16, 17]. Most of these algorithms use static analysis to determine which transitions \nmay be dependent with one another. As a result, these algorithms must be conservative. Unless two transitions \nmust always be independent, the search must assume that they may be dependent. In the next section, we \nreview using dynamic information to reduce the search space. 2.4.3 Dynamic partial-order reduction Dynamic \nPartial-Order Reduction (DPOR) computes per\u00adsistent sets on-the-.y [6]. Unlike static conservative depen\u00addence \ndetection [4, 8, 18], DPOR detects dependences accu\u00adrately at runtime. DPOR performs a depth-.rst search \nof the state space and keeps track of the most recent access to each variable. When a con.icting access \noccurs, DPOR inserts a backtrack point to reverse the order of the dependent ac\u00adcesses in a future execution. \nFlanagan and Godefroid prove that DPOR explores a persistent set of transitions from each state and show \nthat it signi.cantly reduces the search space for a few sample programs [6]. The DPOR algorithm is very \neffective it maintains cov\u00aderage guarantees while signi.cantly reducing search time. DPOR does not provide \nany incremental guarantees, how\u00adever. If the search space is large and the test does not termi\u00adnate suf.ciently \nquickly, then the tester has no guarantees. Additionally, DPOR works only with acyclic state spaces. \nWe wrote unit tests for a set of concurrent data structures us\u00ading publicly-available source code for \nthe .NET 4.0 frame\u00adwork and found that all of the concurrent data structures in\u00adternally use spin loops \nthat result in cyclic state spaces and therefore cannot bene.t from DPOR. We would like bounded search \nto provide incremental guarantees, and to prune cyclic state spaces so that DPOR can explore them. Combining \nDPOR with bounded search is not straightforward, however. We address this problem to make DPOR more widely \napplicable. Next, we introduce bounded search, including several bound functions.  2.5 Bounded search \nBounded search explores only executions that do not exceed a bound [5, 12, 14]. The bound may be any \nproperty of a sequence of transitions. A bound evaluation function Bv(S) computes the bounded value for \na sequence of transitions S. A bound evaluation function Bv and bound c are inputs to Figure 4. Preemption-bounded \nsearch explores executions that contain fewer preemptions .rst. The u and v labels are threads. Lighter-colored \nstates require fewer preemptions. Numbers indicate bound values.  bounded search. Bounded search may \nnot visit all relevant reachable states; it visits only those that are reachable within the bound. If \na search explores all relevant states reachable within the bound, then it provides bounded coverage. \nPrior work bounds the depth [8], number of context switches [12], number of preemptive context switches \n[12], and the number of delays that an otherwise deterministic scheduler is allowed [5]. We focus on \npreemption-bounded search because it is more useful than the depth bound [12], and it has been more widely \ntested in practice than the other bounds. We also recast fair stateless model checking as bounded search \n[14]. Testers .nd preemption-bounded search useful for sev\u00aderal reasons. First, it provides an incremental \ncoverage met\u00adric when searching the entire state space in a reasonable time period is infeasible. Second, \npreemption-bounded search .nds bugs effectively many bugs manifest with few pre\u00ademptions [12]. Testers \n.nd fair-bounded search very useful because most realistic state spaces contain cycles [14]. 2.5.1 Preemption-bounded \nsearch Preemption-bounded search limits the number of preemp\u00adtive context switches that occur in an execution \n[12]. The preemption bound is de.ned recursively as follows. De.nition 2.5. Preemption bound [13]. Pb(t) \n= 0 Figure 5. Fair-bounded search explores executions that con\u00adtain fewer trips through cycles in the \nstate space .rst. The u and v labels are threads. Lighter-colored states explore fewer trips. Numbers \nindicate bound values.  2.5.2 Fair-bounded search Fair-bounded search limits the number of cycles in \na cyclic state space. We adapt a fairness criterion from prior work to identify cycles in the state space \ngiven two assumptions [14]: 1. Threads always yield the processor when not making progress 2. Threads \nnever yield the processor when making progress  Fair-bounded search tracks the number of yield operations \nthat each thread u has performed after each sequence of tran\u00adsitions S, Y(S, u), and ensures that this \nvalue never differs among enabled threads by more than the bound. Formally, we de.ne the fair bound recursively \nas follows. De.nition 2.6. Fair bound (Fb). Let Y(S, u) return Thread u s yield count in .nal(S). Fb(t) \n= 0 Fb(S.t) = max(Fb(S), maxu.enabled(.nal(S))(Y(S, t.tid) - Y(S, u))) Figure 5 illustrates fair-bounded \nsearch for a cyclic state space. Although detecting cycles is impossible in stateless search, prior work \nargues that counting yield operations is a good approximation [14]. Essentially, a thread executing a \nbounded number of yield operations is considered to be stuck in a cycle in the state space and thus unable \nto make progress. The tester sets the bound such that the search ne\u00ad Pb(S.t) = . .. .. glects spurious \nyield operations that would otherwise cause Pb(S) + 1 if t.tid = last(S).tid and the search to miss \nrelevant states. We selected this fairness last(S).tid . enabled(.nal(S)) criterion primarily for its \nsimplicity, but other fairness crite\u00ad ria can be expressed as bounds, as well. Unlike prior work, Pb(S) \notherwise this bound does not provide strong fairness [1, 14]. Figure 4 illustrates preemption-bounded \nsearch, in which the executing thread the thread that performed the previous transition never requires \na preemption. If the executing thread is blocked, then no thread requires a preemption. The preemption \nbound increases slowly when the same thread executes repeatedly, so the search may explore deep into \nthe state space with a small bound. Both the preemption and the fairness bound prune por\u00adtions of the \nstate space and may fail to detect bugs in pro\u00adgrams. DPOR, in contrast, explores all relevant states \nand detects all bugs that violate the correctness guarantee. If the state space is so large that the \nprogram does not ter\u00adminate suf.ciently quickly, however, then DPOR does not provide any guarantees. \nIf the state space is cyclic, then DPOR cannot search it. Ideally, combining these techniques would provide \nincremental coverage guarantees for a re\u00adduced state space. The next section compares these tech\u00adniques \nand shows why combining them is not trivial.  3. Partial-order reduction with bounds Combining partial-order \nmethods with bounded search should provide incremental coverage guarantees while reducing re\u00addundant \nwork. We compare DPOR with bounded search for small unit tests based on each technique s state space \ncover\u00adage over time. We implement DPOR in CHESS, a publicly available model checker for concurrent programs \nthat per\u00adforms bounded search. We run these tests on a 2.2 GHz Intel Core 2 Duo processor with 4 GB of \nRAM. We show that bounded search provides little bene.t without partial-order reduction, except when \nthe bound is very small. Figure 6 compares DPOR to bounded search. The x\u00adaxis shows the percent of local \nstates that the search visits or, if the search never explores the entire state space, then the total \nnumber of local states that the search visits. The y-axis shows the time in seconds that the search requires. \nEach data point represents an invocation of CHESS with a particular value for the bound, which we iteratively \nincrease. The single dot at 100% of local states for MRSE and FFT represents an invocation of CHESS with \nunbounded DPOR. We provide the dotted line for easier comparison. DPOR does not complete within our 3 \nhour limit on Exception and DPOR cannot explore the cyclic state space in Fair. By default, CHESS preempts \nonly prior to synchroniza\u00adtion variable accesses, and uses a data-race detector to iden\u00adtify accesses \nto shared variables that it may have missed as a result [12]. When a test does contain a data-race, CHESS \ntherefore does not explore portions of the state space reach\u00adable by re-ordering the data-racy accesses. \nAs a result, in our tests we force CHESS to preempt at all shared vari\u00adable accesses because otherwise \nthe state spaces that CHESS searches and that BPOR searches are not comparable. All of our test programs \ncontain data races, so CHESS provides insuf.cient coverage on these programs if it preempts only at synchronization \nvariable accesses. An important advan\u00adtage of BPOR is that it allows preemption-bounded search to scale \nwithout sacri.cing coverage for data-racy programs. These results show the limitations of bounded search \nand DPOR. For MRSE and FFT, DPOR explores the entire state space in less time than bounded search requires \nto explore the small subset of the state space reachable with a bound of one or two. Bounded search could \nthus bene.t greatly from partial-order reduction. We enforced a three-hour time limit per test. With \nthis limit, the Exception test does not terminate within 3 hours with DPOR, so DPOR provides no guarantees. \nFair contains a cyclic state space and thus DPOR never terminates in Figure 6(d). Bounded search makes \nlarge state spaces tractable, but it wastes too much time exploring redundant states. DPOR ex\u00ad  (b) \nFFT (c) Exception (d) Fair Figure 6. Coverage vs. time for DPOR and preemption\u00adbounded (Pb) and fair-bounded \n(Fb) search. DPOR ex\u00adplores the entire state space faster than preemption-bounded search explores only \na subset for MRSE and FFT. Fair never terminates with DPOR because its state space is cyclic. Exception \nexceeds our three-hour time limit with DPOR.  plores the entire state space in less time than bounded \nsearch requires to explore a small subset of that state space, pro\u00advided that the state space is acyclic \nand relatively small. The time DPOR requires scales exponentially with the number of successive dependent \naccesses. As this number grows, the state space quickly becomes intractable. Without a bound, if the \nstate space is intractably large, then DPOR runs for hours, days, or longer without providing any guarantees. \nLikewise, if the state space is cyclic, DPOR provides no guarantees. Practical bounded search requires \nDPOR s aggressive state space reduction. Unfortunately, as Figure 1 illustrates, sequences of transitions \nthat lead to the same local state may have different bounded values, so the search cannot guaran\u00adtee \nthat it has taken the cheapest path to each state. DPOR may prune transitions that make new states reachable \nwithin the bound, sacri.cing bounded coverage. This unsoundness arises because the bound introduces dependences \nbetween instructions that are otherwise inde\u00adpendent. If a transition t exceeds the bound in a state \ns then the search cannot explore t from s and t is, in some sense, disabled in s. Any transition that \nalters t s bounded value in s is dependent with it, by De.nition 2.2 of valid depen\u00addence relations. \nThe dependences that the bound introduces are differ\u00adent from dependences in the program under test, \nhowever. Bound dependences are arti.cial. Programmers do not gen\u00aderally care whether their programs are \ncapable of exceed\u00ading the bound or not they care whether or not executions within the bound meet the \nsafety criteria. If we treat bound dependences equivalently to dependences in the program un\u00adder test, \nthen the search must explore each state with all pos\u00adsible bounded values, which is an enormous waste \nof time. Instead, we must differentiate bound dependences from dependences in the program under test. \nBounded search need not explore both orders of bound dependent transitions it must explore only the \ncheapest order. A transition that exceeds the bound is not disabled in the same way that a transition \nwaiting for another thread to release a lock is disabled. Still, the search must compensate for these \nbound dependences when there exists a cheaper path to a given state. In the next section, we show how \nto compensate for these dependences to combine DPOR with bounded search while maintaining bounded coverage. \n4. Bound persistent sets To compensate for the dependences that bounded search cre\u00adates, we introduce \nbound persistent sets, which reduce the size of the state space while guaranteeing bounded cover\u00adage. \nFirst, we establish suf.cient conditions to guarantee ab\u00adsence of local assertion failures among executions \nthat do not exceed the bound. For concision, we put all the proofs in companion materials in the ACM \nDigital Library [3]. Coons dissertation contains a fuller treatment [2]. Algorithm 1 Bounded selective \nsearch 1: Initially, Explore(\u00d8) 2: procedure Explore(S) begin 3: T = Suf.cient set(.nal(S)) 4: for all \n(t . T ) do 5: if (Bv(S.t) = c) then 6: Explore(S.t) 4.1 Suf.cient sets A set of transitions is suf.cient \nin a state s if any relevant state reachable via an enabled transition from s is also reach\u00adable from \ns via at least one of the transitions in the suf.cient set. A search can thus explore only the transitions \nin the suf.cient set from s because all relevant states still remain reachable. The set containing all \nenabled threads is trivially suf.cient in s, but smaller suf.cient sets enable more state space reduction. \nSelective search explores only a suf.cient set of transi\u00adtions from each state [7]. Algorithm 1 performs \nbounded se\u00adlective search. Line 3 returns a nonempty suf.cient set in each state .nal(S), and Lines 4-6 \nrecursively explore only the transitions in that suf.cient set that do not exceed the bound. Requirements \nfor this suf.cient set vary with the bound evaluation function and with the desired safety guar\u00adantee. \nWe identify constraints on a suf.cient set such that Algorithm 1 guarantees absence of local assertion \nfailures among sequences of transitions that do not exceed the bound. We use AG(Bv,c) to refer to a generic \nbounded state space with bound function Bv and bound c. De.nition 4.1. Pre.x([.]) [7]. Pre.x([.]) returns \nthe set containing all pre.xes of all se\u00adquences in the Mazurkiewicz trace de.ned by .. De.nition 4.2. \nLocal suf.cient. A nonempty set T . T of transitions enabled in a state s in AG(Bv,c) is local suf.cient \nin s if and only if for all sequences . of transitions from s in AG(Bv,c), there exists a sequence . \n' from s in AG(Bv,c) such that . . Pre.x([. ' ]) and .1 ' . T . Let AR(Bv,c) be the reduced state space \nthat Algorithm 1 explores if Line 3 returns a nonempty local suf.cient set in each state. Theorem 1. \nLet s be a state in AR(Bv,c), and let l be a local state reachable from s in AG(Bv,c) by a sequence . \nof transitions. Then, l is also reachable from s in AR(Bv,c). Intuitively, there must exist a sequence \n. ' such that . . Pre.x([. ' ]) and . ' . T . Thus, there exists a sequence \u00df 1 such that ..\u00df . [. ' \n]. Any two sequences of transitions that lead to the same global state reach all of the same local states, \nso . ' must lead to l as well. Thus, if Algorithm 1 returns a local suf.cient set of tran\u00adsitions in \neach state at Line 3, then Algorithm 1 explores all local states reachable within the bound. In unbounded \nsearch, persistent sets are local suf.cient, provided that the state space is acyclic [7]. Persistent \nsets may not be local suf\u00ad.cient in bounded search, however. In the next section, we identify properties \nof bound functions that determine how conservative their local suf.cient sets must be.  4.2 Properties \nof bound functions Two properties of bound functions enable bounded partial\u00adorder reduction. We .rst \nde.ne each property for a generic bound function Bv. De.nition 4.3. Stable bound functions. Bound function \nBv is stable if and only if for all sequences . and . ' in AG(Bv,c) . . [. ' ] =. Bv(.) = Bv(. ' ) Intuitively, \nin a stable bound function, any two sequences of transitions that lead to the same global state cost \nthe same amount. This property is desirable because indepen\u00addent transitions remain commutative with \nstable bound func\u00adtions. Partial-order reduction leverages this commutativity to reduce the state space. \nWhen the bound function is not stable, two sequences of transitions that lead to the same global state \nmay have differ\u00adent costs. If a portion of the state space is unreachable within the bound via the path \nthat the search explores .rst, then it must also explore the cheaper path. This redundant execution sacri.ces \npartial-order reduction. The search prunes the state space most ef.ciently if it explores the cheapest \nsequence of transitions .rst. De.nition 4.4. Extensible bound functions. Bound function Bv is extensible \nif and only if for all se\u00adquences of transitions S in AG(Bv,c), for all transitions t such that t.tid \n. enabled(.nal(S)) and for all sequences of tran\u00adsitions a from .nal(S) such that t . a, Bv(S.t.a) = \nmax(Bv(S.t), Bv(S.a)) Extensible bound functions require that independent transi\u00adtions not affect one \nanother s cost. If the bound is not exten\u00adsible, then exploring independent transitions may make local \nstates that were previously reachable within the bound un\u00adreachable. Thus, to ensure local state reachability \nwithin the bound, the search must explore otherwise independent tran\u00adsitions. These independent transitions \nsacri.ce partial-order reduction because they lead to many redundant states. One trivial bound is both \nstable and extensible the bound function that always returns zero. This bound function is equivalent \nto unbounded search and permits full partial\u00adorder reduction. Bounds from prior work [5, 12, 14] bound \nthe total order on a program s transitions and are thus neither stable nor extensible they introduce \narti.cial dependences that partial-order reduction must accommodate.  4.3 Bound suf.cient sets We introduce \nbound suf.cient sets to compensate for depen\u00addences imposed by the bound and thus guarantee bounded coverage. \nWe show that unfortunately the preemption and fair bounds are neither stable nor extensible. However, \nwe can and do de.ne suf.cient sets for preemption-bounded and fair-bounded search and prove that these \nsets are suf.cient to explore the bounded state space soundly, but they reduce partial order reduction \nin some cases. 4.3.1 Preemption-bounded (Pb) search Preemption-bounded search limits the number of preemp\u00adtive \ncontext switches in each execution [14]. The preemp\u00adtion bound is neither stable nor extensible. Each \ntransition t s cost depends upon whether or not the prior transition t ' is enabled, even if t . t '. \nTo compensate for this dependence, a preemption-bound persistent set T requires that each tran\u00adsition \nt . T be independent with the next transition by each thread that is not in T , even if that transition \nis disabled. Preemption-bounded search with DPOR reduces the state space most effectively if it visits \nnew states via the cheapest path .rst. When the executing thread is enabled in a state s, its next transition \nis cheaper than all other enabled transitions in s. Exploring the executing thread .rst is thus a good \nheuristic for reaching new states as cheaply as possible. If the executing thread executes until it blocks, \nthen any transition can execute for free. We exploit this property to perform limited partial-order reduction \neven when transi\u00adtions increment the bound. We use the term release opera\u00adtion below to refer to any \ntransition that may enable another thread, including lock release operations, fork operations, and event \nset operations. We introduce preemption-bound persistent sets to permit limited partial-order reduction \nwith local state reachability for preemption-bounded search. De.nition 4.5. ext(s, t). Given a state \ns = .nal(S) and a transition t . enabled(s), ext(s, t) returns the unique sequence of transitions \u00df from \ns such that 1. .i . dom(\u00df) : \u00dfi.tid = t.tid 2. t.tid . enabled(.nal(S.\u00df ))  Intuitively, ext(s, t) \nreturns the sequence of transitions that results if t.tid executes from s until it blocks. De.nition \n4.6. Preemption-bound persistent sets. A set T . T of transitions enabled in a state s = .nal(S) is preemption-bound \npersistent in s iff for all nonempty sequences a of transitions from s in AG(Pb,c) such that .i . dom(a), \nai . T and for all t . T , 1. Pb(S.t) = Pb(S.a1) 2. if Pb(S.t) < Pb(S.a1), then t . last(a) and t . \nnext(.nal(S.a), last(a).tid)   3. if Pb(S.t) = Pb(S.a1), then ext(s, t) . last(a) and ext(s, t) . next(.nal(S.a), \nlast(a).tid) Assume that in each state of the reduced state space AR(Pb,c), Algorithm 1 returns a preemption-bound \npersistent set. We provide two lemmas to manage the bound, and a theorem stating that a nonempty preemption-bound \npersistent set is local suf.cient. Lemma 2. Let a and \u00df be nonempty sequences of transi\u00adtions from s \n= .nal(S) in AG(Pb,c) such that 1. \u00df . a 2. Pb(S.\u00df1) = Pb(S.a1) 3. .i . dom(\u00df) : \u00dfi.tid = \u00df1.tid 4. \n\u00df . next(.nal(S.a1 . . . ai), ai.tid), 1 = i < len(a) 5. if Pb(S.\u00df1) = Pb(S.a1), then \u00df1.tid . enabled(.nal(S.\u00df)) \n Then, \u00df.a is a sequence of transitions from s in AG(Pb,c). Intuitively, \u00df does not contain any context \nswitches and \u00df . a, so placing \u00df prior to a does not modify the program s behavior or increase the cost \nof the transitions in a. If \u00df contains a release operation, then it still cannot increase the cost of \nany transition in a because \u00df is independent with the next transition by each thread in a. Thus, \u00df can \nnever enable or disable the executing thread in a, and cannot affect whether any transitions in a require \na preemption. Lemma 3. Let T be a nonempty preemption-bound persis\u00adtent set in a state s = .nal(S) in \nAR(Pb,c) and let a.\u00df.. be a sequence of transitions from s in AG(Pb,c) such that a and \u00df are nonempty \nand 1. .i . dom(a) : ai . T 2. \u00df1 . T 3. .i . dom(\u00df) : \u00dfi.tid = \u00df1.tid 4. if Pb(S.\u00df1) < Pb(S.a1) then \nlen(\u00df) = 1 5. if Pb(S.\u00df1) = Pb(S.a1) and . is empty, then \u00df1.tid . enabled(.nal(S.\u00df )) 6. if Pb(S.\u00df1) \n= Pb(S.a1) and . is nonempty, then .1.tid = \u00df1.tid  Then, \u00df.a.. is a sequence of transitions from s \nin AG(Pb,c). The intuitions for Lemma 3 are similar to the intuitions for Lemma 2. To account for preemptions \n.1 may incur, we require that .1.tid = \u00df1.tid. Theorem 4. If T is a nonempty preemption-bound persistent \nset in a state s in AR(Pb,c), then T is local suf.cient in s. The proof of Theorem 4 follows the correctness \nproof for persistent sets, but leverages Lemmas 2 and 3 to show that relevant sequences of transitions \ndo not exceed the bound [2, 3]. By Theorems 4 and 1, if Algorithm 1 explores a nonempty preemption-bound \npersistent set in each state, then it reaches all local states reachable in AG(Pb,c). Next, we de.ne \nlocal suf.cient sets for fair-bounded search. 4.3.2 Fair-bounded (Fb) search Fair-bounded search limits \nthe maximum difference between the executing thread s yield count in each state s and the yield count \nof other enabled threads in s. This bound prunes executions in which a thread yields the processor repeatedly, \nand thus prunes cycles from cyclic state spaces under the assumption that threads that are not making \nprogress always yield the processor [14]. The fair bound is neither stable nor extensible due to release \noperations. A release operation may enable threads with a lower yield count, and thus increase the cost \nof an in\u00addependent, enabled transition. To provide local state reacha\u00adbility for fair-bounded search, \nwe introduce fair-bound per\u00adsistent sets. Fair-bound persistent sets compensate for de\u00adpendences that \nthe fair bound introduces by conservatively scheduling all threads prior to release operations. De.nition \n4.7. Fair-bound persistent sets. A set T . T of transitions enabled in a state s = .nal(S) is fair-bound \npersistent in s if and only if for all nonempty sequences a of transitions from s in AG(Fb,c) such that \n.i . dom(a) : ai . T and for all t . T , 1. Fb(S.t) = c 2. if t is a release operation, then .u . enabled(s) \n: next(s, u) . T 3. t . last(a)  Requirement 1 requires that transitions in the fair-bound per\u00adsistent \nset not exceed the bound. Requirement 2 requires that all enabled threads be scheduled prior to release \noperations. This requirement is conservative a less restrictive require\u00adment might permit more partial-order \nreduction while still providing coverage guarantees. We choose this requirement to compensate for the \ndependences that release operations introduce in fair-bounded search because it is simple and intuitive \nyet still provides partial-order reduction. Require\u00adment 3 requires that transitions reachable via transitions \nnot in the fair-bound persistent set be independent with transi\u00adtions in the fair-bound persistent set. \nLet AR(Fb,c) be the reduced state space explored by Al\u00adgorithm 1 with bound function Fb and bound c. \nAssume that in each state, Algorithm 1 returns a fair-bound persistent set. We provide two lemmas to \nmanage the bound, and a lemma stating that a nonempty fair-bound persistent set in a state s is local \nsuf.cient in s. Lemma 5. Let a be a nonempty sequence of transitions from s = .nal(S) in AG(Fb,c) and \nlet t be a transition enabled in s such that 1. Fb(S.t) = c 2. t is not a release operation 3. t . \na  Then, t.a is a sequence of transitions from s in AG(Fb,c).  Algorithm 2 BPOR with bound function \nBv and bound c 1: Initially, Explore(E) from s0 2: procedure Explore(S) begin 3: Let s = .nal(S) # Add \nbacktrack points 4: for all (u . Tid) do 5: for all (v . Tid | v = u) do # Find most recent dependent \ntransition 6: if (.i = max({i . dom(S) | Si \u00adnext(s, u) and Si.tid = v})) then 7: Backtrack(S, i, u) \n# Continue the search by exploring successor states 8: Initialize(S) 9: Let visited = \u00d8 10: while (.u \n. (enabled(s) n backtrack(s) \\ visited)) do 11: add u to visited 12: if (Bv(S.next(s, u)) = c) then 13: \nExplore(S.next(s, u)) Intuitively, t can increase the cost of a transition in a only ' if t is a release \noperation that enables a transition t with a lower yield count than one of the transitions in a, yet \nt is not a release operation. Lemma 6. Let T be a nonempty fair-bound persistent set in a state s = .nal(S) \nin AR(Fb,c) and let a.t.. be a sequence of transitions from s in AG(Fb,c) such that a is nonempty, .i \n. dom(a) : ai . T , and t . T . Then, t.a.. is a sequence of transitions from s in AG(Fb,c). The intuitions \nfor Lemma 6 are similar to the intuitions for Lemma 5. If a transition in . exceeds the fair bound in \nS .t.a.. then it also exceeds the fair bound in S .a.t.. . Theorem 7. If T is a nonempty fair-bound persistent \nset in a state s in AR(Fb,c), then T is local suf.cient in s. The proof of Theorem 7 follows the correctness \nproof for persistent sets, but it leverages Lemmas 5 and 6 to show that each relevant sequence of transitions \ndoes not exceed the bound [2, 3]. By Theorems 7 and 1, if Algorithm 1 explores a nonempty fair-bound \npersistent set in each state then it reaches all local states reachable in AG(Fb,c). We have identi.ed \nconstraints on suf.cient sets that per\u00admit limited partial-order reduction while providing bounded coverage \nfor preemption-bounded and fair-bounded search. In the next section, we present an algorithm to dynamically \ncompute bound persistent sets for these bound functions at runtime. We generalize DPOR to search a bounded \nstate space, then specialize this algorithm for each bound func\u00adtion and prove the resulting algorithm \ncorrect. 5. Computing bound persistent sets Algorithm 2 presents bounded, dynamic partial-order reduc\u00adtion \n(BPOR), a modi.ed version of DPOR that computes a bound persistent set in each state. We specialize BPOR \nto compute preemption-bound persistent and fair-bound per\u00adsistent sets and prove the resulting algorithms \ncorrect. First, we summarize Algorithm 2 and highlight differences from DPOR. The procedure Explore in \nAlgorithm 2, which is common to all bound evaluation functions, recursively ex\u00adplores the bounded state \nspace from a state s = .nal(S). Lines 4-7 create backtrack points. For each thread u, Line 6 computes \nthe most recent transition in S by each thread v that is dependent with next(.nal(S), u). For each such \ndependence, Line 7 cre\u00adates a backtrack point to reverse the order of the dependent transitions in a \nfuture execution. The original DPOR algo\u00adrithm places a backtrack point only prior to the most recent \ndependent transition, rather than the most recent dependent transition by each thread. We make this change \nbecause we differentiate read and write operations, so we must consider read operations by each thread. \nThis change also simpli.es the proofs and allows us to simplify the algorithm. Lines 8-13 recursively \nexplore the state space from s. Line 12 ensures that the next transition does not exceed bound c. Line \n13 recursively explores Thread u s next tran\u00adsition. This recursive search may add additional threads \nto the backtrack set in s. The Backtrack and Initialize procedures are speci.c to each bound evaluation \nfunction. The Backtrack procedure adds backtrack points to compensate for dependences that the bound \nintroduces. Initialize initializes the bound persis\u00adtent set with at least one enabled transition that \ndoes not ex\u00adceed the bound, if one exists. The initial transition affects the size of the .nal bound \npersistent set, so each bound function carefully selects the initial transition to maximize its like\u00adlihood \nof reaching each state via the cheapest sequence of transitions .rst. We specialize versions of Backtrack \nand Initialize for the preemption and fair bounds. Algorithm 2 is simpli.ed in comparison to the original \nDPOR algorithm to make it easier to describe and to make its proofs more intuitive. We provide results \nfor this simpli.ed algorithm and also for the original DPOR algorithm, which we denote as optimized. \nThe optimizations we omit in the unoptimized version include the following [2]. 1. Backtrack only dependences \nthat are in the transitive reduction on the program s partial order. 2. If the backtracked thread is \ndisabled, backtrack any en\u00adabled thread that is transitively dependent with it. 3. Do not backtrack \nrelease operations backtrack only prior to the matching acquire instead (DPOR only).  The last optimization \ndoes not apply to preemption-bounded or fair-bounded BPOR because both of these algorithms must speci.cally \nbacktrack release operations. We include these optimizations for DPOR in Figure 6 when we compare Algorithm \n3 BPOR for preemption-bounded search  1: procedure Initialize(S) begin 2: if (last(S).tid . enabled(.nal(S))) \nthen 3: add last(S).tid to backtrack(.nal(S)) 4: else 5: add any u . enabled(.nal(S)) to backtrack(.nal(S)) \n6: procedure Backtrack(S, i, u) begin 7: AddBacktrackPoint(S, i, u) 8: if (j = max({j . dom(S) | j = \n0 or Sj-1.tid = Sj.tid and j < i})) then 9: AddBacktrackPoint(S, j , u) 10: procedure AddBacktrackPoint(S, \ni, u) begin 11: if (u . enabled(pre(S, i))) then 12: Add u to backtrack(pre(S, i)) 13: else 14: backtrack(pre(S, \ni)) = enabled(pre(S, i)) DPOR to bounded search. In Section 6, however, we sepa\u00adrate these optimizations \nout because we do not include these optimizations in our formal proofs [2, 3]. 5.1 Computing preemption-bound \npersistent sets Algorithm 3 contains the Initialize and Backtrack proce\u00addures for preemption-bounded \nsearch. Initialize adds the ex\u00adecuting thread to the backtrack set in .nal(S) if it is enabled there. \nOtherwise, Initialize adds any u . enabled(.nal(S)) to the backtrack set because all threads cost the \nsame in .nal(S). The Backtrack procedure adds two backtrack points: at Line 7 it adds one prior to the \nmost recent dependent tran\u00adsition Si, and at Line 9 it adds one prior to the most recent transition to \nSi at which the executing thread changed. The .rst backtrack point satis.es Requirement 2 of De.nition \n4.6 and the second backtrack point satis.es Requirement 3 of De.nition 4.6. The backtrack point at Line \n9 is conservative it may lead to precisely the same states that the backtrack point at Line 7 leads to, \nbut with fewer preemptions. The search cannot know whether these backtrack points lead to the same states \nyet, however, because it has not yet searched the intervening state space. Thus, the search must add \nboth backtrack points. The procedure AddBacktrackPoint adds u to the backtrack set if it is enabled in \npre(S, i); otherwise, it conservatively adds all enabled threads. To prove that Algorithm 2 computes \na preemption-bound persistent set in each state, we create postconditions for preemption-bounded search, \nwhich we derive from the post\u00adconditions that DPOR satis.es in each state [6]. De.nition 5.1. PC for \nExplore(S) Preemption bound. .u.. : if Pb(S..) = c then Post(S.., len(S), u) Algorithm 4 BPOR procedures \nfor fair-bounded search 1: procedure Initialize(S) begin 2: if (len(S) > MAX) then 3: report livelock \nand exit 4: Backtrack(S, len(S), u) where u is a lowest cost en\u00ad abled thread in .nal(S) 5: procedure \nBacktrack(S, i, u) begin 6: if (u . enabled(pre(S, i)) and next(pre(S, i), u) is not a release operation) \nthen 7: add u to backtrack(pre(S, i)) 8: else 9: backtrack(pre(S, i)) = enabled(pre(S, i)) De.nition \n5.2. Post(S, k, u) Preemption bound. .v : if i = max({i . dom(S) | Si -next(.nal(S), u) and Si.tid = \nv}) then 1. if i = k then if u . enabled(pre(S, i)) then u . backtrack(pre(S, i)) else backtrack(pre(S, \ni)) = enabled(pre(S, i)) 2. if j = max({j . dom(S) | j = 0 or Sj-1.tid = Sj .tid and j < i}) and j < \nk then if u . enabled(pre(S, j )) then u . backtrack(pre(S, j )) else backtrack(pre(S, j)) = enabled(pre(S, \nj ))  De.nition 5.1 requires that Post hold for all threads and for all sequences of transitions that \nare reachable within the pre\u00ademption bound. De.nition 5.2 requires an additional back\u00adtrack point to \nguarantee that ext(pre(S, j), next(pre(S, j), u)) is independent with transitions not in the local suf.cient \nset. Requirement 3 of De.nition 4.6 of preemption-bound per\u00adsistent sets requires this backtrack point. \nTheorem 8. Whenever a state s = .nal(S) is back\u00adtracked during the search performed by Algorithm 2 in \nan acyclic state space, the postcondition Post for Explore(S) is satis.ed, and the set T of transitions \nexplored from s is preemption-bound persistent in s. This proof leverages Lemmas 2 and 3 to show that \neach sequence of transitions is reachable within the preemption bound [2, 3]. 5.2 Computing fair-bound \npersistent sets Algorithm 4 contains the Initialize and Backtrack proce\u00addures to compute fair-bound persistent \nsets. Assume that Algorithm 2 calls these procedures. Initialize adds any minimum-cost enabled thread \nto the backtrack set. Back\u00adtrack checks whether Si is a release operation, or u is dis\u00adabled in pre(S, \ni). In either case, the search conservatively adds all enabled threads to the backtrack set. Otherwise, \nLine 7 adds u to the backtrack set. To prove that Algorithm 2 computes a fair-bound persis\u00ad tent set \nin each state, we de.ne postconditions that Algo\u00adrithm 2 guarantees prior to backtracking each state \n[6].  De.nition 5.3. PC for Explore(S) -Fair bound. .u.. : if Fb(S..) = c and len(S..) = MAX then Post(S.., \nlen(S), u) De.nition 5.4. Post(S, k, u) -Fair bound. .v : if i = max({i . dom(S) | Si -next(.nal(S), \nu) and Si.tid = v}) and i = k then if u . enabled(pre(S, i)) and Si is not a release then u . backtrack(pre(S, \ni)) else backtrack(pre(S, i)) = enabled(pre(S, i)) De.nition 5.3 requires that Post hold for all threads \nand for all sequences of transitions that are reachable within the fair bound and within a maximum depth \nparameter, MAX. This maximum depth bound should be very large such that it con\u00adstrains the search only \nwhen the state space contains a cycle that the fair bound cannot break. In such a case, the search reports \na livelock to indicate that a thread likely yielded the processor when it should not have, or failed \nto yield the pro\u00adcessor when it should have. If u is enabled in pre(S, i) and Si is not a release operation, \nthen De.nition 5.4 requires that u be in backtrack(pre(S, i)). Otherwise, De.nition 5.4 conser\u00advatively \nrequires that all enabled threads be in the backtrack set in pre(S, i). Theorem 9. Whenever Algorithm \n2 backtracks a state s = .nal(S), postcondition Post for Explore(S) is satis.ed, and the set T of transitions \nexplored from s is fair-bound persis\u00adtent in s. This proof is similar to DPOR s correctness proof, but \nit leverages the Initialize procedure in Algorithm 4 and Lemma 6 to show that each sequence of transitions \nis within the fair bound [2, 3]. 5.3 Combining bounds Combining bound functions may be advantageous if \nthe bounds serve fundamentally different purposes, as the pre\u00ademption and fairness bounds do. Combining \nthese bounds provides the incremental coverage guarantees of preemption\u00adbounded search but also prunes \ncyclic state spaces. Combin\u00ading these bounds sacri.ces partial-order reduction, however. If a fair-blocked \ntransition does not allow a non-preemptive context switch, then the search may lose coverage. When using \nmultiple bounds, it is also more likely that the search will reach states in which all transitions exceed \na bound. For example, a preemption-bounded, fair-bounded search may leave all transitions exceeding the \nbound, unless you assume that fair blocked threads get a free preemption. If all transitions exceed the \nbound, then the search must con\u00adservatively schedule all threads at their most recent cheaper locations \nto ensure that no states are left unexplored. We conservatively place additional backtrack points when\u00adever \na thread s enabledness changes to eliminate these inter\u00adactions. By exploiting dynamic information about \nthe subse\u00adquent state space, however, these conservative assumptions could likely be optimized. We leave \nthese additional opti\u00admizations to future work. 5.4 Optimizations In the performance results, we report \nin the next section, we always apply sleep sets to DPOR and BPOR. The sleep sets algorithm is complementary \nto the partial-order reduction algorithm and ensures that previously visited transitions are not visited \nagain until after the search explores a dependent transition. To compensate for the bound, we never place \ntransitions in the sleep set if they were conservatively added by BPOR due to the bound. The optimized \nresults also include one additional opti\u00admization that is speci.c to the bound. When all states are reachable \nwithin the bound from a given state, then the search does not add any conservative backtrack points. \nThis optimization ensures that with a suf.ciently large bound, BPOR will behave exactly like DPOR and \nexplore the entire state space with no conservative overhead due to the bound. 6. Results We evaluate \nBPOR by measuring its state space reduction and time required to manifest known bugs. We measure state \nspace coverage over time to ensure that the per-transition overhead of BPOR s bookkeeping is not lost. \nWe compare to bounded search without partial-order reduction, and if the DPOR search terminates, then \nwe compare to DPOR as well. Methodology We compare DPOR, bounded search, and BPOR in CHESS, a publicly \navailable, stateless, dynamic model checker for concurrent software. CHESS places a thin wrapper between \nthe program under test and the Win32 and .NET APIs using binary instrumentation [15]. This wrapper intercepts \ncalls into the Win32 and .NET APIs and provides hooks into CHESS that control thread scheduling completely \nwithout modifying the semantics of the API or the behavior of the program under test. We validate our \nimplementation in several ways. First, we hash local and deadlock states to track the unique states that \nthe search visits. We compare these states with and with\u00adout partial-order reduction to ensure that they \nare the same. Second, we automatically generate random concurrent pro\u00adgrams and compare the states that \nDPOR, bounded search, and BPOR explore for these programs. Third, we explicitly verify that the lemmas \nand postconditions in Section 5 are true at runtime. We test four concurrent unit tests developed by \ntesters for concurrent software and libraries at Microsoft. Exception tests a concurrent exception for \nthe Concurrency Coordi\u00adnation Runtime (CCR) library. MRSE tests a manual reset event for the .NET 4.0 \nconcurrency libraries. FFT is a par\u00adallel fast Fourier transform with eight threads. We also pro\u00advide \nresults for a microbenchmark that we created explic\u00aditly to test fair-bounded search without partial-order \nreduc\u00adtion. Because fair-bounded search prunes only cycles in the    (d) Fair -Fair bound Figure 7. \nCoverage vs. time as the bound increases. state space, its state space is intractably large without partial\u00adorder \nreduction or other bound functions, so we needed a test that was small enough that we could explore the \nentire fair\u00adbounded state space. Coverage time Figure 7 shows coverage over time for each benchmark. \nIf at least one test explores the entire state space, then we show % of local states, otherwise we show \nthe raw number of visited local states on the x-axis. We measure and report time on the y-axis as a function \nof explored states and the bound value to show the overhead of executing BPOR compared to the other approaches. \nEach point on the graphs in Figure 7 represents an invocation of CHESS with a par\u00adticular bound function \nand bound. Lines connect points only for visual clarity we test only integer bounds. Dashed lines represent \nDPOR, which is a single invocation of CHESS that searches the entire state space. We show both optimized \nand unoptimized results because our proofs explicitly cover only the unoptimized algorithms. In practice, \nthe optimized re\u00adsults explore all of the same local states, but we have not proved the optimizations. \nBPOR explores new states signi.cantly more quickly than bounded search without partial-order reduction, \npar\u00adticularly as the bound increases. Note that the y-axis is log\u00adarithmic in Figure 7. We show results \nfor MRSE to com\u00adpare BPOR with DPOR when the search space is small enough that DPOR terminates. We include \nFFT because it is pathological for the preemption bound. FFT contains eight threads that access a shared \nlock. The lock release operations all require conservative backtrack points with preemption\u00adbounded BPOR. \nEach acquire operation also requires a con\u00adservative backtrack point that turns out to be unnecessary. \nStill, BPOR reduces search time. Note that we terminate tests if they do not complete after 3 hours \nwithout BPOR, most tests time out. DPOR does does not terminate within 3 hours for the Exception test, \nand neither does preemption-bounded search with a bound of two or greater. BPOR search does terminate \nwithin three hours with a bound of two. Figure 7(d) shows fair-bounded search with and without partial-order \nreduc\u00adtion. There are no DPOR results for this program because it has a cyclic state space, and thus \nDPOR crashes with a stack over.ow. BPOR improves the rate at which fair-bounded search explores new states \nconsiderably over fair-bounded search without partial-order reduction. Finding Bugs Table 1 shows time \nrequired to .nd several previously known bugs with DPOR, without any partial\u00adorder reduction, and with \nBPOR. Tests marked with - either do not .nd the bug within three hours, or die after the .rst execution \ndue to a stack over.ow as a result of the cyclic state space. All of the DPOR and BPOR tests in Table \n1 use the optimized search settings described in Section 5, and each bounded search uses the minimum \nbound required to detect the bug. This result is a best-case scenario for bounded search because the \nideal bound cannot be known beforehand. DPOR cannot explore the NQueens, Matrix, or RegOwn tests because \nthey have cyclic state spaces. Fair-bounded BPOR .nds each bug, and preemption/fair-bounded BPOR .nds \neach bug more quickly than fair-bounded BPOR de\u00adTable 1. Time required to .nd bugs. Preemption/fair bounded \nsearch without BPOR requires much longer than fair-bounded BPOR requires. Fair-bounded BPOR requires \nlonger than preemption/fair-bounded BPOR.  Unit test Bug Time to manifest bug (s) DPOR No BPOR BPOR \nPb Pb MRSE Deadlock 2 6 1 CCR Assertion Assertion 69 64 39 35 9 8 Fb Pb Fb Fb Pb NQueens Assertion Livelock \nAssertion --- 75 3235 312 5 502 80 4 125 11 Matrix Assertion Livelock Livelock --- 54 1089 - 2 787 694 \n2 137 136 RegOwn Exception - - 3474 1586 spite the conservative approach we chose to combine these bounds. \n7. Conclusions This paper exploits properties of bounds to combine pre\u00ademption and fair-bounded search \nwith dynamic partial-order reduction (DPOR). We show that bounded search alone is insuf.cient to reduce \nthe time to .nd many bugs without partial-order reduction. DPOR is also insuf.cient without a bound if \nthe search space is large or the test program is cyclic. Bounded, dynamic partial-order reduction (BPOR) \nprovides incremental coverage guarantees for a reduced state space. We specialize this algorithm for \npreemption and fair\u00adbounded search, prove its coverage guarantees, and show that it reduces search time \nconsiderably in practice. We de\u00adscribe two desirable bound properties: stability and extensi\u00adbility. \nWe show that neither preemption or fair-bounds have these properties. However, these properties do point \nto ways to choose better bound functions [2]. Bounded partial-order reduction gives testers a more effective \ntool for verifying the correctness of concurrent programs with bounded guaran\u00adtees. Acknowledgments This \nresearch was supported in part by NSF SHF-0910818 and CCF-1018271. Any opinions, .ndings and conclusions \nexpressed herein are the authors and do not necessarily re\u00ad.ect those of the sponsors. We thank Bert \nMaher for endless discussions, proof reading, and graphs. References [1] AP T, K. R., FR A N C EZ , N., \nA N D KAT Z, S. Appraising fair\u00ad ness in languages for distributed programming. In Distributed Computing \n(1988), vol. 2, pp. 226 241. [2] CO ONS, K. E. Fast Error Detection with Coverage Guaran\u00adtees for Concurrent \nSoftware. PhD thesis, The University of Texas at Austin, 2013. [3] CO ONS, K. E., MU SUVATHI , M., AN \nD MCKINL E Y, K. S. Bounded partial order reduction (Proof source material in the ACM Digital Library). \nIn ACM Conference on Object-Oriented Programming, Systems, Languages, and Applica\u00adtions (2013). [4] DW \nY E R , M. B., HAT CLI FF , J., AND RA N GANATH, R. V. P. Exploiting object escape and locking information \nin partial\u00adorder reductions for concurrent object-oriented programs. In Formal Methods in System Design \n(2004), vol. 25, pp. 199 240. [5] EMMI , M., QA D E ER , S., AN D RAKA M A RI C , Z. Delay\u00adbounded scheduling. \nIn ACM SIGACT-SIGPLAN Principles of Programming Languages (POPL) (2011), pp. 411 422. [6] FL ANAG AN, \nC., A N D GOD EFROI D , P. Dynamic partial\u00adorder reduction for model checking software. In ACM SIGPLAN-SIGACT \nPrinciples of Programming Languages (POPL) (2005), pp. 110 121. [7] GO D E F RO ID , P. Partial-Order \nMethods for the Veri.cation of Concurrent Systems: An Approach to the State-Explosion Problem. Springer-Verlag, \n1996. [8] GO D E F RO ID , P. Model checking for programming languages using Verisoft. In ACM SIGACT-SIGPLAN \nPrinciples of Programming Languages (POPL) (1997), pp. 174 186. [9] GO D E F RO ID , P., A ND PIROT TI \nN , D. Re.ning dependen\u00adcies improves partial-order veri.cation methods (extended ab\u00adstract). In International \nConference on Computer Aided Veri\u00ad.cation (CAV) (1993), pp. 438 449. [10] KATZ, S., A N D PE L ED , D. \nDe.ning conditional indepen\u00addence using collapses. In Theoretical Computer Science (1992), vol. 101, \nElsevier Science Publishers, pp. 337 359. [11] MAZUR K IEW IC Z , A. Trace theory. In Advances in Petri \nnets 1986, part II on Petri nets: Applications and relation\u00adships to other models of concurrency (1986), \nSpringer-Verlag, pp. 279 324. [12] MUS U VAT H I , M., A N D QADEE R , S. Iterative context bound\u00ading \nfor systematic testing of multithreaded programs. In ACM Conference on Programming Language Design and \nImple\u00admentation (2007), pp. 446 455. [13] MUS U VAT H I , M., A ND QA D E E R, S. Partial-order reduction \nfor context-bounded state exploration. Tech. Rep. MSR-TR\u00ad2007-12, Microsoft Research, 2007. [14] MUS \nU VAT H I , M., AND QADEE R , S. Fair stateless model checking. In ACM Conference on Programming Language \nDesign and Implementation (2008), pp. 362 371. [15] MUS U VAT H I , M., QA D E E R , S., BA L L , T., \nBA S L E R , G., NAI NA R, A., A N D NE A MT IU , I. Finding and reproducing heisenbugs in concurrent \nprograms. In USENIX Symposium on Operating Systems Design and Implementation (2009). [16] OVE R M A N \n, W. T. Veri.cation of concurrent systems: Func\u00adtion and timing. PhD thesis, University of California, \nLos Angeles, 1981.  [17] VA L M A RI , A. A stubborn attack on state explosion. In [18] VISSER , W., \nHAV E LU N D , K., BR AT, G., A N D PA RK , S. International Workshop on Computer Aided Veri.cation (CAV \nModel checking programs. In Automated Softeware Engineer\u00ad 90) (1990), Springer-Verlag, pp. 156 165. ing \nJournal (2000), pp. 3 12.   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Eliminating concurrency errors is increasingly important as systems rely more on parallelism for performance. Exhaustively exploring the state-space of a program's thread interleavings finds concurrency errors and provides coverage guarantees, but suffers from exponential <i>state-space explosion</i>. Two prior approaches alleviate state-space explosion. (1) <i>Dynamic partial-order reduction</i> (DPOR) provides <i>full coverage</i> and explores only one interleaving of independent transitions. (2) <i>Bounded search</i> provides <i>bounded coverage</i> by enumerating interleavings that do not exceed a bound. In particular, we focus on preemption-bounding. Combining partial-order reduction with preemption-bounding had remained an open problem.</p> <p>We show that preemption-bounded search explores the same partial orders repeatedly and consequently explores more executions than unbounded DPOR, even for small bounds. We further show that if DPOR simply uses the preemption bound to prune the state space as it explores new partial orders, it misses parts of the state space reachable in the bound and is therefore unsound. The bound essentially induces dependences between otherwise independent transitions in the DPOR state space. We introduce Bounded Partial Order Reduction (BPOR), a modification of DPOR that compensates for bound dependences. We identify properties that determine how well bounds combine with partial-order reduction. We prove sound coverage and empirically evaluate BPOR with preemption and fairness bounds. We show that by eliminating redundancies, BPOR significantly reduces testing time compared to bounded search. BPOR's faster incremental guarantees will help testers verify larger concurrent programs.</p>", "authors": [{"name": "Katherine E. Coons", "author_profile_id": "81381599031", "affiliation": "University of Texas at Austin, Austin, TX, USA", "person_id": "P4290471", "email_address": "coonske@cs.utexas.edu", "orcid_id": ""}, {"name": "Madan Musuvathi", "author_profile_id": "81100333862", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P4290472", "email_address": "madanm@microsoft.com", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "Microsoft Research, Redmond, WA &#38; University of Texas at Austin, Austin, TX, USA", "person_id": "P4290473", "email_address": "mckinley@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509556", "year": "2013", "article_id": "2509556", "conference": "OOPSLA", "title": "Bounded partial-order reduction", "url": "http://dl.acm.org/citation.cfm?id=2509556"}