{"article_publication_date": "10-29-2013", "fulltext": "\n Ef.cient Concurrency-Bug Detection Across Inputs Dongdong Deng Wei Zhang Shan Lu University of Wisconsin, \nMadison dongdong@cs.wisc.edu, wzh@cs.wisc.edu, shanlu@cs.wisc.edu Abstract In the multi-core era, it \nis critical to ef.ciently test multi\u00adthreaded software and expose concurrency bugs before soft\u00adware release. \nPrevious work has made signi.cant progress in detecting and validating concurrency bugs under a given \ninput. Unfortunately, software testing always faces large sets of test inputs, and existing techniques \nare still too expensive to be applied to every test input in practice. In this paper, we use open-source \nsoftware to study how existing concurrency-bug detection tools work for a set of inputs. The study shows \nthat an interleaving pattern, such as a data race or an atomicity violation, can often be exposed by \nmany inputs. Consequently, existing bug detectors would inevitably waste their bug detection effort to \ngenerate duplicate bug reports, when applied to a set of inputs. Guided by the above study, we propose \na coverage metric, Concurrent Function Pairs (CFP), to ef.ciently approximate how interleavings overlap \nacross inputs. Using CFP, we have designed a new approach to detecting data races and atomicity-violation \nbugs for a set of inputs. Our evaluation on open-source C/C++ applications shows that our CFP-guided \napproach can effectively accelerate concurrency-bug detection for a set of inputs by reducing redundant \ndetection effort across inputs. Categories and Subject Descriptors D.2.5 [Software En\u00adgineering]: Testing \nand Debugging; D.1.3 [Programming Techniques]: Concurrent Programming Keywords multi-threaded software; \nsoftware testing; con\u00adcurrency bugs; bug detection Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. Copyrights for components of this work owned by others than the author(s) must be honored. \nAbstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. \nOOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright is held by the owner/author(s). \nPublication rights licensed to ACM. ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509539 \n 1. Introduction 1.1 Motivation The rise of the multi-core era dictates the prevalence of multi\u00adthreaded \nsoftware. Unfortunately, concurrency bugs widely exist in multi-threaded software [20] and have caused \nsevere damages in the real world [28, 45, 52]. Therefore, effective bug-detection and testing techniques \nare needed to expose concurrency bugs before software release. Exposing concurrency bugs is challenging, \nrequiring not only bug-triggering inputs but also special orders of shared\u00admemory accesses (i.e., interleavings). \nA multi-threaded pro\u00adgram can take many different inputs, and follow many differ\u00adent interleavings while \nexecuting each input. Facing the huge input space and the even bigger interleaving space, it is im\u00adpractical \nfor in-house testing to expose all hidden bugs. How to expose as many bugs as possible given the time \npressure and resource budget is a critical and open question. The state-of-the-art techniques for in-house \nconcurrency\u00adbug detection and testing often involve three steps: 1. Input Design: a set of test inputs \nare designed to provide code coverage; 2. Bug Detection: for each test input, the program is exe\u00adcuted \nand analyzed by a dynamic bug-detection tool that identi.es potentially buggy interleavings; 3. Bug \nValidation: for each test input, the program is exe\u00adcuted for several more times to exercise suspicious \ninter\u00adleavings identi.ed above.  Ideally, the second and third steps are repeated for every test input \nto provide interleaving coverage, so that concur\u00adrency bugs can be thoroughly detected and validated. \nMuch research has been done to improve each individual step mentioned above. For the .rst step, many \ntechniques are developed to automatically generate inputs that provide good code coverage for single-threaded \nand multi-threaded programs [19, 54, 55]. For the second step, many tools are proposed to detect various \ntypes of buggy interleavings, in\u00adcluding data races [7, 15, 29, 42, 50, 69], atomicity violations [6, \n14, 31, 32, 61, 65], abnormal data-communication pat\u00ad terns [33, 59, 67, 70], and others. For the third \nstep, different schemes are designed to ef.ciently exercise suspicious inter\u00adleavings [37, 38, 43, 44, \n53]. Unfortunately, even with state-of-the-art techniques, bug detection (the second step) and bug validation \n(the third step) still each introduces 10X 100X slowdown for each input. Applying them to every test \ninput hence becomes unacceptably expensive, as software companies already spend more than 30% of their \ndevelopment resources in testing [51]. As a result, a lot of concurrency bugs inevitably escape into \nproduction runs, with concurrency-bug detection and validation techniques only applied to few inputs. \nRecent work, MAPLE [68], speeds up the above process by looking at a set of inputs altogether, rather \nthan one input at a time. Speci.cally, MAPLE speeds up the bug validation step (the third step) by exercising \neach suspicious interleaving only once across different inputs. For example, once a data race is exercised \nunder input A, it will not be exercised again under input B. Although inspiring and promising, MAPLE \ndoes not change the bug detection step (the second step), still requiring 10X 100X slow down to identify \nsuspicious interleavings for each input. Clearly, better techniques are needed to detect concur\u00adrency \nbugs for a set of inputs with limited in-house testing resources.  1.2 Contribution This paper proposes \na new approach to concurrency-bug detection for a set of inputs. This approach reduces redundant analysis \nthat is repeated under different inputs and generates duplicate bug reports, and hence signi.cantly improves \nthe bug-detection performance for a set of inputs. Our approach is based on the following observation: \nexisting concurrency-bug detectors are inef.cient for a set of inputs, because they tend to report the \nsame suspicious interleaving under different inputs. This observation applies to race detectors, atomicity-violation \ndetectors, and others. It is also consistent with previous work [68]: MAPLE is effective in speeding \nup the bug validation step largely because there are many duplicate reports across inputs. This observation \nimplies both opportunities and chal\u00adlenges. If we could reduce the redundant effort that produces duplicate \nbug reports, bug-detection ef.ciency could improve signi.cantly for a set of inputs. Unfortunately, which \nbug report has duplicates across inputs is easy to check after the expensive bug-detection process, yet \nis very dif.cult to pre\u00addict beforehand. To address the challenge and exploit the opportunity, we need \nan easy-to-measure metric to characterize a program s interleaving space under a given input. That is, \nthe metric should be able to approximate what sequences of shared\u00admemory accesses could occur during \nthe program s execution under certain input. Such a metric can approximate how interleaving spaces overlap \nacross inputs and guide bug detection to reduce redundant analysis. Overall, this paper makes the following \ncontributions: 1. Identifying the inef.ciency of existing concurrency-bug detectors on a set of inputs. \nSection 2 will discuss this observation in detail using representative real-world soft\u00adware and test-input \nsets. 2. Proposing a new interleaving-coverage metric, Concurrent Function Pair (CFP), to guide concurrency-bug \ndetection, as well as a carefully designed algorithm to ef.ciently measure this metric. The CFP metric \nmeasures which functions can be executed concurrently under a given input. It strikes a nice balance \nbetween measurement com\u00adplexity and interleaving-space characterization accuracy. It will be presented \nin detail in Section 3. 3. Designing and implementing a new data-race and atomicity-violation detection \nframework based on CFP. This new framework applies bug detection for a set of inputs in three steps. \nAt Step 1, the CFP for each input is measured and the aggregated CFP for all the test inputs is calculated. \nAt Step 2, selected inputs and selected func\u00adtions under each selected input are identi.ed so that they \nprovide a complete coverage for the aggregated CFP. At Step 3, existing data-race and atomicity-violation \ndetec\u00adtors are applied to only the selected inputs and selected functions. The details will be presented \nin Section 4.  We have evaluated the new bug-detection framework on .ve representative open-source applications \nand their test input sets. The evaluation shows that our framework can effectively reduce the bug-report \nduplication rate1 from 3.6 4.5 to 1.0 2.2 for data races, and from 2.7 5.4 to 1.1 2.8 for atomicity violations. \nOverall, the speedup of bug-detection time is 1.5X 6.2X for race detection and 1.6X 5.6X for atomicity \nviolation detection, with no false negatives among failure-inducing bugs. 2. Inef.ciency of Cross-Input \nBug Detection In this section, we apply common concurrency-bug detection algorithms to representative \nmulti-threaded software, and compare the bug reports generated across test inputs. 2.1 Methodology Applications \nAs shown in Table 1, this study uses 5 open\u00ad source applications that represent different types of software. \nAll applications are written in C/C++ and use pthread library as the underlying concurrency framework. \nTest inputs Click and Mozilla-js have test inputs written by their developers and released together with \ntheir source code. For Click, we use all its test inputs that do not require OS\u00adkernel changes. For Mozilla-js, \nwe randomly group the 20 single-threaded JavaScript requests provided by developers into 7 multi-threaded \ninputs. FFT, LU, and PBZIP2 do not have publicly available test inputs. We have designed test inputs \nfor them based on their 1 Measured by the average number of inputs that report each bug. App. Description \nLOC Test Input Set # Inputs #Threads Description Click 1.8.0 A software router [8] 290K 6 2 Designed \nby Click developers FFT A scienti.c computing program 1.2K 8 4 Designed based on command-line options \nfrom SPLASH2 [64] LU A scienti.c computing program 1.1K 8 4 Designed based on command-line options from \nSPLASH2 [64] Mozilla-JS m10 A JavaScript engine [36] 87K 7 4 Designed by Mozilla-JS developers PBZIP2 \n0.9.4 A parallel-compression application [16] 2.0K 8 4 Designed based on command-line options Table \n1: Applications and test inputs in study command-line options. FFT, LU, and PBZIP2 each have 8 non-trivial \ncommand-line con.guration options. Therefore, we write 8 test inputs for each of them, with each input \nspec\u00adifying a unique command-line option not speci.ed by any other inputs. For example, our 8 FFT test \ninputs exercise different computation settings, such as normal FFT, inverse FFT, printing per-thread \nstatistics, and others. As another example, our PBZIP2 inputs exercise compression, decom\u00adpression, error-message \nsuppression, compression-integrity testing, and other con.gurations. To assess the quality of our test \ninputs, we measure their statement coverage using gcov [17]. The result shows that for each program, \nevery test input covers some unique statements that are not covered by any other inputs in the test set. \nBug Detectors Our study focuses on the two most common types of concurrency bugs: data races [42, 50] \nand single\u00ad variable atomicity violations [14, 32, 44, 61]. A data race occurs when two threads can simultaneously \naccess a shared variable, with at least one access being a write [50]. A single\u00ad variable atomicity violation \noccurs when two consecutive accesses to a shared variable from one thread is unserializably interleaved \nby a third access from another thread, as shown in Figure 1. To detect these two types of bugs, we .rst \nuse a run-time tool implemented in Pin [34] to collect per-thread execution traces of global/heap memory \naccesses and synchronization operations. We then analyze traces to detect bugs. Both detectors were implemented \nand used in our previous work [44, 70]. Our race detection uses a lock-set/happens-before hybrid algorithm, \nsimilar to those in many open-source race de\u00adtectors [41, 56]. Speci.cally, two instructions are reported \nas a data race, if they satisfy three conditions: (1) they ac\u00adcess the same memory location from different \nthreads; (2) they are not protected by any common lock; and (3) they have concurrent vector timestamps \ncalculated based on order\u00adenforcing synchronization operations such as barrier, pthread join, and pthread \ncreate. Our atomicity-violation detection follows an algorithm described in CTrigger [44]. Speci.cally, \nan atomicity viola- Thread 1 Thread 2 p : read m  r : write m c : read m Thread 1 Thread 2 p : write \nm  r : write m c : read m Thread 1 Thread 2 p : read m  r : write m c : write m Thread 1 Thread \n2 p : write m  r : read m c : write m  Figure 1: Four scenarios of single-variable atomicity viola\u00ad \ntions that CTrigger [44] detects. shows execution order. m is a shared memory location. # All Race Pairs \n# All Atom. Vio. # Buggy Race Pairs # Buggy Atom. Vio. App. Total Unique DupRate Total Unique DupRate \nTotal Unique DupRate Total Unique DupRate Click 3114 848 3.6 6145 2298 2.7 6 1 6.0 6 1 6.0 FFT 300 66 \n4.5 1423 369 3.8 28 4 7.0 35 5 7.0 LU 238 58 4.1 874 163 5.4 28 4 7.0 21 3 7.0 Mozilla 1991 481 4.1 2459 \n723 3.4 42 6 7.0 42 7 6.0 PBZIP2 293 65 4.5 499 143 3.5 32 8 4.0 39 11 3.5 Table 2: Inef.ciency in data-race \nand atomicity-violation detection across inputs (DupRate measures the average number of inputs that expose \nthe same data race or atomicity violation) tion is reported, if three instructions p, c, and r satisfy \nthree conditions: (1) p and c consecutively access a memory lo\u00adcation m from one thread, while r accesses \nm from another thread; (2) neither locks nor order-enforcing synchronization operations can prevent r \nfrom executing between p and c; (3) the read/write access types of p, c, and r fall into one of the four \nscenarios shown in Figure 1. To carry out the above bug-detection algorithms, our detectors recognize \nthe following synchronization opera\u00adtions in C/C++ programs: pthread mutex (un)lock, pthread create, \npthread join, and the barrier macro in SPLASH2 benchmarks (FFT and LU) [64]. Like almost all other detectors, \nour detectors do not recognize custom synchronization operations. This can lead to false positives. The \nabove data-race and atomicity-violation detection algorithms are designed so that their bug-detection \nresults are fairly stable across runs under one input. That is, executing a program once is suf.cient \nto obtain most, if not all, of the bug reports under one input. Executing the program under the same \ninput for more times could occasionally produce more bug reports, but the marginal bene.t is usually \ntoo small to justify the huge extra overhead in practical software testing. In our experiments, the bug \ndetection results rarely change across runs under one input. Therefore, in this section, we only show \nthe results obtained by executing each application once under each test input for data-race or atomicity-violation \ndetection. We will discuss the impact of multiple bug-detection runs in Section 5. We count each unique \npair of static race instructions as one unique data race, and each unique triplet of static instructions \nthat compose a single-variable atomicity violation as one unique atomicity violation. We tried our best \nto eliminate the background noise effect. For example, for Click, under each test input, we conducted \nour experiment using its previously stored workload trace.  2.2 Observations Table 2 summarizes the \nresult of applying race detection and atomicity-violation detection to 5 benchmarks. We use All Race \nPairs and All Atom. Vio. to represent the raw results from the two detectors. Since many races and atomicity \nviolations do not lead to externally visible failures [4, 40], we use Buggy Race Pairs and Buggy Atom. \nVio. to represent failure-inducing reports from the two detectors. These would be the .nal results presented \nto the developers after the bug\u00advalidation step discussed in Section 1. As shown in Table 2, each data \nrace or atomicity viola\u00ad tion is reported by 2.7 5.4 inputs on average across all benchmarks. The duplication \nrates for truly buggy reports are similar, ranging from 4 to 7 for data races and 3.5 to 7 for atomicity \nviolations. These duplication rates could get even larger with larger test-input sets. Theoretically, \na race or an atomicity violation may lead to failures under one input and remain benign under another \ninput. Therefore, we further investigate which races and atomicity violations could lead to software \nfailures under which inputs. We found that the goodness/badness of a race/atomicity-violation is always \nthe same under different inputs in the studied applications and test sets. In conclusion, interleaving \npatterns, such as races and atomicity violations, overlap signi.cantly across inputs. Concurrency-bug \ndetection would waste substantial effort, if the detection process is not coordinated across inputs. \nIf we have an ideal scheme that can eliminate bug-detection effort spent identifying duplicate bug reports, \nwe can potentially speed up existing bug detection by up to 6 times for even a small set of inputs, with \nlittle or no harm to the bug-exposing capability. 3. Concurrent Function Pairs (CFP) Terminology The \ninterleaving space S of a program P in\u00adcludes all run-time instruction permutations that are possible \nfor P under all possible inputs. The interleaving space of P under an input i includes those instruction \npermutations that are possible when P executes i. Section 2 demonstrates the inef.ciency of applying \nexist\u00ad ing concurrency-bug detectors to a set of inputs. To avoid redundant analysis and reduce duplicate \nbug reports across inputs, we need to predict how different inputs interleaving spaces overlap, so that \nheavy-weight bug detection can be guided to focus on unique interleavings. /*Thread 1*/ foo1(){ lock(L); \nfoo2(); unlock(L); /*Thread 2*/ ... lock(L); ... bar(); } unlock(L); Figure 2: An example of concurrent \nfunctions (For illustration purpose, the vertical position of each code statement in the .gure represents \nwhen the statement is executed) Section 3.1 will introduce a metric, CFP, designed to char\u00ad acterize \nthe interleaving space and approximate interleaving\u00adspace overlap with low cost. Section 3.2 will describe \nthe CFP-measurement algorithm, followed by a qualitative dis\u00adcussion about how CFP can guide concurrency-bug \ndetection in Section 3.3.   3.1 De.nition of CFP Our metric design follows two principles: 1. Characterization \nAccuracy: it has to characterize the interleaving space with decent accuracy, so that it can guide concurrency-bug \ndetection to reduce redundant analysis without missing many bugs. 2. Measurement Complexity: it has \nto be relatively cheap to measure. Otherwise, its measurement cost would out\u00adweigh its bene.t in concurrency-bug \ndetection.  Following these two principles, we have designed Con\u00adcurrent Function Pairs, short as CFP. \nThe CFP of a program P includes all unique pairs of functions that can execute in parallel with each \nother (i.e., concurrent), denoted as CFPP . The CFP of a program P under a speci.c input i includes all \nunique pairs of functions that can execute in parallel with each other under input i (i.e., concurrent \nin i), denoted as CFPP or simply CFPi. i In the de.nition above, we say a pair of functions f1 and f2 \ncan execute in parallel with each other, if and only if the following scenario can occur at run time: \na thread t1 is executing f1 or a callee of f1, while another thread t2 is executing f2 or a callee of \nf2. For example, foo1() and bar() in Figure 2 are executing in parallel, while foo2() and bar() can never \nexecute in parallel. Note that CFP addresses which functions can execute in parallel. Whether the functions \ndid execute in parallel in a particular run does not matter. We believe CFP strikes a good balance between \ncharac\u00adterization accuracy and measurement complexity. Roughly speaking, CFP should be much cheaper to \nmeasure than data races and atomicity violations, because the number of func\u00adtions is much smaller than \nthat of memory accesses in a /* .ent, .exi: function entrance, exit; .ent exi.lockset: lockset protecting \nthe critical section that holds both entrance and exit; .vec time: vector timestamps calculated using \norder-enforcing synchronization. */ Bool concurrentFunction(f1, f2) { if(f1.thread id == f2.thread id) \nreturn false; /*Can f1 start between f2 s entrance and exit?*/ if ((f1.ent.lockset n f2.ent exi.lockset) \n!= 0/) return false; if (f1.ent.vec time < f2.ent.vec time) return false; if (f1.ent.vec time > f2.exi.vec \ntime) return false; /*Can f2 start between f1 s entrance and exit?*/ if ((f2.ent.lockset n f1.ent exi.lockset) \n!= 0/) return false; if (f2.ent.vec time < f1.ent.vec time) return false; if (f2.ent.vec time > f1.exi.vec \ntime) return false;  return true; /*f1 and f2 are concurrent*/ } Figure 3: Pseudo code that judges concurrent \nfunctions program. CFP should also provide a decent accuracy in char\u00adacterizing the interleaving space. \nThe content of P s interleav\u00ading space under an input i is determined by which instructions can be executed \nby P under i and which instructions can be executed concurrently. Since a function is a natural unit \nof instructions in a program, intuitively, CFP provides a decent characterization of the interleaving \nspace. We will elaborate on these in the next two sub-sections. 3.2 How to measure CFP? 3.2.1 When are \ntwo functions concurrent? To measure CFP, we should .rst .gure out whether two given functions, f1 and \nf2, can execute in parallel. A na\u00a8ive solution is to compare every instruction in f1 or f1 s callees \nwith every instruction in f2 or f2 s callees, and see whether two instructions could execute in parallel. \nThis is clearly too expensive. A simpler solution, which we use in this paper, is to compare the entrances \nand exits of functions: if one function s entrance can execute in between the other s entrance and exit, \nthese two functions can execute in parallel. Clearly, to know whether a function s entrance f1ent can \npotentially execute in between another function s entrance and exit, f2ent and f2exi , we need to check \nthe synchronization operations in the program. Mutual-exclusion synchroniza\u00adtion, such as pthread (un)lock, \ncan prevent f1ent from executing between f2ent and f2exi , if and only if f2ent and f2exi are inside \na critical section of lock l that also protects f1ent. In addition, order-enforcing synchronization, \nsuch as pthread create, pthread join, and barrier, can prevent f1ent from executing between f2ent and \nf2exi , if and only if it forces f1ent to happen before f2ent or happen after f2exi . Figure 3 illustrates \nthe above algorithm.  3.2.2 The basic CFP-measurement algorithm To compute the CFP of a program under \ninput i, we simply need to check every pair of functions that can be executed under i to see whether \nthey are concurrent, using the algo\u00adrithm shown in Figure 3. This analysis can be conducted either statically \nor dynamically. In this paper, we calculate CFPi by analyzing the run-time trace. Run-time information, \nsuch as which thread executes which functions and which instruction accesses which memory location, will \nallow our trace analysis to make more informed decision than static analysis. An instrumentation tool \nin LLVM [27] is implemented to log the execution behavior of each thread. Each trace is a list of run-time \nevents following a thread s execution order. There are two types of events in our trace: function entrance/exit \nand synchronization operation. Whenever the program enters or exits a function at run time, the corresponding \nthread s trace is appended with a record that speci.es the unique ID of this function and whether this \nis an entrance or an exit. Whenever the program executes a synchronization operation, the corresponding \ntrace is appended with a record that speci.es the type of synchronization operation and some extra information. \nFor pthread (un)lock, the address of the lock variable is recorded. For pthread create and pthread join, \nthe thread IDs of participating threads are recorded. For the barrier macro in FFT and LU, the address \nof the barrier variable is recorded. Our basic trace-analysis tool computes CFP in two steps. First, \nfor every function entrance/exit record, we calculate the set of locks protecting it, as well as its \nvector timestamp. The vector timestamp is calculated using only order-enforcing synchronization operations, \nincluding pthread create, pthread join and barrier in our implementation, and not mutual-exclusion operations \nlike pthread (un)lock, which do not force any speci.c order between two events. We also pay special attention \nto computing the locks that protect a function s entrance and exit in one critical section (i.e., .ent \nexi.lockset in Figure 3). Speci.cally, we pair each function-entrance record in the trace with its cor\u00adresponding \nexit record, which is referred to as one function instance. We then look for locks that are acquired \nbefore the entrance and not released until after the exit. The complexity of this step is linear to the \ntotal number of function instances. The second step identi.es all pairs of concurrent functions using \nthe lockset and vector-timestamp information calcu\u00adlated above and the algorithm shown in Figure 3. To \nconduct this computation ef.ciently, our analysis .rst groups similar function instances together. Speci.cally, \nwe consider two function instances to be similar, if they share the same (1) function ID, (2) thread \nID, (3) the lockset protecting its en\u00adtrance, (4) the lockset protecting both its entrance and exit in \none critical section, and (5) the vector-timestamps of its void lu0(double* a, int n) { int j, k; for \n(k=0; k<n; k++) { for (j=k+1; j<n; j++) { daxpy(...,...,...,...); /*Parameters omitted for presentation \nsimplicity*/ }}} Figure 4: Functions concurrent with lu0 must be concurrent with daxpy (n is a positive \ninteger). entrance and exit. Based on the algorithm shown in Figure 3, two similar function instances \nhave exactly the same set of concurrent functions and hence only need to be processed once. After grouping \nsimilar function instances, we simply go through every pair of functions and check whether they have \nat least one pair of instances that are concurrent with each other. The complexity of the last step is \nquadratic to the number of unique static functions in the trace. 3.2.3 Optimization for CFP measurement \nAlthough CFP measurement is much cheaper than concurrency-bug detection, it could still take time for \nlarge long-running programs with many function calls. Therefore, we further optimize the basic algorithm \nin two ways. Optimization 1: skip functions that only access stack vari\u00adables. This is a generic optimization \nthat is also conducted by our concurrency-bug detectors discussed in Section 2.1. If f only accesses \nstack variables,2 there is no chance it will help guide concurrency-bug detection. Optimization 2: skip \nfunctions that inherit their callers concurrent functions. This is an optimization specially de\u00adsigned \nfor CFP measurement. It does not work for generic concurrency bug detection. This optimization can be \ndemon\u00adstrated by a real example from LU shown in Figure 4. In this example, we can guarantee that every \nfunction concurrent with lu0 is also concurrent with daxpy for two reasons: (1) whenever lu0 is executed, \nit invokes daxpy; and (2) lu0 does not contain any synchronization operations. Therefore, there is no \nneed to record or analyze the entrance/exit of daxpy inside lu0 for CFP-measurement. We simply need to \nappend the set of functions that are concurrent with lu0 to that of daxpy at the end of our analysis. \nFormally speaking, suppose function f1 calls function f2 through instruction i. We can guarantee that \nevery function concurrent with f1 is also concurrent with f2, if f1 and i satisfy two conditions: (1) \ni post-dominates the entrance instruction of f1, which guarantees that f1 always invokes 2 Theoretically, \ndevelopers could make a stack variable shared among threads. Since it is rare and not recommended, it \nis not considered here.  Figure 5: f can execute in parallel with f2 in another run f2; (2) no synchronization \noperation is executed by f1 or its callees, except for f2 through the call-site i. We brie.y prove this \nconcurrency-inheritance relationship as follows. Suppose a function f is concurrent with f1. By de.nition, \nthere exists a run, during which an instruction of f or f s callees is executed simultaneously with an \ninstruction, denoted as j, of f1 or f1 s callees (shown in Figure 5). If j is an instruction inside f2, \nf must be concurrent with f2. If j is outside f2, the two conditions mentioned in the previous paragraph \nimply that there exists an invocation or a return of f2, so that no synchronization operations are executed \nbetween it and that particular instance of j. Therefore, any code region (e.g., f ) that can execute \nsimultaneously with the latter can also execute simultaneously with the former, which proves that f is \nalso concurrent with f2. The high-level idea of this proof is depicted in Figure 5. Given the above proof, \nhow to conduct Optimization 2 is straightforward. Our static analysis goes through a program s call graph \nfor several passes. It identi.es every pair of caller f1 and callee f2 that satisfy the two optimization \nconditions mentioned above. In our current algorithm, we further check whether the callee f2 satis.es \nthe optimization conditions with all its caller functions. If so, we remove all LLVM instrumentation \ndescribed in Section 3.2.2 that logs the entrance and the exit of f2. Consequently, not only the tracing \ntime is shortened, the trace size and trace-analysis time are also reduced. Our current algorithm does \nnot apply any optimization to a function that only has some, but not all, of its caller functions satisfying \nthe optimization conditions. This design can be changed in the future by differentiating different callers \nof a function. That is, as long as f2 satis.es the optimization conditions with one of its caller f1, \nwe can skip the logging and trace analysis discussed in Section 3.2.2 for dynamic instances of f2 that \nare invoked by f1. More performance improvement will be achieved in that way. There are some caveats \nin our optimization. First, we assume that every loop in a program will execute at least one iteration. \nTake Figure 4 as an example. Without any knowledge about the input range, static analysis cannot determine \nwhether the loops will always execute at least one iteration, and hence cannot guarantee that every instance \nof lu0 would invoke daxpy. Therefore, unless we use this at-least-one-iteration heuristic, static analysis \nwill lose the op\u00adportunity to optimize the daxpy instrumentation. Of course, this heuristic could lead \nto CFP false positives: some concur\u00adrent function pairs may never execute in parallel. However, we believe \nthe optimization bene.t of this heuristic signi.\u00adcantly outweighs any potential negative impact, because \nin our experience the chance of CFP false positives is rare. In addition, false positives in CFP measurement \nwould at worst cause unnecessary and redundant bug detection, and hence slow down our CFP-guided bug \ndetection. It would not di\u00adrectly lead to false positives or negatives in concurrency-bug detection. \n Second, our analysis only considers pthread-related operations and the barrier macro as synchronization \noperations. As most existing tools, we do not consider custom synchronization, which could again lead \nto false positives in CFP measurement. The above optimization is useful for many programs, because many \nfunctions in multi-threaded programs do not contain synchronization operations. In addition, some utility \nfunctions like daxpy in LU are major sources of dynamic function instances. Section 5.8 will evaluate \nthe impact of this optimization in detail. There are other optimization opportunities for CFP mea\u00adsurement. \nFor example, some functions, such as those exe\u00adcuted by the main thread before any child thread is created, \nare not concurrent with any functions. Future work can try using static analysis to identify these functions, \nand then skip these functions during logging and CFP-trace analysis. This will lead to smaller CFP traces \nand faster CFP measurement. We leave further optimization to future work.  3.3 Can CFP guide bug detection? \nAs discussed earlier, CFPP can characterize the interleaving i space of program P under input i to some \nextent. In the following, we discuss in detail the relationship between CFPP i and the set of concurrency \nbugs reported from P under i. 3.3.1 Data races and CFP A data race occurs when two memory accesses, with \nat least one write, to the same memory location from two threads could occur concurrently without synchronization \nin between. If two functions f1 and f2 are not concurrent, such as foo2 and bar in Figure 2, no data \nrace can be found between instructions from them, because no instruction in f1 can execute in parallel \nwith instructions in f2. On the other hand, if f1 and f2 are concurrent, such as main and SlaveStart \nin Figure 6, they likely contain data races when they read and write the same memory locations. Furthermore, \nwhen a concurrent function pair is in multiple inputs CFPs, applying race detection on these inputs is \nat a high risk of generating duplicate race reports. For example, 7 out of 8 test inputs of FFT contain \n{main, SlaveStart} /*Thread 1*/ /*Thread 2*/ void main(...){ void SlaveStart(...){... ... printf( End \nat %f , Gend); Gend = time(); ... ... } } Figure 6: A data-race example from FFT /*Thread 1*/ /*Thread \n2*/ js InitAtomState(...){ js FreeAtomState(...){ state->table = ...; ...; if(!state->table) state->table \n= NULL; return false; ...; } } Figure 7: An atomicity-violation example from Mozilla in their CFPs. Consequently, \nthe data race shown in Figure 6 is repeatedly reported under 7 inputs. Clearly, CFP is useful in coordinating \nrace detection across inputs. For example, if we select inputs so that their CFPs do not overlap, applying \nrace detection to them can guarantee not to generate duplicate bug reports. However, this could cause \ntoo few inputs to be selected and hence miss bugs. For example, suppose the test-input set contains only \ntwo inputs with overlapped CFPs. If we select both inputs, duplicate bug reports could occur due to the \nCFP overlap; if we only select one input, the concurrent function pairs uniquely covered by the other \ninput will be missed in bug detection. We will discuss better ways to use CFP in Section 4.  3.3.2 Atomicity \nviolations and CFP Atomicity violation occurs when a sequence of memory accesses from one thread is unserializably \ninterleaved by memory accesses from another thread [6, 14, 31, 32, 61, 65]. Similar to that in data-race \ndetection, if two functions f1 and f2 are not concurrent, no atomicity violation can be found between \ninstructions from them, because no instruction in f1 (or f2) can execute between a sequence of instructions \nfrom f2 (or f1). On the other hand, if f1 and f2 are concurrent, such as js InitAtomState and js FreeAtomState \nin Figure 7, they may contain atomicity violations. Further\u00ad more, when a concurrent function pair is \nin multiple inputs CFPs, applying atomicity-violation detection on these in\u00adputs risks generating duplicate \natomicity-violation reports. For example, 7 out of 7 test inputs of Mozilla contain {js InitAtomState, \njs FreeAtomState} in their CFPs. Consequently, the atomicity violation shown in Fig\u00adure 7 is repeatedly \nreported under 7 inputs. 4. CFP-Guided Bug Detection 4.1 Overview Our CFP-guided bug detection aims \nto improve the detection ef.ciency over a set of inputs by eliminating redundant analysis. At a high \nlevel, this is achieved by turning on bug detection only when the program executes selected functions \nunder selected inputs. The process of input selection and function selection is guided by CFP and has \ntwo goals for a set of inputs I: 1. Lower the chance of missing bugs: the bug detection process should \nprovide a complete coverage of the CFPI , the union of all test inputs CFPs. We will refer to CFPI as \naggregated CFP. 2. Lower the chance of duplicate bug reports: the bug detec\u00adtion process should avoid \nrepeatedly analyzing the same pair of concurrent functions across inputs, as long as this does not prevent \nus from achieving the .rst goal.  Consequently, the principle of our input and function selection is \nto skip the bug detection over a function f , if f does not contribute to previously unanalyzed concurrent \nfunction pairs. As a simple example, suppose there are two inputs in the test set, i1 and i2. Suppose \nCFPi1 is {{ f1, f2}}and CFPi2 is {{ f1, f2},{ f2, f3}}. If bug detection is applied to i1 .rst, it should \nthen skip f1 under i2, because f1 does not contribute to any unanalyzed concurrent function pair. On \nthe other hand, if bug detection is applied to i2 .rst, it could then completely skip i1 following the \nabove principle. Speci.cally, our CFP-guided concurrency-bug detection includes three steps for a set \nof inputs: 1. Compute the CFP of each input and get the aggregated CFP of the whole input set. 2. Select \ninputs and select functions for each selected input. 3. Apply a race detector or an atomicity-violation \ndetector to selected functions under each selected input.  Next, we discuss these three steps one by \none.  4.2 Step 1: CFP measurement As discussed in Section 3.2, our CFP measurement contains several \nphases. The .rst phase is static analysis. We use static analysis to identify functions that are guaranteed \nto have the same concurrency property with their caller functions, as discussed in Section 3.2.3. We \nthen statically instrument the program using LLVM to log the entrance/exit of every func\u00adtion, except \nthose identi.ed above, and every synchronization operation. This static analysis phase is conducted only \nonce for all inputs. The second phase calculates the CFP for each test input. As discussed in Section \n3.2.2, we execute the instrumented program under an input once, and then analyze the run-time trace to \nidentify concurrent function pairs. Note that, what we get from the trace analysis is not the .nal CFP \nyet. Since Input1: CFP1={{ f1, f2}, { f2, f3}, { f2, f4}, { f4, f5}} Input2: CFP2={{ f1, f2}, { f3, \nf4}, { f3, f5}} Input3: CFP3={{ f2, f3}, { f3, f4}} CFPAggregated ={{ f1, f2}, { f2, f3}, { f2, f4}, \n{ f4, f5}, { f3, f4}, { f3, f5}} Step 1: Selected input -- Input1 Selected functions -- { f1, f2, f3, \nf4, f5} CFPUncovered = {{ f3, f4}, { f3, f5}} Step 2: Selected input -- Input2 Selected functions -- \n{ f3, f4, f5} CFPUncovered = 0/ Figure 8: A toy example of input/function selection we do not log functions \nthat inherit their callers concurrent functions, we need to compute the concurrent function pairs that \ninvolve these functions, as discussed in Section 3.2.3. This gives us the .nal CFP for each input. The \nthird phase calculates the aggregated CFP. It simply takes the union of every individual input s CFP. \nNote that, during the second phase, we calculate CFP based on the log collected from only one run. Theoretically, \ndifferent runs under the same input could produce different logs, and hence different CFPs. However, \nthe concurrency relationship analyzed here is usually stable across runs, similar with that in data-race \ndetection [50] and atomicity\u00ad violation detection [44]. Our experimental results also show that the reported \nCFP is very stable across runs. Even in the worst cases, we see fewer than 0.5% of CFP .uctuation among \ntens of runs. Therefore, we only run each program once under each input to collect the CFP, which is \nconsistent with previous bug-detection work [44, 50].  4.3 Step 2: input and function selection We aim \nto select the smallest set of inputs to cover the aggregated CFP, which unfortunately is an NP-hard problem. \nTo ef.ciently solve it, we use a greedy algorithm that provides approximate results. Speci.cally, our \nalgorithm .rst selects the input that covers the most concurrent function pairs among all inputs. It \nthen keeps selecting the input that covers the most uncovered pairs, until all pairs in the aggregated \nCFP are covered. Function selection is straightforward once we know how to select inputs. During the \nabove input-selection process, for every selected input i, we know which concurrent function pairs covered \nby i are not covered by previously selected inputs. Functions involved in those concurrent function pairs \nbecome the selected functions for i. A toy example that demonstrates the input/function selec\u00adtion process \nis shown in Figure 8.  4.4 Step 3: guided bug detection At this step, we simply apply existing concurrency-bug \ndetectors to the selected inputs and selected functions. The only non-trivial issue here is that most \nrun-time concurrency-bug detectors conduct bug detection unselec\u00adtively on all executed functions. Small \nmodi.cations are needed to integrate an existing bug detector into our bug\u00addetection framework. In our \ncurrent implementation, we slightly modi.ed the Pin-based execution-tracing tool described in Section \n2.1 to take a command-line input .le. This input .le contains the instruction-address range of every \nfunction identi.ed by Step 2, with each range represented by the function entrance instruction address \nand exit instruction address. Our modi.ed tracing tool only logs memory accesses whose instruction addresses \nfall into one of the ranges speci.ed by the input .le. We then apply the same trace-analysis algorithms \ndescribed in Section 2.1 to detect data races and atomicity violations. We believe similar modi.cations \ncan be easily done for many other existing concurrency-bug detectors. 4.5 Bug-detection quality assessment \nThe quality of bug detection is usually measured using three metrics: (1) performance; (2) false negatives; \nand (3) false positives. In the following, we qualitatively compare our CFP\u00adguided bug detection (short \nas CFP below) with traditional bug detection that applies unchanged existing bug detectors to each and \nevery test input (short as Full below). In terms of false positives, CFP will never introduce more false \npositives than Full. In terms of performance, the third step of CFP is clearly faster than Full, because \nonly selected inputs and selected functions are involved. The more inputs and the more CFP\u00adoverlap the \ninputs have, the more advantage CFP has over Full here. However, the .rst step and the second step of \nCFP incur extra cost that is not incurred by Full. In reality, the ben\u00ade.t of CFP tends to signi.cantly \noutweigh its cost, because CFP measurement takes much less time than concurrency\u00adbug detection. We will \nsee more detailed quantitative results in Section 5. There are two ways to measure false negatives. One \nis to ignore the practical resource limit and measure how many unique bugs can be discovered given unlimited \nresource in unlimited amount of time. The other one, which is more real\u00adistic, is to measure how many \nunique bugs can be discovered with limited resources in a limited amount of time. Since the resource \nlimitation is a real concern for a set of inputs, the second way of measurement is more suitable in this \npaper s context. We believe CFP will incur fewer false negatives un\u00adder this way of measurement, because \nCFP would spend less resource in producing duplicate bug reports than Full. We will discuss more about \nthis in Section 5. Of course, it is worth pointing out that, with unlimited resources, CFP will inevitably \nhave more false negatives than Full. In general, CFP guarantees to apply concurrency\u00adbug detection to \nevery pair of concurrent functions under at least one input. However, some bugs may only be detected \nwhen a pair of concurrent functions are executed under a special input. For example, some bugs may hide \nin special basic blocks that are not always exercised when its calling function is executed. As another \nexample, different inputs could bring different states to the same code region, causing two instructions \nto access different memory locations under one input and the same memory location under a different input. \nIn addition, there is an extra source of false negatives for CFP atomicity-violation detection. Sometimes, \nthe in\u00adtended atomic region of an atomicity violation involves code statements from more than one function. \nConcurrent function pairs cannot well predict the existence of these bugs. Overall, CFP is not a panacea. \nIt improves the perfor\u00admance of cross-input bug detection at the risk of missing bugs when there is abundant \nresource to conduct full-blown bug detection on all inputs. It is a good .t for bug detection and testing \nwhen the resource is limited, which is the common case in practice. In fact, in reality, software companies \nmay not have the resource and time to .nish even the CFP-guided bug detection. In this case, the CFP \nmetric can provide de\u00advelopers a quantitative measurement about the completeness of in-house testing. \nWe believe this metric will be useful for multi-threaded software testing, just like how statement cov\u00aderage \nand branch coverage are crucial for sequential software testing. 5. Evaluation 5.1 Methodology We run \nour experiments on an 8-core Intel Xeon machine and LLVM 2.8 compiler. The applications and test-input \nsets used here are identical to those described in Section 2.1. Our evaluation will compare our CFP-guided \nconcurrency\u00adbug detection approach, short as CFP, with the traditional approach that applies full-blown \ndetection to every input, short as Full. We will try both data-race detection and atomicity-violation \ndetection, using the detectors described in Section 2.1. The only difference is that the detectors used \nby CFP can be con.gured to monitor only speci.ed functions, as discussed in Section 4.4. Our evaluation \nwill compare CFP with Full from several aspects: performance, false negatives, bug-report duplication \nrate, and trace size. The impact of our optimization algorithms and other details of CFP will also be \nevaluated.  5.2 Overall results As shown in Table 3 and Table 4, our CFP-guided approach can signi.cantly \nimprove the concurrency-bug detection performance, with few to no false negative and huge reduction in \nbug-detection trace size. The impact on race detection and atomicity-violation detection is similar. \nApp. Speedup False Neg. Rate # False Neg. Trace Re\u00ad (X) All Buggy All Buggy duction (%) Click 3.5 2.0% \n0% 17 0 82% FFT 6.2 4.5% 0% 3 0 82% LU 4.9 1.7% 0% 1 0 76% Mozilla 1.5 2.9% 0% 14 0 39% PBZIP2 3.8 3.1% \n0% 2 0 75% Average 4.0 2.8% 0% 7 0 71%  Table 3: Overall results of CFP-guided race detection, with \nthe traditional full race detection as the baseline. The baseline bug counts are shown in the # All Race \nPairs (Unique) column and # Buggy Race Pairs (Unique) column of Table 2. App. Speedup False Neg. Rate \n#False Neg. Trace Re\u00ad (X) All Buggy All Buggy duction (%) Click 2.5 2.0% 0% 46 0 82% FFT 5.6 3.0% 0% \n11 0 82% LU 4.8 4.3% 0% 7 0 76% Mozilla 1.6 1.9% 0% 14 0 39% PBZIP2 2.4 4.2% 0% 6 0 75% Average 3.4 3.1% \n0% 17 0 71%  Table 4: Overall results of CFP-guided atomicity-violation detec\u00adtion, with the full atomicity-violation \ndetection as the baseline. The baseline bug counts are shown in the # All Atom. Vio. (Unique) column \nand # Buggy Atom. Vio. (Unique) column of Table 2. In terms of performance, CFP achieves 4.0X and 3.4X \nspeedup on average for data-race and atomicity-violation de\u00adtection, respectively, with the best performance \nimprovement achieved for FFT and the worst for Mozilla. In terms of false negatives, CFP does not miss \nany failure\u00adinducing data race or atomicity violation, as shown by the Buggy columns in Table 3 and Table \n4. As discussed in Section 2, the race detector and atomicity-violation detector also report many data \nraces and atomicity violations that do not lead to software failures. When we consider all these reports, \nboth failure-inducing and non-failure-inducing ones, CFP incurs 1.7% to 4.5% false negative rate, a small \nnumber considering the speedup, as shown by the All columns in Table 3 and Table 4. Among all benchmarks, \nClick generates the most data-race and atomicity-violation bug reports, as shown in Table 2. Consequently, \nit also incurs the most false negatives under CFP. CFP also signi.cantly reduces the trace size in concurrency-bug \ndetection. As discussed in Section 2.1, both the race detector and the atomicity-violation detector used \nin our implementation analyze execution traces to discover bugs. By selecting inputs and functions, CFP \nreduces the trace size by 71% on average for 5 benchmarks. Overall, the above results demonstrate that \nour CFP\u00adguided approach can signi.cantly speed up the bug-detection process for a set of inputs during \nin-house bug detection and testing, with negligible effect on the bug-detection coverage.  5.3 Input \nand function selection Our CFP-guided approach runs faster than Full bug detection, because it only applies \nbug detection to selected inputs and selected functions, as shown in Table 5. #Inputs #Functions Trace \nSize (MB) App. Full CFP Full CFP Full CFP Click 6 4 3689 724 94 17 FFT 8 1 135 21 1011 182 LU 8 1 122 \n18 1012 256 Mozilla 7 5 1583 857 10 6 PBZIP2 8 2 782 135 132 33 Table 5: Input/function selection and \ntrace-size changes ( #Func\u00adtions and Trace Size are both aggregated across inputs) As we can see, 1 \n5 inputs are selected for the 5 bench\u00admarks. For FFT, LU, and PBZIP2, only 1 or 2 inputs are suf.cient \nto provide a complete CFP coverage. Apart from input selection, our CFP approach also selects which \nfunctions to monitor while executing each input. The #Functions column in Table 5 shows the sum of the \nnumber of static functions that are executed and monitored by bug\u00addetection tools for every input. As \nwe can see, the number of functions monitored in CFP across inputs is only 15% 54% of those in Full. \nIn Click, even though 4 out of 6 inputs are selected, only few functions are monitored when executing \nsome of the selected inputs. As a result, CFP only monitors 20% of functions monitored by Full across \ninputs, which leads to over 80% of trace-size reduction shown by the Trace Size columns in Table 5. Overall, \nfor all benchmarks, CFP can effectively guide us to identify selective inputs and functions for concurrency\u00adbug \ndetection. Note that the input selection and function selection are conducted only once for a program \nand a set of inputs. Later on, both race detection and atomicity-violation detection will use the same \nselection result.  5.4 Bug-report duplication rate As discussed in Section 2.2, directly applying traditional \nconcurrency-bug detection to a set of inputs is inef.cient, because many duplicate bugs will be reported. \nTable 6 shows that our CFP-guided approach can effectively reduce the bug\u00adreport duplication rate. Speci.cally, \nthe average number of inputs under which each race is reported drops from 3.6 4.5 to 1.0 2.2 for the \n.ve benchmarks. The duplication rate for atomicity violation also drops from 2.7 5.4 to 1.1 2.8. In \nfact, the duplication rate of CFP drops to below 2 for all benchmarks except for Mozilla. The reason \nthat we failed to decrease the duplication rate to 1 can be explained by an example. Suppose we choose \ninput i1 to cover a concurrent function pair { f2, f3}, and i2 to cover pairs { f1, f3} and { f1, f2}. \nA race between instructions in f2 and f3 could be reported by both i1 and i2, because f2 and f3 are monitored \nin both inputs. Future work can Data Race Atomicity Violation App. Full CFP Full CFP Click 3.6 1.2 2.7 \n1.8 FFT 4.5 1.0 3.8 1.3 LU 4.1 1.0 5.4 1.1 Mozilla 4.1 2.2 3.4 2.8 PBZIP2 4.5 1.2 3.5 1.2 # all reports \nTable 6: Bug-report duplication rate ( # unique reports ) design better monitoring schemes or input/function \nselection schemes to further decrease the duplication rate. Overall, our CFP-guided approach signi.cantly \nreduces the bug-report duplication rate across inputs for both data races and atomicity violations. This \nreduction will naturally lead to more effective bug detection less detection time and similar detection \ncoverage. It will also relieve the developers from identifying and discarding duplicate bug reports. \n 5.5 False negatives Our CFP-guided detection has missed only 1.7 4.5% of all races and atomicity violations \nreported by Full. More im\u00adportantly, it incurs no false negative among failure-inducing races and atomicity \nviolations. Almost all false negatives oc\u00adcur when different inputs cover different basic blocks in a \nfunction and the selected input happens to miss those basic blocks containing data races or atomicity \nviolations. Future work can potentially re.ne the granularity of CFP metric to achieve fewer false negatives. \nWe will discuss this in Sec\u00adtion 5.9. Considering the 1.5X 6.2X speedup achieved by our CFP-guided bug \ndetection, the above false negative rate is very low and is a worthy tradeoff in a practical setting \nwhere developers only have a limited amount of time for bug detection and testing. 5.6 Performance breakdown \nTable 7 shows the detailed performance breakdown among the three steps in CFP-guided bug detection. All \nthe numbers shown in the table are normalized, where 1 is the total time of executing a benchmark through \nall test inputs without any monitoring or instrumentation (shown in the Base column). The Step 1 of our \nCFP-guided approach measures the CFP of every input. In general, it is fast, incurring less than 100% \nof overhead comparing with simply running the program without any monitoring or instrumentation. Mozilla \nincurs the largest overhead for several reasons. Most importantly, it contains many small functions with \njust a few accesses to heap/global variables. These functions lead to considerable overhead in CFP measurement. \nFurthermore, many functions in Mozilla execute synchronization operations, either directly or indirectly \nthrough their callees. As a result, our CFP\u00admeasurement optimization does not help Mozilla as much as \nit does for some other benchmarks. Data Race Atomicity Violation Base CFP CFP CFP CFP Full CFP CFP CFP \nCFP Full App. (sec.) Step1 Step2 Step3 Tot. Tot. Step1 Step2 Step3 Tot. Tot. Click 1.71 1.79 0.04 23.72 \n25.55 87.27 1.79 0.04 10.63 12.46 30.40 FFT 0.03 1.94 0.01 170.66 172.61 1078.22 1.94 0.01 109.95 111.90 \n628.71 LU 0.94 1.89 0.0005 41.53 43.42 212.76 1.89 0.0005 41.26 43.15 207.12 Mozilla 0.30 4.48 0.10 60.39 \n64.88 99.24 4.48 0.10 39.85 44.43 69.88 PBZIP2 5.40 1.06 0.0006 4.87 5.93 19.84 1.06 0.0006 1.38 2.44 \n5.93 Table 7: Performance breakdown of concurrency-bug detection (all the numbers are normalized by \nthe time in Base column, which shows the total time in seconds of running a benchmark through all test \ninputs without any instrumentation or bug detection) Note that measuring the CFP of every input ( CFP-Step1 \n) takes much less time than running full-blown bug detection on every input ( Full-Tot. ). For example, \nthe former takes only 0.2% 5.3% of the time of full-blown race-detection for all benchmarks. This con.rms \nthat CFP has successfully stuck to the simplicity design principle. The Step 2 selects inputs and functions. \nIt takes the smallest amount of time among all three steps. The Step 3 applies concurrency-bug detection \nto selected inputs and functions. Not surprisingly, it is the most time\u00adconsuming step. Of course, it \nis signi.cantly faster than full\u00adblown concurrency-bug detection ( Full Tot. columns). Finally, CFP Tot. \nand Full Tot. columns compare the total time of our CFP-guided detection with that of traditional full-blown \nbug detection. CFP Tot. is the sum of the above three steps. As we can see, our CFP-guided approach is \nsigni.cantly faster than the traditional approach, which is also illustrated in Table 3 and Table 4. \nThe speedup of our CFP-guided approach is mainly de\u00adtermined by the number of selected inputs and functions. \nIntuitively, the fewer selected, the faster our CFP-guided bug detection is. Strictly speaking, the CFP-measurement \ntime would also affect the speedup. However, since it takes much less time than bug detection (Step 3), \nits impact is negligible. For example, our CFP-guided race (atomicity-violation) detection is more than \n6 (5) times as fast as the full race (atomicity-violation) detection for FFT. The reason is that only \n1 out of 8 test inputs is selected. On the other hand, only about 1.6X speedup is achieved by our CFP-guided \napproach for Mozilla, because 5 out of 7 test inputs are selected and about 54% of functions still need \nto be analyzed across inputs. Note that, our CFP-guided approach achieves about 3X speedup in Click, \nalthough as many as 4 out of 6 inputs are selected. The reason is that only about 20% of functions are \nselected across inputs. As can be seen from the performance breakdown, al\u00adthough the .rst two steps of \nour CFP-guided approach in\u00adcurs additional cost, this cost can be easily compensated by the reduction \nof bug-detection time in the third step of our CFP-guided approach. In addition, the CFP-measurement \nand input/function-selection results of the .rst two steps can be shared by race detection and atomicity-violation \ndetection. The speedup will become more signi.cant when we consider these two together. 5.7 Multiple \nbug-detection runs for each input As discussed in Section 2.1, by default, we execute each application \nonly once under each (selected) input for data\u00adrace or atomicity-violation detection, which is the common \npractice in resource-limited software testing. In this sub\u00adsection, we investigate the impact of multiple \nbug-detection runs under each input. We will use the subscript M to differentiate these new settings \nfrom the default settings used earlier, with the details shown in Table 8. Since no extra failure-inducing \nbug reports are generated under multi-run settings, we only discuss the results about all bug reports \nbelow. # of runs for each application under each input Full 1 CFP 1 in Step 1; 1 in Step 3 FullM 10 CFPM \n1 in Step 1; 10 in Step 3 CFP+ 10 in Step 1; 10 in Step 3 M Table 8: Different settings evaluated by \nour experiments. FullM , CFPM, and CFP+ M conduct multiple bug-detection runs for each input, and are \nevaluated in Section 5.7. App. Data Race Atomicity Violation FullM CFPM CFPM + FullM CFPM CFPM + Click \nFFT LU Mozilla PBZIP2 6 0 0 3 1 3 0 0 1 1 3 0 0 6 1 10 0 0 5 2 5 0 0 2 2 5 0 0 8 2  Table 9: The numbers \nof extra bug reports generated by FullM (compared with Full), CFPM and CFP+ M (compared with CFP). As \nwe can see in Table 9, extra bug-detection runs produce few extra bug reports. For example, comparing \nFullM with Full, only 0 10 extra bug reports are generated by the extra bug-detection runs. Click produces \nthe most extra bug reports: 6 extra data races and 10 extra atomicity violations, Clearly, the optimization \neffects are different for different which contribute to only 0.7% and 0.4% increase of the Full applications. \nFor these .ve benchmarks, Optimization 2 is bug-report numbers.3 This result shows that conducting one \nmore effective than Optimization 1. However, neither one bug-detection run for each selected test input \nis suf.cient in of them can replace the other. For example, Optimization resource-limited software testing \nenvironment. 1 is the most effective for Mozilla, where 22% of CFP- CFP-Trace Size (MB) CFP-Step1 Time \n(normalized) App. No Opt. Opt.1 Opt.2 Opt.1+2 No Opt. Opt.1 Opt.2 Opt.1+2 Click 93.40 93.24 9.28 9.25 \n18.36 17.24 1.95 1.79 FFT 0.95 0.94 0.93 0.92 3.10 2.89 2.44 1.94 LU 836.27 835.43 4.85 4.31 74.13 72.65 \n1.97 1.89 Mozilla 10.21 8.41 8.68 7.81 7.44 5.78 5.81 4.48 PBZIP2 0.98 0.93 0.87 0.67 1.14 1.09 1.09 \n1.06 Table 10: The effect of CFP-measurement optimization (CFP-Trace Size and CFP-Step1 Time are both \nthe summation across all inputs; CFP-Step1 Time is normalized as that in Table 7)  Click and Mozilla \nare the only two applications where measurement time and 18% of CFP traces are saved. The the numbers \nof extra bug reports are different among FullM , other four benchmarks all have fewer functions that \nonly CFPM , and CFP + M . access stack variables, and hence bene.t much less from M setting discovers \n3 extra pairs of concurrent functions during its CFP measurement. Conse-and Click, reducing more than \n90% of CFP traces and saving + In Mozilla, the CFPOptimization 1. Optimization 2 is the most effective \nfor LU quently, CFP M discovers more concurrency bugs than CFPM . On the other hand, no extra concurrent \nfunction pair is dis\u00ad + M in Click. As a result, CFPM and CFPM generate the same number of extra bug \nreports. + + covered by CFPmore than 90% of trace-analysis time. The size of CFPs One might wonder how \nmany concurrent function pairs are there for a benchmark. Section 5.8 presents In both Mozilla and Click, \nCFPM generates fewer extra bug reports than FullM . Some of these are caused by concur\u00ad rent function \npairs that are not identi.ed by one-run of CFP measurement. Some of these are located in rarely executed \npaths. By exercising the same functions for multiple times under more inputs, FullM has more chances \nto discover these bugs. Overall, full-blown concurrency-bug detection can bene.t a little bit more from \nextra bug-detection runs for some applications than our CFP approach. However, this bene.t is usually \ntoo small to justify the extra cost incurred by extra bug-detection runs during resource-limited testing. \n 5.8 Other results Optimization effect Section 3.2.3 presents two optimiza\u00ad tions for CFP measurement. \nTable 10 shows their impact on reducing CFP-trace size and CFP-measurement time. Overall, the combined \noptimization effect is signi.cant (comparing Opt.1+2 with No Opt. in Table 10). Since both optimizations \nskip many function entrance/exit records during CFP measurement, up to 99% reduction is achieved for \nCFP-measurement trace size and up to 98% reduction is achieved for total CFP-measurement time. Without \nthese two optimizations, CFP measurement can take up to 74 times the base-line program execution time. \nWith these optimizations, the slowdown is mostly below 2X. 3 The total numbers of bug reports generated \nby Full are shown in the Unique columns of Table 2 the size of aggregated CFP for each benchmark the \ntotal number of unique pairs of concurrent functions across all inputs. For reference, the total number \nof classes, the total number of static functions of each program and the total number of unique functions \nexecuted by test inputs are also listed. Naturally, programs with more (executed) functions have more \nconcurrent functions. At the same time, many executed functions clearly are not concurrent with each \nother due to synchronization. App. # Class # Fun. # Executed Fun. Size of Aggregated CFP Click 133 1889 \n964 2504 FFT 0 23 23 42 LU 0 21 21 62 Mozilla 0 1050 268 22970 PBZIP2 0 125 125 426 Table 11: Total \nnumber of classes, functions and CFPs The Bene.t of Function Selection The performance im\u00adprovement \nof our CFP approach over the Full approach comes from two sources: the reduction in the number of test \nin\u00adputs and the reduction in the number of functions monitored during Step 3. To better understand the \ncontributions from these two sources, we evaluate the total testing time with all functions monitored \nfor selected inputs during Step 3, and compare it with the Full approach and the CFP approach. As shown \nin Table 12, even without selecting functions, input selection alone can provide signi.cant speedup over \nthe Full approach 3.5X for race detection and 3.1X for atomicity-violation detection on average. Function \nselection further improves the performance, achieving 4.0X speedup for race detection and 3.5X speedup \nfor atomicity-violation detection on average. App. w/ function selection w/o function selection Race \nAtom. Vio. Race Atom. Vio. Click 3.5X 2.5X 1.9X 1.4X FFT 6.2X 5.6X 6.2X 5.6X LU 4.9X 4.8X 4.9X 4.8X Mozilla \n1.5X 1.6X 1.3X 1.3X PBZIP2 3.8X 2.4X 3.2X 2.3X Average 4.0X 3.4X 3.5X 3.1X Table 12: The speedup over \nFull approach with and without function selection.  The impact of function selection is different for \ndifferent benchmarks. Click gets the most bene.t. Without function selection, the testing time almost \ndoubles for Click. On the other hand, function selection makes no difference for FFT and LU. The reason \nis that only one input is selected for FFT and LU, respectively. Consequently, all functions are selected \nto test this input under the CFP approach. Overall, both function selection and input selection are useful \nin shortening the testing time. Among these two, input selection has a larger impact for our benchmarks. \n 5.9 Limitations and discussion What about random input selection? An alternative of our CFP-guided \napproach is to randomly select inputs for concurrency-bug detection. We believe that our CFP-guided approach \nhas advantages for several reasons. First, our CFP-guided approach allows us to select not only inputs \nbut also functions for concurrency-bug detection. The capability of selecting functions for a selected \ninput is crucial for bug-detection ef.ciency (Click is an example for this). This cannot be achieved \nby random input selection. Second, random input selection incurs unpredictable false negatives. It may \nhappen to expose many bugs and may as well cause many false negatives. For example, in FFT, when we randomly \nselect one input from the 8-input test set, there is 87.5% probability that applying race detection to \nthis selected input would incur a false-negative rate between 41% and 100%, comparing with the full-blown \ndetection on all inputs. In contrast, our CFP-guided approach will deterministically select the input \nwith the largest CFP and incur only 4.5% false-negative rate. As another example, in PBZIP2, each failure-inducing \nbug report can be exposed by fewer than half of all the test inputs. In fact, there are two failure-inducing \natomicity violations in PBZIP2 that can only be discovered by two and one input out of the 8 test inputs, \nrespectively. It is dif.cult to predict the bug-detection capability when only a couple of inputs are \nrandomly selected. Third, the CFP-guided approach is more informative. Ran\u00addom input selection does not \ntell developers how thorough the bug detection is and when the bug detection can stop. As a coverage \nmetric, CFP provides a quantitative measure\u00adment to developers just like how traditional statement/branch \ncoverage metrics help developers test sequential software. Developers can also combine CFP with other \ninformation, such as which part of the program is more prone to bugs, to further enhance the testing \nquality. Fourth, the only advantage of random input selection over our CFP-guided approach is that it \ntakes less time to select inputs. However, since the time spent running concurrency\u00adbug detectors is \nhuge (Step 3 in Table 7), much longer than the time spent measuring CFP and selecting inputs/functions \nbased on CFP (Step 1 and Step 2 in Table 7), this small performance advantage is negligible in the big \npicture. We should also note that traditional coverage metrics designed for sequential software, such \nas counting how many functions/branches/statements are executed, are not effective at guiding concurrency-bug \ndetection. As a simple example, in FFT and LU, we can design two inputs that have exactly the same command-line \noptions, except that one executes the program in single-threaded mode and the other in multi\u00adthreaded \nmode. These two inputs would cover exactly the same set of functions. However, they clearly have different \ncapabilities in exposing concurrency bugs. What if there are more test inputs? In practice, test-input \nsets used during in-house testing are much larger than the ones available to us and evaluated by us in \nthis paper. We believe the bene.t of our CFP approach will not diminish and can likely get more signi.cant \nfor larger input sets. Most importantly, the phenomenon of inputs sharing common pairs of concurrent \nfunctions and exposing the same concurrency bugs widely exists in reality. During software testing, test \ninput sets are usually designed to achieved good control-.ow coverage and/or good data-.ow coverage. \nIn order to cover a previously uncovered statement, function, or de.ne-use pair, a test input usually \nneeds to execute a lot of statements or functions already covered by other inputs. As a result, there \nis a natural code-coverage overlap across test inputs. Consequently, CFP-coverage overlap and bug-report \nduplication naturally exist among test inputs. Since the time used to measure CFP is only a small percentage \nof that used to detect bugs, we believe the CFP approach will maintain its advantage over traditional \nbug detection for large test-input sets in reality. In fact, for a given program, as the input set grows \nlarger, more bug reports are likely to be duplicates and more CFP are likely to overlap. Thus, the performance \nadvantage of our CFP approach would likely increase. Does CFP work for other detectors? As a general \nmetric characterizing the interleaving space, we believe CFP can help many concurrency-bug detectors \nto work on a set of in\u00adputs. Of course, the bene.t would decrease, if a concurrency\u00adbug detector runs \nmuch faster than the detectors used in this paper (e.g., more than 10 times faster). However, techniques \nused to speed up concurrency-bug detection [63, 71] can po\u00ad tentially also help speed up CFP measurement, \nin which case the bene.t of CFP-guided bug detection will remain. Does CFP work for all applications? \nDifferent applica\u00adtions may bene.t differently from the CFP approach. For ex\u00adample, some I/O-intensive \napplications have relatively small overhead in run-time concurrency-bug detection, and hence will bene.t \nless from CFP-guided bug detection. As discussed in Section 5.5, most false negatives of our CFP approach \noccur in functions that are long and have complicated control .ows. Applications with more functions \nof this type may suffer more false negatives. At the same time, the bene.t of the CFP approach is unlikely \nto decrease when the application gets larger, as long as the ratio of long functions does not increase. \nConsidering that long functions can hurt software modularity and maintainability, we expect most applications, \nno matter small ones or large ones, to bene.t from the CFP approach with only small numbers of false \nnegatives. How to further improve CFP? What is presented above is just a starting point to improving \nmulti-input concurrency\u00adbug detection. There is still room for improvement. In terms of performance, \nfuture work can explore more ef.cient CFP\u00admeasurement techniques with help from static analysis. In terms \nof functionality, a function may not be the best unit for interleaving-space characterization. Sometimes, \na function may be too small as a unit: monitoring the entrances and exits of utility functions that only \nhave a couple of global or heap memory accesses leads to a huge overhead. Sometimes, a function may be \ntoo big as a unit. For example, synchroniza\u00adtion operations inside a function can cause different parts \nof a function to have different logical timestamps; some large functions may include different paths \naccessing completely different global/heap variables. As described in Section 5.5, many false negatives \nin our current implementation occur within these big functions. Future work can explore how to extend \nCFP to better guide cross-input bug detection. 6. Related Work Many tools are designed to detect data \nraces [7, 15, 42, 49, 50, 69], atomicity violations [6, 14, 31, 32, 61, 65], and other types of concurrency \nbugs [25, 33, 59, 67, 70]. Since concurrency-bug detection often involves monitoring many memory accesses \nacross threads and complicated concur\u00adrency analysis, most of these tools incur large overhead. Sampling \n[2, 13, 24, 35], hardware support [21, 46, 47, 57, 63], and other optimization techniques [29] have been \nproposed to improve the performance of each concurrency\u00adbug detection run. This paper has a different \nperspective and can well complement the above techniques. Speci.cally, all the previous works are oblivious \nto the selection of inputs. This paper prioritizes test inputs by their potential to cover the most unexplored \nconcurrent function pairs, so that bug detection across a set of inputs becomes more ef.cient. In addition, \nmany existing performance-enhancing techniques focus on data-race detection. Our CFP-based approach can \nalso help other types of concurrency bugs. The work mentioned above all focuses on dynamic bug detection. \nThe problem of how to ef.ciently detect bugs for a set of inputs applies to only dynamic tools, but not \nstatic tools [12, 39] static tools do not take inputs into account. Of course, static tools encounter \ntheir own challenges in scalability and accuracy, especially for large C/C++ programs, which is partly \nwhy so much work has focused on dynamic techniques. We believe static analysis, such as may-happen\u00adin-parallel \nanalysis [1], can help further improve our CFP\u00ad guided bug detection in the future. Different metrics \nhave been proposed to measure the coverage of interleaving testing [10, 22, 26, 30, 58, 60, 66]. Apart \nfrom the traditional coverage-based adequacy criteria, saturation-based adequacy criteria [58] are also \nproposed to help apply these interleaving-coverage metrics into testing for multi-threaded software. \nOur CFP metric complements traditional interleaving-coverage metrics and adequacy criteria by focusing \non a different target: to select bug-detection inputs from a given input set. Due to this different target, \nour design also faces different challenges. For example, the measurement of CFP coverage has to be ef.cient, \nand the CFP metric has to strongly correlate with the follow-up concurrency-bug detector(s). We did not \ndirectly reuse traditional coverage metrics, because they tend to be too expensive to measure before \nbug detection or not strongly correlated with data races or atomicity violations. Of course, the traditional \nadequacy criteria, especially the saturation\u00adbased adequacy criteria, can help our CFP approach to judge \nwhether more test inputs are needed. Many techniques are proposed [3, 10, 37, 44, 53] to effectively \nexplore the interleaving space of each input. Different from these techniques, this paper tries to coordinate \nbug detection across inputs. It can help identify testing candidates more ef.ciently for these interleaving-testing \ntechniques on a set of inputs. A recent position paper [9] by the authors presents the preliminary version \nof this work. That position paper focuses on understanding the interleaving-space overlap across inputs \nand across different versions of a software project. A prelimi\u00adnary idea of CFP-guided race detection \nwas proposed there. However, the algorithm to measure CFP in that paper was not accurate enough and may \nmiss many concurrent function pairs. As no optimizations are proposed, the CFP measure\u00adment in that paper \nwas also slow, causing as much as 30X slowdown for Mozilla. In addition, cross-input atomicity\u00adviolation \ndetection was not discussed. Symbolic execution has been used for testing sequential software [5, 19, \n55] and unit testing multi-threaded software [54]. Model checking for multi-threaded software has been \nwell studied [11, 18, 23, 48, 62]. The observation that inter\u00ad leavings overlap across inputs is not \nnew in model check\u00ading and partial-order reduction is often used to avoid repeat\u00adedly exploring the same \nstate [18]. Unfortunately, this ob\u00ad servation has never been studied in the context of dynamic concurrency-bug \ndetection and related testing. Due to the different goals and approaches in these two .elds, new ap\u00adproaches \nare needed to exploit interleaving-space overlap. 7. Conclusions This paper proposes improving the quality \nof concurrency\u00adbug detection and multi-threaded software testing by avoiding redundant analysis across \ninputs. Our study of open-source applications shows that a signi.cant number of races and single-variable \natomicity violations overlap across inputs. Based on this study, we propose a new metric, concurrent \nfunction pairs (CFP), to guide multi-input concurrency-bug detection. Our evaluation using 5 open-source \napplications shows that CFP-guided concurrency-bug detection can effec\u00adtively reduce redundant bug detection \nand improve the overall bug-detection ef.cacy. Acknowledgments We thank the anonymous reviewers for their \ninsightful feed\u00adback which has substantially improved the content and presen\u00adtation of this paper. We \nthank Borui Wang and Peisen Zhao for their assistance in building an early protocol of this work. This \nwork is supported in part by NSF grants CCF-1018180, CCF-1054616, and CCF-1217582; and a Clare Boothe \nLuce faculty fellowship. Last but not the least, one of our authors, Shan Lu, wants to thank her daughter, \nSilu, for waiting pa\u00adtiently for seven days beyond her due date. Coming to the world three hours before \nthe OOPSLA submission deadline, Silu gave her mom enough time to .nish writing this paper. References \n[1] S. Agarwal, R. Barik, V. Sarkar, and R. K. Shyamasundar. May-happen-in-parallel analysis of x10 programs. \nIn PPoPP, 2007. [2] M. D. Bond, K. E. Coons, and K. S. McKinley. Pacer: Proportional detection of data \nraces. In PLDI, 2010. [3] S. Burckhardt, P. Kothari, M. Musuvathi, and S. Nagarakatte. A randomized scheduler \nwith probabilistic guarantees of .nding bugs. In ASPLOS, 2010. [4] J. Burnim and K. Sen. Asserting and \nchecking determinism for multithreaded programs. In FSE, 2009. [5] C. Cadar, D. Dunbar, and D. R. Engler. \nKLEE: Unassisted and automatic generation of high-coverage tests for complex systems programs. In OSDI, \n2008. [6] F. Chen, T. F. Serbanuta, and G. Rosu. jPredictor: a predictive runtime analysis tool for Java. \nIn ICSE, 2008. [7] J.-D. Choi et al. Ef.cient and precise datarace detection for multithreaded object-oriented \nprograms. In PLDI, 2002. [8] Click. The Click Modular Router Projec. http://read.cs.ucla.edu/click/click. \n [9] D. Deng, W. Zhang, B. Wang, P. Zhao, and S. Lu. Understand\u00ading the interleaving-space overlap across \ninputs and software versions. In 4th USENIX Workshop on Hot Topics in Paral\u00adlelism, 2012. [10] O. Edelstein, \nE. Farchi, Y. Nir, G. Ratsaby, and S. Ur. Multi\u00adthreaded Java program test generation. IBM Systems Journal, \n2002. [11] M. Emmi, S. Qadeer, and Z. Rakamari c.\u00b4Delay-bounded scheduling. In POPL, 2011. [12] D. Engler \nand K. Ashcraft. RacerX: Effective, static detection of race conditions and deadlocks. In SOSP, 2003. \n[13] J. Erickson, M. Musuvathi, S. Burckhardt, and K. Olynyk. Effective data-race detection for the kernel. \nIn OSDI, 2010. [14] C. Flanagan and S. N. Freund. Atomizer: a dynamic atomicity checker for multithreaded \nprograms. In POPL, 2004. [15] C. Flanagan and S. N. Freund. Fasttrack: ef.cient and precise dynamic race \ndetection. In PLDI, 2009. [16] J. Gilchrist. Parallel BZIP2, Data Compression Software. http://compression.ca/pbzip2/. \n[17] GNU. gcov. http://www.linuxcommand.org/man pages/gcov1.html. [18] P. Godefroid. Partial-Order Methods \nfor the Veri.cation of Concurrent Systems: An Approach to the State-Explosion Problem. Springer-Verlag \nNew York, Inc., 1996. [19] P. Godefroid, N. Klarlund, and K. Sen. Dart: directed auto\u00admated random testing. \nIn PLDI, 2005. [20] P. Godefroid and N. Nagappan. Concurrency at Microsoft an exploratory survey. Technical \nreport, Microsoft Research, MSR-TR-2008-75, May 2008. [21] J. L. Greathouse, Z. Ma, M. I. Frank, R. Peri, \nand T. M. Austin. Demand-driven software race detection using hardware performance counters. In ISCA, \n2011. [22] M. J. Harrold and B. A. Malloy. Data .ow testing of paral\u00adlelized code. In Proceedings of \nthe International Conference on Software Maintenance, 1992. [23] G. J. Holzmann. The SPIN Model Checker: \nPrimer and Reference Manual. Addison-Wesley Professional, 2003. [24] G. Jin, A. Thakur, B. Liblit, and \nS. Lu. Instrumentation and sampling strategies for Cooperative Concurrency Bug Isolation. In OOPSLA, \n2010. [25] P. Joshi, C.-S. Park, K. Sen, and M. Naik. A randomized dy\u00adnamic program analysis technique \nfor detecting real deadlocks. In PLDI, 2009. [26] P. V. Koppol and K.-C. Tai. An incremental approach \nto structural testing of concurrent software. In ISSTA, 1996. [27] C. Lattner and V. Adve. LLVM: A compilation \nframework for lifelong program analysis &#38; transformation. In CGO, 2004. [28] N. Leveson and C. S. \nTurner. An investigation of the Therac-25 accidents. In IEEE Computer, 1993. [29] D. Li, W. Srisa-an, \nand M. B. Dwyer. SOS: saving time in dynamic race detection with stationary analysis. In OOPSLA, 2011. \n[30] S. Lu, W. Jiang, and Y. Zhou. A study of interleaving coverage criteria. In FSE, 2007. [31] S. \nLu, S. Park, C. Hu, X. Ma, W. Jiang, Z. Li, R. A. Popa, and Y. Zhou. MUVI: Automatically inferring multi-variable \naccess correlations and detecting related semantic and concurrency bugs. In SOSP, 2007. [32] S. Lu, J. \nTucek, F. Qin, and Y. Zhou. AVIO: detecting atomicity violations via access interleaving invariants. \nIn ASPLOS, 2006. [33] B. Lucia and L. Ceze. Finding concurrency bugs with context\u00adaware communication \ngraphs. In MICRO, 2009. [34] C.-K. Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S. Wallace, \nV. J. Reddi, and K. Hazelwood. Pin: building cus\u00adtomized program analysis tools with dynamic instrumentation. \nIn PLDI, 2005. [35] D. Marino, M. Musuvathi, and S. Narayanasamy. Effective sampling for lightweight \ndata-race detection. In PLDI, 2009. [36] Mozilla. SpiderMonkey, Mozilla s JavaScript engine. https://developer.mozilla.org/en/SpiderMonkey. \n[37] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. A. Nainar, and I. Neamtiu. Finding and reproducing \nheisenbugs in concurrent programs. In OSDI, 2008. [38] S. Nagarakatte, S. Burckhardt, M. M. K. Martin, \nand M. Musu\u00advathi. Multicore acceleration of priority-based schedulers for concurrency bug detection. \nIn PLDI, 2012. [39] M. Naik, A. Aiken, and J. Whaley. Effective static race detection for Java. In PLDI, \n2006. [40] S. Narayanasamy, Z. Wang, J. Tigani, A. Edwards, and B. Calder. Automatically classifying \nbenign and harmful data racesallusing replay analysis. In PLDI, 2007. [41] N. Nethercote and J. Seward. \nValgrind: a framework for heavyweight dynamic binary instrumentation. In PLDI, 2007. [42] R. H. B. Netzer \nand B. P. Miller. Improving the accuracy of data race detection. In PPoPP, 1991. [43] C.-S. Park and \nK. Sen. Randomized active atomicity violation detection in concurrent programs. In FSE, 2008. [44] S. \nPark, S. Lu, and Y. Zhou. CTrigger: Exposing atomicity violation bugs from their .nding places. In ASPLOS, \n2009. [45] PCWorld. Nasdaq s Facebook Glitch Came From Race Condi\u00adtions. http://www.pcworld.com/businesscenter/article/255911/ \nnasdaqs facebook glitch came from race conditions.html. [46] M. Prvulovic. Cord:cost-effective (and nearly \noverhead-free) order-reordering and data race detection. In HPCA, 2006. [47] M. Prvulovic and J. Torrellas. \nReEnact: Using thread-level speculation mechanisms to debug data races in multithreaded codes. In ISCA, \n2003. [48] S. Qadeer and D. Wu. Kiss: keep it simple and sequential. In PLDI, 2004. [49] R. Raman, J. \nZhao, V. Sarkar, M. T. Vechev, and E. Yahav. Ef.cient data race detection for async-.nish parallelism. \nIn RV, 2010. [50] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. An\u00adderson. Eraser: A dynamic \ndata race detector for multithreaded programs. ACM TOCS, 1997. [51] SDTimes. Testers spend too much \ntime testing. http://www.sdtimes.com/SearchResult/31134. [52] SecurityFocus. Software bug contributed \nto blackout. http://www.securityfocus.com/news/8016. [53] K. Sen. Race directed random testing of concurrent \nprograms. In PLDI, 2008. [54] K. Sen and G. Agha. Automated systematic testing of open distributed programs. \nIn FSE, 2006. [55] K. Sen, D. Marinov, and G. Agha. CUTE: a concolic unit testing engine for c. In ESEC/SIGSOFT \nFSE, 2005. [56] K. Serebryany and T. Iskhodzhanov. Thread\u00adsanitizer, a valgrind-based detector of data \nraces. http://code.google.com/p/data-race-test/wiki/ThreadSanitizer. [57] T. Sheng, N. Vachharajani, \nS. Eranian, R. Hundt, W. Chen, and W. Zheng. Racez: a lightweight and non-invasive race detection tool \nfor production applications. In ICSE, 2011. [58] E. Sherman, M. B. Dwyer, and S. Elbaum. Saturation-based \ntesting of concurrent programs. In FSE, 2009. [59] Y. Shi, S. Park, Z. Yin, S. Lu, Y. Zhou, W. Chen, \nand W. Zheng. DefUse: De.nition-use invariants for detecting concurrency and sequential bugs. In OOPSLA, \n2010. [60] R. N. Taylor, D. L. Levine, and C. D. Kelly. Structural testing of concurrent programs. IEEE \nTrans. Softw. Eng., 1992. [61] M. Vaziri, F. Tip, and J. Dolby. Associating synchronization constraints \nwith data in an object-oriented language. In POPL, 2006. [62] W. Visser, K. Havelund, G. Brat, S. Park, \nand F. Lerda. Model checking programs. Automated Soft. Eng. Journal, 2003. [63] E. Vlachos, M. L. Goodstein, \nM. A. Kozuch, S. Chen, B. Fal\u00adsa., P. B. Gibbons, and T. C. Mowry. Paralog: enabling and accelerating \nonline parallel monitoring of multithreaded appli\u00adcations. In ASPLOS, 2010. [64] S. C. Woo, M. Ohara, \nE. Torrie, J. P. Singh, and A. Gupta. The SPLASH-2 programs: Characterization and methodological considerations. \nIn ISCA, 1995. [65] M. Xu, R. Bod\u00b4ik, and M. D. Hill. A serializability violation detector for shared-memory \nserver programs. In PLDI, 2005. [66] C.-S. D. Yang, A. L. Souter, and L. L. Pollock. All-du-path coverage \nfor parallel programs. In ISSTA, 1998. [67] J. Yu and S. Narayanasamy. A case for an interleaving constrained \nshared-memory multi-processor. In ISCA, 2009. [68] J. Yu, S. Narayanasamy, C. Pereira, and G. Pokam. \nMaple: a coverage-driven testing tool for multithreaded programs. In OOPSLA, 2012. [69] Y. Yu, T. Rodeheffer, \nand W. Chen. Racetrack: Ef.cient detection of data race conditions via adaptive tracking. In SOSP, 2005. \n[70] W. Zhang, J. Lim, R. Olichandran, J. Scherpelz, G. Jin, S. Lu, and T. Reps. ConSeq: Detecting concurrency \nbugs through sequential errors. In ASPLOS, 2011. [71] P. Zhou, R. Teodorescu, and Y. Zhou. HARD: Hardware\u00adassisted \nlockset-based race detection. In HPCA, 2007.   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>In the multi-core era, it is critical to efficiently test multi-threaded software and expose concurrency bugs before software release. Previous work has made significant progress in detecting and validating concurrency bugs under a given input. Unfortunately, software testing always faces large sets of test inputs, and existing techniques are still too expensive to be applied to every test input in practice.</p> <p>In this paper, we use open-source software to study how existing concurrency-bug detection tools work for a set of inputs. The study shows that an interleaving pattern, such as a data race or an atomicity violation, can often be exposed by many inputs. Consequently, existing bug detectors would inevitably waste their bug detection effort to generate duplicate bug reports, when applied to a set of inputs.</p> <p>Guided by the above study, we propose a coverage metric, Concurrent Function Pairs (CFP), to efficiently approximate how interleavings overlap across inputs. Using CFP, we have designed a new approach to detecting data races and atomicity-violation bugs for a set of inputs.</p> <p>Our evaluation on open-source C/C++ applications shows that our CFP-guided approach can effectively accelerate concurrency-bug detection for a set of inputs by reducing redundant detection effort across inputs.</p>", "authors": [{"name": "Dongdong Deng", "author_profile_id": "81504681981", "affiliation": "University of Wisconsin, Madison, Madison, WI, USA", "person_id": "P4290463", "email_address": "dongdong@cs.wisc.edu", "orcid_id": ""}, {"name": "Wei Zhang", "author_profile_id": "81458642805", "affiliation": "University of Wisconsin, Madison, Madison, WI, USA", "person_id": "P4290464", "email_address": "wzh@cs.wisc.edu", "orcid_id": ""}, {"name": "Shan Lu", "author_profile_id": "81100052818", "affiliation": "University of Wisconsin, Madison, Madison, WI, USA", "person_id": "P4290465", "email_address": "shanlu@cs.wisc.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509539", "year": "2013", "article_id": "2509539", "conference": "OOPSLA", "title": "Efficient concurrency-bug detection across inputs", "url": "http://dl.acm.org/citation.cfm?id=2509539"}