{"article_publication_date": "10-29-2013", "fulltext": "\n OC T E T: Capturing and Controlling Cross-Thread Dependences Ef.ciently * Michael D. Bond Milind Kulkarni \nMan Cao Minjia Zhang Meisam Fathi Salmi Swarnendu Biswas Aritra Sengupta Jipeng Huang Ohio State University \nPurdue University mikebond@cse.ohio-state.edu, milind@purdue.edu, {caoma,zhanminj,fathi,biswass,sengupta,huangjip}@cse.ohio-state.edu \n Abstract Parallel programming is essential for reaping the bene.ts of parallel hardware, but it is notoriously \ndif.cult to de\u00advelop and debug reliable, scalable software systems. One key challenge is that modern \nlanguages and systems provide poor support for ensuring concurrency correctness proper\u00adties atomicity, \nsequential consistency, and multithreaded determinism because all existing approaches are impracti\u00adcal. \nDynamic, software-based approaches slow programs by up to an order of magnitude because capturing and \ncontrol\u00adling cross-thread dependences (i.e., con.icting accesses to shared memory) requires synchronization \nat virtually every access to potentially shared memory. This paper introduces a new software-based concurrency \ncontrol mechanism called OC T E T that soundly captures cross-thread dependences and can be used to build \ndynamic analyses for concurrency correctness. OC T E T achieves low overheads by tracking the locality \nstate of each potentially shared object. Non-con.icting accesses conform to the lo\u00adcality state and require \nno synchronization; only con.icting accesses require a state change and heavyweight synchro\u00adnization. \nThis optimistic tradeoff leads to signi.cant ef.\u00adciency gains in capturing cross-thread dependences: \na proto\u00adtype implementation of O C T E T in a high-performance Java virtual machine slows real-world \nconcurrent programs by only 26% on average. A dependence recorder, suitable for * This material is based \nupon work supported by the National Science Foundation under Grants CAREER-1253703 and CSR-1218695. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. Copyrights for components of this work owned \nby others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, \nUSA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2374-1/13/10. \n. . $15.00. http://dx.doi.org/10.1145/2509136.2509519 record &#38; replay, built on top of OC T E T \nadds an additional 5% overhead on average. These results suggest that OC T E T can provide a foundation \nfor developing low-overhead an\u00adalyses that check and enforce concurrency correctness. Categories and \nSubject Descriptors D.3.4 [Programming Languages]: Processors Compilers, Run-time environ\u00adments Keywords \nconcurrency correctness; dynamic analysis 1. Introduction Software must become more concurrent to reap \nthe bene\u00ad.ts of the ever-increasing parallelism available in modern hardware. However, writing correct \nparallel programs on modern hardware is notoriously dif.cult. Widely used, high\u00adperformance programming \nlanguages provide shared mem\u00adory and locks, which are hard to use to develop software that is both correct \nand scalable. Programmers must deal with data races, atomicity violations, deadlock, poor scalability, \nweak memory models, and nondeterminism. To address the challenges of writing parallel programs, researchers \nhave proposed a wide variety of language and system support to guarantee correct concurrent execution \nei\u00adther by enforcing crucial correctness properties or by check\u00ading such properties and detecting when \nthey are violated [5, 6, 10, 13 16, 18, 21, 23, 30 32, 34, 35, 37 39, 41, 45, 50, 52, 53]. For the rest \nof the paper, we refer to all such ap\u00adproaches as analyses for brevity. Examples include enforc\u00ading determinism \nvia record &#38; replay (e.g., DoublePlay [50]) and deterministic execution (e.g., CoreDet [6]); enforcing \natomicity (transactional memory [21]); checking atomicity (e.g., Velodrome [18]); and checking sequential \nconsistency and data-race freedom (e.g., DRFx [37]). While these proposed analyses provide desirable \ncorrect\u00adness properties that ease the task of parallel programming, they are all impractical in various \nways (Section 2.1). Many require expensive whole-program analysis or custom hard\u00adware support. A promising \nclass of analyses are dynamic and software-only, avoiding both complex compiler analy\u00adsis and specialized \nhardware. Unfortunately, such analyses suffer from very high overhead, slowing down applications by roughly \nan order of magnitude. In this paper, we focus on the key cost that makes dy\u00adnamic, software-only analyses \nexpensive: the challenge of dealing with cross-thread dependences, the dependences that arise between \nthreads as they access shared memory. Virtually all analyses rely, at their root, on soundly captur\u00ading \ncross-thread dependences and using this information to enforce or check concurrency correctness. However, \nbecause shared-memory programs can introduce cross-thread depen\u00addences on any shared-memory access, correctly \ndetecting all such dependences requires existing software analyses to in\u00adsert synchronized instrumentation \nat every access (unless the access can be proven to be well-synchronized). The synchro\u00adnization overhead \nof this instrumentation is proportional to the number of program accesses to potentially shared data, \nand hence slows programs substantially. This paper presents OC T E T, a novel concurrency con\u00adtrol mechanism \nthat soundly captures all cross-thread de\u00adpendences without heavyweight synchronized instrumenta\u00adtion. \nIn fact, through an object-based, cache-coherence-like mechanism, OC T E T s synchronization overheads \nare merely proportional to the number of cross-thread dependences in a program, rather than all accesses. \nContributions The key insight undergirding OC T E T is that most accesses to memory even shared memory \nexhibit thread locality. The same thread repeatedly accesses an object, or multiple threads perform only \nreads, so the accesses cannot introduce cross-thread dependences. By ef.ciently detecting (without synchronization) \nwhether an access is de.nitely not involved in a cross-thread dependence, OC T E T is able to avoid ex\u00adpensive \nsynchronization in most cases, and only incurs high overhead when a cross-thread dependence occurs. OC \nT E T achieves these performance gains by implement\u00ading a cache-coherence-like system at the granularity \nof ob\u00adjects. Each potentially shared object has a thread local\u00adity state associated with it. This state \nexpresses whether a thread can access the object without synchronization. A thread T can have write access \n(WrExT), exclusive read ac\u00adcess (RdExT), or shared read access (RdSh); these states correspond to the \nM, E, and S states, respectively, in a MESI cache coherence protocol [44]. OC T E T s instrumentation \nchecks, before each object access, whether the access is compatible with the object s state. As in hardware \ncache coherence, if a thread tries to access an object that is already in a compatible state, no other \nwork is necessary. There is no possibility of a cross-thread dependence, and OC T E T needs no synchronization. \nInstead, synchronization is only necessary when a thread tries to access an object in an in\u00adcompatible \nstate. In such situations, OC T E T uses various protocols to safely perform the state change before \nallowing the thread s access to proceed. Section 3 describes the de\u00adsign of OC T E T in more detail, \nincluding elaborating on these state change protocols. Section 3.7 proves the soundness of the scheme \n(i.e., all cross-thread dependences are captured) and its liveness (i.e., OC T E T does not deadlock). \n We have implemented OC T E T in a high-performance JVM; Section 4 describes the details. Section 5 evaluates \nthe performance and behavior of our OC T E T implementation on 13 large, multithreaded Java benchmark \napplications. We present statistics that con.rm our hypotheses about the typ\u00adical behavior of multithreaded \nprograms, justifying the de\u00adsign principles of OC T E T, and we demonstrate that our im\u00adplementation \nof OC T E T has overheads of 26% on average signi.cantly better than the overheads of prior mechanisms, \nand potentially low enough for production systems. Because a variety of analyses rely on taking action \nupon the detection of cross-thread dependences, OC T E T can serve as a foundation for designing new, \nef.cient analyses. Sec\u00adtion 6 discusses opportunities and challenges for implement\u00ading ef.cient analyses \non top of OC T E T. Section 7 presents an API for building analyses on top of OC T E T, and we describe \nand evaluate how a low-overhead dependence recorder uses this API, suggesting OC T E T can be a practical \nplatform for new analyses. 2. Background and Motivation Language and system support for concurrency correctness \noffers signi.cant reliability, scalability, and productivity bene.ts. Researchers have proposed many \napproaches that leverage static analysis and language support (e.g., [9, 10, 39]) and custom hardware \n(e.g., [23, 35, 38]). Unfortunately, these techniques are impractical for contemporary systems: suf.ciently \nprecise static analyses do not scale to large ap\u00adplications; language-based approaches do not suf.ce \nfor ex\u00adisting code bases; and hardware support does not exist in current architectures. As a result of \nthese drawbacks, there has been substantial interest in dynamic, sound, 1 software-only approaches guar\u00adanteeing \nconcurrency correctness. Section 2.1 explains why software-based analyses suffer from impractically high \nover\u00adhead. Section 2.2 illuminates the key issue: the high over\u00adhead of capturing or controlling cross-thread \ndependences. 2.1 Dynamic Support for Concurrency Correctness This section motivates and describes key \nanalyses and sys\u00adtems, which we call simply analyses, that guarantee concur\u00adrency correctness properties \nby either enforcing or checking these properties at run time. For each analysis, we describe the drawbacks \nof prior work and then identify the key per\u00adformance challenge(s). Multithreaded record &#38; replay. \nRecord &#38; replay of mul\u00adtithreaded programs provides debugging and systems ben\u00ad 1 Following prior \nwork, a dynamic analysis or system is sound if it guaran\u00adtees no false negatives for the current execution. \n e.ts. Of.ine replay allows reproducing production failures at a later time. Online replay allows running \nmultiple in\u00adstances of the same process simultaneously with the same interleavings, enabling systems \nbene.ts such as replication\u00adbased fault tolerance. Recording execution on multiproces\u00adsors is hard due \nto the frequency of potentially racy shared memory accesses, which require synchronized instrumenta\u00adtion \nto capture soundly [30]. Chimera rules out non-racy ac\u00adcesses using whole-program static analysis, which \nreports many false positives [31]. To achieve low overhead, Chimera relies on pro.ling runs to identify \nmostly non-overlapping accesses and expand synchronization regions. Many multithreaded record &#38; replay \napproaches sidestep the problem of capturing cross-thread dependences explic\u00aditly, but introduce limitations. \nSeveral support only online or of.ine replay but not both (e.g., [32, 45]). DoublePlay requires twice \nas many cores as the original program to pro\u00advide low overhead, and it relies on data races mostly not \ncausing nondeterminism, to avoid frequent rollbacks [50]. The key performance challenge for recording \nmulti\u00adthreaded execution is tracking cross-thread dependences. Deterministic execution. An alternative \nto record &#38; replay is executing multithreaded programs deterministically [6, 14, 34, 41]. As with \nrecord &#38; replay, prior approaches avoid capturing cross-thread dependences explicitly in software \nbecause of the high cost. Existing approaches all have seri\u00adous limitations: they either do not handle \nracy programs [41], add high overhead [6], require custom hardware [14], or pro\u00advide per-thread address \nspaces and merge changes at syn\u00adchronization points, which may not scale well to programs with .ne-grained \nsynchronization [34]. The performance challenge for multithreaded determinis\u00adtic execution is controlling \nthe ways in which threads inter\u00adleave, i.e., controlling cross-thread dependences. Guaranteeing sequential \nconsistency and/or data race freedom. An execution is sequentially consistent (SC) if threads operations \nappear to be totally ordered and respect program order [29]. An execution is data-race-free (DRF) if \nall con.icting accesses are ordered by the happens-before relationship [28], i.e., ordered by synchronization. \nLanguage memory models typically guarantee SC for DRF execu\u00adtions [1, 2, 36]. Sound and precise checking \nof SC or DRF is expen\u00adsive (even with recent innovations in happens-before race detection [16]). An attractive \nalternative to checking SC or DRF is the following relaxed constraints: DRF execu\u00adtions must report no \nviolation, and SC-violating executions must report a violation, but SC executions with races may or may \nnot report a violation [19]. Recent work applies this insight by checking for con.icts between overlapping, \nsynchronization-free regions [35, 37], but it relies on custom hardware to detect con.icting accesses \nef.ciently. The main performance challenge of detecting data races soundly is tracking cross-thread dependences. \nPrecision is also important: the detector must maintain when variables were last accessed in order to \navoid false positives. Checking atomicity. An operation is atomic if it appears to happen all at once \nor not at all. Dynamic analysis can check that existing lock-based programs conform to an atomic\u00adity \nspeci.cation [18, 53]. Velodrome soundly and precisely checks con.ict serializability (CS), a suf.cient \ncondition for atomicity, by constructing a region dependence graph that requires capturing cross-thread \ndependences [18]. It cannot check for atomicity violations in production because it slows programs by \nabout an order of magnitude. Similarly to detecting races, the performance challenges of detecting atomicity \nviolations are tracking cross-thread dependences soundly and providing precision. Enforcing atomicity. \nTransactional memory (TM) sys\u00adtems enforce programmer-speci.ed atomicity annotations by speculatively \nexecuting atomic regions as transactions, which are rolled back if a region con.ict occurs [21]. Custom-hardware-based \napproaches offer low overhead [20], but any real hardware support will likely be limited [5],2 making \nef.cient software TM (STM) an important long\u00adterm goal. Existing STMs suffer from two major, related \nproblems poor performance and weak semantics. Exist\u00ading STM systems slow transactions signi.cantly in \norder to detect con.icting accesses soundly. Furthermore, these sys\u00adtems typically provide only weak \natomicity semantics be\u00adcause strong atomicity (detecting con.icts between transac\u00adtions and non-transactional \ninstructions) slows all program code substantially in order to detect con.icts between trans\u00adactional \nand non-transactional code. Achieving ef.cient, strongly atomic STM is challeng\u00ading because of the cost \nof detecting and resolving cross\u00adthread dependences throughout program execution. STMs also need some \nprecision about when variables were last ac\u00adcessed (e.g., read/write sets [38]) in order to determine \nif a con.icting variable was accessed in an ongoing transaction. In addition to these speci.c analyses, \nresearchers have recently stressed the general importance of dynamic ap\u00adproaches for providing programmability \nand reliability guar\u00adantees and simplifying overly-complex semantics [1, 12]. 2.2 Capturing and Controlling \nCross-Thread Dependences Despite its bene.ts, ef.cient software support for check\u00ading and enforcing concurrency \ncorrectness properties has re\u00admained elusive: existing software-based analyses slow pro\u00adgrams signi.cantly, \noften by up to an order of magnitude. What makes this diverse collection of analyses and systems so slow? \nAs seen in the previous section, a recurring theme emerges: to capture concurrent behavior accurately, \nthese 2 http://software.intel.com/en-us/blogs/2012/02/07/ transactional-synchronization-in-haswell/ \n// T1: // T2: ... = obj1. f ; obj2 . f = ...; Figure 1. Potential cross-thread dependence. do {lastState \n= obj. state ; // load per-object metadata } while ( lastState == LOCKED ||! CAS(&#38;obj.state, lastState \n, LOCKED)); if ( lastState != WrEx) { T handlePotentialDependence (...) ; } obj . f = ...; // program \nwrite memfence; obj . state = WrExT; // unlock and update metadata Figure 2. A write barrier that captures \ncross-thread dependences. Before the write, the per-object metadata obj.state should be WrExT (T is the \ncurrent thread), or else there is a potential cross\u00adthread dependence. LOCKED indicates the metadata \nis locked. analyses must capture cross-thread dependences, two ac\u00adcesses executed by different threads \nthat have any kind of data dependence: a true (write read), anti (read write), or output (write write) \ndependence. Figure 1 shows a potential cross-thread dependence. Since this dependence arises only when \nobj1 and obj2 reference the same object, and when the threads interleave in a particular manner, it is \nhard to accu\u00adrately predict this dependence statically. Capturing cross-thread dependences typically \ninvolves adding barriers3 that access and update shared metadata at each program read and write. Because \nprogram accesses may not be well synchronized (i.e., may have a data race), the barriers must use synchronization \nto avoid data races on the metadata accesses. Figure 2 shows how a barrier enforces atomic metadata accesses. \nIt uses per-object metadata, an\u00adnotated as obj.state, to track the last thread to access the metadata. \nThe value of obj.state might be WrExT (write ex\u00adclusive for thread T) where T is the last thread that \nwrote the object referenced by obj, or RdSh (read shared) if any thread last read the object. Tracking \nthese per-object states is a natural way to capture cross-thread dependences; we use the notations obj.state, \nWrExT, and RdSh to closely match the notations used by our new approach (Section 3). If another thread \naccesses obj, it handles the the potential cross-thread dependence in an analysis-speci.c way and up\u00addates \nthe state. The barrier locks the per-object metadata using an atomic compare-and-swap (CAS) instruction,4 \ncre\u00adating a small critical section while handling the potential dependence and performing the program \nmemory access. Our evaluation shows that adding such a barrier to all potentially shared memory accesses \nslows programs by 6.2X 3 A read (or write) barrier is instrumentation that executes before every read \n(write) [54]. 4 The atomic instruction CAS(addr, oldVal, newVal) attempts to update addr from oldVal \nto newVal, returning true on success. (We use the common term CAS, but our CAS acts like a test-and-set \noperation.)  on average even without performing any analysis-speci.c operations, i.e., handlePotentialDependence() \nas a no-op (Section 5.4). These overheads are unsurprising because atomic instructions and memory fences \nserialize in-.ight instructions, and atomic instructions invalidate cache lines. The next section introduces \nOC T E T, a system that cap\u00adtures cross-thread dependences ef.ciently. Section 6 de\u00adscribes how OC T \nE T can be used to build new analyses that check and enforce concurrency correctness, and Section 7 sketches \nhow OC T E T can be used to build a low-overhead system for recording concurrent execution. 3. Capturing \nand Controlling Cross-Thread Dependences Ef.ciently This section describes our approach for ef.ciently \nand ac\u00adcurately detecting or enforcing cross-thread dependences on shared objects:5 data dependences \ninvolving accesses to the same variable by different threads. In prior work, captur\u00ading or controlling \ncross-thread dependences in software has proven expensive (Section 2). We have designed a dynamic analysis \nframework called OC T E T 6 to track cross-thread dependences at low overhead. OC T E T s approach is \nbased on a key insight: the vast major\u00adity of accesses, even to shared objects, are not involved in a \ncross-thread dependence. If OC T E T can detect ef.ciently whether an access cannot create a cross-thread \ndependence, it can perform synchronization only when con.icting ac\u00adcesses occur, and dramatically lower \nthe overhead of detect\u00ading cross-thread dependences. To achieve this goal, OC T E T associates a thread-locality \nstate with each potentially shared object. This state describes which accesses will de.nitely not cause \ncross-thread depen\u00addences. These accesses proceed without synchronization or changing the object s state \n(the fast path), while accesses that imply a potential cross-thread dependence trigger a co\u00adordination \nprotocol involving synchronization to change the object s state so that the access is permitted (the \nslow path). OC T E T s synchronization costs are thus proportional to the number of con.icting accesses \nin a program, rather than all accesses or even shared accesses. OC T E T s state transitions provide \ntwo key features: 1. Each state transition results in the creation of a happens\u00adbefore edge,7 as shown \nin the following section. All cross-thread dependences are guaranteed to be ordered (transitively) by \nthese happens-before edges (as proved in Section 3.7). 5 This paper uses the term object to refer to \nany unit of shared memory. 6 Optimistic cross-thread explicit tracking. O C T E T optimistically as\u00adsumes \nmost accesses do not con.ict and supports them at low overhead, at the cost of more expensive coordination \nwhen accesses con.ict. 7 Happens-before is a partial order on program and synchronization or\u00adder [28]. \nCreating a happens-before edge means ensuring a happens-before ordering from the source to the sink of \nthe edge. Fast/slow Transition Old New Synchronization Cross-thread path type state Access state needed \ndependence? WrExT R or W by T Same Fast Same state RdExT R by T Same None No RdShc R by T if T.rdShCount \n= c Same Upgrading RdExT RdExT1 W by T R by T2 WrExT RdShgRdShCount CAS No Potentially yes Fence RdShc \nR by T if T.rdShCount < c (T.rdShCount = c) Memory fence Potentially yes Slow WrExT1 W by T2 WrExInt \nT2 . WrExT2 Con.icting WrExT1 RdExT1 R by T2 W by T2 RdExInt T2 . RdExT2 WrExInt T2 . WrExT2 Roundtrip \ncoordination Potentially yes RdShc W by T WrExInt T . WrExT Table 1. OC TET state transitions fall into \nfour categories that require different levels of synchronization. 2. Both fast-path state checks and \nslow-path state transitions execute atomically with their subsequent access. Client analyses can hook \ninto the fast and/or slow paths without requiring extra synchronization, allowing them to per\u00adform actions \nsuch as updating analysis state or perturbing program execution. The remainder of this section describes \nOC T E T s operations. 3.1 OC T E T States A thread-locality state for an object tracked by OC T E T \ncap\u00adtures access permissions for that object: it speci.es which accesses can be made to that object without \nchanging the state. We say that such accesses are compatible with the state. Accesses compatible with \nthe state de.nitely do not create any new cross-thread dependences. The possible OC-T E T states for \nan object are: WrExT: Write exclusive for thread T. T may read or write the object without changing the \nstate. Newly allocated objects start in the WrExT state, where T is the allocating thread. RdExT: Read \nexclusive for thread T. T may read (but not write) the object without changing the state. RdSh: Read \nshared. Any thread T may read the object c without changing the state, subject to an up-to-date thread-local \ncounter: T.rdShCount = c (Section 3.4). Note the similarity of the thread-locality states to coherence \nstates in the standard MESI cache-coherence protocol [44]. Modi.ed corresponds to WrExT, Exclusive corresponds \nto RdExT, and Shared corresponds to RdSh. Invalid corre\u00ad c sponds to how threads other than T see WrExT \nor RdExT objects. While the RdSh and RdEx states are not strictly neces\u00adsary, their purpose is similar \nto E and S in the MESI protocol. Without the RdEx state, we have found that scalability de\u00adgrades signi.cantly, \ndue to more RdSh.WrEx transitions. We introduce additional intermediate states that help to implement \nthe state transition protocol (Section 3.3): WrExInt: Intermediate state for transition to WrExT. T RdExInt: \nIntermediate state for transition to RdExT. T  3.2 High-Level Overview of State Transitions OC T E T \ninserts a write barrier before every write of a poten\u00adtially shared object: if ( obj . state != WrEx) \n{ T /* Slow path: change obj . state &#38; call slow-path hooks */ } /* Call fast-path hooks */ obj \n. f = ...; // program write and a read barrier before every read: if ( obj . state != WrExT &#38;&#38; \nobj.state != RdExT &#38;&#38; c !( obj . state == RdSh&#38;&#38; T.rdShCount >= c)) { /* Slow path: change \nobj . state &#38; call slow-path hooks */ } /* Call fast-path hooks */ ... = obj. f ; // program read \nWhen a thread attempts to access an object, OC T E T checks the state of that object. If the state is \ncompatible with the access, the thread takes the fast path and proceeds without synchronization; the \noverhead is merely the cost of the state check. Otherwise, the thread takes the slow path, which initiates \na coordination protocol to change the object s state. A key concern is what happens if another thread \nchanges an object s state between a successful fast-path check and the access it guards. As Section 3.3 \nexplains in detail, OC-T E T s protocols ensure that the other thread coordinates with the thread taking \nthe fast path before changing the state, preserving atomicity of the check and access. A state change \nwithout coordination might miss a cross-thread dependence. (Imagine a cache coherence protocol that allowed \na core to upgrade a cache line from Shared to Modi.ed without waiting for other cores to invalidate the \nline in their caches!) Table 1 overviews OC T E T s behavior when a thread at\u00adtempts to perform an access \nto an object in various states. As described above, when an object is already in a state that per\u00admits \nan access, no state change is necessary, while in other situations, an object s state must be changed. \nState changes imply a potential cross-thread dependence, so OC T E T must perform some type of coordination \nto ensure that the object s state is changed safely and a happens-before edge is created to capture that \npotential dependence. OC-T E T uses different kinds of coordination to perform state changes, depending \non the type of transition needed:  Some transitions directly indicate the two or more threads involved \nin a potential dependence, as the transitions themselves imply a .ow (write read), anti (read write), \nor output (write write) dependence. These con.icting transitions require coordinating with other threads \nto es\u00adtablish the necessary happens-before relationships (Sec\u00adtion 3.3).  Other transitions do not directly \nindicate the threads in\u00advolved in the dependence. For example, transitioning from RdExT1 to RdSh due \nto a read by T2 implies a potential dependence with the previous writer to an ob\u00adject. Upgrading transitions \ncapture this dependence tran\u00adsitively by establishing a happens-before relationship be\u00adtween T1 and T2 \n(Section 3.4).  Finally, when a thread reads an object in RdSh state, it must establish happens-before \nwith the last thread to write that object, which might be any thread in the sys\u00adtem. Fence transitions \nestablish this happens-before tran\u00adsitively by ensuring that each read of a RdSh object hap\u00adpens after \nthe object s transition to RdSh (Section 3.4).  Note that while a particular state change may not directly \nim\u00adply a cross-thread dependence, the combined set of happens\u00adbefore edges created by OC T E T ensures \nthat every cross\u00adthread dependence is ordered by these happens-before rela\u00adtionships. Section 3.7 proves \nOC T E T s soundness, and Sec\u00adtions 6 and 7 describe how analyses can build on OC T E T s happens-before \nedges, by de.ning hooks called from the slow path, in order to capture all cross-thread dependences soundly. \n 3.3 Handling Con.icting Transitions When an OC T E T barrier detects a con.ict, the state of the object \nmust be changed so that the con.icting thread may ac\u00adcess it. However, the object state cannot simply \nbe changed at will. If thread T2 changes the state of an object while an\u00adother thread, T1, that has access \nto the object is between its state check and its access, then T1 and T2 may perform racy con.icting accesses \nwithout being detected even if T2 uses synchronization. At a high level, a thread called the requesting \nthread that wants to perform a con.icting state change requests access by sending a message to the thread \ncalled the re\u00adsponding thread that currently has access. The responding thread responds to the requesting \nthread when the respond\u00ading thread is at a safe point, a point in the program that is de.nitely not between \nan OC T E T barrier check and its cor\u00adresponding access. (If the responding thread is blocked at a safe \npoint, the response happens implicitly when the request\u00ading thread observes this fact atomically.) Upon \nreceiving the response, the requesting thread can change the object s state and proceed with its access. \nThis roundtrip coordination be\u00adtween threads results in a happens-before relationship being established \nbetween the responding thread s last access to the object and the requesting thread s access to the object, \ncapturing the possible cross-thread dependence. To handle RdSh.WrEx transitions, which involve mul\u00adtiple \nresponding threads, the requesting thread performs the coordination protocol with each responding thread. \nSafe points. OC T E T distinguishes between two types of safe points: non-blocking and blocking. Non-blocking \nsafe points occur during normal program execution (e.g., at loop back edges); at these safe points, the \nresponding thread checks for and responds to any requests explicitly. Block\u00ading safe points occur when \nthe responding thread is blocked (e.g., while waiting to acquire a lock, or during .le I/O). In this \nscenario, the responding thread cannot execute code, so it instead implicitly responds to any requests \nby setting a per-thread .ag that requesting threads can synchronize with. Safe point placement affects \nboth correctness and per\u00adformance. To provide atomicity of OC T E T instrumentation and program accesses, \nsafe points cannot be between a state check (or state transition) and its corresponding access. To avoid \ndeadlock, any point where a thread might block needs a safe point, and code must not be able to execute \nan un\u00adbounded number of steps without executing a safe point. In\u00adcreasing this bound but keeping it .nite \nwould hurt perfor\u00admance but not correctness. It suf.ces to place non-blocking safe points at loop back \nedges and method entries, and treat all potentially blocking operations as blocking safe points. Language \nVMs typically already place non-blocking safe points at the same points (e.g., for timely GC), and threads \nenter a special blocking state at potentially blocking opera\u00adtions (e.g., to allow GC while blocked). \nRequest queues. Conceptually, every thread maintains a request queue, which serves as a shared structure \nfor coordi\u00adnating interactions between threads. The request queue for a (responding) thread respT allows \nother (requesting) threads to signal that they desire access to objects that respT cur\u00adrently has access \nto (i.e., objects in WrExrespT, RdExrespT, or RdSh states). The queue is also used by respT to indicate \nto any requesting threads that respT is at a blocking safe point, implicitly relinquishing ownership \nof any requested objects. The request queue interface provides several methods. The .rst four are called \nby the responding thread respT: requestsSeen() Returns true if there are any pending re\u00ad quests for objects \nowned by respT. handleRequests() Handles and responds to pending re\u00ad quests. Performs memory fence behavior. \nhandleRequestsAndBlock() Handles requests as above, and atomically8 places the request queue into a blocked \nstate indicating that respT is at a blocking safe point. resumeRequests() Atomically unblocks the queue. \n8 An atomic operation includes memory fence behavior. At non-blocking At blocking safe points: safe \npoints: 3 handleRequestsAndBlock(); 1 if ( requestsSeen () ) 4 /* blocking actions */ 2 handleRequests() \n; 5 resumeRequests(); (a) Responding thread (thread respT) To move obj from WrExor RdExto WrEx: respT \nrespT reqT 6 currState = obj. state ; // WrExor RdExexpected respT respT 7 while ( currState == any intermediate \nstate || 8 ! CAS(&#38;obj.state, currState , WrExInt ) ) { reqT 9 // obj . state . WrExInt failed reqT \n10 // non-blocking safe point : 11 if ( requestsSeen () ) { handleRequests() ; } 12 currState = obj. \nstate ; // re-read state 13 } 14 handleRequestsAndBlock(); // start blocking safe point 15 response = \nrequest(getOwner(currState)) ; 16 while (! response ) { 17 response = status(getOwner(currState)) ; 18 \n} 19 resumeRequests(); // end blocking safe point 20 obj . state = WrEx; reqT 21 /* proceed with access \n*/ (b) Requesting thread (thread reqT) Figure 3. Protocol for con.icting state changes. (a) Explicit \nprotocol: (1) respT ac\u00adcesses obj; (2) reqT puts obj in WrExInt (line 8); (3) reqT issues reqT request \nto respT with CAS (line 15); (4) respT receives request (line 1). (5) respT issues a fence, estab\u00adlishing \nhb1, and responds (line 2); (6) reqT sees response and un\u00adblocks, which includes fence behav\u00adior (lines \n16 19), establishing hb2; (7) reqT places obj in WrExreqT and performs write (lines 20 21).  (b) Implicit \nprotocol: (1) respT ac\u00adcesses obj; (2) respT blocks with CAS (line 3); (3) reqT places obj in WrExInt \n(line 8) and blocks reqT (line 14); (4) reqT issues a request to respT with a CAS and observes that it \nis blocked, establishing hb2 (line 15); (5) respT unblocks with CAS, establishing hb1 (line 5); (6) reqT \nplaces obj in WrExreqT and performs write (lines 20 21). Figure 4. Operation of (a) explicit and (b) \nimplicit coordination protocols for con.icting transitions. Line numbers refer to Figure 3. The next \ntwo methods are called by the requesting thread reqT and act on the queue of the responding thread respT: \nrequest(respT) Makes a request to respT with a CAS. Returns true if reqT s queue is in the blocked state. \nstatus(respT) Returns true if reqT s request has been seen and handled by respT. Appendix C describes \na concrete implementation of these methods. Figure 3 shows the protocol for handling con.icting ac\u00adcesses.9 \nFigure 4 shows how both the explicit and implicit protocols establish happens-before relationships between \nthreads. To see how the request queue is used to coordi\u00adnate access to shared objects, consider the case \nwhere respT has access to object obj in WrExrespT or RdExrespT state, and reqT wants to write to the \nobject (reads work similarly). Requesting thread. reqT .rst atomically places obj into the desired .nal \nstate with an intermediate .ag set (line 8 in Figure 3(b)). This intermediate .ag indicates to all threads \nthat this object is in the midst of a state transition, prevent\u00ading other attempts to access the object \nor change its state until the coordination process completes. We refer to any state with the intermediate \n.ag set as an intermediate state. The use of intermediate states prevents races in the state transition \nprotocol (obviating the need for numerous tran\u00adsient states, as exist in cache coherence protocols). \nNote that reqT s CAS may fail, if a third thread simultaneously at\u00adtempts to access obj. Before reqT \nretries its CAS (lines 7 13 in Figure 3(b)), it responds to any pending requests on its re\u00adquest queue, \nto avoid deadlock (line 11 in Figure 3(b)). After placing obj into an intermediate state, reqT adds a \nrequest to respT s request queue by invoking request(respT) (line 15 in Figure 3(b)). Responding thread. \nTo respond to a request, respT uses either the explicit or implicit response protocol. When re\u00adspT is \nat a non-blocking safe point, it can safely relinquish control of objects using the explicit protocol. \nrespT checks if there are any pending requests on its request queue and calls handleRequests to deal \nwith them (lines 1 2 in Fig\u00adure 3(a)). Because handleRequests performs a fence, the ex\u00adplicit response \nprotocol establishes two happens-before re\u00adlationships (Figure 4(a)): (1) between reqT s initial request \nfor access to obj and any subsequent attempt by respT to ac\u00adcess obj (respT will then .nd obj in WrExInt \nstate) and (2) reqT between respT s response to reqT and reqT s access to obj. The implicit response \nprotocol is used when respT is at a blocking safe point. Intuitively, if reqT knows that respT is at \na blocking safe point, it can assume a response implicitly. Before blocking, respT sets its request queue \nas blocking, letting reqT see that respT is at a blocking safe point (line 3 9 Note that our pseudocode \nmixes Java code with memory fences (mem\u00adfence) and atomic operations (CAS). The JVM compiler must treat \nthese operations like other synchronization operations (e.g., lock acquire and release) by not moving \nloads and stores past these operations.  Figure 5. Example execution illustrating upgrading and fence \ntransitions. After T1 writes o, o is in the WrExT1 state. Then T2 reads o, triggering a con.icting transition \nto RdExT2. Next, T3 reads o, triggering an upgrading transition to RdShc. T4 then reads o, triggering \na fence transition, assuming it has not already read an object with state RdShc+1. T5 reads o, but the \nread does not trig\u00adger a fence transition because T5 already read an object p that T6 transitioned to \nRdShc+1. in Figure 3(a)). This protocol establishes a happens-before relationship from reqT to respT \nas in the explicit protocol. Further, it establishes a relationship between respT s enter\u00ading the blocked \nstate and reqT s access to obj (Figure 4(b)).  3.4 Handling Upgrading and Fence Transitions Accesses \nmay be non-con.icting but still require a state change. These upgrading and fence transitions do not \nneed the coordination protocol, but they do require synchro\u00adnization in order to avoid state-change races \nand establish happens-before ordering. Upgrading from RdEx. A write by T1 to a RdExT1 object triggers \nan upgrading transition to WrExT1, which requires an atomic instruction in order to avoid races with \nother threads changing the object s state. This transition does not need to establish any happens-before \nedges, since any cross\u00adthread dependences will be implied by the happens-before edges added by other \ntransitions. A read by T3 to a RdExT2 object, as in Figure 5, triggers an upgrading transition to RdSh \nstate. Note that the coordi\u00adnation protocol is not needed here because it is okay for T2 to continue \nreading the object after its state changes. In order to support fence transitions (described next), OC-T \nE T globally orders all transitions to RdSh states. To do so, it increments a global counter gRdShCount \natomically at each transition to RdSh and uses the incremented value as c in the new state RdSh. c Fence \ntransitions. When a thread reads an object o in RdShstate, there is a potential dependence with the last \nc thread to write to o. The value of c in o s state establishes that any write must have happened before \ngRdShCount was incremented to c. Each thread T has a counter T.rdShCount that indicates the last time \nthe thread synchronized with gRd-ShCount. If T.rdShCount = c, then the thread synchronized recently enough \nthat the read must be after the write. Other\u00adwise, the slow path issues a fence and updates T.rdShCount, \nestablishing the necessary happens-before. To see how this protocol works, consider the actions of threads \nT4 T6 in Figure 5. T4 reads o in the RdSh c state. To capture the write read dependence, T4 checks if \nT4.rdShCount = c. If not, T4 triggers a fence transition to ensure that it reads o after o was placed \nin RdShstate. c This transition issues a load fence to ensure a happens-before relationship with o s \ntransition to RdShby T3, and updates c T4.rdShCount . c. T5 s read of o does not trigger a fence transition \nbe\u00adcause T5 already read p in the RdShstate and c+1 > c+1 c. The write read dependence is captured transitively \nby the happens-before edge on gRdShCount from T3 to T6 and the fence-transition-based happens-before \nedge from T6 to T5. 3.5 Correctness Assumptions OC T E T introduces additional synchronization into \nprograms when cross-thread dependences arise. Hence, it is critical that this new synchronization does \nnot introduce the pos\u00adsibility of deadlock or livelock into a program that would otherwise be free of \nsuch pathologies. In other words, there must never be a scenario where all threads are stuck at OC-T \nE T barriers and are unable to continue; at least one thread must be able to complete its access. O C \nT E T s protocols make the following two assumptions: 1. The thread scheduler is fair; no thread can \nbe desched\u00aduled inde.nitely. This assumption holds true on most sys\u00adtems. 2. Attempts to add requests \nto a thread s request queue will eventually succeed. This assumption is true in practice for most concurrent \nqueue implementations, given the .rst assumption.  Note that no other assumptions are needed (e.g., \nwe need not assume that CASing objects into the intermediate state succeed). Under these conditions, \nwe can show: Theorem 1. OC T E T s protocol is deadlock and livelock free. Proof. The proof of Theorem \n1 is in Appendix A.1. Another assumption is that shared counters such as gRdSh-Count and request counters \n(Appendix C) will not over.ow. An implementation could reset counters periodically; toler\u00adate over.ow \nwith a wraparound-aware design; or use 64-bit counters, making over.ow unlikely.  3.6 Scalability Limitations \nOur current design and implementation of OC T E T have sev\u00aderal limitations related to scalability. We \ndiscuss them here and suggest potential opportunities for future work. OC T E T magni.es the cost of \nprogram con.icts since they require a roundtrip coordination protocol. OC T E T can add high overhead \nto programs with many con.icting transi\u00adtions. Future work could extend the OC T E T protocol with new \npessimistic states for highly con.icted objects, which would require synchronization on every access \nbut never re\u00adquire roundtrip coordination. Some programs partition large arrays into disjoint chunks \nthat threads access independently. The current OC T E T de\u00adsign and implementation assign each array \na single state, which would lead to many false con.icts in such programs. A future implementation could \ndivide arrays into chunks of k elements each and assign each chunk its own OC T E T state. The global \nRdSh counter (gRdShCount) may be a scala\u00adbility bottleneck since only one RdEx.RdSh transition can update \nit at once. A potential solution is to deterministically map each object to a counter in a modestly sized \narray of RdSh counters (many objects map to each counter), allow\u00ading multiple RdEx.RdSh transitions to \nproceed in parallel. RdSh.WrEx transitions require roundtrip coordination with all threads. Future work \ncan explore strategies includ\u00ading the following. (1) The RdSh counter could help iden\u00adtify threads that \nhave de.nitely not read a given RdShob\u00ad c ject (if T.rdShCount < c). This optimization would be more \nprecise with the multiple RdSh counters described above. (2) A new multi-read exclusive state could track \nmultiple, known reader threads, limiting RdSh.WrEx transitions.  3.7 Soundness of OC T E T OC T E T \nis sound: it correctly creates happens-before rela\u00adtionships between all cross-thread dependences, and \nallows actions to be taken whenever such cross-thread dependences are detected. This behavior is the \nfoundation of the various analyses that might be built on top of OC T E T (Section 6). Theorem 2. OC \nT E T creates a happens-before relationship to establish the order of every cross-thread dependence. \nProof. The proof of Theorem 2 is in Appendix A.2. At a high level, the proof proceeds by showing that \nfor every possible cross-thread dependence, the various OC T E T coordination and synchronization protocols \nensure that there is a happens-before edge between the source and the sink of that dependence. Crucially, \nmany cross-thread dependences are not directly implied by a single OC T E T state transition but must \ninstead be established transitively. For example, if T1 writes an object, then T2, T3, and T4 read that \nobject in succession, there is a cross-thread read-after-write depen\u00addence between T1 and T4 even though \nT4 .nds the object in RdSh state. The happens-before edge is established by a combination of con.icting, \nupgrading, and fence transitions. By synchronizing on all cross-thread dependences, OC-T E T provides \nsequential consistency with respect to the com\u00adpiled program, even on weak hardware memory models. 4. \nImplementation We have implemented a prototype of O C T E T in Jikes RVM 3.1.3, a high-performance Java \nvirtual machine [3] that per\u00adforms competitively with commercial JVMs.10 OC T E T is publicly available \non the Jikes RVM Research Archive.11 OC T E T s instrumentation. Jikes RVM uses two dynamic compilers \nto transform bytecode into native code. The base\u00adline compiler compiles each method the .rst time the \nmethod executes, translating bytecode directly to native code. The optimizing compiler recompiles hot \n(frequently executed) methods at increasing levels of optimization. We modify both compilers to add barriers \nat every read and write to an object .eld, array element, or static .eld. The compilers do not add barriers \nat accesses to .nal (immutable) .elds, nor to a few known immutable classes such as String and Integer. \nThe modi.ed compilers add barriers to application methods and Java library methods (e.g., java.*). A \nsigni.cant fraction of OC T E T s overhead comes from barriers in the libraries, which must be instrumented \nin order to capture cross-thread dependences that occur within them. Since Jikes RVM is written in Java, \nboth the application and VM call the libraries. We have modi.ed Jikes RVM to com\u00adpile two versions of \neach library method: one called from the application context and one called from the VM context. The \ncompilers add OC T E T barriers only to the version called from the application context. OC T E T s metadata. \nTo track OC T E T states, the imple\u00admentation adds one metadata word per (scalar or array) ob\u00adject by \nadding a word to the header. For static .elds, it in\u00adserts an extra word per .eld into the global table \nof stat\u00adics. Words are 32 bits because our implementation targets the IA-32 platform. The implementation \nrepresents WrExT, RdExT, WrExInt, and RdExInt as the address of a thread ob- T T ject T and uses the \nlowest two bits (available since objects are word aligned) to distinguish the four states. The most common \nstate, WrExT, is represented as T. The implemen\u00adtation represents RdShas simply c, using a range that \ndoes c not overlap with thread addresses: [0xc0000000, 0x.... ]. Jikes RVM already reserves a register \nthat always points to the current thread object (T), so checking whether a state is WrExT or RdExT is \nfast. To check whether an object s state is RdShand T.rdShCount is up-to-date with c, the c barrier \ncompares the state with c. The implementation actu\u00adally decrements the gRdShCount and T.rdShCount values \nso that a single comparison can check that T.rdShCount is up-to-date with respect to c (without needing \nto check that 10 http://dacapo.anu.edu.au/regression/perf/ 11 http://www.jikesrvm.org/Research+Archive \n the state is also RdSh). The implementation thus adds the following check before writes: if (o . state \n!= T) { /* slow path */ } And it adds the following check before reads: if (( o . state &#38; ~0x1) != \nT &#38;&#38; o . state <unsigned T.rdShCount) { /* slow path */ } The implementation initializes the \nOC T E T state of newly allocated objects and newly initialized static .elds to the WrExT state, where \nT is the allocating/resolving thread. (The implementation could, but does not currently, handle another \nthread seeing an uninitialized state guaranteed to be zero [36] by waiting for the state to get initialized.) \nStatic optimizations. To some extent, OC T E T obviates the need for static analyses that identify accesses \nthat cannot con.ict, because it adds little overhead at non-con.icting accesses. However, adding even \nlightweight barriers at every access adds nontrivial overhead, which we can reduce by identifying accesses \nthat do not require barriers. Some barriers are redundant because a prior barrier for the same object \nguarantees that the object will have an OC T E T state that does not need to change. We have implemented \nan intraprocedural static analysis that identi.es and removes redundant barriers at compile time. Appendix \nB describes and evaluates this analysis in more detail. Alternative implementations. Although we implement \nOC T E T inside of a JVM, alternative implementations are possible. Implementing OC T E T in a dynamic \nbytecode in\u00adstrumentation tool (e.g., RoadRunner [17]) would make it portable to any JVM, but it could \nbe hard to make certain low-level features ef.cient, such as per-object metadata and atomic operations. \nOne could implement OC T E T for native languages such as C/C++ by modifying a compiler. A na\u00adtive implementation \nwould need to (1) add safe points to the code and (2) handle per-variable metadata differently, e.g., \nby mapping each chunk of k bytes to an OC T E T metadata word via shadow memory [40]. 5. Evaluation This \nsection argues that OC T E T is a suitable platform for capturing cross-thread dependences by showing \nthat its op\u00adtimistic tradeoff is worthwhile for real programs. We evalu\u00adate this tradeoff by measuring \nOC T E T s run-time characteris\u00adtics and performance, and comparing with two alternatives: a purely pessimistic \napproach and a prior optimistic approach. 5.1 Methodology Benchmarks. The experiments execute our modi.ed \nJikes RVM on the parallel DaCapo Benchmarks [8] versions 2006-10-MR2 and 9.12-bach (2009) including a \n.xed lu\u00adsearch [55], and .xed-workload versions of SPECjbb2000 and SPECjbb2005 called pjbb2000 and pjbb2005. \n12 Table 2 12 http://users.cecs.anu.edu.au/~steveb/research/ research-infrastructure/pjbb2005 Total threads \nMax live threads eclipse6 17 18 11 12 hsqldb6 402 60 102 lusearch6 65 65 xalan6 9 9 avrora9 27 27 jython9 \n3 2 3 luindex9 2 2 lusearch9 # cores # cores pmd9 5 5 sun.ow9 # cores\u00d72 # cores xalan9 # cores # cores \npjbb2000 37 9 pjbb2005 9 9  Table 2. The total number of threads executed by each program and the maximum \nnumber that are running at any time. Some values are ranges due to run-to-run nondeterminism. shows the \nnumber of application threads that each program executes: total threads executed (Column 1) and maximum \nthreads running at any time (Column 2). DaCapo bench\u00admark names from the 2006 and 2009 versions have \nsuf.xes of 6 and 9, respectively. Some benchmarks by default spawn threads based on the number of available \ncores. A potential threat to validity is if the benchmarks, which are derived from real programs, are \nnot representative of real-world parallel programs. Recent work suggests that some DaCapo benchmarks \nperform limited communica\u00adtion [24], which may or may not make them representative of real-world program \nbehavior. Platform. Experiments execute on an AMD Opteron 6272 system with 4 16-core processors running \nLinux 2.6.32. Un\u00adless stated otherwise, experiments run on 32 cores (using the taskset command) due to \nan anomalous result with 64 cores. Measuring performance. To account for run-to-run vari\u00adability due \nto dynamic optimization guided by timer-based sampling, we execute 25 trials for each performance result \nand take the median (to reduce the impact of performance outliers due to system noise). We also show \nthe mean, as the center of 95% con.dence intervals. We build a high-performance con.guration of Jikes \nRVM (FastAdaptiveGenImmix) that optimizes the VM ahead of time and adaptively optimizes the application \nat run time; it uses the default high-performance garbage collector and chooses heap sizes adaptively. \nExecution times naturally include dynamic compilation costs. 5.2 State Transitions Table 3 shows the \nnumber of OC T E T transitions executed, including accesses that do not change the state (i.e., fast \npath only). The three groups of columns show increasing lev\u00adels of synchronization that correspond to \nthe transitions in Table 1. Alloc counts the number of objects allocated (and static .elds initialized); \nfor each such event, the allocating thread T initializes an OC T E T metadata word to WrExT. Alloc or \nsame state Alloc WrEx RdEx RdSh Upgrading or fence RdEx.WrEx RdEx.RdSh RdSh.RdSh WrEx .WrEx Con.icting \nWrEx .RdEx RdEx.WrEx RdSh.WrEx eclipse6 1.2\u00d71010 (99.9983%) 2.6% 86% 2.0% 9.4% 7.0\u00d7104 (.00056%) .000078% \n.00048% .0000038% .000043% 1.4\u00d7105 (.0012%) .00089% .000035% .00019% hsqldb6 6.5\u00d7108 (99.78%) 6.5% 88% \n.45% 4.7% 5.5\u00d7105 (.083%) .056% .020% .0082% .045% 8.9\u00d7105 (.14%) .078% .00069% .013% lusearch6 2.5\u00d7109 \n(99.99971%) 3.1% 91.0% 4.9% 1.0% 2.8\u00d7103 (.00011%) .000071% .0000040% .000037% .0000085% 4.5\u00d7103 (.00018%) \n.00017% 0% .0000027% xalan6 1.1\u00d71010 (99.77%) 2.2% 86% .13% 11% 9.9\u00d7106 (.091%) .091% .000016% .000030% \n.049% 1.5\u00d7107 (.14%) .091% .0000000037% .0000013% avrora9 6.1\u00d7109 (99.80%) 1.2% 89% 8.3% 1.4% 6.2\u00d7106 \n(.10%) .018% .014% .069% .046% 6.1\u00d7106 (.099%) .037% .0037% .013% jython9 5.4\u00d7109 (99.9999970%) 6.2% \n91.3% .0022% 2.4% 5.8\u00d7101 (.0000011%) .00000020% .00000084% .000000037% 0% 1.0\u00d7102 (.0000019%) .0000019% \n0% 0% luindex9 3.5\u00d7108 (99.99983%) 3.9% 96.0% .078% .012% 2.1\u00d7102 (.000062%) .000052% .0000093% .00000029% \n.00000058% 3.8\u00d7102 (.00011%) .00011% .00000029% .0000012% lusearch9 2.4\u00d7109 (99.99977%) 3.1% 95.8% .00046% \n1.1% 2.6\u00d7103 (.00011%) .000025% .000023% .000058% .0000033% 2.9\u00d7103 (.00012%) .00011% .0000014% .0000082% \npmd9 6.3\u00d7108 (99.988%) 11% 86% .062% 3.2% 3.6\u00d7104 (.0057%) .0040% .00083% .00093% .00025% 4.2\u00d7104 (.0067%) \n.0063% .0000018% .00017% sun.ow9 1.7\u00d71010 (99.99986%) 1.3% 40% .0060% 59% 1.8\u00d7104 (.00010%) .0000010% \n.000029% .000073% .0000067% 6.9\u00d7103 (.000040%) .000031% 0% .0000017% xalan9 1.0\u00d71010 (99.70%) 2.8% 90% \n.18% 7.1% 1.2\u00d7107 (.12%) .12% .000062% .00026% .065% 1.9\u00d7107 (.18%) .12% .000000011% .000017% pjbb2000 \n1.8\u00d7109 (99.90%) 5.1% 78% 1.6% 16% 9.1\u00d7105 (.050%) .044% .0055% .000085% .00030% 9.5\u00d7105 (.052%) .051% \n.0000027% .000082% pjbb2005 6.9\u00d7109 (98.8%) 4.2% 89% 1.9% 3.3% 3.5\u00d7107 (.51%) .21% .087% .21% .21% 5.0\u00d7107 \n(.72%) .37% .061% .085% Table 3. OC T ET state transitions, including fast-path-only barriers that do \nnot change the state. Con.icting transitions involve a temporary transition to an intermediate state \n(WrExInt or RdExInt). For each program, the .rst row is the sum of the column group, and the second row \nT T breaks down each transition type as a percentage of all transitions. We round each count to two \nsigni.cant digits, and each percentage x as much as possible such that x and 100%-x each have at least \ntwo signi.cant digits. The table shows that the vast majority of accesses do not require synchronization. \nLightweight fast-path instrumenta\u00adtion handles these transitions. Upgrading and fence transi\u00adtions occur \nroughly as often as Con.icting transitions; the con.icting transitions are of greater concern because \nthey are more expensive. Con.icting transitions range from fewer than 0.001% of all transitions for a \nfew benchmarks, to 0.10 0.72% for a few benchmarks (xalan6, avrora9, xalan9, pjbb2005). The relative \ninfrequency of con.icting transi\u00adtions provides justi.cation for OC T E T s optimistic design.  5.3 \nPerformance Figure 6 presents the run-time overhead OC T E T adds to program execution. Each bar represents \noverall execution time, normalized to unmodi.ed Jikes RVM (Base). The con.guration Octet w/o coordination \nadds OC T E T states and barriers, but does not perform the coordina\u00adtion protocol, essentially measuring \nonly OC T E T s fast-path overhead. (This con.guration still performs state transitions; otherwise fast-path \nchecks repeatedly fail, slowing execu\u00adtion.) O C T E T s fast path adds 13% overhead on average. Octet \nperforms the coorindation protocol and adds 13% overhead on average over Octet w/o coordination. Overall \nOC T E T overhead is 26% on average. Unsurprisingly, the co\u00adordination protocol adds more overhead to \nprograms with higher fractions of con.icting transitions (hsqldb6, xalan6, avrora9, xalan9, and pjbb2005). \nThe program pjbb2005 adds the most overhead and has the highest fraction of con\u00ad.icting transitions. \nInterestingly, pjbb2005 has especially low overhead for approaches that are less optimistic (more pessimistic), \neven though these approaches perform poorly overall (Sections 5.4 and 5.5), showing the potential for \nan adaptive approach that converts roundtrip coordination op\u00aderations to atomic or fence operations (Section \n3.6). De\u00adspite hsqldb6 s high fraction of con.icting transitions, its overhead is lower than other high-con.ict \nprograms because more of its con.icting transitions use the implicit protocol. Scalability. As mentioned \nearlier, these experiments run on only 32 cores. We .nd that OC T E T s overhead is lower running on \n64 cores (20% on average), primarily because of cases like xalan9, on which Octet consistently outperforms \nBase. Investigating this anomalous result, we have found that simply modifying Jikes RVM (without O C \nT E T) to in\u00adsert a barrier before every access that does useless work (5 40 empty loop iterations) causes \nxalan9 to run faster on 64 cores (with the best performance for 20 loop iterations). This result suggests \nthat the O C T E T con.gurations improve per\u00adformance simply by slowing down reads and writes, rather \nthan anything speci.cally related to OC T E T. We suspect that the Opteron 6272 machine we use is experiencing \noversat\u00aduration on the interconnect that links together its 8 NUMA nodes (with 8 cores each). Although \nwe have not been able  Figure 6. OC TET run-time performance. The ranges are 95% con.dence intervals \ncentered at the mean. (a) avrora9 (b) pmd9 (c) xalan9 (d) pjbb2005 Figure 7. Scalability of Jikes RVM \nwith and without OC T E T on 1 64 cores (the x-axis) for four representative programs. to fully understand \nthis issue, our investigations demonstrate that the effect is not speci.c to OC T E T but rather an effect \nof Jikes RVM running xalan9 on this architecture. Figure 7 shows how the execution times of unmodi.ed \nJikes RVM (Base) and OC T E T vary with different num\u00adbers of cores. Each point is the median of 10 trials, \nwith 95% con.dence intervals (mostly too short to see) centered around the mean. Although OC T E T adds \nfairly low overhead to avrora9 for 8 64 cores, it slows the program by up to 2X on 2 4 cores for the \nfollowing reason. When avrora9 which always runs 27 threads (Table 2) runs on few cores, the OS thread \nscheduler maps several threads to the same core. A descheduled thread may or may not be blocked at a \nsafe point; if not, the thread cannot respond to coordination protocol requests until it is scheduled. \nThis issue could be mitigated by implementing better cooperation between the JVM and the OS thread scheduler, \ne.g., descheduling threads only at safe points. (In contrast, hsqldb6 has more threads than cores, but \nits threads block frequently, so descheduled threads are often blocked at a safe point.) Figure 7(b) \nshows that OC T E T adds fairly consistent over\u00adhead to pmd9 for 1 64 cores. With or without OC T E T, \nJikes RVM running pmd9 on this architecture stops scaling around 8 cores (Table 2 shows pmd9 executes \nonly 5 threads). For 16 64 cores, run time increases; we .nd most of this effect is due to increasing \nGC time. We suspect that the parallel GC does not scale well on the NUMA architecture. The nine programs \nnot shown in these plots show scalability similar to pmd9: OC T E T adds fairly consistent overhead regardless \nof the number of cores, while each program scales differently depending on its thread count and inherent \nparallelism. Sev\u00aderal of these programs scale better than pmd9: lusearch6, xalan6, lusearch9, and sun.ow9. \n Figure 7(c) shows that xalan9 scales up to 16 32 cores, and O C T E T adds fairly consistent overhead. \nFor 64 cores, we observe the anomaly described above in which OC T E T actu\u00adally outperforms unmodi.ed \nJikes RVM. Figure 7(d) shows that OC T E T slows pjbb2005 by about 2X for 4 64 cores (due to con.icting \ntransitions, as described previously). For 8 64 cores, execution time increases with the number of cores, \nwhich we believe is caused by pjbb2005 s 9 threads (Table 2) being dispersed across more cores, increasing \nthe effects of remote cache misses and NUMA communication. 5.4 Comparison to Pessimistic State Model \nWe have implemented and evaluated the pessimistic state model used for Figure 2 using the same implementation \nframework and experimental setup as for OC T E T. Every memory access requires instrumentation that performs \nan atomic operation such as CAS. For simplicity of implemen\u00adtation, our barriers execute the program \ns write outside of the atomic region, an optimization that is acceptable for an\u00adalyses that capture only \nunordered con.icting accesses. We .nd that adding the barrier to all potentially shared memory accesses \nslows programs by 4.5 6.2X: the geomean is 4.5X excluding sun.ow9 or 6.2X including sun.ow9, which the \npessimistic barriers slow by 345X (\u00b14X). OC T E T not only outperforms the pessimistic model on average, \nbut is faster on all benchmarks except pjbb2005, which pessimistic barri\u00aders slow down by only 1.3X. \nPessimistic barriers slow every other program by at least 2X. We .nd that accesses to sun.ow9 s RdSh \nobjects have little same-thread locality, i.e., threads interleave reads to each RdSh object, so performing \na write to each read-shared access leads to numerous remote cache misses. In con\u00adtrast, pessimistic barriers \nperform well for pjbb2005 because nearly all of its RdSh accesses have same-thread locality, so performing \na write at each read-shared access does not add many remote cache misses.  5.5 Comparison to Alternative \nState Transition Model For comparison purposes, we have implemented a state tran\u00adsition model from prior \nwork. Von Praun and Gross describe an approach for detecting races based on tracking thread ownership \nof objects [51]. Their ownership system dynam\u00adically identi.es shared objects, allowing the race detector \nto restrict its attention to those objects. Like OC T E T, their approach uses unsynchronized checks, \nwith requests and polling for state changes. Their ownership model allows ob\u00adjects in an exclusive state \nto avoid synchronization, but ob\u00adjects in a shared modi.ed or shared read state require syn\u00adchronization \non every access, and objects that enter shared states cannot return to an exclusive state. In contrast, \nOC T E T supports transitioning back to exclusive states, and accesses require synchronization only on \nstate changes. We have implemented von Praun and Gross s state tran\u00adsition model with as many similarities \nto the OC T E T im\u00adplementation as possible. We adapt their model, which is designed for detecting races \nwith the lockset algorithm, to address the problem of tracking dependences. Our adapted model, which \navoids lockset operations and some state tran\u00adsitions, should be no more expensive than the original \nmodel. An object starts in an exclusive state for its allocating thread; an access by a second thread \ntriggers a con.icting transi\u00adtion to an exclusive state for the second thread; any subse\u00adquent cross-thread \naccess triggers a con.icting transition to a shared read or shared modi.ed state in which all accesses \nrequire an atomic operation such as a CAS. Shared read and shared modi.ed objects cannot transition back \nto exclusive states. The model thus avoids con.icting transitions but can require a CAS for repeated \naccesses to shared objects. Table 4 summarizes the transitions incurred by this model. Most accesses \nare to objects in the exclusive states, for which same-state accesses do not require synchroniza\u00adtion \n(No sync.). Few accesses trigger a con.icting transi\u00adtion (Roundtrip coord.), since the same object cannot \ntrigger more than two con.icting transitions. However, a substan\u00adtial fraction of accesses are to objects \nin shared states (CAS); each of these accesses requires an atomic operation. No sync. (excl. state) Roundtrip \ncoord. (con.. trans.) CAS (shared state) eclipse6 90.6% 0.00089% 9.4% hsqldb6 94.3% 0.022% 5.6% lusearch6 \n99.2% 0.00017% 0.98% xalan6 87% 0.00033% 13% avrora9 98.4% 0.0055% 1.6% jython9 97.6% 0.0000021% 2.4% \nluindex9 99.99982% 0.000098% 0.000062% lusearch9 99.0% 0.000095% 1.0% pmd9 97.0% 0.0023% 3.0% sun.ow9 \n42% 0.000056% 58% xalan9 90.9% 0.00049% 9.1% pjbb2000 86% 0.017% 14% pjbb2005 90.6% 0.013% 9.3%  Table \n4. State transitions for our implementation of von Praun and Gross s model. Rounding follows Table 3. \nWe .nd that this model slows programs by 2.2 3.1X (ge\u00adomean is 2.2X excluding sun.ow9 or 3.1X including \nit). For sun.ow9, the slowdown is 221X (\u00b19X) since it per\u00adforms many accesses for which the instrumentation \nrequires a CAS. Results for pessimistic barriers also show that these accesses are especially expensive \nfor sun.ow9 due to adding synchronized writes to mostly read-shared accesses. The other slowdowns vary \nsigni.cantly across programs: 17X for xalan9, 8.1X for xalan6, 1.3X for pjbb2005 (similar to the result \nfor pessimistic barriers), and 1.3 2.2X for the rest. These results suggest that, when capturing cross-thread \nde\u00adpendences, OC T E T s design, which provides greater .exibil\u00adity for handling shared access patterns, \nprovides a signi.cant advantage over von Praun and Gross s model. 6. Developing New Analyses This section \ndescribes how OC T E T can be used as a building block for new analyses. As described in preceding sections, \nOC T E T guards all potentially shared accesses with barriers that coordinate access to objects. In addition, \nOC T E T pro\u00advides hooks, which the next section describes in more de\u00adtail, that allow analyses to perform \nanalysis-speci.c behav\u00adior when the barriers perform various actions. These features allow OC T E T to \nprovide two key capabilities that analyses can leverage: Lightweight locks. Some analyses, such as software \ntrans\u00adactional memory (STM) for enforcing atomicity [20], essentially need locks on all shared objects. \nOC T E T s thread locality states allow OC T E T barriers to function effectively as lightweight, biased, \nread/write locks on ev\u00adery shared object. By implementing hooks into OC T E T s state transitions, analyses \ncan perturb the execution, e.g., by resolving con.icts in an STM or by controlling inter\u00adleavings to \nprovide deterministic execution.  Tracking dependences and detecting con.icts. Many an\u00adalyses rely on \ndetecting when a cross-thread dependence exists or when two accesses con.ict, and taking some action \nin response. Analyses can implement slow-path    Lightweight Track Precise locks dependences accesses \nRecord deps. , Deterministic exec. , Check DRF/SC , , Check atomicity , , ,* Enforce atomicity , , Table \n5. The analyses from Section 2.1 have different require\u00adments. *When enforcing atomicity, last-access \ninformation such as read/write sets need not be fully precise. hooks (hooks that execute when OC T E \nT performs a state transition) to identify all cross-thread dependences. The next section describes how \na dependence recorder can build on OC T E T to capture all cross-thread dependences. Table 5 presents \nthe .ve analyses from Section 2.1. The Lightweight locks and Track dependences columns show which analyses \nrequire these features. In general, checking and recording analyses need to track dependences, while \nenforcing analyses need lightweight locks. STM imple\u00admentations probably need both for con.ict detection \nand resolution, respectively. Our preliminary experience building on top of OC T E T to enforce and check \natomicity informs our understanding of these features, and suggests that Section 7 s hooks are generally \nuseful [7, 49, 56]. Developing precise analyses Some analyses in particular, checking analyses require \nprecision in order to avoid reporting false positives. While hooking into OC T E T s transitions allows \ntracking depen\u00addences soundly, these dependences are not precise. The rest of this section identi.es \nsources of precision that analyses re\u00adquire. A common theme is that providing precision is largely a \nthread-local activity, so analyses may be able to provide precision with reasonable overhead. Prior accesses. \nMany checking analyses need to know not only that a cross-thread dependence exists, but also when the \nsource of the dependence occurred. For example, when T2 reads o, an atomicity checker needs to know not \nonly that T1 last wrote o but also when T1 wrote o (e.g., T1 s atomic block or non-atomic access that \nwrote o) to identify the exact dependence and ultimately decide whether an atomicity vio\u00adlation exists. \nAnalyses can provide this precision by storing thread-local read and write sets [38]. For example, atom\u00adicity \nchecking would maintain read and write sets for each executed region. Inferring precise dependences from \nimprecise transitions. To be sound, an analysis must assume the existence of cross\u00adthread dependences \nimplied by OC T E T s happens-before edges. However, many of these dependences may not exist. A transition \nfrom RdSh to WrExT implies a potential depen\u00addence from all other threads to T, but this dependence may \nnot exist for every thread. In Figure 5, to capture the write read dependence from T1 to T5, an analysis \nmust capture all of the happens-before edges that imply this dependence. However, other implied dependences \ndo not actually exist, e.g., no dependence exists from T1 s write of o to the reads of p by T5 and T6. \n Field granularity. Furthermore, all happens-before edges imply potentially imprecise dependences because \nof gran\u00adularity mismatch: OC T E T tracks state at object granular\u00adity for performance reasons, but precise \nanalyses typically need .eld (and array element) granularity. While an analysis could modify OC T E T \nto track dependences at .eld granu\u00adlarity to increase precision, the other forms of imprecision would \nstill require mechanisms such as read/write sets to provide full precision. The Precise accesses column \nof Table 5 identi.es analyses that require precision. Detecting data races, SC violations, or atomicity \nviolations requires perfect precision to avoid reporting false positives. Although enforcing atomicity \nwith STM does not require full precision, since false positives affect performance but not correctness, \nthis analysis would bene.t from some precision about prior accesses, to avoid aborting transactions on \nevery OC T E T con.icting transition. A precise analysis may be able to harness OC T E T s im\u00adprecise \nhappens-before edges as a sound, ef.cient .rst\u00adpass .lter. Transitions that indicate a potential dependence \nwould require a second, precise pass using precise thread\u00adlocal accesses. If an analysis could avoid \ninvoking the sec\u00adond pass frequently, it could achieve high performance by avoiding most synchronization \nin the common case. 7. Dependence Recorder This section describes how we implement an analysis that records \nall of OC T E T s happens-before edges, which soundly imply all cross-thread dependences. We describe \nhooks that OC T E T provides for building new analyses and show how the dependence recorder implements \nthese hooks. Framework for soundly capturing dependences. In gen\u00aderal, every analysis has some notion \nof what we call dy\u00adnamic access location (DAL). A DAL could be de.ned as a dynamically executed region \nor transaction, in the case of an atomicity checker or STM. In the case of a dependence recorder, a DAL \nis a static program location plus per-thread dynamic counter, allowing an analysis to record when depen\u00addences \noccurred with enough granularity to support replay. The dependence recorder needs to capture both the \nsource and sink DAL of each happens-before edge that O C T E T es\u00adtablishes. The sink DAL is always the \ncurrent memory ac\u00adcess triggering an OC T E T state transition; .guring out the source DAL is more challenging. \nOC T E T provides a hook for each state transition in Ta\u00adble 1 that analyses can implement in order to \nperform custom behavior when happens-before edges are established:  handleCon.ictingTransitionAsRespondingThread(obj, \noldState, accessType): During the explicit protocol, it is the responding thread that invokes this hook \nsince the requesting thread is safely paused while the responding thread is active. The source DAL for \nthe happens-before relationship is whichever DAL the responding thread is currently at. In Figure 4(a), \nthe source DAL is point #4.  handleCon.ictingTransitionAsRequestingThread(obj, oldState, accessType): \nCalled during the implicit coordi\u00adnation protocol. The requesting thread must do any work to handle the \ncon.icting transition, as the responding thread is blocked. The dependence recorder uses the safe point \nat which the responding thread is blocked as the source DAL of the happens-before edge. In Figure 4(b), \nthe source DAL is point #2.  handleUpgradingTransitionToRdSh(obj, oldState, ac-cessType):13 When transitioning \nobj from RdExT1 to RdSh, T2 establishes a happens-before relationship with  c T1. Because the DAL of \nT1 s read may be hard to cap\u00adture ef.ciently, the recorder captures the DAL from T1 s most recent transition \nof any object to RdExT1, transi\u00adtively capturing the necessary happens-before. The up\u00adgrading transition \nalso establishes a happens-before edge from the last DAL to transition an object to RdSh (i.e., to RdSh). \nThe recorder captures this DAL by having c-1 every upgrading transition to RdSh assign its DAL to a global \nvariable, gLastRdSh, the most recent DAL to transition to RdSh. handleFenceTransition(obj, state, accessType): \nThe de\u00adpendence recorder uses the DAL recorded in gLastRdSh to capture the needed happens-before relationship. \n extendFastPath(obj, state, accessType): Tracking de\u00adpendences does not require performing actions at \nthe fast path, but other analyses can use this hook, e.g., to update local read/write sets.  Implementation \nand evaluation. We implement the de\u00adpendence recorder on top of OC T E T by implementing the hooks as \ndescribed above. As each thread executes, it records happens-before edges in a per-thread log .le. In \naddition to adding OC T E T s instrumentation, the compilers add instru\u00admentation at method entries and \nexits and loop back edges to update a per-thread dynamic counter (to help compute DAL), and each safe \npoint records the current static pro\u00adgram location, in case the safe point calls into the VM and responds \nto a coordination protocol request. Figure 8 shows the overhead that the recorder adds on 32 cores. OC \nT E T s overhead is the same as shown in Fig\u00adure 6. We .nd it adds 5% on average over OC T E T for an \noverall overhead of 31% (all percentages relative to baseline 13 Upgrading transitions from RdExto WrExcapture \na potential intra- T T thread dependence, so the dependence recorder, and most other analyses, do not \nneed to hook onto it.  Figure 8. Run-time overhead that the dependence recorder adds on top of OCT E \nT and overall. execution). By running subcon.gurations of the recorder (not shown), we .nd that about \nhalf of the recorder s over\u00adhead over OC T E T comes from the instrumentation added to track program \nlocation, and about half comes from actually recording OC T E T s happens-before edges. Unsurprisingly, \nthe recorder adds signi.cant overhead to pjbb2005, which has the highest fraction of accesses trigger\u00ading \nan OC T E T state change. The recorder adds the most over\u00adhead (over OC T E T) to hsqldb6, which performs \na nontrivial number of RdSh.WrEx transitions (Table 3) which in\u00advolve as many as 100 responding threads \n(Table 2). OC T E T performs these transitions inexpensively since most coordi\u00adnation uses the implicit \nprotocol (Section 5.3), but recording a RdSh.WrEx transition incurs the cost of writing to each responding \nthread s log. These results suggest that we can use OC T E T to build a low-overhead dependence-recording \nanalysis, potentially enabling online multithreaded record &#38; replay. 8. Related Work This section \ncompares OC T E T s concurrency control mech\u00adanism with prior work. While prior work employs optimistic \nsynchronization for tracking ownership of shared memory, OC T E T s design and purpose differ from prior \napproaches. Biased locking. Prior work proposes biased locking as an optimistic mechanism for performing \nlock acquires without atomic operations [11, 25, 43, 46]. Each lock is biased to\u00adward a particular thread \nthat may acquire the lock without synchronization; other threads must communicate with the biasing thread \nbefore acquiring the lock. In contrast, OC T E T applies an optimistic approach to all program accesses, \nnot just locks, and it introduces WrEx, RdEx, and RdSh states in order to support different sharing patterns \nef.ciently. Hind\u00adman and Grossman present an approach similar to biased locking for tracking reads and \nwrites in STM [22]. As with biased locking, their approach does not ef.ciently handle read-shared access \npatterns. Cache coherence. OC T E T s states correspond to cache co\u00adherence protocol states; its con.icting \nstate transitions corre\u00adspond to remote invalidations (Section 3.1). Thus, program behavior that leads \nto expensive OC T E T behavior may al\u00adready have poor performance due to remote cache misses. Cache \ncoherence has been implemented in software in distributed shared memory (DSM) systems to reduce coher\u00adence \ntraf.c [4, 26, 27, 33, 47, 48]. Shasta and Blizzard-S both tag shared memory blocks with coherence states \nthat are checked by barriers at each access [47, 48]. A coherence miss triggers a software coherence \nrequest; processors peri\u00adodically poll for such requests. While each unit of shared memory can have different \nstates in different caches, each unit of shared memory has one OC T E T state at a time. While cache \ncoherence provides data consistency, OC T E T provides concurrency control for analyses that need to \ncapture cross-thread dependences. Identifying shared memory accesses. Static approaches can identify \nde.nitely non-shared objects or non-racy ac\u00adcesses [13, 15, 52]. These approaches could complement OC \nT E T by eliminating unnecessary barriers. Chimera lowers the cost of tracking cross-thread dependences \nby using static race detection, but it relies on pro.ling for ef.ciency [31]. Aikido avoids instrumenting \naccesses to non-shared mem\u00adory by using OS paging and binary writing to limit instru\u00admentation to reads \nand writes that might access shared mem\u00adory [42]. As the remaining shared accesses still incur signif\u00adicant \noverhead, Aikido is complementary to OC T E T. Von Praun and Gross track thread ownership dynamically \nin order to detect races [51]. Section 5.5 compared our im\u00adplementation of their state model with OC \nT E T. 9. Summary OC T E T is a novel concurrency control mechanism that cap\u00adtures cross-thread dependences, \nwithout requiring synchro\u00adnization at non-con.icting accesses. We have designed a state-based protocol, \nproven soundness and liveness guaran\u00adtees, and described a framework for designing ef.cient an\u00adalyses \nand systems on top of OC T E T. An evaluation of our prototype implementation shows that real programs \nbene\u00ad.t from OC T E T s optimistic tradeoff, and OC T E T achieves overheads substantially lower than \nprior approaches. A. OC T E T Correctness Proofs   A.1 Proof of Theorem 1 Recall that we assume both \nfair thread scheduling and that all operations on queues will succeed. We begin by showing the following: \nLemma 1. A thread will always eventually respond to OC-T E T requests from other threads. Proof. A thread \nhas two means of responding to OC T E T re\u00adquests. A thread can explicitly respond to requests at safe \npoints, and it will implicitly respond to requests as de\u00adscribed in Section 3.3 if it is blocked. Hence, \nas long as non\u00adblocked threads eventually block or reach a safe point, all re\u00adquests will be responded \nto. Fair scheduling means that non\u00adblocked threads make forward progress. Hence, it suf.ces to ensure \nthat safe points are placed so that a thread cannot ex\u00adecute inde.nitely without encountering one. As \ndiscussed in Section 3.3, safe points occur at least at loop back edges and method entries, and within \nall loops of the OC T E T protocol outlined in Figure 3, ensuring that a non-blocked thread will eventually \nblock or reach a safe point. The preceding Lemma readily yields the following: Lemma 2. A thread that \nchanges the OC T E T state of an object o will eventually be able to access o. Proof. If thread T changes \no s state to any other state that does not require roundtrip coordination (i.e., T performs a RdEx . \nRdSh or RdExT . WrExT transition), then the access can proceed immediately. If T places an object in \nan intermediate state, then T cannot proceed until it receives responses from the necessary threads (a \nsingle thread in the case of a transition from WrEx or RdEx, or all threads in the case of a transition \nfrom RdSh). Lemma 1 says that all necessary responses will eventually arrive, and hence T can remove \nthe intermediate .ag and proceed with its access. We can now prove the following: Theorem 1. OC T E T \ns protocol is deadlock and livelock free. Proof. We note that showing deadlock and livelock freedom \nrequires that at least one thread make progress when encoun\u00adtering an O C T E T barrier. We can thus \nshow that a thread at an O C T E T barrier will either (a) successfully pass the bar\u00adrier and complete \nits access; or (b) retry the barrier because a second thread has completed or will complete its access. \nWe thus consider a thread T attempting to access an ob\u00adject o, and consider each possibility under which \nthe thread may attempt its access. These cases are labeled using tuples of the form (S, a), where S is \nthe state o is in when T ar\u00adrives at the O C T E T barrier, and a denotes whether T wants to perform \na read (r), a write (w), or either (r/w). (WrExT, r/w), (RdExT, r): These are the simple cases. The OC \nT E T barrier takes the fast path and T immedi\u00ad ately proceeds to its access. (Any intermediate state, \nr/w): If T .nds o in an interme\u00ad diate state, the OC T E T protocol causes T to loop. How\u00ad ever, in this \nsituation, a second thread, T , has put o into an intermediate state, and, by Lemma 2, will eventually \n complete its access. (WrExT. , r/w), (RdExT. , w), (RdSh, w): In each of these c cases, the con.icting \ntransition protocol causes T to at\u00adtempt to CAS o to the appropriate intermediate state. If the CAS fails, \nthen some other thread T put o into a different state, and, by Lemma 2, will make forward  progress. \nIf the CAS succeeds, then T makes forward progress, instead. (RdSh, r): If necessary, T can update T.rdShCount \nwith\u00ad c out blocking. T then proceeds to its access. (RdExT , r): T attempts to atomically increment \ngRdSh-Count. If the increment succeeds, T then attempts to CAS o s state to RdSh. If the CAS succeeds, \nT proceeds with c its access, and if it fails, then some other thread T per\u00adformed a state change and \nis making forward progress by Lemma 2. If the atomic increment fails, then some thread T is attempting \nthe same transition, but in the success\u00adful increment case, and thus some thread is making for\u00adward progress. \n(RdExT, w): T attempts to upgrade o s state with a CAS. If the CAS succeeds, T proceeds with its access. \nIf it fails, then some other thread changed o s state, and by Lemma 2 will complete its access. Hence, \nin all cases, either T will eventually be able to pro\u00adceed past the OC T E T barrier and perform its \naccess, or some other thread will successfully complete its access, and no deadlock or livelock is possible. \n A.2 Proof of Theorem 2 We next show that OC T E T creates happens-before relation\u00adships between all \ncross-thread dependences. Note that OC-T E T does not concern itself with non-cross-thread depen\u00addences \nas they are enforced by the compiler and hardware. We note that any cross-thread dependence only involves \na single object. Dependences that involve two objects (e.g., loading a value from a .eld of one object \nand storing it into the .eld of a second) must happen within a single thread. We also assume that OC \nT E T s instrumentation correctly ensures that an object is in a valid state before a thread performs \nits access (e.g., for T to write obj, obj must be in state WrExT). Notation. We denote a read by thread \nT as rT, and a write by T as wT. A dependence between two accesses is denoted with .. Hence, .ow (true) \ndependences are written w . r, anti-dependences, r . w, and output dependences, w . w. A cross-thread \ndependence is a dependence whose source access is on one thread and whose dependent access is on another. \nWe will denote the object over which a dependence is carried as obj. We will also use special notation \nfor certain actions per\u00adformed by threads when interacting with OC T E T. S .T means that thread T put \nan object into OC T E T state S. recvT means T received a request on its request queue; respT means T \nre\u00adsponded; blockT means T has blocked its request queue; and unblockT means T unblocked its request \nqueue. Theorem 2. OC T E T creates a happens-before relationship to establish the order of every cross-thread \ndependence. Proof. We need only concern ourselves with cross-thread dependences that are not transitively \nimplied by other depen\u00addences (cross-thread or otherwise). We thus break the proof into several cases: \n wT1 . wT2: OC T E T s barriers enforce that when T1 writes obj, obj must be in WrExT1 state. When T2 \nattempts to perform its write, it will still .nd obj in WrExT1 (because the dependence is not transitively \nimplied, no other con\u00ad.icting access to obj could have happened in the interim). T2 will put obj into \nWrExInt and make a request to T1. T2 In the case of the explicit response protocol, when T1 re\u00adceives \nthe request, it establishes WrExInt .T2.hb resp T2 T1 (transitively implied by edge hb1 in Figure 4(a)) \nand en\u00adsures that T1 will now see obj in state WrExInt (prevent- T2 ing future reads and writes by T1 \nto obj). When T2 sees the update of T1 s response, it issues a fence, moves obj to state WrExT2, and \nproceeds with its write, establish\u00ading recvT1 .hb wT2 (transitively implied by edge hb2 in Figure 4(a)) \nand hence wT1 .hb wT2. In the implicit response protocol, T2 moves obj to WrExT2 only after observing \nthat T1 is blocked. We thus have WrExInt unblockT1 (transitively implied by T2 .T2.hb edge hb1 in Figure \n4(b)), ensuring that subsequent ac\u00adcesses by T1 happen after obj is moved to WrExInt, and T2 blockT1 \n.hb wT2 (transitively implied by edge hb2 in Figure 4(b)). Since wT1 .hb blockT1, wT1 .hb wT2 holds transitively. \n rT1 . wT2: This dependence has two cases to handle. Case 1: T2 .nds the object in an exclusive state \n(either RdExT1 or WrExT1). rT1 .hb wT2 is established by the same roundtrip mechanism as in the prior \nscenario. Case 2: T2 .nds the object in RdSh state. In this case, the protocol for dealing with RdSh \nobjects requires that T2 perform roundtrip coordination with all threads, es\u00adtablishing rT1 .hb wT2. \nwT1 . rT2: For thread T1 to write to obj, the object must be in WrExT1 state. There are then three scenarios \nby which this dependence could occur. Case 1: T2 is the .rst thread to read obj after the write by T1, \nso it will .nd obj in WrExT1 state. This triggers roundtrip coordination and establishes wT1 .hb rT2. \nCase 2: T2 is the second thread to read obj after the write by T1. This means that there was some thread \nT3 that left the object in state RdExT3. By the previous case, we know wT1 .hb RdExT3 .T3, with a fence \nbetween respT1 (or blockT1 in the case of the implicit protocol) and RdExT3 .T3. Hence, when T2 uses \na CAS to move the object to state RdSh, it establishes respT1 .hb RdSh .T2 (or blockT1 .hb RdSh .T2 in \nthe case of the implicit protocol), enforcing wT1 .hb rT2 transitively. Case 3: T2 .nds obj in RdShstate \nupon reading it. Note c that by the previous case, there must be some thread T3 that placed obj in RdShc \n(establishing wT1 .hb RdSh.T3). To access obj in RdShstate, T2 checks c c T2.rdShCount = c and, if the \ncheck fails, updates T2.rdShCount with a fence to ensure that T2 last saw the value of gRdShCount no \nearlier than when T3 put obj in RdSh. Hence, we have RdSh.T3.hb rT2, estab\u00ad  c c lishing wT1 .hb rT2 \ntransitively. Thus, OC T E T establishes a happens-before relationship be\u00adtween the accesses of every \ncross-thread dependence. B. Eliminating Redundant Barriers Not all OC T E T barriers are necessary. A \nparticular barrier may be redundant because a prior barrier for the same ob\u00adject guarantees that the \nobject will have an OC T E T state that does not need to change. The key insight in eliminating re\u00addundant \nbarriers is that a thread can only lose access to an object when it reaches a safe point. Thus, an access \ndoes not need a barrier if it is always preceded by an access that guar\u00adantees it will have the right \nstate, without any intervening operations that might allow the state to change. The follow\u00ading sections \ndescribe a redundant barrier analysis (RBA) and evaluate its effects on OC T E T performance. A barrier \nat an access A to object o is redundant if the following two conditions are satis.ed along every control\u00ad.ow \npath to the access: The path contains a prior access P to o that is at least as strong as A. Writes \nare stronger than reads, but reads are weaker than writes, so A s barrier is not redundant if A is a \nwrite and P is a read.  The path does not execute a safe point between A and any last prior access P \nthat is at least as strong as A.  We have designed a sound, .ow-sensitive, intraprocedu\u00adral data-.ow \nanalysis that propagates facts about accesses to all objects and statics, and merges facts conservatively \nat control-.ow merges. The analysis is conservative about aliasing of object references, assuming they \ndo not alias ex\u00adcept when they de.nitely do. A potential safe point kills all facts, except for facts \nabout newly allocated objects that have de.nitely not escaped. Handling slow paths. Responding threads \nrespond to re\u00adquests explicitly or implicitly at safe points, allowing other threads to perform con.icting \nstate changes on any object. Potential safe points include every object access because a thread may lose \naccess to any object except the accessed object if it takes a slow path, which is a safe point. Thus, \nat an access to o, the safe form of our analysis kills data-.ow facts for all objects except o. We have \nalso explored an unsafe variant of our analy\u00adsis that does not kill data-.ow facts at object accesses. \nThis variant is interesting because some analyses built on OC-T E T could use it. Speculatively executed \nregions (e.g., using transactional memory [20]) can use the unsafe variant be\u00adcause losing any object \ntriggers rollback. No RBA Safe RBA (default) Unsafe RBA eclipse6 8.6\u00d7109 6.2\u00d7109 5.4\u00d7109 hsqldb6 3.7\u00d7108 \n3.3\u00d7108 3.2\u00d7108 lusearch6 1.6\u00d7109 1.2\u00d7109 1.1\u00d7109 xalan6 6.8\u00d7109 5.4\u00d7109 5.0\u00d7109 avrora9 4.0\u00d7109 3.1\u00d7109 \n2.5\u00d7109 jython9 3.5\u00d7109 2.7\u00d7109 2.5\u00d7109 luindex9 2.2\u00d7108 1.7\u00d7108 1.6\u00d7108 lusearch9 1.6\u00d7109 1.2\u00d7109 1.1\u00d7109 \npmd9 4.0\u00d7108 3.1\u00d7108 2.8\u00d7108 sun.ow9 1.2\u00d71010 8.7\u00d7109 5.3\u00d7109 xalan9 6.3\u00d7109 5.1\u00d7109 4.7\u00d7109 pjbb2000 \n1.1\u00d7109 9.2\u00d7108 8.4\u00d7108 pjbb2005 4.7\u00d7109 3.5\u00d7109 3.3\u00d7109  Table 6. Dynamic barriers executed under three \nRBA con.gura\u00adtions: no analysis, safe analysis, and unsafe analysis. Example. The following example code \nillustrates how bar\u00adriers can be redundant: o . f = ... /* ... no loads or stores ; no safe points ; \nno defs of o ... */ // barrier not required ... = o.f ; // read barrier on p may execute slow path ... \n= p.g; // barrier required by safe variant (not by unsafe variant ) ... = o.f ; The .rst read barrier \nfor o is unnecessary because o s state will de.nitely be WrExT. The second read barrier for o is necessary \nfor the safe variant because the barrier for p may execute the slow path, which is a safe point. Performance \nimpact. All of the O C T E T results presented in Section 5 eliminate redundant barriers based on the \nsafe variant of RBA. This section evaluates the bene.t of the analysis by comparing to con.gurations \nwithout RBA and with the unsafe variant of RBA. Table 6 shows the number of dynamic barriers executed \nwithout and with RBA. No RBA is barriers inserted without RBA; Safe RBA (default) and Unsafe RBA are \nthe barriers inserted after using RBA s safe and unsafe variants. We see that the safe variant of RBA \nis effective in reducing the number of OC T E T barriers executed, and the unsafe variant usually eliminates \na modest amount of additional barriers. Evaluating performance with and without RBA, we .nd that safe \nRBA (the default) improves performance by 1% on average (relative to baseline execution time) compared \nwith no RBA. Unsafe RBA improves performance on average by 1% over safe RBA. Unsafe RBA improves the \nperformance of sun.ow9 by 7% on average over safe RBA; sun.ow9 is also the program for which unsafe RBA \nremoves the largest fraction of barriers over safe RBA (Table 6). While we focus here on objects that \nwere previously ac\u00adcessed, other analyses could potentially identify objects that de.nitely cannot be \nshared and thus do not need barriers. Ul\u00adtimately, these optimizations may be bene.cial, but, as with \nRBA, we expect them to have a muted effect on overall over\u00adhead, as non-shared objects will always take \nfast paths at OC T E T barriers, and OC T E T s fast-path overheads are low. C. Implementing the Coordination \nProtocol We have implemented the abstract protocol from Section 3.3 as follows. For its request queue, \neach thread maintains a linked list of requesting threads represented with a single word called req. \nThis word combines three values using bit.elds so that a single CAS can update them atomically: counter: \nThe number of requests made to this thread. head: The head of a linked list of requesting threads. isBlocked: \nWhether this thread is at a blocking safe point. The linked list is connected via next pointers in the \nrequest\u00ading threads. Because a requesting thread may be on multiple queues (if it is requesting access \nto a RdSh object), it has an array of next pointers: one for each responding thread. Each thread also \nmaintains a counter resp that is the number of requests to which it has responded. A responding thread \nresponds to requests by increasing its resp counter; in this way, it can respond to multiple requests \nsimultaneously. Figure 9 shows the concrete implementation of the ab\u00adstract request queue using the req \nword and resp counter. Each request increments req.counter; it adds the request\u00ading thread to the linked \nlist if using the explicit protocol. Responding threads process each requesting thread in the linked \nlist in an analysis-speci.c way (represented with the call to processList) in reverse order if FIFO behav\u00adior \nis desired and they respond by updating resp, which requesting threads observe.14 The linked list allows \nrespond\u00ading threads to know which requesting thread(s) are making requests, which allows the responding \nthread to perform con.ict detection based on a requesting thread s access, for example. On the other \nhand, analyses that only need to es\u00adtablish a happens-before relationship can elide all of the linked \nlist behavior and use only the counters; in this case, the CAS loop in handleRequestsHelper must be replaced \nwith a memory fence. Acknowledgments We thank Luis Ceze, Brian Demsky, Dan Grossman, Kathryn McKinley, \nMadan Musuvathi, and Michael Scott for valu\u00adable discussions and feedback; and the anonymous reviewers \nfor helpful and detailed comments and suggestions. References [1] S. V. Adve and H.-J. Boehm. Memory \nModels: A Case for Rethinking Parallel Languages and Hardware. CACM, 53:90 101, 2010. [2] S. V. Adve \nand M. D. Hill. Weak Ordering A New De.nition. In ISCA, pages 2 14, 1990. 14 If an analysis processes \nthe requesting threads, the responding thread may need to issue a fence before updating resp, to ensure \nproper ordering. requestsSeen() { return this . req . counter > this. resp ; }handleRequests() {handleRequestsHelper( \nfalse ) ; }handleRequestsAndBlock () {handleRequestsHelper(true) ; } handleRequestsHelper( isBlocked \n) { do { newReq = oldReq = this.req; newReq.isBlocked = isBlocked; newReq.head = null; } while (! CAS(&#38;this.req, \noldReq, newReq)); processList (oldReq.head); this . resp = oldReq.counter;  } resumeRequests() { do \n{ newReq = oldReq = this.req; newReq.isBlocked = false;  } while (! CAS(&#38;this.req, oldReq, newReq)); \n} (a) Methods called by responding thread respT, i.e., this is respT. request(respT) {do { newReq = \noldReq = respT.req; if (! oldReq. isBlocked ) { this . next [ respT ] = oldReq.head; newReq.head = this; \n } newReq.counter = oldReq.counter + 1; if (CAS(&#38;respT.req, oldReq, newReq)) return oldReq. isBlocked \n;  } while (true) ; }status(respT) { return respT . responses >= newReq.counter; } (b) Methods called \nby requesting thread reqT, i.e., this is reqT. Note that status() uses the value of newReq from request(). \nFigure 9. Concrete implementation of request queues. [3] B. Alpern, S. Augart, S. M. Blackburn, M. Butrico, \nA. Cocchi, P. Cheng, J. Dolby, S. Fink, D. Grove, M. Hind, K. S. McKinley, M. Mergen, J. E. B. Moss, \nT. Ngo, and V. Sarkar. The Jikes Research Virtual Machine Project: Building an Open-Source Research Commu\u00adnity. \nIBM Systems Journal, 44:399 417, 2005. [4] H. E. Bal, M. F. Kaashoek, and A. S. Tanenbaum. Orca: A Language \nFor Parallel Programming of Distributed Systems. IEEE TSE, 18:190 205, 1992. [5] L. Baugh, N. Neelakantam, \nand C. Zilles. Using Hardware Mem\u00adory Protection to Build a High-Performance, Strongly-Atomic Hybrid \nTransactional Memory. In ISCA, pages 115 126, 2008. [6] T. Bergan, O. Anderson, J. Devietti, L. Ceze, \nand D. Grossman. Core-Det: A Compiler and Runtime System for Deterministic Multithreaded Execution. In \nASPLOS, pages 53 64, 2010. [7] S. Biswas, J. Huang, A. Sengupta, and M. D. Bond. DoubleChecker: Ef.cient \nSound and Precise Atomicity Checking. Unpublished, 2013. [8] S. M. Blackburn, R. Garner, C. Hoffman, \nA. M. Khan, K. S. McKin\u00adley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, \nA. Hosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, D. von Dincklage, \nand B. Wiedermann. The DaCapo Benchmarks: Java Benchmarking Development and An\u00adalysis. In OOPSLA, pages \n169 190, 2006.  [9] R. L. Bocchino, Jr., V. S. Adve, S. V. Adve, and M. Snir. Parallel Programming Must \nBe Deterministic by Default. In HotPar, pages 4 9, 2009. [10] C. Boyapati, R. Lee, and M. Rinard. Ownership \nTypes for Safe Programming: Preventing Data Races and Deadlocks. In OOPSLA, pages 211 230, 2002. [11] \nM. Burrows. How to Implement Unnecessary Mutexes. In Com\u00adputer Systems Theory, Technology, and Applications, \npages 51 57. Springer Verlag, 2004. [12] L. Ceze, J. Devietti, B. Lucia, and S. Qadeer. A Case for System \nSupport for Concurrency Exceptions. In HotPar, 2009. [13] J.-D. Choi, K. Lee, A. Loginov, R. O Callahan, \nV. Sarkar, and M. Srid\u00adharan. Ef.cient and Precise Datarace Detection for Multithreaded Object-Oriented \nPrograms. In PLDI, pages 258 269, 2002. [14] J. Devietti, B. Lucia, L. Ceze, and M. Oskin. DMP: Deterministic \nShared Memory Multiprocessing. In ASPLOS, pages 85 96, 2009. [15] T. Elmas, S. Qadeer, and S. Tasiran. \nGoldilocks: A Race and Transaction-Aware Java Runtime. In PLDI, pages 245 255, 2007. [16] C. Flanagan \nand S. N. Freund. FastTrack: Ef.cient and Precise Dy\u00adnamic Race Detection. In PLDI, pages 121 133, 2009. \n[17] C. Flanagan and S. N. Freund. The RoadRunner Dynamic Analysis Framework for Concurrent Programs. \nIn ACM Workshop on Program Analysis for Software Tools and Engineering, pages 1 8, 2010. [18] C. Flanagan, \nS. N. Freund, and J. Yi. Velodrome: A Sound and Complete Dynamic Atomicity Checker for Multithreaded \nPrograms. In PLDI, pages 293 303, 2008. [19] K. Gharachorloo and P. B. Gibbons. Detecting Violations \nof Sequen\u00adtial Consistency. In SPAA, pages 316 326, 1991. [20] T. Harris, J. Larus, and R. Rajwar. Transactional \nMemory. Morgan and Claypool Publishers, 2nd edition, 2010. [21] M. Herlihy and J. E. B. Moss. Transactional \nMemory: Architectural Support for Lock-Free Data Structures. In ISCA, pages 289 300, 1993. [22] B. Hindman \nand D. Grossman. Atomicity via Source-to-Source Trans\u00adlation. In MSPC, pages 82 91, 2006. [23] D. R. \nHower, P. Montesinos, L. Ceze, M. D. Hill, and J. Torrellas. Two Hardware-Based Approaches for Deterministic \nMultiprocessor Replay. CACM, 52:93 100, 2009. [24] T. Kalibera, M. Mole, R. Jones, and J. Vitek. A Black-box \nApproach to Understanding Concurrency in DaCapo. In OOPSLA, pages 335 354, 2012. [25] K. Kawachiya, A. \nKoseki, and T. Onodera. Lock Reservation: Java Locks Can Mostly Do Without Atomic Operations. In OOPSLA, \npages 130 141, 2002. [26] P. Keleher, A. L. Cox, S. Dwarkadas, and W. Zwaenepoel. Tread-Marks: Distributed \nShared Memory on Standard Workstations and Op\u00aderating Systems. In USENIX, pages 115 132, 1994. [27] L. \nI. Kontothanassis and M. L. Scott. Software Cache Coherence for Large Scale Multiprocessors. In HPCA, \npages 286 295, 1995. [28] L. Lamport. Time, Clocks, and the Ordering of Events in a Distributed System. \nCACM, 21(7):558 565, 1978. [29] L. Lamport. How to Make a Multiprocessor Computer That Correctly Executes \nMultiprocess Programs. IEEE Computer, 28:690 691, 1979. [30] T. J. LeBlanc and J. M. Mellor-Crummey. \nDebugging Parallel Pro\u00adgrams with Instant Replay. IEEE TOC, 36:471 482, 1987. [31] D. Lee, P. M. Chen, \nJ. Flinn, and S. Narayanasamy. Chimera: Hybrid Program Analysis for Determinism. In PLDI, pages 463 474, \n2012. [32] D. Lee, B. Wester, K. Veeraraghavan, S. Narayanasamy, P. M. Chen, and J. Flinn. Respec: Ef.cient \nOnline Multiprocessor Replay via Speculation and External Determinism. In ASPLOS, pages 77 90, 2010. \n[33] D. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. \nS. Lam. The Stanford Dash Multiprocessor. IEEE Computer, 25:63 79, 1992. [34] T. Liu, C. Curtsinger, \nand E. D. Berger. Dthreads: Ef.cient Determin\u00adistic Multithreading. In SOSP, pages 327 336, 2011. [35] \nB. Lucia, L. Ceze, K. Strauss, S. Qadeer, and H.-J. Boehm. Con.ict Exceptions: Simplifying Concurrent \nLanguage Semantics with Precise Hardware Exceptions for Data-Races. In ISCA, pages 210 221, 2010. [36] \nJ. Manson, W. Pugh, and S. V. Adve. The Java Memory Model. In POPL, pages 378 391, 2005. [37] D. Marino, \nA. Singh, T. Millstein, M. Musuvathi, and S. Narayanasamy. DRFx: A Simple and Ef.cient Memory Model for \nConcurrent Programming Languages. In PLDI, pages 351 362, 2010. [38] K. E. Moore, J. Bobba, M. J. Moravan, \nM. D. Hill, and D. A. Wood. LogTM: Log-based Transactional Memory. In HPCA, pages 254 265, 2006. [39] \nM. Naik and A. Aiken. Conditional Must Not Aliasing for Static Race Detection. In POPL, pages 327 338, \n2007. [40] N. Nethercote and J. Seward. How to Shadow Every Byte of Memory Used by a Program. In ACM/USENIX \nInternational Conference on Virtual Execution Environments, pages 65 74, 2007. [41] M. Olszewski, J. \nAnsel, and S. Amarasinghe. Kendo: Ef.cient De\u00adterministic Multithreading in Software. In ASPLOS, pages \n97 108, 2009. [42] M. Olszewski, Q. Zhao, D. Koh, J. Ansel, and S. Amarasinghe. Aikido: Accelerating \nShared Data Dynamic Analyses. In ASPLOS, pages 173 184, 2012. [43] T. Onodera, K. Kawachiya, and A. Koseki. \nLock Reservation for Java Reconsidered. In ECOOP, pages 559 583, 2004. [44] M. S. Papamarcos and J. H. \nPatel. A Low-Overhead Coherence Solution for Multiprocessors with Private Cache Memories. In ISCA, pages \n348 354, 1984. [45] S. Park, Y. Zhou, W. Xiong, Z. Yin, R. Kaushik, K. H. Lee, and S. Lu. PRES: Probabilistic \nReplay with Execution Sketching on Multiprocessors. In SOSP, pages 177 192, 2009. [46] K. Russell and \nD. Detlefs. Eliminating Synchronization-Related Atomic Operations with Biased Locking and Bulk Rebiasing. \nIn OOP-SLA, pages 263 272, 2006. [47] D. J. Scales, K. Gharachorloo, and C. A. Thekkath. Shasta: A Low \nOverhead, Software-Only Approach for Supporting Fine-Grain Shared Memory. In ASPLOS, pages 174 185, 1996. \n[48] I. Schoinas, B. Falsa., A. R. Lebeck, S. K. Reinhardt, J. R. Larus, and D. A. Wood. Fine-Grain Access \nControl for Distributed Shared Memory. In ASPLOS, pages 297 306, 1994. [49] A. Sengupta, S. Biswas, M. \nD. Bond, and M. Kulkarni. EnforSCer: Hybrid Static Dynamic Analysis for End-to-End Sequential Consis\u00adtency \nin Software. Technical Report OSU-CISRC-11/12-TR18, Com\u00adputer Science &#38; Engineering, Ohio State University, \n2012. [50] K. Veeraraghavan, D. Lee, B. Wester, J. Ouyang, P. M. Chen, J. Flinn, and S. Narayanasamy. \nDoublePlay: Parallelizing Sequential Logging and Replay. In ASPLOS, pages 15 26, 2011. [51] C. von Praun \nand T. R. Gross. Object Race Detection. In OOPSLA, pages 70 82, 2001. [52] C. von Praun and T. R. Gross. \nStatic Con.ict Analysis for Multi-Threaded Object-Oriented Programs. In PLDI, pages 115 128, 2003. [53] \nL. Wang and S. D. Stoller. Runtime Analysis of Atomicity for Multi\u00adthreaded Programs. IEEE TSE, 32:93 \n110, 2006. [54] X. Yang, S. M. Blackburn, D. Frampton, and A. L. Hosking. Barriers Reconsidered, Friendlier \nStill! In ISMM, pages 37 48, 2012. [55] X. Yang, S. M. Blackburn, D. Frampton, J. B. Sartor, and K. S. \nMcKin\u00adley. Why Nothing Matters: The Impact of Zeroing. In OOPSLA, pages 307 324, 2011. [56] M. Zhang, \nJ. Huang, and M. D. Bond. LarkTM: Ef.cient, Strongly Atomic Software Transactional Memory. Technical \nReport OSU\u00adCISRC-11/12-TR17, Computer Science &#38; Engineering, Ohio State University, 2012.  \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Parallel programming is essential for reaping the benefits of parallel hardware, but it is notoriously difficult to develop and debug reliable, scalable software systems. One key challenge is that modern languages and systems provide poor support for ensuring concurrency correctness properties - atomicity, sequential consistency, and multithreaded determinism - because all existing approaches are impractical. Dynamic, software-based approaches slow programs by up to an order of magnitude because capturing and controlling cross-thread dependences (i.e., conflicting accesses to shared memory) requires synchronization at virtually every access to potentially shared memory.</p> <p>This paper introduces a new software-based concurrency control mechanism called OCTET that soundly captures cross-thread dependences and can be used to build dynamic analyses for concurrency correctness. OCTET achieves low overheads by tracking the locality state of each potentially shared object. Non-conflicting accesses conform to the locality state and require no synchronization; only conflicting accesses require a state change and heavyweight synchronization. This optimistic tradeoff leads to significant efficiency gains in capturing cross-thread dependences: a prototype implementation of OCTET in a high-performance Java virtual machine slows real-world concurrent programs by only 26% on average. A dependence recorder, suitable for record &#38; replay, built on top of OCTET adds an additional 5% overhead on average. These results suggest that OCTET can provide a foundation for developing low-overhead analyses that check and enforce concurrency correctness.</p>", "authors": [{"name": "Michael D. Bond", "author_profile_id": "81100148693", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4290442", "email_address": "mikebond@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Milind Kulkarni", "author_profile_id": "81331496893", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P4290443", "email_address": "milind@purdue.edu", "orcid_id": ""}, {"name": "Man Cao", "author_profile_id": "83358644157", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4290444", "email_address": "caoma@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Minjia Zhang", "author_profile_id": "83358727157", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4290445", "email_address": "zhanminj@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Meisam Fathi Salmi", "author_profile_id": "83358639257", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4290446", "email_address": "fathi@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Swarnendu Biswas", "author_profile_id": "83358747257", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4290447", "email_address": "biswass@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Aritra Sengupta", "author_profile_id": "83358944057", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4290448", "email_address": "sengupta@cse.ohio-state.edu", "orcid_id": ""}, {"name": "Jipeng Huang", "author_profile_id": "83358714957", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P4290449", "email_address": "huangjip@cse.ohio-state.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509519", "year": "2013", "article_id": "2509519", "conference": "OOPSLA", "title": "OCTET: capturing and controlling cross-thread dependences efficiently", "url": "http://dl.acm.org/citation.cfm?id=2509519"}