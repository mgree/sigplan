{"article_publication_date": "10-29-2013", "fulltext": "\n Do Developers Bene.t from Generic Types? An Empirical Comparison of Generic and Raw Types in Java Michael \nHoppe, Stefan Hanenberg University of Duisburg-Essen, Institute for Computer Science and Business Information \nSystems, Essen, Germany michael.hoppe&#38;#169;stud.uni-due.de stefan.hanenb erg&#38;#169;icb.uni-due.de \n Abstract Type systems that permit developers to express themselves more precisely are one of the primary \ntopics in programming language research, as well as in industrial software develop\u00adment. While it seems \nplausible that an expressive static type system increases developer productivity, there is little empir\u00adical \nevidence for or against this hypothesis. Generic types in Java are an example: as an extension of Java \ns original type system, some claim that Java 1.5 improves the type system s expressiveness. Even if this \nclaim is true, there exists lit\u00adtle empirical evidence that claimed expressiveness leads to a measurable \nincrease in developer productivity. This paper in\u00adtroduces an experiment where generic types (in comparison \nto raw types) have been evaluated in three different direc\u00adtions: (1) the documentation impact on undocumented \nAPIs, (2) the time required for .xing type errors, and (3) the ex\u00adtensibility of a generic type hierarchy. \nThe results of the ex\u00adperiment suggest that generic types improve documentation and reduce extensibility \n without revealing a difference in the time required for .xing type errors. Categories and Subject Descriptors \nD.3.3 [Programming Languages]: Language Constructs and Features Keywords programming languages, type \nsystems, generic types, empirical research 1. Introduction Static type systems (see e.g., [3, 26]) are \none of the primary research topics in programming language design. Such sys\u00adtems identify whether computer \ncode contains type errors at Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nCopyrights for components of this work owned by others than ACM must be honored. Abstracting with credit \nis permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October \n29 31, 2013, Indianapolis, Indiana, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. . . \n$15.00. http://dx.doi.org/10.1145/2509136.2509528 compile time, potentially increasing programmer productiv\u00adity \nby .nding errors earlier. While many programming languages used in industry include a static type system \n(e.g., Java, C++, Haskell, or Scala), others especially those used in web technologies have a dynamic \ntype system (e.g., Ruby, PHP, JavaScript, or Smalltalk). While static type systems have existed for some \ntime, the research community contiues to investigate them, for example in developing static type systems \nfor languages that lack one (e.g., developing a static type system for JavaScript with TypeScript1). \nSimilarly, there is interest in the evolution of static type systems Java is an example for this. Java, \nwhich has extraordinary industrial relevance, is an example of a language where the type system has evolved: \ngeneric types [24] were introduced in version 1.5 in 2004, about eight years after the initial release \nof Java. With the aid of generic types, Java then supported parametric polymor\u00adphism (see for example \n[5]) and F-bounded polymorphism (see for example [4]). This new language feature had a di\u00ad rect in.uence \non the design of standard libraries such as the Collection API, where the applied parametric polymorphism \npermits developers to reason more precisely. However, generic types in Java were also designed to be \nbackwards compatible, where generics were not avail\u00adable (see [2, 24]): generic types can be used as \nraw types, where the type parameters are omitted and where Java s type system handles them as ordinary \nnominal types. For example, an object x of type Collection<String> can be declared as type Collection \nand all methods declared in Collection can be used on x. However, when an object is retrieved from the \ncollection it is of type Dbject the usage of raw typed collections implies that the additional in\u00adformation \nabout the kind of contained objects is lost. Conse\u00adquently, the use of raw types (and especially raw \ntyped col\u00adlections) typically leads to an increase in type casts. Poten\u00adtially, it may also lead to collections \nbeing used incorrectly. 1 http://www.typescriptlang.org/  Although the introduction of generic types \nin Java was a large change (at least to its type system), and although signif\u00adicant effort was required, \nto provide corresponding support for generic types in IDEs, the actual evidence, from observa\u00adtions of \nprogrammer behavior, remains scant. For example, given the signi.cant effort it took to implement the \ngeneric type system in Java, would observations of actual behavior using the systems under reasonable \nexperimental conditions con.rm or refute that such effort was bene.cial? The liter\u00adature has little reliable \ndata on the bene.t of using generic types in comparison to raw types (which nowadays co-exist in Java \nsoftware development projects), making this question dif.cult to answer in a scienti.cally replicable \nway. This paper introduces an experiment which is part of a larger experiment series (see [12, 13, 20, \n21, 32, 33] as described in more detail in section 2). This series focuses on the impact of type systems \non software development. Previous experiments focused on the difference of static and dynamic type systems \nand, to some extent, already provide evidence that the assumed bene.t of static type systems is at least \nnot easy to replicate, it is even possible that under some circumstancs the opposite can be measured \n[12, 13],  the impact of type casts, which supporters of dynamic languages often consider as major argument \nagainst static type systems, may not be as big of an issue as the detrac\u00adtors of static type systems \nclaim [33],  static type systems reduce the time required for .xing type errors [20, 31], and that \n static type systems improve the usability of undocu\u00admented APIs [20, 21].  Because of these previous \nstudies, it seems obvious to test whether these statements hold for generic in comparison to raw types \nas well. The here introduced experiment studies three facets of generic types in comparison to raw types \nin Java: 1. whether generic types that are used in undocumented APIs as parameter types or return types \nimprove the usability of such APIs, 2. whether generic types help (in comparison to raw types) to .x \ntype errors more quickly, and 3. whether generic types imply a change in the effort re\u00adquired to extend \npieces of software.  Two of these facets (1 and 2) were derived from previous experiments from which \nit is already known that static type systems are bene.cial. The result of the experiment is that generic \ntypes have a positive impact on the usage of an un\u00addocumented API, in comparison to raw types. However, \nno difference between generic types and raw types could be measured with respect to the time required \nto .x type errors. With respect to extensibility, the experiment uses a generic type implementation of \nthe strategy design pattern imple\u00admentation (as proposed in [24], see [9] for a desciption of the pattern). \nResults in this case show that generic types have a negative impact on users, increasing the effort required \nto extend a type hierarchy. We now discuss the structure of this paper. First, section 2 reviews related \nwork, focusing on experiments with static type systems. Section 3 describes the research questions and \nexperimental design. Section 4 analyzes the data. After discussing the experiment s results in section \n5, section 6 summarizes and concludes. 2. Related Work There is a long history of works on parametric \npolymor\u00adphism in general (see e.g., [5, 23, 26]) and integration of parametric polymorphism into programming \nlanguages such as Java (see [2, 17, 24]). In this section, we consider pri\u00ad marily works that use empirical \nmethods (see [18, 34] for a general introduction into this research method) to study the adoption of \ngeneric types as well as the impact of type sys\u00adtems in general as being closely related to this work. \n 2.1 Adoption of Generic Types The work by Parnin et al. (see [25]) studied to what extent generic types \nhave been adopted in Java software develop\u00adment projects since their creation. In this work, 20 popular \nsoftware development projects (according to ohloh.net) with a total of approximately 550 million lines \nof code were analyzed. An interesting observation of this work is that despite the fact that the introduction \nof generics in Java 1.5 is typically considered as a large step in the evolution of the Java programming \nlanguage generic types only played a minor role in the software projects. Further, the use of generic \ntypes seems to occur mainly in situations where de\u00advelopers access collections: Indeed, had the language \nde\u00adsigners of Java generics instead opted to introduce a single StringList class, then they would have \nsucceeded in satis\u00adfying 25% of Java generic usage [25, p. 12]. Additionally, developers that actually \nused generic types were in the mi\u00adnority. As Parnin et al. suggests: the broad adoption by the project \ncommunity is uncommon [25, p. 10]. Hence, following the argumentation in the study by Parnin et al., \ngenerics do not seem to play a major role in software development at least if adoption is being used \nas the argument. Nevertheless, even if a language feature is used rarely, this does not imply that it \nis not useful (or even crucial) for a minority. In addition to the work by Parnin et al., the work by \nMeyerovich and Rabkin (see [22]) should be mentioned, which studies in general the adoption of programming \nlan\u00adguages. One of the most interesting results of the study is that developers seem to use a certain \nlanguage (or to switch to a different language) mainly because of the domain and use of the language \ninstead of its linguistic features.  2.2 Controlled Experiments on Usability of Type Systems We are \naware of relatively few controlled experiments that focus on the usability of type systems. Given this, \nwe .rst describe studies performed by other authors. Then, we de\u00adscribe our own experiment series focusing \non type systems, of which this paper is a part. 2.2.1 Studies by different authors In the late 1970s, \nGannon showed a positive impact of static type systems in a programming experiment on thirty-eight subjects \n[10]. The experiment was a two-group, within\u00adsubject experiment. It revealed an increase in programming \nreliability for subjects using static type checking. Prechelt and Tichy studied the impact of static \ntype checking on procedure arguments by using the program\u00adming languages ANSI C and K&#38;R C [28]. The \nexperiment divided 34 subjects into four groups where each group had to solve two programming tasks, \none using ANSI C and one using K&#38;R C. The result of the study showed that sub\u00adjects were faster for \none task when using the statically type checked ANSI C. For the other programming task, no sig\u00adni.cant \ndifference was measured. In a qualitative pilot-study, Daly et al. [6] observed pro\u00ad grammers who used \na new static type system for the lan\u00adguage Ruby. The authors concluded from their work that the bene.ts \nof static typing could not be shown but it should be emphasized that the study was performed on a very \nsmall number of subjects. Such a small number potentially reduced the chance to observe the potential \nbene.t or disadvantage of the static type system. Another study by Prechelt compared seven different \npro\u00adgramming languages [27]. Although the experiment had not directly type systems in its focus, Prechelt \nconcluded that humans writing in programming languages with a dynamic type system (the scripting languages) \nrequire less develop\u00adment times. While the previous studies concentrated on the devel\u00adoper s productivity, \nthe work by Ste.k and Siebert con\u00adcentrates on the syntactical characteristics of programming languages \n(see [30]). The authors performed a controlled experiment with novice programmers as subjects and an\u00adalyzed \nwhat syntactical elements from programming lan\u00adguages have effects on the correctness of the novice pro\u00adgrammer \ns use of these constructs. While the study contains a number of amazing insights, the most relevant for \nthe here introduced paper is, that it turns out that novices have prob\u00adlems with the syntax of type declarations. \n 2.2.2 Our own experiment series The study presented here is part of a larger experiment series that \nanalyzes the impact of static type systems on software development (see [14]). The study by Hanenberg \nstudied the impact of statically and dynamically typed programming languages to imple\u00adment two programming \ntasks [13]. Subjects using the dy\u00ad namically typed programming language had a signi.cant positive time \nbene.t on a smaller task, while no signi.cant difference could be measured for a larger task. Stuchlik \nand Hanenberg analyzed to what extent type casts, a language feature required by languages with a static \ntype system, in.uenced the development of simple pro\u00adgramming tasks [33]. Twenty-one subjects divided \ninto two groups took part in a within-subjects design. It turned out that type casts do in.uence the \ndevelopment time of rather trivial programming tasks in a negative way, while tasks with more than eleven \nlines of code showed no signi.cant difference. An experiment by Steinberg and Hanenberg analyzed to what \nextent static type systems help to identify and .x type errors as well as semantic errors in an application \n[32]. The study was based on 31 subjects. The results showed that static type systems have a positive \nimpact on the time required to .x type errors, but did not reveal any differences with respect to .xing \nsemantic errors. Mayer et al. studied to what extent static types help to use undocumented APIs (see \n[21]). In that work, 3 of 5 programming tasks showed that the application of static types were bene.cial. \nWhile one of the other two tasks was considered by the authors to be problematic (the results should \nnot be over-interpreted), still at least one task showed a positive impact of dynamic types. Kleinschmager \net al. performed an experiment on 33 sub\u00adjects which combined repetitions of previous experiments [20]. \nAmong others, the key .ndings from Mayer et al. that static type systems help using undocumented APIs \n were con.rmed; this time without any exception. Spiza and Hanenberg performed an experiment in order \nto check, whether the .ndings of the previous experiments were related to static typing or only to the \nspeci.c character\u00adistics of declarative type systems where type names are part of the source code [29] \n 20 subjects worked with dynam\u00ad ically typed code with and without type declarations in the code. The \nresult of this study was, that to a certain extent the type names in the code already provide some bene.t \n but the study also showed that wrong type names (which possi\u00adbly occured because of the dynamic type \ncheck) signi.cantly reduce the developers performance. 3. Experiment Description We start the experiment \ndescription by providing initial con\u00adsiderations on the experimental design. Once complete, we outline \nthe programming tasks being used in the experiment, then describe the experiment execution. Finally, \nthreats to validity are discussed.  3.1 Initial Considerations The overall goal of the experiment series \nis to objectively evaluate the impact of type systems on software devel\u00adopment. Previous experiments \nstudied whether static type checking in programming languages which also require type declarations in \nthe code (such as Java, C++, etc.) have a measurable effect on the usage of undocumented APIs (ex\u00adcept \ntheir raw source code as documentation). These studies revealed a positive effect in terms of development \ntime (see section 2.2.2). Given the results of these initial experiments, several follow-up questions \nappear to be relevant, namely measuring with: (a) a different static type system, and by comparing (b) \nstatic type systems that differ with respect to expressiveness. Hence, the experimental analysis of generic \ntypes (in comparison to raw types) seems reasonable. Because the previous studies showed a bene.t for \nstatic type systems in comparison to dynamic type systems, we suspect that the expressiveness of a type \nsystem may impact API usage with human users. Consequently, applied to raw and generic types, our .rst \nhypothesis is: H01: Generic types in undocumented APIs improve (in comparison to raw types) the time \nrequired to use such APIs. Additionally, we think that it is necessary to take the speci.c characteristics \nof the type system being studied into account. For generic types, there has been (and still is) a long \ndiscus\u00adsion about the complexity in the literature. There are authors who frequently argue that generics \nincrease the software s complexity (see for example [1]). These authors assume that different features \nsuch as wildcards, bounds, etc. might be responsible for additional complexity2 . After studying the \nbook by Naftalin and Wadler [24] we .nd it plausible that that generics have the potential to in\u00adcrease \ncomplexity in some situations. For example, the pro\u00adposed implementation of the strategy design pattern \n[9] by using generic types based on recursive bounds (see [24, pp. 131 136]) leads from our perspective \nto complicated code where the type relationships are dif.cult to understand or extend. Given this, we \nconsider a programming task where such code needs to be extended as appropriate for an exper\u00adiment. Hence, \nwe intent to examine the following second hy\u00adpothesis: H02: Generic types used for connecting class hierarchies \nincrease the time required to extend those hierarchies (in comparison to raw types). However, even if \nthis second hypothesis turns out to be valid there is possibly still an additional bene.t of generic \ntypes: two experiments con.rmed that static type systems improve the time required for .xing type errors. \nHence, we assume that this claim may holds for more expressive static type 2 See also discussion at: \nhttp://lambda-the-ultimate.org/node/804 systems (such as generic types) in comparison to rather less \nexpressive type systems (such as the use of raw types). Hence, our intention is to check the following \nthird hy\u00adpothesis: H03: Generic types reduce (in comparison to raw types) the time required to .x type \nerrors. Our broad goal of the experiment is study one speci.c lan\u00adguage feature (generic types) in comparison \nto the absence of such a language feature (raw types), while focusing more closely on our three stated \nhypotheses. While it should be obvious to most readers, any experimental design comes with threats to \nvalidity and ours is no exception. One impor\u00adtant threat we want to make clear before discussing our \nex\u00adperiment in detail is that it is possible, or even likely, that the presence of a language feature \nin.uences the design of soft\u00adware. For example, APIs that use generic types often provide generic classes \nsuch as Pair3 or Tuple4 classes which are rarely found if generic types are not available. Without generic \ntypes, we suspect developers tend toward domain\u00adspeci.c classes instead of general classes. Succinctly, \nissues of style in programming may very well impact the results of the experiment. As such, we encourage \nother readers to test the same hypotheses under different experimental conditions to either con.rm or \nrefute our .ndings here. Our perspective is that we should try to make the exper\u00adimental groups in the \nexperiment as similar as possible, so as to make a fair comparison, i.e. we chose not to consider potential \ndifferences in application design. The reason for this is three-fold: First, in case we measure differences \nin API usage between generic and raw types, we are not able to determine to what extent this is caused \nby the differences in the design if we alter the experimental groups too much. Second, although we believe \nthat raw typed programs do not use general classes that often, we (still) frequently .nd such usages \nin raw typed applications. Finally, by focusing our hypotheses tightly, under carefully controlled conditions, \nit facilities running a series of experiments where we test the hypotheses in different ways. This procedure \nis often used in other disciplines to increase the con.dence as to what is true and false.  3.2 General \nexperiment design The general experimental design follows its predecessors: We designed the experiment \nas a two-group within-subject crossover design (see for example [11, 34]). The subjects were randomly \nassigned to one of the two groups. Each of the groups started the programming tasks either with generic \nor with raw types. After the .rst set of tasks in the .rst round is being done (raw or generic), the \nsubjects switch the 3 For example, a class Pair that only consists of two (generic) .elds with corresponding \ngetter and setter methods can be found in the Android SDK, see http://developer.android.com/reference/android/util/Pair.html. \n4 http://docs.oracle.com/javaee/6/api/javax/persistence/Tuple.html  technique and do the tasks once \nmore (which we then call round two) with the other technique as illustrated in Table 1. Round 1 Round \n2 Group Raw Raw Types Generic Types Group Generic Generic Types Raw Types Table 1. General experimental \ndesign As explained in [21, p. 687, 698 699], the potential prob\u00ad lem of learning effects does not play \na major role as long as the learning effect is not larger than the main effect: if it is not the case \nthat for both groups the second round turns out to be faster than the .rst round, this experiment design \nhas the bene.t that it permits differences between two tech\u00adniques, although the experiment is just performed \non a rela\u00adtively small number of subjects.  3.3 Programming Tasks According to previous experiments \nby Kleinschmager et al. and Mayer et al. (see [20, 21]), we constructed two different versions of an \nAPI: the .rst (with raw types) was a Shopping API; the second (with generic types) was a Sports Club \nAdministration API. Both APIs (each consisting of less than 3000 lines of code) were arti.cially constructed \nin order to provide methods and types which were appropriate for the experiment. Both APIs were identical \nwith respect to their code the Shopping API was constructed .rst and then the Sports Club Administration \nAPI was constructed by renaming classes, methods, etc. of the Shopping API. Three kinds of programming \ntasks were designed to test hypothesis H01, one for H02 and one for H03. In order to reduce variance \namong subjects, the code was simple and only required participants to understand the relationships between \ndifferent objects: no loops, conditions, etc. were required to solve the tasks a design decision which \nthis experiment has in common with its predecessors. 3.3.1 Class Identi.cation Tasks (CIT 1 -3) Test \nfor H01 Three different programming tasks were designed in order to test H01. All tasks required developers \nto use a correspond\u00ading API. The programming tasks were similar with respect to the expected lines of \ncode (LOC). But in order to solve each task, it was necessary to understand the different types expected \nor delivered by the participating classes and meth\u00adods. The characteristic of the class identi.cation \ntasks (or short C IT) is that classes and types need to be identi.ed and used. From our point of view, \nall three tasks are comparable in their complexity, although they do have some notable differences. While \nCIT 1 is mainly the representation of a list-like data structure, CIT 2 contains a more chain-like data \n/ / P o s s i b l e s o l u t i o n f o r t a s k C I T 1 p u b l i c s t a t i c v o i d a d d M e m \nb e r s ( A n u a l R e p o r t s ) { S e t < P e r s o n > s e t = s . g e t U p d a t e ( ) . f o \nu r ( ) . g e t L e f t S e t ( ) ; S e t < P e r s o n > t a r g = s . g e t U p d a t e ( ) . t w o \n( ) . g e t M e m b e r s ( ) ; t a r g . a d d A l l ( s e t ) ; } / / C l a s s e s a n d i n t e r \nf a c e s r e q u i r e d t o s o l v e C I T 1 p u b l i c c l a s s A n u a l R e p o r t { . . . p \nu b l i c F o u r < S e a s o n T o k e n , C l u b , C o n f e r e n c e L o c a t i o n , T u p l e \nS e t < P e r s o n , R e g i s t r a t i o n >> g e t U p d a t e ( ) { . . . } . . . } p u b l i c \nc l a s s F o u r <N1 , N2 , N3 , N4> . . . { . . . p u b l i c N2 t w o ( ) { r e t u r n t w o ; } \np u b l i c N4 f o u r ( ) { r e t u r n f o u r ; } . . . } p u b l i c c l a s s T u p l e S e t <LEFT \n, RIGHT> . . . { p u b l i c S e t <LEFT> g e t L e f t S e t ( ) { . . . } . . . } p u b l i c c l a \ns s C l u b . . . { p u b l i c S e t < P e r s o n > g e t M e m b e r s ( ) { . . . } . . . } p u b \nl i c i n t e r f a c e S e t <A> . . . { . . . b o o l e a n a d d A l l ( S e t < ? e x t e n d s A> \nc ) ; . . . } Figure 1. Example solutions for programming task CIT 1 structure, and CIT 3 contains a \ntree. In order to ease reading of this paper, we describe here only one of the tasks5 . Figure 1 shows \na possible solution of the .rst CIT and an excerpt of the API that is needed to solve the task. In order \nto solve it, subjects would need to add all new mem\u00adbers that are listed in the current annual report \nto the club. The annual report is a class which makes use of gener\u00adics; it has a quadruplet as a .eld \nwhich expresses a rela\u00adtionship to multiple objects. A type Four<SeasonToken, Club,ConferenceLocation,TupleSet<Person, \nRegistration>\u00bb> is returned by the method getUpdate() which provides the corresponding object6 . Using \nthis ob\u00adject, participants obtain a reference to the set of persons (via the method getLeftSet() on the \nforth component of the generic type). Then, this set must be added to the member list which is received \nvia the club reference.7 5 A complete list of solutions for the programming tasks and an excerpt from \nthe API required to solve the tasks is described in the appendix. 6 It should be noted that although \ntypes (and type names) such as Four seem to be rather inappropriate, similar examples can be found in \nliterature (such as the generic class Pair which can be found in [24, p.126]). 7 Types such as Set have \nbeen newly created for the experiment in order to force all subjects to take a look into the delivered \nAPI and not to bene.t from previous experiences with Java.  While the generic type de.nition gives concrete \ninforma\u00adtion about what kind of objects are returned by getUpdate() (which likely makes it easier to \n.nd the class de.nitions), this is not the case for the raw types. In that instance, subjects only get \ninformation that the result of the update method is an object of type Four. The programming tasks CIT \n2 and CIT 3 are comparable to CIT 1 the generic imple\u00admentations use complex generic types in their \nsignatures, in contrast to nominal types in the raw version.  3.3.2 Extension Task (EX T T) Test for \nH02 The extension task is based on a strategy design pattern implementation (see [9]) which change dynamic \nbehavior of a, so-called, context object. The task was to add a class to the type hierarchy which contains \nthe strategy implementation. a b s t r a c t c l a s s S p o r t E v e n t <P e x t e n d s S p o r \nt E v e n t <P>> { p r i v a t e L o c a t i o n <P> l o c a t i o n ; . . . } c l a s s R u n n i n \ng e x t e n d s S p o r t E v e n t < R u n n i n g > { . . . } a b s t r a c t c l a s s L o c a t \ni o n <SE e x t e n d s S p o r t E v e n t <SE>> { a b s t r a c t v o i d r e s e r v e D a t e F o \nr E v e n t ( SE p ) ; } c l a s s R u n n i n g L o c a t i o n e x t e n d s L o c a t i o n < R u \nn n i n g > { . . . } Figure 2. Class de.nitions for EX T T -strategy design pat\u00adtern implementation \nIn the API, the sports club contains an event management system that permits reserving dates. The sport \nevent plays the role of the context in the design pattern, which is pa\u00adrameterized with a location. According \nto [24], the strategy, as well as the context, are generic types. Furthermore, the context is de.ned \nusing recursive bounds the generic type SportEvent uses itself as an upper bound for the type pa\u00adrameter \n(see Figure 2) . The task is to add a new context (type StagedRunning) to the class hierarchy. While \nStagedRunning is conceptu\u00adally a subclass of Running, it cannot be directly added as a subclass to Running \nbecause in that case the type of the lo\u00adcation of StagedRunning would be just a Running where it should \nbe of a StagedRunning location. As a consequence, the generic solution requires introduc\u00ading an additional \nabstract class and changing the inheritance hierarchy of the existing classes, in addition to moving \nmeth\u00adods. Developers of the raw solution, where the strategy im\u00adplementation also permits to assigning \nthe wrong raw ob\u00adjects to a given context, only need to declare that the new class extends an existing \none.  3.3.3 Type Error Fixing Task (TE F T) Test for H03 For the type error .xing task, subjects needed \nto identify and .x a type error that was given in the code. The code is related to the strategy implementation \nas given above. In the code, a wrong strategy object was instantiated and assigned to a given context \nobject. As a solution, the right strategy object must be instantiated. While generic type developers \nwere effectively told by the type checker that a wrong strategy type was used in the code, developers \nusing the raw version need to identify this on their own. The resulting behavior of the context object, \ndue to the wrong strategy object, thus was wrong.8  3.3.4 Summary of programming tasks Five programming \ntasks were de.ned which test the three different hypotheses. While the three CIT tasks are based on previous \nexperiments, the role of EX T T in combination with is TE F T different: while we are convinced that \nEX T T requires more time for the developers using generic types, we think that TE F T a task which \nis based on the same kind of code as EX T T requires less time using generic types. Hence, both tasks \nin combination are able (if the results are as expected) to reveal an interesting insight: that the use \nof generic types in some situations reduce the extensibility, but in the same situation reduce development \ntime for .xing type errors. 3.4 Programming Environment and Measurements Programming environment: The \nsubjects were equipped with a primitive IDE, which includes a text editor and a tree\u00adview. This shows \nall class de.nitions in the current project. Additionally, this IDE permits running code and it shows \nthe errors of the type checker as well as output from test runs in a separate frame. The motivation for \nusing a primitive IDE is two-fold. First, if an existing IDE would have been used, subjects familiar \nwith that IDE bene.t from their experience. Second, the main intention of the experiment is to study \nthe impact of the language feature generic types and not to study tool support for generics or raw types. \nGiven that IDEs such as Eclipse do not provide any additional support for raw types, but provide such \ninformation for generics (such as code completion that also relates to the generic types), this might \ngive an unfair advantage to the generics group. Test_cases and measurements: Subjects were given a set \nof compiled test cases (without their source code) that could be executed. A programming task was considered \nto be solved when all test cases were ful.lled. The test cases reported on missing or erroneous places \nin the code, i.e. they provided additional feedback to developers in order to deter\u00admine what was missing \nor what was incorrect. The primary measurement in the experiment was the time from deliver\u00ading the programming \ntask until the time when all test cases were ful.lled. The IDE logged different development steps so \nthat it was possible to determine precisely when a task has been ful.lled (see [20, 21] for more details). \nIn addition to the pure development time, additional measurements such as the number of .le switches \n(i.e. when a developer switches 8 A more detailed description of the task can be found in the appendix. \n from one .le into another) as well as screen recordings were taken.  3.5 Experiment Execution: Time \nLimit and Abandoning Tasks Participants needed to solve the programming tasks in the order CIT 1-3, TE \nF T, and ExtT, i.e. all were required to solve the tasks in the same order. In this short section, we \ndescribe some additional issues related to the experiment in relation to timing and abandoning tasks. \nAbandoning tasks: We allowed participants to abandon programming tasks if they thought they were unable \nto com\u00adplete it. This choice was mainly driven by the observation (in a pilot study) that EX T T was \nvery hard it was unclear whether anyone could solve it within 55 minutes. In those pilot studies, it \nturned out that many gave up after time, be\u00adcause they had no idea how to do the extension in the generic \ntyped version in reasonable time. Hence, by giving subjects the ability to abandon a task, our goal was \nto keep these in\u00addividuals motivated in continuing the experiment. Time limit: Additionally, we de.ned, \nfor each task, an upper limit of 55 minutes for .nishing each programming task. The motivation for this \nupper limit (as well as the ability to skip a task) was largely practical. Our intention was to reduce \nthe maximum time required for single subjects: While in previous experiments9 the number of hours per \nsubjects was relatively high, these restrictions permitted us to say upfront clearly what the maximum \namount of time was. The potential risk of this decision is that handling of time will lead to subjects \nbeing unable to solve many tasks, es\u00adpecially for underperformers. In order to exclude these sub\u00adjects, \nwe de.ned upfront, that we require a subject to solve at least three of the ten programming tasks in \norder to be included in the analysis of the experiment results. We are aware that this is a relatively \narbitrarily chosen border. How\u00adever, we motivate this by the background from previous ex\u00adperiments from \nwhich we assumed that subjects using the raw typed version will have potential problems in solving a \nprogramming task at all (except EX T T where we expected that the generic typed version cannot be solved \nwhile the raw version will be done quite quickly). And by even considering that it might be even hard \nto solve all of the residual tasks, our perspective was that three tasks was at least reasonable. Data \nfor participants not included in the experiment, regard\u00adless of our decision, is available on request. \nCombination of time limit and abandoning tasks: The choice of giving subjects the opportunity to abandon \na task as well as the time limit has a consequence that it is unclear how to interpret the resulting \ntime measurement. One possi\u00adbility would be to leave out the subjects for a failed task from the analysis. \nHowever, since we assumed that most subjects will run into the time limit, ignoring this fact would be \nin\u00ad 9 Experiments which were not related to type systems (see [7, 15]). appropriate: the most interesting \npart documenting who and why users may have run into the time limit would be ignored. Thus, we took \nthe following approach: no matter whether a subject ran into a time limit or whether the sub\u00adjects abandoned \nthe task, we assumed that the task would have been solved in the very next moment. This decision is, \nadmittedly imperfect and others may wish to run the exper\u00adiment differently. However, we assumed that \nthis problem might only play a role for EX T T we will discuss this issue in more detail in the following \nsection. It should be empha\u00adsized that we carefully documented, for each user, whether a task has been \nsolved or whether the task has been aban\u00addoned.  3.6 Threats to Validity This experiment shares its \ninternal as well as external threats to validity with previous experiments threats which refer to the \ngeneral experimental setup. Other threats are speci.c to this experiment and mainly refer to the ability \nthat subjects were permitted to abandon a task and that an upper time limit was de.ned. We will start \nwith the discussion of these speci.c threats .rst. Abandoning Tasks Internal Threat: One potential threat \nto internal validity is that (1) subjects are permitted to abandon tasks and (2) that the very next moment \nis as\u00adsumed to be the moment when subjects have .nished the tasks. The danger in this setup is that if \nsubjects abandon a task too early that the resulting measurement would lead to spurious results. For \nexample, if all subjects decided af\u00adter a few seconds in the raw version to abandon the tasks while they \nactually required only a few minutes to solve the tasks with the generic version, the resulting measurements \nwould lead to the conclusion that they required less time for the generic version. In other words, if \nthis were to occur, it would be an incorrect result. The reason why we still per\u00admitted abandonment is \nbecause of EX TT. This task required a deep understanding of generic types, the type relationships between \ndifferent classes, and requires creativity on how the class hierarchy needs to be reorganized. Our assumption \nwas (also based on experiences with pilot studies on the experi\u00adment) that subjects would only abandon \nthe generic imple\u00admentation of EX T T. This assumption turned out to be true (see section 4.1). Based \non this assumption, using the mo\u00ad ment when the task was abandoned as the reference time is likely in \nfavor for the generic typed version, as we assume this version takes more time. As with any empirical \nexperi\u00adment, we are not arguing that our choice is the right one, but we are trying to open in documenting \nthe decisions we made so that others can reproduce our .ndings or adjust our experiment. Time out Internal \nThreat: Like the previous threat, the choice of an upper limit for the experiment is a potential internal \nthreat to validity. We think there are two potential problems with this design decision. First, it might \nbe possi\u00adble that the upper limit was too small so that the raw as well as the generic group will run \ninto the time limit. The con\u00adsequence of this is that the experiment would measure no difference between \nboth approaches, even if an effect exists in the natural world. The second problem is that one group \nmay be too close to the upper limit, while the other would hit the upper limit. In such a scenario, the \nresulting anal\u00adysis would also detect no difference between both groups, again even if a difference is \nreal. Thus, an upper limit at all has the potential to be misleading. We tried to reduce this problem \nby choosing a relatively generous upper limit where we assumed that only those subjects that would otherwise \nspend a large additional amount of time on the task would run into the limit. Additionally, due to experience \nwith pre\u00advious experiments, as with a pilot study on this experiment, we suspected that the differences \nbetween groups were large enough that the time limit played a minor role. With that said, our 55 minute \nlimit was the result of our pilot study was likely very speci.c to the sample we had chosen (stu\u00addents \nfrom the University of Duisburg Essen, see section 4), so experiment replicators should not take this \nlimit as a given. We consider it likely that different samples of people may require different time limits. \nAgain, only through care\u00adful replication under varied conditions can we get closer to the truth.  Programming \nTasks External Threat: Empirical studies in general suffer from the problem that the program\u00adming tasks \ntypically do not permit generalization to an arbi\u00adtrary set of programming tasks, because it is unclear \nto what extent a programming task can be considered as representa\u00adtive. Our experiment is no exception. \nHowever, the program\u00adming tasks were designed in such a way that generic types play a central role, and \nwe were careful to at least attempt to make them reasonably representative of what a programmer might \ntry and accomplish. It is reasonable to ask what extent source code in Java contains any generic types \nat all (see [25] and discussion in section 2). Next, the programming tasks differ quite a bit. While \nthe CIT tasks require subjects to use an undocumented API (and check whether Java s generic types help \nusing an undocumented API), EX T T is taken di\u00adrectly from the literature about generic types (see [24]). \nWe think that EX T T is an example where the use of generic types is problematic. However, we are not \naware of any general principle that proves this is the case when used by human users. From the same perspective, \nit can be argued that the TE F T only represents a speci.c kind of situation during programming (the \n.xing of a type error) other situ\u00adations that involve generic types, obviously, can and should be tested \nin order to get a well-rounded picture of human behavior. The programming tasks also from another per\u00adspective \nthreat the external validity: the code to be written was kept very simple (no loops, conditions, etc.). \nIn case we would add such constructs to the experiment, we assume that the deviation among subjects will \nincrease and as a consequence the statistical power will be reduced. We think that in an replication \nit would be possible to used constructs such as conditions and loops, but then the sample size must be \nhigher. Hard Extension Task External Threat: The exten\u00adsion tasks and the type error .xing tasks were \nchosen from the same scenario: the implementation of the strategy design pattern. The experimenters assumed \nthat both show two ex\u00adtremes of generic types in the same scenario: that they are helpful in this scenario \nfor .xing type errors while generics are problematic with respect to extensibility. However, the experimenters \nfeeling is, that the extension tasks is a rather hard one and it is at least questionable whether an \naver\u00adage programming task with generics found in industry is comparable hard. Simpli.ed IDE External \nThreat: The use of an over\u00adsimpli.ed and arti.cial IDE is an external threat to valid\u00adity: The IDE here \nis the same as the ones used in [20, 21] and does not provide features such as code completion, etc. \nwhich are common in modern IDEs. In correspondence to [20, 21], our argument for this arti.cial IDE is \nthat we sep\u00adarate the impact of the language itself from the concept of tooling in the presence of (different \nkinds of) type sys\u00adtems. We think the issue of tooling is important and have such experiments planned \nfor future work. However, due to the missing features, it seems clear that the measured times cannot \nbe considered as comparable to development times in situations where appropriate tool support would have \nbeen available. Students as Subjects External Threat: As discussed elsewhere (see [13]), the use of \nstudents of subjects might be a potential threat to the external validity. Without trying to repeat the \narguments, it should be mentioned that there are at least serious doubts that the statement that students \nshould not be used as subjects holds (see [16]). Furthermore, there are formal studies (see for example \n[8, 19]) that show that even the distinction among subjects is unclear: Simple demographic data collections \nwhich can be frequently found in literature (see the literature overview in [8]) do not although often \nrequested by other researchers reveal much of use to researchers in practice. Program Design Internal \nand External Threat: As already discussed in section 3.1, a potential threat is that the presence of \ngeneric types might in.uence the program design. While developers using generic types seem to design \ngeneric classes such as Pair (which only consist of two .elds and corresponding set and get methods), \nsuch classes do not seem to occur that often in raw code. The resulting (internal) threat is, that the \nwrong code is being used for measurement, i.e. instead of measuring generic vs. raw types, generic types \nvs. raw types based on a generic design is being measured. The resulting external threat is, that the \nkind of code being used does not generalize, because such design cannot be found in practice.  4. Measurements \nand Analysis10 The experiment had been executed in multiple sessions at the University of Duisburg-Essen, \nGermany.11 The participants were bachelor students as well as master students. We per\u00admitted subjects \nwith different educational background (bach\u00adelor students and master students) to participate, because \nbased on previous observations (see [8, 19]), the pure dis\u00ad tinction of educational background does not \nguarantee that subjects are better prepared to successfully participate in a programming experiment. \nGiven this observation, we consider the criterion being used in this experiment (at least three tasks \nsolved, see sec\u00adtion 3.5) as appropriate. In total, 24 subjects participated in the experiment. However, \nonly 16 of them could be used in the analysis, because eight did not solve at least three tasks. 4.1 \nMeasured Data Table 2 gives an overview of the data (measured in seconds) and the descriptive statistics \nfor the measurements. If the task was abandoned by the subject or the maximum time was reached, the corresponding \ndata is marked with a star. The number behind the star is either 3300 seconds (which means that the subject \nran out of time) or a different number which indicates when developers abandoned a programming task. \nThe raw measurements reveal a relatively clear tendency with respect to the programming tasks: Only \nEX T T was abandoned: It turns out that the sub\u00adjects made little use of the possibility to abandon the \npro\u00adgramming task. For EX T T, we see that four subjects (7, 12, 13, 16) abandoned the programming task \n(two sub\u00adjects starting with the generic and two starting with the raw implementation) and in all cases \nthe subjects aban\u00addoned the generic implementation. Furthermore, we can see for these subjects that they \nabandoned the task much later in comparison to the solution time they required to solve the raw typed \nimplementation.  High number of time-outs in raw versions (except EX TT): If we compare the ratio raw \ntime outs versus generic time outs, we see (except EX TT) a clear tendency for time outs in the raw version: \n6/1 (CIT 1), 5/1 (CIT 2), 8/3 (C IT 3), and 3/1 (TE F T). For EX T T the opposite appears to hold (0/11). \n Clear tendency in CIT tasks: If we consider the sums of development times for the class identi.cation \ntasks, we see that no single subject required more time using the generic types in comparison to using \nthe raw types.  10 All experiment data can be downloaded from the webpage: http://dawis2.cs.uni-due.de/ests/esgt \n11 It should be noted that these students were already trained in generic types as part of studies: courses \non programming and programming languages at the University of Duisburg-Essen already have the application \nof generic types in their curriculum. If we consider just the CITs, we observe the following: CIT 1: \nFor CIT 1 only subjects 2, 9, and 15 were able to solve the programming tasks faster with the raw types \nin comparison to the generic types. All of these subjects started with the generic version  CIT 2: Zero \nsubjects required more time for CIT 2 using the generic version than using the raw version. One subject \n(subject 12) ran into a time-out for both versions  hence, it cannot be determined for this subject \nwhich version required more time. CIT 3: While three subjects (subject 1, 6, and 12) were not able to \nsolve the task in either the raw or generic version, for all other subjects the time for the raw version \nis higher than the generic version. It is worth mentioning that no subjects ran out of time with the \ngeneric version without also running out of time with the raw version. Taking into account that only \nfor .ve pairs among the 3x16 selected C IT pairs subjects ran out of time gives us reasonable evidence \nthat our time limit was accept\u00adable. While the results for the CITs are clear, this is not the situation \nfor the type error .xing time: TE F T: Six of the sixteen subjects where faster with the raw version \nthan with the generic version, where one of these subjects (subject 12) did not solve the task in the \ngeneric version a .rst indicator that the expected result for TE F T did not show up in the experiment. \nThe extension task again shows clear results: ExtT: While only one single subject (subject 14) was able \nto solve the task using the generic version, all sub\u00adjects had time with the generic, as opposed to the \nraw, version, even with the subjects who abandoned the task. Consequently, it looks like the use of the \nupper limit did little harm no matter how long the subjects would have actually required to solve the \ngeneric version, it always took them longer than with the raw version. Looking at the sum of development \ntimes, i.e. taking a look into the aggregated values, the rather clear results seem to vanish (or appear \nat least much reduced): four of the sixteen subjects were quicker in delivering their solutions using \nthe raw version (subjects 6, 7, 10, 15). This is an indicator, that a potential positive effect in the \n.rst tasks was compensated by the latter tasks. The descriptive statistics in Table 212 seem to strengthen \nthe impression that generic types have a positive impact on all programming tasks, with the exception \nof ExtT. This statement holds for the arithmetic mean and the median. 12 We did not give information \nabout the maximum value for each task: because of the chosen upper limit for the tasks, the maximum value \nwould be 3300 second in all cases except E X T T which then is a meaningless value.  CIT 1 CIT 2 CIT \n3 SUM(CIT) TE F T ExtT Sum(all) Subject Start Raw Generic Raw Generic Raw Generic Raw Generic Raw Generic \nRaw Generic Raw Generic 1 Raw *3300 552 *3300 1289 *3300 *3300 9900 5141 2112 2350 347 *3300 12359 10791 \n2 Gen 817 1382 2714 1576 *3300 1054 6831 4012 *3300 531 283 *3300 10414 7843 3 Gen *3300 1156 *3300 2085 \n*3300 793 9900 4034 2280 2899 804 *3300 12984 10233 4 Raw 2164 958 3098 602 *3300 532 8562 2092 2282 \n1364 281 *3300 11125 6756 5 Gen 2925 1334 2246 1150 *3300 1255 8471 3739 1362 615 131 *3300 9964 7654 \n6 Raw *3300 765 1159 322 *3300 *3300 7759 4387 685 1691 329 *3300 8773 9378 7 Gen 2095 506 663 347 649 \n434 3407 1287 1052 478 38 *3083 4497 4848 8 Raw 2403 279 746 347 1776 617 4925 1243 1229 860 329 *3300 \n6483 5403 9 Gen 1333 1360 *3300 793 *3300 1356 7933 3509 1017 2126 78 *3300 9028 8935 10 Gen 2283 1204 \n1050 753 1503 724 4836 2681 1164 897 280 *3300 6280 6878 11 Raw *3300 479 1621 976 3200 423 8121 1878 \n2787 2087 430 *3300 11338 7265 12 Raw *3300 821 *3300 *3300 *3300 *3300 9900 7421 2875 *3300 368 *1836 \n13143 12557 13 Gen *3300 *3300 3125 587 2167 1040 8592 4927 *3300 2443 1306 *2699 13198 10069 14 Raw \n2888 728 1056 434 3162 309 7106 1471 975 1722 413 1959 8494 5152 15 Gen 1569 2749 1696 1008 2195 1432 \n5460 5189 *3300 516 225 *3300 8985 9005 16 Raw 2083 599 *3300 483 2998 505 8381 1587 1511 436 657 *2624 \n10549 4647 Time-outs 6 1 5 1 8 3 Abandoned 0 0 0 0 0 0 3 1 0 11 0 0 0 4  Min 817 279 663 322 649 309 \n3407 1243 685 436 38 1836 4497 4647 Mean 2523 1136 2230 1003 2753 1273 7505 3412 1952 1520 394 3031 9851 \n7963 Median 2646 890 2480 773 3250 916 8027 3624 1812 1528 329 3300 10189 7748 Std. Dev. 805 819 1057 \n785 833 1062 1953 1775 937 941 310 493 2582 2325 Table 2. Measured development times (time in seconds), \nnumber of time-outs, number of abandoned tasks, and descriptive statistics Start = Technique subjects \nstarted with, Raw = Raw Types, Gen. = Generic Types, measurements annotated with a * indicate that a \nsubject did not .nish in time and the time limit is being used as measurement While the previous description \nof the measured data gives dent variable task (within-subjects, CIT 1-3, TE F T, and a .rst intuitive \nview on the data, it is still necessary to EX T T) and the dependent variable of development time. perform \ncorresponding signi.cance tests on the results. We For the .rst round, we observe13 a signi.cant effect \nof perform here an analysis which is comparable to the one the programming task (p<.05, . 2 p =.18) and \na signi.cant in\u00ad performed in [20, 21]: .rst, we perform a repeated measures teraction effect between \nthe variables programming task and ANOVA in order to check whether an overall effect could kind of used \ntypes (p<.001, . 2 p =.71). An effect size of .71 is be observed. Then, we perform an analysis on a \ntask by task considered extremely large in human studies this is a ma\u00adbasis. jor effect. Further, the \nbetween-subjects effect of type sys\u00ad tem approaches signi.cance (p<.055, . 2 p =.24). This effect is \n 4.2 Repeated Measures ANOVA on all tasks We ran two repeated measures ANOVAs [11] on the mea\u00ad surements. \nFirst, we ran a repeated measures ANOVA on all .ve programming tasks in round one and then on all pro\u00adgramming \ntasks in round two. I.e. we compare the effect of the independent variable type system on all measured \ndata independent of the programming task. By comparing the two different rounds, we conducted a between-subject \ncompari\u00adson between the different sets of tasks (because the second measurement for each subject with \nthe different type system is not part of the analysis) while each set of task is a within\u00adsubject variable. \nHence, the repeated measures ANOVA is being per\u00adformed on the independent variable kind of type system \n(between-subjects effect, generic versus raw), the indepen\u00adrather large, especially given our small sample \nsizes. This means that independent of whether generic or raw types have been used, a difference between \nthe tasks was observed. Further, 18% of the variance in the measurements can be explained in terms of \nthe differences between pro\u00adgramming tasks themselves. The signi.cant interaction ex\u00adpresses that whether \ngenerics are helpful is task dependent (where 71% of the variance can be explained by the inter\u00adaction). \nA reasonable interpretation of the between-subjects effect of types approaching signi.cance is that, \nwhile the im\u00adpact of generics is highly task-dependent, that the bene.ts do slightly outweigh the consequences \noverall, at least for our experiment. 13 The Greenhouse-Geisser was applied since the sphericity test \nturned out to be signi.cant with p<0.04.  For the second repeated measures ANOVA, the val\u00adues differ. \nWhile there is still a signi.cant interaction ef\u00adfect between the variables programming task and type \nsys\u00ad tem (p<.001, .=.54), the programming tasks themselves Task CIT 1 CIT 2 CIT 3 P-values RAW FI R S \nT .012 .018 .043 Less time Generic Generic Generic 2 p (within-subjects) were not signi.cant (p=.262). \nSimilarly, the between-subjects effect (generic versus raw) was not sig\u00adni.cant (p=.273). Consequently, \nwe see in both cases, that a task-by-task analysis (or at least the consideration of differ\u00adent kinds \nof tasks) is necessary, due to the signi.cant, and very large, interaction between the variables programming \ntask and kind of used types.  4.3 Task-speci.c analysis GE N E R I C FI R S T .237 .012 .012 Less time \n Generic Generic Result Less time Generic Generic Generic Table 3. CIT development times individual \nwithin-subject analysis (Wilcoxon-test) for both groups RAW FI R S T and GE N E R I C FI R S T clear \npositive impact of generic types For a task-speci.c analysis it is necessary to keep in mind that there \nare three different kinds C IT, TE F T and EX T T. For the last two, we have only a single measurements, \nfor the C ITs we have three. Consequently, it seems reasonable to analyze the CITS as a group as well \nas on an individual basis. 4.3.1 Class Identi.cation Tasks (CIT) If we analyze the class identi.cation \ntasks as a group, this means that again a repeated measures ANOVA is being per\u00adformed, but only with \nCIT 1-3 for the variable programming task. An implication of this approach is, that the possible different \nkind of effects of the other variables do not in.u\u00adence the results of the CITS. Again, the application \nof the repeated measures ANOVA requires us to analyze round one and two separately. For the .rst round, \nthe programming task approaches sig\u00adshown If we go one step further and perform an analysis for each \nindividual CIT, we perform a non-parametric Wilcoxon-test (which now considers the within-subject measurements \nfor each task) for both groups in separation and then combine both results. Table 3 shows the result \nof this analysis: for both groups we see a signi.cant positive impact for CIT 2 and CIT 3 hence, a clear \npositive impact of generic types is shown for these tasks. For task CIT 1, the group GE N E R I C FI \nR S T shows no signi.cant difference while group RAW FI R S T does. As argued in [21], this is acceptable, \nbecause of the assumed learning effect and the interpretation is (still) that we mea\u00adsured an overall \npositive impact of generic types.  4.3.2 Type Error Fixing Task (TE F T) ni.cance (p<.069, .ni.cant \ninteraction between programming task and kind of 2 p =.199). Now, there is no longer a sig-Since the \nexperiment only used one programming task that used types (p>0.161), but the kind of used types is clearly \nconcentrates on .xing type errors, it is not necessary to signi.cant (p<0.001, . 2 p =.73). For the second \nround, pro\u00ad gramming task is non-signi.cant (p>.163), there is no sig\u00aduse a repeated measured ANOVA. \nThe appropriate test for this situation is either a t-test or a non-parametric Mann\u00ad ni.cant interaction \nbetween programming task and kind of Whitney-U-Test. The reader should consider that, given that used \ntypes (p>0.578) and (again) there is a signi.cant effect there are fewer data points, and the test we \nused is non\u00ad of the variable of types (p<0.004, . 2 p =.451). These results are clear: regardless of \nthe programming parametric (we chose the Mann-Whitney-U-Test), we would expect the results to be less \nreliable. tasks that have been performed in the .rst or the second round, there is a signi.cant effect \nof using generics. Further, the partial-eta squared value are considered quite large by statistical standards, \nand thus they explain large parts of the variance between measurements. With that said, the effect on \nround one appears larger than on round two: while in round one, 73% of the variance could be explained \nby the kind of types, it is only 45% in round two. Further still, the effect of the kind of types on \nall three programming tasks seems to be quite homogeneous no signi.cant interactions could be found \nbetween the kind of types and the programming tasks. Finally, it is worth mentioning that the three programming \ntasks themselves did not appear to be very different. While this is a minor result, it implies that other \nresearchers can use these tasks for further study, as we have shown they hold similarities, making future \ntests easier to conduct. Applying the Mann-Whitney-U-Test to the TE F T reveals no signi.cant differences, \nneither in the .rst (p>.234) nor in the second round (p>.645). Consequently, we (again) per\u00adform a within-subject \nanalysis for each group in separation and approach which now increases the statistical power be\u00adcause \nwe have now two measurements for each subject (for both treatments of the variable kind of used types). \nHowever, again the analysis does not reveal any signi.cant differences the RAW FI R S T group (p=.779) \nas well as the GE N E R I C -FI R S T group (p=.208) are far from showing signi.cant dif\u00adferences. Thus, \nwe cannot determine whether generics had an impact for this task.  4.3.3 Extension Task (EX T T) The \nsituation for EX T T is comparable to the previous task only a single task was being provided. Like \nbefore, we Table 4. TE F T development times within-subject analy\u00adsis (Wilcoxon-test) for both groups \nRAW FI R S T and GE N E R -I C FI R S T no reliable difference between generic and raw types was detected \n RAW FI R S T GE N E R I C FI R S T p-values .779 .208 RAW FI R S T GE N E R I C FI R S T p-values \n.012 .012 Less time Raw Raw Table 5. EX T T development times within-subject analysis (Wilcoxon-test) \nfor both groups RAW FI R S T and GE N E R I C -FI R S T positive impact of raw types shown performed \na Mann-Whitney-U-Test between both groups in each round in separation. Then, we analyze both groups in \nseparation. Then, we make use of the within-subject mea\u00adsurement. The result of the Mann-Whitney-U-Test \nis, that for both rounds a signi.cant positive impact of the raw types was measured: in both cases we \nreceive p<0.001. Although the results are already quite clear, we still perform the Wilcoxon test on \nboth groups in separation. Again, we measure a clear positive impact of raw types both groups show a \nsigni.cant difference pro raw types. 5. Discussion We now move to a discussion of our broad experiment, \nin\u00adcluding the results and research questions. Then, we give a possible interpretation overall of what \nwe think the experi\u00adment means. After a critical discussion about the number of subjects, we .nally discuss \nexperimental design. Discussion of experiment results: Two of the three dif\u00adferent hypotheses (for the \nCIT as well as ExtT) that referred to the measured development times showed quite clear re\u00adsults: Class \nidenti.cation tasks (CIT): the experiment showed a clear advantage for generic types in regards to devel\u00adopment \ntimes in identi.cation tasks even the separate analysis showed a positive impact of generic types for \ntwo of three tasks. Only in one case (C IT 1) did the group starting with generics show a non-signi.cant \ndif\u00adference (while the other group showed a positive impact of generic types). In our view, while this \ndoes not set\u00adtle the issue, it is the strongest empirical evidence to date that generics may be bene.cial \nto human users for at least this type of programming task.  Type error .xing task (TE F T): the experiment \nshowed no signi.cant difference between raw and generic types for .xing type errors.  Extension task \n(EX T T): the experiment showed a sig\u00adni.cant positive impact of raw types for the extension task. Given \nthat the CITS were bene.ted by generics, but this task showed the opposite result is an example of the \ninteraction effect already discussed. The class identi.cation results con.rm that the additional type \ninformation developers get from generic types helps them use an undocumented API more quickly. This result \ncan be related to the results in [21] and [20]. In [21] we showed that systems which provide static type \ninformation to the developers help them to use undocumented APIs. This statement was con.rmed in [20]. \nConsequently, this new experiment extends this .nding to generics, with the caveat that generics do not \nhelp universally only under certain conditions. Further, while the previous experiments only compared \nstatic and dynamic types systems, this experiment used only a static type system. Thus, it not only matters \nthat a programming language has a static type system it matters what kind of static type system it has. \nWe caution readers not to read into this result. Those that want to claim that their particular type \nsystem choices are bene.cial are obligated to providence beyond a proof that the system is consistent; \ndata from human users is also required. From a different point of view, some argue that generics are \ntoo complex, a claim that does not conform to the results of this experiment. Authors that are serious \nabout scienti.c objectivity must provide evidence showing the exact condi\u00adtions under which this complexity \nholds and a correspond\u00ading experiment, with humans, to verify their hypothesis. In other words, in situations \nwhere the developer has to use an undocumented API, they help developers in understand\u00ading how the API \ncan be used. Thus, claiming complexity is hereto seen as an insuf.cient argument. With respect to the \ntype error .xing task, we found the result surprising. While a previous experiment [31] showed a clear \npositive impact of static types in comparison to dy\u00adnamic types, this does not appear to hold for the \ncomparison of generic and raw types in all cases. It could be argued that this is just the result of \na relatively small sample size, which may be the case. However, the p-values in the experiment provide \nlittle guidance toward raw or generic types (p-values = .234 respectively .645 for the Mann-Whitney-U-test), \nand as such, we are hesitant to make an argument either way. Thus, we think a conservative interpretation \nwould be that, if generics help or hinder for this kind of task, the impact is small. Why users doing \nthe type error .xing task did not bene.t from generics, however, is not clear. It might be the case that \nwhen .xing type errors, generics had inherent complexity. Alternatively, our chosen task (the strategy \ndesign pattern implementation) could be too complex and that other kinds of type error .xing tasks would \nbe easier. Unfortunately, the experiment does not reveal more data on these phenomena14 . While the reasons \nare mysterious, the result at least gives future researchers another area to investigate.  With respect \nto the extension task, it was taken directly from the literature [24]. This introduces the implementation \nof the strategy design pattern with generic types. While a minor result, the experiment suggests that \nsuch an imple\u00admentation might be rather dif.cult for human users. Look\u00ading more carefully at the the \nmeasurements reveals that even the slowest participant in the raw type group programmed approximately \n34% faster than the fastest participant in the generics group. It should be emphasized that only one \nsub\u00adject was capable to solve the task with generic types within the time limit at all. Consequently, \nalthough the experiment cannot give any exact approximation how much faster sub\u00adjects are with the raw \nin comparison to the generic version, it seems clear that the difference is large. Studying reduced ex\u00adtensibility \nin more detail in the future by identifying in what situations such a phenomenon occurs could be an interesting \nresearch topic. An interesting and surprising outcome of the experiment is the combination of the results \nfor the type .xing task and the extension task. The original idea for the experiment was to show that \nalthough generic types possibly suffer from the problem of extensibility (at least for the strategy implementation), \nthey still have advantages when type errors need to be .xed. Interestingly, the experiment only provided \nevidence for the .rst argument but not for the latter. Critical re.ection on the number of subjects: \nIt is common and appropriate to be critical of controlled experiments with small subject counts, including \nin our case, where the num\u00adber of subjects that were analyzed was only 16. Hence, there might be the \ntendency to argue that (despite the fact that an experiment showed signi.cant results) the results cannot \nbe taken seriously because of the low sample size. While more subjects is always better, given in.nite \ntime and re\u00adsearch dollars, the .eld of statistics has created tests speci.\u00adcally to account for small \nsample sizes. Small samples with too much random variance, by de.nition, show as non\u00adsigni.cant results. \nThus, given that our sample was small, our effect sizes are quite astonishing. Given that our results \nhere, in addition to being signi.cant, also help replicate a previous experiment [20], our result is \ngiven credence. With that said, we obviously think that other research labs can, and should try to replicate \nour .ndings, con.rming or deny\u00ading them on any sample size possible for them to achieve. Insights on \nexperimental design: The experiment gave an\u00adother interesting insight beyond the actual hypotheses being \ntested. For all programming tasks, the effect being detected would have been found even with a simpler \nexperimental de\u00ad 14 It should be noted that the experiment was designed with only one type error .xing \ntask, because the experimenters were convinced that again a large bene.t of the system which reveals \nthe type error via a static type check would show up. sign: the effect for the class identi.cation tasks \nas well as for the extension task was already shown by a between-subject comparison. Additionally, no \ndifferences were shown for the type error .xing task, neither using the between-subject analysis nor \nthe within-subject analysis. This implies that the whole experiment could have been executed with only \nhalf the time required by the subjects, and the effort for de\u00adsigning the experiment (building a new \nAPI for the control group, designing tasks that the expected learning effect is rather small) could have \nbeen much smaller without chang\u00ading the results. Of course, the within-subject measurement gives some \nadditional trust, given that replication is built in to the experimental design. With that said, within-subjects \nexperiments take more time, so those looking to replicate might considering running only one iteration \nof our repeated measures design. One .nal implication of the experiment is that the appli\u00adcation of time \nlimits and the possibility to permit to aban\u00addon programming tasks does not necessarily cause harm. In\u00addeed, \nin our case, it made the experiment less time consum\u00ading, without harming the results. Speci.cally, only \nin one task did the time limit play a major role (for the extension task). Given the time savings, this \nseems acceptable. With that said, a possible negative impact of the time limit was that we were not able \nto consider 8 subjects in the analy\u00adsis, because they have solved too few tasks. Thus, the timing issue \ndid cause a reduction in the statistical power of the results. From our perspective, we still think that \nthis was a reasonable tradeoff. 6. Summary and Conclusion This paper introduced an experiment which studies \nthe ef\u00adfect of generic in comparison to raw types in the program\u00adming language Java. The motivation for \nthis experiment was that there is relatively little empirical knowledge about the bene.t of generics. \nThe experiment, which was performed on 24 subjects (from which 16 subjects were actually used in the \nanaly\u00adsis) concentrated on three different kinds of programming tasks: (1) the use of undocumented APIs, \n(2) the .xing of a type error and (3) the extension of a class hierarchy. The .rst two kinds of programming \ntasks came from different experiments that study the impact of static types in compar\u00adison to dynamic \ntypes, the third kind of programming task was an adaption of a proposal about the use of generic types \nbeing made in literature. The primary measurement in the experiment was development time until the tasks \nhave been completed correctly, given a test suite. The result of the ex\u00adperiment is the following: 1. \nGeneric types improve usability of undocumented APIs in comparison to raw types. 2. With respect to \ntype error .xing, no signi.cant difference between raw and generic types was measured.   3. With respect \nto extensibility, the experiment showed a negative impact of generic types for the implementation of \nthe strategy design pattern. Consequently, a conservative interpretation of the experi\u00adment is that generic \ntypes can be considered as a tradeoff between the positive documentation characteristics and the negative \nextensibility characteristics. The exciting part of the study is that it showed a situation where the \nuse of a (stronger) static type system had a negative impact on the development time while at the same \ntime the expected bene\u00ad.t the reduction of type error .xing time did not appear. We think that such \ntasks could help in future experiments in identifying the impact of type systems. While our experiment \nattempted to isolate the program\u00adming language features from the environment in which they exist (e.g., \nIDEs), the impact of tooling within a modern de\u00advelopment environment is important and also needs to \nbe studied. From our point of view, it seems plausible that soft\u00adware tools such as IDEs bene.t humans \nusing type systems, given that they include modern features like code comple\u00adtion, refactoring, or other \nfeatures. Studies involving tooling are in progress. In conclusion, this study contributes a formal controlled \nexperiment on the impact of static type systems, including what may be the .rst insights into the impact \nof generic types on human users. From our perspective, it would be desirable if these kinds of experiments \nwould be performed more often, so that we can gather an ever-increasing knowledge base of data. Acknowledgments \nWe would like to thank the volenteers who participated in the experiment and who made this work possible. \nAddition\u00adally, we would like to thank Andreas Ste.k, University of Nevada, Las Vegas, for his time and \neffort he spent on edit\u00ading and correcting the paper. References [1] AR NOL D, K. Generics considered \nharmful. blog en\u00adtry available at http://weblogs.java.net/blog/arnold/archive/ 2005/06/generics_consid_1.html, \nlast visit Dec 2012, 2005. [2] BR AC HA, G., OD ER S K Y, M., STO U TAM I RE , D., A N D WADL E R, P. \nMaking the future safe for the past: Adding genericity to the java programming language. In Proceedings \nof the 1998 ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages &#38; Applications \n(OOPSLA 98), Vancouver, British Columbia, Canada, October 18-22, 1998 (1998), ACM, pp. 183 200. [3] BRU \nC E , K. B. Foundations of object-oriented languages: types and semantics. MIT Press, Cambridge, MA, \nUSA, 2002. [4] CA N N I N G , P., CO O K , W., HIL L , W., OLT H OFF , W., A N D MI TC H E L L, J. C. \nF-bounded polymorphism for object\u00adoriented programming. In Proceedings of the fourth interna\u00adtional conference \non Functional programming languages and computer architecture (New York, NY, USA, 1989), FPCA 89, ACM, \npp. 273 280. [5] CA R D ELL I, L., AND WE G N E R, P. On understanding types, data abstraction, and polymorphism. \nACM Comput. Surv. 17, 4 (Dec. 1985), 471 523. [6] DA LY, M. T., SA Z AWAL, V., AND FO S TER , J. S. Work \nin progress: an empirical study of static typing in ruby. Workshop on Evaluation and Usability of Programming \nLanguages and Tools (PLATEAU),Orlando, October 2009 (2009). [7] ENDR IKAT, S., A N D HA N E N BE R G \n, S. Is aspect-oriented pro\u00adgramming a rewarding investment into future code changes? A socio-technical \nstudy on development and maintenance time. In Proceedings of the 2011 IEEE 19th Interna\u00adtional Conference \non Program Comprehension (Kingston, CA, 2011), ICPC 11, IEEE Computer Society, pp. 51 60. [8] FE IG EN \nS PAN, J., K\u00c4 S TN ER , C., LI EB IG, J., APE L , S., AND HA N E N BER G , S. Measuring programming experience. \nIn IEEE 20th International Conference on Program Comprehen\u00adsion, ICPC 2012, Passau, Germany, June 11-13, \n2012 (2012), pp. 73 82. [9] GA M MA, E., HE LM , R., JO H N SON , R., A N D VL ISSI D E S , J. Design \npatterns: elements of reusable object-oriented soft\u00adware. Addison-Wesley Professional, 1995. [10] GA \nN NON , J. D. An experimental evaluation of data type conventions. Commun. ACM 20, 8 (1977), 584 595. \n[11] GRAV ETT E R, F., A ND WA L L NAU, L. Statistics for the Be\u00adhavioral Sciences. Cengage Learning, \n2008. [12] HA N E N BER G , S. Doubts about the positive impact of static type systems on programming \ntasks in single devel\u00adoper projects -an empirical study. In ECOOP 2010 -Object-Oriented Programming, \n24th European Conference, Maribor, Slovenia, June 21-25, 2010. Proceedings (2010), LNCS 6183, Springer, \npp. 300 303. [13] HA N E N BER G , S. An experiment about static and dynamic type systems: Doubts about \nthe positive impact of static type systems on development time. In Proceedings of the ACM international \nconference on Object oriented programming systems languages and applications (New York, NY, USA, 2010), \nOOPSLA, ACM, pp. 22 35. [14] HA N E N BER G , S. A chronological experience report from an initial experiment \nseries on static type systems. In 2nd Workshop on Empirical Evaluation of Software Composition Techniques \n(ESCOT) (Lancaster, UK, 2011). [15] HA N E N BER G , S., KL E INSC H M AG E R , S., AND JO S U P E IT-WALTER \n, M. Does aspect-oriented programming increase the development speed for crosscutting code? an empirical \nstudy. In Proceedings of ESEM (2009), pp. 156 167. [16] H\u00d6 S T, M., RE G N E L L, B., A N D WO HL I N \n, C. Using students as subjects a comparative study of students and profession\u00adals in lead-time impact \nassessment. Empirical Softw. Engg. 5, 3 (2000), 201 214. [17] IG AR A S H I, A., PI ER CE , B. C., A \nN D WA D LER , P. Feather\u00adwieght java: A minimal core calculus for java and gj. In Pro\u00adceedings of the \n1999 ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages &#38; Applications (OOPSLA \n99), Denver, Colorado, USA, November 1-5, 1999  (1999), ACM, pp. 132 146. [18] JU R I S TO , N., A N \nD MO RE N O, A. M. Basics of Software Engineering Experimentation. Springer, 2001. [19] KLEI N S CHM \nAGE R, S., A ND HANENB E R G, S. How to rate programming skills in programming experiments? A prelim\u00adinary, \nexploratory study based on university marks, pretests, and self-estimation. In Workshop PLATEAU at SPLASH \n(Portland, USA, October 2011), pp. 15 24. [20] KLEI N S CHM AGE R, S., HA N E N B ER G , S., ROB B E \nS , R., TA N -T E R, \u00c9., A N D ST E FI K , A. Do static type systems improve the maintainability of software \nsystems? An empirical study. In IEEE 20th International Conference on Program Compre\u00adhension, ICPC 2012, \nPassau, Germany, June 11-13 (2012), pp. 153 162. [21] MAY ER , C., HA N E N BER G , S., RO BB E S , R., \nTA NT E R, \u00c9., AN D ST EFI K , A. An empirical study of the in.uence of static type systems on the usability \nof undocumented software. In Proceedings of the 27th Annual ACM SIGPLAN Conference on Object-Oriented \nProgramming, Systems, Languages, and Applications, OOPSLA 2012, part of SPLASH 2012, Tucson, AZ, USA, \nOctober 21-25, 2012 (2012), ACM, pp. 683 702. [22] ME Y E ROV IC H , L. A., A ND RAB K IN, A. Empirical \nanaly\u00adsis of programming language adoption. In Proceedings of the Conference on Object-Oriented Programming \nSystems, Lan\u00adguages &#38; Applications (OOPSLA 13), Indianapolis, Indiana, USA, October 26 31, 2013 (2013), \nACM. [23] MI LNER , R. A theory of type polymorphism in programming. Journal of Computer and System Sciences \n17 (1978), 348 375. [24] NA F TA LI N , M., AND WA D L E R , P. Java generics and collec\u00adtions. O Reilly, \n2006. [25] PAR N IN, C., BIR D , C., A ND MU RPH Y-HI L L , E. R. Java generics adoption: how new features \nare introduced, cham\u00adpioned, or ignored. In Proceedings of the 8th International Working Conference on \nMining Software Repositories, MSR 2011 (Co-located with ICSE), Waikiki, Honolulu, HI, USA, May 21-28, \n2011, Proceedings (2011), IEEE, pp. 3 12. [26] PIER C E , B. C. Types and programming languages. MIT \nPress, Cambridge, MA, USA, 2002. [27] PREC H E LT, L. An empirical comparison of seven program\u00adming languages. \nIEEE Computer 33 (2000), 23 29. [28] PREC H E LT, L., AN D TIC H Y, W. F. A controlled experiment to \nassess the bene.ts of procedure argument type checking. IEEE Trans. Softw. Eng. 24, 4 (1998), 302 312. \n[29] SP I ZA, S., A ND HAN ENB E R G, S. Type names without static type checking already improve the \nusability of apis as long as the type names are correct: An empirical study submitted to gpce 2013. \n[30] ST E FI K , A., , A N D SI EB E RT, S. An empirical investigation into programming language syntax. \nACM Transactions on Computing Education (2013, In Print). [31] ST E I N B ER G , M. What is the impact \nof static type sys\u00adtems on maintenance tasks? An empirical study of differences in debugging time using \nstatically and dynamically typed languages. Master Thesis, Institute for Computer Science and Business \nInformation Systems, University of Duisburg-Essen, http://dawis2.cs.uni-due.de/ests/2011 MA MarvinSteinberg \nSH.pdf, January, 2011. [32] ST E INB E R G, M., A N D HA N E N B E RG , S. What is the impact of static \ntype systems on debugging type errors and semantic errors? -submitted. [33] ST UC H L I K , A., AND HA \nNE N B ER G , S. Static vs. dynamic type systems: An empirical study about the relationship be\u00adtween \ntype casts and development time. In Proceedings of the 7th symposium on Dynamic languages (Portland, \nOregon, USA, 2011), DLS 11, ACM, pp. 97 106. [34] WO HL IN , C., RU N E S O N , P., H\u00d6 ST, M., OH L S \nS O N , M. C., RE G N E L L, B., AND WE S S L \u00c9 N , A. Experimentation in soft\u00adware engineering: an introduction. \nKluwer Academic Pub\u00adlishers, Norwell, MA, USA, 2000. A. Appendix Solution code for CITS and TE F TS \nThis section described the code being given to or expected from the subjects. For the CITs as well as \nT E F Ts the text that described the programming tasks for the subjects is given. Additionally, this \nsection show excerpts from the classes and interfaces which were required to know in order to solve the \nprogramming tasks. For the API only a small excerpt is given, because the whole source code of the API \nis too large in order to be printed in a paper: only those methods are described which are required for \nthe solution. The complete source code delivered to the subjects can be downloaded from http://dawis2.cs.uni-due.de/ests/esgt/. \nIn order to ease the presentation of the code, we omitted keywords such as private, public, etc.   \n A.1 CIT solutions Figure 3 shows for each CIT the task desciptions that was given to the subjects and \na possible source code that solves each task (left column). The right column describes the API that was \nrequired to solve the tasks. Note that for each programming task a new set of classes in the API was \ngiven. The motivation for this is, that otherwise the learning effect for each subject would be too high. \nFor example, in CIT 2 there is a generic class called Couple which just stores a pair of objects (whose \ntypes are generic). There is a similar class in CIT 3 which does exactly the same thing but which has \na different name. Because of lack of space, only the code for the generic programming tasks is given \n the code for the raw typed pro\u00adgramming tasks is equivalent without any type parameters in the code \n(but for the different problem domain).  A.2 Solution code for T E F Ts Figure 4 contains the text the \nIDE delivered to the subjects at the beginning of the task.  Possible solution code for programming \ntask Required classes and interfaces (excerpt) CIT 1 / * Ad d a l l t h e m e m b e r s t o t h e e x \ni s t i n g c l u b m e mb er l i s t . T h e new a n d o l d m e m b e r s a r e e n c l o s e d b y \nt h e A n u a l R e p o r t U p d a t e o b j e c t . S p e c i a l A d v i c e : Do n o t u s e l o \no p s . T h e y a r e n o t n e e d e d t o s o l v e t h i s t a s k a n d s h o u l d n o t b e u s \ne d ! * / s t a t i c v o i d a d d M e m b e r s ( A n u a l R e p o r t s ) { S e t < P e r s o n > \ns e t = s . g e t U p d a t e ( ) . f o u r ( ) . g e t L e f t S e t ( ) ; S e t < P e r s o n > t a \nr g = s . g e t U p d a t e ( ) . t w o ( ) . g e t M e m b e r s ( ) ; t a r g . a d d A l l ( s e t \n) ; } c l a s s A n u a l R e p o r t { . . . p u b l i c F o u r < S e a s o n T o k e n , C l u b , \nC o n f e r e n c e L o c a t i o n , T u p l e S e t < P e r s o n , R e g i s t r a t i o n >> g e \nt U p d a t e ( ) { . . . } . . . } c l a s s F o u r <N1 , N2 , N3 , N4> . . . { . . . p u b l i c N2 \nt w o ( ) { r e t u r n t w o ; } p u b l i c N4 f o u r ( ) { r e t u r n f o u r ; } . . . } c l a \ns s T u p l e S e t <LEFT , RIGHT> . . . { p u b l i c S e t <LEFT> g e t L e f t S e t ( ) { . . . } \n. . . } c l a s s C l u b . . . { p u b l i c S e t < P e r s o n > g e t M e m b e r s ( ) { . . . } \n. . . } i n t e r f a c e S e t <A> . . . { . . . b o o l e a n a d d A l l ( S e t < ? e x t e n d s \nA> c ) ; . . . } CIT 2 / * T h e A c t i v i t y B u n d l e c o n t a i n s a n a c t i v i t y o v \ne r v i e w a n d a n u m b e r o f d e p a r t m e n t s . Add t h e f i r s t d e p a r t m e n t t \no t h e a c t i v i t y o v e r v i e w . */ s t a t i c v o i d a s s i g n C l u b D e p a r t m e \nn t ( A c t i v i t y B u n d l e a b ) { c l a s s A c t i v i t y B u n d l e { . . . C o u p l e < \nA c t i v i t y O v e r v i e w < S p o r t s > , E n t r y < C l u b D e p a r t m e n t >> e n t r \ny ( ) { . . . } } c l a s s C o u p l e <A , B> { . . . A g e t A ( ) { r e t u r n a ; } B g e t B ( \n) { r e t u r n b ; } . . . } c l a s s E n t r y <E> { . . . C l u b D e p a r t m e n t c d = a b . \ng e t U p d a t e ( ) . g e t B ( ) . r e t r i e v e ( ) ; a b . g e t U p d a t e ( ) . g e t A ( ) \n. g e t A c t i v i t i e s ( ) . a p p e n d ( c d ) ; } p u b l i c E r e t r i e v e ( ) { . . . } \n. . . } c l a s s A c t i v i t y O v e r v i e w <A> { . . . C l u s t e r < A c t i v i t y < A>> g \ne t A c t i v i t i e s ( ) { . . . } . . . } c l a s s C l u s t e r <S> . . . { . . . p u b l i c b \no o l e a n a p p e n d ( S a ) { . . . } . . . } CIT 3 / * S e t t h e f i r s t c o m p e t i t o r \nf r o m t h e c o m p e t i t o r o v e r v i e w t o t h e t o u r n a m e n t a w a r d . T h e c o \nm p e t i t o r o v e r v i e w a s w e l l a s t h e t o u r n a m e n t a w a r d a r e m a i n t a \ni n e d b y t h e d e l i v e r e d T o u r n a m e n t R e p o r t o b j e c t . * / s t a t i c v o \ni d a d d Te a mT o Aw a r d ( T o u r n a m e n t R e p o r t r e p ) { c l a s s T o u r n a m e n \nt R e p o r t { R e f e r e n c e <Award < R e f e r e n c e < T r o p h y , C o m p e t i t o r > > \n, R e f e r e n c e < E n t r y < C o m p e t i t o r > , N e s t I n d e x < E n t r y < C o m p e t \ni t o r >>>> g e t R e p o r t ( ) { . . . } . . . } . . . } c l a s s R e f e r e n c e <A , B> { . \n. . A g e t A ( ) { . . . } C o m p e t i t o r c= r e p . g e t R e p o r t ( ) . g e t B ( ) . g e \nt A ( ) . g e t S u b j e c t ( ) ; r e p . g e t R e p o r t ( ) . g e t A ( ) . g e t D e t a i l s \n( ) . s e t B ( c ) ; } B g e t B ( ) { . . . } . . . } c l a s s Award <A> { . . . A g e t D e t a i \nl s ( ) { . . . } . . . } p u b l i c c l a s s E n t r y <E> { . . . E r e t r i e v e ( ) { . . . } \n. . . } Figure 3. Task descriptions, possible solutions, and excerpt from the API for the CIT tasks \nThere is a small difference in the words, because for the raw typed task there is no compiler error and \nthe test cases need to be executed. The second column contains the error messages shown to the subjects. \nIn both cases Java generates the error messages: for the generic typed version the message is a compiler \nerror, for the raw typed version it is a message from the Java Vir\u00adtual Machine (JVM). The error message \nfrom the virtual ma\u00adchine also contains the complete stack trace which is omitted here in this paper \ndue to lack of space. Both messages were slightly different presented to the subjects: the compiler er\u00adror \nwas shown in a console-like frame in the IDE, the JVM message was shown in a JUnit window outside the \nIDE (but which did not permit subjects to navigate though the code by clicking on stack trace elements). \nFor the generic typed version, the code at the beginning could not be executed (be\u00adcause of the presence \nof a type error). The last column shows the source code in the original form (without the comments) that \nneeded to be corrected by the subjects in order to solve the programming tasks.  In both cases, one \nline of code the construction of the strategy object needed to be replaced by a different strategy \nthat matches the context. For the generic typed version, the context object has the type Running that \nrequires a strategy object of type RunningLocation. The raw version has the context object of type DversizedProduct \nthat requires a strategy object of type DversizedDelivery. In both versions, the context object has a \nreference to the strategy object: in the generic typed version, the reference to the strategy is declared \nin the generic class SportEvent, in the raw typed version it is declared in class Product. The signature \nof the strategy method (called execute in the book by Gamma et al., [9]) differs in both versions due \nto differences in the type system. While for the generic typed version the strategy method reserveDateForEvent \nhas a generic parameter type, the raw version has a non-parametric one (which is the type Product the \nsupertype for all context classes). The raw typed version does not cause a type error, be\u00adcause the strategy \nis declared of type Delivery (which is the superclass of all strategies).  Task decription Error message \nExpected correction and related classes a) On executing main.test.FunctionalTest a compile error occurs. \nPlease Fix it! Hint: run the test suite with Test Project button to see the error (compile time error \n Java compiler message) StagedRunningLocation.java:25: error: method setLocation in class SportEvent<P> \ncannot be applied to given types; o.setLocation(temp); required: Location<Running> found: StagedRunningLocation \nreason: actual argument StagedRunningLocation cannot be converted to Location<Running> by method invocation \nconversion where P is a type-variable: P extends SportEvent<P> declared in class SportEvent 1 error v \no i d s e p a r a t e T o S i n g l e R u n s ( . . . ) { R u n n i n g o = . . . . . . S t a g e d R \nu n n i n g L o c a t i o n t = new S t a g e d R u n n i n g L o c a t i o n ( ) ; / / N e e d s t o \nb e c o r r e c t e d t o / / R u n n i n g L o c a t i o n t e m p = / / ne w R u n n i n g L o c a \nt i o n ( ) ; o . s e t L o c a t i o n ( t e m p ) ; } / / T h e c o n t e x t c l a s s e s c l a s \ns R u n n i n g e x t e n d s S p o r t E v e n t < R u n n i n g > { . . . } a b s t r a c t c l a s \ns S p o r t E v e n t < P e x t e n d s S p o r t E v e n t <P >> { L o c a t i o n <P> l o c a t i o \nn ; . . . } / / T h e s t r a t e g y c l a s s e s c l a s s R u n n i n g L o c a t i o n e x t e n \nd s L o c a t i o n < R u n n i n g > { . . . } a b s t r a c t c l a s s L o c a t i o n < SE e x t \ne n d s S p o r t E v e n t <SE>> { a b s t r a c t v o i d r e s e r v e D a t e F o r E v e n t ( SE \np ) ; . . . } b) The test case still shows errors in the project. Please Fix it! Hint: run the test suite \nwith Test Project (run time error Java run time message) test(main.test.FunctionalTest): main.product.OversizedProduct \np u b l i c v o i d s p l i t ( . . . ) { O v e r s i z e d P r o d u c t o = . . . ; K i t c h e n D \ne l i v e r y t =new K i t c h e n D e l i v e r y ( ) ; / / N e e d s t o b e c o r r e c t e d t o \n/ / O v e r s i z e d D e l i v e r y t e m p = button to see the error cannot be cast to main.product.KitchenProduct \nat KitchenDelivery.deliver() [...rest of stack trace...] / / new O v e r s i z e d D e l i v e r y ( \n) ; o . s e t D e l i v e r y ( t ) ; } / / C o n t e x t c l a s s e s c l a s s K i t c h e n P r o \nd u c t e x t e n d s O v e r s i z e d P r o d u c t { . . . } c l a s s O v e r s i z e d P r o d u \nc t e x t e n d s P r o d u c t { . . . } c l a s s P r o d u c t { D e l i v e r y d e l i v e r y ; \n. . . } p u b l i c c l a s s P r o d u c t { p r i v a t e D e l i v e r y d e l i v e r y ; } / / S \nt r a t e g y c l a s s e s c l a s s K i t c h e n D e l i v e r y e x t e n d s O v e r s i z e d D \ne l i v e r y { . . . } p u b l i c c l a s s O v e r s i z e d D e l i v e r y e x t e n d s D e l i \nv e r y { a b s t r a c t v o i d d e l i v e r ( P r o d u c t p ) ; . . . } Figure 4. Task descriptions, \nshown errors, possible solution source codes and related classes for the TE F Ts for a) the generic types, \nand b) the raw types  \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Type systems that permit developers to express themselves more precisely are one of the primary topics in programming language research, as well as in industrial software development. While it seems plausible that an expressive static type system increases developer productivity, there is little empirical evidence for or against this hypothesis. Generic types in Java are an example: as an extension of Java's original type system, some claim that Java 1.5 improves the type system's \"expressiveness.\" Even if this claim is true, there exists little empirical evidence that claimed expressiveness leads to a measurable increase in developer productivity. This paper introduces an experiment where generic types (in comparison to raw types) have been evaluated in three different directions: (1) the documentation impact on undocumented APIs, (2) the time required for fixing type errors, and (3) the extensibility of a generic type hierarchy. The results of the experiment suggest that generic types improve documentation and reduce extensibility -- without revealing a difference in the time required for fixing type errors.</p>", "authors": [{"name": "Michael Hoppe", "author_profile_id": "81371591644", "affiliation": "University of Duisburg-Essen, Essen, Germany", "person_id": "P4290398", "email_address": "michael.hoppe@stud.uni-due.de", "orcid_id": ""}, {"name": "Stefan Hanenberg", "author_profile_id": "81100540390", "affiliation": "University of Duisburg-Essen, Essen, Germany", "person_id": "P4290399", "email_address": "stefan.hanenberg@icb.uni-due.de", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509528", "year": "2013", "article_id": "2509528", "conference": "OOPSLA", "title": "Do developers benefit from generic types?: an empirical comparison of generic and raw types in java", "url": "http://dl.acm.org/citation.cfm?id=2509528"}