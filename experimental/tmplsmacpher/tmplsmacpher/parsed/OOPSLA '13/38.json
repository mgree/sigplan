{"article_publication_date": "10-29-2013", "fulltext": "\n Input-Covering Schedules for Multithreaded Programs Tom Bergan Luis Ceze Dan Grossman University of \nWashington, Department of Computer Science &#38; Engineering {tbergan,luisceze,djg}@cs.washington.edu \nAbstract We propose constraining multithreaded execution to small sets of input-covering schedules, which \nwe de.ne as follows: given a program P, we say that a set of schedules S covers all inputs of program \nP if, when given any input, P s execution can be constrained to some schedule in S and still produce \na semantically valid result. Our approach is to .rst compute a small S for a given program P, and then, \nat runtime, constrain P s execution to always follow some schedule in S, and never deviate. We have designed \nan algorithm that uses symbolic execution to systematically enumerate a set of input-covering schedules, \nS. To deal with programs that run for an unbounded length of time, we partition execution into bounded \nepochs, .nd input-covering schedules for each epoch in isolation, and then piece the schedules together \nat runtime. We have im\u00adplemented this algorithm along with a constrained execution runtime for pthreads \nprograms, and we report results. Our approach has the following advantage: because all possible runtime \nschedules are known a priori, we can seek to validate the program by thoroughly verifying each sched\u00adule \nin S, in isolation, without needing to reason about the huge space of thread interleavings that arises \ndue to conven\u00adtional nondeterministic execution. Categories and Subject Descriptors D.1.3 [Programming \nLanguages]: Concurrent Programming; D.3.4 [Program\u00adming Languages]: Processors Compilers, Run-time envi\u00adronments \nKeywords static analysis; symbolic execution; constrained execution; determinism 1. Introduction Multithreaded \nprograms are notoriously dif.cult to test and verify. In addition to the already daunting task of reason- \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. Copyrights for components of \nthis work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To \ncopy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. ing about program behavior over \nall possible inputs, testing and veri.cation tools must reason about a large number of possible thread \ninterleavings for each input the number of possible interleavings grows exponentially with the length \nof a program s execution. Tools can systematically explore the interleaving space in part, but in practice, \nthe interleaving space is too massive to be explored exhaustively [8, 31]. We avoid this problem by constraining \nexecution to a small set of input-covering schedules. Given a program P, we say that a set of schedules \nS covers the program s inputs if, for all inputs, there exists some schedule S . S such that P s execution \ncan be constrained to S and still produce a semantically valid result. In our system, a schedule is simply \na partial order of dynamic instances of program statements paired with thread ids, i.e., a happens-before \ngraph. Given a program P, we .rst enumerate a small input\u00adcovering set S using symbolic execution. Then, \nwe attach a custom runtime system to P that constrains execution to follow only those schedules in S. \nThis combination of pro\u00adgram and runtime system is essentially a new program, P', that accepts all possible \ninputs and produces semantically correct behavior, like the original program, but uses fewer schedules. \nWe always run the constrained program P' in de\u00adployment. The result is that S contains the complete set \nof schedules that might be encountered during deployment this simpli.es the veri.cation problem by reducing \nthe num\u00adber of schedules that must be considered. It is not obvious that small sets of input-covering \nsched\u00adules should exist for realistic multithreaded programs. The key word is small an input-covering \nset S is of no help when it is so intractably large that it cannot be enumer\u00adated in a reasonable time. \nAn important contribution of this work is de.ning S in a way that makes .nding small input\u00adcovering sets \nmore tractable. Notably, programs that run for unbounded periods of time can require unboundedly many \nschedules, making the set S intractably large. We avoid this problem by partitioning execution into bounded \nepochs we .nd input-covering schedules for each epoch in isolation, and then piece those schedules together \nat runtime. 1.1 System Overview OOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, USA. Enumerating \nS. We use an algorithm based on symbolic Copyright is held by the owner/author(s). Publication rights \nlicensed to ACM. execution to systematically enumerate input-covering sched- ACM 978-1-4503-2374-1/13/10. \n. . $15.00. http://dx.doi.org/10.1145/2509136.2509508 ules for a given program. Figure 1 gives a demonstration. \nOn  1 input X 2 global Lock A,B 3 4 Thread 1 Thread 2 5 for (i in 1..5) { for (i in 1..5) { 6 if (X \n== 0) { if (X == 0) { 7 lock(A) lock(A) 8 unlock(A) unlock(A) 9 } else { } else { 10 lock(B) lock(B) \n11 unlock(B) unlock(B) 12 } } 13 } }  Figure 1. On the left is a simple multithreaded program. On the \nright is one set of input-covering schedules for the program. the right side of Figure 1 is a set of \ninput-covering sched\u00adules, S, that our algorithm might produce when given the program on the left. Each \nschedule in S is paired with an in\u00adput constraint that describes the set of inputs under which the schedule \ncan be followed. Schedules are speci.ed as happens-before orderings of synchronization statements. Runtime \nSystem. At runtime, we constrain execution to follow schedules in S. We have implemented a custom runtime \nsystem that captures the program s inputs, .nds a pair (I,S) . S such that the program s inputs satisfy \ninput constraint I, and then constrains execution to S, ensuring that execution never deviates from S. \nVeri.cation Strategy. Finally, and most importantly, testing and veri.cation become simpler under the \nassump\u00adtion that programs always execute using our custom run\u00adtime system. Given this assumption, the \ninput-covering set S contains the complete set of schedules that might be fol\u00adlowed at runtime, and as \na result, veri.cation tools can focus on schedules in S only, avoiding the need to reason about a massive \nnondeterministic interleaving space. For a simple example, consider deadlocks. We can de\u00adtermine if a \nschedule deadlocks by simply looking at it if any thread does not terminate with an exit statement, then \nthe schedule deadlocks. We can perform this check for each schedule in S independently. If a deadlocking \nschedule is found, we can use the schedule s associated input constraint to present the programmer with \na concrete input and sched\u00adule that leads to deadlock. If no deadlocking schedules are found, we have \nproven that we will never encounter a dead\u00adlock when execution is constrained by our runtime system. \nMore generally, we can reason about each schedule in iso\u00adlation by serializing the original multithreaded \nprogram into |S| single-threaded programs, where each single-threaded program Pi is constructed by serializing \nthe original mul\u00adtithreaded program P according to schedule Si . S. This re\u00adduces the worst-case number \nof possible program behaviors from O(k! \u00b7 i) to O(|S| \u00b7 i), where k is the length of execu\u00adtion and i \nis the number of possible inputs. This approach is called schedule specialization and it has been shown \nto have real bene.ts [39]. For example, consider the following code: Thread 1 Thread 2 lock(L) lock(L) \nif (x%2!=0) if (x%2!=0) x++ fail() unlock(L) unlock(L) There is an assertion failure in T2 when it executes \nbefore T1 with an odd value for x. This bug can be dif.cult to .nd in conventional systems because it \ndepends on speci.c combi\u00adnations of input (x) and schedule (ordering of lock acquires). Our approach \ncomputes just one schedule for this code snip\u00adpet (say, T1 before T2), which reduces the veri.cation \nprob\u00adlem from a hard thread interleaving problem to a simpler (but still dif.cult) single-threaded reachability \nproblem. We refer to Wu et al. [39] and Yang et al. [41] for more detailed arguments in favor of schedule \nspecialization. Assumptions. Our schedule enumeration algorithm as\u00adsumes data race freedom. When this \nassumption is broken, we do not compute a true input-covering set execution may diverge from the expected \nschedule after a data race. We make this assumption to simplify our analysis in a number of important \nways that will be mentioned later. We also assume that each program has a bounded number of live threads \nat any given moment. If the number of live threads is input-dependent, we expect the programmer to supply \nan upper bound for that input. Bounding the number of threads allows us to represent each thread explicitly \nin the schedule, as shown in Figure 1.  1.2 Comparison to Related Work Deterministic Execution. Our \nruntime system selects schedules deterministically for each input, giving our system all the bene.ts \nof determinism that have been championed by many prior authors (see [3] for a summary). Since we as\u00adsume \ndata race freedom, we provide weak determinism as in Kendo [34]. However, our primary goal is not determinism \nper se we could just as easily record multiple schedules for each input constraint in S and randomly \nselect from those schedules at runtime. This added .exibility increases schedule diversity, which has \npotential bene.ts for security, fault-tolerance, and performance [3].  Schedule Memoization. Our system \ngeneralizes ideas introduced by TE R N [11] and PE R E G R I N E [12]. Those sys\u00adtems memoize schedules \nfrom a few tested inputs, so they provide best-effort schedule memoization only, while our system enumerates \na complete input-covering set. Comput\u00ading input-covering sets requires solving a number of techni\u00adcal \nchallenges not faced by any prior system.  1.3 Contributions and Outline The primary contribution of \nthis paper is the identi.cation of the input-covering schedules problem, which to our knowl\u00adedge has \nnot been introduced previously. Our solution to this problem depends on a carefully determined representation \nof schedules that we describe in \u00a72. Additionally, we make the following contributions: We have designed \nan algorithm for .nding input-covering schedules (\u00a73). We use a number of optimizations to avoid combinatorial \nexplosion problems (\u00a74) and to im\u00adprove the performance and utility of epochs (\u00a75).  We have implemented \nour algorithm on the Cloud9 [7] symbolic execution engine, and we have implemented a runtime system that \nconstrains execution to input\u00adcovering schedules produced by our algorithm (\u00a76). Our implementation targets \nC programs that use pthreads.  We have performed the .rst empirical evaluation to ad\u00address the fundamental \nquestion: how large are sets of input-covering schedules? (\u00a77) We organize our eval\u00aduation as a set of \ncase studies to carefully characterize the program analysis challenges inherent to enumerat\u00ading input-covering \nschedules for realistic multithreaded C programs.  This paper focuses on the fundamental problem of \nenu\u00admerating input-covering schedules. We built a runtime sys\u00adtem to demonstrate its feasibility and \nto check the correct\u00adness of our schedule enumeration algorithm, but have not yet optimized the runtime \nsystem thoroughly. We also have not yet explored veri.cation strategies in any signi.cant detail, though \nwe have implemented a simple deadlock checker that we describe in \u00a76.3. We end this paper with a formal \nstate\u00adment of the guarantees provided by our system (\u00a78), a more thorough discussion of related work \n(\u00a79), and concluding re\u00admarks (\u00a710). 2. Representing Schedules We represent each schedule as a happens-before \ngraph over a .nite execution trace, where graph nodes are labeled by the triple (program-counter, thread-id, \ndynamic-counter) and edges are induced from program order and synchroniza\u00adtion in the usual way, such \nas between release and acquire operations on the same lock. The program-counter label represents a synchronization \nstatement in the program, such as a call to pthread mutex lock, and the pair (thread-id, dynamic-counter) \nis a Lamport timestamp [26]. Notice that ordinary memory accesses are not included in the happens\u00adbefore \ngraph, as we assume data race freedom. We return to the example in Figure 1. On the left is a sim\u00adple \nprogram in which each thread acquires a different global lock depending on the value of the input X. \nA conventional nondeterministic execution might follow one of 240 possible schedules (5! when X==0, and \nanother 5! when X!=0). How\u00adever, just two schedules are necessary to cover all inputs for this program \none schedule for X==0, and another for X!=0. This is illustrated by the right side of Figure 1, which \nshows one possible set of input-covering schedules, S. (The sched\u00adules have been abbreviated for space.) \nImportantly, for each pair (I,S) . S, the constraint I should include only those conditions that affect \nwhether the schedule S can be followed. That is, constraint I should be a weakest precondition of the \nschedule S. For example, suppose we modify the program in Figure 1 to perform a complex computation in \neach loop iteration. As long as this computation does not mutate X or perform synchronization, the set \nof input-covering schedules shown in Figure 1 will be equally correct for our modi.ed program. The above \nrepresentation works well for programs that read their entire input up front (e.g., from the command \nline or a .le) and then perform a bounded-length computation on that input. We extend the above representation \nto support unbounded-length programs in \u00a72.1. To support programs that read inputs continuously, we follow \nthe suggestion made by TE R N [11] to represent schedules using a schedule tree in which nodes are program \nstatements that read new input and edges are partial schedules that start at an input statement and end \nat either another input statement or program exit. As most details of this approach are prior work, from \nhere forward we make the simplifying assumption that programs read all inputs at program entry. We will \nreturn to continuous inputs in \u00a76.2. 2.1 Bounded Epochs We support programs of unbounded length by partitioning \nexecution into bounded epochs. In practice, we care not only about programs of truly unbounded length, \nbut also about programs that execute for a very long time. For example, consider the following simple \nprogram with two threads: Thread 1 Thread 2 for (i in 1..X) { for (i in 1..Y) { lock(L) lock(L) unlock(L) \nunlock(L) } } If X and Y are program inputs, then any set of input-covering schedules must have a unique \nschedule for each pair (X,Y). If X and Y are 32-bit integers, there are 264 possible inputs, so any set \nof input-covering schedules must contain 264 total schedules. Equally problematic: the longest of these \nschedules must contain 264 total synchronization operations.  Our basic idea is to de.ne schedules one \nloop iteration at a time. We do this by partitioning the program into bounded epochs that are separated \nby epoch markers. We statically analyze the program to .nd all loops that perform synchro\u00adnization, and \nthen place an epoch marker at the entry of such loops. The details of this process are explained in \u00a73.1. \nIn short, the above program would be annotated as follows: Thread 1 Thread 2 for (i in 1..X) { for (i \nin 1..Y) { epochMarker() epochMarker() lock(L) lock(L) unlock(L) unlock(L) } } Epoch markers act as barriers \nduring program execution, forcing threads to execute in a bulk-synchronous manner. For example, suppose \na program s threads begin executing from some initial state. The threads will execute concur\u00adrently until \neach thread is blocked on synchronization, has terminated, or has reached a future epoch marker (possibly \nthe same epoch marker the thread started at, e.g., if the thread went back around the same loop). This \nquantum of execution corresponds to a single bounded epoch. Execution repeats in this bulk-synchronous \nmanner until all threads terminate. We include is blocked in the end-of-epoch condition to avoid deadlock \nwhen thread T1 attempts to acquire a lock that is held by T2 while T2 is stalled at an epoch marker. \nNote that, in practice, we can use loop unrolling to reduce the frequency of epoch markers (see \u00a75). \nWe now require a set of input-covering schedules for each bounded epoch. A bounded epoch E is named by \na list of pairs (pci, callstacki), where pci represents the current pro\u00adgram counter of thread Ti (i.e., \nthe pc of an epoch marker) and callstacki is a list of return addresses that represents the calling context. \nOur algorithm, de.ned in \u00a73, enumer\u00adates all reachable bounded epochs E and computes an input\u00adcovering \nset SE for each E . E. The initial bounded epoch starts at program entry, and its inputs are the program \ns in\u00adputs. All other epochs start from a point in the middle of a program s execution. The input to these \nepochs is, po\u00adtentially, the entire state of memory, which introduces chal\u00adlenges for our runtime system \nthat we will address in \u00a76.2.  2.2 Discussion Bounded epochs make an intractable problem tractable they \nlimit combinatorial explosion by bounding both the length of each computed schedule as well as the total \nnumber of schedules but they introduce necessary approximations, as we will demonstrate in \u00a73.3. Further, \nbounded epochs do not eliminate all causes of explosion in the size of S. For example, consider a thread \nthat determines which locks to acquire using a sequence of conditionals as in the following: In this \ncase, the set of locks acquired by thread T1 is uniquely determined by the value of the bitvector X. \nIf X has 32 bits, any set of input-covering schedules must have 232 unique schedules. This is a source \nof explosion in the size of S that we can think of no good way to eliminate. Since our under\u00adlying problem \nis undecidable, anyway, we focus our current work on programs without such pathological behavior. Thread \n1 if (X[0]) { if (X[1]) { if (X[n]) { lock(L[0]) lock(L[1]) ... lock(L[n]) unlock(L[0]) unlock(L[1]) \nunlock(L[n]) } } } Challenges. Bounded epochs introduce two challenges that we will address in \u00a75. First, \nhow can epochs be made performant? Since epochs are runtime barriers, a concern is imbalance of work \nacross threads. Second, since schedules terminate at epoch boundaries, how can veri.cation be effective? \nWe observe the following: any bug that can be detected by examining a single point of execution can be \ndetected by examining a single epoch in isolation, perhaps by using schedule specialization on each epoch. \nBugs identi.able from a single point of execution include deadlocks and assertion failures. However, \nas we will describe shortly, some schedules may actually be infea\u00adsible leading to false-positives and \nsome mechanism of detecting those infeasible schedules is desirable. Other bugs can be detected only \nby examining sequences of instructions. Atomicity violations are one such example. To simplify de\u00adtection \nof these bugs, epochs should be long enough so that most buggy instruction sequences will be contained \nwithin either one epoch or one short sequence of epochs. 3. Finding Input-Covering Schedules Our algorithm \nfor enumerating input-covering schedules is shown in Figures 2 5. The input is a program P, and the output \nis a mapping from epochs E . E to a set of input\u00adcoverings schedules SE for each epoch, where E is a \nset of bounded epochs that may be reachable. We .rst invoke PlaceEpochMarkers to instrument the program \nwith epoch markers. We then invoke Search to tra\u00adverse all reachable bounded epochs, starting from an \nini\u00adtial bounded epoch representing the call to main(). For each epoch E, Search invokes SearchInEpoch(E), \nwhich performs a depth-.rst search to enumerate a set of input\u00adcovering schedules for E along with the \nset of epochs reach\u00adable from E. We describe each function below. 3.1 Placing Epoch Markers The basic \nconstraint for epoch marker placement is the fol\u00adlowing: we must ensure there are a bounded number of \nsyn\u00adchronization operations between each epoch marker. This ensures that schedules cannot grow to an \nunbounded length. Na\u00a8ively, we could satisfy this requirement by plac\u00ading epoch markers in all loops \nthat perform synchroniza\u00ad  1 PlaceEpochMarkers(p: Program) { 2 covered = {\"epochMarker\",\"pthread_barrier_wait\"} \n3 worklist = {all fns that directly perform sync} 4 while (!worklist.empty()) { 5 F = worklist.popfront() \n6 foreach (loop L in F, bottom-up) { 7 if (L may perform synchronization 8 &#38;&#38; !IsTrivialLoop(L) \n9 &#38;&#38; . epoch marker that must-execute in L) 10 place epoch marker at L.entry 11 } 12 if (. epoch \nmarker that must-execute in F) 13 covered.add(F) 14 worklist.pushback(immediate callers of F) 15 } 16 \n} Figure 2. Algorithm to place epoch markers tion, including loops that perform synchronization either \ndirectly (e.g., by calling pthread mutex lock) or indi\u00adrectly (e.g., by calling a function that transitively \ncalls pthread mutex lock).1 However, it is important to min\u00adimize the number of epoch markers a large \nnumber of epoch markers can lead to a large number of epochs in E. Our actual algorithm, PlaceEpochMarkers, \nis more careful. We use a bottom-up traversal of the call graph start\u00ading from functions that directly \nperform synchronization (lines 3 5 and 14). For each visited function, we place epoch markers in all \nloops that perform synchronization and are not pruned by one of the following three optimizations: Ignore \ntrivial loops. We ignore simple loops of the form: while (!condition) pthread_cond_wait(cvar, mutex) \n This is the common idiom for using pthreads condition vari\u00adables. We observe that this loop can execute \nan unbounded number of synchronization operations only if the condi\u00adtion variable cvar can be noti.ed \nan unbounded number of times. So, as long as we ensure that all loops containing no\u00adti.cations are covered \nby an epoch marker, we can avoid placing an epoch marker in the above loop. Similarly, we ignore loops \nwhere the only form of syn\u00adchronization is a call to pthread create or pthread join. These loops must \nbe bounded since we assume a bounded number of threads are live at any given moment (recall \u00a71.1). Don \nt Cover the Same Loop Twice. We consider a loop covered when there exists an epoch marker that must\u00adexecute \non each iteration of the loop. For example, if loop A contains loop B where B contains an epoch marker, \nand if at least one iteration of B must-execute for each iteration 1 Our current implementation does \nnot support recursive functions that synchronize. This can be remedied by transforming recursive functions \ninto equivalent iterative functions, either manually, or automatically as in [28]. let hd = thread. slice.head \nin if (!Postdominates(hd, branch) || WritesLiveVarBetween(branch, hd) || SyncOpBetween(branch, hd) ) \nTake(branch) Figure 3. How precondition slicing handles branches (our additions are in italics) of A, \nthen we can avoid placing an epoch marker in loop A because that is subsumed by the marker placed in \nloop B. We implement this optimization by visiting the loop for\u00adest bottom-up (line 6). Then, we ignore \neach loop that must\u00adexecute a previously placed epoch marker (line 9). The vari\u00adable covered contains \na set of functions that must execute an epoch marker, so the check at line 9 is implemented by checking \nif there must-exist a call to a function in the covered set notice that at line 2, we initialize covered \nto include the epochMarker function. Barriers are epoch markers. Since epoch boundaries are runtime barriers, \nwe might as well end epochs at explicit program barriers. So, at line 2, we initialize covered to in\u00adclude \npthread barrier wait so the optimization at line 9 will ignore loops that must-execute a barrier. In \nthis way, each call to pthread barrier wait is treated as an im\u00adplicit epoch marker.  3.2 Enumerating \nSchedules for a Single Epoch The function SearchInEpoch (Figure 5) uses ExecutePath to symbolically execute \na single path from a given initial state. This path completes when all threads have reached an epoch \nmarker, terminated, or deadlocked. ExecutePath can follow any path and may context switch between threads \narbitrarily, as long as it follows a path that is feasible given the initial input constraint. If the \npath did not end in program termination or deadlock, it ended at a new bounded epoch that we add to the \nset of reachable epochs (lines 18 19). EpochId extracts the epoch identi.er (recall from \u00a72.1 that an \nepoch is named by the calling contexts from which each of its threads begins execution). For each path, \nwe extract the schedule and then com\u00adpute a conservative weakest precondition of the schedule us\u00ading \nprecondition slicing [10], where a precondition slice is computed from an execution trace and includes \nonly those statements from the trace that might affect whether the .nal statement was executed. The set \nof branching statements in a precondition slice combine to form a precondition of the .nal statement. \nWe have modi.ed the algorithm from [10] to instead enumerate all statements from the trace that might \naffect the set of synchronization operations that would be performed. We call this a synchronization-preserving \nslice. The original algorithm in [10] works much like a stan\u00addard dynamic backwards slicing algorithm: \nit iterates back\u00adwards over an execution trace, uses a live set to track data dependencies, and adds \nstatements to the slice if they modify  1 Search(p: Program) { 2 worklist = {MakeInitialState(p)} 3 \noutput = {} 4 5 while (!worklist.empty()) { 6 // Explore another bounded epoch 7 state = worklist.remove() \n8 (schedules, reachable) = SearchInEp och(state) 9 10 // Found an input-covering set for this epoch 11 \noutput.add(EpochId(state), schedules) 12 13 // Add unexplored epochs to the worklist 14 for (e in reachable) \n15 if (e not yet visited) 16 worklist.add(MakeStateForEpoch( e)) 17 } 18 19 return output 20 } Figure \n4. Exploring all reachable epochs items in the live set. Branches are handled as shown in Fig\u00adure 3: \na branch is included in the slice if either (a) the current head-of-slice is control-dependent on the \nbranch (this is the Postdominates check, which is computed with a standard postdominators analysis), \nor (b) some other path through the branch (not taken in the given trace) might modify an item in the \nlive set (this is the WritesLiveVarBetween check, which is computed with a static alias analysis). We \nmake three modi.cations. First, we include all syn\u00adchronization statements in the slice to ensure that \nall control and data dependencies of synchronization are included in the slice. Second, we construct \na separate slice for each thread so that all control-.ow checks in Figure 3 remain single\u00adthreaded. Finally, \nwe include a branch in the slice if some other path through the branch (not taken in the given trace) \nmight perform synchronization (this is the SyncOpBetween check in Figure 3). The .nal addition ensures \nthat a branch is included in the slice if it may affect synchronization. Because we assume data race \nfreedom, our slicing al\u00adgorithm does not need to account for potentially-racing ac\u00adcesses when computing \ndata dependencies. Relaxing this as\u00adsumption would involve a much more complicated imple\u00admentation of \nWritesLiveVarBetween that would require a conservative may-race analysis, as we describe in \u00a79. Shortest-Path \nFirst. It is correct for ExecutePath to follow any feasible path. However, it is optimal for ExecutePath \nto execute the shortest feasible path longer paths should be executed only as necessary to cover inputs \nnot covered by the shortest path. Determining the true short\u00adest feasible path is not decidable, so at \neach branch our heuristic is to select the branch edge with the shortest static distance to a statement \nthat either returns from the current function or exits the current loop. 1 SearchInEpoch(initState: SymbolicState) \n{ 2 reachableEpochs = {} 3 schedules = {} 4 constraints = {true} 5 6 while (!constraints.empty()) { 7 \n// Explore a new input constraint 8 state = initState.clone() 9 state.applyConstraint(constraints.remove()) \n 10 (finalState, trace) = ExecutePath(state) 11 12 // Update set of schedules 13 slice = PrecondSlice(trace) \n14 schedules.add(MakeConstraint(slice.branches), 15 trace.schedule) 16 17 // Update set of reachable \nepochs 18 if (!IsTerminatedOrDeadlocked(finalState)) 19 reachableEpochs.add(EpochId(finalState)) 20 21 \n// Accumulate unexplored input constraints 22 inputConstraint = true 23 for (b in slice.branches) { 24 \nc = inputConstraint . \u00acb 25 if (c not yet covered) 26 constraints.add(c) 27 inputConstraint = inputConstraint \n. b 28 } 29 } 30 31 return (schedules, reachableEpochs) 32 } Figure 5. Enumerating schedules within a \nsingle epoch  3.3 Exploring All Reachable Epochs The function Search enumerates input-covering schedules \nfor all epochs that are uncovered by SearchInEpoch. In Search, the key is a call to MakeStateForEpoch, \nwhich computes, for a given epoch, an initial symbolic state that will be explored by SearchInEpoch. \nEach symbolic state includes a set of calling contexts (one per thread), along with a set of constraints \non memory. The calling contexts are provided directly by the epoch identi.er, but the memory constraints \nmust be computed by MakeStateForEpoch. How does MakeStateForEpoch compute the initial memory constraints? \nThe dif.culty is that we must compute constraints that are abstract enough to cover all possible concrete \ninitial states of the epoch. The most conservative option is to use a completely unconstrained initial \nmemory, represented by the constraint true, but this is obviously an over-approximation SearchInEpoch \nwill waste time ex\u00adploring many infeasible paths. The most precise option is to symbolically enumerate \nall paths from program entry to the beginning of the epoch, then summarize those paths to com\u00adpute a \nvery precise initial state, but this will be prohibitively expensive it suffers from exactly the sort \nof state-space explosion that bounded epochs are designed to avoid.  Our approach uses a collection \nof static data.ow analyses as a compromise between those two extremes. The data.ow analyses were designed \nto remove a few common sources of infeasible paths, but they are necessarily conservative.2 We refer \nto a separate technical report for the details [4], but the following example demonstrates the general \nidea: 1 Thread 1 Thread 2 2 void RunA() { void RunB() { 3 Foo(&#38;thelock) Bar(&#38;thelock) 4 ... ... \n5 } } 6 void Foo(Lock *a) { void Bar(Lock *b) { 7 for (i in 1..X) { for (k in 1..Y) { 8 epochMarke r() \nepochMarke r() 9 lock(a) lock(b) 10 ... ... Suppose we are given an epoch in which threads T1 and T2 \nbegin executing from line 8. To execute this epoch symboli\u00adcally, ExecutePath needs to answer questions \nsuch as: Do a and b alias? (If so, the critical sections in T1 and T2 must be serialized.) And, does \nany thread hold lock a when the epoch begins? (If so, T1 must block until the lock is released.) The \ntechniques described in [4] enable precise answers to these questions in many common scenarios, including \nthe above scenario. For example, we learn that a and b refer to the same lock (namely, &#38;theLock), \nand we learn that no locks are held at the beginning of the epoch at line 8. Although the analyses from \n[4] are adept at removing common sources of imprecision, they are necessarily con\u00adservative, so infeasible \npaths may remain. 4. Avoiding Combinatorial Explosion Avoiding combinatorial explosion is essential. \nThis section describes two categories of optimization: First, we de.ne optimizations that exploit redundant \nschedules (\u00a74.1). These optimizations allow us to cover more inputs with fewer schedules. Precondition \nslicing can be viewed as one such optimization, but the optimizations in \u00a74.1 go further by observing \nthat schedules that are not obviously the same can sometimes be treated as if they are. Second, we deal \nwith unbounded loops that contain syn\u00adchronization using bounded epochs, but what about un\u00adbounded loops \nthat do not contain synchronization? We are hesitant to place epoch markers in every loop since a large \nnumber of epoch markers can lead to a large number of epochs. Instead, we deal with unbounded synchronization\u00adfree \nloops using a technique we call input abstraction (\u00a74.2). 2 Our data race freedom assumption makes these \nanalyses more effective. For example, it enables using interference-free regions to reason about cross-thread \ninterference [13]. 4.1 Pruning Redundant Schedules 4.1.1 Ignoring Pre.x Schedules Programs are often \nimplemented using a defensive coding style: they frequently check for errors (e.g., via assertions or \nby checking return codes from system calls) and termi\u00adnate the program when a failure is detected. Since \nwe in\u00adclude thread exit events in our schedules, it appears that enumerating a complete set of input-covering \nschedules re\u00adquires enumerating all ways in which the program can exit. In the limit, this requires enumerating \nall feasible assertion failures, which is a very hard problem on its own. We avoid this problem using \nthe concept of pre.x sched\u00adules. Suppose a thread executes the following code fragment: lock(A) if (X \n== 0) { abort() } lock(B) Concretely, there are two feasible schedules: (1) the thread locks A and then \naborts the process, and (2) the thread locks A and then locks B. We consider the .rst schedule a pre.x \nof the second schedule: at runtime, execution can always fol\u00adlow the second schedule, and then stop early \nif the abort statement is reached. To support pre.x schedules, we mod\u00adify ExecutePath and PrecondSlice \nto ignore branches that exit the process before performing any synchronization. For the above fragment, \nour optimized algorithm outputs just the second schedule, paired with the input-constraint true. We arrive \nat this output by ignoring the true branch of if(X==0). Note, however, that we would require two sched\u00adules \nif there was a call to lock() just before the abort(). 4.1.2 Ignoring Library Synchronization Users \nof our tool can opt to ignore internal synchronization used by library functions such as printf to ensure \nconsis\u00adtency of internal library data structures. With this option, our algorithm produces schedules \nthat do not include internal library synchronization such synchronization will be per\u00adformed nondeterministically \nat runtime. Our rationale is that developers are more concerned about testing their own code than library \ninternals, so it is sensible to ignore library inter\u00adnals and construct input-covering schedules for \napplication code only. This option works especially well with the pre\u00ad.x schedules optimization (\u00a74.1.1), \nas programs often call printf just before aborting the program.  4.1.3 Symbolic Thread Ids Redundant \nschedules can also arise across epoch boundaries. For example, consider a program in which N threads \neach execute the following code: 1 while(X) { epochMarker() ... } 2 ... 3 epochMarker() If X evaluates \ndifferently for each thread, then na\u00a8ively, we need one epoch in which all threads start at the epoch \nmarker at line 1, another in which T1 starts at line 3 while all other threads start at line 1, another \nin which just T2 starts at line 3, another in which just T1 and T2 start at line 3, and so on. In total, \nthere are 2N epochs.  The above combinatorial explosion arises if we assign each thread a concrete thread \nid during symbolic execution. We can avoid this problem by instead assigning each thread a symbolic thread \nid during symbolic execution. Now, we need to consider just N+1 total epochs: one epoch in which all \nthreads start at line 1, another epoch in which one thread starts line 3, another in which two threads \nstart at line 3, and so on. The idea is that the speci.c assignment of thread ids to calling contexts \ndoes not matter, so the EpochId function should return a multiset of calling contexts, rather than an \nordered list. This optimization requires some cooperation with our runtime system. Speci.cally, at each \nruntime epoch boundary, we must dynamically map each symbolic thread id to a concrete thread id we defer \ndetails to \u00a76.2. More generally, if we extend the above example to use k epoch markers, then we explore \nkN epochs using concrete nn ee k thread ids, and just epochs using symbolic thread ids, N nn ee k where \nis k-choose-N with repetitions. However, if N we further modify the above example so that each thread \nTi executes a unique function fi, where each fi includes k epoch markers, then we must explore kN epochs \nbecause there are that many unique combinations of calling contexts.  4.1.4 Redundancy from Code Duplication \nWe run our schedule enumeration algorithm after a com\u00adpiler optimization pass, as this has been shown \nto speed-up symbolic execution [9]. However, optimizations can some\u00adtimes introduce schedule redundancies \nby duplicating syn\u00adchronization. One example is the following transformation, which is called jump threading \nin LLVM: if (X == 0) { f() } if (X == 0) lock() => { f(); lock(); g() } if (X == 0) { g() } else { lock() \n} Our algorithm starts by executing one path through the optimized code (on the right). Suppose we execute \nthe false branch. Precondition slicing will notice the lock() call in the true branch and direct us down \nthat path as well, and the end result is an input-covering set with two schedules, one for X==0 and X!=0. \nHowever, both of these schedules are the same schedule the choice of schedule has no real dependency \non input X. Our current approach is to disable all transformations that might duplicate code, but unfortunately, \nthis is not al\u00adways possible. Notably, we cannot disable the following loop transformation because it \nis fundamental to the way many compilers reason about loops: while (foo()) { if (foo()) { ... => do { \n... } while (foo()) } } If foo() performs synchronization or contains an epoch marker, duplication of \nthe call to foo() can lead to redun\u00addant schedules that we cannot avoid.  4.2 Abstracting Input Constraints \nUnbounded synchronization-free loops can cause an explo\u00adsion in the number of paths explored by SearchInEpoch. \nThe following code fragment is a good example: TreeNode* T = TreeSearch(x) if (T) { lock(L) ... } In \nthis example, a thread searches for a value in a binary tree, and then performs synchronization if the \nvalue is found. Our problem is that symbolic execution will eagerly enumerate all concrete trees for \nwhich the expression T!=0 evaluates to true. Speci.cally, it attempts to enumerate the following in.nite \nset of input constraints: root->x == x root->x > x &#38;&#38; root->left &#38;&#38; root->left->x == \nx root->x > x &#38;&#38; root->left &#38;&#38; root->left->x > x ... ... Our approach is a form of abstraction: \ninstead of execut\u00ading TreeSearch symbolically, we treat TreeSearch as an uninterpreted function and add \nTreeSearch(x)!=0 to the path constraint. There are two subtleties in this approach: What Do We Abstract? \nWe abstract all synchronization\u00adfree loops and recursive functions that produce live-out val\u00adues that \nmight affect synchronization. Notice that we do not abstract loops that contain synchronization, since \nthose loops are already bounded by epoch markers. We start by assuming that no loops or recursive functions \nneed to be ab\u00adstracted. Then, during each call to PrecondSlice (line 13 of Figure 5), we check if any \nvalue added to the slice s live set was de.ned in a synchronization-free loop L or recursive function \nR. If such an L or R is found, it must be abstracted. How Do We Construct Abstractions? We construct \na symbolic function FL(x) = y, where x is the set of live\u00adins for loop L and y is the set of live-outs, \nwhere x and y can potentially be constructed with some form of summa\u00adrization, such as the summarization \nalgorithms proposed by Godefroid et al. [16, 18] (see \u00a79 for a discussion). However, this is dif.cult \nin general since x and y can each include unboundedly many heap objects. Due to this dif.culty, we currently \nconstruct each FL manually. This process is inter\u00adactive: we .rst run our algorithm from \u00a73; if our algorithm \n.nds a loop L that must be abstracted, it halts and reports L; we then produce a hand-written abstraction \nfor L and re-run our algorithm. During symbolic execution, we execute the abstraction FL in place of \nthe actual loop L. Each FL should model the terminating behaviors of L. We require each FL to terminate \nto ensure that our algorithm terminates as well. Of course, the actual loop L may not terminate, and \nwe preserve that behavior when the program is executed with our runtime system, we execute the actual \nloop L, not FL.  Each FL is allowed to be an over-approximation of loop L s terminating behaviors. This \neases construction of FL but adds potential to explore infeasible paths. Producing each FL is usually \nnot hard in practice, as the loops to abstract are often hidden behind natural abstraction boundaries. \nCon\u00adtinuing the above example, suppose the binary tree inter\u00adface includes TreeAdd and TreeDelete. These \nappear dif\u00ad.cult to abstract since they can mutate unboundedly many heap objects (e.g., to rebalance \nthe tree), but as long as all modi.cations and traversals are performed behind the Tree* interface, we \ncan conservatively model TreeAdd and TreeDelete by simply generating a fresh symbolic value that represents \nthe new root of the tree. Although the above explanation is phrased in terms of loops, recursive functions \ncan be abstracted in the same way. 5. Forming Ef.cient Bounded Epochs So far we have assumed that in \na given epoch, no thread exe\u00adcutes beyond its next epoch marker. Why might this be inef\u00ad.cient? First, \nruntime performance is optimal when threads execute a balanced amount of work per epoch, but na\u00a8ively \nstopping at the next epoch marker can lead to imbalance. Second, epochs should be long enough so that \nordering\u00addependent bugs, such as atomicity violations, are usually contained within a single epoch. It \nis more ef.cient to allow each thread to bypass a .\u00adnite number of epoch markers within each bounded \nepoch. Since epoch markers are placed in loops, we consider this is a form of loop unrolling. This optimization \ncoordinates with our runtime system as follows: for each epoch marker bypassed by ExecutePath, we add \na special node to the current happens-before schedule so that our runtime system will bypass that marker \nat runtime. We use the following heuristics to bypass epoch markers: Minimum Epoch Length. A large body \nof prior work has made the empirical observation that most ordering\u00addependent bugs occur over a short \nexecution window con\u00adtaining at most W instructions per thread. For example, [30] estimates that W is \nthousands of instructions in the worst case, but hundreds in the common case; [29] estimates W = 3000; \n[8] and [35] support this observation but do not give concrete estimates for W , although [8] observes \nthat many hard atomicity violations have the form if(x) compute(x), where W spans the short window between \nthe condition and the computation. This prior work suggests the simple heuristic that each thread should \nexecute a minimum of W instructions per epoch. Balanced Epoch Lengths. Each thread should execute approximately \nthe same number of instructions per epoch. For example, suppose we are about to end an epoch with T1 \nand T2 stalled at epoch markers. If len(T1) > len(T2) + k, where len(Ti) is the number of instructions \nexecuted by Ti in the current epoch and k is a heuristically-chosen constant, then we continue executing \nT2 up to its next epoch marker. 6. Implementation 6.1 Symbolic Execution Engine We implemented the above \nalgorithms in a version of the Cloud9 [7] symbolic execution engine extended with the techniques described \nin [4]. Cloud9 executes multithreaded C programs that use pthreads and compile to LLVM byte\u00adcode. To \nsupport unmodi.ed C programs, Cloud9 includes hand-written symbolic models for the Linux system call \nlayer and the pthreads library, and it models other C li\u00adbrary functions by linking with an actual libc \nimplementa\u00adtion (uClibc). We have instrumented Cloud9 s pthreads li\u00adbrary to dynamically capture a happens-before \nschedule dur\u00ading symbolic execution. Limitations. Our implementation has a few limitations that we consider \nminor but list for completeness: async sig\u00adnals, C++ libraries, and .oating point arithmetic. First, \nwe do not support asynchronous delivery of POSIX signals. This has not been a problem so far. Should \nit become an is\u00adsue, we can support asynchronous delivery by buffering sig\u00adnals until epoch boundaries, \nsimilarly to [5] or [24] such a buffering scheme would eliminate the need to reason about a combinatorial \nexplosion of possible signal delivery points. Second, Cloud9 ships with a standard C library (uClibc) \nbut not a standard C++ library, and this limits our ability to run C++ programs. Third, our underlying \ntheorem prover, STP [15], does not support .oating-point arithmetic. Cloud9 makes progress through .oating \npoint arithmetic by con\u00adcretizing values, which means the resulting path constraints will be incomplete \nfor paths that branch on the result of a .oating-point computation. This is often not an issue for our \nalgorithm in practice, since many programs compute .oating-point results but do not using .oating-point \nvalues to decide when to synchronize. However, this does prevent us from analyzing some programs, as \nwe discuss in \u00a77. Challenges. The effectiveness of precondition slicing is heavily dependent on the presence \nof a good whole-program alias analysis. The critical operation is the WritesLiveVar-Between check (Figure \n3) alias analysis imprecision can lead to the incorrect belief that a live variable was written, which \nresults in an overly strong schedule precondition, which results in the exploration of redundant schedules. \nOur implementation uses DSA [27], which, in whole\u00adprogram mode, degrades to a .eld-sensitive Steensgaard \n(equality-based) analysis. Our experience suggests that an inclusion-based analysis is vital. The problem \nintensi.es be\u00adcause we link with an entire C library all pointer variables passed to library functions \nare effectively merged in the points-to graph. We unfortunately could not .nd a publicly available alias \nanalysis for LLVM that is more powerful, so we duct-taped this problem by dividing pointer variables \ninto two classes: application code and library code. Variables in the later class are assumed to alias \nanything, while variables in the former class are analyzed with DSA.  Global state struct ScheduleFragment \n{ nextSelectorId: int schedule: map (threadId, list of H-B-Node) } selectors: map(int, (void)->ScheduleFragment*) \ncurrCallstacks: map(threadId, int) currFragments: list of ScheduleFragment* Schedule selection at epochs \nEpochBarrier() { isLast = barrier.arrive() if (isLast) { // last thread? epochId = hash(sort(currCallstacks.values)) \ncurrFragments.clear() currFragments.append(selectors[epochId]()) barrier.release() } else { barrier.wait() \n} } Figure 6. Key components of our runtime system  6.2 Compiler Instrumentation and Runtime System \nOur symbolic execution engine outputs a database of input\u00adcovering schedules that our runtime system \nfollows faith\u00adfully. Recall from \u00a73 that this database maps each epoch E . E to an input-covering set \nSE for E. At a high-level, our runtime system is mostly straight\u00adforward. The global variable currFragments \ncontains the happens-before schedule for the currently executing epoch. At the beginning of the program, \nwe compare the current in\u00adputs with the database of input constraints to select the initial schedule. \nSimilarly, epoch markers are turned into barriers, and when all threads reach an epoch barrier, a single \nthread is selected (arbitrarily) to update currFragments for the next epoch. Then, at each synchronization \nstatement, the runtime system inspects the calling thread s current happens-before node, waits until \nall incoming happens-before dependencies are satis.ed, and then advances to the next node. A more detailed \nview is given in Figure 6. We map each epoch E to a schedule selector function FE for each epoch. Schedules \ncontain a list of happens-before nodes for each thread. There are three technical challenges: At each \nepoch barrier, how do we ef.ciently determine the next epoch id E? How do schedule selector functions \ncheck input constraints? And, how do we deal with continuous inputs? Determining the Next Epoch. Each \nepoch id E is de\u00ad.ned by a multiset of per-thread call stacks (recall \u00a74.1.3). We instrument the program \nto record each thread s call stack in a globally-visible location. Then, the last thread to arrive at \nan epoch barrier can compute the next epoch id E by sort\u00ading this list of call stacks (note that a multiset \ncan be repre\u00adsented by a sorted list). The sorting operation is made ef.\u00adcient by representing callstacks \nusing hash values as in [6]. The algorithm in [6] has only probabilistic guarantees that each calling \ncontext is given a unique hash value, but since we know the complete set of epoch ids, we can ensure \na pri\u00adori that a unique hash value is computed for each epoch. Schedule Selector Functions. When invoked, \nthe se\u00adlector FE looks for a pair (I,S) . SE such that constraint I matches the current input, then it \nreturn S. This is im\u00adplemented by compiling SE into a decision tree. Our cur\u00adrent implementation selects \neach schedule as a determinis\u00adtic function of the given input, though this could easily be changed to \nselect schedules nondeterministically when mul\u00adtiple options are available. Recall that an epoch s input \ncan include the state of mem\u00adory. The dif.culty is that the choice of schedule can de\u00adpend on thread-local \nvariables. Since FE is executed by one thread only, how does it reason about state local to other threads? \nOur solution is to instrument the program to main\u00adtain a globally-visible shadow copy of each local variable \nthat is used in input constraints. In practice this is a very small percentage of all variables, as we \ndemonstrate in \u00a77.3. Note that we must also make shadow copies of variables that are needed to reach \nheap objects used in input con\u00adstraints. For example, if a constraint depends on the value of x->next->data, \nwhere x is a local variable, then we must maintain a shadow copy of x to ensure that the data .eld is \nglobally reachable. Supporting Continuous Inputs. We represent sched\u00adules as a tree of schedule fragments. \nAt the beginning of an epoch, each thread follows the initial fragment, represented in Figure 6 as currFragments[0]. \nEach fragment f ends in program exit, in an epoch boundary, or with a new input read by thread T . In \nthe later case, thread T invokes the selec\u00adtor function named by f->nextSelectorId, then appends the \nselected fragment to currFragments. As other threads arrive at the end of fragment f, they must wait \nfor T to se\u00adlect the next fragment before proceeding. We ignore inputs that are pruned by precondition \nslicing (\u00a73.2), so updates to currFragments occur only after the arrival of inputs that can affect synchronization. \nContinuous inputs introduce a further challenge, best il\u00adlustrated by the following sequence of events: \n1 EpochBarrier() 2 z += 5 3 ReadInput(&#38;x) 4 if (x == z &#38;&#38; y == w) { lock() } The selector \nfunction invoked at line 3 will evaluate the term x == z0+5, where z0 is the value of z at the beginning \nof the epoch. This value has been lost due to the update at line 2, so we need to snapshot z at line \n1. Note, however, that we do not need to snapshot y or w the condition y==w does not depend on input \nx, so it can be lifted into the epoch s selector function that is invoked at line 1.  Chances for Further \nOptimization. Runtime system optimization has not been our focus. We see at least three potential improvements: \n(1) we can apply a transitive reduc\u00adtion [40] on each happens-before schedule to reduce cross\u00adthread \nsynchronization; (2) for each term evaluated by se\u00adlector function FE, we can memoize the value of that \nterm as computed by FE to avoid recomputation during actual program execution; and (3) we can parallelize \nFE to avoid serializing FE at each epoch boundary (this last proposed improvement is perhaps the most \ncomplex).  6.3 Verifying Deadlock Freedom We already check for deadlocks during our search for input\u00adcovering \nschedules (see Figure 5, line 18). So, in a sense, we get deadlock checking for free. Our algorithm either \noutputs a set of non-deadlocking schedules, in which case we are guaranteed to never deadlock at runtime, \nor its output will include at least one pair (I,S) where schedule S dead\u00adlocks, in which case we may \ndeadlock at runtime. In the later case, we cannot prove that deadlock will actually oc\u00adcur at runtime \nbecause input constraint I may be infeasible (recall \u00a73.3). In this way, our deadlock checker is imperfect. \nCurrently, we manually inspect deadlocking schedules to de\u00adtermine if they are actually feasible, but \nwe hope to use more sophisticated strategies for removing infeasible paths in fu\u00adture work to make these \nmanual checks unnecessary. 7. Evaluation Our evaluation is organized in three parts. We start with a \nset of case studies (\u00a77.1) that evaluate the effectiveness of our schedule enumeration algorithm on a \nrange of applica\u00adtions. Our case studies include selections from the SP L A S H 2 and PA R S E C benchmark \nsuites, as well as pfscan, a parallel implementation of grep. We also characterize the effective\u00adness \nof our optimizations (\u00a77.2) and runtime system (\u00a77.3). We ran all experiments on a 4-core (2-way hyper-threaded) \n2.4 GHz Intel Xeon E5462 with 10GB RAM. For each ap\u00adplication, we marked all command-line parameters \nas input, with the exception of the num threads parameter, which we .x to the values 2, 4, and 8 to reveal \nhow our analysis and our runtime system scale with increasing thread counts. Cap\u00adturing command-line \ninputs required a minor code change of about 10 lines per application, though other inputs (i.e., values \nreturned by system calls) are captured automatically. In some applications, we made two additional code \nchanges as described in \u00a77.1.2 and \u00a77.1.4, respectively. We attempted to analyze most programs that were \nana\u00adlyzed by the related schedule memoization system PE R E -G R I N E [12], but occasionally ran into \nlimitations of our implementation (recall \u00a76.1 and footnote 1). Speci.cally, we could not run: barnes \nand ffm from SP L A S H 2, which perform synchronization in recursive functions; pbzip, which uses C++ \nlibraries; and ocean, fluidanimate, and streamcluster, which use .oating-point arithmetic to control \nsynchronization. 7.1 Case Studies For each case study, we address the following major ques\u00adtions: Is \na set of input-covering schedules enumerable in a reasonable amount of time? And if so, how large is \nE and how large is each SE? We also attempt to characterize how many of those schedules are infeasible. \nOverall results for our fully optimized algorithm are sum\u00admarized in Table 1. Column 2 gives the maximum \nnum\u00adber of threads live at any given instant (this is a function of the application s num threads parameter, \nwhich we .x to 2, 4, and 8 as described above). Columns 3 9 summa\u00adrize our algorithm s .nal output: Column \n3 is the number of reachable epochs (|E|); Columns 4 6 give statistics that summarize the number of schedules \nper epoch (|SE |); and Columns 7 9 give statistics that summarize schedules across all epochs (|S|), \nincluding the total number of infeasible schedules and deadlocking schedules. Column 10 states the number \nof input abstractions needed for the given applica\u00adtion (recall \u00a74.2). Column 11 gives the overall analysis \nrun\u00adtime in seconds (s), minutes (m), or dnf when our algorithm did not .nish within 2 hours. For all \nbut one program, we proved that S was deadlock\u00adfree. We determined the number of infeasible schedules \nthrough manual inspection of S. For pfscan, the sched\u00adules were too numerous for manual inspection, so \nwe give a lower bound in Table 1. Table 2 characterizes the bene.ts of our optimizations. For brevity, \nTable 2 includes just a representative subset of applications. Columns 3 4 give the number of epoch mark\u00aders \nadded by PlaceEpochMarkers, both with and without optimizations described in \u00a73.1. Columns 5 16 show \nthe re\u00adsults of running our algorithm with speci.c optimizations disabled. For na\u00a8ive \u00a73.3 , we use a \nna\u00a8ive implementation of MakeStateForEpoch that leaves memory completely un\u00adconstrained, and for the \nremaining column groups, we dis\u00adable the stated optimization. In each group of columns, |E|is the number \nof enumerated epochs, |S| is the total number of enumerated schedules (summed across all epochs), and \ntime is the analysis runtime. We refer to Table 2 in the case studies below, and give a further discussion \nin \u00a77.2. 7.1.1 The Trivial Case: Fork-Join Parallelism blackscholes (from PA R S E C) uses fork-join \nparallelism with no other synchronization. It is so simple that we consider it the hello world of synchronization \nanalysis. swaptions (also from PA R S E C) is equally simple. Our algorithm easily infers that these \napplications need exactly one schedule for a given thread count. 7.1.2 Case Study: Barrier-Synchronized \nParallelism fft (from SP L A S H 2) uses fork-join parallelism with a static number of barriers. Our \nalgorithm infers that fft needs just  App # Thr |E| min |SE|max avg Summary of Schedules (|S|) Total \nInfeasible Deadlocked # Input Abstracts. Analysis Runtime blackscholes swaptions 3,5,9 3,5,9 1 1 1 1 \n1 1 1 1 1 0 0 1 0 0 0 0 5 s, 6 s, 14 s 4 s, 9 s, 65 s fft 2,4,8 1 2 2 2 2 0 0 0 7 s, 306 s, 90 m lu* \n2,4,8 2 1 2 1.5 3 0 0 0 8 s, 7 s, 11 s radix* 2 5 1 2 1.8 9 2 0 0 9 s radix* 4 5 1 8 3.4 17 10 0 0 10 \ns radix* 8 5 1 64 14.8 74 65 0 0 53 s pfscan* 3 30 2 1209 110.7 3321 203+unk. 203 1 343 s pfscan* 5 50+ \n2 3792 404.8 12774+ unk. 87+ 1 dnf Table 1. Overall results. This is the fully-optimized algorithm. \nApplications marked with * use join on all threads (\u00a77.1.2). App # Thr # epoch markers w/ \u00a73.1 no \u00a73.1 \nna\u00a8ive \u00a73.3 |E| |S| time |E| no \u00a74.1.1 |S| time |E| no \u00a74.1.3 |S| time no \u00a74.1.4 |E| |S| time blackscholes \n9 0 2   1 3 6 s lu 8 0 2 2 486+ dnf 1+ 8 dnf 5 9 22 s 4 7 18 s radix 8 2 4 2+ 131+ dnf 5 86 53 s 42 \n622 619 s pfscan 3 3 7 159 11272 799 s 30 3356 350 s 50 4772 405 s Table 2. Cost of removing optimizations. \nCompare bold values in columns 5 16 with columns 3, 7, and 11 in Table 1. We mark columns with a dash \n( ) when the corresponding optimization has no effect. The time limit for dnf is 2 hours. two schedules \nfor a given thread count. The high analysis runtime is due to the presence of long sequences of condi\u00adtionals \nthat use division and modulo arithmetic in a way that our SMT solver (STP) .nds pathologically challenging. \nlu (from S P L A S H 2) synchronizes using a dynamic num\u00adber of barriers. Our algorithm divides lu s \nschedules into two epochs: one that begins at program entry (E1), and one that begins within lu s main \nparallel loop (E2). We need just one schedule for epoch E1 and two schedules for epoch E2. In epoch E2, \none schedule traverses one lock-step iteration of the parallel loop, and the other exits the program. \nlu introduces two program analysis challenges. First, we must infer that all threads execute the main \nparallel loop in lockstep. Failure to infer this fact results in infea\u00adsible schedules, as shown in column \n6 of Table 2 a na\u00a8ive MakeStateForEpoch does not make this inference. Second, lu makes calls of the form \npthread join(t[i]) that are deceptively dif.cult to analyze. We are unable to uniquely identify each \nt[i], so we must analyze three paths per call: one in which t[i] is an invalid thread id, another in \nwhich t[i] refers to a thread that has already exited, and an\u00adother in which t[i] refers to a thread \nthat has not yet exited. In total, to join with N threads, we analyze 3N paths, even though just one \nof those paths is feasible. We avoid this dif\u00ad.culty by replacing calls to pthread join with a high-level \njoin on all threads operation that is easy to analyze. Each application marked with an asterisk in Table \n1 was modi.ed to use this operation in place of pthread join.  7.1.3 Case Study: Barriers and Semaphores \nradix (from S P L A S H 2) is barrier-synchronized like lu, but with the addition of two parallel phases \nthat use semaphores to coordinate a tree-based reduction. These semaphores present the major dif.culty \nas shown in Table 1, we ex\u00adplore a number of infeasible schedules. The following example demonstrates \nthe problem: 1 Thread 1 Thread 2 2 for (...) { for (...) { 3 epochMarker() epochMarker() 4 sem_wait(&#38;s) \nsem_post(&#38;s) 5 ... ... MakeStateForEpoch cannot prove that s.count==0 at the beginning of the epoch. \n(In the actual code, this is dif.cult because each &#38;s is selected from an array.) As a result, we \nexplore an infeasible schedule in which T1 does not block at line 4 because it assumes that s.count>0. \nThis schedule incorrectly synchronizes T1 and T2, which can lead to the (incorrect) conclusion that the \nprogram contains data races. It is actually quite easy to prove that the above schedule is infeasible. \nOur insight is to exploit S, which pairs each schedule with an input constraint I. For the above schedule, \nI is s.count>0. Let E be the epoch containing that sched\u00adule. Our job is to show that each schedule Si \n. S that ter\u00adminates at epoch E always terminates with \u00acI resolving to true. This is easy to show using \nrely-guarantee reasoning: we add \u00ac(s.count>0) as an assertion to the end of each Si; we add \u00ac(s.count>0) \nas an assumption to the beginning of epoch E; and then we symbolically execute each epoch in E to ensure \nthat the assertions are always satis.ed. Note that the assumption is necessary because epoch E contains \na schedule that loops back to itself. It would be possible to automatically discharge the neces\u00adsary \nveri.cation conditions. We have not implemented this feature, but we have applied this approach to radix \nby man\u00adually annotating the program with assumption and assertion annotations to drive the veri.cation \nprocedure. We veri.ed that the 65 schedules listed as infeasible in Table 1 are truly infeasible.  Why \nwere we able to prove infeasibility so easily both in the above example and for radix s 65 infeasible \nschedules? The reason is that each input constraint I happens to be a function of synchronization state \nonly. If some I was instead a function of arbitrary program state, we would need to consider all paths \nthat terminate at epoch E, rather than just all schedules. The interesting novelty in our proof is that, \nsince we had a small number of schedules to consider, we could reason about each schedule in isolation \nby reusing sequential rely-guarantee reasoning techniques. 7.1.4 Case Study: Task Queues and Locks pfscan \nuses task parallelism with one producer thread and multiple worker threads, and it uses locks to guard \nshared data. The queue is implemented with locks and condition variables. pfscan has the following high-level \nstructure: 1 Producer Consumers 2 for (f in files) while (dequeue(&#38;f)) 3 enqueue(f) scanfile(f) \nscanfile implements string matching. We had to abstract one loop (in scanfile) using the technique described \nin \u00a74.2. This loop computes the next matching substring its live-ins include a string buffer and a current \nposition, and its live-out is the position of the next match. With 5 threads, were unable to enumerate \na complete set of input-covering schedules within a two hour time limit. Interestingly, the pre.x schedules \noptimization (no \u00a74.1.1 in Table 2) does not help pfscan much at all. The reason is that pfscan acquires \na lock on almost every failure path to perform logging. In fact, the majority of schedules enumer\u00adated \nfor pfscan are needed to handle these failure paths: with 3 threads, at least one thread executed a failure \npath on 2092 out of 3321 total schedules. Since all failure paths ac\u00adquire the same lock, they could \nconceivably be merged into one schedule this is an interesting direction for future re\u00adsearch. For pfscan, \nour algorithm produces a set S that in\u00adcludes deadlocking schedules. These deadlocks are all in\u00adfeasible. \nThe deadlocks include two scenarios: (1) the pro\u00adducer believes the queue is full while the consumers \nhave already exited, and (2) the consumers believe the producer has exited without .rst setting the done \n.ag. We enu\u00admerate these false deadlocks because our implementation of MakeStateForEpoch is not powerful \nenough to pro\u00adduce constraints that precisely relate the queue s capacity, count, and done .elds. We \nalso explore redundant schedules that arise from code duplication. Recall from \u00a74.1.4 that compilers \ntransform while loops to an if-then-do-while form. This transforma- App IPE LI DTree Size max avg Norm. \nExec Time 2thr 4thr 8thr blackscholes fft lu radix pfscan all all 200M 1B 6K 0 0 1 4 7 0 0 1 1 4 1.75 \n6 2.95 24 2.2 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.05 1.05 1.6 Table 3. Runtime system characterization: \nIPE is avg. in\u00adstructions per epoch; LI is # local variables instrumented. tion duplicates the dequeue \ncall made by each consumer thread in line 2 of the above code snippet. If this transfor\u00admation could \nbe disabled, we would explore 7 fewer epochs and 326 fewer schedules for 3 threads, and at least 11 fewer \nepochs and 479 fewer schedules for 5 threads.  7.2 Optimizations Characterization Table 2 shows that \nan effective MakeStateForEpoch is vital. An effective MakeStateForEpoch not only reduces the number of \nexplored paths, but it also improves analysis runtime by providing a more tightly constrained initial \nstate that our symbolic engine has n easier time reasoning about. To see this effect, compare the rate \nof schedule enumeration in columns 6 7 of Table 2 (4 schedules per minute for lu) with the rate of enumeration \nin Table 1 (16 schedules per minute for lu). The pre.x schedules optimization (\u00a74.1.1) is also essen\u00adtial. \nWithout this optimization enabled, our algorithm ex\u00adplores an essentially unbounded number of failure \npaths in lu and never escaped the .rst epoch. (The number of failure paths could be bounded by applying \ninput abstraction to two loops, but the key point is that lu does not require input ab\u00adstraction when \nthe pre.x schedules optimization is enabled.) The optimization to avoid code duplication (\u00a74.1.4) im\u00adproved \nlu only. However, as we observed in \u00a77.1.4, we ex\u00adplored redundant schedules in pfscan as a result of \na form of code duplication that this optimization could not elimi\u00adnate.  7.3 Runtime System Characterization \nTable 3 characterizes our runtime system in three ways, as described below. All numbers in Table 3 are \nbased on ex\u00adecutions of the .nal instrumented program which is linked with our custom runtime system. \nThese executions are con\u00adstrained to the input-covering schedules summarized in Ta\u00adble 1, and, except \nwhere otherwise mentioned, the num threads was set to 2 for pfscan and 4 for everything else. In Table \n3, Column 2 reports the average number of in\u00adstructions executed per thread in a single epoch (IPE). \nWe counted instructions by instrumenting LLVM bytecode, so actual the number of x86 instructions executed \nmay differ slightly. As discussed in \u00a75, IPE should ideally be large enough to span most ordering bugs. \nOur average IPE is well over the window of 3K instructions that was suggested by Lucia et al. in [29]. \nAlthough the averages are generally much higher than 3K, we noticed some variability. For ex\u00adample, IPE \nfor radix .uctuated between 30K instructions and 1B instructions, depending on the parallel phase.  \nColumns 3 5 characterize the amount of work performed to compute a new schedule. Column 3 states the \nnumber of local variables instrumented to maintain shadow copies, and columns 4 and 5 state the maximum \nand average number of arithmetic and boolean operators used by schedule selector decision trees (recall \n\u00a76.2). Columns 4 6 characterize our runtime overheads. Each column states the execution time of the .nal \ninstrumented program (linked with our runtime system) normalized to nondeterministic execution with the \nsame number of threads. A value of 2.0 means twice as long. For benchmark ap\u00adplications, we used standard \nbenchmark workloads, and for pfscan, we performed a search in a directory containing 50 .les. For barrier-synchronized \napplications, overhead is minimal the programs are already designed to execute in a bulk-synchronous \nfashion. For pfscan, we were unable to measure how well our runtime scales with increasing threads, since \nwe were unable to enumerate input-covering schedules for more than 2 threads within our time limit. 8. \nDiscussion of Guarantees Given a program P, our schedule enumeration algorithm out\u00adputs a set of bounded \nepochs E along with a set of input\u00adcovering schedules SE for each epoch E . E. Our schedule enumeration \nalgorithm and runtime system combine to pro\u00advide the following guarantees, which we state without proof: \nProperty 1 (Completeness of SE0 ). Suppose execution begins from program entry with initial memory state \nM0, where M0 contains nothing except the program s inputs. If SE0 is the set of input-covering schedules \nfor E0, the epoch at program entry, then for all valid M0, there must exist a pair (I , S ) . SE0 such \nthat M0 satis.es constraint I. Property 2 (Soundness and Completeness of E and all SE). Suppose execution \nbegins from a program context corre\u00adsponding to some epoch E . E, and suppose the initial memory state \nis M. Then, for all pairs (I , S ) . SE where M satis.es con\u00adstraint I, if our runtime system forces \nexecution to follow S, then either: (a) execution will encounter a data race; or (b) execution will follow \nschedule S without deviation. In case (b), schedule S must terminate at program exit, at a dead\u00adlock, \nor at some subsequent epoch E ' . E. If schedule S ter\u00adminates at epoch E ', then execution must arrive \nat E ' with a memory state M ' such that there exists a pair (I ' , S ') . SE' ' ' where M satis.es constraint \nI . Properties 1 and 2 establish that our system is both sound and complete for race-free programs. By \nsound, we mean that for any epoch E . E and any pair (I , S ) . SE, it must be possible for execution \nto follow schedule S when given an appropriate initial memory state. By complete, we mean that, for all \npossible program inputs, execution will proceed through a (possibly nonterminating) sequence of epochs \nE0, E1, E2, \u00b7 \u00b7 \u00b7 , where each Ei exists in E, and as execution arrives at each epoch Ei, there must \nexist a sched\u00adule Si . SEi such that execution can be constrained to Si within that epoch. Soundness \nis established by Property 2, and completeness is established by Property 1 combined with inductive application \nof Property 2. The important consequence of Properties 1 and 2 is that veri.cation tools can reason soundly \nand completely even when they consider only those schedules contained in S. Of course, these properties \nhold only when the program s execu\u00adtion is constrained by our runtime system when execution does not \nuse our runtime system, S under-approximates the set of schedules that might be followed and our veri.cation \nguarantees are voided. This is why we intend to use our run\u00adtime system in all executions of a given \nprogram. Additionally, our system is subject to two categories of limitations that we summarize below: \nFundamental Assumptions. As stated in \u00a71.1, our ap\u00adproach fundamentally assumes, .rst, that programs \nare data race free, and second, that programs have a bounded number of live threads at any moment. When \nthe .rst assumption is broken, our schedule enumeration algorithm is unsound and execution can diverge \nfrom the expected schedule at run\u00adtime. When the second assumption is broken, our schedule enumeration \nalgorithm will not terminate. Limitations of our Implementation. As stated in \u00a76.1, our implementation \nhas limited support for async signals, C++ libraries, and .oating point arithmetic. As stated in footnote \n1 in \u00a73.1, our implementation does not support re\u00adcursive functions that synchronize. Properties 1 and \n2 do not hold for programs that exceed these limitations. However, these limitations are speci.c to our \nimplementation and are not fundamental to our approach. Full proofs of Properties 1 and 2 are beyond \nthe scope of this paper. Full proofs would require a model of execution, a model of the runtime constraint \nsystem, and either assuming correct or proving correct our slicing algorithm (based on precondition slicing, \nwhich was described without a formal proof of correctness [10]), our underlying symbolic execu\u00adtion engine \n[7], and our underlying SMT solver [15]. 9. Related Work Schedule Memoization. The most closely related \nwork is schedule memoization from TE R N [11] and PE R E G R I N E [12]. As already mentioned, those \nsystems provide best-effort schedule memoization only, while our system enumerates a complete input-covering \nset. Our notion of epochs is re\u00adlated to TE R N s idea of windowing, which handles a speci.c kind of \nunboundedness event loops in server programs.  TE R N s windowing requires programmer annotations, while \nwe introduce epoch boundaries automatically. PE R E G R I N E uses a similar slicing algorithm to approx\u00adimate \nweakest preconditions. A key technical difference is that PE R E G R I N E s algorithm does not assume \ndata race free\u00addom. Instead, PE R E G R I N E uses a static may-race analysis to .nd memory access pairs \nthat may-race and then adds a happens-before edge to the schedule for each such pair. Adopting this approach \nin our setting would result in a more complex analysis and more symbolic path explosion due to the need \nto consider many possible may-race pairs. Note that path explosion is not a problem in PE R E G R I N \nE s set\u00adting, where slicing is used to compute an input constraint for tested paths only, but not to \nselect more symbolic paths. Finally, TE R N allows developers to explore multiple schedules for the same \ninput and then discard those sched\u00adules that trigger bugs. This is a form of automatic bug avoid\u00adance \nthat could be adopted by our system as well. Deterministic Execution. See our comparison in \u00a71.2. A further \ncomparison can be found in the critique by Yang et al. [41], who observe that, while determinism reduces \nthe number of schedules to one schedule per input, determinism does not necessarily reduce the total \nnumber of schedules by a signi.cant amount. Yang et al. argue and we agree that reducing the total number \nof schedules provides a more signi.cant bene.t to testing and veri.cation tools. Synchronization Analysis. \nStatic analyses have been developed to summarize synchronization behavior. These include may-happen-in-parallel \nanalysis [33] and barrier matching [1, 42] see Rinard s survey for a summary [37]. These analyses are \ncheap to compute but they construct schedules approximately, while our analysis is expensive to compute \nbut constructs schedules precisely; thus, these approaches occupy two opposite ends of the performance/\u00adprecision \ntradeoff. Symbolic Execution. We faced three main technical challenges: infeasible paths (\u00a73.3), redundant \nschedules (\u00a74.1), and unbounded loops (\u00a72 and \u00a74.2). Each challenge represents a speci.c instance of \nsymbolic execution s path explosion problem, which has been studied extensively. For example, our optimizations \nto avoid schedule redundancies are reminiscent of various partial-order reductions devel\u00adoped for model \nchecking [14, 20], though that prior work identi.es schedule redundancies given a .xed input, while we \nidentify redundancies across inputs (cf. \u00a74.1.1). Further, in \u00a74.2 we observed that some form of input \nab\u00adstraction is necessary to achieve good scalability of sym\u00adbolic execution. Other authors have made \nthe same obser\u00advation, most notably Anand et al. [2] and Godefroid [17]. Anand et al. [2] propose using \nmanually-written abstrac\u00adtions (as we do), and they propose a methodology for writing those abstractions. \nRelatedly, abstractions can be created us\u00ading programmer-written contracts [38] or via automatically \nconstructed summaries [16, 18]. Finally, one can view our use of bounded epochs as a form of path merging \n[19, 21], where epoch boundaries rep\u00adresent path merge-points. Classical path merging algorithms execute \nall paths connecting control-.ow points A and B, then merge the resulting states at B. This is made feasible \nby keeping the distance between A and B short. Our algorithm does not execute all paths within each epoch, \nmaking it more challenging to construct initial states for middle-of-program epochs, so we rely on other \ntechniques [4]. Program Veri.cation. A large body of prior work has focused on the veri.cation of multithreaded \nprograms. Much of this work has used preemption bounding a schedule is preemption bounded to depth k \nif the schedule includes no more than k preemptions. Musuvathi et al. used this idea in a model checker \nfor multithreaded programs [31, 32]. Qadeer and Wu showed how to reduce a multithreaded program into \na sequential program given a .xed k [36], and subsequent authors have de.ned more advanced reductions \n[22, 23, 25]. Although this approach grows exponentially more costly as k increases, it has been shown \nempirically that many concurrency bugs can be found with k = 2, making this approach practical. For example, \nQadeer and Wu used this technique to .nd data race bugs in Windows device drivers. The idea to analyze \na multithreaded program by .rst re\u00adducing it to an equivalent sequential program is shared by the technique \nof schedule specialization [39] that we summa\u00adrized in \u00a71.1. Unfortunately, all of the above reductions \nare incomplete in practice, in the sense that they do not analyze all possible schedules. Speci.cally, \npreemption bounding is incomplete unless k 8, and schedule specialization is in\u00adcomplete unless all \nschedules are available. By constraining execution to a small set of input-covering schedules, our ap\u00adproach \ncan make these promising reductions complete. No\u00adtably, the approach to schedule specialization taken \nby Wu et al. [39] can be directly applied to our proposed system by producing a specialized program for \neach schedule in S. 10. Conclusions This paper opens two new research directions. First, we have introduced \nthe input-covering schedules problem. We showed how to make this problem more tractable by par\u00adtitioning \nexecution into bounded epochs. We designed and implemented an algorithm for enumerating input-covering \nschedules. An empirical evaluation demonstrates that it is possible to enumerate a complete set of input-covering \nschedules for at least some realistic programs. Second, by constraining execution to a set of input\u00adcovering \nschedules, we open the door for new veri.cation techniques that exploit the fact that, in such a constrained \nenvironment, all thread schedules are known a priori. We took a small .rst step in this direction by \ndesigning and implementing a simple deadlock checker. The source code for our implementation will be \nmade available at http://sampa.cs.washington.edu/.  Acknowledgements We thank the anonymous reviewers \nfor their valuable com\u00adments. We also thank the members of the Sampa and PLSE groups at UW for their \nfeedback and fruitful discussions. This work was supported in part by the National Science Foundation \nCAREER award 0846004, a Google Ph.D. Fel\u00adlowship awarded to Tom Bergan, and a gift from Microsoft. References \n[1] A. Aiken and D. Gay. Barrier Inference. In POPL, 1998. [2] S. Anand, C. S. P.areanu, and W. Visser. \nSymbolic Execution as.with Abstract Subsumption Checking. In SPIN, 2006. [3] T. Bergan, J. Devietti, \nN. Hunt, and L. Ceze. The Determin\u00adistic Execution Hammer: How Well Does it Actually Pound Nails? In \nWorkshop on Determinism and Correctness in Par\u00adallel Programming (WoDet), 2011. [4] T. Bergan, D. Grossman, \nand L. Ceze. Symbolic Execution of Multithreaded Programs from Arbitrary Program Contexts. Technical \nReport UW-CSE-13-08-01, Univ. of Washington. [5] T. Bergan, N. Hunt, L. Ceze, and S. Gribble. Deterministic \nProcess Groups in dOS. In OSDI, 2010. [6] M. D. Bond and K. S. McKinley. Probabilistic Calling Con\u00adtext. \nIn OOPSLA, 2007. [7] S. Bucur, V. Ureche, C. Zam.r, and G. Candea. Parallel Sym\u00adbolic Execution for Automated \nReal-World Software Testing. In EuroSys, 2011. [8] S. Burckhardt, P. Kothari, M. Musuvathi, and S. Nagarakatte. \nA Randomized Scheduler with Probabilistic Guarantees of Finding Bugs. In ASPLOS, 2010. [9] C. Cadar, \nD. Dunbar, and D. Engler. KLEE: Unassisted and Automatic Generation of High-Coverage Tests for Complex \nSystems Programs. In OSDI, 2008. [10] M. Costa, M. Castro, L. Zhou, L. Zhang, and M. Peinado. Bouncer: \nSecuring Software by Blocking Bad Input. In SOSP, 2007. [11] H. Cui, J. Wu, C. che Tsai, and J. Yang. \nStable Deterministic Multithreading Through Schedule Memoization. In OSDI, 2010. [12] H. Cui, J. Wu, \nJ. Gallagher, H. Guo, and J. Yang. Ef.cient Deterministic Multithreading through Schedule Relaxation. \nIn SOSP, 2011. [13] L. Ef.nger-Dean, H.-J. Boehm, P. Joisha, and D. Chakrabarti. Extended Sequential \nReasoning for Data-Race-Free Pro\u00adgrams. In Workshop on Memory Systems Performance and Correctness (MSPC), \n2011. [14] C. Flanagan and P. Godefroid. Dynamic Partial-Order Reduc\u00adtion for Model Checking Software. \nIn POPL, 2005. [15] V. Ganesh and D. L. Dill. A Decision Procedure for Bit\u00advectors and Arrays. In CAV, \n2007. [16] P. Godefroid. Compositional Dynamic Test Generation. In POPL, 2007. [17] P. Godefroid. Higher-Order \nTest Generation. In PLDI, 2011. [18] P. Godefroid and D. Luchaup. Automatic Partial Loop Sum\u00admarization \nin Dynamic Test Generation. In ISSTA, 2011. [19] T. Hansen, P. Schachte, and H. Sondergaard. State Joining \nand Splitting for the Symbolic Execution of Binaries. In Intl. Conf. on Runtime Veri.cation (RV), 2009. \n[20] V. Kahlon, C. Wang, and A. Gupta. Monotonic Partial Order Reduction: An Optimal Symbolic Partial \nOrder Reduction Technique. In CAV, 2007. [21] V. Kuznetsov, J. Kinder, S. Bucur, and G. Candea. Ef.cient \nState Merging in Symbolic Execution. In PLDI, 2012. [22] S. La Torre, P. Madhusudan, and G. Parlato. \nContext-Bounded Analysis of Concurrent Queue Systems. In TACAS, 2008. [23] S. La Torre, P. Madhusudan, \nand G. Parlato. Reduc\u00ading Context-Bounded Concurrent Reachability to Sequential Reachability. In CAV, \n2009. [24] O. Laadan, N. Viennot, and J. Nieh. Transparent, Lightweight Application Execution Replay \non Commodity Multiprocessor Operating Systems. In SIGMETRICS, 2010. [25] A. Lal and T. Reps. Reducing \nConcurrent Analysis Under a Context Bound to Sequential Analysis. In CAV, 2008. [26] L. Lamport. Time, \nClocks, and the Ordering of Events in a Distributed System. Communications of the ACM, 21(7), July 1978. \n[27] C. Lattner. Macroscopic Data Structure Analysis and Opti\u00admization. PhD thesis, Computer Science \nDept., University of Illinois at Urbana-Champaign, Urbana, IL, May 2005. [28] Y. A. Liu and S. D. Stoller. \nFrom Recursion to Iteration: What are the Optimizations? In PEPM, 1999. [29] B. Lucia, L. Ceze, and K. \nStrauss. ColorSafe: Architectural Support for Debugging and Dynamically Avoiding Multi-Variable Atomicity \nViolations. In ISCA, 2010. [30] B. Lucia, J. Devietti, K. Strauss, and L. Ceze. Atom-Aid: Detecting and \nSurviving Atomicity Violations. In ISCA, 2008. [31] M. Musuvathi and S. Qadeer. Iterative Context Bounding \nfor Systematic Testing of Multithreaded Programs. In PLDI, 2007. [32] M. Musuvathi, S. Qadeer, T. Ball, \nG. Basler, P. A. Nainar, and I. Neamtiu. Finding and Reproducing Heisenbugs in Concurrent Programs. In \nOSDI, 2008. [33] G. Naumovich, G. S. Avrunin, and L. A. Clarke. An Ef.cient Algorithm for Computing MHP \nInformation for Concurrent Java Programs. In FSE, 1999. [34] M. Olszewski, J. Ansel, and S. Amarasinghe. \nKendo: Ef.cient Deterministic Multithreading in Software. In ASPLOS, 2009. [35] S. Park, S. Lu, and Y. \nZhou. CTrigger: Exposing Atomicity Violation Bugs from their Hiding Places. In ASPLOS, 2009. [36] S. \nQadeer and D. Wu. KISS: Keep It Simple and Sequential. In PLDI, 2005. [37] M. Rinard. Analysis of Multithreaded \nPrograms. In Static Analysis Symposium (SAS), 2001. [38] S. Tobin-Hochstadt and D. Van Horn. Higher-Order \nSymbolic Execution via Contracts. In OOPSLA, 2012. [39] J. Wu, Y. Tang, G. Hu, H. Cui, and J. Yang. Sound \nand Pre\u00adcise Analysis of Parallel Programs through Schedule Special\u00adization. In PLDI, 2012. [40] M. Xu, \nM. Hill, and R. Bodik. A Regulated Transitive Reduc\u00adtion for Longer Memory Race Recording. In ASPLOS, \n2006. [41] J. Yang, H. Cui, and J. Wu. Determinism Is Overrated: What Really Makes Multithreaded Programs \nHard to Get Right and What Can Be Done About It. In HotPar, 2013. [42] Y. Zhang and E. Duesterwald. Barrier \nMatching for Programs With Textually Unaligned Barriers. In PPoPP, 2007.     \n\t\t\t", "proc_id": "2509136", "abstract": "<p>We propose constraining multithreaded execution to small sets of <i>input-covering schedules</i>, which we define as follows: given a program P, we say that a set of schedules &#8721; <i>covers</i> all inputs of program P if, when given any input, P's execution can be constrained to some schedule in &#8721; and still produce a semantically valid result.</p> <p>Our approach is to first compute a small &#8721; for a given program P, and then, at runtime, constrain P's execution to always follow some schedule in &#8721;, and never deviate. We have designed an algorithm that uses symbolic execution to systematically enumerate a set of input-covering schedules, &#8721;. To deal with programs that run for an unbounded length of time, we partition execution into <i>bounded epochs</i>, find input-covering schedules for each epoch in isolation, and then piece the schedules together at runtime. We have implemented this algorithm along with a constrained execution runtime for pthreads programs, and we report results</p> <p>Our approach has the following advantage: because all possible runtime schedules are known <i>a priori</i>, we can seek to validate the program by thoroughly verifying each schedule in &#8721;, in isolation, without needing to reason about the huge space of thread interleavings that arises due to conventional nondeterministic execution.</p>", "authors": [{"name": "Tom Bergan", "author_profile_id": "81350584960", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P4290439", "email_address": "tbergan@cs.washington.edu", "orcid_id": ""}, {"name": "Luis Ceze", "author_profile_id": "81100112680", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P4290440", "email_address": "luisceze@cs.washington.edu", "orcid_id": ""}, {"name": "Dan Grossman", "author_profile_id": "81405594870", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P4290441", "email_address": "djg@cs.washington.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509508", "year": "2013", "article_id": "2509508", "conference": "OOPSLA", "title": "Input-covering schedules for multithreaded programs", "url": "http://dl.acm.org/citation.cfm?id=2509508"}