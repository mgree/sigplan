{"article_publication_date": "10-29-2013", "fulltext": "\n Data-Driven Equivalence Checking Rahul Sharma Eric Schkufza Berkeley Churchill Stanford University \nStanford University Stanford University sharmar@cs.stanford.edu eschkufz@cs.stanford.edu bchurchill@cs.stanford.edu \nAlex Aiken Stanford University aiken@cs.stanford.edu Abstract We present a data driven algorithm for \nequivalence check\u00ading of two loops. The algorithm infers simulation relations using data from test runs. \nOnce a candidate simulation rela\u00adtion has been obtained, off-the-shelf SMT solvers are used to check \nwhether the simulation relation actually holds. The algorithm is sound: insuf.cient data will cause the \nproof to fail. We demonstrate a prototype implementation, called DDEC, of our algorithm, which is the \n.rst sound equiva\u00adlence checker for loops written in x86 assembly. Categories and Subject Descriptors \nD.1.2 [Automatic Programming]: Program Transformation; D.2.4 [Program Veri.cation]: Correctness proofs; \nD.3.4 [Processors]: Com\u00adpilers; D.3.4 [Processors]: Optimization Keywords Binary Analysis; Compilers; \nMarkov Chain Monte Carlo; Optimization; Superoptimization; SMT; Veri\u00ad.cation; x86 1. Introduction Equivalence \nchecking of loops is a fundamental problem with potentially signi.cant applications, particularly in \nthe area of compiler optimizations. Unfortunately, the current state of the art in equivalence checking \nis quite limited: given two assembly loops, no existing technique is capable of verifying equivalence \nautomatically, even if they differ only in the application of standard loop optimizations. In this paper, \nwe present the .rst practically useful, automatic, and sound equivalence checking engine for loops written \nin x86. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. Copyrights for components of \nthis work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, \nUSA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509509 \n Proofs of equivalence at the binary level are more desir\u00adable than proofs at the source or RTL level, \nbecause such proofs minimize the trusted code base. For example, a bug in the code generator of a compiler \ncan invalidate a proof performed at the RTL level or we can have a what you see is not what you execute \n(WYSINWYX) phenomenon [3] and the generated binary can deviate from what is intended in the source. Existing \ntechniques for proving equivalence can be clas\u00adsi.ed into three categories: sound algorithms for loop-free \ncode [1, 6, 10, 11, 23]; algorithms that analyze .nite un\u00adwindings of loops or .nite spaces of inputs \n[17, 20, 28, 30]; algorithms that require knowledge of the particular transfor\u00admations used for turning \none program into another [24, 36] and the order in which the transformations have been ap\u00adplied [14, \n25, 29]. In contrast, our approach to handling equivalence checking of loops does not assume any knowl\u00adedge \nabout the optimizations performed. We have implemented a prototype version of our algo\u00adrithm in a system \ncalled DDEC (Data-Driven Equivalence Checker). This tool checks the equivalence of two loops written \nin x86 assembly. In outline the tool works as follows. First, DDEC guesses a simulation relation [25]. \nRoughly speaking, a simulation relation breaks two loops into a set of pairs of loop-free code fragments. \nLogical formulas asso\u00adciated with each pair describe the relationship of the input states of the fragments \nto the output states of the fragments. Second, DDEC generates veri.cation conditions encoding the x86 \ninstructions contained in each loop-free fragment as SMT [7] constraints. Finally, DDEC constructs queries \nwhich verify that the guessed relationships between the code fragments in fact hold. By construction, \nif the queries suc\u00adceed they constitute an inductive proof of equivalence of the two loops. It is worth \nstressing that DDEC works directly on un\u00admodi.ed binaries for x86 loops. The x86 instruction set is large, \ncomplex, and dif.cult to analyze statically. The key idea that makes DDEC effective in practice, and \neven sim\u00adply feasible to build, is that the process of guessing a sim\u00adulation relation (step 1 above) \nis constructed not via static code analyses, but by using data collected from test cases. Because DDEC \nis data driven, it is able to directly exam\u00adine the precise net effect of code sequences without .rst \ngo\u00ading through a potentially lossy abstraction step. Of course, the use of test cases is an under-approximation \nand may not capture all possible loop behaviors. Nonetheless, DDEC is sound; a lack of test coverage \nmay cause equivalence check\u00ading to fail, but it cannot result in the unsound conclusion that two loops \nare equivalent when they are not. While our main result is simply a sound and effective ap\u00adproach to \nchecking the equivalence of loops, we also show that suf.ciently powerful equivalence checking tools \nsuch as DDEC have novel applications. In particular, we are able to verify the equivalence of binaries \nproduced by com\u00adpletely different compiler toolchains. For a representative set of benchmarks, we automatically \nprove the equivalence of code produced by C O M P CE RT and gcc (with optimizations enabled), which allows \nus to certify the correctness of the entire gcc compilation or, conversely, to produce more per\u00adformant \nCO M P CE RT code. Additionally, we extend the ap\u00adplicability of STOKE [33], a superoptimizer for straight\u00adline \nprograms, to loops. We replace STOKE s validator by DDEC and the resulting implementation is able to \nperform optimizations beyond STOKE s original capabilities, in fact producing veri.ed code which is comparable \nin performance to gcc -O3. DDEC is not without limitations. In particular, DDEC restricts the expressiveness \nof invariants required for a proof to be conjunctions of linear or nonlinear equalities. However, for \nmost interesting intra-procedural optimizations, simple equalities appear to be suf.ciently expressive \n[25, 32, 36]. DDEC is also currently unable to reason about .oating point computations, simply because \nthe current generation of off\u00adthe-shelf SMT solvers do not support .oating point reason\u00ading. Floating \npoint reasoning is orthogonal to our contribu\u00adtions; when even limited .oating point reasoning becomes \navailable, DDEC can incorporate it immediately. However, the current state of the art limits the demonstration \nof DDEC to loops that work only with non-.oating point values. We begin by providing a detailed, but \ninformal, worked example (Section 2), which is followed by a presentation of the equivalence checking \nalgorithm (Section 3). Next we describe the implementation of D DE C (Section 4). Our evaluation (Section \n5) shows DDEC is competitive with the state of the art in equivalence checking and presents the novel \napplications described above. Discussions of related work, limitations, and future work are included \nin Sections 6 and 7. 2. A Worked Example Figure 1 shows two versions of a function taken from [36]. The \nstraightforward implementation X is optimized using a strength reduction [2] to produce the code Y ; \ncorresponding x86 assembly codes T and R are shown beneath each source code function. We use primes (') \nto represent program points and registers corresponding to the optimized code, which we call the rewrite \nR, and unprimed quantities for those corre\u00adsponding to the unoptimized code, which we call the target \nT , throughout the paper. The generation of R also involves some low level compiler optimizations such \nas the use of an x86 conditional-move (instruction 11' of R) to eliminate a jump (instruction 12 of T \n). We are unaware of any fully au\u00adtomatic technique capable of verifying the equivalence of T and R. \nNonetheless, our goal is to verify equivalence in the absence of source programs, manually written expert \nrules for equivalence, source code of the compiler(s) used, and compiler annotations in other words, \ngiven only the two assembly programs and no other information. All of these features are necessary requirements \nto check equivalence at the assembly level when no knowledge is available about the relationship between \nthe two codes. This situation arises, for example, in verifying the correctness of STOKE optimiza\u00adtions \n[33]. To verify that T and R are equivalent, we must con.rm that whenever T and R begin execution in \nidentical machine states and T runs to completion, R is guaranteed to terminate in the same machine state \nas T and with the same return value. In this example, which does not use memory, we limit our discussion \nof machine states to a valuation of the subset of hardware registers that the two codes use: eax, ebx, \necx, esi, and edi (our technique does not make this assumption in general and handles memory reads and \nwrites soundly). We also assume that we know which registers are live on exit from the function, which \nin this case is just the return value eax. We stress that the following discussion takes place entirely \nat the assembly level and does not assume any information about the sources X or Y . We use the well-known \nconcept of a cutpoint [39] to de\u00adcompose equivalence checking of two loops into manage\u00adable sub-parts. \nA cutpoint is a pair of program points, one in each program. Cutpoints are chosen to divide the loops \ninto loop-free segments. The cutpoints in Figure 1, labeled a, b, and c, segment the programs as follows: \n1) the code from a to b excluding the backedge of the loop, 2) the code that starts from b, takes the \nbackedge once, and comes back to b, and 3) the code that starts from b, exits the loop, and ter\u00adminates \nat c. Using these three cutpoints, we can produce an inductive proof of equivalence which proceeds as \nfollows. Our goal is to show that the executions of T and R move together from one cutpoint to the next \nand that at each cut\u00adpoint, certain invariants are guaranteed to hold. The required invariants at a and \nc follow directly from the problem state\u00adment. At point a, we require that T and R agree on the initial \nmachine states. Similarly at point c, we require that T and R agree on the return value stored in eax. \n Figure 1. Equivalence checking for two possible compilations: (A) no optimizations applied either by \nhand or during compilation, (B) optimizations applied. Cut points (a,b,c) and corresponding paths (i-vi) \nare shown. We now encounter two major dif.culties in producing a proof of equivalence. The .rst problem \nwe encounter is iden\u00adtifying the invariant that must hold at b. This invariant, I, is a relation between \nthe machine states of T and R. In this paper, we consider invariants that consist of equality rela\u00adtionships \nbetween elements of the two machine states. Once we have identi.ed the appropriate invariant I, an inductive \nproof would take the following form: 1. If T and R begin in a with identical machine states and transition \nimmediately to c, they terminate with identical return values. 2. If T and R begin in a with identical \nmachine states and transition to b, they both satisfy I.  3. If T and R begin in b satisfying I, and \nreturn to b, then I still holds. 4. If T and R begin in b satisfying I and transition to c, they terminate \nwith identical return values.  However, this proof is still incomplete. It does not guarantee that \nif T makes a transition, then R makes the same transi\u00adtion: we do not for instance want T to transition \nfrom a to b while R transitions from b to c. The proof can be completed as follows. Every transition \nbetween cutpoints is associated with some instructions of T and R: these are the instructions, or code \npaths, which need to be executed to move from one cutpoint to the next. For example, in moving from cutpoint \nb to c, T and R execute the instructions shown in code paths vi of Figure 1. A code path pA of T corresponds \nto a code path pB of R if they start and end at the same cutpoints and when T executes pA then R executes \npB. Figure 1 shows a complete set of corresponding paths: i and ii are associated with transition a-b, \niii is associated with a-c, iv and v are associated with b-b and vi is associated with b-c. We need to \ncheck that when T executes a code path then R can only execute the corresponding paths. Identifying these \ncorre\u00adsponding paths is a second major dif.culty. The question of what invariants hold at b is crucial, \nas these must be strong enough to statically prove that the execution of R follows the corresponding \npaths of T and that the executions of both T and R proceed through the same sequence of cutpoints. Our \nsolution to both problems, identifying the equalities that hold at b and the corresponding paths of the \ntwo pro\u00adgrams, is to analyze execution data. We identify correspond\u00ading paths by matching program traces \nfor both loops on the same test inputs against the set of cutpoints. We observe the instructions executed \nby T and R in moving from one cut\u00adpoint to the next and label them as corresponding. For exam\u00adple, for \na test case with initial state edi = 0 and esi = 1, T begins its execution from a and executes instructions \n1 to 16 to reach b. It then exits the loop and transitions from b to c by executing instruction 17. R \nbegins its execution from a and executes instructions 1 ' to 14 ' to reach b. It then executes instruction \n15 ' to reach c. From this test case, we observe that code paths with instructions 1 to 16 correspond \nto instructions 1 ' to 14 ' (i) and 17 corresponds to 15 ' (vi). The equality conditions at b can be \ndetermined by insert\u00ading instrumentation at b to record the values of the live pro\u00adgram registers for \nboth programs and observing the results. For a test input where edi = 0 and esi = 2, we obtain the following \nmatrix where each row corresponds to the values in live registers when both programs pass through b: \n. eax esi edi ecx eax ' ecx ' edx ' esi ' . . 0 1 1 2 0 5 1 2 . 5 2 2 2 5 10 2 2 The .rst row says that \nwhen T reaches b for the .rst time the registers have the values shown in columns eax , esi, edi, and \necx; when R reaches b for the .rst time, the reg\u00ad ' ' isters have values shown in eax , ecx , edx ', \nand esi '. The second row shows the values of registers when b is reached the second time, i.e., in the \nnext iteration of T and R. Us\u00ading standard linear algebra techniques, it is possible to ex\u00adtract the \nfollowing relationships, which are suf.cient candi\u00ad ' dates for the equalities which must hold at b: \neax = eax , ' 5*esi = ecx , edi = edx ', and ecx = esi '. The use of lin\u00adear algebra on test data for \ninvariant inference is well studied in software veri.cation and the limitations are known. The process \nmay generate spurious equality relationships [26], however these can be systematically eliminated using \na the\u00adorem prover [34]. We note that this method assumes that equality relationships are suf.cient to \nprove program equiv\u00adalence and we do not, for example, consider invariants which contain inequalities. \nPrevious work on translation valida\u00adtion [25, 36] makes the same assumption, which we .nd to be largely \nsuf.cient in practice (see Section 5). Although it is possible that the tests used to generate these \nvalues do not produce suf.cient coverage, and that either more corresponding paths exist, or spurious \nequality rela\u00adtionships are discovered at b, the consequence is simply that the proof will fail, and \nwe will report that the two functions may be different. We may then reattempt the proof with more test \ncases, but a lack of test data cannot cause an unsound result. Barring this possibility, almost all of \nthe limitations of this technique can be mapped to the restricted expres\u00adsiveness of invariants. Better \ninvariant generation techniques will only improve performance. 3. Algorithm We now present a formal description \nof the algorithm sketched in Section 2. We assume we are given two functions T and R, each containing \none natural loop and no function calls. We infer a candidate simulation relation consisting of cutpoints \nand linear equalities as invariants. Then we check whether the candidate is an actual simulation relation, \nand if so then we have a proof of equivalence. We consider func\u00adtions containing multiple loops, calling \nother functions, and inference of non-linear invariants in Section 3.1.3. As is standard, we make a distinction \nbetween T , the target or reference code, and R, the rewrite or proposed replacement for the target T \n. A program state consists of a valuation of registers, current stack frame, and memory. The memory consists \nof the whole virtual memory except the current stack frame. Generally, we will refer to the state at \na program point; in such instances we will limit the state to only the live elements. We .rst de.ne a \nsuitable notion of equality: De.nition 1. Target T is equivalent to rewrite R if for all possible states \ns, when execution of T is started from s and ' T terminates in .nal state s without aborting then when \nrewrite R is executed from the initial state s, R terminates ' in state s without aborting. This de.nition \ncaptures the fact that if T terminates on an input then so does R. Hence, this de.nition is richer than \npartial equivalence. A segmentation fault or a .oating point exception are examples of aborting. The \nasymmetric notion of equality in De.nition 1 seems necessary to validate several useful compiler optimizations. \nIf the target T aborts on some inputs, optimizing compilers are free to use an R with any behavior, de.ned \nor unde.ned, on that input. The notion of equality we use is captured in the veri.cation conditions (VCs) \ngenerated to symbolically compare the behavior of T and R. Since our VCs are in a rich logic, by modifying \nthe VCs that are generated our approach can handle a wide range of speci.cations of equality, including \nrequiring the target and rewrite to behave identically on every input or allowing the rewrite to differ \nfrom the target on any input that causes the target to abort. Let t be a code path, that is, a sequence \nof instructions in T , r a code path in R, and C the pair (t, r). Abusing the notation of Hoare logic, \nwe de.ne proof obligations. De.nition 2. For predicates P and Q and code paths t and r, a proof obligation \n{P }(t, r){Q}, states that if t starts execution from a state s1, r start execution from a state s2, \n' P (s1, s2) holds, and t terminates in a .nal state s without 1 aborting, then r does not abort and \nfor all possible .nal ' '' states s of r, Q(s1, s 2) holds. 2 For example {eax = eax '}(mul $2 eax, shl \n$1 eax){eax = eax ' } says if t and r begin with values of eax equal and t performs an unsigned multiplication \nby two and r shifts eax left by one bit then the resulting states agree on the value of eax. We want \nto generate suf.ciently many proof obligations, such that if they are all satis.ed then they constitute \nan inductive proof of equivalence of T and R. 3.1 Generating Proof Obligations Each proof obligation \n{P }C{Q} has two components: the code paths C and the predicates P and Q on the states of T and R. We \ndiscuss the generation of each component. 3.1.1 Generating Corresponding Paths We generate corresponding \npaths t and r for proof obliga\u00adtions using cutpoints [39]. A cutpoint n is a pair of program points (.T \n, .R) where .T . T and .R . R. We select cut\u00adpoints using the following heuristic: Choose pairs of pro\u00adgram \npoints where the corresponding memory states agree on the largest number of values. Cutpoints with higher \nagree\u00adment between the memory states of T and R have simpler invariants than cutpoints where the relationship \nbetween the two memory states is more involved. The cutpoints are discovered using a brute force search \nthat compares the execution data for pairs of program points. We create a candidate cutpoint for every \nprogram point pair s s (.T , .R). For every test s, we .nd m(resp. m), the .T .R number of times control \n.ow passes through .T (resp. .R) when the execution of T (resp. R) starts in the state s. If s there \ndo not exist constants a, b s.t. .s.ms = am+ b .T .R then (.T , .R) is rejected as a candidate cutpoint. \nWe also re\u00adject candidates for which the number of heap locations in the observed heap states for T and \nR at (.T , .R) is not a con\u00adstant across all tests. Note that this operation is possible as we run only \nterminating tests and thus the memory footprint is bounded. For the remaining candidates, we assign them \na penalty representing how different the observed heap states are for T and R at (.T , .R). We pick candidate \ncutpoints in increasing order of penalty until T and R are decomposed into loop free segments. Next, \nredundant candidates are re\u00admoved so that the minimum number of program point pairs are selected as candidate \ncutpoints; a candidate n is redun\u00addant if the decomposition is still loop free after removing n. The \nchoice of a minimum set of cutpoints may not be unique and we simply try multiple proofs, with one proof \ncorresponding to every available choice. Multiple cutpoints can be associated with the same program point \nof T or R. For example, if a loop unrolling has been performed then there might be several cutpoints \nsharing the same program point of T . If satisfactory cutpoints cannot be found then our tech\u00adnique fails, \nwhich highlights a limitation of our technique: we fail to prove equivalence for programs which differ \nfrom each other at an unbounded number of memory locations. For example, a loop fusion transformation \nor reordering an array traversal results in loops that cannot be proven equiv\u00adalent using our method. \nHandling such loops requires quan\u00adti.ed invariants, for which the current state of the art in in\u00advariant \ninference is less mature than for quanti.er-free in\u00advariants. Generally, the techniques for quanti.ed \ninvariants require manually provided templates; the templates prevent the introduction of an unbounded \nnumber of quanti.ed vari\u00adables. Moreover, theorem provers are not as robust for quan\u00adti.ed formulas as \nthey are for quanti.er-free formulas. The set of cutpoints is called a cutset S. For every pair of cutpoints \nn1 and n2 in a cutset S, we de.ne a transition t = n1 . n2 from n1 to n2. This transition is associated \nwith a set of static code paths P1 of T and P2 of R. P1 and P2 consists of code paths which go from n1 \nto n2 without pass\u00ading through some other cutpoint n3 . S . The instructions executed for a terminating \nand non-aborting execution of T and R can be represented by a sequence of transitions. For example, in \nFigure 1, let us denote by t1 the transition from a to b and t2 from b to c. Then the execution discussed \nin Section 2 represents the sequence of transitions t1t2. Code paths i and ii of Figure 1 are associated \nwith t1 and vi is associated with t2. Now we de.ne corresponding paths formally: De.nition 3. Given a \ntarget T , a rewrite R, and a cutset S, a code path t of T corresponds to a code path r of R if they \nare associated with the same transition t in S and for some execution t1, . . . , t, . . . , tm, for \nthe transition t, if T executes the code path t then R executes r. Now we discuss our data-driven algorithm \nfor generation of corresponding paths. T and R are run on a test input and we record the trace of instructions \nexecuted in both functions. We analyze this pair of traces to build a set C of corresponding paths. The \nalgorithm walks over the traces in parallel until a cutpoint is reached in both. The pair of paths taken \nfrom the previous pair of cutpoints is added to C. This process is repeated until we reach the end of \nthe traces. We then run the two functions on another input and repeat the analysis. If test coverage \nis suf.cient, C will contain all corresponding paths. We add additional proof obligations to ensure \nthat we have not missed any corresponding paths. For static paths in T between cutpoints that no test \nexecutes, we add an ar\u00adbitrary path from R between the same cutpoints as a corre\u00adsponding path in C. \nFor a path p of T associated with a tran\u00adsition t, we add the veri.cation condition that if the target \nexecutes p then the rewrite can only execute the paths which correspond to p in C. If these proof obligations \nfail then this means that our data is insuf.cient and there are more corre\u00adsponding paths to be discovered. \n 3.1.2 Generating Invariants Following Necula [25], we consider invariants that are equal\u00adities over \nfeatures, which are registers, stack locations and a .nite set . of heap locations. Also as in [25], \nwe replace reads from and writes to stack locations by reads and writes to named temporaries. We perform \na liveness analysis over T and R and for each cutpoint (.1, .2) obtain the live features (see Section \n4). Be\u00adcause x86 has sub-registers, these features can be of differ\u00adent bit-widths. For example, it might \nbe that only one byte of a 64-bit register is live. We create a set of matrices, one for each cutpoint \n(.1, .2), whose columns are the live features at .1 and .2. If the bit-width of the longest live feature \nis x bits, we create one column for every x bit live feature. For every live feature of fewer than x \nbits, we create two columns: one with the feature s value zero-extended to x bits and the other with \nthe value sign-extended. The sign-and zero-extensions are needed because x86 instructions implicitly \nperform these operations on registers of different bit-widths. We instrument the functions to record \nthe values of the features at the cutpoints; a snapshot of the state at one cut\u00adpoint corresponds to \na single matrix row. If the cutpoint (.1, .2) is executed m times then we have m rows in the matrix for \n(.1, .2). Note that there are no negative values in the matrices: negative numbers become large positive \nnum\u00adbers. We use elementary linear algebra to compute the linear equality relationships between features. \nFor the matrix asso\u00adciated with each cutpoint, we compute its nullspace or ker\u00adnel. Every vector of the \nnullspace corresponds to an equality relationship between features at that cutpoint for all test in\u00adputs. \nWe take a conjunction of equalities generated by all vectors in the basis of the nullspace and return \nthe resulting predicate as a candidate invariant for our candidate simula\u00adtion relation. For example, \nif our matrix has three features and two rows: . . ' eax ebx eax . . 11 1 22 2 then one possible basis \nof the nullspace consists of the vec\u00adtors [1, -1, 0] and [0, 1, -1]. These vectors correspond to ' the \nequalities eax * (1) + ebx * (-1) + eax * 0 = 0 and eax * 0 + ebx * (1) + eax ' * (-1) = 0. Hence the \ncandidate invariant is eax = ebx . ebx = eax '. Note that the heap locations, which are not included \nin features, are implicitly constrained to have identical values for T and R. Hence, the above invariant \nincludes an implicit equality H = H ', that is, for all heap addresses, at the cutpoint, T and R have \niden\u00adtical values. We use a nullspace algorithm for rational ma\u00adtrices, which ensures that any discovered \nequality is exact. The cutpoints are labeled with these equalities. One desir\u00adable feature of nullspaces \nis that no sound equality relation\u00adship is missed. It can produce spurious equality relationships (for \nlack of suf.cient data) but if an equality holds statically then it will be generated [34]. Intuitively, \nthis is because ev\u00adery possible equality is contained in the candidate invariant unless there is a test \nthat violates it. In the extreme, when we have zero states in our data then the candidate invariant consists \nof every possible equality between the features, and hence it also includes the equalities present in \nthe true invari\u00adants. We generalize the results in [34], which uses nullspaces to compute invariants \nfor a single program, to features: Lemma 1. If x is a set of features at a cutpoint n, and I(x) is the \nstrongest invariant at n that holds statically and is ex\u00adpressible by conjunctions of linear equalities, \nthen the can\u00addidate invariant I(x) obtained by computing the nullspace of test data is a sound under-approximation \nof I, that is, I . I . Before we discuss how candidate invariants are checked to see if they are in fact \ninvariants, we brie.y digress to discuss how our approach extends to more complex function bodies. 3.1.3 \nExtensions Our approach easily generalizes to functions with multiple natural loops, including nested \nloops. We identify the cutset of T and R and identify cutpoints and transitions between two cutpoints \nif there is a static path from one cutpoint to the other. As is standard, we want every loop to contain \nat least one cutpoint of the cutset. The algorithm described above for generating proof obligations remains \nunchanged: run tests, generate corresponding paths, and generate equality relationships at cutpoints. \nWe can also extend our approach to handle loops that call other functions. The function call is a cutpoint \nrequiring the invariant that the value of arguments and memory is same across T and R and after the call \nwe can assume that the memory and return values are equal in the proof obligations. We generate additional \nobligations to check that the order of function calls is preserved. Our approach easily extends to generate \nnon-linear equal\u00adities of a given degree d for invariants using ideas from in\u00advariant inference: We simply \ncreate a new feature for ev\u00adery monomial up to the degree d from the existing fea\u00adtures [26, 34]. Say \nif eax and ebx are features and the cho\u00ad 2 sen degree is 2 then we add eax , ebx2, and eax * ebx to \nthe existing set of features. Now the nullspace yields linear equalities over the extended features which \nrepresent poly\u00adnomial equalities over the original features. Since non-linear invariants of degree d \ncan represent disjunctions of d linear 2 2 equalities (e.g., x= y= x = y . x = -y), the ex\u00adpressiveness \nof the invariants we can infer includes boolean combinations of linear equalities with a given number \nof dis\u00adjunctions.  3.2 Checking Proof Obligations For every transition t between cutpoints n1 and n2 \nwhere the candidate invariant at n1 is equality relationship P , the candidate invariant at n2 is equality \nrelationship Q, and C is a pair of corresponding paths associated with t, we construct a proof obligation \n{P }C{Q}. As noted in Section 2, we also generate additional veri.cation conditions or VCs to check that \nall pairs of corresponding paths are covered. Once the VCs have been obtained, they can be sent to an \noff-the\u00adshelf theorem prover. These queries are in the quanti.er-free theory of bit-vectors. More details \nabout generating VCs can be found in Section 4.6. The proof obligations {P }C{Q}are of three types: \n{E}C{Q}, where E represents exact equivalence be\u00adtween states. Here the corresponding paths are code \npaths from the start of T and R to a cutpoint and Q is the equal\u00adity relationship associated with that \ncutpoint.  {P }C{Q}, where for C = (t, r), t and r start at a cutpoint n1 and end at a cutpoint n2. \nP and Q are the equality relationships at n1 and n2.  {P }C{F }, where the corresponding paths start \nat a cut\u00adpoint and end at a return statement of T and R. Here F expresses that the return values are \nequal and the memory states are equivalent.  If proof obligation {E}C{Q} or {P }C{Q} fails then we can \nobtain a counter-example from the decision procedure and follow the approach of [34]: If the counter-example \nviolates some equality of Q then we incorporate the data from the counter-example in the appropriate \nmatrix. Next, we recompute the nullspace to obtain new equality relation\u00adships satisfying both the data \nand the counter-example. If the nullspace is just the null vector then we return the trivial invariant \ntrue. This process systematically guides our data\u00addriven algorithm to generate equality relationships \nthat are actual invariants. It also removes some artifacts of test cases: for example, in all tests some \npointer p is assigned the same address a then we can infer a spurious equality relationship that p = \na. A counter-example can remove this spurious equality. However, if the counter-example satis.es Q then \nthe proof fails; this can happen because r might abort on some state for which t does not. Note that \n[34] contains a completeness theorem, which is possible because it deals with a restricted language with \nno heap and has no speci.cation to verify. Since we han\u00addle memory, the possibility of aborting, and \nneed to prove equivalence, we are only able to prove a soundness theorem: Theorem 1. If all VCs are \nproven then the target is equiva\u00adlent to the rewrite and the cutset and the invariants together constitute \na simulation relation. 4. Implementation DDEC is a prototype implementation of the algorithm dis\u00adcussed \nin Section 3. We discuss the most important and in\u00adteresting features of the implementation below. 4.1 \nLiveness Computation Several components of D DE C require the live variables at a program point. For \nthe most part, liveness is computed us\u00ading a standard data.ow algorithm, but we note the following modi.cation \nfor the x86 architecture. As mentioned previ\u00adously, for 64-bit x86 some general purpose registers have \nsubregisters. For example, dil, di, and edi refer to the low\u00adest 8, 16, and 32-bits of rdi, respectively: \ndil . di . edi . rdi To account for this register aliasing, when calculating the read set of an instruction \nwe include the registers it reads, along with all subsets of those registers. An instruction that reads \ndi, for example, also reads dil. Similarly, when cal\u00adculating the write set of an instruction we include \nthe reg\u00adisters it writes, along with all super-and subsets of those registers. Some instructions also \nproduce unde.ned register values, which are treated as killing any live values. 4.2 Testcase Generation \nFor programs that deal exclusively with stack and register values, DDEC is able to produce tests automatically. \nUsing the control .ow graph of the target (recall the distinction between the target and rewrite introduced \nin Section 3), DDEC identi.es the set of live-in registers and generates random values as necessary. \nFor programs that dereference heap locations, DDEC requires a user-de.ned environment in which to execute \nthe target. For large software projects, we expect that a representative set of whole-program tests will \nexist, and DDEC may simply observe the behavior of the code during normal program execution. When no \nsuch inputs exist, a user must provide a small unit test harness which will establish whatever heap structures \nare necessary to enable the functions to execute correctly. 4.3 Target Tracing To observe the values \nof live program variables at cut-points, we rely on the ability to instrument the target and observe \nthose values dynamically as they appear under representa\u00adtive inputs. This instrumentation is performed \nusing a cus\u00adtom light-weight JIT assembler for 64-bit x86; the trace() function is shown in Figure 2. \nPrior to execution, an array Figure 2. Implementation of instrumentation for target and rewrite tracing \nusing a JIT assembler. Targets are assumed safe and instrumented using the trace() function. Rewrites \nare instrumented using the sandbox() function which is parameterized by values observed during the execution \nof the target.  of trace states are allocated on the heap. For every instruc\u00adtion executed by the target, \nwe emit a call to the record() function to copy the majority of the hardware state (general purpose, \nSSE, and condition registers) as well as values read from or written to the heap, to the back of the \narray. Array bounds are tracked (not shown), and in the event of over.ow additional capacity is added \nas necessary. When the target function completes execution, the trace array holds a com\u00adplete record \nof the values manipulated by the target in both registers and memory. Because we already assume that \nusers supply tests for target functions that access data structures in memory, in our prototype we have \nalso assumed that the target is safe and will not crash the machine or corrupt DDEC s state on any of \nthe supplied inputs. This assumption held for the experiments in this paper, however for a production \ntool it would be necessary to isolate the target from the tool using well-known but more involved techniques \n(e.g., emulation).  4.4 Rewrite Tracing The correctness assumptions that we make for the target do not \nhold for all the rewrites in our experiments. For exam\u00adple, some candidate rewrites can and do dereference \ninvalid memory locations (see Section 5.4). Thus, our prototype instruments rewrites using the heavier-weight \nsandbox() function (see Figure 2). We can, however, take advantage of information observed in tracing \nthe target to simplify the tracing of the rewrite. From the execution of the target, we obtain the maximum \nstack size used along with the minimum and maximum heap addresses that were dereferenced. Using these \nvalues, we de\u00ad.ne stack and heap sandboxes to guard the execution of the rewrite. Speci.cally, we identify \nthe stack pointer, s, which de.nes the upper bound on the frame used by the rewrite, and from this value \nidentify s ', a location which de.nes a frame no larger than the one used by the target. Similarly, we \nallocate a heap sandbox array large enough to contain each heap dereference performed by the target without \naliasing containing values which are initialized to the live-in mem\u00adory values dereferenced by the target. \nMemory dereferences are guarded by the sandbox addr() function, which pre\u00adserves stack accesses inside \nthe bounds of the stack sandbox, redirects valid heap accesses to the heap sandbox, and traps all other \naccesses and instead produces a safe premature ter\u00admination. If necessary, both sandboxes may be scaled \nby a constant user-de.ned factor to allow for the possibility of a rewrite with greater memory requirements \nthan the target. In addition to sandboxing memory accesses, several other behaviors need to be checked \nto protect D DE C from unde\u00ad.ned behavior. First, there is the possibility that the rewrite will go into \nan in.nite loop. This behavior is guarded by a call to sandbox jump(), which counts the number of times \nthat a backedge is taken and causes a premature termination if a bound calculated from the number of \nbackwards jumps taken in the target execution is exceeded. Second, we must guarantee that return instructions \ntake place only after cer\u00adtain invariants speci.c to the x86 application binary interface have been restored. \nThis property is guaranteed by a call to sandbox return(), which checks that the value of callee\u00adsaved \nregisters are restored to the state they held when the rewrite began executing.  4.5 Invariant Generation \nInvariants are computed using the nullspace function of the Integer Matrix Library [5] which is specialized \nfor com\u00adputing the nullspace of integer and rational matrices using p\u00adadic arithmetic. DDEC uses sixteen \nrandom tests to generate data for invariant computation. For the experiments in this paper, these tests \nwere suf.cient for obtaining sound invari\u00adants and all necessary corresponding paths. For a production \nsystem, a larger number of tests would likely be necessary. However at under 2 ms for each null space \ncomputation, we do not expect that this computation would be a bottleneck.  4.6 VC Generation Proof \nobligations are discharged using Z3 [7]. Because Z3 does not currently provide support for .oating point \noper\u00adations, D DE C s ability to reason about such instructions is limited as well. However, Z3 does \nhave a complete algo\u00adrithm for real numbers and one might be tempted to use this capability to reason \nabout .oating point programs. Unfor\u00adtunately, .oating point semantics are not over-approximated by reals \na simple example is the non-associativity of .oat\u00ading point addition. Hence, sound optimizations using \nreal se\u00admantics may be unsound using .oating point semantics. We manually encode x86 instructions as \nZ3 formulas, pri\u00admarily due to lack of access to formal speci.cation of x86 instructions at the hardware \nlevel. For example, some x86 in\u00adstructions, such as popcnt, are informally described by Intel as loops. \nWherever possible, we deferred to corresponding loop-free implementations given in A Hacker s Delight \n[40]. Our formulas precisely model the complexity of the x86 instruction set, which in some cases is \nquite sophisticated (consider the crc instruction, which considers bit-vectors as polynomials in Z2 and \nperforms a polynomial division). For\u00admula correctness is a necessary prerequisite for proof cor\u00adrectness, \nand unfortunately, testing is the only available op\u00adtion for ensuring this property. Each formula was \ntested ex\u00adtensively against hardware semi-automatically to check cor\u00adrectness. In the process we rediscovered \nknown instances in which the x86 instruction set deviates from its speci.ca\u00adtion [12]. For such instances, \nour formulas encode a sound over-approximation of the observed hardware behavior and the speci.cation. \nWe generate VCs in the quanti.er-free theory of bit\u00advector arithmetic. Z3 is complete for this theory \n[41] and is able to soundly analyze the effect of x86 instructions on the machine state with bit-wise \nprecision. When generating constraints, registers are modeled, depending on bit-width, as between 8-and \n128-bit bit-vectors. Memory is modeled as two vectors: one of 64-bit addresses and one of corre\u00adsponding \n8-bit values (x86 is byte addressable). For 32-bit x86, addresses are restricted to 32 bits. D DE C translates \nproof obligations {P }(t, r){Q}, where P and Q are predicates over registers (the invariants can be more \ngeneral as described in Section 3.1.2), to a VC as follows. D DE C .rst asserts the constraint P . It \nthen iter\u00adates over the instructions in t, written in SSA form, and for each instruction asserts a constraint \nwhich encodes the trans\u00adformation the instruction induces on the current hardware state. These constraints \nare chained together to produce a constraint on the .nal state of live outputs with respect to t. Analogous \nconstraints are asserted for r. In our implemen\u00adtation, operators that are very expensive for Z3 to analyze, \nsuch as bit vector multiplication and division, are replaced by uninterpreted functions constrained by \nsome common ax\u00adioms. Finally, for all pairs of memory accesses at addresses addr1 and addr2 , DDEC asserts \nadditional constraints re\u00adlating their values: addr1 = addr2 . val1 = val2 . These aliasing constraints \ngrow quadratically in the number of ad\u00addresses, though dependency analysis can simplify the con\u00adstraints \nin many cases. Using these constraints, DDEC constructs a Z3 query which asks whether there exists an \ninitial state satisfying P that causes the two code paths to produce values for live outputs which either \nviolate Q or result in different mem\u00adory states. If the answer is no, then the obligation is dis\u00adcharged, \notherwise the prover produces a counter example, and DDEC fails to verify equivalence. If every VC is \ndis\u00adcharged successfully, then DDEC has proven that the two functions are equivalent. Although the implementation \ndescribed above is cor\u00adrect, it is overly conservative with respect to stack accesses. Speci.cally, an \naccess to a spill slot [2] will appear indis\u00adtinguishable from a memory dereference, and Z3 will dis\u00adcover \na counter-example in which the input addresses to t and r alias with respect to that slot. As a result, \nany sound optimized code which eliminates stack traf.c will be re\u00adjected. We address this issue by borrowing \nan idea from Necula [25], who solves this problem by replacing spill slots with temporary registers that \neliminate the possibil\u00adity of aliasing between addresses passed as arguments and the stack frame. For \nwell-studied compilers such as gcc and CO M P CE RT, simple pattern matching heuristics are known to \nbe well-suited to this task [25]. For example, for code compiled with gcc, all stack accesses are at \nconstant offsets from the register rbp. We can create a new register for rbp-8, rbp-16, and so on and \nmodel loads and stores from the stack frame by loads and stores from these temporaries. Model\u00ading spill \nslots in this way could result in unsound results if an input address is used to form an offset into \nthe current stack frame. However, this behavior is unde.ned even in the high-level languages that in \ntheory permit it (e.g., C) and, to the best of our knowledge, existing optimizing compilers are also \nunsound for such programs. Given that we have gone to the effort to construct a faith\u00adful symbolic encoding \nof the x86 instruction set, the reader might wonder why we do not simply obtain the simula\u00adtion relation \nstatically what does data add? The answer is that inference is harder than checking. Consider an exam\u00adple \nwhere we compute the dot product of two 32-bit arrays, where the multiplication of two 32-bit unsigned \nnumbers is used to produce a 64-bit result. Say the target uses Karat\u00adsuba s trick [19] and performs \nthree 32-bit signed multiplica\u00adtions to obtain a 64-bit result, whereas the rewrite uses a spe\u00adcial x86 \ninstruction that performs an unsigned multiplication of two 32-bit numbers and directly produces a 64-bit \nresult. A static analysis that attempts to discover the relationship between Karatsuba s trick and the \nspecial x86 instruction has a very large search space containing many other plausible proof strategies \nto sift through, and in fact we know of no in\u00adference technique that can reliably perform such reasoning. \nIn contrast, with DD EC the equality simply manifests itself in the data, and knowing that this speci.c \nequality is what needs to be checked narrows the search space to the point that off-the-shelf solvers \ncan verify this fact automatically. 5. Experiments We summarize our experiments with DDEC on a num\u00adber \nof benchmarks drawn from both the literature and from existing compiler test suites. In doing so, we \ndemonstrate the relationship between DD EC and the state of the art in translation validation, the use \nof DDEC in the optimiza\u00adtion of a production codebase, and show how DDEC en\u00adables the design of novel \napplications using existing com\u00adpiler toolchains. Speci.cally, we use DDEC to combine the correctness \nguarantees of CO M P CE RT with the performance properties of gcc -O2, and to extend the applicability \nof a binary superoptimizer to functions containing loops. Program LOC #Loop Run-time lerner1a1b 29/26 \n1 19.39s lerner3b3c 32/32 2 102.89s unroll 13/20 1 75.04s off-by-one 15/14 1 0.13s  Table 1. Performance \nresults for micro-benchmarks. LOC shows lines of assembly of target/rewrite pair. Program LOC #Loop Run-time \nSpeedup Test lerner1a 39/17 1 12.94s none 4 lerner3b 43/27 2 53.72s 1.49x 5 bansal 19/10 1 9.89s 1.02x \n2 chomp 30/22 1 11.00s none 3 fannkuch 30/23 1 17.03s 1.11x 4 knucleotide 28/21 1 6.56s none 3 lists \n21/17 1 1.40s none 3 nsievebits * 70/38 2 36.44s 1.20x 5 nsieve * 44/31 2 166.31s 1.06x 5 qsort * 64/56 \n3 140.87s 1.05x 7 sha1 * 79/28 1 12888.87s 1.07x 8 Table 2. Performance results for CO M P CE RT and \ngcc equivalence checking for the integer subset of the CO M -PCE RT benchmarks and the benchmarks in \nthis paper. LOC shows lines of assembly code for the binaries generated by CO M P CE RT/gcc. Test is \nthe maximum number of random tests required to generate a proof. A star indicates that a uni\u00ad.cation \nof jump instructions was required. Program LOC #Loop Run-time Speedup O0/O3 Bansal 9/6 1 44s/5492.75s \n1.58x/1.04x SAXPY 9/9 1 211s/0.62s 9.22x/1.48x  Table 3. Performance results for gcc and S TOKE+DDEC \nequivalence checking for the loop failure benchmarks in [33]. LOC shows lines of assembly code for the \nbina\u00adries generated by STOKE/STO K E+DD EC. Run-times for search/veri.cation are shown along with speedups \nover gcc -O0/O3. Experiments were performed on a 3.40 GHz Intel Core i7-2600 CPU with 16 GB of RAM. For \neach experiment we report the number of assembly instructions for target and rewrite, the number of loops \ncontained in each function, the run-time required for DDEC to verify equivalence, and where applicable \nthe observed performance improvement between target and rewrite. Run-times for all experiments were tractable, \nvarying from under a second to several hours, depending on the complexity of the constraints. Memory \nusage was not an issue and did not exceed 500Mb. It is worth noting that many of the invariants discovered \nby DDEC, such as 4*eax = edx ' + 3 and 10*eax +edx = ecx ', were non-trivial. In all cases the run-time \nrequired to run tests, infer equality relationships and generate proof obligations was minimal (a few \nmilliseconds) compared to the run-time required to discharge the VCs. Essentially all of DDEC s run-time \nwas spent in Z3 queries. // Target: while( p != null ) { p=p->next; ... } // Rewrite (Unrolled once): \n while( p != null ) { p=p->next; ... if ( p != null ) { p=p->next; ... } } Figure 3. Abstracted codes \n(target and rewrite) for unroll of Table 1. // Target: for ( i = len -1; i >= 0; --i ) { ... } // Rewrite \n(with off-by-one error): for ( i = len /*BUG*/; i >= 0; --i ) { ... } Figure 4. Abstracted codes (target \nand rewrite) for off-by-one of Table 1. 5.1 Micro-benchmarks In this section, we attempt to provide \na detailed compari\u00adson between D DE C and techniques based on equality satu\u00adration [18, 36]. Equality \nsaturation [36] is a state of the art technique for verifying compiler optimizations which relies on \nexpert-written equality rules, such as multiplication by two is equivalent to a bit shift . Beginning \nfrom both the tar\u00adget and the rewrite, the repeated application of these rules is used to attempt to \nidentify a common representation of both functions, the existence of which implies equality. Un\u00adsurprisingly, \nthe technique is precisely as good as the ex\u00adpert rules that it has at its disposal. Missing rules correspond \nto optimizations for which equivalence cannot be proven. Furthermore, identifying such rules for a CISC \narchitecture such as x86 is a daunting task. As a result, the application of equality saturation has \nbeen limited to the veri.cation of optimizations in several intermediate RTL languages. We compare DDEC \nto equality saturation using the two motivating examples described in the original paper [36], the .rst \nof which appears in Section 2; these correspond to the .rst two rows in Table 1. Each example consists \nof two pairs of programs demonstrating strength reductions. We compile each pair with gcc -O0 to both \ntransform the functions into a format that DDEC accepts and to preserve the structure of the original \noptimizations as best as possible. DDEC successfully proves the equivalence of the resulting binaries. \nEquality saturation, in its current form, is unable to han\u00addle important optimizations such as loop unrolling \n[36]. The unroll benchmark in Table 1 is an equivalence checking task involving loop unrolling. We take \na program which walks over a linked list and unroll it to obtain a new program that walks over two elements \nin every iteration (Figure 3). Hence, we are able to handle optimizations that equality sat\u00aduration cannot \neven in the presence of an extensive set of expert written rules. When given two programs that are semantically \ndiffer\u00adent, the process of equality saturation is not guaranteed to terminate. It simply continues applying \nrules inde.nitely in an attempt to prove that the two programs are equal. When given two semantically \ndifferent programs, DDEC always terminates with a counter-example (Z3 is complete for the VCs we use). \nNote that counter-examples are also possible even for equivalent programs because DDEC is incomplete. \nCounter-examples from a failed proof can be used to explain why DDEC believes that a target/rewrite pair \nare not equivalent. To demonstrate this ability, we take a pro\u00adgram which walks backwards over an array \nand introduce an off-by-one error (Figure 4). We compile both programs with gcc -O0 and use the correct \nprogram as the target and the buggy version as the rewrite. When we ask DDEC to prove the equivalence \nof the two programs, DD EC fails and terminates with a counter-example (off-by-one in Ta\u00adble 1). Hence, \nDDEC can be used for .nding equivalence bugs. However, in general, it is dif.cult to map the counter\u00adexample \nat the assembly level to the source code, and espe\u00adcially in the presence of compiler optimizations. \nA version of DDEC that works on source code and .nds equivalence bugs is left as future work. 5.2 Full \nSystem Benchmark One criticism of DDEC is that it is not purely static: it re\u00adquires tests. However, \nwe believe that for many systems ex\u00adisting regression tests should suf.ce. To validate this be\u00adlief, \nwe perform a case study with OP E N SSL. The core computation routine of OP E N SSL is big number multiplica\u00adtion: \na loop which multiplies n-bit numbers stored as arrays. OP E N SSL includes performance tests which use \nthis mul\u00adtiplication loop extensively. For example, the RSA perfor\u00admance test included with OP E N SS \nL executes the core more than .fteen million times. We instrument O P E N S SL to obtain these tests \nand se\u00adlect sixteen tests at random from the large number per\u00adformed. We compile the core using gcc -O0 \nand gcc -O3 and use the tests to drive DDEC, which is then able to prove the equivalence in about two \nhours. We also .nd that the maximum time taken by any individual VC is ten minutes. Since checking proof \nobligations is an embarrassingly par\u00adallel task, if we assign a single cpu for every obligation then \nthe proof can be obtained in ten minutes. When we measure the sensitivity of DDEC to the number of tests \nfor this ex\u00adample, we .nd that 6-8 randomly selected tests are suf.cient to obtain a proof. Hence, for \nverifying the optimizations of kernels, it seems possible to obtain suf.cient tests from existing test \nsuites to drive veri.cation. The tests included with programs might not exercise all parts of the code \nbut they will exercise the performance-critical parts well and hence we believe DDEC can be successfully \napplied to the performance-critical ker\u00adnels. Being data-driven, our technique will fail to prove in\u00adteresting \noptimizations performed on dead code or the part of code exercised rarely by tests; a static equivalence \ncheck\u00ading engine for x86, if it existed, could have succeeded here.  5.3 CompCert CO M P CE RT is a \ncerti.ed compiler for a subset of the C programming language: it is guaranteed to produce binaries that \nare semantically equivalent to the input source code. CO M P CE RT 1.12, the current version, does not \nperform loop optimizations and its compilation model does not .t well with CISC architectures such as \nx86, due to their paucity of [general purpose] registers [21]. We use DDE C to verify the equivalence \nof binaries gen\u00aderated by CO M P CE RT and gcc -O2 -m32. The -m32 .ag is necessary for compatibility \nwith CO M P CE RT, which only produces 32-bit x86 binaries. Furthermore, optimization is restricted to \n-O2 because, for these benchmarks, higher op\u00adtimization levels produce only syntactic differences that \ndo not affect DDEC. In doing so, we are able to extend the cor\u00adrectness guarantees made by CO M P CE \nRT to the more per\u00adformant but uncerti.ed code generated by gcc. Performance data is shown in Table 2 \nfor the integer sub\u00adset of the CO M P CE RT compiler test suite (the benchmarks in the test/c directory \nof the CO M P CE RT 1.12 download) and several of the benchmarks described in this section. For the CO \nM P CE RT benchmarks, we pro.le each program and re\u00adport results for the one loop (possibly containing \nother loops) that dominates execution time. The results are encouraging; DDEC is able to prove equivalence \nin all cases. We note, however, that DD EC did encounter dif.culty with some of the benchmarks due to \nthe restrictive logic that it uses. The problem is illustrated by the following example: Target: if(0<n) \n{for (i=0; i<n; i++);} return i; Rewrite: if(0<n) {for (i=0; i!=n; i++);} return i; To prove the equivalence \nof these two functions, DDEC re\u00adquires the inductive invariant i = i ' . n = n ' . i = n. Cru\u00adcially, \ni = n is inexpressible with equalities. For the .nal four benchmarks in Table 2 (the ones marked with \na star), DDEC reported failure due to gcc replacing an inequality by a dis-equality. For these benchmarks \nit was necessary to manually make the test predicates identical before perform\u00ading a successful equality \nveri.cation. DDEC can also be combined with an abstract interpreter over binaries [31, 37]: these are \ncapable of .nding invariants over state elements of a single program (such as i = n). It is also straightforward \nto extend DDEC to recognize this common special case, but a solution that incorporates inequality relationships \nbetween state elements of two programs in a general way is challeng\u00ading and we leave it for future work. \n Finally, we note that the long veri.cation time required for sha1 is due to the large numeric constants \nused by this cryptographic computation that lead to poor performance in the underlying Z3 theorem prover. \nWe con.rmed that if these constants are replaced by small integers the proof obligations are discharged \nin a few minutes. 5.4 STOKE STO KE [33] is an x86 binary superoptimizer based on the principle that \nprogram optimization can be cast as a stochas\u00adtic search problem. Given a suitable cost function and \nrun for long enough (which may be an extremely long time), STO KE is guaranteed to produce a code with \nthe lowest cost [33]. STOKE performs binary optimization by execut\u00ading up to billions of small random \nmodi.cations to a pro\u00adgram. For each modi.cation, STOKE evaluates a cost func\u00adtion representing a combination \nof correctness and perfor\u00admance metrics. STO KE accepts all modi.cations that de\u00adcrease the value of \nthis function, and with some probability also accepts modi.cations that increase the value. Although \nmost of the programs that STOKE considers are ill-formed, the sheer volume of programs that it evaluates \nleads it in practice to discover correct optimizations. STOKE runs for a user-de.ned amount of time and \nreturns the lowest cost program that it can prove equivalent to the original program. Previous work on \nSTOKE demonstrated promising re\u00adsults for loop-free kernels. Beginning from code compiled using llvm \n-O0, STOKE produces programs that outper\u00adform code produced by gcc -O3 and in some cases outper\u00adforms \nhandwritten assembly. Unfortunately, STOKE s ap\u00adplication to many interesting high-performance kernels \nis limited by its inability to reason about equivalence of codes containing loops. We address this limitation \nby extending STO KE s set of program transformations to include moves which enable loop optimizations \nand replacing S TOKE s equivalence checker by D D EC. Our version of STO K E is able to produce optimizations \nfor the loop benchmarks that could not be fully optimized in [33]. STOKE s underlying program representation \nis a con\u00adstant length sequence of 64-bit x86 instructions. STOK E uses a special [UNUSED] token in place \nof a valid x86 instruction to represent programs that are shorter than this length. STOKE explores candidate \nrewrites using the fol\u00adlowing transformations. Instruction moves replace a random instruction with either \nthe [UNUSED] token or with a com\u00adpletely different random instruction. Operand and opcode moves randomly \nreplace the value of a random operand or opcode respectively. And .nally, swap moves interchange two \ninstructions at random. S TOKE s program transforma\u00adtions are all intra-basic block, and it does not \nallow moves which modify control .ow structure. Our implementation of STOKE remedies these shortcomings \nby introducing an inter-basic block swap move (Figure 5f) and a basic block resizing move (Figure 5g), \nwhich changes the number of in\u00adstructions allowed inside a basic block. These simple exten\u00adFigure 5. \nSTOKE moves applied to a representative code (a). Instruction moves randomly produce the UNUSED token \n(b) or replace both opcode and operands (c). Opcode moves randomly change opcode (d). Operand moves randomly \nchange operand (e). Swap moves interchange two instructions either within or across basic blocks (f). \nResize moves randomly change the  allocation of instructions to basic blocks (g). sions are suf.cient \nto allow for the exploration of rewrites which include loop-invariant code motion, strength reduc\u00adtion, \nand most other classic loop optimizations. The cost function used by STOKE to evaluate transfor\u00admations \nincludes two terms, a correctness term and a per\u00adformance term. We leave the correctness term described \nby [33] unmodi.ed. Candidate rewrites are executed in a mem\u00adory sandbox on a representative set of tests. \nValues con\u00adtained in live memory and registers are compared against the corresponding values produced \nby the target and penal\u00adties are assessed for bits which are incorrect or appear in the wrong location. \nSTOKE approximates the performance of a candidate rewrite by summing the expected latencies of its constituent \ninstructions in nanoseconds. We modify this function to account for loop nesting depth, nd(\u00b7) as follows, \nwhere w is a constant which we set to 20. nd(bb) perf(C) =Elatency(i)\u00b7 w bb.Ci.bb We now discuss the \nfailure cases of STOKE: the bench\u00admarks of [33] for which STOKE produces code inferior to gcc -O3. The \nother benchmarks of [33] are loop free and are not relevant here. Our modi.ed version of STOKE is able \nto produce the two optimizations shown (simpli.ed) in the lower half of Figure 6. The optimizations produced \nby the original version of STOKE are shown above for refer\u00adence. Figure 6a is a linked-list traversal. \nThe kernel iterates over the elements in the list until it discovers a null pointer and multiplies each \nelement by two. Our modi.ed version of STOKE is able to cache the value of the head pointer in a register \nacross loop iterations. Figure 6b performs the BLAS vector function a \u00b7 x + xy. Whereas the original \nversion of STOKE is able to produce an optimization using vector in\u00adtrinsics, our version is able to \nfurther improve on that opti\u00admization by .rst performing a register renaming and remov\u00ading the invariant \ncomputation of a 128-bit constant from the Figure 6. Simpli.ed versions of the optimizations produced \nand veri.ed by our modi.ed version of STOKE. The itera\u00adtion variable in (a) is cached in a register (a \n). The compu\u00adtation of the 128-bit constant in (b) is removed from an inner loop (b ).  loop. The code \nmotion is not possible if the registers are not suitably renamed .rst. Performance data for these experiments \nare shown in Ta\u00adble 3. We note that the time spent verifying bansal is sig\u00adni.cantly greater than what \nwas reported for the COMPCERT benchmarks. This is because STOKE produces 64-bit as op\u00adposed to 32-bit \nx86 binaries, which results in more complex memory equality constraints. The aim of our experiments is \nto describe the effectiveness of our core ideas and stan\u00addard heuristics for constraint simpli.cation, \nwhich we have not implemented, can be applied to achieve better perfor\u00admance [8]. To illustrate just \nhow important constraint sim\u00adpli.cation is, for the SAXPY benchmark, following [42] we perform slicing \non VCs to eliminate constraints that are not relevant to the veri.cation task. The result is that veri.cation \nrequires less than a second, a signi.cant improvement and the fastest veri.cation time in our benchmark \nsuite. With\u00adout these constraint simpli.cations, DDEC times out after four hours. This suggests that \nthere is substantial room left for further performance optimization of DDE C s generated constraints; \nhowever such optimizations increase the size of the trusted code base, which is currently just the circuits \nfor instructions, VC generator, and the theorem prover. Since STOKE starts optimizations from llvm -O0 \ncompilations, our veri.cation results imply that STOKE+D DE C can pro\u00adduce binaries comparable to gcc \n-O3 in performance and provably equivalent to unoptimized binaries generated by llvm. 6. Related Work \nOur approach borrows a number of ideas from previous work on equivalence checking [6], translation validation \n[25], and software veri.cation [34]. Combining these ideas with our technique for guessing the simulation \nrelation from tests yields the .rst equivalence checking engine for loops written in x86. Program equivalence \nchecking is an old problem with references that date back to the 1950s. Equivalence check\u00ading is common \npractice in hardware veri.cation, where it is well known that cutpoints play a critical role in determin\u00ading \nequality. In sequential equivalence checking, for exam\u00adple, state-carrying hardware elements constitute \ncutpoints. Equivalence checking of low-level code has also been stud\u00adied for embedded software [1, 6, \n10, 11, 35]. None of these techniques support while loops and so are not applicable to our benchmarks. \nOur goal is more ambitious than the state of the art in equivalence checking for general purpose languages: \nDDEC is a practically useful, automatic, and sound procedure for checking equivalence of loops. UC-KLEE \n[30] performs a bounded model checking that checks equivalence for all in\u00adputs up to a certain size. \nIn another version of bounded model checking, differential symbolic execution [28] and SymDiff [20] bound \nthe number of iterations of loops. Se\u00admantic Diff [17] checks only whether dependencies are pre\u00adserved \nin two procedures. The approach in [23] handles nei\u00adther while loops nor pointers. Regression veri.cation \n[13] handles only partial equivalence: it does not deal with termi\u00adnation. As an alternative to DDEC, \none can imagine com\u00adposing two programs into a single program and then using an abstract interpreter \n[27]. However, the composition of [27] relies on syntactic heuristics that seem dif.cult to apply to \nbinaries. Even if the composition step is successful, it is not clear how one would argue about the termination \nbehaviors of the target and the rewrite (De.nition 1) from the com\u00adposed program and indeed [27] assumes \nterminating execu\u00adtions. Fractal symbolic analysis [24] and translation valida\u00adtion [14, 25, 29] are \ntwo techniques that reason about loops in general. Both rely on information about the compiler, such \nas the speci.c transformations that the compiler can per\u00adform. Hence, these are not directly applicable \nto the problem of equivalence checking of code of unknown provenance. If one is simply given two assembly \nprograms to check for equivalence there is no translation and hence translation validation is not applicable. \nConceptually, if one tries to port the approach of Nec\u00adula [25] to x86, then one will need to build a \nstatic analysis for x86 which is sound and precise enough to generate sim\u00adulation relations. This is \na decidedly non-trivial engineer\u00ading task and has never been done (see Section 4.6); DDEC side-steps \nthis issue by using concrete executions (i.e., tests) for .nding cutpoints and generalizing from tests \nto invari\u00adants. In addition, the constraints generated by symbolically executing x86 opcodes are complicated \nenough that we be\u00adlieve that the decision procedure of [25] will fail to infer equalities in most cases. \nNonetheless, compiler annotations are a rich source of high level information, and as a re\u00adsult translation \nvalidation techniques can handle transforma\u00adtions that D D EC currently cannot, such as reordering traver\u00adsals \nover matrices. For D D E C to handle such programs we would need better invariant inference algorithms \nthat can in\u00adfer quanti.ed invariants fully automatically. Generation of invariants from test data for \nveri.cation was pioneered by Daikon [9]. In a previous work [34], we computed equality invariants for \na single program using nullspace computations for a restricted language with as\u00adsignments, branches, \nand loops. Similar to DDEC, the can\u00addidate invariants were checked by a theorem prover. We have shown \nthat by choosing features appropriately, nullspaces can help infer simulation relations for the much \nmore com\u00adplex x86 language. Some of the most recent work on equivalence checking includes random interpretations \n[15] and equality satura\u00adtion [36, 38]. The former represents programs as polynomi\u00adals which it requires \nto be of low degree. Unfortunately, bit\u00admanipulations and other similar machine-level instructions are \nespecially problematic for this technique. For example, a shift left by one bit has a polynomial of degree \n263 for the carry .ag. Unlike equality saturation, DDEC does not rely on expert provided equality relationships \nbetween program constructs, which would be dif.cult to produce by hand for a CISC architecture such as \nx86. Further comparison with equality saturation is in Section 5.1. Superoptimizations and the related \nsynthesis task have previously been limited to sequences of loop-free code [4, 16, 18, 22]. Using DDEC, \nwe have presented the .rst re\u00adsults for a superoptimizer able to synthesize provably correct loops. 7. \nConclusion In this paper we describe a data-driven procedure for veri\u00adfying equivalence of two loops, \nusing it to verify the equiv\u00adalence of binaries produced by a certi.ed compiler against those produced \nby an optimizing compiler and also extend\u00ading a stochastic superoptimizer to perform loop optimiza\u00adtions. \nOther interesting applications are possible, such as checking that code refactoring preserves equivalence, \nand we hope to explore these in the future. The main limitations of the current implementation are restricted \nexpressiveness of the inferred invariants and the inability to handle .oating point instructions. With \nadvances in invariant inference and decision procedures, we hope to remove both limitations. Acknowledgments \nWe thank George Necula, Jan Vitek, and the anonymous reviewers for their constructive comments. This \nwork was supported by the Army High Performance Computing Re\u00adsearch Center and NSF grant CCF-0915766. \nThis material is also based on research sponsored by the Air Force Re\u00adsearch Laboratory, under agreement \nnumber FA8750-12-2\u00ad0020. The U.S. Government is authorized to reproduce and distribute reprints for Governmental \npurposes notwithstand\u00ading any copyright notation thereon. References [1] T. Arons, E. Elster, L. Fix, \nS. Mador-Haim, M. Mishaeli, J. Shalev, E. Singerman, A. Tiemeyer, M. Y. Vardi, and L. D. Zuck. Formal \nveri.cation of backward compatibility of mi\u00adcrocode. In CAV, pages 185 198, 2005. [2] D. F. Bacon, S. \nL. Graham, and O. J. Sharp. Compiler trans\u00adformations for high-performance computing. ACM Comput. Surv., \n26(4):345 420, 1994. [3] G. Balakrishnan and T. W. Reps. WYSINWYX: What you see is not what you execute. \nACM Trans. Program. Lang. Syst., 32(6), 2010. [4] S. Bansal and A. Aiken. Automatic generation of peephole \nsuperoptimizers. In ASPLOS, pages 394 403, 2006. [5] Z. Chen and A. Storjohann. A BLAS based C library \nfor exact linear algebra on integer matrices. In ISSAC, pages 92 99, 2005. [6] D. W. Currie, A. J. Hu, \nand S. P. Rajan. Automatic formal veri.cation of DSP software. In DAC, pages 130 135, 2000. [7] L. M. \nde Moura and N. Bj\u00f8rner. Z3: An ef.cient SMT solver. In TACAS, pages 337 340, 2008. [8] I. Dillig, T. \nDillig, and A. Aiken. Small formulas for large programs: On-line constraint simpli.cation in scalable \nstatic analysis. In SAS, pages 236 252, 2010. [9] M. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant, \nC. Pacheco, M. S. Tschantz, and C. Xiao. The Daikon sys\u00adtem for dynamic detection of likely invariants. \nSci. Comput. Program., 69(1-3):35 45, 2007. [10] X. Feng and A. J. Hu. Automatic formal veri.cation \nfor scheduled VLIW code. In LCTES-SCOPES, pages 85 92, 2002. [11] X. Feng and A. J. Hu. Cutpoints for \nformal equivalence veri.cation of embedded software. In EMSOFT, pages 307 316, 2005. [12] P. Godefroid \nand A. Taly. Automated synthesis of symbolic instruction encodings from I/O samples. In PLDI, pages 441 \n452, 2012. [13] B. Godlin and O. Strichman. Regression veri.cation. In DAC, pages 466 471, 2009. [14] \nB. Goldberg, L. D. Zuck, and C. W. Barrett. Into the loops: Practical issues in translation validation \nfor optimizing com\u00adpilers. Electr. Notes Theor. Comput. Sci., 132(1):53 71, 2005. [15] S. Gulwani. Program \nanalysis using random interpretation. In Ph.D. Dissertation, UC-Berkeley, 2005. [16] S. Gulwani, S. Jha, \nA. Tiwari, and R. Venkatesan. Synthesis of loop-free programs. In PLDI, pages 62 73, 2011. [17] D. Jackson \nand D. A. Ladd. Semantic Diff: A tool for summa\u00adrizing the effects of modi.cations. In ICSM, pages 243 \n252, 1994. [18] R. Joshi, G. Nelson, and Y. Zhou. Denali: A practical al\u00adgorithm for generating optimal \ncode. ACM Trans. Program. Lang. Syst., 28(6):967 989, 2006. [19] D. E. Knuth. The Art of Computer Programming, \nVolume II: Seminumerical Algorithms, 2nd Edition. Addison-Wesley, 1981. ISBN 0-201-03822-6. [20] S. \nK. Lahiri, C. Hawblitzel, M. Kawaguchi, and H. Reb elo. SYMDIFF: A language-agnostic semantic diff tool \nfor imper\u00adative programs. In CAV, pages 712 717, 2012. [21] X. Leroy. The CompCert C veri.ed compiler \ndocumentation and users manual, 2013. URL http://compcert.inria.fr/man/manual.pdf. [22] H. Massalin. \nSuperoptimizer -a look at the smallest program. In ASPLOS, pages 122 126, 1987. [23] T. Matsumoto, H. \nSaito, and M. Fujita. Equivalence checking of C programs by locally performing symbolic simulation on \ndependence graphs. In ISQED, pages 370 375, 2006. [24] V. Menon, K. Pingali, and N. Mateev. Fractal symbolic \nanaly\u00adsis. ACM Trans. Program. Lang. Syst., 25(6):776 813, 2003. [25] G. C. Necula. Translation validation \nfor an optimizing com\u00adpiler. In PLDI, pages 83 94, 2000. [26] T. Nguyen, D. Kapur, W. Weimer, and S. \nForrest. Using dynamic analysis to discover polynomial and array invariants. In ICSE, pages 683 693, \n2012. [27] N. Partush and E. Yahav. Abstract semantic differencing for numerical programs. In SAS, pages \n238 258, 2013. [28] S. Person, M. B. Dwyer, S. G. Elbaum, and C. S. Pasareanu. Differential symbolic \nexecution. In SIGSOFT FSE, pages 226 237, 2008. [29] A. Pnueli, M. Siegel, and E. Singerman. Translation \nvalida\u00adtion. In TACAS, pages 151 166, 1998. [30] D. A. Ramos and D. R. Engler. Practical, low-effort \nequiva\u00adlence veri.cation of real code. In CAV, pages 669 685, 2011. [31] T. W. Reps, S. Sagiv, and G. \nYorsh. Symbolic implementation of the best transformer. In VMCAI, pages 252 266, 2004. [32] M. Rinard. \nCredible compilers. Technical report, Mas\u00adsachusetts Institute of Technology, 1999. [33] E. Schkufza, \nR. Sharma, and A. Aiken. Stochastic superopti\u00admization. In ASPLOS, pages 305 316, 2013. [34] R. Sharma, \nS. Gupta, B. Hariharan, A. Aiken, P. Liang, and A. V. Nori. A data driven approach for algebraic loop \ninvari\u00adants. In ESOP, pages 574 592, 2013. [35] K. C. Shashidhar, M. Bruynooghe, F. Catthoor, and G. \nJanssens. Veri.cation of source code transformations by program equivalence checking. In CC, pages 221 \n236, 2005. [36] R. Tate, M. Stepp, Z. Tatlock, and S. Lerner. Equality satura\u00adtion: a new approach to \noptimization. In POPL, pages 264 276, 2009. [37] A. V. Thakur and T. W. Reps. A method for symbolic compu\u00adtation \nof abstract operations. In CAV, pages 174 192, 2012. [38] J.-B. Tristan, P. Govereau, and G. Morrisett. \nEvaluating value\u00adgraph translation validation for LLVM. In PLDI, pages 295 305, 2011. [39] A. Turing. \nChecking a large routine. In The early British computer conferences, pages 70 72. MIT Press, Cambridge, \nMA, USA, 1989. [40] H. S. Warren. Hacker s Delight. Addison-Wesley Long\u00adman Publishing Co., Inc., Boston, \nMA, USA, 2002. ISBN 0201914654. [41] C. M. Wintersteiger, Y. Hamadi, and L. M. de Moura. Ef.\u00adciently \nsolving quanti.ed bit-vector formulas. Formal Meth\u00adods in System Design, 42(1):3 23, 2013. [42] Y. Xie \nand A. Aiken. Scalable error detection using boolean satis.ability. In POPL, pages 351 363, 2005.  \n \n\t\t\t", "proc_id": "2509136", "abstract": "<p>We present a data driven algorithm for equivalence checking of two loops. The algorithm infers simulation relations using data from test runs. Once a candidate simulation relation has been obtained, off-the-shelf SMT solvers are used to check whether the simulation relation actually holds. The algorithm is sound: insufficient data will cause the proof to fail. We demonstrate a prototype implementation, called DDEC, of our algorithm, which is the first sound equivalence checker for loops written in x86 assembly.</p>", "authors": [{"name": "Rahul Sharma", "author_profile_id": "81488651499", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4290382", "email_address": "sharmar@stanford.edu", "orcid_id": ""}, {"name": "Eric Schkufza", "author_profile_id": "81388601846", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4290383", "email_address": "eschkufz@cs.stanford.edu", "orcid_id": ""}, {"name": "Berkeley Churchill", "author_profile_id": "81488644957", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4290384", "email_address": "bchurchill@cs.stanford.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4290385", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509509", "year": "2013", "article_id": "2509509", "conference": "OOPSLA", "title": "Data-driven equivalence checking", "url": "http://dl.acm.org/citation.cfm?id=2509509"}