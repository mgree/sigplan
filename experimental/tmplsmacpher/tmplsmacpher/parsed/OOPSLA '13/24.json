{"article_publication_date": "10-29-2013", "fulltext": "\n Code Optimizations Using Formally Veri.ed Properties Yao Shi Bernard Blackham Gernot Heiser NICTA and \nNICTA and NICTA and University of New South Wales University of New South Wales University of New South \nWales yao.shi@nicta.com.au bernard.blackham@nicta.com.au gernot@nicta.com.au Abstract Formal program \nveri.cation offers strong assurance of cor\u00adrectness, backed by the strength of mathematical proof. Con\u00adstructing \nthese proofs requires humans to identify program invariants, and show that they are always maintained. \nThese invariants are then used to prove that the code adheres to its speci.cation. In this paper, we \nexplore the overlap between formal ver\u00adi.cation and code optimization. We propose two approaches to reuse \nthe invariants derived in formal proofs and integrate them into compilation. The .rst applies invariants \nextracted from the proof, while the second leverages the property of program safety (i.e., the absence \nof bugs). We reuse this in\u00adformation to improve the performance of generated object code. We evaluated \nthese methods on seL4, a real-world formally-veri.ed microkernel, and obtained improvements in average \nruntime performance (up to 28%) and in worst\u00adcase execution time (up to 25%). In macro-benchmarks, we \nfound the performance of para-virtualized Linux running on the microkernel improved by 6 16%. Categories \nand Subject Descriptors D.2.4 [Software Engi\u00adneering]: Software/Program Veri.cation; D.3.4 [Program\u00adming \nLanguages]: Processors; D.4.8 [Operating Systems]: Performance Keywords formal veri.cation; micro-kernel; \noptimization 1. Introduction Formal veri.cation is the only way to ensure that the im\u00adplementation of \na software system of non-trivial size meets its speci.cation. In the .eld of operating systems, formal \nveri.cation has been advocated and attempted since the Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. \n. . $15.00. http://dx.doi.org/10.1145/2509136.2509513 1970s [9, 25]. The seL4 microkernel was the .rst \ngeneral\u00adpurpose operating system (OS) kernel to be successfully veri.ed by theorem proving. The expected \ncost of a new formally veri.ed kernel would be only twice the cost of a traditional system with no assurance, \nand would be half as expensive as the industry rules-of-thumb of $1k/LOC for Common Criteria EAL 6 certi.cation \nwith less assur\u00adance [13]. Hence, veri.cation is already cost-competitive for high-assurance software, \nand, with on-going improve\u00adment in veri.cation techniques, we can expect its use to be\u00adcome more widespread. \nA number of other OS veri.cation projects are now underway [1, 10], and an argument has been made for \npervasively verifying applications [8]. While program correctness is obviously an important goal, and \nthe primary driver of formal veri.cation efforts, in this paper we explore another potential bene.t \nthat of im\u00adproved code optimization. A program which has been ver\u00adi.ed by theorem proving has many precisely-known \nprop\u00aderties: about 150 invariants were proved about the models which describe the ~9,000 lines of C code \nof seL4 [13]. Most of these are quite deep and require signi.cant insight, and are therefore infeasible \nto .nd by static analysis, and are therefore out of reach of the analysis performed by a com\u00adpiler. We \npropose the use of such properties in the optimization phase of the compiler. Speci.cally, we leverage \ntwo kinds of properties resulting from formal veri.cation: (1) explicit invariants which can be extracted \nfrom the veri.cation arte\u00adfacts and fed into the compiler, and (2) an overall program safety property, \nwhich implies that a potentially unsafe op\u00aderation contained in the code can be trusted to be safe. Our \ntechniques can also be applied to unveri.ed software, using informal reasoning about a program s behaviour. \nWe report our experience with a prototype framework for veri.cation-assisted compiler optimization built \naround GCC. Our approach uses highly aggressive inlining to per\u00adform call-site-speci.c optimizations. \nWe .nd performance improvements of up to 28% on average in seL4 micro\u00adbenchmarks and 6% 16% for some \nI/O intensive applica\u00adtions on a Linux system running virtualized on seL4. We ob\u00adserve similar improvements \nin the worst-case execution time of seL4 system calls.  This paper makes the following contributions: \n we demonstrate the extraction of useful properties from formal veri.cation and their insertion into \nthe compila\u00adtion work .ow;  we show how these properties can be used to aid compiler optimization without \nmodifying the source code of the program to be compiled;  we show that, combined with inlining, this \nallows us to perform optimizations speci.c to particular calling contexts;  we apply this approach to \na real-world operating-system kernel, and show signi.cant improvements in average\u00adand worst-case execution \ntimes.  2. Background Klein et. al. successfully demonstrated the .rst full func\u00adtional correctness \nproof of a complete real-world microker\u00adnel, called seL4 [13]. They prove that the C implementa\u00adtion \nof seL4 adheres to an abstract high-level speci.cation of the kernel. We leverage their work and proof \neffort, using knowledge obtained from the proof to create a compiler plu\u00adgin that performs code transformations \nand optimizations. This section provides relevant background on seL4. 2.1 seL4 Proof and Invariants \nThe abstract speci.cation of seL4 is written in the formal theorem proving language Isabelle/HOL and \nis primarily concerned with the user-visible behavior of the kernel i.e., what the kernel does. There \nis a large amount of non\u00addeterminism at the abstract level, leaving the precise imple\u00admentation details \nunspeci.ed. However, several kernel-wide invariants are speci.ed at this level, and any correct imple\u00admentation \nof the speci.cation will not violate them. Invari\u00adants here include statements about kernel integrity, \nfrom lo\u00adcal properties such as every non-NULL pointer points to an object of the correct type , to broader \nones such as all page directory objects have a consistent view of the kernel s mappings . Conversely, \nthe C implementation precisely speci.es the details of how kernel operations are carried out. This in\u00adcludes \nthe layout of data structures in memory and the al\u00adgorithms used to achieve a high-performance microkernel. \nThere is very little non-determinism in the C implementa\u00adtion. However, due to the immense detail and \nprecision of the C source, it is dif.cult to reason about high-level proper\u00adties and system-wide invariants \nbased on the C code alone. The relationship between the abstract speci.cation and the C implementation \nis demonstrated in Figure 1. Over 200,000 lines of hand-written, machine-checked proof text form the \nre.nement proofs, which prove that the C imple\u00admentation correctly implements the abstract speci.cation. \nThese re.nement proofs are a classic method of code ver\u00adi.cation. Figure 1. seL4 uses re.nement proofs \nto show that the C implementation adheres to an abstract speci.cation. Most invariants are given on the \nabstract speci.cation. void deleteCallerCap(tcb_t *receiver) { callerSlot = TCB_PTR_CTE_PTR(receiver, \n3); deleteCap(callerSlot); } void deleteCap(slot_t* slot) { ... ret = finalizeCap(slot->cap, ...); ... \n } int finalizeCap(cap_t cap, ...) { switch (getCapType(cap)) { case cap_null_cap: case cap_reply_cap: \n ... return 0; case cap_endpoint_cap: ... case cap_async_endpoint_cap: ... } ... } Figure 2. An example \nof C code which can be optimized using program invariants. Note the shorter path for the two bolded cases. \nFor our purposes, we seek to utilize the information avail\u00adable at the abstract and executable speci.cations, \nsuch as kernel-wide invariants, and apply them to code transforma\u00adtions of the C implementation. It is \ndif.cult to automatically transfer knowledge at the abstract level down to the C code, so we instead \ncreate a database of relevant translation rules that have been proven at the abstract level, represented \nin terms of C expressions. A signi.cant challenge is minimiz\u00ading effort in creating this database, as \nthere is a wealth of knowledge available at the abstract level but not all of it is relevant or useful \nfor optimization. We will address this issue in Section 4.1 when discussing demand-driven rule gener\u00adation. \n 2.2 Motivating Examples Let us consider the C code shown in Figure 2. The function finalizeCap can \nbe called from different contexts, and its  tcb_cnode_index 3 . (. _ thread_state . case thread_state \nof BlockedOnReceive e d . (op = NullCap) | _ . (op . {ReplyCap, NullCap})) Figure 3. Part of an abstract \ninvariant which applies to the program in Figure 2. The invariant speci.es that the cap at index 3 of \na TCB cnode will be one of the types shown in bold. int setThreadPriority(...) { ... return setThreadParams(..., \nNULL); } int setThreadParams(..., slot_t *slot) { if (...) { cap = slot->cap; ... } ... } Figure 4. An \nexample of code where the program safety invariant can be applied. behavior is affected by the type of \nthe cap , which is given by the value of getCapType(cap). The kernel invariant shown in Figure 3 tells \nus that the cap at index 3 of a TCB cnode (the precise meaning of this expression is unimportant) will \nbe either of type NullCap or ReplyCap , depending on some state. Under the calling context of deleteCallerCap \n. deleteCap . finalizeCap, we know that we are looking speci.cally at the cap at index 3 of a TCB cnode \n, as this is exactly the object being accessed in the .rst line of deleteCallerCap. In this context, \nonly the .rst case of the switch in finalizeCap is relevant, as our invariant tells us the only two cap \ntypes possible are cap null cap and cap reply cap (as represented in C). By generating a spe\u00adcialized \nversion of the function for this context, we can im\u00adprove the performance of this code path by eliminating \nthe invocation of getCapType(), the branching of the switch statement, and the unused remainder of the \nfunction. Fur\u00adthermore, the specialized function is then small enough to be inlined and may bene.t from \nsubsequent optimizations. Our work seeks to apply invariants such as these to improve performance. A \nsecond method to improve performance of veri.ed pro\u00adgrams is to utilize knowledge of program safety. \nWe lever\u00adage the proven guarantee that a program is free of errors such as NULL-pointer dereferences, \nuninitialized memory reads, memory leaks and buffer over.ows, in order to per\u00adform much more aggressive \ncode optimizations. Any code path through a program that violates a safety guarantee of the kernel is \ndeemed infeasible and can be eliminated. We note that compilers are already permitted to optimize away \nFigure 5. Overview of the framework. The shaded blocks are our implementation. unde.ned behaviour, but \nin practice they are conservative in what optimizations are performed. We discuss this further in Section \n7. Such a case can be seen in Figure 4. The function setThreadPriority calls setThreadParams with NULL \nas the last argument slot. In setThreadParams, the pointer slot is dereferenced within a conditional \nblock. If the condition was true, the code would dereference a NULL pointer. However, the program is \nguaranteed to be free of NULL-pointer dereferences, so we can infer that the path from the call site \nof setThreadParams to the pointer deref\u00aderence is infeasible. In this case, the condition on the if state\u00adment \nmust be false and therefore the if statement and its en\u00adtire body can be removed under this context. \nNote that in this scenario, the branch condition does not decide if the pointer slot is NULL. So the \nconditional block is not obvious dead code that traditional compilers can elim\u00adinate. 3. Architectural \nOverview Our framework is implemented as a plugin for the GCC compiler. We interpose our framework into \nthe early stages of GCC s inter-procedural analysis, immediately after the creation of the call graph. \nAt this critical point, we can make best use of the inter-procedural analysis, and in.uence subsequent \ninter-and intra-procedural optimizations in particular, inlining. The architecture of the framework \nis shown in Fig\u00adure 5. There are three key stages to our framework: inter\u00adprocedural analysis, rule application \nand code transforma\u00adtion. The inter-procedural analysis stage serves two purposes: (1) it identi.es questions \nabout speci.c expressions in C code, which may be later answered by proof invariants (de\u00adscribed further \nin Section 4.1); and (2) it detects NULL\u00ad  Question Expression relative to finalizeCap(): getCapType(cap) \nExpression relative to deleteCallerCap(): getCapType(TCB PTR CTE PTR( receiver,3)->cap) C = [deleteCallerCap4, \ndeleteCap4, finalizeCap7] Answer Possible values: {cap null cap, cap reply cap} Impossible values: {} \nFigure 6. An example of a question, its context (C) and its answer. The subscripts following each function \nname identify the speci.c invocation to disambiguate cases where there are multiple such calls within \na function. pointer dereferences which could be eliminated by lever\u00adaging knowledge of program correctness \n(described further in Section 4.3). The questions generated are passed to the next stage, while the detected \nNULL-pointer dereferences are used to determine infeasible paths which can be elimi\u00adnated. In the second \nstage, proof invariants are identi.ed to an\u00adswer as many questions as possible by creating translation \nrules. Some dif.cult questions may generate proof obliga\u00adtions these are possible invariants which a \nveri.cation en\u00adgineer can either prove correct or reject as bogus. As most invariants apply to higher-level \nabstractions than the C code, we use translation rules to map high-level statements onto C expressions, \nand accumulate this information in a database. Our compiler plugin searches the database to .nd possi\u00adble \nanswers to the questions. The developer maintains the database and creates the translation rules corresponding \nto the questions of.ine. This keeps the entire compilation/opti\u00admization sequence uninterrupted. Figure \n6 shows an exam\u00adple question and answer pair. We discuss this in further depth in Section 4.2. Finally, \nin the third stage, we have a set of answered ques\u00adtions (many questions may have gone unanswered), and \na set of infeasible paths derived from the NULL-pointer deref\u00aderence detection. Our GCC plugin clones \nfunctions called under different contexts and applies inter-procedural code transformations, before applying \nits existing optimizations. We describe this further in Section 4.4. 4. Optimizations In this section, \nwe elaborate on the algorithms used to per\u00adform our code optimizations. First, we must introduce some \nterminology and de.nitions, as follows. A context C denotes a call stack represented as an or\u00addered list \nof call sites, which is NumLevels(C) lev\u00adels deep. Ci is the ith call site in the list (1 = i = NumLevels(C)). \nFi is the callee function of Ci. F0 is the root caller of the context (but C0 is unde.ned). Questions \nhold the full context from the root function while answers and rules contain only partial contexts. \n An expression E is derived from a branch condition.  A set of values V speci.es the range an expression \nmay evaluate to.  A question Q = (C, E0, E1) asks under the context C, what are the possible (or impossible) \nvalues of the ex\u00adpression E1? Here, E0 is the expression in FNumLevels(C) while E1 is the equivalent \nexpression in the root caller F0. Later, we will need to know the possible values of E0.  An answer \nA = (C, E, V0, V1) denotes that, under the context C, the expression E (in FNumLevels(C)) may eval\u00aduate \nto a value in V0 and will never evaluate to a value in V1. As it is impossible for V0 to enumerate all \npossible values, we use the empty set to denote the universal set. Thus, an empty V0 means that E can \nbe anything except for the values in V1, and vice versa for an empty V1.  A basic block B represents \na branch-free sequence of in\u00adstructions with only one entry point and one exit point. BasicBlock(S) refers \nto the basic block that the state\u00adment S belongs to. The statement S may be a call site or an expression. \nFunctionEntry(F) is the basic block containing the en\u00adtry point of the function F.  A path I = (C, B) \ndenotes a speci.c basic block in the function FNumLevels(C), under the calling context C.  The function \nUnmodi.ed(E, F, B) is true iff the value of the expression E is not modi.ed between FunctionEntry(F) \nand B (where B must be contained in F).  4.1 Applying Invariants Formally applying invariants from \nthe abstract level to C source code requires a substantial effort by a veri.cation engineer although \nthere is nothing inherently dif.cult in doing so, it is something that is not yet automated in the veri.cation \nof seL4. We instead use translation rules to apply the invariants onto the C code. These rules are presently \nconstructed by hand, but can be proved correct to ensure a total proof of correctness. The abstract level \ncontains a large number of invariants, yet only a small handful of these are useful for code opti\u00admization. \nOur challenge is to .nd the useful invariants au\u00adtomatically and represent them at the C level. We propose \na solution we call demand-driven rule generation. As shown in Figure 5, the inter-procedural analysis \nstage generates questions about the C source. These questions pro\u00advide hints about what would assist \ncode optimizations and are used to manually create translation rules. A question Qconsists of the calling \ncontext, the original expression in the leaf callee, and the equivalent expression in the root caller. \nA question is derived from branch conditions on conditional branches.  The equivalent expression in \nthe root caller is computed by a bottom-up traversal of the call graph: given an expres\u00adsion, if all \nvariables in the expression are parameters to the function and are not modi.ed between function entry \nand where it is used in the branch condition or call site, we can represent the expression as the equivalent \nform in the caller function. Function Expression .nalizeCap getCapType(cap) deleteCap getCapType(slot->cap) \ndeleteCallerCap getCapType(TCB PTR CTE PTR( receiver, 3)->cap) Table 1. Forms of the same expression \nin different func\u00adtions. Consider our motivating example program in Figure 2. Table 1 shows the different \nforms of the expression getCapType(cap) in function finalizeCap at different levels of the call stack. \nIf we can apply proof invariants to compute the possible (or impossible) values of any one of these expressions, \nthen this knowledge can be applied to all expressions, as they are equivalent. In this case, we can use \ninvariants of the expression in deleteCallerCap to com\u00adpute getCapType(cap) in finalizeCap, and use it \nto per\u00adform code transformation. The algorithm used to .nd questions which can possibly be answered by \ninvariants is shown in Algorithm 1. In sum\u00admary, an expression may be represented in different forms \nat many levels of the call stack, which are all equivalent. At this stage, we do not know which one may \nbe solved, so we pass all forms with their contexts as questions to the rule generation stage. A question \ncan be answered if any one of its equivalent questions is answered.  4.2 Rule Generation A translation \nrule consists of a question and its answer. We use three rule databases, which accumulate translation \nrules as they are added by the developer. These databases are the C rule database, the abstract rule \ndatabase, and the abstract/C translation rule database. There will be simple questions that the developer \ncan con.dently answer directly in the C language, which can be immediately added to the C rule database \nas a translation rule. More dif.cult questions may require the developer to use the abstract speci.cation \nto .nd the answer. In these cases, the question should be converted to the abstract level according to \nrules in the abstract/C translation rule database. The developer can then prove the answer at the abstract \nlevel and add the question and answer to the abstract rule database. Finally, the answer at the abstract \nlevel is translated back to C using the abstract/C translation rule database. Algorithm 1: Finds candidate \nquestions to apply invari\u00adants to. Output: Questions: Q = (C, E0, E1) 1 function RaiseQuestion(C, E0, \nE1, L) begin 2 C' . (CL, . . . , CNumLevels(C)); 3 Q . (C', E0, E1); 4 Output Q; 5 end 6 function Analyze(C, \nE0, E1, L) begin 7 RaiseQuestion(C, E0, E1, L); 8 if Unmodi.ed(E1, FL, BasicBlock(E1)) and all variables \nin E1 are parameters and L > 0 then 9 E' . the corresponding form of E1 at the upper level FL-1; 10 Analyze(C, \nE0, E', L - 1); 11 end 12 end 13 function FindCandidateQuestions() begin 14 foreach C do 15 foreach E \nin branch condition do 16 Analyze(C, E, E, NumLevels(C)); 17 end 18 end 19 end Figure 7 shows some examples \nof translation rules. The syntax of the rules is designed to be simple and familiar for developers. A \nrule consists of a pattern to match C or ab\u00adstract expressions, and can optionally include arguments \nand a context. In the .rst example of Figure 7, the C level macro TCB PTR CTE PTR has two arguments $1 \nand $2 and cor\u00adresponds to the expression ($1, tcb cnode index($2)) in abstract level. If a translation \nrule applies over the whole program, the context is unnecessary. Otherwise, the devel\u00adoper needs to specify \nthe context in which it applies. This is expressed as a list of functions denoting the call stack. Consider \nthe last example of Figure 7. Here, sendIPC1 and scheduleTCB1 indicate a call stack where sendIPC calls \nthe .rst invocation of scheduleTCB, which in turn calls the .rst invocation of isRunnable within it. \nNote that the databases are searched automatically for translation rules. Inserting translation rules \ninto the databases is done manually, of.ine. The developer does not need to answer all questions. They \nmay not know the answer, or the question may be impossible to answer. In fact, most of the questions \ndo not need to be answered. For example, consider the expressions in Table 1. Each of these expressions, \nwhen taken in the context of the given function, forms a question ( what are the possible values of the \nexpression under the given context? ). The .rst two cannot be answered in general, so can be ignored. \n  Figure 8. An example of question processing. C . Abstract TCB PTR CTE PTR($1,$2) . ($1,tcb cnode index($2)) \nAbstract . Abstract get cap($1, tcb cnode index(3)) . NullCap or ReplyCap Abstract . C NullCap . cap \nnull cap ReplyCap . cap reply cap C . C (including the context) isRunnable(thread) . false C = [sendIPC1, \nscheduleTCB1] Figure 7. Examples of translation rules. However, we have information at the abstract level \nshown in Figure 3 which can provide an answer for the expression getCapType(TCB PTR CTE PTR(receiver, \n3).cap) in the function deleteCallerCap. More importantly, we can prove that this expression is either \ncap reply cap or cap null cap at abstract level. That means the expression getCapType(cap) in finalizeCap, \nwhich is speci.ed in the source code, is either cap reply cap or cap null cap in this context. This information \nis passed to the code transformation stage. Figure 8 demonstrates the process of answering the above \nquestion using the translation rules. The compiler asks the C question what is the value of getCapType(TCB \nPTR CTE PTR(receiver,3).cap)? Extracting rules (1) and (2) from the C.Abstract database and applying \nthese to the C question, it obtains the abstract question for the value of get cap(receiver, tcb cnode \nindex(3)). Rule (3) from Abstract.Abstract database produces the abstract answer NullCap or ReplyCap. \nFinally, applying rules (4) and (5) from the Abstract.C database yields the C answer, cap null cap or \ncap reply cap.  4.3 Use of Program Safety As mentioned in Section 2.2, we can leverage the veri.ed guarantee \nof program safety. If we detect an unsafe path in the program, we can eliminate it because the program \nis known to be safe. In this paper, we only use the safety of pointer dereferences, allowing us to eliminate \nany path which dereferences a NULL pointer. We could make use of other veri.ed safety properties, such \nas the absence of uninitialized memory reads, buffer over.ows and memory leaks. Our algorithm identi.es \ncode paths that will not be exe\u00adcuted under the program safety assumption (i.e., those paths that dereference \nNULL pointers), and eliminates them from the program. This alone does not achieve signi.cant gains, as \nsuch code paths are rare. However, there is much more scope for optimization when considering infeasible \npaths across function boundaries. We can create specialized versions of callee functions that are optimized \nfor their calling context. Algorithm 2 identi.es infeasible paths through a bottom-up traversal of each \ncontext s call stack. This can be applied to our motivating example shown in Figure 4. This demonstrates \nan easy way of leveraging program safety. More precise and complex detection methods may lead to larger \noptimization improvements. We do not discuss this further as our focus is on leveraging formal veri.cation \nartefacts.  4.4 Code Transformation We can apply the information obtained by the techniques in the previous \nsections to perform code transformations.  Figure 9. Context separation by function cloning, showing \nthe call graph at each stage of the process. Algorithm 2: NULL-pointer dereference detection. Output: \nInfeasible paths I = (C, B) 1 function ReportInfeasiblePath(C, B, L) begin C ' 2 . (CL, . . . , CNumLevels(C)); \n3 I . (C ' , B); 4 Output I; 5 end 6 function Backtrace(C, B0, P, L, B1) begin 7 if Unmodi.ed(P, FL, \nB1) and L > 0 then ' /* P is the equivalent form of P in the calling function FL-1 */ 8 if P ' is N U \nLL then 9 ReportInfeasiblePath(C, B0, L); 10 else 11 Backtrace(C, B0, P ' , L - 1, BasicBlock(CL)); 12 \nend 13 end 14 end 15 function NullPtrDetect() begin 16 foreach C do 17 foreach E, where E is the dereferencing \nof parameter pointer P do 18 Backtrace(C, BasicBlock(E), P, NumLevels(C), BasicBlock(E)); 19 end 20 end \n 21 end 4.4.1 Context Separation The information we obtain from the invariants is only cor\u00adrect under \nthe relevant context. This means we cannot ap\u00adply code transformations to existing functions which may \nbe called from anywhere. We solve this using an approach called context separation, which creates clones \nof functions to be called from different contexts. Speci.cally, given a context C = (C1, C2, . . . , \nCn), we clone the functions F1, F2, . . . , Fn to generate the corre\u00ad '' ' sponding functions F1, F2, \n. . . , F , and replace the respec\u00ad n tive call sites with calls to the newly cloned functions. Al\u00adgorithm \n3 describes the details of this method. A concrete example of context separation applied to the program \nfrom Figure 4 is shown in Figure 9. Algorithm 3: Context separation. Input: Context C = (C1, C2, . . \n. , Cn) Output: New context C ' = (C ' 1, C ' 2, . . . , C ' n) 1 function CloneFunction(C) begin ' 2 \nF 0 . F0; 3 for i . 1 to n do ' 4 Clone Fi to F i; ' ' 5 Let C ' i in F i-1 call F i; 6 end 7 C ' . (C \n' 1, C ' 2, . . . , C ' n); 8 return C ' ; 9 end As context separation replicates many functions, it \nunsur\u00adprisingly increases total code size. Both intuition and past research suggest that this may incur \na performance penalty due to the larger instruction cache footprint [15]. In practice, we have not observed \nany negative performance impact in our experiences. We discuss this aspect further in Section 5.  4.4.2 \nApplying Answers We have used the questions and invariants to create a set of answers which describe \nthe possible or impossible values for a given expression. The use of these answers for transform\u00ading \ncode depends on the type of answer: Only one possible value. In this case, we can directly replace the \nexpression with the value. Subsequent opti\u00admizations propagate the information for this context.  A \nset of possible values. This information can be prop\u00adagated to switch statements by removing labels that \nare not members of the set. It can also be applied to branch conditions by testing the condition against \neach of the possible values of the expression. If all tests fail, the con\u00addition is set to false. If \nall tests succeed, the condition is set to true.   A set of impossible values. As above, switch statements \ncan be modi.ed by removing labels that are members of the set. If a branch condition requires the expression \nto evaluate to one of a set of possible values in order to be true, and this is a subset of the impossible \nvalues, then the condition is set to false. Similar logic can check if the branch condition can be set \nto true.  4.4.3 Applying Infeasible Paths from Program Safety The output of Algorithm 2 gives an infeasible \npath which is represented by a context C = (C1, C2, . . . , Cn) and a basic block B within the function \nFn invoked by Cn. If B is not executed on every path through Fn, i.e. B does not post-dominate the entry \nof Fn, we can simply remove B. Otherwise, the entire function Fn cannot be executed under the given context. \nIn this case, we recursively work backwards through the calling context, removing the infea\u00adsible basic \nblocks and functions. Algorithm 4 describes the method in detail. Algorithm 4: Remove infeasible path. \nInput: Context C = (C1, C2, . . . , Cn) Input: Basic block B 1 function RemovePath (C, L, B) begin 2 \nif B post-dominates FunctionEntry(FL) then 3 RemoveFunction(FL); 4 RemovePath(C, L - 1, BasicBlock(CL)); \n 5 else 6 RemoveBasicBlock(B); 7 end 8 end 9 function RemoveInfeasiblePath(C, B) begin 10 RemovePath(C, \nNumLevels(C), B); 11 end  4.5 Effects for Subsequent Optimizations After our code transformations, the \ncompiler continues its normal analysis and optimization routines, ultimately gener\u00adating executable objects. \nMany of our code transformations provide performance gains only after subsequent optimiza\u00adtion passes \nin GCC. We describe the key steps below. 4.5.1 If-Switch Conversion and If-Conversion In many cases, \nwhen a switch expression is optimized, the resulting statement has only a few possible labels. The best \ncase is that only one label remains. In this case, the switch can be eliminated entirely, leaving just \nthe code of the re\u00admaining case. If there are two or three labels, the compiler can transform the switch \nstatement to an if statement. This avoids a jump table, reducing code size. It also exposes the opportunity \nfor compilers to apply if-conversion during low-level code generation on the ARM architecture, which \neliminates further branches by using predicated instructions. 4.5.2 Dead Code Elimination The code transformations \nabove may modify branch condi\u00adtions. This can cause related branch conditions (e.g. alternate switch \ncases) to become unreachable, allowing those code sections to be eliminated. Additionally, some code \nmay be eliminated indirectly. For example, consider the program in Figure 2. The switch statement is \nsimpli.ed to the single case, which ends in a return statement that terminates the function. The remainder \nof the function becomes unreach\u00adable and can be eliminated. 4.5.3 Redundant Parameter Elimination After \nperforming context separation, some function parame\u00adters become unnecessary. For example, a given context \nmay enforce only one possible value for a parameter. Therefore, this parameter can be replaced by a constant \nand eliminated under this context. This optimization reduces the code size and reduces register pressure \naround call sites during code generation, making it less likely that expressions will spill onto the \nstack. It is especially crucial for architectures such as ARM which use general-purpose registers in \ntheir stan\u00addard calling conventions.  4.5.4 Inlining Inlining is known to be one of the most important \noptimiza\u00adtions in the modern compiler, as it converts inter-procedural issues into intra-procedural issues \nthat are easier to analyze. In general, the inlining decision is based on the trade-off be\u00adtween the \ncode size (i.e. the size of the callee function) and the bene.t after inlining. For our optimizations, \nafter dead code elimination, the size of some functions may be signi.cantly smaller. This provides opportunities \nto inline many functions that were originally prohibitively large. For example, in the program in Figure \n2, under the context of the caller deleteCallerCap, the function finalizeCap which originally has tens \nof statements is reduced to a simple return statement which can be inlined. 5. Evaluation and Discussion \nWe implemented our techniques as a plugin for GCC 4.5.2, cross-compiling from an x86 host to an ARM target. \nOur target platform is a BeagleBoard-xM with a TI DM3730 processor. This processor features a 1 GHz ARM \nCortex-A8 CPU core. It also contains separate L1 data and instruc\u00adtion caches, each 32 KiB 4-way set-associative, \nas well as a 128 KiB uni.ed L2 cache. We ran the analysis and compi\u00adlation on our host platform which \nis a 2.66 GHz Intel Xeon system with 10 GiB of memory, running Linux. We measured the impact on runtime \nperformance of three sets of benchmarks: micro-benchmarks of seL4: we measure the critical fac\u00adtor in \nmicrokernel performance inter-process commu\u00adnication (IPC) as well as several other microkernel op\u00aderations. \n   micro-benchmarks of virtualized Linux running on seL4: these are obtained by running the LMbench \n3.0 test suite.  macro-benchmarks on virtualized Linux running on seL4: we speci.cally selected I/O \nintensive applications (rather than CPU-bound ones), as their performance is signi.cantly affected by \nthe virtualization platform due to frequent context switching.  As seL4 is primarily designed to support \ndifferent OS personalities, native seL4 applications are uncommon. We note that micro-benchmarks on Linux \ncan be considered macro-benchmarks for the seL4 API. The virtualized Linux platform we tested is based \non the Linux 2.6.38 kernel, and is paravirtualized to run on seL4. All of our tests were run using the \n-O2 optimization level in GCC. We also ran the micro-benchmarks at the -O3 opti- Figure 12. Relative \nperformance of each approach against an unmodi.ed compiler individually, generated with GCC -O2 optimization \nlevel. mization level, and also with the -fwhole-program .ag. As these introduce extra optimization effort \nfrom the compiler, we present these results to show that our analysis improves performance beyond what \nthe unmodi.ed compiler achieves at its most aggressive optimisation level. 5.1 seL4 Micro-benchmarks \nFigure 10 shows the improvement in performance of seL4 s micro-benchmarks using our optimizations. The \n.rst bench\u00admark in this graph is the seL4 IPC fastpath this is a highly\u00adtuned C code path for handling \nthe most frequently exercised part of the kernel as quickly as possible. Signi.cant effort was previously \ndevoted to optimizing this code path [4]. As such, there is little room for improvement here. The remain\u00ading \nitems in the graph show slowpath IPC (the less common case), as well as several typical con.guration \nprimitives used to con.gure threads in seL4. These generally gain 17% 28% improvements when compiled \nwith -O2. As noted previously, our optimizations clone many func\u00adtions due to context separation. These \ncloned functions are typically invoked only once in any execution and may be in\u00adlined by the compiler. \nThis has an impact on performance distinct to that of our own optimizations. We quanti.ed this impact \nseparately by measuring the runtime performance af\u00adter applying only context separation and inlining. \nNone of the proof-based optimizations were used for these measure\u00adments. These results are shown in Figure \n11. We see that in al\u00admost all cases the improvements are much smaller than in Figure 10. After applying \nour optimizations, we see perfor\u00admance improvements in all cases. Finally, we quantify the impact of \napplying invariants and using program safety separately, shown in Figure 12. Both techniques in isolation \nimprove performance by be\u00adtween 12% 25%, and unsurprisingly are even better when combined.    5.2 \nVirtualized Linux Micro-benchmarks Running a virtualized Linux machine in parallel with a mission-critical \napplication is a common use-case of micro\u00adkernels. As such, the LMbench suite provides some insights \ninto the performance of virtualized Linux, as it heavily exer\u00adcises the Linux kernel and the microkernel/hypervisor. \nThe results of LMbench on virtualized Linux are shown in Fig\u00adures 13 to 17. All results are normalized \nto a baseline of 1 representing native (un-virtualized) Linux. We also show the performance improvement \nobtained using only context separation and inlining, to clearly identify the improvement obtained by \nour invariant-based optimization. Some perfor\u00admance improvements using only context separation are de.\u00adnitely \nnegative, which re.ect the intuition that more inlining is not always better. In these tests, only the \nseL4 microkernel was optimized no changes were made to the Linux kernel or user level pro\u00adgrams. As \nthe execution of seL4 system calls are only a part of the entire execution of a test case, the performance \nim-provements are naturally less than those of the seL4 micro\u00adbenchmarks. Figure 15. LMbench: relative \ncommunication latency in virtualized Linux on seL4, normalized to a baseline of 1 for native Linux (smaller \nis better).   On all latency micro-benchmarks from LMbench, we see improvements of up to 21% with our \noptimized version of seL4. The improvement in bandwidth throughput (Fig\u00adure 17) is not as dramatic because \nthese benchmarks are pri\u00admarily memory-bound. The observant reader will notice that the AF/UNIX benchmark \nshows better performance virtualized than non\u00advirtualized, both with the normal build as well as with \nour optimized seL4. This is due to fragility in the benchmark, which also exists in the pipe benchmark. \nThese two bench\u00admarks pass messages over UNIX sockets and pipes, respec\u00adtively. However, neither of these \ntransports are designed to preserve message boundaries the message boundaries may be in.uenced by variations \nin scheduling and inter\u00adrupts. Therefore, the number of operations required to send a message of a given \nsize varies. Scheduling under virtual\u00adized Linux consistently favours better throughput for these benchmarks, \nas despite the fact operations are more expen\u00adsive virtualized, fewer operations are required.  5.3 \nMacro-benchmarks Finally, we evaluated some typical applications running in the virtualized Linux environment \non seL4. These macro\u00adbenchmarks give a realistic, tangible measurement of real\u00adworld use cases. We examined \nthe performance impact on I/O-intensive applications which make frequent interactions with the Linux \nkernel. We tested the performance of tar and cp operations, which are common I/O-intensive commands on \nUnix systems. We ran tar and cp on a single .le, whilst varying the size of this .le. The average improvements \nof tar and cp are both 6.4%. See Figure 18. We also ran tar and cp on batches of .les and direc\u00adtories. \nWith 25 empty .les in each directory, we varied the number of directories to exercise the .lesystem, \nwith results shown in Figure 19. The average improvements of tar and cp for batches of .les and directories \nare 16.0% and 12.9%, respectively. Figure 18 and Figure 19 also show the performance im\u00adprovement from \ncontext separation and inlining alone. The results indicate that the main contributor to improved perfor\u00admance \nis our invariant-based optimizations. 5.4 Worst-Case Execution Time The worst-case execution time (WCET) \nof a program is the theoretical upper bound on its execution time. Knowing the WCET of a program is essential \nfor designing hard real\u00adtime systems. A smaller WCET reduces the margin by which hardware must be over-provisioned \nby in order to guarantee correct and timely behaviour in all circumstances. WCET can be computed using \nof.ine static analysis or through measurement-based analysis [28].  Figure 19. Performance of tar and \ncp for batch of .les and directories on Linux virtualized on seL4. The geometric means of the improvements \nof tar and cp are 16.0% and 12.9%, respectively. The seL4 kernel has previously had its WCET computed \nvia static analysis with a view towards supporting safety\u00adcritical hard real-time systems [5]. We examined \nthe effects of our optimizations on the WCET of seL4. We found that the WCET was improved in all cases, \nas shown in Figure 20. The WCET of our optimized kernel is 25.5% less than that of the original kernel \nfor a normal system call. The improve\u00adment can be mostly attributed to eliminated code which re\u00adduces \nbranch mispredictions and cache misses.  5.5 Impact on Code Size The compiled seL4 kernel increased \nin size due to context separation and inlining. This is quanti.ed in Figure 21 which shows the overall \nkernel size, including code, data and heap sections produced by the compiler, as well as the effect on \nthe code size alone. For each optimization level, we show the size of the original and optimized kernels, \nas well as for a kernel compiled with context-separation and inlining. This demonstrates that our invariant-based \noptimizations elimi\u00adnate a signi.cant amount of dead code. In all cases where any optimization was used \n(-O1 or higher), the overall ker\u00adnel size increased to at most 2 3\u00d7 of the original.   Despite the \nincreases in kernel and code size, we see better performance. We believe this to be for the following \nreasons: The code size of a single function is usually reduced, never expanded. That means for any single \nfunction, the local cache performance will be improved using our op\u00adtimizations.  Context separation \nclones the entire call chain which ac\u00adtually improves the likelihood of cache hits. Consider a function \na which calls function b, and b is cloned to b ' . The scenario detrimental to cache performance occurs \nwhen b is in the cache but a calls b '. However, this is not a common scenario. Additionally, modern \ncompilers  Lines of C code 9525 Number of functions 276 Number of contexts 217439 Number of processed \nexpressions 106334 Inter-procedural analysis time 12.22 seconds Rule generation time 0.04 seconds Code \ntransformation time 5.23 seconds Total compilation time 201.09 seconds Original total compilation time \n16.89 seconds Number of C questions 10223 Number of C answers 1540 Number of rules 125 Number of separated \ncontexts 4671 Table 2. Properties of seL4 and our optimizations. The times are measured with GCC -O2. \nimplement mitigating techniques which reorder functions and basic blocks [16]. These techniques reorder \nthe posi\u00adtion of b ' or the basic blocks in b ' after inlining to increase the probability of cache hits. \nThis may make up for the performance loss due to increased instruction cache size by code expansion. \nGiven this, we assert that a code base of seL4 s size is well suited to context separation. The increase \nin code size of less than 200 KiB (at -O2 or above) is insigni.cant on to\u00adday s protected-mode platforms \n(even embedded ones). On large applications or resource-constrained devices, heuristic\u00adbased inlining \nbecomes necessary to ensure that the code size increases remain manageable. However, this is a com\u00adplex \ntopic beyond the scope of this paper.  5.6 Scalability Table 2 gives some basic information on the seL4 \ncode base used and our optimizations. We counted the number of func\u00adtions using the GCC intermediate \nrepresentation (GIMPLE) rather than in the source code. The number of expressions counts multiple occurrences \nseveral times if they occur un\u00adder different contexts. The number of contexts is greater than the number \nof expressions because some functions (and therefore contexts) do not have a branch or only branch con\u00additions \nthat cannot be processed. These numbers show that it is possible to enumerate all contexts and process \nthem in a reasonable amount of time. For programs much larger than seL4, users may select the appropriate \ncontexts or use a BDD-based system [27] for context processing. The table also shows the time required \nfor each stage of the analysis. The inter-procedural analysis time includes both applying invariants \nand using program safety. Most of this time is spent enumerating all contexts and expressions. The compilation \ntime is signi.cantly increased, dispropor\u00adtionately so compared to the expansion in code size. How\u00adever, \ngiven the bene.t in runtime performance, we believe this is acceptable for many use cases.  The last \nsection in Table 2 shows some statistics of the rule generation phase. For the seL4 micro-kernel, applying \ninvariants raises 10,223 questions in the C language. 1540 of them can be answered with our 125 manual \ntranslation rules. This shows that the rules are effective at answering many questions in practice. Although \nwe separated 4671 contexts, most of the cloned functions were eliminated in subsequent optimizations. \nThis reinforces the code size results seen in Figure 21: that with no optimizations, the code size is \nsigni.cantly larger. 6. Limitations and Applicability 6.1 Implementation Correctness At present, although \nwe make use of invariants derived from the formal veri.cation, the translation rules themselves are not \nformally veri.ed. Thus, human error in writing the rules can lead to an incorrect binary. This is not \nan inherent lim\u00aditation in our approach, as it should be possible to formally verify each of the translation \nrules, eliminating this weak\u00adness from the trusted computing base. Bugs in the compiler or our plugin \ncan lead to incorrect object code being produced. GCC implements hundreds of optimizations, none of which \nare formally veri.ed. Without any correctness guarantees, veri.ed translation rules can be easily undermined. \nThere are at least two possible solutions to these issues. The .rst is to implement our approach in a \nformally veri.ed compiler such as CompCert [7, 14]. However, the state\u00adof-the-art in veri.ed compilers \ncannot currently match the performance achieved by mainstream optimizing compilers such as GCC and LLVM. \nA second solution is to verify the compiled assembly code directly against the C code. This was recently \nachieved by Sewell et al. for the seL4 microkernel [18]. By integrating this veri.cation step with the \nformal proof of seL4, we can apply the existing proven invariants, which would wholly re\u00admove the compiler, \nour plugin, and the translation rules from the trusted computing base, thereby avoiding any reliance \non their correctness. The proof would guarantee that the binary is correct (with respect to the kernel \nspeci.cation), in the presence of all optimizations. 6.2 Applicability The main components of our approach, \ni.e., applying invari\u00adants and program safety, are only loosely coupled with com\u00adpiler internals. This \nmakes our approach applicable to both veri.ed and non-veri.ed compilers. There are two scenarios for \nthe potential application of our idea. Firstly, for software that demands the strong assurances of formal \nveri.cation, the developer can input the invariants into a veri.ed compiler such as CompCert. In addition \nto im\u00adplementing the code transformation within the compiler, the correctness of the transformations \nneeds to be proven using the invariants. Such a proof may require extra developer ef\u00adfort, but is required \nto maintain the strong assurances of for\u00admal veri.cation. However, as mentioned previously, Com\u00adpCert \ndoes not currently achieve the same levels of perfor\u00admance as mainstream optimizing compilers. On the \nother hand, our work can also be applied to soft\u00adware that is not formally veri.ed. For unveri.ed software, \nif a developer informally believes a program invariant to be true, our techniques can use this to optimize \nthe compilation process. Most applications today use optimizing compilers for performance, and rely on \nindustry standard testing prac\u00adtices to catch bugs. The impact of an incorrect invariant in these circumstances \nis comparable to any regular program bug where a developer had assumed the invariant to be true. The \nrequirements of a speci.c application dictate the acceptable trade-offs between correctness and performance. \nIf high assurance is required, system designers may choose to sacri.ce performance for veri.ed correctness. \n 6.3 Human Effort Another limitation of our work is the human effort required to generate the translation \nrules. Although the process of applying rules at compile time is automatic and transparent, the entire \noptimization process cannot be performed without this manual work. In our approach, questions have full \ncalling contexts while rules and answers have only partial contexts. There\u00adfore, one partial context \nmay match several full contexts. For example, of the 10,223 questions (as shown in Table 2), many are \nidentical except for the context. 117 questions are similar to the example in Table 1 and can be answered \nby a single rule. Furthermore, if a question cannot be answered, we can .lter out many similar questions \nwith different contexts to reduce human effort. For seL4, we translated 125 invariants into rules, which \nanswer 1,540 questions. It took us 2-3 days in total to review all 10,223 questions and provide the necessary \ntranslation rules (given existing familiarity with the codebase). Providing these rules proved much easier \nthan manual code optimization. In our experience, it is easy to maintain most of the trans\u00adlation rules \nwhen the source code changes. Many rules are relatively primitive and context-independent so that source \ncode changes usually do not affect these rules. Let us con\u00adsider the translation rules in Figure 7. The \n.rst three rules are primitive and context-independent. So the code changes generally do not affect the \ncorrectness of these rules except for the changes of the infrastructure. However, the last rule is context-dependent \nwhich may not hold after some general code changes. Despite all this, the human effort required is still \na lim\u00aditation of our approach. We plan to investigate methods to derive these rules automatically in \nthe future.  7. Related Work Since the 1970s, formal program veri.cation, and in particu\u00adlar OS veri.cation, \nhas been an active research area. As per\u00adformance is a key consideration in most practical operating \nsystems, many veri.cation approaches have focused on im\u00adplementations in low-level languages such as \nC/C++ [2, 11 13, 19, 25]. Optimization of these systems traditionally fo\u00adcuses on modi.cations to the \nsource code e.g. using better algorithms. Vandevoorde describes a prototype compiler for the Speckle \nprogramming language which makes use of a for\u00admal speci.cation written in the Larch Shared Language [24]. \nIn that work, the formal speci.cation is targeted for op\u00adtimizations, not for a proof of program correctness. \nThus the speci.cation only guarantees the correctness of the op\u00adtimizations performed, but not that of \nthe original program. The speci.cation also requires multiple implementations for general purpose and \nmore speci.c optimizations. In contrast, our motivation is to leverage the existing speci.cations for \nthe program correctness, which are not necessarily designed for optimizations. We do not need to modify \nthe speci.cation and its implementation, and do not need multiple implemen\u00adtations. Furthermore, Vandevoorde \ns work is not based on a full-featured programming language and omits some impor\u00adtant features such as \nnon-local variables. It measured only one simple benchmark of AC-uni.cation and gained 11% improvement \ncomparing with an unknown baseline. In com\u00adparison, our approach is more practical and has successfully \noptimized a real-world microkernel written in C. Recently, Blackham and Heiser investigated optimizing \nthe IPC fastpath in the seL4 microkernel [4]. Unlike tradi\u00adtional fastpaths which are hand-coded in assembly, \nthe seL4 fastpath is written in C. They hand-tuned C code to provide the compiler with more optimization \nopportunities. They used knowledge from kernel invariants to ensure that re\u00adordered code was still safe \n(e.g. that pointers could be safely dereferenced earlier). The CompCert project investigates the formal \nveri.cation of realistic compilers [6, 7, 14]. They have successfully created the formally veri.ed CompCert \nC compiler, and have veri.ed several optimization algorithms [3, 17, 21 23]. However, none of these apply \nformal veri.cation of their inputs to improve the performance of generated binaries. To the best of our \nknowledge, no formally veri.ed real\u00adworld software has been automatically optimized using in\u00adformation \nobtained from the formal veri.cation process. We note that C compilers are permitted to treat unde.ned \nbehaviour however they choose. Wang et al. found numer\u00adous bugs in critical software due to compilers \noptimizing code that accidentally used unde.ned constructs [26]. They demonstrate a case where a NULL-pointer \ndereference in Linux caused security-critical checks to be omitted by the compiler. Exploiting unde.ned \nbehaviour can improve the optimizations available to a compiler, but compiler writers must be cautious, \nas many existing programs unknowingly invoke unde.ned behaviour which can result in serious bugs or security \nvulnerabilities [20]. In this paper, because seL4 is proven to be free of unde.ned behaviour, we can \nuse much more aggressive optimizations without fear of introducing such bugs. 8. Conclusion Formal veri.cation \nis traditionally only used to demonstrate program correctness. We have demonstrated a technique which \ncan leverage existing proof effort to improve code op\u00adtimizations. We proposed two approaches, applying \ninvari\u00adants and program safety, both of which are pervasive for for\u00admally veri.ed programs. We have shown \nhow to reuse information such as explicit invariants from the formal veri.cation of a program and in\u00adtegrate \nit into the compilation of the code in order to improve its runtime performance. We also leverage the \nguarantee of program safety provided by formal veri.cation, to perform further optimizations by detecting \ninfeasible code paths. In\u00advariants may also be derived through informal reasoning and applied to unveri.ed \nsoftware. We evaluated our approaches and applied them to the formally-veri.ed seL4 microkernel. Integrating \nexplicit in\u00advariants gave a performance increase of up to 24% in some micro-benchmarks. When combined \nwith the guarantees of program safety, the maximum performance gains increase to 28%. On I/O-heavy macro-benchmarks, \nwe observe up to 16% improvements. Worst-case execution time was also re\u00adduced by 20 25%. Our experiments \nhave shown our techniques to be prac\u00adtical and effective for real-world applications. Constructing translation \nrules in order to make use of the invariants is cur\u00adrently a necessary but tedious process. Future work \nwill in\u00advestigate how to automatically extract the relevant informa\u00adtion from a formal speci.cation. \nAcknowledgements NICTA is funded by the Australian Government as repre\u00adsented by the Department of Broadband, \nCommunications and the Digital Economy and the Australian Research Coun\u00adcil through the ICT Centre of \nExcellence program. References [1] E. Alkassar, M. Hillebrand, D. Leinenbach, N. Schirmer, and A. Starostin. \nThe Verisoft approach to systems veri.cation. In N. Shankar and J. Woodcock, editors, VSTTE 2008, volume \n5295 of LNCS, pages 209 224. Springer, 2008. [2] J. Andronick, B. Chetali, and C. Paulin-Mohring. Formal \nVeri.cation of Security Properties of Smart Card Embedded Source Code. In J. Fitzgerald, I. J. Hayes, \nand A. Tarlecki, ed\u00aditors, FM, volume 3582 of LNCS, pages 302 317, Newcastle, UK, Jul 2005. Springer. \n[3] Y. Bertot, B. Gregoire, and X. Leroy. A structured approach to proving compiler optimizations based \non data.ow analysis.  In Types for Proofs and Programs, Workshop TYPES 2004, volume 3839 of LNCS, pages \n66 81, 2006. [4] B. Blackham and G. Heiser. Correct, fast, maintainable choose any three! In 3rd APSys, \npages 13:1 13:7, Seoul, Korea, Jul 2012. [5] B. Blackham, Y. Shi, S. Chattopadhyay, A. Roychoudhury, \nand G. Heiser. Timing analysis of a protected operating sys\u00adtem kernel. In 32nd RTSS, pages 339 348, \nVienna, Austria, Nov 2011. [6] S. Blazy and X. Leroy. Formal veri.cation of a memory model for C-like \nimperative languages. In International Conference on Formal Engineering Methods, vol. 3785 of LNCS, pages \n280 299, 2005. [7] S. Blazy, Z. Dargaye, and X. Leroy. Formal veri.cation of a C compiler front-end. \nIn 14th FM, volume 4085 of LNCS, pages 460 475. Springer, 2006. [8] M. Daum, N. W. Schirmer, and M. Schmidt. \nFrom operating\u00adsystem correctness to pervasively veri.ed applications. In IFM, volume 6396 of LNCS, pages \n105 120, Nancy, France, 2010. Springer. [9] R. J. Feiertag and P. G. Neumann. The foundations of a provably \nsecure operating system (PSOS). In AFIPS Conf. Proc., 1979 National Comp. Conf., pages 329 334, New York, \nNY, USA, Jun 1979. [10] L. Gu, A. Vaynberg, B. Ford, Z. Shao, and D. Costanzo. CertiKOS: A certi.ed kernel \nfor secure cloud computing. In 2nd APSys, 2011. [11] C. L. Heitmeyer, M. Archer, E. I. Leonard, and J. \nMcLean. Formal speci.cation and veri.cation of data separation in a separation kernel for an embedded \nsystem. In CCS, pages 346 355, Alexandria, VA, USA, 2006. [12] M. Hohmuth and H. Tews. The VFiasco approach \nfor a veri.ed operating system. In 2nd PLOS, Glasgow, UK, Jul 2005. [13] G. Klein, K. Elphinstone, G. \nHeiser, J. Andronick, D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt, R. Kolanski, M. Nor\u00adrish, T. Sewell, \nH. Tuch, and S. Winwood. seL4: Formal ver\u00adi.cation of an OS kernel. In 22nd SOSP, pages 207 220, Big \nSky, MT, USA, Oct 2009. ACM. [14] X. Leroy. Formal certi.cation of a compiler back-end, or: Programming \na compiler with a proof assistant. In J. G. Morrisett and S. L. P. Jones, editors, 33rd POPL, pages 42 \n54, Charleston, SC, USA, 2006. ACM. [15] J. Liedtke. On \u00b5-kernel construction. In 15th SOSP, pages 237 \n250, Copper Mountain, CO, USA, Dec 1995. [16] A. Ramirez, J. l. Larriba-pey, C. Navarro, J. Torrellas, \nand M. Valero. Software trace cache. In 13th Intl. Conference on Supercomputing, 1999. [17] S. Rideau \nand X. Leroy. Validating register allocation and spilling. In Compiler Construction, volume 6011 of LNCS, \npages 224 243, 2010. [18] T. Sewell, M. Myreen, and G. Klein. Translation validation for a veri.ed OS \nkernel. In PLDI, pages 471 481, Seattle, Washington, USA, Jun 2013. ACM. [19] H. Tews, T. Weber, and \nM. V \u00a8olp. A formal model of memory peculiarities for the veri.cation of low-level operating-system code. \nIn R. Huuck, G. Klein, and B. Schlich, editors, 3rd SSV, volume 217 of ENTCS, pages 79 96, Sydney, Australia, \nFeb 2008. Elsevier. [20] The GCC Team. GCC 4.8 release series: Changes, new features, and .xes. http://gcc.gnu.org/gcc-4.8/ \nchanges.html, 2013. [21] J.-B. Tristan and X. Leroy. Formal veri.cation of translation validators: A \ncase study on instruction scheduling optimiza\u00adtions. In 35th POPL, pages 17 27, 2008. [22] J.-B. Tristan \nand X. Leroy. Veri.ed validation of lazy code motion. In 2009 PLDI, pages 316 326, 2009. [23] J.-B. Tristan \nand X. Leroy. A simple, veri.ed validator for software pipelining. In 37th POPL, pages 83 92, 2010. [24] \nM. T. Vandevoorde. Speci.cations can make programs run faster. In Theory and Practice of Software Development, \nLNCS, volume 668, pages 215 229, 1993. [25] B. J. Walker, R. A. Kemmerer, and G. J. Popek. Speci.cation \nand veri.cation of the UCLA Unix security kernel. CACM, 23(2):118 131, 1980. [26] X. Wang, H. Chen, A. \nCheung, Z. Jia, N. Zeldovich, and M. F. Kaashoek. Unde.ned behavior: what happened to my code? In 3rd \nAPSys, pages 9:1 9:7, New York, NY, USA, 2012. ACM. [27] J. Whaley and M. S. Lam. Cloning-based context-sensitive \npointer alias analysis using binary decision diagrams. In 2004 PLDI, 2004. [28] R. Wilhelm, J. Engblom, \nA. Ermedahl, N. Holsti, S. Thesing, D. Whalley, G. Bernat, C. Ferdinand, R. Heckmann, T. Mitra, F. Mueller, \nI. Puaut, P. Puschner, J. Staschulat, and P. Sten\u00adstr \u00a8om. The worst-case execution-time problem overview \nof methods and survey of tools. ACM Trans. Emb. Comput. Syst., 7(3):1 53, 2008. ISSN 1539-9087.   \n \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Formal program verification offers strong assurance of correctness, backed by the strength of mathematical proof. Constructing these proofs requires humans to identify program invariants, and show that they are always maintained. These invariants are then used to prove that the code adheres to its specification.</p> <p>In this paper, we explore the overlap between formal verification and code optimization. We propose two approaches to reuse the invariants derived in formal proofs and integrate them into compilation. The first applies invariants extracted from the proof, while the second leverages the property of program safety (i.e., the absence of bugs). We reuse this information to improve the performance of generated object code.</p> <p>We evaluated these methods on seL4, a real-world formally-verified microkernel, and obtained improvements in average runtime performance (up to 28%) and in worst-case execution time (up to 25%). In macro-benchmarks, we found the performance of para-virtualized Linux running on the microkernel improved by 6-16%.</p>", "authors": [{"name": "Yao Shi", "author_profile_id": "81493651498", "affiliation": "NICTA and University of New South Wales, Sydney, Australia", "person_id": "P4290390", "email_address": "yao.shi@nicta.com.au", "orcid_id": ""}, {"name": "Bernard Blackham", "author_profile_id": "81493659884", "affiliation": "NICTA and University of New South Wales, Sydney, Australia", "person_id": "P4290391", "email_address": "bernard.blackham@nicta.com.au", "orcid_id": ""}, {"name": "Gernot Heiser", "author_profile_id": "81310487607", "affiliation": "NICTA and University of New South Wales, Sydney, Australia", "person_id": "P4290392", "email_address": "gernot@nicta.com.au", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509513", "year": "2013", "article_id": "2509513", "conference": "OOPSLA", "title": "Code optimizations using formally verified properties", "url": "http://dl.acm.org/citation.cfm?id=2509513"}