{"article_publication_date": "10-29-2013", "fulltext": "\n Multiverse: Ef.ciently Supporting Distributed High-Level Speculation Kaushik Ravichandran Georgia Institute \nof Technology kaushikr@gatch.edu Abstract Algorithmic speculation or high-level speculation is a promis\u00ading \nprogramming paradigm which allows programmers to speculatively branch an execution into multiple independent \nparallel sections and then choose the best (perhaps fastest) amongst them. The continuing execution after \nthe specula\u00adtively branched section sees only the modi.cations made by the best one. This programming \nparadigm allows pro\u00adgrammers to harness parallelism and can provide dramatic performance improvements. \nIn this paper we present the Multiverse speculative pro\u00adgramming model. Multiverse allows programmers \nto exploit parallelism through high-level speculation. It can effectively harness large amounts of parallelism \nby speculating across an entire cluster and is not bound by the parallelism avail\u00adable in a single machine. \nWe present abstractions and a run\u00adtime which allow programmers to introduce large scale high\u00adlevel speculative \nparallelism into applications with minimal effort. We introduce a novel on-demand address space shar\u00ading \nmechanism which provide speculations ef.cient trans\u00adparent access to the original address space of the \napplication (including the use of pointers) across machine boundaries. Multiverse provides single commit \nsemantics across specu\u00adlations while guaranteeing isolation between them. We also introduce novel mechanisms \nto deal with scalability bottle\u00adnecks when there are a large number of speculations. We demonstrate that \nfor several benchmarks, Multiverse achieves impressive speedups and good scalability across entire clusters. \nWe study the overheads of the runtime and demonstrate how our special scalability mechanisms are cru\u00adcial \nin scaling cluster wide. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, \nIndianapolis, IN, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509525 \nSantosh Pande Georgia Institute of Technology santosh@cc.gatech.edu Categories and Subject Descriptors \nD.1.3 [Programming Techniques]: Concurrent Programming Distributed pro\u00adgramming; D.3.3 [Programming Languages]: \nLanguage Constructs and Features Concurrent programming struc\u00adtures; D.3.4 [Programming Languages]: Processors \nRun\u00adtime environments Keywords speculation; parallelism; cluster; multiverse; al\u00adgorithmic speculation; \nfunction speculation 1. Introduction Speculation is a powerful method of harnessing the avail\u00adable parallelism. \nSpeculation has long been used at the hard\u00adware level in the form of branch prediction and prefetching. \nBranch prediction guesses which way a branch will go be\u00adfore it is known for sure. Prefetching guesses \nfuture mem\u00adory accesses and uses this to fetch information before it is actually needed. Compilers employ \nspeculation in the form of Thread Level Speculation (TLS) [14, 36]. Hardware and compiler based speculation \nmechanisms are typically not controlled by the programmer and happen under the hood. Programmer controlled \nspeculation has the potential to provide even greater performance improvements. As it be\u00adcomes increasingly \ndif.cult to extract more parallelism from applications through traditional means at the application level, \nspeculation brings a refreshingly orthogonal view to the problem. Different proposals to exploit speculation \nat the application level exist. Prabhu et al. introduced a program\u00adming model which can speculatively \nparallelize loops by us\u00ading value speculation [30]. Software Transactional Memory (STM) systems [15] \nand models such as Galois [23] allow programmers to speculate about the concurrency of different operations. \nHigh-level (also known as algorithmic or function-level) speculation is another type of speculation that \ncan be in\u00adtroduced by the programmer. Many important applications demonstrate variance in execution time \neven on the same in\u00adput. Other applications are amenable to being solved through multiple approaches \nwith varying execution times. For these kinds of applications, programmers can use high-level spec\u00adulation \nto speculatively run multiple instances of the (same or different) function or algorithm in parallel \nand then dy\u00adnamically choose to incorporate the results from the best one. We will refer to this kind \nof speculation as high-level speculation and it is the focus of this paper.  Speculation leads to an \neasier path from sequential code to parallel code. Designing parallel algorithms is tough, writing a \nparallel program (based on parallel algorithms) is tougher, writing a correct parallel program is even \ntougher and writing a correct and highly ef.cient parallel program is the toughest! But per Amdahl s \nlaw overall performance will not scale unless the least parallelizable parts can scale up. So the question \nis: how do we support ef.cient speculation that scales with the number of available processors. High-level \nspeculation has shown to improve perfor\u00admance signi.cantly for a wide variety of applications [6, 31, \n37]. Programming models and frameworks which support high-level speculation aim to provide the requisite \ninfras\u00adtructure to allow programmers to quickly and ef.ciently ex\u00adploit high level speculative parallelism \nwith minimal effort. However, the current programming models and frameworks are severely limited by both \nthe amount of speculation that they support and their ease of use. The amount of speculation that these \nmodels are designed to support is strictly limited by the amount of parallelism available on a single \nmachine. Their designs do not allow for scaling outside a single ma\u00adchine. For example, if the machine \nbeing used has 8-cores, the maximum number of parallel speculative threads that can be run at a time \nis 8. Over-provisioning threads to cores serializes speculations defeating their purpose. To speculate \nat large scales moving to a distributed (cluster) environment is inevitable. Distributed speculation, \nwith the fundamental shift from shared memory to distributed memory, requires a completely different \napproach and new designs. Distributed speculation brings along with it a host of transparency, ef\u00ad.ciency, \nscalability and ease of use issues. In this paper we present several novel techniques and elegant mechanisms \nto tackle these non-trivial issues. Like previous work [6, 31, 37] our goal is to provide a framework \nwhich programmers can easily leverage to incorporate high level speculation into their applications with \nminimal effort. We now brie.y dis\u00adcuss and motivate the need for large scale distributed high\u00adlevel speculation \nwhile empirically demonstrating the inad\u00adequacy of current models. 1.1 Motivation While variance in \napplication execution time based on input data is expected, many exhibit variance even on the same input \ndata. There are a wide variety of reasons why a vari\u00adance may arise. For example, it may be due to randomness \nin the algorithm. It may be due to the ability to solve a prob\u00adlem using multiple approaches, where each \napproach has a drastically different execution time. Each of these multiple approaches may even produce \nresults with different levels of desirability (quality). Non-deterministic behavior may even be due to \nissues such as scheduling. This variance in ex\u00adecution time is the key to speculatively parallelizing \nthese applications. To leverage this kind of variance at the appli\u00adcation level the programmer can branch \nthe execution of a program at a speculation point into multiple independent speculations running in parallel. \nFrom these parallel specu\u00adlations, the best one is chosen and its results are used as the execution continues \npast this speculative region (depicted by Figure 2). Let s now see why previous approaches which are \nlimited to speculation (using the parallelism) in a single ma\u00adchine are not suf.cient to extract all \nthe available parallelism in the application.   signments and .ips to .nd satisfying solutions. Due \nto the 1.2 Multiverse Execution Model and Terminology random nature of the algorithm, WalkSAT exhibits \ndramatic variability in execution time. Figure 1(a) plots the execution time of 500 runs of WalkSAT on \na sample DIMACS [33] dataset 1. A majority of the runs exhibit high execution times while a few of them \nexhibit dramatically reduced execution times. This is simply due to the random nature of the algo\u00adrithm. \nNote again that these execution times are for the same input dataset and also note the log scale on the \ny-axis. Fig\u00adure 1(b) plots the cumulative distribution frequency (CDF) of the execution time. It shows \nthe probability (y-axis) that the runtime will be less than a particular value (x-axis). We observe that \nthe probability of getting a very low execution time is fairly small. Instead of running just a single \ninstance of WalkSAT to solve a SAT problem, if the original application were to speculatively run multiple \ninstances of the WalkSAT func\u00adtion in parallel and choose the fastest (best) one it can get signi.cant \nperformance improvements. To quantify the im\u00adprovements of execution time of the WalkSAT function in \nour example we need to compute the expected execution time with respect to the number of parallel instances. \nWe assume that each individual instance of the function runs on a single processing core. To get the \nexpected execution time for X cores we take X random values from the 500 execution samples and choose \nthe minimum value as the expected ex\u00adecution time (averaged over a large number of tries). Figure 1(c) \nshows the expected execution time with an increasing number of instances (up to 256). Signi.cant speedup \ncan be observed as the number of instances increase. In fact, the speedup is super linear! This is simply \ndue to the wide vari\u00adability in the underlying execution times and the CDF for this example. Importantly, \nwe observe good scalability go\u00ading well past the typical number of cores on a single ma\u00adchine. This points \nto the fact that speculation outside a single machine has signi.cant bene.ts and can enable sustained \nscalability at large scales. In this paper we propose the Multiverse speculative pro\u00adgramming model for \nthe C language. Multiverse is designed from the ground up to deal with large scale distributed spec\u00adulation. \nWe present abstractions and a runtime which allow programmers to introduce large scale speculation into \nappli\u00adcations with minimal effort. The programming model allows for multiple independent speculations \nto run concurrently in a single address space (across a cluster) while transpar\u00adently maintaining isolation \nbetween them. Multiverse pro\u00advides single commit semantics, ensuring that the changes made by only the \nbest speculation transparently becomes visible to the rest of the program as it continues its execu\u00adtion \npast a speculative region. Multiverse allows program\u00admers to scalably harness the available parallelism \nacross an entire cluster and is not limited by the parallelism in a single machine. 1Dataset used was \npar16-5.cnf Let s consider an application that can bene.t from the specu\u00adlative execution model (Figure \n2). In general it can be repre\u00adsented by the code prior to the speculation (pre-speculation code), the \nspeculative code itself (each individual instance is called a speculation) and the code that executes \nafter the speculation (post-speculation code). The speculative region is the part of the code where multiple \noptions are specu\u00adlated on (and run in parallel). Speculative boundaries sep\u00adarate speculative regions \nfrom the rest of the code. If n spec\u00adulations are launched in a speculative region, then the degree of \nspeculation is said to be n. Only changes made by the best speculation are visible to post-speculation \ncode while others are discarded. Figure 2. Basic Multiverse Speculative Execution Model Each individual \nspeculation can execute the same code (as in the WalkSAT example) or it could execute different pieces \nof code. For example, if an application wanted to perform sorting speculatively, each individual speculation \ncould run a different sorting algorithm. An application may have multiple speculative regions. Multiverse \nenables spec\u00adulative execution even across machine boundaries (without the presence of physical shared \nmemory) while still provid\u00ading a single uni.ed address space for all speculative and non\u00adspeculative \ncode to run in. This means that speculative code does not need to be written in any special way and still \ngets transparent access to the original address space of the pro\u00adgram. It even supports the use of arbitrary \npointers across speculation boundaries. This is a very powerful abstraction and is key to the Multiverse \nframework as it allows program\u00admers to write speculative code just like normal code with minimal effort. \nMultiverse is designed to harness the parallelism across a typical cluster comprising of a set of compute \nnodes (the word node is used interchangeably with machine) each with some number of processing cores. \nThe nodes are connected by a high speed interconnect without the presence of any physically shared memory. \n  1.3 Contributions We make the following contributions in this paper: We discuss the need for large \nscale distributed specula\u00adtion, demonstrate inadequacy of current approaches and motivate the need for \na completely new approach to scale.  We present a novel on-demand address space sharing mechanism which \nenables seamless ef.cient access to the original address space of the application (including arbi\u00adtrary \npointer accesses) even across machine boundaries with minimal programmer effort.  We present novel runtime \ntechniques to scale speculative execution across an entire cluster. Transparent address space sharing \ncan quickly become a performance bottle\u00adneck and we present techniques to effectively tackle scal\u00adability \nconcerns.  We present a simple API which can be incorporated into any C application with minimal effort. \n We evaluate our framework on a set of benchmarks and demonstrate dramatic performance improvements. \n The remainder of this paper is organized as follows. In Section 2 we go over the usage of the Multiverse \nprogram\u00adming model. In Section 3 and Section 4 we discuss Mul\u00adtiverse s design and how it achieves its \ngoals. In Section 5 we discuss scalability issues. Section 6 evaluates Multiverse over several benchmarks \nand discusses other performance implications. We present related work in Section 7 and con\u00adclude in Section \n8. 2. Multiverse Usage Multiverse allows programmers to easily describe specula\u00adtive regions in code. \nTo understand what Multiverse allows programmers to do and how it does it, let us consider an ap\u00adplication \nwhich solves a SAT problem using WalkSAT. Let s say the original application invokes the WalkSAT function \nwith some arguments like: walksat(param1, param2, problem_ptr, solution_ptr); Let s say that this function \nexhibits variance in execution time which can be exploited through speculative paralleliza\u00adtion. Multiverse \nallows the programmer to speculate on mul\u00adtiple instances of it by simply wrapping it with the keywords \nspeculate with and by specifying the degree of speculation (in this case 100). speculate_with(100, walksat(param1, \nparam2, problem_ptr, solution_ptr)); This can be read as speculate with a 100 instances of this walksat \ninvocation. Let s look at this function call in a slightly larger context and see how exactly this would \nwork and the challenges it involves. Consider the code fragment in Figure 3 (while this is just demonstrative \npseudo-code it contains all necessary modi.cations to use Multiverse). # i n c l u d e m u l t i v e \nr s e . h s o l v e s a t ( S a t P r o b l e m * p r o b l e m p t r ) { / / Some s t a c k v a r i \na b l e s . i n t p a r a m 1 , p a r a m 2 ; / / A l l o c a t e d o n h e a p . S a t S o l u t i o \nn * s o l u t i o n p t r = m a l l o c ( s i z e o f ( S o l u t i o n ) ) ; / / S p e c u l a t e w \ni t h 1 0 0 i n s t a n c e s o f Wa l k SAT . s p e c u l a t e w i t h ( 1 0 0 , w a l k s a t ( p \na r a m 1 , p a r a m 2 , p r o b l e m p t r , s o l u t i o n p t r ) ) ; / / A n on-s p e c u l a \nt i v e e x e c u t i o n w o u l d l o o k l i k e : / / w a l k s a t ( pa r a m1 , pa r a m2 , p r \no b l e m p t r , s o l u t i o n p t r ) ; / / V e r i f y t h e s o l u t i o n . v e r i f y s a \nt ( p r o b l e m , s o l u t i o n ) ; } / / Cod e w h i c h e x h i b i t s v a r i a n c e . w a l \nk s a t ( i n t p a r a m 1 , i n t p a r a m 2 , S a t P r o b l e m *p r o b l e m p t r , S o l u \nt i o n * s o l u t i o n p t r ) {. . . R e a d / w r i t e a r b i t r a r y d a t a . . . . R e a \nd a n d s o l v e p r o b l e m f r o m p r o b l e m p t r . l o o p { . . . R e a d / w r i t e a r \nb i t r a r y d a t a . } s o l u t i o n p t r ->s o l v e d = 1 ; s o l u t i o n p t r ->s o l u \nt i o n = <u p d a t e s o l u t i o n >; } Figure 3. Example Multiverse Usage When execution reaches \nthe speculate with call, Multi\u00adverse takes charge, creates 100 different instances (specula\u00adtions) of \nthe walksat function and launches them on mul\u00adtiple machines as needed. Like any normal function, each \nindividual walksat function has access to variables in its scope (this is non-trivial to achieve for \nprocesses on dif\u00adferent machines). It uses param1 and param2 to control its behavior. It reads the SAT \nproblem from problem ptr, solves it and updates the solution pointed to by solution ptr (which was dynamically \nallocated on the heap in solve sat). Each speculative invocation of the function can indepen\u00addently modify \nvariables and only changes made by the best speculation will be made visible to post-speculation code. \nDue to the cluster environment, the original application and the speculations are all implemented as \ndistinct pro\u00adcesses. The process running the original application is re\u00adferred to as Process0 and the \nothers which execute specula\u00adtive code are referred to as speculative processes. MPI (the OpenMPI implementation) \nis used for communication. Ease of use: In this example, the addition of the speculate with (and corresponding \n#include) is the only code change that needs to be made. speculate with is syn\u00adtactic sugar and is implemented \nas a macro which makes calls to the Multiverse runtime. Note that Multiverse does not require programmers \nto annotate or mark or wrap variables for cross machine/speculative access and requires no complicated \nchanges to the code. This makes Multiverse fairly easy to use for any programmer.  Implications: This \nsimplicity has several implications on the runtime. Speculative processes need to continue execu\u00adtion \nfrom exactly the point that Process0 began the specula\u00adtion. Data modi.cations need to be private to \neach specula\u00adtion. Changes made by only the best speculation need to be made visible. Isolation between \nspeculations is easily satis.ed by the use of distinct processes with private address spaces. The biggest \nchallenge is now providing the illusion of a single address space to each of these speculations in different \npro\u00adcesses across machines scalably. We discuss how Multiverse achieves this in Section 3 and discuss \nscalability in detail in Section 5. 2.1 Limitations While Multiverse tries to make it as easy as possible \nto incorporate high level speculation into applications, it does have some limitations: Code executing \nin speculative regions must not perform I/O.  Code must use malloc/f ree (and not mmap or custom allocators \nunless tuned for Multiverse).  More generally, code in speculative regions must not use system calls \nwith side effects as Multiverse does not support transparent access of open sockets, .les and other system \nspeci.c constructs across speculative boundaries.  Further, our current implementation has the following \nlimitations: Speculative code must not use global variables (we dis\u00adcuss workarounds in Section 3.4). \n Multiverse does not support nested speculation.  No support for multi-threaded code across speculative \nboundaries.  Our implementation requires a 64-bit address space.  3. Multiverse Design Multiverse enables \ntransparent execution of speculations in a distributed setting by using a unique on-demand address space \nsharing mechanism. To understand how this works let us look at what describes the execution state of \na process at a particular point in time. It is the content of its address space as well as its execution \ncontext. The address space consists of the code, global, stack and heap sections (as shown in Figure \n5(a)), note that we are referring to the virtual ad\u00address space of a process here. The execution context \nof a process contains the contents of machine registers amongst other items (for example it contains \nthe current stack pointer and program counter). Speculative processes need a copy of the address space \nand execution context at the speculative boundary from Process0. Multiverse sends copies of these to \nspeculative processes when a speculate with call (a specu\u00adlative boundary) on Process0 is reached. Once \nthe best spec\u00adulation completes, Multiverse performs the reverse operation by copying that speculation \ns address space and execution context back to Process0 to continue execution. During the execution of \nthe speculative region Process0 acts a Speculative Region Server. The Speculative Region Server is the \nMultiverse component in charge of orchestrat\u00ading the execution of different speculations (including starting \nspeculations, stopping speculations and determining which is the best speculation). This execution model \nis depicted in Figure 4. Figure 4. Multiverse Speculative Execution Model This is the basic idea behind \nMultiverse s execution model. However, as one can imagine this approach comes with many dif.culties and \ncan potentially be very inef.cient at a large scale. We discuss how Multiverse makes this solu\u00adtion practical \nand ef.cient. Large Unused Virtual Address Space In principle, a 64\u00adbit processor can address 16 exabytes \nof memory with a 64 bit virtual address. Most operating systems and applications will not need such a \nlarge address space for the foreseeable future so implementing such wide virtual addresses simply increases \nthe complexity and cost of address translation with no real bene.t. Current implementations limit the \nvirtual ad\u00address space to only the least signi.cant 48 bits of a virtual address and require that the \nremaining bits be sign extended [28], resulting in 256 terabytes of virtual address space. Re\u00adcall that \neach process running on a machine has its own in\u00addependent virtual address space fully available to it. \nMost applications don t even use a tiny fraction of this address space. Multiverse leverages this fact \nand uses this large un\u00adused virtual address space to enable transparent execution of speculations.  \n3.1 Code Section Since Multiverse does not place restrictions on functions that are called in speculative \nregions, the code section in specula\u00adtive processes must be identical to that of Process0 s. There are \ntwo broad approaches to get the same code section into another process at the start of a speculative \nregion. The .rst approach is to launch multiple identical processes with the same code section as Process0, \nall at Process0 s startup time. In this approach all processes except Process0 go to sleep and wait to \nbe woken up (at the start of a speculative region). The second approach is to spawn new speculative processes \nwhen a speculative region is encountered in Process0. While conceptually the second approach seems cleaner, \nspawning many new processes across a cluster every time a new spec\u00adulative region is encountered is inef.cient \nand incurs too much overhead. The .rst approach of keeping prelaunched processes in a sleep state is \nmore ef.cient since the spawning is done only once and in practice these sleeping processes take up virtually \nno resources. Hence Multiverse uses this approach. This is akin to the use of thread pools instead of \nindividually spawning threads as and when they are needed [12]. Therefore when a Multiverse enabled application \nis launched, multiple copies of the same executable are started up on multiple machines to form a pool \nof available pro\u00adcesses that can execute speculative regions of code. These processes are referred to \nas the Speculative Process Pool (SPP). Note that all the processes in the SPP do not need to participate \nin a particular speculative region but participa\u00adtion is based on the degree of speculation as speci.ed \nby the programmer. Those processes of the SPP which participate in a speculative region are referred \nto as Participating Specu\u00adlative Processes (PSPs). mpirun [29] is used to start up pro\u00adcesses in the \nSpeculative Process Pool. mpirun is a highly con.gurable OpenMPI command that can be used to effec\u00adtively \ncontrol the distribution of the pool across the cluster and is highly optimized to launch MPI processes \nonto thou\u00adsands of machines. Now that we have a mechanism to get the code section on to processes in \nthe SPP we can shift our focus on how to move the execution context and stack.  3.2 Execution Stack \nand Context The execution context describes the current execution state of an application. It contains \nthe process s machine registers, signal mask and a pointer to the current execution stack. The execution \ncontext can be used to allow processes in the SPP to continue exactly at the point that Process0 left \noff. The current prototype uses several Linux system calls to manipulate the execution context: setcontext, \ngetcontext, makecontext and swapcontext [19]. Multiverse uses these calls to get a copy of the execution \ncontext at speculative boundaries and to move them between processes as needed. To enable transparent \nexecution of PSPs a copy of the execution stack from Process0 is also needed. Multiverse allows programmers \nto use pointers to stack locations trans\u00adparently across speculative boundaries. This means that pointers \nmust point to valid locations as they move from Process0 to the PSPs and back. To do this there are two \ndifferent approaches. In the .rst approach, Multiverse can create a copy of the execution stack from \nProcess0, reload it at a free memory location in the speculative process and then translate all the pointers \nto point to the right memory locations. Scanning the stack and translating pointers is inef.cient and \ncan be inac\u00adcurate as it may not be possible to accurately discover and translate pointers in all situations. \nHence we use a second approach, in which the stack from Process0 is copied into exactly the same address \nin the PSPs. In this approach, there is no need to translate any pointers since all the memory lo\u00adcations \nare in exactly the same locations as in Process0 and is hence more ef.cient. Note that since we are always \nre\u00adferring to the virtual address space of a process and not the physical address space we are guaranteed \nof its availability even on a different machine. Figure 5. Virtual Address Space To provide the illusion \nof transparent execution between speculative boundaries two distinct stacks need to be main\u00adtained for \neach process. One on which application code ex\u00adecutes, the multiverse stack, and one which runtime code \ncan use when application code is not executing, the standard stack. Multiverse allocates the multiverse \nstack in an unallo\u00adcated section of memory. During startup Multiverse ensures that this section is free \nacross all process and reserves nec\u00adessary memory using the mmap [19] system call (a 64-bit virtual address \nspace makes this easily achievable). At this point there are two stacks mapped into the address space \non all processes, as shown in Figure 5(b). Multiverse switches between them as needed and only one stack \nwill be in use at a given time. We refer to this behavior as stack switching . When processes startup \n(Process0 as well as processes in the SPP) all of them begin their execution on the standard stack (regular \nC execution stack). The behavior past this point is different for Process0 and for processes in the SPP. \nWe dis\u00adcuss stack switching in the next couple of paragraphs (Fig\u00adure 6 depicts this for pseudocode). \n Process0 Process0 starts up in the standard stack. How\u00adever, before the .rst user instruction in main \neven runs, Multiverse transparently creates a fresh multiverse stack and switches the execution context \nonto this stack. The applica\u00adtion code as written by the programmer executes in this new multiverse stack \n(shown in Figure 6(a)). Multiverse performs context and stack switching using the makecontext and swapcontext \nsystem calls [19]. As execution continues and a speculative boundary is reached, Multiverse saves the \ncur\u00adrent execution context and switches the context back on to the standard stack. Subsequent Multiverse \nruntime code ex\u00adecutes on the standard stack. At this point Process0 changes its role to become the Speculative \nRegion Server (still on the standard stack). It selects PSPs from the SPP and sends them a copy of the \ncurrent multiverse stack and the saved execu\u00adtion context. Processes in the SPP Processes in the SPP \nstart up in the standard stack as well (Figure 6(b)), and begin their execu\u00adtion by calling into the \nMultiverse framework. Multiverse puts these processes to sleep and waits for a message from Process0 \nto begin execution of a speculative region. Once such a message is received the process wakes up and \nbe\u00adcomes a Participating Speculative Process (PSP). The PSP then waits for a copy of the current multiverse \nstack and execution context from Process0. Once received, the mul\u00adtiverse stack is directly mapped into \nmemory (at the multi\u00adverse stack address) and the saved execution context from Process0 is used to switch \nexecution of the PSP over to the multiverse stack. At this point the PSP is executing on a copy of the \nmultiverse stack from Process0 (with pointers intact). Execution continues until the end of the speculative \nregion at which point a call to Multiverse is automatically made. The reverse operation is performed \nat the end of a spec\u00adulative region after the best speculation is chosen. The con\u00adtext and multiverse \nstack contents are sent from the best PSP back to the Speculative Region Server which then maps it into \nProcess0 s multiverse stack and switches the execution back. Note that this method only copies the multiverse \nstack between processes. The standard stack is left in tact in pro\u00adcesses and is never copied or copied \ninto. The multiverse stack becomes the execution stack for the application code (as speci.ed by the programmer) \nwhereas the standard stack serves as the stack that the Multiverse runtime uses. So far we ve dealt with \nmoving the stack to and from Process0 and PSPs and transparently dealing with pointers Figure 6. Stack \nSwitching (Multiverse Stack and Standard Stack) to stack memory. Typically stack sizes are small and \ntrans\u00adferring the stack between processes does not incur a large overhead (evaluation in Section 6.6). \nThe next step is to deal with dynamically allocated heap memory.  3.3 Heap Section C applications use \nmalloc/f ree to dynamically allocate heap memory. The stack is a limited resource and when a large amount \nof memory is needed, allocations occur on the heap. This makes transparent access to the heap section \na fundamentally different problem from the smaller .xed stack section. With a large and varying heap \nallocation it might not make sense for Multiverse to transfer all of it over to PSPs because code executing \nin the speculative region might not even need access to all of it. Predicting before hand the memory \nthat is needed by PSPs is impractical in situations where pointers are used and can vary based on the \nsituation. To combat these issues Multiverse uses a novel on-demand sharing mechanism where only the \nheap mem\u00adory from Process0 that is actually needed during execution of a PSP is transparently transferred \nover. Multiverse creates special heap sections in the address space, shown in Figure 5(c). All malloc \nallocations happen in these specially allocated sections. These sections are all allocated at page boundaries \nand are the same across all pro\u00adcesses. Since the virtual address space (independent to each process \nrunning on a machine) is so large, Multiverse can easily .nd unused addresses and map it for use. Multiverse \nprovides custom malloc,f ree implementations which are transparently hooked into the program to ensure \nallocations happen in these sections. Heap allocations in Multiverse be\u00adhave differently based on whether \nthey are occurring during the execution of regular code (in Process0) or during execu\u00adtion of a speculative \nregion (in a PSP). Heap Memory allocated on Process0 Heap allocations on Process0 happen in the Process0 \nHeap. When a PSP is ex\u00adecuting code in a speculative region it might need access to some memory from \nthis heap. Multiverse provides trans\u00adparent, ef.cient on-demand access to this memory. Before a PSP starts \nexecuting, Multiverse protects the entire memory range in the Process0 Heap section in its address space \nusing the mprotect system call [19]. The mprotect call ensures that any accesses to these memory locations \nwill lead to seg\u00admentation faults. The PSP can start executing even without any of the heap memory, it \nsimply needs the execution con\u00adtext and the stack (which have been made available to it). Once the execution \non a PSP encounters a heap memory ac\u00adcess, a segmentation fault (due to the mprotect call being used) \nwill be generated. Multiverse installs signal handlers at the start of execution on processes in the \nSPP to catch these segmentation faults. If a signal handler catches a seg\u00admentation fault it .rst computes \nthe page that the fault is arising on (based on the address of the fault and the system wide virtual \nmemory page size). It then checks if that is in Process0 s Heap section. If so, an MPI request is sent \nover to Process0 which is currently behaving as the Speculative Region Server (pseudo-code in Figure \n7).  Page Server: In addition to acting as the Speculative Region Server, Process0 also takes on the \nrole of a Page Server during the execution of a speculative region. The Page Server services requests \nfor pages in the Process0 Heap. Note that the Page Server is executing in the con\u00adtext of Process0 itself \nand as such has direct access to all of Process0 s memory. If a request for a heap memory page is received \nfrom a PSP it simply sends over the entire page of memory that is being requested. The Page Server does \nnot need to do any heavy book keeping but only simple valida\u00adtions. Once the page is received by the \nPSP it maps it into its memory at the same location (obviating the need to update pointers), updates \nits dirty page information and continues execution. All pages that are received from the Page Server \nare marked as dirty. When the speculative region completes and the best speculation (best PSP) is chosen, \nall the pages that have been fetched from Process0 in the best PSP are transferred back to Process0. \nThis ensures that all changes made to these memory pages are available for the post\u00adspeculation code. \nAn optimization, which has not been cur\u00adrently implemented, is to mark pages as read only when .rst brought \nin and then mark them as dirty only on a write fault. Thus reducing the number of pages that need to \nbe trans\u00adferred back to only those which have been modi.ed. Apart from being transparent to the programmer \nthis ap\u00adproach of transferring memory provides multiple bene.ts: Once a page is brought into memory \nby the signal han\u00addler, subsequent accesses to that page will not generate another fault. This implies \nthat only one fault will be gen\u00aderated per page of memory that is accessed by the PSP.  Since the memory \ntransfer is done at a page level there is much use of spatial locality. Transfers done at a per\u00advariable/object \nbasis introduce too much traf.c and slows down execution of the PSP signi.cantly.  v o i d s i g n a \nl h a n d l e r ( v o i d * a d d r e s s ) {i f ( ! PSP ) {r e t u r n ; } i f ( ! ( a d d r e s s i \nn v a l i d r a n g e ) ) { r e t u r n ; } v o i d *b a s e P a g e A d d r e s s = c o m p u t e P \na g e A d d r e s s ( a d d r e s s ) ; s e n d t o P a g e S e r v e r r e q u e s t f o r b a s e P \na g e A d d r e s s ; r e c e i v e f r o m P a g e S e r v e r b a s e P a g e c o n t e n t s a s \nb a s e P a g e ; map b a s e P a g e i n t o b a s e P a g e A d d r e s s ; u p d a t e d i r t i e \nd p a g e s i n f o r m a t i o n ; } Figure 7. Pseudo-code for Signal Handler Different PSPs might \nneed to access disjoint areas of the heap section and this approach ensures that only relevant memory \nis transferred to each individual PSP.  Process0 might have large objects/structures allocated in the \nheap which are not related to the speculative region code at all and hence do not need to be transferred. \n Heap Memory allocated in a PSP One practical alterna\u00adtive is to disallow sharing of memory that is \nallocated in a PSP. Recall that memory can always be allocated in the pre-speculative code and passed \nin as a pointer and the PSP would have complete transparent access to it. To allow heap memory that is \nallocated in a particular PSP to be available even after a speculative region, Multi\u00adverse uses a separate \nSpeculative Process Heap (as shown in Figure 5(c)). All malloc allocations in any PSP happen in this \nsection independently. This section is protected us\u00ading mprotect as well. Before a PSP starts execution, \nall the malloc/free state information from previous speculative re\u00adgion executions are transferred to \nit (along with the execu\u00adtion context and stack). For new memory allocations an un\u00adused page is found \nin this section, protections are removed on the page, and it is used. If accesses are made to mem\u00adory \nthat was previously allocated in the Speculative Process Heap the access page faults as in the previous \nexample and the page is brought in from Process0 and execution contin\u00adues. Note that only the memory \nallocations made by the best speculation are transferred back to Process0 along with its malloc/free \nstate information. In this method all heap memory allocated in the best speculation is transferred back \nto Process0 at the end of a speculative region. An alternative could be to make the best PSP the new \nProcess0. However, this would either entail transferring the remainder of Process0 s address space to \nthe best PSP or it would necessitate each of the processes in the SPP (which have been best PSPs in the \npast) to act as page servers (answering page requests for their memory) throughout their execution, slowing \ndown future speculative region executions. Multiverse, hence, simply transfers back all allocated heap \nmemory to Process0.  One glaring issue with this entire approach is that Pro\u00adcess0 is in charge of .elding \npage requests from all PSPs and could potentially become a bottleneck. We discuss how to solve this scalability \nissue in Section 5.  3.4 Globals Section As pointed out in Section 2.1, Multiverse does not support \ntransparent access of information stored in the global section (including static data) across speculative \nboundaries. While it is technically possible to simply copy the global section and restore it on the \nPSP (similar to how the stack section is dealt with) this is generally not desirable. The global section \ntypically contains machine speci.c information like the communication environment which should not be \nmoved around. This implies that in our implementation the programmer cannot access globally visible data \nacross speculative bound\u00adaries. An easy way around this is to privatize global variables by passing them \naround in a structure. If this is not feasible to do or the programmer is not willing to, automatic source\u00adto-source \nconversion techniques which privatize global vari\u00adables such as [27] can be used to transparently perform \nthis conversion. In any case, the use of global variables is widely considered a bad programming practice \nand its use is highly discouraged [41]. 4. Complete Execution Flow and API We shall now summarize the \nentire execution .ow (shown in Figure 8). At startup time Process0 starts executing ap\u00adplication code \nand processes in the SPP go to sleep until a message is received from Process0. Process0 executes appli\u00adcation \ncode normally until it encounters a speculative region. At the start of the speculative region a copy \nof the current ex\u00adecution stack and context is made and Process0 switches to the role of Speculative \nRegion Server/Page Server as it or\u00adchestrates the execution of the speculative region. The Spec\u00adulative \nRegion Server sends a start message to PSPs in the SPP with copies of the execution context, stack and \nmalloc information. The PSPs awake from their sleep and switch their execution context to that of Process0 \ns. They execute the code in the speculative region. Any faults to unallocated memory in the PSPs are \ncaught and sent over to the Page Server on Process0. The Page Server handles memory re\u00adquests from PSPs \nand sends back the requested memory. Execution Termination When a PSP .nishes execution, Multiverse will \nautomatically send a noti.cation to the Spec\u00adulative Region Server. If the best speculation is not to \nbe de\u00adtermined by which speculation .nishes .rst but rather the quality of a result generated, a call \nto a Multiverse API spec\u00adifying the quality (desirability) can be made (see Section 4.1). The Speculative \nRegion Server waits until it can .nd a best speculation. Once it does, it will send a message to Figure \n8. Detailed Execution Model all other executing PSPs to terminate execution early. All PSPs periodically \ncheck for such a message using a timer interrupt (programmer can modify default interval). On an interrupt \nMultiverse simply checks for a terminate message and if found the PSP s execution is stopped. Multiverse \nau\u00adtomatically sets up timer interrupts for PSPs. The best speculation as chosen by the Speculative Region \nServer, sends over its execution context, stack, dirty pages and newly allocated heap pages. All speculations \ndiscard their states after executing and go back to sleep waiting for another speculative region to begin. \n 4.1 API and Usage Multiverse has a fairly simple API. To make use of the framework the application should \ninclude header .les and link with the library. A speculation with 100 instances of f oo can be started \nwith: speculate_with(100, foo(a, b)); To start speculations with different functions simply use the following \nsyntax: speculate_with(100, foo1(a, b), 100, foo2(a, b, c), 100, foo3(d)); If these calls are used the \nbest speculation is automati\u00adcally de.ned to be the one which .nishes .rst. Multiverse also supports \nthe use of an arbitrary quality parameter. In this case Multiverse will continue executing speculations \nun\u00adtil atleast one of them produces a result with quality greater than or equal to the speci.ed value \n(in the following example quality is 42):  speculate_with_quality(100, foo1(a, b), 100, foo2(a, b, c), \n42); Similar APIs for quality less than and equal to are also available. Multiverse uses the end of function \nexecution to automatically determine the end of the speculative region execution on a PSP. However, if \nwe need to return a quality parameter or if we need to update the current quality metric (during execution) \nwith the runtime, the following APIs can be used: multiverse_running_with_quality(43); multiverse_completed_with_quality(43); \nOther API calls are available which allows code to in\u00adspect if it is running in a speculative region \nand if so which speculation number it is and the degree of speculation. Nested speculations are not allowed \nat this point of time. A discussion of possible designs for this are outside the scope of this paper. \nAlternative Implementations A Checkpoint/Restart de\u00adsign was an alternative we considered. There has \nbeen much research in checkpoint/restart schemes over the years [1, 10, 32]. However, checkpointing typically \nincurs many magnitudes higher overhead than our current approach sim\u00adply because the entire address space \nis checkpointed and then reloaded. Spawning processes during the start of every speculative region across \nan entire cluster and reloading the checkpoint .le was simply not scalable in our experiments. Alternative \nimplementations such as the use of an existing global/distributed shared memory framework for Multiverse \ninternals was too heavy weight and had numerous restric\u00adtions which were unsuitable for us. They often \nrequired pro\u00adgrammers to annotate shared variables, did not allow for access to stack variables across \nprocesses and none were de\u00adsigned to deal with scalability issues that arise uniquely in the speculative \nmodel (see Section 5). 5. Scalability In this section we discuss how we tackle the issue of scal\u00adability. \nThere are two potential bottlenecks for scalability. One is in transferring the stack and execution context \nto all PSPs across a cluster at the start of a speculative region. The other is due to Process0 taking \non the responsibility of an\u00adswering page requests (for heap memory) from all PSPs. The second concern \nis far more worrisome since large amounts of heap memory can potentially be accessed by numerous PSPs \nquickly making Process0 a bottleneck. Let us .rst ad\u00address this second potential bottleneck. 5.1 Heap \nMemory Accesses It should be noted that the contents of the address space (heap section) that is being \nfetched from Process0 is com\u00adpletely static and does not change until the end of the specu\u00adlative region. \nEach individual PSP can make its own changes to this memory but those changes will not be visible until \nthe end of the speculative region (after the best speculation is chosen). We can leverage this fact by \nmaking copies of all allocated heap pages in multiple processes and machines. This alleviates the need \nto always go to Process0. This still does incur some copy overhead, since before the PSPs begin execution \nwe need to duplicate all allocated heap pages into several copy processes (across different machines) \nso that no single machine becomes a bottleneck. However, one more critical optimization can be made. \nThere really is no need to create copies of all pages, especially if most of it won t be used. Instead \nof setting up a full copy in these copy processes we can make each of these copy processes instead act \nas a cache for page requests. We refer to these processes as Page Server Caches and we organize them \nin the form of a tree, the Page Server Cache Tree. Page Server Cache Tree (PSCT) The organization of \nthe tree is shown in Figure 9. The page server caches are orga\u00adnized as an m-ary tree. PSPs are grouped \ntogether in bunches of n and are served by 1 page server cache. Each page server cache serves all page \nrequests from processes in 1 PSP Group as well as page requests from all its children page server caches. \nProcess0 acts as the root page server cache (Page Server Cache 0). Figure 9. Page Server Cache Tree \nOperation The operation of an individual page server cache is in fact similar to the way heap memory \nis brought in on PSPs. At the start of a speculative region it mprotects the relevant portions of the \naddress space. When a request for a page comes in from a PSP (or any child) it simply tries to access \na piece of data from the page. If the page has not been mapped into memory a segmentation fault will \noccur, the signal handler will be called and the page will be fetched. The page is not fetched directly \nfrom the root page server cache but from its parent server in the tree. Once the page is fetched it is \nmapped into memory (just like a PSP) and it sends the page back to the requester. Subsequent requests \nto the same page will not cause segmentation faults and hence will be serviced directly.  Note that \nthis operation is performed recursively up the tree. For a freshly set up tree on the very .rst request, \nseg\u00admentation faults will happen all the way up to the root node and as the page is being sent down each \nserver keeps a copy in its address space. This has the nice property of automat\u00adically creating copies \nin multiple nodes so that subsequent requests under those nodes don t go up the tree. This ensures that \nat no point is the root server overwhelmed (see Section 6.6 and Appendix I). In the default con.guration \neach page server cache serves 8 PSPs and the PSCT is organized as a binary tree.  5.2 Stack and Execution \nContext The stack and execution context needs to be transferred to all PSPs at the start of their execution. \nThis is not as much a concern because the stack size is limited and .xed compared to the arbitrary large \nsizes the heap can take on. The transfer of the stack and execution context is performed at the start \nof a speculative region to all PSPs. Our initial approach (Linear approach) of Process0 transferring \nthis information to each PSP in a serial, sequentialized fashion was not scalable to larger cluster sizes. \nWe therefore, switched over to a tree\u00adlike algorithm (Tree approach). The processes are arranged in a \nbinary tree rooted at Process0 with each parent node disseminating copies of the stack and execution \ncontext to its children. This tree based approach alleviates any concerns of excessive overheads for \nthese transfers (see Section 6.6). 6. Evaluation In this section we present a comprehensive evaluation \nof the Multiverse framework. We demonstrate the utility of large scale distributed speculation using \nseveral benchmarks which demonstrate execution time variance. We also analyze the overheads of the Multiverse \nframework and its ability to scale. We integrated the Multiverse framework prototype into several benchmarks: \nWalkSAT, All Interval Series, Costas Arrays, Perfect Square and TSP (Traveling Salesman Prob\u00adlem). The \nmetric for choosing the best PSP for the .rst four benchmarks is execution time, while the metric for \nthe TSP benchmark is quality based and is an acceptably low cost path. Modi.cations to the code were \nminimal (as shown in Figure 3) and required almost no understanding of the algo\u00adrithm itself. Benchmarks \nhave phases such as initialization phase, a speculative phase and veri.cation and output phases. The \nveri.cation and output phases (present in the original code as well) verify in memory data structures \nensuring correct operation and display detailed operational statistics of the algorithm based on many \nstatistics stored in memory over the course of the execution. While one might choose to use a simple \nscript which launches many copies of the benchmark executable and col\u00adlects results from the one which \n.nishes .rst (killing oth\u00aders) instead of using Multiverse, such an approach would not achieve the expected \nresults (for example: due to bench\u00admarks complete loss in ability to perform any non-revocable operations \nand operations with side-effects such as in\u00adput/output) without several code modi.cations, would not \nbe practical for benchmarks such as TSP (which use a quality metric for completion instead of execution \ntime) and would also introduce redundant code execution (non-speculative phases) across the cluster (leading \nto resource wastage at large scales). Multiverse enables large scale distributed spec\u00adulation through \na cleaner, more ef.cient approach and is generally applicable as a framework to other C codes as well. \nMethodology We measure the speedup obtained using Multiverse with a varying number of PSPs. When Multi\u00adverse \nruns it uses a certain numbers of PSPs (each on 1 core) and 1 Process0 (an additional core). Speedup \nis de.ned as the ratio of execution time of the original application with\u00adout using Multiverse to the \nexecution time of the application using Multiverse. The original application without Multi\u00adverse ran \na single instance of the function which exhibits variance (like a regular execution .ow) on a single \ncore and did not link with Multiverse at all. A speedup of 1 indicates that both versions had the same \nexecution time. This is in\u00addicated by the dotted line (Baseline 1) in the graphs which show speedup. \nThe speedup graphs show speedup numbers with varying number of PSPs (+1). For example, if the X\u00adaxis \nvalue shown in the graph is 64, then there are 63 PSPs along with the 1 Process0 each running on individual \ncores. Each PSP in a given benchmark accessed the same area of the heap (as other PSPs that were launched \nin parallel). All benchmarks were run on two different clusters: A 86-core 43-node cluster (Cluster \nA) Intel Dell Pow\u00aderEdge 1850 Linux cluster with dual Pentium4 Xeon EMT64 processors (3.20GHz) using \nIn.niband intercon\u00adnects. The cluster was running Linux version 2.6.18 and running gcc 4.1.2.  A 1176-core \n98-node cluster (Cluster B) With each node containing two 2.6 GHz six-core AMD Opteron processors (Istanbul) \nusing the Cray SeaStar2+ intercon\u00adnect. The cluster was running the Cray Linux Environ\u00adment (CLE) 3.1 \nand was running gcc 4.6.2.  We wanted the PSCT processes to share the same cores that PSPs were running \non (not to use additional cores). We compared performance between using additional cores and multiplexing \nPSCT processes onto PSP cores on Cluster A and observed no difference in performance (PSCT processes \nare small in number and not CPU intensive). However, Clus\u00adter B s Cray ALPS scheduler prevented over-subscription \nof any MPI processes to cores and hence the PSCT processes were running on additional cores on Cluster \nB (for Cluster A, the cores were oversubscribed).  We .rst look at the individual benchmarks and then \ndis\u00adcuss Overheads and Scalability in Section 6.6. All values were averaged over 10 runs (including the \nbaseline case of an application without Multiverse). The speedups plot the arithmetic mean of these runs. \nMinimum, maximum, median and standard deviation as well as runtime overhead numbers are included in Appendix \nII. 6.1 WalkSAT WalkSAT is a randomized SAT solver that exhibits good per\u00adformance. At each step the \nsolver picks a random unsatis.ed clause and .ips a variable in the clause. We used the C based implementation \nof this application from [34]. Figure 10 shows the speedups that were obtained using several SAT inputs \nfrom different application domains: par16-1.cnf 2 and par16-5.cnf 3 instances which arises from the \nproblem of learning the parity function from the DIMACS benchmarks [33].  ii32c4.cnf 4 an instance from \nan inference problem [22].  frb45-21-1.cnf 5 and frb45-21-2.cnf 6 CSP instances of Model RB used in \nAI [42].  Discussion We observe that large scale speculation clearly bene.ts WalkSAT on a variety of \nbenchmark datasets. The benchmark datasets demonstrate good scalability and in fact the par16-5.cnf dataset \n(on Cluster A) and frb45-21-1.cnf (on Cluster B) show super linear speedups! Recall that this is due \nto the underlying distribution of execution times (dis\u00adcussed in Section 1.1). The speedups vary from \ndataset to dataset and are a characteristic of the underlying distribution of execution times (different \ndatasets have different CDF 21015 variables and 3310 clauses 31015 variables and 3358 clauses 4759 variables \nand 20862 clauses 5945 variables and 61855 clauses 6945 variables and 61855 clauses  plots like Figure \n1(b)). We note that all these datasets ex\u00adhibit good scalability going well beyond the number of cores \navailable on a single machine. Pointing to the fact, that the use of Multiverse is crucial in extracting \nmaximal speedup. We were able to secure resources to run just the frb45-21\u00ad2.cnf dataset with a much \nlarger number of cores. Figure 11 reports the results we obtained with approximately 4 times more PSPs \n(these extra resources where obtained from a larger pool of machines in Cluster B itself). We notice \nthat this dataset is able to scale all the way up to 4096 cores. The next three benchmarks use a multi-walk \nmethod where multiple concurrent explorations of the search space are performed starting from different \ninitial con.gurations. An adaptive search framework is used [5]. The framework makes use of a short term \nadaptive memory to avoid getting stuck in loops and local minimas.  6.2 All Interval Series Problem \nThis problem is inspired by a well-known problem occurring in serial musical composition and was .rst \nused in [17]. It can be stated as: given the twelve standard pitch-classes in music (c, c#, d, ...), \nrepresented by numbers 0,1,...,11, .nd a series in which each pitch-class occurs exactly once and in \nwhich the musical intervals between neighboring notes cover the full set of intervals from the minor \nsecond (1 semitone) to the major seventh (11 semitones). We used the C based implementation of this application \nfrom [9]. Figure 12 shows the speedups we were able to obtain using an increasing number of cores for \ntwo different problem sizes. Discussion For both problem sizes (600 and 800) we see similar speedups, \nwhile not super linear as in the case of WalkSAT these examples also show that we can achieve very reasonable \nperformance gains with an increasing number of cores up to 256. For these two problem sizes we observe \nno sizable gain in performance going beyond 256 cores. This simply means we have exhausted the amount \nof parallelism we can gain. Larger problems will still bene.t well beyond this limit, however these larger \nproblems simply take too (a) Cluster A long to run with a smaller number of cores (1 -64) and hence \ndo not lend themselves easily to speedup measurements. This example starkly indicates that improvements \nwill taper off after a certain core count for smaller problems. We refer interested readers to prior \nwork [6] for techniques on how to automatically determine the right degree of speculation for such problems. \n 6.3 Costas Arrays A costas array can be regarded as a set of n points lying on the squares of a n*n \ncheckerboard. Each row or column con\u00adtains only one point and all of the n(n - 1)/2 displacement vectors \nbetween each pair of dots are distinct [13]. These ar\u00adrays are very useful in applications such as sonar \nand radar. We used the C based implementation of this application from [9]. Figure 13 shows the speedups \nwe were able to obtain us\u00ading an increasing number of cores for two different problem sizes. Discussion \nThis benchmark again demonstrates high scal\u00adability and improvements are sizable all way up to a maxi\u00admum \nof 1024 cores on Cluster B. The benchmark with side 21 is actually a very dif.cult instance and takes \nmore than an hour and a half on average to complete and speculative parallelization brings this down \nto a minute and a half on  (a) Cluster A (a) Cluster A (b) Cluster B Cluster A and mere seconds in \nCluster B, both of which are far more acceptable.  6.4 Perfect Square Problem The perfect square placement \nproblem (also called the squared square problem) is to pack a set of squares with different integer sizes \ninto a bigger square in such a way that no squares overlap each other and all square borders are parallel \nto the border of the big square [8]. We used the C based implementation of this application from [9]. \nFig\u00adure 13 shows the speedups we were able to obtain using an increasing number of cores for two different \nproblems 7 8 . Discussion Similar to previous experiments we observe good speedups in the case of both \nproblems in both the clusters. We observe different amounts of speedups between the two datasets, similar \nto the situation that we observed in some of the WalkSAT benchmarks.  6.5 Traveling Salesman Problem \n(TSP) The TSP problem asks the following question: given a list of cities and the distances between each \npair of cities, what is 7 Prob1 Con.g: (48,25,147,(3,4,5,6,8,9,10,12,13,14,15,16,17,19,20,23,25,27,32,33,34,40,41,73,74)) \n8 Prob2 Con.g: (49,25,208,(1,2,3,4,5,7,8,11,12,17,18,24,26,28,29,30,36,39,44,45,50,59,60,89,119)) (b) \nCluster B Figure 14. Results from the Perfect Square Benchmark the shortest possible route that visits \neach city exactly once and returns to the origin city? It is an NP-hard problem in combinatorial optimization. \nIt has many real world applica\u00adtions such as planning, logistics, microchip manufacturing, DNA sequencing \nand in operations research. We used an im\u00adplementation of the Lin-Kernighan heuristic obtained from [16]. \n While the metric for choosing the best PSP in the previ\u00adous benchmarks was execution time, for the TSP \nproblem the Multiverse runtime is looking for a solution which is within some X units (speci.ed below) \nof the optimal (each speculation updates its current quality with the runtime using an API call). Finding \nthe optimal value is prohibitively ex\u00adpensive (NP-hard) so for such applications an inferior solu\u00adtion \nis commonly deemed acceptable. Cluster A was unavail\u00adable to us at the time of running these experiments, \nhence we report results only from Cluster B. Figure 15 shows the speedups we were able to obtain using \nan increasing number of cores for two different datasets:  Nicaragua Containing 3,496 cities in Nicaragua \nde\u00adrived from data from the National Imagery and Mapping Agency database of geographic feature names. \nIt has an optimal tour of 96,132. Multiverse waits until a solution which is 96,182 units or better is \nfound and chooses that as the best speculation.  bgf4475 A 4,475-city instance, derived from VLSI data \nfrom the Forschungsinstitut fr Diskrete Mathematik. The optimal tour length for this instance is unknown. \nMulti\u00adverse waits until a solution which is 13,223 units or better is found and chooses that as the best \nspeculation.  Discussion The bgf4475 dataset demonstrates lower speedup than Nicaragua but improves \nslightly with increasing core counts. The Nicaragua dataset on the other hand demon\u00adstrates improvements \nup to 512 cores and then performance remains almost stationary. As discussed previously, larger problems \nwould demonstrate improvement to larger core counts and these datasets would also bene.t from techniques \nwhich automatically determine the right degree of specula\u00adtion (we refer to prior work [6]).  6.6 Overheads \n&#38; Scalability In this section we discuss overheads and scalability issues. The main overheads are \nduring start/end of speculative re\u00adgions and heap memory access during speculative regions. The overheads \nat the start and end of a speculative region are fairly modest and simply involve typical collective \ncom\u00admunication across a cluster. At the start up of a speculative region the stack, execution context \nand some bookkeeping information needs to be sent to all the PSPs. At the end of the speculative region \nall PSPs are contacted, their execu\u00adtion is terminated and the best speculation sends its execu\u00adtion \ncontext, stack and some bookkeeping information back to Process0. Table 1 shows the breakup of these \noverheads for an application with a 4MB stack which uses 85 PSPs across the 43 machines in Cluster A \nand 1023 PSPs across 86 machines in in Cluster B. Table 1 con.rms that a lin\u00adear approach (of contacting \neach PSP in a serial manner) for startup and shutdown does not scale across a cluster and a tree based \ncommunication pattern decreases any overheads to minimal levels as discussed in Section 5.2. The time \nit takes to transfer the execution context and stack back from the best speculation to Process0 does \nnot change (between tree or linear) since it is based on a simple point to point communication. The tree \nbased approach reduces overheads signi.cantly and Multiverse is able to startup a speculation across \nCluster A in little over half a second and Cluster B in less than 25 ms. While, the trends are similar \nin both clus\u00adters, Cluster B exhibits better performance since it is a HPC cluster with a much faster \ninterconnect. Approach Tree (ms) Linear (ms) Startup time (A) 581.55 31522.84 Startup time (B) 22.31 \n3913.66 Shutdown time (A) 3.54 4.52 Shutdown time (B) 0.5268 0.8837 Best Spec. Transfer Time (A) 357.90 \n357.90 Best Spec. Transfer Time (B) 0.526 0.526 Table 1. Overheads in Cluster A and Cluster B  (b) \nCluster B To observe how Multiverse behaves in a situation where a large amount of heap memory needs \nto be transferred, we designed a special benchmark to test Multiverse s scalability with a larger amount \nof heap memory (128MB) transferred to each PSP and back. This benchmark simply allocates a large number \nof pages in heap memory and code in the speculative region reads and writes to all allocated pages. We \nplot the execution times of two versions of this benchmark. One using the Page Server Cache Tree and \none without. Figure 16 reports total benchmark execution times with an increasing number of PSPs. Each \npage server cache in the Page Server Cache Tree serves 8 PSPs and is organized as a binary tree (default \ncon.guration).  Discussion If the Page Server Cache Tree is not used, per\u00adformance quickly degrades \nwith an increasing number of PSPs. Execution time shows an almost linear increase with number of PSPs. \nThis is expected since Process0 becomes a bottleneck as it handles all requests for pages. By using the \nPage Server Cache Tree on the other hand we observe excellent scalability. The Page Server Cache Tree \nis suited perfectly, and it removes almost all scaling issues with dra\u00admatically reduced execution times \nincreasing only in a loga\u00adrithmic fashion (see Appendix I for an analysis of speedups and how they are \nalmost ideal). 7. Related Work There has been much research in use of speculation at the lower levels \nof execution such as in hardware or through compilers [14, 36, 38, 43]. Programmer controlled speculation \nis another direction of leveraging speculation. Prabhu et al. [30] allow program\u00admers to speculatively \nparallelize loops using domain speci.c functions to predict the value of dependencies. Such value speculation \nis orthogonal to our work. The Grace framework [4] allows programmers to write speculative code but fo\u00adcuses \non using this to eliminate concurrency issues, a differ\u00adent use of speculation. The PetaBricks programming \nmodel [2] allows programmers to specify multiple implementations for an operation. The compiler tunes \nthe selection of the ac\u00adtual implementations that are used at runtime and doesn t directly follow a speculative \napproach. There has been a signi.cant amount of interest in auto\u00adtuning based frameworks [11, 20, 24, \n39] which compare and select good candidates of code and parameters. While auto-tuning approaches are \ngood for converging on a best parameter con.guration they are not suited for exploiting applications \nwhich demonstrate a variance in execution time. Auto-tuning can be used in addition to our work. Programming \nmodels which execute multiple variants of an algorithm or different heuristics and choose amongst them \nhave been proposed [6, 31, 37]. Each of these mod\u00adels differ by the exact type of speculations they support, \ntheir ease of use, isolation guarantees and execution model. However, they are all strictly limited by \nthe parallelism in a single machine. Multiverse makes an explicit case for large scale distributed speculation \nand presents novel techniques and mechanisms to deal with the non-trivial issues that come with this \nfundamental shift to a distributed environment. While Multiverse expands the scope of these frameworks \nto the distributed setting, it does comes with some limitations (see Section 2.1) and in that sense is \nsomewhat less general than approaches such as CPE [37]. There is strong evidence for variance in algorithmic \nex\u00adecution time [5, 25, 26]. Individual applications have used the idea of launching multiple instances \nto speed up the ex\u00adecution, for example ManySAT [40] and even domains as diverse as security [7] and \nreliability [35]. While these ap\u00adplications illustrate the utility of this approach, their mecha\u00adnisms \nare application speci.c and do not allow programmers to speculatively distribute computations within \nan existing application across a cluster. Hosek et al. [18] use a similar idea of launching multiple \nversions of code in parallel in a single machine, for a differ\u00adent purpose, to improve reliability of \nLinux applications by providing safe software updates. Our approach of transferring execution state across \npro\u00adcesses bears similarity to that of PM2 [3], which also in\u00adspired the Charm++ implementation [21]. \nPM2 and Charm++ use this ability to migrate tasks exclusively for purposes such as load balancing and \nfault tolerance. Multiverse on the other hand uses a similar idea to enable speculative parallelization \nof applications, which has different requirements due to it s one-to-many process fan out. It incorporates \nan on-demand component and is designed to deal with scalability issues for large scale speculation. 8. \nConclusion and Future Work In this paper we presented the Multiverse programming model which can be used \nto easily write large scale dis\u00adtributed speculations. We motivated and discussed the need for large \nscale distributed speculation and the challenges it involves both in terms of scalability and ease of \nuse. We in\u00adtroduce special mechanisms to transparently and ef.ciently allow code to continue execution \nin speculative processes even on different machines (allows use of pointers). We deal with potential \nbottlenecks in scalability and introduced the page server cache tree. We implemented our contributions \nin an easy to use C library and demonstrate the Multiverse framework on a number of applications. We \nreport signif\u00adicant performance improvements over several benchmarks. We feel large scale distributed \nspeculation can be used to speedup many applications and we hope that Multiverse s ease of use and special \nscalability techniques enable this. Future Work There are several avenues for future work that we are \ncurrently pursuing. Firstly, we are working on ways to enable nested speculations to allow aggressive \nspec\u00adulation. Secondly, we are studying the effect that placement of page server caches have on performance. \nThirdly, we are investigating the possibility and bene.ts of providing .rst class language constructs \nwhich support speculation.  Acknowledgments The authors would like to thank the anonymous reviewers \nfor their comments. The authors would also like to thank Changhee Jung for his suggestions and comments. \nThe authors would like to thank NICS for allowing us to use Kraken to run our experiments under an XSEDE \ngrant. The authors would also to thank the CERCS center at Georgia Tech for the use of their compute \nresources. The authors also gratefully acknowledge the support of NSF grants CCF-1018544 and CCF-0916962. \nA. APPENDIX I  heap memory between Process0 and the 85 PSPs. That is equivalent to 32,768 pages of \nmemory to each PSP (using a 4KB page size). If the Page Server Cache Tree (PSCT) is not used requests \nfor every page of memory goes to the Page Server on Process0 and it ends up having to reply to 2,785,280 \nrequests (Figure 17(a)). This is a gigantic burden on Process0 and the performance in Figure 16 re.ects \nthis (as expected). However, with the use of the PSCT we can dramatically decrease the number of requests \nthat need to be answered by any single process. Figure 17(b) shows how the number of accesses split up \nin the case of 85 PSPs. Here the PSCT is con.gured as a binary tree and each Page Server Cache (PSC) \nserves 8 PSPs (only 2 of the 11 PSP groups are shown in Figure 17(b)). That means at max each PSC replies \nto requests from 10 processes (8 PSPs and 2 children PSCs). That means at max, each one would serve 32,768 \npages for 10 processes. A maximum of 327,680 pages compared to the maximum of 2,785,280 in the case of \nFigure 17(a) an 88% reduction in number of requests that need to be served. In the experiment (Figure \n16) we observe an 86% reduction in execution time (with and without PSCT) demonstrating that fact that \nthe PSCT is able to provide excellent scalability thereby allowing Multiverse to provide performance \nbene.ts close to the theoretical limit. B. APPENDIX II We provide detailed statistics of the various \nexperiments that were performed on Cluster B (larger cluster). Since the ex\u00adperiments dealt with execution \ntimes that vary signi.cantly, Tables 2 to 14 report various execution time statistics as well as runtime \noverheads from these runs. Tables 2 to 6 are for WalkSAT. Tables 7 and 8 are for All Interval Series. \nTa\u00adbles 9 and 10 are for Costas Array. Tables 11 and 12 are for Costas Array. Tables 13 and 14 are for \nTSP. These values were obtained after 10 independent runs of each benchmark. The values reported are \nthe minimum, maximum, arithmetic mean, median, standard deviation, speculation startup and shutdown overheads \nof these runs. Num PSPs + 1 Max(s) Min(s) Mean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) Serial \n586.49 11.85 282.16 205.7 224.11 0 0 4 288.97 14.73 103.74 89.84 82.64 13.23 2.13 \u00b7 10-3 8 109.3 0.68 \n38.11 32.7 35.22 17.59 6.7 \u00b7 10-3 16 50.63 2.26 23.16 19.99 16.86 20.78 8.1 \u00b7 10-3 32 58.93 1.45 14.39 \n6.03 18.17 19 2.9 \u00b7 10-2 64 21.25 0.75 7.38 4.44 7.15 15.46 5.79 \u00b7 10-2 128 5.99 0.37 2.32 1.73 1.91 \n11.38 1.29 256 5.88 0.12 1.9 1.14 2.04 13.55 0.17 512 3.7 0.11 0.7 0.39 1.07 18.99 0.32 1024 1.76 0.19 \n0.55 0.39 0.47 22.61 21.2 Table 2. WalkSAT: par16-1.cnf References [1] H. Abdel-Sha., E. Speight, and \nJ. K. Bennett. Ef.cient user-level thread migration and checkpointing on windows nt clusters. In Proceedings \nof the 3rd conference on USENIX Windows NT Symposium -Volume 3, WINSYM 99, pages  Num PSPs + 1 Max(s) \nMin(s) Mean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) Num PSPs + 1 Max(s) Min(s) Mean(s) Median(s) \nStd Dev(s) Startup(ms) Shutdown(ms) Serial 9,360.7 1,188.78 4,711.66 5,095.04 2,702.74 0 0 Serial 4,064.52 \n133.78 1,784.6 1,026.66 1,639.84 0 0 4 2,920.67 156.41 1,064.23 921.73 911.01 13.23 2.6 \u00b7 10-3 4 2,355.2 \n135.17 927.56 594.38 797.61 13.13 2.6 \u00b7 10-3 8 2,320.93 59.44 659.74 493.55 631.84 16.31 7.06 \u00b7 10-3 \n8 746.36 109.59 310.19 234.66 203.69 17.79 4.8 \u00b7 10-3 16 1,453.05 6.11 461.07 276.73 489.59 19.82 7.8 \n\u00b7 10-3 16 500.69 68.16 163.58 126.13 128.96 20.75 1.13 \u00b7 10-2 32 824.53 10.25 236.94 144.39 248.7 17.16 \n3.23 \u00b7 10-2 32 368.19 29.02 99.25 76.12 102.13 18.35 3.46 \u00b7 10-2 64 101.23 6.93 53.69 44.34 32.07 15.75 \n7.08 \u00b7 10-2 64 160.71 36.17 73.44 59.06 39.05 15.9 6.67 \u00b7 10-2 128 107.57 4.25 41.93 35.37 30.52 15.19 \n0.13 128 138.38 39.36 73.18 68.55 31.43 11.71 0.12 256 67.81 1.55 18.52 9.98 20.69 15.63 2.17 256 64.08 \n28.91 41.37 40.37 10.75 13.81 1.72 512 15.57 3.09 8.24 7.52 4.16 19.4 21.96 512 48.52 24.06 32.17 31.27 \n6.88 21.93 0.3 1024 17.68 1.18 6.1 3.92 6.03 22.48 8.51 1024 40.59 25.01 31.63 31.75 4.65 22.25 0.62 \n2048 11.42 0.12 2.86 1.63 3.49 30.46 0.97 4096 3.73 0.26 1.49 1.15 1.08 38.71 1.96 Table 7. All Interval \nSeries Problem Size: 600 Table 3. WalkSAT: frb45-21-2.cnf Num PSPs + 1 Max(s) Min(s) Mean(s) Median(s) \nStd Dev(s) Startup(ms) Shutdown(ms) Serial 19,546.58 1,015.91 5,996.73 3,932.02 5,604.4 0 0 Num PSPs \n+ 1 Max(s) Min(s) Mean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) 4 3,901.26 223 1,445.72 969.94 \n1,274.67 13.2 2.11 \u00b7 10-3 Serial 10,713.78 184.9 2,341.08 595.84 3,456.42 0 0 8 3,440.38 480.59 1,383.94 \n893.96 1,036.97 16.34 5 \u00b7 10-3 4 2,166.78 36.87 415.74 148.02 641.56 13 2.4 \u00b7 10-3 16 627.35 171.14 372.29 \n366.78 168.67 20.23 1.22 \u00b7 10-2 8 733.94 42.26 311.52 281.03 252.46 16.32 6.53 \u00b7 10-3 32 786.55 114.64 \n339.36 249 228.52 16.94 4.17 \u00b7 10-2 16 206.17 39.32 126.26 131.11 46.93 20.74 7 \u00b7 10-3 64 439.6 76.59 \n255.18 256.61 124.38 15.73 7.28 \u00b7 10-2 32 204.96 4.46 101.32 101.59 67.19 16.65 3.58 \u00b7 10-2 128 255.34 \n84.84 141.94 124.65 59.98 14.88 0.19 64 126.57 3.21 29.27 16.79 36.66 16.5 6.76 \u00b7 10-3 256 264.23 80.17 \n127.45 113.13 53.03 20.2 0.32 128 55.84 1.06 18.62 12.6 15.77 10.99 0.77 512 217.82 73.41 122.97 107.64 \n48.8 12.32 0.12 256 15.98 2.13 7.18 5.33 4.73 13.92 0.19 1024 110.75 90.66 99.52 97.74 6.77 24.1 1.23 \n512 28.98 0.15 6 3.86 8.52 29.79 0.34 1024 9.54 0.14 2.97 1.13 3.33 22.13 0.59 Table 8. All Interval \nSeries Problem Size: 800 Table 4. WalkSAT: ii32c4.cnf Num PSPs + 1 Max(s) Min(s) Mean(s) Median(s) Std \nDev(s) Startup(ms) Shutdown(ms) Serial 2,250.7 167.91 638.23 403.17 626.81 0 0 Num PSPs + 1 Max(s) Min(s) \nMean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) 4 163.17 20.39 93.14 82.26 51.09 13.07 1.8 \u00b7 10-3 \nSerial 3,154.21 16.16 659.74 272.29 963.54 0 0 8 116.6 0.25 37 28.86 34.2 16.16 4.3 \u00b7 10-3 4 473.76 16.41 \n159.84 142.61 142.97 13.15 2 \u00b7 10-3 16 56.13 0.94 27.81 24.9 20.32 20.06 1.01 \u00b7 10-2 8 217.91 1.15 73.15 \n47.78 70.32 16.14 6 \u00b7 10-3 32 48.5 3.3 16.71 11.43 16.03 18.32 3.36 \u00b7 10-2 16 66.84 2.11 21.35 21.3 18.49 \n19.55 7.8 \u00b7 10-3 64 14.48 0.12 7.3 6.96 5.22 16.78 0.62 32 54.32 2.25 14.78 9.45 16.16 16.82 3.4 \u00b7 10-2 \n128 10.69 0.18 4.45 3.29 3.91 11.42 0.11 64 26.86 0.1 13.33 13.26 9.49 15.45 6.19 \u00b7 10-3 256 3.79 0.2 \n1.61 1.23 1.32 15.84 0.47 128 12.97 0.13 4.74 4.81 3.81 11.08 0.11 512 2.14 0.24 0.94 0.73 0.64 18.83 \n0.31 256 3.3 0.3 1.57 0.83 1.23 14.53 0.38 1024 1.8 0.28 0.79 0.52 0.53 23.74 0.84 512 2.87 0.1 1.32 \n1.18 0.9 19.74 0.28 1024 2.56 0.13 0.76 0.55 0.74 22.39 3.48 Table 9. Costas Array Problem Side: 20 \nTable 5. WalkSAT: par16-5.cnf Num PSPs + 1 Max(s) Min(s) Mean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) \n Serial 10,402.37 376 4,415.5 4,115.93 3,678.28 0 0 Num PSPs + 1 Max(s) Min(s) Mean(s) Median(s) Std \nDev(s) Startup(ms) Shutdown(ms) 4 4,322.53 174.05 1,835.78 1,674.63 1,522.12 12.93 1.9 \u00b7 10-3 8 2,880.72 \n210.91 699.02 426.71 807.62 16.28 4.6 \u00b7 10-3 Serial 10,510.07 447.03 3,860.48 2,077.42 3,955.56 0 0 16 \n414.43 10.58 161.48 116.53 152.77 19.53 1.1 \u00b7 10-2 4 5,032 68.03 1,036.88 734.04 1,460.69 13.11 2.27 \n\u00b7 10-3 32 288.13 14.65 120.77 116.78 85.82 17.74 3.42 \u00b7 10-2 8 1,745.16 120.89 650.08 431.76 619.69 16.42 \n7.33 \u00b7 10-3 64 261.12 11.38 94.9 70.7 81.04 15.66 6.71 \u00b7 10-2 16 475.33 18 206.07 189.86 152.18 19.91 \n8.4 \u00b7 10-3 128 55.49 2.16 26.05 26.66 18.43 13.34 0.12 32 524 10.16 195.15 138.37 162.1 16.93 3.79 \u00b7 \n10-2 256 58.66 0.84 16.62 10.64 19.03 16.16 0.4 64 152.91 28.49 82.68 89.34 40.41 16.26 7.42 \u00b7 10-2 512 \n23.76 1.46 8.67 5.68 7.52 25.8 42.69 128 119.98 1.15 28.78 17.3 35.89 10.91 0.12 1024 17.48 0.22 6.52 \n4.64 5.54 23.19 63.02 256 21.56 0.54 8.12 4.76 7.13 14.74 11.24 512 23.21 0.28 7.24 3.86 7.57 27.02 0.3 \n1024 16.38 0.9 3.48 2.33 4.63 24.71 0.58  Table 10. Costas Array Problem Side: 21 Table 6. WalkSAT: \nfrb45-21-1.cnf //dl.acm.org/citation.cfm?id=645611.662349. 1 1, Berkeley, CA, USA, 1999. USENIX Association. \n[4] E. D. Berger, T. Yang, T. Liu, and G. Novark. Grace: safe URL http://dl.acm.org/citation.cfm?id=1268427. \nmultithreaded programming for c/c++. In Proceedings of 1268428. the 24th ACM SIGPLAN conference on Object \noriented programming systems languages and applications, OOPSLA [2] J. Ansel, C. Chan, Y. L. Wong, M. \nOlszewski, Q. Zhao, 09, pages 81 96, New York, NY, USA, 2009. ACM. ISBN A. Edelman, and S. Amarasinghe. \nPetabricks: a language 978-1-60558-766-0. . URL http://doi.acm.org/10. and compiler for algorithmic choice. \nIn Proceedings of the 1145/1640089.1640096. 2009 ACM SIGPLAN conference on Programming language design \nand implementation, PLDI 09, pages 38 49, New [5] Y. Caniou, D. Diaz, F. Richoux, P. Codognet, and S. \nAbreu. York, NY, USA, 2009. ACM. ISBN 978-1-60558-392-1. . Performance analysis of parallel constraint-based \nlocal search. URL http://doi.acm.org/10.1145/1542476.1542481. In Proceedings of the 17th ACM SIGPLAN \nsymposium on Principles and Practice of Parallel Programming, PPoPP 12, [3] G. Antoniu, L. Boug\u00b4An ef.cient \nand e, and R. Namyst. pages 337 338, New York, NY, USA, 2012. ACM. ISBN transparent thread migration \nscheme in the pm2 runtime 978-1-4503-1160-1. . URL http://doi.acm.org/10. system. In Proceedings of the \n11 IPPS/SPDP 99 Workshops 1145/2145816.2145883. Held in Conjunction with the 13th International Parallel \nProcessing Symposium and 10th Symposium on Parallel and [6] R. E. Cledat, T. Kumar, and S. Pande. Ef.ciently \nspeeding Distributed Processing, pages 496 510, London, UK, UK, up sequential computation through the \nn-way programming 1999. Springer-Verlag. ISBN 3-540-65831-9. URL http: model. In Proceedings of the 2011 \nACM international con\u00ad Num PSPs + 1 Max(s) Min(s) Mean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) \nSerial 3,105.47 28.62 1,500.68 1,497.66 1,004.87 0 0 4 1,904.34 41.16 720.45 645.01 562.59 13.15 1.9 \n\u00b7 10-3 8 401.16 61.03 188.14 149.53 123.49 16.36 6.3 \u00b7 10-3 16 407.88 6.26 121.62 47.78 142.47 19.35 \n1.32 \u00b7 10-2 32 146.42 3.83 56.37 37.81 55.99 18.13 3.51 \u00b7 10-2 64 63.66 0.58 21.32 9.3 23.06 15.89 6.49 \n\u00b7 10-2 128 58.93 0.2 16.41 9.04 18.65 11.13 0.11 256 13.92 0.2 5.8 3.61 4.87 15.8 0.22 512 3.89 0.13 \n2.34 2.48 1.43 18.17 0.3 1024 7.44 0.11 1.78 0.82 2.48 24.73 0.69 Table 11. Perfect Square Problem: Prob1 \nNum PSPs + 1 Max(s) Min(s) Mean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) Serial 12,900.39 221.34 \n2,877.75 1,556.31 3,894.76 0 0 4 1,493.38 43.69 740.27 760.6 525.03 13.04 2.5 \u00b7 10-3 8 1,018.43 65.04 \n406.6 347.62 345.2 16.06 6.3 \u00b7 10-3 16 902.31 16.29 323.19 275.27 243.63 19.88 1.37 \u00b7 10-2 32 343.89 \n12.25 202.39 248.7 136.36 18.96 3.7 \u00b7 10-2 64 155.12 2.47 48.04 30 53.11 15.48 0.64 128 81.93 1.48 30.14 \n21.58 26.5 11.66 0.12 256 48.61 1.94 18.24 18.7 15.39 15.86 0.18 512 27.3 0.12 9.9 9.19 8.94 19.51 0.31 \n1024 9.64 0.22 5 6.2 3.49 24.58 0.56 Table 12. Perfect Square Problem: Prob2 Num PSPs + 1 Max(s) Min(s) \nMean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) Serial 7,212.45 183.32 1,660.98 1,161.46 2,072.86 \n0 0 4 1,814.47 45.24 540.51 431.43 485.92 12.94 2.3 \u00b7 10-3 8 516.36 32.95 222.99 155.22 187.68 16.57 \n5.8 \u00b7 10-3 16 240.09 37.58 108.08 70.5 79.7 19.7 9 \u00b7 10-3 32 140.45 38.84 85.37 75.85 37.45 17.32 3.82 \n\u00b7 10-2 64 125.39 42.32 67.93 54.98 28.87 15.55 6.63 \u00b7 10-2 128 83.4 42.21 61.11 66.16 15.71 11.14 0.12 \n256 69.44 42.5 50.13 48.47 8.02 16.19 4.54 512 52.51 29.16 44.44 46.33 6.93 19 0.32 1024 50.51 38.67 \n43.92 43.88 3.35 24.77 1.33 Table 13. Travelling Salesman Problem: Nicaragua Num PSPs + 1 Max(s) Min(s) \nMean(s) Median(s) Std Dev(s) Startup(ms) Shutdown(ms) Serial 5,105.05 351.5 2,014.42 2,029.18 1,300.32 \n0 0 4 1,372.73 164.13 560.71 478.39 376.28 13.16 2.1 \u00b7 10-3 8 1,127.06 84.09 257.12 147.04 317.15 16.23 \n4.9 \u00b7 10-3 16 300.26 84.51 155.98 134.53 65.45 19.44 7.9 \u00b7 10-3 32 144.5 86.33 105.49 96.26 19.38 17.09 \n3.73 \u00b7 10-2 64 124.04 84.44 101.44 99.86 11.54 15.61 6.92 \u00b7 10-2 128 118 83.84 98.74 99.35 9.6 11.23 \n0.12 256 98.26 68.47 86.3 90.54 10.7 14.11 0.19 512 86.06 69.75 78.13 78.14 4.69 19.29 0.3 1024 64.75 \n60.1 61.41 61.07 1.28 23.76 0.76 Table 14. Travelling Salesman Problem: bgf4475 ference on Object oriented \nprogramming systems languages and applications, OOPSLA 11, pages 537 554, New York, NY, USA, 2011. ACM. \nISBN 978-1-4503-0940-0. . URL http://doi.acm.org/10.1145/2048066.2048109. [7] B. Cox, D. Evans, A. Filipi, \nJ. Rowanhill, W. Hu, J. Davidson, J. Knight, A. Nguyen-Tuong, and J. Hiser. N-variant systems: a secretless \nframework for security through diversity. In Proceedings of the 15th conference on USENIX Security Symposium \n-Volume 15, USENIX-SS 06, Berkeley, CA, USA, 2006. USENIX Association. URL http://dl.acm. org/citation.cfm?id=1267336.1267344. \n[8] H. Croft, K. Falconer, and R. Guy. Unsolved problems in geometry. Problem books in mathematics. Springer-Verlag, \n1991. ISBN 9780387975061. URL http://books. google.com/books?id=xF4PAQAAMAAJ. [9] D. Diaz, P. Codognet, \nand S. Abreu. Adaptive search distribution, http://cri-dist.univ-paris1.fr/diaz/adaptive/. [10] W. R. \nDieter and J. E. Lumpp, Jr. User-level checkpoint\u00ading for linuxthreads programs. In Proceedings of the \nFREENIX Track: 2001 USENIX Annual Technical Confer\u00adence, pages 81 92, Berkeley, CA, USA, 2001. USENIX \nAs\u00adsociation. ISBN 1-880446-10-3. URL http://dl.acm. org/citation.cfm?id=647054.715766. [11] G. Fursin, \nA. Cohen, M. O Boyle, and O. Temam. A practical method for quickly evaluating program optimiza\u00adtions. \nIn Proceedings of the First international conference on High Performance Embedded Architectures and Com\u00adpilers, \nHiPEAC 05, pages 29 46, Berlin, Heidelberg, 2005. Springer-Verlag. ISBN 3-540-30317-0, 978-3-540-30317-6. \n. URL http://dx.doi.org/10.1007/11587514_4. [12] R. P. Garg and I. Sharapov. Techniques for Optimizing \nApplications: High Performance Computing. Prentice Hall Professional Technical Reference, 2002. ISBN \n0130091189. [13] S. Golomb and H. Taylor. Constructions and properties of costas arrays. Proceedings \nof the IEEE, 72(9):1143 1163, 1984. ISSN 0018-9219. . [14] L. Hammond, M. Willey, and K. Olukotun. Data \nspeculation support for a chip multiprocessor. In Proceedings of the eighth international conference \non Architectural support for programming languages and operating systems, ASPLOS VIII, pages 58 69, New \nYork, NY, USA, 1998. ACM. ISBN 1-58113-107-0. . URL http://doi.acm.org/10.1145/ 291069.291020. [15] T. \nHarris, J. Larus, and R. Rajwar. Transactional Memory, 2nd Edition. Morgan and Claypool Publishers, 2nd \nedition, 2010. ISBN 1608452352, 9781608452354. [16] K. Helsgaun. An effective implementation of the link\u00adernighan \ntraveling salesman heuristic. European Journal of Operational Research, 126(1):106 130, 2000. ISSN 0377-2217. \n. URL http://www.sciencedirect.com/ science/article/pii/S0377221799002842. [17] H. Hoos. Stochastic Local \nSearch -Methods, Models, Applications. IOS Press. ISBN 9783898382151. URL http://books.google.com/books?id=xc\\_4OtzmHUUC. \n[18] P. Hosek and C. Cadar. Safe software updates via multi\u00adversion execution. In Proceedings of the \n2013 International Conference on Software Engineering, ICSE 13, pages 612 621, Piscataway, NJ, USA, 2013. \nIEEE Press. ISBN 978-1\u00ad4673-3076-3. URL http://dl.acm.org/citation.cfm? id=2486788.2486869. [19] T. IEEE \nand T. O. Group. The open group base speci.cations, issue 6, ieee std 1003.1, 2004. [20] C. Jung, D. \nLim, J. Lee, and S. Han. Adaptive execution tech\u00adniques for smt multiprocessor architectures. In Proceedings \nof the tenth ACM SIGPLAN symposium on Principles and prac\u00adtice of parallel programming, PPoPP 05, pages \n236 246, New York, NY, USA, 2005. ACM. ISBN 1-59593-080-9. . URL http://doi.acm.org/10.1145/1065944.1065976. \n[21] L. V. Kale and S. Krishnan. Charm++: a portable concurrent object oriented system based on c++. \nIn Proceedings of the eighth annual conference on Object-oriented programming systems, languages, and \napplications, OOPSLA 93, pages 91 108, New York, NY, USA, 1993. ACM. ISBN 0-89791\u00ad  587-9. . URL http://doi.acm.org/10.1145/165854. \n165874. [22] A. P. Kamath, N. K. Karmarkar, K. G. Ramakrishnan, and M. G. C. Resende. A continuous approach \nto inductive inference. Math. Program., 57(2):215 238, Nov. 1992. ISSN 0025-5610. . URL http://dx.doi.org/10.1007/ \nBF01581082. [23] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic \nparallelism requires abstractions. In Proceedings of the 2007 ACM SIGPLAN conference on Programming language \ndesign and imple\u00admentation, PLDI 07, pages 211 222, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-633-2. \n. URL http://doi.acm.org/10.1145/1250734.1250759. [24] J. Lau, M. Arnold, M. Hind, and B. Calder. Online \nper\u00adformance auditing: using hot optimizations without get\u00adting burned. In Proceedings of the 2006 ACM \nSIG-PLAN conference on Programming language design and implementation, PLDI 06, pages 239 251, New York, \nNY, USA, 2006. ACM. ISBN 1-59593-320-4. . URL http://doi.acm.org/10.1145/1133981.1134010. [25] M. Luby \nand W. Ertel. Optimal parallelization of las vegas algorithms. In Proceedings of the 11th Annual Symposium \non Theoretical Aspects of Computer Science, STACS 94, pages 463 474, London, UK, UK, 1994. Springer-Verlag. \nISBN 3\u00ad540-57785-8. URL http://dl.acm.org/citation.cfm? id=646510.695012. [26] M. Mitzenmacher and E. \nUpfal. Probability and Computing: Randomized Algorithms and Probabilistic Analysis. Cam\u00adbridge University \nPress, 2005. ISBN 9780521835404. URL http://books.google.com/books?id=0bAYl6d7hvkC. [27] S. Negara, G. \nZheng, K.-C. Pan, N. Negara, R. E. Johnson, L. V. Kal\u00b4e, and P. M. Ricker. Automatic mpi to ampi program \ntransformation using photran. In Proceedings of the 2010 conference on Parallel processing, Euro-Par \n2010, pages 531 539, Berlin, Heidelberg, 2011. Springer-Verlag. ISBN 978-3\u00ad642-21877-4. URL http://dl.acm.org/citation.cfm? \nid=2031978.2032050. [28] AMD64 Architecture Programmers Manual, Volume 2, 2010. [29] P. S. Pacheco. Parallel \nprogramming with MPI. 1996. ISBN 1-55860-339-5. [30] P. Prabhu, G. Ramalingam, and K. Vaswani. Safe pro\u00adgrammable \nspeculative parallelism. In Proceedings of the 2010 ACM SIGPLAN conference on Programming language design \nand implementation, PLDI 10, pages 50 61, New York, NY, USA, 2010. ACM. ISBN 978-1-4503-0019-3. . URL \nhttp://doi.acm.org/10.1145/1806596.1806603. [31] H. K. Pyla, C. Ribbens, and S. Varadarajan. Exploiting \ncoarse-grain speculative parallelism. In Proceedings of the 2011 ACM international conference on Object \noriented programming systems languages and applications, OOPSLA 11, pages 555 574, New York, NY, USA, \n2011. ACM. ISBN 978-1-4503-0940-0. . URL http://doi.acm.org/10. 1145/2048066.2048110. [32] M. Rieker, \nJ. Ansel, and G. Cooperman. Transparent user\u00adlevel checkpointing for the native posix thread library \nfor linux. In The 2006 International Conference on Parallel and Distributed Processing Techniques and \nApplications, Jun 2006. [33] Rutgers. Dimacs benchmarks, http://dimacs.rutgers.edu/challenges/. [34] \nB. Selman, H. A. Kautz, and B. Cohen. Noise strategies for improving local search. In Proceedings of \nthe twelfth national conference on Arti.cial intelligence (vol. 1), AAAI 94, pages 337 343, Menlo Park, \nCA, USA, 1994. American Association for Arti.cial Intelligence. ISBN 0-262-61102-3. URL http: //dl.acm.org/citation.cfm?id=199288.178090. \n[35] A. Shye, J. Blomstedt, T. Moseley, V. J. Reddi, and D. A. Connors. Plr: A software approach to transient \nfault tolerance for multicore architectures. IEEE Trans. Dependable Secur. Comput., 6(2):135 148, Apr. \n2009. ISSN 1545-5971. . URL http://dx.doi.org/10.1109/TDSC.2008.62. [36] J. Steffan and T. Mowry. The \npotential for using thread-level data speculation to facilitate automatic parallelization. In Proceedings \nof the 4th International Symposium on High-Performance Computer Architecture, HPCA 98, pages 2 , Washington, \nDC, USA, 1998. IEEE Computer Society. ISBN 0-8186-8323-6. URL http://dl.acm.org/citation. cfm?id=822079.822712. \n[37] O. Trachsel and T. R. Gross. Variant-based competitive parallel execution of sequential programs. \nIn Proceedings of the 7th ACM international conference on Computing frontiers, CF 10, pages 197 206, \nNew York, NY, USA, 2010. ACM. ISBN 978-1-4503-0044-5. . URL http: //doi.acm.org/10.1145/1787275.1787325. \n[38] S. P. Vanderwiel and D. J. Lilja. Data prefetch mechanisms. ACM Comput. Surv., 32(2):174 199, June \n2000. ISSN 0360\u00ad0300. . URL http://doi.acm.org/10.1145/358923. 358939. [39] M. J. Voss and R. Eigenmann. \nHigh-level adaptive program optimization with adapt. In ACM SIGPLAN Notices, pages 93 102. ACM Press, \n2001. [40] C. M. Wintersteiger, Y. Hamadi, and L. Moura. A concurrent portfolio approach to smt solving. \nIn Proceedings of the 21st International Conference on Computer Aided Veri.cation, CAV 09, pages 715 \n720, Berlin, Heidelberg, 2009. Springer-Verlag. ISBN 978-3-642-02657-7. . URL http://dx.doi. org/10.1007/978-3-642-02658-4_60. \n[41] W. Wulf and M. Shaw. Global variable considered harmful. SIGPLAN Not., 8(2):28 34, Feb. 1973. ISSN \n0362-1340. . URL http://doi.acm.org/10.1145/953353.953355. [42] K. Xu and W. Li. Exact phase transitions \nin random constraint satisfaction problems. Journal of Arti.cial Intelligence Research, 12:93 103, 2000. \n[43] T.-Y. Yeh and Y. N. Patt. A comparison of dynamic branch predictors that use two levels of branch \nhistory. In Proceedings of the 20th annual international symposium on computer architecture, ISCA 93, \npages 257 266, New York, NY, USA, 1993. ACM. ISBN 0-8186-3810-9. . URL http://doi.acm.org/10.1145/165123.165161. \n    \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Algorithmic speculation or high-level speculation is a promising programming paradigm which allows programmers to speculatively branch an execution into multiple independent parallel sections and then choose the best (perhaps fastest) amongst them. The continuing execution after the speculatively branched section sees only the modifications made by the best one. This programming paradigm allows programmers to harness parallelism and can provide dramatic performance improvements.</p> <p>In this paper we present the Multiverse speculative programming model. Multiverse allows programmers to exploit parallelism through high-level speculation. It can effectively harness large amounts of parallelism by speculating across an entire cluster and is not bound by the parallelism available in a single machine. We present abstractions and a runtime which allow programmers to introduce large scale high-level speculative parallelism into applications with minimal effort. We introduce a novel on-demand address space sharing mechanism which provide speculations efficient transparent access to the original address space of the application (including the use of pointers) across machine boundaries. Multiverse provides single commit semantics across speculations while guaranteeing isolation between them. We also introduce novel mechanisms to deal with scalability bottlenecks when there are a large number of speculations.</p> <p>We demonstrate that for several benchmarks, Multiverse achieves impressive speedups and good scalability across entire clusters. We study the overheads of the runtime and demonstrate how our special scalability mechanisms are crucial in scaling cluster wide.</p>", "authors": [{"name": "Kaushik Ravichandran", "author_profile_id": "81470650291", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P4290409", "email_address": "kaushikr@gatech.edu", "orcid_id": ""}, {"name": "Santosh Pande", "author_profile_id": "81409594751", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P4290410", "email_address": "santosh@cc.gatech.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509525", "year": "2013", "article_id": "2509525", "conference": "OOPSLA", "title": "Multiverse: efficiently supporting distributed high-level speculation", "url": "http://dl.acm.org/citation.cfm?id=2509525"}