{"article_publication_date": "10-29-2013", "fulltext": "\n On-The-Fly Capacity Planning Nick Mitchell Peter F. Sweeney IBM T.J. Watson Research Center {nickm, \npfs}@us.ibm.com Abstract When resolving performance problems, a simple histogram of hot call stacks \ndoes not cut it, especially given the highly .uid nature of modern deployments. Why bother tuning, when \nadding a few CPUs via the management console will quickly resolve the problem? The .ndings of these tools \nare also presented without any sense of context: e.g. string con\u00adversion may be expensive, but only matters \nif it contributes greatly to the response time of user logins. Historically, these concerns have been \nthe purview of capacity planning. The power of planners lies in their ability to weigh demand versus \ncapacity, and to do so in terms of the important units of work in the application (such as user logins). \nUnfortunately, they rely on measurements of rates and latencies, and both quantities are dif.cult to \nobtain. Even if possible, when all is said and done, these planners only relate to the code as a black-box: \nbut, why bother adding CPUs, when easy code changes will .x the problem? We present a way to do planning \non-the-.y: with a few call stack samples taken from an already-running system, we predict the bene.t \nof a proposed tuning plan. We accom\u00adplish this by simulating the effect of a tuning action upon execution \nspeed and the way it shifts resource demand. To identify existing problems, we show how to generate tun\u00ading \nactions automatically, guided by the desire to maximize speedup without needless expense, and that these \ngenerated plans may span resource and code changes. We show that it is possible to infer everything needed \nfrom these samples alone: levels of resource demand and the units of work in the application. We evaluate \nour planner on a suite of mi\u00adcrobenchmarks and a suite of 15,000 data sets that come from real applications \nrunning in the wild. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, \nIndianapolis, Indiana, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/10.1145/2509136.2509540 \nCategories and Subject Descriptors C.4 [Computer Sys\u00adtems Organization]: Reliability, availability, serviceability \nKeywords Capacity Planning, Expert System, Inference, Performance Tuning, 1. Introduction With a bit \nof planning, the arduous task of optimization can lead to large improvements in maintenance costs and \nperfor\u00admance. We have seen many situations where a few straight\u00adforward changes were enough to tip an \napplication, from the slow death of severe resource contention, to the smooth .ow of work through the \nsystem. Adding processors to a database machine, splitting the execution of a single-process program \ninto several processes, or updating code to use a concurrent data structure these changes can yield \nbig reductions in the resources necessary to support anticipated load at desired costs. This paper presents \na system that guides the selection and parametrization of tuning actions. Teams need guidance because, \nwith every turn of the tun\u00ading crank, they are again faced with reasoning through how their changes altered \nthe landscape of resource constraints. Did this latest change remove the lock contention bottle\u00adneck? \nIf so, then why is performance still bad? Did all of our tuning efforts simply expose another, heretofore \nlatent, bottleneck somewhere else? Latent Bottlenecks, Zero-sum Games, and Head Fakes System tuning is \noften cursed with a richness of possibilities. The best course of action is often not obvious, because \nthe most prevalent activity may not be what you need to .x. For example, if demand for a critical section \nis high, hun\u00addreds of threads may sit idle, waiting for access to the guard\u00ading monitor. If the machine \ns CPU is already saturated, then eliminating the lock, at least as an isolated tuning action, may be \na futile endeavor. Doing so will only shift resource demand, away from the lock and to an already-saturated \nCPU. The saturated CPU resource is a latent bottleneck, at least with respect to the prima facie problem, \nthat of hun\u00addreds of threads backed up on a lock. This scenario is also an example of a zero-sum game: \nthose threads already consuming CPU will execute more slowly, due to increased contention for processors, \nand those that were previously waiting on the lock can now complete more quickly in equal measure. \n Even in the absence of these confounding problems, other kinds of head fakes can occur. For example, \nconsider a case with two program tasks A and B competing for a saturated pool of processors. If you are \ndissatis.ed with the performance of A and happy with that of B, it may, counterintuitively, be easier \nto address the seemingly less important task, B, .rst. The right choice depends on which has greater \nleverage. If B is 10 times as concurrent as A, then, for every doubling of the speed of A, you need only \n.nd a way to make B 10% faster. The Role of Capacity Planning in Performance Tuning This relationship \nbetween concurrent demand and resource capacity is at the heart of the .eld of capacity planning. When \ndone well, capacity planning tools can predict changes to response time and throughput, based on various \nwhat-if scenarios, with uncanny precision [6, 7, 10, 14, 16, 21]. For example, a capacity planner might \npredict that adding two processors to the database machine will result in a 1.3 second decrease in the \nresponse time of the user login scenario. This is valuable information, but comes at a cost of ex\u00adtensive, \nand largely manual, data collection. In a typical ca\u00adpacity planning exercise, one must provide a list \nof available resources and the way they are interconnected, specify the queuing semantics of each resource, \nde.ne the set of pro\u00adgram tasks (such as the Servlets), and measure load charac\u00adteristics such as the \narrival rate of work. After this manual data collection, a human often must then babysit the system through \na period of curve .tting. These predictive formulae are what enables them to make such precise predictions. \nIn addition to the user burden, these tools operate in a black box fashion with respect to the code. \nDespite all of this input data, the best capacity planning tools can say about the code context is the \nname of the program tasks and this, only because the users themselves provided it to the tool. Our Contribution \nFigure 1 summarizes the main com\u00ad ponents of our solution. We .rst infer properties of re\u00adsource consumption \nand resource availability from readily and cheaply available data, collected as the application runs. \nWe then simulate the effect of tuning actions, as they shift demand between resources. We model the effect \nof tuning actions upon demand via an abstract simulation that models resources as typed nodes, and concurrent \ndemand for these resources as typed tokens (nicknamed marbles). The type of a resource depends on how \nit responds to saturation; e.g. a thread pool or Java monitor gates access to a limited number of concurrent \ncon\u00adsumers, whereas a CPU is multiplexed amongst concurrent demand. The type of a consumer that places \ndemand on a re\u00adsource (a marble in the model) is given by the program task it is servicing; e.g. user \nlogins and checkout actions would be assigned two distinct marble types. Figure 1. The Marbles Capacity \nPlanner can operate in two modes: evaluating what-if scenarios, or, via the Plan Generator, providing \nremedies to performance problems. We model tuning actions as the movement of marbles between nodes. For \nexample, speeding up the critical section of a lock has the effect of shifting a share of demand, away \nfrom the lock, and to those resources used in the critical section. We show how to model the bene.t of \na tuning action by observing the initial and .nal placement of marbles. This kind of simulation-driven \nmodeling is similar in spirit to the discrete event simulations employed by some conventional capacity \nplanners [11, 12], except that our events are tuning actions, rather than the arrival of work. These \ntuning actions can either be provided by a human, as part of the evaluation of what-if scenarios, or \nthe actions can be provided by the system itself, via the Plan Generator. The plan generator seeks to \nexplain, and provide remedy for, performance problems in the system as it exists now a resource or an \naspect of the code is only part of the problem if it is part of a highly ranked solution. We will show \nhow this automated exploration is guided by the goal of eliminating extant, latent, and looming bottlenecks. \nWe show that this dual duty, of performance analysis and extrapalatory planning, can be done on the .y \ni.e. using only easily obtained information from an already\u00adrunning system. The only information needed \nis moment-in\u00adtime snapshots of thread states, and information about the capacity of resources. This data \nlacks any kind of temporal information, such as the rate of arrival of work and the la\u00adtency to complete \neach piece of work. We show that, rather than being compromised by a paucity of information, on\u00adthe-.y \nplanning provides more explanatory detail than either conventional capacity planning systems or hotspot \nidenti.\u00adcation tools, without harm to the accuracy of the predictions.  2. Planning: Background and \nTerminology Capacity planning is typically seen as the task of identifying the amount of a resource that \nis necessary to provide quick service to current or anticipated levels of load. For this pa\u00adper, we consider \na plan to be a set of tuning actions, each action applied either to the hardware resources of the system \n(such as processors), to the software resources of the system (such as thread pools), or to the executing \ncode. On this last point, we also consider an opposite goal of planning: that of reducing load (via code \ntuning) so that it runs within a given set of resource constraints. A good plan is one that matches the \nload on CPU, thread pools, monitors, etc. with the capacity of those resources to execute such load within \ngiven time constraints. Under\u00adshooting this crossover point, of supply and demand, usu\u00adally results in \nunsatis.ed time bounds, and overshooting this goldilocks point usually results in unnecessary expenses. \n2.1 System Load Becomes Resource Demand As load arrives for processing it will begin to place demand \nupon the resources of the system. For example, if single\u00adthreaded CPU-intensive tasks arrive at a rate \nof R per sec\u00adond, and the CPU resource requires L seconds to complete the execution this task, a concurrent \ndemand on CPU of RL will be observed. These two values are termed the arrival rate of load (R per second) \nand the service time of a resource (L seconds of CPU time). If there are brief spikes that dou\u00adble the \narrival rate of new tasks, matching brief spikes in concurrent demand can be witnessed. Realizing resource \ndemand in units of concurrency is im\u00adportant, because most resources can tolerate a certain level of \nconcurrent demand without a degradation in service time. This limit of tolerance for demand is the concurrent \ncapac\u00adity of a resource. Continuing our example: if the system has RL CPUs, then the concurrent demand \nis well-matched to the concurrent capacity: there will neither be a backlog of requests, nor will there \nbe an excess of CPU capacity. For those brief spikes of arrival rate, service time will degrade, as multiple \ntasks are multiplexed to the CPUs. However, as long as the long-term average demand does not exceed the \nproduct RL, the system can still process the requests in a healthy manner. This matching of capacity \nand demand is an expression of the well-known Little s Law of queueing theory [15], which we will return \nto in a later section. The seminal treatment of queueing network models [14] denotes tasks as customers, \nand the nature of the work as the customer class . In this paper, we use the terminology Work Unit to \ncapture this property, because they represent the unit of accomplishment, and thus the metric of performance. \nFor example, an application might have two Work Units, one for the user logins and one for the shopping \ncart checkout trans\u00adaction. The health of the system hinges upon the successful and timely completion \nof these units of work.  2.2 Making Predictions Knowing Only Demand This paper introduces a predictive \nmodel that does not re\u00adquire any temporal information. Our model can predict the bene.t of a tuning plan \nknowing neither the arrival rate of load nor the service time of the resources. We will show how to do \nthis by observing only levels of concurrent de\u00admand and concurrent capacity, and by modeling the effect \nof tuning plans as a shift of demand between resources. 3. Call Stack and Utilization Samples For our \npurposes, a snapshot of the running state of an ap\u00adplication consists of a summary of the execution state \nof its live threads, along with information about overall levels of resource consumption. The information \nwe collect origi\u00adnates from data feeds provided either by the operating sys\u00adtem or the application s \nruntime environment. This informa\u00adtion is readily available, and requires no a priori metrics for throughput, \nsuch as transaction rate, nor a built-in mecha\u00adnism for extracting such metrics. Most commercial Java \nVirtual Machines (JVMs) provide a facility for acquiring a snapshot of the execution state of the live \nthreads in a running application. Each snapshot contains the full call stack of each thread, whether \nawake or idling, in a running application. IBM JVMs call these snapshots javacores . These thread snapshots \noften also include information about monitors in the running application, in the form of the lock graph. \nThe lock graph is a bipartite graph between monitors and threads. This graph indicates, for each critical \nsection, the current owning thread and the threads that are queued up, waiting to enter that section \nof code. The second aspect of information we sample are the oper\u00adating system resource utilization feeds, \nsuch as vmstat and iostat on UNIX platforms. 3.1 Our Data Sets For the purposes of evaluating this work, \nwe have a rich set of input data from which to draw upon. The data is pro\u00advided by an existing performance \nanalysis service in wide use. This service provides us with a catalog of over .fteen thousand inputs \ncollected during the execution of a variety of Java batch and server workloads. The data was collected, \nby users of the service, in a wide variety of settings. For example, users include a plurality of developers, \ntest orga\u00adnizations, service teams responsible for debugging problems in deployed applications, and operations \nstaff triaging issues with the systems they maintain.   Figure 2. For testing and validation purposes, \nwe have 15,000 input data sets. This .gure shows a histogram of the number of call stack snapshots, across \nthese inputs. A risk in using data submitted from the wild is that the input call stack samples may be \ntoo few in number to ac\u00adcurately represent the true nature of resource consumption. If an application \nspends on average of 10% of its time con\u00adsuming a resource, then one would need a minimum of 1431 samples \nto have a 95% con.dence of re.ecting exactly that average. Usually, such accuracy is not necessary. If \nwe can tolerate a 5% spread in either direction, i.e. if we can tolerate our samples reporting anywhere \nfrom 5% 15% instead of the actual 10%, then we need only 21 samples to have 95% con.dence in that spread. \nIf the application spends 20% of its time in a resource, then to have 95% con.dence of ob\u00adserving a such \na spread, we would need only 11 samples.1 Figure 2 shows a histogram of the number of call stack sam\u00ad \nples. There are 2598 inputs with at least 10 call stack sam\u00adples, and 3898 inputs with at least 20 call \nstack samples. 4. Tuning Actions and Tuning Plans A tuning action is a change to the code or the environment \nin which the application runs. For this paper, we consider the goal of a tuning action to be reducing \nthe time required to complete tasks. This is often referred to as service time. Our Suite of Tuning Actions \nA tuning action manipulates the system by either: 1. Directly Reducing Service Time, such as via code \noptimizations or SQL query tunings that tar\u00adget speci.c operations or queries. Section 9 shows how to \nguide the speci.city of these actions. 2. Adding Capacity, such as adding CPUs. Sec\u00ad tion 7 shows that \ncapacity-adding actions are ben\u00ad e.cial to the extent that they reduce service time. 3. Adding Load, \nsuch as increasing the number of load-driving machines in a testing environment.  A tuning action is \nalso parametrized by a multiplica\u00adtive tuning factor; e.g. a 2x increase in CPUs. 1 These distributions \nderive from the Poisson probability function. Figure 3. A thread that places a unit of concurrent demand \nagainst a resource is modeled as a colored marble mapped to this resource; a color represents the task \nbeing serviced. While not addressed in this paper, changes which move in the opposite direction are also \nimportant. We believe that supporting slowdown actions, useful when trying to mini\u00admize cost-to-bene.t \nratios, should be straightforward exten\u00adsions to the content of this paper. A tuning action makes one \nalteration. A tuning plan is an unordered set of tuning actions, with the implication that their impact \non performance be considered as a unit. Section 6 and Section 7 show how to predict the implications \nof one tuning action. Section 9 extends this to tuning plans. 5. The Marbles Model The speed of an application \ncan be quanti.ed either in terms of completion throughput or in terms of the time to complete each invocation \nof a program task. The latter, often called response time in the case of server workloads, is a quantity \nthat must be attached to a speci.c program task.2 These program tasks might include the servlets in \na Java web application, or the phases in a batch application. Re\u00adsponse time is generally expressed in \nthose terms: e.g. user logins complete in 2 seconds. In this paper, we focus on the response time performance \nmetric. We use the term Work Units to describe these program tasks, because they are the atomic units \nof accomplishment in applications. For now, we will assume that our input data, the call stack samples, \nexplicitly represents two facts that we will later infer: the Work Unit in progress and the resources \nconsumed by that thread, at the moment in time the sample was taken. For example, a thread might be executing \nthe user login Work Unit, and, while servicing this task, be observed to place demand on a CPU and simultaneously \nown a lock. Section 8 shows how to extract these facts automatically, in the case that they are not explicit. \n2 The terminology of the planning community here isn t ideal: response time is the end-to-end time to \ncompletion of an entire unit of work. It is also useful to portion out service times ; e.g. the response \ntime of user logins is dominated by the slow service time of the database queries.  We call this relationship \nbetween a thread sample and re\u00adsources: demand. When viewed over all call stacks sampled at a moment \nin time, this demand relation tells us how much concurrency the application throws at each resource. \nFour threads observed to place demand upon 8 CPUs at a moment in time indicates a concurrent demand of \n4 placed against a concurrent capacity of 8.3 We de.ne a model that represents the level of concurrent \ndemand for a set of resources at a moment in time. The unit of concurrent demand is a thread. The resources \nin this abstraction may either represent local hardware or software resources, or may be proxies for \nremote resources, in the case that the application runs in a distributed fashion. The Marbles Model In \nan instance of a marbles model, each unit of de\u00admand is represented by a colored token, called a marble. \nThe color of a marble is the Work Unit in progress in that thread, at the moment the thread sam\u00adple was \nacquired. Each marble also has a fractional share that represents the portion of the call stack it represents; \nits initial value, prior to any tuning, is 1. Each marble is mapped to the resource it demands. A resource \nhas a concurrent capacity, and is either multiplexing (e.g. CPUs) or gating (e.g. locks) in the way it \ntolerates concurrent demand in excess of concurrent capacity. 6. Plan Simulation (When No Resource is \nSaturated) This section shows how, knowing only the Work Unit and demand relations, to predict the effects \nof a tuning plan. For now, we assume that a plan is provided as input; it might be, if a user wishes \nto explore what if scenarios. Section 9 shows how to automate the generation of plans that remedy existing \nperformance problems. In the following discussion, we will present how to simu\u00adlate a single tuning action. \nSection 9 discusses the simulation of a set of actions, i.e. a tuning plan. Speedup and Residual Demand \nGiven an application with k Work Units, W1,. .., Wk,a Tuning Action Simulator must: 1. Compute the speedup \nfactor of the given action upon each Work Unit Wi. We de.ne the speedup of a tuning action on a Work \nUnit to be the ratio, before versus after tuning, of the expected time to completion of that task. 2. \nPredict the residual demand that future samples would witness, were they to be taken after the tuning \naction is applied to the system.  3 Our concurrent capacity is elsewhere available parallelism . We \nstart by exploring the differences between server and batch workloads, and then show how to bridge these \ndiffer\u00adences with a common simulation model. 6.1 The Demand Invariant of Server Workloads In a server \napplication, work arrives on an ongoing basis. Thus, the amount of demand placed on resources is a func\u00adtion \nof rate at which work arrives, and the time that the re\u00adsources require to process each unit of work. \nTable 1 steps through the mental exercise of increasing arrival rate without a matching increasing the \nnumber of processors. For a while (the .rst three rows) an increase in arrival rate is tolerated gracefully \nby the system. In this regime of load, the server runs in a dynamic steady state: arrival rate may have \nspikes and lulls, but, as long as long\u00adterm average rate is not greater than 4 per second, the system \ncan consume load without any backlog. Observe from the .rst three rows that, as long as the server remains \nin a nice steady state, the level of demand for resources is a good predictor of the change in load (which \nis otherwise hidden from us). Demand is a good predictor of speedup because of an invariance that relates \nthe properties hidden from us to the property, demand, that we can observe. In a dynamic steady state, \nthe product of arrival rate and service time remains equal to the observed demand, inde\u00adpendent of changes \nin load or service time. This invariant is named Little s Law [15]:4 Demand = Arrival Rate . Latency \n(Little s Law) This law applies both at the granularity of individual resources (where Latency is the \nservice time of the resource), and also at the granularity of a Work Unit (where Latency is the total \nresponse time of that Work Unit). From this, we can predict both speedup and the residual demand that \nresult from a tuning action. For a tuning action that does not affect arrival rate, Little s Law implies \n.rst that speedup, i.e. a ratio of response times, is equal to the ratio of demand; and second, that \nif a tuning action increases the speed with which a resource can complete each unit of work, future demand \nfor that resource drops by that factor. For example, if we start out with a concurrent demand of 4 for \nCPU, and then speed up the operations by a factor of 2, we would witness two effects. First would be \na reduction in demand for CPU, from 4 to 2; this tuning action would take us from third row to the second \nrow in Table 1. Second would be a factor of two speedup in response time.  6.2 The Demand Invariant \nfor Batch Workloads For batch workloads, in contrast to server workloads, the total level of concurrent \nresource demand is unaffected by code becoming quicker. For example, if a part of the batch application \nis tuned so as to require less CPU, this will not 4 History credits Burton Smith with the restatement, \nin 1995, of this law in terms relatable to computer science [3]. We use adopt his terminology here. \n CPU Thread Pool Service Time Effective Service Time Resulting Capacity (C) Capacity (P ) Arrival Rate \nof CPU Operations (due to competition) Concurrent Demand 4 4 4 20 20 20 1 per second 2 per second 4 \nper second 1 second 1 second 1 second 1 second 1 second 1 second 1 thread 2 threads 4 threads 4 4 20 \n20 8 per second 16 per second 1 second 1 second 5 seconds 5 seconds 20 threads 20 threads Table 1. A \nmental exercise for a server workload: increase arrival rate without a matching increase in the number \nof processors. Eventually, the CPUs saturate, at which point the Thread Pool guards against divergent \nconsumption it bounds the badness by denying access to new consumers. A conventional planner measures \nArrival Rate and Latency. The columns shown in bold are those that we measure. Note how the measured \namount of Demand is a lossy representation of the state of affairs. reduce the amount of concurrent demand \nfor CPUs (though it will reduce the total CPU time, of course). To predict the residual demand, it is \nhelpful to know the distribution of demand to resources, for each work unit. The Work Unit-Resource Distribution \nFor a work unit W , the resource distribution of W is a probability density function over resources of \nthe likelihood that, during the servicing of W , a thread will be observed to be placing demand on a \nresource. Say a phase of a batch application is initially observed to place a concurrent demand of 4 \nthreads on CPU, and 6 threads on database resources. The resource distribution for this work unit is \n(0.4, 0.6). If a tuning action speeds up the CPU portion by a factor of two, this will not reduce total \ndemand from 10 to 8; there is already enough concurrency of the units of work in this phase to feed 10 \nworker threads, and speeding up an aspect of these Work Units does not change this. After tuning, we \nhave in effect 2 worker threads freed to consume resources in the ratio of 2/8 and 6/8: those freed up \nworker threads have a 25% chance of being seen consuming CPU and a 75% chance of being seen consuming \ndatabase resources. We refer to this new distribution (0.25, 0.75) as the residual resource distribution. \nThus, those 2 threads would be expected to place 0.5 and 1.5 demand on the two resources, leading to \na residual demand of 2.5 to CPU and 7.5 to the database resources.  6.3 Modeling Speedup: The Sink Construct \nUnlike with a server workload, the speedup of a tuning action in a batch workload is not given simply \nby the ratio of initial and .nal demand the demand invariant of batch workloads keeps the total level \nof demand constant across tuning actions. Fortunately, there is commonality that we can leverage that \nwill let us predict speedup for these two workloads in a uniform fashion. Observe that, for a server \nworkload, the factor f to which a tuning action decreases the service time of a resource is the factor \nby which demand decreases; this is a straightforward re.ection of Little s Law. As a bookkeeping crutch, \nwe can model a decrease in demand as an increase in demand for a special resource that we call the Sink \n. In this way, speedup can be measured by inspection, after applying a number of tuning actions, of the \ndemand for the Sink resource. The Sink Resource Speedup Rule We introduce a virtual resource called the \nSink. Any tuning action that reduces the service time of a marble by a factor of f results in 1. 1 of \nits share .owing to f 1 Sink, and remaining as demand for that resource. f We model this by splitting \nthe marble into two, each with its respective share. For any workload type, the response time speedup \nfactor to a Work Unit W, making sure to count only the demand due to marbles of W s color, is: Initial \nDemand Speedup = Initial Demand . Sink Demand For a batch application, we can consider this also to be \nthe case, except that additional load (obeying the residual re\u00adsource distribution) soaks up any threads \nmade available by that speedup. In order to restore the Batch Demand Invari\u00adant, a simulated Add Load \naction is performed. If k marble shares are moved to Sink, then we add k new marble shares, placed according \nto the residual work unit-resource distribu\u00adtion. In the previous example, where two units of demand \n.owed out of CPU and into Sink, and these two units were replaced according to the residual resource \ndistribution of (0.25, 0.75). 7. Plan Simulation (When Demand Exceeds Capacity) With enough load, it \nis possible for concurrent demand to exceed the concurrent capacity of a resource. For example, in one \nsnapshot, we might observe 5 threads placing demand on 4 CPUs. When demand exceeds capacity, one of three \nsituations occurs. Either demand is time-multiplexed to the available capacity, demand queues up waiting \nfor access to available capacity, or performance diverges, as work arrives faster than it can be serviced. \nWe deal with these three situations in turn.  7.1 Multiplexed Resources (CPUs and Network Fabric) In \na server workload, momentary spikes in arrival rate or la\u00adtency can cause periods of backlog. Backlog \nresults because the amount of concurrent demand exceeds the concurrent ca\u00adpacity of one or more resources. \nFor a server, as long as the long-term average demand for a resource does not exceed its concurrent capacity, \nbrief backlogs will be gracefully tol\u00aderated: between the mountains of excess, the valleys in demand \nwill give the system time to drain pent-up demand. For a server in a state of brief excess, time to completion \ndegrades linearly, as resources are time-multiplexed between aspiring consumers. If, for a server workload, \nthe long-term average product of arrival rate and service time rises above available concurrent capacity, \ncatastrophic effects ensue; we will return to this shortly. This multiplexing before concerns us because \nit opens up the possibility that a tuning action focused on capacity may indirectly result in a decrease \nin service time. The Speedup Effect of Capacity Actions If a resource with concurrent capacity C is faced \nwith concurrent demand D, then the multiplexing slow- D down is max(1, ), termed the overcommit factor. \nC A tuning action that increases the concurrent capacity of this resource by a factor of a will cause \na reduction in service time by a factor of a, bounded by the overcommit factor. Therefore, the effective \nservice time reduction factor of a capacity action of magnitude a ? 1 is D fcapacity =min(a, max(1, )) \nC For example, a 4-way machine with 6 threads demanding access to CPU has an overcommit ratio of 1.5. \nWe can now apply the Sink Resource Speedup Rule from Section 6.3. By plugging in this effective speedup \nfactor, we can compute the expected service time speedup of a capacity action.  7.2 Gating Resources \n(Monitors and Thread Pools) The second kind of behavior that may occur when demand exceeds capacity is \ngating. A gating resource, such as a Java monitor, restricts access to a set of associated resources. \nThey serve as concurrency limiters for the multiplexed re\u00adsources used in the critical section of the \ngate. For a moni\u00adtor, that would be whatever resources are demanded by the block of code executed while \nthe monitor is owned. CPU Capacity=1 CPU Capacity=3 Gate Capacity=1  Gate Capacity=2 Residual Concurrent \nDemand Average Service Time Gate Capacity=3 Figure 4. A gate that guards a critical section, which in \nturn places demand on CPU resources. CPU is a potential latent or looming bottleneck, when considering \nincreasing the ca\u00adpacity of the gating resource: beyond a point, increasing the capacity of the gate \n(as a solitary action) may reap no bene.t. All threads that wish to pass through the gate, the pent-up \ndemand, lie idle. For every gating resource, we also associate a queue, and any marbles that represent \npent-up demand for the resource are mapped to this queue. Consider the example shown in Figure 4, which \nshows three Work Units (the three colors of marbles), and explores increasing the capacity of a gating \nresource that limits access to a CPU-bound critical section. Observe that, when the CPU has a concurrent \ncapacity of 1, increasing the capacity of the gating resource does not reduce the average service time \nof the three Work Units. This is an example of a latent bottleneck: the CPU resource is already saturated. \nIf instead the CPU resource has a capacity of 3, then increasing the capacity of the gate does result \nin a speedup to the Work Units, but only to the point at which the CPU resource becomes saturated. This \nis a looming bottleneck. Latent and looming bottlenecks occur with gating re\u00adsources due to demand shifting. \nIn the case for a multiplex\u00ading resource, a capacity increase may reduce the slowdown due to time sharing. \nFor a gating resource, a capacity in\u00adcrease moves demand: from the pent-up demand queue to the resources \nof the critical section. The Demand Shifting Effect for Gates Let the length of the pent-up demand queue \nfor a gat\u00ading resource be Q, and the initial concurrent capacity of the gate be C. If a tuning action \nincreases the ca\u00adpacity of a gating resource by a factor of a ? 1, then the number of marbles that will \nshift is Ca -C, but bounded by the length of the queue Q. Thus the frac\u00adtion of each pent-up marble that \nshifts is this value divided by Q: C(a -1) Gate Shifting Fraction = min(1, ) Q When load has shifted \nto the critical section, this has same effect as an Add Load tuning action on the resources of the critical \nsection. Therefore, when simulating an increase in     7.3 Divergent Demand: In.ow Exceeds Out.ow \nThe third and .nal scenario of excess demand affects only server workloads. If new work arrives more \nquickly than existing work can be dispatched by the processors, work will begin to pile up. This pile-up \nwill only reach catastrophic proportions if the long-term average product of arrival rate and service \ntime exceeds the capacity of any resource in the system (an application of Little s Law): response time \nwill diverge to in.nity.5 A thread pool bounds this badness, providing a way to specify the maximum \nfactor by which a resource should be Ghost Demand of Critical Section overcommitted. In this case, for \na pool of capacity P serving Figure 5. With gating resources, such as monitors and as a gate, through \nwhich requests must pass, over a resource thread pools, one thread can simultaneously place demand on \nmultiple resources: a share of the gate itself and the resources necessary to service the critical section \nguarded by the gate. We create ghost marbles to represent ownership of the gate resources. Those threads \nwaiting to pass through with capacity C, the factor is P C ; in Table 1, this overcommit ratio is 5, \nmeaning that we are willing to overcommit our CPUs by a factor of 5. The resulting amount of Demand that \nwill be witnessed is the minimum of P and product arrival rate and service time. the gate are placed \nin an associated queue resource. the capacity of a gating resource, we also simulate Add Load actions \nin that fashion. There is a second way to reduce contention for a gat\u00ad ing resource: tuning the critical \nsection. Any thread that has passed through the gate will simultaneously place de\u00ad mand on multiple resources \nof a single machine. For exam- Thus, as shown in the last two rows of Table 1, in a regime of divergence, \ndemand for resources is likely to be an imperfect re.ection of the underlying load. In practice, thus, \nwe cannot parametrize tuning actions with any hope of accuracy, but this does not render the actions \nthemselves moot. For example, diverging due to CPU saturation is a good sign that this is where tuning \neffort should be focused, even if we cannot predict how many processors to add. This topic, of tuning \nplan generation, is the topic of Section 9. ple, while a monitor owner executes a CPU intensive critical \nsection, it places demand on both the monitor and CPU re\u00adsources, as illustrated in Figure 5. We represent \nthis simulta\u00ad neous ownership with a special marble, called a ghost. A ghost marble is a normal marble \nbut with the additional property of being related to exactly one other marble m; m is either mapped to \na multiplexing resource, or, recursively, to another ghost marble, e.g. in the case of nested locks. \nWe refer to this as the critical section relation. For monitors, this information is represented explicitly \nin Java thread stack snapshots: the list of live monitors and, for each, the owning and waiting threads. \nFor other gating re\u00adsources, such as thread and connection pools, this informa\u00adtion is only implicit \nand must be inferred (see Section 8.2).  f The Critical Section Tuning Effect Let m be a marble that \nis bene.ted by a service time reduction according to The Sink Resource Speedup Rule. Thus this marble \nwill be split in two, and a = 1 -1 will .ow to Sink. If m lies in the range of the critical section relation, \nthen also .ow marbles according to The Demand Shifting Effect, using this factor a.  7.4 How Accurate \nare Our Predictions? We now demonstrate how accurately our implementation predicts speedup and residual \ndemand. For these experi\u00adments, we use a microbenchmark that simulates three con\u00adcurrent Work Units executing \nin either a server or batch workload. In one experiment, the Work Units exercise only multiplexing resources, \nand in another we throw a gating re\u00adsource, in the form of a lock, into the mix. The experiment works \nas follows: we selectively elimi\u00adnate certain of the Work Units; e.g. what would happen if the .rst and \nthird Work Units were eliminated? We refer\u00adence these tuning plans in a binary notation: 010 denotes \nthat particular what-if scenario. There are thus six plans for each type of workload (23 excluding 111 \nand 000, i.e. no tuning, and nothing to do). We measure the actual speedup in each case (relative to \n111), and also measure the actual residual demand (by observing the call stack samples after the tuning \nplan has been implemented). We compare these realities to what our simulation says about speedup and \ndemand. Figure 6 summarizes the accuracy of our predictions of speedup. Observe the close tracking of \nactual and predicted speedup: the linear regression correlation coef.cient be\u00adtween the two is 0.96.6 \nFor the most part, the predictions 5 This divergence due to excess load is nicknamed the hockey stick \neffect. 6 In this experiment, underlying each data point in the chart is at least 5 runs, each with 7 \nsnapshots, and each of those contain 36 call stack samples.  (a) Only Multiplexing Resources  (b) \nWith a Gating Resource Figure 6. Accuracy of speedup predictions on a mi\u00adcrobenchmark with three Work \nUnits, using a server work\u00adload. The triples denote tuning plans; e.g. 101 eliminates the second Work \nUnit. Speedup here represents the average across the Work Units. The inter-run variance is 5 10%. underestimate \nspeedup. Figure 7 continues this experiment, showing good predictions of residual demand. Knowing the \nnature of the workload is important for accurate predictions. As shown in Figure 8, if we mistakenly \ntreat a batch workload as if it were a server workload, our predictions of residual demand are poor. \n8. Inferring Work Units and Demand As Section 6 and Section 7 showed, to simulate the bene.t of a plan, \nwe require a collection of traits to be associated with the input data: Work Unit of a call stack: From \nthe standpoint of in\u00adcreasing performance, we primarily care about those threads observed to be servicing \na work item of the ap\u00adplication. It is these units of work that we desire to make faster, or, as load \nincreases, the tasks for which we need to maintain acceptable levels of performance.  Resources Demanded \nby a call stack: The speedup and residual demand of a tuning action depend upon the level of concurrent \ndemand placed on each software and hard\u00adware resource.  Figure 7. Accuracy of residual demand predictions. \n Figure 8. Mistakenly treating a batch workload as if it were a server workload leads to poor predictions. \nTo save space, here we plot total residual demand across all resources. Workload Type of the application \nas a whole: Sec\u00ad tion 6.1 and Section 6.2 showed that the effect of a tuning action depends upon the \nworkload: server or batch. The simulation aspect of the Marbles Planner is not hard\u00adwired to depend on \nany particular source for this informa\u00adtion. However, the input data we use does not represent these \ntraits explicitly. Thus, some inference work is necessary to pull this information out of the call stacks. \nEven traits as seemingly simple to know, such as whether the thread is Runnable (in the sense that the \nthread is ei\u00adther executing on CPU, or is on the run queue) are not im\u00admediately obvious from a thread \nstack sample. Often, this particular aspect of demand is nominally available, but it is usually unreliable. \nThe JVM has to quiesce the threads be\u00adTable 2. The built-in Work Unit rules match on the package, class, \nand method name of the method invoked in a frame. When a rule .res, it assigns a Work Unit label to the \nstack.  Frame Matching Rule Work Unit Class.contains(Filter | Handler | Action | Command | Operation \n| Service | Task | Runner | Runnable | Request | Session | Servlet | Worker) Method.equals(doGet | doPost) \nPackage.startsWith(com/ibm/ jsp |org/apache/jsp) Class.Method Class Class fore writing out a consistent \nsnapshot of the call stacks of live threads. Knowing both the run state of a thread and its call stack, \nas a consistent pair of information, is challenging. For this reason, many JVMs falsely report Runnable \nthreads as being not-Runnable, and vice versa; earlier work .rst ad\u00addressed this particular issue [2]. \n To infer Work Unit and Demand from a call stack sample, we use a rules-based approach. Each rule codi.es \na mapping from an invocation (the triple of package, class, and method name) to a Work Unit or demanded \nresources. This approach has proven effective, without an explosion in rules, because most Java applications \nadopt common class naming conven\u00adtions, and use a common set of frameworks. A rules-based approach also \npermits easy tailoring to han\u00addle the strange coding practices that crop up in applications from time \nto time. For example, when analyzing our own code for performance issues, we realized that we used fairly \nnon-standard naming conventions for our tasks. Adding a half dozen Work Unit rules quickly resolved this \nissue. We will now describe the rules systems for inferring Work Unit and Demand, and show how, in each \ncase, to know when the rules are likely to be insuf.cient.7  8.1 Inferring Work Units We provide a built-in \nset of rules that map invocations on a stack to the Work Unit being serviced by that stack sample. The \nrule engine scans a stack, from leaf to root, and applies the rules shown in Table 2. Each rule matches \non the pack\u00ad age, class, and method names of the method invocation of a call stack frame. When a match \nsucceeds, the rule asserts a Work Unit label for the stack as a whole. For example, if a stack contains \nan invocation of Login\u00adServlet.doGet, it would be reasonable to conclude that this sampled thread is \nservicing the Login Work Unit. The third row in this table covers the classes for servlets that are gen\u00aderated \nfrom JSP (Java Server Pages) source by two common servlet containers (IBM WebSphere and Apache Tomcat). \n7 Unfortunately, space limitations do not allow discussion of Workload Type inference. Elevator Summary: \nmost batch workloads are executed in phases, and a program has phases if samples are likely to have only \none Work Unit.  (b) A measure of false negatives It is possible for one call stack to have more than \none matching frame. Multiple matches occur for two primary reasons: data-driven dispatching, and .lters. \nThe former oc\u00adcurs when, for example, a single servlet acts as a dispatcher to nested work units. Rather \nthan having a separate servlet for each work unit, this coding architecture creates a single servlet \nthat in turn hands control to the actual Work Unit, based on the parameters to the servlet invocation. \nA Java servlet .lter is a stage in a pipeline of processing elements that contribute to the servicing \nof a main request [9]. In this coding architecture, there are often apparently a great many Implicant \nExample Method  isNetRead SocketInputStream.socketRead0 isNetConnection PlainSocketImpl.socketConnect \nisNetAccept PollsetSelectorImpl.doSelect isDiskOperation RandomAccessFile.read isSleep Thread.sleep isNativeRunnable \nDeflater.deflateBytes Table 3. The periphery of the Java space: all demand, except Java-level CPU consumption, \nemanates here. Work Units active on the stack, even though all but one of them are links in a long chain \nof .lters. In both cases, data-driven dispatching and .lters, we must make sure to tolerate multiple \nmatches. Our .rst line of defense is scanning from leaf invocation to root invocation. By doing so, the \nrule engine attempts to assign the most speci.c Work Unit assignment that it can. This very often overcomes \nthe multiplicity that stems from .lter chaining. Our second line of defense deprioritizes certain class \nnames, in particular those with Abstract as a subsequence. These lower priority matches are only used \nas fail-safes, in the case that no better matches are found. Finally, we have a blacklisting mechanism \nin place for common frameworks. For example, the Apache Jakarta ActionServlet is never a meaningful Work \nUnit on its own; this class is part of the code that services the real servlets, which have been coded \nin a declarative form. 8.1.1 Validation of Work Unit Inference Figure 9 presents a histogram, across \nour inputs, of the number of inferred work units. The distribution is fairly heavily weighted towards \na smaller number of Work Units: 33% of the data sets have fewer than 5 Work Units; 48% have fewer than \n10, and 76% have fewer than 20. Figure 10 shows two charts that summarize the logic our regression testing \nuses to validate the work unit assignment. If a call stack is very shallow, fewer than say 8 frames deep, \nthen it is very unlikely to be in the middle of an important unit of work. Thus, if we have assigned \nthese shallow stacks a Work Unit, this is likely to be a false positive identi.cation of Work Unit. Similarly, \ndeep stacks that haven t been as\u00adsigned a Work Unit are likely to signal false negatives. The charts \nshow that our current rate of false negatives is low: 91% of call stacks, across our 15,000 data sets, \nhave a 0 1% false negative rate. We also do well with false positives: 57% have a 0 1% false positive \nrate and 85% of stacks have a 0 5% false positive rate.  8.2 Inferring Resource Demand To infer the \nresources demanded by a given call stack sam\u00adple, we use a system of implications that builds up facts \nabout the activities being performed by the stack. We start from a few simple observations about how \nthe thread is in\u00adteracting with the outside world, and then combine these into more meaningful predicates. \nFor example, a thread is likely to be waiting for data to return from a remote network operation if: \nit is performing a read from a network socket, and it is in the middle of servicing a request, and it \nis executing data access logic, and it is not waiting on a monitor. The .rst alone at .rst sight seems \nsuf.cient; e.g. if we saw a socketRead at the top of the stack, it seems reasonable to interpret this \nliterally, i.e. as a read of data from a socket, and thus conclude that the thread is fetching data. \nUnfortunately, the .rst implicant is a necessary but insuf\u00ad.cient predictor of demand. For example, it \nis common to implement a client-server pattern by having the server per\u00adform a blocking socket read. \nThis has the effect of the server appearing to be waiting for data from a query, when it is really waiting \nfor work. This is an important distinction, be\u00adcause the former places demand on network resources (and \npossibly a remote database machine), and the latter places demand on no resources.8 8.2.1 Native Implicants: \nThe Java-Native Periphery Table 3 presents the ways a Java thread can interoperate with the world outside \nof Java. This list of native operations, and the implicating methods for each, have both proven to be \ninfrequently changing and small. Currently, the total number of rules is about 450. We will shortly show \nthat these suf.ce to de.ne the native periphery for our 15,000 submissions. Some basic implicants are \nnot represented by an invoca\u00adtion, but rather in metadata associated with a call stack sam\u00adple. In particular, \nhow a thread interacts with monitors is not entirely a function of which methods it has invoked; e.g. \none thread waiting to enter a synchronized lexical scope, and a second that is executing in that critical \nsection both of these will have the same method on the top of their stacks.  8.2.2 Data Access: isDataAccess \nand Beans In our experience, the most dif.cult distinction to draw is that of a thread waiting for data \nto return from a remote data query versus a thread waiting for work to arrive. Usually, accessing data \nrequires going through some sort of data access layer. These layers translate application requests to \nand from the wire protocol required by a remote data source. If we observe the code of a data access \nlayer on a call stack, then a network read operation is much more likely to be fetching data rather than \nwaiting for work. We have de.ned two kinds of mapping rules to capture data access. The .rst, more primitive \nlayer, de.nes the pro\u00adtocols; we have rules for the common relational databases (e.g. when we see an \nexecuteQuery on the stack, we can have some con.dence that the thread is fetching data from a rela\u00adtion \ndatabase), and the common marshalling protocols (e.g. Java object serialization and SOAP). If any of \nthese proto\u00ad 8 Modeling memory consumption is a worthy subject of future research.  (a) Multiplexing \nResources (b) Gating Resources Demand Predicate Demand Predicate Implication CPU Disk isRunnable isWaitingForLocalData \nLock isWaitingToEnter isOwner Queued up Share Consumed Network isWaitingForRemoteData Thread Pool isWaitingForWork \nisInRequest Share Available Share Consumed Connection Pool isWaitingForConnection isWaitingForRemoteData \nQueued up Share Consumed (c) Some Example Production Rules Rule Implied Predicate isNetAccept &#38; \n!isInRequest 7-! isWaitingForWork isNetRead &#38; !isDataAccess &#38; !isInRequest 7-! isWaitingForWork \nisNetRead &#38; isDataAccess &#38; isInRequest 7-! isWaitingForData !isWaitingForData &#38; !isWaitingForWork \n&#38; !isWaitingToEnter &#38; . . . 7-! isRunnable Table 4. We establish levels of demand by directly \nobserving or inferring demand predicates, such as isRunnable and isWaitingForData. In addition, for gating \nresources, we need to distinguish between consumption and desire for consumption. cols are matched by \na frame on the call stack, we associate the isDataAccess implicant with the stack. The second layer attempts \nto give names to the data types being fetched. For example, if we see EmployeeBean.fetch on the stack, \nthen it is likely that an instance of the Employee data type is coming over the wire. We call these labels \nBeans. To infer the Bean of a call stack, we employ a pattern\u00admatching approach analogous to that used \nin inferring Work Units; e.g. if the class name contains Bean or Factory or Per- Fraction of Veri.able \nData Sets 60% 45% 30% 15% 0% <5% 10% 15% 20% 25% 30% 35% 40% 45% 50%+ sister or Loader or Entity, we \ncan have some con.dence that the class name de.nes the data type being accessed. If so, then we associate \nthis class name as the Bean of the stack. By combining these two levels in disjunction (isDataAc\u00adcess \n| hasBean) we increase the coverage of the data access implicants. Furthermore, Section 9 shows how to \nleverage the Bean information to generate tuning plans with a greater degree of speci.city: rather than \nsuggesting to tune all re\u00admote data access, we can suggest to tune the queries that fetch Employee instances. \n 8.2.3 Demand Rules From these basic implicants, we can begin to construct the rules that predict the \nresources demanded by a call stack. Ta\u00ad ble 4 summarizes the multiplexing and gating resources con\u00ad sidered \nin this paper: CPU, Locks, Disk, Network Requests, etc. We associate a predicate with each aspect of \ndemand; e.g. a stack with the isRunnable predicate will be assumed to place demand on the CPU resource. \nTable 4(c) gives some example implication rules that we use to infer these predi\u00adcates of demand. For \nall resources, we need to infer the level of demand. For gating resources, we need an extra property, \nthe amount Error CPU Utilization Prediction Figure 11. The disparity between observed and inferred level \nof CPU demand, limited to the veri.able reports i.e. those for which we have true CPU utilization information. \nof pent-up demand. Section 7.2 shows why this information is necessary to faithfully simulate the effect \nof a tuning ac\u00adtion upon a gating resource: it is the pent-up demand that shifts to the resources of \nthe critical section. Table 4(b) sum\u00ad marizes our approach for distinguishing consuming demand from pent-up \ndemand. For locks, the level of pent-up demand is given to us directly in the stack sample data: the \nthreads waiting to enter a monitor are indicated as such. For thread pools, we can know the number of \nidle worker threads available by simply counting the number of thread samples with the isWaitingForWork \nimplicant.  8.2.4 Gauging the Accuracy of Demand Predictions To gauge the success of this approach for \ninferring the de\u00admand relation, we need to measure how accurately it predicts actual resource consumption. \nFigure 11 shows a histogram of our data sets, arranged into buckets according to the dis\u00adparity between \nactual processor utilization and the level of predicted by the mapping rules. We gauge actual processor \nutilization by observing the output of tools such as vmstat on UNIX, or typeperf on Windows. For this \nreason, we can only validate against the 6000 (of our 15,000 reports) that have this CPU utilization \ninformation.  For the large majority of the submissions, 5362 of 6077 (88% of them) the inferred consumption \nrelation predicts ac\u00adtual consumption within 20%. For 3499 of the submissions (58% of them), the inferred \nvalue differs from actual level of processor utilization by at most 5%. Figure 12 shows a breakdown of \nthe demanded resources, across our data sets. There is always a plurality of resources consumed. Note \nthat it is rare for applications to suffer from over-consumption of a single resource. Rather, is the \nnorm for an application to consume a wide variety of resources. These consumed resources include those \ninternal to the pro\u00adcess, such as locks, local to the machine, such as CPUs and disks, and those external \nto the machine. 9. Automatically Generating Tuning Plans The discussion of tuning action simulation in \nSection 6 and Section 7 assumed that a plan was provided as input to the simulation. This works well \nfor analyzing what-if scenar\u00adios that hypothesize future possibilities, but isn t as helpful when trying \nto resolve a problem in the here and now. In this section, we present an algorithm for automatically \ngen\u00aderating tuning plans whose aim is to eliminate extant bot\u00adtlenecks. The plan generation algorithm \nstrives to maximize bene.t, while heuristically minimizing the amount of need\u00adless expenditure of tuning \neffort. 9.1 Parametrizing Single-action Tuning Plans When generating a single-action tuning plan, we \nneed to enumerate the possible actions, and, for each, choose a value for its tuning factor. A good choice \nof factor is one that isn t overkill . A tuning action is overkill if it buys no additional bene.t over \nlesser plans. Overkill and The Goldilocks Factor Let f be the tuning factor of a given tuning action. \nWe consider this tuning plan to be overkill if there exists a tuning factor f0 <f such that the Speedup \n(as considered by The Sink Resource Speedup Rule of Section 6) when using f0 is at least that when using \ntuning factor f. Whereas, if there does not exist an f0 whose Speedup exceeds that of f, then f is the \nmaximum effective tuning factor it is just right. Using this somewhat blunt de.nition,9 the maximum \nef\u00adfective tuning factor of a capacity action is exactly the over\u00adcommit factor of the resource: if there \nis twice as much con\u00adcurrent demand for CPU as there is available capacity, then we needn t increase \nthe capacity of CPUs by any more than this factor of 2. For tuning actions that directly reduce service \ntime, the maximum effective tuning factor is unbounded: e.g. the faster the code becomes, the more effective \nthe action is at reducing service time. With a more sophisticated speci\u00ad.cation of desired performance \ngoals, this can be re.ned. For example, if the user states that Work Unit W needs a 2x decrease in service \ntime, we can establish a maximum effective tuning factor for service time-reducing actions.  9.2 Tuning \nActions Speci.city Next, to enumerate the set of possible action types, we in\u00adspect the mapping from \ncall stacks to resources demanded. The range of this mapping de.nes the set of resources in the system. \nWe thus evaluate one capacity action to each re\u00adsource, whether multiplexing or gating, parametrized \nwith the maximum effective tuning factor at that resource. To select service time-reducing actions, we \nhave two choices for the speci.city of the action. We can either render the resource itself faster, or \nwe can render the operations faster. We employ the following heuristics. First, we con\u00adsider it unlikely \nthat resources can be made magically faster. When simulating what-if scenarios, this may be a valid ac\u00adtion, \nand is not precluded in the simulation. However, from the standpoint of suggesting tuning plans for the \nhere and now , we have chosen to avoid including actions that speed up resources. We are thus left with \nactions that increase the speed of code. But which code? Here, we employ a second heuris\u00adtic. We consider \neither code that is demanding CPU or code that is performing a data access (according to subsubsec\u00ad tion \n8.2.2). For the former, we create one tuning action for every leaf-most method; i.e. for every operation \nthat is exe\u00adcuting on the CPU, we consider a tuning action that renders this speci.c operation faster. \nFor the latter, the data accesses, we create one for every Bean; i.e. for every data type being 9 We \nbelieve it is easy to generalize this discussion to replace the notion of strict overkill with one of \nmarginal added bene.t. Doing so would require some guidance from the user, to shape these cost-bene.t \ntrade-offs.  reconstituted via a remote data access, we consider a tuning action that renders this speci.c \nquery faster. 9.3 Creating Multi-action Tuning Plans We now have a set of parametrized, single-action, \ntuning plans. We evaluate the Speedup of each, via the simulation of Section 6 and Section 7. It is possible \nthat the Speedup will be zero. For every zero-bene.t tuning plan, we consider whether there was a latent \nor looming bottleneck. Latent and looming bottlenecks can only occur for actions that result in demand \nshifting from one resource to another. To witness of a latent bottleneck: observe a tuning action that \nresults in some demand shift but has a Speedup of zero. In these cases, we must add at least one more \ntuning action to the plan, in order to maximize the effectiveness of that .rst tuning action. For this \npaper, we consider the remedy for a latent or looming bottleneck to be the capacity actions, focused \non those resources that previously were not overcommit\u00adted and now are. Each of these added capacity \nactions is parametrized according to the maximum effective factor. We repeat this process iteratively \nuntil the .nal action incurs no latent or looming bottlenecks. 10. Using the Marbles Capacity Planner \nFinally, we discuss two main uses of the on-the-.y capacity planner presented in this paper. The .rst \nis as a .lter over problems: by generating candidate plans to .x performance problems, we can also know \nwhich call stack samples will not bene.t from any plan. The second use that we discuss is as the foundation \nfor exploring the impact of constraints on the planning engine: every what-if scenario takes the form \nof a number of bounds on the plan generator. 10.1 Plan Generator as Filter: Has-No-Bene.t The plan generator \nhas an important use as a .lter over potential problems. If there does not exist a plan, of any size \nor action constituency, that bene.ts a stack, there is little point in displaying this stack to the user. \nFor example, a common way to implement the pattern of waiting for work involves lock contention. A worker \nthread, when it becomes available, attempts to acquire a monitor that guards the work queue. In this \nsituation, .xing the lock contention, even in tandem with other actions, will neither have bene.t on \nthe response time of any Work Unit, nor will it bene.t the throughput of the system. The Marbles Planner \ninfers this, as it futilely attempts to generate plans addressing this lock contention. We refer to this \nproperty as hasNoBene.t. Figure 13 summarizes the fraction of call stack samples in a given data set \nthat the Marbles Planner estimates as hav\u00ading the hasNoBene.t property. The horizontal axis of this chart \nrepresents the CPU overcommit factor. As the factor by which CPU is overcommitted increases, there is \na mod\u00aderate increase in the fraction of samples that bene.t from Figure 13. The Plan Generator can be \nused as a .lter over call stack samples: if there does not exist a plan that bene.ts a call stack sample \n(a property we term hasNoBene.t), then a tool user interface can exclude them from the presentation. \nHere, we plot the fraction of observations in each data set that have this property, versus the overcommit \nfactor of CPU. tuning. Also observe that, across the spectrum of overcom\u00admit factors, a majority of \nsamples in every data set are not worth tuning. Thus, when presenting .ndings to a user, even if the \nuser does not care about the plans themselves, she is gaining bene.t from the planning system. She no \nlonger has to be concerned with misleading .ndings in the tool. It is often the case that this kind of \nmock lock contention is prevalent enough so as to dwarf the real problems. 10.2 Exploring What If Scenarios \nWe now demonstrate two .nal points. Firstly, the Marbles Planner has the capability to answer a variety \nof what-if questions. Secondly, asking these questions is important, because the answers, as to how to \ngo about tuning your application, vary radically depending on the nature of the questions being posed. \nEvery what-if scenario places constraints on the plan gen\u00aderator. At one end of the spectrum, the plan \ngenerator can be given free reign to propose tuning plans with high .nan\u00adcial cost, and in producing \na high volume of proposed plans. Realities usually impose constraints on one or the other of these variables. \nIn the next two subsections, we explore these two dimensions of constraints: the economics of affordabil\u00adity, \nand the issues of ranking extant problems. 10.2.1 What if There are Budget Limitations? To experiment \nwith the economic side of planning, we ex\u00adplore three what-if scenarios. In the .rst, we pose the what\u00adif \nscenario where code or query tuning actions can in.nitely speed up the code or query, and where there \nis an in.nite budget for adding hardware capacity. This corresponds to no reality, but we feel that it \nre.ects the perspective encouraged by a typical hotspot-.nding tool: show me what is hot, and prioritize \nproblems in that way.10 At the other extreme, we 10 This highly optimistic view of the potential of tuning \nactions is implicitly embodied by state of the art performance analysis tools: by not taking into  \n(a) Tuning Actions Yield an In.nite Speedup, Unlimited Budget for CPUs  (c) Tuning Actions Yield 10% \nSpeedup, Budget for Doubling CPUs constrain the budget to a more austere 10% maximum reduc\u00adtion in service \ntime for code tuning, and a maximum factor of 2x for capacity additions. In this, and all of the studies \nin this section, locks are always assumed to be fully .xable. To explore a point in the middle, we constraint \nthe budget to a maximum 2x reduction in service time for code tuning, and a maximum of 4x for capacity \nadditions. The study then proceeds as follows. For each call stack sample, the plan generator may produce \nmultiple tuning options; e.g. adding CPUs or tuning the CPU-consuming account demand and capacity, and \neven sometimes even focusing only on CPU activity, these tools may overstate matters. (a) Tuning Actions \nYield an In.nite Speedup, Unlimited Budget for CPUs (c) Tuning Actions Yield 10% Speedup, Budget for \nDoubling CPUs   code; or .xing a lock or tuning the critical section. In both cases, it is likely that \none plan will produce a larger bene.t. Thus, we inspect each call stack sample and place the best plan \ninto one of .ve buckets: one-action tuning plans that tune a CPU-demanding operation, that tune a data \naccess query, that .x a lock, that add capacity, or multi-action tuning plans consisting of a mixture \nof actions. Figure 14(a) shows the population distribution of these buckets for the case of in.nite tuning. \nUnder the optimistic scenario of unlimited budget, the system predicts that code tuning is most often \nthe best plan. Furthermore, as the over\u00adcommit factor of CPU increases, code tuning is even more likely \nto be the best course of action. This is as expected, for when CPU is saturated (corresponding to an \novercommit factor of greater than 1), code tuning has a double bene.t: it both decreases service time \ndirectly, and reduces the penalty of multiplexing.  At the more austere end of the spectrum of economics, \nFigure 14(c) shows a very different story. When the over\u00ad commit factor is below 1 (meaning that CPU \nhas excess capacity), single-action plans that focus on the code (either code tuning or .xing locks) \nshow a peak they are more often to compose the best plan when CPU is not overcom\u00admitted. When the overcommit \nfactor CPU rises above 1, ca\u00adpacity actions become increasingly important. This is as ex\u00adpected: once \nthe CPU is overcommitted, latent bottlenecks cap the bene.t of single-action tuning plans. Also observe \nthat the relative importance of single-action plans that .x lock contention problems diminishes as the \nlatent and loom\u00ading bottlenecks begin to rear their heads, once the CPU over\u00adcommit factor rises above \n1. Figure 14(b) shows the breakdown of best plans for a case somewhat in the middle. As expected, the \nshape of the curves shows a point on the spectrum from the austere to the unlimited cases. Figure 15 \nfurther breaks down the multi-action plans from Figure 14. When tuning has less power to optimize (as \nin Figure 15(c)), three-action plans dominate. When given moderate power to optimize (as in Figure 15(b)), \nthere is a plurality of the best multi-action plans, split between those requiring two tuning actions \nand those requiring three; in this case, the arithmetic mean fraction of call stacks whose best tuning \nplan involves two actions, across all CPU over\u00adcommit factors, is 40%; for 3-and 4-action plans, this \nmean is 46% and 6.1%, respectively. 10.2.2 What if Only One Plan is Allowed? A second aspect of budgeting \nis the time spent investigating potential problems. All three scenarios from the previous study assume \nthat the users could devote the time to study a separate plan for every call stack sample. This luxury \nshares its unrealism with the scenario that allows for in.nite tuning (a) Tuning Actions Yield an In.nite \nSpeedup, Unlimited Budget for CPUs (c) Tuning Actions Yield 10% Speedup, Budget for Doubling CPUs  \n and an unbounded budget for hardware they are useful limit studies, but not something to be employed \nin practice. Just as there is a spectrum of what-if scenarios along the dimension of cost (of which we \nexplored three points in the previous study), there is also a spectrum along the dimension of the volume \nof suggested plans. At one extreme, the plan\u00adner can be used, as with the previous study, to generate \none plan per call stack sample. At the other end of this dimension of volume, the planner would suggest \na single plan, one that is estimated to have the highest impact on performance. We explore this scenario \nnow.11 11 There are other interesting points along this spectrum of volume worthy of further study. \nFor example, the planner could also be tasked with identi\u00adfying a separate best plan for each Work Unit. \n To identify a single best plan requires establishing a rank\u00ading order over plans, so that we may present \nthe top-ranked plan to the user.12 A ranker should take into account the level of dissatisfaction that \nthe users have with the Work Units in their application; from this, a single plan can be identi\u00ad.ed that \nhas the highest bene.t to the Work Unit with great\u00adest dissatisfaction.13 For this paper, we unfortunately \ncannot leverage such information, as it is not represented in our cor\u00adpus of 15,000 data sets. In this \npaper, we introduce a simple ranking function over Work Units, and leave a full treatment of this important \ntopic to future work. We rank problems by .rst ranking Work Units by their prevalence, and then selecting \nthe plan with highest bene.t to the most prevalent Work Unit. The Prevalence Ranking Function We de.ne \nthe prevalence of a Work Unit as the sum of two separate ranks: the likelihood of occurrence rank and \nthe concurrency rank. Given two Work Units A and B, the likelihood of occurrence of A is greater than \nthat of B if A is active in more snapshots of the thread state. The concurrency of A is greater than \nthat of B if, while active, A is on average active in more threads than the similar .gure for B. By combining \nlikelihood and average concurrency, we seek to avoid what we term the mountains and valleys problem of \nranking. If you use only average concurrency in order to rank, as does any simple method pro.ler, frequently \nwitnessed but single-threaded Work Units will be dwarfed by infrequent but highly concurrent Work Units. \nFigure 16 illustrates this situation. The charts of Figure 17 mirror those of the previous study shown \nin Figure 14: we experiment with the same three what-if scenarios of cost, but this time we choose the \nhighest bene.t plan for the Work Unit with highest prevalence. For the lower-cost scenarios, i.e. comparing \nFigure 14(c) to Figure 17(c) it is more often the case the best plan involves .xing locks or multi-action \nplans (and, though not shown, these multi-action plans all include an action of .xing a lock). This implies \nthat it is common to have egregious lock contention in our corpus of reports. Contrast this with the \ndecreased importance of query tuning. 11. Related Work There are a number of existing strategies for \nperformance tuning, performance modeling, and capacity planning. 12 In our user interface, we also use \nthis ranking order to organize the .ndings. 13 For example, if the Work Units have an associated service \nlevel agree\u00adment that governs the upper bounds on resource consumption, a metric of dissatisfaction can \nbe established with respect to these bounds. Performance Tuning There are many commercial tools available \nthat assist in .nding hot spots in code [1, 23, 24, 26, 27]. These tools collect either trace or the \nsame sort of call stack sample information that we rely upon, and present views that aid in the identi.cation \nof hot methods. When based on full trace information, they have the potential for identifying .ner-grained \nperformance problems, at the expense of considerable instrumentation overhead. Because these tools focus \non execution time it is dif.cult to use them to analyze idle time. Some tools focus exclusively on certain \naspects of idle time, such as identifying lock contention [25]. Analytical Approaches One approach to \nunderstanding performance problems, and planning out solutions, con\u00adstructs analytical models. These \nmodels take the form of parametrized formulas whose output predicts the service times of the application \n[4, 8, 13, 17, 22]. Queuing Network Models Queuing network models [14] model resources as queues with \na capacity and a service time. Capacity planning tools centered around queuing net\u00adworks [6, 21, 28], \noften require a large degree of customiza\u00ad tion. Users must construct the model s topology, and assert \nthe properties each resource (queued or gating, and the ele\u00adments of the Kendall s notation [20], e.g. \nthe discipline for prioritizing work). Then follows a period of tweaking to en\u00adsure a good .t for the \npredictive formulae. Though quite a bit of work, this offers the potential for precise predictions. Just \nas we have discussed batch and server workloads, other work has identi.ed the vital importance of the \nnature of the workload to model [19]. There are alternative state-transition models that are un\u00adsuited \nfor our problem. Petri Nets [18] are often used to model distributed processes. The notion of colored \ntokens were the inspiration for our colored marbles. With Petri Nets tokens transition only as a result \nof demand exceeding a threshold, which does not seem to match the tuning action simulation we need. In \nthe operations research community, conceptual models such as the theory of constraints [5] of\u00ad fers a \nclose analog to the computational model of queueing networks. As with Petri Nets, the theory of constraints \nis in\u00adtended to model consumption and .ow in a way that is not well suited for capturing bottlenecks \nin computing systems. 12. Conclusions We have introduced a system for planning performance opti\u00admizations \nand resource provisioning. We have demonstrated that it works well, on both a suite of predictable microbench\u00admarks, \nand 15,000 data sets from the wild. Through use of this system within IBM, we have been able to track \nthe role of our users. We have found them to be a satisfying mix of developers, testers, operations, \nand support teams and even the occasional manager. The Marbles Capacity Planner has been implemented, \nand the public is free to use it: http://wait.ibm.com. You may view the report gallery (or analyze your \nown applica\u00adtions), and, from there, view the tuning plans recommended by the planner. As discussed in \nSection 10.1, even if you do not view the plans themselves, the user interface is .ltered based on the \nplanner s evaluation of the hasNoBene.t prop\u00aderty.  The design of the UI you will see is an important \ntopic of future discussion: we feel strongly that any planning sys\u00adtem needs a good way to communicate \nits .ndings. Among the important topics is plan ranking, introduced in subsub\u00ad section 10.2.2: how can \nwe guide the user towards what is important? We have also completed support for tuning plans that cross \nmachine boundaries (such as adding CPUs to the bank of database machines), and are studying the effect \nof insuf.cient information on the quality of the predictions. A complete discussion of our modeling of \ndisk resources and connection pools have also been omitted from these discus\u00adsions, and can be presented \nin future work. References [1] W. P. Alexander, R. F. Berry, F. E. Levine, and R. J. Urquhart. A unifying \napproach to performance analysis in the java envi\u00adronment. IBM Systems Journal, 39(1), 2000. [2] E. Altman, \nM. Arnold, S. Fink, and N. Mitchell. Performance analysis of idle programs. In Object-oriented Programming, \nSystems, Languages, and Applications, pages 739 753, 2010. [3] D. H. Bailey. Little s law and high performance \ncomputing. Technical report, In RNR Technical Report, 1997. [4] F. Brosig, S. Kounev, and K. Krogmann. \nAutomated extrac\u00adtion of palladio component models from running enterprise java applications. In VALUETOOLS, \n2009. [5] E. M. Goldratt. Theory of Constraints. North River Press, 1999. [6] L. Grinshpan. Multi-tiered \napplications sizing methodology based on load testing and queuing network models. In Int. CMG Conference. \nComputer Measurement Group, 2008. [7] N. Gunther. Guerrilla Capacity Planning: A Tactical Ap\u00adproach to \nPlanning for Highly Scalable Applications and Ser\u00advices. Springer, 2006. [8] J. Happe, D. Westermann, \nK. Sachs, and L. Kapov. Statistical inference of software performance models for parametric per\u00adformance \ncompletions. In Research into Practice Reality and Gaps, volume 6093, pages 20 35. 2010. [9] J. Hunter \nand W. Crawford. Java servlet programming. O Reilly, Beijing, 1998. [10] Y. Jiang, C. shing Perng, T. \nLi, and R. Chang. Self-adaptive cloud capacity planning. In IEEE International Conference on Services \nComputing (SCC), pages 73 80, June 2012. [11] S. Kounev, K. Bender, F. Brosig, N. Huber, and R. Okamoto. \nAutomated simulation-based capacity planning for enterprise data fabrics. In International ICST Conference \non Simulation Tools and Techniques, pages 27 36, 2011. [12] S. Kounev and A. Buchmann. Simqpn: a tool \nand method\u00adology for analyzing queueing petri net models by means of simulation. Perform. Eval., 63(4):364 \n394, May 2006. [13] S. Kraft, S. Pacheco-Sanchez, G. Casale, and S. Dawson. Estimating service resource \nconsumption from response time measurements. In VALUETOOLS, 2009. [14] E. D. Lazowska, J. Zahorjan, G. \nS. Graham, and K. C. Sevcik. Quantitative System Performance. Prentice-Hall, 1984. [15] J. D. C. Little. \nA proof for the queueing formula: L = W . Operations Research, 9(3):383 387, 1961. [16] Z. Liu, L. Wynter, \nC. H. Xia, and F. Zhang. Parameter inference of queueing models for it systems using end-to-end measurements. \nPerform. Eval., 63(1):36 60, Jan. 2006. [17] G. Paci.ci, W. Segmuller, M. Spreitzer, and A. Tantawi. \nDy\u00adnamic estimation of cpu demand of web traf.c. In VALUE-TOOLS, 2006. [18] J. L. Peterson. Petri Net \nTheory and the Modeling of Systems. Prentice Hall, 1981. [19] B. Schroeder, A. Wierman, and M. Harchol-Balter. \nOpen versus closed: a cautionary tale. In Proceedings of the 3rd conference on Networked Systems Design \n&#38; Implementation -Volume 3, NSDI 06, pages 18 18, Berkeley, CA, USA, 2006. USENIX Association. [20] \nV. Singh. System Modeling and Simulation. New Age Inter\u00adnational, 2009. [21] TeamQuest. TeamQuest Model \nsoftware. [22] B. Urgaonkar, G. Paci.ci, P. Shenoy, M. Spreitzer, and A. Tantawi. Analytic modeling of \nmultitier internet applica\u00adtions. ACM Trans. Web, 1(1), May 2007. [23] Borland Software Corporation. \nOptimizeItTM Suite. [24] Compuware. Compuware Vantage Analyzer. [25] IBM. Thread and Monitor Dump Analyzer \nfor Java. http: //www.alphaworks.ibm.com/tech/jca. [26] Sun Microsystems. HPROF JVM pro.ler. http: //java.sun.com/developer/technicalArticles/ \nProgramming/HPROF.html, 2005. [27] Yourkit LLC. Yourkit pro.ler. [28] L. Zhu, Y. Liu, N. B. Bui, and \nI. Gorton. Revel8or: Model driven capacity planning tool suite. In ICSE, 2007.    \n\t\t\t", "proc_id": "2509136", "abstract": "<p>When resolving performance problems, a simple histogram of hot call stacks does not cut it, especially given the highly fluid nature of modern deployments. Why bother tuning, when adding a few CPUs via the management console will quickly resolve the problem? The findings of these tools are also presented without any sense of context: e.g. string conversion may be expensive, but only matters if it contributes greatly to the response time of user logins.</p> <p>Historically, these concerns have been the purview of <i>capacity planning</i>. The power of planners lies in their ability to weigh demand versus capacity, and to do so in terms of the important <i>units of work</i> in the application (such as user logins). Unfortunately, they rely on measurements of rates and latencies, and both quantities are difficult to obtain. Even if possible, when all is said and done, these planners only relate to the code as a black-box: but, why bother adding CPUs, when easy code changes will fix the problem?</p> <p>We present a way to do planning <i>on-the-fly</i>: with a few call stack samples taken from an already-running system, we predict the benefit of a proposed tuning plan. We accomplish this by simulating the effect of a tuning action upon execution speed and the way it <i>shifts</i> resource demand. To identify existing problems, we show how to generate tuning actions automatically, guided by the desire to maximize speedup without needless expense, and that these generated plans may span resource and code changes. We show that it is possible to infer everything needed from these samples alone: levels of resource demand and the units of work in the application. We evaluate our planner on a suite of microbenchmarks and a suite of 15,000 data sets that come from real applications running in the wild.</p>", "authors": [{"name": "Nick Mitchell", "author_profile_id": "81100359733", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P4290474", "email_address": "nickm@us.ibm.com", "orcid_id": ""}, {"name": "Peter F. Sweeney", "author_profile_id": "81479657966", "affiliation": "IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P4290475", "email_address": "pfs@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509540", "year": "2013", "article_id": "2509540", "conference": "OOPSLA", "title": "On-the-fly capacity planning", "url": "http://dl.acm.org/citation.cfm?id=2509540"}