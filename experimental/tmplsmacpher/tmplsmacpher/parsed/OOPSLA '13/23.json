{"article_publication_date": "10-29-2013", "fulltext": "\n Synthesis Modulo Recursive Functions Etienne Kneuss1 Viktor Kuncak1 Ivan Kuraj1 Philippe Suter1 , 2 \n1 \u00b4 Ecole Polytechnique F\u00b4ed \u00b4erale de Lausanne (EPFL), Switzerland 2IBM T.J. Watson Research Center, \nYorktown Heights, NY, USA {firstname.lastname}@epfl.ch, psuter@us.ibm.com Abstract We describe techniques \nfor synthesis and veri.cation of re\u00adcursive functional programs over unbounded domains. Our techniques \nbuild on top of an algorithm for satis.ability modulo recursive functions, a framework for deductive \nsyn\u00adthesis, and complete synthesis procedures for algebraic data types. We present new counterexample-guided \nalgorithms for constructing veri.ed programs. We have implemented these algorithms in an integrated environment \nfor interac\u00adtive veri.cation and synthesis from relational speci.cations. Our system was able to synthesize \na number of useful recur\u00adsive functions that manipulate unbounded numbers and data structures. Categories \nand Subject Descriptors D.2.4 [Software Engi\u00adneering]: Software/Program Veri.cation; F.3.1 [Logics and \nMeaning of Programs]: Specifying and Verifying and Rea\u00adsoning about Programs Keywords software synthesis; \ninductive learning; satis.a\u00adbility modulo theories 1. Introduction Software construction is a dif.cult \nproblem-solving activity. It remains a largely manual effort today, despite signi.cant progress in software \ndevelopment environments and tools. The development becomes even more dif.cult when the goal is to deliver \nveri.ed software, which must satisfy speci.ca\u00adtions such as assertions, pre-conditions, and post-conditions. \nThe thesis of this paper is that the development of veri.ed software can be helped through tools that \nadd synthesis tech\u00adniques on top of a system with automated veri.cation and error-.nding capabilities. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. Copyrights for components of \nthis work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To \ncopy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, \nIndianapolis, Indiana, USA. Copyright is held by the owner/author(s). Publication rights licensed to \nACM. ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509555 Whereas the construction \nof arbitrarily complex veri.ed software is possible in principle, verifying programs af\u00adter they have \nbeen developed is extremely time-consuming [21, 27] and it is dif.cult to argue that it is cost-effective. \nOur research therefore explores approaches that support in\u00adtegrated software construction and veri.cation. \nAn impor\u00adtant aspect of such approaches are modular veri.cation tech\u00adniques which can check that a function \nconforms to its local speci.cation. In such an approach, the veri.cation of an in\u00addividual function against \nits speci.cation can start before the entire software system is completed and the resulting veri.\u00adcation \ntasks are partitioned into small pieces. As a result, tools can provide rapid feedback that allows speci.cations \nand implementations to be developed simultaneously. Based on such philosophy of continuous rapid feedback, \nwe have developed Leon, a veri.er that quickly detects errors in func\u00adtional programs and reports concrete \ncounterexamples, yet can also prove the correctness of programs [4, 46 48]. We have integrated Leon into \na web-browser-based IDE, result\u00ading in a tool for convenient development of veri.ed pro\u00adgrams [4]. This \nveri.er is the starting point of the tool we present in this paper. Moving beyond veri.cation, we believe \nthat the devel\u00adopment of veri.ed software can bene.t from techniques for synthesis from speci.cations. \nSpeci.cations in terms of re\u00adlational properties generalize existing declarative program\u00adming language \nparadigms by allowing the statement of con\u00adstraints between inputs and outputs [16, 22] as opposed to \nalways specifying outputs as functions from input to out\u00adputs. Unlike deterministic implementations, \nconstraints can be composed using conjunctions, which enables description of the problem as a combination \nof orthogonal requirements. This paper introduces synthesis algorithms, techniques and tools that integrate \nsynthesis into the development pro\u00adcess for functional programs. We present a synthesizer that can construct \nthe bodies of functions starting solely from their contracts. The programs that our synthesizer produces \ntypically manipulate unbounded data types, such as alge\u00adbraic data types and unbounded integers. Thanks \nto the use of deductive synthesis and the availability of a veri.er, when the synthesizer succeeds with \na veri.ed output, the generated code is correct for all possible input values.  Our synthesizer uses \nspeci.cations as the description of the synthesis problems. While it can additionally accept in\u00adput/output \nexamples to illustrate the desired functionality, we view such illustrations as a special form of input/output \nrelation: whereas input/output examples correspond to tests and provide a description of a .nite portion \nof the desired functionality, we primarily focus on symbolic descriptions, which ensure the desired behavior \nover an arbitrarily large or even in.nite domain. From such descriptions, our syn\u00adthesizer can automatically \ngenerate input/output examples when needed, but can also directly transform speci.cations into executable \ncode. A notable degree of automation in our synthesizer comes from synthesis procedures [15, 23, 24], \nwhich compile speci\u00ad .cation fragments expressed in decidable logics. The present work is, in fact, the \n.rst implementation of the synthesis procedure for algebraic data types we previously developed [46]. \nNote however, that, to capture a variety of scenarios in software development, we also support the general \nprob\u00adlem of synthesis from speci.cations expressed in a Turing\u00adcomplete language. We achieve this using \na framework for cost-guided application of deductive synthesis rules, which decompose the problems into \nsubproblems. We have integrated our synthesizer into Leon, where it tightly cooperates with the underlying \nveri.er, allowing it to achieve orders of magnitude better performance than us\u00ading simpler generate-and-test \napproaches. Techniques we use include symbolic transformation based on synthesis pro\u00adcedures, as well \nas synthesis of recursive functions us\u00ading counterexample-guided strategies. We have evaluated a number \nof system architectures and trade-offs between sym\u00adbolic and concrete reasoning in our implementation \nand ar\u00adrived at an implementation that appears successful, despite the large space of possible programs. \nWe thus believe we have achieved a new level of automation for a broad domain of recursive functional \nprograms. We consider as a particular strength of our system that it can synthesize code that satis\u00ad.es \na given relational speci.cation for all values of inputs, and not only given input/output pairs. Despite \naiming at a high automation level, we are aware that any general-purpose automated synthesis procedure \nwill ultimately face limitations: the user may wish to synthesize larger code than the scalability of \nautomated synthesis al\u00adlows, or they may wish to control the structure and not only the observational \nbehavior of the code to be constructed. We therefore deploy the synthesis algorithm as an interactive \nas\u00adsistance that allows the developer to interleave manual and automated development steps. In our system, \nthe developer can decompose a function and leave the subcomponents to the synthesizer, or, conversely, \nthe synthesizer can decom\u00adpose the problem, solve some of the subproblems, and leave the remaining open \ncases for the developer. To facilitate such synergy, we deploy an anytime synthesis procedure, which \nmaintains a ranked list of current problem decompositions. The user can interrupt the synthesizer at \nany time to display the current solution and continue manual development. 1.1 Contributions The overall \ncontribution of this paper is an integrated syn\u00adthesis and development system for automated and interac\u00adtive \ndevelopment of veri.ed programs. A number of tech\u00adniques from deductive and inductive reasoning need \nto come together to make such system usable. Implemented synthesis framework. We developed a de\u00adductive \nsynthesis framework that can accept a given set of synthesis rules and apply them according to a cost \nfunction. The framework accepts 1) a path condition that encodes pro\u00adgram context, and 2) a relational \nspeci.cation. It returns the function from inputs to outputs as a solution, as well as any necessary \nstrengthening of the precondition needed for the function to satisfy the speci.cation. We have deployed \nthe framework in a web-browser-based environment with con\u00adtinuous compilation and the ability to interrupt \nthe synthesis to obtain a partial solution in the form of a new program with a possibly simpler synthesis \nproblem. Within the above framework we have implemented rules for synthesis of algebraic data type equations \nand disequa\u00adtions [46], as well as a number of general rules for decom\u00ad posing speci.cations based on \ntheir logical structure or case splits on commonly useful conditions. Support for recursion schemas and \nsymbolic term genera\u00adtors. One of the main strengths in our framework is a new form of counterexample-guided \nsynthesis that arises from a combination of several rules. A set of built-in recursion schemas can solve \na problem by generating a fresh recursive function. To ensure well\u00adfoundedness we have extended our veri.er \nwith termi\u00adnation checking, and therefore generate only terminating function calls using this rule. \n To generate bodies of functions, we have symbolic term generators that systematically generate well-typed \npro\u00adgrams built from a selected set of operators (such as alge\u00adbraic data type constructors and selectors). \nTo test candi\u00addate terms against speci.cations we use the Leon veri.er. To speed up this search, the \nrule accumulates previously found counterexamples. Moreover, to quickly bootstrap the set of examples \nit uses systematic generators that can enumerate in a fair way any .nite pre.x of a countable set of \nstructured values. The falsi.cation of generated bodies is done by direct execution of code. For this \npur\u00adpose, we have developed a lightweight bytecode compiler for our functional implementation language \n(a subset of Scala), allowing us to use code execution as a component of counterexample search in the \nconstraint solver.  Function generation by condition abduction. We also present and evaluate a new method, \nimplemented as a rule in our framework, for synthesis of recursive functions, with the following properties. \n  The most distinctive aspect of this rule is the handling of conditional expressions. The rule synthesizes \nexpressions by collecting relevant terms that satisfy a notable number of derived test inputs, and then \nsynthesizing predicates that imply the correctness of candidate terms. This is an alternative to relying \non splitting rules to eagerly split the speci.cation according to simple commonly found con\u00additions. \nEffectively, the additional rule performs abduc\u00adtion of conditions until it covers the entire input space \nwith a partition of conditions, where each partition is as\u00adsociated with a term.  Instead of specialized \nterm evaluators, the rule uses a general expression enumerator based on generating all expressions of \na given type [12]. This results in a broad coverage of expressions that the rule can synthesize. The \nrule uses a new lazy enumeration algorithm for such ex\u00adpressions with polynomial-time access to the next \nterm to enumerate [26]. It .lters well-typed expressions us\u00ad ing counterexamples generated from speci.cations \nand previous function candidates, as well as from structured value generators.  Evaluation. We evaluate \nthe current reach of our synthe\u00adsizer in fully automated mode by synthesizing recursive functions on \nnested algebraic data types, including those that perform operations on lists de.ned using a general \nmecha\u00adnism for algebraic data types, as well as on other custom data types (e.g. trees) de.ned by the \nuser. This paper presents a description of all the above techniques and a snapshot of our results. We \nbelieve that the individual techniques are inter\u00adesting by themselves, but we also believe that having \na sys\u00adtem that combines them is essential to understand the poten\u00adtial of these techniques in addressing \nthe dif.cult problem as synthesis. To gain full experience of the feeling of such a development process, \nwe therefore invite readers to explore the system themselves at http://lara.epfl.ch/w/leon 2. Examples \nof Synthesis in Leon We start by illustrating through a series of examples how de\u00advelopers use our system \nto write programs that are correct by construction. We .rst illustrate our system and the nature of interaction \nwith it through familiar operations on (sorted) user-de.ned lists, re.ecting along the way on the useful\u00adness \nof programming with speci.cations. We then present a somewhat more complex example to illustrate the \nreach of automated synthesis steps in our system. 2.1 List Manipulation We start by showing how our \nsystem behaves when synthe\u00adsizing operations on lists. The developer partially speci.es lists using their \neffect on the set of elements. As shown in sealed abstract class List case class Cons(head: Int, tail: \nList) extends List case object Nil extends List def size(lst : List) : Int = lst match {case Nil . 0 \ncase Cons( ,rest) . 1 + size(rest) } ensuring( = 0) def content(lst : List) : Set[Int] = lst match {case \nNil . Set.empty case Cons(e,rest) . Set(i) ++ content(rest) } Figure 1. User-de.ned list structure with \nthe usual size and content abstraction functions. Here and throughout the paper, the content abstraction \ncomputes a set of elements, but it can easily be extended to handle multisets (bags) using the same techniques \n[8] if stronger contracts are desired Figure 1 we start from a standard recursive de.nition of lists \nalong with recursive functions computing their size as a non\u00adnegative integer, and their content as a \nset of integers. Splitting a list. We .rst consider the task of synthesizing the split function as used \nin, e.g., merge sort. As a .rst attempt to synthesize split, the developer may wish to try the following \nspeci.cation: def split(lst : List) : (List,List) = choose { (r : (List,List)) . content(lst) == content(r. \n1) ++ content(r. 2) } Because it tends to generate simpler solutions before more complex ones, Leon here \ninstantly generates the following function: def split(lst : List) : (List,List) = (lst, Nil) Although \nit satis.es the contract, it is not particularly useful. This does show the dif.cult in using speci.cations, \nbut the advantage of a synthesizer like ours is that it allows the de\u00adveloper to quickly re.ne the speci.cation \nand obtain a more desirable solution. To avoid getting a single list together with an empty one, the \ndeveloper re.nes the speci.cation by en\u00adforcing that the sizes of the resulting lists should not differ \nby more than one: def split(lst : List) : (List,List) = choose { (r : (List,List)) . content(lst) == \ncontent(r. 1) ++ content(r. 2) &#38;&#38; abs(size(r. 1)-size(r. 2)) = 1 } Again, Leon instantly generates \na correct, useless, program: def split(lst : List) : (List,List) = (lst, lst) We can re.ne the speci.cation \nby stating that the sum of the sizes of the two lists should match the size of the input one: def split(lst \n: List) : (List,List) = choose { (r : (List,List)) . content(lst) == content(r. 1) ++ content(r. 2) &#38;&#38; \nabs(size(r. 1) - size(r. 2)) = 1 def isSorted(lst : lst) : Boolean = lst match {  case Nil . true case \nCons( , Nil) . true case Cons(x1, xs @ Cons(x2, )) . x1 = x2 &#38;&#38; isSorted(xs) } def insertSorted(lst \n: List, v: Int): List = { require(isSorted(lst)) choose { (r: List) . isSorted(r) &#38;&#38; content(r) \n== content(lst) ++ Set(v) } } def sort(lst : List): List = choose { (r: List) . isSorted(r) &#38;&#38; \ncontent(r) == content(lst) } Figure 2. Speci.cation of sorting suitable for insertion sort def insertSorted(lst: \nList, v: Int): List = {require(isSorted(lst)) lst match { case Nil . Cons(v, Nil) case Cons(h, tail) \n. val r = insertSorted(t, v) if (v > h) Cons(h, r) else if (h == v) r else Cons(v, Cons(h, t)) } def \nsort(lst : List): List = lst match {case Nil . Nil case Cons(h, t) . insertSorted(sort(t), h) } Figure \n3. Synthesized insertion sort for Figure 2 &#38;&#38; (size(r. 1) + size(r. 2)) == size(lst) } We then \n.nally obtain a useful split function: def split(lst: List): (List, List) = lst match { case Nil . (Nil, \nNil) case Cons(h, Nil) . (Nil, Cons(h, Nil)) case Cons(h1, Cons(h2, t2)) . val r = split(t2) (Cons(h1, \nr. 1), Cons(h2, r. 2)) } We observe that in this programming style, users can write (or generate) code \nby conjoining orthogonal requirements, such as constraints on the sizes and contents, which are only \nindirectly related. The rapid feedback makes it possible to go through multiple candidates rapidly, strengthening \nthe speci.cation as required. We believe this rapid feedback is mandatory when de\u00adveloping from speci.cations. \nOne reason is that, since con\u00adtracts are typically partial, results obtained from under\u00adspeci.cations \ncan be remote from the desired output. Thus, a desirable strategy is to rapidly iterate and re.ne speci.ca\u00adtions \nuntil the output matches the expectations. Insertion sort. Sorting is an example often used to illus\u00adtrate \ndeclarative descriptions of problems. We therefore con\u00adtinue this overview of Leon s synthesis capabilities \nby show\u00ading how it synthesizes an implementation of several sorting algorithms, starting from insertion \nsort. Figure 2 shows the speci.cation of the problem. From this, Leon generates the solution in Figure \n3 within seconds and without further hints. Advantages of Speci.cations. Comparing Figure 2 and Figure \n3, which have similar size, the reader may wonder what we have gained by using speci.cations instead \nof im\u00adplementations. Whereas only widespread use of synthesis systems will give the true answer, we anticipate \nat least three reasons (with 3. partly following from 2.): 1. .exibility: by supporting synthesis from \nspeci.cations, we do not eliminate the ability to directly write imple\u00admentations when this is more desirable, \nbut rather add the freedom and the expressive power to describe prob\u00adlems in additional ways that may \nbe appropriate; the new mechanism does not harm performance of readability when not used; 2. narrower \ngap between requirements and software: natural language and mathematical descriptions of struc\u00adtures \noften have the form of conjunctions that more di\u00adrectly map to choose constructs than to recursive func\u00adtions \nthat compute the precise objects. We view the sort\u00ading process as one of the many possible ways of obtain\u00ading \na collection that has (1) the same elements and (2) is sorted, as opposed to thinking a particular sorting \nalgo\u00adrithm. 3. reusability when introducing new operations: once we specify key invariants and abstraction \nfunctions, we can reuse them to de.ne new versions of these operations; a related concept is the ability \nto express orthogonal requirements independently [14].  We next illustrate the last point using examples \nof reusabil\u00adity as we add new operations: synthesizing removal from a sorted list given the speci.cation \nfor insertion, and synthe\u00adsizing merge sort given a speci.cation for sort. Removal and merge for sorted \nlists. Suppose that, after synthesizing insertion into a sorted list, the developer now wishes to specify \nremoval and merge of two sorted lists. Fig\u00adure 4 shows the speci.cation of these operations. Note that, \nonce we have gone though the process of de.ning the invari\u00adant for what a sorted list means using function \nisSorted in Figure 2, to specify these two new operations we only need to write the concise speci.cation \nin Figure 4. The system then automatically synthesizes the full implementations in Figure 5. We expect \nthat the pay-off from such re-use grows as the complexity of structures increases. Merge sort. Suppose \nnow that the developer wishes to en\u00adsure that the system, given sorting speci.cation, synthesizes merge \nsort instead of insertion sort. To do this, the devel\u00adoper may then, instead of the insertSorted function, \ntry to introduce the function merge into the scope. In our current version of the system, Leon then synthesizes \nthe following code in less than ten seconds:  def delete(in1: List, v: Int) = {require(isSorted(in1)) \nchoose { (out : List) . isSorted(out) &#38;&#38; (content(out) == content(in1) --Set(v)) } } def merge(in1: \nList, in2: List) = {require(isSorted(in1) &#38;&#38; isSorted(in2)) choose { (out : List) . isSorted(out) \n&#38;&#38; (content(out) == content(in1) ++ content(in2)) } } Figure 4. Speci.cation of removal from \na sorted list. def delete(in1: List, v: Int): List = {require(isSorted(in1)) in1 match { case Nil . Nil \ncase Cons(h, t) . { if (v == h) delete(t, v) else Cons(h, delete(t, v)) }} } ensuring {(out : List) \n. isSorted(out) &#38;&#38; (content(out) == content(in1) --Set(v))} def merge(in1: List, in2: List): \nList = {require(isSorted(in1) &#38;&#38; isSorted(in2)) in1 match { case Nil . Nil case Cons(h, t) . \nunion(t, insertSorted(in2, h)) }} ensuring {(out : List) . isSorted(out) &#38;&#38; (content(out) == \ncontent(in1) ++ Set(v))} Figure 5. Implementation synthesized for Figure 4 def sort(lst : List): List \n= lst match { case lst @ Nil . lst case Cons(h, tail) . merge(Cons(h, Nil), sort(tail)) } Although a \nvalid synthesis output according to the given contract, the result is actually still an implementation \nof the insertion sort algorithm, because merge is called on a list that is split in a systematically \nunbalanced way. Even if the split function we synthesized or implemented before is in the scope, the \nsystem may decide not to use it in the generated code. Interactive synthesis and veri.ed refactoring. \nIn situation such as above, where more control is needed, we allow the developer to re.ne the code, either \nthough manual edits or by applying synthesis rules in the form of veri.ed refactoring steps (such as \nthose around which entire systems were built [6]). For example, using our long induction rule , which \nsolves the problem by induction on the structure of list with two base cases, with two clicks the user \ncan re.ne the speci\u00ad.cation into a recursion pattern, then insert manually a local invocation of split. \nThe result is the following: def sort(lst : List): List = lst match {case Nil . choose { (res : List) \n. contents(res) == contents(list) &#38;&#38; isSorted(res) } case Cons( , Nil) . choose { (res : List) \n. contents(res) == contents(list) &#38;&#38; isSorted(res) } case . { val p = split(list) choose { (res \n: List) . contents(res) == contents(list) &#38;&#38; isSorted(res) }}} From this point, Leon can perform \nsynthesis with automatic search by .lling out the implementation of the introduced cases individually, \nto synthesize the following code, in less than .fteen seconds: def sort(lst : List): List = lst match \n{case Nil . lst case Cons( , Nil) . lst case . { val p = split(list) merge(sort(p.fst), sort(p.snd)) \n }} This is a valid code implementing the merge sort algorithm.  2.2 Address Book We next illustrate \nautomated synthesis of somewhat larger user code from speci.cations. In this case, the user de.nes their \nown structure of addresses with relevant information encapsulated in a separate Scala case class. (Note \nthat we replace de.nitions that are clear from the context by ellipses. The full code is in available \nin the Appendix.) case class Info( address: Int, zipcode: Int, local: Boolean ) case class Address(info: \nInfo, priv: Boolean) Addresses are categorized as private or business, accord\u00ading to a boolean .ag, \nand each address book structure con\u00adtains two lists, one per each category. sealed abstract class List \ncase class Cons(a: Address, tail: List) extends List case object Nil extends List case class AddressBook(business: \nList, pers: List) After de.ning some simple operations and predicates: def content(l: List) : Set[Address] \n= . . . def size(l: List) : Int = . . . def size(ab: AddressBook): Int = size(ab.business) + size(ab.pers) \ndef isEmpty(ab: AddressBook) = size(ab) == 0 def content(ab: AddressBook) : Set[Address] = content(ab.pers) \n++ content(ab.business)  def makeAddressBook(l: List): AddressBook = l match {case Nil . AddressBook(Nil, \nl) case Cons(a,tail) . if (allPrivate(l)) {AddressBook(Nil, l) } else if (allPrivate(tail)) {AddressBook(Cons(a, \nNil), tail) } else if (a.priv) {AddressBook(makeAddressBook(tail).business, Cons(a, makeAddressBook(tail).pers)) \n} else {AddressBook(Cons(a, makeAddressBook(tail).business), makeAddressBook(tail).pers) }} Figure 6. \nSynthesized AddressBook.make example and de.ning an invariant that should hold for each valid address \nbook: def allPrivate(l: List): Boolean = . . . def allBusiness(l: List): Boolean = . . . def invariant(ab: \nAddressBook) = allPrivate(ab.pers) &#38;&#38; allBusiness(ab.business) the user can formulate a synthesis \nproblem for constructing an address book from a list of addresses as: def makeAddressBook(l: List): AddressBook \n= choose { (res: AddressBook) . size(res) == size(l) &#38;&#38; invariant(res) } After less than nine \nseconds our system synthesizes the solution in Figure 6, which has around 50 syntax tree nodes. The solution \nis not minimal, but is correct; the synthesizer introduced two additional branches due to eager approach \nto infer branches of interest. If instead of invoking the synthesizer at this point, the user had decided \nto introduce two helper functions that add an address to the private and business category of an address \nbook respectively: def addToPers(ab: AddressBook, adr: Address) = AddressBook(ab.business, Cons(adr, \nab.pers)) def addToBusiness(ab: AddressBook, adr: Address) = AddressBook(Cons(adr, ab.business), ab.pers) \nthe synthesizer would have found a more compact solution def makeAddressBook(l: List): AddressBook = \nl match { case Nil . AddressBook(Nil, l) case Cons(a,tail) . if (a.priv) { addToPers(makeAddressBook(tail), \na) } else { addToBusiness(makeAddressBook(tail), a) }} in less than .ve seconds. Among other transformations \non address books is merg\u00ading two address books which can be implemented with the help of a merge function \nsimilar as de.ned in previous ex\u00adamples: def merge(l1: List, l2: List): List = . . . Leon solves the \nproblem given in the following de.nition: def mergeAddressBooks(ab1: AddressBook, ab2: AddressBook) = \n{ require(invariant(ab1) &#38;&#38; invariant(ab2)) choose { (res: AddressBook) . invariant(res) &#38;&#38; \n(sizeA(res) == sizeA(ab1) + sizeA(ab2)) }} in less than nine seconds while outputting a compact and \nvalid solution: def merge(l1: List, l2: List): List = AddressBook(merge(ab1.business, ab2.business), \nmerge(ab2.pers, ab1.pers)) 3. Background: the Veri.er within Leon The results presented in this paper \nfocus on the synthesis component of Leon. They rely in important ways on the un\u00adderlying veri.er which \nwas the subject of previous work [4, 48]. The language of Leon is a subset of Scala, as il\u00ad lustrated \nthrough the examples of Section 2. Besides inte\u00ad gers and user-de.ned recursive data types, Leon supports \nbooleans, sets and maps. Solver algorithm. At the core of Leon s veri.er is an al\u00adgorithm to reason about \nformulas that include user-de.ned recursive functions, such as size, content, and isSorted in Section \n2. The algorithm proceeds by iteratively examin\u00ad ing longer and longer execution traces through the recursive \nfunctions. It alternates between an over-approximation of the executions, where only unsatis.ability \nresults can be trusted, and an under-approximation, where only satis.ability re\u00adsults can be concluded. \nThe status of each approximation is checked using the state-of-the-art SMT solver Z3 from Mi\u00adcrosoft \nResearch [7]. The algorithm is a semi-decision pro\u00adcedure, meaning that it is theoretically complete \nfor coun\u00adterexamples: if a formula is satis.able, Leon will eventually produce a model [48]. Additionally, \nthe algorithm works as a decision procedure for a certain class of formulas [47]. In the past, we have \nused this core algorithm in the con\u00adtext of veri.cation [48], but also as part of an experiment in providing \nrun-time support for declarative programming us\u00ading constructs similar to choose [22]. We have in both \ncases found the performance in .nding models to be suitable for the task at hand. 1 1 We should also \nnote that since the publication of [48], our engineering efforts as well as the progress on Z3 have improved \nrunning times by 40%.  Throughout this paper, we will assume the existence of an algorithm for deciding \nformulas containing arbitrary recur\u00adsive functions. Whenever completeness is an issue, we will mention \nit and describe the steps to be taken in case of, e.g. timeout. Compilation-based evaluator. Another \ncomponent of Leon on which we rely in this paper is an interpreter based on on-the-.y compilation to \nthe JVM. Function de.nitions are typically compiled once and for all, and can therefore be optimized \nby the JIT compiler. This component is used dur\u00ading the search in the core algorithm, to validate models \nand to sometimes optimistically obtain counterexamples. We use it to quickly reject candidate programs \nduring synthesis (see sections 5 and 6). Ground term generator. Our system also leverages Leon s generator \nof ground terms and its associated model .nder. Based on a generate-and-test approach, it can generate \nsmall models for formulas by rapidly and fairly enu\u00admerating values of any type. For instance, enumerating \nLists will produce a stream of values Nil(), Cons(0, Nil()), Cons(0, Cons(0, Nil())), Cons(1, Nil()), \n. . . 4. Deductive Synthesis Framework The approach to synthesis we follow in this paper is to derive \nprograms by a succession of independently validated steps. In this section, we brie.y describe the formal \nreasoning behind these constructive steps and provide some illustrative examples. Whereas an earlier \n(purely theoretical) version of the framework was presented in [15], the new framework supports the notion \nof path condition, and is the .rst time we report on the practical realization of this framework. 4.1 \nSynthesis Problems A synthesis problem is given by a predicate describing a desired relation between \na set of input and a set of output variables, as well as the context (program point) at which the synthesis \nproblem appears. We represent such a problem as a quadruple [a\u00af(. l f) x\u00af] where: a\u00afdenotes the set \nof input variables,  x\u00afdenotes the set of output variables,  f is the synthesis predicate, and  . \nis the path condition to the synthesis problem.  The free variables of f must be a subset of a\u00af. x\u00af. \nThe path condition denotes a property that holds for input at the program point where synthesis is to \nbe performed, and the free variables of . should therefore be a subset of a\u00af. As an example, consider \nthe following call to choose: def f(a : Int) : Int = {if(a = 0) { choose((x : Int) . x = 0 &#38;&#38; \na + x = 5) } else . . . } The representation of the corresponding synthesis problem is: [a (a = 0 l x \n= 0 . a + x = 5) x] (1)  4.2 Synthesis Solutions We represent a solution to a synthesis problem as a \npair \u00af (P | T ) \u00af where P is the precondition, and T is the program term. \u00af The free variables of both \nP and T must range over a\u00af. The intuition is that, whenever the path condition and the precon\u00ad \u00af dition \nare satis.ed, evaluating f[ \u00afx . T ] should evaluate to \u00af true, i.e. T are realizers for a solution to \nx\u00afin f given the inputs a\u00af. Furthermore, for a solution to be as general as pos\u00adsible, the precondition \nmust be as weak as possible. Formally, for such a pair to be a solution to a synthesis problem, denoted \nas \u00af [a\u00af(. l f) x\u00af] f (P | T ) the following two properties must hold: Relation re.nement: \u00af . . P |= \nf[ \u00afx . T ] This property states that whenever the path-and precon\u00addition hold, the program T\u00afcan be \nused to generate values for the output variables x\u00afsuch that the predicate f is sat\u00adis.ed. Domain preservation: \n. . (.x\u00af: f) |= P This property states that the precondition P cannot ex\u00adclude inputs for which an output \nwould exist such that f is satis.ed. As an example, a valid solution to the synthesis problem (1) is \ngiven by: (a = 5 | 0) The precondition a = 5 characterizes exactly the input values for which a solution \nexists, and for all such values, the constant 0 is a valid solution term for x. Note that the solution \nis in general not unique; alternative solutions for this particular problem include, for example, (a \n= 5 |5 - a), or (a = 5 | if(a < 5) a + 1 else 0). A note on path conditions. Strictly speaking, declaring \nthe path condition separately from the precondition does not add expressive power to the representation \nof synthe\u00adsis problems: one can easily verify that the space of solu\u00adtion terms for [a\u00af(. l f) x\u00af] is \nisomorphic to the one for [a\u00af(true l . . f) x\u00af]. Note however that with the second representation, solving \nthe synthesis problem would always result in a precondition at least as strong as ., since it ap\u00adpears \nin the synthesis predicate and does not contain any out\u00adput variables. Keeping the path condition explicit \nthus avoids computing this redundant information, an effect we found to be important in our implementation. \n  4.3 Inference Rules for Synthesis Building on our correctness criteria for synthesis solutions, we \nnow describe inference rules for synthesis. Such rules de\u00adscribe relations between synthesis problems, \ncapturing how some problems can be solved by reduction to others. We have shown in previous work how \nto design a set of rules to ensure completeness of synthesis for a well-speci.ed class of formulas, e.g. \ninteger linear arithmetic relations [23] or simple term algebras [15]. In the interest of remaining self\u00ad \ncontained, we shortly describe some generic rules. We then proceed to presenting inference rules which \nallowed us to derive synthesis solutions to problems that go beyond such decidable domains. The validity \nof each rule can be established independently from its instantiations, or from the contexts in which \nit is used. This in turn guarantees that the programs obtained by successive applications of validated \nrules are correct by construction. Generic reductions. As a .rst example, consider the rule ON E -P O \nI N T in Figure 7. It reads as follows; if the predicate of a synthesis problem contains a top-level \natom of the form x0 = t, where x0 is an output variable not appearing in the term t, then we can solve \na simpler problem where t is \u00af substituted for x0, obtain a solution (P | T ) and reconstruct a solution \nfor the original one by .rst computing the value for t and then assigning as the result for x0 . Conditionals. \nIn order to synthesize programs that include conditional expressions, we need rules such as CA S E -S \nP L I T in Figure 7. The intuition behind CA S E -S P L I T is that a dis\u00adjunction in the synthesis predicate \ncan be handled by an if\u00adthen-else expression in the synthesized code, and each sub\u00adproblem (corresponding \nto predicates f1 and f2 in the rule) can be treated separately. As one would expect, the precon\u00addition \nfor the .nal program is obtained by taking the disjunc\u00adtion of the preconditions for the subproblems. \nThis matches the intuition that the disjunctive predicate should be realiz\u00adable if and only if one of \nits disjuncts is. Note as well that even though the disjunction is symmetrical, in the .nal pro\u00adgram \nwe necessarily privilege one branch over the other one. This has the interesting side-effect that we \ncan, as shown in the rule, add the negation of the precondition P1 to the path condition of the second \nproblem. This has the poten\u00adtial of triggering simpli.cations in the solution of f2. An extreme case \nis when the .rst precondition is true and the else branch becomes unreachable. The CA S E -S P L I T \nrule as we presented it applies to dis\u00adjunctions in synthesis predicates. We should note that it is sometimes \ndesirable to explicitly introduce such disjunc\u00adtions. For example, our system includes rules to introduce \nbranching on the equality of two variables, to perform case analysis on the types of variables (pattern-matching), \netc. These rules can be thought of as introducing .rst a disjunct, e.g. a = b . a = b, then applying \nCA S E -S P L I T. Recursion schemas. We now show an example of an in\u00adference rule that produces a recursive \nfunction. A common paradigm in functional programming is to perform a compu\u00adtation by recursively traversing \na structure. The rule LI S T-R E C captures one particular form of such a traversal for the List recursive \ntype used in the examples of Section 2. The goal of the rule is to derive a solution consists of a single \nin\u00advocation to a recursive function rec. The recursive function has the following form: def rec(a0, a\u00af) \n= { require(.2) a0 match { \u00af case Nil . T1 case Cons(h, t) . lazy val r\u00af= rec(t, a\u00af) \u00af T2 } } ensuring(r\u00af. \nf[ \u00afx . r\u00af]) where a0 is of type List. The function iterates over the list a0 while preserving the rest \nof the input variables (the envi\u00adronment) a\u00af. Observe that its postcondition corresponds ex\u00adactly to \nthe synthesis predicate of the original problem. The premises of the rule are as follows. The condition \n(.1 . P ) =. .2 ensures that the initial call to rec in the .nal program satisfy its precondition. We \ncan often let P = true and .2 = .1.  The condition .2[a0 . Cons(h,t)] =. .2[a0 . t] states that the \nprecondition of rec should be inductive, i.e. whenever it holds for a list, it should also hold for \nits tail. This is necessary to ensure that the recursive call will satisfy the precondition.  The subproblem \n[a\u00af(.2 l f[a0 . Nil]) x\u00af] corresponds to the base case (Nil), and thus does not contain the input variable \na0.  The .nal subproblem is the most interesting, and corre\u00adsponds to the case where a0 is a Cons, represented \nby the fresh input variables h and t. Because the recursive structure is .xed, we can readily represent \nthe result of the invocation rec(t,a\u00af) by another fresh variable r. We can assume that the postcondition \nof rec holds for that particular call, which we represent in the path condition as f[a0 . t, x\u00af . r\u00af]. \nThe rest of the problem is obtained by substituting a0 for Cons(h,t) in the path condition and in the \nsynthesis predicate.   \u00af [a\u00af(. l f[x0 . t]) x\u00af] f (P | T ) x0 ./vars(t) M |= f vars(f) n a\u00af= \u00d8 ONE \n-P O I NT GRO U N D \u00af [a\u00af(. l x0 = t . f) x0 , x\u00af] f (P | val x\u00af:= T ; (t , x\u00af)) [a\u00af(. l f) x\u00af] f (true \n| M) \u00af\u00af CA S E-S P L IT [a\u00af(. l f1) x\u00af] f (P1 | T1) [a\u00af(. . \u00acP1 l f2) x\u00af] f (P2 | T2) [a\u00af(. l f1 . f2) \nx\u00af] f (P1 . P2 | if(P1) {T\u00af1} else {T\u00af2}) \u00af (.1 . P ) =. .2 .2[a0 . Cons(h,t)] =. .2[a0 . t] [a\u00af(.2 \nl f[a0 . Nil]) x\u00af] f (true | T1)\u00af r , h , t , a\u00af(.2[a0 . Cons(h,t)] . f[a0 . t, x\u00af . r\u00af] l f[a0 . Cons(h,t)]) \nx\u00af] f (true | T2) [\u00af LI ST-R E C [a0 , a\u00af(.1 l f) x\u00af] f (P | rec(a0,a\u00af)) Figure 7. Selected synthesis \ninference rules for one-point rule, static computation, and splitting, as well as an illustrative instance \n(for lists) of a general rule for structural recursion on algebraic data types (see text for the de.nition \nof rec) LI S T-R E C generates programs that traverse lists. Our sys-The core idea behind STE is to symbolically \nrepresent tem automatically generates such a rule for each recursive many possible terms (programs), \nand to iteratively prune datatype occurring in the context of synthesis. The rule for them out using \ncounterexamples and test case generation a binary tree type, for example, will spawn a subproblem until \neither 1) a valid term is proved to solve the synthesis for the Leaf case and another problem with two \nrecursive problem or 2) all programs in the search space have been calls for the Node case. Our rule \nfor integers uses a recursion shown to be inadequate. Since we already have rules that scheme that approaches \nzero for both negative and positive take care of introducing branching constructs or recursive integers. \nfunctions, we focus STE on the search for terms consisting only of constructors and calls to existing \nfunctions. Terminal rules. For a synthesis problem to be completely solved, some rules must be terminal, \nthat is, not generate any Recursive generators. We start from a universal non\u00adsubproblem. Terminal rules \nare by de.nition the only ones deterministic program that captures all the (deterministic) which can \nclose a branch in the derivation tree. programs which we wish to consider as potential solutions. One \nsuch example is GRO U N D in Figure 7. The rule We then try to resolve the non-deterministic choices \nin such states that if a synthesis problem does not refer to any input a way that the program realizes \nthe desired property. Resolv\u00advariable, then it can be treated as a satis.ability problem: ing the choices \nconsists in .xing some values in the pro\u00adany model for the predicate f can then be used as a ground gram, \nwhich we achieve by running a counterexample driven solution term for x\u00af. search. All terminal rules \nthat we consider have the form: We describe our non-deterministic programs as a set of recursive non-deterministic \ngenerators. Intuitively, a gener\u00ad \u00af .a\u00af: . =. f[ \u00afx . T ] ator for a given type is a program that produces \narbitrary TER M INA L \u00af [a\u00af(. l f) x\u00af] f (true | T ) values of that type. For instance, a generator \nfor positive in\u00adtegers could be given by: which essentially encodes relation re.nement. Domain def genInt() \n: Int = if(*) 0 else (1 + genInt()) preservation is automatically enforced by considering only where \n* represents a non-deterministic boolean value. Sim\u00ad solutions where the precondition is true, as the \nrule shows. ilarly a non-deterministic generator for the List type could While we could in principle \ndevise algorithms that try to take the form: solve for a program term and a minimal precondition at \nthe same time, we found the approach of deriving the precon-def genList() : List = if(*) Nil else Cons(genInt(), \ngenList()) ditions using non terminal rules only to be suf.cient for our It is not required that generators \ncan produce every value test cases. The remaining challenge is therefore to ef.ciently for a given type; \nwe could hypothesize for instance that our \u00af compute T given . and f. Sections 5 and 6 detail two algo\u00ad \nsynthesis solutions will only need some very speci.c con\u00adrithms to this effect. stants, such as 0, 1 \nor -1. What is more likely is that our synthesis solutions will need to use input variables and ex\u00ad 5. \nSymbolic Term Exploration Rule isting functions. Our generators therefore typically include In this \nsection, we describe the .rst of our two most impor-variables of the proper type that are accessible \nin the synthe\u00adtant terminal rules, which is responsible for closing many of sis environment. Taking these \nremarks into account, if a and the branches in derivation trees. We call it Symbolic Term b are integer \nvariables in the scope, and f is a function from Exploration (STE). Int to Int, a typical generator for \nintegers would be: def genInt() : Int = if(*) 0 else if(*) 1 else if(*) -1 else if(*) a else if(*) b \nelse f(genInt()) From generators to formulas. Generators can in principle be any function with unresolved \nnon-deterministic choices. For the sake of the presentation, we assume that they are .at , that is, they \nconsist of a top-level non-deterministic choice between n alternatives. (Note that the examples given \nabove all have this form.) Encoding a generator into an SMT term is done as fol\u00adlows: introduce for each \ninvocation of a generator an un\u00adinterpreted constant c of the proper type, and for each non\u00addeterministic \nchoice as many boolean variables \u00afb as there are alternatives. Encode that exactly one of the \u00afb variables \nmust be true, and constrain the value of c using the \u00afb variables. Recursive invocations of generators \ncan be handled simi\u00adlarly, by inserting another c variable to represent their value and constraining \nit appropriately. Naturally, these recursive instantiations must stop at some point: we then speak of \nan instantiation depth. As an example, the encoding of the genList generator above with an instantiation \ndepth of 1 and assuming for simplicity that genInt only generates 0 or a is: (b1 . b2) . (\u00acb1 . \u00acb2) \n. b1 . c1 = Nil . b2 . c1 = Cons(c2,c3) . (b3 . b4) . (\u00acb3 . \u00acb4) . b3 . c2 = 0 . b4 . c2 = a . (b5 . \nb6) . (\u00acb5 . \u00acb6) . b5 . c3 = Nil . b6 . c3 = Cons(c4,c5) . \u00acb6 The clauses encode the following possible \nvalues for c1: Nil, Cons(0, Nil) and Cons(a, Nil). Note the constraint \u00acb6 which enforces the instantiation \ndepth of 1, by preventing the values beyond that depth (namely c4 and c5) to participate in the expression. \nFor a given instantiation depth, a valuation for the \u00afb vari\u00adables encodes a determinization of the generators, \nand as a consequence a program. We solve for such a program by running a re.nement loop. Re.nement loop: \ndiscovering programs. Consider a syn\u00adthesis problem [a\u00af(. l f) x\u00af], where we speculate that a generator \nfor the types of x\u00afcan produce a program that real\u00adizes f. We start by encoding the non-deterministic \nexecution of the generator for a .xed instantiation depth (typically, we start with 0). Using this encoding, \nthe problem has the form: \u00af f . B(\u00afa, b, c\u00af) . C(\u00afc, x\u00af) (2) where f is the synthesis problem, B is the \nset of clauses obtained by encoding the execution of the generator and C is a set of equalities tying \nx\u00afto a subset of the c\u00afvariables. Note that by construction, the values for c\u00af(and therefore for x\u00af) \nare uniquely determined when a\u00afand \u00afb are .xed. We start by .nding values for a\u00afand \u00afb such that (2) \nholds. If no such values exist, then our generators at the given instantiation depth are not expressive \nenough to encode a solution to the problem. Otherwise, we extract for the model the values \u00afb0. They \ndescribe a candidate program, which we put to the test. Re.nement loop: falsifying programs. We search \nfor a solution to the problem: \u00af \u00acf . B(\u00afa, b0, c\u00af) . C(\u00afc, x\u00af) (3) Note that \u00afb0 are constants, and \nthat c\u00afand x\u00afare therefore uniquely determined by a\u00afthis intuitively comes from the fact that \u00afb0 encodes \na deterministic program, that c\u00afencodes intermediate values in the execution of that program, and that \nx\u00afencodes the result. With this in mind, it becomes clear that we are really solving for a\u00af. If no such \na\u00afexist, then we have found a program that realizes f and we are done. If on the other hand we can .nd \na\u00af0, then this constitutes an input that witnesses that our program does not meet the speci.cation. In \nthis case, we can o \u00af discard the program by asserting \u00ac b, and going back to (2). Eventually, because \nthe set of possible assignments to \u00afb is .nite (for a given instantiation depth) this terminates. If \nwe have not found a program, we can increase the instantiation depth and try again. When the maximal \ndepth is reached, we give up. Filtering with concrete execution. While termination is in principle guaranteed \nby the successive elimination of pro\u00adgrams in the re.nement loop, the formula encoding the non\u00addeterministic \nterm typically grows exponentially as the in\u00adstantiation depth increases. As the number of candidates \ngrows, the dif.culty for the solver to satisfy (2) or (3) also increases. As an alternative to symbolic \nelimination, we can often use concrete execution on a set of input tests to rule out many programs. To \nexecute these such concrete tests, we .rst generate a set of input candidates that satisfy the path condition. \nFor this, we use Leon s ground term generator (see Section 3). We then test candidate programs on this \nset of inputs. Pro\u00adgrams that fail on at least one input can be discarded. To make testing ef.cient in \nour implementation, we com\u00adpile the expression f[ \u00afx . genX(\u00afb)] on the .y to JVM byte\u00adcode, where genX \ndenotes the non-deterministic generator for the types of x\u00af, and \u00afb are boolean literals that control \nthe decisions in genX. In other words, the expression uses both the inputs a\u00afand an assignment to \u00afb \nto compute whether the program represented by the choices \u00afb succeeds in producing a valid output for \na\u00af. This encoding of all candidate programs into a single ex\u00adecutable function allows us to rapidly test \nand potentially discard hundreds or even thousands of candidates within a fraction of a second. Whenever \nthe number of discarded can\u00addidates is deemed substantial, we regenerate a new formula for (2) with much \nfewer boolean variables and continue from there. The speedup achieved through .ltering with concrete \nexecutions is particularly important when STE is applied to a problem it cannot solve. In such cases, \n.ltering often rules out all candidate programs and symbolic reasoning is never applied.  6. Condition \nAbduction Rule We next describe our second major terminal rule, which synthesizes recursive functions \nwith conditional statements. We refer to the underlying technique (and its implementation as a rule in \nour system) as Condition Abduction (CA). We assume that we are given a function header and a post\u00adcondition, \nand that we aim to synthesize a recursive function body. The body expression must be 1) a well-typed \nterm with respect to the context of the program and 2) valid according to the imposed formal speci.cation. \nAn approach to solv\u00ading such a synthesis problem could be based on searching the space of all expressions \nthat can be built from all decla\u00adrations visible at the program point, .ltering out those that do not \ntype-check or return the desired type, and .nd one that satis.es the given formal speci.cation. Unless \nthe pro\u00adcess of generating candidate solutions is carefully guided, the search becomes unfeasible (as \nwe have observed in pre\u00advious versions of our tool). 6.1 Condition Abduction Our idea for guiding the \nsearch and incremental construction of correct expressions is in.uenced by abductive reasoning [18, 19]. \nAbductive reasoning, sometimes also called infer\u00ad ence to the best explanation , is a method of reasoning \nin which one chooses a hypothesis that would explain the ob\u00adserved evidence in the best way. The motivation \nfor applying abductive reasoning to program synthesis comes from exam\u00adining implementations of practical \npurely functional, recur\u00adsive algorithms. The key observation is that recursive func\u00adtional algorithms \noften consist of a top-level case analysis expression (if-then-else or pattern matching) with recursive \ncalls in the branches. This pattern is encoded with a branch\u00ading control .ow expression that partitions \nthe space of input values such that each branch represents a correct implemen\u00adtation for a certain partition. \nSuch partitions are de.ned by conditions that guard branches in the control .ow. This allows us to synthesize \nbranches separately, by searching for implementations that evaluate correctly only for certain inputs, \nthus reducing the search space. Rather than speculatively applying CA S E -S P L I T rule to obtain sub\u00adproblems \nand .nding solutions for each branch by case anal\u00adysis (as described in Section 4), this idea applies \na similar strategy in the reverse order .rst obtaining a candidate pro\u00adgram and then searching for a \ncondition that makes it correct. Abductive reasoning can guess the condition that de.nes a valid partition, \ni.e. abduce the explanation for a partial im\u00adplementation with respect to a given candidate program. \nOur Algorithm 1 Synthesis with condition abduction Require: path condition ., predicate f, a collection \nof expressions s . synthesis problem [a\u00af(. l f) x\u00af] ' 1: p= true . maintain the current partition 2: \nsol = (.x.x) . maintain a partial solution 3: M = SA M P L E MO D E L S (a\u00af) . set of example models \n4: repeat 5: get a set of expressions E from s . candidates 6: for each e in E do . count passed examples \npe for e 7: pe = |{m . M | e(m) is correct}| . evaluate 8: r\u00af= arg max e.E pe . the highest ranked expression \n9: if solution (. . p' | r\u00af) is valid then 10: return (. | (sol r\u00af)) . a solution is found 11: else 12: \nextract new counterexample model m 13: M = M . m . accumulate examples 14: c = BR A N C H SY N(\u00af. call \nAlgorithm 2 r, p, q, s) 15: if c = FA L S E then . a branch is synthesized 16: sol = (.x. (sol (if c \nthen r\u00afelse x))) ' 17: p= p' . \u00acc . update current partition 18: until s is empty rule CA progressively \napplies this technique and enables ef\u00adfective search and construction of a control .ow expression that \nrepresents a correct implementation for more and more input cases, eventually constructing an expression \nthat is a solution to the synthesis problem.  6.2 Synthesizing Conditional Recursive Functions Algorithm \n1 presents our rule that employs a new technique for guiding the search with ranking and .ltering based \non counterexamples, as well as constructing expressions from partially correct implementations. The algorithm \napplies the idea of abducing conditions to progressively synthesize and verify branches of a correct \nim\u00adplementation for an expanding partition of inputs. The input to the algorithm is a path condition \n., a predicate f (de.ned by synthesis problem [a\u00af(. l f) x\u00af]), and a collection of expressions s (see \nSection 6.3 below for details on how we obtain a suitable s in our implementation). Condition p' de.nes \nwhich inputs are left to consider at any given point in the algorithm; these are the inputs that belong \nto the current partition. The initial value of p' is true, so the algorithm starts with a partition that \ncovers the whole initial input space constrained only by the path con\u00addition .. Let p1, . . . , pk, where \nk > 0, be conditions ab\u00ad ' duced up to a certain point in the algorithm. Then prepre\u00adsents the conjunction \nof negations of abduced conditions, i.e. ' p= \u00acp1 . . . . . \u00acpk. Together with the path condition, it \nde.nes the current partition which includes all input values for which there is no condition abduced \n(nor correct imple\u00admentation found). Thus, the guard condition for the current partition is de.ned by \n. . p'. The algorithm maintains the partial solution sol, encoded as a function. sol encodes an expression \nwhich is correct for all input values that satisfy any of the abduced conditions and this expression \ncan be re\u00adturned as a partial solution at any point. Additionally, the al\u00adgorithm accumulates example \nmodels in the set M. Ground term generator, described in Section 3, is used to construct the initial \nset of models in M. To construct a model, for each variable in a\u00af, the algorithm assigns a value sampled \nfrom the ground term generator. Note that more detailed discussion on how examples are used to guide \nthe search is deferred to Section 6.3.  The algorithm repeats enumerating all possible expres\u00adsions \nfrom the given collection until it .nds a solution. In each iteration, a batch of expressions E is enumerated \nand evaluated on all models from M. The results of such eval\u00aduation are used to rank expressions from \nE. The algorithm considers the expression of the highest rank r\u00afas a candidate solution and checks it \nfor validity. If r\u00afrepresents a correct ' implementation for the current partition, i.e. if (. . p | \nr\u00af)is a valid solution, then the expression needed to complete a valid control .ow expression is found. \nThe algorithm re\u00adturns it as solution for which [a\u00af(. l f) x\u00af] f (. | (sol r\u00af))holds. Otherwise, the \nalgorithm extracts the counterexample model m, adds it to the set M, and continues by trying to synthesize \na branch with expression r\u00af(it does so by calling Algorithm 2 which will be explained later). If BR A \nN C H SY N returns a valid branch condition, the algorithm updates the partial solution to include the \nadditional branch (thus extend\u00ading extending the space of inputs covered by the partial so\u00adlution), and \nre.nes the current partition condition. The new partition condition reduces the synthesis to a subproblem, \nensuring that the solution in the next iteration covers cases where c does not hold. The algorithm eventually, \ngiven the appropriate terms from s, .nds an expression that forms a complete correct implementation for \nthe synthesis problem. Algorithm 2 Synthesize a branch ' Require: expression r\u00af, condition p , predicate \nq, and a collection of expressions s passed from Algorithm 1 ' 1: function BR A N C H SY N(\u00af, q, s) r, \np 2: M ' = \u00d8 set of accumulated counterexamples 3: get a set of expressions E ' from s candidates 4: \nfor each c in E ' do 5: if for each model m in M ' , c(m) = f alse then 6: if solution (. . c | r\u00af) is \nvalid then 7: return c a condition is abduced 8: else 9: extract the new counterexample model m 10: M \n' = M ' . m accumulate counterexamples 11: return FA L S E no condition is found Algorithm 2 tries to \nsynthesize a new branch by abducing a valid branch condition c. It does so by enumerating a set of expressions \nE ' from s and checking whether it can .nd a valid condition expression, that would guard a partition \nfor which the candidate expression r\u00afis correct.The algorithm accumulates counterexample models in M \n' and considers a candidate expression c only if it prevents all accumulated counterexamples. The algorithm \nchecks this by evaluating c on m, i.e. c(m), for each accumulated counterexample m. If a candidate expression \nc is not .ltered out, the algorithm checks if c represents a valid branch condition, i.e. whether (. \n. c | r\u00af) is a valid solution. If yes, the algorithm returns c which, together with r\u00af, comprises a valid \nbranch in the solution to [a\u00af(. l f) x\u00af]. Otherwise, it adds a new coun\u00adterexample model to M ' and continues \nwith the search. If no valid condition is in E ', the algorithm returns FA L S E.  6.3 Organization \nof the Search For getting the collection of expressions s, CA uses a term enumerator that generates all \nwell-typed terms constructible from the datatypes and functions in the scope of the syn\u00adthesis problem \n(in our implementation, we reused the typed expression enumerator from the InSynth tool [12, 26]). The \ncompleteness property of such generators ensures systematic enumeration of all candidate solutions that \nare de.ned by the set of given type constraints. For veri.cation, CA uses the Leon veri.er component, \nthat allows checking validity of expressions that are supported by the underlying theories and obtaining \ncounterexample models. The context of CA as a rule in the Leon synthesis frame\u00adwork imposes limits on \nthe portion of search space explored by each instantiation. This allows incremental and system\u00adatic progress \nin search space exploration and, due to the mix\u00adture with other synthesis rules, offers bene.ts in both \nexpres\u00adsiveness and performance of synthesis. CA offers .exibility in adjusting necessary parameters \nand thus a .ne-grain con\u00adtrol over the search. For our experiments, the size of candi\u00addate sets of expressions \nenumerated in each iteration n is 50 (and doubled in each iteration) for Algorithm 1 and 20 for Algorithm \n2. Using (counter-)examples. A technique that brings signif\u00adicant performance improvements when dealing \nwith large search spaces is guiding the search and avoiding consider\u00ading candidate expressions according \nto the information from examples generated during synthesis. After checking an un\u00adsatis.able formula, \nCA queries Leon s veri.er for the wit\u00adness model. It accumulates such models and uses them to narrow \ndown the search space. Algorithm 2 uses accumulated counterexamples to .lter out unnecessary candidate \nexpressions when synthesizing a branch. It makes sense to consider a candidate expression for a branch \ncondition, c, for a check whether c makes r\u00afa correct implementation, only if c prevents all accumulated \ncounterexamples that already witnessed unsatis.ability of the correctness formula for r\u00af, i.e. if .m \n. M ' . c . \u00acm. Otherwise, if .m . M ' .\u00ac(c . \u00acm), then m is a valid counterexample to the veri.cation \nof (. . c | r\u00af). This ef\u00adfectively guides the search by the results of previous veri\u00ad.cation failures \nwhile .ltering out candidates before more expensive veri.cation check are made.  Algorithm 1 uses accumulated \nmodels to quickly test and rank expressions by evaluating models according to the speci.cation. The current \nset of candidate expressions E is evaluated on the set of accumulated examples M and the re\u00adsults of \nsuch evaluation are used to rank the candidates. We call an evaluation of a candidate e on a model m \ncorrect, if m satis.es path condition . and the result of the evaluation satis.es given predicate f. \nThe algorithm counts the num\u00adber of correct evaluations, ranks the candidates accordingly, and considers \nonly the candidate of the highest rank. The ra\u00adtionale is that the more correct evaluations, the more \nlikely the candidate represents a correct implementation for some partition of inputs. Note that evaluation \nresults may be used only for ranking but not for .ltering, because each candidate may represent a correct \nimplementation for a certain parti\u00adtion of inputs, thus incorrect evaluations are expected even for valid \ncandidates. Because the evaluation amounts to exe\u00adcuting the speci.cation, this technique is ef.cient \nin guiding the search towards correct implementations while avoiding unnecessary veri.cation checks. \n7. Exploring the Space of Subproblems In the previous three sections, we described a general formal framework \nin which we can describe what constitutes a syn\u00adthesis problem and a solution. We have shown rules that \nde\u00adcompose problem into pieces and presented two more com\u00adplex terminal rules that solve larger sub-problems: \nsymbolic term exploration and condition-abduction. In this section, we describe how all these rules are \ninstantiated in practice, and how they work together to derive a complete solution to a synthesis problem. \nInference rules are non-deterministic by nature. They jus\u00adtify the correctness of a solution, but do \nnot by themselves describe how one .nds that solution. Our search for a solu\u00adtion alternates between \nconsidering 1) which rules apply to given problems, and 2) which subproblems are generated by rule instantiations. \nThe task of .nding rules that apply to a problem intu\u00aditively correspond to .nding an inference rule \nwhose con\u00adclusion matches the structure of a problem. For example, to apply GRO U N D, the problem needs \nto mention only out\u00adput variables. Similarly, to apply LI S T-R E C to a problem, it needs to contain \nat least one input variable of type List. Computing the subproblems resulting from the applica\u00adtion of \na rule is in general straightforward, as they corre\u00adspond to problems appearing in its premise. The GRO \nU N D rule, for example, generates no subproblem, while LI S T-R E C generates two. A N D /O R search. \nTo solve one problem, it suf.ces to .nd a complete derivation from one rule application to that prob\u00adlem. \nHowever, to fully apply a rule, we need to solve all Figure 8. An A N D /O R search tree used to illustrate \nour search mechanism. Circles are O R nodes and represent prob\u00adlems, while boxes are A N D nodes and \nrepresent our rule ap\u00adplications. Nodes in grey are closed (solved). generated subproblems. This corresponds \nto searching for a closed branch in an A N D /O R tree [31]. We now describe the expansion of such a \ntree using an example. Consider the problem of removing a given element e from a list a. In our logical \nnotation using a as an abbre\u00adviation for content the problem is: [a , e (true l a(x) = a(a) \\ {e}) x] \nWe denote this problem by 1 in the tree of Figure 8. While we haven t given an exhaustive list of all \nrules used in our system, it is fair to assume that more than one can apply to this problem. For example, \nwe could case-split on the type of a, or apply L I S T-R E C to a. We represent these two options by \nA and B respectively in the tree. Following the option B and applying LI S T-R E C with the path condition \n.2 = true trivially satis.es the .rst two premises of the rules, and generates two new problems (4 and \n5). Problem 5 is: [e (true l a(x) = a(Nil) \\ {e}) x] where the predicate simpli.es to a(x) = \u00d8. This \nmakes it possible to apply the G RO U N D rule (node G). This generates no subproblem, and closes the \nsubbranch with the solution solution (true | Nil). Problem 4 has the form: [r , h , t , e (a(r) = a(t) \n\\ {e}l a(x) = a(Cons(h,t)) \\ {e}) x] Among the many possible rule applications, we can choose to case-split \non the equality h = e (node F). This generates two subproblems. Problem 6 [r , h , t , e (a(r) = a(t) \n\\ {e} . e = hl a(x) = a(Cons(h,t)) \\ {e}) x] and a similar problem 7, where e = h appears in the path \ncondition instead of e = h. Both subproblems can be solved by using a technique we will describe in Section \n5 to derive a term satisfying the synthesis predicate, effectively closing the complete branch from the \nroot. The solutions for prob\u00adlem 6 and 7 are (true | r) and (true | Cons(h,r)) respec\u00adtively. A complete \nreconstruction of the solution given by the branch in gray yields the program:  def rec(a : List) : \nList = a match { case Nil . Nil case Cons(h,t) . val r = rec(t) if(e == h) r else Cons(h,r) } In the \ninterest of space, we have only described the derivations that lead to the solution. In practice, not \nall cor\u00adrect steps are taken in the right order. The interleaving of expansions of A N D and O R nodes \nis driven by the estimated cost of problems and solutions. Cost models. To drive the search, we assign \nto each prob\u00adlem and to each rule application an estimated cost, which is supposed to under-approximate \nthe actual .nal cost of a closed branch. For O R nodes (problems), the cost is sim\u00adply the minimum of \nall remaining viable children, while for A N D nodes (rule applications) we take the sum of the cost \nof each children plus a constant. That constant intuitively corresponds to the extra complexity inherent \nto a particular rule. A perfect measure for cost would be the running time of the corresponding program. \nHowever, this is particularly hard to estimate, and valid under-approximations would most likely be useless. \nWe chose to measure program size instead, as we expect it to be a reasonable proxy for com\u00adplexity. We \nmeasure the size of the program as the number of branches, weighted by their proximity to the root. We \nfound this to have a positive in.uence on the quality of solutions, as it discourages near-top-level \nbranching. Using this metric, the cost inherent to a rule application roughly corresponds to the extra \nbranches it introduces in the program. We use a standard algorithm for searching for the best solution \n[31], and the search thus always focuses on the current most promising solution. In our example in Figure \n8, we could imagine that after the case split at F, the B branch temporarily became less attractive. \nThe search then focuses for a while on the A branch, until expansion on that side (for example, by case-splitting \non the type of the list) reaches a point where the minimal possible solution is worse than the B branch. \nWe note that the complete search takes about two seconds. Anytime synthesis. Because we maintain the \nsearch tree and know the current minimal solution at all times, we can stop the synthesis at any time \nand obtain a partial program that is likely to be good. This option is available in our im\u00adplementation, \nboth from the console mode and the web in\u00adterface. In such cases, Leon will return a program containing \nnew invocations of choose corresponding to the open sub\u00adproblems. Operation Size Calls Proof sec. v List.Insert \n3 0 0.6 v List.Delete 19 1 1.8 v List.Union 12 1 2.1 v List.Diff 12 2 7.6 v List.Split 27 1 9.3 SortedList.Insert \nSortedList.InsertAlways SortedList.Delete SortedList.Union SortedList.Diff SortedList.InsertionSort SortedList.MergeSort \nStrictSortedList.Insert 34 36 23 19 13 10 17 34 1 1 1 2 2 2 4 1 v v v v v v v 9.9 7.2 4.1 4.5 4.0 4.2 \n14.3 14.1 StrictSortedList.Delete StrictSortedList.Union UnaryNumerals.Add UnaryNumerals.Distinct UnaryNumerals.Mult \nBatchedQueue.Checkf BatchedQueue.Snoc 21 19 11 12 12 14 7 1 2 1 0 1 4 2 v v v v v v 15.1 3 1.3 1.1 2.7 \n7.4 3.7 AddressBook.Make 50 14 8.8 AddressBook.MakeHelpers 21 5 4.9 AddressBook.Merge 11 3 8.9 Figure \n9. Automatically synthesized functions using our system. We consider a problem as synthesized if the \nsolu\u00adtion generated is correct after manual inspection. For each generated function, the table lists \nthe size of its syntax tree v and the number of function calls it contains. indicates that the system \nalso found a proof that the generated program matches the speci.cation: in many cases proof and synthe\u00adsis \nare done simultaneously, but in rare cases merely a large number of automatically generated inputs passed \nthe speci\u00ad.cation. The .nal column shows the total time used for both synthesis and veri.cation. Parallelization \nThe formulation of our search over pro\u00adgrams using A N D /O R graphs with independent sub-problems allows \nus to parallelize its exploration. We implemented the parallel search in a system composed of several \nworker ac\u00adtors managed by a central coordinator. The share-nothing ap\u00adproach of the actor model is particularly \nadequate in our case given the independence of sub-problems. For all non-trivial benchmarks, the speed-up \ninduced by concurrent workers exceeds the setup and communication overheads. 8. Implementation and Results \nWe have implemented these techniques in Leon, a system for veri.cation and synthesis of functional program, \nthus extending it from the state described in Section 3. Our im\u00ad plementation and the online interface \nare available from http://lara.epfl.ch/w/leon/.  As the front end to Leon, we use the .rst few phases \nof the reference Scala compiler (for Scala 2.10). The Scala compiler performs parsing, type checking, \nand tasks such as the expansion of implicit conversions, from which Leon directly bene.ts. We then .lter \nout programs that use Scala features not supported in Leon, and convert the syntax trees to Leon s internal \nrepresentation. Leon programs can also execute as valid Scala programs. We have developed several interfaces \nfor Leon. Leon can be invoked as a batch command-line tool that accepts veri.cation and synthesis tasks \nand outputs the results of the requested tasks. If desired, there is also a console mode that allows \napplying synthesis rules in a step-by-step fashion and is useful for debugging purposes. To facilitate \ninteractive experiments and the use of the system in teaching, we have also developed an interface that \nexecutes in the web browser, using the Play framework of Scala as well as JavaScript editors. Our browser-based \nin\u00adterface supports continuous compilation of Scala code, al\u00adlows verifying individual functions with \na single keystroke or click, as well as synthesizing any given choose expres\u00adsion. In cases when the \nsynthesis process is interrupted, the synthesizer can generate a partial solution that contains a program \nwith further occurrences of the choose statement. In order to evaluate our system, we developed bench\u00admarks \nwith reusable abstraction functions. The example sec\u00adtion already pointed out to some of the results \nwe obtained. We next summarize further results and discuss some of the remaining benchmarks. The synthesis \nproblem descriptions are available in the appendix, along with their solution as computed by Leon. Our \nset of benchmarks displayed in Figure 9 covers the synthesis of various operations over custom data structures \nwith invariants, speci.ed through the lens of abstraction functions. These benchmarks use speci.cations \nthat are both easy to understand and shorter than resulting programs (ex\u00adcept in trivial cases). Most \nimportantly, the speci.cation functions are easily reused across synthesis problems. We believe these \nare key factors in the evaluation of any synthe\u00adsis procedure. Figure 9 shows the list of functions we \nsuccessfully syn\u00ad thesized. In addition to the address book and sorted list ex\u00adamples shown in Section \n2, our benchmarks include opera\u00ad tions on unary numerals, de.ned as is standard as zero or successor \n, and on an amortized queue implemented with two lists from a standard book on functional data structure \nimplementation [34]. Each synthesized program has been manually validated to be a solution that a programmer \nmight expect. Synthesis is performed in order, meaning that an op\u00aderation will be able to reuse all previously \nsynthesized ones, thus mimicking the usual development process. For instance, multiplication on unary \nnumerals is synthesized as repeated invocations of additions. Our system typically also proves automatically \nthat the resulting program matches the speci.cation for all inputs. In some cases, the lack of inductive \ninvariants prevents fully\u00adautomated proof of the synthesize code (we stop veri.cation after a timeout \nof 3 seconds). In most cases the synthesis succeeds suf.ciently fast for a reasonable interactive expe\u00adrience. \nAmong the largest benchmarks is synthesis of cre\u00adation of the address book (AddressBook.make), which \nauto\u00admatically derives a function to classify a list into two sub\u00adlists that are inserted into the appropriate \n.elds of an address book. This example is at the frontier of what is possible to\u00adday; our system can \nsynthesize it but would need additional inductive strengthening of the speci.cation to verify it. In \nsuch cases, it is possible that the returned solution is incor\u00adrect for the inputs that our veri.er and \ncounterexample .nder did not consider within a given time limit. On the one hand, this shows a limitation \nwhen requiring fully veri.ed solu\u00adtions. On the other hand, it shows the importance of veri\u00ad.cation, \nand points to another possible use of our system: it can automatically synthesize buggy benchmarks for \nsoft\u00adware testing and veri.cation tools, benchmarks that are close to being correct yet for which it \nis dif.cult to automatically .nd test inputs that witness the buggy behavior. 9. Related Work Our approach \nis similar in the spirit to deductive synthesis [29, 30, 39], which incorporates transformation of speci.\u00ad \ncations, inductive reasoning, recursion schemes and termi\u00adnation checking, but we extend it with modern \nSMT tech\u00adniques, new search algorithms, and a new cost-based syn\u00adthesis framework. The type-driven counterexample-guided \nsynthesis with condition abduction (Section 6) directly uses the complete completion technique [12] including \nthe suc\u00ad cinct representation of types. However, our use adds sev\u00aderal crucial dimensions to the basic \nfunctionality of gener\u00adating well-typed terms: we add mechanisms to ensure that the terms make sense \nsemantically and not only in terms of types, though the use of a veri.er and automatically gen\u00aderated \ncounterexamples. Moreover, this is only our starting point and the main novelty is the addition of the \ninference of conditional recursive programs. We currently do not use the full power of [12] because we \nmake no use of 1) ranking of solutions based on the occurrence of symbols in the corpus nor 2) the ability \nof [12] to generate .rst-class functions (the functions we try synthesize here are .rst-order). The origins \nof our deductive framework is in complete functional synthesis, which was used previously for inte\u00adger \nlinear arithmetic [24]. In this paper we do not use synthesis rules for linear integer arithmetic. Instead, \nwe here use synthesis procedure rules for algebraic data types [15, 46], which were not reported in an \nimplemented sys\u00ad tem before. This gives us building blocks for synthesis of recursion-free code. To synthesize \nrecursive code we devel\u00adoped new algorithms, which build on and further advance the counterexample-guided \napproach to synthesis [40], but applying it to the context of an SMT instead of SAT solver, and using \nnew approaches to control the search space.  Deductive synthesis frameworks. Early work on synthesis \n[29, 30] focused on synthesis using expressive and undecid\u00ad able logics, such as .rst-order logic and \nlogic containing the induction principle. Programming by re.nement has been popularized as a manual activity \n[3, 53]. Interactive tools have been devel\u00ad oped to support such techniques in HOL [6]. A recent exam\u00ad \nple of deductive synthesis and re.nement is the Specware system from Kesterel [39]. We were not able \nto use the sys\u00ad tem .rst-hand due to its availability policy, but it appears to favor expressive power \nand control, whereas we favor au\u00adtomation. A combination of automated and interactive development is \nanalogous to the use of automation in interactive theorem provers, such as Isabelle [33]. However, whereas \nin veri.ca\u00ad tion it is typically the case that the program is available, the emphasis here is on constructing \nthe program itself, starting from speci.cations. Work on synthesis from speci.cations [44] resolves some \nof these dif.culties by decoupling the problem of inferring program control structure and the problem \nof synthesizing the computation along the control edges. The work lever\u00adages veri.cation techniques that \nuse both approximation and lattice theoretic search along with decision procedures, but appears to require \nmore detailed information about the struc\u00adture of the expected solution than our approach. Synthesis \nwith input/output examples. One of the .rst works that addressed synthesis with examples and put induc\u00adtive \nsynthesis on a .rm theoretical foundation is the one by Summers [45]. Subsequent work presents extensions \nof the classical approach to induction of functional Lisp-programs [13, 20]. These extensions include \nsynthesizing a set of equations (instead of just one), multiple recursive calls and systematic introduction \nof parameters. Our current system lifts several restrictions of previous approaches by support\u00ading reasoning \nabout arbitrary datatypes, supporting multiple parameters in concrete and symbolic I/O examples, and \nal\u00adlowing nested recursive calls and user-de.ned declarations. Inductive (logic) programming that explores \nautomatic synthesis of (usually recursive) programs from incomplete speci.cations, most often being input/output \nexamples [9, 32], in.uenced our work. Recent work in the area of pro\u00ad gramming by demonstration has shown \nthat synthesis from examples can be effective in a variety of domains, such as spreadsheets [38]. Advances \nin the .eld of SAT and SMT solvers inspired counter-example guided iterative synthe\u00adsis [11, 40], which \ncan derive input and output examples from speci.cations. Our tool uses and advances these tech\u00adniques \nthrough two new counterexample-guided synthesis approaches. ES C H E R, recently presented inductive \nsynthesis algo\u00adrithm that is completely driven by input/output examples and focuses on synthesis of recursive \nprocedures, shares some similarities with some of our rules [1]. By following the goal graph, which is \nsimilar in function as the A N D /O R search tree, ES C H E R tries to detect if two programs can be \njoined by a conditional. The split goal rule in ES C H E R can specu\u00adlatively split goals and is thus \nsimilar to our splitting rules. One of the differences is that ES C H E R can split goals based on arbitrary \nchoices of satis.ed input/output example pairs, while our rules impose strictly prede.ned conditions \nthat correspond to common branching found in programs. We found it dif.cult to compare the two frameworks \nbecause ES C H E R needs to query the oracle (the user) for input/out\u00adput examples each time a recursive \ncall is encountered (in the SAT U R AT E rule). We do not consider it practical to al\u00adlow the synthesizer \nto perform such extensive querying, be\u00adcause the number of recursive calls during synthesis tends to \nbe very large. Thus, ES C H E R appears more suitable for sce\u00adnarios such as reverse-engineering a black-box \nimplementa\u00adtion from its observable behavior than for synthesis based on user s speci.cation. Our approach \ncomplements the use of SMT solvers with additional techniques for automatic generation of input/out\u00adput \nexamples. Our current approach is domain-agnostic al\u00adthough in principle related to techniques such as \nKorat [5] and UDITA [10]. Synthesis based on .nitization techniques. Program sketching has demonstrated \nthe practicality of program syn\u00adthesis by focusing its use on particular domains [40 42]. The algorithms \nemployed in sketching are typically focused on appropriately guided search over the syntax tree of the \nsynthesized program. The tool we presented shows one way to move the ideas of sketching towards in.nite \ndomains. In this generalization we leverage reasoning about equations as much as SAT techniques. Reactive \nsynthesis. Synthesis of reactive systems gener\u00adates programs that run forever and interact with the envi\u00adronment. \nKnown complete algorithms for reactive synthe\u00adsis work with .nite-state systems [37] or timed systems \n[2]. Finite-state synthesis techniques have applications to control the behavior of hardware and embedded \nsystems or concur\u00adrent programs [50]. These techniques usually take speci.\u00ad cations in a fragment of \ntemporal logic [36] and have re\u00ad sulted in tools that can synthesize useful hardware compo\u00adnents [17]. \nRecently such synthesis techniques have been ex\u00ad tended to repair that preserves good behaviors [51], \nwhich is related to our notion of partial programs that have remaining choose statements. These techniques \nhave been applied to the component-based synthesis problem for .nite-state com\u00adponents [28]; we focus \non in.nite domains, but for simpler, input/output computation model. TR A N S I T combines synthesis \nand model checking to bring a new model for programming distributed protocols [49], which is a challenging \ncase of a reactive system. Spec\u00ad i.cation of a protocol is given with a .nite-state-machine description \naugmented with snippets that can use concrete and symbolic values to capture intended behavior. Similarly \nto our STE rule, the main computational problem solving in TR A N S I T is based on the counter-example \nguided induc\u00adtive synthesis (CEGIS) approach while the execution of con\u00adcrete speci.cation is used to \nprune the parts of the synthesis search space. Although similarity exists between the concept of progressive \nsynthesis of guarded transitions in TR A N S I T and inferring branches in our case splitting and condition \nabduction rules, the crucial difference is that our framework infers the entire implementation, including \nthe control .ow (with recursive calls), without the need of approximate con\u00adtrol .ow speci.cation. On \nthe other hand, the effectiveness of TR A N S I T is increased by focusing on a particular appli\u00adcation \ndomain, which is the direction we will leave for the future.  Automated inference of program .xes and \ncontracts. These areas share the common goal of inferring code and rely on specialized software synthesis \ntechniques [35, 52]. Inferred software .xes and contracts are usually snippets of code that are synthesized \naccording to the information gath\u00adered about the analyzed program. The core of these tech\u00adniques lies \nin the characterization of runtime behavior that is used to guide the generation of .xes and contracts. \nSuch characterization is done by analyzing program state across the execution of tests; state can be \nde.ned using user-de.ned query operations [52], and additional expressions extracted from the code [35]. \nGeneration of program .xes and con\u00ad tracts is done using heuristically guided injection of (se\u00adquences \nof) routine calls into prede.ned code templates. Our synthesis approach works with purely functional \npro\u00adgrams and does not depend on characterization of program behavior. It is more general in the sense \nthat it focuses on synthesizing whole correct functions from scratch and does not depend on already existing \ncode. Moreover, rather than using execution of tests to de.ne starting points for synthe\u00adsis and SMT \nsolvers just to guide the search, our approach utilizes SMT solvers to guarantee correctness of generated \nprograms and uses execution of tests to speedup the search. Coupling of .exible program generators and \nthe Leon ver\u00adi.er provides more expressive power of the synthesis than .lling of prede.ned code schemas. \nOur approach does not .nd errors and infer contracts, but can be utilized for those tasks if the appropriate \nreformulation of the synthesis prob\u00adlem is made -desired code needs to be in the place of a priori located \nerrors or inside contracts. 10. Conclusions and Analysis Software synthesis is a dif.cult problem but \nwe believe it can provide substantial help in software development. We have presented a new framework \nfor synthesis that combines transformational and counterexample-guided ap\u00adproaches. Our implemented system \ncan synthesize and prove correct functional programs that manipulate unbounded data structures such as \nalgebraic data types. We have used the system to synthesize algorithms that manipulate list and tree \nstructures. Our approach leverages the state of the art SMT solving technology and an effective mechanism \nfor solving certain classes of recursive functions. Thanks to this tech\u00adnology, we were already able \nto synthesize programs over unbounded domains that are guaranteed to be correct for all inputs. Our automated \nsystem can be combined with man\u00adual transformations or run-time constraint solving [25] to cover the \ncases where static synthesis does not fully solve the problem. It can further be improved by adding additional \nrules for manually veri.ed refactoring and automatic syn\u00adthesis steps [24], by informing the search using \nstatistical information from a corpus of code [12] and using domain\u00ad speci.c higher-order combinators \n[43], as well as by further improvements in decision procedures to enhance the class of veri.able programs. \nAcknowledgments We thank Tihomir Gvero for the InSynth implementation [12] whose modi.cation [26] we \nuse in our condition\u00adabduction rule to enumerate terms of a given type. We thank R \u00b4egis Blanc for his \ncontribution to the Leon veri.cation in\u00adfrastructure, including the implementation of rules for Pres\u00adburger \narithmetic synthesis. We thank Ruzica Piskac and Mika \u00a8el Mayer for useful discussions on synthesis. \nReferences [1] A. Albarghouthi, S. Gulwani, and Z. Kincaid. Recursive program synthesis. In CAV, pages \n934 950, 2013. [2] E. Asarin, O. Maler, and A. Pnueli. Symbolic controller synthesis for discrete and \ntimed systems. In Hybrid Systems, pages 1 20, 1994. [3] R.-J. Back and J. von Wright. Re.nement Calculus. \nSpringer-Verlag, 1998. [4] R. W. Blanc, E. Kneuss, V. Kuncak, and P. Suter. An overview of the Leon veri.cation \nsystem: Veri.cation by translation to recursive functions. In Scala Workshop, 2013. [5] C. Boyapati, \nS. Khurshid, and D. Marinov. Korat: automated testing based on Java predicates. In ISSTA, pages 123 133, \n2002. [6] M. Butler, J. Grundy, T. Langbacka, R. Ruksenas, and J. von Wright. The re.nement calculator: \nProof support for program re.nement. In Formal Methods Paci.c, 1997. [7] L. de Moura and N. Bj\u00f8rner. \nZ3: An ef.cient SMT solver. In TACAS, pages 337 340, 2008. [8] L. de Moura and N. Bj\u00f8rner. Generalized, \nef.cient array decision procedures. In FMCAD, 2009. [9] P. Flener and D. Partridge. Inductive programming. \nAutom. Softw. Eng., 8(2):131 137, 2001.  [10] M. Gligoric, T. Gvero, V. Jagannath, S. Khurshid, V. Kuncak, \nand D. Marinov. Test generation through programming in UDITA. In ICSE, pages 225 234, 2010. [11] S. Gulwani, \nS. Jha, A. Tiwari, and R. Venkatesan. Synthesis of loop-free programs. In PLDI, pages 62 73, 2011. [12] \nT. Gvero, V. Kuncak, I. Kuraj, and R. Piskac. Complete completion using types and weights. In PLDI, pages \n27 38, 2013. [13] M. Hofmann. IgorII -an analytical inductive functional pro\u00adgramming system (tool demo). \nIn PEPM, pages 29 32, 2010. [14] D. Jackson. Structuring Z speci.cations with views. ACM Trans. Softw. \nEng. Methodol., 4(4):365 389, 1995. [15] S. Jacobs, V. Kuncak, and P. Suter. Reductions for synthesis \nprocedures. In VMCAI, pages 88 107, 2013. [16] J. Jaffar and J.-L. Lassez. Constraint logic programming. \nIn POPL, pages 111 119, 1987. [17] B. Jobstmann and R. Bloem. Optimizations for LTL synthesis. In FMCAD, \npages 117 124, 2006. [18] J. R. Josephson. Abductive Inference: Computation, Philoso\u00adphy, Technology. \nCambridge University Press, 1994. [19] A. C. Kakas, R. A. Kowalski, and F. Toni. Abductive logic programming. \nJ. Log. Comput., 2(6):719 770, 1992. [20] E. Kitzelmann and U. Schmid. Inductive synthesis of func\u00adtional \nprograms: An explanation based generalization ap\u00adproach. JMLR, 7:429 454, 2006. [21] G. Klein, J. Andronick, \nK. Elphinstone, G. Heiser, D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt, R. Kolanski, M. Nor\u00adrish, \nT. Sewell, H. Tuch, and S. Winwood. seL4: Formal veri\u00ad.cation of an OS kernel. In SOSP, pages 207 220, \n2009. [22] A. S. K \u00a8oksal, V. Kuncak, and P. Suter. Constraints as control. In POPL, pages 151 164, 2012. \n[23] V. Kuncak, M. Mayer, R. Piskac, and P. Suter. Complete functional synthesis. In PLDI, pages 316 \n329, 2010. [24] V. Kuncak, M. Mayer, R. Piskac, and P. Suter. Software synthesis procedures. CACM, 55(2):103 \n111, 2012. [25] V. Kuncak, E. Kneuss, and P. Suter. Executing speci.cations using synthesis and constraint \nsolving (invited talk). In Run\u00adtime Veri.cation (RV), 2013. [26] I. Kuraj. Interactive code generation. \nMaster s thesis, EPFL, February 2013. [27] D. Leinenbach and T. Santen. Verifying the Microsoft Hyper-V \nhypervisor with VCC. In FM, pages 806 809, 2009. [28] Y. Lustig and M. Y. Vardi. Synthesis from component \nli\u00adbraries. In FOSSACS, pages 395 409, 2009. [29] Z. Manna and R. J. Waldinger. Toward automatic program \nsynthesis. Commun. ACM, 14(3):151 165, 1971. [30] Z. Manna and R. J. Waldinger. A deductive approach \nto program synthesis. ACM Trans. Program. Lang. Syst., 2(1): 90 121, 1980. [31] A. Martelli and U. Montanari. \nAdditive AND/OR graphs. In IJCAI, pages 1 11, 1973. [32] S. Muggleton and L. D. Raedt. Inductive logic \nprogramming: Theory and methods. J. Log. Program., 19/20:629 679, 1994. [33] T. Nipkow, L. C. Paulson, \nand M. Wenzel. Isabelle/HOL: A Proof Assistant for Higher-Order Logic, volume 2283 of LNCS. Springer-Verlag, \n2002. [34] C. Okasaki. Purely functional data structures. Cambridge University Press, 1999. [35] Y. Pei, \nY. Wei, C. A. Furia, M. Nordio, and B. Meyer. Code-based automated program .xing. ArXiv e-prints, 2011. \narXiv:1102.1059. [36] N. Piterman, A. Pnueli, and Y. Sa ar. Synthesis of reactive(1) designs. In VMCAI, \npages 364 380, 2006. [37] A. Pnueli and R. Rosner. On the synthesis of a reactive module. In POPL, 1989. \n[38] R. Singh and S. Gulwani. Synthesizing number transforma\u00adtions from input-output examples. In CAV, \npages 634 651, 2012. [39] D. R. Smith. Generating programs plus proofs by re.nement. In VSTTE, pages \n182 188, 2005. [40] A. Solar-Lezama, L. Tancau, R. Bod\u00b4ik, S. A. Seshia, and V. A. Saraswat. Combinatorial \nsketching for .nite programs. In ASPLOS, pages 404 415, 2006. [41] A. Solar-Lezama, G. Arnold, L. Tancau, \nR. Bod\u00b4ik, V. A. Saraswat, and S. A. Seshia. Sketching stencils. In PLDI, pages 167 178, 2007. [42] A. \nSolar-Lezama, C. G. Jones, and R. Bod\u00b4ik. Sketching concurrent data structures. In PLDI, pages 136 148, \n2008. [43] A. Spielmann, A. N \u00a8otzli, C. Koch, V. Kuncak, and Y. Klonatos. Automatic synthesis of out-of-core \nalgorithms. In SIGMOD, 2013. [44] S. Srivastava, S. Gulwani, and J. S. Foster. From program veri.cation \nto program synthesis. In POPL, pages 313 326, 2010. [45] P. D. Summers. A methodology for LISP program \nconstruc\u00adtion from examples. JACM, 24(1):161 175, 1977. [46] P. Suter. Programming with Speci.cations. \nPhD thesis, EPFL, December 2012. [47] P. Suter, M. Dotta, and V. Kuncak. Decision procedures for algebraic \ndata types with abstractions. In POPL, pages 199 210, 2010. [48] P. Suter, A. S. K \u00a8oksal, and V. Kuncak. \nSatis.ability modulo recursive programs. In SAS, pages 298 315, 2011. [49] A. Udupa, A. Raghavan, J. \nV. Deshmukh, S. Mador-Haim, M. M. K. Martin, and R. Alur. TRANSIT: specifying pro\u00adtocols with concolic \nsnippets. In PLDI, pages 287 296, 2013. [50] M. T. Vechev, E. Yahav, and G. Yorsh. Inferring synchroniza\u00adtion \nunder limited observability. In TACAS, pages 139 154, 2009. [51] C. von Essen and B. Jobstmann. Program \nrepair without regret. In CAV, pages 896 911, 2013. [52] Y. Wei, C. A. Furia, N. Kazmin, and B. Meyer. \nInferring better contracts. In ICSE, pages 191 200, 2011. [53] N. Wirth. Program development by stepwise \nre.nement. Commun. ACM, 14(4):221 227, 1971.  A. Benchmark Problems and Solutions We report the key \nparts of the input source code and the synthesis result for several benchmarks shown in Section 8.  \n  A.1 SortedList De.nitions. sealed abstract class List case class Cons(head : Int, tail : List) extends \nList case object Nil extends List def size(l : List) : Int = . . . def content(l : List) : Set[Int] = \n. . . def isSorted(list : List) : Boolean = list match {case Nil . true case Cons( , Nil) . true case \nCons(x1, Cons(x2, )) if(x1 > x2) . false case Cons( , xs) . isSorted(xs) } def insert(in1 : List, v : \nInt) = {require(isSorted(in1)) choose { (out : List) . isSorted(out) &#38;&#38; (content(out) == content(in1) \n++ Set(v)) }} def delete(in1 : List, v : Int) = {require(isSorted(in1)) choose { (out : List) . isSorted(out) \n&#38;&#38; (content(out) == content(in1) --Set(v)) }} def merge(in1 : List, in2 : List) = {require(isSorted(in1) \n&#38;&#38; isSorted(in2)) choose { (out : List) . isSorted(out) &#38;&#38; (content(out) == content(in1) \n++ content(in2)) }} def di.(in1 : List, in2 : List) = {require(isSorted(in1) &#38;&#38; isSorted(in2)) \nchoose { (out : List) . isSorted(out) &#38;&#38; (content(out) == content(in1) --content(in2)) }} def \nsplit(list : List) : (List,List) = { choose { (res : (List,List)) . val s1 = size(res. 1) val s2 = size(res. \n2) abs(s1 - s2) = 1 &#38;&#38; s1 + s2 == size(list) &#38;&#38; content(res. 1) ++ content(res. 2) == \ncontent(list) }} def sort(list : List) : List = choose {(res : List) . isSorted(res) &#38;&#38; content(res) \n== content(list) } Synthesized solutions. def insert(in1 : List, v : Int) = {require(isSorted(in1)) in1 \nmatch { case Nil . Cons(v, Nil) case Cons(h, t) . if (v < h) { Cons(v, in1) } else { Cons(h, insert(t, \nv)) }}} def delete(in1 : List, v : Int) : List = {require(isSorted(in1)) in1 match { case Nil . Nil \ncase Cons(h, t) . if (v == h) { delete(t, v) } else { Cons(h, delete(t, v)) }}} def merge(in1 : List, \nin2 : List) : List = {require(isSorted(in1) &#38;&#38; isSorted(in2)) in1 match { case Nil . Nil case \nCons(h, t) . merge(t, insert(in2, h)) }} def di.(in1 : List, in2 : List) : List = {require(isSorted(in1) \n&#38;&#38; isSorted(in2)) in2 match { case Nil . in1 case Cons(h, t) . di.(delete(in1, h), t) }} def \nsplit(in : List) : (List,List) = in match {case Nil . (Nil, Nil) case Cons(h, Nil) . (in, Nil) case Cons(h1, \nCons(h2, t2)) . val (s1, s2) = split(t2) (Cons(h1, s1), Cons(h2, s2)) } def sort(lst : List) : List \n= lst match {case Nil . lst case Cons( , Nil) . lst case . { val p = split(list) merge(sort(p.fst), sort(p.snd)) \n}}  A.2 UnaryNumerals De.nitions. sealed abstract class Num case object Z extends Num case class S(pred \n: Num) extends Num def value(n :Num) : Int = (n match { case Z . 0 case S(p) . 1 + value(p) }) ensuring \n( = 0) def add(x : Num, y : Num) : Num = {choose { (r : Num) . value(r) == value(x) + value(y) }} def \nmult(x : Num, y : Num) : Num = {choose { (r : Num) . value(r) == value(x) * value(y) }} def distinct(x \n: Num, y : Num) : Num = { choose { (r : Num) . value(r) != value(x) &#38;&#38; value(r) != value(y) }} \n  Synthesized solutions. def add(x : Num, y : Num) : Num = x match {case Z . y case S(p) . add(p, S(y)) \n} def mult(x : Num, y : Num) : Num = y match {case Z . Z case S(p) . add(x, mult(x, y-1)) } def distinct(x \n: Num, y : Num) : Num = x match {case Z . S(y) case S(p) . y match { case Z . S(x) case S(p) . Z }} \n A.3 BatchedQueue De.nitions. sealed abstract class List case class Cons(head : Int, tail : List) extends \nList case object Nil extends List def content(l : List) : Set[Int] = . . . case class Queue(f : List, \nr : List) def content(p : Queue) : Set[Int] = content(p.f) ++ content(p.r) def isEmpty(p : Queue) : Boolean \n= p.f == Nil def revAppend(aList : List, bList : List) : List = aList match { case Nil . bList case Cons(x, \nxs) . revAppend(xs, Cons(x, bList)) } ensuring (res . content( ) == content(aList) ++ content(bList)) \ndef invariantList(q : Queue, f : List, r : List) = {revAppend(q.f, q.r) == revAppend(f, r) &#38;&#38; \nq.f != Nil || q.r == Nil } def reverse(list : List) = revAppend(list, Nil) ensuring (content( ) == content(list)) \ndef checkf(f : List, r : List) : Queue = choose { (res : Queue) . invariantList(res, f, r) } def tail(p \n: Queue) : Queue = p.f match {case Nil . p case Cons( , xs) . checkf(xs, p.r) } def snoc(p : Queue, x \n: Int) : Queue = { choose { (res : Queue) . content(res) == content(p) ++ Set(x) &#38;&#38; (isEmpty(p) \n|| content(tail(res)) ++ Set(x) == content(tail(res))) } } Synthesized solutions. def checkf(f : List, \nr : List) : Queue = r match { case r @ Nil . Queue(f, r) case . f match { case f @ Cons( , ) = Queue(f, \nr) case . Queue(reverse(r), Nil) }} def snoc(p : Queue, x : Int) : Queue = Queue(p.f, Cons(x, p.r)) \nA.4 AddressBooks De.nitions. (Section 2.2 shows solutions)  case class Info(address : Int, zipcode : \nInt, phoneNumber : Int) case class Address(info : Info, priv : Boolean) sealed abstract class List case \nclass Cons(a : Address, tail :List) extends List case object Nil extends List def content(l : List) : \nSet[Address] = . . . def size(l : List) : Int = . . . def allPrivate(l : List) : Boolean = l match {case \nNil . true case Cons(a, l1) . if (a.priv) allPrivate(l1) else false } def allBusiness(l : List) : Boolean \n= l match {case Nil . true case Cons(a, l1) . if (a.priv) false else allBusiness(l1) } case class AddressBook(business \n: List, pers : List) def size(ab : AddressBook) : Int = size(ab.business) + size(ab.pers) def isEmpty(ab \n: AddressBook) = size(ab) == 0 def content(ab : AddressBook) : Set[Address] = content(ab.pers) ++ content(ab.business) \ndef invariant(ab : AddressBook) = allPrivate(ab.pers) &#38;&#38; allBusiness(ab.business) def makeAddressBook(l \n: List) : AddressBook = choose { (res : AddressBook) . size(res) == size(l) &#38;&#38; invariant(res) \n} def merge(l1 : List, l2 : List) : List = l1 match {case Nil . l2 case Cons(a, tail) . Cons(a, merge(tail, \nl2)) } def mergeAddressBooks( ab1 : AddressBook, ab2 : AddressBook) = {require(invariant(ab1) &#38;&#38; \ninvariant(ab2)) choose { (res : AddressBook) . (size(res) == size(ab1) + size(ab2)) &#38;&#38; invariant(res) \n}}   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>We describe techniques for synthesis and verification of recursive functional programs over unbounded domains. Our techniques build on top of an algorithm for satisfiability modulo recursive functions, a framework for deductive synthesis, and complete synthesis procedures for algebraic data types. We present new counterexample-guided algorithms for constructing verified programs. We have implemented these algorithms in an integrated environment for interactive verification and synthesis from relational specifications. Our system was able to synthesize a number of useful recursive functions that manipulate unbounded numbers and data structures.</p>", "authors": [{"name": "Etienne Kneuss", "author_profile_id": "81472656474", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne, Lausanne, Switzerland", "person_id": "P4290386", "email_address": "etienne.kneuss@epfl.ch", "orcid_id": ""}, {"name": "Ivan Kuraj", "author_profile_id": "81758642457", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne, Lausanne, Switzerland", "person_id": "P4290387", "email_address": "ivan.kuraj@epfl.ch", "orcid_id": ""}, {"name": "Viktor Kuncak", "author_profile_id": "81100277693", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne, Lausanne, Switzerland", "person_id": "P4290388", "email_address": "viktor.kuncak@epfl.ch", "orcid_id": ""}, {"name": "Philippe Suter", "author_profile_id": "81453631112", "affiliation": "&#201;cole Polytechnique F&#233;d&#233;rale de Lausanne, Lausanne, Switzerland &#38; IBM T.J. Watson Research Center, Yorktown Heights, NY, USA", "person_id": "P4290389", "email_address": "psuter@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509555", "year": "2013", "article_id": "2509555", "conference": "OOPSLA", "title": "Synthesis modulo recursive functions", "url": "http://dl.acm.org/citation.cfm?id=2509555"}