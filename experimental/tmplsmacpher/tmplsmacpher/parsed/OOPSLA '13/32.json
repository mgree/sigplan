{"article_publication_date": "10-29-2013", "fulltext": "\n Isolation for Nested Task Parallelism Jisheng Zhao Roberto Lublinerman Zoran Budimli \u00b4c Rice University \nGoogle, Inc. Rice University jisheng.zhao@rice.edu rluble@google.com zoran@rice.edu Swarat Chaudhuri \nVivek Sarkar Rice University Rice University swarat@rice.edu vsarkar@rice.edu Abstract Isolation the \nproperty that a task can access shared data without interference from other tasks is one of the most \nbasic concerns in parallel programming. While there is a large body of past work on isolated task-parallelism, \nthe in\u00adtegration of isolation, task-parallelism, and nesting of tasks has been a dif.cult and unresolved \nchallenge. In this pa\u00adper, we present a programming and execution model called Otello where isolation \nis extended to arbitrarily nested par\u00adallel tasks with irregular accesses to heap data. At the same time, \nno additional burden is imposed on the programmer, who only exposes parallelism by creating and synchroniz\u00ading \nparallel tasks, leaving the job of ensuring isolation to the underlying compiler and runtime system. \nOtello extends our past work on Aida execution model and the delegated isolation mechanism [22] to the \nsetting of nested parallelism. The basic runtime construct in Aida and Otello is an assembly: a task \nequipped with a region in the shared heap that it owns. When an assembly A con.icts with an assembly \nB, A transfers or delegates its code and owned region to a carefully selected assembly C in a way that \nwill ensure isolation with B, leaving the responsibility of re-executing task A to C. The choice of C \ndepends on the nesting relationship between A and B. We have implemented Otello on top of the Habanero \nJava (HJ) parallel programming language [8], and used this im\u00adplementation to evaluate Otello on collections \nof nested task\u00adparallel benchmarks and non-nested transactional bench\u00admarks from past work. On the nested \ntask-parallel bench- Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, \nIndianapolis, Indiana, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509534 \nmarks, Otello achieves scalability comparable to HJ pro\u00adgrams without built-in isolation, and the relative \noverhead of Otello is lower than that of many published data-race de\u00adtection algorithms that detect the \nisolation violations (but do not enforce isolation). For the transactional benchmarks, Otello incurs \nlower overhead than a state-of-the-art software transactional memory system (Deuce STM). Categories and \nSubject Descriptors D.1.3 [Programming Techniques]: Concurrent Programming; D.3.2 [Program\u00adming Languages]: \nLanguage Classi.cations Concurrent, distributed, and parallel languages General Terms Languages, Design \nKeywords Isolation, Programming abstractions, Irregular parallelism, Contention 1. Introduction The demand \nfor parallel programming is now higher than ever: inexpensive multicore processors are ubiquitous, and \nthe bottleneck is now software rather than hardware. And yet, it is increasingly clear that current foundations \nfor parallel programming, such as locks and messages, are low-level, complex, error-prone, and non-modular. \nConse\u00adquently, many research groups in industry and academia are seeking high-level models and languages \nfor parallel pro\u00adgramming that are intuitive as well as scalable. A critical challenge in the foundations \nof parallel pro\u00adgramming models is that of isolation: the property that a task can access dynamic subsets \nof shared data structures without interference from other tasks. A programming model pro\u00adviding high-level \nguarantees of dynamic isolation is able to rule out the perennial dual headaches of deadlocks and data \nraces, while also relieving the burden of low-level reasoning about memory consistency models. Such high-level \nmodels of isolation have received much attention in the research community in the last decade. Of existing \napproaches to this problem, the most well-known is the transactional memory model [20]. However, integra\u00adtion \nof these high-level models, including transactions, with nested parallelism remains a dif.cult and unresolved \nchal\u00adlenge. This is unfortunate because nested parallel tasks form a natural programming idiom in many \nreal-life settings. For example, suppose we declare a task A to be isolated, but want to extract additional \nparallelism within it. A natural way to do so is to let A fork parallel tasks B and C. However, the tasks \nB and C may need to be isolated from each other, and may need to synchronize (join) at some point. In \ngeneral, B and C might fork other parallel tasks as well. As yet, there is no transactional memory solution \nthat guarantees isolation while supporting nested parallelism of this sort. For paral\u00adlel programming \nmodels that do support nested-parallelism  (e.g. OpenMP 3.0[27], Cilk [5]), locks are still the major \nap\u00adproach for implementing mutual exclusion. In this paper, we present a programming and execution model, \ncalled Otello, that allows parallel tasks that are ar\u00adbitrarily nested, and make irregular accesses to \nobjects in a shared heap, to run in isolation. In our programming model, parallel tasks are declared \nto be isolated by a single keyword, and the programmer only exposes parallelism by creating and synchronizing \nparallel tasks, respectively using struc\u00adtured async and finish constructs, as in Habanero Java [8] and \nX10 [10]. The underlying compiler and runtime system are responsible for ensuring isolation between tasks. \nOtello s implementation relies on extensions to the Aida execution model and the delegated isolation \nmechanism in\u00adtroduced in [22] to support nested isolated task-parallelism. Speci.cally, the Otello runtime \nviews each task as being equipped with a set of shared objects that it owns. We re\u00adfer to this combination \nof a task and its owned objects as an assembly. An assembly can only access objects that it owns, thus \nguaranteeing isolation. When an assembly A needs to acquire ownership of a datum owned by assembly B \n(i.e., a con.ict happens), A transfers or delegates its code and owned region to assembly C in a way \nthat will ensure iso\u00adlation with B, leaving the responsibility of re-executing task A to C. The choice \nof C, as well as the precise mechanics of delegation depends on the nesting relationship between A and \nB. Aside from isolation, Otello offers some fairly strong progress guarantees. Speci.cally, Otello is \ndeadlock and livelock-free; also, one can quantitatively bound the ratio of con.icts (cases when delegation \nneeds to happen) to com\u00admits (cases when a control in a task reaches the task s end point) using a parameter \nbased on the nesting depth of tasks. We note that guarantees like the last one exploit the struc\u00adtured \nnature of async-finish parallelism. In fact, the obser\u00advation that structured parallel constructs simplify \nand clean up the semantics of nested isolated task-parallelism is a core contribution of this paper. \nFrom the viewpoints of programmability, Otello s high\u00adlevel isolation construct offers many bene.ts. \nWhen paral\u00adlelizing a sequential program, the programmer only needs to pay attention to parallel decomposition \nwithout worry\u00ading about semantic issues arising from con.icting accesses performed by concurrent tasks. \nIsolation also helps support composability of parallel software, since an isolated library function call \nthat creates internal tasks is guaranteed to be isolated from the task that made the call. The programmer \ncan reason about interleavings of tasks rather than interleav\u00adings of instructions among tasks, thereby \nsimplifying the problem of debugging parallel programs. Further, the pro\u00adgrammer can assume a sequential \nconsistency model when reasoning about task interleavings, even if the program ex\u00adecutes on a platform \nthat supports weaker memory models. Finally, the programmer can obtain all these bene.ts when using general \nnested parallel program structures. As for performance, we have implemented Otello on top of the Habanero \nJava parallel programming language [8], and used this implementation to evaluate Otello on col\u00adlections \nof nested-parallel benchmarks and transactional benchmarks. For the nested-parallel benchmarks, our re\u00adsults \nshow that the low overhead of enabling isolation-by\u00addefault in semantics Otello s isolation-by-default \nmakes it a promising approach for adoption in practice. For 16-core executions, the average slowdown \nresulting from turning on Otello s isolation-by-default semantic across six nested\u00adparallel benchmarks \nwas 1.32\u00d7, with a maximum slowdown factor of 1.75\u00d7. This is signi.cantly lower than the rela\u00adtive overhead \nof many published data-race detection algo\u00adrithms [28] which only detects isolation con.icts do not enforce \nisolation 1. Interestingly, one benchmark (spanning tree) showed a speedup for Otello over the default \nHJ im\u00adplementation, due to the Otello version revealing more par\u00adallelism. For the transactional benchmarks, \nour results show that Otello incurs lower overhead than a state-of-the-art software transactional memory \nsystem (Deuce STM) [11]. For 16-core executions, the maximum speedup obtained for Otello relative to \nDeuce was 184.09\u00d7, the minimum speedup was 1.20\u00d7, and the geometric mean of the speedup across seven \ntransactional benchmarks was 3.66\u00d7. The rest of the paper is organized as follows. Section 2 summarizes \nthe Habanero Java task parallel model, Aida del\u00adegated isolation model [22], and motivates the need for \nisola\u00adtion in nested task parallelism. Section 3 introduces the core Otello model with details of its \nsyntax, semantics, and prop\u00aderties. Section 4 introduces a more general programming language version \nof the core Otello model and describes our implementation of this language. Section 5 summarizes our \nexperimental results. Section 6 discusses related work, and Section 7 contains our conclusions. Appendix \nA contains de\u00adtails on the formal semantics of Otello. 1 The key reason for this difference is that a \ndata race detector must monitor accesses at the location level to avoid false alarms, whereas isolation \ncan be enforced at the object level.  2. Background In this paper, we will use the Habanero Java (HJ) \nlan\u00adguage [8] as a representative of nested task-parallel lan\u00adguages, and demonstrate how it can be extended \nwith iso\u00adlation using the Otello programming and execution model. The basic ideas in Otello of adding \nisolation to nested task parallelism can be applied to other nested task-parallel pro\u00adgramming models \nincluding Cilk [5] and OpenMP 3.0 [27]. Section 2.1 provides a brief summary of HJ s async, finish and \nisolated constructs, Section 2.2 summaries Aida s delegated isolation model [22], Section 2.3 contrasts \ntasks in HJ with assemblies in Aida and Section 2.4 uses a sim\u00adple example to illustrate the challenges \nthat arise in ensuring isolated execution of parallel tasks. 2.1 Async, Finish and Isolated statements \nin HJ The basic primitives of task parallelism relate to creation and termination of tasks. In HJ, these \nprimitives are manifest in the async and finish statements, which were in turn derived from the X10 language \n[10]. async: The statement async (stmt) causes the parent task to create a new child task to execute \n(stmt) asynchronously (i.e., before, after, or in parallel) with the remainder of the parent task. Following \nstandard convention, we use ances\u00adtor to refer to the transitive closure of the parent relation on tasks. \nFigure 1 illustrates this concept by showing a code schema in which the parent task, T0, uses an async \ncon\u00adstruct to create a child task T1. Thus, STMT1 in task T1 can potentially execute in parallel with \nSTMT2 in task T0. Figure 1. An example code schema with async and finish constructs .nish: finish is \na generalized join operation. The state\u00adment .nish (stmt) causes the parent task to execute (stmt) and \nthen wait until all async tasks created within (stmt) have completed, including transitively spawned \ntasks. Each dy\u00adnamic instance TA of an async task has a unique Immedi\u00adately Enclosing Finish (IEF) instance \nF of a finish state\u00adment during program execution, where F is the innermost finish containing TA [29]. \nThe IEF of TA must belong to an ancestor task of TA. There is an implicit finish scope surrounding the \nbody of main() so program execution will only end after all async tasks have completed. As an example, \nthe finish statement in Figure 1 is used by task T0 to ensure that child task T1 has completed executing \nSTMT1 before T0 executes STMT3. If T1 created a child async task, T2 (a grandchild of T0), T0 will wait \nfor both T1 and T2 to complete in the finish scope before executing STMT3. isolated: isolated (stmt1) \ndenotes an isolated statement, HJ provides a weak atomicity semantics for isolated state\u00adments any dynamic \ninstance of an isolated statement is guarenteed to be performed in mutual exclusion with re\u00adspect to \nall other potentially parallel dynamic instances of isolated statement. Thus far, the only viable approach \nto im\u00adplement this isolation semantics with nested parallelism has been through the use of locks. pr, \nguarantees that each instance of (stmt1) will be per\u00adformed in mutual exclusion with all other potentially \nparallel interfering instances of isolated statement (stmt). 2.2 Aida Aida [22] is a programming model \nwhich provides a notion of delegation among concurrent isolated tasks (known in Aida as assemblies). \nAn assembly A is equipped with a region in the shared heap that it owns the only objects accessed by \nA are those it owns, guaranteeing isolation. The region owned by A can grow or shrink .exibly however, \nwhen A needs to own a datum owned by B, A delegates itself, as well as its owned region, to B. From now \non, B has the responsibility of re-executing the task A set out to complete. Delegation as above is the \nonly inter-assembly communication primitive in Aida. However, Aida does not permit general nesting of \niso\u00adlated tasks. Instead, it requires all child tasks to be created at the very end of the parent task \nafter all accesses to shared objects have completed and been committed.  2.3 Tasks vs. Assemblies It \nis worth taking some time here to make clear the distinc\u00adtion between tasks and assemblies. A task is \na piece of code intended to be executed asynchronously and (potentially) in parallel with other tasks. \nHabanero Java s async and Cilk s spawn are example of constructs that create tasks. An assembly is a \nruntime concept used to implement delegated isolation. It is a collection consisting of the cur\u00adrently \nrunning task, all the data owned by the assembly, and all the tasks that have been delegated to this \nassembly through the delegation mechanism. Each assembly starts its life with a single task, and acquires \ndata (either through ac\u00adquiring individual objects or through delegation) and other tasks (through delegation) \nas it executes the original task and the tasks delegated to the assembly. Each assembly ends its life \neither when it .nishes the execution of all the tasks it contains, at which point it releases all the \ndata it owns and dies, or when it discovers a con.ict with another assembly, at which point it delegates \nthe tasks it contains and data it owns to that other assembly and then terminates. Throughout this paper, \nwe will use the term task when referring to chunks of code intended for asynchronous exe\u00adcution (speci.ed \nby the programmer using async). We will use the term assembly when referring to the runtime entities \n 1 2 3 used to implement delegation, isolation and nesting.  2.4 Example 4 5 Though the async and finish \nconstructs provide a gen-6 7 8 eral framework for expressing nested parallelism, a key chal\u00adlenge still \nremains for programmers how to deal with con-9 10 11 .icts2 among tasks that execute in parallel? As \na simple example, consider the sequential depth-.rst 12 13 algorithm shown in Figure 2 to compute a spanning \ntree of an undirected graph. It is relatively easy for a programmer to reason about parallel decompositions \nfor such an algorithm. For instance, an examination of Figure 2 may reveal that the recursive calls to \nchild.visit() could potentially ex-1 ecute in parallel. It is then straightforward to use async and 2 \nfinish constructs to obtain parallel HJ code based on this 4 3 observation, as shown in Figure 3. However, \nwhen viewed as 5 a pure HJ program, this is an incorrect algorithm because of 6 7 the con.ict when two \ntasks attempt to become the parent of 8 the same node. This con.ict arises from a potential data race \n10 9 between reads/writes to the same child.parent location 11 12 by multiple tasks. 13 Fixing this problem \nis not easy. Figure 4 shows the Cilk code for a parallel depth-.rst-search algorithm that uses a global \nlock to avoid this con.ict. The lock is used to im\u00adplement a critical section. It is well known that \nthe choice and placement of locks can be both tricky and fragile from 1 a software engineering viewpoint \nsince under-locking and 2 over-locking can lead to data races or deadlock respec-4 3 tively. Further, \nthe use of a single global lock can limit the 5 amount of parallelism in the program. 7 6 Unlike Cilk, \nHJ uses an isolated construct [8] instead 8 of locks. Though the use of isolated guarantees the ab-9 \nsence of deadlocks, the programmer still has to decide where to place the isolated statements so as to \nexpose suf.cient parallelism without creating data races. Further, the current HJ de.nition does not \npermit new task creation in the body of an isolated statement. Similar constraints exist in other task \nparallel languages e.g., X10 does not permit new tasks to be created within an atomic statement. In this \npaper, we go beyond previous approaches and completely remove the programmer s burden of reasoning about \nisolated statements or locks. Core Otello treats all async tasks as being isolated by default (General \nOtello allows selected tasks, denoted by async-w for weak async , to be non-isolated.). With isolation-by-default \nsemantics, the code in Figure 3 will work correctly despite the potential for con.icts among accesses \nto the same child.parent loca\u00adtion. It is the responsibility of the language implementation to ensure \nthat any two con.icting tasks execute as though they had been performed in a sequential order that is \nconsis\u00adtent with the parallel program semantics. 2 We use the standard de.nition of a con.ict viz., two \nparallel operations on a shared location such that one of the operations is a write. v o i d visit () \n{ f o r ( i n t i= 0; i < n e i g h b o r s . l e n g t h ; i + + ) { N o d e c h i l d = n e i g h b \no r s [ i ] ; i f ( c h i l d . p a r e n t = = n u l l ) { c h i l d . p a r e n t = t h i s ; child \n. visit (); } } } / / v i s i t ( ) . . . root . visit (); . . . Figure 2. Sequential depth-.rst algorithm \nin HJ v o i d visit () { f o r ( i n t i= 0; i < n e i g h b o r s . l e n g t h ; i + + ) { N o d e \nc h i l d = n e i g h b o r s [ i ] ; i f ( c h i l d . p a r e n t = = n u l l ) { c h i l d . p a \nr e n t = t h i s ; a s y n c c h i l d . v i s i t ( ) ; } } } / / v i s i t ( ) . . . f i n i s h \nr o o t . v i s i t ( ) ; . . . Figure 3. Parallel algorithm in General Otello C i l k _ l o c k v a \nr m y l o c k ; i n t * * G ; i n t * p a r e n t ; i n t n o d e s ; c i l k v o i d d f s ( i n t p \n) { f o r ( i n t i = 0 ; G [ p ] [ i ] ! = -1 ; + + i ) { C i l k _ l o c k ( m y l o c k ) ; i n t \nv i s i t e d = ( p a r e n t [ G [ p ] [ i ] ] ! = -1 ) ; i f ( ! v i s i t e d ) p a r e n t [ G [ \np ] [ i ] ] = p ; C i l k _ u n l o c k ( m y l o c k ) ; i f ( ! v i s i t e d ) s p a w n d f s ( G \n[ p ] [ i ] ) ; } } Figure 4. Cilk code for parallel depth-.rst-search (dfs.cilk) 3. Core Otello In this \nsection, we present the Otello programming and ex\u00adecution model. For easier exposition, we use a foundational \nversion of Otello that focuses on the model s essential fea\u00adtures. We call this model Core Otello. The \nmain task ab\u00adstraction in Otello (and Core Otello) is called an (object) assembly [21, 22], or a task \nthat has explicit ownership of a region in the shared heap. An assembly can only ac\u00adcess objects in the \nregion that it owns, hence by de.ni\u00adtion, it embodies isolation. Now we present the formal syn\u00adtax and \nsemantics of the Core Otello language. Core Otello uses the same concurrency constructs as our implementa\u00adtion \nof Otello; however, the former makes several simplify\u00ading assumptions about the sequential language underlying \nthe model. For example, statements in Core Otello do not call methods on objects, create new objects, \nor declare new local variables. Also, we let objects be untyped and assume that all objects are shared. \n 3.1 Language syntax Otello is implemented on top of a framework of fork-join parallelism. In Core Otello \nas well as our implementation, the programmer creates assemblies by enclosing imperative code blocks \nwithin the construct async {...} . The con\u00adstruct finish {...} de.nes a scope at the end of which all \nassemblies created within the scope must join. Generalizing prior work on delegated isolation [22] as \nwell prior work on .at models of finish statements studied in [4], we allow finish and async blocks to \nbe arbitrarily nested. Formally, let us assume a universe Var of variable names and a universe F of .eld \nnames. The syntax of programs in Core Otello is shown in Figure 5. Here, programs are given by the nonterminal \nProg , and the code executed by an assembly is given by the nonterminal Block . We assume that all variables \nappearing in the text of a program P are implicitly declared at the beginning of P . Note that Core Otello \nimposes some restrictions on the structure of nested async and finish blocks. For example, the body of \na finish block can only contain async state\u00adments. The body of an async statement can be sequential code, \nor a group of finish blocks followed by a group of async statements. Sequential code can only appear \ninside of an async. These restrictions are imposed in order to simplify the operational semantics of \nthe Core Otello. A more general language called General Otello is described in Section 4. Prog ::= Finish \nFinish ::= finish {[Async; ]*} Async ::= async {[Finish; ]* [Async; ]*} | async {Block } Block ::= Block \nBlock | v := u.f; | v.f := u; Figure 5. Syntax of Core Otello. (Here u, v . Var and f . Fields. For \nnonterminals X and Y , [X]* denotes zero or more successive occurrences of X, and (X | Y ) denotes syntax \ngiven either by X or Y .) The source of parallelism in the constructs in Figure 5 arises from the two \noccurrences of [Async; ]* . From the Otello viewpoint, the programmer can assume that the async tasks \nin these two lists can execute in any order, can leave the determination of which tasks can safely be \nexecuted in parallel to the implementation.  3.2 Semantics: Delegation and Nesting Con.icts between \ntasks (assemblies) are resolved using a runtime mechanism called delegation. This mechanism is a generalization \nof the delegation mechanism introduced in the Aida model [22], to a setting where parallel tasks can \nbe arbitrarily nested. The core idea is as follows. Suppose an assembly A detects a con.ict3 while accessing \nan object O owned by a different assembly B. In that case, assembly 3 In the current version of Otello, \nwe do not have any notion of read\u00adonly ownership of objects. Hence, a con.ict includes scenarios where \nboth assemblies intend to read the same object. An extension of the assembly A rolls back its effects \non the heap and delegates itself to B, which executes it sequentially at a later point in time. Delegation \ncould also result in the transfer of the region owned by A to a different assembly C, depending on the \nnesting relationship between A and B. Delegation becomes especially challenging in the pres\u00adence of nested \nparallelism. In this section, we sketch some of the scenarios that we must consider for delegation. We \nuse the tree of .nish scopes in Figure 6 as an example to guide our discussion. This tree represents \na snapshot of a state of the program at any point in time. Each .nish scope is a set of assemblies. Each \nassembly can contain at most one ac\u00adtive .nish scope at any point in time. Note that references to A, \nB, C are intended to denote general assemblies, whereas A1, A2, . . . and F1, F2, . . . refer to speci.c \nassemblies and .nish scopes respectively in Figure 6. Note also that each as\u00adsembly belongs to exactly \none Immediately Enclosing Finish (IEF) (in our example, A5 belongs to F3). The parent of an assembly \nA is the assembly that created the IEF of A (in our example, the parent of A5 is A3). Suppose an assembly \nA detects a con.ict when accessing object O owned by assembly B. If assembly A and assembly B are both \nin the same IEF then A will delegate to B (as\u00adsembly A3 will delegate itself to assembly A4 on Figure \n6, for example). If they are not in the same IEF then either 1) assembly B is in some IEF that is nested \nwithin the IEF of assembly A, or 2) it is not. When assembly B is in some IEF nested within the IEF of \nassembly A, then let C be the assembly that contains the .nish scope of B and it has the same IEF as \nassembly A. A will then delegate itself to C. A will be executed sequen\u00adtially after C completes, analogously \nto what happens in the absence of nesting. In our example, if A2 detects a con.ict with A6, then A2 will \ndelegate itself to A1. When assembly A s IEF is nested within the IEF of B, then let C be the parent \nassembly of A, containing the IEF of A. A will delegate itself to C but will execute at the closing of \nits IEF. In our example, if assembly A5 con.icts with assembly A4, then A5 will delegate itself to A3, \nbut the code of A5 will be enqueued for execution after F3. When neither A nor B are nested within each \nother s IEFs, then let C be the parent assembly of A, containing the IEF of A. A will again delegate \nitself to C but will enqueue the code for A for execution at the closing of its IEF. In our example, \nif A5 con.icts with A7 (for example, when trying to acquire object O owned by A7) then A5 will delegate \nitself to A3, and schedule itself for execution after F3. Assembly A3, when executing the code for A5 \nafter F3, may again con.ict with A7 or one of its ancestors (when A7 commits it will transfer the ownership \nof its objects to its parent assembly) when trying to acquire O, and delegate itself to its parent assembly \n(in the cloud ), and so on, in the abstraction with an separate category of read-only ownership is however \neasy to de.ne, and will be implemented in future versions of Otello.  Figure 6. An example of a .nish \ntree. F2 and F4 are de\u00adscendants of F1, with clouds representing arbitrary levels of nesting of .nish \nscopes. worst case resulting in A1 delegating itself to A2. However, if A2 (and its whole subtree of \nassemblies and .nish scopes, including A7) .nishes its execution before A3 tries acquiring O again while \nexecuting the code for A5, then A3 will be able to acquire O and proceed with the execution. Now we sketch \nthe operational semantics of Core Otello. A more detailed semantics is presented in Appendix A. The central \ndata structure in Core Otello is the shared\u00admemory heap, which maintains the state of all shared muta\u00adble \ndata accessed by a program. We abstractly view a heap as a directed graph whose nodes are objects, and \nedges are pointers labeled with .eld names. Now consider a piece of code K = S1 ; . . . ; Sm that can \nbe executed by an assembly, where each Si is either an assignment, or a nested async or finish-block. \nA closure of K is a data structure that contains information about: (1) the part of K that still remains \nto be executed, and (2) the mapping of variables used in K to objects in the heap. Now, let us assume \na universe of assembly IDs. Consider an assembly with ID A that we are currently executing, and let G \nbe the heap that A accesses. An assembly state of A is a data structure that contains information about: \n(1) the clo\u00adsure that A is currently executing; (2) the delegation queue of A i.e., a list of closures \nthat have been delegated to A, and will be executed by A in sequence once the current closure .nishes \nexecuting; and (3) a region a set of nodesowned by G. The state of an entire Core Otello program at any \ngiven time (called a concurrent state) is de.ned as a tree-shaped structure where a node is an ID for \na speci.c .nish-scope, and each node F is associated with a collection of assembly IDs T (F ). Intuitively, \nthe assemblies in T (F ) have F as their immediately enclosing .nish. Unlike in usual trees, edges here \ngo from assembly IDs (associated with tree nodes) to tree nodes. There is a tree edge from an assembly \nID A to a tree node F if the .nish\u00adscope represented by F is nested within the asynchronous task represented \nby A. At any point of an execution, each assembly ID is associated with a speci.c assembly state. These \nassembly states must be such that the regions owned by the corresponding assembly in them are disjoint. \nThis guarantees that an object in our model is owned by at most one assembly. The objects in G that are \nnot owned by any assembly are said to be free. The dynamics of a Core Otello program is de.ned by a set \nof transitions between concurrent states. Most of these transitions select an assembly from the current \nconcurrent state and execute the next statement in it. For an assembly to be scheduled for execution, \nit must be a leaf in the tree that is the current concurrent state. The need for delegation arises when \nan assembly tries to access an object owned by a different assembly. Speci.cally, we sketch the more \ninteresting classes of transitions: Same-level con.ict: We have a class of transitions that de.ne delegation \namong assemblies that are siblings of each other in a concurrent state. Let A and B be assem\u00adblies, and \nlet QA and QB be the list of closures that A and B obligated to execute, respectively. (By convention, \nlet us assume that the closures that A and B are currently executing are the .rst elements of these lists.) \nLet the .rst element of QA be .; by our convention, this closure is currently under execution. Now suppose \n. reads or writes an object u that is currently owned by the assembly B (i.e., a con.ict happens). Consequently, \nA must now delegate its work and owned region to B. After the rule .res, the state of B becomes such \nthat: (1) Its owned region includes all objects owned by A or B before the rule .red; (2) Its delegation \nqueue is obtained by appending QA at the end of QB. One important issue is that A may have modi.ed certain \nobjects in its region while it was executing .. In this case, before delegation, the runtime rolls back \nthe effect of the code already executed by .. However, because A is re\u00adquired to be a leaf in the tree \nthat constitutes the concur\u00adrent state, and also because of the syntactic restrictions in Core Otello, \nthis code is solely a collection of heap updates it does not include any async or finish state\u00adments. \nThe implementation of this rollback operation is therefore straightforward. Con.ict below: We have a \nclass of transitions capturing scenarios when an assembly A con.icts with an assembly B, and there is \nan assembly C that is a sibling of A and an ancestor of the node containing B. In this case, A delegates \nto C.  Con.ict with ancestor: We have a class of transitions that handle the case when A tries to access \nan object u owned by an ancestor B in the concurrent state. Be\u00adcause B is not a leaf node in the current \n.nish-tree, it is currently suspended ; therefore, A can safely steal u from B.  Con.ict with unrelated: \nThese transitions handle the re\u00admaining con.ict scenarios. Here, if A . T (F ) con.icts   with B, then \nA transfers its owned region to the parent C of the node F . The code executed by A is now to be ex\u00adecuted \nafter all the assemblies in T (F ) .nish executing, and is put in the closure queue for F .  Async creation: \nThese transitions de.ne the semantics of assembly creation. The rule creates a new assembly with an empty \nset of owned objects. The assembly be\u00adlongs to the same tree node (.nish-scope) as the assembly that \ncreated it.   Finish: These transitions de.ne the semantics of finish \u00adstatements executed by an assembly \nA. Here a new tree node F ' is created; A is the parent of F ', and T (F ') = \u00d8. The code inside the \nfinish-block is transferred to the closure queue of F .  3.3 Properties of Core Otello Isolation The \nproperty of isolation demands that a concur\u00adrent task read or write shared-memory objects without inter\u00adference \nfrom other tasks. In Core Otello, an assembly can only read or write objects in its own region; also, \nif A dele\u00adgates work to B, the ability of B to read or write its objects is not compromised. Therefore, \nCore Otello guarantees iso\u00adlation. Deadlock-freedom There are only two synchronization operations in \nCore Otello: scoped joining of assemblies and delegation. As the former operation does not depend on \ndata at all, it clearly does not introduce deadlocks (The tree struc\u00adture also ensures that deadlock \nis not possible due to join operations.). As for delegation, the only plausible deadlock scenario involving \ntwo assemblies is the following: Assem\u00adbly A tries to delegate to assembly B, B tries to delegate to \nA, and neither can progress. This scenario, however, is im\u00adpossible in Otello. If A and B try to simultaneously \ndelegate to each other, then one of the two requests (let us say the one from B) will be nondeterministically \nselected and hon\u00adored. This can lead to two outcomes: (a) The ownership of all objects owned by B would \nbe transferred to A; or (b) The ownership of all objects owned by B would be transferred to the parent \nB' of the .nish-tree node where B resides. In the former case, the request from A is no longer a con.ict \nthe request from A to access u will succeed, and A will be able to progress. In the latter case, A may \ncon.ict with B', in which case the ownership of u may be transferred to the parent B'' of the .nish-scope \nof B', and so on. However, the number of times such con.icts over u may occur is bounded by the number \nof hops between the root of the .nish-tree and the node to which B belongs. Formally, let us de.ne the \n.nish-depth Depth(P ) of a program P inductively as follows: If P = finish{P1 . . . Pn}, where each \nPi is an async\u00adblock, then Depth(P ) = 1 + maxi Depth(Pi)  If P is a Block , then Depth(P ) = 0  If \nP = async{Q1 . . . Qm P1 . . . Pn}, where each Qi is a finish-block and each Pi is an async-block, then \nDepth(P ) = max(maxi Depth(Pi), maxi Depth(Qi)). In the above scenario, the maximum number of con.icts \nbetween A and other assemblies B, B', B'', . . . over u is bounded by Depth(P ). It is easy to generalize \nthe above argument to cyclic deadlocks among n assemblies. Livelock-freedom In Core Otello, two assemblies \nA and B would be livelocked if they constantly try to delegate to each other, none of them progressing. \nAs an analogy, there is always a non-zero probability that such a livelock sce\u00adnario may occur in a transactional \nmemory system with re\u00adpeated rollback-retry operations. However, in such a sce\u00adnario, Otello would destroy \none of the two assemblies, del\u00adegating its work the other assembly would then be able to progress. Bound \non con.icts/commit ratio Finally, a key property of Core Otello is that in any execution of a program \nP , the number of con.icts (number of applications of the del\u00adegation rules in Figure 11) is bounded \nby Depth(P ) times the number of commits. 4 As Depth(P ) is small in prac\u00adtice, this property works as \na performance guarantee in high contention-scenarios, where, in many state-of-the-art trans\u00adactional \nmemory systems, there may possibly be an un\u00adbounded number of aborts and too few commits. Naturally, \nour guarantee comes at a cost; due to delegation, some tasks in Otello may be performed sequentially, \nand in certain situ\u00adations, this loss of parallelism may be a limitation. 4. Implementation 4.1 General \nOtello Core Otello is a simple and elegant language that can be implemented using the techniques described \nbelow in Sec\u00adtion 4.3, and that provides important correctness guarantees. However, in practice, such \na core model is too minimalistic for programmers. Given this, we embed our core model in a fuller-featured \nparallel programming language called Gen\u00aderal Otello. General Otello removes many of the syntactic restric\u00adtions \npresent in Core Otello for example, that Block can only appear inside an Async, and that Finish can only \nap\u00adpear at the beginning of an Async. Also, General Otello permits non-isolated parallel tasks in addition \nto isolated ones (assemblies). As before, the latter are demarcated in the source code by keyword async \n. A block of code ex\u00adecuted as a non-isolated task is marked by the keyword async-w (read as weak async \n). Such non-isolated tasks allow programmers or compilers to completely eliminate the 4 We note that \ncommits are somewhat tricky to de.ne in the setting of nested parallelism. This is because a task that \nhas been committed may be have to be aborted subsequently because the parent of the task is aborted. \nIn this paper, we count each time control reaches the end point of a task to be a commit.  (small, but \nnot negligible) overhead of delegated isolation in cases where the programmer or compiler knows in ad\u00advance \nthat the tasks will be accessing completely disjoint parts of the data. However, this extra ef.ciency \ncomes at the cost of some guarantees. General Otello does not make any guarantees about isolation between \nasync-w tasks or isolation between async-w and async tasks; it relies completely on the underlying Habanero \nJava implementa\u00adtion to execute async-w tasks. In other words, it imple\u00adments the weak atomicity semantics \n[24] for programs con\u00adtaining both async-w and async . In relationship to cur\u00adrent HJ, General Otello \ns async-w and async constructs can be viewed as equivalent to async and async isolated constructs in \nHJ. Further, HJ s isolated construct can be viewed as equivalent to .nish async in General Otello. The \nsyntax of General Otello shown in Figure 7. Prog ::= finish {Stmt} Stmt ::= async {Stmt} | Stmt; Stmt \n| async-w {Stmt} | finish{Stmt} | Block Figure 7. General Otello. Block is arbitrary sequential Java \ncode  4.2 The General Otello compiler Now we present a set of compiler transformations (Figure 8) for \ntranslating programs written in General Otello into a form that simpli.es the runtime implementation \ndescribed below in Section 4.3. While most of these transformations are self-explanatory, transformation \n5 deserves special atten\u00adtion. This transformation moves spawning of an async{S1}that is followed by \na statement S2 to the end of the enclos\u00ading async by capturing the local context of S1 (all the lo\u00adcal \nvariables that are free in S1) at the original point of the spawn (coded as C1 = C(S1)), then spawning \nthe async at the end of the enclosing async with the free local vari\u00adables substituted with the corresponding \nvariables from the captured context (coded as SC=C1 (S1)). Note that this may result in some loss of \nparallelism, since this transformation always moves an async to the end of the enclosing async, even \nif S1 does not con.ict with S2 and the two could po\u00adtentially run in parallel. Transformation 5 is the \nonly one on Figure 8 that has a potential loss of parallelism, all the oth\u00aders preserve the amount of \nwork that is potentially done in parallel in General Otello. The reason for this transformation is implementation\u00addependent. \nSince in our implementation, the assembly that discovers the con.ict always delegates itself to the assembly \nthat already owns the object, we cannot allow the possibility of code in S2 discovering a con.ict with \nS1 and attempting to delegate the outer async to async{S2}, since the outer async contains the spawn \nof async{S2}. A different im\u00adplementation could always decide to delegate inner async to the outer async \nwhenever a con.ict occurs between the two, but would require a complex synchronization mecha\u00adnism between \nthe two assemblies as the outer async would have to notify the inner async that it needs to abort, roll\u00adback \nand delegate itself to the outer async, and than wait for that to happen before proceeding. Another approach \nis a compiler analysis that will determine if S1 and S2 are com\u00admutative and allow them to run in parallel, \nwhich is a subject for future work. The syntax of General Otello in Figure 7 does not include control \n.ow. To allow arbitrary control .ow within a .nish block that contains arbitrary nesting of .nishes and \nasyncs we have implemented a simple runtime strategy. Whenever an async is created inside of arbitrary \ncontrol .ow, the con\u00adtext is captured at the point of creation, but the actual spawn\u00ading of the async \nis delayed until the end of the enclosing finish scope. At .rst glance this may look even more re\u00adstrictive \nthan the transformation 5 from Figure 8 that only moves the async to the end of the outer async and not \nthe enclosing finish scope, but the end result will be the same, since the resulting code will not have \nany effectual code be\u00adtween the outer async and the end of the enclosing .nish. Combined, the transformations \nfrom Figure 8 and the runtime strategy in this section jointly ensure that all asyncs that are created \nwithin a finish scope are spawned at the end of that finish scope.  4.3 The General Otello runtime In \nthis section we present some of the key mechanisms in our implementation of the General Otello execution \nmodel. Rollback log. When an assembly commits, rollback logs for all modi.ed objects are merged with \nthe rollback logs of its parent. These rollback logs are needed in case the parent assembly has to be \nrolled back, requiring the rollback of all its effects, including those caused by nested asyncs. If both \nthe parent and the child have a rollback log for the same object, the child s log is discarded. Ownership. \nWhen an assembly commits, the ownership of all the objects it acquired is also transferred to the parent \nassembly. This is to ensure proper nesting semantics, so that no other assembly can observe the state \ncommitted by the child assembly until the parent assembly commits. Other assemblies running in parallel \nto the parent assembly will still con.ict with it if they try to acquire an object that was modi.ed and \ncommitted by the child assembly. Delegated Task Queues. There are two types of dele\u00adgated task queues. \n1. Assembly Queues. These are the queues that contain del\u00adegated tasks that are to be executed sequentially \nafter the current task commits, as in Aida [22]. An assembly del\u00adegates itself by transferring the ownership \nto the target assembly, and appending its Assembly Queue to the tar\u00adget Assembly Queue.  1. T (f inish{S1}) \n. f inish{TA(S1)} 2. TA(async{S1}; S2) . async{TF A (S1)}; TA(S2)  3. TA(B1; S2) . async{f inish{async{B1}}; \nTF A (S2)} 4. TA(f inish{S1}; S2) . async{f inish{TA(S1)}; TF A (S2)}; 5. TF A (async{S1}; S2) . f \ninish{async{C1 = C(S1)}}; TF A (S2); async{TF A (SC=C1 (S1))} 6. TF A (f inish{S1}; S2) . f inish{TA(S1)}; \nTF A (S2)  7. TF A (B1; S2) . f inish{async{B1}}; TF A (S2)  Figure 8. Compiler transformations to \nconvert General Otello programs into Core Otello 2. Finish Queues. New in Otello are queues of delegated \ntasks that are to be executed sequentially at the end of a finish scope. Tasks might end up in this queue \nwhen a con.ict forces an assembly to delegate itself to the parent assembly, as described above. The \nassembly delegates itself by transferring the ownership and rollback logs to the parent assembly, but \ntransferring its assembly queue to the Finish Queue of the immediately enclosing .nish. Task Queues. \nEvery finish scope contains a queue for assemblies spawned inside of it. This queue is different from \nthe Finish Queue. If an assembly cannot be moved to the end of its outer assembly using transformation \n5 from Figure 8 (i.e. if the spawning of the assembly is inside of some control .ow), then the assembly \nis moved to the Task Queue of the enclosing .nish. When the execution reaches the end of the .nish, it \n.rst spawns in parallel all the assemblies from its Task Queue. These assemblies run in parallel and \nin isolation. If any of them discover a con.ict, they may delegate themselves to their parent assembly \nand transfer their code and their assembly queue to the corresponding Finish Queue. Object acquisition. \nWhen an assembly attempts to acquire an object, the object has to fall into one of the following categories: \n1. Free. These are objects that are not owned be any assem\u00adbly; i.e. their owner pointer is either null \nor points to a dead owner. 2. Owned by ancestor. The object is owned by an ancestor assembly. Ancestor \nassembly has an active finish scope with which the current assembly synchronizes. 3. Owned by a sibling. \nThe object is owned by an assembly that belongs to the same finish scope as the current assembly. 4. \nOwned by an assembly that is not an ancestor or a sibling.  When an assembly requests an object that \nis either free or owned by ancestor the request is successful and the current assembly acquires the ownership \nof the object. Note that it is safe to inherit an object that is owned by an ancestor, as by design the \nancestor is not executing meaningful code (code that accesses ownable objects) at that time. Also note \nthat only one assembly may inherit this speci.c object, as all subsequent acquisition attempts by other \nassemblies will fall into the owned by ancestor or owned by sibling category. When the assembly .nishes, \nit transfers the ownership to its parent, which will (eventually) result in the object ownership being \ntransferred back to ancestor from which it was inher\u00adited. When the object is owned by a sibling or owned, \nthat constitutes a con.ict. If the object is owned by a sibling, the current assembly delegates itself \nto the sibling that owns the object. The current assembly will rollback, transfer its ownerships to the \nsibling, and add its assembly queue to the end of the sibling s assembly queue. Finally if the requested \nobject is owned then the current assembly will delegate itself to its parent assembly, perform\u00ading the \nfollowing steps: 1. First the assembly rolls back its changes. 2. The assembly transfers the ownership \nof all its objects to its parent. 3. The assembly appends its assembly queue to the end of the corresponding \n.nish queue of its parent. 4. The assembly synchronizes with the .nish (as if ended normally) and ends. \n Completing the assembly When a nested assembly .n\u00adishes its execution (including all the assemblies \nin its assem\u00adbly queue) it transfers the ownership of all its objects to its parent and merges its rollback \nlog with the parent s rollback log, ensuring that a parent rollback includes a rollback of the effects \nof nested assemblies. The merge is performed so that the parent s rollback information supersedes the \nchild; if both the parent and the child have rollback information for the same object, the child s rollback \ninformation for that ob\u00adject is discarded, otherwise the child s rollback information for the object \nis added to the parent s rollback log. State backup and recover Otello uses a per-object based log mechanism \nthat backs up the entire object when it is ac\u00adquired by an assembly. For arrays, Otello backs up a chunk \nof the array or the entire array when any of the array ele\u00adments is acquired by an assembly. Most of \nthe other implementation details are similar to what has been done in Aida [22], including a Union-Find \ndata structure for quick test and transfer of object ownership,  and the nonblocking implementation \nof the delegation mech\u00ad 9 anism. However, as mentioned earlier, the Aida approach 10 11 does not guarantee \nisolation with nested task parallelism, 12 which is the main motivation for Otello. 13 14 It may be \ntempting to implement Otello using locks for 15 con.ict detection. However, such an implementation will \n16 17 likely incur prohibitively large overhead. In addition, with\u00ad 18 out using reentrant locks, the \nimplementation would also 19 20 have to handle potential deadlocks. However, this is a direc\u00ad 21 tion \nthat may be worth pursuing in the future if lightweight reentrant locks become available with hardware \nsupport. 5. Evaluation In this section, we present an experimental evaluation of the Otello programming \nmodel implemented as an extension to the HJ compilation and runtime framework [8]. We used this implementation \nto evaluate Otello on collections of nested\u00adparallel benchmarks and transactional benchmarks. 5.1 Experimental \nSetup Our experimental results were obtained on a 16-core (quad\u00adsocket, quad-core per socket) Intel Xeon \n2.4GHz system with 30GB of memory, running Red Hat Linux (RHEL 5) and Sun JDK 1.6 (64-bit version) (with \na heap size was set to 12GB). The software transaction memory (STM) system used for some of the performance \ncomparisons is version 1.3.0 of the Deuce STM [11] which implements the TL2 [12] algorithm. We chose \nsix benchmarks for the nested task-parallel col\u00adlection. One of these benchmarks is the spanning tree \nex\u00adample discussed in Section 2.4. The remaining .ve are HJ ports of the OpenMP 3.0 BOTS benchmarks [13] \nhealth, floorplan, strassen, fft and nqueens. These HJ ports are not new to this paper; they have also \nbeen used in ear\u00adlier performance evaluation, e.g. [3] and [26]. Also, the HJ versions of these benchmarks \nare fundamentally the same as the OpenMP versions; the primary change (in addition to translating C code \nto Java code) is that the OpenMP 3.0 task, taskwait and critical directives were replaced by async, finish \nand isolated statements in HJ, respec\u00adtively. The Otello versions of these benchmarks are iden\u00adtical \nto the HJ versions with one minor syntactic change: eliminating the isolated keyword when moving from \nHJ to Otello, since all async s were assumed to be isolated by default in the Otello version. To understand \nprecisely how the Otello and HJ versions of these codes differ, consider the code listed below, which \nshows the HJ version of nqueens benchmark: 1 v o i d n q u e e n s _ k e r n e l ( f i n a l b y t e \nn , f i n a l b y t e j , 2 f i n a l b y t e [ ] a , f i n a l i n t d e p t h ) { 3 i f (n == j) { \n4 i s o l a t e d { / / e l i m i n a t e d i n O t e l l o v e r s i o n 5 t h i s . t o t a l _ c o \nu n t + + ; 6 } 7 r e t u r n ; 8 } f i n i s h { f o r ( b y t e i = ( b y t e ) 0 ; i < n ; i + + \n) { final byte i0 =i; a s y n c { f i n a l b y t e [ ] b = new b y t e [ j + 1 ] ; S y s t e m . a r \nr a y c o p y ( a , 0 , b , 0 , j ) ; b[j] = i0; i f ( o k ( ( b y t e ) ( j + 1 ) , b ) ) n q u e e \nn s _ k e r n e l ( n , ( b y t e ) ( j + 1 ) , b , d e p t h ) ; } } } } The isolated statement in line \n4 protects shared variable this.total count. HJ currently uses a global lock to im\u00adplement isolation. \nIn the Otello version, the isolated key\u00adword in line 4 is eliminated, since all async tasks run in an \nisolated mode by default in Otello. For the transactional benchmarks, we used seven bench\u00admarks from \nJSTAMP [16] (a Java port of the STAMP bench\u00admark suite [25]), implemented in the Deuce STM [11]. The \nseven benchmarks are Bayes, Genome, Vacation, SSCA2, Intruder, KMeans and Labyrinth3D 5 .  5.2 Comparing \nOtello with standard HJ To evaluate the runtime overhead of the isolation-by-default mechanisms in Otello, \nwe compared the execution times of isolated-by-default Otello and the explicit-isolated-as\u00adneeded HJ \nversions of the six nested task-parallel bench\u00admarks. Figure 9 presents results for these benchmarks \nrun using different numbers of worker threads. In general, Otello shows better scalability than HJ but \nalso fairly high over\u00adhead when ran on a small number of threads. For 16-core executions, the maximum \nslowdown resulting from turn\u00ading on Otello s isolation-by-default semantic was 1.75\u00d7, and the geometric \nmean of the slowdown across six nested\u00adparallel benchmarks was 1.32\u00d7. Interestingly, the minimum slowdown \nwas 0.76\u00d7 i.e., for this benchmark (spanning tree) the execution time with Otello s isolation-by-default \napproach was faster than that of the original HJ program due to the uncovering of more parallelism through \nspeculative parallelism with delegated isolation. In summary, for the nested-parallel benchmarks, our \nre\u00adsults show that Otello can achieve comparable scalability to standard HJ programs (without isolation-by-default \nseman\u00adtic), with a relative overhead that is much lower than that (for example) of many published data-race \ndetection algo\u00adrithms [28].  5.3 Comparing Otello with Deuce STM In this section, we compare the performance \nof Otello with the non-nested transactional parallelism approach in Deuce STM. Figure 10 presents the \nperformance comparison be\u00adtween Otello and Deuce STM, a state-of-the-art Java-based STM implementation. \nOtello offers better performance and 5 We omitted the MatrixMul, and Yada benchmarks, since MatrixMul \nhas a simple critical section that can be implemented as a reduction and Yada exhibits a bug when running \nwith multiple Java threads.  Benchmark Suite. Name Has Isolation Input Size BOTS strassen fft health \n.oorplan nqueens No No Yes Yes Yes nodes: 1,000,000 elements: 1,000,000 largest size size: 15 12 HJ Bench \nspanning tree Yes nodes: 100,000 neighbors: 100 JSTAMP Bayes Genome Vacation SSCA2 Intruder KMeans Labyrinth3D \nYes Yes Yes Yes Yes Yes Yes vars: 32 records: 4096 numbers: 10 percent: 40% edges: 8 inserts: 2 gene \nlength: 16,384 segment length: 64 segments: 1,677,216 clients: 16 queries: 90 relations: 1,048,576 transactions; \n4,194,304 scale: 20 partial edges: 3 max path length: 3 incr edges: 1 attacks: 10 length: 16 numbers: \n1,000,000 nodes: 65536 clusters: 40 128 \u00d7 128 grid Table 1. Benchmarks and relevant input size. Figure \n9. Performance comparison between Otello and HJ. hj : HJ implementation; Otello : Otello implementation; \nX-axis: number of parallel threads (cores) used; Y-axis: execution time in seconds. scalability than \nDeuce STM across all benchmarks with one exception (Bayes on 8 threads). The performance difference is \nespecially evident in KMeans and Labyrinth3D, which are array-intensive programs, where Deuce STM s contention \nmanagement backs up all array elements, while the Otello implementation performs backup of a chunk of \nthe array, rather than a per-element backup (see Section 4). For 16-core executions, the max\u00adimum speedup \nobtained for Otello relative to Deuce was 184.09\u00d7, the minimum speedup was 1.20\u00d7, and the geo\u00admetric \nmean of the slowdowns was 3.66\u00d7. 6. Related Work We use Table 2 to guide the discussion in this section. \nThis table qualitatively classi.es programming models according to the following attributes: Programmability/expressiveness: \nhow easy is it to ex\u00adpress a wide range of parallel programming patterns in the model?  Expertise: how \nmuch expertise in concurrent program\u00adming does the programmer need?  Correctness guarantees: does the \nmodel provide cor\u00adrectness guarantees such as deadlock-freedom, livelock\u00adfreedom, and progress guarantees? \n Scalability: how well does performance scale with an increase in the number of processor cores and \nhardware threads?  Overhead: how much overhead does the model impose relative to a sequential implementation? \n   Parallel prog. model Expressiveness Expertise Safety Scalability Overhead (higher is better) (higher \nis better) (higher is better) (higher is better) (lower is better) STM [15] High Very Low implementation \ndependent Low High Java with .ne-grained locking High High Very low High Low Cilk reducers [5] Medium \nMedium Low High Low Chorus [21] Medium Low High Medium Medium Galois [7, 18, 19] Medium Low Medium High \nLow Aida [22] High Low High High Low Otello [this paper] High Very Low High High Low Table 2. Comparison \nof several parallel programming models. For simplicity, we focus our comparison on current ap\u00adproaches \nto parallel programming with isolation and nest\u00ading. Otello provides a high-level minimalistic programming \nmodel similar to Transactional Memory [15], with a sin\u00adgle construct (async) to de.ne blocks of code \nto be exe\u00adcuted concurrently and in isolation. All parallel tasks in Core Otello are isolated by default, \nthe programmer only needs to worry about what tasks should run in parallel, and all the con.ict detection \nand resolution is done automatically by the implementation. Otello guarantees livelock-freedom, unlike \ntransactional memory where livelock freedom is probabilis\u00adtic and implementation-dependent. The assemblies \nin Chorus [21] and Aida [22] provide a high-level mechanism for irregular data parallelism and have been \nthe main inspiration for assemblies in Otello. The Chorus assemblies, however, are restricted to cautious \napplications [23], while neither Chorus nor Aida support nested parallelism. The delegation mechanism \nin Aida [22] is the main inspi\u00adration for the delegation mechanism in Otello. The delega\u00adtion in Aida, \nhowever, only considers the .at model, where all tasks belong to the same .nish scope. The main contribu\u00adtion \nof Otello is an execution model that performs task del\u00adegation in the presence of arbitrary .nish and \nisolated task nesting. There has also been a lot of work in the transactional memory community on enabling \ndifferent forms of nesting of transactions; however, very few efforts allow the integra\u00adtion of nested \ntransactions with task parallelism. Agrawal et al. [1] propose a design to support Cilk-style nested \nparal\u00adlelism inside transactions, but they do not have an implemen\u00adtation or evaluation. OpenTM [2] allows \ntransactions within OpenMP constructs, but does not allow nested parallelism within transactions. NePaLTM \n[30] is the only TM system we are aware of that integrates nested transactions with task parallelism \n(OpenMP). Otello is built on top of, and adds nested iso\u00adlated tasks to HJ, a more .exible and dynamic \nprogramming model that allows creation of more general task graphs than OpenMP. None of the STM systems \nmentioned use delega\u00adtion as the mechanism for con.ict resolution, nor do they offer an upper bound on \ncon.ict-to-commit ratio in heavily\u00adcongested scenario as Otello does. Programming models such as OpenMP \n[9] and Cilk [5] provide ef.cient support for reductions in deterministic par\u00adallel programs. However, \nthose constructs (or the constructs offered by DPJ [6]) are not applicable to the nondeterminis\u00adtic, \nirregular parallel programs supported by Otello.  Orthogonal to Otello is prior work on data-race detec\u00adtion \n[14]. Otello gives the option of proceeding (with well\u00adde.ned semantics) in the presence of data races, \nwhile the other models do not. If there is a desire to warn the user about interference among parallel \naccesses to certain classes of objects, past work on data-race detection can be applied to Otello for \nthat purpose. Herlihy and Koskinen proposed a form of checkpointing and continuations [17] to handle \npartial commits and roll\u00adback of transactions. We implemented a similar mechanism in Otello as an optimization \nto allow partial commits of long\u00adrunning asyncs to avoid super.cial con.icts. 7. Conclusions and Future \nWork Isolation has long been one of the most basic concerns in parallel programming. Despite the recent \nattention paid to software and hardware approaches to transactional mem\u00adory, there is no transactional \nmemory solution in past work that guarantees isolation by default for all code while sup\u00adporting nested \nparallelism. In this paper, we presented a programming and execution model called Otello that sup\u00adports \nan isolation-by-default approach. In our model, the programmer only exposes parallelism by creating and \nsyn\u00adchronizing parallel tasks (using async and finish con\u00adstructs), leaving it to the Otello compiler \nand runtime sys\u00adtem to ensure isolation among parallel tasks. Otello s pro\u00adgramming model offers several \nhigh-level correctness guar\u00adantees while requiring minimal programmer expertise. The practicality of \nOtello stems from the novel compiler and runtime techniques presented in this paper. We used these techniques \nto implement Otello on top of the Habanero Java parallel programming language, and to evaluate Otello \non collections of nested-parallel benchmarks and transactional benchmarks. For the nested-parallel benchmarks, \nthe maxi\u00admum slowdown resulting from turning on Otello s isolation\u00adby-default support was 1.75\u00d7, and \nthe geometric mean of the slowdown across six nested-parallel benchmarks was 1.32\u00d7 which is signi.cantly \nlower than the relative overhead of many published data-race detection algorithms. For the transactional \nbenchmarks, our results show that Otello incurs lower overhead than a state-of-the-art software transactional \nmemory system (Deuce STM). For 16-core executions, the maximum speedup obtained for Otello relative to \nDeuce was 184.09\u00d7, the minimum speedup was 1.20\u00d7, and the geomet\u00adric mean of the speedup across seven \ntransactional bench\u00admarks was 3.66\u00d7. Future work includes developing compiler and runtime optimizations \nto further reduce the overhead of ensuring iso\u00adlation by default, exploration of new strategies for delega\u00adtion, \nand supporting greater integration of Otello with addi\u00adtional HJ constructs for task parallelism, including \nfutures, data-driven tasks, and phasers [29]. Acknowledgments This work was supported in part by NSF \naward CCF\u00ad0964520. Any opinions, ndings and conclusions or recom\u00admendations expressed in this material \nare those of the au\u00adthors and do not necessarily re.ect those of the National Science Foundation. We \nwould like to thank members of the Habanero group at Rice for valuable discussions related to this work, \nand contributions to the Habanero Java infrastruc\u00adture used in this research. We are grateful to the \nanonymous reviewers for their comments and suggestions. Finally, we would like to thank Keith Cooper \nfor providing access to the Xeon system used to obtain the performance results reported in this paper. \nReferences [1] Kunal Agrawal, Jeremy T. Fineman, and Jim Sukha. Nested parallelism in transactional memory. \nIn Proceedings of PPoPP 08, Salt Lake City, UT, USA, pages 163-174. [2] Woongki Baek, Chi Cao Minh, Martin \nTrautmann, Christos Kozyrakis, and Kunle Olukotun. The OpenTM transactional application programming interface. \nIn Proceedings of PACT 07, pages 376387. [3] Rajkishore Barik, Jisheng Zhao, and Vivek Sarkar. Interproce\u00addural \nstrength reduction of critical sections in explicity-parallel programs. In Proceedings of PACT 13, 2013. \n[4] Ganesh Bikshandi, Jose G. Castanos, Sreedhar B. Kodali, V. Krishna Nandivada, Igor Peshansky, Vijay \nA. Saraswat, Sayantan Sur, Pradeep Varma, and Tong Wen. Ef.cient, portable implementation of asynchronous \nmulti-place programs. In Proceedings of PPoPP 09.  [5] Robert D. Blumofe and Charles E. Leiserson. Scheduling \nmultithreaded computations by work-stealing. In Proceedings of FOCS 94, 1994. [6] Robert L. Bocchino, \nStephen Heumann, Nima Honarmand, Sarita V. Adve, Vikram S. Adve, Adam Welc, and Tatiana Shpeisman. Safe \nnondeterminism in a deterministic-by-default parallel language. In Proceedings of POPL 11, 2011. [7] \nMartin Burtscher, Milind Kulkarni, Dimitrios Prountzos, and Keshav Pingali. On the scalability of an \nautomatically parallelized irregular application. In Proceedings of LCPC 08, pages 109 123. [8] Vincent \nCave , Jisheng Zhao, Jun Shirako, and Vivek Sarkar. Habanero-Java: the New Adventures of Old X10. In \nProceedings of PPPJ 11, 2011. [9] Robit Chandra, Ramesh Menon, Leo Dagum, David Kohr, Dror Maydan, and \nJeff McDonald. Parallel programming in OpenMP. Morgan Kaufmann Publishers Inc., 2001. [10] P. Charles \net al. X10: an object-oriented approach to non\u00aduniform cluster computing. In Proceedings of OOPSLA 05, \nNew York, NY, USA, 2005. [11] Deuce STM -Java Software Transactional Memory. http://www.deucestm.org/. \n[12] David Dice, Ori Shalev, and Nir Shavit. Transactional locking II. In Proceedings of DISC 06, pages \n194208, 2006.  [13] Alejandro Duran et al. Barcelona OpenMP Tasks Suite: a set of benchmarks targeting \nthe exploitation of task parallelism in OpenMP. In Proceedings of ICPP 09, 2009. [14] Cormac Flanagan \nand Stephen N. Freund. FastTrack: ef.cient and precise dynamic race detection. In Proceedings of PLDI \n09, pages 121 133. ACM, 2009. [15] Tim Harris, James R. Larus, and Ravi Rajwar. Transactional Memory, \n2nd Edition. Morgan and Claypool, 2010. [16] JSTAMP. Jstamp. http://demsky.eecs.uci.edu/software.php. \n[17] Eric Koskinen and Maurice Herlihy. Checkpoints and continuations instead of nested transactions. \nIn Proceedings of SPAA 08, pages 160168, jun 2008. [18] Milind Kulkarni, Martin Burtscher, Rajasekhar \nInkulu, Keshav Pingali, and Calin Cascaval. How much parallelism is there in irregular applications? \nIn Proceedings of PPoPP 09, pages 314. [19] Milind Kulkarni, Keshav Pingali, Bruce Walter, Ganesh Ramanarayanan, \nKavita Bala, and L. Paul Chew. Optimistic parallelism requires abstractions. In Proceedings of PLDI 07, \npages 211222, New York, NY, USA, 2007. ACM. [20] J. Larus and R. Rajwar. Transactional Memory. Morgan \nand Claypool, 2006. [21] Roberto Lublinerman, Swarat Chaudhuri, and Pavol Cerny. Parallel programming \nwith object assemblies. In Proceedings of OOPSLA 09, New York, NY, USA, 2009. ACM. [22] Roberto Lublinerman, \nJisheng Zhao, Zoran Budimlic , Swarat Chaudhuri, and Vivek Sarkar. Delegated isolation. In Proceedings \nof OOPSLA 11, NY, USA, 2011. [23] Mario Me ndez-Loj, Donald Nguyen, Dimitrios Prountzos, Xin Sui, M. \nAmber Hassaan, Milind Kulkarni, Martin Burtscher, and Keshav Pingalil. Structure-driven optimizations \nfor amor\u00adphous data-parallel programs. In Proceedings of PPoPP 10, pages 3 14, New York, NY, USA, 2010. \nACM. [24] Vijay Menon, Ali reza Adl-tabatabai, Steven Balensiefer, Richard L. Hudson, Adam Welc, Tatiana \nShpeisman, and Bratin Saha. Practical weak-atomicity semantics for java stm. In Proceedings of ACM Symposium \non Parallellism in Algorithms and Architectures, 2008. [25] Chi Cao Minh, JaeWoong Chung, Christos Kozyrakis, \nand Kunle Olukotun. STAMP: Stanford transactional applications for multiprocessing. In Proceedings of \nIISWC 08, pages 3546, 2008. [26] V. Krishna Nandivada, Jun Shirako, Jisheng Zhao, and Vivek Sarkar. A \ntransformation framework for optimizing task-parallel programs. ACM Trans. Program. Lang. Syst., 35(1):3, \n2013. [27] OpenMP Application Program Interface, version 3.0, May 2008. http://www.openmp.org/mp-documents/spec30.pdf. \n[28] Raghavan Raman, Jisheng Zhao, Vivek Sarkar, Martin Vechev, and Eran Yahav. Scalable and precise \ndynamic datarace detection for structured parallelism. In Proceedings of PLDI 12, pages 531 542. [29] \nJun Shirako, David M. Peixotto, Vivek Sarkar, and William N. Scherer. Phasers: a uni.ed deadlock-free \nconstruct for collective and point-to-point synchronization. In Proceedings of ICS 08, pages 277 288. \n[30] Haris Volos, Adam Welc, Ali-reza Adl-tabatabai, Tatiana Sh\u00adpeisman, Xinmin Tian, and Ravi Narayanaswamy. \nNePaLTM: Design and implementation of nested parallelism for trans\u00adactional memory systems. In Proceedings \nof ECOOP 09, jun 2009.  Appendix A: Formal Semantics of Otello Heaps The central data structure in Core \nOtello is the shared-memory heap, which maintains the state of all shared mutable data accessed by a \nprogram. We abstractly view a heap as a directed graph whose nodes are objects, and edges are pointers \nlabeled with .eld names. A region R in a heap G is a subset of the nodes of G. Assembly states Let us \n.rst de.ne a variable state of pro\u00adgram P over a heap G. Such a state is a function \u00b5 that maps the variables \nof P either to objects in G, or to the symbol null. We emphasize that an assembly is not required to \nown the objects to which its variables point. However, once it tries to read from or write to such an \nobject u, it acquires owner\u00adship of u, or alternately delegates to the owner of u. Consider the code \nK = S1 ; . . . ; Sm that can be executed by an assembly, where each Si is either an assignment, or a \nnested async or finish-block. We interpret such a block as a list [S1 , . . . , Sm ]. A closure of K \nis a triple (K1 , K2 , \u00b5), where K1 and K2 are the sublists of K that, respectively, have not and have \nbeen executed (we have K = K2 ; K1 ), and \u00b5 is a variable state of P . An assembly state over a heap \nG is a tuple of the form N = ((K1 , K2 , \u00b5), R, Q), where (K1 , K2 , \u00b5) is the closure currently being \nexecuted, R is the region of G currently owned by A, and Q is a list where head (Q) is the current closure, \nand tail(Q) is the list of closures that A will execute in sequence once the current closure is committed. \nTwo assembly states N1 and N2 are disjoint if the heap regions R1 and R2 referenced in N1 and N2 have \nno nodes in common. Concurrent state A concurrent state of a Core Otello pro\u00adgram is de.ned as a tree-shaped \nstructure where a node is an ID for a speci.c .nish-scope, and each node is associated with a collection \nof assembly IDs. There is a tree edge from an assembly ID A to a node F if the .nish-scope represented \nby F is nested within the asynchronous task represented by A. At any point of an execution, each assembly \nID is associ\u00adated with a speci.c assembly state. Formally, let us assume a universe of assembly IDs and \na universe of node IDs (we assume these two sets to be disjoint). We de.ne a .nish-tree to be a pair \nT = (VT , .T , --+T ), where VT is a set of node IDs, .T is a map that associates each F . V to a set \nof assembly IDs, and --+T is a set of edges. An edge in a .nish-tree has the form ' ' (A, F ), where \nF . V and A . F for some F . V . If A --+T F , then A is said to be the parent of F . We denote by --++ \nthe transitive closure in T , de.ned as the least set T of pairs (A, F ) such that, for some F ' , A \n', we have (A, F ' ), A ' . F ', and A ' --+* F '. If A --+* F ', then A is an T T ancestor of F . It \nis required that: For any two distinct F1, F2 . V , .(F1) n .(F2) = \u00d8. The structure T is tree-shaped. \nThis means that, .rst of all, there is no sequence F0, A0, F1, A1 . . . , Fk, Ak such that: (1) for all \ni, Fi . V , Ai . Fi and Ai --+T Fi+1, and (2) Ak --+T F1. Second, there exists a root node F0 such that \nfor each node F = F0 in T , there exists an assembly ID A such that A . F0 and A --++ F . T Abusing notation, \nwe write F . T = (VT , .T , --+T ) if F . VT , and T (F ) to denote the set .T (F ). Assemblies A1 and \nA2 are said to be siblings A1, A2 . T (F ) for some node F in T . We say that an assembly A is a leaf \nin T , and write Leaf (A, T ) if there is no node F such that A --+T F . A concurrent state of a program \nP is now de.ned as a tuple (G, T , S, .), where G is a heap, T is a .nish-tree, S is a function that \nassigns an assembly state S(A) to each assembly ID A in T , and . is a function that associates each \nnode F in T with a list of closures. For any two distinct assembly IDs A1 and A2, we require S(A1) and \nS(A2) to be disjoint. In other words, an object in our model is owned by at most one assembly. The objects \nin G that do not belong to the regions referenced by S(A), for assembly IDs A, are said to be free; we \ndenote the set of all free objects by Free (S). Transitions Transition relation -. over states de.nes \nthe operational semantics of Core Otello. The rules de.ning this relation are shown in Figures 11 and \n12. Here, S[A1 . N1; . . . ; Ak . Nk; A ..] denotes a map S ' such that S ' (Ai) = Ni, S ' (A) is unde.ned, \nand S ' (A ' ) = S(A ' ) for all other A ' in the domain of S (a similar notation is de.ned for the map \n.). Also, T [F . A] and T [A --+ F ] respectively denote a tree T ' where T ' (F ) = A and that is identical \nto T otherwise, and a tree T ' that is obtained by adding a node F and an edge A --+T F to T . The transitions \nin -. update the concurrent state of a program P . Most of these transitions select an assembly from \nthe current .nish-tree and execute the next statement in it. For an assembly to be scheduled for execution, \nit must be a leaf in the current .nish-tree. The need for delegation arises when an assembly tries to \naccess an object owned by a different assembly. Speci.cally, the rule CO N FL I C T-SA M E -LE V E L \nis used to de.ne delegation among assemblies that are siblings of each other. In the premise of the rule, \nA1 and A2 are assemblies, respectively at states N1 and N2, and Q1 is the list of clo\u00adsures that A1 is \nobligated to execute. Further, the .rst ele\u00adment head (Q1) of this list is already under execution. The \ncode for this closure is of the form K ' ; K1 . Of this code, K ' 1 1 has already been executed; the \nstatement that is to be exe\u00adcuted now is head (K1 ). However, the statement head (K1 ) reads or writes \nan ob\u00adject u that is currently owned by the assembly A2 (i.e., a con.ict happens). Consequently, A1 must \nnow delegate its work and owned region to A2. After the rule .res, the state of A2 becomes N '. Note \nthat the closure queue of A2 is now append (Q2, Q1). Since A1 may have modi.ed certain objects in its \nregion while it was executing head (Q1), the runtime rolls back  (CO NFL IC T) N1 = ((K1 , K11 , \u00b51), \nR1, Q1) head (K1 ) . {(x.f := t), (t := x.f)}N2 = ((K2 , K12 , \u00b52), R2, Q2) \u00b51(x) = u u . R2 con.ict \n(N1, N2, u) (CO NFL IC T-SA M E -LE V E L) F . T A1, A2 . T (F ) Leaf (A1, T ) S(A1) = ((K1 , K11 , \n\u00b51), R1, Q1) S(A2) = ((K2 , K12 , \u00b52), R2, Q2) con.ict (S(A1), S(A2), u) N1 = ((K2 , K12 , \u00b52), R1 . \nR2, append (Q2, Q1)) S1 = S[A1 ..; A2 . N1] (G, T , S, .) -. (rollback (G, K11 ), V, E, S1, .) (CO NFL \nIC T-BEL OW ) F, F 1 . T A1, A3 . T (F ) A2 . T (F 1) A3 --++ F 1 T Leaf (A1, T ) S(A1) = (A1, (K1 \n, K11, \u00b51), R1, Q1) S(A2) = ((K2 , K21, \u00b52), R2, Q2) con.ict (S(A1), S(A2), u) N3 = ((K3 , K13 , \u00b53), \nR3, Q3) N 1 = ((K3 , K13 , \u00b53), R1 . R3, append (Q3, Q1)) S1 = S[A1 ..; A3 . N1] (G, T , S, .) -. (rollback \n(G, K11 ), T , S1, .) (CO NFL IC T-ANC E S TO R) F . T A1 . T (F ) A2 --++ T F Leaf (A1, T ) S(A1) = \n((K1 , K11, \u00b51), R1, Q1) S(A2) = ((K2 , K21, \u00b52), R2, Q2) con.ict (S(A1), S(A2), u) N11 = ((K1 , K11 \n, \u00b51), R1 . {u}, Q1) 11 N2 = ((K2 , K21, \u00b52), R1 \\ {u}, Q2) S1 = S[A1 . N11; A2 . N2] (G, T , S, .) -. \n(G, T , S1, .) (CO NFL IC T-UNR E LAT E D ) F, F 1 . T A2 . T (F ) A1 . T (F 1) A3 --+T F 1 T F A1--++ \nA2 T F 1 Leaf (A1, T ) 1 , \u00b51), R1, Q1) --++ S(A1) = ((K1 , K1 S(A2) = ((K2 , K12 , \u00b52), R2, Q2) S(A3) \n= ((K3 , K31, \u00b53), R3, Q3) con.ict (S(A1), S(A2), u) N1 = ((K3 , K13 , \u00b53), R1 . R3, Q3) S1 = S[A1 ..; \nA3 . N1] .1 = .[F 1 . append (.(F ), Q1)] (G, T , S, .) -. (rollback (G, K11 ), T , S, .1) (LOC A L \n-AC CESS-1) Leaf (A, T ) S(A) = ((K1 , K2 , \u00b5), R, Q) head (K1 ) . {(x.f := t), (t := x.f)} head (K1 \n) \u00b5(x) = u u . R or u . Free (S) \u00b5 -. \u00b5 1 head (K1 ) N1 G1 S1 = ((tail(K1 ), K2 ;head (K1 ), \u00b51), R \n. {u}, Q) G -. = S[A . N1] (G, T , S, .) -. (G1, T , S1, .) Leaf (A, T ) S(A) = ((K1 , K2 , \u00b5), R, Q) \nhead (K1 ) head (K1 ) has form (x := t) \u00b5 -. \u00b5 1 N1 = ((tail(K1 ), (K2 ; head (K1 )), \u00b51), R, Q) S1 \n= S[A . N1] (LOC A L -AC CESS-2) (G, T , S, .) -. (G1, T , S1, .) F . T A . T (F ) Leaf (A, T ) S(A) \n= ((K1 , K2 , \u00b5), R, Q) head (K1 ) = async{ K3 } = ((tail(K1 ), K2 ; head (K1 ), \u00b5), R, Q) N1 A11 is \na fresh assembly ID . = (K3 , E, \u00b5) N 11 = (., \u00d8, {.}) S1 = S[A . N1; A11 . N11] T 1 = T [F . T (F ) \n. {A11}] (AS Y NC ) (G, T , S, .) -. (G, T 1, S1, .) Figure 11. Operational semantics of Core Otello. \n F . T A . T (F ) Leaf (A, T ) S(A) = (E, R, Q)S1 = S[A . (second (Q), R, tail(Q))] (NEX T-CL OSUR E \n) (G, T , S, .) -. (G, T , S1 , .) F . T A . T (F ) Leaf (A, T ) S(A) = (E, R, \u00d8) = T [F . T (F ) \\ \n{A}] A1 --+T F T 1 S(A1) = ((K1 1 , K1 2 , \u00b5 1), R1, Q1)N1 = ((K1 1 , K1 2 , \u00b5 1), R . R1, Q1) S1 = \nS[A .., A1 . N1] (EM P T Y-QU E U E) (G, T , S, .) -. (G, T , S1 , .) F . T A . T (F ) Leaf (A, T ) \nS(A) = ((K1 , K2 , \u00b5), R, Q)head (K1 ) = finish{ K3 } F 1 is a fresh node ID = T [F 1 . \u00d8; A --+ F 1] \nT 1 . = (K3 , E, \u00b5) .1 = .[F 1 . {.}] (FINI S H ) (G, T , S, .) -. (G, T 1 , S, .1) F . T T (F ) = \u00d8 \n.(F ) = \u00d8 A is a fresh assembly ID N1 = (E, \u00d8, .(F )) S1 = S[A . N1] (NEX T-CL OSUR E -FINI S H ) (G, \nT , S, .) -. (G, T [F . {A}], S1 , .[F . \u00d8]) F .T T (F ) = \u00d8 .(F ) = \u00d8 (EN D -FI N ISH ) (G, T , S, \n.) -. (G, T [F ..], S, .[F ..]) Figure 12. Operational semantics of Core Otello (Continued). the effect \nof the code K ' before delegation. We denote by 1 rol lback (G, K ' ) the result of atomically applying \nthis roll\u00ad 1 back on G. Note that because A1 is required to be a leaf in the .nish\u00adtree T and also because \nof the syntactic restrictions in Core Otello, the code K ' is solely a collection of heap updates it \n1 does not include any async or finish statements. The im\u00adplementation of this rollback operation is \ntherefore straight\u00adforward. The rule CO N FL I C T-B E L OW describes delegation when A1 con.icts with \nA2, and there is an assembly A3 that is a sibling of A1 and an ancestor of the node containing A2. In \nthis case, A1 delegates to A3. The rule CO N FL I C T-AN C E S TO R handles the case when A1 tries to \naccess an object u owned by an ancestor A2 of its node. Because A2 is not a leaf node in the current \n.nish-tree, it is currently suspended ; therefore, A1 can safely steal u from A2. The rule CO N FL I \nC T-UN R E L AT E D handles the remaining con.ict scenarios. Here, if A1 . T (F ) con.icts with A2, then \nA1 transfers its owned region to the parent A3 of the node F . The code executed by A1 is now to be executed \nafter all the assemblies in T (F ) .nish executing, and is put in the closure queue for F . The rule \nLO C A L -AC C E S S -1 formalizes read and write accesses by an assembly A to objects not owned by others. \n' The local state of A changes to \u00b5 from \u00b5 on executing head (K1 ) head (K1 ) (this is denoted by \u00b5 -. \n\u00b5 '); the heap changes head (K1 ) from G to G ' (this is denoted by G -. G '). If the object is not already \nin A s region, it is added. However, there is no delegation. The rule LO C A L -AC C E S S -2 formalizes \nactions that update an assembly s variable state, but do not modify the heap. The rule AS Y N C de.nes \nthe semantics of assembly cre\u00adation. The rule creates a new assembly A ' with an empty set of owned objects. \nThe assembly A ' belongs to the same tree node (.nish-scope) as the assembly that created it. NE X T-C \nL O S U R E: when an assembly .nishes executing a closure (we denote a closure that has executed all \nits code by E) i.e., the closure is committed, it selects for execution the second item in its closure \nqueue (the .rst item is the closure that just .nished executing); if Q has just one element, then second \n(Q) is de.ned to be E. EM P T Y-QU E U E: when the queue of closures for an as\u00adsembly becomes empty, \nthe assembly can be removed from the concurrent state of the current .nish-tree node. The rule FI N I \nS H de.nes the semantics of finish \u00adstatements executed by an assembly A. Here a new tree node ' F is \ncreated; A is the parent of F ', and T (F ' ) = \u00d8. The code inside the finish-block is transfered to \nthe closure queue of F . Executions and termination Consider a program P = finish{K} in Core Otello. \nAn initial state of P is a concur\u00adrent state p0 = (G, T , S, .), where T consists of a single node F \n, T (F ) = \u00d8, S is the empty map, and .(F ) = . for . = (K, E, \u00b5) for some \u00b5. A terminating state of \na Core Otello program is a con\u00adcurrent state pn = (G, T , S, .) where T is the empty tree. A Core Otello \nprogram is said to terminate if for all initial states p0, there is a terminating state pn such that \np0 -.* pn.    \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Isolation--the property that a task can access shared data without interference from other tasks--is one of the most basic concerns in parallel programming. Whilethere is a large body of past work on isolated task-parallelism, the integration of isolation, task-parallelism, and nesting of tasks has been a difficult and unresolved challenge. In this pa- per, we present a programming and execution model called Otello where isolation is extended to arbitrarily nested parallel tasks with irregular accesses to heap data. At the same time, no additional burden is imposed on the programmer, who only exposes parallelism by creating and synchronizing parallel tasks, leaving the job of ensuring isolation to the underlying compiler and runtime system.</p> <p>Otello extends our past work on Aida execution model and the delegated isolation mechanism [22] to the setting of nested parallelism. The basic runtime construct in Aida and Otello is an assembly: a task equipped with a region in the shared heap that it owns. When an assembly A conflicts with an assembly B, A transfers--or delegates--its code and owned region to a carefully selected assembly C in a way that will ensure isolation with B, leaving the responsibility of re-executing task A to C. The choice of C depends on the nesting relationship between A and B.We have implemented Otello on top of the Habanero Java (HJ) parallel programming language [8], and used this implementation to evaluate Otello on collections of nested task-parallel benchmarks and non-nested transactional benchmarks from past work. On the nested task-parallel benchmarks, Otello achieves scalability comparable to HJ programs without built-in isolation, and the relative overhead of Otello is lower than that of many published data-race detection algorithms that detect the isolation violations (but do not enforce isolation). For the transactional benchmarks, Otello incurs lower overhead than a state-of-the-art software transactional memory system (Deuce STM).</p>", "authors": [{"name": "Jisheng Zhao", "author_profile_id": "81361600781", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P4290413", "email_address": "jisheng.zhao@rice.edu", "orcid_id": ""}, {"name": "Roberto Lublinerman", "author_profile_id": "81317497568", "affiliation": "Google, Inc., Menlo Park., CA, USA", "person_id": "P4290414", "email_address": "rluble@google.com", "orcid_id": ""}, {"name": "Zoran Budimli&#263;", "author_profile_id": "81100227584", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P4290415", "email_address": "zoran@rice.edu", "orcid_id": ""}, {"name": "Swarat Chaudhuri", "author_profile_id": "81309496839", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P4290416", "email_address": "swarat@rice.edu", "orcid_id": ""}, {"name": "Vivek Sarkar", "author_profile_id": "81100597290", "affiliation": "Rice University, Houston, TX, USA", "person_id": "P4290417", "email_address": "vsarkar@rice.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509534", "year": "2013", "article_id": "2509534", "conference": "OOPSLA", "title": "Isolation for nested task parallelism", "url": "http://dl.acm.org/citation.cfm?id=2509534"}