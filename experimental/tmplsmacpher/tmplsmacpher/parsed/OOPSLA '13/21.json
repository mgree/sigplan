{"article_publication_date": "10-29-2013", "fulltext": "\n Ball-Larus Path Pro.ling Across Multiple Loop Iterations Daniele Cono D Elia Camil Demetrescu Dept. \nof Computer, Control and Management Dept. of Computer, Control and Management Engineering Engineering \n Sapienza University of Rome Sapienza University of Rome delia@dis.uniroma1.it demetres@dis.uniroma1.it \n Abstract Identifying the hottest paths in the control .ow graph of a routine can direct optimizations \nto portions of the code where most resources are consumed. This powerful method\u00adology, called path pro.ling, \nwas introduced by Ball and Larus in the mid 90 s [4] and has received considerable at\u00adtention in the \nlast 15 years for its practical relevance. A shortcoming of the Ball-Larus technique was the inability \nto pro.le cyclic paths, making it dif.cult to mine execu\u00adtion patterns that span multiple loop iterations. \nPrevious re\u00adsults, based on rather complex algorithms, have attempted to circumvent this limitation at \nthe price of signi.cant per\u00adformance losses even for a small number of iterations. In this paper, we \npresent a new approach to multi-iteration path pro.ling, based on data structures built on top of the \noriginal Ball-Larus numbering technique. Our approach allows the pro.ling of all executed paths obtained \nas a concatenation of up to k Ball-Larus acyclic paths, where k is a user-de.ned parameter. We provide \nexamples showing that this method can reveal optimization opportunities that acyclic-path pro\u00ad.ling would \nmiss. An extensive experimental investigation on a large variety of Java benchmarks on the Jikes RVM \nshows that our approach can be even faster than Ball-Larus due to fewer operations on smaller hash tables, \nproducing compact representations of cyclic paths even for large val\u00adues of k. Categories and Subject \nDescriptors C.4 [Performance of Systems]: Measurement Techniques; D.2.2 [Software Engi\u00adneering]: Tools \nand Techniques programmer workbench; D.2.5 [Software Engineering]: Testing and Debugging diagnostics, \ntracing Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. Copyrights for components of \nthis work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, \nUSA. Copyright c &#38;#169;2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509521 \nGeneral Terms Algorithms, Measurement, Performance. Keywords Pro.ling, dynamic program analysis, instru\u00admentation. \n1. Introduction Path pro.ling is a powerful methodology for identifying per\u00adformance bottlenecks in a \nprogram. The approach consists of associating performance metrics, usually frequency coun\u00adters, to paths \nin the control .ow graph. Identifying hot paths can direct optimizations to portions of the code that \ncould yield signi.cant speedups. For instance, trace scheduling can improve performance by increasing \ninstruction-level paral\u00adlelism along frequently executed paths [13, 22]. The sem\u00adinal paper by Ball and \nLarus [4] introduced a simple and elegant path pro.ling technique. The main idea was to im\u00adplicitly number \nall possible acyclic paths in the control .ow graph so that each path is associated with a unique compact \npath identi.er (ID). The authors showed that path IDs can be ef.ciently generated at runtime and can \nbe used to update a table of frequency counters. Although in general the num\u00adber of acyclic paths may \ngrow exponentially with the graph size, in typical control .ow graphs this number is usually small enough \nto .t in current machine word-sizes, making this approach very effective in practice. While the original \nBall-Larus approach was restricted to acyclic paths obtained by cutting paths at loop back edges, pro.ling \npaths that span consecutive loop iterations is a de\u00adsirable, yet dif.cult, task that can yield better \noptimization opportunities. Consider, for instance, the problem of elim\u00adinating redundant executions \nof instructions, such as loads and stores [7], conditional jumps [6], expressions [9], and array bounds \nchecks [8]. A typical situation is that the same instruction is redundantly executed at each loop iteration, \nwhich is particularly common for arithmetic expressions and load operations [7, 9]. To identify such \nredundancies, paths that extend across loop back edges need to be pro.led. An\u00adother application is trace \nscheduling [22]: if a frequently exe\u00adcuted cyclic path is found, compilers may unroll the loop and perform \ntrace scheduling on the unrolled portion of code. Tallam et al. [20] provide a comprehensive discussion \nof the bene.ts of multi-iteration path pro.ling. We provide further examples in Section 3. Different \nauthors have proposed techniques to pro.le cyclic paths by modifying the original Ball-Larus path numbering \nscheme in order to identify paths that extend across multi\u00adple loop iterations [17, 19, 20]. Unfortunately, \nall known so\u00adlutions require rather complex algorithms that incur severe performance overheads even for \nshort cyclic paths, leaving the interesting open question of .nding simpler and more ef.cient alternative \nmethods. Contributions. In this paper, we present a novel approach to multi-iteration path pro.ling, \nwhich provides substan\u00adtially better performance than previous techniques even for long paths. Our method \nstems from the observation that any cyclic execution path in the control .ow graph of a routine can be \ndescribed as a concatenation of Ball-Larus acyclic paths (BL paths). In particular, we show how to accurately \npro.le all executed paths obtained as a concatenation of up to kBL paths, where kis a user-de.ned parameter. \nThe main results of this paper can be summarized as follows: 1. we reduce multi-iteration path pro.ling \nto the problem of counting n-grams, i.e., contiguous sequences of n items from a given sequence. To compactly \nrepresent collected pro.les, we organize them in a forest of pre.x trees (or tries) [14] of depth up \nto k, where each node is labeled with a BL path, and paths in a tree represent concatenations of BL paths \nthat were actually executed by the program, along with their frequencies. We also present an ef.cient \nconstruction algorithm based on a variant of the k-SF data structure presented in [3]; 2. we discuss \nexamples of pro.le-driven optimizations that can be achieved using our approach. In particular, we show \nhow the pro.les collected with our method can help to restructure the code of an image processing applica\u00adtion, \nachieving speedups up to about 30% on an image .lter based on convolution kernels; 3. to evaluate the \neffectiveness of our ideas, we developed a Java performance pro.ler in the Jikes Research Virtual Machine \n[1]. To make fair performance comparisons with state-of-the-art previous pro.lers, we built our code \non top of the BLPP pro.ler developed by Bond [10, 15], which provides an ef.cient implementation of the \nBall-Larus acyclic-path pro.ling technique. Our Java code was endorsed by the OOPSLA 2013 Artifact Evaluation \nCommittee and is available on the Jikes RVM Research Archive. We also provide a C implementation of our \npro.ler for manual source code instrumentation; 4. we performed a broad experimental study on a large \nsuite of prominent Java benchmarks on the Jikes Research Virtual Machine, showing that our pro.ler can \ncollect pro.les that would have been too costly to gather using previous multi-iteration techniques. \n  Ball-Larus path numbering and tracing framework BL path profiler k-iter. path profiler Figure 1: \nOverview of our approach: classical Ball-Larus pro.ling versus k-iteration path pro.ling, cast in a common \nframework. Techniques. Differently from previous approaches [17, 19, 20], which rely on modifying the \nBall-Larus path numbering to cope with cycles, our method does not require any modi.\u00adcation of the original \nnumbering technique described in [4]. The main idea behind our approach is to fully decouple the task \nof tracing Ball-Larus acyclic paths at run time from the task of concatenating and storing them in a \ndata structure to keep track of multiple iterations. The decoupling is per\u00adformed by letting the Ball-Larus \npro.ling algorithm issue a stream of BL path IDs (see Figure 1), where each ID is generated when a back \nedge in the control .ow graph is tra\u00adversed or the current procedure is abandoned. As a conse\u00adquence \nof this modular approach, our method can be imple\u00admented on top of existing Ball-Larus path pro.lers, \nmaking it simpler to code and maintain. Our pro.ler introduces a technical shift based on a smooth blend \nof the path numbering methods used in in\u00adtraprocedural path pro.ling with data structure-based tech\u00adniques \ntypically adopted in interprocedural pro.ling, such as calling-context pro.ling. The key to the ef.ciency \nof our approach is to replace costly hash table accesses, which are required by the Ball-Larus algorithm \nto maintain path coun\u00adters for larger programs, with substantially faster operations on trees. With this \nidea, we can pro.le paths that extend across many loop iterations in comparable time, if not faster, \nthan pro.ling acyclic paths on a large variety of industry\u00adstrength benchmarks. Organization of the paper. \nIn Section 2 we describe our approach and in Section 3 we discuss examples of possible performance optimizations \nbased on the proposed technique. Section 4 describes the algorithmic ideas behind our tool and Section \n5 provides implementation details. The results of our experimental investigation are detailed in Section \n6 and related work is surveyed in Section 7. Concluding remarks are given in Section 8. 2. Approach \nIn this section we provide an overview of our approach to multi-iteration path pro.ling. From a high-level \npoint of view, illustrated in Figure 1, the entire process is divided into two main phases: 1. instrumentation \nand execution of the program to be pro\u00ad.led (top of Figure 1); 2. pro.ling of paths (bottom of Figure \n1).  The .rst phase is almost identical to the original approach described in [4]. The target program \nis statically analyzed and a control .ow graph (CFG) is constructed for each rou\u00adtine of interest. The \nCFG is used to instrument the original program by inserting probes, which allow paths to be traced at \nrun time. When the program is executed, taken acyclic paths are identi.ed using the inserted probes. \nThe main dif\u00adference with the Ball-Larus approach is that, instead of di\u00adrectly updating a frequency \ncounters table here, we emit a stream of path IDs, which is passed along to the next stage of the process. \nThis allows us to decouple the task of tracing taken paths from the task of pro.ling them. The pro.ling \nphase can be either the original hash table\u00adbased method of [4] used to maintain BL path frequencies \n(bottom-left of Figure 1), or other approaches such as the one we propose, i.e., pro.ling concatenations \nof BL paths in a forest-based data structure (bottom-right of Figure 1). Dif\u00adferent pro.ling methods \ncan be therefore cast into a common framework, increasing .exibility and helping us make more accurate \ncomparisons. We start with a brief overview of the Ball-Larus path tracing technique, which we use as \nthe .rst stage of our pro.ler. 2.1 Ball-Larus Path Tracing Algorithm The Ball-Larus path pro.ling (BLPP) \ntechnique [4] identi\u00ad.es each acyclic path that is executed in a routine. Paths start on the method entry \nand terminate on the method exit. Since loops make the CFG cyclic, loop back edges are substituted by \na pair of dummy edges: the .rst one from the method en\u00adtry to the target of the loop back edge, and the \nsecond one from the source of the loop back edge to the method exit. After this (reversible) transformation, \nthe CFG of a method becomes a DAG (directed acyclic graph) and acyclic paths can be enumerated. The Ball-Larus \npath numbering algorithm, shown in Fig\u00adure 2, assigns a value val(e)to each edge e of the CFG such that, \ngiven N acyclic paths, the sum of the edge values along any entry-to-exit path is a unique numeric ID \nin [0, N-1]. A CFG example and the corresponding path IDs are shown in Figure 3: notice that there are \neight distinct acyclic paths, procedure bl path numbering(): 1: for each basic block v in reverse topological \norder do 2: if v is the exit block then 3: numPaths(v). 1 4: else 5: numPaths(v). 0 6: for each outgoing \nedge e = (v, w) do 7: val(e)= numPaths(v) 8: numPaths(v)+= numPaths(w) 9: end for 10: end if 11: end \nfor Figure 2: The Ball-Larus path numbering algorithm. numbered from 0 to 7, starting either at the \nmethod s entry A, or at loop header B (target of back edge (E, B)). BLPP places instrumentation on edges \nto compute a unique path number for each possible path. In particular, it uses a variable r, called probe \nor path register, to compute the path number. Variable r is .rst initialized to zero upon method entry \nand then is updated as edges are traversed. When an edge that reaches the method exit is executed, or \na back edge is traversed, variable r represents the unique ID of the taken path. As observed, instead \nof using the path ID r to increase the path frequency counter (count[r]++), we defer the pro.ling stage \nby emitting the path ID to an output stream (emit r). To support pro.ling over multiple invocations of \nthe same routine, we annotate the stream with the special marker * to denote a routine entry event. Instru\u00admentation \ncode for our CFG example is shown on the left of Figure 3. 2.2 k-Iteration Path Pro.ling The second \nstage of our pro.ler takes as input the stream of BL path IDs generated by the .rst stage and uses it \nto build a data structure that keeps track of the frequencies of each and every distinct taken path consisting \nof the concatenation of up to k BL paths, where k is a user-de.ned parameter. This problem is equivalent \nto counting all n-grams, i.e., contiguous sequences of n items from a given sequence of items, for each \nn = k. Our solution is based on the notion of pre.x forest, which compactly encodes a list of sequences \nby representing repetitions and common pre.xes only once. A pre.x forest can be de.ned as follows: De.nition \n1 (Pre.x forest). Let L = (x1, x2,. . . , xq)be any list of .nite-length sequences over an alphabet H. \nThe pre.x forest F(L)of L is the smallest labeled forest such that, . sequence x = (a1, a2,. . . , an)in \nL there is a path p = (.1, .2,. . . , .n)in F(L)where .1 is a root and .j . [1, n]: 1. .j is labeled \nwith aj, i.e., l(.j)= aj . H; 2. .j has an associated counter c(.j)that counts the num\u00adber of times \nsequence (a1, a2,. . . , aj)occurs in L.   emit * r=0 ABCEBCEBDEBDEBCEBCEBDEBDEBCEBCEBDEBDEBCEBCEF \nFull execution path r+=4 r+=2 emit r r=0 r+=1 Full execution path broken into BL paths  Stream of BL \npath IDs path frequency  BL path ID BDE 0 BDEF 1 BCE 2 BCEF 3 ABDE 4 ABDEF 5 ABCE 6 ABCEF 7 emit r \n Figure 3: Control .ow graph with Ball-Larus instrumentation modi.ed to emit acyclic path IDs to an output \nstream and running example of our approach that shows a 4-iteration path forest (4-IPF) for a possible \nsmall execution trace. Notice that L is a list and not a set to account for multiple occurrences of the \nsame n-grams. By De.nition 1, each sequence in Lis represented as a path in the forest, and node labels \nin the path are exactly the symbols of the sequence, in the same order. The notion of minimality implies \nthat, by removing even one single node, there would be at least one sequence of L not counted in the \nforest. Observe that there is a distinct root in the forest for each distinct symbol that occurs as .rst \nsymbol of a sequence. k-iteration path forest. The output of our pro.ler is a pre\u00ad.x forest, which we \ncall k-Iteration Path Forest (k-IPF), that compactly represents all observed contiguous sequences of \nup to k BL path IDs: De.nition 2 (k-Iteration Path Forest). Given an input stream S representing a sequence \nof BL path IDs and * mark\u00aders, the k-Iteration Path Forest (k-IPF) of S is de.ned as k-IPF = F(L), where \nL = {list of all n-grams of Sthat do not contain *, with n = k}. By De.nition 2, the k-IPF is the pre.x \nforest of all consecu\u00adtive subsequences of up to k BL path IDs in S. Example 1. Figure 3 provides an \nexample showing the 4-IPF constructed for a small sample execution trace consist\u00ading of a sequence of \n44 basic blocks encountered during one invocation of the routine described by the control .ow graph on \nthe left. Notice that the full (cyclic) execution path starts at the entry basic block A and terminates \non the exit basic block F. The .rst stage of our pro.ler issues a stream S of BL path IDs that are obtained \nby emitting the value of the probe register r each time a back edge is traversed, or the exit basic block \nis executed. Observe that the sequence of emitted path IDs induces a partition of the execution path \ninto Ball-Larus acyclic paths. Hence, the sequence of exe\u00adcuted basic blocks can be fully reconstructed \nfrom the se\u00adquence Sof path IDs. The 4-IPF built in the second stage contains exactly one tree for each \nof the 4 distinct BL path IDs (0, 2, 3, 6) that occur in the stream. We observe that path frequen\u00adcies \nin the .rst level of the 4-IPF are exactly those that traditional Ball-Larus pro.ling would collect. \nThe second level contains the frequencies of taken paths obtained by concatenating 2 BL paths, etc. Notice \nthat the path labeled with (2,0,0,2)in the 4-IPF, which corresponds to the path (B, C, E, B, D, E, B, \nD, E, B, C, E) in the control .ow graph, is a 4-gram that occurs 3 times in S and is one of the most \nfrequent paths among those that span from 2 up to 4 loop iterations. Properties. A k-IPF has some relevant \nproperties: 1. .nodes a .k-IPF, k > 0: c(a)= c(\u00dfi); \u00dfi : edg e (a,\u00dfi ).k-IPF 2. .k > 0, k-IPF .(k+ 1)-IPF. \n By Property 1, since path counters are non-negative, they are monotonically non-increasing as we walk \ndown the tree. The inequality = in Property 1 may be strict (>) if the execution trace of a routine invocation \ndoes not end at the exit basic block; this may be the case when a subroutine call is performed at an \ninternal node of the CFG. Property 2 implies that, for each tree T1 in the k-IPF there is a tree T2 in \nthe (k+1)-IPF such that T2 is equal to T1 after removing leaves at level k+ 1. Notice that a 1-IPF includes \nonly acyclic paths and yields exactly the same counters as a Ball-Larus pro.ler [4]. 3. Application Examples \nIn this section we consider examples of applications where k-iteration path pro.ling can reveal optimization \nopportu\u00adnities or help developers comprehend relevant properties of #define NEIGHBOR(m,i,dy,dx,w) \\ \n(*((m)+(i)+(dy)*(w)+(dx))) #define CONVOLUTION(i) do { \\ val = NEIGHBOR(img_in, (i), \\ -2, -2, cols)*filter[0]; \n\\ val += NEIGHBOR(img_in, (i), \\ -2, -1, cols)*filter[1]; \\ ... val += NEIGHBOR(img_in, (i), +2, +2, \ncols)*filter[24]; \\ val = val*factor+bias; \\ img_out[i] = (unsigned char) \\ (val < 0 ? 0 : val > 255 \n? 255 : val); \\ } while(0) void filter_conv(unsigned char* img_in, unsigned char* img_out, unsigned \nchar* mask, char filter[25], double factor, double bias, int rows, int cols) { int val; long n = rows*cols, \ni; for (i = 0; i < n; i++) if (mask[i]) img_out[i] = img_in[i]; else CONVOLUTION(i); }  Figure 4: \nMasked image .ltering code based on a convolution matrix. a piece of software by identifying structured \nexecution pat\u00adterns that would be missed by an acyclic-path pro.ler. Our discussion is based on idealized \nexamples found in real pro\u00adgrams of the kind of behavior that can be exploited using multi-iteration \npath pro.les. Since our methodology can be applied to different languages, we addressed both Java and \nC applications1 . 3.1 Masked Convolution Filters in Image Processing As a .rst example, we consider a \nclassic application of con\u00advolution .lters to image processing, addressing the problem of masked .ltering \nthat arises when the user applies a trans\u00adformation to a collection of arbitrary-shaped subregions of \nthe input image. A common scenario is face anonymization, illustrated in the example of Figure 5. The \ncase study dis\u00adcussed in this section shows that k-iteration path pro.ling with large values of k can \nidentify regular patterns spanning 1We pro.led Java programs using the k-BLPP tool described in Section \n5 and C programs with manual source code instrumentation based on a C implementation of the algorithms \nand data structures of Section 4, available at http://www.dis.uniroma1.it/~demetres/kstream/. multiple \nloop iterations that can be effectively exploited to speed up the code. Figure 4 shows a C implementation \nof a masked image .l\u00adtering algorithm based on a 5\u00d7 5 convolution matrix2. The function takes as input \na grayscale input image (8-bit depth) and a black &#38; white mask image that speci.es the regions of \nthe image to be .ltered. Figure 5 shows a sample input image (left), a mask image (center), and the output \nimage (right) generated by the code of Figure 4 by applying a blur .lter to the regions of the input \nimage speci.ed by the mask. Notice that the .lter code iterates over all pixels of the input image and \nfor each pixel checks if the corresponding mask is black (zero) or white (non-zero, i.e., 255). If the \nmask is white, the original grayscale value is copied from the input image to the output image; otherwise, \nthe grayscale value of the output pixel is computed by applying the convolution kernel to the neighborhood \nof the current pixel in the input image. To avoid expensive boundary checks in the convolu\u00adtion operation, \nthe mask is preliminarily cropped so that all values near the border are white (this operation takes \nnegli\u00adgible time). Figure 6 shows a portion of the 10-IPF forest containing the trees rooted at the BL \npath IDs that correspond to: the path entering the loop (ID=0), the copy branch taken in the loop body \nwhen the mask is non-zero (ID=1), and the convolution branch taken in the loop body when the mask is \nzero (ID=2). The 10-IPF was generated on the workload of Figure 5 and was pruned by removing all nodes \nwhose counters are less than 0.01% of the counter of their parents or less than 0.01% of the counter \nof their roots. For each node v in the forest, if v has a counter that is X% of the counter of its parent \nand is Y% of the counter of the root, then the edge leading to v is labeled with X%(Y%) . A visual analysis \nof the forest shows that: the copy branch (1) is more frequent than the convolution branch (2);  98.9% \nof the times a copy branch (1) is taken, it is repeated consecutively at least 10 times, and only 0.1% \nof the times is immediately followed by a convolution branch;  95% of the times a convolution branch \n(2) is taken, it is repeated consecutively at least 10 times, and only 0.6% of the times is immediately \nfollowed by a copy branch;  This entails that both the copy and the convolution opera\u00adtions are repeated \nalong long consecutive runs. The above properties are typical of masks used in face anonymiza\u00adtion and \nother common image manipulations based on user\u00adde.ned selections of portions of the image. The collected \npro.les suggest that consecutive iterations of the same branches may be selectively unrolled as shown \nin Figure 7. Each iteration of the outer loop, designed for a 64-bit plat\u00ad 2The source code of our example \nis provided at http://www.dis.uniroma1.it/~demetres/kstream/. Figure 5: Masked blur .lter example: original \n3114 \u00d7 2376 image (left), .lter mask (center), .ltered image (right). Figure 6: 10-IPF forest of the \ncode of Figure 4 on the workload of Figure 5. for (i = 0; i < n-7; i += 8) { if (*(long*)(mask+i) == \n0xFFFFFFFFFFFFFFFF) *(long*)(img_out+i) = *(long*)(img_in+i); else if (*(long*)(mask+i) == 0) { CONVOLUTION(i); \nCONVOLUTION(i+1); CONVOLUTION(i+2); CONVOLUTION(i+3); CONVOLUTION(i+4); CONVOLUTION(i+5); CONVOLUTION(i+6); \nCONVOLUTION(i+7); } else for (j = i; j < i+8; j++) if (mask[j]) img_out[j] = img_in[j]; else CONVOLUTION(j); \n } Figure 7: Optimized 64-bit version of the loop of Figure 4. form, works on 8 (rather than 1) pixels \nat a time. Three cases are possible: 1. the next 8 mask entries are all 255 (white): the 8 corre\u00adsponding \ninput pixel values are copied to the output im\u00adage at once with a single assignment instruction; 2. \nthe next 8 mask entries are all 0 (black): the kernel is applied sequentially to each of the next 8 input \npixels; 3. the next 8 mask entries are mixed: an inner loop performs either copy or convolution on the \ncorresponding pixels.  Performance analysis. To assess the bene.ts of the opti\u00admization performed in \nFigure 7, we conducted several tests on recent commodity platforms (Intel Core 2 Duo, Intel Core i7, \nLinux and MacOS X, 32 and 64 bits, gcc -O3), consid\u00adering a variety of sample images and masks with regions \nof different sizes and shapes. We obtained non-negligible speedups on all our tests, with a peak of about \n21% on the workload of Figure 5 (3114 \u00d7 2376 pixels) and about 30% on a larger 9265 \u00d7 7549 image with \na memory footprint of about 200 MB. In general, the higher the white entries in the mask, the faster \nthe code, with larger speedups on more recent machines. As we expected, for entirely black masks the \nspeedup was instead barely noticeable: this is due to the fact that the convolution operations are computationally \nde\u00admanding and tend to hide the bene.ts of loop unrolling. Discussion. The example discussed in this \nsection shows that both the ability to pro.le paths across multiple itera\u00adtions, and the possibility \nto handle large values of k played a crucial role in optimizing the code. Indeed, acyclic-path pro.ling \nwould count the number of times each branch is taken, but would not reveal that they appear in consecu\u00adtive \nruns. Moreover, previous multi-iteration approaches that only handle very small values of k would not \ncapture the long runs that make the proposed optimization effective. For the example of Figure 5, an \nacyclic-path pro.le would indicate that the copy branch is taken 81.7% of the times, but not how branches \nare interleaved. From this information, we would be able to deduce that the average length of a sequence \nof consecutive white values in the mask is = 4. Our pro.le shows that, 98.9% of the times the actual \nlength is at least 10, fully justifying our optimization that copies 8 bytes at a time. The advantage \nof k-iteration path pro.ling increases for masks with a more balanced ratio between white and black pixels: \nfor a 50-50 ratio, an acyclic-path pro.le would indicate that the average length of consecutive white/black \nruns is = 1, yielding no useful information for loop unrolling purposes. The execution pattern where \nthe same branches are re\u00adpeatedly taken over consecutive loop iterations is common to several other applications, \nwhich may bene.t from op\u00adtimizations that take advantage of long repeated runs. For instance, the LBM \nperformStreamCollide function of the lbm benchmark included in the SPEC CPU 2006 suite it\u00aderates over \na 3D domain, simulating incompressible .uid dynamics based on the Lattice Boltzmann Method. An in\u00adput \ngeometry .le speci.es obstacles that determine a steady state solution. The loop contains branches that \ndepend upon the currently scanned cell, which alternates between obsta\u00adcles and void regions of the domain, \nproducing a k-IPF sim\u00adilar to that of Figure 6 on typical workloads. 3.2 Instruction Scheduling Young \nand Smith [22] have shown that path pro.les span\u00adning multiple loop iterations can be used to improve \nthe con\u00adstruction of superblocks in trace schedulers. Global instruction scheduling groups and orders \nthe in\u00adstructions of a program in order to match the hardware re\u00adsource constraints when they are fetched. \nIn particular, trace schedulers rely on the identi.cation of traces (i.e., sequences of basic blocks) \nthat are frequently executed. These traces are then extended by appending extra copies of likely succes\u00adsors \nblocks, in order to form a larger pool of instructions for reordering. A trace that is likely to complete \nis clearly prefer\u00adable, since instructions moved before an early exit point are wasted work. Superblocks \nare de.ned as sequences of basic blocks with a single entry point and multiple exit points; they are \nuse\u00adful for maintaining the original program semantics during a global code motion. Superblock formation \nis usually driven by edge pro.les: however, path pro.les usually provide bet\u00adter information to determine \nwhich traces are worthwhile to enlarge (i.e., those for which execution reaches the ending block most \nof the times). Figure 8 shows how superblock construction may bene.t from path pro.ling information for \n 20 59 C 40 20 do { if (<condition 1>) { /* A */ /* B*/ else /* C */ } while (<condition 2>); /* D \n*/ frequency counter BL path ID  Phased behavior Alternating behavior C (a) Classical unrolling (b) \nUnrolling optimized (c) Unrolling optimized based on edge profiling for alternating behavior for phased \nbehavior Figure 8: Superblockconstruction usingcyclic-pathpro.les. two different behaviors, characterizedby \nthe same edge pro\u00ad.le, of a do . . . while loop. Path pro.ling techniques that do not span multiple loop \niterations chop execution traces into pieces separatedat back edges, hence the authors collect execution \nfrequencies for general paths [23], whichcontain anycontiguous sequences of CFG edges up to a limiting \npath length; they use a path length of 15 branches in the experiments. Example. Phased and alternating \nbehaviors as in Figure 8 are quite common among many applications, thus offering interesting optimization \nopportunities. For instance, the con\u00advolution .lter discussedin the previous section is a clear ex\u00adample \nof phasedbehavior. An alternating behavior is shown by the checkTaskTag method of class Scanner in the \norg.eclipse.jdt.internal.compiler.parser pack\u00adage of the eclipse benchmark included in the DaCapo re\u00adlease \n2006-10-MR2. In Figure 9 we show a subtree of the 11-IPF generated for this method; in the subtree, we \npruned all nodes with counters less than 10% of the counter of the root. Notice that, after executing \nthe BL path with ID 38, in 66% of the executions the program continues with 86, and in 28% of the executions \nwith BL path 87. When 86 fol\u00adlows 38, in 100% of the executions the control .ow takes the path (86,86,86,755), \nwhich spans four loop iterations and may be successfully unrolled to perform instruction scheduling. \nInterestingly, sequence (38,86,86,86,755,38, 86,86,86,755,38)of 11 BL path IDs, highlighted in Fig\u00adure \n9, accounts for more than 50% of all executions of the .rst BL path in the sequence, showing that sequence \n(38,86,86,86,755) is likely to be repeated consecutively more than once. Discussion. The workpresentedin \n[22] focusedon assess\u00ading the bene.ts of using general paths for global instruction scheduling, rather \nthan on how to pro.le them. As we will see in Section 7, compared to our approach the technique   \n    Figure 9: Subtree of the 11-IPF of method org.eclipse.jdt. internal.compiler.parser.Scanner.checkTaskTag \ntaken from release 2006-10-MR2ofthe DaCapo benchmark suite. proposed by Young [23] for pro.ling general \npaths scales poorly for increasing path lengths both in terms of space usage and running time. We believe \nthat our method, by substantially reducing the overhead of cyclic-path pro.ling, has the potential to \nprovide a useful ingredient for making pro.le-guided global instruction scheduling more ef.cient in modern \ncompilers. 4. Algorithms In this section, we show how to ef.cientlyconstruct a k-IPF pro.le starting \nfrom a stream of BL path IDs. We observe that building explicitly a k-IPF concurrently with program execution \nwould require updating up to k nodes for each stream item: this may considerably slow down the program \neven for small values of k. To overcome this problem, we construct an intermediate data structure that \ncan be updated quickly, and then convert it into a k-IPF more ef.ciently when the stream is over. As \nintermediate data structure, we use a variant of the k-slab forest (k-SF)introduced in [3]. Main idea. \nThe variant of the k-SF we present in this paper is tailored to keep track of all n-grams of a sequence \nof symbols, for all n = k. The organization of our data structure stems from the following simple observation: \nif we partition a sequence into chunks of length k - 1, then anysubsequence of lengthup to k willbe entirelycontained \nwithin two consecutive chunks of the partition. The main idea is therefore to consider allsubsequences \nthat start at the beginning of a chunk and terminate somewhere in the next chunk, and join them in a \npre.x forest (the k-SF). Such a forest contains a distinct tree for each distinct symbol that appears \nat the beginning of any chunk and, as we will see later on in this section, encodes information about \nall n\u00adgrams with n = k. Indeed, by De.nition 1, node counters in the k-SF keep track of the number of \noccurrences of each distinct subsequence of length between k and 2k - 2 that starts at the beginning \nof a chunk. The number of occurrences of a given n-gram can be reconstructed off\u00adline by .nding all subpaths \nof length n of the k-SF labeled with the symbols of the n-gram and by summing up the counters of the \nend-nodes of the subpaths. The partition of the sequence into chunks induces a division of the forest \ninto upper and lower regions (slabs) of height up to k -1. This organization implies that the k-SF can \nbe constructed on\u00adline as stream items are revealed to the pro.ler by adding or updating up to two nodes \nof the forest at a time (one in the upper region and one in the lower region), instead of knodes as we \nwould do if we incremented explicitly the frequencies of n-grams as soon as they are encountered in the \nstream. The following example applies the concepts described above to a stream of BL path IDs. Example \n2. Let us consider again the example given in Figure 3. For k = 4, we can partition the stream into maximal \nchunks of up to k - 1 = 3 consecutive BL path IDs as follows: S = (*, 6, 2, 0 , 0, 2, 2 , 0, 0, 2 , 2, \n0, 0 , 2, 3 ). ' -v \" ' -v \" ' -v \" ' -v \" ' -v \" c1 c2 c3 c4 c5 The 4-SF of S, de.ned in terms of chunks \nc1,. . . , c5, is shown in Figure 10. Notice for instance that 2-gram (0,0) occurs three times in S and \n.ve times in the 4-SF. However, only three of them end in the bottom slab and hence are counted in the \nfrequency counters. To obtain a k-IPF starting from the k-SF, for each BL path ID that appears in the \nstream we will eventually construct the set of nodes in the k-SF associated with it and join the subsequences \nof length up to kstarting from those nodes into a pre.x forest. Formal k-SF de.nition. The above description \nof the k-SF can be more formally and precisely summarized as follows: De.nition 3 (k-slab forest). Let \nk = 2and let c1, c2, c3,. . . , cm be the chunks of Sobtained by: (1) splitting Sat * mark\u00aders, (2) removing \nthe markers, and (3) cutting the remaining subsequences every k-1consecutive items. The k-slab for\u00adest \n(k-SF) of Sis de.ned as k-SF = F(L), where L = {list of all pre.xes of c1 \u00b7 c2 and all pre.xes of length \n= k of ci \u00b7 ci+1 , .i . [2, m -1]}and ci \u00b7 ci+1 denotes the concate\u00adnation of ci and ci+1 . By De.nition \n3, since each chunk ci has length up to k-1, then a k-SF has at most 2k -2levels and depth 2k -3. As \nobserved above, the correctness of the k-SF representation stems from the fact that, since each occurrence \nof an n-gram with n = kappears in ci \u00b7 ci+1 for some i, then there is a tree in the k-SF representing \nit.  Example 3. In accordance with De.nition 3, the forest of Figure 10 for the stream of Example 2 \nis F(L), where L = ( (6),(6,2),(6,2,0),(6,2,0,0),(6,2,0,0,2),(6,2,0,0,2,2), (0,2,2,0),(0,2,2,0,0),(0,2,2,0,0,2),(0,0,2,2),(0,0,2, \n2,0), (0,0,2,2,0,0), (2,0,0,2),(2,0,0,2,3)). k-SF construction algorithm. Given a stream Sformed by * \nmarkers and BL path IDs, the k-SF of Scan be constructed by calling the procedure process bl path id(r)shown \nin Figure 11 on each item r of S. The streaming algorithm, which is a variant of the k-SF construction \nalgoritm given in [3] for the different setting of bounded-length calling contexts, keeps the following \ninformation: a hash table R, initially empty, containing pointers to the roots of trees in the k-SF, \nhashed by node labels; since no two roots have the same label, the lookup operation find(R, r)returns \nthe pointer to the root containing label r, or null if no such root exists;  a variable n that counts \nthe number of BL path IDs processed since the last * marker;  a variable t (top) that points either \nto null or to the current k-SF node in the upper part of the forest (levels 0 through k-2);  a variable \n\u00df (bottom) that points either to null or to the current k-SF node in the lower part of the forest (levels \nk-1through 2k-3).  The main idea of the algorithm is to progressively add new paths to an initially \nempty k-SF. The path formed by the .rst k - 1 items since the last * marker is added to one tree of the \nupper part of the forest. Each later item r is added at up to two different locations of the k-SF: one \nin the upper part of the forest (lines 13 17) as a child of node t (if no child of t labeled with r already \nexists), and the other one in the lower part of the forest (lines 21 25) as a child of node \u00df (if no \nchild of \u00df labeled with r already exists). Counters of processed nodes already containing r are incremented \nby one (either line 27 or line 29). procedure process bl path id(r): 1: if r = * then 2: n . 0 3: t \n. null 4: return 5: end if 6: if n mod (k - 1) = 0 then 7: \u00df . t 8: t . find(R, r) 9: if t = null then \n 10: add root t with l(t) = r and c(t) = 0 to k-SF and R 11: end if 12: else 13: .nd child . of node \nt with label l(.) = r 14: if . = null then 15: add node . with l(.) = r and c(.) = 0 to k-SF 16: add \narc (t, .) to k-SF 17: end if 18: t . . 19: end if 20: if \u00df .null then = 21: .nd child . of node \u00df with \nlabel l(.) = r 22: if . = null then 23: add node . with l(.) = r and c(.) = 0 to k-SF 24: add arc (\u00df, \n.) to k-SF 25: end if 26: \u00df . . 27: c(\u00df) . c(\u00df) + 1 28: else 29: c(t) . c(t) + 1 30: end if 31: n . n \n+ 1 Figure 11: Streaming algorithm for k-SF construction. Both t and \u00df are updated to point to the child \nlabeled with r (lines 18 and 26, respectively). The running time of the algorithm is dominated by lines \n8 and 10 (hash table accesses), and by lines 13 and 21 (node children scan). Assuming that operations \non R require constant time, the per-item processing time is O(d), where d is the maximum degree of a \nnode in the k-SF. Our experiments revealed that d is on average a typically small constant value. As \nan informal proof that each subsequence of length up to k is counted exactly once in the k-SF, we .rst \nob\u00adserve that, if the subsequence extends across two consecutive chunks, then it appears exactly once \nin the forest (connect\u00ading a node in the upper slab to a node in the lower slab). In contrast, if the \nsubsequence is entirely contained in a chunk, then it appears twice: once in the upper slab of the tree \nrooted at the beginning of the chunk, and once in the lower slab rooted in at the beginning of the preceding \nchunk. However, only the counter in the lower part of the forest is updated (line 27): for this reason, \nthe sum of all counters in the k-SF is equal to the length of the stream. procedure make k ipf(): 1: \nI . \u00d8 2: for each node . . k-SF do 3: if l(.) .. I then 4: add l(.) to I and let s(l(.)) . \u00d8 5: end if \n6: add .to s(l(.)) 7: end for 8: let the k-IPF be formed by a dummy root f 9: for each r . I do 10: \nfor each . . s(r) do 11: join subtree(., f, k) 12: end for 13: end for 14: remove dummy root f from the \nk-IPF procedure join subtree(., ., d): 1: d . child of . in the k-IPF s.t. l(d) = l(.) 2: if d = null \nthen 3: add new node d as a child of . in the k-IPF 4: l(d) . l(.) and c(d) . c(.) 5: else 6: c(d) . \nc(d) + c(.) 7: end if 8: if d > 1 then 9: for each child s of .in the k-SF do 10: join subtree(s, d, \nd - 1) 11: end for 12: end if Figure 12: Algorithm for converting a k-SF into a k-IPF. k-SF to k-IPF \nconversion. Once the stream Sis over, i.e., the pro.led thread has terminated, we convert the k-SF into \na k-IPF using the procedure make k ipf shown in Figure 12. The key intuition behind the correctness of \nthe conversion algorithm is that for each sequence in the stream of length up to k, there is a tree in \nthe k-SF containing it. The algorithm creates a set I of all distinct path IDs that occur in the k-SF \nand for each r in I builds a set s(r) containing all nodes . of the k-SF labeled with r (lines 2 7). \nTo build the k-IPF, the algorithm lists each distinct path ID r and joins to the k-IPF the top k -1 levels \nof the subtrees of the k-SF rooted at the nodes in s(r). Subtrees are added as children of a dummy root, \nwhich is inserted for the sake of convenience and then removed. The join operation is speci.ed by procedure \njoin subtree, which performs a traversal of a subtree of the k-SF of depth less than k and adds nodes \nto k-IPF so that all labeled paths in the subtree appear in the k-IPF as well, but only once. Path counters \nin the k-SF are accumulated in the corresponding nodes of the k-IPF to keep track of the number of times \neach distinct path consisting of the concatenation of up to kBL paths was taken by the pro.led program. \n 5. Implementation In this section we describe the implementation of our pro\u00ad.ler, which we call k-BLPP, \nin the Jikes Research Virtual Machine [1]. 5.1 Adaptive Compilation The Jikes RVM is a high-performance \nmetacircular virtual machine: unlike most other JVMs, it is written in Java. Jikes RVM does not include \nan interpreter: all bytecode must be .rst translated into native machine code. The unit of compilation \nis the method, and methods are compiled lazily by a fast non-optimizing compiler the so-called baseline \ncompiler when they are .rst invoked by the program. As execution continues, the Adaptive Optimization \nSystem monitors program execution to detect program hot spots and selectively recompiles them with three \nincreasing levels of optimization. Note that all modern production JVMs rely on some variant of selective \noptimizing compilation to target the subset of the hottest program methods where they are expected to \nyield the most bene.ts. Recompilation is performed by the optimizing compiler, that generates higher-quality \ncode but at a signi.cantly larger cost than the baseline compiler. Since Jikes RVM quickly recompiles \nfrequently executed methods, we imple\u00admented k-BLPP in the optimizing compiler only. 5.2 Inserting Instrumentation \non Edges k-BLPP adds instrumentation to hot methods in three passes: 1. building the DAG representation; \n 2. assigning values to edges; 3. adding instrumentation to edges.  k-BLPP adopts the smart path numbering \nalgorithm pro\u00adposed by Bond and McKinley [11] to improve performance by placing instrumentation on cold \nedges. In particular, line 6 of the canonical Ball-Larus path numbering algorithm shown in Figure 2 is \nmodi.ed such that outgoing edges are picked in decreasing order of execution frequency. For each basic \nblock edges are sorted using existing edge pro.ling in\u00adformation collected by the baseline compiler, \nthus allowing us to assign zero to the hottest edge so that k-BLPP does not place any instrumentation \non it. During compilation, the Jikes RVM generates yield points, which are program points where the running \nthread deter\u00admines if it should yield to another thread. Since JVMs need to gain control of threads quickly, \ncompilers insert yield points in method prologues, loop headers, and method epi\u00adlogues. We modi.ed the \noptimizing compiler to also store the path pro.ling probe on loop headers and method epi\u00adlogues. Ending \npaths at loop headers rather than back edges causes a path that traverse a header to be split into two \npaths: this difference from canonical Ball-Larus path pro\u00ad.ling is minor because it only affects the \n.rst path through a loop [10]. Note that optimizing compilers do not insert yield points in a method \nwhen either it does not contain branches (hence its pro.le is trivial) or it is marked as uninterruptible. \nThe second case occurs in internal Jikes RVM methods only; the compiler occasionally inlines such a method \ninto an appli\u00adcation method, and this might result in a loss of information only when the execution reaches \na loop header contained in the inlined method. However, this loss of information ap\u00adpears to be negligible \n[10]. 5.3 Path Pro.ling The k-SF construction algorithm described in Section 2.2 is implemented using \na standard .rst-child, next-sibling rep\u00adresentation for nodes. This representation is very space\u00adef.cient, \nas it requires only two pointers per node: one to its leftmost child and the other to its right nearest \nsibling.  Tree roots are stored and accessed through an ef.cient im\u00adplementation3 of a hash map, using \nthe pair represented by the Ball-Larus path ID and the unique identi.er associated to the current routine \n(i.e., the compiled method ID) as key. Note that this map is typically smaller than a map required by \na traditional BLPP pro.ler, since tree roots represent only a fraction of the distinct path IDs encountered \nduring the execution. Consider, for instance, the example shown in Figure 13: this control .ow graph \nhas N acylic paths after backedges have been removed. Since cyclic paths are trun\u00adcated on loop headers, \nonly path IDs 0and 1can appear after the special marker * in the stream, thus leading to the cre\u00adation \nof an entry in the hash map. Additional entries might be created when a new tree is added to the k-SF \n(line 10 of the streaming algorithm shown in Figure 11); however, experimental results show that the \nnumber of tree roots is usually small, while N increases with the complexity (i.e., number of branches \nand loops) of the routine. 6. Experimental Evaluation In this section we report the result of an extensive \nexper\u00adimental evaluation of our approach. The goal is to assess 3HashMapRVM is the stripped-down implementation \nof the HashMap data structure used by core parts of the Jikes RVM runtime and by Bond s BLPP path pro.ler. \n the performance of our pro.ler compared to previous ap\u00adproaches and to study properties of path pro.les \nthat span multiple iterations for several representative benchmarks. 6.1 Experimental Setup Bechmarks. \nWe evaluated k-BLPP against a variety of prominent benchmarks drawn from three suites. The DaCapo suite \n[5] consists of a set of open source, real-world appli\u00adcations with non-trivial memory loads. We use \nthe superset of all benchmarks from DaCapo releases 2006-10-MR2 and 9.12 that can run successfully with \nJikes RVM, using the largest available workload for each benchmark. The SPEC suite focuses on the performance \nof the hardware proces\u00adsor and memory subsystem when executing common gen\u00aderal purpose application computations4 \n. Finally, we chose two memory-intensive benchmarks from the Java Grande 2.0suite [12] to further evaluate \nthe performance of k-BLPP. Compared codes. In our experiments, we analyzed the na\u00adtive (uninstrumented) \nversion of each benchmark and its instrumented counterparts, comparing k-BLPP for differ\u00ad 4Unfortunately, \nonly a few benchmarks from SPEC JVM2008 can run successfully with Jikes RVM due to limitations of the \nGNU classpath. ent values of k (2, 3, 4, 6, 8, 11, 16) with the BLPP pro\u00ad.ler developed by Bond [10, \n15], which implements the canonical Ball-Larus acyclic-path pro.ling technique. We upgraded the original \ntool by Bond to take advantage of na\u00adtive threading support introduced in newer Jikes RVM re\u00adleases; \nthe code is structured as in Figure 1, except that it does not produce any intermediate stream, but it \ndirectly per\u00adforms count[r]++. The software evaluated in this section has been analyzed and endorsed \nby the OOPSLA 2013 Arti\u00adfact Evaluation Committee, and the source code is available in the Jikes RVM \nResearch Archive. Platform. Our experiments were performed on a 2.53GHz Intel Core2 Duo T9400 with 128KB \nof L1 data cache, 6MB of L2 cache, and 4 GB of main memory DDR3 1066, run\u00adning Ubuntu 12.10, Linux Kernel \n3.5.0, 32 bit. We ran all of the benchmarks on Jikes RVM 3.1.3 (default production build) using a single \ncore and a maximum heap size equal to half of the amount of physical memory. Metrics. We considered a \nvariety of metrics, including wall-clock time, number of operations per second performed by the pro.led \nprogram, number of hash table operations, data structure size (e.g., number of hash table items for BLPP \nand number of k-SF nodes for k-BLPP), and statistics such as average node degree of the k-SF and the \nk-IPF and average depth of k-IPF leaves. To interpret our results, we also pro.led our pro.ler by collecting \nhardware perfor\u00admance counters with perf [18], including L1 and L2 cache miss rate, branch mispredictions, \nand cycles per instruction (CPI).  Methodology. For each benchmark/pro.ler combination, we performed \n10 trials, each preceded by a warmup execu\u00adtion, and computed the arithmetic mean. We monitored vari\u00adance, \nreporting con.dence intervals for performance metrics stated at a 95% con.dence level. Performance measurements \nwere collected on a machine with negligible background ac\u00adtivity. 6.2 Experimental Results Performance \noverhead. In Figure 14 we report for each benchmark the pro.ling overhead of k-BLPP relative to BLPP. \nThe chart shows that for 12 out of 16 benchmarks the overhead decreases for increasing values of k, provid\u00ading \nup to almost 45% improvements over BLPP. This is ex\u00adplained by the fact that hash table accesses are \nperformed by process bl path id every k-1items read from the input stream between two consecutive routine \nentry events (lines 8 and 10 in Figure 11). As a consequence, the number of hash table operations for \neach routine call is O(1 + N/(k -1)), where N is the total length of the path taken during the in\u00advocation. \nIn Figure 15 we report the measured number of hash table accesses for our experiments, which decreases \nas predicted on all benchmarks with intense loop iteration activity. Notice that, not only does k-BLPP \nperform fewer hash table operations, but since only a subset of BL path IDs are inserted, the table is \nalso smaller yielding further performance improvements. For codes such as avrora and hsqldb, which perform \non average a small number of iter\u00adations, increasing k beyond this number does not yield any bene.t. \nOn eclipse, k-BLPP gets faster as k increases, but dif\u00adferently from all other benchmarks in this class, \nit remains slower than BLPP by at least 25%. The reason is that, due to structural properties of the \nbenchmark, the average number of node scans at lines 13 and 21 of process bl path id is rather high (58.8 \nfor k = 2 down to 10.3 for k = 16). In contrast, the average degree of internal nodes of the k-SF is \nsmall (2.6 for k = 2 decreasing to 1.3 for k = 16), hence there is intense activity on nodes with a high \nnum\u00adber of siblings. No other benchmark exhibited this extreme behavior. We expect that a more ef.cient \nimplementation of process bl path id, e.g., by adaptively moving hot chil\u00addren to the front of the list, \ncould reduce the scanning over\u00adhead for this kind of worst-case benchmarks as well. Benchmarks compress, \nscimark.monte carlo, heap\u00adsort, and md made an exception to the general trend we ob\u00adserved, with performance \noverhead increasing, rather than decreasing, with k. To justify this behavior, we collected and analyzed \nseveral hardware performance counters and noticed that on these benchmarks our k-BLPP implemen\u00adtation \nsuffers from increased CPI for higher values of k. Figure 16 (a) shows this phenomenon, comparing the \nfour outliers with other benchmarks in our suite. By analyzing L1 and L2 cache miss rates, reported in \nFigure 16 (b) and Figure 16 (c), we noticed that performance degrades due to poor memory access locality. \nWe believe this to be an issue of our current implementation of k-BLPP, in which we did not make any \neffort aimed at improving cache ef.ciency in accessing the k-SF, rather than a limitation of the general \nap\u00adproach we propose. Indeed, as nodes may be unpredictably scattered in memory due to the linked structure \nof the for\u00ad Cycles per instruction (CPI) for k-BLPP L1 cache misses for k-BLPP L2 cache misses for k-BLPP \n18   16 24 14 20 % of data cache loads 12 10 8 6 % of L2 cache loads 16 12 8 4 0.5 4 2 0 0 02 3 4 \n6 8 11 16 2 3 4 6 8 11 16 2 3 4 6 8 11 16 kk k (a) (b) (c) Figure 16: Hardware performance counters for \nk-BLPP: (a) cycles per instruction, (b) L1 cache miss rate, (c) L2 cache miss rate. Number of counters \nin use for k-SF 1e+08 blpp k=3 k=6 11 =k k=2 k=4 k=8 16 =k    1e+07 1e+06 100000 10000 1000 100 \n10  Number of counters CPI   Techniques Pro.led paths Average cost w.r.t. BLPP Variables Full accuracy \nInter-procedural v BLPP [4] BL paths - 1 v SPP [2], TPP [16], PPP [11] subset of BL paths smaller 1 or \nmore v Tallam et al. [20] overlapping paths larger many v Roy et al. [19] k-iteration paths larger many \nv Li et al. [17] .nite-length paths larger 1 or more v Young [23] general paths larger - v This paper \nk-iteration paths smaller 1 Table 1: Comparison of different path pro.ling techniques. Structural properties \nof collected pro.les. As a .nal ex\u00adperiment, we measured structural properties of the k-IPF such as average \ndegree of internal nodes (Figure 19) and the average leaf depth (Figure 20). Our tests reveal that the \nav\u00aderage node degree generally decreases with k, showing that similar patterns tend to appear frequently \nacross different it\u00aderations. Some benchmarks, however, such as sunflow and heapsort exhibit a larger \nvariety of path rami.cations, wit\u00adnessed by increasing node degrees at deeper levels of the k-IPF. The \naverage leaf depth allows us to characterize the loop iteration activity of different benchmarks. Notice \nthat for some benchmarks, such as avrora and hsqldb, most cycles consist of a small number of iterations: \nhence, by in\u00adcreasing kbeyond this number, k-BLPP does not collect any additional useful information. \nDiscussion. From our experiments, we could draw two main conclusions: 1. Using tree-based data structures \nto represent intraproce\u00addural control .ow makes it possible to substantially re\u00adduce the performance \noverhead of path pro.ling by de\u00adcreasing the number of hash operations -and also by op\u00aderating on smaller \ntables. This approach yields the .rst pro.ler that can handle loops that extend across mul\u00adtiple loop \niterations faster than the general Ball-Larus technique based on hash tables for maintaining path fre\u00adquency \ncounters, while collecting at the same time signif\u00adicantly more informative pro.les. We observed that, \ndue to limitations of our current implementation of k-BLPP such as lack of cache friendliness for some \nworst-case scenarios, on a few outliers our pro.ler was slower than Ball-Larus, with a peak of 3.76x \nslowdown on one bench\u00admark. 2. Since the number of pro.led paths in the control .ow graph typically \ngrows exponentially for increasing val\u00adues of k, space usage can become prohibitive if paths spanning \nmany loop iterations have to be exhaustively pro.led. We noticed, however, that most long paths have \nsmall frequency counters, and are therefore uninteresting for identifying optimization opportunities. \nHence, a use\u00adful addition to our method, which we do not address in this work, would be to prune cold \nnodes on-the-.y from the k-SF, keeping information for hot paths only.  7. Related Work The seminal \nwork of Ball and Larus [4] has spawned much research interest in the last 15 years, in particular on \npro\u00ad.ling acyclic paths with a lower overhead by using sam\u00adpling techniques [10, 11] or choosing a subset \nof interesting paths [2, 16, 21]. On the other hand, only a few works have dealt with cyclic-path pro.ling. \nTallam et al. [20] extend the Ball-Larus path numbering algorithm to record slightly longer paths across \nloop back edges and procedure boundaries. The extended Ball-Larus paths overlap and, in particular, are \nshorter than two itera\u00adtions for paths that cross loop boundaries. These overlap\u00adping paths enable very \nprecise estimation of frequencies of potentially much longer paths, with an average imprecision in estimated \ntotal .ow of those paths ranging from -4% to +8%. However, the average cost of collecting frequencies \nof overlapping paths is 4.2 times that of canonical BLPP on average. Roy and Srikant [19] generalize \nthe Ball-Larus algorithm for pro.ling k-iteration paths, showing that it is possible to number these \npaths ef.ciently using an inference phase to record executed backedges in order to differentiate cyclic \npaths. One problem with this approach is that, since the number of possible k-iteration paths grows exponentially \nwith k, path IDs may over.ow in practice even for small values of k. Furthermore, very large hash tables \nmay be required. In particular, their pro.ling procedure aborts if the number of static paths exceeds \n60,000, while this threshold is reached on several small benchmarks already for k = 3 [17]. This technique \nincurs a larger overhead than BLPP: in particular, the slowdown may grow to several times the BLPP-associated \noverhead as k increases. Li et al. [17] propose a new path encoding that does not rely on an inference \nphase to explicitly assign identi.ers to all possible paths before the execution, yet ensuring that any \n.nite-length acyclic or cyclic path has a unique ID. Their path numbering algorithm needs multiple variables \nto record probe values, which are computed by using addition and multiplication operations. Over.owing \nis handled by using breakpoints to store probe values: as a consequence, instead of a unique ID for each \npath, a unique series of breakpoints is assiged to each path. At the end of program s execution, the \nbackwalk algorithm reconstructs the executed paths starting from breakpoints. This technique has been \nintegrated with BLPP to reduce the execution overhead, resulting in a slow\u00addown of about 2 times on average \nwith respect to BLPP, but also showing signi.cant performance loss (up to a 5.6 times growth) on tight \nloops (i.e., loops that contain a small num\u00adber of instructions and iterate many times). However, the \nex\u00adperiments reported in [17] were performed on single meth\u00adods of small Java programs, leaving further \nexperiments on larger industry-strength benchmarks to future work. Of a different .avor is the technique \nintroduced by Young [23] for pro.ling general paths, i.e., .xed-length se\u00adquences of taken branches that \nmight span multiple loop iterations (see Section 3.2). Unfortunately, this technique scales poorly for \nincreasing path lengths l both in terms of space usage and running time. In particular, the running time \nis proportional not only to the length of the stream of taken branches, but also to the number of possible \nsequences of length l, that is likely to be exponential in l. In order to re\u00adduce the per-taken-branch \nupdate time, the algorithm uses also additional space with respect to that required for storing the path \ncounters and identi.ers; such space is proportional to the number of possible sequences of length l as \nwell. A comparison of different path pro.ling techniques known in the literature with our approach is \nsummarized in Table 1. The variables column reports the number of probes required to construct path IDs \nand does not apply to the work by Young [23], which uses a different approach. 8. Conclusions In this \npaper we have presented a novel approach to cyclic\u00adpath pro.ling, which combines the original Ball-Larus \npath numbering technique with a pre.x tree data structure to keep track of concatenations of acyclic \npaths across multiple loop iterations. A large suite of experiments on a variety of prominent benchmarks \nshows that, not only does our approach collect signi.cantly more detailed pro.les, but it can also be \nfaster than the original Ball-Larus technique by reducing the number of hash table operations. An interesting \nopen question is how to use sampling\u00adbased approaches such as the one proposed by Bond and McKinley [10] \nto further reduce the path pro.ling over\u00adhead. We believe that the bursting technique, introduced by \nZhuang et al. [24] in the different scenario of calling-context pro.ling could be successfully combined \nwith our approach, allowing an overhead reduction while maintaining reason\u00adable accuracy in mining hot \npaths. In particular, a bursting pro.ler allows the analyzed application to run unhindered between two \nsampling points, then it collects a burst of pro\u00ad.ling events for an interval de.ned as burst length. \nOverhead is further reduced through adaptive mechanisms that inhibit redundant pro.ling due to repetitive \nexecution sequences. Another way to reduce the pro.ling overhead may be to exploit parallelism. We note \nthat our approach, which decouples path tracing from pro.ling using an intermediate data stream, is amenable \nto multi-core implementations by letting the pro.led code and the analysis algorithm run on separate \ncores using shared buffers. A promising line of research is to explore how to partition the data structures \nso that portions of the stream buffer can be processed in parallel. Finally, we observe that, since our \napproach is exhaustive and traces taken paths regardless of their hotness, it would be interesting to \nexplore techniques for reducing space usage, by pruning cold branches of the k-SF on the .y to keep the \nmemory footprint smaller, thus allowing us to deal with even longer paths. Acknowledgments. A warm acknowledgment \ngoes to Irene Finocchi for her contributions to the design of the approach presented in this paper and \nfor her help in setting up our experimental package for the OOPSLA 2013 artifact evalu\u00adation process. \nWe are indebted to Michael Bond for several interesting discussions, for his invaluable support with \nhis BLPP pro.ler, and for shedding light on some tricky as\u00adpects related to the Jikes RVM internals. \nWe would also like to thank Erik Brangs for his help with the Jikes RVM and Jos \u00b4e Sim ao for his hints \nin selecting an adequate set of benchmarks for the Jikes RVM. Finally, we are grateful to the anonymous \nOOPSLA 2013 referees for their extremely thorough reviews and for their many useful comments. References \n[1] B. Alpern, C. R. Attanasio, J. Barton, M. G. Burke, P. Cheng, J. D. Choi, A. Cocchi, S. Fink, D. \nGrove, M. Hind, S. F. Hummel, D. Lieber, V. Litvinov, M. F. Mergen, T. Ngo, J. R. Russell, V. Sarkar, \nM. J. Serrano, J. Shepherd, S. E. Smith, V. Sreedhar, H. Srinivasan, and J. Whaley. The Jalape no virtual \nmachine. IBM Systems Journal, 39(1):211 238, 2000. ISSN 0018-8670. [2] T. Apiwattanapong and M. J. Harrold. \nSelective path pro.ling. In Proc. ACM SIGPLAN-SIGSOFT workshop on Program analysis for software tools \nand engineering, pages 35 42. ACM, 2002. [3] G. Ausiello, C. Demetrescu, I. Finocchi, and D. Firmani. \nk\u00adcalling context pro.ling. In Proc. ACM SIGPLAN Conference on Object-Oriented Programming, Systems, \nLanguages, and Applications, OOPSLA 2012, pages 867 878, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1561-6. \n[4] T. Ball and J. R. Larus. Ef.cient path pro.ling. In MICRO 29: Proceedings of the 29th annual ACM/IEEE \ninternational symposium on Microarchitecture, pages 46 57, 1996. [5] S. M. Blackburn, R. Garner, C. Hoffman, \nA. M. Khan, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. \nHosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, D. von Dincklage, \nand B. Wiedermann. The DaCapo benchmarks: Java benchmarking development and analysis. In OOPSLA 06: Proceedings \nof the 21st annual ACM SIGPLAN confer\u00adence on Object-Oriented Programing, Systems, Languages, and Applications, \npages 169 190, New York, NY, USA, Oct. 2006. ACM Press.  [6] R. Bod\u00b4ik, R. Gupta, and M. L. Soffa. Interprocedural \ncon\u00additional branch elimination. In Proceedings of the ACM SIG-PLAN 1997 conference on Programming language \ndesign and implementation, PLDI 97, pages 146 158, New York, NY, USA, 1997. ACM. ISBN 0-89791-907-6. \n[7] R. Bod\u00b4ik, R. Gupta, and M. L. Soffa. Load-reuse analysis: design and evaluation. In Proceedings \nof the ACM SIGPLAN 1999 conference on Programming language design and im\u00adplementation, PLDI 99, pages \n64 76, New York, NY, USA, 1999. ACM. ISBN 1-58113-094-5. [8] R. Bod\u00b4ik, R. Gupta, and V. Sarkar. Abcd: \neliminating array bounds checks on demand. In Proceedings of the ACM SIG-PLAN 2000 conference on Programming \nlanguage design and implementation, PLDI 00, pages 321 333, New York, NY, USA, 2000. ACM. ISBN 1-58113-199-2. \n[9] R. Bod\u00b4ik, R. Gupta, and M. L. Soffa. Complete removal of redundant expressions. SIGPLAN Not., 39(4):596 \n611, Apr. 2004. ISSN 0362-1340. [10] M. D. Bond and K. S. McKinley. Continuous path and edge pro.ling. \nIn Proc. 38th annual IEEE/ACM International Sym\u00adposium on Microarchitecture, pages 130 140. IEEE Com\u00adputer \nSociety, 2005. [11] M. D. Bond and K. S. McKinley. Practical path pro.ling for dynamic optimizers. In \nCGO, pages 205 216. IEEE Com\u00adputer Society, 2005. [12] J. Bull, L. Smith, M. Westhead, D. Henty, and \nR. Davey. A methodology for benchmarking Java Grande applications. In Proceedings of the ACM 1999 conference \non Java Grande, pages 81 88. ACM, 1999. [13] J. A. Fisher. Trace scheduling: A technique for global mi\u00adcrocode \ncompaction. IEEE Trans. Comput., 30(7):478 490, July 1981. ISSN 0018-9340. [14] E. Fredkin. Trie memory. \nCommunications of the ACM, 3(9): 490 499, 1960. [15] Jikes RVM Research Archive. PEP: continuous path \nand edge pro.ling. http://jikesrvm.org/Research+Archive. [16] R. Joshi, M. D. Bond, and C. Zilles. Targeted \npath pro.ling: Lower overhead path pro.ling for staged dynamic optimiza\u00adtion systems. In CGO, pages 239 \n250, 2004. [17] B. Li, L. Wang, H. Leung, and F. Liu. Pro.ling all paths: A new pro.ling technique for \nboth cyclic and acyclic paths. Journal of Systems and Software, 85(7):1558 1576, 2012. ISSN 0164-1212. \n[18] perf: Linux pro.ling with performance counters. https://perf.wiki.kernel.org/. [19] S. Roy and Y. \nN. Srikant. Pro.ling k-iteration paths: A generalization of the ball-larus pro.ling algorithm. In CGO, \npages 70 80, 2009. [20] S. Tallam, X. Zhang, and R. Gupta. Extending path pro\u00ad.ling across loop backedges \nand procedure boundaries. In Proceedings of the international symposium on Code gener\u00adation and optimization: \nfeedback-directed and runtime opti\u00admization, CGO 04, pages 251 264, Washington, DC, USA, 2004. IEEE Computer \nSociety. ISBN 0-7695-2102-9. [21] K. Vaswani, A. V. Nori, and T. M. Chilimbi. Preferential path pro.ling: \ncompactly numbering interesting paths. In POPL, pages 351 362. ACM, 2007. [22] C. Young and M. D. Smith. \nBetter global scheduling using path pro.les. In Proceedings of the 31st annual ACM/IEEE international \nsymposium on Microarchitecture, MICRO 31, pages 115 123, 1998. [23] R. C. Young. Path-based compilation. \nPhD thesis, Division of Engineering and Applied Sciences, Harvard University, Cambridge, MA, 1998. [24] \nX. Zhuang, M. J. Serrano, H. W. Cain, and J.-D. Choi. Accu\u00adrate, ef.cient, and adaptive calling context \npro.ling. In PLDI, pages 263 271, 2006.   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Identifying the hottest paths in the control flow graph of a routine can direct optimizations to portions of the code where most resources are consumed. This powerful methodology, called <i>path profiling</i>, was introduced by Ball and Larus in the mid 90's [4] and has received considerable attention in the last 15 years for its practical relevance. A shortcoming of the Ball-Larus technique was the inability to profile cyclic paths, making it difficult to mine execution patterns that span multiple loop iterations. Previous results, based on rather complex algorithms, have attempted to circumvent this limitation at the price of significant performance losses even for a small number of iterations. In this paper, we present a new approach to multi-iteration path profiling, based on data structures built on top of the original Ball-Larus numbering technique. Our approach allows the profiling of <i>all executed paths</i> obtained as a concatenation of up to <i>k</i> Ball-Larus acyclic paths, where <i>k</i> is a user-defined parameter. We provide examples showing that this method can reveal optimization opportunities that acyclic-path profiling would miss. An extensive experimental investigation on a large variety of Java benchmarks on the Jikes RVM shows that our approach can be even faster than Ball-Larus due to fewer operations on smaller hash tables, producing compact representations of cyclic paths even for large values of <i>k</i>.</p>", "authors": [{"name": "Daniele Cono D'Elia", "author_profile_id": "81485656450", "affiliation": "Sapienza University of Rome, Rome, Italy", "person_id": "P4290379", "email_address": "delia@dis.uniroma1.it", "orcid_id": ""}, {"name": "Camil Demetrescu", "author_profile_id": "81100357279", "affiliation": "Sapienza University of Rome, Rome, Italy", "person_id": "P4290380", "email_address": "demetres@dis.uniroma1.it", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509521", "year": "2013", "article_id": "2509521", "conference": "OOPSLA", "title": "Ball-Larus path profiling across multiple loop iterations", "url": "http://dl.acm.org/citation.cfm?id=2509521"}