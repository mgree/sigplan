{"article_publication_date": "10-29-2013", "fulltext": "\n River Trail: A Path to Parallelism in JavaScript Stephan Herhut Richard L. Hudson Tatiana Shpeisman \nJaswanth Sreeram Intel Labs Santa Clara, CA, USA stephan.a.herhut, rick.hudson, tatiana.shpeisman, jaswanth.sreeram@intel.com \n Abstract JavaScript is the most popular language on the web and is a crucial component of HTML5 applications \nand services that run on consumer platforms ranging from desktops to phones. However, despite ample amount \nof hardware par\u00adallelism available to web applications on such platforms, JavaScript web applications \nremain predominantly sequen\u00adtial. Common parallel programming solutions accepted by other programming \nlanguages failed to transfer themselves to JavaScript due to differences in programming models, the additional \nrequirements of the web and different developer expectations. In this paper we present River Trail a \nparallel pro\u00adgramming model and API for JavaScript that provides safe, portable, programmer-friendly, \ndeterministic parallelism to JavaScript applications. River Trail allows web applications to effectively \nutilize multiple cores, vector instructions, and GPUs on client platforms while allowing the web developer \nto remain within the environment of JavaScript. We describe the implementation of the River Trail compiler \nand runtime and present experimental results that show the impact of River Trail on performance and scalability \nfor a variety of realistic HTML5 applications. Our experiments show that River Trail has a dramatic positive \nimpact on overall perfor\u00admance and responsiveness of computationally intense Java-Script based applications \nachieving up to 33.6 times speedup for kernels and up to 11.8 times speedup for realistic web applications \ncompared to sequential JavaScript. Moreover, River Trail enables new interactive web usages that are \nsim\u00adply not even possible with standard sequential JavaScript. Categories and Subject Descriptors D.1.3 \n[Programming Techniques]: Concurrent Programming Parallel Program- Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. Copyrights for components of this work owned by others than the \nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to \npost on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request \npermissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright \nis held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2374-1/13/10. . . \n$15.00. http://dx.doi.org/10.1145/2509136.2509516 ming; D.3.2 [Programming Languages]: Language Classi\u00ad.cations \n JavaScript; D.3.4 [Programming Languages]: Processors Compilers Keywords Parallelism; JavaScript 1. \nIntroduction The power wall has put a hold to the ever increasing growth of processor frequencies, forcing \nhardware manufactures to look for alternative ways to improve program performance. Modern microprocessors \nfor all form factors routinely fea\u00adture multiple cores, vector instructions, and tightly coupled graphics \nprocessing units. Parallel hardware has become a commodity. In response, language extensions or APIs \nfor parallelism have emerged for most popular programming languages, slowly pushing parallel programming \ninto the mainstream; however, one popular programming language, JavaScript, the lingua franca of the \nweb, has surprisingly es\u00adcaped this trend and remains predominantly sequential. JavaScript has long outgrown \nits original use as a light\u00adweight scripting language for web pages [2]. Increasingly, it is used for \nlarge scale applications that have as big a poten\u00adtial to bene.t from parallel execution as any other \nclient-side application. In combination with HTML5 [4], JavaScript is the sole universally supported \nprogramming environment for browser-based web applications; it is also emerging as a full\u00adscale software \ndevelopment stack for stand-alone applica\u00adtions, especially in the mobile space [20]. HTML and Java-Script \napplications are rapidly gaining access to the capa\u00adbilities traditionally reserved for native applications, \nsuch as fast 2D and 3D graphics, of.ine storage, video and audio content, camera capture and geo location, \nto name just a few. With these new capabilities come new usages and appli\u00adcation scenarios, of which \nmany quickly outgrow the per\u00adformance envelope offered by the current, sequential Java-Script implementations. \nIn the context of 3D graphics, op\u00aderations like skinning for character animation, collision de\u00adtection \nfor gaming or simulation of particle systems come to mind. Access to the camera built into a user s device \nen\u00adables new applications from in-browser video conferencing to novel ways of human device interaction. \nIn both scenarios, processing the video stream in (near) real time is essential. Fast 2D rendering enables \nnew forms of data visualization, which, in particular in combination with large data sets, re\u00adquire speedy \nlayout computations. For these and many yet to be conceived applications, access to parallel hardware \nand its promised performance gain will be required to realize the web platform s full potential.  The \nopen web has a unique combination of requirements that preclude the use of existing parallel programming \nAPIs. First is the hard requirement of safety and security. In con\u00adtrast with most platforms, the user \nhas little control over what code is executed in the browser. Applications are run by simply visiting \na web page. Introducing additional mal\u00adware attack surfaces is simply not an option. Second is the web \ndeveloper community s demand for a single familiar programming model that builds upon the knowledge and \ntools the developers already possess. In particular, program\u00admers have grown used to deterministic program \nbehavior, which is typically undermined by concurrent programming. Third is the need to support a wide \nrange of form factors and hardware architectures. A parallel programming model has to be generic enough \nto support multi-core CPUs, vec\u00adtor instruction sets, and programmable GPUs from a single source base. \nThis requires a higher level of abstraction than commonly found in existing APIs. Finally, for the model \nto be successful, it should be capable of extracting dramatic performance improvements from the available \nparallel hard\u00adware. In this paper we present River Trail, our vehicle to ex\u00adplore the design space for \na parallel programming API that addresses those four challenges. We have used the same lan\u00adguage and \nAPI design model that HTML5 and JavaScript are successfully applying to move the web forward: rapid pro\u00adtotyping \nof APIs and a lively language designer/application developer feedback cycle. To enable the former, we \nhave im\u00adplemented River Trail as a sequential library on top of Java-Script that runs in all modern browsers. \nThis allowed us to quickly evolve the API with minimal development overhead while covering most client \nsystems. We also implemented a prototype compiler on top of OpenCL to be able to evaluate our design \nagainst the fourth requirement: performance. At the same time, we have gathered as much developer feed\u00adback \nas possible. We have developed our prototype in the open on GitHub1 to give developers early access. \nWe also re\u00adcruited a group of developers ranging from college students to professional web developers \nto gather early feedback dur\u00ading the design of the API. This helped us particularly in ad\u00addressing the \nsecond challenge: providing a comfortable API. Another advantage of involving developers early on is \nthat it helped seed our set of benchmarks. Bringing par\u00adallelism to the web is not an incremental change. \nInstead, we aim to change the way users experience web applica\u00adtions. Existing benchmark suites such \nas SunSpider, V8 and Kraken do not re.ect real web applications [26] and web applications are typically \nwell tuned to stay within avail\u00ad 1 http://github.com/RiverTrail/RiverTrail able performance characteristics. \nInstead, our benchmarks, including for instance real time video processing, motion tracking and game \nphysics, look at applications that are not possible in browsers today. The more recent Octane suite consists \nof programs taken from V8 and several other pro\u00adgrams that have been compiled from C/C++ to JavaScript. \nWe believe that these programs do not represent how Java-Script is actually used by programmers for developing \nweb applications either -the applications we consider are all de\u00adveloped in JavaScript. Our design has \nconverged on an API2 centered around a single new data type ParallelArray as a basic abstraction for \nparallel computation, accompanied by six methods that implement well known parallel patterns: map, reduce, \nscan, scatter and .lter. The ParallelArray API provides the high\u00adlevel of abstraction necessary to support \nmultiple platforms and hardware parallelism .avors, such as multiple cores, vector instructions and GPU \nexecutions. Our API provides deterministic parallelism for the majority of programs. We guarantee deterministic \nexecution equivalent to a sequen\u00adtial execution except for scan and reduce operations that use non-commutative \nor non-associative operations. This level of determinism is similar to that of Google s map-reduce programming \nmodel [9] and seems to be an acceptable trade off well received by programmers. Finally, our API trivially \nprovides the same level of safety and security as JavaScript because it stays within the boundaries of \nthe same program\u00adming language. We have evaluated our API with multiple web appli\u00adcations that demonstrate \nrealistic usage scenarios such as web-based gaming, real-time video effect .ltering and 3D computer animations, \nand several kernels. Most applications were developed by external parties. Our experiments show that \nRiver Trail is capable of delivering an order of magni\u00adtude performance improvement on an off-the-shelf \nsystem for a realistic web application, and even larger, up to 33.6x times, performance improvements \nfor fully parallelized ker\u00adnels. These speedups are suf.ciently large that they not only improve the \nquality of user experience (e.g., less jitter while applying a video effect) but also enable applications \nthat are simply performance infeasible without River Trail, such as Bugs an interactive gesture based \nbrowser game. River Trail, thus, delivers on our objectives: a simple programming model with dramatic \nperformance improvements. The rest of this paper is organized as follows. Section 2 describes River Trail \nlanguage design and APIs. Section 3 presents the implementation of the River Trail prototype. Section \n4 presents the experimental results. We describe the related work in Section 5 and conclude in Section \n6. 2 We have further distilled our results into Parallel JavaScript [15], a pro\u00adposal for a parallel \nprogramming API put in front of the ECMA TC39 com\u00admittee, which is tasked to evolve the JavaScript standard. \n 2. Language Design River Trail was designed from ground up to meet the re\u00adquirements of the open web. \nTo ful.ll the web s safety re\u00adquirements, we have designed River Trail to enable the use of the same \nsafety mechanisms that typical JavaScript im\u00adplementations use. This includes, for instance, the absence \nof pointers, bounds checked array accesses, and automatic heap management. River Trail also makes use \nof high-level parallel patterns which we believe are a requirement to enable platform porta\u00adbility. Like \nJavaScript, we refrain from exposing device spe\u00adci.c types and use a generic number type instead. Last \nbut not least, River Trail puts ease of use over per\u00adformance. Designing for web developers productivity \nfor us meant two things: Reducing the likelihood of program\u00admer errors and keeping the annotation burden \ndue to paral\u00adlelism low, both while achieving performance at an accept\u00adable level. The next two sections \nelaborate on our approach. 2.1 Concurrency Model Various programming models aim to harness the concur\u00adrency \navailable in modern hardware. Shared memory mod\u00adels commonly found in languages like Java, C# or C++ \nuse mutation of global state as means for communication be\u00adtween concurrent threads. Such models come \nwith a host of programming hazards such as complicated memory mod\u00adels, data races, dead lock and live \nlock. These concepts were deemed suf.ciently dangerous to the web development com\u00admunity that they were \nrejected outright as a non-starter for language design. On the other hand, Actors and other message passing \nbased programming models and languages segregate both computation and data and use messages to communicate. \nSome of the better known examples include Actors and Er\u00adlang. HTML5 also adopted this approach in the \nform of web workers. They were designed for of.oading of computations to hide the latency of long running \noperations. Message pass\u00ading approaches in general, but web workers in particular suf\u00adfer from a high \ncost of communication. Finally functional languages generally discourage muta\u00adtion of state. Instead, \ncomputations typically produce a fresh state, which may then be safely shared. However, such an approach \nis also not a perfect .t due to the object-oriented nature of JavaScript and the performance necessity \nto mutate state. In River Trail, we adopt a compromise: In the spirit of functional programming, our \nmodel allows spawned tasks immutable access to their parent s state thus ameliorating the need to pass \nmessages or call by value. In contrast to a purely function design, siblings are free to allocate and \nmutate their local heap. However, they cannot communicate with each other. This approach ensures safety \nwhile keeping runtime overheads low. It is important to note here that the parent thread is sus\u00adpended \nwhile its children are running and it therefore cannot mutate its own local state. Therefore, from the \nchildrens per\u00adspective, the global state does not change. However, once all children have completed and \nthe parent thread resumes, it is free to mutate its own local state. Ultimately, during sequen\u00adtial execution, \nthe entire heap is mutable, which corresponds to the current JavaScript heap model. We call this approach \ntemporal immutability.  2.2 API River Trail provides a data-parallel API using the concept of parallel \narray as a fundamental abstraction of parallel computation. The API is built out of three components: \na new data type ParallelArray, a set of parallel methods that specify the computation pattern and the \nconcept of an elemental function that speci.es the computation performed on each parallel array element. \nFigure 1 shows a simple example of a River Trail computation. A pair-wise addition is performed by calling \nthe map method. It takes as an argument an anonymous elemental function that computes a sum of two values. \nThe ParallelArray type features a minimalistic API sum\u00admarized in Table 1. ParallelArray objects consist \nof scalar values (single or double-precision .oating point values or in\u00adtegers). ParallelArray objects \ncan be created either from ex\u00adisting array-like data structures (such as, for example, Java-Script Array \nobjects, another ParallelArray or HTML5 typed arrays) or by using the comprehension constructor, as illus\u00adtrated \nin lines 2-3 of Figure 2. Here, the ParallelArray ones is computed as a vector of length a.length containing \nthe values returned by the elemental function passed as a sec\u00adond argument of the constructor in this \ncase all ones. A ParallelArray s elements can be processed using one of the six fundamental methods: \nmap, combine, reduce, scan, .lter and scatter. Map, reduce, scan and .lter methods have standard data-parallel \nsemantics. Combine is similar to map, except that it exposes the current index to the elemental function. \nThe scatter method distributes elements from the ParallelArray into a new ParallelArray according to \na given sequence of indices (akin to NESL s permute operation). River Trail s scatter, unlike the classical \nversion, is mostly deterministic. In the situation when multiple values scatter to the same memory location, \nthey are combined using a spe\u00adcial con.ict resolution function, which is passed to scatter as an argument. \nIn case of a con.ict with no con.ict resolution function speci.ed, scatter throws an exception. This \nguaran\u00adtees deterministic execution of scatter, provided the con.ict resolution function is commutative \nand associative. Because of this additional functionality, scatter effectively acts like a reduce from \nthe map-reduce programming paradigm [9]. This usage scenario is illustrated in Figure 2, where a his\u00adtogram \nof a ParallelArray s elements is computed by apply\u00ading scatter to an array of all ones with addition \nas the con.ict resolution function.  Constructors: Signature Creates ParallelArray(); ParallelArray(arr); \nParallelArray(pa 1, pa 2, ..pa n); ParallelArray(s, f, ...args ); ParallelArray([s0, s1, ..], f, ...args \n); An empty ParallelArray A ParallelArray from an Array-like object arr A ParallelArray object of shape \n(n, shape (pa 1)) A ParallelArray of length s from a comprehension of f A ParallelArray of shape (s0, \ns1, ..) from a comprehen\u00adsion of f Methods: Name Signature Elemental Function Map Combine Reduce Scan \nScatter Filter Flatten Partition map(f, ...args *) combine(s * , f, ...args *) reduce(f, ...args *) scan(f, \n...args *) scatter(indices , defaultvalue * , f * , length * ) .lter(f, ...args *) .atten() partition(s) \nf(p0, ...args ) f(index, ...args ) f(p0, p1, ...args ) f(p0, p1, ...args ) f(p0, p1) f(index, ...args \n) Table 1: The ParallelArray API. A * indicates optional arguments. f denotes a function object, s and \nsi are scalars, pai is a ParallelArray object and pi is either a scalar or a ParallelArray object. shape(pai) \nreturns a shape vector denoted by () that describes the number of elements in each dimension of pai. \nA shape vector is always .at i.e., its elements are scalar values. For a detailed description of the \nAPI we refer the reader to the language speci.cation at [14]. ParallelArray objects are immutable, i.e., \nonce they have been created, their values can no longer be changed. Instead, operations on ParallelArray \nobjects return a freshly minted ParallelArray (except for reduce, which returns a scalar). This restriction \n.ts well with the overall programming model and allows us to perform optimizations, such as using a more \nef.cient .at storage layout. Note that our API does not feature many methods com\u00admonly found in other \ndata-parallel languages, such as add, sum, pre.xSum or gather. This minimalistic approach al\u00adlows us \nto minimize the size of the compiler implementation, focus the language s design on the fundamental issues \nand, consequently, minimize the length of the re-design develop\u00adment cycle. Other methods can be easily \nimplemented on top of our API as a JavaScript library. For example, sum can be implemented via reduce, \nwhile gather can be implemented via the comprehension constructor. In principle, we could have gone even \nfurther, and eliminate, map and combine, as both of these methods can also be expressed by means of the \ncomprehension constructor. We include them nonetheless, as programmers expect built in map operations. \nElemental functions are essentially arbitrary JavaScript functions with the restriction that they may \nnot mutate global state. Elemental functions are, thus, side-effect free. Elemen\u00adtal functions may mutate \nlocal state and access shared global state in a read-only fashion. It is the responsibility of the implementation \nto detect, possibly conservatively, a global state mutation and reject the execution of the corresponding \nParallelArray method by throwing an exception. As elemen\u00adtal functions are side-effect free, the exception \ncan be thrown at any point of the ParallelArray method execution cycle. This gives the implementation \nthe .exibility to detect viola\u00adtions during either just-in-time compilation or at run time. River Trail \ns programming model guarantees determinis\u00adtic execution of map, combine and .lter methods, equivalent \nto any sequential execution of these methods. This fact is a direct consequence of elemental functions \nbeing side-effect free. Reduce, scan and scatter are deterministic as long as their elemental or con.ict \nresolution functions are commu\u00adtative and associative. Otherwise, their execution is equiva\u00adlent to some \nsequential execution. This trade-off is similar to that of the map-reduce programming model and seems \nto be well accepted by programmers. While requiring both commutativity and associativity for guaranteed \ndeterministic execution seems restricitive from a programmers perspective, it imposes the least constraints \non concurrent implementations of reduce, scan and scatter. Such .exibility is particularly bene.cial \nduring prototyping and design space exploration. Yet, the current semantics also allow for re.ning the \nAPI in future versions by, e.g., lower\u00ading the requirements to just associativity, without breaking exisiting \ncode, River Trail naturally supports nested ParallelArray ob\u00adjects, i.e., ParallelArray objects whose \nelements are also ParallelArrays. It also supports generic n-dimensional pro\u00adgramming inspired by languages \nlike APL [16]. A Parallel\u00ad  1 function add(a, b) { 2 return a.map(function(e1, e2) {return e1 + e2;}, \n3 b); 4 } Figure 1: Pair-wise addition in River Trail. Array object may encapsulate multiple dimensions \nand we consistently use index vectors instead of scalar indices in our API. As selection in JavaScript \nis based on scalars, we have added the get method that implements vector based selec\u00ad tion. Unlike a \nnested ParallelArray object, a multi-dimensional ParallelArray object is restricted in its shape: For \neach di\u00ad mension, all elements have to have the same length. This al\u00ad lows us to encode a ParallelArray \nobject s shape in a single vector of extents, available to the programmer via the get- Shape method. \n.atten and partition methods serve to change the dimensionality of multi-dimensional ParallelArray ob\u00ad \njects. Figure 3 shows a straight-forward implementation of ma\u00ad trix matrix multiplication written using \ntwo-dimensional ParallelArray objects. The example uses a two-dimensional combine operator to compute \neach element of the resulting matrix in parallel, as implemented by the elemental function matMultElement. \nLine 12 shows the corresponding method call. The .rst argument to combine is the number of dimen\u00ad sions \nto iterate in this case two, so that the elements to consider will be scalar values. Note that the elemental \nfunc\u00ad tion, de.ned on Line 1, expects two arguments: the index and the second array B. A typical JavaScript \nprogram would simply use the closure bound B from the surrounding scope instead. However, due to the \nrestrictions of our prototype im\u00ad plementation, we cannot support closure bound variables in elemental \nfunctions. JavaScript does not provide a re.ection API to access the bindings of a function s closure. \nWhile this is a reasonable design choice in general, e.g., to ensure encapsulation, it prevents our prototype \ncompiler, which is written in JavaScript, from reasoning about closure bound variables. A deeper embedding \ninto the JavaScript runtime and just-in-time compiler would certainly provide access to closures but \nat the cost of a more complex implementation. Instead, we have chosen to adapt our API in the prototype \nand use the notion of extra arguments: All arguments follow\u00ad ing the second one in combine are directly \npassed through to the elemental function for iteration. We expect a product\u00ad quality implementation to \nuse an API tailored towards clo\u00ad sure bound variables, instead. 3. Implementation Our prototype consists \nof three major building blocks as de\u00adpicted in Figure 4. First, we have written a JavaScript library \nthat implements the ParallelArray API on top of JavaScript. Additionally, we implemented support for \nparallel execution 1 function histogram(a) { 2 var ones = new ParallelArray(a.length, 3 function(i) \n{return 1;}); 4 5 return ones.scatter(a, 0, 6 function(e1, e2) {return e1 + e2;}); 7 } Figure 2: Histogram \nin River Trail. 1 function matMultElement(index, B) { 2 var i = index[0]; var j = index[1]; 3 var sum \n= 0; var len = this.getShape()[1]; 4 5 for(var k = 0; k < len; k++) { 6 sum += this.get([i, k]) * B.get([k, \nj]); 7 } 8 return sum; 9 } 10 11 function MatrixMultiply(A, B) { 12 return A.combine(2, matMultElement, \nB); 13 } Figure 3: Na\u00a8ive implementation of Matrix Multiply in River Trail. Figure 4: Design of the \nprototype implementation (River Trail components in bold) for the Firefox browser, consisting of a compiler \nfrom Java-Script to OpenCL [21] and an OpenCL binding for Firefox. Our library implementation is portable \nacross modern browser engines and provides sequential execution for code using the River Trail API. Much \ncare has been taken to implement the same semantics as in the compiled parallel mode. However, to make \nthe use of River Trail practical in sequential code, we have prioritized performance where need be. The \nsequential implementation is based on JavaScript s typed arrays C like one-dimensional arrays that are \nlaid out continuously in memory. By default the sequential li\u00adbrary uses Float64Array arrays, but programmer \nannota\u00adtions can specify other array types such as Float32Array. This enables direct interfacing with \nexisting HTML5 APIs like WebGL or canvas and also simpli.es the implementa\u00adtion of our parallel runtime \n(cf. Section 3.2). As typed ar\u00adrays are one-dimensional, we had to implement nesting and n-dimensional \nindexing on top of the .at data storage. We do this by wrapping the typed array object into our own ParallelArray \nobject and implementing the index conver\u00adsions in the get method. To reduce runtime overheads, we dy\u00adnamically \nspecialize the implementation of get to the dimen\u00adsionality of the array. We also implemented regular \nJava-Script indexing with [] operations by intercepting property lookup using JavaScript s proposed proxy \nfeature. However, runtime overheads turned out to be intolerable. Therefore, the library, unlike the \ncompiler, does not support direct in\u00addexing.  The sequential library also implements all parallel meth\u00adods, \nalbeit in sequential versions. We do not check for side effects of elemental functions in the library \nimplementation, nor do any of the limitations of the compiler apply. How\u00adever, we do enforce homogeneity \nand shape constraints on the computed ParallelArray objects. If a result does not com\u00adply, we gracefully \nfall back to nested JavaScript arrays to en\u00adable debugging. However, performance signi.cantly suffers \nin those cases. Parallel execution is enabled by our compiler. It is in\u00advoked by the sequential implementation \nin case of the com\u00adprehension constructor and the map and combine meth\u00adods. When such a method is called, \nwe dynamically check whether the compiler is present and was successfully initial\u00adized. If both conditions \nare met, we pass control to the com\u00adpiler which translates the elemental function into an OpenCL kernel \nfunction. On successful compilation, the generated kernel is executed using the OpenCL runtime binding. \nWe then transform the result into a new ParallelArray object and resume sequential execution. On failure \nor if the compiler is not present, we fall back to the sequential implementation. Using OpenCL as the \nunderlying runtime and compila\u00adtion backend allows us to target different hardware plat\u00adforms, like multi-core \nCPUs and programmable GPUs, from various vendors without investing into dedicated compila\u00adtion support \nfor each target. While this signi.cantly eases the implementation of a portable runtime, it also comes \nwith a range of challenges. Firstly, JavaScript and OpenCL differ signi.cantly from a language perspective. \nWhereas the former is a dynami\u00adcally typed language aimed at designers and beginner pro\u00adgrammers that \nabstracts from hardware speci.cs as much as possible, the latter is a statically typed language from \nthe C family of languages designed for performance experts and exposes many hardware speci.cs. A major \npart of our im\u00adplementation therefore is concerned with bridging this se\u00admantic gap from dynamic to static \nand hardware agnostic to hardware speci.c. Secondly, River Trail employs a restricted shared mem\u00adory \nprogramming model where all threads during concur\u00adrent execution have restricted read-only access to \nthe global heap. OpenCL on the other hand is based upon a distributed memory model where data is only \naccessible during concur\u00adrent execution if it has previously been explicitly mapped. Thus, our implementation \nhas to infer the correct mapping and manage the communication. Lastly, OpenCL and its runtime use explicit \nmemory management in the spirit of C, where all data structures are explicitly allocated and, more importantly, \nhave to be explicitly freed. JavaScript, on the other hand, uses a fully managed runtime and employs \ngarbage collection to free unused resources. This poses two problems: In code that we translate to OpenCL, \nwe have to convert JavaScript s implicit allocations to explicit allocations in OpenCL. This is complicated \nfurther by the lack of per-thread heaps in OpenCL. Another challenge in this context is to ensure that \nthe lifetime of memory allocated in OpenCL covers what is required by the semantics of the translated \ncode. The second issue arises in the implementation of the actual runtime. As the lifetime of heap allocated \nobjects in JavaScript is ulti\u00admately determined by the garbage collector, we have to keep the state of \nthe OpenCL runtime alive until the garbage col\u00adlector of the JavaScript runtime signals that it is no \nlonger needed. OpenCL runtimes are typically not optimized for the resulting deallocation patterns, leading \nto intolerable garbage collection latencies. The design of our embedding of OpenCL into the Firefox browser \ntakes this into account. In the following, we describe our solutions to the above challenges and discuss \nthe design decisions involved. 3.1 Translating JavaScript to OpenCL We have not aimed at writing a general \npurpose compiler for JavaScript to C like languages. Instead, the development of our compiler was largely \ndriven by use cases and the as\u00adsociated developer feedback. It is geared towards numeri\u00adcal codes and \nonly supports those language features that are typically required in that setting. In particular, our \nprototype does not support Closure bound variables As discussed earlier, supporting closures would require \na deeper integration into the Java- Script engine. User thrown exceptions Using throw triggers a fall \nback to the sequential implementation. For exceptions thrown by the JavaScript runtime or supported library \nfunctions, we ensure that concurrent execution is aborted and switch to sequential execution to produce \nthe actual exception. Objects Exceptions are array objects and special objects like the Math object whose \nmethods provide essential arithmetic operations required for numerical workloads. We also support the \nuse of objects as records in limited contexts. Value polymorphism In order to be accepted by our proto\u00adtype \ncompiler, variables need to refer to data of the same type throughout their lifetime. This is typically \nthe case in numerical workloads. Note, however, that functions may be used polymorphically.  Strings \nWe did not encounter a use case for strings in our benchmark applications. As strings typically use imple\u00admentations \nin today s jit engine that trigger internal side effects, we have deferred an implementation until the \nneed arises. These restrictions aside, we have taken great care to ensure that the semantics of the compiled \ncode is in line with Java\u00adScript s semantics. The majority of restrictions mentioned above are due to \nour prototype implementation and not a property of the pro\u00adgramming model itself. By embedding the compiler \ndeeper into the JavaScript engine, most of the above functional\u00adity can be implemented. A notable exception \nis objects, in particular non-native objects like DOM nodes. As their se\u00admantics are external to JavaScript, \nit is dif.cult to reason about their side effects or even dynamically capture their be\u00adhaviour. Native \nJavaScript objects, in contrast, have known semantics and could be analyzed by the JavaScript compiler. \nWe will further discuss above limitations and their respective reasons inline with the following description \nof the compiler. The compiler itself is written in JavaScript and runs alongside the programmer provided \nscripts. This brings two bene.ts: Firstly, the compiler is portable across different browser engines \nand thus only the runtime has to be adapted. Secondly, embedding the compiler in the existing JavaScript \ncontext gives us an easy way to intercept calls to Parallel-Array methods and divert them to OpenCL execution \nin\u00adstead. In principle, this would also work the other way round: a malicious web site could intercept \ncalls to the runtime and inject some exploit code. This problem, however, is not unique to the work presented \nhere and in particular Firefox, which itself is partially implemented in JavaScript, offers means to \nprevent such attacks [10]. Although possible, hardening our prototype compiler using those techniques \nis outside the scope of this work. Figure 5 gives an overview of the compiler stages. First, as most \ncompilers, we parse the elemental function into a syntax tree using the parser from Mozilla s Narcissus \nmeta\u00adcircular JavaScript interpreter3. We use JavaScript s ability to re.ect the source of any function \nby calling the function object s toString method. Second, we annotate the syntax tree with types. For \nthis, we have implemented an inference for a simple .rst-order type language with size information. We \nreject programs that make polymorphic use of variables and resolve function polymorphism by specialization. \nThis enables us to produce ef.cient OpenCL code without the need for tagged data representations. We \ndiscuss details in Section 3.1.1. 3 Narcissus is available at https://github.com/mozilla/narcissus Figure \n5: Overview of compilation stages of our prototype compiler The type inference infers types only up to \nthe JavaScript level. It does not take OpenCL s address spaces nor the different number types in OpenCL \ninto account. This gap is closed in the following two stages. During Address Space Propagation we compute \naddress spaces for all local variables. The parameters and return value of an elemental function are \nalways allocated in the global address space, as they need to be accessible from the JavaScript environment. \nAll other variables are by de\u00adfault allocated in the private address space. However, to emit valid OpenCL \ncode, we have to prevent data from the global heap to be referenced by variables from the private address \nspace and vice versa. To that effect, we forward propagate the global address space along the data-.ow \ngraph, promot\u00ading variables to the global address space where needed. Rare con.icts where both private \nand global data .ows to the same local variable are resolved by copying the global data into the private \nheap. Address spaces are computed for the whole kernel including local functions. Consequently, up\u00addates \nin address spaces may trigger further function special\u00adization. Beyond code generation, we also use the \ncomputed ad\u00address space partitioning to statically check for side effects due to array updates. We enforce \nthe constraint that only local arrays may be mutated by checking that the updated array was allocated \nin the private address space. This con\u00addition ensures that no globally visible side effects are per\u00adformed. \nIn the rare case that a global value was privatized to resolve an address space con.ict, we might allow \nan update that would otherwise be illegal. However, due to privatiza\u00adtion, the generated code is still \nside effect free. In a second stage, we introduce OpenCL speci.c types. It often is bene.cial to differentiate \nbetween Integers and .oating point values at runtime. Most JavaScript engines implement support for 32 \nbit integers to that effect. To decide whether a value would over.ow a 32 bit Integer at runtime, we \nnext perform a range analysis (cf. Section 3.1.2). Apart from computing ranges for all local variables, \nthe analysis  1 __kernel void RT_matMultElement(__global double* opThis, 2 __global double* RTl_B, 3 \n__global double* retVal) 4 { 5 int _id_0 = get_global_id(0); 6 int _id_1 = get_global_id(1); 7 int _writeoffset \n= _id_1+1024*_id_0; 8 double RTl_sum; 9 int RTl_len; 10 int RTl_k; 11 12 RTl_sum = (double) 0; 13 RTl_len \n= 1024; 14 for (RTl_k = 0; (RTl_k<RTl_len); RTl_k++) { 15 RTl_sum += opThis[RTl_k + 1024*id_0] 16 * RTl_B[id_1 \n+ 1024*RTl_k]; 17 } 18 retVal[_writeoffset] = RTl_sum; 19 return; 20 } Figure 6: Pseudo OpenCL code generated \nfor Matrix Multi\u00adply when applied to a 1024 \u00d7 1024 matrix. also infers whether a variable is known to \nbe integral. The representation analysis stage then uses the information to decide a global representation \nfor each variable. Next, we perform static memory allocation. OpenCL does not provide thread local heaps \nnor allow dynamic allo\u00adcation on the stack. Instead, only objects of statically known size may be allocated \nusing C99 style local arrays. Based on the size information computed by type inference, we stati\u00adcally \ncompute allocation sets. We use a path aware analysis to reduce the pressure on the stack, i.e., we overlap \nalloca\u00adtions for different branches of conditionals. As a last optimization, we eliminate bounds checks. \nIn JavaScript, all array accesses need to be bounds checked. Our current implementation na\u00a8ively inserts \ntwo condition\u00adals for each array access. However, as we have precise size information available from \nour type inference and also ap\u00adproximations of variable ranges, eliminating bounds checks is straight-forward. \nFinally, we emit OpenCL code. Figure 6 gives pseudo code for a compilation of the Matrix Multiply example \nfrom Figure 3 assuming a 1024 \u00d7 1024 matrix as input. As can be seen, the code is structurally very similar \nto its JavaScript counterparts. However, arrays are represented as pointers of appropriate type. Note \nin particular the use of an Integer induction variable and the absence of bounds checks in the inner \nloop. Before discussing our OpenCL runtime binding, we .rst give more details on type and range analysis. \n 3.1.1 From Untyped to Typed We use a straight-forward .rst order, monomorphic type sys\u00adtem. This allows \nus to directly map the untyped JavaScript program to OpenCL without the need for boxed data repre\u00adsentations \nand value polymorphic code. Although JavaScript p . number | bool | string a . array( ( a | p ) , n ) \n| parray( ( a | p ) , [ n [ , n ]* ] ) o . object( [ ( L , ( a | p ) ) ]* ) f . function( ( p | a | o \n)[ , ( p | a | o ) ]* ) Figure 7: Type language used for type inference. L is a set of labels and n represents \nnatural numbers. code in general has been found to be very dynamic [27], this seems not apply in the \nrestricted setting of elemental functions. In our experience, elemental functions focus on computationally \ndense numerical operations, which do not require polymorphism. Furthermore, our prototype only has limited \nsupport for method invocations, which is a signif\u00adicant source of polymorphism found in general JavaScript \ncode. Overall, these restrictions had a surpising little impact on the actual JavaScript workloads we \ncan compile. We also include size information in our types, thereby effectively performing a form of \nshape analysis at the same time. Having precise shape information available allows us to compute the \nsize of all heap objects and thereby enables static allocation. We omit a formal discussion of the type \nsystem, as it closely follows text book approaches [24]. To nonetheless give the reader a .avor, we present \nthe type language in Fig\u00adure 7. p denotes the set of types for primitives as supported by JavaScript. \nNote that, although our type language sup\u00adports the string primitive, we do not have implemented support \nfor it in the backend. We also do not support the un\u00adde.ned or null value, thereby ensuring that each \nexpression produces an actual value. We represent arrays by two .avors of array types a. The .rst, array, \nis used for ordinary JavaScript arrays as well as typed arrays. It has two parameters: the type of the \nelements of the array and the length of the array. As this shows, we require arrays to be homogeneous, \ni.e., all their elements need to be of the same type, and their length needs to be statically known. \nWe allow ordinary JavaScript arrays to be nested, i.e., the elements of such an array may be other arrays. \nOur second array type, parray, covers ParallelArray ob\u00adjects. It differs from the ordinary array type \nin that the length parameter is a vector of numbers to be able to model multi\u00addimensional ParallelArray \nobjects. The main reason to use two separate types lies in their different APIs. To correctly resolve \nmethod calls, we need to know what kind of array we are dispatching for. Next, production rule o of our \ntype language introduces object types. For simplicity, we do not support objects be\u00adyond the two array \ntypes mentioned above. However, we in\u00adclude a limited form of nameless structural objects to sup\u00adport \na JavaScript pattern commonly used to return multiple values:  1 function foo() { 2 return {desc: \"answer\", \nvalue: 42}; 3 } 4 var answer = foo().value; To be able to return two results from function foo, we construct \na nameless object in Line 2 that we use as a set of name-value pairs. The calling context can then extract \nthe different result values using their name, as shown in Line 4. Lastly, we have a function type. In \nour system, all func\u00ad tions have to return a value, represented by the .rst parame\u00ad ter, and may accept \nan arbitrary number of arguments. Note here that we, unlike JavaScript, require the number of func\u00ad tion \nparameters and arguments in a call to match. This lim\u00ad itation, which has little impact on expressiveness, \nensures that no unde.ned values creep into our supported subset of JavaScript. For function types, monomorphism \nmakes a difference. Whereas JavaScript developers write monomorphic code when it comes to values, they \noften write rather polymor\u00ad phic functions. Having two array types further increases the amount of polymorphism: \nProgrammers typically write helper functions on arrays only once. We solve this issue by aggressive specialization \nfor different primitive types, dif\u00ad ferent array kinds (parallel vs. ordinary) and even different array \nsizes. Whilst this potentially could lead to dramatic code growth, our experience shows that the amount \nof spe\u00ad cialization remains manageable. In the codes we studied, typically only few different array sizes \nare used. Specializ\u00ad ing for the two kinds of arrays can maximally double code size but leads to less \ncode duplication in practice.  3.1.2 Lightweight Range Analysis We have implemented an abstract interpretation \nphase that computes the lower and upper bounds for each expression and whether it is guaranteed to evaluate \nto an integer value at runtime. To support generic n-dimensional codes, our analysis computes vector \nbounds for vector typed variables. Certain expressions, like the index parameter of an elemental function \nor integer constants, are known to be integer values and seed the analysis. Bounds are propagated across \na set of arithmetic operations and derived from conditionals in the usual way. Predicate expressions \nin loops, however, are treated special. To compute range information for loops, typically a trip count \nis required to project the range of variables after the loop exits. However, given our time constraints \nboth in com\u00ad piler runtime and development, we have opted not to im\u00ad plement loop analysis. Instead we \nuse abstract interpretation directly. Consider the following loop: 1 var i = 0; var s = 0; 2 while (i \n<= 10) { 3 s = s + A[i++]; 4 } When evaluating the above code, we treat the loop pred\u00adicate i <= 10 \nnot as a constraint like we do with ordinary conditionals. Instead, we interpret it as upper bound for \nthe variable i. Thus, in the loop body, i has lower bound 0 de\u00adrived from the assignment in Line 1 and \nupper bound 10 derived from the loop predicate. For the above example, con\u00adtinuing the analysis with \nthis information gives us the correct bounds (i is assigned [0, 10] and s is assigned the unknown bounds). \nHowever, in general, the loop predicate might not be good enough to correctly predict all ranges if the \nloop has further induction variables that are only indirectly con\u00adstrained. Below is an example: 1 var \ni = 0; var j = 0; var s = 0; 2 while (i <= 10) { 3 s = s + A[i++] * B[j++]; 4 } After the .rst round \nof analysis, i is annotated with the correct range as before, whereas j has range [0, 1], which is incorrect. \nTo detect these cases, we rerun the analysis once more, taking the previous results into account. After \nthe second run, i still has the range [0, 10], as its upper bound is constrained by the loop predicate. \nThe range for j however has changed to [0, 2], as the variable is not constrained. If we detect such \na situation, we invalidate the range information of the corresponding variables and propagate an unde.ned \nrange instead. As our analysis is monotonic, two iterations are suf.cient to detect variables with unconstrained \nranges. If a bound in\u00ad creased in the second iteration compared to the .rst, it will further increase \nin the third iteration. Likewise, if a vari\u00ad able s range did not change in the second iteration, it \nwill not change in the third either. Furthermore, our inferred ranges are accurate, i.e., if we are able \nto compute a variable s range, the computed range is always a superset of the ob\u00ad served range at runtime. \nAlthough our analysis only covers a subset of the loops that a full loop analysis would handle, we have \nfound it to be suf.cient for the workloads we have considered.  3.2 Integrating the OpenCL runtime \nwith Firefox All components of the prototype that we have described so far are written completely in \nJavaScript and run fully in\u00adside the browser engine. However, to actually execute the OpenCL kernel code \nthat our compiler has generated, we have to leave the browser and employ the help of an OpenCL runtime. \nFor this task we have implemented a lightweight embedding of the OpenCL runtime into Firefox s JavaScript \nengine. Other than the rest of the prototype, this compo\u00adnent is written in C++. We use Firefox s XPCOM \nbinary extension mechanism to load our extension into release ver\u00adsions of Firefox and the JSAPI interface \nto SpiderMonkey, Mozilla s JavaScript engine, to communicate with the Java-Script world. Our implementation \ndoes not aim to provide a full interface to OpenCL. Instead, we have tailored the API to the needs of \nour River Trail implementation.  3.2.1 Runtime Optimizations Our runtime implements a range of common \noptimizations to reduce JIT and OpenCL runtime overheads. To increase the portability of our runtime, \nwe have aimed to implement as much of the required logic in JavaScript and keep the C++ component as \nsmall as possible. To reduce the cost of just-in-time compilation, we cache compiled kernels as long \nas the corresponding JavaScript function object is alive. We realize this by exposing com\u00adpiled kernels \nas JavaScript objects in our C++ interface and attaching those kernel objects to the respective JavaScript \nfunction object. By making compiled kernels explicit in the runtime, we can reuse the existing garbage \ncollection mecha\u00adnisms of the JavaScript engine. During .nalization of a Java-Script kernel object, we \nalso free the corresponding OpenCL data structure. We use a similar approach to manage data transfers \nand OpenCL heap mapping. Before data can be used by an OpenCL kernel, it has to be explicitly mapped. \nEven though hardware trends towards shared memory systems, mapping data still imposes noticeable overhead. \nTherefore, a common runtime optimization is to reduce transfers where possible. We, too, have implemented \na form of lazy materialization. As before, the key technique is to expose mapped memory as explicit JavaScript \nobjects. This allows us to cache mapped data by attaching the object that represents the mapping to the \nJavaScript object that has been mapped. Again, lifetime management is performed by the existing JavaScript \ngarbage collection. Note that we only cache ParallelArray objects, as these are immutable and therefore \nguaranteed to remain con\u00adsistent with their mapped copy during their lifetime. Extend\u00ading our approach \nto mutable objects would require some form of consistency protocol that forwards changes to an object \nfrom the JavaScript heap to the OpenCL heap s coun\u00adterpart. Our lazy materialization goes one step further: \nFor kernel results, we do not map the result back to the JavaScript heap unless the value is actually \nread. We implement this in Java-Script by replacing the data store of a ParallelArray object with the \nobject that represents the corresponding OpenCL memory buffer. To ensure that the data is mapped on read, \nwe dynamically override all reading methods with special implementations that .rst map the data back \nto the Java-Script heap. Our implementation exploits the dynamic na\u00adture of JavaScript, in particular \nthe ability to add new meth\u00adods to an object at runtime, thereby overriding methods in\u00adherited from the \nobject s prototype. To keep overheads low, we reset those methods to their default implementation as \nsoon as the data has been mapped. Reusing JavaScript s existing garbage collection mech\u00adanisms to manage \nthe lifetime of OpenCL runtime objects has proven to be an ef.cient implementation strategy. How\u00adever, \na too na\u00a8ive implementation may show unexpected run\u00adtime costs caused by the interplay of garbage collection \nand OpenCL runtimes. The OpenCL runtime, unlike JavaScript, uses explicit memory management with reference \ncounting. Under that regime, memory is reclaimed as soon as the last reference to it is surrendered. \nConsequently, an OpenCL runtime is typ\u00adically tuned for single free operations that are intertwined with \noverall program execution. Garbage collected runtimes, on the other hand, free many objects at once whenever \na col\u00adlection cycle is completed. In our prototype, this includes the OpenCL runtime objects and their \nJavaScript represen\u00adtations. The resulting bursts of free requests result in signi.\u00adcant overheads of \nup to 200ms. In real time centric use cases like video processing or 3D animations, such delays are very \nnoticeable. To alleviate the effect, we have decoupled freeing the wrapper objects from releasing the \nOpenCL runtime objects. Instead, we now maintain a global free queue that contains OpenCL runtime objects \nthat are no longer needed. During garbage collection, instead of calling the OpenCL API func\u00adtion to \nrelease an object, we simply add it to the free queue. During certain points of execution, in particular \nall situa\u00adtions where new OpenCL runtime objects are allocated, we inspect the queue and release a small \namount of pending ob\u00adjects. Using this approach, we were able to reduce garbage collection latency to \n20ms, without otherwise noticeable ef\u00adfect on overall performance. Last but not least, we have implemented \nmemory align\u00adment: In particular for vectorization on CPUs, data typically has to be aligned in memory. \nOpenCL runtimes report this alignment requirement at runtime. This allows implemen\u00adtations with explicit \nmemory management to allocate heap structures accordingly, typically by allocation a larger chunk of \nmemory and computing an offset to gain an aligned start address. However, our shallow embedding does \nnot allow us to directly in.uence the allocator of the used JavaScript run\u00adtime. Instead, we exploit \na feature of the typed array speci.ca\u00adtion: Typed arrays in JavaScript are de.ned as views over a byte \nbuffer, which can be allocated independently. In partic\u00adular, the view may begin at an offset in the \nunderlying buffer. The last missing ingredient is a way to compute this offset. This cannot be done in \nJavaScript, as JavaScript has no no\u00adtion of pointers and does not expose the address of objects to the \nprogram. We have added this missing pointer re.ection mechanism in our C++ component.  3.2.2 Hybrid \nExecution Using OpenCL as backend technology allows us to trans\u00adparently run River Trail workloads on \nthe CPU and GPU. Even more, we can also execute workloads on both devices at the same time. To evaluate \nthe bene.ts of such a hybrid approach, we have implemented a simple static scheduler. The basic idea \nis to map all read-only inputs, i.e., all kernel parameters, to both the CPU and GPU devices. The work\u00adload \ndistribution is determined by an of.oad factor speci.ed at runtime that determines the portion of the \niteration space that should be computed on each device. After this partition\u00ading, computation proceeds \nas usual with each of the CPU and GPU devices producing results into their own regions of the result \nbuffers. After completion of execution on all de\u00advices we map all sub buffers back to the host and return \nthe combined result.  4. Experimental Evaluation We present an experimental evaluation of the River \nTrail prototype compiler using several realistic web applications. Speci.cally, we focus on the following \naspects: 1. Performance and scaling on a modern CPU relative to sequential JavaScript. 2. Performance \nimpact of optimizations, speci.cally dy\u00adnamic bounds check elimination and lazy materializa\u00adtion. 3. \nPerformance on a modern integrated graphics unit relative to sequential JavaScript with and without hybrid \nexecution.  All experiments were run using Firefox 16 on a machine with an Intel Core i7-3770 CPU with \n4 cores (8 hardware threads) clocked at 3.4GHz and with 4GB of memory. This machine also contains an \nIntel HD Graphics 4000 integrated GPU with 16 execution units (each running 8 threads, for a total of \n128 threads) clocked at 650MHz and with 1.7GB of shared memory. 4.1 Workload Selection River Trail is \na disruptive technology and as such no suitable workloads exist. Today s benchmarks like Sun Spider or \nthe V8 benchmark suite [30] focus on sequential performance and are not representative of real web sites \n[26]. Even more, current web applications are designed to run within the com\u00adputational envelope provided \nby today s JavaScript engines and thus are not very computationally intense. To escape this chicken-and-egg \nproblem, we have imple\u00admented and sourced a range of workloads, summarized in Table 2. Of these, 5 were \ndeveloped by third parties who .rst developed sequential versions and then ported them to the River Trail \nAPI. These third parties included web designers, professional developers, academic researchers as well \nas un\u00addergraduate students. We believe that the set of applications we have chosen is representative \nof emerging HTML5 ap\u00adplications and allows us a fair evaluation of River Trail s po\u00adtential. Below, we \nbrie.y describe each of these workloads and relate them to the performance speedups and scalability shown \nin Figure 8.  4.2 Parallel Speedup The chart in Figure 8 shows the impact of the number of hardware \nthreads on parallel speedups observed. The Y-axis is the ratio of time taken for sequential execution \nand time taken for River Trail execution. Note that the processor uses hyper threading. Thus, bars labeled \n2, 4, 6 and 8 hardware threads correspond to runs on 1, 2, 3 and 4 cores, respec\u00adtively, with two hardware \nthreads each. The bar labeled as 1 hardware thread, however, runs an exclusive hardware thread on a single \ncore. We attribute the reduction in speedup for 8 of 11 programs on two hardware threads compared to \nthe ex\u00adecution on a single hardware thread to hyper threading and runtime effects. We rely on the OpenCL \nruntime to spawn the right number of user threads and decide an ef.cient work assignment. We therefore \ndo not explicitly control nor know the precise concurrency during execution. Aside from scal\u00adability \nthis chart also implicitly shows the contributions of parallelism and code generation to parallel speedup. \nThe second chart in Figure 8 shows the impact of the op\u00adtimizations described in Section 3. All bars \nshow the rel\u00adative speedup compared to a fully optimized version with all optimizations described in \nSection 3 turned on -in the discussion below, we refer to this optimized version as All-Opts. The bars \nlabeled NoLazyMaterialization show rela\u00adtive speedups when lazy materialization is turned off but all \nthe other optimizations are on. The bars labeled NoDy\u00adnamicBoundsChecks show relative speedups when no \ndy\u00adnamic bounds checks are emitted. These bars therefore il\u00adlustrate the performance speedup that can \nbe achieved with an ideal Range Analysis. Note that this arti.cial scenario is used only for evaluating \neffectiveness of our RangeAnalysis and is not an optimization itself. The bars labeled FullDy\u00adnamicBoundsChecks \nshow relative speedups with all array selection operations guarded with checks. These bars there\u00adfore \nshow speedup without any static Range Analysis. TourDeBlock is a game application that features a rigid \nbody physics engine to simulate realistic collisions and movement. The collision detection phase of the \nprogram accounts for 50-60% of total execution time per frame. This phase consists of two sub-phases \nBroadphase and Narrow\u00adphase each of which are made parallel using River Trail. We observe a parallel \nspeedup of around 2.84 (Figure 8a) for the collision detection phase alone and 1.6 for the en\u00adtire application. \nThe elemental functions for this workload contain dense irregular control .ow and as a result they do \nnot bene.t from auto-vectorization. This program also does not bene.t from lazy materialization as the \ntwo parallel sub\u00adphases are separated by a sequential computation that reads the results of the .rst \nsub-phase. XML3D simulates an interactive walk-through of a virtual museum in which the user views speci.c \nart installations. The scene consists of several human characters that are made of high-detail meshes \nand a bone and joint model for realistic movement. These meshes are skinned and animated in real\u00adtime, \nwhich is the most compute-intensive portion of this application and takes up on average roughly 50-70% \nof execution time per frame during sequential execution. With  Name Bugs threads in both cases) these \ncomponents parallelized using River Trail we measure a speedup of 7.21 (Figure 8a) for the parallelized \nportion and a speedup of roughly 2 for the whole application. Video-* workloads are part of an in-browser \nvideo editor application in which users apply various effects to a video stream. Video-Sepia, Video-EdgeDetect \nand Video-Sharpen refer to the user applying sepia toning, edge detection and image sharpening, respectively. \nVideo-3D refers to the input stream being transformed to stereoscopic 3D in real-time. We observe speedups \nranging from 0.87 (for Video-Sepia) to 5.25 (for Video-EdgeDetect and Video-Sharpen). Lazy materialization \ndoes not provide any improvement since the results of the elemental functions are rendered to the screen \nimmediately after parallel execution. However, the static range analysis is effective in eliminating \nbounds checks as seen by the slowdown with FullDynamicBoundsChecks in Figure 8b. Figure 8: (a) Parallel \nspeedup relative to plain sequential JavaScript versions and (b) the impact of optimizations on performance \n-the bars show speedup without speci.c optimizations relative to execution with all optimizations turned \non (using 8 hardware The slowdown for Video-Sepia in Figure 8a (in contrast with the signi.cant speedups \nfor the other Video-* pro\u00adgrams) is a consequence of two factors. Firstly, the amount of work parallelized \nin Video-Sepia is relatively low, which emphasizes overheads. Secondly, sepia can be computed in place \nby the sequential implementation by directly updat\u00ading pixel values of the original image buffer. River \nTrail s programming model does not allow for this. The other pro\u00adgrams, however, cannot be performed \nin place due to loop\u00adcarried dependencies. Matrix-Multiply workload implements the standard O(n3) algorithm \nfor dense matrix multiplication which is ex\u00adcerpted in Figure 1. We observe a speedup of 33.6 for this \nprogram (Figure 8a). Effective range analysis contributes signi.cantly to this speedup with FullDynamicBound\u00adsChecks \nthe speedup is about 8.8x over plain sequential JavaScript (i.e., a speedup of 0.26x over the optimized \ncase which was 33x faster than sequential JavaScript). These dy\u00adnamic bounds checks are also an impediment \nto vectoriza\u00ad  tion which the All-Opts case bene.ts signi.cantly from. For comparison we wrote a native \nversion of this workload us\u00ading C and OpenCL running on the CPU that uses the same O(n3) algorithm. We \nobserved that the River Trail program runs 0.97x as fast as this native OpenCL implementation on 1024 \nx 1024 matrices. For this program, our compiler is therefore able to generate code whose performance \nis on par with native code. Nbody implements an O(n2) simulation and animation of particles that are \nmoving through a force .eld and interact\u00ading with each other gravitationally. Roughly 98% of the ex\u00adecution \ntime in each frame is spent computing positions and velocities of the particles and these computations \ncan be par\u00adallelized effectively resulting in a parallel speedup of 21.5 for these computations and a \nroughly equivalent improve\u00adment in overall frame rate (Figure 8a). The impact of our range analysis is \nparticularly evident in this program in the FullDynamicBoundsChecks case the parallel speedup is only \n1.12 (Figure 8b). On the other hand speedup for the No-DynamicBoundsChecks is roughly equal to the speedup \nfor All-Opts, i.e., our analysis is able to eliminate most of the performance critical bounds checks. \nLiquid-Resize implements content-aware image resizing based on the algorithm in [3]. The portion of this \nprogram parallelized using River Trail accounts for roughly 85% of the total sequential execution time. \nThe parallel speedup in just the parallelized portion of the program is approximately 10.9 (Figure 8a) \nand the overall application speedup is 2.2 over sequential JavaScript execution. The speedups are not \nsigni.cantly affected by lazy materialization. Bugs is an interactive game in which players use gestures \nto interact with and move bugs in a scene. This application implements dense optical .ow tracking using \nFarneback s method [12]. The parallel version of this method is imple\u00admented with a chain of 10 relatively \nshort, vectorizable ele\u00admental functions that are invoked for each input frame. We observe parallel a \nspeedup of about 11.8 for the All-Opts case. The chain of elemental functions produce and consume large \nParallelArray objects without intervening accesses in sequential code. Therefore in this case lazy materialization \nhas a substantial impact on parallel performance without this optimization the speedup is roughly 4.5. \nBounds check elimination also has a big impact on speedup parallel speedup is improved by 2.6x (11.8x \nvs. 4.5x) compared to FullDynamicBoundsChecks. OctreeCollider is a physics engine that simulates collision \ndetection and dynamics on rigid meshes. Unlike TourDe-Block which uses a brute force O(n2) broadphase \nfor detect\u00ading collisions, this application uses an octree-based scheme to ef.ciently prune out non-colliding \nobjects. Parallel exe\u00adcution for this program is 4.3 times faster than sequential JavaScript and lazy \nmaterialization does not affect parallel performance signi.cantly. However performance with No-DynamicBoundsChecks \nis signi.cantly better than All-Opts. The sole elemental function contains a while loop whose termination \ncondition is data-dependent. Moreover most ar\u00adray selection operations within this loop use indices that \nare also data-dependent. Range analysis is thus unable to resolve bounds precisely. It is important to \nnote here, that, beyond just speeding up applications, River Trail also acts as an enabler for new kinds \nof applications. For instance, the NBody program runs at 2 frames/second in sequential JavaScript. With \nRiver Trail it runs at around 43 frames per second suf.ciently fast for a near real-time experience. \nSimilarly the Bugs game runs at less than 1 frame per second in sequential JavaScript. With River Trail, \nit runs at about 12 frames per second suf.cient performance for interactive play. The same is true for \nVideo-EdgeDetect and Video-Sharpen: Each runs at around 4 frames/second sequentially whereas the parallel \nversions run at around 21 frames/second, which is closer to the original playback rate and is discernibly \nsmoother. Figure 10 shows the total cost in milliseconds of JIT\u00adcompiling all the elemental functions \ninvoked for each work\u00adload during parallel execution. The elemental functions in Tour De Block and the \nOctreeCollider are both quite large and consist of deeply nested loops. This is re.ected in the relatively \nlarge amount of time spent in Type Inference and the time taken for Range Analysis to converge. However \nthe cost of JIT compilation is low relative to the application ex\u00adecution time for most of the studied \nprograms. Applications that are particularly sensitive to the pauses due to JIT com\u00adpilation (for example \nTourDeBlock) are able to pre-compile the elemental functions during page or application loading. We observed \nthat re-compilation (or re-specialization) is rarely triggered (thereby avoiding jitter in an otherwise \nsmooth animation or video rendering for example).  4.3 Hybrid Execution The River Trail runtime supports \nexecution of kernels on GPU devices transparently to the programmer and the com\u00adpiler. The performance \nspeedup due to hybrid execution for NBody and Matrix-Multiply-1D from Table 1 are shown in Figure 9 for \nvarious input sizes. The iteration space for a ParallelArray operation is partitioned between the CPU \nand GPU statically: CPUx-GPUy means that x% of the itera\u00adtion space is computed on the CPU and y% on \nthe GPU. CPU-only and GPU-only refer to computing the full itera\u00adtion space on CPU only and GPU only, \nrespectively. The parallel performance of NBody for 4000 bodies in\u00adcreases as the portion of the iteration \nspace of.oaded to the GPU in increased up to 75%. With GPU-only execution the speedup is lower than with \nCPU25-GPU75. This can be at\u00adtributed to the signi.cant amount of rendering that this pro\u00adgram performs \non the GPU interleaved with computation. When the problem size is scaled up, the amount of rendering \n Figure 9: Parallel speedup for (a) NBody (on the left) and (b) Matrix-Multiply (on the right) with \nhybrid execution relative to sequential JavaScript execution. The Matrix-Multiply-1D program shown here \nuses a .attened 1D representation of the matrices and is distinct from the 2D version in Table 2. performed \non the GPU also increases. The best performance then is achieved instead with a 50-50% partitioning. \nWe also observe that the performance scales better with prob\u00adlem size on the GPU-only and hybrid con.gurations \nthan on the CPU-only con.guration. The plot on the right-hand side of Figure 9 shows the per\u00adformance \nof hybrid execution for Matrix-Multiply. This pro\u00adgram both performs better and scales better than the \nNBody workload. We attribute this to two main reasons: Unlike NBody, this program does not perform any \nrendering on the GPU and the elemental function contains no signi.cant control .ow divergence. The latter \nalso makes it particu\u00adlarly suitable for GPU execution: GPU-only con.guration produces the best speedups \nand performance decreases with increased CPU utilization. These experiments indicate the large number \nof factors that in.uence the best partitioning for hybrid execution. In addition, there is the overhead \nof spawning elemental func\u00adtions on a GPU device. To be effective, a hybrid execution or partitioning \nscheme has to balance these considerations care\u00adfully to produce good results across a variety of workloads. \nThere has been a signi.cant amount of exploration in this area by other researchers (see [25] and references \ntherein). 5. Related Work Data-parallel programming is a well understood concept. It has been extensively \nstudied [5] and approaches have been developed for a wide range of languages. However, to our knowledge \nnobody so far has addressed JavaScript. Most existing approaches (for example ArBB (C++) [22], DpH (Haskell) \n[8], Lime (JAVA) [11], Firepile (Scala) [23], Accelerator (.NET) [28], to name few) cover statically \ntyped languages that are compiled ahead-of-time. ASDP [25] comes close by extending ActionScript, which \nis similar to JavaScript but has type annotations. In their DSL, they map Figure 10: Breakdown of JIT \ncompilation times for the workloads in Table 1. a subset of those types to OpenCL without the need of \ntype inference. However, they do not support the dynamic Num\u00adber type. Also, they use ahead-of-time compilation. \nIkra [18] implements ahead-of-time compilation of a subset of Ruby to GPUs. Similar to our approach, \nthe authors propose a new data type PArray and they also use code analysis to decide whether an operation \nmay run in parallel. However, their system only supports map and inject (reduce) operations and, while \nshowing promising .rst results, is still in an early stage of development. CopperHead [7] supports ahead-of\u00adtime \nand just-in-time compilation of a subset of Python to CUDA. Their work solves many of the challenges \naddressed by our solution. However, their compiler is fundamentally a static compiler. In particular, \nthey do not specialize gen\u00aderated code to the extent a dynamic compiler like the one presented here can. \n WebCL [1] proposes an embedding of OpenCL into Java-Script at the language rather than implementation \nlevel. Un\u00adlike River Trail, it is targeted at limited usage scenarios that allow for a violation of JavaScript \nsecurity model, such as, for example, natively installed HTML5 applications. Further related work can \nbe found in the context of pro\u00adgramming models for deterministic parallelism. In the con\u00adtext of statically \ntyped languages, approaches like DPJ [6] (Java) and CSOLVE [17] (C) have been proposed. Their concurrency \nmodel is more general than ours. However, they need program annotations to prove determinism. [19] presents \na concurrency model similar to ours that also al\u00adlows sibling threads read-only access to the parent \ns state. However, it is tailored towards task parallelism and uses ex\u00adplicit operations to encode sibling/sibling \ndependencies. The latter cannot arise in our model. Finally, we have discussed this and related work \nin earlier publications. [13] is a position paper that argues for the need of a parallel programming \nmodel for JavaScript. In [29], we have presented materials that teach concurrency with the help of River \nTrail. The design process, details of the imple\u00admentation and results of our in depth performance evaluation \nhave not been published before. 6. Conclusion We have reported on River Trail, a joint expedition of \nlan\u00adguage designers and web developers towards a parallel pro\u00adgramming API for JavaScript. During the \ndesign, we have co-evolved the API and its implementation with new work\u00adloads and usage scenarios. The \nresulting API is tailored for the needs of the web: it is safe and secure, builds on existing developer \nknowledge and offers performance portability. We have described our prototype implementation and outlined \nthe key techniques used to bring River Trail to multi-core CPUs and GPUs. Our evaluation proves that \nsigni.cant per\u00adformance improvements up to an order of magnitude can be achieved for realistic web applications. \nRapid prototyping of the compiler and runtime during the different design iterations has proven very \nbene.cial, especially to collect early developer feedback. Yet, design should ultimately lead to a .nished \nproduct. To that effect, we have proposed River Trail to the ECMAScript standards committee to further \nevolve the API [15]. In parallel, we are working jointly with Mozilla on a production quality implementation. \nReferences [1] T. Aarnio and M. Bourges-S `evenier. WebCL working draft. https://cvs.khronos.org/svn/repos/registry/ \ntrunk/public/webcl/spec/latest/index.html, Octo\u00adber 2012. [2] M. Anttonen, A. Salminen, T. Mikkonen, \nand A. Taivalsaari. Transforming the web into a real application platform: new technologies, emerging \ntrends and missing pieces. In Pro\u00adceedings of the 2011 ACM Symposium on Applied Computing, SAC 11, pages \n800 807, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0113-8. [3] S. Avidan and A. Shamir. Seam carving \nfor content-aware image resizing. ACM Trans. Graph., 26, July 2007. ISSN 0730-0301. [4] R. Berjon, T. \nLeithead, E. Doyle Navara, E. O Connor, S. Pfeiffer, and I. Hickson. HTML5. http://dev.w3.org/ html5/spec/, \nNovember 2012. [5] G. E. Blelloch, J. C. Hardwick, J. Sipelstein, M. Zagha, and S. Chatterjee. Implementation \nof a portable nested data\u00adparallel language. J. Parallel Distrib. Comput., 21(1):4 14, Apr. 1994. ISSN \n0743-7315. [6] R. L. Bocchino, Jr., V. S. Adve, D. Dig, S. V. Adve, S. Heumann, R. Komuravelli, J. Overbey, \nP. Simmons, H. Sung, and M. Vakilian. A type and effect system for de\u00adterministic parallel Java. In Proceedings \nof the 24th ACM SIGPLAN conference on Object oriented programming sys\u00adtems languages and applications, \nOOPSLA 09, pages 97 116, New York, NY, USA, 2009. ACM. ISBN 978-1-60558\u00ad766-0. [7] B. Catanzaro, M. Garland, \nand K. Keutzer. Copperhead: com\u00adpiling an embedded data parallel language. In Proceedings of the 16th \nACM symposium on Principles and practice of par\u00adallel programming, PPoPP 11, pages 47 56, New York, NY, \nUSA, 2011. ACM. ISBN 978-1-4503-0119-0. [8] M. M. T. Chakravarty, R. Leshchinskiy, S. Peyton Jones, G. \nKeller, and S. Marlow. Data parallel haskell: a status re\u00adport. In Proceedings of the 2007 workshop on \nDeclarative aspects of multicore programming, DAMP 07, pages 10 18, New York, NY, USA, 2007. ACM. ISBN \n978-1-59593-690-5. [9] J. Dean and S. Ghemawat. Mapreduce: simpli.ed data pro\u00adcessing on large clusters. \nCommun. ACM, 51(1):107 113, Jan. 2008. ISSN 0001-0782. [10] V. Djeric and A. Goel. Securing script-based \nextensibility in web browsers. In Proceedings of the 19th USENIX conference on Security, USENIX Security \n10, pages 23 23, Berkeley, CA, USA, 2010. USENIX Association. ISBN 888-7-6666\u00ad5555-4. [11] C. Dubach, \nP. Cheng, R. Rabbah, D. F. Bacon, and S. J. Fink. Compiling a high-level language for gpus: (via lan\u00adguage \nsupport for architectures and compilers). In Proceed\u00adings of the 33rd ACM SIGPLAN conference on Programming \nLanguage Design and Implementation, PLDI 12, pages 1 12, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1205-9. \n[12] G. Farneb\u00a8ack. Two-frame motion estimation based on polyno\u00admial expansion. In Proceedings of the \n13th Scandinavian con\u00adference on Image analysis, SCIA 03, pages 363 370, Berlin, Heidelberg, 2003. Springer-Verlag. \nISBN 3-540-40601-8. [13] S. Herhut, R. L. Hudson, T. Shpeisman, and J. Sreeram. Par\u00adallel programming \nfor the web. In Proceedings of the 4th USENIX conference on Hot Topics in Parallelism, HotPar 12, pages \n1 1, Berkeley, CA, USA, 2012. USENIX Association. [14] S. Herhut, R. L. Hudson, T. Shpeisman, and J. \nSreeram. ParallelArray API speci.cation. https://github.com/  RiverTrail/RiverTrail/wiki/ParallelArray, \nOcto\u00adber 2012. [15] R. L. Hudson and S. Herhut. Parallel EcmaScript (River Trail) API. http://wiki.ecmascript.org/doku.php? \nid=strawman:data_parallelism, October 2012. [16] K. E. Iverson. A programming language. John Wiley &#38; \nSons, Inc., New York, NY, USA, 1962. ISBN 0-471430-14-5. [17] M. Kawaguchi, P. Rondon, A. Bakst, and \nR. Jhala. Deter\u00administic parallelism via liquid effects. SIGPLAN Not., 47(6): 45 54, June 2012. ISSN \n0362-1340. [18] H. Masuhara and Y. Nishiguchi. A data-parallel extension to ruby for gpgpu: toward a \nframework for implementing domain-speci.c optimizations. In Proceedings of the 9th ECOOP Workshop on \nRe.ection, AOP, and Meta-Data for Software Evolution, RAM-SE 12, pages 3 6, New York, NY, USA, 2012. \nACM. ISBN 978-1-4503-1277-6. [19] N. D. Matsakis. Parallel closures: a new twist on an old idea. In Proceedings \nof the 4th USENIX conference on Hot Topics in Parallelism, HotPar 12, Berkeley, CA, USA, 2012. USENIX \nAssociation. [20] Mozilla. Firefox OS. http://www.mozilla.org/en-US/ firefoxos/, November 2012. [21] \nA. Munshi. OpenCL speci.cation 1.1. http://www. khronos.org/registry/cl/specs/opencl-1.1.pdf, June 2011. \n[22] C. J. Newburn, B. So, Z. Liu, M. McCool, A. Ghuloum, S. D. Toit, Z. G. Wang, Z. H. Du, Y. Chen, \nG. Wu, P. Guo, Z. Liu, and D. Zhang. Intel s array building blocks: A retargetable, dynamic compiler \nand embedded language. In Proceedings of the 9th Annual IEEE/ACM International Symposium on Code Generation \nand Optimization, CGO 11, pages 224 235, Washington, DC, USA, 2011. IEEE Computer Society. ISBN 978-1-61284-356-8. \n[23] N. Nystrom, D. White, and K. Das. Firepile: run-time compi\u00adlation for gpus in scala. In Proceedings \nof the 10th ACM inter\u00adnational conference on Generative programming and compo\u00adnent engineering, GPCE \n11, pages 107 116, New York, NY, USA, 2011. ACM. ISBN 978-1-4503-0689-8. [24] B. C. Pierce. Types and \nprogramming languages. MIT Press, Cambridge, MA, USA, 2002. ISBN 0-262-16209-1. [25] A. Pignotti, A. \nWelc, and B. Mathiske. Adaptive data paral\u00adlelism for internet clients on heterogeneous platforms. In \nPro\u00adceedings of the 8th symposium on Dynamic languages, DLS 12, pages 53 62, New York, NY, USA, 2012. \nACM. ISBN 978-1-4503-1564-7. [26] P. Ratanaworabhan, B. Livshits, and B. G. Zorn. Jsmeter: comparing \nthe behavior of JavaScript benchmarks with real web applications. In Proceedings of the 2010 USENIX confer\u00adence \non Web application development, WebApps 10, Berke\u00adley, CA, USA, 2010. USENIX Association. [27] G. Richards, \nS. Lebresne, B. Burg, and J. Vitek. An analysis of the dynamic behavior of JavaScript programs. In Proceed\u00adings \nof the 2010 ACM SIGPLAN conference on Programming language design and implementation, PLDI 10, pages \n1 12, New York, NY, USA, 2010. ACM. ISBN 978-1-4503-0019-3. [28] S. Singh. Declarative data-parallel \nprogramming with the accelerator system. In Proceedings of the 5th ACM SIGPLAN workshop on Declarative \naspects of multicore programming, DAMP 10, New York, NY, USA, 2010. ACM. ISBN 978-1\u00ad60558-859-9. [29] \nJ. Sreeram, S. Herhut, R. L. Hudson, and T. Shpeisman. Teaching parallelism with river trail. In Proceedings \nof the 2012 workshop on Developing competency in parallelism: techniques for education and training, \nDCP 12, pages 1 8, New York, NY, USA, 2012. ACM. ISBN 978-1-4503-1840-2. [30] D. Tiwari and Y. Solihin. \nArchitectural characterization and similarity analysis of sunspider and Google s v8 Java-Script benchmarks. \nIn Performance Analysis of Systems and Software (ISPASS), 2012 IEEE International Symposium on, pages \n221 232, april 2012.    \n\t\t\t", "proc_id": "2509136", "abstract": "<p>JavaScript is the most popular language on the web and is a crucial component of HTML5 applications and services that run on consumer platforms ranging from desktops to phones. However, despite ample amount of hardware parallelism available to web applications on such platforms, JavaScript web applications remain predominantly sequential. Common parallel programming solutions accepted by other programming languages failed to transfer themselves to JavaScript due to differences in programming models, the additional requirements of the web and different developer expectations.</p> <p>In this paper we present River Trail - a parallel programming model and API for JavaScript that provides <i>safe, portable, programmer-friendly, deterministic</i> parallelism to JavaScript applications. River Trail allows web applications to effectively utilize multiple cores, vector instructions, and GPUs on client platforms while allowing the web developer to remain within the environment of JavaScript. We describe the implementation of the River Trail compiler and runtime and present experimental results that show the impact of River Trail on performance and scalability for a variety of realistic HTML5 applications. Our experiments show that River Trail has a dramatic positive impact on overall performance and responsiveness of computationally intense JavaScript based applications achieving up to 33.6 times speedup for kernels and up to 11.8 times speedup for realistic web applications compared to sequential JavaScript. Moreover, River Trail enables new interactive web usages that are simply <i>not even possible</i> with standard sequential JavaScript.</p>", "authors": [{"name": "Stephan Herhut", "author_profile_id": "81392604753", "affiliation": "Intel Labs, Santa Clara, CA, USA", "person_id": "P4290452", "email_address": "stephan.a.herhut@intel.com", "orcid_id": ""}, {"name": "Richard L. Hudson", "author_profile_id": "81100566849", "affiliation": "Intel Labs, Santa Clara, CA, USA", "person_id": "P4290453", "email_address": "rick.hudson@intel.com", "orcid_id": ""}, {"name": "Tatiana Shpeisman", "author_profile_id": "81100439172", "affiliation": "Intel Labs, Santa Clara, CA, USA", "person_id": "P4290454", "email_address": "tatiana.shpeisman@intel.com", "orcid_id": ""}, {"name": "Jaswanth Sreeram", "author_profile_id": "81338490999", "affiliation": "Intel Labs, Santa Clara, CA, USA", "person_id": "P4290455", "email_address": "jaswanth.sreeram@intel.com", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509516", "year": "2013", "article_id": "2509516", "conference": "OOPSLA", "title": "River trail: a path to parallelism in JavaScript", "url": "http://dl.acm.org/citation.cfm?id=2509516"}