{"article_publication_date": "10-29-2013", "fulltext": "\n Targeted and Depth-.rst Exploration for Systematic Testing of Android Apps Tanzirul Azim University \nof California, Riverside mazim002@cs.ucr.edu Abstract Systematic exploration of Android apps is an enabler \nfor a variety of app analysis and testing tasks. Performing the ex\u00adploration while apps run on actual \nphones is essential for exploring the full range of app capabilities. However, ex\u00adploring real-world \napps on real phones is challenging due to non-determinism, non-standard control .ow, scalability and \noverhead constraints. Relying on end-users to conduct the exploration might not be very effective: we \nperformed a 7-user study on popular Android apps, and found that the combined 7-user coverage was 30.08% \nof the app screens and 6.46% of the app methods. Prior approaches for au\u00adtomated exploration of Android \napps have run apps in an emulator or focused on small apps whose source code was available. To address \nthese problems, we present A3E, an approach and tool that allows substantial Android apps to be explored \nsystematically while running on actual phones, yet without requiring access to the app s source code. \nThe key insight of our approach is to use a static, taint-style, data.ow analysis on the app bytecode \nin a novel way, to construct a high-level control .ow graph that captures legal transitions among activities \n(app screens). We then use this graph to de\u00advelop an exploration strategy named Targeted Exploration \nthat permits fast, direct exploration of activities, including activities that would be dif.cult to reach \nduring normal use. We also developed a strategy named Depth-.rst Exploration that mimics user actions \nfor exploring activities and their constituents in a slower, but more systematic way. To mea\u00adsure the \neffectiveness of our techniques, we use two metrics: activity coverage (number of screens explored) and \nmethod coverage. Experiments with using our approach on 25 pop\u00adular Android apps including BBC News, \nGas Buddy, Amazon Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from Permissions@acm.org. OOPSLA 13, October 29 31, 2013, \nIndianapolis, Indiana, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509549 \nIulian Neamtiu University of California, Riverside neamtiu@cs.ucr.edu Mobile, YouTube, Shazam Encore, \nand CNN, show that our exploration techniques achieve 59.39 64.11% activity cov\u00aderage and 29.53 36.46% \nmethod coverage. Categories and Subject Descriptors D.2.4 [Software Engi\u00adneering]: Software/Program Veri.cation \nReliability, Vali\u00addation; D.2.5 [Software Engineering]: Testing and Debugging Testing tools,Tracing General \nTerms Languages, Reliability, Veri.cation Keywords Google Android, GUI testing, Systematic ex\u00adploration, \nTest case generation, Code coverage, Greybox testing, Dynamic analysis, Taint analysis 1. Introduction \nUsers are increasingly relying on smartphones for compu\u00adtational tasks [1, 2], hence concerns such as \napp correct\u00adness, performance, and security become increasingly press\u00ading [6, 8, 30, 31, 38]. Dynamic \nanalysis is an attractive ap\u00adproach for tackling such concerns via pro.ling and moni\u00adtoring, and has \nbeen used to study a wide range of proper\u00adties, from energy usage [38, 39] to pro.ling [40] and secu\u00adrity \n[31]. However, dynamic analysis critically hinges on the availability of test cases that can ensure good \ncoverage, i.e., drive program execution through a signi.cant set of repre\u00adsentative program states [36, \n37]. To facilitate test case construction and exploration for smartphone apps, several approaches have \nemerged. The Monkey tool [15] can send random event streams to an app, but this limits exploration effectiveness. \nFrameworks such as Monkeyrunner [24], Robotium [18] and Troyd [20] sup\u00adport scripting and sending events, \nbut scripting takes manual effort. Prior approaches for automated GUI exploration [9 12, 17, 34] have \none or more limitations that stand in the way of understanding how popular apps run in their natural \nenvironment, i.e., on actual phones: running apps in an em\u00adulator, targeting small apps whose source \ncode is available, incomplete model extraction, state space explosion. For illustration, consider the \ntask of automatically ex\u00adploring popular apps, such as Amazon Mobile, Gas Buddy, YouTube, Shazam Encore, \nor CNN, whose source code is not available. Our approach can carry out this task, as shown in Section \n6, since we connect to apps running naturally on the phone. However, existing approaches have multiple \ndif.cul\u00adties due to the lack of source code or running the app on the emulator where the full range of \nrequired sensor inputs (camera, GPS, microphone) or output devices (e.g., .ash\u00adlight) is either unavailable \n[32] or would have to be simu\u00adlated. To tackle these challenges, we present Automatic An\u00addroid App Explorer \n(A3E), an approach and open-source tool1 for systematically exploring real-world, popular apps Android \napps running on actual phones. Developers can use our approach to complement their existing test suites \nwith automatically-generated test cases aimed at systematic ex\u00adploration. Since A3E does not require \naccess to source code, users other than the developers can execute substantial parts of the app automatically. \nA3E supports sensors and does not require kernel-or framework-level instrumentation, so the typical overhead \nof instrumentation and device emulation can be avoided. Hence we believe that researchers and prac\u00adtitioners \ncan use A3E as a basis for dynamic analyses [36] (e.g., monitoring, pro.ling, information .ow tracking), \ntest\u00ading, debugging, etc. In this paper, our approach is focused on improving cov\u00aderage at two granularity \nlevels: activity (high-level) and method (low-level). Activities are the main parts of Android apps an \nactivity roughly corresponds to a different screen or window in traditional GUI-based applications. Increasing \nactivity coverage means, roughly, exploring more screens. For method coverage we focus on covering app \nmethods, as available in the Dalvik bytecode (compiled from Java), that runs on the Dalvik VM on an actual \nphone; an activity s implementation usually consists of many methods, so by improving method coverage \nwe allow the functionality as\u00adsociated with each activity to be systematically explored and tested. In \nSection 2 we provide an overview of the Android platform and apps, we de.ne the graphs that help drive \nour approach, and provide de.nitions for our coverage metrics. To understand the level of exploration \nattained by An\u00addroid app users in practice, we performed a user study and measured coverage during regular \ninteraction. For the study, we enrolled 7 users that exercised 28 popular Android apps. We found that \nacross all apps and participants, on average, just 30.08% of the app screens and 6.46% of the app methods \nwere explored. The results and reasons for these low levels of coverage are presented in Section 3. In \nSection 4 we present our approach for automated ex\u00adploration: given an app, we construct systematic exploration \ntraces that can then be replayed, analyzed and used for a variety of purposes, e.g., to drive dynamic \nanalysis or as\u00adsemble test suites. Our approach consists of two techniques, Targeted Exploration and \nDepth-First Exploration. Targeted Exploration is a directed approach that .rst uses static byte\u00adcode \nanalysis to extract a Static Activity Transition Graph and then explore the graph systematically while \nthe app runs 1 http://spruce.cs.ucr.edu/A3E/ on a phone. Depth-First Exploration is a completely dynamic \napproach based on automated exploration of activities and GUI elements in a depth-.rst manner. In Section \n5 we provide an overview of A3E s imple\u00admentation: hardware platform, tools and measurement pro\u00adcedures. \nIn Section 6 we provide an evaluation of our ap\u00adproach on 25 apps (3 apps could not be explored because \nthey were written mainly in native code rather than byte\u00adcode). We show that our approach is effective: \non average it attains 64.11% and 59.39% activity coverage via Tar\u00adgeted and Depth-.rst Exploration, respectively \n(a 2x in\u00adcrease compared to what the 7 users have attained); it also at\u00adtains 29.53% and 36.46% method \ncoverage via Targeted and Depth-.rst Exploration, respectively (a 4.5x increase com\u00adpared to the 7 users). \nOur approach is also ef.cient: average .gures are 74 seconds for Static Activity Transition Graph construction, \n87 minutes for Targeted Exploration and 104 minutes for Depth-.rst Exploration. In summary, this work \nmakes the following contributions: A qualitative and quantitative study of coverage attained in practice \nby 7 users for 28 popular Android apps.  Two approaches, Targeted Exploration and Depth-.rst Exploration, \nfor exploring substantial apps running on Android smartphones.  An evaluation of the effectiveness of \nTargeted and Depth\u00ad.rst Exploration on 25 popular Android apps.  2. Android Activities, Graphs and \nMetrics We have chosen Android as the target platform for our A3E implementation as it is currently the \nleading mobile platform in the US [4] and worldwide [3]. We now describe the high\u00adlevel structure of \nAndroid platform and apps; introduce two kinds of Activity Graphs that de.ne the high-level work.ow within \nan app; and de.ne coverage based on these graphs. 2.1 Android App Structure Android platform and apps. \nAndroid apps are typically written in Java (possibly with some additional native code). The Java code \nis compiled to a .dex .le, containing com\u00adpressed bytecode. The bytecode runs in the Dalvik virtual machine, \nwhich in turn runs on top of a smartphone-speci.c version of the Linux kernel. Android apps are distributed \nas .apk .les, which bundle the .dex code with a manifest (app speci.cation) .le named AndroidManifest.xml. \nAndroid app work.ow. A rich application framework fa\u00adcilitates Android app construction, as it provides \na set of li\u00adbraries, a high-level interface for interaction with low-level devices, etc. More importantly, \nfor our purposes, the applica\u00adtion framework orchestrates the work.ow of an app, which makes it easy \nto construct apps but hard to reason about con\u00adtrol .ow. A typical Android app consists of separate screens \nnamed Activities. An activity de.nes a set of tasks that can be grouped together in terms of their behavior \nand corresponds Figure 1. An example activity transition scenario from the popular Android app, Amazon \nMobile.  App Type Category Size # Down\u00adloads Kinst. KBytes Amazon Mobile Free Shopping 146 4,501 58,745 \nAngry Birds Free Games 167 23,560 1,586,884 Angry Birds Space P. Paid Games 179 25,256 14,962 Advanced \nTask Killer Free Productivity 9 75 428,808 Advanced Task Killer P. Paid Productivity 3 99 4,638 BBC News \nFree News&#38;Mag. 77 890 14,477 CNN Free News&#38;Mag. 204 5,402 33,788 Craigslist Mobile Free Shopping \n56 648 61,771 Dictionary.com Free Books&#38;Ref. 105 2,253 285,373 Dictionary.com Ad-free Paid Books&#38;Ref. \n49 1,972 2,775 Dolphin Browser Free Communication 248 4,170 1,040,437 ESPN ScoreCenter Free Sports 78 \n1,620 195,761 Facebook Free Social 475 3,779 6,499,521 Tiny Flashlight + LED Free Tools 47 1,320 1,612,517 \nMovies by Flixster Free Entertainment 202 4,115 398,239 Gas Buddy Free Travel&#38;Local 125 1,622 421,422 \nIMDb Movies &#38; TV Free Entertainment 242 3,899 129,759 Instant Heart Rate Free Health&#38;Fit. 63 \n5,068 100,075 Instant Heart R.-Pro Paid Health&#38;Fit. 63 5,068 6,969 Pandora internet radio Free Music&#38;Audio \n214 4,485 968,714 PicSay -Photo Editor Free Photography 49 1,315 96,404 PicSay Pro -Photo E. Paid Photography \n80 955 18,455 Shazam Free Music&#38;Audio 308 4,503 432,875 Shazam Encore Paid Music&#38;Audio 308 4,321 \n18,617 WeatherBug Free Weather 187 4,284 213,688 WeatherBug Elite Paid Weather 190 4,031 40,145 YouTube \nFree Media&#38;Video 253 3,582 1,262,070 ZEDGE Free Personalization 144 1,855 515,369 to a window in \na conventional desktop GUI. Developers implement activities by extending the android . app. Activity \nclass. As Android apps are GUI-centric, the programming model is based on callbacks and differs from \nthe traditional main()-based model. The Android framework will invoke the callbacks in response to GUI \nevents and developers can control activity behavior throughout its life-cycle (create, paused, resumed, \nor destroy) by .lling-in the appropriate callbacks. An activity acts as a container for typical GUI elements \nsuch as toasts (pop-ups), text boxes, text view objects, spin\u00adners, list items, progress bars, check \nboxes. When interact\u00ading with an app, users navigate (i.e., transition between) different activities \nusing the aforementioned GUI elements. Therefore in our approach activities, activity transitions and \nactivity coverage are fundamental, because activities are the main interfaces presented to an end-user. \nFor this reason we primarily focused on activity transition during a normal ap\u00adplication run, because \nits role is very signi.cant in GUI test\u00ading. Activities can serve different purposes. For example in \na typical news app, an activity home screen shows the list of current news; selecting a news headline \nwill trigger the transition to another activity that displays the full news item. Activities are usually \ninvoked from within the app, though some activities can be invoked from outside the app if the host app \nallows it. Naturally, these activity transitions form a graph. In Fig\u00adure 1 we illustrate how activity \ntransitions graphs emerge as a result of a user interaction in the popular Android app, Amazon Mobile. \nOn top we have the textual description of users actions, in the middle we have an actual screen shot, \nand on the bottom we have the activities and their transitions. Initially the app is in the Main Activity; \nwhen the user clicks the search box, the app transitions to the Search Activity (note the different screen). \nThe user searches for items by typing in item names, and a textual list of items is presented. When the \nuser presses Go , the screen layout changes as the app transitions to the Search List Activity. Table \n1. Overview of our examined apps. We now proceed to de.ning the activity transitions graphs that form \nthe basis of our work. 2.2 Static Activity Transition Graph The Static Activity Transition Graph (SATG) \nis a graph GS = (VS , ES) where the set of vertices, VS, represents the app activities, while the set \nof edges, ES , represents possible activity transitions. We extract SATG s automatically from apps using \nstatic analysis, as described in Section 4.1. Figure 2 shows the SATG for the popular shopping app, Craigslist \nMobile; the reader can ignore node and edge colors as well as line styles for now. Note that activities \ncan be called independently, i.e., without the need for entering into another activity. Therefore, the \nSATG can be a disconnected graph. SATG s are useful for program understanding as they provide an at-a-glance \nview of the high-level app work.ow. 2.3 Dynamic Activity Transition Graph The Dynamic Activity Transition \nGraph (DATG) is a graph GD = (VD, ED) where the set of vertices, VD, represents the app activities, while \nthe set of edges, ED, represents actual activity transitions, as observed at runtime. A DATG captures \nthe footprint of dynamic exploration or user interaction in an intuitive way and is a subgraph of the \nSATG. Figure 2 contains the DATG for the popular shop\u00adping app, Craigslist Mobile: the DATG is the subgraph \ncon\u00adsisting of solid edges and nodes. Paths in DATG s illustrate sequences of actions required to reach \na particular state of an app, which is helpful for constructing test cases or repro\u00adFigure 2. Static \nActivity Transition Graph extracted automatically by our approach from the Craigslist Mobile app. Grey \nnodes and associated edges have been explored by users. Solid-contour nodes (grey or white) and solid-line \nedges were traversed dynamically by our exploration. Dashed-contour nodes and dashed-line edges remained \nunexplored. Activity names are simpli.ed for legibility.  ducing bugs. In the Appendix (Figure 7) we \npresent a second DATG example based on runs from 5 different users, which is illustrates how different \nusers exercise the app differently.  2.4 Coverage Metrics We chose two coverage metrics as basis for \nmeasuring and assessing the effectiveness of our approach: activity cover\u00adage and method coverage. We \nchose these metrics because they strike a good balance between utility and collection overhead: .rst, \nactivities and methods are central to app con\u00adstruction, so the numeric values of activity and method \ncov\u00aderage are intuitive and informative; second, the runtime per\u00adformance overhead associated with collecting \nthese metrics is low enough so that user experience and app performance are not affected. We now proceed \nto de.ning the metrics. Activity coverage. We de.ne activity coverage (AC) as the ratio of activities \nreached during execution (AR) to the total number of activities de.ned in the app (AT ), that is, AR \nAC = . Intuitively, the higher the AC for a certain run, AT the more screens have been explored, and \nthe more thorough and complete the app exploration has been. We retrieve the AR dynamically, and the \nAT statically, as described in Section 5.2. Method coverage. Activity coverage is intuitive, as it indi\u00adcates \nwhat percentage of the screens (that is, functionality at a high level) are reached. In addition, users \nmight be inter\u00adested in the thoroughness of exploration measured at a lower, method-level. Hence we use \na .ner-grained metric what percentage of methods are reached to quantify this aspect. We de.ne method \ncoverage (M C ) as the ratio of methods called during execution (M E) to the total number of meth- M \nE ods de.ned in the app (M T ), that is, M C = . M T We found that all the examined apps, except Advanced \nTask Killer, ship with third-party library code bundled in the app s APK .le; we exclude third-party \nmethods from M E and M T computations as these methods were not de.ned by app developers hence we consider \nthat including them would be misleading. We measured the M E using runtime pro.ling information and the \nM T via static analysis, as described in Section 5.2. 3. User Study: Coverage During Regular Use One \npossible approach to exploration is to rely on (or at least seed the exploration with) actual runs, i.e., \nby observing how end-users interact with the app. Unfortunately, this approach is not systematic: as \nour measurements indicate, during nor\u00admal user interaction, coverage tends to be low, as users ex\u00adplore \njust a small set among the features and functionality of\u00adfered by the app. Therefore, relying on users \nmight have lim\u00adited utility. To quantify the actual coverage attained by end\u00adusers, we have performed \na user study, as described next. App dataset. As of March 2013, Google Play, the main Android app market, \nlists more than 600,000 apps. We se\u00adlected a set of 28 apps for our study; the apps and their characteristics \nare presented in Table 1. The selection was based on several criteria. First, we wanted a mix of free \nand paid apps, so for 7 apps we selected both the free and the paid versions (column 2). Second, we wanted \nrepresentation across different categories such as productivity, games, en\u00adtertainment, news; in total, \nour dataset has apps from 17 dif\u00adferent categories (column 3). Third, we wanted substantial apps; the \nsizes of our selected apps, in thousands of bytecode instructions and KB, respectively, are shown in \ncolumns 4 and 5. Finally, we wanted to investigate popular apps; in the last column we show the number \nof downloads as listed on Google Play as of March 28, 2013; the number of down\u00adloads varied from 2,775 \nto 6,499,521. We believe that this set covers a good range of popular, real-world mobile apps. Methodology. \nWe enrolled 7 different users in our study; one high-coverage minded user (called User 1) and six regular \nusers (User 2 User 7). Each app was exercised by each user for 5 minutes, which is far longer than the \ntypical average app session (71.56 seconds) [35]. To mirror actual app use in the wild, the six regular \nusers were instructed to interact with the app as they normally would; that is, reg\u00adular users were not \ntold that they should try to achieve high coverage. However, User 1 was special because the user s stated \ngoal was to achieve maximum coverage within the time limit. For each run, we collected runtime information \nso we could replicate the experiment later. We then analyzed the 192 runs2 to quantify the levels of \nactivity coverage (sep\u00adarate screens) and method coverage attained in practice. 3.1 Activity Coverage \nWe now turn to discussing the levels of activity coverage that could be attained based on end-user coverage \n(separate and combined across users) for each metric. Cumulative coverage. As different users might explore \ndifferent app features, we developed a technique to merge different executions of the same app. More \nspeci.cally, given two DATG s G1 and G2 (as de.ned in Section 2.3), we construct the union, i.e., a graph \nG = G1 . G2 that contains the union of G1 and G2 s nodes and edges. This technique can potentially increase \ncoverage if the different executions explore different parts of the app. We use this graph union\u00adbased \ncumulative coverage as a basis for comparing manual exploration with automated exploration. Results. \nIn Table 2, we present the activity count and a summary of the activity coverage achieved manually by \nthe 7 users. Column 2 presents the number of activities in each app, including ads. Column 3 presents \nthe number of activ\u00adities, excluding ads (hence these numbers indicate the max\u00adimum the number of activities \nusers can explore without clicking on ads). Column 4 shows the cumulative activity coverage, i.e., when \ncombining coverage via graph union. The percentages are calculated with respect to column 3, i.e., non-ad \nactivities; we decided to exclude ads as they are not related to core app functionality. The complete \ndataset (each app, each user) is available in Table 5 in the Appendix. 2 We had access to 192 (28 \u00d7 7 \n- 4) instead of 196 runs; due to the unavailability of two user study subjects, we could not collect \napp execution data for two users for the apps IMDb Movies &#38; TV and BBC News. We can see that in \nregular exploration cumulative cover\u00adage is quite low across users: mean3 cumulative coverage is 30.08% \nacross all apps. We now proceed to explain why that is the case. Why are so few activities explored? \nThe Missed activi\u00adties group of columns in Table 2 shows, for each app, the number of activities that \nall users missed (.rst column in the group), and the reason why these activities were missed (the remaining \n6 columns in the group). We were able to group the missing activities into the following categories: \n Unexplored features. Speci.c features can be missed because users are not aware of/interested in those \nfea\u00adtures. For example, apps such as Dictionary.com or Tiny Flashlight + LED, provide a widget feature, \ni.e., an app interface typically wider than a desktop icon to provide easy to access functionality. Another \nexample is voice search functionality in the Dolphin Browser browser, which is only explored when users \nsearch by voice.  Social network integration. Many apps offer the op\u00adtion to share information on social \nnetworking sites third-party sites such as Facebook or Twitter, or the app s own network, e.g., Shazam. \nDuring normal app use, users do not necessarily feel compelled to share information. These missed activity \ntypes appear in the social col\u00adumn.  Account. Many apps can function, e.g., watch videos on YouTube, \nwithout the user necessarily logging-in. If an user logs into her account, she can see her pro.le and \nhave access to further activities, e.g., account settings or play-lists on YouTube. In those cases where \nusers did not have (or did not log into) an account, account-speci.c activities were not exercised. \n Purchase. E-commerce apps such as Amazon Mobile of\u00adfer functionality such as buy/sell items. If test \nusers do not conduct such operations, those activities will not be explored.  Options. When users are \ncontent with the default settings of the app and do not change settings, e.g., by accessing the options \nmenu, options activities are not exercised.  Ads. Many free apps contain ad-related activities. For \nexample, in Angry Birds, all the activities but one (play game) were ad-related. Therefore, in general, \nfree apps contain more activities than their paid counterparts see Angry Birds, Advanced Task Killer, \nDictionary.com. When users do not click on ads, the ad-related activities are not explored.  3.2 Method \nCoverage Since activity coverage was on average about 30%, we would expect method coverage to be low \nas well, as the 3 We use geometric mean for all mean computations due to large standard deviations. \nApp Activities Activity coverage (%) Missed activities Methods Method coverage (%) Total # Excludingads \nUsers 1 7(cumulative) # Missed Features Social Account Purchase Options Ads Users 1 7(cumulative) Amazon \nMobile 39 36 25.64 30   7,154 4.93 Angry Birds 8 1 100 6  6,176 10.98 Angry Birds Space Premium 1 \n1 100 0 7,402 0.68 Advanced Task Killer 7 6 70 3  3,836 11.46 Advanced Task Killer Pro 6 6 57 2  \n427 21.32 BBC News 10 10 52.34 3  257 7.69 CNN 42 39 19.05 10  7,725 4.97 Craigslist Mobile 17 15 \n42 35   2,095 10.76 Dictionary.com 22 18 61 11   2,784 13.83 Dictionary.com Ad-free 15 15 73.33 \n4  1,272 19.10 Dolphin Browser 56 56 12.5 49  13,800 13.26 ESPN ScoreCenter 5 5 60 2  4,398 1.35 \nFacebook 107 107 5.60 95  21,896 1.69 Tiny Flashlight + LED 6 4 66.67 4  1,578 15.91 Movies by Flixster \n68 67 23.3 48   7,490 5.32 Gas Buddy 38 33 30.2 29    5,792 9.13 IMDb Movies &#38; TV 39 37 25.64 \n30  8,463 4.60 Instant Heart Rate 17 14 29.4 15   2,002 4.60 Instant Heart Rate -Pro 17 16 13.2 \n16   1,927 5.13 Pandora internet radio 32 30 12.5 30   7,620 3.21 PicSay -Photo Editor 10 10 10 \n9  1,580 4.39 PicSay Pro -Photo Editor 10 10 33.33 9  -a - Shazam 38 37 15.8 36   9,884 9.43 Shazam \nEncore 38 37 22.3 33    9,914 9.32 WeatherBug 29 24 29 24   7,948 8.15 WeatherBug Elite 28 28 14.30 \n24  8,194 6.39 YouTube 18 18 27.77 17  11,125 5.13 ZEDGE 34 34 38.9 18  6287 9.27 Mean 30.08 6.46 \naWe could not get method pro.ling data for PicSay Pro as the pro.ler could not analyze it. Table 2. \nThe results of our user study. methods associated with unexplored activities will not be in\u00advoked. The \nlast group of columns in Table 2 shows the total number of methods for each app, as well as the percentages \nof methods covered by Users 1 and 2 7, respectively. We can see that method coverage is quite low: 6.46% \nis the mean cu\u00admulative coverage for users 1 7. The complete dataset (each app, each user) is available \nin Table 6 in the Appendix. In Section 6.1 we provide a detailed account of why method coverage is low. \n4. Approach We now present the two thrusts of our approach: Targeted Exploration, whose main goal is \nto achieve fast activity ex\u00adploration, and Depth-.rst Exploration, whose main goal is to systematically \nexplore app states. The two strategies are not complementary; rather, we devised them to achieve speci.c \ngoals. Depth-.rst Exploration tests the GUI similarly to how an user would, i.e., clicking on objects \nor editing text boxes, going to newer activities and then returning to the previous ones via the back \nbutton. Targeted Exploration is designed to handle some special circumstances: it can list all the activ\u00adities \nwhich can be called from other apps or background ser\u00advices directly without user intervention, and generates \ncalls to invoke those activities directly. The Targeted strategy was required because not all activities \nare invoked through user interaction. Both strategies can start the exploration in the  // class NewsListActivity \nextends TitleActivity public void onItemClick (...) { Intent localIntent = new Intent(this , NewsStoryActivity \n. class ); ... startActivityWithoutAnimation ( localIntent ); } // class TitleActivity extends RootActivity \npublic startActivityWithoutAnimation ( Intent paramIntent) { super . startActivityWithoutAnimation (paramIntent \n); } // class RootActivity protected void startActivityWithoutAnimation ( Intent paramIntent) { startActivity \n(paramIntent );...} Figure 4. Intent passing through superclasses in NPR News. app entry points, inject \nuser-like GUI actions and generate callbacks to invoke certain activities. To illustrate the main principles \nbehind these strategies, let us get back to the .ow of the Amazon Mobile app shown in Figure 1. In Targeted \nExploration, the SATG is con\u00adstructed via static analysis, and our exploration focuses on quickly traversing \nactivities in the order imposed by the SATG in the Amazon Mobile case, we quickly and au\u00adtomatically \nmove from Main Activity to Search Activity to Search List Activity. In Depth-.rst Exploration, we use \nthe app entry points from the SATG (that is, nodes with no incoming edges) to start the exploration. \nThen, in each activity, we retrieve the GUI elements and exercise them systematically. In the Amazon \nMobile case, we start with the Main Activity and exercise all its contained GUI ele\u00adments systematically \n(which will lead to eventually explor\u00ading Search Activity and from there, Search List Activity); this \nis more time-consuming, but signi.cantly increases method coverage. We .rst discuss how the SATG is constructed \n(Sec\u00adtion 4.1), then show how it drives Targeted Exploration (Section 4.2); next we present Depth-.rst \nExploration (Sec\u00adtion 4.3) and .nally how our approach supports test case generation and debugging (Section \n4.4). 4.1 SATG Construction Determining the correct order in which GUI elements of an Android app should \nbe explored is challenging. The main problem is that the control .ow of Android apps is non\u00adstandard: \nthere is no main(), but rather apps are centered around callbacks invoked by the Android framework in \nre\u00adsponse to user actions (e.g., GUI events) or background ser\u00advices (e.g, GPS location updates). This \nmakes reasoning about control .ow in the app dif.cult. For example, if the current activity is A and \na transition to activity B is possible as a result of user interaction, the methods associated with A \nwill not directly invoke B. Instead, the transition is based on a generic intent passing logic, which \nwe will discuss shortly. We realized that intent passing and consequently SATG construction can be achieved \nvia data-.ow analysis, more speci.cally taint tracking. Hence our key insight is that SATG construction \ncan be reduced to a taint-tracking prob\u00adlem. Coming back to our prior example with the A and B ac\u00adtivities, \nusing an appropriately set-up taint analysis, we taint B; if the taint can reach an actual invocation \nrequest from A, that means activity B is reachable from A and we add an A.B edge to the SATG. The glue \nfor activity transi\u00adtions is realized by objects named Intents. Per the of.cial Android documentation \n[33], an Intent is an abstract de\u00adscription of an operation to be performed. Intents can be used to start \nactivities by passing the intent as an argument to a startActivity -like method. Intents are also used \nto start services, or send broadcast messages to other apps. Hence tracking taint through intents is \nkey for understanding activ\u00adity .ow. We now provide several examples to illustrate SATG con\u00adstruction \nusing taint analysis over the intent passing logic. In Figure 3, on the left we have valid Android Java \ncode for class A that implements an activity. Suppose the programmer wants to set up a transition to \nanother activity class, B. We show three examples of how this can be done by initializing the intent \ninitialized in a method of A, say A.foo() , and cou\u00adpling it with the information regarding the target \nactivity B. In Example 1, we make the A.B connection by passing the class names to the intent constructor. \nIn Example 2, the con\u00adnection is made by setting the B s class as the intent s class. In Example 3, B \nis set to be called as a component of the intent. Our analysis will tag these Intent object declarations \n(new Intent ()) as sources. Next, the taint analysis will look for sinks; in our example, the tagged \nsinks are startActivity , startActivityForResult , and startActivityIfNeeded . Of course, while here \nwe show the Java code for clarity, our analysis operates on bytecode. Taint tracking is shown in Figure \n3 (center): after tagging sinks and sources, the taint analysis will propagate data.ow facts through \nthe app code, and in the end check whether tainted sources reach sinks. For all (source, sink) pairs \nfor which taint has been detected, we add an edge in the SATG (Figure 3 (right)). Hence the gen\u00aderal \nprinciple for constructing the SATG is to identify Intent construction points as sources, and activity \nstart requests as sinks. A more complicated, real-world example, of how our analysis tracks taint through \na class hierarchy is shown in Figure 4, a code snippet extracted from the NPR News app. An Intent is \ninitialized in NewsListActivity . onItemClick (...) , tagged as a source, and passed through the superclass \nTitleActivity to its superclass RootActivity . The startActivity (on the last line) is tagged as a sink. \nWhen the analysis con\u00adcludes, based on the detected taint, we add an edge from NewsListActivity to NewsStoryActivity \nin the SATG. 4.2 Targeted Exploration We now proceed to describing how we perform Targeted Exploration \nusing the SATG as input. Figure 5 provides an overview. The automatic explorer, running on a desktop \nor Tagging sources and sinks Static analysis Resulting SATG public class A extends Activity { ... public \nvoid foo () { /* Example 1 */ Intent intent1 = new Intent(A, B); //tagged as source ... A: intent=new \nIntent (A, B) startActivity ( intent1 ); //tagged as sink  SCanDroid /* Example 2 */ Track taint Intent \nintent2=new Intent(); //tagged as source through app intent . setClass ( this , B.class ); ... startActivityForResult \n( intent2 , requestCode); //tagged as sink A: startActivity(intent) /* Example 3 */  Intent intent3 \n= new Intent(); // Intent object tagged as source intent . setComponent(new ComponentName( package.name \n, B )); ... startActivityIfNeeded ( intent3 , requestCode); //tagged as sink }} Figure 3. Constructing \nSATG s with taint analysis: sources and sinks are tagged automatically (left), taint is tracked by SCanDroid \n(center); the resulting SATG (right). Figure 5. Overview of Targeted Exploration in A3E. laptop, orchestrates \nthe exploration. The explorer .rst reads the SATG constructed by SCanDroid (a static data.ow ana\u00adlyzer \nthat we customized to track intent tainting, as described in Section 4.1) from the app s bytecode, and \nthen starts the app on the phone. Our SATG construction algorithm lists all the exported activities, \nand entry point activities. Exported activities are activities that can be independently called from \nwithin or outside the app; they are marked as such by setting the parameter exported=true in the manifest \n.le. Note that not all activities can be called from outside some have to be reached by the normal process, \nprimarily for security reasons and to maintain application work.ow. For example, when an activity can \nreceive parameters from a previous ac\u00adtivity, the parameters may contain security information that is \nlimited to the application domain. Therefore, we cannot just jump to any arbitrary activity. Next, the \nexplorer runs the Targeted Exploration algo\u00adrithm, which we will describe shortly. The explorer controls \nthe app execution and communicates with the phone via the Android Debugging Bridge. The result of the \nexploration consists of a replayable trace a sequence of events that can be replayed using our RERAN \ntool [7] as well as coverage information. We now proceed to describing the algorithm behind tar\u00adgeted \nexploration; parts of the algorithm run on the phone, parts in the automatic explorer. In a nutshell, \nthe SATG con\u00adtains edges A.B indicating legal activity transitions. As\u00adsuming we are currently exploring \nactivity A, we have two Algorithm 1 Targeted Exploration Input: SATG GS = (VS , ES) 1: procedure TA \nR G E T E D EX P L O R AT I O N(GS) 2: for all nodes Ai in VS that are entry points do 3: Switch to activity \nAi 4: currentActivity . Ai 5: for all edges Ai . Aj in ES do 6: if Aj is exportable then 7: Switch to \nactivity Aj 8: currentActivity . Aj 9: G.. subgraph of GS from starting S node Aj 10: TA R G E T E D \nEX P L O R AT I O N(G.) S 11: end if 12: end for 13: guiE lementS et . EX T R AC T GU I EL E -M E N T \nS(currentActivity) 14: for each guiElement in guiE lementS et do 15: exercise guiElement 16: if there \nis an activity transition to not-yet\u00adexplored activity An then 17: G.. subgraph of GS from starting S \nnode An 18: currentActivity . An 19: TA R G E T E D EX P L O R AT I O N(G.) S 20: end if 21: end for \n22: end for 23: end procedure cases for B: (1) B is exportable ,4 that is, reachable from A but not a \nresult of local GUI interaction in A; or (2) B is reached from A as a result of local GUI interaction \nin A. In case (1) we switch to B directly, and in case (2) we switch to B when exploring A s GUI elements. \nIn either case, explo\u00adration continues recursively from B. Algorithm 1 provides a precise description \nof the Tar\u00adgeted Exploration approach. The algorithm starts with the SATG as input. First, we extract \nthe app s entry point ac\u00adtivities from the SATG (line 2) and start exploration at one of these entry \npoints Ai (lines 3 4). We look for all the ex\u00adportable activities Aj that have an incoming edge from \nAi (lines 5 6). We then switch to each of these exportable ac\u00adtivities and invoke the algorithm recursively \nfrom Aj (lines 7 10). Activities An that are not exportable but reachable from Ai will be switched to \nautomatically as a result of lo\u00adcal GUI exploration (lines 13 16) and then we invoke the algorithm recursively \nfrom An (lines 17 19). The advantage of Targeted Exploration is that it can achieve activity coverage \nfast we can switch to exportable activities without .ring GUI events. 4 The list of exportable activities \nis available in the AndroidManifest.xml .le included with the app. Figure 6. Overview of Depth-.rst \nExploration in A3E. Algorithm 2 Depth-First Exploration Input: Entry point activities |A| 1: procedure \nDFE(|A|) 2: for all nodes Ai in |A| do 3: Switch to activity Ai 4: DE P T H FI R S T EX P L O R AT I \nO N(Ai) 5: end for 6: end procedure 7: 8: procedure DE P T H FI R S T EX P L O R AT I O N(Ai) 9: guiE \nlementS et . EX T R AC T GU I EL E M E N T S(Ai) 10: for each guiE lement in guiE lementS et do 11: \nexcercise guiE lement 12: if there is an activity transition to not-yet\u00adexplored activity An then 13: \nDE P T H FI R S T EX P L O R AT I O N(An) 14: Switch back to activity Ai 15: end if 16: end for 17: end \nprocedure 4.3 Depth-First Exploration We now proceed to presenting Depth-First Exploration, an approach \nthat takes more time but can achieve higher method coverage. As it is a dynamic approach, Depth-First \nExplo\u00adration can be performed even when the tester does not have activity transition information (i.e., \nthe SATG) beforehand. As the name suggests, this technique employs depth-.rst search to mimic how an \nactual user would interact with the app. Figure 5 provides an overview. In this case, no SATG is used, \nbut the automatic explorer runs a different, Depth\u00ad.rst Exploration algorithm, which we will describe \nshortly. The rest of the operations are identical with Targeted Explo\u00adration, that is, the explorer orchestrates \nthe exploration and the results are a replayable trace and coverage information. Algorithm 2 provides \nthe precise description of the Depth-.rst Exploration approach. Similar to Targeted Ex\u00adploration, we \n.rst extract the entry point activities from the app s APK; these activities will act as starting points \nfor the exploration. We then choose a starting point Ai and start depth-.rst exploration from that point \n(lines 1 5). For each activity Ai, we extract all its GUI elements (line 9). We then systematically exercise \nthe GUI elements by .ring their cor\u00adresponding event handlers (lines 10 11). Whenever we de\u00adtect a transition \nto a new activity An, we apply the same algorithm recursively on An (line 13). This process contin\u00adues \nin a depth-.rst manner until we do not .nd any transition to a newer activity after exercising all the \nGUI elements in that screen. We then go back to the previous activity and continue exploring its view \nelements (line 14).  4.4 Replayable Test Cases and Debugging During exploration, A3E automatically records \nthe event stream using RE R A N, a low-overhead record-and-replay tool [7], so that the exploration, \nor parts thereof, can be replayed. This feature helps users construct test cases that can later be executed \nvia RE R A N s replay facility. In addi\u00adtion, the integration with RE R A N facilitates debugging if \nthe app crashes during exploration, we have the exact event stream that has led to the crash; assuming \nthe crash is deter\u00administic, we can reproduce it by replaying the event stream. 5. Implementation We \nnow proceed to presenting the experimental setup, im\u00adplementation details, and measurement procedures \nused for constructing and evaluating A3E. 5.1 Setup and Tools The smartphones used for experiments were \nMotorola Droid Bionic running Android version 2.3.4, Linux kernel version 2.6.35. The phones have Dual \nCore ARM Cortex-A9 CPUs running at 1GHz. We controlled the experiments from a MacBook Pro laptop (2.66 \nGHz dual-core Intel Core i7 with 4GB RAM), running Mac OS X 10.8.3. For the user study, we used RE R \nA N, a tool we developed previously [7] to record user interaction so we could replay and analyze it \nlater. SCanDroid is a tool for static analysis on Dalvik bytecode developed by other researchers and \nus [25]. For this work we extended SCanDroid in two directions: (1) to tag intents and activity life-cycle \nmethods as sinks and sources so we can construct the SATG, and (2) to list all the app-de.ned methods \nthis information was used for method coverage analysis.  5.2 Measuring Coverage Activity coverage. The \nautomatic explorer keeps track of AR, the number of successfully explored activities, via the logcat \nutility provided by Android Debug Bridge (adb) tool from the Android SDK. The total number of activities, \nAT , was obtained of.ine: we used the open source apktool to extract the app s man\u00adifest .le from the \nAPK and parsed the manifest to list all the activities. From the AT and AR we exclude outside activities, \nas those are not part of the app s code base. Exam\u00adples of outside activities are ad-related activities \nand external system activities (browser, music player, camera, etc.) Method coverage. Android OS provides \nan Application Manager (am) utility that can create method pro.les on-the\u00ad.y, while the app is running. \nTo measure M E, the number of methods called during execution, we extracted the method entries from the \npro.ling data reported by am. We measured M T , the total number of methods in an app, via static anal\u00adysis, \nby tailoring SCanDroid to .nd and list all the virtual and declared method calls within the app. Note \nthat third\u00adparty code is not included in M E and M T computation (Section 3.2). 5.3 Automatic Explorer \nGUI element extraction and exercising is required for both Targeted and Depth-.rst Exploration. To explore \nGUI ele\u00adments, A3E rips the app, i.e., extracts its GUI elements dy\u00adnamically using the Troyd tool [20] \n(which in turn is based on Robotium [18]). Robotium can extract and .re event handlers for a rich set \nof GUI elements. This set includes lists, buttons, check boxes, toggle buttons, image views, text views, \nimage buttons, spinners, etc. Robotium also pro\u00advides functionality for editing text boxes, clearing \ntext .elds, clicking on text, clicking on screen positions, clicking on hardware home menu, and back \nbutton. Troyd allows devel\u00adopers to write Ruby scripts that can drive the app using the aforementioned \nfunctionality offered by Robotium (though Troyd does not require access to the app s source code). A3E \nis built on top of Troyd. We modi.ed Troyd to allow automatic navigation through the app, as follows. \nEach An\u00addroid screen consists of GUI elements linked via event han\u00addlers. Simply invoking all the possible \nevent handlers and .ring the events associated with them would be incorrect the app has to be in a state \nwhere it can accept the events, which we detected by interacting with the live app. Hence A3E relies \non live extraction of the GUI elements that are ac\u00adtually present on the screen (we call a collection \nof such ele\u00adments a view). We then systematically map the related event handlers, and call them mimicking \na real user. This run-time knowledge of views was essential for our automated explorer to work. Once \nwe get the information of the views, we can systematically .re the correct actions. As described in Section \n3.1, our test users tended to skip features such as options, ads, settings, or sharing via social networks. \nTo cover such activities and functionality, A3E employs several strategies. A3E automatically detects \nactivities related to special responsibility, such as log in screen, social networking, etc. We created \nsets of credential information (e.g., username/password pairs) that A3E then sends to the app just like \na user would do to get past the screen, and continues the exploration from there. As we implemented \nour approach on top of the Robotium testing framework, we had to compensate for its limitations. One \nsuch limitation was Robotium s inability to generate and send gestures, which would leave many kinds \nof views incompletely exercised when using Robotium alone. To ad\u00address this limitation we wrote a library \nof common simple gestures (horizontal and vertical swipes, straight line swipes, scrolling). We leave \ncomplex multi-touch gestures such as pinching and zooming to future work; as explained in our prior work \n[7], synthesizing complex, multi-touch gestures is non-trivial. In addition to the gesture library and \nlog-on functional\u00adity, A3E also supports microphone, GPS, compass, and ac\u00adcelerometer events. However, \ncertain apps functionality re\u00adquired complex inputs, e.g., processing a user-selected .le. Feeding such \ninputs could be achieved via OS-level record and replay, a task we leave to future work. With this library \nof input events, and GUI-element invo\u00adcation strategies at hand, A3E uses the appropriate explo\u00adration \nalgorithm depending on the kind of exploration we perform, as described next.  5.4 Targeted Exploration \nSection 4.1 described the intent passing logic among inter\u00adnal app activities, and Targeted Exploration \nuses the pre\u00adconstructed SATG to explore these intra-app activity paths. However, the Android platform \nalso allows activities to be called from external apps through implicit messaging by in\u00adtents, as follows: \napps can de.ne intent .lters to notify the OS that certain app activities accept intents and can be started \nwhen the external app sends such an intent. There\u00adfore, when systematically exercising an app s GUI elements, \nthere is a chance that these externally-callable activities are missed if they are not reachable by internal \nactivities. In ad\u00addition to intent .lters, developers can also identify activities as exported, by de.ning \nandroid:exported=\"true\" in the manifest .le. This will allow activities to be called from outside the \napp. When implementing Targeted Exploration we also invoked these externally-callable activities so we \ndo not miss them when they are not reachable from internal ac\u00adtivities.  5.5 Depth-First Exploration \nWith the infrastructure for dynamic GUI element extraction and event .ring at hand, we implemented Depth-.rst \nEx\u00adploration using a standard depth-.rst strategy: each time we .nd a transition to a new activity, we \nswitch to that activ\u00adity and thus we enter deeper levels of the activity hierarchy. This process continues \nuntil we cannot detect any more tran\u00adsition from the current activity. At this point we recursively go \nback to the last activity and start exploring from there. 6. Evaluation We now turn to presenting experimental \nresults. From the 28 examined apps presented in Section 3, we were able to explore 25; 3 apps could not \nbe explored (Angry Birds, Angry Birds Space premium, Facebook) because they are written primarily in \nnative code, rather than bytecode, as explained in Section 6.2. We .rst evaluate our automated exploration \ntechniques on these apps in terms of effectiveness and ef.ciency (Sec\u00adtion 6.1), then discuss app characteristics \nthat make them more or less amenable to our techniques (Section 6.2). 6.1 Effectiveness and Ef.ciency \nActivity coverage. We present the activity coverage results in Table 3. Column 2 shows the number of \nnodes in the SATG, that is, the number of activities in each app, exclud\u00ading ads. The grouped columns \n3 5 show the activity cover\u00adage in percents, in three scenarios: the cumulative coverage for users 1 \n7, coverage attained via Targeted Exploration, and coverage attained via Depth-.rst Exploration. We make \nseveral observations. First, systematic exploration increases activity coverage by a factor of 2x, from \n30.08% attained by 7 users cumulatively to 64.11% and 59.39% attained by Tar\u00adgeted and Depth-.rst Exploration, \nrespectively. Hence our approach is effective at systematically exploring activities. Second, SATG construction \npays off; because it relies on statically-discovered transitions, Targeted Exploration will be able to \n.re transitions in cases where Depth-.rst Explo\u00adration cannot, and overcome a limitation of dynamic tools \nthat start the exploration inside the app. Method coverage. The method coverage results are shown in \nthe last columns of Table 3. The methods column shows the number of methods de.ned in the app, excluding \nthird\u00adparty code. The next columns show activity coverage in per\u00adcents, in three scenarios: the cumulative \ncoverage for users 1 7, coverage attained via Targeted Exploration, and cov\u00aderage attained via Depth-.rst \nExploration. We make several observations. First, systematic exploration increases method coverage by \nabout 4.5x, from 6.46% attained by 7 users cu\u00admulatively to 29.53% and 36.46% attained by Targeted and \nDepth-.rst Exploration, respectively. Hence our approach is effective at systematically exploring methods. \nSecond, the lengthier, systematic exercising of each activity element per\u00adformed by Depth-.rst Exploration \ntranslates to better explo\u00adration of methods associated directly (or transitively) with that activity. \nExploration time. In Table 4 we show the time required for exploration. Column 2 contains the static \nanalysis time, which is required for SATG construction; this is quite ef\u00ad.cient, at most 10 minutes but \ntypically 4 minutes or less. We measured exploration time by letting both Targeted and Depth-.rst explorations \nrun to completion, that is until we have explored all entry point activities and activities we App Acti\u00advities \nActivity coverage (%) Methods Method coverage (%) Users 1 7(cumulative) Targeted Depth-.rst Users 1 7(cumulative) \nTargeted Depth-.rst Amazon Mobile 36 25.64 63.90 58.30 7,154 4.93 28.1 45.10 Angry Birds - 100 - - - \n10.98 - - Angry Birds Space Premium - 100 - - - 0.68 - - Advanced Task Killer 6 70 83.33 83.33 420 11.46 \n59.76 62.86 Advanced Task Kill. P. 6 57 83.30 83.30 257 21.32 39.30 73.20 BBC News 10 52.34 80.00 80.00 \n3,836 7.69 31.80 37.40 CNN 39 19.05 69.23 61.54 9,269 4.97 29.88 29.97 Craigslist Mobile 15 42 73.30 \n66.70 2,095 10.76 30.50 41.10 Dictionary.com 18 61 83.33 72.22 3,881 13.83 44.29 44.62 Dictionary.com \nAd Free 15 73.33 100 80 1,846 19.10 47.72 49.13 Dolphin Browser 56 12.50 42.86 37.50 17,007 13.26 42.92 \n43.37 ESPN ScoreCenter 5 60 80.00 80.00 4,398 1.35 16.10 31.20 Facebook 107 5.60 - - - 1.69 - - Tiny \nFlashlight + LED 4 66.67 75 75 1,837 15.91 28.03 47.63 Movies by Flixster 67 23.30 77.60 61.20 10,151 \n5.32 29.50 31.80 Gas Buddy 33 30.20 72.70 63.60 5,792 9.13 31.40 47.80 IMDb Movies &#38; TV 37 25.64 \n54.10 62.10 11,950 4.60 29.80 32.40 Instant Heart Rate 14 29.40 42.86 35.71 2,407 4.60 20.40 23.18 Instant \nHeart Rate -Pro 16 13.20 37.50 37.50 2,514 5.13 26.05 26.21 Pandora internet radio 30 12.50 80.0 76.70 \n7,620 3.21 21.10 31.70 PicSay -Photo Editor 10 10 50 40 1,458 4.39 25.58 27.43 PicSay Pro -Photo Editor \n10 33.33 50 40 - - - - Shazam 37 15.80 45.95 45.95 12,461 9.43 34.74 35.67 Shazam Encore 37 22.30 45.90 \n51.40 9,914 9.32 29.10 36.30 WeatherBug free 24 29 54.17 45.83 7,744 8.15 40.05 40.33 WeatherBug Elite \n24 14.30 91.70 87.50 7,948 6.39 17.20 25.70 YouTube 18 27.77 55.56 50 14,550 5.13 26.95 26.99 ZEDGE 34 \n38.90 67.60 67.60 6,287 9.27 16.60 24 Mean 30.08 64.11 59.39 6.46 29.53 36.46 Table 3. Evaluation results: \nactivity and method coverage. could transitively reach from them. We imposed no time\u00adout. Columns 3 and \n4 show the dynamic exploration time, 18 236 minutes for Targeted Exploration and 39 239 min\u00adutes for \nDepth-.rst Exploration. Hence our approach per\u00adforms systematic exploration ef.ciently. As expected, \nTar\u00adgeted Exploration is faster, even after adding the SATG con\u00adstruction time, because it can .re activity \ntransitions directly (there were two exceptions to this, explained shortly). We believe that these times \nare acceptable and well worth it, con\u00adsidering the provided bene.ts: a replayable trace that can be used \nas basis for dynamic analysis or constructing test cases. Targeted vs. Depth-.rst Exploration. While \nthe two ex\u00adploration techniques implemented in A3E have similar goals (automated exploration) they have \nseveral differences. Targeted Exploration requires a preliminary static data\u00ad.ow analysis stage to construct \nthe SATG. However, once the SATG is constructed, targeted exploration is fast, espe\u00adcially if a high \nnumber of activities are exportable, hence we can quickly achieve high activity coverage by directly \nswitching to activities. Depth-.rst Exploration does not require a SATG, but the exploration phase is \nslower systematically exercising all GUI elements will prolong exploration. However, this pro\u00adlonged \nexploration could be a worthy trade-off since Depth\u00ad.rst Exploration achieves higher method coverage. \nWe now present some examples that illustrate tradeoffs and shed light on some counterintuitive results. \nFor Ad\u00advanced Task Killer and ESPN ScoreCenter we attain similar activity coverage but for ESPN ScoreCenter \nmethod coverage is signi.cantly lower. The reason for this is app structure: Table 4. Evaluation results: \ntime taken by SATG construc\u00adtion and exploration. Note the different units across columns (seconds vs. \nminutes). App Time SATG (seconds) Targeted (minutes) Depth-.rst (minutes) Amazon Mobile 222 123 131 \nAdvanced Task Killer 39 41 47 Advanced Task Kill. P. 24 27 58 BBC News 68 18 52 CNN 14 158 161 Craigslist \nMobile 43 83 91 Dictionary.com 66 113 131 Dictionary.com Ad Free 45 153 156 Dolphin Browser 595 171 179 \nESPN ScoreCenter 42 22 44 Tiny Flashlight + LED 52 33 39 Movies by Flixster 53 228 219 Gas Buddy 157 \n109 124 IMDb Movies &#38; TV 107 135 126 Instant Heart Rate 56 47 51 Instant Heart Rate -Pro 50 48 49 \nPandora internet radio 92 89 111 PicSay -Photo Editor 36 119 121 PicSay Pro -Photo Ed. 40 112 129 Shazam \n64 236 239 Shazam Encore 248 188 230 WeatherBug free 120 69 107 WeatherBug Elite 119 115 124 YouTube \n200 131 135 ZEDGE 124 97 114 Mean 74 87 104 ESPN ScoreCenter employs complex activities, i.e., differ\u00adent \nlayouts within an activity with more features to use. The Targeted algorithm quickly switches the activities \nwithout exploring layout elements in depth, while Depth-.rst takes longer to exercise the layout elements \nthoroughly. For the same app structure reason, Targeted exploration .nishes sig\u00adni.cantly faster for \nAdvanced Task Killer Pro and BBC News. Most apps show better activity coverage for Targeted than Depth-.rst \nExploration. This is primarily because they have multiple entry points, or they have activities with \nintent .l\u00adters to allow functionality to be invoked from outside the app starting the exploration from \nwithin the app for intent\u00ad.lter based activities which are not invoked from within the app will fail \nto discover those activities. For instance, Ama\u00adzon Mobile has a bar-code search activity which was missed \nduring the Depth-.rst search, but the Targeted Exploration succeeded to call the activity with the information \nfrom the SATG. An exception from this were IMDb Movies &#38; TV and Shazam Encore: both apps have lower \nactivity coverage in Targeted Exploration than Depth-.rst. After investigation we found that some activities \ncould be invoked by Targeted Exploration using intent .lters with parameters, but Targeted Exploration \nfailed to exercise the activities; this was due to speci.c input parameters what Targeted exploration \nfailed to produce. The Depth-.rst search exercised the app as a user would and landed on those particular \npages from the right context with correct input parameters, achieving higher cov\u00aderage. For example the \nplay trailer activity in IMDb Movies &#38; TV was not run during Targeted Exploration as it does not \nhave the required parameter, in this case the video .le name or location. The exploration times depicted \nin Table 4 also have some interesting data points. The exploration time largely de\u00adpends on the size \nand GUI complexity of the app. Normally, Depth-.rst Exploration is slower than Targeted because of the \nswitch back (line 14 in Algorithm 2). Two apps, though, Movies by Flixster and IMDb Movies &#38; TV do \nnot conform to this. Upon investigation, we found that activities in these apps form (almost) complete \ngraphs, with many activities being callable from both inside the app or outside the app (but when called \nfrom outside, parameters need to be set correctly). Depth-.rst reached the activities naturally with \nthe correct parameters, whereas Targeted had to back off re\u00adpeatedly for some activities when attempting \nto invoke those activities before parameters were properly constructed. 6.2 Automatic Exploration Catalysts \nand Inhibitors We now re.ect on our experience with the 28 apps and dis\u00adcuss app features that facilitate \nor hinder exploration via A3E. The reasons that prevent us from achieving 100% cov\u00aderage are both fundamental \nand technical. Fundamental rea\u00adsons include the presence of activities that cannot be auto\u00admatically \nexplored due to their nature, and for a good reason. For example, in Amazon Mobile, purchase-related \nactivities could not be explored because server-side validation would be required: to go to the purchased \nstate, we would .rst need to get past the checkout screen where credentials and credit card are veri.ed. \nComplex gestures and inputs. Our techniques can get good traction for apps built around GUI elements \nprovided by the Android SDK, such as text boxes, buttons, images, text views, list views, check boxes, \ntoggle buttons, spinners, etc. However, some apps rely on complex, app-speci.c ges\u00adtures. For example, \nin the PicSay-Photo Editor app, most of the view elements are custom-designed objects, e.g., artwork \nand images that are manipulated via speci.c gestures. Our gesture and sensor libraries (Section 5.3) \npartially address this limitation. Task switching. Another inhibitor is task switching: if our app under \ntest loses focus, and another app gains focus, we cannot control this second app. For example, if an \napp invokes the default .le browser to a speci.c .le location, the A3E explorer will stop tracking, because \nthe .le browser runs in a different process, and will resume tracking when the app under test regains \nfocus. This is for a good reason, as part of the Android app isolation policy, but we cannot track GUI \nevents in other apps. Service-providing apps. Some apps can act as service providers by implementing \nservices that run in the back\u00adground. For example, WeatherBug runs in the background reporting the weather; \nAdvanced Task Killer runs in the back\u00adground and kills apps which were not accessed for a spe\u00adci.c amount \nof time. Hence for such apps we cannot explore methods that implement background service-providing func\u00adtionality \nbecause they are not reachable from the GUI. Native code. Finally, our static analysis works on Dalvik \nVM bytecode. If the app uses native code instead of Dalvik bytecode to handle GUI elements, we cannot \napply our tech\u00adniques. For example, the two Angry Birds apps use native Open GL code to manage drawing \non the canvas, hence our GUI element extraction does not have access to the native GUI information. We \ncould not explore the Facebook app for the same reason. 7. Related Work The work of Rastogi et. al. [10] \nis most closely related to ours. Their system, named Playground, runs apps in the An\u00addroid emulator on \ntop of a modi.ed Android software stack (TaintDroid); their end goal was dynamic taint tracking. Their \nwork introduced an automated exploration technique named intelligent execution (akin to our Depth-.rst \nExplo\u00adration). Intelligent execution involves starting the app, dy\u00adnamically extracting GUI elements \nand exploring them (with pruning for some apps to ensure termination) according to a sequencing policy \nauthors have identi.ed works best explore input events .rst, then explore action providers such as buttons. \nThey ran Playground on an impressive 3,968 apps and achieved 33% code coverage on average. The are several \ndifferences between their approach and A3E. First, they run the apps on a modi.ed software stack on top \nof the Android emulator, whereas we run apps on actual phones using an unmodi.ed software stack. The \nemulator has several limita\u00adtions [32], e.g., no support for actual calls, USB, Bluetooth, in addition \nto lacking physical sensors (GPS, accelerome\u00adter, camera), which are essential for a complete app expe\u00adrience. \nSecond, Playground, just like our Depth-.rst Explo\u00adration, can miss activities, as Table 4 shows hence \nour need for Targeted Exploration which uses static analysis to .nd all the possible activities and entry \npoints. Third, their GUI element exploration strategy is based on heuristics, ours is depth-.rst; both \nstrategies have advantages and disadvan\u00adtages. Fourth, since we ran our experiments on actual phones \nwith unmodi.ed VMs we could not collect instruction cov\u00aderage, so we cannot directly compare our coverage \nnumbers with theirs. Memon et. al. s line of work on GUI testing for desktop applications [12 14] is \ncentered around event-based model\u00ading of the application to automate GUI exploration. Their ap\u00adproach \nmodels the GUI as an event interaction graph (EIG); the EIG captures the sequences of user actions that \ncan be executed on the GUI. While the EIG approach is suitable for devising exploration strategies for \nGUI testing in applica\u00adtions with traditional GUI design, i.e., desktop applications, several factors \npose complications when using it for touch\u00adbased smartphone apps. First, and most importantly, transi\u00adtions \nassociated with non-activity elements cannot be easily captured as a graph. There is a rich set of user \ninput fea\u00adtures associated with smartphone apps in general (such as gestures swipes, pinches and zooms) \nwhich are not tightly bound to a particular GUI object such as a text box or a but\u00adton, so there is not \nalways a current node as with EIG to determine the next action. For example, if the GUI consists of a \nwidget overlapped on a canvas, each modeled as graphs, then the graph corresponding to the widget and \nthe canvas combined has a set of nodes of size proportional to the prod\u00aduct of number of nodes in the \nwidget and canvas graphs; this quickly becomes intractable. Moreover, the next user action can affect \nthe state of the canvas, widget, both, or neither, which again is intractable as it leads to an explosion \nin the number of edges in the combined graph. For example, activ\u00adity com.aws.android.lib.location.LocationListActivity \nin the WeatherBug app contains different layouts, each contain\u00ading multiple widgets; a horizontal swipe \non any widget can change the layout, hence with EIGs we would have to rep\u00adresent this using a bipartite \ngraph with a full set of edges among widgets in the two layouts. Second, as mobility is a core feature \nof smartphones, smartphone apps are built around multimodal sensors and sensor event streams (ac\u00adcelerometer, \ncompass, GPS); these sensor events can change the state of the GUI, but are not easily captured in the \nEIG paradigm many sensors do not exist on desktop systems and their supported actions are far richer \nthan clicks or drags. Modeling such events to permit GUI exploration requires a different scheme compared \nto EIG; our event library (Sec\u00adtion 5.3) and dynamic identi.cation of next possible states allows us \nto generate multimodal events to permit systematic exploration. Third, Android app GUI state can be changed \nfrom outside the app, or by a background service. For ex\u00adample, an outside app can invoke an activity \nof another app through a system-wide callback which in the EIG model would a spontaneous transition into \na node with no incom\u00ading edge. The behavior of the callback requests can certainly modify GUI states. \nHence creating lists of action sequences that can be executed by a user on an interface will lead to \nex\u00adploring only a subset of GUI states. This is the reason why, while constructing the SATG, we analyze \nactivities that ac\u00adcept intent .lters and take appropriate action to design ex\u00adploration test cases automatically. \nGUITAR [16] is a GUI testing framework for Java and Windows applications based on EIG. Android GUITAR \n[17] applies GUITAR to Android by extending Android SDK s MonkeyRunner tool to allow users to create \ntheir own test cases with a point-and-click in\u00adterface that captures press events. In our approach, test \ncase creation is automated.  Yang et. al. [11] implemented a tool for automatic ex\u00adploration called \nOR B I T. Their approach uses static analysis on the app s Java source code to detect actions associated \nwith GUI states and then use a dynamic crawler (built on top of Robotium) to .re the actions. We use \nstatic analysis on app bytecode to extract the SATG, as activities are stable, but then use dynamic GUI \nexploration to cope with dynamic layouts inside activities. They achieved signi.cant statement coverage \n(63% 91%) on a set of 8 small open source apps; exploration took 80 480 seconds. We focus on a different \nproblem domain: large real-world apps for which the source code is not available, so exploration times \nand coverage are not directly comparable. Anand et al. [34] developed an approach named ACTEV E for concolic \ngeneration of events for testing Android apps whose source code is available. Their focus is on cover\u00ading \nbranches while avoiding the path explosion problem. ACTEV E generated test inputs for .ve small open \nsource in 0.3 2.7 hours. Similarly, Jensen et al. [5] have used con\u00adcolic execution to derive event sequences \nthat can lead to a speci.c target state in an Android app, and applied their approach to .ve open source \napps (0.4 33KLOC) and show that their approach can reach states that could not be reached using Monkey. \nOur focus and problem domains are different: GUI and sensor-driven exploration for substantial, popular \napps, rather than focusing on covering speci.c paths. We believe that using concolic execution would \nallow us to in\u00adcrease coverage (especially method coverage), but it would require a symbolic execution \nengine robust enough to work on APKs of real-world substantial apps. Monkey [15] is a testing utility \nprovided by the Android SDK that can send a sequence of random and deterministic events to the app. Random \nevents are effective for stress test\u00ading and fuzz testing, but not for systematic exploration; de\u00adterministic \nevents have to be scripted, which involves effort, whereas in our case systematic exploration is automated. \nMonkeyRunner [24] is an API provided by the Android SDK which allows programmers to write Python test \nscripts for exercising Android apps. Similar to Monkey, scripts must be written to explore apps, rather \nthan using automated explo\u00adration as we do. Robotium [18] is a testing framework for Android that supports \nboth black-box and white-box testing. Robotium facilitates interaction with GUI components such as menus, \ntoasts, text boxes, etc., as it can discover these elements, .re related events, and generate test cases \nfor exercising the elements. However, it does not permit automated exploration as we do. Troyd [20] is \na testing and capture-replay tool built on top of Robotium that can be used to extract GUI widgets, record \nGUI events and .re events from a script. We used parts of Troyd in our approach. However, Troyd cannot \nbe used directly for either Targeted or Depth-.rst Exploration, as it needs input scripts for exercising \nGUI elements. Moreover, in its unmodi.ed form, Troyd had a substantial performance overhead which slowed \ndown exploration considerably we had to modify it to reduce the performance overhead. TEMA [41] is a \ncollection of model-based testing tools which have been applied to Android. GUI elements form a state \nmachine and basic GUI events are treated as key\u00adwords like events. Within this framework, test scripts \ncan be designed and executed. In contrast, we extract a model either statically or dynamically and automatically \nconstruct test cases. Android Ripper [9] is a GUI-based static and dynamic testing tool for Android. \nIt uses a state-based approach to dy\u00adnamically analyze GUI events and can be used to automate testing \nby separate test cases. Android Ripper preserves the state of the application where state is actually \na tuple of a particular GUI widget and its properties. An input event trig\u00adgers the change in the state \nand users can write test scripts based on the tasks that can modify the state. The approach works only \non the Android emulator and thus cannot mimic sensor events properly like a real world application. Several \ncommercial tools provide functionality somewhat related to our approach, though their end-goals differ \nform ours. Testdroid [26] can record and run the tests on multi\u00adple devices. Ranorex [27] is a test automation \nframework. Eggplant [29] facilitates writing automated scripts for test\u00ading Android apps. Framework for \nAutomated Software Test\u00ading (FAST) [28] can automate the testing process of Android apps over multiple \ndevices. Finally, there exist a variety of static [19, 21, 23, 25] and dynamic [22, 31] analysis tools \nfor Android, though these tools are only marginally related to our work. We apply static analysis for \nSATG construction but our end goal is not static analysis. However, our replayable traces can .t very \nwell into a dynamic analysis scenario as they provide signi.cant coverage. 8. Conclusions We have presented \nA3E, an approach and tool that allow An\u00addroid apps to be explored systematically. We performed a user \nstudy that has revealed that users tend to explore just a small set of features when interacting with \nAndroid apps. We have introduced Targeted Exploration, a novel technique that leverages static taint \nanalysis to facilitate fast yet effec\u00adtive exploration of Android app activities. We have also in\u00adtroduced \nDepth-.rst Exploration, a technique that does not use static analysis, but instead performs a thorough \nGUI ex\u00adploration which results in increased method coverage. Our approach has the advantage of permitting \nexploration with\u00adout requiring the app source code. Through experiments on 25 popular Android apps, we \nhave demonstrated that our techniques can achieve substantial coverage increases. Our approach can serve \nas basis for a variety of dynamic analy\u00adsis and testing tasks. Acknowledgments We thank Lorenzo Gomez, \nGavin Huang, Shashank Reddy Kothapalli, and Xuetao Wei for their assistance with the user study; Steve \nSuh and Jinseong Jeon for their valuable sug\u00adgestions on the implementation; and the anonymous referees \nfor their helpful comments. This research was supported in part by NSF grant CNS-1064646. References \n[1] Gartner. Inc. Gartner Says Worldwide PC Shipment Growth Was Flat in Second Quarter of 2012. July, \n2012. URL http://www.gartner.com/it/page.jsp?id=2079015. [2] Gartner. Inc. Gartner Highlights Key Predictions \nfor IT Organizations and Users in 2010 and Beyond. January, 2010. URL http://www.gartner.com/it/page.jsp?id= \n1278413. [3] IDC. Android and iOS Surge to New Smartphone OS Record in Second Quarter, According to IDC. \nAugust, 2012. URL http://www.idc.com/getdoc.jsp?containerId= prUS23638712. [4] CNET. Android reclaims \n61 percent of all U.S. smartphone sales. May, 2012. URL http://news.cnet.com/8301-1023_3-57429192-93/ \nandroid-reclaims-61-percent-of-all-u. s-smartphone-sales/. [5] Casper S Jensen, M. R. Prasad, and A. \nM\u00f8ller. Automated testing with targeted event sequence generation. In Proceedings of the 2013 International \nSymposium on Software Testing and Analysis, pages 67-77. [6] C. Hu and I. Neamtiu. Automating GUI testing \nfor Android applications. In AST 11, pages 77-83. [7] L. Gomez, I. Neamtiu, T.Azim, and T. Millstein. \nRERAN: Timing-and Touch-Sensitive Record and Replay for Android. In ICSE 13. [8] Jessica Guyn. Facebook \nusers give iPhone app thumbs down. In Los Angeles Times. July, 2011. URL http: //latimesblogs.latimes.com/technology/2011/07/ \nfacebook-users-give-iphone-app-thumbs-down.html, July 21. [9] Domenico Amal.tano, Anna Rita Fasolino \nand Salvatore De. Using GUI Ripping for Automated Testing of Android Applications. In ASE 2012, pages \n258-261. [10] Vaibhav Rastogi, Yan Chen and William Enck. AppsPlay\u00adground: automatic security analysis \nof smartphone applications. In CODASPY 2013, pages 209-220. [11] Wei Yang, Mukul Prasad and Tao Xie. \nA Grey-box Approach for Automated GUI-Model Generation of Mobile Applications. In FASE 13, pages 250-265. \n[12] Xun Yuan and Atif M. Memon. Generating Event Sequence-Based Test Cases Using GUI Run-Time State \nFeedback. In IEEE Transactions on Software Engineering, 2010, pages 81-95. [13] Xun Yuan and Atif M. \nMemon. Using GUI Run-Time State as Feedback to Generate Test Cases. In ICSE 07, pages 396-405. [14] Atif \nM. Memon. An event-.ow model of GUI-based applications for testing. In Software Testing, Veri.cation \nand Reliability, 2007, pages 137-157. [15] Android Developers. UI/Application Exerciser Monkey. August, \n2012. URL http://developer.android.com/ tools/help/monkey.html. [16] Atif Memon. GUITAR. August, 2012. \nURL guitar. sourceforge.net. [17] Atif Memon. Android GUITAR. August, 2012. URL http: //sourceforge.net/apps/mediawiki/guitar/index. \nphp?title=Android_GUITAR. [18] Google Code. Robotium. August, 2012. URL http: //code.google.com/p/robotium/. \n[19] SONY. APK Analyzer. January, 2013. URL http://developer.sonymobile.com/knowledge-base/ tool-guides/analyse-your-apks-with-apkanalyser/. \n[20] Jinseong Jeon and Jeffrey S. Foster. Troyd. January, 2013. URL https://github.com/plum-umd/troyd. \n[21] Google Code. Androguard. January, 2013. URL http: //code.google.com/p/androguard/. [22] Google Code. \nDroidbox. January, 2013. URL http: //code.google.com/p/droidbox/. [23] Google Code. Android Assault. \nJanuary, 2013. URL http://code.google.com/p/android-assault/. [24] Android Developers. MonkeyRunner. \nAugust, 2012. URL http://developer.android.com/guide/developing/ tools/monkeyrunner_concepts.html. [25] \nVarious Authors. SCanDroid. January, 2013. URL https://github.com/scandroid/scandroid. [26] Bitbar. Automated \nTesting Tool for Android -Testdroid. January, 2013. URL http://testdroid.com/. [27] Ranonex. Android \nTest Automation -Automate your App Testing. January, 2013. URL http: //www.ranorex.com/mobile-automation-testing/ \nandroid-test-automation.html. [28] W. River. Wind River Framework for Automated Software Testing. January, \n2013. URL http://www.windriver.com/ announces/fast/. [29] TestPlant. eggPlant for mobile testing.. January, \n2013. URL http://www.testplant.com/products/eggplant/ mobile/. [30] Pamela Bhattacharya, Liudmila Ulanova, \nIulian Neamtiu and Sai Charan Koduru. An Empirical Analysis of the Bug-.xing Process in Open Source Android \nApps. In CSMR 13. [31] W. Enck, P. Gilbert, B. G. Chun, L. P. Cox, J. Jung, P. McDaniel, and A. N. Sheth. \nTaintdroid: An information-.ow tracking system for realtime privacy monitoring on smartphones. In OSDI, \n2010, pages 393-407. [32] Android Developers. Android Emulator Limitations. March, 2013. URL http://developer.android.com/tools/ \ndevices/emulator.html#limitations. [33] Android Developers. Android Intents. March, 2013. URL http://developer.android.com/reference/android/ \ncontent/Intent.html. [34] S. Anand, M. Naik, M. J. Harrold, and H. Yang. Automated concolic testing of \nsmartphone apps. In FSE 12, pages 1-11. [35] M. B \u00a8oning, A. Kr \u00a8ohmer, B. Hecht, J. Sch \u00a8uger, and G. \nBauer. Falling asleep with Angry Birds, Facebook and Kindle: a large scale study on mobile application \nusage. In MobileHCI 11, pages 47-56. [36] B. Cornelissen, A. Zaidman, A. van Deursen, L. Moonen, and \n R. Koschke. A Systematic Survey of Program Comprehension through Dynamic Analysis. In Software Engineering, \nIEEE Transactions on, 2009, pages 684-702.  [37] Michael D. Ernst. Static and dynamic analysis: Synergy \nand duality. In WODA 2003: Workshop on Dynamic Analysis, May 9, pages 24-27. [38] S. Hao, D. Li, W. \nHalfond, and R. Govindan. Estimating Android applications CPU energy usage via bytecode pro.ling. In \nGreen and Sustainable Software (GREENS), 2012 First International Workshop on, 2012, pages 1-7. [39] \nM. Dong and L. Zhong. Self-constructive high-rate system energy modeling for battery-powered mobile systems. \nIn MobiSys 11, pages 335-348. [40] X. Wei, L. Gomez, I. Neamtiu, and M. Faloutsos. Pro.le-Droid: multi-layer \npro.ling of android applications. In Mobicom 12, pages 137-148. [41] T. Takala, M. Katara, and J. Harty. \nExperiences of system\u00adlevel model-based GUI testing of an Android application. In ICST 11, pages 377-386. \n Appendix  Figure 7. Dynamic Activity Transition Graph for the BBC News app, constructed based on runs \nfrom 5 different users: colors represent users and labels on the edges represent the sequence in which \nthe edges are explored. App Activities Activity coverage (%) total # excluding ads User 1 User 2 User \n3 User 4 User 5 User 6 User 7 Amazon Mobile 39 36 18 13 15.4 10.26 13 25.64 13 Angry Birds 8 1 100 100 \n100 100 100 100 100 Angry Birds Space Premium 1 1 100 100 100 100 100 100 100 BBC News 10 10 70 20 30 \n- - 70 20 Advanced Task Killer 7 6 28.6 43 43 28.6 28.6 28.6 57 Advanced Task Killer Pro 6 6 50 33.33 \n50 16.66 16.66 16.66 33.33 CNN 42 39 19.05 9.5 14.29 12 12 19 14.29 Craigslist Mobile 17 15 23.5 35.3 \n41.2 23.5 29.4 29.4 35.3 Dictionary.com 22 18 41 41 59 32 41 59 41 Dictionary.com Ad-free 15 15 33.33 \n60 53.33 20 20 73.33 33.33 Dolphin Browser 56 56 12.5 8.9 1.78 1.78 1.78 1.78 1.78 ESPN ScoreCenter 5 \n5 60 40 20 20 20 20 20 Facebook 107 107 5.60 2.8 4.67 4.67 6.54 3.73 3.73 Tiny Flashlight + LED 6 4 50 \n50 50 50 50 50 66.67 Movies by Flixster 68 67 8.8 14.7 5.9 13.2 8.8 20.6 7.3 Gas Buddy 38 33 29 29 23.6 \n21 29 26 15.8 IMDb Movies &#38; TV 39 37 25.64 15.4 15.4 - - 20.5 12.8 Instant Heart Rate 17 14 23.5 \n29.4 29.4 23.5 23.5 23.5 23.5 Instant Heart Rate -Pro 17 16 11.8 17.65 11.8 11.8 11.8 11.8 11.8 Pandora \ninternet radio 32 30 9.4 9.4 12.5 12.5 12.5 9.4 12.5 PicSay -Photo Editor 10 10 10 10 10 10 10 10 10 \nPicSay Pro -Photo Editor 10 10 10 30 10 10 10 10 10 Shazam 38 37 5.3 15.8 5.3 5.3 5.3 5.3 8 Shazam Encore \n38 37 15.8 21 21 8 8 10.5 10.5 WeatherBug 29 24 27.6 24.13 20.7 20.7 20.7 20.7 27.6 WeatherBug Elite \n28 28 10.71 14.3 14.28 7.14 7.14 3.57 3.57 YouTube 18 18 11.11 11.11 11.11 11.11 11.11 11.11 27.77 ZEDGE \n34 34 35.29 29.41 32.35 29.41 20.58 11.76 23.52 Table 5. Activity count and coverage. User 1 has explicitly \ntried to achieve high coverage, while 2 7 are regular users. Table 6. Method count and coverage. App \nMethod count External packages App speci.c method count Method coverage (%) User 1 User 2 User 3 User \n4 User 5 User 6 User 7 Amazon Mobile 13151 5 7154 4.21 1.36 3.93 1.64 4.93 3.99 2.92 Angry Birds 12245 \n4 6176 10.81 10.27 10.37 10.98 10.94 10.43 10.17 Angry Birds Space Premium 12953 4 7402 0.68 0.33 0.37 \n0.63 0.42 0.31 0.19 BBC News 4918 1 427 11.46 6.52 4.92 9.37 10.30 11.24 11.00 Advanced Task Killer 525 \n0 257 19.26 17.51 16.73 14.01 16.34 18.29 16.54 Advanced Task Killer Pro 257 4 3836 3.96 3.18 2.77 - \n- 3.47 7.69 CNN 13029 11 7725 4.86 4.72 4.12 4.44 1.89 4.44 4.44 Craigslist Mobile 2765 4 2095 6.88 5.78 \n8.78 3.10 9.27 10.07 10.69 Dictionary.com 4664 6 2784 0.97 0.97 5.96 10.86 12.31 4.64 11.02 Dictionary.com \nAd-free 2199 4 1272 18.64 17.55 15.33 15.65 17.37 18.08 15.80 Dolphin Browser 23701 6 13800 13.26 9.98 \n10.06 7.54 3.95 4.17 11.15 ESPN ScoreCenter 5511 5 4398 0.55 0.45 0.23 0.28 0.28 1.04 1.20 Facebook 34883 \n12 21896 1.67 1.61 1.64 1.48 1.59 1.56 1.53 Tiny Flashlight + LED 2121 3 1578 15.59 15.85 4.81 13.68 \n0.70 15.85 15.05 Movies by Flixster 12476 8 7490 3.53 3.30 4.60 4.66 2.86 3.41 4.68 Gas Buddy 7841 4 \n5792 7.38 5.51 3.82 7.84 5.52 3.53 5.31 IMDb Movies &#38; TV 19781 9 8463 4.60 1.78 0.98 - - 0.89 0.47 \nInstant Heart Rate 3044 5 2002 2.69 8.44 1.35 3.30 2.30 3.60 4.90 Instant Heart Rate -Pro 3044 5 1927 \n6.49 7.79 6.43 2.07 1.14 6.54 7.84 Pandora internet radio 13704 7 7620 2.88 2.01 1.44 2.07 3.24 2.75 \n2.18 PicSay -Photo Editor 1580 0 1580 2.97 3.04 2.66 2.59 4.37 1.39 2.97 Shazam 22071 13 9884 9.40 7.61 \n5.23 8.46 7.93 8.89 6.25 Shazam Encore 22071 9 9914 6.92 6.52 6.72 6.66 6.99 7.03 9.24 WeatherBug 9581 \n10 7948 3.70 8.02 3.11 3.82 4.52 3.93 5.75 WeatherBug Elite 9688 8 8194 5.06 4.24 6.36 3.41 6.12 5.89 \n3.83 YouTube 19902 10 11125 4.85 5.01 2.04 3.08 3.86 2.83 5.12 ZEDGE 8308 11 6287 6.96 3.00 4.82 6.44 \n7.32 8.44 5.75    \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Systematic exploration of Android apps is an enabler for a variety of app analysis and testing tasks. Performing the exploration while apps run on actual phones is essential for exploring the full range of app capabilities. However, exploring real-world apps on real phones is challenging due to non-determinism, non-standard control flow, scalability and overhead constraints. Relying on end-users to conduct the exploration might not be very effective: we performed a 7-use study on popular Android apps, and found that the combined 7-use coverage was 30.08% of the app screens and 6.46% of the app methods. Prior approaches for automated exploration of Android apps have run apps in an emulator or focused on small apps whose source code was available. To address these problems, we present A<sup>3</sup>E, an approach and tool that allows substantial Android apps to be explored systematically while running on actual phones, yet without requiring access to the app's source code. The key insight of our approach is to use a static, taint-style, dataflow analysis on the app bytecode in a novel way, to construct a high-level control flow graph that captures legal transitions among activities (app screens). We then use this graph to develop an exploration strategy named <i>Targeted Exploration</i> that permits fast, direct exploration of activities, including activities that would be difficult to reach during normal use. We also developed a strategy named <i>Depth-first Exploration</i> that mimics user actions for exploring activities and their constituents in a slower, but more systematic way. To measure the effectiveness of our techniques, we use two metrics: activity coverage (number of screens explored) and method coverage. Experiments with using our approach on 25 popular Android apps including BBC News, Gas Buddy, Amazon Mobile, YouTube, Shazam Encore, and CNN, show that our exploration techniques achieve 59.39--64.11% activity coverage and 29.53--36.46% method coverage.</p>", "authors": [{"name": "Tanzirul Azim", "author_profile_id": "83358746257", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P4290430", "email_address": "mazim002@cs.ucr.edu", "orcid_id": ""}, {"name": "Iulian Neamtiu", "author_profile_id": "81100589658", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P4290431", "email_address": "neamtiu@cs.ucr.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509549", "year": "2013", "article_id": "2509549", "conference": "OOPSLA", "title": "Targeted and depth-first exploration for systematic testing of android apps", "url": "http://dl.acm.org/citation.cfm?id=2509549"}