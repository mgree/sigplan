{"article_publication_date": "10-29-2013", "fulltext": "\n Class Hierarchy Complementation: Soundly Completing a Partial Type Graph George Balatsouras Yannis \nSmaragdakis Department of Informatics University of Athens, 15784, Greece {gbalats,smaragd}@di.uoa.gr \nAbstract We present the problem of class hierarchy complementa\u00adtion: given a partially known hierarchy \nof classes together with subtyping constraints ( A has to be a transitive sub\u00adtype of B ) complete the \nhierarchy so that it satis.es all con\u00adstraints. The problem has immediate practical application to the \nanalysis of partial programs e.g., it arises in the process of providing a sound handling of phantom \nclasses in the Soot program analysis framework. We provide algorithms to solve the hierarchy complementation \nproblem in the single inheritance and multiple inheritance settings. We also show that the problem in \na language such as Java, with single in\u00adheritance but multiple subtyping and distinguished class vs. \ninterface types, can be decomposed into separate single-and multiple-subtyping instances. We implement \nour algorithms in a tool, JPhantom, which complements partial Java byte\u00adcode programs so that the result \nis guaranteed to satisfy the Java veri.er requirements. JPhantom is highly scalable and runs in mere \nseconds even for large input applications and complex constraints (with a maximum of 14s for a 19MB binary). \nCategories and Subject Descriptors F.3.2 [Logics and Meanings of Programs]: Semantics of Programming \nLanguages Program Analysis; D.1.5 [Programming Techniques]: Object-oriented Programming General Terms \nAlgorithms, Languages, Performance Keywords type hierarchy; Java; single inheritance; multi\u00adple inheritance; \nJPhantom; bytecode engineering Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright is held by the owner/author(s). Publication \nrights licensed to ACM. ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509530 \n 1. Introduction Whole-program static analysis is essential for clients that re\u00adquire high-precision \nand a deeper understanding of program behavior. Modern applications of program analysis, such as large \nscale refactoring tools [9], race and deadlock detectors [16], and security vulnerability detectors [11, \n15], are virtu\u00adally inconceivable without whole-program analysis. For whole-program analysis to become \ntruly practical, however, it needs to overcome several real-world challenges. One of the somewhat surprising \nreal-world observations is that whole-program analysis requires the availability of much more than the \nwhole program . The analysis needs an overapproximation of what constitutes the program. Further\u00admore, \nthis overapproximation is not merely what the anal\u00adysis computes to be the whole program after it has \ncom\u00adpleted executing. Instead, the overapproximation needs to be as conservative as required by any intermediate \nstep of the analysis, which has not yet been able to tell, for instance, that some method is never called. \nConsider the example of trying to analyze a program P that uses a third-party library L. Program P will \nlikely only need small parts of L. However, other, entirely separate, parts of L may make use of a second \nlibrary, L'. It is typically not possible to analyze P with a whole program analysis framework without \nalso supplying the code not just for L but also for L', which is an unreasonable burden. In modern languages \nand runtime systems, L' is usually not necessary in order to either compile P or run it under any input. \nThe problem is exacerbated in the current era of large-scale li\u00adbrary reuse. In fact, it is often the \ncase that the user is not even aware of the existence of L' until trying to analyze P. Unsurprisingly, \nthe issue has arisen before, in di.erent guises. The FAQ document1 of the well-known Soot frame\u00adwork \nfor Java analysis [19, 20] contains the question: How do I modify the code in order to enable soot to \ncontinue loading a class even if it doesn t .nd some of it[s] references? Can I create a dummy soot class \nso it can continue with the load? How? 1 http://www.sable.mcgill.ca/soot/faq.html This frequently asked \nquestion does not lead to a solution. The answer provided is: You can try -use-phantom-refs but often \nthat does not work because not all analyses can cope with such references. The best way to cope with \nthe problem is to .nd the missing code and provide it to Soot. The phantom refs facility of Soot, referenced \nin the above answer, attempts to model missing classes (phan\u00adtom classes) by providing dummy implementations \nof their methods referenced in the program under analysis. However, there is no guarantee that the modeling \nis in any way sound, i.e., that it satis.es the well-formedness requirements that the rest of the program \nimposes on the phantom class. Our research consists precisely of addressing the above need in full generality. \nGiven a set of Java class and inter\u00adface de.nitions, in bytecode form, we compute a program complement \n, i.e., skeletal versions of any referenced miss\u00ading classes and interfaces so that the combined result \nconsti\u00adtutes veri.able Java bytecode. Our solution to this practical problem has two parts: A program \nanalysis part, requiring analysis of bytecode and techniques similar to those employed by the Java ver\u00adi.er \nand Java decompilers. This analysis computes con\u00adstraints involving the missing types. For instance, \nif a vari\u00adable of a certain type C is direct-assigned to a variable of a type S , then C must be a subtype \nof S .  An algorithmic part, solving a novel typing problem, which we call the class hierarchy complementation, \nor simply hierarchy complementation, problem. The problem consists of computing a type hierarchy that \nsatis.es a set of subtyping constraints without changing the direct par\u00adents of known types.  The algorithmic \npart of our solution, i.e., solving the hierarchy complementation problem, constitutes the main novelty \nof our approach. The problem appears to be fun\u00addamental, and even of a certain interest in purely graph\u00adtheoretic \nterms. For a representative special case, consider an object-oriented language with multiple inheritance \n(or, equivalently, an interface-only hierarchy in Java or C#).2 A partial hierarchy, augmented with constraints, \ncan be repre\u00adsented as a graph, as shown in Figure 1a. The known part of the hierarchy is shown as double \ncircles and solid edges. Unknown (i.e., missing) classes are shown as single circles. Dashed edges represent \nsubtyping constraints, i.e., indirect 2 We avoid the terms subclassing or inheritance as synonyms for \ndirect subtyping to prevent confusion with other connotations of these terms. In our context, we only \ncare about the concept of subtyping, i.e., of a (monomorphic) type as a special case of another. Subtyping \ncan be direct (e.g., when a Java class is declared to extend another or implement an interface) or indirect, \ni.e., transitive. We do, however, use the compound terms single inheritance and multiple inheritance \nas they are more common in the classi.cation of languages than single subtyping and multiple subtyping \n. (a) Constraint Graph (b) Solution Figure 1: Example of constraints in a multiple inheritance set\u00adting. \nDouble-circles signify known classes, single circles signify unknown classes. Solid edges ( known edges \n) signify direct sub\u00adtyping, dashed edges signify transitive subtyping. subtyping relations that have \nto hold in the resulting hierar\u00adchy. In graph-theoretic terms, a dashed edge means that there is a path \nin the solution between the two endpoints. For in\u00adstance, the dashed edge from C to D in Figure 1a means \nthat the unknown part of the class hierarchy has a path from C to D. This path cannot be a direct edge \nfrom C to D, however: C is a known class, so the set of its supertypes is .xed. In order to solve the \nabove problem instance, we need to compute a directed acyclic graph (DAG) over the same nodes,3 so that \nit preserves all known nodes and edges, and adds edges only to unknown nodes so that all dashed-edge \nconstraints are satis.ed. That is, the solution will not con\u00adtain dashed edges (indirect subtyping relationships), \nbut ev\u00adery dashed edge in the input will have a matching directed path in the solution graph. Figure \n1b shows one such pos\u00adsible solution. As can be seen, solving the constraints (or determining that they \nare unsatis.able) is not trivial. In this example, any solution has to include an edge from B to E, for \nreasons that are not immediately apparent. Accord\u00adingly, if we change the input of Figure 1a to include \nan edge from E to B, then the constraints are not satis.able any at\u00adtempted solution introduces a cycle. \nThe essence of the algo\u00adrithmic di.culty of the problem (compared to, say, a simple topological sort) \nis that we cannot add extra direct parents to known classes A and C any subtyping constraints over these \ntypes have to be satis.ed via existing parent types. This corresponds directly to our high-level program \nrequirement: we want to compute de.nitions for the missing types only, without changing existing code. \nFor a language with single inheritance, the problem is similar, with one di.erence: the solution needs \nto be a tree 3 Inventing extra nodes does not contribute to a solution in this problem.  (a) Constraint \nGraph (b) Solution Figure 2: Example of full-Java constraint graph. Double circles de\u00adnote known classes/interfaces, \nwhose outgoing edges in the solu\u00adtion are already determined (solid input edges). White nodes are classes, \nblack nodes are interfaces, grey nodes are unknown types that are initially undetermined (i.e., the input \ndoes not explicitly identify them as classes or interfaces, although constraint reason\u00ading may do so \nlater). instead of a DAG. (Of course, the input in Figure 1a already violates the tree property since \nit contains known nodes with multiple known parents.) We o.er an algorithm that solves the problem by \neither detecting unsatis.ability or always ordering the nodes in a tree that respects all constraints. \nThe practical version of the hierarchy complementation problem is more complex. Mainstream OO languages \noften distinguish between classes and interfaces and only allow single direct subtyping among classes \nand multiple direct subtyping from a class/interface to an interface a combi\u00adnation often called single-inheritance, \nmultiple subtyping . In this case, the graph representation of the problem is less intuitive. Consider \nFigure 2a that gives a problem instance. (A possible solution for these constraints is in Figure 2b, \nbut is given purely for reference, as it is not central to our discus\u00adsion.) There are now several node \ntypes: classes, interfaces (both known and unknown), as well as undetermined nodes. There are also more \nimplicit constraints on them: classes can only have an edge to one other class, interfaces can only have \nedges to other interfaces. The latter constraint, for instance, forces D to be an interface and H to \nbe a class. Thus, we see that the full version of the problem requires additional reasoning. We show \nthat such reasoning can be performed as a pre-processing step. The problem can be subsequently broken \nup into two separate instances of the aforementioned single-and multiple-inheritance versions of hierarchy \ncom\u00adplementation. In brief, the contributions of our work are as follows:  We introduce a new typing \nproblem, motivated by real\u00adworld needs for whole program analysis. To our knowl\u00adedge, the hierarchy complementation \nproblem has not been studied before, in any context.  We produce algorithms that solve the problem in \nthree di.erent settings: single inheritance, multiple inheritance, and mixture of the two, as in Java \nor C#.  We implement our algorithms in JPhantom: a practical tool for Java program complementation that \naddresses the soundness shortcomings of previous Java phantom class approaches. We show that JPhantom \nscales well and takes only a few seconds to process even large benchmarks with complex constraints e.g., \nless than 6sec for a 3.2MB bi\u00adnary that induces more than 100 constraints.  We discuss the problem of \nhierarchy complementation in more general settings. The simplicity of our approach is a result of only \nassuming (for the input) and satisfying (for the output) the fairly weak Java bytecode requirements. \nWe show that the problem becomes harder at the level of the type system for the source language.  2. \nMotivation and Practical Setting We next discuss the practical setting that gives rise to the hierarchy \ncomplementation problem. Our interest in hierarchy complementation arose from ef\u00adforts to complement \nexisting Java bytecode in a way that sat\u00adis.es the soundness guarantees of the Java veri.er. Consider \na small fragment of known Java bytecode and the constraints it induces over unknown types. (We present \nbytecode in a slightly condensed form, to make clear what method names or type names are referenced in \nevery instruction.) In this code, classes A and B are available, while types X, Y, and Z are phantom, \ni.e., their de.nition is missing. public void foo(X, Y) 0: aload_2 // load on stack 2nd argument (of \ntype Y) 1: aload_1 // load on stack 1st argument (of type X) 2: invokevirtual X.bar:(LA;)LZ; //method \nZ bar(A) in X 3: invokevirtual B.baz:()V; //method void baz() in B ... The instructions of this fragment \ninduce several con\u00adstraints for our phantom types. For instance: X has to be a class (and not an interface) \nsince it contains a method called via the invokevirtual bytecode instruction.  X has to support a method \nbar accepting an argument of type A and returning a value of type Z.  Y has to be a subtype of A, since \nan actual argument of declared type Y is passed to bar, which has a formal parameter of type A. This \nconstraint also means that if A is known to be a class (and not an interface) then Y is also a class. \n  Z has to be a subtype of B, since a method of B is invoked on an object of declared type Z (returned \non top of the stack by the earlier invocation). The goal of our JPhantom tool is to satisfy all such \ncon\u00adstraints and generate de.nitions of phantom types X, Y, and Z that are compatible with the bytecode \nthat is available to the tool (i.e., exists in known classes). Compatibility with exist\u00ading bytecode \nis de.ned as satisfying the requirements of the Java veri.er, which concern type well-formedness. Of \nthese constraints, the hardest to satisfy are those in\u00advolving subtyping. Constraints on members (e.g., \nX has to contain a Z bar(A) ) are easy to satisfy by just adding guarantee makes a valuable contribution: \nit is much better to analyze a partial program in a way such that the Java ver\u00adi.er requirements (for \ntype-level well-formedness) are sat\u00adis.ed than to ignore any correctness considerations, as past approaches \ndo. 3. Hierarchy Complementation for Multiple Inheritance We begin with a modeling of the hierarchy \ncomplementation problem in the setting of multiple inheritance. This means that every class in our output \ncan have multiple parents. We can model our problem as a graph problem. Our input . is a directed graph \nG means that the problem in the core of JPhantom is solving nodes V Vknown type-correct dummy members \nto the generated classes. This = (V, E), with two disjoint sets of the class hierarchy complementation \nproblem, as presented E = Edirect in the introduction and de.ned rigorously in later sections. The binding \nof the problem to practical circumstances de\u00adserves some discussion, however. First, note that, in our \nsetting of the problem, we explic\u00aditly disallow modi.cation of known code, e.g., in order to re\u00admove \ndependencies, or to add a supertype or a member to it. Such modi.cations would have a cascading e.ect \nand make it hard to argue about what properties are really preserved. Additionally, we do not assume \nany restrictions on the in\u00adput, other than the well-formedness condition of being legal Java bytecode \n(according to the veri.er). Strictly speaking, our well-formedness condition for the input is de.ned \nas fol\u00adlows: a legal input is bytecode that can be complemented (by only adding extra class and interface \nde.nitions) so that it passes the Java veri.er. Note that this well-formedness con\u00addition does not depend \non the program complement that our approach produces: an input is legal if there is some comple\u00adment \nfor it, not necessarily the one that JPhantom computes. A .nal interesting point concerns the practical \nimpact of the JPhantom soundness condition. For most program analy\u00adses, omitting parts of the code introduces \nunsoundness, if we make no other assumptions about the program or the omitted part. E.g., it is impossible \nto always soundly compute points\u00adto information, or may-happen-in-parallel information when part of the \nprogram is missing. Therefore, guaranteed sound\u00adness for all clients is inherently unachievable for any \npartial program analysis approach. The practical reality is that there is a large need for facilities \nfor handling partial programs. For instance, the Soot phantom class machinery has been one of the most \ncommon sources of discussion and questions on the Soot support lists, and it has been a central part \nof several Soot revisions.4 The only correctness condition that Soot phantom class support is trying \nto achieve, however, is the low-level the analyzer should not crash . Given the practical interest for \nthe solution of a worst\u00adcase unsolvable problem, we believe that our soundness 4 Even the most recent \nSoot release, 2.5.0, lists improved support for phan\u00adtom classes and excluding methods from an analysis \nas one of the major changes in the release notes. . E , where E V V (i.e., direct . \u00d7path direct known \nVphantom and two disjoint sets of edges edges have to originate from known nodes the converse is not \ntrue, as known nodes can be inferred to subtype unknown ones due to assignment instructions in the bytecode). \nThe set of nodes V is a set of types, while the set of edges E corresponds to our subtyping constraints. \nThat is, an edge (vs, vt) encodes the constraint vs <: vt. The Edirect subset encodes the direct-subtype \nconstraints. The output of our algorithm should be a DAG (with edges from children to their parents), \nGD = (V, E'), such that: 1. .vs . Vknown : (vs, vt) . E' . (vs, vt) . Edirect (i.e., all direct edges \nfrom known nodes are preserved and no new ones are added to such nodes) 2. (vs, vt) . Epath . there \nis a path from vs to vt in GD  Note that our only limiting constraint here is that we cannot have cycles \nin the resulting hierarchy. Moreover, since each type may have multiple supertypes in this setting, a \ndirected acyclic graph is .tting as our intended output. In contrast to the general case, the problem \nis trivial if we have a phantom-only input, i.e., if we ignore Vknown and Edirect . It su.ces to employ \na cycle-detection algorithm, and if no cycles are present return the input constraint graph as our solution: \nall path edges can become direct subtyping edges. If our input graph contains a cycle, then our problem \nis unsolvable. If not, our solution would probably contain some redundant edges (i.e., edges connecting \nnodes that are already connected by another path) that we could prune to minimize our output. In either \ncase, our solution would be valid w.r.t. our constraints. The problem becomes much more interesting when \nwe take Vknown into account. The source of the di.culty is the combination of cycle detection with nodes \nwhose outgoing edge set cannot be extended. Consider .rst the pattern of Figure 3. This pattern is a \nbasic instance of interesting reasoning in the case of multiple inheritance. We have A . Vknown such \nthat (A, B), (A, C), (A, D) . Edirect and (A, E) . Epath . We cannot, however, satisfy the path ordering \nconstraint by adding edges to the known node A. Therefore the output =  Figure 3: In any solution of \nthese constraints, either B or C or D have to be ordered below E, since no new outgoing edges can be \nadded to A and the path constraint to E needs to be satis.ed. Figure 4: The phantom projection set of \nA is {C, E, H}. In order to satisfy path-edge (A, B) we can either add a path-edge (C, B), (E, B), or \n(H, B). The last one creates a cycle. must have one of B, C, D ordered below E. We refer to the set of \n{B, C, D} as the projection set of node A, which is a more generally useful concept. De.nition 3.1. Projection \nSet. A node t . Vphantom belongs to the projection set of a node s . Vknown i. t is reachable from s \nthrough a path of direct edges. proj(s) = {t . Vphantom : (s, t) . Edirect +} with the + symbol denoting \ntransitive closure. That is, for each known node we can follow its outgoing direct-edges recursively, \nending each path when we reach a phantom node. For instance, in Figure 4, the phantom projection set \nfor node A is {C, E, H}. Referring again to Figure 4, we can see that if H is chosen from the projection \nset of A in order to satisfy the path-edge (A, B), and therefore edge (H, B) is added, then this would \nimmediately create a cycle because of the existing (B, H) edge. Our algorithm should prevent such a cycle \nby making the correct choice from the relevant projection set. Combining this projection set choice with \ncycle detection leads to interesting search outcomes. Figure 5a shows an example of unsatis.able input. \nThe path edge (B, D) makes either E or F be subtypes of D, and similarly the path edge (A, C) makes either \nE or F be subtypes of C. Nevertheless, any choice leads to cycles. In contrast, Figure 5b shows an input \nfor which a solution is possible, and which we use to illustrate our algorithm.  (a) Unsatis.able input. \n(b) Satis.able input. Figure 5: Multiple Inheritance Examples Algorithm 3.1 solves in polynomial time \n(an easy bound is O(|V|\u00b7|E|)) any instance of the hierarchy complementation problem in the multiple inheritance \nsetting. The main part of the algorithm is function stratify(), which computes a strat\u00adi.cation with \nthe property that any constraint edge is facing upwards (i.e., from a lower to a higher stratum). Moreover, \nthis strati.cation ensures that, for any path-edge (s, t) origi\u00adnating from a known node, there will \nexist a phantom node p in the projection set of s that is placed lower than t. Given this strati.cation, \nit is easy to compute the .nal solution (as in function solve()). To satisfy any such path-edge (s, t), \nwe add a direct-edge from p to t. This respects our invariant of all edges facing upwards, thus ensuring \nthat no cycles will be present in our solution. Function stratify() starts from a single stratum, and \nthen computes on each iteration a new strati.cation, S i+1, by building on the strati.cation of the previous \nstep, S i, and advancing some nodes to a higher stratum in order to sat\u00adisfy constraints. This process \nis repeated until we converge to the .nal strati.cation, which will respect all of our con\u00adstraints (line \n21). If no new node converges at some step (i.e., all nodes that reached a certain stratum advance to \nthe next), then we can be certain that we are dealing with un\u00adsatis.able input, and terminate, thus avoiding \nin.nite recur\u00adsion (line 23). The nodes to be advanced at each step are determined at line 18, which \ncaptures the essence of the al\u00adgorithm. The new stratum of a node t will be either (i) its current stratum, \n(ii) the stratum right above the source of an edge (s, t), or (iii) the one right above the lowest projection \nnode of the source of a path-edge (s, t) originating from a known node whichever is higher. These conditions \nraise the stratum of a node to the minimum required to satisfy the natural constraints of the problem, \nper our above discussion: edges in the solution should be from lower to higher strata. Figure 6 presents \nan illustration of the algorithm s ap\u00adplication to the example of Figure 5b. The sets {C, D} and {E, \nF} are the projection sets of nodes A and B respectively.  (a) Step 1 (b) Step 2 (c) Step 3 (d) Step \n4 (e) Step 5 (f) Step 6 Figure 6: An example of the strati.cation produced by the multiple-inheritance \nsolver for Example 5b. Algorithm 3.1 Multiple-inheritance solver 1: function solve(G = (V, E)) 2: S . \nstratify(G) 3: U . {(s, t) . Epath : s . Vknown} 4: ES . E \\ U 5: for all (s, t) . U do 6: let p . proj(s) \n: S [p] < S [t] 7: ES . ES . {(p, t)} 8: end for 9: return ES 10: end function 11: function stratify(G \n= (V, E)) 12: U . {(s, t) . Epath : s . Vknown} 13: for all t . V do 14: S 0[t] . 0 15: end for 16: for \ni = 0 . |V| - 1 do 17: for all t . V do . S i[t] . . max {1 + S i[s]} 18: S i+1[t] . max . (s,t).E . \n. . .max {1 + min {S i[p]}}. (s,t).U p.proj(s) 19: end for 20: if .v . V : S i+1[v] = S i[v] then 21: \nreturn S i t reached a .xpoint 22: else if .v . V : S i+1[v] = S i[v] . S i[v] = S i-1[v] then 23: break \nt no progress made on this step 24: end if 25: end for 26: return error t unsolvable constraint graph \n27: end function t such p always exists . At the .rst step, all nodes will be placed in the lowest stra\u00adtum. \nNote that, at this point, all nodes could be placed in topological order: Figure 6a is perfectly valid \nas the output of a topological sort. However, this is not a solution by our standards, since node A cannot \nsatisfy the edge to G because both of its projection nodes, D and C, are placed after G. Adding an edge \nfrom either one would be subject to creat\u00ading cycles. At the next step, our algorithm advances every \nnode except A and B, since all are edge targets. At step 3, things become more interesting. Nodes D, \nC have to be ad\u00advanced by the same criterion, since node H contains edges to both, and they all reside \nin the same stratum at step 2. However, nodes H and G have to be advanced for a di.erent reason, since \nthey are targets of path-edges originating from known nodes, namely A and B, whose projections ({D, C}and \n{E, F} respectively) were on the second stratum during the previous step. At step 4, this condition ceases \nto exist for node H, since nodes E, F have stabilized at a lower stratum. This in turn causes node D \nto stabilize at step 5. At step 6, G can also stay put, since it is in a higher stra\u00adtum than the lowest \nprojection of A, namely D. No nodes are advanced at step 7 (which is omitted in Figure 6), thus signi\u00adfying \nthat our strati.cation has successfully converged to its .nal form. It is therefore simple to compute \na solution, by adding edges (H, D), (H, C), (G, C), (D, G) and either (F, H) or (E, H) to the direct-edges \n(A, C), (A, D), (B, E), (B, F). This set of edges will constitute our .nal solution. It is also easy \nto see that our algorithm would soundly detect that the example of Figure 5a is unsatis.able. At the \n.rst step, only known nodes A, B would remain in the lowest stratum, but on the next iteration all remaining \nnodes would advance again, thus triggering the condition of failure (line 23), since an iteration passed \nwith no progress made. A detailed proof of the correctness of our algorithm can be found in Appendix \nA. 4. Hierarchy Complementation for Single Inheritance The problem for a single inheritance setting has \na very sim\u00adilar statement as in the earlier case of multiple inheritance, but markedly di.erent reasoning \nintricacies and solution ap\u00adproaches, due to a newly arising constraint: every class in this setting \ncan only have a single parent. Formally, our problem is modeled in much the same way as before. Our input \nis again a directed graph G = (V, E), with two disjoint sets of nodes V = Vknown .Vphantom and two \ndisjoint sets of edges E = Edirect .Epath , where Edirect . Vknown \u00d7 V. The di.erence is that the output \nof our algorithm should be a directed tree (instead of a DAG), GT = (V, E'), such that the same conditions \nas in the earlier case are satis.ed: 1. .vs . Vknown : (vs, vt) . E' . (vs, vt) . Edirect 2. (vs, vt) \n. Epath . there is a path from vs to vt in GT Without loss of generality, we assume that there exists \na root node nr . Vknown that is a common supertype for all of our types. If no such type exists, we can \ncreate an arti.cial one, by adding extra constraint edges. In this way, we can be certain that computing \na graph with a single outgoing edge for all nodes (but one) will form a tree instead of a forest. The \nproblem is quite hard in its general setting. There are several patterns that necessitate a complex search \nin the space of possibilities. Figures 7a-7d show some basic pat\u00adterns that induce complex constraints. \nAll nodes reachable from a single one need to be linearly ordered (Figure 7a shows the simplest case). \nThis requires computing an or\u00addering (i.e., guessing a permutation) of these nodes. Other constraints \ncan render some of the permutations invalid. The basic pattern behind such restrictions is that of Figure \n7b: there are hierarchies that cannot be related. Combining the two patterns suggests that there needs \nto be a search in the space of permutations for a valid one: Figures 7c and 7d show some simple cases. \nComposing such constraints into more complex hierar\u00adchies gives an idea of the di.culty of the search \ninvolved. Figure 8 shows an example where it is hard to see without complex reasoning which of the E, \nF, G nodes have to be placed above A and which cannot. Clearly the problem can be modeled as a constraint \nsat\u00adisfaction problem instance, where Vphantom is our set of vari\u00adables and V is the domain of values \n(representing the vari\u00adable s direct supertype). The path-edges and the absence of cycles constitute \nour constraints. This requires an exponen\u00adtial search in the worst case. Indeed, our implementation per\u00ad \n (a) B and C (b) D and E cannot must be subtype-be subtype-related. related (in either (c) A has to \nbe a subtype (d) A has to be a subtype of ei\u00adof F. ther G or H. Figure 7: Single Inheritance Basic Patterns \n Figure 8: Harder composite example of single-inheritance con\u00adstraints. The (undirected) path from B \nto C through E, F, G implies that (A <: E) . (A <: F) . (A <: G). However, since F is the .rst common \nknown supertype of M and N, and A just a supertype of both, F <: A, and thus (A <: E) . (A <: G). forms \nprecisely such an exhaustive search, but with a heuris\u00adtic choice of nodes so that the search tries to \nsatisfy the con\u00adstraints introduced by the patterns in Figures 7a and 7b i.e., the pattern of Figure \n7a is identi.ed, all induced permuta\u00adtions are tried, and the pattern of Figure 7b is used to prune them \neagerly, instead of waiting to detect failure later. Most importantly, our approach provides special \nhandling for a simple but practically quite common case. In this spe\u00adcial case, there is a polynomial \nalgorithm for solving the problem and exhaustive search is avoided. Simpli.ed setting: No direct-edges \nto phantom nodes. It is easy to solve the problem in the case that there are no di\u00ad 1: function solve(G \n= (V, E)) 2: let R be the root node of V 3: let S be the tree of known nodes (Vknown , Edirect ) 4: for \nall (s, t) . Epath : s . Vknown do 5: if 7path s . t in S then 6: return error (unsatis.able constraint) \n7: end if 8: Epath . Epath \\ {(s, t)} t remove already satis.ed edge 9: end for 10: for all v . Vphantom \ndo 11: makeSet(v) t create single-element disjoint sets 12: end for 13: for all (s, t) . Epath : t . \nVphantom do 14: union(s, t) t merge two connected (phantom) components 15: end for t result: undirected \nconnected components (UCCs) 16: for all v . Vphantom do 17: k . find(v) 18: top[k] . R t initially root \n19: end for t init UCC s lowest common known superclass (LCS) 20: for all (s, t) . Epath : t . Vknown \ndo t must be s . Vphantom 21: k . find(s) 22: if . path t . top[k] in S then 23: top[k] . t t lower superclass \nfound, update LCS 24: else if 7 path top[k] . t in S then 25: return error (unsatis.able constraint) \n26: end if 27: end for 28: for all k . v in top do t for each UCC and its LCS 29: U . {(s, t) . Epath \n: t . Vphantom . find(s) = k}t directed subgraph of original over nodes of this UCC 30: L . a topological \norder of U t linearize subgraph 31: hd . the top node of L 32: S . S . L . {(hd, v)} 33: end for 34: \nreturn S 35: end function rect edges from known nodes to phantom nodes. Since we are in a single-inheritance \nsetting, this means that no class in the known part of the program has a superclass in the com\u00adplement \nthat we are trying to produce. In this case, we have that Edirect . Vknown \u00d7 Vknown . The extra condition \nallows us to employ a fast polynomial time algorithm. This interesting case of our problem is very common \nin practice. Intuitively, the ease of dealing with this case stems from avoiding the search in the space \nof permutations when the input contains patterns such as those in Figure 7c: if two permutations have \nelements in common (e.g., the permutation of B and F, and that of F and C in Figure 7c) they cannot include \nnodes that are guaranteed to be subtype-unrelated (such as B and C in this example) and all unknown nodes \nhave to be below the known ones in any solution. Algorithm 4.1 .rst removes path-edges originating from \nknown-nodes, after verifying that the corresponding paths  (a) Constraint Graph (b) Solution Figure \n9: Algorithm 4.1 -Example indeed exist. It then uses union/.nd data structures to com\u00adpute connected \ncomponents of phantom nodes, while treat\u00ading path-edges as undirected edges: anything connected through \nsuch edges can safely end up in a single linear or\u00addering. Then, for each phantom undirected connected \ncom\u00adponent, it computes the lowest known-node to serve as the .rst-common-supertype of all of this component \ns phantom nodes. Note that when two known-nodes are reachable by two phantom nodes of the same connected \ncomponent (in the phantom subgraph), then one of them ought to be a su\u00adpertype of the other, or else \nno solution can exist in a single inheritance setting. This condition is captured in line 24. Af\u00adter \nthe .rst common (known) supertype for every connected component has been computed, a mere topological \nsort, i.e. placing all relevant nodes in a total order, is enough to sat\u00adisfy all of this component s \nconstraints. This may introduce many super.uous edges in the solution: these edges are not actually required \nby our constraints (since a topological or\u00adder is a total order). In practice, we produce a partial order \nby using a variant of topological sort that generates a tree instead of a list as its result, but a full \ntopological sort also satis.es the correctness requirements of the algorithm. (We return to the topic \nof why we actually want a weaker order\u00ading in Section 5.) In the example of Figure 9, Algorithm 4.1 .rst \nchecks and removes the (F, A) path-edge. Then the phantom nodes are divided in the following phantom \nconnected components: {G, H, I}, {J, K, L}, and {M}. The .rst common known su\u00adpertype for each component \nis B, F, and F respectively. Each component is then linearized, which generates the fol\u00adlowing complete \norders that are appended to the output: I <: H <: G <: B, K <: J <: L <: F, and M <: F. 5. Single Inheritance, \nMultiple Subtyping: Classes and Interfaces It is easy to combine the single-and multiple-inheritance \napproaches of the last two sections in the context of a lan\u00adguage that has single inheritance but multiple \nsubtyping. It is a common case for strongly-typed languages to allow mul\u00adtiple inheritance only for a \nsubset of types. Java and C# in\u00adterfaces [10, 12], and Scala traits [17] are such examples. In order \nto support such a separation, we have to intro\u00adduce a new dimension to our problem that can be simulated \nas a graph coloring variant. Each node in V can be assigned a color denoting its inheritance type. A \nblack node can have many direct supertypes (i.e., multiple inheritance), while a white node can only \nhave one (i.e., single inheritance). We will use the terms white node (resp. black node ) and class (resp. \ninterface ) interchangeably. Note that, initially, our input may not fully determine the .nal color for \neach of its types. Thus, we have to introduce a new color (grey) to refer to the subset of nodes whose \ncolor is yet undetermined. In the end, our solution should soundly determine a safe color (black or white) \nfor each of the (grey) input nodes, so that no constraints of the veri.er will be violated. Therefore, \nour solution in this new setting is a synthesis of a single inheritance and a multiple inheritance solution. \nThat is, the output of our algorithm should be a DAG that satis.es the same conditions as those in the \nmultiple inheritance setting, GS = (V, E'), and a function fc : V . {black, white}, such that the restriction \nof GS to {v . V : fc(v) = white} (i.e., white nodes) is a tree. To safely decompose our problem into \ntwo di.erent sub\u00adproblems (one for single and one for multiple inheritance), we assign colors to all \nnodes as a preprocessing step. There are two kinds of constraints that lead to restricting the col\u00adors \nof a node. First, we have local constraints: we may get a node color from the initial input i.e., an \nobserved bytecode instruction (such as invokeinterface) may directly restrict the color of a phantom \ntype. (More constraints of this form are discussed in Section 6.) Second, we may get transitive constraints, \ndue to restrictions on subtyping. Interfaces can only subtype interfaces (except for the Object class \nin Java). This leads to two types of transitive constraints: If a black node s has a path to node t, \nthen t must also be black (inter\u00adfaces can only extend interfaces). Symmetrically, if a node s has a \npath to a white node t, then s must also be white (classes can only be extended by other classes). Furthermore, \nphantom nodes with no color constraints can be safely assumed to be interfaces (black), for maximum .exibility \nin solving other constraints. It is always easier to satisfy a given set of constraints in a multiple \ninheritance setting instead of a single inheritance setting, since the con\u00additions of single inheritance \nare stricter (a tree is a DAG). As a result of the above observations, we can color all nodes by applying \nlocal or transitive constraints to the orig\u00adinal input before solving a single and a multiple inheritance \nhierarchy complementation problem separately. That is, we can follow every possible path from any node \nwhose color has already been set and mark the nodes we .nd along the way accordingly. The color of our \nsource node determines the direction of movement (i.e., from white source nodes, we have to go backwards). \nWhen this process is over, we can as\u00adsign the color black to all remaining undetermined (in terms of \ncolor) nodes. An example of this process can be seen in our earlier Figure 2. Once we have assigned a \nblack-or-white color to every node, we can split our constraint graph into two subgraphs by isolating \nwhite-to-white edges (and feed\u00ading them to a single inheritance solver). After we have de\u00adtermined our \nclass hierarchy, we can proceed with satisfying the rest of the edges using multiple inheritance rules. \n The key to this approach is that the single inheritance solver does not need the output of the multiple \ninheritance solver to compute a solution, and vice versa. All we need to ensure (for the multiple inheritance \nsolver) is that we take into account class supertypes that are reachable through di\u00adrect edges of a known \nclass when determining the class s projection set. Thus, the class/interface decomposition in\u00addeed produces \ntwo independent subproblems that can be solved separately. The composition of the two solutions will \ncertainly not create any cycles, if its two subparts do not con\u00adtain any. If that was not the case, then \nthere would be a cycle that contained at least one class and one interface, which is impossible since \nno interface can be a subtype of a class (other than Object) in Java. As for our arbitrary choice of \ndefaulting undetermined nodes to interfaces, suppose that a solution exists if a sub\u00adset U of those undetermined \nnodes were treated as classes. We could then transform this solution to another one where these nodes \nwere interfaces instead. The single inheritance solution could be produced by replacing each node in \nU with its parent (in the former single inheritance solution), w.r.t. its incoming edges, and then removing \nit, until no nodes in U were present. This process would still satisfy all constraints on the remaining \nclass nodes. A multiple in\u00adheritance solution also exists. Consider the union of the for\u00admer multiple \nplus single inheritance solution. The result is a DAG that respects all of the multiple inheritance setting \ncon\u00adstraints. Again, we can erase any edges to class-determined nodes (i.e., all class nodes that are \nnot in U) in a way that all subtype relations involving the rest of the nodes remain unaltered, i.e., \nby iteratively replacing an edge to a class\u00addetermined node with edges to all of its direct supertypes, \nuntil no edges to class-determined nodes are left. This pro\u00adcess would yield a valid multiple inheritance \nsolution that can be safely combined with the single inheritance one. Therefore, marking undetermined \nnodes as interfaces does not a.ect the outcome of our algorithm, i.e., no solution will be found if and \nonly if no solution existed. 6. Implementation and Practical Evaluation We next discuss practical aspects \nof our implementation. First, we consider the program analysis part of our work, which solves the problem \nof producing complements of a partial Java program by appealing to the solver of the class hierarchy \ncomplementation problem. Subsequently, we present experiments applying our JPhantom tool to real pro\u00adgrams. \n 6.1 JPhantom Implementation JPhantom is a practical and scalable tool for program com\u00adplementation, \nbased on the algorithms we have presented in this paper. 5 JPhantom uses the ASM library [7] to read \nand trasform Java bytecode. Given a jar .le that contains phan\u00adtom references, it produces a new jar \n.le with dummy im\u00adplementations for each phantom class. The resulting jar .le satis.es all formal constraints \nof the JVM Speci.cation [14]. We give a brief explanation of the di.erent stages of compu\u00adtation for \nthe analysis of an input jar .le by JPhantom. JPhantom execution consists of the following steps. It \n(1) performs a .rst pass over the jar contents in order to recreate the existing class hierarchy (type \nsignatures only) and store the .eld and method declarations of the contained classes, then (2) makes \na second pass to extract all phantom refer\u00adences and store the full class representations. A third pass \n(3) extracts all relevant type constraints, before (4) they are fed to JPhantom s hierarchy complementation \nsolver, which computes a valid solution, if such a solution is possible. At this point, we can proceed \nto (5) bytecode generation, where we create new class .les for our missing (phantom) types. Finally, \nwe compute method bodies to add to each type. For instance, when the solver determines that a phantom\u00adclass \ntype X must implement an interface type Y, all missing methods of Y should be added to X, so that the \nresulting bytecode is valid. After all such methods have been com\u00adputed, they are added in the last (6) \nstep of execution. Phantom references include references to missing classes, as well as references to \nmissing .elds and methods. Note that both phantom and existing classes may have references to missing \nmembers, since there are cases of existing classes calling a method or referencing a .eld declared in \none of their phantom supertypes. JPhantom detects all such refer\u00adences and adds the relevant missing \ndeclarations to its out\u00adput. If a member is missing from a phantom class, we add it directly to that \nclass as part of JPhantom s output. Otherwise, if a member is missing from an existing class, we add \nit to an appropriate phantom supertype in its projection set instead. We encode these declarations as \nadditional constraints over the missing classes, generated in the second step of JPhan\u00adtom s execution. \nIt su.ces to use the existing class hierarchy and declared members (step 1), to perform member lookup \n5 JPhantom is available online at https://github.com/gbalats/ jphantom. for the purpose of determining \nif a member is missing and where it should be added. The most interesting aspects of the above steps \nhave to do with analyzing the bytecode to produce the constraints (step 3) used as input to the hierarchy \ncomplementation algorithm. In order to extract type constraints, we have to simulate a symbolic execution \nof Java bytecode by following every possible execution path, while computing the types of stack and local \nvariables. This is necessary because, in general, bytecodes receive some untyped arguments whose types \nwe need to infer, in order to extract our constraints. This process is analogous to Pass 3 [14, Section \n4.9.2] of the bytecode veri.cation process. When computing such type information for stack and local \nvariables, there are points where we have to merge two di.erent paths of execution. That is, the two \npaths may map the same variable to di.erent types, in which case we have to merge two di.erent types \ninto a new one. Typically, when merging two types A, B the resulting type is the .rst common superclass \nof A and B. In Java, there always exists such a common superclass since every reference type (interfaces \nincluded) is a subtype of java.lang.Object. In our case, however, since we do not have the complete type \nhierarchy at the time of constraint extraction, we cannot compute the .rst common superclass for any \ntwo nodes. This is why we apply the alternative technique of storing sets of reference types, as presented \nin alternative veri.er designs [18]. I.e., our bytecode analyzer stores not a single type, but a set \nof types for each variable at every point of execution. Figure 10 lists the constraints that may be generated \nby the analyzer for certain bytecodes. Since our analyzer generates constraints due to widening reference \nconversions, it is easy to see that storing a set of reference types .ts our needs well. Consider the \nfollowing case: class Test { A foo(B, C); A foo(B b, C c) { Code: return (b == null) ? 0: aload_1 c : \nb; 1: ifnonnull 8 } 4: aload_2 } 5: goto 9 8: aload_1 9: areturn Our analyzer will compute that the \nstack contains a sin\u00adgle item with type {B, C}, before position 9, which is the outcome of merging the \ntwo di.erent execution paths. Let us also assume that A and B are phantom classes. This toy example demonstrates \nwhy we have chosen to store sets of types, since we cannot compute the .rst common superclass of B, C. \nAfter our tool has completed the analysis of method foo(), it will generate (because of the ARETURN instruc\u00adtion) \nthe constraint B <: A . C <: A. Opcode Types Stack Types Constraints AASTORE a : E[] i : int v : V V \n<: E ARETURN obj : S S <: Rm ASTORE T obj : S S <: T ATHROW obj : S S <: java.lang.Throwable GETFIELD \nT.F obj : S isClass(T ) . S <: T PUTFIELD T.F obj : S v : U isClass(T ) . S <: T . U <: F PUTSTATIC T.F \nv : U isClass(T ) . U <: F INVOKEINTERFACE T.(A)R arg0 : S 0 arg1 : S 1 . . . isIface(T ) . S 0 <: T \nINVOKEVIRTUAL T.(A)R arg0 : S 0 arg1 : S 1 . . . isClass(T ) . S 0 <: T INVOKESPECIAL T.(A)R arg0 : S \n0 arg1 : S 1 . . . name = <init> . isClass(T ) . S 0 <: T INVOKESTATIC T.(A)R arg1 : S 1 . . . isClass(T \n) INVOKE* T.(A)R (arg0 : S 0) arg1 : S 1 . . . S i <: Ai, .i = 1, ... Figure 10: Generated Bytecode Constraints. \nAt this point, our analyzer has already computed the (sets of) types for every stack and local variable \nat every point of execution (bytecode in method). For simplicity, we assume that each set of reference \ntypes contains a single element (3rd column). Each bytecode may involve some declared types (2nd column) \nby references in the constant pool or by entries in the local variable table (if such exists). Also, \nlet Rm be the containing method s return type. 6.2 JPhantom in Practice We next detail a typical usage \nscenario of JPhantom, to\u00adgether with the complications that would arise in its absence. Consider performing \na static analysis of a large Java program. For instance, the Doop framework [6, 13] inte\u00adgrates points-to \nanalysis with call-graph construction, com\u00adputation of heap object points-to information, and various \nclient analyses (escape analysis, virtual call elimination, class cast elimination). Doop uses Soot as \na front-end and post-processes the facts generated by Soot. When faced with an incomplete program, the \nuser of the analysis is faced with various issues. To illustrate and quantify them we created a synthetic \nincomplete program, antlr-minus, by arti.cially subtracting parts of the antlr parser generator jar. \n(We also use antlr-minus as a performance benchmark in the next sec\u00adtion.) A user that tries to analyze \nantlr-minus will encounter the following issues: Crash in Soot. Earlier versions of Soot, e.g., Soot \n2.3.0, will often crash when trying to analyze a pro\u00adgram that contains phantom references. Soot provides \nthe -allow-phantom .ag, as a command-line option that the user can set to inform Soot that its input \ncontains phan\u00adtom references, and that Soot should try to handle them instead of terminating with an \nerror. However, for sev\u00aderal Soot versions the .ag is not su.cient to prevent Soot from crashing in some \ncases.  Need to handle phantom references in the client of Soot.  Although the latest version of Soot \n(2.5) has increased its tolerance of phantom references to the point where it no longer crashes, this \nonly prevents against crashes in Soot itself and does not yield any meaningful handling of phantom references. \nThe problem is propagated to the client of Soot. The client analysis (any external tool that uses Soot) \nnow needs to have special-case code for han\u00addling phantom classes, in whichever way makes sense to the \nclient. There is no evident general-purpose solution to .xing the Soot output for any client without \nadding code to deal with phantom references, essentially duplicating what JPhantom does already. In our \ncase, if the Doop front-end that reads Soot information tried to just handle phantom references as regular \nreferences, it would crash (as we have con.rmed experimentally), since it needs to encode for every variable \nits full type information (e.g., member methods). (The Doop front-end does not crash in practice because \nit handles phantom references specially, by merely ignoring them, as we discuss next.) In contrast, JPhantom \nallows any tool completely unaware of phan\u00adtom references, such as the Doop front-end, to be able to \nrun without unexpected behavior, as long as its input is .rst transformed by JPhantom. Incompleteness \nwhen analyzing with Doop. The Doop front-end is coded so that it avoids crashes but only at the cost \nof completely ignoring any reference to a phantom class. A method that takes phantom types as arguments \nis just skipped. This handling has been the default for Doop since its original version. Unfortunately, \nthis leads to incompleteness in the resulting analysis performed by Doop. Figure 11 presents a Venn diagram \nover the sets of reach\u00adable methods as computed by Doop for three di.erent inputs: (i) the original antlr \njar, (ii) our synthetic bench\u00admark, antlr-minus, and (iii) the output of JPhantom after analyzing antlr-minus, \nthat is, a transformed version of  Figure 11: A Venn diagram that shows how three di.erent sets of \nreachable methods relate to each other. These three sets (i) Original, (ii) Minus, and (iii) Minus+JPhantom \ncorrespond to the outcomes of analyzing (i) the antlr jar (original), (ii) the antlr\u00adminus jar (subset \nof the original jar), and (iii) the antlr-minus jar after being transformed by JPhantom, respectively. \nThe sets are not drawn to scale: the size of each subset is indicated only by the number in it. the antlr-minus \njar with no phantom references. The orig\u00adinal jar yields 52, 357 reachable methods, out of which only \n42, 337 are detected in the presence of phantom references (antlr-minus), without using JPhantom. Ad\u00additionally, \nphantom references introduce 500 false posi\u00adtives that correspond to non-existing methods.6 After em\u00adploying \nJPhantom to alleviate the e.ect of phantom refer\u00adences, Doop manages to .nd 7, 392 of the 10, 020 miss\u00ading \nreachable methods, resulting in 73.77% recall (over the missing methods alone, or 95% over all methods). \nThe false positives of directly analyzing antlr-minus dis\u00adappear but 681 new ones emerge, yielding a \nprecision of 98.65%. Even so, this allows us to discover almost 3 out of every 4 missing reachable methods, \nwhich originally constituted 19.14% of the total reachable methods, drop\u00adping this percentage to just \n5.02%. It is notable that this high recall is achieved although recall could, in principle, be arbitrarily \nlow. JPhantom is trying to guess the structure of missing code with as much information as remains in \nexisting code but this could be a tiny fraction of the missing information. The missing code could be \nhiding a huge portion of the application, and expose only a handful of phantom types on the unknown/known \ncode boundary. In summary, JPhantom avoids problems with crashes when encountering phantom references \nas well as incom\u00ad 6 It may seem surprising that eliminating code can introduce new (falsely) reachable \nmethods. The reason is that a non-existent method m in class C may be reported reachable, based on method \nsignature information on the call-site alone, whereas in the original code the true reachable method \nm was de.ned in a, now missing, superclass S of C, and not in C. pleteness when phantom references are \nmerely ignored. In practice, it is e.ective in discovering large parts of the in\u00adterface for missing \nmethods and the produced complement respects the requirements of the Java VM veri.er, i.e., the most \nfundamental Java well-formedness rules for types. 6.3 Performance Experiments We use a 64-bit machine \nwith a quad-core Intel i7 2.8GHz CPU. The machine has 8GB of RAM. We ran our experi\u00adments using JDK 1.7 \n(64-bit). Our benchmarks consist of (1) antlr, a parser genera\u00adtor, (2) antlrworks, the GUI Development \nEnvironment for antlr, (3) c3p0, a library that provides extensions to tradi\u00adtional JDBC drivers, (4) \njruby and (5) jython, implementa\u00adtions of Ruby and Python programming languages respec\u00adtively that run \non top of the JVM, (6) logback-classic and (7) logback-core, two modules of the logback logging frame\u00adwork, \n(8) pmd, a Java source code analyzer, (9) postgres, the PosgreSQL JDBC driver, (10) sablecc, a compiler \ngenera\u00adtor, and (11) antlr-minus, a synthetic benchmark described in the previous section. Every benchmark \nis just a jar .le that serves as JPhantom s input, which then detects all phantom references and generates \nthe complemented jar. We encountered most of these benchmarks in our own work doing static program analysis \nwith the Doop frame\u00adwork [6, 13]. For many of the benchmarks it was, upon orig\u00adinal encounter, an unexpected \ndiscovery that they could not be analyzed due to dependencies to unknown classes in other libraries. \nFigure 12 presents input features and the running time of JPhantom for each of our benchmarks. The .rst \ncolumn is the name of the benchmark. The second column is the size of the output, i.e., the complemented \njar, divided into the original size of the input (benchmark) and the size of the complement itself (i.e., \nthe size of the generated phantom classes). The third and fourth columns are the number of phantom classes \nand constraints detected respectively. The last column is the running time of JPhantom, including the \ntime to analyze the input, compute a type hierarchy that respects all of the constraints detected, create \nthe phantom classes with the required members and supertypes, augment the input jar and .ush its contents \nto disk. Even the largest benchmark (jruby) takes seconds to com\u00adplete. Moreover, the size of the input \nis highly correlated with the running time of JPhantom and much less correlated with the number of constraints. \nThis suggests that most of the time is spent on reading and analyzing the input, rather than on the type \nhierarchy solver. The only slight exception is the logback-classic benchmark, which requires about 1.8 \nseconds to complete despite its small size. This is due to the large number of phantom classes and constraints \nthis bench\u00admark produces, which is to be expected since it is built on top of logback-core (which is \nnot supplied as part of the in\u00adput). This practice of creating such a strong dependency is probably justi.ed \nby logback s design. The framework im\u00adplements the SLF4J (Simple Logging Facade for Java) pro\u00adtocol, \nwhich acts as a common interface for a variety of logging frameworks, and hides the actual framework \n(called binding) to be used underneath. From both logback-classic and antlr-minus we can see that JPhantom \nscales well as the number of constraints increases. Input jar Size Phantom Constraints Time antlr 3.3M \n+ 0.7K 1 2 4.82s antlrworks 3.5M + 2.2K 5 7 6.11s c3p0 jruby jython logback-classic logback-core pmd \npostgres sablecc 597K + 1.8K 19M + 5.9K 2.5M + 4.0K 247K + 55K 358K + 7.9K 1.2M + 11K 499K + 0 306K + \n2.3K 4 16 8 148 22 28 0 5 2 20 9 212 22 36 0 8 2.05s 13.70s 3.26s 1.76s 1.61s 2.62s 1.95s 1.59s antlr-minus \n3.2M + 17K 37 103 5.82s Figure 12: Results of experiments.  To see the constraints and their solution \nfor a benchmark instance, consider the list below, which is the actual execu\u00adtion output of a JPhantom \nrun on the jruby benchmark: Phantom Classes Detected: [constraint] org.apache.tools.ant.BuildException \nmust be a class org.apache.tools.ant.Task must be a class org.apache.tools.ant.Project org.apache.bsf.util.BSFFunctions \nmust be a class org.apache.bsf.util.BSFEngineImpl must be a class org.apache.bsf.BSFException must be \na class org.apache.bsf.BSFManager must be a class org.apache.bsf.BSFDeclaredBean must be a class org.apache.bsf.BSFEngine \norg.osgi.framework.Bundle must be an interface org.osgi.framework.BundleReference org.osgi.framework.FrameworkUtil \nmust be a class org.osgi.framework.BundleException org.osgi.framework.BundleContext must be an interface \njava.dyn.Coroutine must be a class java.dyn.CoroutineBase Constraints: org.apache.bsf.BSFException <: \nThrowable org.apache.tools.ant.BuildException <: Throwable org.osgi.framework.BundleException <: Throwable \norg.jruby.embed.bsf.JRubyEngine <: org.apache.bsf.util.BSFEngineImpl org.jruby.embed.bsf.JRubyEngine \n<: org.apache.bsf.BSFEngine org.jruby.ant.RakeTaskBase <: org.apache.tools.ant.Task org.jruby.javasupport.bsf.JRubyEngine \n<: org.apache.bsf.BSFEngine org.jruby.ext.fiber.CoroutineFiber$1 <: java.dyn.Coroutine org.jruby.javasupport.bsf.JRubyEngine \n<: org.apache.bsf.util.BSFEngineImpl Class Hierarchy * class java.lang.Object * class org.apache.bsf.BSFManager \n * class org.osgi.framework.FrameworkUtil * class Throwable (implements java.io.Serializable) * class \norg.osgi.framework.BundleException * class org.apache.tools.ant.BuildException * class org.apache.bsf.BSFException \n  * class org.apache.bsf.BSFDeclaredBean * class org.apache.bsf.util.BSFFunctions * class org.apache.tools.ant.Task \n * class org.jruby.ant.RakeTaskBase * class java.dyn.Coroutine * class org.jruby.ext.fiber.CoroutineFiber$1 \n * class org.apache.bsf.util.BSFEngineImpl (implements org.apache.bsf.BSFEngine) * class org.jruby.javasupport.bsf.JRubyEngine \n * class org.jruby.embed.bsf.JRubyEngine   Interface Hierarchy * interface org.osgi.framework.Bundle \n * interface org.apache.bsf.BSFEngine * interface java.io.Serializable * interface org.osgi.framework.BundleContext \n It is evident that the .nal hierarchy respects all of the reported constraints. Some interesting points \nare that: (i) org.apache.bsf.BSFEngine de\u00adfaults to interface since no constraint determines whether \nit is actually an interface or a class, (ii) org.osgi.framework.BundleException is inferred to be a class \nsince it is a subtype of the class Throwable, and (iii) two known classes, org.jruby.javasupport.bsf.JRubyEngine \nand org.jruby.embed.bsf.JRubyEngine, used as sub\u00adtypes of interface org.apache.bsf.BSFEngine, add the \nlatter to the supertypes of their phantom projection, org.apache.bsf.util.BSFEngineImpl. 7. Discussion \nWe next discuss the problem of hierarchy complementation speculatively, in settings di.erent from ours. \nThe general problem is that of complementing programs so that they re\u00adspect static well-formedness requirements. \nThus, the prob\u00adlem applies to language-level type systems, static analyses (e.g., complement this program \nso that it passes this anal\u00adysis, de.ned a priori ) and other settings more general than our Java bytecode \ndomain. Indeed, much of our ability to solve the problem e.ectively has to do with the simple type checking \nperformed by the Java bytecode veri.er. The veri\u00ad.er e.ectively checks monomorphic types, i.e., that \na refer\u00adence to an object is statically guaranteed to refer to memory with the expected layout. If we \nwere to transpose the problem to the domain of Java source code, the constraints to be derived are richer \nand more complex than the ones we encountered. The Java language\u00adlevel type system has intricate requirements \nrelative to over\u00adriding, casts, exceptions, and more. By way of example, we discuss some of these complications \nbelow. Exception handling at the Java language level immediately introduces very powerful constraints \nfor types. The Java language requires that a method that overrides another may throw an exception only \nif it was already declared to be thrown. Consider two methods: class S { void foo() throws A, B {...} \n} class C extends S { void foo() throws X, Y, Z {...} } The requirement in this case is hard to reason \nabout with\u00adout an exhaustive search. It can be stated as: for C.foo to be a valid overriding of S.foo, \nX, Y and Z have to be subtypes of either A or B. Consider how this rich con\u00adstraint would a.ect our ability \nto solve the hierarchy com\u00adplementation problem at the source level. Imagine that S is a known class \nwhile C, X, Y, and Z are phantom classes. If the language allowed us to infer through observation of \nother code that C is a subtype of S and that it provides a method void foo() throws X, Y, Z then in order \nto generate a complement we would need to satisfy the fol\u00adlowing: C <: S . .t . {X,Y,Z} : (t <: A . t \n<: B). In contrast, the bytecode veri.er only ensures a much simpler constraint: that a type declared \nto be thrown by a method is a subtype of Throwable. A similar kind of constraint at the Java language \nlevel is also produced by the overriding rule for return types. Java (5 and above) allows overriding \nmethods to have a covari\u00adant return type. That is, the overriding method can declare to return a subtype \nof the overridden method s return type. Much as in the case of exceptions, this induces complex constraints, \nespecially when combined with search to ex\u00adamine whether a type can be a subtype of another. Consider \nthe following case: interface S { R foo(); } // R,X,Y phantom types // we know X contains method \"Y foo()\" \nFor phantom types R and X, if some other constraint (e.g., of the kind induced in the case of multiple \ninheritance in Section 3) can be satis.ed by making X a sybtype of S, then we get the additional constraint: \nif X becomes a subtype of S then Y must be a subtype of R . This is again a very ex\u00adpressive constraint \nkind and, consequently, hard to reason about. For instance, the above constraint allows us to deter\u00admine \nthat two phantom types cannot be subtype-related. If two types declare methods with identical argument \ntypes but guaranteed-incompatible return types (e.g., void and Object), then the types are guaranteed \nto not be ordered by the subtyping relation, in either direction. At the bytecode level, subtyping together \nwith signa\u00adture conformance does not imply other subtyping relation\u00adships, in the above manner. By merely \nhaving a method with the same argument types, we are not guaranteed that it overrides the respective \nsuperclass method. Instead, over\u00adloading is perfectly legal among methods that di.er only in their return \ntypes. The bytecode method call resolution procedure does not rely on name/type lookup but on direct \nidenti.ers of methods.  Casts yield no constraints at the bytecode level although they do at the source \nlevel. The reason is that the bytecode elides all unnecessary casts (i.e., upcasts). For instance, at \nthe source level, upon seeing in code that passes the type checker a statement of the form (X) new C() \nwe can be certain that (assuming X and C are both classes) the classes X and C are subtype-related: the \ncast can be either an up\u00adcast or a downcast, otherwise it would fail statically. At the bytecode level, \nhowever, a corresponding checkcast X instruction, when the object at the top of the argument stack is \nof static type C, allows no inference. The corre\u00adsponding source code could well have been (X)((Object) \nc) , with the intervening upcast elided during compilation to bytecode.  Constraints can be induced \nnot just by varying the require\u00adments for the output but also by varying the assumptions for the input. \nIn our setting, we only assumed that the in\u00adput is legal Java bytecode when complemented with some extra \nde.nitions. This is distinctly di.erent from assuming that the input has been produced by the translation \nof Java source code. (Bytecode could well have been produced via compilers for other high-level languages \nor via byte\u00adcode generators.) For instance, the Java language maintains types for all local variables. \nAt the source level, if we call methods on the outcome of a conditional expression, we are guaranteed \nto be able to assign a type to it. Consider:  A a; B b; x = (foo()? a : b); x.meth(); // I::meth() x.meth2(); \n// J::meth2() In Java source, the above code means that there exists some type X (the type given to \nvariable x) such that X is a subtype of I and X is a subtype of J, while also A and B are subtypes of \nX. An equivalent conditional in bytecode form does not need to assign a type to X. The constraints will \nbe merely: A and B are subtypes of both I and J, without allowing us to infer the existence of such an \nunknown type X. Our constraint solving process is signi.cantly simpli.ed by the fact that we never need \nto infer the existence of more types.  The above is just a sampling of complications that arise if the \nhierarchy complementation problem is transposed to other domains, requiring the satisfaction of di.erent \nstatic requirements. The e.ectiveness and e.ciency of our ap\u00adproach is largely due to the simplicity \nof the Java byte\u00adcode veri.cation requirements. However, other domains give rise to challenging problems, \nwith a wealth of di.erent con\u00adstraints, possibly appropriate for future work. 8. Related Work The hierarchy \ncomplementation problem is in principle new, although indirectly related to various other pieces of work \nin the literature. From a theory standpoint, our problem is an attempt to more fully determine the structure \nof a partially ordered set. There is no exact counterpart of our algorithms in the liter\u00adature. However, \nthere has been work on sorting a poset, i.e., completely determining the partial order [8]. The challenge \nin such algorithmic research, however, is to perform the sort\u00ading with a minimal number of queries. None \nof the interest\u00ading devices of our algorithms are present. Speci.cally, the device of the single inheritance \ncase (if a node can reach two others, they have to be ordered relative to each other) does not apply, \nand neither does the interesting constraint of the multiple inheritance case (we cannot add direct supertypes \nto a known node). Complementing a program so that the result respects static properties is analogous \nto analyzing only parts of a program but giving guarantees on the result. There are few examples of program \nanalyses of this nature. Notably, Lho\u00adtak et al. recently introduced a technique [1] for analyz\u00ading an \napplication separately from a library, while keeping enough information (from the library analysis) to \nguarantee that the application-level call-graph is correct. Furthermore, the Averroes system [2] uses \nthe assumption that the missing code is independently developed in order to produce a worst\u00adcase skeletal \nlibrary. That is, Averroes takes an existing li\u00adbrary and strips away the implementation, keeping only \nthe interface between the library and the application. The imple\u00admentation is replaced with code (at \nthe bytecode level) that performs worst-case actions on the arguments passed into the library, for the \npurposes of call-graph construction (i.e., the generated code calls all methods the eliminated original \ncode could ever call). Averroes is related to JPhantom but at rather opposite ends of the spectrum: Averroes \nproduces worst-case skeletal implementations, while JPhantom pro\u00adduces minimal, best-case implementations \nthat still respect well-formedness at the type level. At the same time, Aver\u00adroes assumes the library \ninterface is available and just tries to avoid analyzing library implementations, while JPhantom applies \nprecisely when the library is completely missing. Thus, JPhantom truly applies to the case of partial \nprograms, whereas Averroes analyzes a partial program but under the assumption that the whole program \nwas available to begin with. It would be interesting to treat a partial program .rst with JPhantom and \nthen apply Averroes to the JPhantom\u00adproduced program complement to obtain the worst-case be\u00adhavior of \na plausible interface for the missing code. Our hierarchy complementation problem bears a super.\u00adcial \nresemblance to the principal typings problem [3 5]. The principal typings problem consists of computing \ntypes for a Java module in complete isolation from every other module it references. I.e., principal \ntypings aim to achieve a more aggressive form of separate compilation, by computing the minimal type \ninformation on other classes that a class needs in order to typecheck and compile. Thus, the motivation \nis fairly similar to ours, but the technical problem is quite dif\u00adferent. First, in our setting we already \nhave the result of com\u00adpilation in the form of bytecode, and bytecode only. Second, our emphasis is on \nsatisfying constraints instead of captur\u00ading them as a rich type. Finally, our constraints are of a very \ndi.erent nature from any in the principal typings literature. As discussed in Section 7, the input and \noutput language as\u00adsumptions crucially determine the essence of an incomplete program analysis problem. \n9. Conclusions We introduced the class hierarchy complementation prob\u00adlem. The problem consists of .nding \nde.nitions to comple\u00adment an existing partial class hierarchy together with extra subtyping constraints, \nso that the resulting hierarchy satis.es all constraints. In the context of Java bytecode and the con\u00adstraints \nof the bytecode veri.er, our problem is the core chal\u00adlenge of complementing partial programs soundly, \ni.e., so that they pass the checks of the veri.er when loaded together with the generated complement. \nWe o.ered algorithms for the hierarchy complementation problem in both the single and the multiple inheritance \nsetting, and linked it to practice with our JPhantom bytecode complementation tool. We believe that the \nhierarchy complementation problem is fundamental and is likely to arise in di.erent settings in the future, \nhopefully aided by our modeling of the problem and some of its solution avenues. Acknowledgments We gratefully \nacknowledge funding by the European Union under a Marie Curie International Reintegration Grant and a \nEuropean Research Council Starting/Consolidator grant; and by the Greek Secretariat for Research and \nTechnology under an Excellence (Aristeia) award. The anonymous re\u00adviewers o.ered several useful comments \nthat improved the paper. Eric Bodden and Ond.rej Lhot \u00b4 ak gave valuable early feedback on the paper \ns high-level idea. References [1] K. Ali and O. Lhot\u00b4ak. Application-only call graph construc\u00adtion. In \nProc. of the 26th European Conf. on Object-Oriented Programming, ECOOP 12, pages 688 712. Springer, 2012. \n [2] K. Ali and O. Lhot\u00b4ak. Averroes: Whole-program analysis without the whole program. In Proc. of the \n27th European Conf. on Object-Oriented Programming, ECOOP 13, pages 378 400. Springer, 2013. [3] D. Ancona, \nF. Damiani, S. Drossopoulou, and E. Zucca. Poly\u00admorphic bytecode: compositional compilation for Java-like \nlanguages. In Proc. of the 32nd ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages, POPL \n05, pages 26 37, New York, NY, USA, 2005. ACM. [4] D. Ancona, F. Damiani, S. Drossopoulou, E. Zucca, \nand D. U. D. Genova. Even more principal typings for Java-like languages. In ECOOP Workshop on Formal \nTechniques for Java Programs (FTfJP), 2004. [5] D. Ancona and E. Zucca. Principal typings for Java-like \nlanguages. In Proc. of the 31st ACM SIGPLAN-SIGACT Symp. on Principles of Programming Languages, POPL \n04, pages 306 317, New York, NY, USA, 2004. ACM. [6] M. Bravenboer and Y. Smaragdakis. Strictly declarative \nspec\u00adi.cation of sophisticated points-to analyses. In Proc. of the 24th Annual ACM SIGPLAN Conf. on Object \nOriented Pro\u00adgramming, Systems, Languages, and Applications, OOPSLA 09, New York, NY, USA, 2009. ACM. \n[7] E. Bruneton, R. Lenglet, and T. Coupaye. ASM: A code ma\u00adnipulation tool to implement adaptable systems. \nIn Adaptable and extensible component systems, 2002. [8] C. Daskalakis, R. M. Karp, E. Mossel, S. Riesenfeld, \nand E. Verbin. Sorting and selection in posets. In Proc. of the 20th Annual ACM-SIAM Symp. on Discrete \nAlgorithms, SODA 09, pages 392 401, Philadelphia, PA, USA, 2009. Society for Industrial and Applied Mathematics. \n[9] D. Dig. A refactoring approach to parallelism. IEEE Software, 28(1):17 22, Jan. 2011. [10] J. Gosling, \nB. Joy, G. Steele, and G. Bracha. The JavaTMLanguage Speci.cation, Third Edition. Addison-Wesley Professional, \n2005. [11] S. Guarnieri and B. Livshits. GateKeeper: mostly static en\u00adforcement of security and reliability \npolicies for Javascript code. In Proc. of the 18th USENIX Security Symposium, SSYM 09, pages 151 168, \nBerkeley, CA, USA, 2009. USENIX Association. [12] A. Hejlsberg, S. Wiltamuth, and P. Golde. C# Language \nSpeci.cation. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 2003. [13] G. Kastrinis and \nY. Smaragdakis. Hybrid context-sensitivity for points-to analysis. In Proc. of the 2013 ACM SIGPLAN Conf. \non Programming Language Design and Implementa\u00adtion, PLDI 13. ACM, 2013. [14] T. Lindholm and F. Yellin. \nJava Virtual Machine Speci.ca\u00adtion. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 2nd \nedition, 1999. [15] M. Madsen, B. Livshits, and M. Fanning. Practical static anal\u00adysis of Javascript \napplications in the presence of frameworks and libraries. Technical Report MSR-TR-2012-66, Microsoft \nResearch, July 2012. [16] M. Naik, A. Aiken, and J. Whaley. E.ective static race detec\u00adtion for Java. \nIn Proc. of the 2006 ACM SIGPLAN Conf. on Programming Language Design and Implementation, PLDI 06, pages \n308 319. ACM, 2006. [17] M. Odersky, P. Altherr, V. Cremet, B. Emir, S. Maneth, S. Micheloud, N. Mihaylov, \nM. Schinz, E. Stenman, and M. Zenger. An overview of the Scala programming language. Technical Report \nIC/2004/64, EPFL Lausanne, Switzerland, 2004. [18] R. F. St\u00a8ark and J. Schmid. The problem of bytecode \nveri.ca\u00adtion in current implementations of the JVM. Technical report, ETH Z \u00a8 urich, 2000. [19] R. Vall\u00b4ee-Rai, \nP. Co, E. Gagnon, L. Hendren, P. Lam, and V. Sundaresan. Soot -a Java bytecode optimization frame\u00adwork. \nIn Proc. of the 1999 Conf. of the Centre for Advanced Studies on Collaborative research, CASCON 99. IBM \nPress, 1999. [20] R. Vall\u00b4ee-Rai, E. Gagnon, L. J. Hendren, P. Lam, P. Pom\u00adinville, and V. Sundaresan. \nOptimizing Java bytecode using the Soot framework: Is it feasible? In Proc. of the 9th Int. Conf. on \nCompiler Construction, CC 00, pages 18 34, 2000. A. Multiple Inheritance Correctness Proof We shall call \nthe path-edges originating from known-nodes kp-edges. We will also use the symbols S 0, S 1, . . . , \nS 8 to denote the various strati.cations computed at each step of Algorithm 3.1. Note that our algorithm \nwill actually produce a .nite number of strati.cations (at most |V|) but we can disregard both the upper \nlimit of the main loop and the early\u00adfailure condition (line 23) for proving correctness. Instead we \nfocus on the main computation (line 18) and the in.nite sequence of strati.cations that would be produced \nif it was allowed to run forever (even after reaching a .xpoint). Lemma 1. For all v . V , the sequence \n{S 0[v], S 1[v], . . .} is non-decreasing. Proof. Direct consequence of line 18 of the algorithm. D Lemma \n2. For all i . N, 0 = S i[v] = i, .v . V . Proof. Induction on step i. 1. Base: For i = 0, we have that \nS i[v] = S 0[v] = 0, .v . V. 2. Inductive Step: Assume that 0 = S n[v] = n, .v . V for some value of \nn. We must show that 0 = S n+1[v] = n+1, .v . V. Let k be a node in V. Either S n+1[k] = S n[k], and \ntherefore 0 = S n+1[k] = n, or there will exist a node s, s.t. S n+1[k] = S n[s] + 1, in which case 1 \n= S n+1[k] = n + 1.  D De.nition A.1. A node v . V is i-stabilized if and only if S i[v] = S i+1[v] \nand either i = 0 or S i-1[v] < S i[v]. Theorem 1. (Once a node s stratum does not change, it will not \nchange again.) If S i[v] = S i+1[v] for some node v . V and a value i . N, then S j[v] = S i[v], . j \n. Nsuch that j = i. Proof. Induction on step i. 1. Base: For i = 0, let v be a node in V, such that \nS 0[v] = S 1[v]. From Lemma 2, we have that S 0[v] = S 1[v] = 0, which can happen if and only if v has \nno incoming edges (otherwise an edge would cause the node to move to a higher stratum on iteration 1). \nIt is therefore impossible for v to change in the following iterations since it has no constraining edges. \n 2. Inductive Step: Assume that the theorem holds for all i < n for some value of n . N. Let t . V be \na node, such that S n[t] = S n+1[t]. We will show that t s stratum will not change in the future. It \nsu.ces to prove that, for each of t s constraining edges, there will be a node s that has already been \nstabilized at a lower stratum than t, and can be used to satisfy the constraint at this point. Therefore, \nthe constraint will remain satis.ed in future iterations due to s, which will remain in the same stratum \nfrom now on (induction hypothesis). For ordinary path-edges, node s is no other than the source of the \nedge itself, while for kp-edges, it is the lower phantom projection of the edge s source at step n that \nwe may use instead. Let us consider ordinary path-edges .rst, in more detail. From Lemma 2, we have that \n0 = S n[t] = n, and thus 0 = S n+1[t] = n. Let (s, t) . E be an incoming edge of t. We have that 0 = \nS n[s] < S n+1[t] = n which entails that 0 = S n[s] = n - 1. Therefore, according to Lemma 2, we have \nthat 0 = S i[s] = n - 1, .i . {0, . . . , n}. By the pigeonhole principle, and due to Lemma 1, there \nmust surely exist two consecutive values i, i + 1, s.t. S i[s] = S i+1[s] and i < n. From the induction \nhypothesis, we know that s will therefore not change and its constraint on t will be irrelevant in future \niterations. We proceed similarly, for a kp-edge (s, t) (where we use the lowest phantom projection of \ns at this point, instead of s itself). Thus, t will not change in the future, since every constraint \nof t will remain satis.ed after this iteration.  D Corollary 1. For all v . V and n . N+, S n-1[v] * \nS n[v] . S n[v] = n. Theorem 2. The strati.cation sequence S 0, S 1, . . . will di\u00adverge (i.e., not reach \na .xpoint) if and only if at some compu\u00adtation step, n, no new nodes stabilize and not all nodes have \nalready stabilized that is, .n . N+, such that: for some v . V , S n+1[v] * S n[v] and for all v . V \n, S n+1[v] = S n[v] . S n[v] = S n-1[v]. Proof. 1. (If ) Let n be a computation step, such that (.v . \nV) (S n-1[v] * S n[v] . S n[v] * S n+1[v]). We can disre\u00adgard any node u such that S n-1[u] = S n[u], \nand observe that for each remaining node, there must exist at least a constraining edge that cannot be \nsatis.ed with a node that has already been stabilized . That said, due to Corol\u00adlary 1, each remaining \nnode v . V, s.t. S n-1[v] * S n[v], will be placed at a higher (by 1) stratum at this point, i.e., S \ni[v] = i, .i . {0, . . . , n + 1}. Since the relative positions of all the remaining nodes will be the \nsame at step n + 1 as they had been at step n, there is no way for a node to be stabilized at this last \niteration. In other words, there is a cyclic dependency between the remaining nodes that will remain \nunaltered, thus eliminating the possibility of reaching a .xpoint. 2. (Only If ) Due to Theorem 1, we \nknow that we need at most |V| computation steps, in order to reach a .xpoint, if at each computation \nstep there exists at least a new node that gets stabilized. In other words, we need a .nite number of \nsteps to reach a .xpoint, if each step results in some progress. Thus, failure to reach a .xpoint requires \nan iteration where no progress has been made, i.e., no new nodes get stabilized. D Therefore, the optimization \nin Algorithm 3.1 of detecting this exact condition (line 23) and terminating would be trig\u00adgered if and \nonly if no .xpoint would be reached whatsoever, had the algorithm continued its execution. Soundness. \nWe need to show that, if our algorithm com\u00adputes a solution, this solution will be sound. Firstly, our \nal\u00adgorithm maintains the invariant that .(s, t) . E, node s will eventually i.e., once we reach a .xpoint \nbe placed some\u00adwhere lower than node t (otherwise this condition would trigger yet another iteration). \nTherefore, our solution will contain no cycles since all of its edges will be facing up\u00adwards, i.e., \nfrom a lower to a higher stratum. Furthermore, it is evident that, for each kp-edge (s, t), there will \nalways exist a node p . proj(s), such that p will be placed at a lower stra\u00adtum than t in our .nal solution. \nThus, we can add the edge (p, t) without introducing any cycles if none existed so far. This process \nwill therefore generate a valid solution. D Completeness. We need to show that, if a solution exists \nfor a given constraint graph, then our algorithm will also be able to compute a solution, or equivalently \n(according to Theorem 2) that the strati.cation sequence being computed will reach a .xpoint. Consider \nsuch a (posited but unknown) solution. For such a solution we may generate a strati.cation (since the \nsolution may contain no cycles), such that each of its edges is facing upwards and no empty strata exist, \nthat is, .(s, t) . Esol : S sol[s] < S sol[t], where Esol are the edges that form the solution, and S \nsol is its strati.cation. We .rst show an important lemma. Lemma 3. Let S sol denote the strati.cation \nof a valid solu\u00adtion of the problem instance. We have that: .i . N, .v . V : S i[v] = S sol[v]. Proof. \nSuppose that there is a step k . N, such that it contains at least one node u . V with S k[u] > S sol[u], \nand without loss of generality, suppose that k is the smallest such integer, i.e., that before that point \nour strati.cation was upper bounded by that of the unknown solution: . j . {n . N | 0 = n < k}, .v . \nV : S j[v] = S sol[v]. For u to be placed at a higher stratum by our algorithm there must exist an edge \n(s, u) . E such that either (i) (s, u) was a constraining ordinary path-edge: S k[u] = S k-1[s] + 1, \nor (ii) (s, u) was a kp-edge and .p . proj(s) : S k[u] = S k-1[p] + 1. In the .rst case, we have that: \nS sol[u] = S k-1[s] = S sol[s], and since (s, u) must also be present in the solution, this violates \nthe single-direction edge property. In the second case, S sol[u] = S k-1[p] = S sol[p], .p . proj(s), \nwhich leads to another contradiction, since there must exist a node p . proj(s), such that a path exists \nfrom p to u in the solution, which can only happen if p was placed at a strictly lower stratum than u. \nThus, since all possible cases lead to a contradiction, we have proved our initial proposition: our algorithm \nalways assigns to every node a stratum that is lower than, or equal to, that of any true solution of \nthe problem instance. D Let ssol =v.V S sol[v] and si =v.V S i[v], .i . N. It follows that si = ssol, \n.i . N, for any such possible solution. Additionally, because of Lemma 1 and our two theorems, we know \nthat each step but the last will strictly increase the sum of all strata values over all nodes. E.g., \nif our algorithm ended its execution at step n, then we would have: s0 < s1 < ... < sn-1 = sn. Suppose \nthere is a solution but our algorithm fails to compute one (i.e., no .xpoint will ever be reached). Since \nsi strictly increases at each step of our algorithm, and the only way to return is by .nding a valid \nsolution, we know that there will exist a step n, such that sn > ssol. However, this contradicts our \nproven proposition that si = ssol, .i . N. Therefore, we conclude that if valid a solution exists, our \nalgorithm will also be able to compute one. D Theorem 3 (Principality). Any solution produced by Algo\u00adrithm \n3.1 will have a minimum number of strata. That is, for any possible solution of the problem instance, \nwith t denot\u00ading the solution s total strata, we have that n = t, where n is the total number of strata \nproduced by Algorithm 3.1. Proof. Let S sol denote the strati.cation of a valid solution of the problem \ninstance, and t its total number of strata. Without loss of generality we assume that strata are denoted \nas consecutive integers starting from 0, beginning from the lowest stratum. Thus, .v . V : S sol[v] < \nt. Since a solution exists and completeness has been proved, we know that Algorithm 3.1 will also be \nable to terminate successfully at some step n . N, yielding its own solution. Let ns be the total number \nof strata, and x . V be a node at the highest stratum of the solution computed by our algo\u00adrithm. That \nis, .v . V : S n[v] = S n[x]. Node x will also be the node that was changed last, which is at step n \n- 1, i.e., S n[x] = S n-1[x] * S n-2[x]. Therefore, from Corollary 1, we have: S n[x] = S n-1[x] = n \n- 1. Since strata are consecutive integers starting from 0, we have that: ns = S n[x] + 1 = n. According \nto Lemma 3, we have: n = ns = S n[x] + 1 = S sol[x] + 1 = t < t + 1. Thus, we have shown that our algorithm \nminimizes the total number of strata it produces. D  \n\t\t\t", "proc_id": "2509136", "abstract": "<p>We present the problem of class hierarchy complementation: given a partially known hierarchy of classes together with subtyping constraints (\"A has to be a transitive subtype of B\") complete the hierarchy so that it satisfies all constraints. The problem has immediate practical application to the analysis of partial programs--e.g., it arises in the process of providing a sound handling of \"phantom classes\" in the Soot program analysis framework. We provide algorithms to solve the hierarchy complementation problem in the single inheritance and multiple inheritance settings. We also show that the problem in a language such as Java, with single inheritance but multiple subtyping and distinguished class vs. interface types, can be decomposed into separate single- and multiple-subtyping instances. We implement our algorithms in a tool, JPhantom, which complements partial Java bytecode programs so that the result is guaranteed to satisfy the Java verifier requirements. JPhantom is highly scalable and runs in mere seconds even for large input applications and complex constraints (with a maximum of 14s for a 19MB binary).</p>", "authors": [{"name": "George Balatsouras", "author_profile_id": "83358952957", "affiliation": "University of Athens, Athens, Greece", "person_id": "P4290406", "email_address": "gbalats@di.uoa.gr", "orcid_id": ""}, {"name": "Yannis Smaragdakis", "author_profile_id": "81100614708", "affiliation": "University of Athens, Athens, Greece", "person_id": "P4290407", "email_address": "smaragd@di.uoa.gr", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509530", "year": "2013", "article_id": "2509530", "conference": "OOPSLA", "title": "Class hierarchy complementation: soundly completing a partial type graph", "url": "http://dl.acm.org/citation.cfm?id=2509530"}