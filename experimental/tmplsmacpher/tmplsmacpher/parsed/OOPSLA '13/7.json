{"article_publication_date": "10-29-2013", "fulltext": "\n CDSC H E C K E R: Checking Concurrent Data Structures Written with C/C++ Atomics Brian Norris and Brian \nDemsky University of California, Irvine {banorris,bdemsky}@uci.edu Abstract Writing low-level concurrent \nsoftware has traditionally re\u00adquired intimate knowledge of the entire toolchain and often has involved \ncoding in assembly. New language standards have extended C and C++ with support for low-level atomic \noperations and a weak memory model, enabling developers to write portable and ef.cient multithreaded \ncode. Developing correct low-level concurrent code is well\u00adknown to be especially dif.cult under a weak \nmemory model, where code behavior can be surprising. Building reli\u00adable concurrent software using C/C++ \nlow-level atomic op\u00aderations will likely require tools that help developers dis\u00adcover unexpected program \nbehaviors. In this paper we present CDSCH E C K E R, a tool for ex\u00adhaustively exploring the behaviors \nof concurrent code under the C/C++ memory model. We develop several novel tech\u00adniques for modeling the \nrelaxed behaviors allowed by the memory model and for minimizing the number of execution behaviors that \nCDSCH E C K E R must explore. We have used C DSC H E C K E R to exhaustively unit test several concurrent \ndata structure implementations on speci.c inputs and have discovered errors in both a recently published \nC11 imple\u00admentation of a work-stealing queue and a single producer, single consumer queue implementation. \nCategories and Subject Descriptors D.2.4 [Software Engi\u00adneering]: Software/Program Veri.cation; F.3.1 \n[Logics and Meanings of Programs]: Specifying and Verifying and Rea\u00adsoning about Programs Keywords relaxed \nmemory model; model checking Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nCopyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright is held by the owner/author(s). Publication \nrights licensed to ACM. ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509514 \n1. Introduction With the wide-scale deployment of multi-core processors, software developers must write \nparallel software to realize the bene.ts of continued improvements in microprocessors. Many developers \nin industry have adopted a parallel pro\u00adgramming model that uses threads to parallelize computa\u00adtion \nand concurrent data structures to coordinate and share data between threads. Careful data structure design \ncan improve scalability by supporting multiple simultaneous operations and by reduc\u00ading the time taken \nby each individual data structure oper\u00adation. Researchers and practitioners have developed a wide range \nof concurrent data structures designed with these goals in mind [14, 33]. Such data structures often \nuse .ne-grained con.ict detection and avoid contention. Concurrent data structures often use a number \nof so\u00adphisticated techniques including the careful use of low-level atomic instructions (e.g. compare \nand swap (CAS), atomic increment, etc.), careful orderings of loads and stores, and .ne-grained locking. \nFor example, while the standard Java hash table implementation can limit program scalability to a handful \nof processor cores, carefully designed concurrent hash tables can scale to many hundreds of cores [14]. \nTra\u00ad ditionally, developers had to target their implementation of such data structures to a speci.c platform \nand compiler, us\u00ading intimate knowledge of the platform details and even cod\u00ading some data structure \ncomponents in assembly. 1.1 C/C++ Memory Model Recently, standardization committees extended the C and \nC++ language standards with support for low-level atomic operations [2, 3, 12] which allow experts to \ncraft ef.cient concurrent data structures that avoid the overheads of locks. The accompanying memory \nmodel provides for memory op\u00aderations with weaker semantics than sequential consistency; however, using \nthese weak atomic operations is extremely challenging, as developers must carefully reason about often \nsubtle memory model semantics to ensure correctness. Even experts often make subtle errors when reasoning \nabout such memory models. The potential performance gains of low-level atomics may lure both expert \nand novice developers to use them. In fact some common parallel constructs (e.g., sequen\u00adtial locks) \nrequire ordinary developers to use atomics in C/C++ [11]. In the absence of proper tool support, devel\u00ad \nopers will likely write concurrent code that they hope is cor\u00adrect and then rely on testing to .nd bugs. \nAdequately testing concurrent code that uses C/C++ atomics is nearly impossi\u00adble. Even just exploring \nthe behaviors of a given binary on a given architecture can be tricky as some bugs require pre\u00adcise timing \nto trigger. Moreover, neither existing processors nor compilers make full use of the freedoms provided \nby the C/C++ memory model. As future compiler updates imple\u00adment more aggressive optimizations, compilers \nwill leverage the freedom provided by the memory model and produce bi\u00adnaries that exhibit new (but legal) \nbehaviors that will likely expose existing bugs. 1.2 Tool Support While it is possible to use a formal \nspeci.cation of the C/C++ memory model [8] to prove code correct, experience suggests that most software \ndevelopers are unlikely to do so (e.g., because they lack expertise or time). There is a press\u00ading need, \nthen, for tools that allow developers to unit test portions of their code to discover what behaviors \nthe mem\u00adory model allows. Such tools could guarantee soundness of properly abstracted code via exhaustive \nexploration. Typi\u00adcally, concurrent data structures are amenable to such a sce\u00adnario; developers reason \nabout (and rigorously test) their im\u00adplementation in isolation from the details of a larger client program, \nthen provide that abstraction to users, who only must ensure correct use of the abstraction. We present \na new approach for exhaustively exploring the behaviors of code under the C/C++ memory model, based on \nstateless model-checking [22]. Stateless model-checkers typically explore a program s possible behaviors \nor state space by repeatedly executing the program under different thread interleavings. However, exhaustive \nsearch of potential thread interleavings becomes computationally intractable as programs grow to any \nreasonable length. Thus, state-of-the-art model-checking rests on a class of optimization techniques \nknown as dynamic partial-order reduction (DPOR) [20]. The DPOR algorithm can reduce the explored state \nspace by exploring only those execu\u00adtions whose visible behavior may differ from the behavior of previously-explored \nexecutions. During its state-space ex\u00adploration, DPOR identi.es points at which it must explore program \noperations in more than one interleaving (e.g., two concurrent stores to the same object con.ict, whereas \ntwo loads do not). Con.ict points are recorded in a backtrack\u00ading set, so that the exploration can return \n(or backtrack) to the recorded program point during a future execution and attempt a different thread \ninterleaving. DPOR targets a sequentially consistent model, preventing its direct application to the \nC/C++ memory model, as C Figure 1. CDSCH E C K E R system overview and C++ provide no guarantee of a \ntotal execution order in which loads see the value written by the most recent store. The C/C++ memory \nmodel instead de.nes the relation between loads and the values they see in terms of a reads\u00adfrom relation \nwhich is subject to a number of constraints. We present a new approach that exhaustively explores the \nset of legal reads-from relations, with some optimizations in.uenced by DPOR. In C/C++, shared variables \nmust be either clearly anno\u00adtated using the new <atomic> library (or higher-level thread support libraries, \nsuch as <mutex>), or else protected from con.icting concurrent access through use of these atomics or \nother synchronization primitives; any pair of con.icting ac\u00adcesses to non-atomic variables without proper \nsynchroniza\u00adtion constitutes a data race, which yields unde.ned behav\u00adior [2]. Thus, we simply designed \nCDSCH E C K E R as a dy\u00adnamic library implementation of these threading and atomic libraries, as shown \nin Figure 1, and generally left other op\u00ad erations uninstrumented. Such a design can readily support \na broad range of real-world applications, as users simply compile their code against our library with \ntheir compiler of choice. At runtime, CDSCH E C K E R schedules program fragments sequentially and determines \nthe values returned by atomic memory operations. To model all program behaviors, C D SCH E C K E R im\u00adplements \na backtracking-based system which performs re\u00adpeated, controlled program execution until it has explored \nall necessary program behaviors. CD SCH E C K E R reports di\u00adagnostic information for all data races, \ndeadlocks, uninitial\u00adized atomic loads, and user-provided assertion failures that occur for the provided \ninput. All failure reports include a full trace of all thread and atomic operations performed in the \nprogram, a short description of the detected bug(s), and a representation of the reads-from relation \nfor the execution. Some tools already exist for testing program behavior ac\u00adcording to the C/C++ memory \nmodel. C P P M E M [8] enumer\u00ad ates all potential modi.cation orders and reads-from rela\u00adtions in programs \n(under a limited subset of C/C++ language constructs), then eliminates the infeasible ones according \nto the formal speci.cation of the memory model. CP P M E M lacks support for fences and only supports \nloops with a priori loop iteration bounds. We contribute an exponentially more ef.cient approach that \nmakes it possible to check real code. Our approach reduces the search space by avoiding explic\u00aditly enumerating \norderings that produce equivalent execution behavior. We also contribute support for fences and loops \nwithout .xed iteration bounds. Relacy [37] explores possible behaviors of real C++ programs using library-based \ninstru\u00admentation, but it cannot model all types of relaxed behavior allowed by C/C++. Our approach fully \nmodels the relaxed behavior of real C and C++ code.  1.3 Limitations Generally, CDSCH E C K E R will \nexplore every distinct exe\u00adcution behavior allowed by the C/C++ memory model, pro\u00adviding exhaustive test \ncoverage under a particular program input. However, there are a few considerations in the design and \nimplementation of CDS CH E C K E R that leave room for incompleteness. We summarize the limitations here \nand pro\u00advide more thorough explanation in the body of the paper. Supporting memory order consume requires \na com\u00adpiler s deep knowledge of data dependences. We opted instead to make C DSC H E C K E R compiler-agnostic. \n Unbounded loops present in.nite state spaces, which cannot be completely explored by a stateless model\u00adchecker. \nWe explore such loops under the restriction of a fair schedule: either through bounded fairness enforced \nby our scheduler (bounds adjustable) or through the use of CH E S S [34] yield-based fairness.  Some \nprograms rely on a live memory system in order to terminate. For such programs, we impose bounded liveness \nvia an adjustable run-time option.  C D SCH E C K E R may not explore all behaviors involv\u00ading satisfaction \ncycles. Not only are satisfaction cycles dif.cult to generate in a model-checker, but they are a thorny, \nunsolved issue in the current C and C++ speci.\u00adcations, which do not make it clear exactly which behav\u00adiors \nshould be allowed and disallowed. See Appendix B for further discussion.  C D SCH E C K E R uses a system \nof promises to allow loads to read from stores that appear later in the execution (Section 6). However, \nwe do not allow these promises to remain forever in an execution which will never satisfy them. Thus, \npromise expiration theoretically may be a source of incompleteness.  In rare circumstances, CD SCH E \nC K E R can generate false positives (behaviors not allowed by the memory model) due to an absence of \ndependence information. Given ad\u00additional information from the compiler, however, it would be straightforward \nto check each generated trace for these false positives.  1.4 Contributions This paper makes the following \ncontributions: Basic Approach: It presents new techniques that enable the stateless model-checking of \nC/C++ code under the C/C++ memory model. Our approach is the .rst that can model-check unit tests for \nreal-world C/C++ data struc\u00adture implementations under the C/C++ memory model.  Constraints-Based Modi.cation \nOrder: It introduces the .rst technique for model-checking the C/C++ mem\u00adory model without explicitly \nenumerating the modi.ca\u00adtion order of atomic objects, exponentially decreasing the search space.  Relaxed \nMemory Model Support: It develops new techniques to support the full variability of the mem\u00adory model, \nincluding allowing loads to observe the val\u00adues written by stores that appear later in the execution \norder while at the same time maintaining compatibility with uninstrumented code in libraries.  Partial \nOrder Reduction: It combines our new re\u00adlaxed model-checking techniques with existing schedule\u00addriven \npartial order reduction to ef.ciently support se\u00adquentially consistent memory actions.  Bug Finding: \nIt shows that our techniques can .nd bugs in real world code including .nding a new bug in a published, \npeer-reviewed implementation of the Chase-Lev deque.  Evaluation: It presents an evaluation of the model\u00adchecker \nimplementation on several concurrent data struc\u00adtures. With runtimes averaging only a few seconds and \nno test taking over 11 seconds, empirical results show that our tool is ef.cient in practice.  The remainder \nof the paper is organized as follows. Sec\u00adtion 2 presents an example. Section 3 reviews important as\u00ad \npects the C/C++ memory model. Section 4 gives an overview of our approach. Section 5 presents our constraint-based \nap\u00ad proach to modi.cation orders. Section 6 provides more in\u00ad sight on how we support the relaxed memory \nmodel. Sec\u00adtion 7 discusses release sequence support. Section 8 dis\u00ad cusses how we handle fairness and \nmemory liveness. Sec\u00adtion 9 evaluates CDSCH E C K E R. Section 10 presents related work. We conclude \nin Section 11. 2. Example To explore some of the key concepts of the memory\u00adordering operations provided \nby the C/C++ memory model, consider the example in Figure 2, assuming that two in\u00ad dependent threads \nexecute the methods threadA() and threadB(). This example uses the C++11 syntax for atom\u00adics; shared, \nconcurrently-accessed variables are given an atomic type, whose loads and stores are marked with an ex\u00adplicit \nmemory order governing their inter-thread ordering and visibility properties (discussed more in Section \n3). In this example, a few simple interleavings of threadA() and threadB() show that we may see executions \nin  1 a t o m i c < i n t > x ( 0 ) , y ( 0 ) ; 2 3 v o i d t h r e a d A ( ) { 4 i n t r 1 = y . l \no a d ( m e m o r y _ o r d e r _ r e l a x e d ) ; 5 x . s t o r e ( 1 , m e m o r y _ o r d e r _ r \ne l a x e d ) ; 6 p r i n t f ( \" r 1 = % d \\ n \" , r 1 ) ; 7 } 8 v o i d t h r e a d B ( ) { 9 i n t \nr 2 = x . l o a d ( m e m o r y _ o r d e r _ r e l a x e d ) ; 10 y . s t o r e ( 1 , m e m o r y _ \no r d e r _ r e l a x e d ) ; 11 p r i n t f ( \" r 2 = % d \\ n \" , r 2 ) ; 12 } Figure 2. C++11 Code \nExample which {r1 = r2 = 0}, {r1 = 0 .r2 = 1}, or {r1 = 1 . r2 = 0}, but it is somewhat counter-intuitive \nthat we may also see {r1 = r2 = 1}, in which both load state\u00adments read from the store statements that \nappear after the other load. While this latter behavior cannot occur under a sequentially-consistent \nexecution of this program, it is, in fact, allowed by the relaxed memory ordering used in the example \n(and achieved, e.g., by compiler reordering). Now, consider a modi.cation of the same example, where \nthe load and store on variable y (Line 4 and Line 10) now use memory order acquire and memory order release, \nre\u00adspectively, so that when the load-acquire reads from the store-release, they form a release/acquire \nsynchronization pair. Then in any execution where r1 = 1 and thus the load-acquire statement (Line 4) \nreads from the store-release statement (Line 10), the synchronization between the store\u00ad release and \nthe load-acquire forms an ordering between threadB() and threadA() particularly, that the actions in \nthreadA() after the acquire must observe the effects of the actions in threadB() before the release. \nIn the termi\u00adnology of the C/C++ memory model, we say that all actions in threadB() sequenced before \nthe release happen before all actions in threadA() sequenced after the acquire. So when r1 = 1, threadB() \nmust see r2 = 0. In summary, this modi.ed example allows only three of the four previously-described \nbehaviors: {r1 = r2 = 0}, {r1 = 0 . r2 = 1}, or {r1 = 1 . r2 = 0}. 3. C/C++ Memory Model The C/C++ memory \nmodel describes a series of atomic oper\u00adations and the corresponding allowed behaviors of programs that \nutilize them. Note that throughout this paper, we primar\u00adily discuss atomic memory operations that perform \neither a write (referred to as a store or modi.cation operation) or a read (referred to as a load operation). \nThe discussion gen\u00aderalizes to operations that perform both a read and a write (read-modify-write, or \nRMW, operations). Appendix A de\u00ad scribes how C DSCH E C K E R supports fences. Any operation on an atomic \nobject will have one of six memory orders, each of which falls into one or more of the following categories. \nrelaxed: memory order relaxed weakest memory or\u00ad dering release: memory order release, memory order \nacq rel, and memory order seq cst a store-release may form release/consume or re\u00adlease/acquire synchronization \nconsume:1 memory order consume a load-consume may form release/consume synchronization acquire: memory \norder acquire, memory order acq rel, and memory order seq cst a load-acquire may form release/acquire \nsynchroniza\u00adtion seq-cst: memory order seq cst strongest memory or\u00addering To ease programming burden, \natomic operations default to using memory order seq cst when no ordering is spec\u00adi.ed. 3.1 Relations \nThe C/C++ memory model expresses program behavior in the form of binary relations or orderings. The following \nsubsections will brie.y summarize the relevant relations. Some of this discussion resembles the preferred \nmodel from the formalization in [8], adapted to suit its usage in C D-SCH E C K E R. Sequenced-Before \nThe order of program operations within a single thread of execution establishes an intra\u00adthread sequenced-before \n(sb) relation. Note that while some operations in C/C++ provide no intra-thread ordering the equality \noperator (==), for example we ignore this detail and assume that sb totally orders all operations in \na thread. Reads-From The reads-from (rf ) relation consists of store\u00adload pairs (X, Y ) such that Y reads \nits value from the effect rf of X or X -. Y . In the C/C++ memory model, this relation is non-trivial, \nas a given load operation may read from one of many potential stores in the program execution. Synchronizes-With \nThe synchronizes-with (sw) relation captures synchronization that occurs when certain atomic operations \ninteract across two threads. For instance, re\u00adlease/acquire synchronization occurs between a pair of \natomic operations on the same object: a store-release X and a load-acquire Y . If Y reads from X, then \nX synchronizes sw with Y or X -. Y . Synchronization also occurs between consecutive unlock and lock \noperations on the same mutex, between thread creation and the .rst event in the new thread, and between \nthe last action of a thread and the completion of a thread-join operation targeting that thread. 1 We \ndon t support consume due to implementation obstacles in detecting data dependencies. See Section 4.4. \n Note that our discussion of sw is incomplete here. We will complete it when we introduce release sequences \nin Section 7. Happens-Before In CDSCH E C K E R, we avoid consume operations, and so the happens-before \n(hb) relation is simply the transitive closure of sb and sw. Sequential Consistency All seq-cst operations \nin a pro\u00adgram execution form a total ordering (sc) so that, for in\u00adstance, a seq-cst load may not read \nfrom a seq-cst store prior to the most recent store (to the same location) in the sc order\u00ading, nor from \nany store that happens before that store. The sc order must be consistent with hb. Modi.cation Order \nEach atomic object in a program ex\u00adecution has an associated modi.cation order (mo) a total order of \nall stores to that object which informally repre\u00adsents a memory-coherent ordering in which those stores \nmay be observed by the rest of the program. Note that in general the modi.cation orders for all objects \ncannot be combined to form a consistent total ordering. For instance, the surprising behavior in Section \n2 s example shows an instance where the union of sb and rf is cyclic, and we can easily extend the example \nto demonstrate a cyclic union of sb and mo. 4. CDSCH E CK ER Overview C DSC H E C K E R s model-checking \nalgorithm (presented in Section 4.1) builds on partial order reduction concepts from [20]. However, the \nC/C++ memory model is signi.\u00ad cantly more complex than DPOR s sequentially-consistent model, and thus \nsimply controlling thread interleavings does not suf.ce to reproduce the allowed behaviors. Thus it was \nnecessary to develop a new approach to explore the richer set of behaviors allowed by the C/C++ memory \nmodel and new partial order reduction techniques to minimize the ex\u00adploration of redundant executions. \nOne signi.cant departure from DPOR is that the C/C++ memory model splits memory locations and operations \ninto two categories: (1) normal locations and operations and (2) atomic locations and operations. The \nmemory model forbids data races on normal memory operations (and assigns unde\u00ad.ned semantics to programs \nwith such races), but allows ar\u00adbitrary interleavings of atomic operations. This enables CD-SCH E C K \nE R to make a signi.cant optimization over existing model-checkers it detects and reports data races \n(a simple feat) on all instrumented normal memory accesses while ex\u00adhaustively exploring interleavings \n(an expensive, combina\u00adtorial search) only for atomic memory operations. If a nor\u00admal memory access can \nexhibit more than one behavior un\u00adder the synchronization pattern established by the atomic op\u00aderations \nin a given execution, then it has a data race and is forbidden by the C/C++ speci.cations. CDS CH E C \nK E R s design leverages this optimization; it exhaustively enumerates the behaviors of atomic memory \naccesses and simply checks for data races between normal 1: Initially: EX P L O R E(\u00d8) 2: function EX \nP L O R E(S) 3: s . last(S) 4: PRO C E S S AC T I O N(S) 5: if .p0 . enabled(s) then 6: threads(s) . \n{p0} 7: threadsdone . \u00d8 8: while .p . threads(s) \\ threadsdone do 9: t . next(s, p) 10: behaviors(t) \n. {Initial behaviors} 11: behavedone . \u00d8 12: while .b . behaviors(t) \\ behavedone do 13: EX P L O R E(S.(t, \nb)) 14: behavedone . behavedone . {b} 15: end while 16: threadsdone . threadsdone . {p} 17: end while \n18: end if 19: end function Figure 3. CDS CH E C K E R algorithm memory operations, reporting any data \nraces to the user. This cheapens the instrumentation required for normal memory operations and reduces \nthe search space explored for racy (i.e., buggy) programs. 4.1 CDSCH E C K E R Architecture We next \ndiscuss the high-level architecture of C D-SCH E C K E R, beginning with our algorithm (Figure 3) and \nits relation to existing literature. In our discussions, we adapt several terms and symbols from [20]. \nWe associate every state transition t taken by processes (i.e., threads) p with the dynamic operation \nthat effected the transition, then de.ne the execution order2 S of these operations as the total or\u00addering \ngiven by the sequence of scheduling choices made in Figure 3, Line 8. We say that next(s, p) is the next \ntransi\u00adtion in thread p at a given state s; last(S) is the most recent state visited in S; S.t denotes \nextending S with an additional transition t; and enabled(s) is the set of all threads enabled in state \ns (threads can be disabled, e.g., when waiting on a held mutex or when completed). We base the C DSC \nH E C K E R algorithm on standard back\u00adtracking algorithms; we perform a depth-.rst exploration of the \nprogram s state space (recursive calls to EX P L O R E, Line 13) by iterating over a set of threads whose \nnext transi\u00ad tion must be explored from the given state s (the outer loop, excluding Lines 10 to 15). \nMost of our algorithmic exten\u00ad sions correspond to the inner loop, which performs a similar (but distinct) \nbacktracking to explore the different possible 2 We use the term execution order instead of transition \nsequence to make clear the fact that a transition in our model-checker cannot be easily char\u00adacterized \nas simply a function of the current state. For example, a load tran\u00adsition can depend on future stores. \n behaviors of the transition t that was selected in the outer loop. Section 4.2 further describes the \npurpose of the behav\u00adiors set. Note that as presented in Figure 3, the outer loop will only select a \nsingle initial execution order (i.e., each state s threads set only ever contains the initial thread \nselection p0). The PRO C E S S AC T I O N procedure examines the last transi\u00adtion and may add additional \nbacktracking points for previ\u00adous states as needed to exhaustively explore the state space. For clarity \nof presentation, we describe PRO C E S S AC T I O N s behavior in prose throughout the rest of the paper. \n 4.2 Transitions with Multiple Behaviors We now discuss another major departure from DPOR, which comes \nfrom the nature of relaxed memory models. On one hand, DPOR assumes that all memory modi.cations form \na consistent total ordering and that all memory accesses read only the last prior value written to memory. \nHowever, it is clear that the relaxed C/C++ memory model does not .t this model. More pecisely, while \nthe union of the sb, hb and sc relations must be acyclic and consistent with some interleav\u00ading of threads, \nthe addition of rf and mo introduces the pos\u00adsibility of cycles. Therefore, in order to explore a program \ns state space using a linear, totally-ordered execution trace, we must account for behaviors which are \ninconsistent with the execution order. In order to explore a relaxed model, a backtracking-based search \nnot only must select the next thread to execute, but also must decide how that thread s next step should \nbehave. We represent this notion in Figure 3 as a backtracking itera\u00ad tion not only over threads but \nover behaviors (the aforemen\u00adtioned inner loop). Together, a thread choice and behavior selection de.ne \na unique state transition. A key source of different transition behaviors arises from the reads-from \nrelation in C/C++, loads can read from modi.cations besides simply the last store to an object. We introduce \nthe concept of a may-read-from set to enumerate the stores that appear earlier in the execution order \nthat a given load may read from.3 When we execute a load Y , we build the may-read-from set as a subset \nof stores(Y ) (the set of all stores to the same object from which Y reads): hb may-read-from(Y ) = {X \n. stores(Y ) | \u00ac(Y -. X) . hbhb (-Z . stores(Y ). X -. Z -. Y )}. hb The clause \u00ac(Y -. X) prevents a \nload from seeing values from stores that are ordered later by happens-before, and the hbhb clause (-Z \n. stores(Y ). X -. Z -. Y ) prevents a load from observing stores that are masked by another store. Successive \nexecutions then iterate over this set, explor\u00ading executions in which a load may read from each one of \n3 Loads can also read from stores that appear later in the execution order. Section 6 presents our approach \nfor handling this case. the potential stores. Each execution forms a different rf re\u00adlation (and, by \nextension, mo and hb relations). If -X . hb may-read-from(Y ) such that X -. Y , then we report an uninitialized \nload a bug in the program under test. The reads-from mechanism allows CDSCH E C K E R to ex\u00adplore most \nof the behaviors of the C/C++ memory model without rearranging the thread schedule. In fact, in the ab\u00adsence \nof synchronization or sequentially consistent opera\u00adtions, CDSCH E C K E R does not use the DPOR backtracking \nmechanism to change the thread schedule at all. 4.3 Handling Sequential Consistency The memory model \nguarantees a total ordering sc over all seq-cst operations. CDSCH E C K E R forces the sc relation to \nbe consistent with the execution order. Thus CDSCH E C K E R relies on a modi.ed DPOR-like algorithm \nto rearrange the scheduled execution order to implement sequentially con\u00adsistent operations CDSCH E C \nK E R identi.es con.icting se\u00adquentially consistent operations and sets backtracking points as described \nin the DPOR algorithm. We combine DPOR with sleep sets [21]. Note that in addition to using DPOR\u00ad style \nbacktracking for maintaining a consistent sc ordering, we use it to ensure that hb is consistent with \nthe execution or\u00adder (see Section 6.3) and to explore the behaviors of higher\u00ad level synchronization \nprimitives (e.g., mutexes).  4.4 Happens-Before and Clock Vectors In the absence of consume operations, \nhappens-before is simply the transitive closure of synchronizes-with and sequenced-before. Thus, CDSCH \nE C K E R represents happens-before succinctly using a Lamport-style clock vec\u00adtor [28]. Events consist \nof atomic loads and stores, thread creation and join, mutex lock and unlock, and other synchro\u00adnizing \nactions. Every event increments its own thread s clock (representing a step in sb), and then CDSCH E \nC K E R tags the event with the current thread s clock vector. Synchronization sw between two threads \nTi and Tj , where Ti -. Tj should merge Ti s clock vector with Tj s clock vector, according to a pairwise \nmaximum over all the thread clocks. We assign the resulting vector to the synchronizing event in Tj. \nSome processor architectures (e.g., Power and ARM) re\u00adspect low-level data dependencies such that while \nsynchro\u00adnization is generally expensive it can be cheapened for op\u00aderations that are data-dependent on \na synchronizing memory access. Thus, C and C++ provide release/consume atomics as a weaker, dependency-based \nsynchronization alternative to release/acquire. However on stronger architectures (e.g., x86), consume \nprovides no bene.t over acquire, so we .nd it reasonable to omit support of consume in favor of mini\u00admizing \nCDSCH E C K E R s compiler-speci.c dependencies. Still, given compiler and runtime support for computing \nthe intra-thread carries a dependency to relation, we can ex\u00adtend our approach to support release/consume \nsynchroniza\u00adtion. One approach is to associate a secondary clock vector with a program event if it is \ndependency ordered (\u00a71.10p9\u00ad10 [3]) after a store-release from a different thread never forwarding the \nclock vector to subsequent actions ordered only by sequenced-before. When present, the model-checker \nwould use this secondary clock vector for detecting data races and computing may-read-from sets. A store-release \nthat follows operations that are dependency ordered would then merge the clock vectors for all operations \nsequenced be\u00adfore the store, transferring them to any operation with which it synchronizes.  4.5 Deadlock \nDetection C DSC H E C K E R can easily detect deadlocks during its state space search. Given our knowledge \nof the next transition next(s, p) for each thread p, it is straightforward to check if a thread s next \ntransition is disabled (i.e., blocking due to a mutex operation or a thread join) and waiting on another \nthread. Then, CDSCH E C K E R can simply check for a circular wait by traversing the chain of waiting \nthreads whenever a thread takes a step; if that thread can reach itself, then we report a deadlock to \nthe user. 5. Constraints-based Modi.cation Order The modi.cation order relation presents unique challenges \nand opportunities in model-checking C/C++, as program ex\u00adecutions never directly observe it. One approach \ntaken by other tools (e.g., CP P M E M) is to exhaustively enumerate both the mo and rf relations, discarding \nany executions that violate memory model constraints. In the following subsec\u00adtions, we present a new \napproach, in which we record mo not as an exhaustively explored ordering but as a constraints graph, \nin order to reduce (by an exponential factor) the work spent on both infeasible and redundant executions. \n5.1 Motivation We could constructively maintain the modi.cation order using an approach similar to CP \nP M E M as soon as CD-SCH E C K E R executes a store, we could assign it an absolute ordering within \nmo. However, at the time of a store s exe\u00adcution, a program has not formed many constraints for its modi.cation \norder, so we would have to choose its ordering arbitrarily, then explore an additional exponential space \nof reorderings to enumerate all possible choices. This would of\u00adten incur a very large overhead, as constraints \nobserved later in the execution often invalidate many orderings and many different modi.cation orderings \nproduce no visible differ\u00adence in program behavior. Therefore, rather than constructively (and expensively) \nmaintaining mo as a total ordering, we chose a lazy approach to the modi.cation order. CDS CH E C K E \nR represents mo as a set of constraints, built as a constraints graph the modi.\u00adcation order graph, or \nmo-graph. A node represents a single store in the execution and an edge directed from a node A to mo \na node B represents the constraint A -. B. CD SCH E C K E R dynamically adds edges to the mo-graph as \nhb and rf rela\u00adtions are formed, as described in Section 5.5. Then, C D-SCH E C K E R must only ensure \nthat exploration of a partic\u00adular execution yields a satis.able set of mo constraints or equivalently, \nan acyclic mo-graph. A cyclic mo-graph im\u00adplies an ill-formed execution, and so CDSCH E C K E R can discard \nthe current execution and move on to explore the next execution. 5.2 Representing the Memory Model as \nConstraints The memory model speci.es several properties governing the interplay of rf, hb, sc, and mo. \nWe contribute the insight that these properties can be formulated as constraints on the modi.cation order. \nThus, we present them as implications, shown in the left-to-right progressions in Figure 4. For ex\u00ad ample, \nconsider RE A D -RE A D CO H E R E N C E (\u00a71.10p16 [3]); we can say that any pair of loads A and B with \na correspond\u00ading pair of stores X and Y (all operating on the same object v), where rf rf hb X -. A, \nY -. B, and A -. B imply a particular modi.cation ordering for X and Y mo namely, that X -. Y . In other \nwords, such a constraint pre\u00advents other loads from observing X and Y in the reverse or\u00adder. The reader \ncan examine similar W R I T E -RE A D, RE A D -WR I T E, or WR I T E -W R I T E coherence requirements. \nIn addition to CO H E R E N C E, we summarize the following memory model requirements: SE Q -C S T / \nMO CO N S I S T E N C Y: A pair of seq-cst stores must form mo consistently with sc (\u00a729.3p3 [3])  SE \nQ -C S T WR I T E -RE A D CO H E R E N C E : A seq-cst load must read from a store no earlier (in mo) \nthan the most recent (in sc) seq-cst store (\u00a729.3p3 [3])  RMW / MO CO N S I S T E N C Y: A read-modify-write \nmust be ordered after the store from which it reads (\u00a729.3p12 [3])  RMW ATO M I C I T Y: A read-modify-write \nmust be or\u00addered immediately after the store from which it reads (\u00a729.3p12 [3])   5.3 Example We examine \nthe application of these constraints in the mo\u00adgraph using the following example. 1 a t o m i c < i n \nt > x ( 0 ) ; 2 3 void threadA() { 4 x . s t o r e ( 1 , m e m o r y _ o r d e r _ r e l a x e d ) ; \n/ / A 5 x . s t o r e ( 2 , m e m o r y _ o r d e r _ r e l a x e d ) ; / / B 6 } 7 void threadB() { \n8 i n t r 1 = x . l o a d ( m e m o r y _ o r d e r _ r e l a x e d ) ; / / C 9 i n t r 2 = x . l o a \nd ( m e m o r y _ o r d e r _ r e l a x e d ) ; / / D 10 } As CDSCH E C K E R executes the stores in \nthreadA(), the WR I T E -WR I T E CO H E R E N C E constraint implies a mo edge  RE A D -RE A D CO H \nE R E N C E =. WR I T E -RE A D CO H E R E N C E =. RE A D -WR I T E CO H E R E N C E =. WR I T \nE -WR I T E CO H E R E N C E =. SE Q -C S T / MO CO N S I S T E N C Y =. SE Q -C S T WR I T E -RE \nA D CO H E R E N C E =.  RMW / MO CO N S I S T E N C Y =. RMW ATO M I C I T Y =. Figure 4. Modi.cation \norder implications. On the left side of each implication, A, B, C, X, and Y must be distinct. from store \nA to store B. Consider an example execution where load C has read from store B. Now consider the possi\u00adbility \nof load D reading from store A. In such a case, RE A D -RE A D CO H E R E N C E would require a mo-graph \nedge from B to A forming a mo-graph cycle between A and B and pro\u00adhibiting such an execution.  5.4 Search \nSpace Reduction We will demonstrate in a short example how our approach to constraints-based modi.cation \norder reduces the inef\u00ad.cient exploration of redundant and infeasible execution behaviors in comparison \nto simpler approaches, such as CP P M E M s. Consider the following program, written in the syntax style \nof C P P M E M, where {{{ statement1; ||| statement2; }}} means that statement1 and statement2 execute \nin parallel. 1 a t o m i c < i n t > x = 0 ; 2 3 { { { x . s t o r e ( 1 , r e l a x e d ) ; 4 | | | \nx . s t o r e ( 2 , r e l a x e d ) ; 5 | | | x . s t o r e ( 3 , r e l a x e d ) ; } } } 6 7 r 1 = x \n. l o a d ( r e l a x e d ) ; CP P M E M s search algorithm considers that a load may read from any \nstore in the program, and that those stores may have any arbitrary (total) modi.cation ordering; it per\u00adforms \nno analysis of the interplay of reads-from, modi.ca\u00adtion order, and happens-before when enumerating candidate \nexecutions. Thus in this program, it enumerates 24 potential modi.cation orderings for the 3 stores and \n1 initialization (the permutations of a 4-element sequence) and considers 4 potential stores to read \nfrom at line 7, yielding 96 com\u00ad binations. However, one can easily see that there are actu\u00adally only \n3 valid behaviors for this program: those repre\u00adsented by the results r1 = 1, r1 = 2, or r1 = 3. In fact, \nmany of the modi.cation orderings are impossible; none of the stores can be ordered before the initialization, \ndue to WR I T E -WR I T E CO H E R E N C E. Additionally, many of the re\u00admaining modi.cation orderings \nare irrelevant; this program only cares which of the stores appears last in the order, as this is the \nstore from which the load must read. CDSCH E C K E R s constraint construction captures ex\u00adactly the \nobservations of the previous paragraph because it only establishes modi.cation orders as they are observed. \nSo for example, when line 7 reads a value of 3, CDSCH E C K E R rules that line 5 must be ordered after \nall of the other stores (due to WR I T E -RE A D CO H E R E N C E), but it doesn t bother enumerating \nthe modi.cation ordering of the remain\u00ading stores, since no operations observe their ordering. Ad\u00additionally, \nCDSCH E C K E R can avoid exploring executions where line 7 reads a value of 0, since such a rf relation \nwould immediately generate a mo-cycle. In fact, CDSCH E C K E R explores exactly the 3 consistent behaviors \nwithout enumer\u00adating the other 93 redundant or inconsistent orderings.  5.5 Optimized Constraint Construction \nCDSCH E C K E R derives its mo-graph using the implications presented in Figure 4. However, these requirements \nare non\u00ad trivial to implement, as a na\u00a8ive approach involves a search of the entire execution history \nevery time we update hb, sc, rf, or mo that is, at least once for every program operation. But with a \nfew observations and optimizations, we can ef.\u00adciently build this graph.  CO H E R E N C E : Because \nthe antecedents of the four coher\u00adence implications involve only the rf and hb relations on a single \nobject, we must compute additional mo edges only on exploration of new loads and stores or when rf or \nhb are updated. Now, consider an implementation of RE A D -RE A D CO H E R E N C E. Rather than searching \nfor all pairs of loads or\u00addered by happens-before, we conclude that when exploring a new load Y , we \nonly need to consider the most recent load Xi, from each thread i, which happens before Y and reads from \nthe same object. For any other load Z (reading the same object) that happens before Y , either Z = Xi \nfor some sb i, or else Z -. Xj for some j. By induction, then, CD-SCH E C K E R must already have considered \nany prior loads. The other three coherence conditions have similar induc\u00adtive behavior, and so we can \nlimit the number of computa\u00adtions necessary: two rules correspond to a new load (RE A D -RE A D and WR \nI T E -R E A D), and two rules correspond to a new store (RE A D -WR I T E and W R I T E -WR I T E); \nall four ap\u00adply to a read-modify-write. Furthermore, by a similar induc\u00adtive argument, we can combine \nthe coherence rules such that it is only necessary to search for the most recent load or store (and not \nboth). Finally, note that lazy updates of hb (see Sec\u00adtion 7) must trigger similar constraint updates. \nSE Q -C S T / M O CO N S I S T E N C Y: Upon exploration of a new seq-cst store, CDSCH E C K E R must \nadd an edge from the most recent seq-cst store to the same object in the execu\u00adtion order (and hence, \nin sc) to the current store. By a sim\u00adple induction, this computation will soundly cover all seq-cst \nstores, if applied at exploration of each new seq-cst store. SE Q -C S T WR I T E -RE A D CO H E R E \nN C E : In similar fash\u00adion to SE Q -C S T / MO CO N S I S T E N C Y, CDSCH E C K E R must search for \nthe most recent seq-cst store upon exploration of a seq-cst load. RMW / MO CO N S I S T E N C Y: Consistency \nis trivial; CD-SCH E C K E R simply adds a mo-graph edge whenever a read\u00admodify-write executes. RMW ATO \nM I C I T Y: Not only must CDSCH E C K E R be sure to order a RMW B after the store A from which it reads \n(i.e., R M W / MO CO N S I S T E N C Y), it must also ensure that any store C ordered after A is also \nordered after B. Thus, C DSC H E C K E R records metadata in each graph node A to show which RMW (if \nany) reads from A; a new edge from A to C then creates an additional edge from B to C. Note that R M \nW CO N S I S T E N C Y and ATO M I C I T Y combine to ensure that two RMW s cannot read from the same \nstore. If two RMW s, B and C, each read from A, then the mo-graph forms a cycle between B and C, invalidating \nthe current execution.  5.6 Modi.cation Order Rollback A na\u00a8ive implementation of our mo-graph approach \nwould have to rollback the entire execution whenever it assigns a load to read from a store that results \nin immediate violations of mo-graph constraints. To optimize for this common case, our mo-graph supports \nrolling back the most recent updates. Then in Section 5.3 s example, for instance, CDSCH E C K E R can \ncheck whether it is feasible for load D to read from store A before committing D to read from A. This \nreduces the number of infeasible executions that must be backtracked. 6. Relaxing Reads-from The framework \nas described thus far can only simulate loads that read from stores that appear earlier in the execution \nor\u00adder. However, the C/C++ memory model allows executions in which the union of the rf and sb relations \nis cyclic, im\u00adplying that regardless of the execution order, this strategy will not suf.ce to model all \nlegal executions. The counter\u00adintuitive result (i.e., {r1 = r2 = 1}) from our example in Figure 2 is \none such execution. To fully model all behaviors allowed by the memory model, CD SCH E C K E R must also \nmodel executions in which values .ow backwards in the execution order, allowing loads to read from stores \nwhich have not yet occurred at the time of the load we say that such loads are observing future values. \nThe key idea for modeling future values is to leverage backtracking of transition behaviors to allow \nloads to read from stores that appear later in the execution order. As an illustrative example, consider \nwithout loss of generality an execution order of the example from Figure 2 in which all statements in \nthreadA appear before all statements in threadB. In such an execution, it is relatively easy to see how \nto simulate r2 = 1 from the counterintuitive result. However, simulating r1 = 1 requires that the load \nin Line 4 of threadA read from the store in Line 10 of threadB. The challenge here is that this load \nappears before C D-SCH E C K E R has even seen the store. To address this challenge, we introduce an \nextension to our may-read-from set: the futurevalues set which associates pairs (v, t) with loads X, \nwhere v is a predicted future value (written by thread t) that X may read. Suppose an execution encounters \na store Y and a number of loads X1, X2, . . . , Xn from earlier in the execution order. As long as Xi \ndoes not happen before Y , it may read from Y , and so C D -SCH E C K E R will add the pair (value(Y \n), thread(Y )) to the set futurevalues(Xi) for each i = 1, . . . , n (if Y s thread did not yet exist \nat the time of Xi, it will use an appropriate ancestor thread). On subsequent executions, CDSCH E C K \nE R will diverge from previous behavior and explore executions in which load Xi chooses a pair (v, t) \nfrom futurevalues(Xi) and reads the value v. In our example, this allows C D -SCH E C K E R to simulate \nthe load reading the value 1 that is written by the later store. We next need to verify that a later \nstore (from thread t or one of its descendants) will still write the value 1 and that the memory model \nconstraints allow the load to read from the store.  6.1 Promising a Future Value When CDSC H E C K E \nR backtracks in order to evaluate a load using a future value a speculative load behavior we can\u00adnot \nprecisely associate the future value with a particular store that will generate it; any dependencies \non the value observed might cause divergent program behavior, so that in the new execution, several later \nstores may generate the observed value (validating the observation), or else such a store may no longer \noccur (making the observation infeasible). For every speculative load (v, t) made in an execu\u00adtion, CDSCH \nE C K E R establishes a promised future value (or promise) an assertion that, at some later point in \nthe execu\u00adtion, thread t (or one of its descendants) will perform a store that can legally pass value \nv back to the speculative load. In our example, CDSCH E C K E R would generate a promise when it simulates \nthe load in Line 4 reading a future value of 1 from the pair (1, threadB). This promise asserts that \na later store from threadB will write the value 1 and that the load can read from that store. Once CDSCH \nE C K E R detects such a store, we consider the promise to be satis.ed, and we can remove the promise \nfrom future consideration. In our example, the promise would be satis.ed when the store in Line 10 writes \nthe value 1. If, however, CDS CH E C K E R .nds that such a store is no longer possible, then the promise \nfails and we must discard the execution as invalid. Because programs create threads dynamically, loads \nmay read future values from stores whose threads do not yet exist at the time of the load. So when establishing \na promise from an observed future value pair (v, t), we assert that thread t or one of its descendants \nwill write the value v. Thus, a promise must track the set of threads which are available to satisfy \nthe promise. This set grows when one of its member threads forks another thread, and it shrinks when \none of its member threads can no longer satisfy the promise (e.g., the thread has completed). If at any \npoint this set is empty, then the promise fails. We must allow a speculative load to read not only from \nthe .rst satisfactory store to follow it in the execution order but also from subsequent stores. Thus, \nwe model two execu\u00adtion behaviors for each store: one in which the store chooses to satisfy a matching \npromise and one in which it chooses not to do so, instead allowing a later store to satisfy it. Sending \nback a future value may cause an execution to diverge such that its promise is never satis.ed nor can \nthe model-checker ever rule out the possibility that it will even\u00adtually be satis.ed. To address this, \npromises expire if they are not resolved by the expected number of program opera\u00adtions plus a tunable \nthreshold. 6.2 Treating Promises as Stores After a load observes a promised future value, we assume \nthat some store will eventually satisfy it, and so we must allow subsequent loads to observe the same \nstore. Rather than generating independent promises for each instance of the observed value, we track \nthese speculative rf relations by treating the promise as a placeholder for a future store; we can then \nadd this placeholder to the appropriate loads may-read-from set. In practice, then, the may-read-from \nset for a given load is composed of three separate components: stores from earlier in the execution order; \nits futurevalues set; and the set of yet-unsatis.ed promises for the same memory location. Over the lifetime \nof a promised future value (that is, the period between its generation and satisfaction/invalidation), \nwe can build a form of modi.cation order constraints for it in much the same way as with non-speculative \nstores. For example, whenever a promise can be satis.ed only by a single thread, we can order it after \nall operations in that thread (in the sb relation, and therefore in hb and mo); and we know which loads \nread from the promised value, so we can apply the CO H E R E N C E implications. These mo constraints \nare useful for reasoning about the feasibility of a promise. For instance, if an execution ob\u00adserves \npromised values in an inconsistent order, we can de\u00adtect a graph cycle and terminate the execution. Addition\u00adally, \nthe modi.cation order can tell us when a thread can no longer satisfy a promise, aiding us in eliminating \nunsat\u00adis.able promises. For example, CO H E R E N C E implies that a load A cannot read from a store \nC whenever there exists a hbmo store B such that A -. B -. C. Thus, when such a B exists, we eliminate \nC s thread from satisfying a promise to load A. If instead we encounter a store that satis.es a promise, \nwe can merge the promise and store nodes in the constraints graph, retaining the constraint information \nthat we already gathered.  6.3 Synchronization Ordering Allowing loads to see values written by stores \nthat appear later in the execution order may yield a synchronization re\u00adlation directed backward in the \nexecution order. Such a syn\u00adchronization would break any code (e.g., libraries or oper\u00adating system calls) \nthat used uninstrumented memory ac\u00adcesses to implement normal loads and stores. Moreover, it would require \ncomplicated mechanisms to ensure that nor\u00admal shared memory accesses observe the correct values. We observe \nthat since the speci.cation guarantees that happens-before is acyclic (\u00a71.10p12 [3]), we can address \nthis problem by ensuring that we always direct hb forward in the execution trace (note that hb must be \nacyclic). If hb is always consistent with the execution order of program frag\u00adments, normal loads and \nstores (including those in libraries and in many operating system invocations) will behave as expected; \nreading the last-written value from memory will always be consistent with the happens-before behavior \nin\u00adtended in the original program. This also explains another design decision made in C DSC H E C K E \nR: rather than instru\u00admenting all shared memory loads to read from the correct stores, CDSCH E C K E \nR generally leaves non-atomic memory accesses uninstrumented (with the exception of a happens\u00adbefore \nrace detector).  We now present a few observations we use in guaran\u00adteeing that hb remains consistent \nwith the execution order. Because sequenced-before is trivially consistent, our guar\u00adantee reduces (in \nthe absence of memory order consume) simply to the guarantee that synchronizes-with forms con\u00adsistently \nwith the execution order. We achieve this guaran\u00adtee in two parts. First, whenever we detect an inconsistent \nsynchronization relation, we terminate the execution as in\u00adfeasible. Second, we must ensure that whenever \nsuch termi\u00adnation occurs, we will also explore an equivalent execution with well-formed synchronization; \nthus, we backtrack when\u00adever an execution trace encounters a load-acquire ordered before a store-release \nto the same location. Finally, note that if we extend our approach to include consume operations as described \nin Section 4.4, this discussion of synchronizes\u00adwith, load-acquire, and store-release can be trivially \nex\u00adtended to dependency-ordered-before, load-consume, and store-release. 7. Release Sequences Thus far, \nour discussion has assumed that release/acquire synchronization only occurs when a load-acquire reads \nfrom a store-release. Unfortunately, such a simplistic synchro\u00adnization criteria would force implementations \nof common synchronization constructs to declare more atomic opera\u00adtions with release or acquire memory \norders instead of re\u00adlaxed and thus generate suboptimal compiler output (e.g., with extraneous fence \ninstructions). To address this prob\u00adlem, the C/C++ memory model de.nes a release sequence (\u00a71.10p7 [3]), \nwhich both extends the conditions under which a load-acquire and store-release synchronize and cor\u00adrespondingly \nincreases the complexity of determining syn\u00adchronization relationships as they form. We summarize the \nde.nition (note that all operations in consideration must act on the same atomic object): a release sequence \nconsists of a release operation A the release head followed by a contiguous subsequence of the modi.\u00adcation \norder consisting only of (1) stores in the same thread as A or (2) read-modify-write operations; a non-RMW \nstore from another thread breaks the sequence. Figure 5 shows a release sequence headed by A and followed \nby modi.cations B, C, and D; note how a chain of RMW s (encircled with a dotted boundary) may extend \nthe release sequence beyond the thread which contained the release head A. Then, we rede.ne release/acquire \nsynchronization4: a store-release A synchronizes with a load-acquire B if B reads from a modi.cation \nM in the release sequence headed by A [3]. In Figure 5, the load-acquire E reads from D, which is part \nof the release sequence headed by A so sw A -. E. This de.nition of release/acquire synchronization poses \nseveral challenges as we attempt to eagerly form the sw re\u00adlation, since CDS CH E C K E R does not establish \nthe modi.\u00adcation order eagerly. For one, future values allow the possi\u00adbility of lazily-satis.ed reads-from \nrelationships, so we may not establish the modi.cation order of a read-modify-write operation until its \nread portion is satis.ed. More generally, recall that CD SCH E C K E R uses a constraints-based approach \nto establishing modi.cation order, so at a given point in a program execution, two modi.cations may be \nunordered with respect to each other, leaving us uncertain as to whether or not a given sequence of modi.cations \nis contiguous (e.g., in Figure 5 we must guarantee that no non-RMW store M momo exists in another thread \nsuch that A -. M -. B). Either of these two factors may prevent CDSCH E C K E R from ea\u00adgerly deciding \nsynchronization when exploring load-acquire operations, so we resort to lazy evaluation. Lazy evaluation \nof release sequences means that for any release/acquire pair whose corresponding release sequence we \ncannot establish or invalidate with certainty at .rst dis\u00adcovery, C D SCH E C K E R leaves the pair unsynchronized5 \nand places it into a set of pending release sequences, along with any unconstrained (or loose) stores \nwhich might break up the release sequence. By the end of the execution, a pro\u00adgram will usually build \nup enough constraints to mo such that CDSCH E C K E R can resolve these pending release se\u00adquences deterministically \nand drop the release/acquire pair from the pending set. However, if at the end of a program execution \nthe constraints are still undecided for one or more pending release sequences, then CDSCH E C K E R must \nsearch for a particular constraints solution by selecting one of two possibilities for each pending sequence: \neither that one of the loose stores breaks the sequence, or that the sequence is contiguous, causing \nthe release/acquire pair to synchronize. Selections may not be independent (one contiguous release sequence \nmay imply another, for instance) and so many so\u00adlutions are infeasible. Now, sound model-checking does \nnot require exploration of all possible solutions, as some solutions only allow a sub\u00adset of behaviors \nexhibited by an equivalent, less-constrained execution. Particularly, in a constraints problem where \none solution might result in no additional synchronization and a second solution results in one or more \nadditional release/ac\u00adquire synchronizations, the .rst solution must exhibit a su\u00adperset of the erroneous \nbehaviors (e.g., data races) exhib\u00adited by the second one. Thus, an optimized search would prioritize \nconstraint solutions where all pending release se\u00ad 4 This de.nition subsumes the previous de.nition; \na store-release is in the release sequence headed by itself. 5 Lazy synchronization is acceptable because \nan execution in which syn\u00adchronization does not occur can only exhibit a superset of behaviors seen in \nthe equivalent synchronizing execution.   Figure 5. An example release sequence. Program execution \nis generally ordered from left to right. quences are broken (i.e., non-contiguous); such a minimally\u00adsynchronizing \nsolution precludes the need to explore other release sequence combinations in which the release se\u00adquences \nresolve to some non-empty set of synchronizations. The discussion so far has failed to account for the \nef\u00adfect of delayed synchronization on the rest of the model\u00adchecking process, where we previously assumed \nthat CD-SCH E C K E R establishes synchronization eagerly. When de\u00adlayed resolution of a release sequence \ncauses synchroniza\u00adtion, CDSCH E C K E R must perform a number of updates for all clock vectors and mo-graph \nedges that are dependent on sw this update. A newly-established relation X -. Y , where Y is in the interior \nof the execution trace, must generate a cascading update in the clock vectors for all operations which \nhave previously synchronized with Y (previously\u00adunordered operations are unaffected). Additionally, each \nup\u00addated clock vector may yield new information about mo con\u00ad sw straints. Thus, after such a lazy synchronization \nX -. Y , C DSC H E C K E R performs an iterative pass over all opera\u00adtions ordered after Y in the execution \norder, recalculating the happens-before clock vectors and mo constraints. Lazy synchronization presents \na few other problems for C DSC H E C K E R. For one, it may reveal that rf is inconsis\u00adtent with hb long \nafter the relevant load, causing unneces\u00adsary state-space exploration. Furthermore, because lazy syn\u00adchronization \nmay protect the memory accesses which previ\u00adously constituted a data race, our happens-before race detec\u00adtor \nmust delay realizing data races until there are no pending synchronizations. Despite this discussion \nof the complexity involved in re\u00adlease sequences, we suspect that most valid programs will never incur \nsigni.cant overhead when resolving release se\u00adquences. In our tests of real data structures, all release \nse\u00adquences have been trivially-resolvable: either a load-acquire reads directly from a store-release \nor it reads from a chain of one or more RMW s.6 With the former, synchronization is immediately evident, \nand with the latter, the chain of RMW s guarantees a contiguous subsequence of the modi.cation or\u00adder \n(see Figure 4, R M W ATO M I C I T Y). Such programs will 6 We also observed simple release sequences \nin the presence of fence oper\u00adations (see Appendix A). never incur the costs of the more complicated \nconstraints checks for determining a contiguous subsequence of mo. 8. Liveness and Fairness Some programs \npresent in.nite spaces of execution when al\u00adlowed to continually read a particular value from an atomic \nobject, even after new values have been written; C and C++ require that these new values become visible \nto all other threads in a .nite period of time (\u00a71.10p25 [3]), posing a practicality problem for our \nexhaustive search. We conclude that, for some programs which rely on memory system live\u00adness, we must \ntrade off state-space coverage for liveness. CDSCH E C K E R provides users with a runtime-con.gurable \nbound on the number of times a thread can read from the same store while the modi.cation order contains \nanother later store to the same location. A related issue arises for sequentially consistent atom\u00adics; \nthread starvation can prevent some algorithms from ter\u00adminating. CDSCH E C K E R supports the C H E S \nS [34] fairness algorithm through the use of thread-yields placed in the program under test. Or, if a \nuser cannot insert appropriate yields, we also support a tunable fairness parameter such that threads \nwhich are enabled suf.ciently many times within an execution window without taking a step should receive \npri\u00adority for execution, allowing users to automatically balance fairness and completeness. 9. Evaluation \nBecause C++11 is so new, there are few tools that test pro\u00adgrams under its memory model and few benchmarks \nagainst which to run. For those tools that do exist, there are limita\u00adtions either on scalability (they \ncan only test very small pro\u00adgrams) or on soundness (they miss a signi.cant number of potential program \nbehaviors). We evaluated CDSCH E C K E R against these tools where possible, while separately measur\u00ading \nC DSCH E C K E R s performance on real data structures. We ran our evaluations on an Ubuntu Linux 12.04 \nmachine with an Intel Core i7 3770 CPU. We have made both our model-checker and benchmarks publicly available \nat http: //demsky.eecs.uci.edu/c11modelchecker.html. We compiled and ran our evaluations with compiler \nop\u00adtimizations enabled (GCC s -O3 .ag). However, because we implement instrumented versions of atomic \noperations within CDSCH E C K E R s (opaque) shared library, the com\u00adpiler has limited ability to reorder \nthe atomic operations in the unit tests, and so compiler optimizations performed on the program under \ntest do not affect the correctness of model-checking. To verify this, we studied the implemen\u00adtation \nof atomic operations in GCC and clang/LLVM. Both compilers utilize library headers which we can easily \nsub\u00adstitute with CDSCH E C K E R s header; thus, we transform atomic operations into function calls which \ncannot be re\u00adordered. Additionally, a simple experiment showed no be\u00ad  Benchmark # Executions # Feasible \nTotal Time (s) Chase-Lev deque 748 81 0.14 SPSC queue 18 11 0.01 SPSC queue (bug free) 19 16 0.02 Barrier \n10 10 0.01 Dekker critical section 19,319 2,313 3.22 MCS lock 18,035 14,017 3.61 MPMC queue 40,148 13,028 \n7.66 M&#38;S queue 272 114 0.07 Linux RW lock 54,761 1,366 10.56 Figure 6. Benchmark Results havioral \ndifferences in our benchmarks results when using GCC to compile them with and without optimization. \n9.1 Data Structure Benchmarks For testing CDSCH E C K E R on real code, we have gath\u00adered .ve data structure \nimplementations a synchroniza\u00adtion barrier, a mutual exclusion algorithm, a contention\u00adfree lock, and \ntwo different types of concurrent queues downloaded from various publicly-accessible Internet web\u00adsites, \nand a work stealing deque taken from [29]. Addition\u00ad ally, we ported our own implementations of the Linux \nker\u00adnel s reader-writer spinlock from its architecture-speci.c as\u00adsembly implementations and the Michael \nand Scott queue from its original C and MIPS source code [33]. Most benchmarks were originally written \nsimply as data structure implementations, so we wrote test drivers for many of them in order to run them \nunder CDSCH E C K E R. We brie.y describe each data structure, our test methodology, and our performance \nresults and analysis. For our perfor\u00admance results (Figure 6), we record the total number of times C \nDSC H E C K E R executed the test program (# Executions) and the number of executions whose behavior \nwas consistent with the memory model (# Feasible). The ratio of the feasi\u00adble executions to the total \nnumber of executions provides a measure of the overhead of exploring infeasible executions. Many benchmarks \nhave an in.nite space of executions under memory systems that do not guarantee liveness, so for all our \ntests, we ran CDSCH E C K E R with a memory liveness parameter of 2 (see Section 8). For all benchmarks \nwith non-atomic shared memory, we manually instrumented the normal memory accesses to check for data \nraces. Chase-Lev Deque: We took this implementation from a peer-reviewed, published C11 adaptation of \nthe Chase-Lev deque [29]. It predominantly utilizes relaxed operations (for ef.ciency) while utilizing \nfences and release/acquire syn\u00adchronization to establish ordering. While the paper proves an ARM implementation \ncorrect, it does not contain a cor\u00adrectness proof for its C11 implementation. Our test driver for this \nbenchmark utilizes two threads in which the thread that owns the deque pushes 3 work items and takes \n2 work items while the other thread steals a work item. Our model-checker discovered a bug in the published \nim\u00adplementation. The bug occurs when both a steal and push operation occur concurrently and the push \noperation resizes the deque. The bug reveals itself as a load from a potentially uninitialized memory \nlocation. We contacted the paper s au\u00adthors and they con.rmed the bug in the C11 implementation. SPSC \nqueue: This single-producer, single-consumer queue allows concurrent access by one reader and one writer \n[7]. We utilize the test driver provided along with the queue, which uses two threads one to enqueue \na sin\u00adgle value and the other to dequeue it and verify the value. This queue utilizes seq-cst atomics, \na C++ mutex/condi\u00adtion variable and only a few non-seq-cst atomics, allowing CDSCH E C K E R to easily \nreduce the search space. It con\u00adtained a known bug a deadlock which CDSCH E C K E R detected on its .rst \nexecution, pruning the search space early and resulting in fewer executions for the buggy benchmark than \nfor our modi.ed bug-free version. Barrier: This implements a synchronizing barrier [1], where a given \nset of threads may wait on the barrier, only continuing when all threads have reached the barrier. The \nbarrier should synchronize such that no memory operation occurring after the barrier may race with a \nmemory operation placed before the barrier. The implementation is simple and contentious, as the .rst \nn - 1 threads will spin on a global .ag, waiting for the nth thread to reach the barrier. Our test driver \nutilizes two threads with a non-atomic shared memory operation executed on either side of the barrier, \none in each thread. Because the barrier is implemented with seq-cst atomic operations, it exhibits relatively \nfew behaviors those deter\u00admined by simple thread interleavings. Under a fair schedule, this test required \nonly 7 executions. Dekker critical section: This implements a simple crit\u00adical section using Dekker s \nalgorithm [4], where a pair of non-atomic data accesses are protected from concurrent data access. This \nbenchmark successfully utilizes sequentially consistent, release, and acquire fences to establish ordering \nand synchronization. Contention-free lock: This contention-free lock imple\u00adments the algorithm proposed \nby Mellor-Crummey and Scott (known as an MCS lock) [5, 32]. The lock acts like a concurrent queue, where \nwaiting threads are queued .rst\u00adin, .rst-out. Our test driver uses two threads, each of which alternates \nbetween reading and writing the same shared vari\u00adable, releasing the lock in between operations. As with \nseveral other benchmarks, heavy usage of non\u00adseq-cst operations in multiple threads required exploration \nof a larger state space; weak loads and stores provide many more potential combinations of store/load \npairs in the rf relation. MPMC queue: This multiple-producer, multiple\u00adconsumer queue allows concurrent \naccess by multiple readers and writers [6]. Our test driver runs two identical threads. Each thread .rst \nenqueues an item and then dequeues an item.  M&#38;S queue: This benchmark is an adaptation of the Michael \nand Scott lock free queue [33] to the C/C++ mem\u00ad ory model. Our adaptation uses relaxed atomics when \npos\u00adsible. Our test driver runs two identical threads. Each thread .rst enqueues an item and then dequeues \nan item. Linux reader-writer lock: A reader-writer lock allows either multiple readers or a single writer \nto hold the lock at any one time but no reader can share the lock with a writer. We ported this benchmark \nfrom a Linux kernel implementa\u00adtion, likely making this the most deployed example out of all our benchmarks. \nTo test the Linux reader-writer lock, our test driver runs two identical threads, with a single rwlock \nt protecting a shared variable. Each thread reads the variable under a reader lock, then writes to the \nvariable under the protection of a writer lock. This benchmark utilizes a large number of relaxed mem\u00adory \noperations, thoroughly testing the ef.ciency of our re\u00adlaxed model optimizations. In fact, our na\u00a8ive \nearly imple\u00admentations of future values typically took thirty or more minutes to complete, whereas the \ncurrent results show an exploration time of under 11 seconds.  9.2 Litmus Tests To help verify that \nC DSC H E C K E R performs sound explo\u00adration of the memory model, we tested it against a set of litmus \ntests, including the tests described in Nitpicking [10] as well as a few of our own custom tests. With \nthe Nitpick\u00ading litmus tests, we wrote assertion-based tests when possi\u00adble, and manually checked other \nproperties (e.g., when test\u00ading for the existence, rather than avoidance, of a particular behavior). \nWe ran all the listed relaxed, release/acquire and seq-cst tests, all of which exhibited the expected \nbehaviors. Whereas the Nitpicking litmus tests only tested the memory ordering behaviors of loads and \nstores, we per\u00adformed additional tests to verify the treatment of, e.g., read-modify-writes in CDS CH \nE C K E R. In one such test we ran two threads, with each thread performing n identical fetch add(1) \noperations on a single variable. We veri.ed that we see the correct number of distinct execution behav\u00adiors \n(enumerating rf ) and that each execution yields a sum of 2n. We performed other similar tests and checked \nthe com\u00adbinatorial behavior.  9.3 Comparison to CP P M E M and Nitpick Researchers have developed two \ntools CP P M E M [8] and Nitpick [10] for exploring the behaviors of short code fragments under the C/C++ \nmemory model. Both of these tools are targeted toward understanding the memory model and not toward testing \nreal code. Additionally, Nitpick is not publicly available, and due to various constraints of CP P M \nE M, it is impossible to port our benchmarks to CP P -M E M. Hence, we cannot directly compare these \ntools to CD-SCH E C K E R using our benchmarks. Instead, to roughly compare C DSC H E C K E R to Nitpick, \nwe reconstructed the largest relaxed WRC example for which they published results. Their example contained \nad\u00additional constraints to limit the exploration to a subset of the legal executions by constraining \nloads to speci.c values, while CDSCH E C K E R is intended to explore all legal execu\u00adtions of the program \nand hence CDSCH E C K E R must explore a much larger space of executions. CD SCH E C K E R took 0.03 \nseconds to explore all possible executions for this example, while the published results show that Nitpick \ntook 982 sec\u00adonds to explore a subset of the results. We then ran our un\u00adrestricted version of the benchmark \non CP P M E M, and it took 472.87 seconds to complete. C DSC H E C K E R is signi.cantly faster than \nboth CP P M E M and Nitpick as both of those tools make modi.cation orders explicit. CDSC H E C K E R \navoids enumerating modi.cation orders, thereby exponentially de\u00adcreasing its search space. The other \ntwo tools also use generic search or SAT solving frameworks whereas C D-SCH E C K E R has been designed \nspeci.cally for the C/C++ memory model and can leverage memory model constraints to prune its search. \n10. Related Work Researchers have created tools to .nd bugs in concur\u00adrent data structures. State-based \nmodel-checkers such as SPIN [23] can be used to debug designs for concurrent data structures. The C H \nE S S [34] tool is designed to .nd and re\u00ad produce concurrency bugs in C, C++, and C#. It systemat\u00adically \nexplores thread interleavings. However, it can miss concurrency bugs as it does not explore all thread \ninterleav\u00adings nor does it reorder memory operations. Line-Up [13] extends C H E S S to check for linearization. \nLike CH E S S, it can miss bugs that are exposed by reordering of memory operations. The Inspect tool \ncombines stateless and stateful model-checking to model-check C and C++ code [38 40]. The Inspect tool \nchecks code using the sequential consis\u00adtency model rather than the more relaxed memory model of the \nC/C++ standards and therefore may miss concurrency bugs arising from reordered memory operations. Adversarial \nmemory increases the likelihood of observing relaxed mem\u00adory system behavior during testing [19]. While \nit helps to uncover rare erroneous behaviors, it makes no guarantee of exhaustive testing. Moreover, \nadversarial memory is unable to simulate executions in which a load observes the value of a store that \nhas not yet happened and therefore cannot catch bugs that are exposed by such behavior. CDSCH E C K E \nR can exhaustively explore a data structure s behavior for a given input and simulates loads that observe \nvalues of stores that appear later in the execution order. State-based model-checkers have been developed \nfor C# [24] and Java [15] that use reordering tables. As the C/C++11 memory model is not based on reordering \ntables, these approaches are not applicable to C/C++.  Other tools have been developed that systematically \nex\u00adplore interleavings and memory operation reorderings. The Relacy race detector [37] systematically \nexplores thread in\u00ad terleavings and memory operation reorderings for C++11 code. The Relacy race detector \nhas a number of limitations that cause it to miss executions allowed by the C/C++ mem\u00adory model. Like \nCDS CH E C K E R, Relacy imposes an execu\u00adtion order on the program under test. However, Relacy can\u00adnot \nproduce executions (allowed by the memory model) in which loads read from stores that appear later in \nthe execu\u00adtion order. Moreover, Relacy derives the modi.cation order from the execution order; it cannot \nsimulate (legal) execu\u00adtions in which the modi.cation order is inconsistent with the execution order. \nRelacy also does not support partial order reduction. Researchers have formalized the C++ memory model \n[8]. The CP P M E M tool is built directly from the formalized spec\u00adi.cation with a primary goal of allowing \nresearchers to ex\u00adplore implications of the memory model. It explores all le\u00adgal modi.cation orders and \nreads-from relations a source of redundancy and therefore must search a signi.cantly larger search space \nthan CDS CH E C K E R, whose search al\u00adgorithm limits redundancy by only exploring the space of legal \nreads-from relations. Furthermore, at this point CP P -M E M lacks support for much of the C/C++ language. \nNitpick translates the memory model constraints into SAT problems and then uses a SAT solver to .nd legal \nexecutions [10]. Sim\u00ad ple experiments reveal that CDSCH E C K E R is signi.cantly faster than either \nof these tools. Several tools have been designed to detect data races in code that uses standard lock-based \nconcurrency control [16 18, 30, 36]. These tools typically verify that all accesses to shared data are \nprotected by a locking discipline. They are not designed to check concurrent code that makes use of low-level \natomic operations. In the context of relaxed hardware memory models, re\u00adsearchers have developed tools \nfor inferring the necessary fences [27] and stateful model-checkers [25, 26, 35]. Researchers have also \nargued that reasoning about re\u00adlaxed memory models is challenging and have made a case that compilers \nshould preserve sequential consistency [31]. Whether such approaches can replace the need for a re\u00adlaxed \nmemory model depends to some degree on the mem\u00adory models of future processors. We agree with the authors \nregarding the dif.culty of reasoning about relaxed memory models, and we believe that tool support is \nnecessary. 11. Conclusion The C/C++ memory model promises to make it possible to write ef.cient, portable \nlow-level concurrent data structures. The weak memory model that C/C++ provides for these low\u00adlevel operations \ncan result in unexpected program behav\u00adiors and can make writing correct code challenging. CD-SCH E C \nK E R is the .rst tool that can both test real concurrent data structures while still simulating all \nof the weak memory model behaviors that C/C++ implementations are likely to produce. Our results indicate \nthat CDSCH E C K E R can suc\u00adcessfully test real low-level concurrent code. Acknowledgments This project \nwas partly supported by a Google Research Award and by the National Science Foundation under grants CCF-0846195, \nCCF-1217854, CNS-1228995, and CCF\u00ad1319786. References [1] http://stackoverflow.com/questions/8115267/ \nwriting-a-spinning-thread-barrier-using-c11-atomics. Oct. 2012. [2] ISO/IEC 9899:2011, Information technology \n programming languages C. [3] ISO/IEC 14882:2011, Information technology program\u00adming languages C++. \n [4] http://www.justsoftwaresolutions.co.uk/ threading/. Dec. 2012.  [5] http://cbloomrants.blogspot.com/2011/07/ \n 07-18-11-mcs-list-based-lock_18.html. Oct. 2012. [6] http://cbloomrants.blogspot.com/2011/07/ 07-30-11-look-at-some-bounded-queues.html. \n Oct. 2012. [7] https://groups.google.com/forum/#!msg/comp. programming.threads/nSSFT9vKEe0/7eD3ioDg6nEJ. \n Oct. 2012. [8] M. Batty, S. Owens, S. Sarkar, P. Sewell, and T. Weber. Math\u00adematizing C++ concurrency. \nIn Proceedings of the Symposium on Principles of Programming Languages, 2011. [9] M. Batty, M. Dodds, \nand A. Gotsman. Library abstraction for C/C++ concurrency. In Proceedings of the Symposium on Principles \nof Programming Languages, 2013. [10] J. C. Blanchette, T. Weber, M. Batty, S. Owens, and S. Sarkar. \nNitpicking C++ concurrency. In Proceedings of the 13th International ACM SIGPLAN Symposium on Principles \nand Practices of Declarative Programming, 2011. [11] H. Boehm. Can seqlocks get along with programming \nlan\u00adguage memory models? In Proceedings of the 2012 ACM SIGPLAN Workshop on Memory Systems Performance \nand Correctness, 2012. [12] H. J. Boehm and S. V. Adve. Foundations of the C++ con\u00adcurrency memory model. \nIn Proceedings of the 2008 ACM SIGPLAN Conference on Programming Language Design and Implementation, \n2008. [13] S. Burckhardt, C. Dern, M. Musuvathi, and R. Tan. Line-up: A complete and automatic linearizability \nchecker. In Proceed\u00adings of the 2010 ACM SIGPLAN Conference on Programming Language Design and Implementation, \n2010. [14] C. Click. A lock-free hash table. http://www. azulsystems.com/events/javaone_2007/2007_ LockFreeHash.pdf, \nMay 2007.  [15] A. De, A. Roychoudhury, and D. D Souza. Java memory model aware software validation. \nIn Proceedings of the 8th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering, \n2008. [16] T. Elmas, S. Qadeer, and S. Tasiran. Goldilocks: A race and transaction-aware Java runtime. \nIn Proceedings of the 2007 ACM SIGPLAN Conference on Programming Language Design and Implementation, \n2007. [17] D. Engler and K. Ashcraft. RacerX: Effective, static detec\u00adtion of race conditions and deadlocks. \nIn Proceedings of the Nineteenth ACM Symposium on Operating Systems Princi\u00adples, 2003. [18] C. Flanagan \nand S. N. Freund. FastTrack: Ef.cient and precise dynamic race detection. In Proceedings of the 2009 \nACM SIGPLAN Conference on Programming Language Design and Implementation, 2009. [19] C. Flanagan and \nS. N. Freund. Adversarial memory for de\u00adtecting destructive races. In Proceedings of the 2010 ACM SIGPLAN \nConference on Programming Language Design and Implementation, 2010. [20] C. Flanagan and P. Godefroid. \nDynamic partial-order reduc\u00adtion for model checking software. In Proceedings of the Sym\u00adposium on Principles \nof Programming Languages, Jan. 2005. [21] P. Godefroid. Partial-order methods for the veri.cation of \ncon\u00adcurrent systems: An approach to the state-explosion problem. Lecture Notes in Computer Science, 1996. \n[22] P. Godefroid. Model checking for programming languages us\u00ading VeriSoft. In Proceedings of the Symposium \non Principles of Programming Languages, 1997. [23] G. J. Holzmann. The SPIN Model Checker: Primer and \nReference Manual. Addison-Wesley Professional, 1st edition, 2003. [24] T. Q. Huynh and A. Roychoudhury. \nA memory model sensi\u00adtive checker for C#. In Proceedings of the 14th International Conference on Formal \nMethods, 2006. [25] B. Jonsson. State-space exploration for concurrent algorithms under weak memory orderings. \nSIGARCH Computer Archi\u00adtecture News, 36(5):65 71, June 2009. [26] M. Kuperstein, M. Vechev, and E. Yahav. \nAutomatic inference of memory fences. In Proceedings of the Conference on Formal Methods in Computer-Aided \nDesign, 2010. [27] M. Kuperstein, M. Vechev, and E. Yahav. Partial-coherence abstractions for relaxed \nmemory models. In Proceedings of the 2011 ACM SIGPLAN Conference on Programming Language Design and Implementation, \n2011. [28] L. Lamport. Time, clocks, and the ordering of events in a distributed system. Communications \nof the ACM, 21(7):558 565, July 1978. [29] N. M. L e, A. Pop, A. Cohen, and F. Zappa Nardelli. Correct \nand ef.cient work-stealing for weak memory models. In Pro\u00adceedings of the 18th ACM SIGPLAN Symposium \non Principles and Practice of Parallel Programming, 2013. [30] B. Lucia, L. Ceze, K. Strauss, S. Qadeer, \nand H. Boehm. Con\u00ad.ict exceptions: Simplifying concurrent language semantics with precise hardware exceptions \nfor data-races. In Proceed\u00ad ings of the 37th Annual International Symposium on Com\u00adputer Architecture, \n2010. [31] D. Marino, A. Singh, T. Millstein, M. Musuvathi, and S. Narayanasamy. A case for an sc-preserving \ncompiler. In Proceedings of the 2011 ACM SIGPLAN Conference on Pro\u00adgramming Language Design and Implementation, \n2011. [32] J. M. Mellor-Crummey and M. L. Scott. Synchronization without contention. In Proceedings of \nthe Fourth Interna\u00adtional Conference on Architectural Support for Programming Languages and Operating \nSystems, pages 269 278, 1991. [33] M. M. Michael and M. L. Scott. Simple, fast, and practi\u00adcal non-blocking \nand blocking concurrent queue algorithms. In Proceedings of the Fifteenth Annual ACM Symposium on Principles \nof Distributed Computing, 1996. [34] M. Musuvathi, S. Qadeer, P. A. Nainar, T. Ball, G. Basler, and I. \nNeamtiu. Finding and reproducing Heisenbugs in concur\u00adrent programs. In Proceedings of the 8th Symposium \non Op\u00aderating Systems Design and Implementation, 2008. [35] S. Park and D. L. Dill. An executable speci.cation \nand veri.er for relaxed memory order. IEEE Transactions on Computers, 48, 1999. [36] S. Savage, M. Burrows, \nG. Nelson, P. Sobalvarro, and T. An\u00adderson. Eraser: A dynamic data race detector for multi\u00adthreaded programs. \nACM Transactions on Computing Sys\u00adtems, 15:391 411, Nov. 1997. [37] D. Vyukov. Relacy race detector. \nhttp://relacy. sourceforge.net/, 2011 Oct. [38] C. Wang, Y. Yang, A. Gupta, and G. Gopalakrishnan. Dy\u00adnamic \nmodel checking with property driven pruning to detect race conditions. ATVA LNCS, (126 140), 2008. [39] \nY. Yang, X. Chen, G. Gopalakrishnan, and R. M. Kirby. Ef.\u00adcient stateful dynamic partial order reduction. \nIn Proceedings of the 15th International SPIN Workshop on Model Checking Software, 2008. [40] Y. Yang, \nX. Chen, G. Gopalakrishnan, and C. Wang. Auto\u00admatic discovery of transition symmetry in multithreaded \npro\u00adgrams using dynamic analysis. In Proceedings of the 16th International SPIN Workshop on Model Checking \nSoftware, pages 279 295, 2009. A. Fences In addition to the atomic loads, stores, and read-modify\u00adwrites \ndiscussed in the body of the paper, C and C++ sup\u00adport atomic fence operations. C/C++ fences loosely \nimitate the low-level fence instructions used in multiprocessors for ordering memory accesses and are \nincluded to allow devel\u00adopers to more ef.ciently represent their algorithms. Fences may use the release, \nacquire, rel acq, or seq cst memory orders (relaxed is a no-op and consume is an alias for acquire, \u00a729.8p5 \n[3]) and have additional modi.cation order constraints and synchronization properties, whose sup\u00adport \nwe will discuss in this appendix.     A.1 Fence Modi.cation Order Constraints C and C++ introduce \nseveral rules governing rf and mo when dealing with sequentially consistent fences. As in Sec\u00adtion 5.2, \nwe transform these rules directly into modi.cation order implications for use by C DSC H E C K E R. \nSC FE N C E S RE S T R I C T RF : Seq-cst fences impose re\u00adstrictions on the oldest store from which \na load may read (\u00a729.3p4-6 [3]).  SC FE N C E S IM P O S E M O: A pair of stores separated by seq-cst \nfences must form mo consistently with sc (\u00a729.3p7 [3]). Notably, the C++ speci.cation leaves out the \nC O L L A P S E D constraints that are presented here, but they are included in the formal model developed \nfor [8]. The report for C++ Library Issue 2130 indicates that the speci.cation committee plans to include \nthese rules in future revisions.  These implications can be applied using similar induction arguments \nto those developed in Section 5.5; because sc is a total order, we can always .nd the last store A in \neach thread that satis.es the left-hand side (if any exists). Any prior store must already be ordered \nbefore A in mo, and so we must look no further than A when building constraints for a newly-explored \nprogram operation. A.2 Fence Synchronization Besides the modi.cation order constraints imposed by se\u00adquentially \nconsistent fences, fences can induce synchroniza\u00adtion (sw) via an extension to release sequences. The \nspeci.\u00adcation de.nes a hypothetical release sequence headed by a store X as the release sequence that \nwould exist if X were a release operation. We will say that if store Y is part of the hrs hypothetical \nrelease sequence headed by X, then X -. Y rs (or similarly, X -. Y for true release sequences). We take \nthe synchronization implications in Figure 8 di\u00ad rectly from \u00a729.8p2-4. Informally, these rules cause \na load\u00adrelaxed followed by a fence-acquire to act like a load-acquire and cause a store-relaxed preceded \nby a fence-release to act like a store-release. These synchronization implications can be easily com\u00adputed \nwith simple extensions to the methods described in Section 7. In fact, a hypothetical release sequence \nmakes synchronization detection even simpler than with traditional release sequences because the loose \nstore problem is no longer an issue; as soon as we .nd any store X such that hrs X -. Y , there is no \nlonger a need to establish a contiguous modi.cation order: we only need to search for the last fence\u00adrelease \nA that is sequenced before X in the same thread. In other words, hypothetical release sequence computations \nonly require knowledge of rf (to follow the RMW chain, if any) and the intra-thread ordering sb (to .nd \nprior fence\u00adreleases) but do not require querying the partially-ordered mo-graph. SC FE N C E S RE S \nT R I C T RF =.  SC FE N C E S RE S T R I C T RF (C O L L A P S E D STO R E ) =.  SC FE N C E S RE \nS T R I C T RF (CO L L A P S E D LOA D ) =.  SC F E N C E S IM P O S E M O  =. SC FE N C E S IM P \nO S E MO (C O L L A P S E D 1S T STO R E )  =. SC FE N C E S IM P O S E MO ( CO L L A P S E D 2ND STO \nR E )  =. Figure 7. Fence modi.cation order implications. On the left side of each implication, A, \nB, C, X, and Y must be distinct.  FE N C E SY N C H RO N I Z AT I O N =.  FE N C E SY N C H RO N \nI Z AT I O N (C O L L A P S E D STO R E ) =.  FE N C E SY N C H RO N I Z AT I O N (CO L L A P S E D \nLOA D ) .  Figure 8. Fence synchronization implications A.3 Fence Backtracking Because fences can synchronize \nwith other loads, stores, or fences, we must order them properly in the execution order such that their \nsynchronization is consistent (recall Section 6.3). We extend our previous backtracking approach to accommodate \nany potential synchronization involving the fence rules in Section A.2. So, whenever CD SCH E C K E R \nobserves an acquire B ordered earlier in the execution than a release A, and we determine that A may \nsynchronize with B sw (A -. B), we must backtrack to allow the thread which performed A to execute before \nB. Note that identifying such A and B may also involve identifying, for instance, an appropriate load/store \npair X and Y (when applying FE N C E SY N C H RO N I Z AT I O N); similar observations can be made for \nthe CO L L A P S E D synchronization rules. As described in Section 4.3, we force sc to be consistent \nwith the execution order and use DPOR backtracking to ex\u00adplore the necessary interleavings of con.icting \nseq-cst oper\u00adations. To extend this to seq-cst fences, we simply say that a seq-cst fence con.icts with \nany other seq-cst operation. B. Satisfaction Cycles An issue with the C/C++ speci.cation is that it allows \npro\u00adgrams to observe various types of out-of-thin-air values through the use of satisfaction cycles, \nin which the effects of an action justify performing the action in the .rst place, pro\u00adducing unintuitive \nresults. The problem of satisfaction cy\u00adcles has already been recognized by other researchers and can \nmake it dif.cult to prove properties of code in many cases [9]. It is clear that the speci.cation authors \nwould like to rule out satisfaction cycles, and they make an attempt to rule these out with requirement \n\u00a729.3p9 that states: An atomic store shall only store a value that has been computed from constants and \nprogram input values by a .nite sequence of program evaluations, such that each evaluation observes the \nvalues of variables as computed by the last prior assignment in the se\u00adquence. The ordering of evaluations \nin this sequence shall be such that: if an evaluation B observes a value computed by A in a different \nthread, then B does not happen before A, and  if an evaluation A is included in the sequence, then every \nevaluation that assigns to the same variable and happens before A is included.  Despite efforts to disallow \nall out-of-thin-air values, in the end the C/C++ authors concede that some sub-optimal behaviors are \nnot ruled out by the speci.cation. They then simply state that implementations should not allow such \nbehavior without providing the details of exactly what this means. For instance, the speci.cation mentions \nthat the formal requirements allow r1 = r2 = 42 in the following pro\u00adgram fragment: 1 a t o m i c < i \nn t > x ( 0 ) , y ( 0 ) ; 2 3 v o i d t h r e a d A ( ) { 4 i n t r 1 = x . l o a d ( m e m o r y _ o \nr d e r _ r e l a x e d ) ; 5 i f ( r 1 = = 4 2 ) 6 y . s t o r e ( r 1 , m e m o r y _ o r d e r _ r \ne l a x e d ) ; 7 } 8 v o i d t h r e a d B ( ) { 9 i n t r 2 = y . l o a d ( m e m o r y _ o r d e r \n_ r e l a x e d ) ; 10 i f ( r 2 = = 4 2 ) 11 x . s t o r e ( 4 2 , m e m o r y _ o r d e r _ r e l a \nx e d ) ; 12 } But because CDS CH E C K E R operates as a runtime frame\u00adwork, it only sees the program \nstatements that are actually executed. So while it will explore all behaviors that do not involve a satisfaction \ncycle, CDSCH E C K E R cannot guaran\u00adtee that it explores behaviors where the behavior circularly justi.es \nitself. One such example is where the only justi.\u00adcation for taking a conditional branch is hidden behind \nthe branch. Thus, CDSCH E C K E R will never see r1 = r2 = 42 in the above example. This variation from \nthe formal reading of the C/C++ speci.cation is desirable since it prevents C D-SCH E C K E R from warning \ndevelopers about program behav\u00adior which is forbidden by the C and C++ speci.cations ( im\u00adplementations \nshould not allow such behavior ) and which should never be produced by a compiler or processor. Satisfaction \ncycles in general make verifying software in\u00adtractable. It remains an open question of how to best con\u00adstrain \nthe C/C++ memory model to disallow satisfaction cy\u00adcles while still allowing common compiler optimizations \nand achieving good performance on all architectures.  With constraints to rule out satisfaction cycles, \nthe cor\u00adrectness of CDSCH E C K E R follows from an induction on the evaluation sequence. C. Pruning \nFuture Values To reduce the search space generated by the exploration of future values, we developed \na few optimizations. With these optimizations, we attempt to avoid introducing future values when their \nintroduction is guaranteed to generate infeasible (or otherwise unnecessary) executions. Reductions in \ninfea\u00adsible future values provide a compounding reduction in over\u00adhead, since such ill-advised values \nmay generate a signi.\u00adcant amount of unproductive exploration space in between the speculative load and \nits promise resolution at which point we .nally realize an execution-ending mo-graph cy\u00adcle. Thus, we \npresent a few derived constraints for pruning those future values which, when observed, would guarantee \na cyclic mo-graph. Additionally, we introduce a few other optimizations for reducing redundant or otherwise \nunneces\u00adsary exploration. rf For any load A and store X, we can show that X . A - whenever there exists \na store B such that hbmo A -. B . B -. X. rf Allowing X -. A would yield a mo cycle in B and X, due to \nRE A D -WR I T E CO H E R E N C E. Therefore, X should never send a future value to A. Without this constraint, \nC DSC H E C K E R would send a future value, not realizing the cycle until it established the rf edge \nconcretely. Similarly, we hb do not send a future value from store B to load A if A -. B. Knowledge of \npromise behavior presents further opportu\u00adnity for optimization of future values. If a store Y is sched\u00aduled \nto satisfy an outstanding promise P , then we limit the cases in which Y sends its future value to prior \nloads Xi we avoid sending Y s future value to any load Xi whose may-read-from set contains P (as a placeholder \nfor Y ). Speci.cally, Y does not send its future value to the load X which .rst generated promise P , \nnor to any load which fol\u00adlows X in the execution since such loads may also read from P (Y can, however, \nsend its future value to loads prior to X). A speculative load X can cause a later store Y to send a \nnew future value back to X, even when Y actually depends on X. Such a cyclic dependence can potentially \ncause C D -SCH E C K E R to explore an in.nite space of infeasible execu\u00adtions. We eliminate these cycles \nby an additional constraint when sending a future value from such a store Y to a load X; we check whether \nthere exists a yet-unresolved promise cre\u00adated by a speculative load Z, where Z is between X and Y in \nthe execution order. If not, then Y can send its future value safely (subject to previously-discussed \nconstraints). If such Z does exist, however, we delay sending the future value until Z s promise is resolved \nbreaking the cycle while still allowing non-cyclic dependences to be resolved. The correctness of this \noptimization follows from the fol\u00adlowing argument. If the satisfying store S for Z does not depend on \nX observing Y s future value, then Z s promise will eventually be resolved and the future value will \nbe sent. If the satisfying store S for Z does depend on X observ\u00ading Y s future value, then either (1) \nX occurs after Z in the execution order and hence does not trigger the delay condi\u00adtion or (2) when Z \neventually reads from a different store, Y can then add its future value to futurevalues(X) (Y can only \ndepend on Z in the presence of a satisfaction cycle); the backtracking algorithm will later revisit the \ncurrent situ\u00adation without the need to send the future value as the value already exists in futurevalues(X). \n \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Writing low-level concurrent software has traditionally required intimate knowledge of the entire toolchain and often has involved coding in assembly. New language standards have extended C and C++ with support for low-level atomic operations and a weak memory model, enabling developers to write portable and efficient multithreaded code.</p> <p>Developing correct low-level concurrent code is well-known to be especially difficult under a weak memory model, where code behavior can be surprising. Building reliable concurrent software using C/C++ low-level atomic operations will likely require tools that help developers discover unexpected program behaviors.</p> <p>In this paper we present CDSChecker, a tool for exhaustively exploring the behaviors of concurrent code under the C/C++ memory model. We develop several novel techniques for modeling the relaxed behaviors allowed by the memory model and for minimizing the number of execution behaviors that CDSChecker must explore. We have used CDSChecker to exhaustively unit test several concurrent data structure implementations on specific inputs and have discovered errors in both a recently published C11 implementation of a work-stealing queue and a single producer, single consumer queue implementation.</p>", "authors": [{"name": "Brian Norris", "author_profile_id": "83358815757", "affiliation": "University of California, Irvine, Irvine, CA, USA", "person_id": "P4290323", "email_address": "banorris@uci.edu", "orcid_id": ""}, {"name": "Brian Demsky", "author_profile_id": "81100338144", "affiliation": "University of California, Irvine, Irvine, CA, USA", "person_id": "P4290324", "email_address": "bdemsky@uci.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509514", "year": "2013", "article_id": "2509514", "conference": "OOPSLA", "title": "CDSchecker: checking concurrent data structures written with C/C++ atomics", "url": "http://dl.acm.org/citation.cfm?id=2509514"}