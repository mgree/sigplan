{"article_publication_date": "10-29-2013", "fulltext": "\n Online Feedback-Directed Optimizations for Parallel Java Code Albert Noll Department of Computer Science \n ETH Zurich Abstract The performance of parallel code signi.cantly depends on the parallel task granularity \n(PTG). If the PTG is too coarse, performance suffers due to load imbalance; if the PTG is too .ne, performance \nsuffers from the overhead that is induced by parallel task creation and scheduling. This paper presents \na software platform that automati\u00adcally determines the PTG at run-time. Automatic PTG selec\u00ad tion is \nenabled by concurrent calls, which are special source language constructs that provide a late decision \n(at run-time) of whether concurrent calls are executed sequentially or con\u00adcurrently (as a parallel task). \nFurthermore, the execution se\u00admantics of concurrent calls permits the runtime system to merge two (or \nmore) concurrent calls thereby coarsening the PTG. We present an integration of concurrent calls into \nthe Java programming language, the Java Memory Model, and show how the Java Virtual Machine can adapt \nthe PTG based on dynamic pro.ling. The performance evaluation shows that our runtime system performs \ncompetitively to Java programs for which the PTG is tuned manually. Com\u00ad pared to an unfortunate choice \nof the PTG, this approach performs up to 3x faster than standard Java code. Categories and Subject Descriptors \nD.3.4 Processors [Pro\u00adgramming Languages]: Compilers Keywords JIT compilation, dynamic optimization, \nfeedback\u00addirected optimization 1. Introduction Writing fast and scalable parallel code is dif.cult. The \nrea\u00adson is that the performance of parallel code depends on nu\u00admerous hardware and software properties \nthat are not al\u00adways known (and obvious) to the programmer. One such important property is the parallel \ntask granularity (PTG). Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. \nTo copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. Request permissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, \nIndianapolis, Indiana, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509518 \nThomas Gross Department of Computer Science ETH Zurich The PTG describes the tradeoff between the overhead \nthat is associated with parallel task execution (e.g., object allo\u00adcation, scheduling) and the potential \nperformance gain. A .ne PTG provides better load balance than a coarse PTG. However, executing .ne-grained \nparallel tasks incurs more run-time overhead than executing coarse-grained parallel tasks. Figure 1 illustrates \nthis tradeoff. Figure 1. Performance impact of parallel task granularity. The x-axis in Figure 1 shows \nthe number of parallel tasks used to execute a .xed amount of work (W). For example, a value of 2 on \nthe x-axis corresponds to an execution in which W is split into 2 equally-sized parallel tasks. The y-axis \nshows the speedup over a sequential execution on a 32-core system1 . The graph in Figure 1 can be divided \ninto three regions. The left region is characterized by a poor load balance and a low overhead. The load \nis imbalanced, since a 32-core sys\u00adtem requires at least 32 parallel tasks to provide all cores with \nwork. The overhead is low, since only a small num\u00adber of parallel tasks is generated. The middle region \nhas a good load balance and incurs an overhead that has no sig\u00adni.cant impact on the execution time. \nThe third region is characterized by a good load balance (a large number of par\u00adallel tasks) but incurs \na signi.cant overhead due to paral\u00adlelism. For 100,000 parallel tasks or more, the parallel ver\u00adsion \nperforms slower than the sequential version. To sum\u00admarize, parallelism is underspeci.ed in the left \nregion, well\u00adspeci.ed in the middle region, and overspeci.ed in the right 1 See Section 7 for the description \nof the system con.guration. region. Both, overspeci.cation as well as underspeci.cation of parallelism \nhave a signi.cant negative impact on the per\u00adformance. Unfortunately, current programming techniques \nsupport only a manual determination of the PTG. In particular, pro\u00ad grammers must decide which parts \nof an application are best executed in parallel and what PTG yields good performance. At the same time, \nwe see a wide range of parallel systems in use, ranging from modest dual-core systems to systems with \n64 cores (or more). Ahead-of-time custom-tuning software for each platform is expensive, if possible \nat all. This paper presents a software platform that automati\u00adcally determines the PTG at run-time. Our \nsystem asks the programmer to overspecify parallelism. The runtime sys\u00adtem evaluates the PTG as provided \nin the source code and adapts the PTG towards a good tradeoff between load bal\u00ad ance and parallelism \noverhead. More speci.cally, we show that the runtime system effectively removes overspeci.ed parallelism \nby (i) serializing parallel tasks that incur a high overhead and (ii) merging two (or more) parallel \ntasks into a single parallel task. Note that if performance suffers from parallelism underspeci.cation, \nthe runtime system would have to auto-parallelize (parts of) the application to speed up execution. Since \ncurrent runtime systems cannot effectively auto-parallelize applications, performance problems due to \nparallelism underspeci.cation cannot be .xed by the run\u00adtime system. The automatic determination of the \nPTG is enabled by concurrent calls. Concurrent calls are special source lan\u00adguage constructs that can \nbe executed sequentially or in par\u00adallel. We discuss the integration of concurrent calls into the Java \nprogramming language and the Java Virtual Machine (JVM), including the just-in-time compiler (JIT compiler), \nand show how the JVM adapts the PTG. In particular, we present a compiler analysis that determines if \nconcurrent calls can be merged (collapsed into a single concurrent call) and the merged concurrent call \nprovides sequential consis\u00adtency [12] for data-race free programs. We therefore present an integration \nof concurrent calls into the Java Memory Model (JMM) [16]. The performance evaluation of a proto\u00ad type \nimplementation in the Jikes Research Virtual Machine (Jikes RVM) [2] shows that our runtime system performs \ncompetitively to Java programs for which the PTG is tuned manually. Compared to an unfortunate or unlucky \nchoice of the PTG, our approach performs up to 3X faster than stan\u00ad dard Java code. 2. System design \nThis section presents an overview of how concurrent calls are integrated into the Java platform. We chose \nthe Java platform for the following reasons: First, Java has a well\u00adde.ned memory model [16] that exactly \nde.nes the seman\u00ad tics of parallel Java code. Second, Java code runs in a man\u00adaged runtime environment, \nthe Java Virtual Machine (JVM), which is typically equipped with a just-in-time compiler (JIT compiler) \nand a dynamic pro.ling infrastructure. Third, Java supports annotations, which can be used to attribute \nJava lan\u00adguage constructs without having to change the language it\u00adself. Finally, there exist several \nopen source implementations of the JVM.  Figure 2. System overview. Figure 2 depicts the system overview. \nWe extend the Java programming language by two annotations: @Concurrent and @Sync. @Concurrent provides \nthe declaration of con\u00adcurrent calls. @Sync provides the declaration of synchroniza\u00adtion calls, which \ncan be used to synchronize concurrent calls. The runtime system extensions include extensions to the \nJIT compiler, the pro.ling system, and the Adaptive Op\u00ad timization System (AOS), which identi.es methods \nthat are recompiled by the JIT compiler. We extend the JIT compiler by a new intermediate representation \n(IR) that represents concurrent calls and synchronization calls explicitly. This IR is called concurrency-aware \nintermediate representation (CIR). The JIT compiler optimizes the parallel task granu\u00ad larity (PTG) in \nCIR. Standard pro.ling systems of modern JVMs are designed to track down performance bottlenecks of sequential \ncode. We present the design of a concurrency-aware dynamic pro\u00ad.ler that measures the behavior of parallel \ncode and pro\u00advides the necessary information for deciding which concur\u00adrent calls are compiled to sequential \ncode, to parallel code, and what concurrent calls are best merged to achieve good performance. The AOS \nof a JVM selects methods that are good can\u00ad didates for dynamic recompilation. We extend the standard \nAOS by the parallel code execution optimization (PCEO) al\u00ad gorithm, which uses pro.le information collected \nby the ex\u00adtended pro.ler to analyze the behavior of a particular PTG. The PCEO algorithm can change a \nparticular PTG by trig\u00ad gering a recompilation that, e.g., merges concurrent regions that were not merged \nin the previous con.guration. 3. Java semantics of @Concurrent and @Sync A method declaration that includes \nthe @Concurrent an\u00adnotation declares a concurrent method. Concurrent methods can be executed sequentially \nor in parallel. The semantics of a sequential execution and a parallel execution of a concur\u00adrent method \nare de.ned as follows:  The sequential execution of a concurrent method results in the same execution \nas if the method is not annotated.  The parallel execution of a concurrent method has the same semantics \nas if a separate thread executes the method call. I.e., a concurrent method executes asyn\u00adchronously \nwith instructions that follow the concurrent method. The separate thread that executes a concurrent call \nis called child thread and the thread that encounters the concurrent method is called parent thread. \n Sequential as well as the parallel executions of con\u00adcurrent methods must result in legal program \nbehavior. It is the responsibility of the programmer to identify code regions that have such a behavior. \nOther explicit parallel program\u00adming languages have similar requirements. For example, the Cilk programming \nlanguage [6] has the same requirements for the spawn keyword as our approach has for concurrent calls. \nAnother popular example that has similar requirements is OpenMP [1]. Parallel regions in OpenMP are executed \nby a team of N threads. The team consists of the master thread and N -1 child threads. The master thread \nis the thread that encounters the parallel region. If the OpenMP runtime sys\u00adtem decides to use only \na single thread to execute a parallel region that thread can be the master thread. This setup cor\u00adresponds \nto a sequential execution of a parallel region. A method that is annotated with @Sync is called synchro\u00adnization \nmethod. Synchronization methods can be used to synchronize concurrent calls. In particular, the execution \nof a synchronization method suspends the calling thread prior to executing the method until all concurrent \nmethods that are issued by the calling thread are retired. A concurrent method retires if all effects \nof the concurrent method are visible to the calling thread. A synchronization method proceeds im\u00admediately \nif there are no concurrent methods in .ight. Figure 3 shows how concurrent methods are declared, called, \nand synchronized. Figure 3(a) shows the declara\u00ad tion of the concurrent method C() and the synchronization \nmethod S(). Figure 3(b) shows an usage example of con\u00ad current methods and synchronization methods. If \nfoo() is called with the parameter x = 1 the output of the program shown in Figure 3(b) can either be \n\"1 2\" or \"2 1\". Since there is no synchronization method between the concurrent method calls in line \n2 and line 3, and concurrent methods execute asynchronously (if executed in parallel), the order in which \nthe concurrent methods are executed is unde.ned. 1 @C o n c ur r e n t 1 i n t f o o ( i n t x ) { 2 \ni n t C ( i n t x ) { 2 f i n a l i n t r 1 = C ( x ) ; 3 p r i n t ( x ) ; 3 f i n a l i n t r 2 = C \n( x + 1 ) ; 4 r e t u r n x + 1 ; 4 5 } 5 S ( ) ; 6 @Sync 6 r e t u r n r 1 + r 2 ; 7 v o i d S ( ) { \n} 7 } (a) Method declarations. (b) Usage example. Figure 3. Concurrent call example. The two concurrent \ncalls return a value. The variable that stores the return value must be declared final. The return value \nis initialized to the default value of the speci.ed type (e.g., null for a reference type) and contains \nthe return value only after the execution a synchronization method. The return values in Figure 3(b) \nare accessed correctly, since the synchronization method is called before the return values are accessed. \nConsequently, the return value of foo() is always 5. The JVM initializes the parameters of concurrent \ncalls and sets the return values. 3.1 Java language integration This section discusses the Java language \nintegration of con\u00adcurrent calls. More speci.cally, we present concurrent calls in the context of polymorphism, \nexceptions, and discuss the interaction with the standard Java threading API. 3.1.1 Polymorphism The \ncurrent design supports the @Concurrent and the @Sync annotation for private (and private static) methods. \nIn Java, private methods can be called only from the class in which the method is declared. As a result, \na concurrent method cannot be called \"accidently\" by, e.g., deriving from a class and calling the concurrent \nmethod of the superclass. The current design is conservative, since it allows only the implementor of \na class to invoke concurrent calls. We choose such a design, since the programmer must ensure that con\u00adcurrent \ncalls yield a legal program execution. However, the current design can be changed easily and changing \nthe de\u00adsign (i.e., relaxing the constraints which methods can be annotated with @Concurrent and @Sync) \nhas no effect on the runtime system extensions. 3.1.2 Exception handling The execution of concurrent \nmethods and synchronization methods can raise unchecked exceptions. An uncaught ex\u00adception aborts the \nexecution of the concurrent method. If the exception occurs in the child thread, the JVM propagates the \nexception to the parent thread. Exceptions that occur in a synchronization method are handled according \nto the Java language speci.cation. 3.1.3 Interaction with the Java threading API Concurrent methods \nand synchronization methods are alter\u00adnative constructs to describe concurrency in the Java source language \nand can be used in combination with the standard library-based Java threading API. Child threads are \nregular Java threads that can acquire and release monitors, wait on objects, and notify (be noti.ed by) \nother threads. One differ\u00adence of declaring concurrency using concurrent calls com\u00adpared to using standard \nJava threads is that concurrent meth\u00adods add concurrency to existing applications without having to de.ne \nnew classes and to instantiate new objects. As a result, the sequential execution of concurrent methods \nis po\u00adtentially faster than the sequential execution of parallel Java code. 4. JIT compiler extensions \nConcurrent methods and synchronization methods are rep\u00adresented explicitly in the intermediate representation \n(IR) of the JIT compiler. The standard, sequential IR is extended by four new IR nodes that describe \nconcurrent regions and the synchronization of concurrent regions. An IR that contains at least one of \nthe four new IR nodes is called CIR. The four new IR nodes are de.ned as follows: cStart: marks the \nbeginning of a concurrent region. In\u00adstructions that follow the cStart CIR-node2 are part of the same \nconcurrent region. All instructions in a concur\u00adrent region are executed by the same thread (parent thread \nor child thread).  cEnd: denotes the end of a concurrent region.  cBarrier: represents a boundary across \nwhich the JIT compiler is not allowed to reorder instructions. I.e., the code generated by the JIT compiler \nmust ensure that all instructions that are ordered before a cBarrier CIR\u00ad node are executed and retired \nbefore all instructions that follow the cBarrier CIR-node. The cBarrier CIR\u00ad nodes guarantees sequential \nconsistency [12] for data\u00ad race free programs, as explained in Section 4.1.  cSync: blocks until all \nconcurrent methods that are is\u00adsued by the thread that encounters cSync have retired.  The example in \nFigure 4(a) shows the implementation of method foo(), which contains two concurrent methods and one synchronization \nmethod. Figure 4(b) shows the repre\u00ad sentation of foo() in the CIR. The method declarations of C() and \nS() are taken from Figure 3(a). 1 i n t f o o ( i n t x ) { 1 c S t a r t ; c B a r r i e r ; 2 f i n \na l i n t r 1 = C ( x ) ; 2 r 1 = c a l l C ( x ) ; 3 3 c B a r r i e r ; cEnd ; 4 4 t1 =x+1; 5 5 c S \nt a r t ; c B a r r i e r ; 6 f i n a l i n t r 2 = C ( x + 1 ) ; 6 r 2 = c a l l C ( t 1 ) ; 7 7 c B \na r r i e r ; cEnd ; 8 8 c S y n c ; c B a r r i e r ; 9 S(); 9 call S(); 10 r e t u r n r 1 + r 2 ; \n10 c B a r r i e r ; 11 } 11 r e t u r n r 1 + r 2 ; (a) Example with two concurrent meth-(b) CIR of \nFigure 4(a). ods and one synchronization method. Figure 4. Representation of concurrent methods and syn\u00adchronization \nmethods in the source code and the CIR. Each call to C() is contained in a separate concurrent region. \nConcurrent region 1 (C1) ranges from line 1 to line 3 and concurrent region 2 (C2) ranges from line 5 \nto line 7. 2 We use the term instruction (or IR node) in the JVM context. All instructions of a concurrent \nregion are enclosed between two cBarrier CIR-nodes, a cStart and a cEnd CIR-node. The cSync CIR-node \nis followed by a cBarrier CIR-node, the synchronization method call, and another cBarrier CIR-node. The \nJIT compiler compiles concurrent methods and synchronization method calls to regular IR nodes. 4.1 Integration \nof CIR into the JMM The Java Memory Model (JMM) [16] de.nes the seman\u00ad tics of parallel Java code. The \nJMM is a relaxed-consistency model that allows the JIT compiler (and the hardware) to reorder independent \ninstructions. Despite the possible in\u00adstruction reordering, the JMM guarantees sequential consis\u00ad tency \n(SC) [12] for correctly synchronized programs, i.e., programs that contain no data races. The JMM de.nes \nstrict rules that de.ne precisely which values can be obtained by a read of a shared .eld that is updated \nby multiple threads. Concurrent methods and synchronization methods can read from and write to shared \nmemory. The explicit representa\u00adtion in the CIR requires a precise de.nition of the JMM se\u00ad mantics to \nensure compliance with the JMM. JMM semantics of cStart and cEnd : Concurrent re\u00adgions begin with the \ncStart CIR-node. cStart is an inter\u00adthread action (an action that can be observed by another thread), \nsince the instructions of a concurrent region can be executed in parallel with instructions that follow \nthe con\u00adcurrent region in program order. As such, cStart semanti\u00adcally starts a new thread that executes \nthe concurrent region. Since starting a thread is an inter-thread action in the JMM, the cStart CIR-node \nis an inter-thread action as well. Start\u00ad ing a new thread has release semantics in the JMM. Release \nsemantics imply that all inter-thread actions that happen\u00adbefore cStart must be visible to and ordered \nbefore all inter-thread actions after cStart. To provide release seman\u00adtics, the local view of memory \n(e.g., .eld values that are cached in registers) of the parent thread is transferred to the child threads. \ncStart has acquire semantics for the child threads, because child threads must update their local view \nof memory with global memory. The cEnd CIR-node delimits the end of a concurrent region. cEnd represents \nthe .nal inter-thread action of a concurrent region. Consequently, all inter-thread actions in the concurrent \nregion must be ordered before cEnd and all inter-thread actions that follow cEnd in program order must \nhappen after cEnd. Child threads commit the local view of memory to global memory. Consequently, cEnd \nhas release semantics for child threads. JMM semantics of cBarrier : The cBarrier CIR-node represents \na barrier across which the JIT compiler is not al\u00ad lowed to reorder instructions. As discussed in Section \n4.4, two (or more) concurrent regions can be merged into one concurrent region. Merged concurrent regions \nare seen by the JIT compiler as a single concurrent region and the re\u00ad ordering rules for intra-region \nreorderings (Section 6) ap\u00ad ply. However, reordering the instructions of merged concur\u00adrent regions can \nintroduce new behaviors. Consequently, the cBarrier node ensures that concurrent region merging re\u00admains \na valid code transformation in the JMM. Consider the following example, which is taken from [16]: Initially, \nx == y == 0 foo() C1() C2() C3() 1: C1(); 1: r1=x; 4: r2=x; 6: r3=y; 2: C2(); 2: if(r1==0) 5: y=r2; 7: \nx=r3; 3: C3(); 3: x=1; (a) Caller. (b) Callees. Figure 5. r1 == r2 == r3 == 1 is not legal. Figure 5(a) \ncontains three concurrent calls. The method bodies of the three concurrent calls C1(), C2(), and C3() \nare illustrated in Figure 5(b). x and y are normal .elds that can be accessed by three different threads. \nr1, r2, and r3 are local registers. An execution in which r1==r2==r3==1 is not legal in the JMM. The \nreason is that r1 must be 0 provided that 1 is assigned to x. The behavior in question (r1==r2==r3==1) \nrequires \"out-of-thin-air\" reads, which violate the security guarantees of Java. Figure 6 shows the effects \nof a compiler optimization that results in an out-of-thin-air read that is possible without the cBarrier \nCIR-node. More precisely, the code in Fig\u00ad ure 6 illustrates a variant of the code shown in Figure 5 \nthat allows r1==r2==r3==1. Initially, x == y == 0 Initially, x == y == 0 C1() and C2() C3() C1() and \nC2() C3() 0: cStart; 10: r3=y; 0: cStart; 10: r3=y; 1: cBarrier; 11: x=r3; 1: cBarrier; 11: x=r3; 2: \nr1=x; 2: y=1; 3: if(r1==0) 3: r1=x; 4: x=1; 4: if(r1==0) 5: cBarrier 5: x=1; 6: r2=x; 6: cBarrier 7: \ny=r2; 7: r2=1; 8: cBarrier; 8: cBarrier; 9: cEnd; 9: cEnd; (a) Effects of \"thread inlining\". (b) Code \noptimizations across con\u00adcurrent regions. Figure 6. Must not allow: r1==r2==r3==1. Figure 6(a) shows \nthe code in which the JIT compiler inlined the calls C1() and C2() into function foo(). In the inlined \nversion, the JIT compiler can determine that the only legal values for x and y are 0 and 1. Consequently, \nthe JIT compiler can replace the read of x in line 6 by 1 and line 7 by y = 1. If the JIT compiler reorders \ninstructions across cBarrier nodes, as shown in Figure 6(b), the behavior in question (r1==r2==r3) is \npossible. The following execution order yields the behavior in question: 0, 1, 2, 10, 11, 3\u00ad 9. The \ncBarrier CIR-node prohibits this illegal behavior, since instruction reorderings are restricted to happen \nwithin a concurrent region, i.e., the body of a concurrent call. JMM semantics of cSync: The cSync CIR-node \nis an inter-thread action since the parent thread can determine if all child threads have .nished execution. \nIn terms of the JMM, cSync has the same semantics as calling join() on every child thread. The join() \noperation ensures that all children have .nished execution and updated their lo\u00adcal view of memory to \nglobal memory. The parent thread synchronizes the local memory with global memory. 4.2 Using existing \ncompiler optimizations with CIR This section discusses the interoperability of the CIR with existing \ncompiler transformations. Existing code transfor\u00admations can reorder independent intra-thread actions \nas well as independent inter-thread actions. Reordering optimiza\u00adtions in CIR can be classi.ed into intra-region \nand inter\u00ad region reordering optimizations. An intra-region reordering optimization reorders an IR-node \nN0 with a set of target IR\u00adnodes NT = {N1, N2, ..., Nn} and NT does not contain a CIR-node. Similarly, \nan inter-region reordering optimiza\u00adtion reorders an IR node N0 with a set of target IR-nodes NT = {N1, \nN2, ..., Nn} and NT contains at least one CIR\u00adnode. Table 1 summarizes the rules that de.ne legal and \nil\u00ad legal reorderings. Type of action Type of optimization intra-thread inter-thread intra-region reordering \nJLS JMM inter-region reordering CI CI Table 1. Reordering rules for intra-and inter-region re\u00adordering \noptimizations. The rules are de.ned in the Java Language Speci.cation (JLS) [7], the Java Memory Model \n(JMM) [16], and in the subsequent de.nition of Con\u00ad currency Invariance (CI). Concurrency invariant intra-and \ninter-thread actions can be moved into concurrent regions. Concurrency-invariance is de.ned as follows: \nAn IR-node N that is de.ned in a home region CH and is concurrency invariant to a set of target re- Cinv \n gions {CT 1, ..., CT n } is de.ned as: N ---. {CT 1, ..., CT n }. If N is concurrency invariant to CT \n, N can either be moved into CT by reordering N with cStart (if N is de.ned be\u00adfore (in program order) \nCT ), or by reordering N with cEnd (if N is de.ned after (in program order) CT ). An IR-node N is concurrency \ninvariant if and only if: N does not change the control .ow;  moving N from CH to CT preserves the \nintra-thread se\u00admantics of CH and CT ;  moving N from CH to CT preserves the inter-thread se\u00admantics \nof CH and CT .  4.2.1 Control .ow Instructions that change the control .ow are not concurrency invariant, \nsince the reordering potentially changes the condi\u00adtion under which a piece of code can be executed in \npar\u00adallel. Consider the example shown in Figure 7, which de\u00ad picts the effects of reordering the cStart \nCIR-node with an if IR-node. Figure 7(a) shows a sequential region (SR) that conditionally executes a \nconcurrent region (CR). I.e., the then part contains a concurrent region and the else part contains sequential \nregions. Figure 7(b) shows the result of moving the if-node into the concurrent region. In Fig\u00adure 7(b) \nthe child thread executes the then part but poten\u00adtially also the else part of the if-statement. Such \na behavior is, in general, illegal since the else part must be executed by the same thread as the SR. \nFor example, the else part can write to a global variable that is read after the if statement by the \nthread that executes the SR. However, if the else part is executed by a separate child thread, the code \ntransforma\u00adtion shown in Figure 7(b) introduces a data race that is not present in the original version \nof the program. (a) Reordering branch instructions. (b) Branch instruction reordering. Figure 7. Effects \nof reordering branch instructions.  4.2.2 Preserving intra-thread semantics Intra-thread actions are \nactions that are not visible to other threads. For example, adding two local variables is an intra\u00adthread \naction, since local variables are stored on the stack and the stack is private to each thread. As a result, \nchanges to local variables that are performed in a concurrent region are not visible outside the concurrent \nregion. The runtime system initializes live [18] local variables that are used in a concurrent region \nto the values of the corresponding vari\u00adables in the parent thread. Concurrency invariant intra-thread \nactions Figure 8 il\u00ad lustrates two sequential regions, one concurrent region, as well as potentially \nconcurrency invariant reorderings (RO1, RO2, and RO3) and reorderings that are always illegal (RO4 and \nRO5). The conditions that specify if an intra-thread ac\u00adtion is concurrency invariant are de.ned in Table \n2. RO4 and RO5 are, in general, illegal for intra-thread actions (see Sec\u00adtion 8 in [16]). The bidirectional \narrows for RO4 and RO5 indicate that reorderings are prohibited in either direction.  Figure 8. Reordering \nconstraints for standard compiler op\u00adtimizations in CIR. Assume that I1 de.nes a local variable l1 and \nI4 de.nes a local variable l2. Furthermore, assume that I1 and I2 are intra-thread actions that do not \nchange the control .ow and cannot throw an exception. The JIT compiler uses standard liveness analysis \n[18] to determine the conditions for RO1, RO2, and RO3. Cinv RO1: I1 ---. {C } if def(l1) is not used \nin S R2. Note that changes to local variables that are performed by the child thread are not visible \nto the parent thread. Cinv RO2: I1 ---. {C } if def(l1) is not used in C. Killing a live def(l1) changes \nthe intra-thread semantics of the child thread. Cinv RO3: I4 ---. {C } if def(l2) does not kill another \ndef(l2) in C. Killing a live def(l2) changes the intra-thread semantics of the child thread. Table 2. \nConcurrency invariant operations: local variables 4.2.3 Preserving inter-thread semantics The section \ndiscusses concurrency invariant inter-thread actions. Inter-thread actions are normal loads and stores, \nvolatile loads and stores, monitorenter and monitorexit bytecodes, wait() and notify(), as well as starting \nand joining threads. Normal loads and stores Reordering RO1, RO2, and RO3 from Figure 8 are concurrency \ninvariant to C if the conditions listed in Table 3 hold. Assume that I1 and I4 do not change the control \n.ow and that a load instruction (ld) loads the value from a location on the heap (o.f) into a local variable \nl and that a store instruction (st) stores the value of v to the .eld o.f. Cinv RO1:ld l = ld(o.f) \n---. {C } if def(l) is not used in S R2 and there is no st(o.f, v) until the next cSync CIR-node. For \nexample, if I4 = st(o.f , v) RO1 in\u00adtroduces a data race, because the two instructions l = ld(o.f) and \nst(o.f, v) are performed by differ\u00adent threads. Cinv RO1:st st(o.f, v) ---. {C } if there is no ld(o.f) \non any path to the next cSync CIR-node. For example, if I4 = ld(o.f) then RO1 introduces a data race. \nCinv RO2:ld l = ld(o.f) ---. {C } if def(l) is not used in C. Cinv RO2:st st(o.f, v) ---. {C } if there \nis no ld(o.f) in C. Note that if I1 = st(o.f , v) and I2 = ld(o.f), I2 must see the update that is performed \nby I1. Cinv RO3:ld l = ld(o.f) ---. {C } if def(l) is no used in C. Cinv RO3:st st(o.f, v) ---. {C \n}. The store of I1 and possible l = ld(o.f) in C are not ordered by a happens\u00adbefore relationship. This \nis a data race. Table 3. Concurrency invariant operations: loads and stores The JIT compiler uses traditional \ncompiler analysis to de\u00ad termine the conditions in Table 3. If the JIT compiler cannot determine the \nconditions, the JIT compiler conservatively assumes that the instruction is not concurrency invariant. \nRO4 and RO5 are not legal, since these reordering can in\u00adtroduce new program behaviors (see Section 8 \nin [16]). All other inter-thread actions are not concurrency invariant (see Appendix, Section A for a \ndetailed discussion).  4.3 Concurrent region merging This section discusses the conditions that enable \nthe JIT compiler to merge concurrent regions. Merged concurrent regions have a coarser PTG compared to \nunmerged concur\u00ad rent regions. 4.3.1 Merging non-merged concurrent regions Assume there are two concurrent \nregions C1 and C2 and that C1 as well as C2 correspond to a single concurrent method in the source code. \nI.e., C1 and C2 are not merged with other concurrent regions. Furthermore, assume that C1 is de.ned in \nbasic block (BB1) and C2 is de.ned in BB2. C1 and C2 can be merged if and only if: 1. C1 and C2 are in \nthe same basic block and there is no instruction between C1 and C2 2. all instructions between C1 and \nC2 are concurrency in\u00advariant to either C1 and/or C2 3. BB1 dominates BB2 and  (a) there is no @Sync \nannotated method on any path from C1 to C2 and (b) there is no synchronization action in a sequential \nre\u00adgion (or concurrent region that is compiled to sequen\u00adtial code) that is on a path from C1 to C2. \nCondition 1: If there is no instruction between C1 and C2, C1 and C2 are contiguous concurrent regions. \nMerging two contiguous concurrent regions is trivially possible. Contigu\u00adous concurrent regions must \nbe contained in the same ba\u00adsic block. Otherwise, there would be an instruction (e.g., a goto) between \nC1 and C2. Condition 2: If all instructions between C1 and C2 are concurrency invariant to either C1 \nand/or C2, the instructions can be moved into C1/C2. As a result, the concurrent regions are contiguous \nand therefore mergable. Condition 3: BB1 must dominate BB2 so that the merged concurrent region can safely \nbe issued at BB2. If BB1 does not dominate BB2, the concurrent region that is de.ned in BB2 is eventually \nnever executed. Consider the examples as shown in Figure 9. Figure 9(a) illustrates the original program. \nThe arrows in Figure 9 indicate transitions between basic blocks that are triggered in a sequential region. \nI.e., the transition from BB2 to BB3 and BB2 to BB4 is not determined in C2. (a) Original code. (b) \nC1+C2. (c) C2+C3;C2+C4. (d) C2+C5. Figure 9. Concurrent region merging examples. Figure 9(b) shows a \nvariant of the original program in which the concurrent regions C1 and C2 are merged. BB1 and BB2 can \nbe combined into a single basic block. As a result, C1 and C2 are contiguous concurrent regions and can \ntherefore be legally merged. Figure 9(c) shows an example in which C2 is merged with C3 and C2 is merged \nwith C4. Note that either merging C2 with C3 or C2 with C4 results in an il\u00adlegal program, since C2 is \nthen not guaranteed to be executed in any path from BB1 to BB5. Finally, Figure 9(d) shows a variant \nof the original program in which C2 is merged with C5. This code transformation is legal if C3 and C4 \nare com\u00adpiled to parallel code. Since BB2 dominates BB5, both con\u00adcurrent regions are guaranteed to be \nexecuted. Merging C2 with C5 determines a particular schedule of execution. Condition 3.a: Two concurrent \nregions C1 and C2 cannot be merged if there is a synchronization method on any path in the control .ow \ngraph (CFG) from C1 to C2. Figure 10 shows the effects of such a merging. Figure 10(a) shows the original \nprogram. Figure 10(b) shows a variant of the orig\u00ad inal program in which C1 and C2 are merged (C12) and \nthe execution of C12 is issued after the synchronization method. Such a code transformation introduces \na new behavior, since there is no guarantee that the effects of C1 are visible to other threads before \nC2 starts executing. Similarly, the code trans\u00adformation as shown in Figure 10(c) potentially introduces \na new behavior, since there is no guarantee that the effects of C1 are visible to other threads before \nC2 starts executing. Recall that these visibility guarantees are provided by the cSync CIR-node.  (a) \nOriginal program. (b) C1+C2. (c) C1+C2. Figure 10. Merging of concurrent regions with cSync Condition \n3.b: If there is a synchronization action (e.g., a volatile memory operation) in a sequential region \non any path in the CFG from C1 to C2, merging C1 with C2 po\u00adtentially introduces new behavior, since \nthe ordering guar\u00adantees of the original program are changed. For example, consider the program as given \nin Figure 10. Assume that C1 contains a volatile store to a .eld (x = 1) and C2 con\u00adtains the following \nvolatile store (x = 0). Furthermore, as\u00adsume that the cSync CIR-node is replaced by the follow\u00ad ing while \nstatement: while (x == 0). Initially, x = 0. The original program, as illustrated in Figure 10(a) guaran\u00ad \ntees that both concurrent regions as well as any region that follows C2 in program order are executed. \nHowever, if C1 and C2 are merged as illustrated in Figure 10(b) the merged concurrent region will never \nbe executed, since the volatile store to .eld x is moved after the volatile read in the while loop. If \nC1 and C2 are merged as illustrated in Figure 10(c) the while statement loops forever. Since both behaviors \nare not possible in the original program, such a code transfor\u00admation is illegal.  4.3.2 Merging merged \nconcurrent regions Assume there are two merged concurrent regions Cx and Cy. Cx and Cy can be merged \nif and only if every concurrent region that is merged into Cx is mergable with every concur\u00adrent region \nthat is merged into Cy.  4.4 Merging example Figure 11 shows concurrent region merging in CIR. Fig\u00ad \nure 11(a) shows the original program and Figure 11(b) de\u00ad picts the CIR of the merged version. In particular, \nfunction calls B() and C(), which are contained in a separate concur\u00adrent region in Figure 11(a) are \nmerged into one concurrent region in Figure 11(b). Note that the cBarrier CIR-node is required to guarantee \nsequential consistency for data race\u00ad free programs. 1 c a l l A 1 c a l l A 2 c S t a r t ; c B a r \nr i e r ; 2 c S t a r t ; c B a r r i e r ; 3 c a l l B 3 c a l l B 4 c B a r r i e r ; cEnd ; 4 5 5 \nc B a r r i e r ; 6 c S t a r t ; c B a r r i e r ; 6 7 c a l l C 7 c a l l C 8 c B a r r i e r ; cEnd \n; 8 c B a r r i e r ; cEnd ; (a) Original program. (b) Merged concurrent regions. Figure 11. Contiguous \nconcurrent region merging.  5. Pro.ling system extension The existing pro.ling infrastructure must be \nextended to ob\u00adtain two necessary performance characteristics of concurrent regions that enable effective \nmerging of concurrent regions: the execution time (Texec) and the number of invocations (I) of a concurrent \nregion. Texec is important, because the over\u00adhead from using a separate thread to execute the concurrent \nregion must be smaller than the performance gain due to a parallel execution to speedup the application. \nI is important since it represents the number of tasks that are generated at runtime. A large number \nof tasks provides good load balance but introduces a running-time overhead. The Jikes Research Virtual \nMachine (Jikes RVM) sam\u00ad pling pro.ler can only provide an estimate of how much time the application \nspent in a particular method (Ttotal). Ttotal does not enable the AOS [3] to draw inferences about Texec \nand I. A large Ttotal can have two reasons: First, Texec is indeed long (and executed infrequently). \nHowever, a large Ttotal can also result from a frequently invoked method with a short Texec. The sampling \ntechnique of the Jikes RVM is also unable to count the number of invocation I of a concur\u00adrent region. \nTo precisely determine Texec as well as the I, we add an instrumenting pro.ler to the Jikes RVM. 5.1 \nConcurrent region instrumentation The JIT compiler instruments concurrent regions as illus\u00ad trated in \nFigure 12. In particular, the JIT compiler uses the rdtsc instruction to measure the execution time. \nThe ex\u00adecution time is only recorded at certain intervals. The if statement in line 2 checks if the time \nsince the last record (thread.rec) is larger than THRESHOLD. To perform this check, each thread has a \n.eld, rec, that stores the time stamp counter when the last sample was recorded. The rec .eld is initialized \nto 0 when a thread is created. If the if state\u00adment in line 2 evaluates to true, the execution time of \nthe concurrent region is recorded and the rec .eld is updated. Otherwise, the sample is not recorded. \n 1 r 1 = r d t s c ; { c o n c u r r e n t r e g i o n } r 2 = r d t s c ; 2 i f ( r 2 - t h r e a d \n. r e c > THRESHOLD ) { 3 r e c o r d ( r 2 - r 1 ) ; 4 t h r e a d . r e c = r 2 ; 5 } Figure 12. Sampling \ninstrumentation pro.ling. We do not record every taken sample to keep the overhead from executing instrumented \nconcurrent regions as small as possible. Concurrent region instrumentation has two sources of overhead. \nThe .rst source of overhead is the execution of the rdtsc instruction. The overhead of the rdtsc instruc\u00adtion \nis negligible, since the rdtsc instruction is (i) not serial\u00adizing and (ii) not ordered with other instructions. \nThe second source of overhead is related to storing and maintaining the gathered pro.le data. To measure \nTexec with a low pro.ling overhead the instrumenting pro.ler records samples only at a certain interval, \nthe record interval. The impact of the record interval is explained in Section 7. There is a tradeoff \nbetween the overhead of the sampling instrumentation and the preci\u00adsion of the measurement of Texec. \nIf THRESHOLD is too large, the number of recorded samples is small. Consequently, if Texec varies heavily \nupon every invocation, the sampled ex\u00adecution time can be imprecise. However, if THRESHOLD is too small, \nthe pro.ling overhead can be signi.cant. To measure the number of concurrent region invocations (I), \nthe JIT compiler adds a local variable to each method that contains a concurrent region that is compiled \nto parallel code. The local variable is incremented in every parallel manifestation of a concurrent region. \nIn a merged concurrent region, the counter is incremented only once. Similar to the record() function \nin Figure 12, the JIT compiler adds a function call record_invocations() as the last operation before \nthe method returns to the caller. This function reports I to the instrumenting pro.ler. Concurrent regions \nare pro.led if they are executed se\u00adquentially, in parallel, and if they are merged with an\u00adother concurrent \nregions. I.e., if two concurrent regions are merged, the execution time of the two individual concurrent \nregions is pro.led. Such a .ne-grained performance pro.le allows the JIT compiler to adapt the merging \ncon.guration based on individual changes of the execution time of a par\u00adticular concurrent region.  \n5.2 Maintaining performance samples Each thread stores the gathered samples to a data structure that \nis private to the thread. The bytecode index of the con\u00adcurrent region and the signature of caller of \nthe concurrent region form the key for a hash map that holds the reference to a circular buffer that \nstores the performance samples. If the circular buffer collected a certain amount of new sam\u00adples (report \nthreshold) the arithmetic average of the samples is reported to the global database, which collects the \nsamples of all threads. The report threshold starts at 1 and increases up to 100 in strides of 10. Such \na reporting strategy ensures that samples of newly discovered concurrent regions are re\u00adported immediately \nand frequently to the global database. The samples of concurrent regions that have been pro.led for a \nlonger time are reported less frequently. Each thread maintains the number of concurrent region invocations \nin a similar manner. There is a second condition that must be ful.lled so that a thread reports a sample \nto the global database. The arithmetic average over the collected samples must differ by least by 30% \ncompared to the last reported average. Reporting similar samples provides no new information. We choose \n30% based on empirical evaluation. If a thread commits a new sample to the global database, the committing \nthread wakes up another thread (which belongs to the AOS) that evaluates the new data asyn\u00adchronously. \nThe next section describes the evaluation pro\u00adcess of the samples in detail. 6. Adaptive optimization \nsystem extension This section describes the parallel code execution optimiza\u00adtion (PCEO) algorithm. The \nPCEO algorithm aims at .nding a setting to execute concurrent regions ef.ciently. The PCEO algorithm \ndetermines if a concurrent region is (i) executed sequentially, (ii) executed in parallel, and (iii) \nis merged with other concurrent regions. The PCEO algorithm uses pro.le information collected at run-time \nto determine the setting of the concurrent regions at method-level granularity. Figure 13 shows the PCEO \nalgorithm in detail. The PCEO algorithm takes the CIR, the new pro.le in\u00ad formation, and the merging \nfunction (see later this section) as an input. The PCEO algorithm uses two thresholds: the serialization \nthreshold (T_SER) and the merging function. T_SER represents the overhead that is associated with a parallel \nexecution. I.e., T_SER corresponds to the execu\u00adtion time of a parallel task at which a parallel execution \nof the task is equally fast as the sequential execution. Conse\u00adquently. T_SER can be used to determine \nif a concurrent re\u00adgion is executed sequentially or in parallel. Our experimental evaluation yields a \nvalue of 10,000 cycles for T_SER. The PCEO algorithm .rst computes the oversubscription factor (OS), \n(line 4), which describes the relation between the number of parallel concurrent region invocations and \nthe number of available cores. An oversubscription factor larger than 1 means that there are more parallel \nconcurrent region invocations than available cores. The algorithm completes if it .nds a new merging \ncon.guration (line 40) or if the work list is empty. From line 9 to line 20, the algorithm handles concurrent \nregions that cannot be merged with other con\u00adcurrent regions. Line 10 to line 15 check if the execution \ntime of the concurrent region is smaller than T_SER. Fur\u00adthermore, the algorithm checks if the concurrent \nregion is currently compiled to parallel code. If so, the PCEO algo\u00adrithm issues a serializing recompilation \n(line 12 and line 13). 1 I n p u t : CIR , i n f ( p r o f i l e s ) , mf ( m e r g e f u n c t i o \nn ) 2 O u t p u t : CIR 3 4 i n t OS = i n f . c R e g i o n I n v o c a t i o n s / n u m P r o c e \ns s o r s ; 5 W o r k l i s t w o r k L i s t = i n f . g e t R e g i o n P r o f i l e s ( ) ; 6 i n \nf . r e c o m p i l e = f a l s e ; 7 w h i l e ( ! w o r k L i s t . i s E m p t y ( ) ) { 8 C R e g \ni o n r = w o r k l i s t . g e t ( ) ; 9 i f ( ! r . h a s M e r g e C a n d i d a t e s ( ) {  10 \ni f ( r . e x e c T i m e < T_SER ) { 11 i f ( r . i s P a r a l l e l ) { 12 r . i s P a r a l l e \nl = f a l s e ; 13 i n f . r e c o m p i l e = t r u e ; 14 } 15 } e l s e i f ( r . e x e c T i m \ne > T_SER ) { 16 i f ( ! r . i s P a r a l l e l ) { 17 r . i s P a r a l l e l = t r u e ; 18 i n \nf . r e c o m p i l e = t r u e ; 19 } 20 } e l s e { 21 b o o l e a n m e r g e = s h o u l d M e \nr g e ( r , OS ) ; 22 b o o l e a n d e m e r g e = s h o u l d D e m e r g e ( r , OS ) ; 23 24 i f \n( m e r g e ) { 25 r . m e r g e C a n d i d a t e s . r e m o v e A l l ( r . m e r g e d W i t h ) \n; 26 r . i s P a r a l l e l = f a l s e ; 27 f o r ( C r e g i o n c : m e r g e C a n d i d a t e s \n) { 28 w o r k l i s t . r e m o v e ( c ) ; 29 r . m e r g e d W i t h . a d d ( c ) ; 30 c . m e r \ng e d W i t h . c l e a r ( ) ; 31 c . i s P a r a l l e l = f a l s e ; 32 d o u b l e m e r g e d E \nx e c T i m e = r . e x e c T i m e ; 33 34 f o r ( C R e g i o n t : r . m e r g e d W i t h ) 35 m \ne r g e d E x e c T i m e += t . e x e c T i m e ; 36 37 i f ( m e r g e d E x e c T i m e > T_SER ) \n{ 38 r . i s P a r a l l e l = t r u e ; 39 i n f . r e c o m p i l e = t r u e ; 40 b r e a k ; 41 } \n42 } 43 } e l s e i f ( d e m e r g e ) { 44 r . m e r g e W i t h . c l e a r ( ) ; 45 i n f . r e c \no m p i l e = t r u e ; 46 } e l s e { 47 w o r k l i s t . r e m o v e A l l ( r . m e r g e d W i t \nh ) ; 48 } 49 } 50 h a n d l e R e m a i n i n g R e g i o n s ( ) ; Figure 13. Parallel code execution \noptimization algorithm. From line 15 to line 19 the algorithm checks if the execution time of the concurrent \nregion is larger than T_SER. If the concurrent region is compiled to sequential code, the PCEO algorithm \nissues a parallelizing recompilation. Line 21 to line 48 handle concurrent region merging/de\u00admerging. \nshouldMerge() and shouldDemerge() return a boolean value that indicates if a concurrent region is merged \nor demerged. Both functions are shown in Figure 14 and Fig\u00ad ure 15, respectively. If the algorithm decides \nto merge con\u00ad current regions, the algorithm .rst removes all concurrent regions from the list of potential \nmerging candidates that are already merged with the concurrent region (line 25). In the next step, the \nalgorithm sets the isParallel .ag of the cur\u00adrent concurrent region to false. The reason is that the \nexe\u00adcution time of the (merged) concurrent region can be lower than T_SER. From line 27 to line 41, the \nalgorithm iterates over the merging candidates. For each merging candidate, the PCEO algorithm (i) removes \nthe merging candidate from the work list, (ii) adds the candidate to the existing set of merged regions, \n(iii) clears the existing merging con.gura\u00adtion of the merging candidate, and (iv) sets the isParallel \n.ag to false. Line 34 to line 35 compute the execution time of the merged concurrent region. If the merged \nexecution time is larger than T_SER, the merged concurrent region is compiled to parallel code (line \n38 and line 39). Line 43 to line 46 handle the demerging. Demerging is currently done by recompiling \nall merged regions to either sequential or parallel code. Line 45 sets the recompile .ag to true. As \na result, all concurrent regions, including the concurrent regions that were originally merged with r \nare handled by handleRemainingRegions() in line 50. This function simply checks for each non-merged concurrent \nre\u00adgion if the execution time is above or below T_SER and sets the isParallel .ag accordingly. 1 b o \no l e a n s h o u l d M e r g e ( P r o f i l e i n f , d o u b l e OS ) { 2 i f ( i n f . e x e c T \ni m e < T_SER ) 3 r e t u r n t r u e ; 4 d o u b l e b = c o m p u t e M e r g e F u n c t i o n ( i \nn f ) ; 5 i f ( ! w i t h i n T o l e r a n c e ( b , O S ) &#38;&#38; OS > b ) 6 r e t u r n t r u e \n; 7 r e t u r n f a l s e ; 8 } Figure 14. Algorithm that calculates merging condition.  Figure 14 shows \nthe shouldMerge() function, which re\u00adturns a boolean value that indicates whether the concurrent region \nshould be merged with other concurrent regions. Line 2 examines the execution time of the concurrent \nregion. If the execution time is below T_SER, shouldMerge() re\u00adturns true. Otherwise, line 4 computes \nthe merge function. The returned value (b) is passed to the withinTolerance() function, which checks \nif the current oversubscription fac\u00adtor (OS) is within +/-10% of the value of the merge func\u00adtion. This \ncheck avoids frequent recompilations that have only minor effects. Furthermore, if the current oversub\u00adscription \nfactor is larger than the computed merge function value, shouldMerge() returns true in line 6. 1 b o \no l e a n s h o u l d D e m e r g e ( P r o f i l e i n f , d o u b l e OS ) { 2 d o u b l e b = c o \nm p u t e M e r g e F u n c t i o n ( i n f ) ; 3 i f ( ! w i t h i n T o l e r a n c e ( b , OS ) &#38;&#38; \nOS < b ) { 4 r e t u r n t r u e ; 5 r e t u r n f a l s e ; 6 } Figure 15. Algorithm that calculates \ndemerging condition.  Figure 15 shows the shouldDemerge() function. Line 2 computes the merge function. \nSimilar to the shouldMerge() function, line 3 checks if the current value of the OS is +/-10% the value \nof the computed merge function value (b). If the current oversubscription factor is smaller than b, shouldDemerge() \nreturns true in line 4. Merge function The merge function de.nes if concurrent region merging or demerging \nis applied. The merge function can be supplied at JVM startup. If no merge function is provided, the \nJVM uses the default merge function that is illustrated in Figure 16. T _S E R OH = Texec The overhead \nfactor is a metric for the impact of the parallel overhead on the execution time of a concurrent region. \nI.e., a low overhead factor indicates that the concurrent region in\u00adcurs a low parallel overhead compared \nto its execution time. An overhead factor larger than 1 implies that execution time of the concurrent \nregion is shorter than the parallel over\u00adhead. As a result, such a concurrent region is always merged \nwith another concurrent region, or the concurrent region is serialized. The y-axis illustrates the oversubscription \nfactor (OS). Recall that an oversubscription factor of 1 means that the JVM generates 1 task for each \navailable core. The JVM computes the overhead factor for every concurrent region and the oversubscription \nfactor for every method that con\u00adtains at least one concurrent region. The main idea behind the merge \nfunction is to provide the user of the JVM with the opportunity to specify the ef.\u00adciency at which parallel \ncode is executed without having to adapt the source code. We de.ne the ef.ciency of a parallel execution \nas the number of generated tasks in relation to the potential performance gain. Example Assume concurrent \nregion C has an OH = 0.05 and that the OS of the method in which C is de.ned has OS = 0.1 (dot in Figure \n16). Furthermore, assume that C is merged with other concurrent regions. The merge function as depicted \nin Figure 16 triggers a demerging recompilation, since C yields a spot in the demerge region. 7. Evaluation \nThe section presents the experimental evaluation of the over\u00adhead that comes from concurrent region pro.ling \nas well as the performance evaluation using the Java Grande Bench\u00admark Suite [22]. The performance evaluation \nuses our pro\u00ad totype implementation of the extensions presented in Sec\u00adtion 4, Section 5, and Section \n6 in the Jikes RVM [2]. 7.1 Experimental setup All experimental results are obtained from a system that \nis equipped with 32 Intel Xeon E7-4830 cores @2.13 GHz and 64 GB of main memory. The system runs Ubuntu \n11.10 (64\u00adbit) with kernel 3.0.0. We use Java version 1.6.0_24, a min\u00adimum (and maximum) heap size of \n2 GB, and four paral\u00adlel garbage collector threads. The Jikes RVM is compiled in the FullAdaptiveCopyMS \ncon.guration. We use this setup since other setups (e.g., the production con.guration) re\u00adsult in an \nunstable behavior of the Jikes RVM. 7.2 Pro.ling overhead The running time overhead of execution time \npro.ling is determined by using a synthetic benchmark. The synthetic benchmark consists of a loop that \ncontains one concurrent region that in turn contains one method call. The loop body is executed 107 times \nto get stable execution times. The base\u00adline for the pro.ling overhead evaluation is the execution time \nof an unpro.led version of the synthetic benchmark that is compiled to sequential code. Sequential code \nis bet\u00adter suited to measure the pro.ling overhead, since sequential code does not incur parallel overheads. \nThe results are there\u00adfore more stable. To measure the pro.ling overhead of paral\u00adlel threads without \nincluding the parallel overhead, the par\u00adallel version of the synthetic benchmark starts Java threads \nthat execute the same concurrent region that is compiled to sequential code. To ensure that all threads \nexecute the main benchmark loop at the same time the synthetic benchmark has a barrier before the main \nbenchmark loop. The performance results presented in this section are gathered by executing the benchmarks \n20 times in the same JVM instance. We use 10 warmup runs and 10 pro.le runs. The warmup runs are not \nincluded in the results to mea\u00adsure steady-state performance. The presented results are the arithmetic \naverage over the 10 pro.le runs. The error bars indicate the standard deviation. The baseline is compared \nagainst an instrumented (pro\u00ad.led) version of the synthetic benchmark that is also com\u00adpiled to sequential \ncode. To measure the impact of the record interval on the pro.ling overhead, the synthetic benchmark \nis executed using different values for the record interval (0 cycles to 106 cycles). The reported pro.ling \noverhead is the sum of the overhead from storing a sample into the thread\u00adlocal database and the overhead \nof committing a thread-local sample to the global database.  Figure 17 illustrates the pro.ling overhead \nusing 32 par\u00ad allel threads. The x-axis contains the record interval for dif\u00adferent concurrent region \nexecution times (11 ns, 25 ns, 125 ns, and 1000 ns). The y-axis illustrates the slowdown factor over \nthe execution without pro.ling. Higher bars indicate a larger pro.ling overhead. The pro.le overhead \ndecreases with an increasing con\u00adcurrent region execution time and an increasing record in\u00adterval. The \nhighest pro.ling overhead is incurred with the shortest concurrent region execution time (10 ns) and \na record interval of 0 cycles. The lowest recording overhead is 13% for the longest concurrent region \nexecution time and the largest record interval. We choose a record interval of 1000k cycles for the experiments \npresented in Section 7.3.  7.3 Java Grande benchmark results The main source of parallelism in the Java \nGrande bench\u00admarks are parallel loops. The Java versions implement par\u00adallel loops by assigning each \nthread an equally-sized chunk of the iteration space of the loop. The versions using con\u00adcurrent methods \nare parallelized by outlining the body of each parallel loop to a separate method that is annotated with \n@Concurrent. To provide the opportunity to merge concurrent regions, we manually unroll parallel loops. \nWe must unroll parallel loops manually, since the loop unrolling pass in the JIT compiler of the Jikes \nRVM produces unsta\u00ad ble code. Note that the manual unrolling of parallel loops is only necessary due \nto this missing standard optimization in the Jikes RVM. Production JVMs like Hotspot perform loop unrolling \nand it is easy for the JIT compiler to iden\u00ad tify concurrent regions that are in a loop. The JIT compiler \ncan force loop unrolling (and therefore generate mergable concurrent regions) if the overhead factor \nof the concurrent region suggests to merge the concurrent region. We unroll parallel loops of all benchmarks \nby a factor of 8. As a re\u00adsult, the PCEO algorithm can coarsen the given parallel code granularity by \na factor of 8. The performance results presented in this section are gathered by executing the benchmarks \n50 times in the same JVM instance. We use 10 warmup runs and 40 pro.le runs. The warmup runs are not \nincluded in the results to mea\u00adsure steady-state performance. The presented results are the arithmetic \naverage over the 40 pro.le runs. The error bars indicate the 95% con.dence interval. Each benchmark is \nexecuted with the same Jikes RVM con.guration. In particular, we use a pro.ling interval (THRESHOLD) \nof 1 million cycles, a serialization thresh\u00adold of 10,000 cycles, and the following function, which de\u00adscribes \nthe merging and demerging threshold. X M = OH - 1 - Y We choose 0.99 for X and for Y. Figure 16 illustrates \nthe merging function. We use this merging function for the fol\u00adlowing reasons: First, the merging function \nprovides a high oversubscription factor (number of tasks per core) for a low overhead factor. As a result, \nthe system is well balanced if the parallel overhead does not dominate the execution time of the application. \nRecall that an overhead factor of 0 means that there is no parallel overhead. An overhead factor of 1 \nmeans that the parallel overhead is equal to the execu\u00adtion time of the task. Second, the oversubscription \nfactor decreases exponentially with an increasing overhead factor. As a result, the ef.ciency of the \nsystem is well maintained. The JVM is equipped with this merge function by default. Figure 18 shows the \nperformance results obtained by our automated approach to determine the PTG compared to three different \nJava versions. The Java versions differ re\u00adgarding the PTG: The PTG in Figure 18(a) is chosen such that \nthe benchmark yields best performance. To .nd the best-performing PTG, we manually try different PTGs. \nI.e., for each parallel loop, we divide the iteration space of the loop into 1, 2, 4, 8, 16, 32, 64, \n128, 256, 512, and 1024 equally-sized parallel tasks that are executed in parallel by 32 threads. Figure \n20 in Appendix B illustrates the perfor\u00ad mance results for the manual PTG performance runs. The sequential \nexecution times of the evaluated benchmarks that are the baseline (a value of 1 on the y-axis) for Figure \n20 are as follows: crypt: 4.9s , series: 40.7s, sor: 2.1s, lu: 1.4s, matmult: 1.6s, and montecarlo: 24.2s. \nThe PTG that yields the best performance for a benchmark is the baseline in Figure 18(a) (512 tasks for \ncrypt, 1024 tasks for series, 64 tasks for sor, 16 tasks for lu, 64 tasks for matmult, and 512 tasks \nfor montecarlo in this order). It is easy to see that .nding a good PTG is dif.cult. The y-axes in Figure \n18 compare the performance of the Java versions against our approach. A value of 1 on the y-axis means \nthat our approach is equally fast compared to a Java version. A value smaller than 1 means that our approach \nis slower than standard Java; a value larger than 1 means that our approach is faster. The error bars \nindicate the 95% con.dence interval over the 40 performance runs. To enable a fair comparison between \nour approach and the Java versions, our approach and the all Java versions use the same thread pool implementation. \nThe performance results of our approach include the overhead from dynamic pro.ling and dynamic recompilations \nthat adapt the PTG. Table 4 in Appendix A shows the number of generated parallel tasks for each benchmark. \n Figure 18(a) shows that our approach performs slightly slower than the manually optimized Java version. \nThis is not surprising, since the manually optimized version uses of.ine pro.ling to determine the PTG. \nThe performance penalty due to dynamic recompilation is insigni.cant, since the number of recompilations \nimposed by adapting the PTG is small. On average less than 30 recompilations are needed to determine \nthe .nal PTG. Figure 18(b) shows the performance of our approach compared to the Java version that assigns \nthe iterations of parallel loops to 8 parallel tasks. Our approach outperforms Java for crypt, series, \nmatmult, and montecarlo, since parallelism is underspeci.ed for a 32-core system. Our ap\u00adproach performs \nslower for the sor and the lu benchmark, since our approach cannot effectively remove all overspec\u00adi.ed \nparallelism. The reason is that parallel loops are man\u00adually unrolled by a factor of 8. If merging all \n8 concurrent regions results in a PTG where parallelism is overspeci.ed, our approach cannot remove more \nparallelism. This is, how\u00adever, an implementation feature of the Jikes RVM. If the Jikes RVM can determine \nthe unroll factor, e.g., an unroll factor larger than 8, more parallelism can be removed. Figure 18(c) \nillustrates the performance of our approach compared to the Java version which splits the iteration space \nof parallel loops into 512 parallel tasks. Our approach per\u00adforms faster for the sor, lu, and the matmult \nbenchmark, since parallelism is overspeci.ed in for these benchmarks. 8. Related work There exists a \nlarge body of related work in the area of par\u00adallelizing compilers and compiler optimizations for paral\u00adlel \nprograms. For example, Lee et al. [14] present compiler algorithms for explicit parallel programs. The \nauthors in\u00adtroduce a concurrent static single assignment form (CSSA) for programs containing cobegin/coend, \nand post/wait state\u00adments. In another paper, Lee et al. show how to apply con\u00adstant propagation on CSSA \n[13]. Sarinivasan et al. present an SSA form for explicit parallel programs [23]. The main difference \nof [13, 14, 23] to CIR is that CIR can be easily integrated into existing JIT compiler infrastruc\u00ad tures. \nFor example, CIR can be transformed to SSA without having to change SSA construction. If code reorderings \nare kept local to a concurrent region (which can be done by giv\u00ading the CIR-nodes a property that disallows \nall reorderings), no modi.cations to existing optimization passes are needed. Furthermore, the compiler \ntransformations in [14, 23] gen\u00ad erate sequentially consistent (SC) code [12]. SC is more re\u00ad strictive \nthan the JMM [16] and therefore forbids many ef\u00ad fective optimizations. SC Java code is 10% -26% slower \nthan Java code compiled according to the JMM [24]. Other examples of parallel program analysis include \nan optimal bit vector analysis [11] for explicit parallel programs that allows to compute reaching de.nition, \nde.nition-use chains, or analysis for code motion. Grunwald et al. present data .ow equations for explicitly \nparallel programs [8]. Fi\u00ad nally, Sarkar presents analysis techniques that are based on a parallel program \ngraph [20]. These efforts all perform pro\u00ad gram analysis in a static compiler. A JIT compiler must gen\u00aderate \nhigh quality code in a short time. To the best of our knowledge, we present the .rst JIT compiler that \noperates on an explicit parallel IR that can produce high quality code. Pramod et al. present a technique \nthat allows to reuse compiler optimizations that are designed for sequential code unmodi.ed to parallel \ncode [9]. This work is similar to our work, since existing optimizations can be applied to CIR without \nany modi.cations. However, the algorithms presented in [9] guarantee sequential consistency, which is \na more restrictive memory model that the JMM. There are several OpenMP compilers that optimize in and \naround parallel regions. E.g., Satoh et al. present a reaching de.nition analysis, memory synchronization \nanalysis and loop data dependence analysis for parallel loops [21]. Tian et al. present multi-entry threading, \na compilation technique that allows to perform optimizations across thread bound\u00adaries [25]. Zhang et \nal. exploit meta information provided by OpenMP directives to analyze barrier synchronization [27]. Cilk \n[6] extends the C/C++ programming language with parallel calls and a construct to synchronize parallel \ncalls. The semantics of concurrent statements correspond to the semantics as proposed in Cilk. The main \ndifference to Cilk is that our approach integrates concurrent regions into a runtime system that allows \ndynamic optimizations such as pro.le-based recompilation [26]. The current Cilk run\u00adtime cannot perform \nsuch optimizations. Rugina et al. [19] present a .ow-sensitive, context-sensitive, inter-procedural pointer \nanalysis algorithm for Cilk programs. X10 [5] provides language-based explicit parallelism. Zhao et al. \n[28] present a compiler framework that aims at re\u00ad ducing the task-creation and termination overhead. \nThe op\u00adtimizations are similar to concurrent region merging. How\u00adever, the authors use a static compilation \napproach, which makes the underlying runtime oblivious of the parallel con\u00adstructs. The authors in [4] \npresent an load elimination algo\u00ad rithm for X10 programs. Load elimination is a reordering optimization \nand the algorithm presented in [4] supports the elimination of load across async statements.  (a) Our \napproach vs. best manual optimized. (b) Our approach vs. 8-task version. (c) Our approach vs. 512-task \nversion. Most approaches to describe parallelism in object-oriented languages are library-based. For \nexample, Intervals [17] pro\u00ad vide an abstraction to explicitly specify the execution order of tasks. \nParallel Java [10] provides OpenMP and MPI con\u00ad structs for Java. Leiden et al. present a task parallel \nlibrary for .NET[15]. Library-based approaches for dynamically compiled languages have the drawback that \nthe JIT com\u00adpiler is unaware of the parallel constructs. As a result, the JIT compiler has less information \nto optimize parallel code. 9. Conclusion This paper presents a novel approach for an automatic, dy\u00adnamic, \nand feedback-directed determination of the parallel task granularity. Our approach is based on two principles \nthat are not present in the current Java Runtime Environ\u00adment. The .rst principle is the separation of \nthe declaration of parallelism from its execution. This principle is provided by concurrent calls and \nallows the system to shift the respon\u00adsibility to determine how parallel code is executed from the application/library \nlevel to the JVM level. The second prin\u00ad ciple is the concurrency-awareness of the JVM, which en\u00ad ables \nthe JVM to gather pro.le information that is relevant to optimize the performance of parallel code. Manually \ntuning the parallel task granularity is tedious, time-consuming, and often impossible in practice. Our \nap\u00adproach represents an important step towards automating this important task. References [1] OpenMP \nApplication Program Interface, 2012. http://openmp.org/wp/. [2] B. Alpern, C. R. Attanasio, J. J. Barton, \nM. G. Burke, P. Cheng, J.-D. Choi, A. Cocchi, S. J. Fink, D. Grove, M. Hind, S. F. Hummel, D. Lieber, \nV. Litvinov, M. F. Mergen, T. Ngo, J. R. Russell, V. Sarkar, M. J. Serrano, J. C. Shepherd, S. E. Smith, \nV. C. Sreedhar, H. Srinivasan, and J. Whaley. The Jalape no virtual machine. IBM Syst. J., 39(1), 2000. \n [3] M. Arnold, S. Fink, D. Grove, M. Hind, and P. F. Sweeney. Adaptive optimization in the jalapeno \nJVM. [4] R. Barik and V. Sarkar. Interprocedural load elimination for dynamic optimization of parallel \nprograms. In PACT 09. [5] P. Charles, C. Grothoff, V. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, \nC. von Praun, and V. Sarkar. X10: an object\u00adoriented approach to non-uniform cluster computing. SIG-PLAN \nNot., 40(10), 2005. [6] M. Frigo, C. E. Leiserson, and K. H. Randall. The implemen\u00adtation of the Cilk-5 \nmultithreaded language. SIGPLAN Not., 33(5), 1998. [7] J. Gosling, B. Joy, G. Steele, and G. Bracha. \nJava(TM) Language Speci.cation, The 3rd Edition. Addison-Wesley Professional, 2005. [8] D. Grunwald and \nH. Srinivasan. Data .ow equations for explicitly parallel programs. SIGPLAN Not., 28, July 1993. [9] \nP. G. Joisha, R. S. Schreiber, P. Banerjee, H. J. Boehm, and D. R. Chakrabarti. A technique for the effective \nand automatic reuse of classical compiler optimizations on multithreaded code. In POPL 11. [10] A. Kaminsky. \nParallel Java: A uni.ed API for shared memory and cluster parallel programming in 100% Java. In IPDPS \n07. [11] J. Knoop, B. Steffen, and J. Vollmer. Parallelism for free: ef.cient and optimal bitvector analyses \nfor parallel programs. ACM Trans. Program. Lang. Syst., 18, May 1996. [12] L. Lamport. How to make a \nmultiprocessor computer that cor\u00adrectly executes multiprocess programs. IEEE Trans. Comput., 28(9), 1979. \n[13] J. Lee, S. P. Midkiff, and D. A. Padua. Concurrent static sin\u00adgle assignment form and constant propagation \nfor explicitly parallel programs. In LCPC 97. [14] J. Lee, D. A. Padua, and S. P. Midkiff. Basic compiler \nalgo\u00adrithms for parallel programs. In PPoPP 99. [15] D. Leijen, W. Schulte, and S. Burckhardt. The design \nof a task parallel library. SIGPLAN Not., 44(10), 2009. [16] J. Manson, W. Pugh, and S. V. Adve. The \nJava memory model. In POPL 05. [17] N. D. Matsakis and T. R. Gross. A time-aware type system for data-race \nprotection and guaranteed initialization. In OOPSLA 10. [18] S. S. Muchnick. Advanced compiler design \nand implementa\u00adtion. Morgan Kaufmann Publishers Inc., 1997. [19] R. Rugina and M. C. Rinard. Pointer \nanalysis for structured parallel programs. ACM Trans. Program. Lang. Syst., 25, January 2003. [20] V. \nSarkar. Analysis and optimization of explicitly parallel programs using the parallel program graph representation. \nIn LCPC 97. [21] S. Satoh, K. Kusano, and M. Sato. Compiler optimization techniques for openmp programs. \nSci. Program., 9, 2001. [22] L. A. Smith, J. M. Bull, and J. Obdrz\u00e1lek. A parallel Java grande benchmark \nsuite. In Supercomputing 01. [23] H. Srinivasan, J. Hook, and M. Wolfe. Static single assign\u00adment for \nexplicitly parallel programs. In POPL 93. [24] Z. Sura, X. Fang, C.-L. Wong, S. P. Midkiff, J. Lee, and \n D. Padua. Compiler techniques for high performance sequen\u00adtially consistent java programs. In PPoPP \n05.  [25] X. Tian, M. Girkar, S. Shah, D. Armstrong, E. Su, and P. Pe\u00adtersen. Compiler and runtime support \nfor running OpenMP programs on pentium-and itanium-architectures. In HIPS 03. [26] J. Whaley. Partial \nmethod compilation using dynamic pro.le information. SIGPLAN Not., 36, October 2001. [27] Y. Zhang, E. \nDuesterwald, and G. R. Gao. Concurrency analy\u00adsis for shared memory programs with textually unaligned \nbar\u00adriers. In LCPC 08. [28] J. Zhao, J. Shirako, V. K. Nandivada, and V. Sarkar. Reducing task creation \nand termination overhead in explicitly parallel programs. In PACT 10. A. Concurrency invariance of inter-thread \nactions Volatile load/store operations Volatile load operations are never concurrency invariant, since \nmaking volatile loads concurrency invariant breaks the ordering guarantees pro\u00advided by volatile semantics. \nConsider the example in Fig\u00adure 19. Figure 19(a) shows the original program and Fig\u00ad ure 19(b) shows \na version of Figure 19(a) in which the JIT compiler moves the volatile load of y of Thread 2 into the \nconcurrent region. The behavior r1 == 3 and r3 == 0 is illegal, since volatile load/store operations \ncannot be re\u00adordered. Consequently, if r1 == 3 Thread 1 must have executed line 2 and as a result also \nline 1. However, Fig\u00adure 19(b) allows r1 == 3 and r2 == 0, since the code in the concurrent region is \nexecuted asynchronously with the code that follows the concurrent region. In particular, line 9 in Figure \n19(b) can be executed prior to line 5. Such a behavior is illegal in the original code. Reordering a \nvolatile load with the cEnd CIR-node (e.g., line 9 in Figure 19(a)) causes an equivalent behavior for \na concurrent region that follows line 9. Moving volatile stores into a concurrent region intro\u00adduces, \nin general, a program behavior that is not possible in the original program and is therefore illegal. \nSimilar to the Initially, x == y == 0; Initially, x == y == 0; y is declared volatile y is declared \nvolatile Thread 1 Thread 2 Thread 1 Thread 2 1: x=1; 3: r1=y+2; 1: x=1; 3: cStart; 2: y=1; 4: cStart; \n5: cBarrier; 2: y=1; 4: cBarrier; 5: r1=y+2; 6: r2=r1*2; 6: r2=r1*2; 7: cBarrier; 8: cEnd; 7: cBarrier; \n8: cEnd; 9: r3=x; 9: r3=x; Figure 19. Reordering volatile load with cStart.  (a) r1==3, r3==0; is disallowed. \n(b) Allows r1==3 and r3==0; example shown in Figure 19, moving a volatile store into a concurrent region \nbreaks the happens-before relationship be\u00adtween the volatile store and the preceding (in program order) \nstore instructions. The reason is the asynchronous execution of a concurrent region. monitorenter and \nmonitorexit The bytecodes monitorenter and monitorexit implement the synchronized state\u00adment. As speci.ed \nin the JLS, monitorenter acquires the intrinsic lock of an object and monitorexit releases the in\u00adtrinsic \nlock. Moving either of both bytecodes into a concur\u00adrent region changes the inter-thread semantics of \nthe child thread and the parent thread, since acquiring/releasing the lock is then performed by a different \nthread. As a result, both bytecodes are not concurrency invariant. wait(), notify(), and notifyAll() \nThe functions wait(), notify(), and notifyAll() are low-level communication primitives. Since wait() \nrequires that the calling thread owns the monitor of the object on which the thread calls wait(), wait() \nis not concurrency invariant. If the JIT compiler moves the wait() call into a concurrent region and \nthe concurrent region is executed by the child thread, the child thread does not own the monitor. Such \na code transformation raises an exception that does not occur in the original program. notify() and notifyAll() \nare not concurrency invariant for similar reasons as wait(). Both calls require that the thread which \nperforms the call owns the corresponding monitor. Starting and joining threads Starting and joining threads \nis not concurrency invariant, since both operations are im\u00adplemented as native function calls that cannot \nbe inlined. B. Additional performance results This section presents additional performance results. Fig\u00adure \n20 illustrates the results of manually tuning the parallel task granularity of the Java Grande benchmarks. \nTable 4 shows the comparison of the number of parallel tasks created by our approach and the number of \nparallel tasks created by the manual tuning as illustrated in Figure 20.  (a) crypt benchmark results. \n(b) series benchmark results. (c) sor benchmark results. (d) lu benchmark results. (e) matmult benchmark \nresults. (f) montecarlo benchmark results. Benchmark Our approach 1 2 4 8 16 32 64 128 256 512 1024 crypt \nlu 10.9 64.8 0.004 1.995 0.008 3.49 0.016 6.45 0.032 12.35 0.064 23.4 0.128 40.25 0.192 72.15 0.320 142.2 \n0.883 282 1.86 500 2.88 500 series 15.6 0.002 0.003 0.007 0.015 0.031 0.063 0.095 0.195 0.415 0.67 1.12 \nmatmult 1.52 0.2 0.4 0.8 1.6 3.2 6.4 12.8 25.6 51.2 102 205 sor 46.75 0.4 0.6 1.4 2.2 5.4 7 13.8 27.8 \n60.6 150 300 montecarlo 9.35 0.002 0.004 0.008 0.016 0.032 0.064 0.096 0.224 0.352 0.61 1.06 Table 4. \nNumber of thousand parallel tasks for our approach and the standard Java versions.    \n\t\t\t", "proc_id": "2509136", "abstract": "<p>The performance of parallel code significantly depends on the <i>parallel task granularity</i> (PTG). If the PTG is too coarse, performance suffers due to load imbalance; if the PTG is too fine, performance suffers from the overhead that is induced by parallel task creation and scheduling.</p> <p>This paper presents a software platform that <i>automatically</i> determines the PTG at <i>run-time</i>. Automatic PTG selection is enabled by <i>concurrent calls</i>, which are special source language constructs that provide a late decision (at run-time) of whether concurrent calls are executed sequentially or concurrently (as a parallel task). Furthermore, the execution semantics of concurrent calls permits the runtime system to merge two (or more) concurrent calls thereby coarsening the PTG. We present an integration of concurrent calls into the Java programming language, the Java Memory Model, and show how the Java Virtual Machine can adapt the PTG based on dynamic profiling. The performance evaluation shows that our runtime system performs competitively to Java programs for which the PTG is tuned manually. Compared to an unfortunate choice of the PTG, this approach performs up to 3x faster than standard Java code.</p>", "authors": [{"name": "Albert Noll", "author_profile_id": "81350596503", "affiliation": "ETH Zurich, Zurich, Switzerland", "person_id": "P4290450", "email_address": "albert.noll@inf.ethz.ch", "orcid_id": ""}, {"name": "Thomas Gross", "author_profile_id": "81332502168", "affiliation": "ETH Zurich, Zurich, Switzerland", "person_id": "P4290451", "email_address": "trg@inf.ethz.ch", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509518", "year": "2013", "article_id": "2509518", "conference": "OOPSLA", "title": "Online feedback-directed optimizations for parallel Java code", "url": "http://dl.acm.org/citation.cfm?id=2509518"}