{"article_publication_date": "10-29-2013", "fulltext": "\n Turning Nondeterminism into Parallelism Omer Tripp * Eric Koskinen Mooly Sagiv * Tel Aviv University \nNew York University Tel Aviv University Israel USA Israel omertrip@post.tau.ac.il ejk@cims.nyu.edu msagiv@post.tau.ac.il \n Abstract Nondeterminism is a useful and prevalent concept in the de\u00adsign and implementation of software \nsystems. An important property of nondeterminism is its latent parallelism: A non\u00addeterministic action \ncan evaluate to multiple behaviors. If at least one of these behaviors does not con.ict with concur\u00adrent \ntasks, then there is an admissible execution of the ac\u00adtion in parallel with these tasks. Unfortunately, \nexisting im\u00adplementations of the atomic paradigm optimistic as well as pessimistic are unable to fully \nexhaust the parallelism potential of nondeterministic actions, lacking the means to guide concurrent \ntasks toward nondeterministic choices that minimize interference. This paper investigates the problem \nof utilizing paral\u00adlelism due to nondeterminism. We observe that nondeter\u00adminism occurs in many real-world \ncodes. We motivate the need for devising coordination mechanisms that can utilize available nondeterminism. \nWe have developed a system fea\u00adturing such mechanisms, which leverages nondeterminism in a wide class \nof query operations, allowing a task to look into the future of concurrent tasks that mutate the shared \nstate during query evaluation and reduce con.ict accord\u00adingly. We evaluate our system on a suite of 12 \nalgorithmic benchmarks of wide applicability, as well as an industrial application. The results are encouraging. \nCategories and Subject Descriptors D.3.3 [Programming Languages]: Language Constructs and Features General \nTerms Algorithms, Measurement, Performance * The research leading to these results has received funding \nfrom the Eu\u00adropean Research Council under the European Union s Seventh Framework Programme (FP7/2007-2013) \n/ ERC grant agreement no [321174-VSSC] Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with \ncredit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. \n. . $15.00. http://dx.doi.org/10.1145/2509136.2509533 Keywords nondeterminism, parallelism, concurrency \ncon\u00adtrol, synchronization, serializability 1. Introduction Nondeterminism arises in the sequential speci.cation \nof many algorithms and software applications. Examples in\u00adclude graph algorithms, such as obtaining some \nminimum spanning tree (MST) from an input graph or .nding some shortest path between two nodes; search \ntechniques, whose objective is to arrive at some admissible goal state (as in the n-queens and combinatorial-assignment \nproblems [18]); learning strategies, such as converging on some classi.er that labels all data instances \ncorrectly [54]; and more gener\u00ad ally, any algorithm for .nding a minimal or maximal subset of a collection \nof values which satis.es some property, or .nding a representative value satisfying the property. The \nconnection to parallelism is natural. Correct paral\u00adlel execution is governed by the requirement for \natomicity, whereby concurrent tasks in the parallel trace should appear as if they executed uninterleaved, \nand thus atomically. The trace is then serializable, in that it is equivalent to a serial trace involving \nthe same tasks [30]. To ensure atomicity, con.ict between tasks (e.g. due to read/write or write/write \naccesses to the same memory) is standardly resolved by se\u00adrializing their execution, thereby reducing \nparallelism. Nondeterminism presents an opportunity to reduce con\u00ad.ict, thereby increasing parallelism: \nWhen there are mul\u00adtiple valid executions of a given task, arising from nonde\u00adterminism, one might coerce \nthat task to choose the execu\u00adtions that have the least amount of con.ict with concurrent tasks (e.g. \nthose whose memory accesses are disjoint from the memory accessed by concurrent tasks). The following \nsimple example, in Java syntax, illustrates this observation: /* global */ Set elems = ...; task 1 : \n@atomic {Iterator it=elems.iterator(); task 2 : @atomic {Object o2 = ...; Object o1=it().next(); elems.remove(o2); \n// assumes: o1 . elems // assumes: o2 /. elems . . . } . . . }  Task 1 above has latent nondeterminism: \nThe Java speci.\u00adcation for iterating over a Set object places no guarantees on iteration order. This \npermits task 1 to choose an object that differs from (the object pointed-to by) o2, thereby avoiding \ncon.ict with task 2. Interestingly, in spite of the strong connection between nondeterminism and parallelism, \nwe are not aware of pro\u00adposals to exploit nondeterminism as a source of available parallelism. The only \nresearch we are aware of regarding the connection between nondeterminism and parallelism is re\u00adstricted \nto the speci.cation level [10, 11], where the idea is to reduce the correctness of a parallel program \nto checking whether every result it produces is also possible under a non\u00addeterministic sequential version \nof the program. Current implementations of the atomic construct are un\u00adable to exhaust the parallelism \nthat inheres in nondeterminis\u00adtic codes. Existing techniques do not have the means to guide nondeterministic \ntasks toward executions that would maxi\u00admize parallelism. Optimistic synchronization approaches most \nnotably, transactional memory (TM) [30] handle po\u00ad tential con.icts through the history of the execution \nwhat has occurred thus far where utilizing nondeterminism re\u00adquires foresight. In the above example, \na standard optimistic protocol would let task 1 make a free choice which object to read from elems, aborting \ndue to a read/write con.ict if that object is o2. The con.ict is semantic, and thus abstract-level synchronization \n[29, 37, 44], which was recently proposed as a means of reducing con.ict, would still force an abort. \nMeanwhile, techniques for pessimistic synchronization (i.e. without the ability to undo operations on \nthe shared state), including recent proposals featuring user interaction and client-driven lock inference \n[12, 39, 42], make conser\u00ad vative forecasts about the future behavior of concurrent tasks for lack of \nappropriate communication mechanisms, which disables the available parallelism due to nondeterminism. \nBecause task 1 may read any object stored in elems (and in particular o2), such synchronization would \nenforce owner\u00adship of the entire elems set by task 1. This would effectively serialize the execution \nof the two tasks. Existing solutions do not utilize the observation that the tasks can coordinate access \nto different objects, thereby avoiding con.ict. Acting upon this observation requires a communication \nmechanism that lets tasks broadcast their in\u00adtentions (i.e. may-modify information), such that other \ntasks can specialize their nondeterministic choices. For example, if task 1 is aware of task 2 s intention \nto remove o2 from elems, then it can read another object. Scope We investigate the problem of exploiting \nparal\u00adlelism resulting from nondeterminism in the presence of mutation operations, where synchronization \nis required. We have found that the key is a concurrency paradigm in which con.ict is based on the future, \nrather than the past. In our model, tasks share information about the behaviors they may exhibit in the \nfuture, and then specialize them\u00adselves (i.e. choose from available nondeterminsitic options) to reduce \ncon.ict. To our knowledge, this is the .rst instance of a general model for extracting parallelism from \navailable nondeterminism. In practice, this is achieved by letting each task view only the portion of \nthe shared state that is invariant over the ef\u00adfects of concurrent tasks, and make its choices based \non this partial view. In the example above, for instance, the avail\u00adable view for task 1 does not contain \n(the object pointed-to by) o2. Later, in Sections 3 4, we discuss the assumptions governing this synchronization \napproach, as well as the con\u00additions under which progress is guaranteed. We have realized our approach \nin TA N G O, a general syn\u00adchronization protocol implemented as a software tool for loop parallelization \nin both Java and C#. TA N G O enforces a pessimistic synchronization scheme guaranteeing deadlock\u00adfree, \nserializable parallel execution. TA N G O exploits nonde\u00adterminism in pure monotonic query operations, \nsuch as test\u00ading an element for membership in a container or checking for connectivity between graph \nnodes. This is achieved by specializing the query, such that it evaluates on an invariant portion of \nthe shared state. For ef.cient runtime query specialization, TA N G O per\u00adforms compile-time analysis \nof task intentions (e.g. the in\u00adtention of task 2 to remove o2). As is the case with other approaches \n[5], TA N G O is best suited for the broad class of applications where task intentions are predictable \nand con\u00adstrained to a small portion of the shared state [22]. This lets concurrent operations view and \nact on most of the shared state already at an early point, enabling a high level of par\u00adallelism. TA \nN G O comes with a library of popular abstract data type (ADT) implementations, including Set, Map and \nGraph, and requires little annotation overhead. As we detail in Section 6, we were able to apply TA N \nG O to challenging loops within a commercial application with relative ease, and obtained speedups of \nup to 3.2x. We also report on experiments over a suite of 12 loop-based algorithms that are considered \nchal\u00adlenging to parallelize due to ordering constraints on loop iterations as well as global-scope queries. \nThe results are encouraging, showing TA N G O to be comparable in perfor\u00admance to parallelization schemes \nthat were tailored speci.\u00adcally for some of our benchmarks. Contributions This paper makes the following \nprincipal contributions: Nondeterminism and Specialization We take a .rst step in developing a concurrent \nprogramming paradigm that turns latent nondeterminism into a source of parallelism. In our approach, \ncon.ict is based on potential future behaviors, and threads make nondeterminsitic choices that avoid \ncon.ict (Section 4). In particular, threads may specialize both their view of the shared state (e.g. \nreads) and their modi.cations to the shared state (e.g. writes). var sortedEdges = edges.OrderBy(x => \n-x.Weight); [Monotonic(FindPath)] foreach (var edge in sortedEdges) {edges.Remove(edge); n1 if (!FindPath(edge.U, \nedge.V)) { 6 edges.Insert(edge); } } n4 Figure 1. The RE V E R S E DE L E T E algorithm in LINQ syn\u00adtax, \nannotated for parallelization under TA N G O Synchronization Protocol We have developed a general rollback-free \nsynchronization protocol that enforces par\u00adallelism due to pure monotonic queries with available nondeterminism \n(Section 5). Our protocol generalizes several recent parallelization schemes that were tailored for speci.c \napplications, and achieves comparable per\u00adformance on these applications. Underlying our protocol is \nthe notion of specialization: a novel synchronization primitive that lets a task determinize its behavior \nto re\u00adduce con.ict. Greedy Algorithms We describe experiments on a suite of 12 benchmarks, 8 of which \nare greedy algorithms (Sec\u00adtion 6.1). These experiments, beyond measuring the per\u00ad formance of our protocol, \nenable understanding of the available parallelism in greedy algorithms according to the way in which \na greedy step depends on the previous steps. Implementation and Evaluation We have implemented our protocol \nas a tool for loop parallelization, available both in Java and in .NET. We present experiments over the \ntool (Section 6) that consists of (i) comparing it with several other synchronization techniques on the \n12 algo\u00adrithmic benchmarks (Section 6.1), and (ii) applying it to a major component in a commercial product \n(Section 6.2). The results of both experiments are encouraging. 2. Running Example We illustrate our \napproach with the RE V E R S E DE L E T E al\u00adgorithm, whose description using LINQ syntax appears in \nFigure 1. (For now, ignore the Monotonic attribute.) RE-V E R S E DE L E T E computes a minimum spanning \ntree (MST) from a connected, edge-weighted graph. It does so in a greedy style. At each step, the algorithm \nconsiders the next heaviest edge, attempting to .nd a path between its incident nodes assuming the edge \nis discarded. If the search fails, then the speculative edge removal is reverted seeing that the edge \nis essential to preserve connectivity. This program is dif.cult to parallelize for at least two reasons. \nFirst, it imposes ordering constraints: The input edges are traversed according to a total order (from \nthe heaviest edge to the lightest one). Second, side effects on the edge set cause loop-carried dependencies \nthat must be preserved. If the i-th iteration deletes its designated edge, 1 1 1 5   n2 n1n2 n2 \n2 2      44 n (a)(b)(c) nnnnnnnnnnn nnnnnnnnnnn n3 nnnnnnnnnnnn  n3  n5 n  n3  n1  3  3 \nn4  n4 n5 n5 Figure 2. Illustrative (weighted, undirected) input graph for the RE V E R S E DE L \nE T E program listed in Figure 1 (a); a view of this graph excluding edges (n1, n2) and (n1, n4) (which \nis also its MST) (b); and another view where only the two lightest edges are kept (c) then iteration \ni + 1 must account for this deletion when searching for a path between its corresponding nodes. However, \ncloser inspection of the semantics of this pro\u00adgram suggests that there is a lot of available parallelism \non certain classes of input graphs. The FindPath call is non\u00addeterministic: Any path returned by this \ncall can serve as a witness for the deletion of the corresponding edge, provided that the edges comprising \nthis path are not deleted by earlier iterations. Thus, the existence of multiple (nontrivial) paths between \na pair of connected nodes is a source of parallelism, allowing several loop iterations to run in parallel \nif each has available a witness path that is invariant over edge deletions by other iterations. To illustrate \nthis, we refer to graph (a) in Figure 2. In this graph, the deletion of edge (n1, n2) can be executed \nin par\u00adallel to deleting (n1, n4) if the supporting path for the dele\u00adtion of (n1, n2) is [n1, n3, n4, \nn5, n2]. Path [n1, n4, n5, n2] cannot, however, support the deletion of (n1, n2), because deleting edge \n(n1, n4) invalidates this path. 3. Our Approach In this section, we describe and illustrate the main \nfeatures of our approach, and discuss its scope and limitations. 3.1 Exploiting Parallelism due to Nondeterminism \nWe take a .rst step in developing synchronization methods that are aware of latent nondeterminism and \nexploit it to in\u00adcrease parallelism. We have identi.ed a prevalent loop cod\u00ading pattern, appearing e.g. \nin greedy algorithms, whereby each iteration tests the program state for some property, and then based \non the result decides which mutation opera\u00adtion to apply (if at all). In RE V E R S E DE L E T E, for \nexample, the iteration checks for connectivity between a pair of pre\u00adviously connected nodes. If the \nanswer is negative, then the nodes incident edge is restored into the graph. In certain cases, such as \nthe FindPath query, the tested property has available nondeterminism. We have designed a general protocol, \nTA N G O, that re\u00adalizes parallelism due to nondeterminism through special\u00adization. Intuitively, specialization \nis the idea of restricting the choices of a nondeterministic command, so that it has less interference \nwith concurrent tasks (e.g. by accessing less shared resources), thereby enabling more parallelism. Speci.cally, \nTA N G O implements the notion of special\u00adization over nondeterministic commmands that are also monotonic. \n1 Indeed, many nondeterministic queries are also monotonic, in that successful evaluation of the query \non a substate implies that the query holds over the entire state. This is the case with R E V E R S E \nDE L E T E, for instance, where if the FindPath test succeeds over graph G, and graph G' contains G, \nthen FindPath is guaranteed to succeed also over G'. Other examples include e.g. membership and re\u00addundancy \nchecking. Roughly speaking, monotonicity permits serializable evaluation of a command on a substate. \nFocusing the com\u00admand on a substate, rather than the entire state, restricts its (nondeterministic) behavior, \nthereby achieving specializa\u00adtion. TA N G O s choice of which substate to evaluate the task on is guided \nby the effects of concurrent tasks. That substate is the portion of the shared state that is disjoint \nfrom the effects of other tasks. In R E V E R S E DE L E T E, for example, restricting the path test \nto a subgraph that does not include an edge e does not interfere with the deletion of e. Projecting this \ndiscussion onto the example of graph (a) in Figure 2, we obtain correct parallel execution of the .rst \ntwo iterations, with respective edges (n1, n4) and (n1, n2), if the second iteration performs connectivity \ntesting over a view of the graph that is disjoint from the possible effects of the .rst iteration; namely, \nwithout edge (n1, n4). The result\u00ading view is shown as graph (b) in Figure 2 (which coincides with the \nMST of the graph in (a)). Thanks to monotonicity, the success of the test, with witness path [n1, n3, \nn4, n5, n2], is serializable.  3.2 Preliminaries: Guards, Methods and Relations The coding idiom motivating \nour concrete protocol, where each loop iteration performs a test and then acts on its re\u00adsult, encourages \nlanguage-level abstractions that separate tests from actions. Dijkstra has formulated this distinction \nin his language of Guarded Commands in terms of guards (being boolean queries) and methods (being state \ntransform\u00aders) [17], which we adopt also for our setting. As an illustra\u00ad tion, in Figure 1 we denote \nthe guard with dotted underline, and the methods with solid underline. We assume that guards are pure \n(i.e. free of side effects). In our technical description, as well as our implementa\u00adtion, we instantiate \nour conceptual approach into a relational setting, whereby the shared state between loop iterations is \n1 We refer to monotonicity in the mathematical sense, whereby a function f between a pair of ordered \nsets is monotonic if it preserves the given order; i.e. .x, y. x < y . f(x) < f (y). represented and \nmanipulated as relations. This style of data organization is familiar from databases, and has been uti\u00adlized \nby several recent works [26, 27, 51].2 The key advantage of the relational form is in abstracting data \nfrom its representation, which permits a clear, high\u00adlevel description of what guards and methods do. \nThis is also the rationale behind recent language features, such as the LINQ queries in C#. In fact, \nour prototype implementation is based on the LINQ language constructs, which offer a convenient means \nof expressing guards. The relational abstraction consists of tuples and relations. A tuple t = (c1 : \nv1, c2 : v2, . . .) maps a set of columns, {c1, c2, . . .}, to values. A relation is a set of tuples \nover identical columns. We support the standard relational opera\u00adtions, which we describe below in ML-like \nsyntax (where !r fetches the current value of r, and r . v denotes assign\u00adment): empty r = r . ref \u00d8 \ninsert r t = r . !r . {t}remove r t = r . !r \\ {t}query r f b = b . f(r) Informally, empty r creates \na fresh relation r; insert r t inserts tuple t into relation r; remove r t removes tuple t from r; and \nquery r f b tests relation r for property f and stores the (boolean) result in b. For simplicity, we \nleave the syntax for f unspeci.ed. This level of detail is not required for our technical description, \nand our prototype tool features the built-in LINQ querying constructs. Revisiting our running example \nin Figure 1, we can ex\u00ad press the shared state as the edges binary relation. Remove and Insert are modeled \ndirectly by their relational counterparts, remove and insert, and FindPath(u,v) is translated into the \nquery (u, v) . tc((u, ), edges), where tc denotes transitive closure, and the entire query checks whether \ntuple (u, v) is in the transitive closure of the tuples t such that fst(t) = u over the edges relation. \n 3.3 The TA N G O Protocol: Informal Sketch The concrete TA N G O protocol leverages the observations \ndiscussed in Section 3.1 as follows:3 User Speci.cation The distinction between guards and methods is \nmade explicit thanks to the relational setting, as explained in Section 3.2. For our current prototype, \nwe ask the user to decorate monotonic guards with the Monotonic attribute, as exempli.ed in Figure 1. \nThis at\u00ad 2 We stress that other approaches, based e.g. on ADTs or on object-oriented notions of encapsulation, \nare also plausible. Recent studies have in fact shown that abstract-level synchronization, according \nto ADT operations and their semantic effect, can improve parallel performance dramatically [29, 37]. \nWe leave this exploration for future research. 3 In our discussion here, and in general, we refer to \ntasks and loop iterations interchangeably, where what we mean by a task is the atomic parallel execution \nof a single iteration. tribute is likely amenable to automatic inference, but we leave this enhancement \nfor future versions of our tool. Compile-time Analysis At compile time, a lightweight static analysis \ncomputes a safe approximation of the ef\u00adfects of each iteration. Speci.cally, for loops iterating over \nsequences, the analysis tries to link these effects to a symbolic representation of the element pointed-to \nby the loop variable (the edge variable in the loop in Figure 1). Runtime Synchronization At runtime, \nduring parallel loop execution, the TA N G O synchronization protocol automat\u00adically builds safe views \nfor monotonic guards, as illus\u00adtrated with graphs (a) and (b) in Figure 2. The view is constructed according \nto the possible effects of higher\u00adimportance concurrent tasks (e.g. earlier iterations, as in RE V E \nR S E DE L E T E). If a monotonic guard evaluates pos\u00aditively based on a limited view of the entire state, \nthen parallel progress has been achieved. Otherwise, there are two options: If the guard fails, but all \nhigher-importance tasks have completed already, then the guard result is se\u00adrializable. Otherwise, view \nconstruction is retried per the observation that the possible effects of parallel tasks only decrease \nwith time (becoming empty when a task com\u00adpletes), permitting a more complete view on retry. A remaining \nchallenge, which we have yet to discuss, is support for parallel method execution (like the edges.Remove \nand edges.Insert calls in Figure 1). If, for instance, the second iteration deletes edge (n1, n2) while \nthe .rst iteration is still running, then parallel execution is not necessarily serializable, since there \nis no sequential run where the .rst iteration observes a state where edge (n1, n2) is absent. To deal \nwith this challenge, TA N G O follows an approach that is similar in spirit to .at combining [28]. Instead \nof di\u00ad rectly applying a method to the shared state, TA N G O records the method application to a task-private \nlog. Logged meth\u00adods are accounted for in views constructed for the task (e.g. the deletion of (n1, n2) \nin graph (b) in Figure 1). This per\u00ad mits a task to proceed through methods without blocking. Moreover, \nif task t is about to complete, but concurrent tasks that must not observe its effects are still running, \nthen instead of either committing t s private log (which is wrong) or blocking t until these tasks .nish \n(which is inef.cient), TA N G O appends t s log of outstanding methods to that of a running task, t ', \nwhere t ' is the live task that immediately ' succeeds t in importance. t can then terminate, and t be\u00adcomes \nresponsible for executing t s methods, ensuring that these are applied to the shared state in a serializable \norder.  3.4 Discussion: Scope and Limitations Our approach generalizes several recent efforts to parallelize \nspeci.c algorithms, including the helper-threads (HT) scheme [34, 45], where auxiliary threads of.oad \nwork from the main thread by performing computations ahead of time, in parallel, either pessimisti\u00adcally \n[34] or speculatively (HT+TM) [45]; and the dynamic-independence (PMS) approach [8], which exploits the \nobservation that in certain greedy algorithms, such as the algorithm for .nding a maximal independent \nset (M IS), each iteration only depends on a subset of the previous iterations to run a task ahead of \nits designated time, as soon as the tasks it depends on have completed. Both HT and PMS enforce pessimistic \nparallelization, let\u00adting background threads execute computations ahead of time when doing so is provably \nserializable. TA N G O generalizes these two approaches in enforcing weaker preconditions for correct \nparallelization. From a practical standpoint, TA N G O asks only for monotonicity annotations, and obtains \ncompa\u00adrable speedups to HT and PMS on the benchmarks motivat\u00ading these approaches. Runtime Overhead The \nmain challenge faced by TA N G O is to constrain the cost of constructing task-speci.c views. The practical \nlimitation this places is that the effects of a task must have a compact description. This is true of \nRE V E R S E DE L E T E, where the substate manipulated by a given loop iteration is a single edge. TA \nN G O incurs tolerable overheads over the 12 algorithmic benchmarks from our suite, the worst case being \na factor of 1.35x compared to a sequential version of the benchmark (cf. Table 2). Applicability As our \nanalysis so far suggests, TA N G O is geared toward computations where guards are nondetermin\u00adistic with \nmultiple admissible witnesses, and the intentions of a task refer to a small portion of the shared state. \nTwo broad categories of applications satisfying this pro.le are pruning algorithms, such as RE V E R \nS E DE L E T E, the alpha-beta search algorithm [47] and decision-tree mini\u00ad mization [41], where a witness \ncontained in a modi.cation\u00ad free substate secures early deletion of an element; and dually,  incremental \nalgorithms, such as K RU S K A L MSF [15], task scheduling [15] and quick elimination [2], where a witness \ncontained in a past version of the evolving solution secures early exclusion of an element.  We emphasize \nthat though TA N G O was built as a tool for loop parallelization, the principles underlying it are general \nand applicable also in other parallel contexts, such as con\u00adcurrent data structures and reactive systems. \nOur experiments (described in Section 6), where we con\u00ad sider benchmarks of varying characteristics, \nindicate that TA N G O is highly effective in parallelizing applications with nondeterministic reads \nand granular intentions, like KRU S K A L MSF and RE V E R S E DE L E T E, and is also able to utilize \nsome of the available parallelism in codes with de\u00adterministic reads. However, algorithms where the intentions \nof a task are initially broad, and undergo re.nement only at a late point, are not good candidates for \nTA N G O. 4. Concurrent Tasks, Specialization, Priority In this section, we provide a complete technical \ndescription of our approach. We begin by stating a restrictive require\u00adment for serializable execution \nof an operation in parallel with concurrent tasks, whereby that operation must com\u00admute with all possible \nfuture behaviors of the tasks. We then introduce specialization as a means of turning this restriction \ninto an advantage: Instead of requiring commutativity over all behaviors, concurrent tasks can select \nspeci.c behaviors that are mutually compatible and commit to these behaviors. Finally, we introduce a \nnotion of priority, and show that this leads to a progress guarantee. 4.1 Execution Model We assume \na generic programming model with standard control structures (if, while, sequential composition, etc). \nThere is a single shared state from an unspeci.ed state space S. There are a .nite number of threads, \neach with a unique identi.er t : T , that access the shared state in only two ways: Guard g : S . {true, \nfalse} is an observation that a thread makes on the shared state.  Method m : S . P(S) is a modi.cation \nthat a thread makes on the shared state.  Guards and methods can be realized in many ways, depend\u00ading \non the particular language context. The most natural ex\u00adample is Dijkstra s language of Guarded Commands \n[17]. Another example is a language that performs low-level memory operations, where read is a guard \nand write is a method. As discussed in Section 3.2, for this paper we have chosen a relational program \nrepresentation, whereby the shared state is expressed as one or more relations, guards are boolean query \ncommands, and the methods are insert and remove. When tasks are executed concurrently without synchro\u00adnization, \nthey do not always behave correctly. There may be executions in which one task interferes with another \ntask. Example 4.1. Assume an unsychronized parallel run of the code in Figure 1 on graph (a) in Figure \n2, where task (n2, n5) is the .rst to execute. That task would then delete edge (n2, n5) based on witness \n[n2, n1, n4, n5], which is in\u00advalid, because in sequential execution of the code, both edge (n1, n4) \nand edge (n1, n2) would be deleted, rendering edge (n2, n5) part of the MST. For safe concurrency, tasks \nmust be aware of the potential effects of concurrent tasks, as well as their assumptions on the shared \nstate. As is standard, we use the notion of commutativity to quantify interference [35, 36]: ' ' m N \ns m .. [ m ; m ' ] s = [ m ; m] s If the commands of two threads (pairwise) commute, then it is safe \nfor them to proceed in parallel. Intuitively, this is because their interleaved execution can be rearranged \ninto a serial execution. However, our work takes a new tack: Our notion of con.ict is based on what may \nhappen in the future rather than what has happened in the past. To this end, we add two criteria: Condition \n4.1. A method m performed by a task t must not change the boolean value of every guard g that may ever \nbe ' observed by a concurrently executing task t . Condition 4.2. A method m performed by a task t must \n ' commute with every method m that may ever be performed ' by a concurrently executing task t . If \nConditions 4.1 and 4.2 hold, then every execution of the concurrent tasks is serializable. These two \nconditions are strong and thus somewhat sur\u00adprising. We are requiring that a method commutes with ev\u00adery \nmethod/guard that any other task may ever perform. This is stronger than the pairwise synchronization \nconditions that hold over histories in traditional systems such as TM or lock\u00adbased mutual exclusion. \nExample 4.2. For the code in Figure 1 and graph (a) from Figure 2, the FindPath guard over (n1, n2) does \nnot com\u00admute with the edges.Remove method over (n1, n4) because of witness path [n1, n4, n5, n2]. Parallel \nexecution of these two commands is possible, however, assuming the (n1, n2) guard can only select witness \n[n1, n3, n4, n5, n2]. Our important insight, which we formulate in the follow\u00ading, is that the strict \nsynchronization model due to Condi\u00adtions 4.1 and 4.2 makes potential con.icts explicit. This lets tasks \ncoordinate their present behaviors in order to avoid as much con.ict as possible in the future. For this \nreason, as we show in Section 6, our approach is in fact competitive with many leading-edge concurrent \nprogramming algorithms. 4.2 Specialization and Preservation Nondeterminism complicates commutativity \njudgments, be\u00adcause there are multiple possible behaviors for a task. Con\u00adcurrent tasks can only take \na next step if that step commutes with all of these behaviors. This is seen in Conditions 4.1 and 4.2, \nwhere m must commute with all possible com\u00admands that may be executed by concurrent tasks. We turn this \nlimitation of synchronizing nondeterministic tasks into an advantage through specialization: A task spe\u00adcializes \nby restricting its possible behaviors, which increases concurrency by relaxing the commutativity checks. \nWe de\u00ad.ne specialization for both methods and guards as follows: De.nition 4.1 (Specialization). Specialization \nof methods M and of guards G is as follows: m1 M m2 = .s. [ m1] s . [ m2] s g1 G g2 = .s. [ g1] s . [ \ng2] s. We will often drop the subscript on as it is clear from context. For the de.nition of dynamic \nspecialization over a speci.c state s, we simply remove the universal quanti.ca\u00adtion. Above a specialized \ncommand is a state transformer that may have less nondeterminism than the original command. If a specialized \nguard holds, then the original guard must hold, but the specialization may have a particular implementation \nthat, for example, is focused on a portion of the state space. Example 4.3. For the setting described \nin Example 4.2, if we specialize the FindPath guard over (n1, n2) by restrict\u00ading its evaluation to edges \nthat are invariant over higher\u00adpriority tasks (namely, by excluding (n1, n4), as illustrated in graph \n(b) in Figure 2), then we obtain witness path [n1, n3, n4, n5, n2]. This witness implies commutativity \nbe\u00adtween the guard and the edges.Remove command over edge (n1, n4). The advantage with specialization \nis that a task can make an informed choice of which behaviors to eliminate and which behaivors to retain \nbased on the behaviors of concur\u00adrent tasks. This permits inter-task coordination, where non\u00addeterminism \nallows tasks to choose behaviors that are mutu\u00adally compatible, thereby enabling safe parallel progress. \nTheorem 4.3 (Preservation). Method specialization, as well as specialization of monotonic guards, preserve \nthe behav\u00adiors and serializability of the original system. Proof Sketch. For a method, specialization \nyields less post\u00adstates than the original method, but these are all admissible due to containment. For \nmonotonic guards, successful eval\u00aduation on a substate implies successful evaluation on the en\u00adtire state \nthanks to monotonicity. Local preservation of be\u00adhaviors and serializability implies a system that is \nglobally serializable. Of course we have to be careful when we specialize that we have not performed \ntrivial specialization, simply eliminat\u00ading all behaviors. We address this concern in the remainder of \nthis section, where we show that if tasks are ordered by priority, then at each point, at least one of \nthe tasks (specif\u00adically, the highest-importance one) is free of specialization obligations, obviating \nthe threat that all tasks specialize triv\u00adially, thereby blocking execution.  4.3 Priority and Progress \nWhile specialization in conjunction with commutativity guarantees preservation, progress is still not \nensured. The dif.culty is in verifying that at least one task can step through a guard, which is not \nnecessarily the case unless the tasks are assigned priorities. Example 4.4. Assume a parallel run of \nRE V E R S E DE L E T E on graph (a) in Figure 2, where the .rst two iterations have completed, and now \nthe third and fourth iterations with respective edges (n4, n5) and (n2, n5) simultaneously specialize, \nresulting in view (c). This is a stuck state, because for both iterations, (i) the FindPath guard fails, \nand (ii) the view is partial, disabling progress through the guard when its evaluation is negative. \nIn response to this problem of nonterminating execu\u00adtions, we introduce task priorities. We show that \nwith this strengthening, tasks can execute in parallel while ensuring both progress and preservation. \nIntuitively, the power of pri\u00adoritization is in breaking the symmetry between tasks, such that a higher-priority \ntask need not account for the behavior of lower-priority tasks. This yields a progress guarantee. The \nrules we have presented thus far provide a serializ\u00adable concurrent model in which tasks specialize their \nnonde\u00adterministic choices in order to proceed in concert. We now revise Conditions 4.1 and 4.2, obtaining \nnew conditions that guarantee progress (provided the tasks terminate sequen\u00adtially). We .rst de.ne priority: \n De.nition 4.2 (Priority). Let \u00ab : (T \u00d7 T ) be a total order on task identi.ers. We can now state the \ntwo conditions. Condition 4.4. A method m performed by task t must not change the boolean value of a \nguard g that was observed by ' ' another task t such that t \u00bb t. Condition 4.5. A method m performed \nby task t must com\u00ad ' mute with every method m that may ever be performed by ' ' another task t such \nthat t \u00bb t. The formal detail of these conditions involves quantify\u00ading over all possible executions, \nand for each execution, per\u00adforming a pairwise check whether m can move to the left of methods and guards \nof other concurrent tasks of higher priority [35]. Intuitively, priority manifests as a discount: It \nlets a task ignore both the assumptions (i.e. guards; Con\u00addition 4.4) and the effects (i.e. methods; \nCondition 4.5) of lesser-priority tasks, which means that at each point, at least one task can progress \nwithout constraints. Example 4.5. We return to Example 4.4 and .x it per Con\u00additions 4.4 and 4.5. The \nthird iteration (with respective edge (n4, n5)) is more important than the fourth one (with edge (n2, \nn5)), which means that it can ignore the effects of the fourth iteration. Thus, the graph view by the \nthird iteration after the .rst two iterations have terminated is complete, co\u00adinciding with the actual \ngraph state. This permits progress through the FindPath guard in spite of its negative evalua\u00adtion. The \nsame situation arises for the fourth iteration after the third iteration completes. Theorem 4.6 (Progress). \nFor tasks t1 . . . tn that each ter\u00adminate in a sequential setting and priority relation \u00ab, either (i) \nall tasks have completed, or else (ii) there exists a task ti that can perform a method or a guard. Proof \nSketch. The highest-priority task (denoted ti) can pro\u00adceed without waiting for other tasks. Condition \n4.4 ensures that the values of all guards of the highest-priority task are not altered by concurrent \ntasks. Condition 4.5 ensures that if another task tj \u00ab ti executes a method m concurrently, then m commutes \nwith all the operations of ti. Thus, there is necessarily an equivalent serial history in which ti happens \nbefore tj. 5. The TANGO Protocol We now describe the TANGO synchronization protocol, which instantiates \nthe formal framework developed in Sec\u00adtion 4. 5.1 Intention Analysis In the concrete TANGO protocol, \nwe model the effects of a task as the substates it may modify. This assumes that the state is decomposable \ninto substates: s = i si for atomic substates si, with the assumption that the state space S is closed \nunder substates. For a concrete state, the standard de\u00adcomposition is into memory locations (e.g. variables \nand ob\u00adject .elds), the (low-level) state description being a mapping from memory locations to their \nassociated value [52]. In TANGO, the shared state is expressed as relations, and so state decomposition \nis in terms of quali.ed tuples. The possible effects of the two simple methods are straightfor\u00adward, \nas follows: mod(insert r t) = (!r, t) mod(remove r t) = (!r, t) To induce correct task-centric state \nviews, containing only portions of the shared state that are invariant over the effects of concurrent \ntasks, the runtime system must be aware of what these tasks may do. Computing the effects of a task on \nthe .y, during a parallel run, is expensive. Instead, TANGO features a lightweight static data-.ow analysis, \ntai\u00adlored for loop structures. The analysis is performed at compile time, and is a for\u00adward analysis \nstarting at the loop head. When the analy\u00adsis encounters a method, it stores its affected substate as \na symbolic access path (e.g. (edges, edge) for the Remove and Insert statements in Figure 1, modeled \nas remove and insert respectively). After the analysis reaches a .xpoint over the control structure of \nthe loop body, each collected access path (r, t)is processed as follows: 1. If (r, t) is a constant access \npath (i.e. both the relation and the tuple are amenable to static resolution), then it .ows into the \nmay-modify set as is. 2. Otherwise, if the loop iterates over a sequence and therefore the loop variable \npoints to a tuple and (r, t)is identical to the (symbolic) tuple pointed-to by the loop variable, then \nagain (r, t) .ows into the may\u00admodify set directly. This is the case e.g. in Figure 1 with (edges, edge). \nThe runtime system can resolve sym\u00adbolic tuples of this sort before the loop starts executing  based \non the elements in the argument sequence. Hence the relevance of this case. 3. A third situation is where \nthe tuple as a whole is unavail\u00adable, but the analysis can statically resolve the relation r. In this \ncase, the abstract tuple (r, *) .ows into the may\u00admodify set, indicating that a given iteration may mutate \nany tuple in relation r. 4. The .nal case is where neither the relation nor the tuple are amenable to \nstatic resolution. This translates into T, meaning that an iteration may modify any tuple in any relation. \n  The results of the compile-time intention analysis are made available to the runtime system as a may-modify \noracle, mod, mapping tasks to symbolic tuple descriptions. 5.2 Runtime Protocol The runtime TANGO protocol \nis described in Figure 3 ac\u00adcording to the operations it synchronizes: method execution (EXECUTEMETHOD), \ntask termination (EXIT) and guard evaluation (TESTGUARD). For clarity, we separate the for\u00admal description \nin Figure 3 into two contexts, treating the most important task, max T , separately from the other tasks. \nmax T executes as if no other task is running in parallel. To avoid from interfering with max T , as \nwell as any other higher-importance task, another task t must therefore log its mutations instead of \nexecuting them directly. To ensure that its control choices governed by guards are serializable, t must \neither block until it becomes the most important task for a general guard g, or ensure that g is successful \nif g is nondeterministic (otherwise retrying until either g becomes successful or t becomes max T ). \nIf t is about to complete prior to becoming max T , then it can exit and transfer its log of outstanding \nmethods to the (live) task succeeding it in importance. Otherwise, upon becoming max T (the t max T --+ \nt = = max T event), t must .rst clear its own log of pending methods (log(t )|t ). When max T is about \nto terminate, it executes all pending methods transferred to it according to the order between the respective \ntasks. We now relate the informal description above to Figure 3. The .rst context is where the argument \ntask t ranks .rst in priority. In this case, methods and guards are evaluated directly over the shared \nstate s (the [ \u00b7] notation). On exit, the task applies the outstanding methods in its log (due to other \ntasks) to the shared state. In the complementary context where t is not the highest\u00adpriority task, methods \nare recorded into the task-private log, rather than performed. When the task is about to termi\u00adnate, \nit delegates its outstanding log to the task succeeding it in priority. Evaluation of a general guard \nmust block until the task becomes .rst in priority. A monotonic guard, how\u00adever, is amenable to premature \nevaluation over the portion of the shared state that is invariant over concurrent tasks  (s \\ t.\u00bbt mod(t \n' )), where we account for logged meth\u00ad Assumption: t = max T EXECUTEMETHOD(t, m) s . [ m] s EXIT(t \n) commit log(t ) in order TESTGUARD(t, g) return [ g] s Assumption: t = max T Event: t max T --+ t == \nmax T commit log(t)|t in order EXECUTEMETHOD(t, m) EXIT(t ) TESTGUARD(t, g) TESTMONOGUARD(t, g) log(t \n) . log(t ) \u00b7 (t, m) log(t 1) . log(t 1) \u00b7 log(t) block until t = max T while t = max T where t 1 = minT\u00bbt \nreturn [ g] s if [ g] s11 where s11 = [ log(t)]]s1 where s1 = s \\ t \u00bbt mod(t1) return true return [ \ng] s Figure 3. The TANGO protocol, described with the following supporting notation: s is the shared \nstate; T is the set of live tasks; and log maps every task t to its log of outstanding methods (quali.ed \nby their original task) ods to re.ect mutations made by the parent task of the guard ([ log(t)]]s '). \nTheorem 5.1 (Correctness). The TANGO protocol ensures both progress and preservation per Theorems 4.3 \nand 4.6. Proof Sketch. For guard evaluation, the only situation where a guard is tested outside its sequential \norder is if that guard is monotonic. In that case, the guard is evaluated over an invari\u00adant substate, \nand progress is conditioned on its successful evaluation. This means that the guard result is serializable. \nFor method execution, the highest-priority task performs its methods in their sequential order. Other \ntasks log their operations, rather than performing them directly, and the log is manipulated according \nto sequential constraints, combin\u00ading logs on task exit with the task succeeding it in impor\u00adtance. Finally, \nwhen a task becomes .rst in priority (t = max T --+ t = max T ), it commits its own methods, log(t)|t \n(but not those combined into its log from other tasks), to enable direct execution of its future methods \nin a serializable fashion, as speci.ed in Figure 3. Progress is ensured thanks to task priorities. (See \nTheo\u00adrem 4.6.) The most important task is oblivious to other tasks, and so if all tasks terminate sequentially, \nthen all tasks are also guaranteed to complete when running in parallel.  5.3 Prototype Implementation \nTANGO is implemented both in Java and in C# (.NET). The TANGO library provides a parameterized implementa\u00adtion \nof the Relation ADT, featuring ef.cient linearizable im\u00adplementations of the primitive relational operations. \nTANGO also provides common specializations of the Relation type (UnaryRelation, BinaryRelation, etc), \nas well as a collec\u00adtion of useful guards (including transitive reachability, as in FindPath, and existential \ntests). Other ADTs, such as Map, Set, List and Graph, are implemented atop the Relation ADT. As stated \nearlier, the C# version of TANGO is built over the LINQ syntax features, enabling concise coding of loops \nwith side effects, as exempli.ed in Figure 1. Finally, TANGO s algorithm for statically approximating \ntask inten\u00adtions makes use of the WALA framework. 6. Experimental Evaluation In this section, we describe \ntwo sets of experiments that we conducted over TANGO. The .rst experiment compares TANGO with several \nspecialized parallelization schemes on 12 algorithmic benchmarks. We then report on our experi\u00adence in \nintegrating TANGO into a commercial application. 6.1 Algorithmic Benchmarks Our benchmark suite for the \n.rst experiment consists of 12 benchmarks: 8 greedy algorithms and 4 dynamic pro\u00adgramming (DP) algorithms. \nThe main characteristics of the benchmarks are summarized in Table 1. Our choice of benchmarks was driven \nby two considerations: First, the chosen algorithms exhibit varying levels of available paral\u00adlelism, \nwhich enables proper investigation of when and why our approach would work. This is also the reason why \nfor all the greedy benchmarks we used two different input pro.les, one permitting more parallelism than \nthe other. Second, all the benchmarks in our suite are popular, being of practical interest and applicability. \nSome of the benchmarks have also been parallelized in the past, which enables comparing our approach \nwith other techniques. Experimental Setting Our experiments are based on the C# version of TANGO. The \nexperiments were conducted on a 16-core IBM X3550 M2 machine with 100GB of RAM run\u00adning Windows Server \n2008 R2 64-bit with Microsoft .NET Framework v4.0.30319. Inputs were generated randomly, as detailed \nin Table 3. Each con.guration of benchmark, input pro.le and parallelization technique was run 11 times. \nThe reported numbers are the average of the 10 last executions (excluding the .rst, cold run). Name \nDomain Description DIJK S TR A SP EDI T DI S TANC E E-N E T GREE DY CO LO R ING JA RVI S MAR C H KNAPS \nAC K KRU S KAL MSF MIS PA RT I T I ON RE V E RSE DE LET E RU L E PRU N I NG SEQ. ALI G N ME NT Graph \nTheory Information Theory Machine Learning Graph Theory Comp. Geometry Combinatorial Opt. Graph Theory \nGraph Theory Combinatorics Graph Theory Data Mining Bioinformatics Produces a shorest path tree Measures \nthe difference between two sequences Computes an E-net from a set of points Computes a vertex-coloring \nsolution for a graph Computes the convex hull of a set of points Solves the 0/1 knapsack optimization \nproblem Computes an MST incrementally Finds a max. independent set of graph vertices Decides if multiset \nhas two equal partitions Computes an MST decrementally Finds a min. set of assoc. rules incrementally \nPerforms global alignment of two sequences Table 1. Benchmark characteristics Name Workload characteristics \nParameters High parallelism Low parallelism DI J K S T R A SP Rand. graphs 104 nodes, 2 \u00b7 105 edges 104 \nnodes, 106 edges ED I T DI S TAN C E Rand. pairs of strings Random strings over 26 characters of length \n500 E-NE T Rand. 2D points Average (Euclidean) distance of 0.01 Average distance of 0.5 GR EEDY CO LO \nR IN G Rand. graphs 104 nodes, 105 edges 104 nodes, 5 \u00b7 105 edges JARV I S MAR CH Rand. 2D points 106 \npoints 105 points KNAP S AC K Rand. integers 500 numbers in the range 1 - 500 KRU SKA L MSF Rand. graphs \n104 nodes, 2 \u00b7 105 edges 104 nodes, 2 \u00b7 106 edges MIS Rand. graphs 5 \u00b7 105 nodes, 2.5 \u00b7 106 edges 105 \nnodes, 106 edges PA RT I T ION Rand. integers 500 numbers in the range 1 - 100 RE VE RS E DE L ETE Rand. \ngraphs 104 nodes, 2 \u00b7 106 edges 104 nodes, 105 edges RU L E PRU N ING Rand. assoc. rules with one consequent \n6 antecedents 8 antecedents SEQ . AL I G NME NT Rand. pairs of strings Random strings over 5 characters \nof length 500 Table 3. Workload characteristics Name Speedup Overhead DI J K S T RA SP ED IT DI S TA \nN CE E-N ET GR E EDY CO L O RI N G JARV IS MA R C H KNA P S AC K KRUSK A L MSF MIS PA RT IT I O N REVER \nS E DE L E TE RUL E PRU N I N G SE Q . AL I GNM EN T 0.95 0.9 2.62 0.76 4.4 1.5 3.5 0.88 0.75 2.55 \n0.825 5.8 3.9 4.2 2.36 0.87 6.25 0.77 5.2 2.4 1.1 1.11 1.25 1.21 1.26 1.35 1.16 1.1 1.14 1.19 \n1.2 1.19 1.09 1.12 1.23 1.16 1.17 1.17 1.21 1.26 Table 2. Parallelization statistics Our choice \nof a machine with a high volume of RAM was deliberate, so as to minimize the impact of garbage collection \non performance. This is particularly effective in the .NET setting, where contrary to the Java Virtual \nMachine (JVM), the Common Language Runtime (CLR) does not set any prede.ned limit on heap size, and instead \ndepends on the host operating system for memory allocation. In addition to TA N G O and the HT and PMS \napproaches, presented in Section 3, we included the following paral\u00ad lelization techniques in our evaluation: \nTop-down Parallelization (TD) [50] This scheme paral\u00adlelizes DP algorithms in a top-down manner, where \nso\u00adlutions to subproblems are cached in a lock-free hash table, and the order in which subproblems are \ncomputed is random. Work Distribution (BSP) [13] This framework works for associative operators, such \nas min and max. Each thread applies the operator to a portion of the candidate values, and broadcasts \nits local candidate to all other threads. Lazy Speculation (OPT) [51] This form of optimistic syn\u00adchronization \ntests for con.icts only at transaction commit time. The transaction operates on a privatized version \nof the shared state. For 8 of the benchmarks, we compared TA N G O with the technique that was originally \nproposed to parallelize that benchmark. Our choice of parallelization technique for the remaining 4 benchmarks \n(GR E E DY CO L O R I N G, RU L E PRU N I N G, RE V E R S E DE L E T E and E-N E T) was guided by their \nsimilarity to the other 8 benchmarks. The available parallelism in GR E E DY CO L O R I N G has the same \nstructure as MI S, being due to the sparse dependencies between the iterations. RU L E PRU N I N G and \nE-N E T are similar to KRU S K A L MSF in that a helper thread can determine, ahead of time, the redundancy \nof an element. Finally, for RE V E R S E DE L E T E, speculation is the only technique out of the above \nthat yields a sound parallelization solution. Performance Results The speedup and overhead numbers appear \nin Table 2. Speedup is calculated as the ratio of sequential running time to parallel running time using \n16 threads. Overhead is the ratio of execution time using 1 thread to sequential execution time. For \nbenchmarks with two inputs pro.les, the left number corresponds to the low\u00adparallelism input. The speedup \ntrends going from 1 to 16 threads are shown in Figures 4 15, where there are either one pair or two pairs \nof plotted graphs in each .gure, vi\u00adsualizing TA N G O compared to its competing appoarch over one or \ntwo workloads. Detailed workload characteristics are provided in Table 3. The main performance penalty \nof TA N G O lies in inducing task-centric views, where a view is computed by factoring in the effects \nof concurrent tasks. On average, across all bench\u00admarks and workload con.gurations, this leads to a slowdown \nof 1.18x (i.e. less than 20%) compared to the sequential implementation. In the worst case (on GR E E \nDY CO L O R -I N G 10Kx500K), TA N G O incurs an overhead of 1.35x. We view TA N G O s overhead as tolerable, \nand expect that with more engineering, this overhead can be mitigated. TA N G O s memory consumption \nis correspondingly ef.\u00adcient. In pro.ling runs over the benchmarks in our suite, the memory footprint \nof TA N G O was no more than 25% higher than the sequential version. This is not surprising. TA N G O \nmaintains may-modify logs only for running tasks (as op\u00adposed e.g. to STM, where read/write logs for \nterminated tasks are still needed by concurrent tasks to determine con\u00ad.ict). This means that at any \npoint during parallel execution, the number of active logs is bounded from above by the num\u00adber of processors \n(16 in our case). Thus, if the may-modify information of a given task can be represented succinctly, \nthen the memory overhead of TA N G O becomes constrained. This is indeed the case with all our benchmarks, \nwherein the effects of a task can be summarized concisely. In RE-V E R S E DE L E T E, for instance, \ntask modi.cations are re\u00adstricted to a single graph edge. At the other extreme, in the pathological case \nof DI J K S T R A SP, task modi.cations span the entire state, which can again be stated concisely. This \nexplains the overhead data in Table 2. Finally, for bottomline performance, TA N G O achieves a speedup \nof over 3x on 6 of the benchmarks, being compa\u00adrable to its competing specialized technique (if not faster) \non 10 out of the 12 applications (D I J K S T R A S P and JA RV I S MA R C H being the exceptions). TA \nN G O s speedup on D I J K -S T R A S P and JA RV I S MA R C H is negative. On DI J K S T R A SP, even \nthe specialized technique of [45] achieves negligi\u00ad ble speedup. On JA RV I S MA R C H, however, TA N \nG O is sig\u00adni.cantly worse than the technique of [13], which obtains a speedup of up to 7x. In contrast, \nTA N G O outperforms lazy speculation on RE V E R S E DE L E T Eby a factor of > 2. Discussion The speedup \nstatistics reported above are con\u00adsistent with our expectations. TA N G O favors nondetermin\u00adism in read \noperations, as well as granular and predictable write operations. The classic example is RE V E R S E \nDE L E T E (cf. Section 3). This explains the poor results on JA RV I S MA R C H and DI J K S T R A SP, \nwhere a task can only commu\u00adnicate conservative intentions until a late point in its execu\u00adtion, and \nread operations are deterministic. The reason why the BSP approach works well for JA RV I S MA R C H \nis that it exploits intra-operator, rather than inter\u00adoperator, parallelism. Instead of parallelizing \nthe entire task of extending the convex hull, BSP parallelizes only the inner loop searching for the \nnext point in the hull. We could do the same, applying TA N G O to the (inner) search task, but preferred \nto remain consistent with the other benchmarks. GR E E DY CO L O R I N G and the DP algorithms (E D I \nT DI S -TA N C E, KNA P S AC K, S E Q . AL I G N M E N T and PA RT I T I O N) are more compatible with \nTA N G O, though still not its ideal clients. While a task has narrow intentions as it commences (which \nis good), read operations are deterministic, leaving limited opportunity for parallel guard evaluation. \nHence, the speedups obtained by TA N G O fall below the competing approaches on these benchmarks. The \nremaining 5 bench\u00admarks (RE V E R S E DE L E T E, E-N E T, RU L E PRU N I N G, MIS and KRU S K A L MS \nF) feature both nondeterministic reads and granular intentions, and thus behave well under TA N G O. \nThough TA N G O outperforms competing approaches only in speci.c cases, we .nd that its advantage comes \nfrom (i) its generality compared to specialized parallelization tech\u00adniques devised for speci.c domains \nand algorithms, as well as (ii) its usability as an encapsulated library in two popu\u00adlar general-purpose \nlanguages, which requires only minimal annotation effort from the programmer. We conclude with two remarks \non the effect of prioritiza\u00adtion on performance. First, most of our benchmarks perform ordered iteration \nover the input collection, and thus force a speci.c prioritization scheme. The remaining algorithms (e.g. \nGR E E DY CO L O R I N G) are unordered, but are normally implemented atop one or more heuristic ordering \nschemes (e.g. prioritizing a node inversely to its degree in GR E E DY CO L O R I N G). We did the same. \nWe can report, however, that in preliminary experiments we performed that involved manipulation of concurrent \ndata structures (like Graphs and Maps) by random clients, where prioritization was decided ad hoc according \nto the starting time of a task, the variance we observed between executions was relatively low. We defer \nfurther analysis of the effect of the prioritization scheme on performance, and optimization thereof, \nfor future work.   Figure 4. Speedup on DIJKSTRA SP Figure 5. Speedup on EDIT DISTANCE Figure 6. Speedup \non E-NET Figure 7. GREEDY COLORING Figure 8. JARVIS MARCH Figure 9. KNAPSACK  Figure 13. REVERSE DELETE \nFigure 14. RULE PRUNING Figure 16. Speedup results Figure 15. SEQ. ALIGNMENT E Web Crawler HTML Page \n<a href=http://x.com?p=v>...</a> -3 E Testing Engine -3 {{T n Extract Test Templates)} { T n )}Send \nTests  { T n )}Report Violations <a href=http://x.com?p=[test]>...</a> GET http://x.com?p=<script>... \nHTTP/1.1 Cross-site Scripting Figure 17. Architecture of the commercial engine, including illustration \nof its .ow   6.2 Case Study: Commercial Security Testing Tool For the second study, we considered a \ntesting module con\u00adtained in IBM AppScan Standard Edition, a commercial product for dynamic detection \nof web security vulnerabili\u00adties. We report on our experience in applying TA N G O sys\u00adtematically to \nheavy (sequential) loops in the code. The methodology and hardware con.guration for this experiment are \nthe same as in the previous one. Tool Description The module s architecture is outlined in Figure 17. \nThe input to the testing engine is an HTML page. Outgoing links are extracted, and injection points , \nsuch as parameter, cookie and session values, are identi.ed. Next, for each candidate injection point, \nthe engine checks for security vulnerabilities by sending further requests for the respective link, this \ntime substituting the original in\u00adput value with different test payloads (e.g. a script block, when checking \nfor a cross-site scripting vulnerability). The response received from the subject application is then \nvali\u00addated to con.rm whether the attack had succeeded, in which case the discovered vulnerability is \nreported to the user. Parallelization Process To parallelize the subject testing tool, we .rst reviewed \nthe tool s pro.ling data to identify heavy sequential loops. Manual inspection of the candi\u00addate loops \ndisclosed three candidates where TA N G O is im\u00admediately applicable. These loops perform greedy pruning, \nwhich renders the guard governing the observable effect of an iteration nondeterministic and monotonic. \nThe .rst loop (Re.ection) iterates over input points, map\u00adping each point to its respective re.ection \ncontexts, and eliminating redundant targets based on context overlap. The other two loops (Testing-{1,2}) \n.re test payloads, checking for each payload whether it is compatible with the tool s on\u00adline model of \nthe site s server-side defenses, and if so, send\u00ading it, as the AppScan documentation explains in detail. \nThe .rst loop builds a coarse model of the site s defenses, which the second loop re.nes if the .rst \nloop fails. Having settled on the parallelization targets, we next ap\u00adplied two transformations: First, \nwe decorated the loops with the Monotonic attribute, and second, we replaced ADT im\u00adplementations accessed \nwithin the loops with their counter\u00adparts from the TA N G O library (e.g. the TA N G O Set imple\u00admentation). \nThese transformations proved straightforward, requiring little time and few code changes. Performance \nResults To measure the impact of our paral\u00adlelization transformations, we compared between the orig\u00adinal \nversion of the tool, where the candidate loops are ex\u00adecuted sequentially, and a version where the loops \nare par\u00adallelized using TA N G O. The data inputs were sampled ran\u00addomly from the tool s test suite, \nand the experiments were conducted in the same way as before. The performance results are summarized \nin Table 4. This table shows the fraction of time TA N G O required to execute each of the loops compared \nto its sequential coun-Table 4. Speedup statistics for TA N G O on the commercial testing module Re.ection \nTesting-1 Testing-2 Overall Fraction Speedup 68% 1.47x 32% 3.12x 41% 2.43x 58% 1.73x  terpart. For example, \n32% for the Testing-1 loop means that if the average sequential execution time of the loop is t sec\u00adonds, \nthen executing this loop under TA N G O requires 0.32\u00b7t seconds, which constitutes a speedup of 3.12x. \nOverall run\u00adning time under TA N G O of the entire code including the code outside the subject loops, \nwhich was executed sequen\u00adtially also under TA N G O was cut down to 58% of the se\u00adquential running time, \nimproving performance by more than 40%. Discussion TA N G O proved effective for the Testing-1 and Testing-2 \nloops, yielding respective speedups of 3.12x and 2.43x. The main reason is that for most data inputs, \nparallel guard evaluation succeeds: The partial model due to early payloads suf.ces for parallel rejection \nof a large number of future payloads, which translates into noticeable perfor\u00admance improvement. In the \nRe.ection loop, however, there is less parallelism to exploit. The majority of discovered re\u00ad.ection \ncontexts are independent, obviating most attempts for (early) concurrent pruning of redundant attack \npoints. 7. Related Work In this section, we survey other parallelization approaches that have features \nin common with TA N G O. We refer the reader to [10] and references therein for a discussion of spec\u00ad \ni.cation, veri.cation and testing tools designed for nondeter\u00administic codes. Language Features NESL \n[6, 7] is a data-parallel pro\u00ad gramming language based on ML. Similarly to TA N G O, NESL enables concise \ncoding of a wide range of algorithms from different .elds, including sequences and strings, graphs and \ncomputational geometry. Pregel [40] is a distributed programming framework, providing a simple API for \npro\u00adgramming graph algorithms while managing the details of distribution invisibly, including messaging \nand fault toler\u00adance. Green-Marl [31] also specializes in graphs, focusing on graph analysis algorithms. \nThe Green-Marl language cap\u00adtures the high-level semantics of the algorithm, allowing the Green-Marl \ncompiler to apply nontrivial parallelization transformations. None of the above languages leverages nondeterminism \nfor parallelism. Another difference is that TA N G O is im\u00adplemented as a library for general-purpose \nlanguages (cur\u00adrently: Java and .NET). As illustrated in Figure 1, TA N G O s LINQ frontend exposes declarative, \nSQL-like syntactic con\u00adstructs for concise parallelization of challenging loops. This is close in spirit \nto Dryad [32] and DryadLINQ [33], which provide a general-purpose distributed execution engine for data-parallel \napplications. These systems, however, are con\u00adstrained to side-effect-free loops, where TA N G O is able \nto parallelize loops with irregular dependencies. Harris and Fraser [24] use conditional critical regions \n(CCRs) for concurrency control. The user guards an atomic region by a boolean condition, with calling \nthreads block\u00ading until the guard is satis.ed, which is reminiscent of the TA N G O .ow. The key difference \nis that [24] ensures the atomicity of a CCR via STM, whereas TA N G O applies pes\u00adsimistic synchronization, \nleveraging guard monotonicity. The retry construct in STM Haskell [25] allows a task to backtrack from \nits current behavior and select an alternate path that is hoped to have less con.ict. However, retry \ndoes not involve coordination between threads, so one is still at the mercy of nondeterministic luck \nand what has been done in the past. Communication and Visibility The TIC programming model [49] extends \nstandard STM by allowing threads to ob\u00ad serve the effects of other threads at selected points, thereby \nsupporting operations that wait or perform irreversible ac\u00adtions inside transactional code. Lesani and \nPalsberg [38] de\u00ad scribe a semantics that combines STM with message passing through tentative message \npassing, which keeps track of de\u00adpendencies between transactions to enable undo of message passing in \ncase a transaction aborts. Certain TM contention managers, such as the greedy contention manager [4, \n21], let a transaction wait for a con.icting transaction or abort it depending on their status and priority. \nGueta et al. [23] describe a methodology for utiliz\u00ading foresight information for effective synchronization \nin general-purpose systems. They apply their approach to safe composition of ADT operations (i.e. without \ndeadlocks and atomicity violations). Their implementation consists of static analysis of the client code \nto compute which ADT operations it may use at different points in the execution, combined with lazy runtime \nsynchronization over a smart implementation of the ADT, which leverages the semantics of the ADT s operations \nto increase parallelism. TA N G O also bases synchronization on foresight, but with the different fo\u00adcus \nof parallelism due to latent nondeterminism, where we introduce task specialization as a synchronization \nprimitive. TA N G O also bases synchronization on foresight, but with the different focus of parallelism \ndue to latent nondeterminism, where we introduce task specialization as a synchronization primitive. \nData.ow programming [3, 16] makes data dependencies between tasks explicit by representing a program \nas a di\u00adrected graph, where nodes represent units of work and arcs denote dependencies. Recently, there \nhave been attempts to integrate data.ow abstractions into STM to enable con\u00adtrolled access to mutable \nshared state while constraining the number of false con.icts [19, 48]. TA N G O takes a pes\u00adsimistic \napproach, while enforcing .ne-grained paralleliza\u00adtion of applications with irregular dependencies, such \nas graph algorithms. Also, the purpose of communication in TA N G O is to publish task intentions, rather \nthan propagate data values. Botincan et al. [9] present a technique for synchroniza\u00ad tion synthesis, \nwhich allows .ne-grained parallelization of resource usage by using separation logic, rather than points\u00adto \nanalysis. The parallelization transformation inserts grant and wait barriers to transfer resources between \nconcurrent tasks, which is reminiscent of the TA N G O communication mechanism. Our focus, however, is \non nondeterministic choice. TA N G O focuses a task on a particular way of evau\u00adlating a nondeterministic \naction, which permits .ne-grained synchronization, reducing contention, similarly to the idea of multiple \ngranularity locking [20, 43]. Beyond Data Independence TA N G O is able to enforce parallelism in the \npresence of data dependencies, as we il\u00adlustrated in Section 3 for RE V E R S E DE L E T E. Another way \nof transcending data dependencies is value speculation [46], where a task is run speculatively based \non a guess of the data values that would .ow into its input parameters at its desig\u00adnated point of execution. \nThe Alter framework [53] identi.es and enforces optimistic parallelism that violates certain de\u00adpendencies \nwhile preserving overall program functionality. This is governed by user annotations allowing reordering \nof loop iterations or stale reads. The novel feature of TA N G O in being able to exploit available parallelism \nin the presence of data dependencies is its reliance on the notion of specializa\u00adtion. The Bloom language \n[1] leverages theoretical results in monotonic logic to achieve eventual consistency in dis\u00adtributed \nprograms. Bloom programs describe relationships between distributed data collections that are updated \nmono\u00adtonically (i.e. in an incremental fashion). Conway et al. [14] generalize Bloom to a lattice-parameterized \nsystem, BloomL . TA N G O is similar to Bloom in leveraging monotonicity, but has wider applicability, \nsupporting general-purpose lan\u00adguages and arbitrary state manipulations, beyond incremen\u00adtal updates \n(as in RE V E R S E DE L E T E). 8. Conclusion and Future Work This paper takes a .rst step in exploiting \nparallelism due to nondeterminism. Existing approaches lack the necessary coordination mechanisms to \nguide concurrent tasks toward noncon.icting nondeterministic choices. We develop a novel concurrency \nparadigm that reduces con.ict by letting non\u00addeterministic operations specialize based on the potential \nfuture behavior of concurrent tasks. We have implemented our approach in TA N G O, a synchronization \nprotocol that ex\u00adtracts parallelism from pure monotonic queries with avail\u00adable nondeterminism. TA N \nG O generalizes several existing parallelization schemes, and achieves encouraging perfor\u00admance results \nin our experiments. In the future, we would like to introduce speculative capa\u00adbilities into TA N G \nO. The idea is to let tasks make nondeter\u00administic choices heuristically and optimistically, where bad \nchoices, resulting in con.ict with other (higher-importance) tasks, are rolled back. (Inverse operations \nare starightfor\u00adward to implement in the relational setting.) Speculation has attached costs (mainly \nlogging and wasted work), but at the same time, it carries several important advantages: First, it obviates \nthe need to approximate and monitor task inten\u00adtions, which could potentially lower the runtime overhead \nof TA N G O; and second, speculation is more permissive than our current model, which could expand the \nscope of TA N G O beyond monotonic queries and improve its performance on irregular benchmarks. As for \nTA N G O s current synchronization model, we in\u00adtend to enhance it to automatically infer monotonicity \nan\u00adnotations, which is facilitated by the relational representa\u00adtion. We are also currently investigating \nhow to perform partial specialization already at compile time by synthesiz\u00ading mutual-exclusion-based \nsynchronization for con.icting pairs of executions. References [1] The bloom language. http://www.bloom-lang.net/. \n[2] S. G. Akl and G. T. Toussaint. A fast convex hull algorithm. Inf. Process. Lett., 7:219 222, 1978. \n[3] Arvind and D. E. Culler. Annual review of computer science vol. 1, 1986. chapter Data.ow architectures, \npages 225 253. Annual Reviews Inc., 1986. [4] H. Attiya, L. Epstein, H. Shachnai, and T. Tamir. Transac\u00adtional \ncontention management as a non-clairvoyant schedul\u00ading problem. In PODC, pages 308 315, 2006. [5] H. \nAttiya and A. Milani. Transactional scheduling for read\u00addominated workloads. In OPODIS, pages 3 17, 2009. \n[6] G. E. Belloch. Nesl: A nested data-parallel language. regular tech report CMU-CS-92-103, Carnegie \nMellon, 1995. [7] G. E. Blelloch. Programming parallel algorithms. Commun. ACM, 39:85 97, 1996. [8] G. \nE. Blelloch, J. T. Fineman, and J. Shun. Greedy sequential maximal independent set and matching are parallel \non aver\u00adage. In SPAA, pages 308 317, 2012. [9] M. Botincan, M. Dodds, and S. Jagannathan. Resource\u00adsensitive \nsynchronization inference by abduction. In POPL, pages 309 322, 2012. [10] J. Burnim, T. Elmas, G. Necula, \nand K. Sen. Ndseq: runtime checking for nondeterministic sequential speci.cations of par\u00adallel correctness. \nIn PLDI, pages 401 414, 2011. [11] J. Burnim, T. Elmas, G. C. Necula, and K. Sen. Ndetermin: inferring \nnondeterministic sequential speci.cations for paral\u00adlelism correctness. In PPOPP, pages 329 330, 2012. \n[12] S. Cherem, T. Chilimbi, and S. Gulwani. Inferring locks for atomic sections. In PLDI, pages 304 \n315, 2008. [13] L. Cinque and C. diMaggio. A bsp realization of jarvis algorithm. In ICIAP, pages 247 \n, 1999. [14] N. Conway, W. Marczak, P. Alvaro, J. M. Hellerstein, and D. Maier. Logic and lattices for \ndistributed programming. Technical Report UCB/EECS-2012-167, EECS Department, University of California, \nBerkeley, http://db.cs.berkeley. edu/papers/UCB-lattice-tr.pdf, june 2012. [15] T. H. Cormen, C. E. \nLeiserson, R. L. Rivest, and C. Stein. Introduction to Algorithms, Third Edition. MIT Press, 2009. [16] \nJ. B. Dennis and D. P. Misunas. A preliminary architecture for a basic data-.ow processor. In ISCA, pages \n126 132, 1975. [17] E. W. Dijkstra. Guarded commands, nondeterminacy and formal derivation of programs. \nCommun. ACM, 18:453 457, 1975. [18] R. W. Floyd. Nondeterministic algorithms. J. ACM, 14:636 644, 1967. \n[19] V. Gajinov, M. Milovanovic, O. Unsal, A. Cristal, E. Ayguade, and M. Valero. Integrating data.ow \nabstractions into transac\u00adtional memory. First Workshop on Systems for Future Multi-Core Architectures, \n2011. [20] J. N. Gray, R. A. Lorie, G. R. Putzolu, and I. L. Traiger. Readings in database systems. chapter \nGranularity of locks and degrees of consistency in a shared data base, pages 94 121. Morgan Kaufmann \nPublishers Inc., 1988. [21] R. Guerraoui, M. Herlihy, and B. Pochon. Toward a theory of transactional \ncontention managers. In PODC, pages 258 264, 2005. [22] R. Guerraoui, M. Kapalka, and J. Vitek. Stmbench7: \na bench\u00admark for software transactional memory. In EuroSys, pages 315 324, 2007. [23] G. G. Gueta, G. \nRamalingam, M. Sagiv, and E. Yahav. Con\u00adcurrent libraries with foresight. In PLDI, pages 263 274, 2013. \n[24] T. Harris and K. Fraser. Language support for lightweight transactions. In OOPSLA, pages 388 402, \n2003. [25] T. Harris, S. Marlow, S. Peyton-Jones, and M. Herlihy. Com\u00adposable memory transactions. In \nPPOPP, pages 48 60, 2005. [26] P. Hawkins, A. Aiken, K. Fisher, M. Rinard, and M. Sagiv. Concurrent data \nrepresentation synthesis. In PLDI, pages 417 428, 2012. [27] P. Hawkins, A. Aiken, K. Fisher, M. C. Rinard, \nand M. Sagiv. Data representation synthesis. In PLDI, pages 38 49, 2011. [28] D. Hendler, I. Incze, N. \nShavit, and M. Tzafrir. Flat combining and the synchronization-parallelism tradeoff. In SPAA, pages 355 \n364, 2010. [29] M. Herlihy and E. Koskinen. Transactional boosting: a methodology for highly-concurrent \ntransactional objects. In PPoPP. ACM, 2008. [30] M. Herlihy and E. B. Moss. Transactional memory: Architec\u00adtural \nsupport for lock-free data structures. In ISCA, 1993. [31] S. Hong, H. Cha., E. Sedlar, and K. Olukotun. \nGreen-marl: a dsl for easy and ef.cient graph analysis. In ASPLOS, pages 349 362, 2012. [32] M. Isard, \nM. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: distributed data-parallel programs from sequential \nbuilding blocks. In EuroSys, pages 59 72, 2007. [33] M. Isard and Y. Yu. Distributed data-parallel computing \nusing a high-level programming language. In COMAD, pages 987 994, 2009. [34] A. Katsigiannis, N. Anastopoulos, \nK. Nikas, and N. Koziris. An approach to parallelize kruskals algorithm using helper threads. Technical \nreport, National Technical University of Athens, 2012. [35] E. Koskinen, M. J. Parkinson, and M. Herlihy. \nCoarse-grained transactions. In POPL, pages 19 30, 2010. [36] M. Kulkarni, D. Nguyen, D. Prountzos, X. \nSui, and K. Pingali. Exploiting the commutativity lattice. In PLDI, 2011. [37] M. Kulkarni, K. Pingali, \nB. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic parallelism requires ab\u00adstractions. \nIn PLDI, 2007. [38] M. Lesani and J. Palsberg. Communicating memory transac\u00adtions. In PPOPP, pages 157 \n168, 2011. [39] J. S. Foster M. Hicks and P. Prattikakis. Lock inference for atomic sections. TRANSACT, \n2006. [40] G. Malewicz, M. H. Austern, A. J.C. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski. \nPregel: a system for large-scale graph processing. In COMAD, pages 135 146, 2010. [41] Y. Mansour. Pessimistic \ndecision tree pruning based on tree size. In ICML, pages 195 201, 1997. [42] B. McCloskey, F. Zhou, D. \nGay, and E. A. Brewer. Au\u00adtolocker: synchronization inference for atomic sections. In POPL, pages 346 \n358, 2006. [43] C. Mohan, D. Haderle, B. Lindsay, H. Pirahesh, and P. Schwarz. Aries: a transaction recovery \nmethod support\u00ading .ne-granularity locking and partial rollbacks using write\u00adahead logging. ACM Trans. \nDatabase Syst., 17:94 162, 1992. [44] Y. Ni, V. S. Menon, A. Adl-Tabatabai, A. L. Hosking, R. L. Hudson, \nE. B. Moss, B. Saha, and T. Shpeisman. Open nesting in software transactional memory. In PPOPP, pages \n68 78, 2007. [45] K. Nikas, N. Anastopoulos, G. I. Goumas, and N. Koziris. Employing transactional memory \nand helper threads to speedup dijkstra s algorithm. In ICPP, pages 388 395, 2009. [46] P. Prabhu, G. \nRamalingam, and K. Vaswani. Safe pro\u00adgrammable speculative parallelism. In PLDI, 2010. [47] S. Russell \nand P. Norvig. Arti.cial Intelligence: A Modern Approach. Prentice Hall, 2003. [48] C. Seaton, D. Goodman, \nM. Lujan, and I. Watson. Applying data.ow and transactions to lee routing. In MULTIPROG, 2012. [49] Y. \nSmaragdakis, A. Kay, R. Behrends, and M. Young. Trans\u00adactions with isolation and cooperation. In OOPSLA, \npages 191 210, 2007. [50] A. Stivala, P. J. Stuckey, M. Garcia de la Banda, M. Hermenegildo, and A. Wirth. \nLock-free parallel dy\u00adnamic programming. J. Parallel Distrib. Comput., 70:839 848, 2010. [51] O. Tripp, \nR. Manevich, J. Field, and M. Sagiv. Janus: exploit\u00ading parallelism via hindsight. In PLDI, pages 145 \n156, 2012. [52] O. Tripp, G. Yorsh, J. Field, and M. Sagiv. Hawkeye: effec\u00adtive discovery of data.ow \nimpediments to parallelization. In OOPSLA, pages 207 224, 2011. [53] A. Udupa, K. Rajan, and W. Thies. \nAlter: exploiting breakable dependences for parallelization. In PLDI, pages 480 491, 2011. [54] V. N. \nVapnik. The nature of statistical learning theory. Springer-Verlag New York, Inc., 1995.   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Nondeterminism is a useful and prevalent concept in the design and implementation of software systems. An important property of nondeterminism is its latent parallelism: A nondeterministic action can evaluate to multiple behaviors. If at least one of these behaviors does not conflict with concurrent tasks, then there is an admissible execution of the action in parallel with these tasks. Unfortunately, existing implementations of the atomic paradigm - optimistic as well as pessimistic - are unable to fully exhaust the parallelism potential of nondeterministic actions, lacking the means to guide concurrent tasks toward nondeterministic choices that minimize interference.</p> <p>This paper investigates the problem of utilizing parallelism due to nondeterminism. We observe that nondeterminism occurs in many real-world codes. We motivate the need for devising coordination mechanisms that can utilize available nondeterminism. We have developed a system featuring such mechanisms, which leverages nondeterminism in a wide class of query operations, allowing a task to look into the future of concurrent tasks that mutate the shared state during query evaluation and reduce conflict accordingly. We evaluate our system on a suite of 12 algorithmic benchmarks of wide applicability, as well as an industrial application. The results are encouraging.</p>", "authors": [{"name": "Omer Tripp", "author_profile_id": "81435610768", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P4290418", "email_address": "omertrip@post.tau.ac.il", "orcid_id": ""}, {"name": "Eric Koskinen", "author_profile_id": "81350575010", "affiliation": "New York University, New York, NY, USA", "person_id": "P4290419", "email_address": "ejk@cims.nyu.edu", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81100150928", "affiliation": "Tel Aviv University, Tel Aviv, Israel", "person_id": "P4290420", "email_address": "msagiv@post.tau.ac.il", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509533", "year": "2013", "article_id": "2509533", "conference": "OOPSLA", "title": "Turning nondeterminism into parallelism", "url": "http://dl.acm.org/citation.cfm?id=2509533"}