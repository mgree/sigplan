{"article_publication_date": "10-29-2013", "fulltext": "\n Combining Concern Input with Program Analysis for Bloat Detection Suparna Bhattacharya K. Gopinath \nMangala Gowri Nanda IBM Research Indian Institute of Science IBM Research bsuparna@in.ibm.com gopi@csa.iisc.ernet.in \nmgowri@in.ibm.com Abstract Framework based software tends to get bloated by accumu\u00adlating optional features \n(or concerns) just-in-case they are needed. The good news is that such feature bloat need not always \ncause runtime execution bloat. The bad news is that often enough, only a few statements from an optional \ncon\u00adcern may cause execution bloat that may result in as much as 50% runtime overhead. We present a novel \ntechnique to analyze the connection between optional concerns and the potential sources of ex\u00adecution \nbloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of \noptional concerns could lead to execution bloat and (2) which particu\u00adlar statements are the likely sources \nof bloat when those con\u00adcerns are not required. The technique combines coarse grain concern input from \nan external source with a .ne-grained static analysis. Our experimental evaluation highlights the effectiveness \nof such concern augmented program analysis in execution bloat assessment of ten programs. Categories \nand Subject Descriptors F.2.3 [Logic and Meaning of Programs]: Semantics of Programming Languages Program \nAnalysis; D.2.5 [Software Engineering]: Testing and Debugging Debugging aids Keywords software bloat, \nprogram concerns, feature ori\u00adented programming 1. Introduction The abundance of redeployable components \nand frameworks has greatly eased software development, by making it possi\u00adble to rapidly construct software \napplications with extensive capabilities. An unintended and almost inevitable side effect Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. Copyrights for components of this work owned \nby others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Request \npermissions from permissions@acm.org. OOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright \nc &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509522 \n of framework based software, however, is the buildup of ex\u00adcess or largely optional features due to \njust-in-case program\u00adming at each framework layer through the reuse of over\u00adgeneral components. This \ncan result in runtime execution bloat, de.ned as the runtime resource overhead induced by the presence \nof excess function and objects, e.g. excessive conditions checks, costly data structures with low utility \n[50] and heavily nested data transformations [32, 33]. For example, in their study of information .ow \npatterns in a stock brokerage framework [32], Mitchell et. al found that as many as 58 transformations \nwere executed just for converting a date .eld in a SOAP message to a Java busi\u00adness object. Of these, \n18 transformations resulted from dec\u00adimal number speci.c processing being needlessly applied to whole \nnumber sub-.elds -a runtime overhead that was in\u00adcurred repeatedly for every request processed. This \nis an ex\u00adample of execution bloat arising due to the use of a (gen\u00aderal) component (DecimalFormat parsing \nclass) which has a program concern1 (i.e. decimal handling) that is in excess in this case where it is \nused to extract .elds which are always whole numbers. Such a conceptual analysis of runtime execution \nbloat has eluded automation till date. Existing approaches for Java bloat analysis lack high level information \nabout the func\u00adtional intent of code, insight that is required in order to iden\u00adtify the excess functionality. \n1.1 The Problem Does the presence of an excess concern (due to feature bloat in an underlying component) \nalways cause execution bloat? If not, what are the potential sources of execution bloat in the concern? \nConsider the example shown in Figure 12 . The code shows a simple Buffer class which provides a logged \nrestorable buffer, an example from [29]. The logit() method is included in order to support the concern \nlog, which logs the buffer s state after each operation. When the class is 1 Concerns are features, design \nidioms or other conceptual considerations that can impact the implementation of a program [41]. 2 The \ntemporary variables, tmp3 in get(), tmp in set() and tmp2 in restore() are generated by our bytecode \ngenerator.  1:public class Buffer { 2: int buf = 0; 3: int back = 0; 4: public Buffer(int x) { 5: this.buf \n= x; 6: } 7: void logit() { 8: System.out.println( LOG: buf = + buf); 9: System.out.println( LOG: back \n= + back); 10: } 11: int get() { 12: int tmp3 = this.buf; 13: logit(); . interaction(LOG, BUFFER) 14: \nreturn tmp3; 15: } 16: void set(int x) { 17: int tmp = this.buf; . interaction(BUFFER, RESTORE) 18: back \n= tmp; . interaction(BUFFER, RESTORE) 19: this.buf = x; 20: logit(); . interaction(LOG, BUFFER) 21: } \n22: void restore() { 23: int tmp2 = this.back; . interaction(BUFFER, RESTORE) 24: logit(); . interaction(LOG, \nRESTORE) 25: this.buf = tmp2; . interaction(BUFFER, RESTORE) 26: } Figure 1. Example: Logged Restorable \nBuffer from [24, 29]. LOG and RESTORE are optional features. The statements needed to make a given combination \nof features X and Y work correctly together are said to constitute the resolution of their feature inter\u00adaction \n[29], marked above as interaction(X, Y). reused in a scenario where the buffer need not be logged, log \nis an excess concern but the logit() method is called nevertheless, incurring a heavy runtime overhead. \nFurther, the code for the function void restore() and all code that applies to the variable back are \nstatements added to support the restore concern which allows the contents of a buffer to be restored \nto its previous value when required. As the Buffer class can be reused in scenarios where the buffer \nneed not be restorable, support for restore may be an optional concern. Under such usage scenarios, Buffer \nis an overgeneral component (class) and restore is an excess concern. However, we notice that the restore() \nmethod by itself does not induce a runtime execution overhead in this situ\u00adation as it would not be called. \nThe get() functionality of the buffer is also unaffected by the excess concern restore. On the other \nhand, the set() method does incur the runtime overhead of saving the previous value. The statements tmp \n= this.buf; back = tmp; in this method, therefore, are a potential source of execution bloat in this \nusage scenario. Secondly, the extra memory allocation and initialization of the back .eld, i.e. the statement \nint back = 0; is another potential source of bloat due to the optional restore concern. In order to automatically \ndetect such statements that are sources of execution bloat due to excess concerns, the chal\u00adlenging issues \nthat need to be satisfactorily addressed are: 1. Is it feasible to enrich program analysis approaches \nfor bloat detection with just enough high level intent infor\u00admation needed for deriving such insights \n? 2. How do we obtain this information in practice ? We observe that the current state of the art in \nsoftware en\u00adgineering research already provides a rich variety of tech\u00adniques for identifying, locating, \nanalyzing, annotating and separating software concerns and features. Feature oriented software development \nmodels [2, 38], software product line engineering [37] and aspect oriented programming [23] provide disciplined \nmechanisms to con\u00adstruct software with explicitly built-in concern assignment information3. A specialized \nvariant can be generated for any combination of features allowed by the feature model rules of the program. \nFor such software, we can identify bloat by using the concern assignment information and the rules that \nspecify optional features, provided the granularity of fea\u00adtures is de.ned at a suf.ciently .ne and detailed \nlevel. As a majority of software components are not created in this fashion, there has been substantial \nresearch on semi\u00adautomatic techniques to aid concern identi.cation and anal\u00adysis [41], addressing closely \nrelated problems such as fea\u00adture location [14, 42], aspect mining [5, 22, 30, 44], code search, concept \nassignment [7, 9] and change impact analy\u00adsis [4, 17]. For instance, a concern location scheme may fol\u00adlow \na query based approach (e.g a directed search from given seed or pattern [42]) while an aspect mining \nalgorithm may employ a generative approach [30] (e.g. discover structural patterns indicative of cross-cutting \nconcerns). Both types of analysis can be static [18] and/or dynamic [12] and ex\u00adploit additional artifacts \nbeyond program source and exe\u00adcutable traces, often combining traditional program analy\u00adsis with information \nretrieval natural language processing, formal concept analysis and some form of human input [16, 39, \n44, 52, 53]. However, in practice, the nature of concern information that is easily recoverable may be \nincomplete or too coarse grained for bloat detection, although possibly good enough for program understanding \nand maintenance. Thus, even when some level of concern information is available, it is not clear how \nexactly such extracted concerns should be systematically used to automate the analysis of bloat. 1.2 \nPreview of the Solution The basic intuition behind our analysis is as follows: some statements corresponding \nto each feature can represent struc\u00adturally intertwined code between multiple features. Such a statement \nis a candidate source that contributes to potential execution bloat when it corresponds to an optional \nfeature, but occurs in a method that belongs to an essential feature. In order to identify these statements, \nwe perform the following steps: 3 Concern or feature assignments associate concerns and their properties, \ni.e. concern intent, with the source code components, methods or even statements where they are implemented, \ni.e. concern extent.  Reverse engineer each potential feature extension based on program dependence \nassuming that every method could be a potential feature (computing microslices).  Build a graph that \nconnects the uses and the de.nitions in the microslices (computing the Microslice Interaction Graph (MSIG)). \n Enrich the graph with concern information (creating a  Concern Augmented Microslice Interaction Graph) \nApply concern analysis information to group meth\u00adods in terms of actual concern properties (optional \nor mandatory). Overlay the MSIG with information about the option\u00adality or the mandatoriness of each \nfeature to generate a Concern Augmented MSIG (the C AM SI G ). Traverse the C AM SI G , applying heuristic \nrules in order to determine which feature extensions (microslices) are in excess and hence potential \nsources of bloat. Computing microslices Starting with every input .eld variable in a method, we compute \na forward intra-procedural slice and starting with every output .eld variable we com\u00adpute a backward \nintra-procedural slice. These slices are then partitioned into microslices such that the input and output \ncombination for each slice (or rather, \u00b5slice) is unique. EX A M P L E 1. In Fig 1, the restore() method \ncontains the mi\u00adcroslices: (restore(), in = {back}, out = {buf}, \u00b5slice = {23, 25}) (restore(), in = \n{logit()}, out = {}, \u00b5slice = {24}) under inputs, fin = {back, logit()}, outputs, fout = {buf} Similarly, \nin Fig 1, the set() method contains the microslices: (set(), in = {buf}, out = {back}, \u00b5slice ={17, 18}) \n(set(), in = {}, out = {buf}, \u00b5slice ={19}) (set(), in = {logit()}, out = {}, \u00b5slice ={20}) under inputs, \nfin = {buf, logit()}, outputs, fout = {back} Suppose line 19 in the program is replaced by this.buf = \ntmp + x;. Now line 17 affects two heap variables, buf and back, unlike line 18 which only affects back. \nThus lines 17 and 18 would no longer belong to the same microslice. Hence, the microslices in this case \nwould be: (set(), in = {buf}, out = {buf, back}, \u00b5slice = {17}) (set(), in = {buf}, out = {back}, \u00b5slice \n= {18}) (set(), in = {buf}, out = {buf}, \u00b5slice = {19}) (set(), in = {logit()}, out = {}, \u00b5slice = {20}) \nThus for each unique combination of input and output data, we compute a unique microslice, and each statement \nbelongs to exactly one microslice. Complete details of com\u00adputing microslices can be found in Section \n3.  Figure 3. Concern Augmented Microslice Interaction Graph cor\u00adresponding to Fig 2. The shaded nodes \nrepresent microslices in methods belonging to optional features (LOG: blue and RE-STORE: gray). Computing \nthe Microslice Interaction Graph Potential dependencies between features in different methods can oc\u00adcur \nwhen the code for a feature in one method uses a .eld and  the code for another feature in a second \nmethod de.nes (updates) the .eld.  This helps determine the related features. For each .eld or object \nthat is shared between methods, we .nd the methods where the .eld is used as an input and identify the \nmicroslices that are in a forward slice of that input criterion. Likewise, we .nd the methods where the \nsame .eld or object is an output and identify the microslices that are in a backward slice affecting \nthat output criterion. Now we create a directed edge from each microslice node in the second set to each \nmicroslice node in the .rst set. This creates a directed graph where a (target node) mi\u00adcroslice is a \ndescendant of the microslices it might require (e.g. parent nodes generate state that is stored in .elds \nwhich might be later be used by statements in the child nodes). A leaf node has no outgoing edges. We \nterm this the Microslice Interaction Graph (MSIG). EX A M P L E 2. In Fig 2, for the microslices represented \nby: (set(), in = {buf}, out = {back}, \u00b5slice ={17, 18}) (restore(), in = {back}, out = {buf}, \u00b5slice \n= {23, 25}) The \u00b5slice = {23, 25} has an incoming edge from the \u00b5slice = {17, 18} labeled back and an \noutgoing edge labeled buf . The complete MSIG for the example in Fig 1 can be found in Fig 2 and details \nof generating the MSIG can be found in Section 3. Computing the Concern Augmented Microslice Interac\u00adtion \nGraph Having computed the MSIG, we now need to augment this graph with Concern related information to \ngenerate what we term the Concern Augmented MSIG (C AM S I G ) graph. Let us assume that we are given \na concern-based abstrac\u00adtion that partitions the concerns of a component into two groups -mandatory (always \nrequired) and optional. The ex\u00adact mechanism used to realize this grouping may range from a purely manual \nassignment by an expert to automated clas\u00adsi.cation rules based on an analysis of representative client \nprograms. We also have information about which methods are introduced by these concerns, for example \nusing a con\u00adcern analysis tool. We perform a Concern Augmented program analysis us\u00ading this information \nand the statically computed MSIG to identify candidate excess statements that cause execution bloat when \nnone of the speci.ed optional concerns are re\u00adquired. We apply some heuristic rules. For example, a mi\u00adcroslice \nin the C AM SI G , that is mandatory, but is used solely by nodes along C AM SI G paths that eventually \ncon\u00adtain an optional node, is considered to be potential bloat. EX A M P L E 3. Consider the \u00b5slice = \n{17, 18} in Fig 3. If we go forward along all outgoing edges, then we always reach an optional node. \nThus, this microslice is marked as potential bloat, for the given optional concerns. The complete Concern \nAugmented MSIG for the example in Figure 1 can be found in Figure 3 and details of generating the Concern \nAugmented MSIG can be found in Section 4.  1.3 Contributions There are several potential applications \nwhere a system\u00adatic approach for integrating concern information could be useful, e.g. in enhancing opportunities \nfor code specializa\u00adtion, speculative optimization and performance summariza\u00adtion not just in product-lines \nbut also in non-product line software. In this paper, we develop and explore the use of concern information \nin assessing execution bloat propensity of optional concerns implemented by a component (where a component \ncould be a class, a package or a framework li\u00adbrary). The contributions of this paper are: An approach \nto utilize externally supplied information about program concerns and their properties, where avail\u00adable, \nby creating concern partitions derived from this in\u00adformation to enrich program analysis techniques for \ntra\u00additionally developed software.  A novel concern augmented static analysis of an optional concern \ns likelihood for execution bloat in a given soft\u00adware component.  Our experimental evaluation indicates \nthat our analysis  correctly detects sources of bloat with a precision varying from 69% to 100%. detects \nall sources of bloat due to a given set of con\u00adcerns with a recall varying from 54% to 100%. By and large \nthe precision and recall is close to 100%, but there are some pathological cases where these val\u00adues \ndrop. In addition, we also observed that removing op\u00adtional concern related bloat can result in more \nthan 50% improvement in performance even for the large bench\u00admarks. 2. The Concern Augmented Program \nAnalysis (CAPA) Framework 2.1 Working De.nitions Some working de.nitions of terms that we use: Concern: \nA concern is any consideration that can im\u00adpact the implementation of a software program. According to \n[41] it could represent anything a stakeholder may want to consider as a conceptual unit, including features, \nnon\u00adfunctional requirements, design idioms, and implementation mechanisms . We use the terms concern \nand feature interchangeably4 in this paper. The implementation of a concern in source code, i.e. its \nextent, may be spread across multiple underlying program units (scattering) and the same program unit \nmay contribute to multiple concerns (tangling). We introduce the term concern partitioning to describe \nthe task of partitioning the statements of a program into groups that implement the same concern or a \nset of concerns 4 as we adopt useful concepts from both concern location and feature ori\u00adented software \ndevelopment  Figure 4. Relationship between different types of bloat and their runtime manifestations \nrelated by their properties (i.e. by concern intent). There may be many ways to partition the program \ndepending on the concerns of interest. Concern Partition: A concern partition divides the state\u00adments \nof a program into groups which implement a set of concerns that either share a common property or are \nrelated in some other manner (by a partitioning rule). Optional / Excess Concern: A concern that is not \nrequired (i.e. is in excess) for the given usage scenario. Mandatory Concern: A concern that is essential \nfor the given usage scenario.  2.2 Execution Bloat in the Context of Software Concerns Runtime bloat \ncan manifest in intricate ways in large frame\u00adwork based applications. In Fig 4 we depict an example \nof the relationship between different forms in which bloat is manifested at runtime as observed at different \nlevels of ab\u00adstraction. We are interested in the sources of execution bloat in\u00adduced by the presence \nof unutilized (hence excess) optional concerns this type of bloat is a typical consequence of the high \nlevel of generality supported by framework based com\u00adponents. The analysis is interesting because the \npresence of an excess concern in itself may not necessarily cause a sig\u00adni.cant runtime overhead as different \nconcerns and different implementations may have a different predisposition for ex\u00adecution bloat. In Fig \n5 the code to canonicalize data from big endian to little endian (line 11-15, including the cost of parsing \nand formatting from string to integer and back, intermediate ob\u00adjects generated and checking isBigE) \nimplements a software concern that may be unnecessary in deployment situations when all systems or data \nsources involved are uniformly big endian. Note that if the method big2LittleEndian() is la\u00ad 1: public \nstring applyConvertors(String s, boolean isBigE) { 2-10: ... 11: if (isBigE) { 12: tmp1 = str2int(s); \n13: tmp2 = big2LittleEndian(tmp1); 14: result = int2str(tmp2); 15: } 16-30: ... 30: } Figure 5. Big Endian \nto Little Endian Transformation beled as an optional concern, we need our analysis to deduce that not \nonly line 13, but also lines 11, 12 and 14 are poten\u00adtial sources of bloat associated with this optional \nconcern. Although this is a highly simpli.ed example, the resource overheads of bloat could be substantial \nwhen a sequence of such data conversions occur repeatedly in a loop (e.g. iter\u00adatively over a collection \nof data elements in each input re\u00adquest in a server side application that receives a high vol\u00adume of \nrequest transactions). For instance, recall the exam\u00adple from [32] described in the introduction where \nMitchell et al. found that the use of a general purpose DecimalFormat class results in 3 needless transformations \nfor each sub-.eld of a date (month, day, year, hour, minute and second) [32]. This alone amounts to 18 \nexcess transformations to convert a single date .eld from a SOAP message to a Java business object for \nevery input transaction request. 2.2.1 When do excess concerns cause execution bloat? Execution bloat \ncan only occur when the excess concerns induce an extra execution cost (e.g. via extra statements or \ndata objects) during the usage of essential concerns. In practice, the code for excess concerns can get \ntangled with the code for essential features or interact with essential concerns via shared data structures. \nThis causes some excess code statements to be executed when the client program is run. The execution \ncost of these statements is the execution bloat incurred by the client program when it runs. Consider \nthe example in Fig 1. This code has three con\u00adcerns named BUFFER, LOG and RESTORE. Let us assume that \nin the initial concern partition, LOG includes statements in the logit() method, RESTORE includes statements \nin the restore() method and BUFFER includes statements in the remaining methods. A client program that \nuses the Buffer class just for the BUFFER feature; does not require the functionality provided by the \nRESTORE or LOG concerns. In this case, BUFFER is an essential concern, while RESTORE and LOG are optional \nconcerns. The statements tmp = this.buf; back = tmp; (lines 17, 18) also correspond to the RESTORE concern, \nbut are exe\u00adcuted by the client program as they are tangled with code for the BUFFER feature. Thus they \ncontribute to execu\u00adtion bloat. Similarly, the logit() method which is invoked through statements in \nthe get() and set() methods (lines 13 and 20) also contribute towards bloat. The problem of .nding statements \nthat might contribute to execution bloat thus involves .nding those statements belonging to excess concerns \n that are structurally intertwined with executable code statements for essential concerns  To identify \nsuch statements automatically, we observe for instance, that statements in a method corresponding to \nan essential concern may write data to some heap variables, which are later read only by methods corresponding \nto op\u00adtional concerns. EX A M P L E 4. Line 18 in the set() method belonging to the BUFFER concern updates \nthe variable back, which is read only by the RESTORE concern. Further, line 17 de.nes a value that is \nused solely by line 18 to update back. Therefore such statements are labeled as contributors to an optional \nconcern. One way to detect these statements is to compute the intra-procedural backward slices of such \noutput .elds (e.g back) in the method corresponding to an essential concern (e.g. set()). Likewise, we \nrequire intra-procedural forward slices when statements in a method assigned to an essential concern \nread (input) .elds whose values are written only by methods corresponding to optional concern. However, \nthe slices computed can contain statements that also affect other .elds, some of which might not be optional. \nHence we need to further partition the statements in each slice in terms of the other .elds they affect. \nWe develop a technique called microslicing to generalize this procedure for .ne\u00adgrained partitioning, \nas a basis for developing bloat detection heuristics. 2.3 CAPA overview Figure 6. Concern Augmented \nProgram Analysis Fig 6 shows an example of a concern augmented program analysis that combines static \nand dynamic program analysis with concern partition analysis. Here, each different type of analysis (static, \ndynamic, concern), can implement multiple partitioning functions and analysis functions. For example, \nthe microslicing static analysis described in the next section can be viewed as a partitioning function \nthat partitions the statements of a program into microslices and an analysis function that computes the \nstructural interactions associated with each part. The concern analysis step creates partitions in terms \nof method level concern assignment information and concern abstraction rules which group concerns based \non user input. Concern related information may be available from a feature model of the program if one \nexists. In the last block shown in the .gure the results of the different types of analyses are combined \nin a task speci.c manner. The analysis may be expressible as a datalog for\u00admula over analysis functions \nand partitioning predicates. In Section 4 we will illustrate some speci.c rules for bloat de\u00adtection. \nIn this paper, we focus on a concern augmented static analysis for analyzing whether a given implementation \nof optional concerns is prone to execution bloat. For this anal\u00adysis, only the static analysis and concern \nanalysis blocks in Fig 6 are relevant. 3. Microslicer We now describe how to build a novel static analysis \nbased representation of a program or component5 that is useful for bloat detection. We call this analysis \nmicroslicing as it breaks up each method into many .ne (micro) slices which repre\u00adsent the smallest incremental \nunits within the method that could possibly be assigned to different features or unique combinations \nof features. Each microslice can contain a non\u00adcontiguous set of statements. 3.1 Preliminary static analysis \nOur implementation is built as an extension of the same underlying static analysis infrastructure used \nin [6, 35]. Escape Analysis To locate data structure .elds that might be shared across methods (and could \ncause structural inter\u00adactions between the concerns assigned to those methods), we rely on escape analysis. \nAfter constructing the control-.ow graph of each method, our underlying tool [6, 35] performs .ow-and \ncontext-sensitive pointer analysis and escape anal\u00adysis. Recall that for each method, escape analysis \ncomputes the following: The formal-in set for a method M contains the set of formal parameters. The \nimplicit this parameter (in non\u00adstatic methods) is also a formal-in.  The formal-out set for a non-void \nmethod M contains a single parameter R, the designated return value. The formal-out set is empty for \na void method.  5 e.g a Java library package  The escape-in set for a method M contains direct and \nin\u00addirect .elds of the formal parameters of M that are used, before possibly being de.ned, in M. These \nrepresent the upwards-exposed uses in M.  The escape-out set for M contains direct or indirect .elds \nof the formal parameters of M and the return value of M that are de.ned in M.  The algorithm associates \nescape-in and formal-in sets with the E ntry node of the CFG, and escape-out and formal-out sets with \nthe E xit node of the CFG. Forward and Backward Slicing Our microslicer employs a combination of intra-procedural \nforward and backward slicing of the escape-in and escape-out parameters identi.ed for each method of \ninterest. Note that although the slices we compute are intra-procedural, the underlying pointer anal\u00adysis \nis inter-procedural and aliasing is accounted for when computing control and data dependencies. An intra-procedural \nforward slice F orwardSlice(M, Si) consists of all CFG nodes of M in the transitive closure of def-use \nsets starting from the node Si, i.e. all nodes that are eventually data dependent on Si (and hence affected \nby Si). An intra-procedural backward slice BackwardS lice(M , Si) consists of all CFG nodes of M in the \ntransitive closure of data and control dependencies of the node Si, i.e. all nodes that affect Si through \na direct or indirect data or control dependence.  3.2 Microslicing Our microslicing analysis is based \non the following: 1. Each method in the component is introduced6 by a poten\u00adtial feature. (The information \nabout which feature intro\u00adduced the method is not used in this static analysis stage but is part of separately \nsupplied concern information as described in Section 4). 2. Statements inside each method can represent \nstructurally intertwined code between multiple features (and consti\u00adtute a resolution of this feature \ninteraction, i.e. the ex\u00adtra code needed to make the features work correctly to\u00adgether)  Continuing \nwith our working example from Fig 1, BUFFER is actually a group of three method level features, INIT \n(init()7), SET (set()) and GET (get()). Observe that lines 17, 18 and 23, 25 represent the resolution \nof a feature inter\u00adaction between BUFFER and RESTORE, while lines 13, 20 represent the resolution of \nfeature interactions between LOG and BUFFER and line 24 between LOG and RESTORE as shown in Fig 1 by \nthe lines marked as interaction(). Here, interaction(X, Y) denotes the resolution of a feature inter\u00ad \n6 while we assume that multiple features could add code to a method, only one of these features can introduce \nthe method in the component; this is the feature which de.nes the method in the .rst place. 7 constructor \nincluding .eld initialization statements in the Buffer class action been two features X and Y, or statements \nconstituting extra code that is needed to make sure that features X and Y work correctly together [29]. \nWe use static analysis to perform an automatic decompo\u00adsition of a program according to this model based \npurely on structural dependencies (e.g. data and control dependence). To achieve this, we further assume \nthat each feature-speci.c statement either uses an additional input .eld or method or affects an additional \noutput .eld (where input .elds corre\u00adspond to escape-in and output .elds to escape-out or formal\u00adout \nnodes for a Java method). Potential interactions between feature-speci.c statements in different methods \ncan occur when a statement in one method uses a .eld and a statement in another method de.nes (updates) \nthe .eld. Therefore an automatic decomposition can be performed using a combination of intra-procedural \nforward and back\u00adward slicing to partition the statements of a method into can\u00addidate feature-speci.c \nstatements (or microslices, as we call them). The slicing criteria A forward slicing criterion is an \ninput variable or .eld directly used within the body of the method, and a backward slicing criteria is \nan output variable or .eld that is directly updated within the body of the method. In addition to input \nand output data, we also introduce a for\u00adward (input) criterion for each method call statement, and a \nbackward (output) slice criterion for the method output. EX A M P L E 5. The set() method in Fig 1 has \ntwo input (forward slicing) criteria, the .eld buf and the method call to logit(); here the .eld buf \nis also an output (backward slicing) criteria as it is updated by the method, as is the .eld back. The \ncase where a method call statement is a slicing cri\u00adterion requires special treatment. Our forward slice \nfor the callsite statement includes all statements in the caller that use the result of the method call \nEX A M P L E 6. Line 14 uses the result of the big2LittleEndian() call in Fig 5. In addition, we include \nin the slice, statements that solely contribute to setting up the method parameters with no other impact. \nEX A M P L E 7. Lines 11 and 12 in the big2LittleEndian exam\u00adple in Fig 5. The rationale is that all \nof these statements exist to correctly support the use of the feature provided by the method called and \nhence constitute a resolution of this feature interaction. Microslices Consider a method with two input \ncriteria and two output criteria. Let us label the intra-procedural forward slices of the input variables \nF1, F2 and the intra-procedural backward slices of the output variables B1, B2. Then, the statements \nmay be partitioned as follows: F1, F1F2, F1B1, F1B2, F2, F2B1, F2B2, B1, B1B2, B2, F1F2B1, F1F2B2, F1B1B2, \nF2B1B2, F1F2B1B2. Here, F1B2 represents the set of statements that are only in the slices for both F1 \nand B2 (and not included in either F2 or B1). The non-empty statement sets among these are mi\u00adcroslices \nand correspond to candidate feature-speci.c state\u00adments (or the resolution of potential feature interactions). \nDE FI N I T I O N 1 . Let fM and fM represent the set of in\u00ad in out put and output slicing criteria of \na method M respectively. M The set of microslices \u00b5of M consists of all tuples \u00b5 = (M, ff , fb, S ) where \nff . fin, fb . fout and S is a non\u00adempty set of statements contained in the microslice, where a statement \ns . S if and only if all the following conditions hold: s belongs to method M s . F orwardS lice(M, f) \n.f . ff s . B ackwardS lice(M, f) .f . fb s /. F orwardS lice(M, f) .f /. ff s /. B ackwardS lice(M, \nf) .f /. fb The above conditions ensure that the microslices do not overlap and that all the statements \nin a single microslice use and update the same set of heap variables and no others. This set of heap \nvariables is a subset of the in\u00adput and output criteria. If a method contains Ns state\u00adments with Nf \ninput criteria and Nb output criteria, at most min(Ns, 2Nf +Nb - 1) microslices could exist. This is \nbecause there are 2Nf +Nb - 1 such possible subsets, the microslices must be non-overlapping and each \nmicroslice should contain at least one statement. The actual number of microslices is typically lower \nas can be seen in Example 1.  3.3 MicroSlice Interaction Graph (MSIG) Next we proceed to .nd associated \n(interacting) statements in other methods for each of these candidate feature state\u00adments by building \na cross-method MicroSlice Interaction Graph (MSIG) as follows: For each .eld or object that is shared \nbetween methods, we .nd the methods where the .eld is used as an input and identify the microslices that \nare in a forward slice of that input criterion. Likewise, we .nd the methods where the same .eld or object \nis an output and identify the microslices that are in a backward slice affecting that output criterion. \nNow we create a directed edge from each microslice node in the second set to each microslice node in \nthe .rst set. This creates a directed graph where a (target node) mi\u00adcroslice is a descendant of the \nmicroslices it might require (e.g. parent nodes generate state that is stored in .elds which might later \nbe used by statements in the child nodes). A leaf node has no outgoing edges. Notice that this graph \ncap\u00adtures potential dependencies regardless of interprocedural calling context since we do not assume \ninformation during the static analysis phase about the client programs or con\u00adtexts in which the component \nis deployed. DE FI N I T I O N 2. A Microslice Interaction Graph (MSIG) of a component is a directed \ngraph where the nodes are mi\u00adcroslices and there is a directed edge from node \u00b5i = (Mi, ffi , fbi , Si) \nto node \u00b5j = (Mj , ffj , fbj , Sj) iff .fi . fbi , fj . ffj such that fi and fj refer to the same .eld, \nvariable or method name and Mi = Mj. Fig 2 represents the MSIG for the example code in Fig 1. Each node \nin this graph represents a microslice and the line numbers that it contains (we have omitted the microslice \npartition labels in this diagram). Each edge represents a potential cross-method interaction between \nmicroslices and is labeled with the dependence criterion (e.g. shared .elds like buf and back, or called \nmethods e.g. logit()). The destination node of a directed edge represents a node that may depend on the \nsource node, i.e. it has a MAY USE feature interaction. In some cases we can detect a MUST USE feature \ninteraction, e.g. if the destination node has a statement that invokes a method call with a single call \ntarget which is de.ned by the source node. Here the notion of use, whether MAY USE or MUST USE, refers \nto feature usage (not necessarily data .ow). For example, note that in the case of a method call, we \ntreat the caller as a user of the method de.nition (e.g. the microslice containing the call to logit() \nin line 20 uses the code feature de.ned by the microslices in line 8 and line 9). The MSIG enables many \ninteresting analyses when com\u00adbined with concern partition information for each node: Edges in this \ngraph represent candidate (structural) fea\u00adture interactions if they connect nodes which correspond to \ndifferent concern groups in a partition, while the mi\u00adcroslices at the corresponding vertices constitute \nthe res\u00adolution of the potential feature interaction. For example, the nodes containing line numbers \n13 and 20 each repre\u00adsent the resolution of the interaction between the features BUFFER and LOG.  Cycles \nin this graph are merged into a single node which constitutes a resolution of a potential feature interaction \nif the constituent nodes belong to different concerns. For example, node (17,18) and node (23,25) are \npart of a cy\u00adcle, which means that they must be present together and are needed only when both the BUFFER \nand RESTORE features are required.  An inclusion relationship between the partition label strings for \ntwo nodes (e.g. F1F2B1 and F1B1) may rep\u00adresent intra-method feature dependencies if the corre\u00adsponding \nmicroslices belong to different concerns.  The leaves of this graph represent microslices that do not \nhave any dependent nodes and hence likely to be easily separable if their constituent statements are \npart of an optional concern.  Although the MSIG is a powerful abstraction to sur\u00adface statements involved \nin inter-method interactions which could give rise to bloat, the microslicer alone does not pos\u00adsess \nthe concern-awareness needed to capture conceptu\u00adally related feature-speci.c statements. It may generate \nnu\u00admerous overly narrow candidate features and microslices be\u00adcause it does not have any information \nabout the true concern partitions of interest8. For this reason, microslicing cannot be used in isolation \nto detect potential execution bloat state\u00adments. We need to perform a concern augmented analysis by combining \nour MSIG representation with externally sup\u00adplied concern information to distinguish mandatory and op\u00adtional \nfeatures and the methods introduced by them. 4. Concern Augmented Static Analysis of Execution Bloat \nLet us assume that we are given a concern-based abstraction that partitions the concerns of a component \ninto two groups -mandatory (always required) and optional. We perform a CAPA analysis using this information \nand the statically computed MSIG to identify candidate excess statements that cause execution bloat when \nnone of the spec\u00adi.ed optional features are required. Estimating the runtime resource overhead induced \nby these potential bloat contribut\u00ading statements (dynamically or statically) provides an ap\u00adproximate \nquantitative measure of execution bloat. 4.1 Bloat detection rules While the microslicing analysis described \nearlier .nds can\u00addidate feature-speci.c statements and possible structural in\u00adteractions between them, \nit cannot conclusively determine which of these are actually resolutions of a feature interac\u00adtion or \npotential bloat contributors. One problem is that the dependence graph is built in a may use fashion, \nso not all edges represent real structural interactions. Also, as we note from the graph, an interaction \nwith an optional feature can involve dependencies in either direction, i.e. both uses and used by relationships \nare possible. For example, in Fig 3, the statements at lines (17,18) constitute a statement which is \nused by the optional RESTORE feature, while the call to logit() in line 13 is a statement that uses the \noptional LOG feature. With multiple may use edges in both directions all through the hierarchy (and the \npossibility of missing edges due to other kinds of dependencies), unresolved decisions accumulate rapidly. \nHence, we need a few heuristics or ad\u00additional information to constrain the space and obtain an ap\u00adproximate \nanalysis of potential bloat. Table 1 lists a few heuristics and the statements from our working example \nthat are detected as potential bloat contrib\u00adutors using each heuristic. To keep the analysis simple, \nif we only use heuristics 1 and 2 from the table, the rules can be expressed as the datalog formula shown \nin Figure 7: 8 as an aside, we note that it could also miss detecting feature-speci.c state\u00adments introduced \nfor intermediate transformations which do not involve ad\u00additional input or output .elds Used By Uses \n1 None Must use at least one optional node [e.g. (13), (20) in Fig 3] 2 Solely used by optional nodes \n[e.g. (3), (17, 18) in Fig 3] Any 3 Solely used by nodes along MSIG paths that eventually contain an \noptional node Any 4 Any Must use at least one optional node Table 1. Bloat detection heuristics in decreasing \nlikelihood of potential bloat contribution (computed only for MSIG nodes that are inside methods which \ncan be executed by the client program even when no optional feature is used explicitly). The .rst rule \nsimply states that a statement is marked as optional if it inside a method that is introduced by a con\u00adcern \nwhich is known to be optional. Concerns that are not optional are conservatively treated as mandatory. \nThus, ac\u00adcording to the second rule, the statements in methods intro\u00adduced by those concerns are marked \nas mandatory. The next rule encodes the heuristic that a statement can be reported as a potential contributor \nto bloat if it is mandatory (contained in a method that is needed) but MUST USE an optional con\u00adcern \nstatement (e.g. calls an optional method) and no state\u00adments from other methods depend on it. The last \nrule marks a statement as potential bloat if it is mandatory but is only required by optional concerns. \n 4.2 Implementation and Scalability As stated in Section 3.1, we exploit an underlying interpro\u00adcedural \nanalysis previously used in [6, 35] that maintains .ow and context sensitive data and control dependencies \nin\u00adcluding escape analysis and pointer analysis (built over the WALA9 framework). Although this preliminary \nanalysis is costly, the underlying implementation has already been care\u00adfully engineered to be reasonably \nscalable [35]. The escape analysis results are used to identify input and output .elds potentially shared \nacross methods (e.g. the escape-in and escape-out sets for each method). The microslices, themselves, \nhowever, are intraprocedu\u00adral and the analysis scales easily. Further, we contain the number of slicing \ncriteria for each method by .ltering out some variables, e.g. pass-through escaping .elds. The under\u00adlying \nanalysis maintains information about the .eld names or access paths. We currently use this for determining \nwhich .elds are shared between which methods; the approach in\u00adtroduces imprecision in some situations \nwhere the matching is not accurate. Computing the microslice interaction graph can be ex\u00adpensive when \nthere are a large number of shared .elds (or 9 http://wala.sourceforge.net  optional(s) : -inM ethod(s, \nm), inC oncern(m, c), isOptional(c) mandatory(s) : -inM ethod(s, m), inC oncern(m, c), isM andatory(c) \n ' ' potentialBloat(s) : -mandatory(s), usedBy(s, null), mustU se(s, s), optional(s) ' ' potentialBloat(s) \n: -mandatory(s), usedBy(s, s), optional(s) Figure 7. Rules expressed as a Datalog formula: Each rule \nin the formula states that the left hand side relation holds for the tuple inside the braces if all the \nrelations on the the right hand side hold methods). However, we were able to contain this with sim\u00adple \nengineering tactics, such as building only the portion of the graph relevant for identifying interactions \ncorresponding to the bloat detection heuristics. With these basic optimizations, the entire analysis \ninclud\u00ading the preliminary analysis step took less than 8 minutes for BerkeleyDB (at 95K bytecode instructions \nand 2382K byte\u00adcodes analyzed in total including library code).  4.3 Sources of Imprecision and Tradeoffs \nWe have used a conservative choice of heuristics to reduce the possibility of incorrectly marking a statement \nas poten\u00adtial bloat (mis-detected bloat) while tolerating some unde\u00adtected bloat10 . However, we found \nthat this is sometimes too conservative and required further engineering tradeoffs to improve effectiveness. \nIn addition to imprecision in our bloat detection logic (where we rely on heuristics), inherent imprecision \nin the underlying static analysis can lead to both misdetected or undetected bloat. For example, when \nmapping virtual call targets, we might miss a potential callee or overestimate callees -the .rst causes \nmissing edges, the second causes extra edges in the MSIG, both of which affect accuracy. We did not attempt \nto study these in depth as there are alternative approaches to handle issues of precision in the underlying \nanalysis (e.g. with supporting dynamic analysis) that are independent of the logic for bloat detection/concern \naugmentation.  4.4 Using the results of CAPA We take a view similar to most existing techniques for \nbloat analysis which are intended as an aid for humans, in that we will focus on highlighting a list \nof candidate bloat statements and do not attempt to de-bloat the program. Even when the analysis output \nis precise, bloat repair is best left to the programmer as it could be more involved than merely dis\u00adabling \nbloat contributing statements or creating specialized versions of functions for the conditions where \nthe concern is optional. Automated de-bloating may be possible in some 10 We avoid using the terms false \npositive and false negative as it does not have the same connotation here as in the context of program \nveri.cation where a sound analysis is associated with program safety properties. For example, unlike \na bug, the presence of bloat does not result in an incorrect program. special cases but we leave this \nas an area of exploration for future work. To aid manual de-bloating we can also use the MSIG to provide \nthe programmer additional detail about the state\u00adments (microslices) that may be affected if the potential \nbloat contributing statement is altered. Sometimes a given implementation of a concern that exhibits \na high tendency for bloat as determined by our CAPA analysis may even need to be re-designed to be less \nprone to bloat. 5. Evaluation Evaluation objectives and considerations: Bloat analysis tools [49, 50] \nthat highlight inef.ciency patterns are typi\u00adcally evaluated for usefulness in terms of the performance \nimprovement that could be achieved by .xing program code to address the candidate bloat patterns highlighted \nby the tools. In addition, our approach amounts to a directed what\u00adif bloat analysis for a given concern \n(or set of concerns) and hence must be evaluated in terms of how well it answers the question what-if \nthis concern were optional, then which statements would be bloat . That is, we evaluate the ability to \nnarrow down the few candidate statements that are po\u00adtential sources of bloat due to the concern, rather \nthan the volume of bloat uncovered. Note that our analysis relies on externally supplied concern information \nwhich is not avail\u00adable by default for standard Java benchmarks such as the DaCapo [10] suite. Three \npronged approach We adopt a three pronged ap\u00adproach to evaluate our technique: In the .rst step, we perform \na correctness check of our tool s output. As there are no existing benchmarks available for cross-validating \nresults of execution bloat analysis, we compile a set of well-understood control examples and case studies \ndrawn mostly from existing literature on feature ori\u00adented programming and library specialization. This \nenables us to create an oracle against which the results of our tool can be veri.ed. In the second step, \nwe evaluate the scalability of our anal\u00adysis by running it on larger examples with some manually veri.able \nform of concern bloat. For one example, we com\u00adpare our results against concern statements independently \nla\u00adbeled by another programmer. For some of the other large benchmark programs we specify what-if questions \nbased on concerns relevant for uncovering speci.c cases of bloat that have been reported by previous \nresearchers or develop\u00aders. In the third and .nal step, we remove the bloat statements and evaluate \nthe gain in performance. Benchmark programs Our set of chosen case studies in\u00adcludes ten Java programs \nof different sizes. Five small to medium sized components with precisely\u00adknown .ne-grained feature assignments \nand optional concerns that cause execution bloat, where we ensure a line by line validation of results \nagainst a pre-determined oracle. Although these examples are not large, they are carefully chosen to \ntest the ability to detect bloat due to various .ne-grained structural interactions. These are 1. Buffer \nis the 3 feature BUFFER-LOG-RESTORE ex\u00adample (Fig 1) from prior work on feature oriented pro\u00adgramming \n[29] described earlier. 2. Adaptor is an adaptor pattern that includes an op\u00adtional big endian to little \nendian converter, illustrated in Fig 5, similar to a feature previously described by Prehofer [38]. We \ncreated this example because many anecdotes of bloat highlight excess or unnecessary transformations \nand checks, and we wanted to deter\u00admine if our technique could detect such patterns. 3. Stack is an \nenhanced implementation of the classic FOP example proposed by Prehofer [38], a 5 fea\u00adture STACK with \nCOUNTER, LOCK, UNDO and BOUND checking. 4. c5list is a (simpli.ed) Java implementation of a LinkedList \nwith support for Views from the C5 collec\u00adtion library for C#. This example has been the main subject \nof a study on different library specialization techniques [1], so it was an interesting experiment to \nexplore whether our technique is capable of auto\u00admatically detecting bloat due to the optional feature \nViews in this example. 5. Graph is the well-known Graph Product Line imple\u00admentation available at the \nFEATUREHOUSE web\u00adsite [3].   Five large benchmark programs with some previously known optional concern \nassignment (via manual labeling or available from previous work) on bloat. 1. xml benchmark from SPECjvm2008 \nis a Java bench\u00admark from SPEC [46]. We speci.ed validation as an optional concern (it is relevant from \na bench\u00admark perspective but could be optional otherwise). We picked this concern as it is likely to \noccur at .ne grained levels in different places in the code. 2. SPECjbb2005 is a Server-side Java benchmark \nfrom SPEC [46]. From an inspection of the code during   prior research on object churn bloat, we have \nob\u00adserved bloat due to transformations to XML format for temporary in-memory storage. XML formatting \nis not strictly an essential feature here. 3. DaCapo BLOAT is a benchmark from the DaCapo 2006 benchmark \nsuite [10]. Other researchers have noted that the bloat benchmark incurs considerable object churn bloat \nin building strings just for the pur\u00adpose of assertions [49, 50] even when the assertion conditions do \nnot occur. 4. BerkeleyDB is a widely used lightweight embedded database toolkit. For our experiments \nwe used a full featured version of the Java Edition of BerkeleyDB included in a set of case study examples \nfor virtual separation of concerns (CIDE [19]). We applied our analysis to identify bloat contributing \nstatements due to the costly optional concern (trace) logging. 5. Xylem Xylem is the original tool that \nthis analysis has been built upon. Originally designed to be a tool to detect null dereferences [35] \nit has over the years grown arms and legs to handle various other analy\u00adses [6, 27, 34, 36]. So we ran \nour analysis to detect bloat generated by these additional concerns.  The general characteristics of \nthese benchmarks are given in Table 2. We do not rely on a speci.c concern location tool in our evaluation. \nOur analysis takes concern information as sup\u00adplied, without regard to how the information was obtained. \n The input to our analysis is a jar .le of the component or program and a list of method name search \npatterns corresponding to one or more optional concerns.  The output is a list of statements that contribute \nto po\u00adtential execution bloat due to the speci.ed optional con\u00adcerns.  5.1 Case study results: Table \n3 summarizes the results for the ten case studies. Ta\u00adble 4 summarizes the performance improvement results \nfor the case studies when run in bloated form and in unbloated form. We observe that our analysis is \neffective in correctly detecting bloat with a reasonable precision in all these exam\u00adples although it \ncan miss some candidate bloat statements. We de.ne precision and recall as bloat sources detected correctly \n P recision = bloat sources detected correctly + bloat sources misdetected bloat sources detected correctly \n Recall = bloat sources detected correctly + bloat sources undetected  Buffer Adaptor Stack c5List Graph \nSPECjvm 2008(xml) SPECjbb 2005 DaCapo BLOAT Berk\u00adleyDB Xylem Functions analysed 12 10 40 15 137 542 864 \n4293 3798 1631 Bytecode statements analyzed 63 53 339 388 1188 13035 33924 124588 91605 145909 Analysis \ntime 4.9s 5.3s 6.5s 5.3s 6s 1m 31s 28.2s 1m 59s 7m 55s 48.5s Table 2. Benchmark characteristics Buffer \nAdaptor Stack c5List Graph SPECjvm 2008(xml) SPECjbb 2005 DaCapo BLOAT Berk\u00adleyDB Xylem No. of microslices \ncomputed 16 8 97 58 281 1928 4769 21867 16695 9472 Optional concerns evaluated for bloat propensity ( \nWhat-if question) RESTORE LOG ENDIAN COUNTER, LOCK, UNDO, BOUND VIEWS WEIGHTED VALIDATE XML ASSERT TRACELOG \nJARIZE Sources of bloat due to evaluated concerns 5 4 20 9 11 33 20 316 - 62 Bloat sources detected correctly \n5 4 18 9 6 21 20 315 159 62 Bloat sources undetected 0 0 2 0 5 12 0 1 - 0 Bloat sources misdetected 0 \n0 0 0 0 1 0 0 37 28 Precision 100% 100% 100% 100% 100% 95% 100% 100% 81% 69% Recall 100% 100% 90% 100% \n54% 64% 100% 99% - 100% Table 3. Results of concern augumented program analysis of the execution bloat \npropensity of a speci.ed set of optional concerns in different case studies. The analysis narrows down \nsources of bloat to the few source code lines instrumental for avoiding execution bloat in situations \nwhere the given concerns are optional. Buffer Adaptor Stack c5List Graph SPECjvm 2008(xml) (startup) \nSPECjbb 2005 DaCapo BLOAT Berk\u00adleyDB Xylem Bloated runtime or throughput 27.4s 12.3s 49.8s 11.5s 2.6s \n28.0s 10813 bops 5.3s 131.3s 82.5 (ant1.5) 148.1s (ant1.65) De-bloated runtime or throughput 0.1s 6.3s \n32.3s 7.6s 1.5s 21.3s 16769 bops 2.3s 14.6s 54.5s (ant1.5) 75.8s (ant1.65) % Improvement 99% 48% 35% \n34% 42% 24% 55% 56% 89% 34 49% Table 4. Benchmark performance results. Measurements were taken on an \nIntel(R) Core(TM) i7 L640, 2.13GHz system with 3.7GB memory, running Linux Fedora 13 and Java 1.6, OpenJDK \n64-bit Server VM (build 14.0-b16, mixed mode). We elaborate on the case studies11 in a little more detail \nbelow: I. Programs with precisely known .ne-grained feature assignments: The concern (feature) assignments \nin these programs are available at statement granularity, which we use to create an oracle12 to compare \nwhether our analysis 11 The .rst four case studies below can be made available for other re\u00adsearchers \non request. The rest of the case studies (other than Xylem) use externally available benchmarks. 12 The \noracle marks a statement belonging to an optional concern as poten\u00adtial bloat if it lies inside a method \nbelonging to a concern that is not known correctly detects bloat. Note that, our analysis is not aware \nof the .ne-grained feature assignments. We only provide it method level concern information for the (optional) \ncon\u00adcerns we evaluate for bloat propensity. Buffer: We specify 2 optional concerns, LOG (the logit() \nmethod) and RESTORE (the restore()) method. The anal\u00adysis correctly located the statements that are the \nsources of execution bloat (lines 3, 13, 17, 18 and 20). It could rec\u00adognize the assignment to back in \nthe set() method that occurs through a temporary variable (via intra-procedural backward slicing) and \nalso recognized the initialization of the .eld back. to be optional. On de-bloating the code based on \nthe analysis results, we observed a large ( 99%) improvement in performance in a test application using \nthis component. This is primarily because the LOG concern involves printing which incurs a huge overhead. \nAdaptor: The method big2LittleEndian was speci\u00ad.ed as the optional concern. Our analysis was able to \nde\u00adtect as bloat, the excess parsing (lines 12, 14) and condition check (line 11) that was needed only \nto enable this converter, besides the call to big2LittleEndian in line 13. On de-bloating the code based \non the analysis results, we observed a 48% improvement in performance and 99% reduction in object creation \nin a microbenchmark application using this component. Stack: We specify 4 optional features: COUNTER, \nLOCK, UNDO and BOUND checking. This example spans multiple classes and captures several possible nuances \nof the way features interact to cause bloat (Appendix A.1). As seen from Table 3, column 3, our technique \nis able to detect most of them. On de-bloating the code based on the analysis results, we observed a \n35% improvement in performance and 30% reduction in object creation in a test application using this \ncomponent. The statements that our analysis missed were calls to a save() method which belongs to the \nUNDO concern but was not listed in the concern information supplied. Even so, our technique was able \nto detect the main state saving statement savedState = new String(state); at line 77 in the save() method \n(Appendix A.1), as excess because it is in a microslice that is only used by the UNDO concern. This illustrates \nthat our analysis may be effective even with slightly incomplete concern information. c5list We specify \nthe optional feature Views in this ex\u00adample. As seen in Table 3, column 4, we found that the anal\u00adysis \ncould indeed detect all known instances of such state\u00adments. For example, it could identify statements \nof the form: if (underlying != null) underlying.counter++; [1] with\u00adout the information that underlying \nis only relevant for Views . On de-bloating the code based on the analysis results, we observed a 34% \nimprovement in performance in a test application using this component. Graph is the well-known Graph \nProduct Line implemen\u00adtation available at the FEATUREHOUSE web-site [3]. We started with a variant that \nsupports weighted directed graphs and used our analysis to .nd execution bloat due to the op\u00adtional weighted \nfeature. There were only 11 such sources, as per our oracle. Our method was able to correctly detect \n6 of them (Table 3, column 5). The remaining 5 sources were all related to a vector .eld weightsList \nwhich is initialized directly in the mandatory Vertex constructor rather than any method that is speci.c \nto the weighted feature. This could be detected if we were to expand concern input provided to include \n.elds matching a concern s keywords. Even though our technique missed these statements, on de-bloating \nthe code based on the analysis results, we ob\u00adserved a 42% improvement in performance and 88% reduc\u00adtion \nin object creation in a test application using this compo\u00adnent. II. Large programs with a manually veri.able \noptional concern: In these cases we had to rely on prior experience / knowledge to specify an optional \nconcern that contributes to bloat which we also know how to .nd manually so that it is practical to validate \nresults line by line. xml benchmark from SPECjvm2008 We speci.ed validation as an optional concern. In \nthis case, there is no pre-determined oracle. We had to rely on another pro\u00adgrammer to manually inspect \nthe code and independently annotate all the methods and lines in the code that belong to the validation \nconcern. As shown in Table 3, column 6, our analysis found 22 sources of bloat due to this concern. We \ncon.rmed that 12 of these coincided with the manual annotation. Of the remain\u00ading 9, one was found to \nbe an instance of mis-detected bloat, while 8 of the sources were found to be in utility methods (such \nas writeDiff(), isUnixNewLine()) which were used only by the validation methods (our tool reports the \nspeci.c microslice statements where the potential use occurs). Hence we treat these 8 sources as correctly \nidenti.ed. In addition there were 12 lines of bloat as per the manual annotation which went undetected \nby our tool. The instance of mis-detected bloat occurred because of imprecision introduced by the underlying \nanalysis in map\u00adping virtual call targets (see Section 4.3). We could deter\u00admine this based on the detailed \noutput of the tool. Among the cases of undetected bloat, several seem to be related to .elds that are \nboth directly updated and used in non validation spe\u00adci.c methods such as setupBenchmark() and hence \ndif.\u00adcult for our analysis to distinguish. One way to cover these cases is to expand concern input provided \nto include .elds matching a concern s keywords in addition to supplying the concern s methods. De-bloating \nthe code based on the analysis output signi.\u00adcantly reduces expensive validation that occurs in the bench\u00admark \nsetup phase, resulting in an 24% improvement in the combined startup and execution time for a single \niteration benchmark run. SPECjbb2005: As XML formatting is not strictly an essential feature here, we \nspeci.ed it as an optional concern. We were then able to use our analysis to .nd the statements that \ncontribute bloat due to this concern (Table 3, column 7). These included string copies that we know (from \nour prior work [6]) to be responsible for 40% temporary object churn. On de-bloating the code based on \nthe analysis results, we observed a 55% improvement in benchmark performance (for 4 warehouses). DaCapo \nBLOAT: We speci.ed assertions as an optional concern and were able to .nd potential bloat statements \n(Ta\u00adble 3, column 8). Even though assertions themselves are fairly straightforward to .nd using a code \nsearch, our ap\u00adproach has the advantage that it can also automatically catch statements that build up \ndata structures just for the purpose of debugging. According to prior work [49], addressing these overheads \ncan signi.cantly reduce data copies in this benchmark. On debloating the code based on our analysis results, \nwe ob\u00adserved a 56% improvement in benchmark performance and 87% reduction in temporary object creation. \nBerkeleyDB: As concern input to our tool, we speci.ed method names which contain the string trace or \nTrace , as optional. To measure the precision of our analysis, we compared the results with the corresponding \nannotation in\u00adformation in CIDE [19]. Our technique detected 159 in\u00adstances of bloat correctly, with \n81% precision overall. We did not measure recall because not all annotated lines that belong to the logging \nconcern contribute to bloat. In\u00adstead, we used CIDE to generate a specialized program vari\u00adant without \nthe concern and measured the total bytecodes executed13 by the variant under a simple insertion retrieval \nbenchmark demo application (Simple JE). We compared this with the total bytecodes executed by a version \nof the code that was manually debloated per our analysis and found it to be close agreement with that \nmeasured for the variant gener\u00adated by CIDE. The total bytecodes executed by the debloated code was 63% \nlower than total bytecodes executed by the original full featured code when running the same bench\u00admark. \nWe observed a signi.cant (89%) improvement in per\u00adformance and 85% reduction in object creation using \nthe code debloated per our analysis. Incidentally, BerkeleyDB already has an option to turn off logging \nwhich exhibits com\u00adparable performance gains. Xylem: One optional concern in Xylem is that after doing \nthe basic data and control dependence analysis, it stores the analysis results in a jar .le to permit \nfaster startup at a lower memory cost for subsequent analysis such as null dereference analysis [35]. \nHowever, for any analysis that is built directly on top of the basic analysis, generating the default \njar .le was becoming a fairly large overhead. To identify bloat due to this concern, we speci.ed as input, \nmethod names which contain the string jarize . While our tool detected every statement that was a source \nof this bloat, it also picked up all the statements related to another optional concern, modelDriver \n[36]. Thus we .nd that we may be unable to separate out bloat related to two cross-cutting concerns, \nunless we somehow mark the .elds for one of the concerns as mandatory. On de-bloating the code based \non the analysis results, we observed a 30 49% improvement in performance depending 13 we used the JP2 \nbytecode pro.ler [8] for these measurements on the size of the input and the size of the (optional) \njar that was generated. 5.2 Discussion 5.2.1 Approaches for Obtaining Concern Input Labels In our proof \nof concept experiments, we have used a very rudimentary keyword pattern match on fully quali.ed method \nnames as our concern input generation tool. There are many other possible sources of concern input that \ncould be used in\u00adstead. Multiple sources of information, techniques and their combinations have previously \nbeen used for aiding concern analysis including source code mining [5, 52], information retrieval, formal \nconcept analysis, slicing [9, 18], execu\u00adtion traces [12], human input, natural language processing, \nand concept databases. Concern inputs can range from ma\u00adchine discovered concerns (e.g. by using topic \nmodels on code [5, 13]) and sophisticated code searches to expert sup\u00adplied programmer annotations for \nconcern separation, such as feature oriented programming [2] or aspect oriented pro\u00adgramming (AOP) [23]. \nOur what-if analysis is applied to concerns that the user of the tool considers to be potentially optional. \nIn the case of machine discovered concerns from an unfamiliar code base, the user marks which of the \ndiscovered concerns to analyze for potential bloat. 5.2.2 Computing the MSIG First vs Adopting a Demand-Driven \nApproach We have treated the static analysis for the MSIG compu\u00adtation and the concern partitioning step \nas parallel stages. These stages can potentially be composed in any order or even blended iteratively \nif appropriate. Our current implementation overlays concern informa\u00adtion on the micro-partitioned program. \nPrecomputing the MSIG in this manner before the concern augmentation step has the advantage that the \nMSIG can be reused to answer multiple what-if questions (e.g. to identify sources of bloat for different \ngroups of optional concerns). It also makes it easier to handle more complex conditional optionality \nrules14 across feature combinations. Further, in situations where obtaining concern labels requires some \nmanual effort, the MSIG can be used to focus the effort only on methods involved in costly structural \ninteractions. Alternatively, the order may be reversed to support a more directed or demand-driven approach \nthat only computes the part of the MSIG relevant for answering a single what\u00adif question corresponding \nto a set of optional concerns. Such approach can also be useful for .ne grained concern location. 14 \nsuch as a high level feature model grammar that speci.es which groups of concerns are mutually optional \nor which other concerns must be enabled when a given concern is required  5.2.3 Restrictions of our \nProgram Dependence Based Interpretation of FOP There are subtle differences between the FOP (Feature \nOri\u00adented Programming) notion of the resolution of a feature in\u00adteraction (e.g. a feature derivative \n[29]) and the pure pro\u00adgram dependence based interpretation we use, but the prin\u00adciple is very similar. \nFor instance, we use a somewhat re\u00adstrictive version of what code a feature extension can add or what \na structural interaction can look like. So unlike FOP our model is limited to whatever could be statically \ndetected based on program dependence relationships. For example in FOP a feature can add any code to \na method, not necessarily one that uses/adds a new variable or calls a method. Also, in FOP even a method \naddition to a class introduced by another feature is termed a feature interaction, while we are focused \nonly on interactions via code introduced within a method. On the other hand, we also overestimate candidate \nfeature interactions and their resolution because of the .ne granu\u00adlarity of micro-slicing. 6. Related \nWork 6.1 Java Bloat Analysis Several approaches have been explored for detecting run\u00adtime bloat in Java \napplications [51]. Data structure health signatures [31] use the structural semantics of Java s data \nmodel to distinguish data structure representation overhead to measure memory bloat. Different notions \nof bloat have been considered in the absence of an explicit model for dis\u00adtinguishing overhead from necessary \ndata or activity. A va\u00adriety of symptoms of excesses have been used to recognize the presence of bloat, \nsuch as high volumes of temporary objects, unbalanced costs vs bene.t of object creation and consumption \n[6, 15, 48 50] and object reuse opportunities in loops [6]. However, bottom up techniques are limited \nby lack of higher level insight about the purpose of code statements responsible for the pattern of excess \nactivity suspected. Aug\u00admenting the analysis with information about program con\u00adcerns and their properties \n(such as when a concern is neces\u00adsary and when it isn t) enables us to tackle the hitherto un\u00adsolved \nproblem of analyzing execution bloat propensity of optional concerns. Existing runtime bloat detectors \ncan also bene.t from such higher level insight to aid de-bloating.  6.2 Concern Analysis There is a \nlarge body of existing literature on concern lo\u00adcation and discovery, most of which is oriented towards \npro\u00adgram comprehension, maintenance and re-engineering tasks. The space of related topics is too vast \nto cover, and traverses sub-.elds that would each merit their own comprehensive survey, such as feature \nlocation [14], aspect mining [22], concept assignment [7] and change impact analyis [4, 17]. Feature \nlocation [14] and impact analysis [17] are geared at locating the extent of a speci.c concern such as \na feature, bug or change request, e.g. source code fragments that either implement or affect the concern. \nAspect mining [22], on the other hand, looks for general indicators of cross-cutting con\u00adcerns to identify \ncandidate aspects, e.g. via a fan-in analysis of methods [30] or measures of topic scattering in source \ncode [5]. A variety of techniques have been employed across these problem domains, ranging from formal \nconcept anal\u00adysis [47], exploiting program topology [40, 41], natural lan\u00adguage processing [43, 44], \ninformation retrieval, graph min\u00ading [30, 52], program slicing [9, 18] and dynamic analy\u00adsis [12]. Solutions \nthat combine multiple approaches [16, 39, 42, 53] and exploit multiple sources of information includ\u00ading \ntest cases, program documentation and evolution history have also been used to improve the quality of \nresults. The existence of these techniques makes the concern aug\u00admentation step in our analysis feasible \nor usable in practice, by supplying method level concern assignment information that we need as input. \nTraditionally program analysis has been used to aid concern location [16, 18]. In contrast we propose \nthe use of externally supplied concern information to enrich program analysis for solving new problems. \nAccording to surveys of techniques for feature loca\u00adtion [14], aspect mining [22] and impact analysis \n[28], most approaches are geared to provide method level granularity of concern information, while support \nfor statement level gran\u00adularity is relatively rarer (even most efforts at creating fea\u00adture location \nbenchmarks have been geared at method level granularity). By requiring no more than method level granu\u00adlarity \nof concern information as input, we ensure that our ap\u00adproach is practical and widely applicable, with \na .ne grained analysis that can be tuned speci.cally for bloat detection. At the same time, we can easily \nreuse accurate .ne-grained statement level concern assignment information, if provided by a concern location \nor aspect mining tool, in which case we would skip our .ne-grained microslicing analysis for .nding feature \ninteraction resolutions. 6.3 Feature Oriented Software Development Our approach in this paper is inspired \nby research in feature oriented programming (FOP). In particular, we exploit the elegant concept of structural \nfeature interactions and their resolution (also referred to as feature derivatives) [2, 24, 29, 38]. \nOur novel contribution is to link this concept to the problem of detecting statements that are potential \nsources of execution bloat. We also devise techniques that enable the practical application of these \nideas to bloat assessment of non FOP software such as framework based components, given only coarse method \nlevel concern information. Recov\u00adering an exact feature oriented decomposition of a legacy program (from \nwhich specialized product line variants can be generated) remains hard to automate [20, 29]. Fortu\u00adnately, \nthe task of detecting candidate sources of bloat has less exacting requirements on precision in order \nto be effec\u00adtive and useful as we only aim to draw programmer attention to potential bloat contributors \nrather than refactor a correct bloat-free sub-program.  6.4 Feature-aware analysis and irrelevant features \nin product lines Feature-aware program analysis [11, 21, 26] has typically been applied in product lines \nand feature-oriented or aspect\u00adoriented software. Sophisticated tools have been developed to reason about \nthe static and dynamic properties of pro\u00adgram variants corresponding to different feature combina\u00adtions \nwithout having to check each and every possible com\u00adbination or product in the product line. For example \ntype\u00adchecking [21] and static analysis [11] can be applied to the entire product line as whole. Kim et \nal. explore test reduc\u00adtion in product lines [25, 26] by using static analysis to .nd features irrelevant \nfor a given test or safety property being monitored, based on whether the feature affects the control \nor data .ow of any feature whose code may be executed by the test. Our work complements these efforts \nin the product line and feature oriented software community by exploiting con\u00adcern location information \nfor analysis such as bloat detection in a broader set of software applications which need not have built-in \ndetailed and .ne grained feature assignments. Siegmund et al. recently introduced a black box approach \nfor detecting runtime feature interactions in order to estimate performance and other non-functional \nproperties of product line variants [45]. This could potentially be used estimate the performance impact \nof bloat due to an optional concern without generating all the variants with and without the fea\u00adture. \nHowever, the approach would still require the ability to automatically generate at least one correct \nrunnable variant of the program without the concern, so it does not apply to non product line software \nwhere exact concern assignments are not typically available.  6.5 Slicing The use of slicing in re.ning \nconcern location, key state\u00adment analysis [18] and building concern dependence graphs for program understanding \nis fairly well-established. Sev\u00aderal heuristics such as the use of barriers during depen\u00addence graph \ntraversal have been proposed for containing the size or (interprocedural) span of slices. Our microslic\u00ading \nanalysis for automatic feature-speci.c statement decom\u00adposition addresses a novel objective -to unearth \ncandidate .ne grained structural feature interactions that could result in execution bloat and to isolate \nthe corresponding feature\u00adspeci.c statements that potentially contribute to bloat. We incorporate concepts \nfrom feature oriented programming in (micro)slicing. Microslices are intra-procedurally modeled as the \nresolution of candidate feature interactions [24, 29]. We then build relations between these microslices \nto enable bloat inference when augmented with requisite concern in\u00adformation. 7. Conclusions In this \npaper, we introduced the use of concern informa\u00adtion in program analysis tasks and demonstrated its appli\u00adcation \nin estimating the propensity for execution bloat of optional concerns in Java programs. We use conservative \nabstractions when concern properties are not available for all statements and microslicing when concern \nassignments are too coarse. The effectiveness of the CAPA methodology is highly dependent on the quality \nof concern information available. Despite this caveat, our results show that it can provide a fresh approach \nto problems such as bloat analy\u00adsis that involve jointly reasoning about intent, structure and dynamics \nof program behavior. While we have concentrated largely on the runtime over\u00adhead caused by code that \nis not needed, there is an additional potential positive effect that code that is not required does not \nneed to be read and understood by developers hence, another bene.t of the approach could be that a reduced \nAPI could be generated which potentially reduces the develop\u00aders effort to use it. An interesting area \nof future work is to develop tech\u00adniques to aid the automatic discovery of information about optional \nconcerns and to explore tools to aid in automatic de-bloating of software. Acknowledgments We thank Giriprasad \nSridhara for contributions to the evalu\u00adation and Rupesh Nasre, Sandya Mannaraswamy, K Vasanta Laxmi \nand our anonymous reviewers for their feedback on the paper. A. Case study example     A.1 Stack \nwith 5 features STACK  COUNTER  LOCK  UNDO  BOUND  1 package multifeature; 2 3 4 public class StackFull \n{ 5 private String state = new String(); 6 private String savedState = ; 7 private char minValue; 8 \nprivate char maxValue; 9 private NestedLock txn = new NestedLock(); 10 private Counter cnt = new Counter(); \n11 12 public StackFull() { 13 } 14 15 private void clipBound() { 16 for (int i=0; i\u00a1state.length(); i++) \n{ 17 if (state.charAt(i) < minValue) { 18 state = state.replace(state.charAt(i), minValue); 19 } 20 if \n(maxValue > 0 &#38;&#38; state.charAt(i) > maxValue) {  21 state = state.replace(state.charAt(i), maxValue); \n22 } 23 } 24 } 25 26 public void bound(char low, char high) { 27 txn.lock(); 28 minValue = low; 29 maxValue \n= high; 30 clipBound(); 31 txn.unlock(); 32 } } 33 34 public void empty() { 35 txn.lock(); 36 save(); \n37 state = ; 38 cnt.reset(); 39 txn.unlock(); 40 } 41 42 public void push(char item) { 43 if (item < \nminValue) { 44 return; 45 } 46 if (maxValue > 0 &#38;&#38; item > maxValue) { 47 return; 48 } 49 txn.lock(); \n50 save(); 51 state = String.valueOf(item).concat(state); 52 cnt.increment(); 53 txn.unlock(); 54 } 55 \n56 public void pop() { 57 txn.lock(); 58 save(); 59 state = state.substring(1); 60 cnt.decrement(); 61 \ntxn.unlock(); 62 } 63 64 public char top() { 65 if (txn.isUnlocked()) { 66 return state.charAt(0); 67 \n} 68 return 0; 69 } 70 71 void push2(char item) { 72 push(item); 73 push(item); 74 } 75 76 private void \nsave() { 77 savedState = new String(state); 78 } 79 80 public void undo() { 81 txn.lock(); 82 state = \nsavedState; 83 cnt.undo(); 84 txn.unlock(); 85 } 86 87 public void print() { 88 System.out.println(state); \n89 cnt.print(); 90 } 91 92 } 93 94 class Counter { 95 private int index = 0; 96 private NestedLock key \n= new NestedLock(); 97 private int oldIndex = 0; 98 99 public Counter() { 100 } 101  102 public void \nreset() { 103 if (key.isUnlocked()) { 104 save(); 105 index = 0; 106 } 107 } 108 109 public void increment() \n{ 110 if (key.isUnlocked()) { 111 save(); 112 index++; 113 } 114 } 115 116 public void decrement() { \n117 if (key.isUnlocked()) { 118 save(); 119 index ; 120 } 121 } 122 123 public void disable() { 124 key.lock(); \n125 } 126 127 public void enable() { 128 key.unlock(); 129 } 130 131 public int size() { 132 return index; \n133 } 134 135 private void save() { 136 oldIndex = index; 137 } 138 139 public void undo() { 140 if (key.isUnlocked()) \n{ 141 index = oldIndex; 142 } 143 } 144 145 public void print() { 146 System.out.println( count = + \nindex); 147 } 148 } } 149 150 class NestedLock { 151 private boolean avail = true; 152 private int nest \n= 0; 153 154 public NestedLock() { 155 } 156 157 public void lock() { 158 if (!avail) { 159 nest++; 160 \n} 161 avail = false; 162 } 163 164 public void unlock() { 165 if (nest < 1) { 166 nest ; 167 return; \n168 } 169 nest = 0; 170 avail = true; 171 } 172 173 public boolean isUnlocked() { 174 return avail; 175 \n} 176 }  References [1] B. Aktemur and S. Kamin. A comparative study of techniques to write customizable \nlibraries. In SAC, 2009. [2] S. Apel and C. Kastner. An overview of feature-oriented software development. \nJournal Of Object Technology, 2009. [3] S. Apel, C. Kastner, and C. Lengauer. FEATURE-HOUSE: Language-independent, \nautomated software compo\u00adsition. ICSE, 2009. URL http://fosd.de/fh. [4] R. S. Arnold. Software Change \nImpact Analysis. IEEE Com\u00adputer Society Press, Los Alamitos, CA, USA, 1996. ISBN 0818673842. [5] P. F. \nBaldi, C. V. Lopes, E. J. Linstead, and S. K. Bajracharya. A theory of aspects as latent topics. In OOPSLA, \n2008. [6] S. Bhattacharya, M. G. Nanda, K. Gopinath, and M. Gupta. Reuse recycle to debloat software. \nIn ECOOP, 2011. [7] T. J. Biggerstaff, B. G. Mitbander, and D. Webster. The concept assignment problem \nin program understanding. In ICSE 93, pages 482 498, Los Alamitos, CA, USA, 1993. IEEE Computer Society \nPress. [8] W. Binder, J. Hulaas, P. Moret, and A. Villaz. Platform\u00adindependent proling in a virtual execution \nenvironment. Soft\u00adware Practice and Experience, 2009. [9] D. Binkley, G. Gold, M. Harman, Z. Li, and \nK. Mahdavi. An empirical study of the relationship between the concepts expressed in source code and \ndependence. volume 81. The Journal of Systems and Software, 2008. [10] S. Blackburn and et al. The DaCapo \nbenchmarks: Java bench\u00admarking development and analysis. In OOPSLA, 2006. [11] E. Bodden, M. Mezini, \nC. Brabrand, T. Tol edo, M. Ribeiro, and P. Borba. Spllift -statically analyzing software product lines \nin minutes instead of years. In PLDI 13, 2013. [12] B. Cornelissen, A. Zaidman, A. Van Deursen, L. Moonen, \nand R. Koschke. A systematic survey of program comprehension through dynamic analysis. volume 99. IEEE \nTSE, Apr. 2009. [13] M. K. Das, S. Bhattacharya, C. Bhattacharyya, and K. Gopinath. Subtle topic models \nand discovering subtly man\u00adifested software concerns automatically. In ICML, 2013. [14] B. Dit, M. Revelle, \nM. Gethers, and D. Poshyvanyk. Feature location in source code: a taxonomy and survey. Journal of Software: \nEvolution and Process, 25(1):53 95, 2013. ISSN 2047-7481. [15] B. Dufour, B. G. Ryder, and G. Sevitsky. \nA scalable technique for characterizing the usage of temporaries in framework\u00adintensive java applications. \nIn SIGSOFT 08/FSE-16, 2008. [16] M. Eaddy, A. V. Aho, G. Antoniol, and Y. G. Guhneuc. Cer\u00adberus: Tracing \nrequirements to source code using informa\u00adtion retrieval, dynamic analysis, and program analysis. ICPC, \n2008. [17] M. Gethers, B. Dit, H. Kagdi, and D. Poshyvanyk. Integrated impact analysis for managing software \nchanges. In ICSE, 2012. [18] M. Harman, N. Gold, R. Hierons, and D. Binkley. Code ex\u00adtraction algorithms \nwhich unify slicing and concept assign\u00adment. In WCRE, 2002. [19] C. K\u00a8astner and S. Apel. Type-checking \nsoft\u00adware product lines a formal approach. In ASE 07, Los Alamitos, CA, 2008. URL http://wwwiti.cs.uni-magdeburg.de/iti \ndb/research/cide. [20] C. K\u00a8astner, A. Dreiling, and K. Ostermann. Variability mining with leadt. Technical \nreport, 2011. [21] C. K\u00a8um, and G. Saake. astner, S. Apel, T. Th \u00a8Type check\u00ading annotation-based product \nlines. ACM Trans. Softw. Eng. Methodol., 21(3), 2012. [22] A. Kellens, K. Mens, P. Tonella, and D. D. \nInformatique. A survey of automated code-level aspect mining techniques. In In Transactions on Aspect \nOriented Software Development, pages 145 164, 2007. [23] G. Kiczales, J. Lamping, A. Mendhekar, C. Maeda, \nC. Lopes, J. M. Loingtier, and J. Irwin. Aspect-oriented programming. In ECOOP. SpringerVerlag, 1997. \n [24] C. H. P. Kim, C. K\u00a8astner, and D. S. Batory. On the modularity of feature interactions. In GPCE, \n2008. [25] C. H. P. Kim, E. Bodden, D. Batory, and S. Khurshid. Re\u00adducing con.gurations to monitor in \na software product line. In 1st International Conference on Runtime Veri.cation (RV), volume 6418 of \nLNCS, pages 285 299. Springer, Nov. 2010. [26] C. H. P. Kim, D. S. Batory, and S. Khurshid. Reducing \ncombinatorics in testing product lines. In AOSD, 2011. [27] M. Kim, S. Sinha, C. Gorg, H. Shah, M. J. \nHarrold, and M. G. Nanda. Automated bug neighborhood analysis for identifying incomplete bug .xes. In \nICST, 2010. [28] B. Li, X. Sun, H. Leung, and S. Zhang. A survey of code\u00adbased change impact analysis \ntechniques. Software Testing, Veri.cation and Reliability, Apr. 2012. [29] J. Liu, D. Batory, and C. \nLengauer. Feature oriented refactor\u00ading of legacy applications. ICSE, 2006. [30] M. Marin, A. V. Deursen, \nand L. Moonen. Identifying cross\u00adcutting concerns using fan-in analysis. TOSEM, 17:3:1 3:37, December \n2007. [31] N. Mitchell and G. Sevitsky. The causes of bloat, the limits of health. In OOPSLA, 2007. [32] \nN. Mitchell, G. Sevitsky, and H. Srinivasan. Modeling runtime behaviour in framework based applications. \nIn ECOOP, 2006. [33] N. Mitchell, E. Schonberg, and G. Sevitsky. Four trends leading to java runtime \nbloat. IEEE Software, 27(1), 2010. [34] A. Nanda and M. G. Nanda. Gaining insight into programs that \nanalyze programs: by visualizing the analyzed program. In ONWARDS, Companion to OOPSLA, 2009. [35] M. \nG. Nanda and S. Sinha. Accurate interprocedural null\u00addereference analysis for Java. In ICSE, 2009. [36] \nM. G. Nanda, S. Mani, V. S. Sinha, and S. Sinha. Demystify\u00ading model transformations: An approach based \non automated rule inference. In OOPSLA, 2009. [37] K. Pohl, G. B \u00a8ockle, and F. J. v. d. Linden. Software \nProduct Line Engineering: Foundations, Principles and Techniques. Springer-Verlag New York, Inc., Secaucus, \nNJ, USA, 2005. ISBN 3540243720. [38] C. Prehofer. Feature-oriented programming: A fresh look at objects. \nIn ECOOP, 1997. [39] M. Revelle, B. Dit, and D. Poshyvanyk. Using data fusion and web mining to support \nfeature location in software. ICPC, 2010. [40] M. P. Robillard. Topology analysis of software dependencies. \nACM TOSEM, Aug. 2008. [41] M. P. Robillard and G. C. Murphy. Representing concerns in source code. ACM \nTrans. Softw. Eng. Methodol., 16(1), Feb. 2007. [42] T. Savage, M. Revelle, and D. Poshyvanyk. Flat3: \nFeature location and textual tracing tool. ICSE, 2010. [43] D. Shepherd, L. Pollock, and T. Tourw\u00b4 e. \nUsing language clues to discover crosscutting concerns. In Proceedings of the 2005 workshop on Modeling \nand analysis of concerns in software, MACS 05. ACM, 2005. [44] D. Shepherd, Z. P. Fry, E. Hill, L. Pollock, \nand K. Vijay-Shanker. Using natural language program analysis to locate and understand action-oriented \nconcerns. In AOSD, 2007. [45] N. Siegmund, S. Kolesnikov, C. Kastner, S. Apel, D. Batory, M. Rosenmuller, \nand G. Saake. Predicting performance via automated feature-interaction detection. In ICSE, 2012. [46] \nStandard Performance Evaluation Corporation. http://www.specbench.org. [47] T. Tourwe and K. Mens. Mining \naspectual views using formal concept analysis. In SCAM 04, 2004. [48] G. Xu and A. Rountev. Detecting \ninef.ciently-used containers to avoid bloat. In PLDI, 2010. [49] G. Xu, M. Arnold, N. Mitchell, A. Rountev, \nand G. Sevitsky. Go with the .ow: pro.ling copies to .nd runtime bloat. In PLDI, 2009. [50] G. Xu, N. \nMitchell, M. Arnold, A. Rountev, E. Schonberg, and G. Sevitsky. Finding low-utility data structures. \nIn PLDI, 2010. [51] G. Xu, N. Mitchell, M. Arnold, A. Rountev, and G. Sevitsky. Software bloat analysis: \nFinding, removing, and preventing performance problems in modern large-scale object-oriented applications. \nIn FoSER, 2010. [52] C. Zhang and H.-A. Jacobsen. Ef.ciently mining crosscutting concerns through random \nwalks. In AOSD, 2007. [53] W. Zhao, L. Zhang, Y. Liu, J. Sun, and F. Yang. Snia.: Towards a static non-interactive \napproach to feature location. volume 15. ACM TOSEM, April 2006.  \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Framework based software tends to get bloated by accumulating optional features (or <i>concerns</i>) just-in-case they are needed. The good news is that such feature bloat need not always cause runtime execution bloat. The bad news is that often enough, only a few statements from an optional concern may cause execution bloat that may result in as much as 50% runtime overhead.</p> <p>We present a novel technique to analyze the connection between optional concerns and the potential sources of execution bloat induced by them. Our analysis automatically answers questions such as (1) whether a given set of optional concerns could lead to execution bloat and (2) which particular statements are the likely sources of bloat when those concerns are not required. The technique combines coarse grain concern input from an external source with a fine-grained static analysis. Our experimental evaluation highlights the effectiveness of such concern augmented program analysis in execution bloat assessment of ten programs.</p>", "authors": [{"name": "Suparna Bhattacharya", "author_profile_id": "81486652540", "affiliation": "IBM Research, Bangalore, India", "person_id": "P4290457", "email_address": "suparnaiisc@gmail.com", "orcid_id": ""}, {"name": "Kanchi Gopinath", "author_profile_id": "81100624506", "affiliation": "Indian Institute of Science, Bangalore, India", "person_id": "P4290458", "email_address": "gopi@csa..iisc.ernet.in", "orcid_id": ""}, {"name": "Mangala Gowri Nanda", "author_profile_id": "81100183505", "affiliation": "IBM Research, New Delhi, India", "person_id": "P4290459", "email_address": "mgowri@in.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509522", "year": "2013", "article_id": "2509522", "conference": "OOPSLA", "title": "Combining concern input with program analysis for bloat detection", "url": "http://dl.acm.org/citation.cfm?id=2509522"}