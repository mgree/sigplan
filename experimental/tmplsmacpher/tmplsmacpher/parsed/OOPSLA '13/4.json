{"article_publication_date": "10-29-2013", "fulltext": "\n Miniboxing: Improving the Speed to Code Size Tradeoff in Parametric Polymorphism Translations Vlad \nUreche Cristian Talau Martin Odersky EPFL, Switzerland {.rst.last}@ep..ch Abstract Parametric polymorphism \nenables code reuse and type safety. Underneath the uniform interface exposed to pro\u00adgrammers, however, \nits low level implementation has to cope with inherently non-uniform data: value types of dif\u00adferent \nsizes and semantics (bytes, integers, .oating point numbers) and reference types (pointers to heap objects). \nOn the Java Virtual Machine, parametric polymorphism is currently translated to bytecode using two competing \nap\u00adproaches: homogeneous and heterogeneous. Homogeneous translation requires boxing, and thus introduces \nindirect ac\u00adcess delays. Heterogeneous translation duplicates and adapts code for each value type individually, \nproducing more byte\u00adcode. Therefore bytecode speed and size are at odds with each other. This paper proposes \na novel translation that sig\u00adni.cantly reduces the bytecode size without affecting the execution speed. \nThe key insight is that larger value types (such as integers) can hold smaller ones (such as bytes) thus \nreducing the duplication necessary in heterogeneous trans\u00adlations. In our implementation, on the Scala \ncompiler, we encode all primitive value types in long integers. The result\u00ading bytecode approaches the \nperformance of monomorphic code, matches the performance of the heterogeneous trans\u00adlation and obtains \nspeedups of up to 22x over the homoge\u00adneous translation, all with modest increases in size. Categories \nand Subject Descriptors D.3.3 [Language Constructs and Features]: Polymorphism; E.2 [Object rep\u00adresentation] \nKeywords Miniboxing; Specialization; Data Representa\u00adtion; Parametric Polymorphism; Erasure; Scala; Java \nVirtual Machine; Bytecode; Generics Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright is held by the owner/author(s). Publication \nrights licensed to ACM. ACM 978-1-4503-2374-1/13/10. . . $15.00. http://dx.doi.org/10.1145/2509136.2509537 \n1. Introduction Parametric polymorphism allows programmers to describe algorithms and data structures \nirrespective of the data they operate on. This enables code reuse and type safety. For the programmer, \ngeneric code, which uses parametric polymor\u00adphism, exposes a uniform and type safe interface that can \nbe reused in different contexts, while offering the same behav\u00adior and guarantees. This increases productivity \nand improves code quality. Modern programming languages offer generic collections, such as linked lists, \narray buffers or maps as part of their standard libraries. But despite the uniformity exposed to programmers, \nthe lower level translation of generic code struggles with funda\u00admentally non-uniform data. To illustrate \nthe problem, we can analyze the contains method of a linked list parameterized on the element type, T, \nwritten in the Scala programming language: def contains(element: T): Boolean = ... When translating \nthe contains method to lower level code, such as assembly or bytecode targeting a virtual ma\u00adchine, a \ncompiler needs to know the exact type of the param\u00adeter, so it can be correctly retrieved from the stack, \nregisters or read from memory. But since the list is generic, the type parameter T can have different \nbindings, depending on the context, ranging from a byte to a .oating point number or a pointer to a heap \nobject, each with different sizes and se\u00admantics. So the compiler needs to bridge the gap between the \nuniform interface and the non-uniform low level imple\u00admentation. Two main approaches to compiling generic \ncode are in use today: heterogeneous and homogeneous. Heterogeneous translation duplicates and adapts \nthe body of a method for each possible type of the incoming argument, thus producing new code for each \ntype used. On the other hand, homoge\u00adneous translation, typically done with erasure, generates a single \nmethod but requires data to have a common represen\u00adtation, irrespective of its type. This common representation \nis usually chosen to be a heap object passed by reference, which leads to indirect access to values and \nwasteful data representation. This, in turn, slows down the program exe\u00adcution and increases heap requirements. \nThe conversions be\u00adtween value types and heap objects are known as boxing and unboxing. A different uniform \nrepresentation, typically re\u00adserved to virtual machines for dynamically typed languages, uses the .xnum \n[44] representation. This representation can encode different types in the same unit of memory by reserv\u00ading \nseveral bits to record the type and using the rest to store the value. Aside from reducing value ranges, \nthis representa\u00adtion also introduces delays when dispatching operations, as the value and type need to \nbe unpacked. An alternative is the tagged union representation [17], which does not restrict the value \nrange but requires more heap space.  C++ [38] and the .NET Common Language Runtime [5, 20] have shown \nthat on-demand heterogeneous transla\u00adtions can obtain good performance without generating sig\u00adni.cant \namounts of low level code. However, this comes at a high price: C++ has taken the approach of on-demand \ncompile-time template expansion, where compiling the use of a generic class involves instantiating the \ntemplate, type checking it and generating the resulting code. This provides the best performance possible, \nas the instantiated template code is monomorphic, but undermines separate compilation in two ways: .rst, \nlibraries need to carry source code, namely the templates themselves, to allow separate compilation, \nand second, multiple instantiations of the same class for the same type arguments can be created during \ndifferent compilation runs, and need to be eliminated in a later linking phase. The .NET Common Language \nRuntime takes a load-time, on\u00addemand approach: it compiles generics down to bytecode with type information \nembedded, which the virtual machine specializes, at load-time, for the type arguments. This pro\u00advides \ngood performance at the expense of more a complex virtual machine and lock-step advancements of the type \nsys\u00adtem and the virtual machine implementation. In trying to keep separate compilation and virtual ma\u00adchine \nbackward compatibility, the Java programming lan\u00adguage [23] and other statically typed JVM languages \n[1 4] use homogeneous translations, which sacri.ce performance. Recognizing the need for execution speed, \nScala special\u00adization [13] allows an annotation-driven, compatible and opportunistic heterogeneous transformation \nto Java byte\u00adcode. Programmers can explicitly annotate generic code to be transformed using a heterogeneous \ntranslation, while the rest of the code is translated using boxing [10]. Specializa\u00adtion is a compatible \ntransformation, in that specialized and homogeneously translated bytecode can be freely mixed. For example, \nif both a generic call site and its generic callee are specialized, the call will use primitive values \ninstead of box\u00ading. But if either one is not specialized, the call will fall back to using boxed values. \nSpecialization is also opportunistic in the way it injects specialized code into homogeneous one. Finally, \nbeing annotation-driven, it lets programmers decide on the tradeoff between speed and code size. Unfortunately \nthe interplay between separate compilation and compatibility forces specialization to generate all het\u00aderogeneous \nvariants of the code during the class compila\u00adtion instead of delaying their instantiation to the time \nthey are used, like C++ does. Although in some libraries this be\u00adhavior is desirable [9], generating \nall heterogeneous variants up front means specializing must be done cautiously so the size of the generated \nbytecode does not explode. To give a sense of the amount of bytecode produced by specialization, for \nthe Scala programming language, which has 9 primitive value types and 1 reference type, fully specializing \na class like Tuple3 given below produces 103 classes, the Carte\u00adsian product of 10 variants per type \nparameter: class Tuple3[A, B, C](a: A, b: B, c: C) In this paper we propose an alternative translation, \ncalled miniboxing, which relies on a very simple insight to reduce the bytecode size by orders of magnitude: \nsince larger value types (such as integers)can hold smaller value types (such as bytes), it is enough \nfor a heterogeneous translation to gener\u00adate variants for the larger value types. In our case, on the \nJava Virtual Machine, miniboxing reduces the number of code variants from 10 per type parameter to just \n2: reference types and the largest value type in the language, the long in\u00adteger. In the Tuple3 example, \nminiboxing only generates 23 specialized variants, two orders of magnitude less bytecode than specialization. \nMiniboxed code is faster than homoge\u00adneous code, as data access is done directly instead of using boxing. \nUnlike .xnums and tagged unions, miniboxing does not attach the type information to values but to classes \nand methods and thus leverages the language s static type system to optimize storage. Furthermore, the \nfull miniboxing trans\u00adformation eliminates the overhead of dispatching operations by using load-time \nclass cloning and specialization (\u00a76). In this context, our paper makes the following contributions: \n Presents an encoding that reduces the number of variants per type parameter in heterogeneous translations \n(\u00a73)and the code transformations necessary to use this encoding (\u00a74);  Optimizes bulk storage (arrays) \nin order to reduce the heap footprint and maintain compatibility to homoge\u00adneous code, produced using \nerasure (\u00a75);  Utilizes a load-time class transformation mechanism to eliminate the cost of dispatching \noperations on encoded values (\u00a76).  The miniboxing encoding can reduce duplication in any heterogeneous \ntranslation, as long as the following criteria are met: The value types of the statically typed target \nlanguage can be encoded into one or more larger value types (which we call storage types)-in the work \npresented here we use the long integer as the single storage type for all of Scala s primitive value \ntypes;  Conversions between the value types and their storage type do not carry signi.cant overhead \n(no-op conversions are preferable, but not required);  The set of operations allowed on generic values \nin the language is .xed (similar to .xing the where clauses in PolyJ [8]);  All value types have boxed \nrepresentations, in order to have a common data representation between homoge\u00adneous and miniboxed code. \nThis representation is used to ensure compatibility between the two translations.  In order to optimize \nthe code output by the miniboxing transformation, this paper explores the interaction between value encoding \nand array optimization on the HotSpot Java Virtual Machine. The .nal miniboxing transformation, im\u00adplemented \nas a Scala compiler plug-in1, approaches the per\u00adformance of monomorphic code, matches the performance \nof specialization, and obtains speedups of up to 22x over the current homogeneous translation, all with \nmodest increases in bytecode size (\u00a77). The paper will .rst explain the specialization transfor\u00admation \n(\u00a72) upon which miniboxing is built. It will then go on to explain the miniboxing encoding (\u00a73), transformation \n(\u00a74), runtime support (\u00a75) and load-time specialization (\u00a76). It will .nish by presenting the evaluation \n(\u00a77), surveying the related work (\u00a78) and concluding (\u00a79). 2. Specialization in Scala This section presents \nspecialization [13], a heterogeneous translation for parametric polymorphism in Scala. Minibox\u00ading builds \nupon specialization, inheriting its main mecha\u00adnisms. Therefore a good understanding of specialization \nand its limitations is necessary to motivate and develop the mini\u00adboxing encoding (\u00a73) and transformation \n(\u00a74). There are two major approaches to translating paramet\u00adric polymorphism to Java bytecode: homogeneous, \nwhich requires a common representation for all values, and hetero\u00adgeneous, which duplicates and adapts \ncode for each type. By default, both the Scala and Java compilers use homoge\u00adneous translation with each \nvalue type having a correspond\u00ading reference type. Boxing and unboxing operations jump from one representation \nto the other. For example, int has java.lang.Integer as its corresponding reference type. Boxing enables \na uniform low level data representation, where all generic type parameters are translated to refer\u00adences. \nWhile this simpli.es the translation to bytecode, it does come with several disadvantages: Initialization \ncost: allocating an object, initializing it and returning a pointer takes longer than simply writing \nto a processor register;  Indirect access: Extracting the value from a boxed type requires computing \na memory address and accessing it instead of simply reading a processor register;  1Available at http://scala-miniboxing.org/. \n Undermined data locality: Seemingly contiguous mem\u00adory storages, such as arrays of integers, become \narrays of pointers to heap objects, which may not necessarily be aligned in the memory. This can affect \ncache locality and therefore slow down the execution;  Heap cost: the boxed object lives on the heap \nuntil it is not referenced anymore and is garbage collected. This puts pressure on the heap and triggers \ngarbage collection more often.  To eliminate the overhead of boxing, the Scala com\u00adpiler features specialization: \nan annotation-driven, compat\u00adible and opportunistic heterogeneous transformation. Spe\u00adcialization is \nbased on the premise that not all code is worth duplicating and adapting: code that rarely gets executed \nor has little interaction with value types is better suited for ho\u00admogeneous translation. Since a compile-time \ntransformation such as specialization has no means of knowing how code will be used, it relies on programmers \nto annotate which code to transform. Recent research in JavaScript interpreters [14, 45] uses pro.ling \nas another method of triggering com\u00adpatible specialization of important traces in the program. With specialization, \nprogrammers explicitly annotate the code to be transformed heterogeneously (\u00a72.1 and \u00a72.2) and the rest \nof the program undergoes homogeneous translation. The bytecode generated by the two translations is compat\u00adible \nand can be freely mixed. This allows specialization to have an opportunistic nature: it injects specialized \ncode, in the form of specialized class instantiations and specialized method calls (\u00a72.3), but the injected \nentities are always com\u00adpatible with the homogeneous translation (\u00a72.4). However, the interaction with \nseparate compilation leads to certain limitations that miniboxing addresses (\u00a72.5). 2.1 Class Specialization \nTo explain how specialization applies the heterogeneous translation, we can use an immutable linked list \nexample: class ListNode[@specialized T] (val head: T, val tail: ListNode[T]) { def contains(element: \nT): Boolean = ... } Each ListNode instance stores an element of type T and a reference to the tail of \nthe list. The null pointer, placed as the tail of a list, marks its end. A real linked list from the \nScala standard library is more sophisticated [26, 34], but for the purpose of describing specialization \nthis example is suf.cient. It is also part of the benchmarks presented in the Evaluation section (\u00a77), \nas it depicts the behavior of non\u00adcontiguous collections that require random heap access. The ListNode \nclass has the generic head .eld, which needs to be specialized in order to avoid boxing. To this end, \nspecialization will duplicate the class itself and adapt its .elds for each primitive value type. Figure \n1 shows the class hierarchy created: the parent class is the homogeneous trans\u00adlation of ListNode, which \nwe also call generic class. The 10 subclasses are the specialized variants. They correspond to the 8 \nJava primitive types, Unit (which is Scala s object\u00adoriented representation of void)and reference types2. \nEach of these specialized classes contains a head .eld of a prim\u00aditive type, and inherits (or overrides) \nmethods de.ned in the generic class. So far, specialization duplicated the class and adapted the .elds, \nbut in order to remove boxing the methods also need to be transformed heterogeneously.  2.2 Method \nSpecialization In the specialized variants of ListNode, the contains method needs to be duplicated and \nadapted to accept prim\u00aditive values as arguments instead of their boxed represen\u00adtations. Since the contains \nmethod is already inherited from the generic class, it actually needs to be overridden. But it cannot \nbe overridden, because its signature after the erasure [10] transformation expects a reference type (java.lang.Object) \nand the specialized signature ex\u00adpects a primitive value. Therefore specialized methods need to be name-mangled, \ngiving birth to new methods such as contains_I for Int and contains_J for Long. The contains method from \nthe generic parent class will be inherited by all the specialized classes. But its code is generic and \ndoes not make use of primitive values, which is suboptimal. Therefore each specialized class overrides \nthe generic contains and redirects it to the corresponding specialized variant, such as contains_I or \ncontains_J. The redirection is done by unboxing the argument received by contains and calling the specialized \nmethod with the value type, as shown in Figure 2. The same transformation is applied for accessors of \nspecialized .elds, such as head in the ListNode class.  2.3 Opportunistic Tree Transformation The program \ncode can only refer to generic classes and methods, not their specialized variants. This happens be\u00adcause \nthe specialization phase, which creates the variants, runs after the type checking phase. Thus the program \nis checked only against the generic classes and methods. But this does not mean specialization duplicates \ncode in vain: aside from creating the variants, specialization also injects the specialized variants \nin the program code. The last step in eliminating boxing is rewriting the Scala abstract syntax tree \n(AST) to instantiate specialized classes and use specialized methods. We call this process rewiring. \nRewiring works across separate compilation, as the special\u00adization metadata is written in the generated \nbytecode. This makes is possible to use specialized code from libraries. The instantiation rewiring injects \nspecialized classes when the new keyword is used. When the instantiated class has a more speci.c specialized \nvariant for the given type arguments, the instantiation is rewired. Despite constructing a different \nclass, the types in the AST are not adjusted to re.ect this: In the example given below, although the \ninstan\u00adtiation is rewired to new ListNode_I, the type of node1 2Technical note: For a single type parameter \nthe reference variant will not be generated and the generic class will be used instead. Figure 1. Class \nhierarchy generated by Specialization. The letters in class suf.x represent the type they are specialized \nfor: V-Scala Unit, Z-Boolean, B-Byte . . . J-Long, L-AnyRef. The names are simpli.ed throughout the paper, \nand we avoid discussing the problem of name mangling, which was ad\u00addressed in [13]. remains ListNode[Int]. \nThis makes specialization com\u00adpatible: whether or not the instantiation is rewired, both the specialized \nclass and the generic class are still subtypes of ListNode[Int]. Rewiring can only be done if the type \narguments are statically known: // before rewiring: val node1: ListNode[Int] = new ListNode[Int](3, \nnull) // after rewiring: val node1: ListNode[Int] = new ListNode_I(3, null) // not rewired if U is \nan abstract type or the // type parameter of an enclosing class/method val node2: ListNode[U] = new \nListNode[U](u, null) The next step of rewiring changes inheritance relations when parent classes have \nspecialized variants that match the type arguments. This injects specialized variants of a class in the \ninheritance chain, making it possible to use unboxed val\u00adues when extending a specialized class. This \nis yet another opportunistic transformation, since the inheritance relation is only rewritten if the \ntype arguments are known statically, as shown by the following example: // before rewiring: class IntNode(head: \nInt, tail: IntNode) extends ListNode[Int](head, tail) // after rewiring: class IntNode(head: Int, tail: \nIntNode) extends ListNode_I(head, tail) // not rewired, T not known statically: class MyNode[T](head: \nT, tail: MyNode[T]) extends ListNode[T](head, tail) The two rewirings above inject specialized classes \nin the code. Still, call sites point to the homogeneous methods, which use boxed values. The last rewiring \naddresses meth\u00adods, which are rewritten depending on the type of their re\u00adceiver. Any call site with \na specialization-annotated receiver for which the type argument is statically known is rewritten to use \nspecialized versions of the methods. In the .rst call site of the example below, the receiver is the \nspecialization\u00adannotated class ListNode and the type argument is stati\u00adcally known to be Int. Therefore \nthe call to contains is rewired to the specialized contains_I:  Figure 2. Method overriding and redirection \nfor ListNode and two of its specialized variants. Constructors and acces\u00adsors are omitted from this diagram. \n// before rewiring: (node1: ListNode[Int]).contains(3) // after rewiring: (node1: ListNode[Int]).contains_I(3) \n // not rewired if U is an abstract type or the // type parameter of an enclosing class/method (node2: \nListNode[U]).contains(u)  2.4 Specialization Compatibility Since the rewiring process only takes place \nfor statically known type arguments, the generic class and its specialized subclasses may be mixed together. \nIn the following snippet, the .rst branch of the if statement is rewired to create an instance of ListNode_I \nwhile the second branch calls the node method, whose type parameter T is not annotated for specialization, \nand thus creates the generic class ListNode. Therefore, the value lst (of static type ListNode[Int]) \nmay be either an instance of ListNode_I or of ListNode, depending on the random condition: // new ListNode[T] \nnot rewired to // ListNode_I since T is a type parameter def node[T](t: T) = new ListNode[T](t, null) \n val lst: ListNode[Int] = if (Random.nextInt().isEven) new ListNode[Int](1, null) // ListNode_I else \n node(2) // ListNode lst.contains(0) // rewired to contains_I Therefore, calling a specialized method, \ncontains_I in this case, can have as receivers both the generic class, ListNode, and the specialized \none, ListNode_I. So both classes must implement the specialized method. To do so, in ListNode, contains \nwill be implemented using generic code and contains_I will box the argument and call contains. In ListNode_I, \ncontains_I will be imple\u00admented using primitive value types and contains will un\u00adbox and redirect. This \ncan be generalized to multiple spe\u00adcialized variants, as can be seen in Figure 2: The generic class at \nthe top of the hierarchy contains all specialized vari\u00adants of the contains method as redirects to the \ngeneric method. Then, each specialized variant of the class inherits from the generic class and overrides \nits corresponding spe\u00adcialized methods (such as contains_I for ListNode_I) with the heterogeneously transformed \ncode and redirects the generic method to the specialized variant. This shows the compatible nature of \nspecialization: in or\u00adder to avoid boxing, both the call site and the receiver need to be rewired, which \nmeans the receiver needs to be specialized and the call site needs to know the type arguments statically \nor be part of code that will be specialized. But if either condi\u00adtion is not ful.lled, the code remains \ncompatible by boxing, either at the call site itself or inside the redirecting method. From the perspective \nof typing the abstract syntax trees, compatibility is achieved because types are assigned before the \nspecialization phase and are not modi.ed later, so they refer to the generic class, even in the presence \nof rewiring. The .rst example in \u00a72.3 shows that despite rewiring the new operator to create an instance \nof ListNode_I, the type of the node1 value remains ListNode[Int]. Thus type-level compatibility is satis.ed \nby ListNode_I being a subtype of ListNode, and the reverse subtyping is not necessary, as types never \nrefer to ListNode_I3 .  2.5 Limitations of Specialization There are two limitations in specialization: \nthe bytecode ex\u00adplosion and the crippled specialized class inheritance. We will describe each problem \nand show how both can be ad\u00addressed by the miniboxing encoding. The specialization mechanism for generating \nvariants is static: whenever the compiler encounters a class annotated for specialization, it generates \nall its variants up front and outputs bytecode for each of them. This is done to support separate compilation. \nTheoretically, the specialized variant creation could be delayed until the actual usage but this requires \nthat the source .les for specialized classes are available in all future compi\u00adlation stages, exactly \nlike in C++. This approach is undesir\u00adable from a user perspective, as it also requires encoding the \noriginal compilation .ags and state, which can in.uence the generated code. Therefore the simplest, although \nbytecode\u00adexpensive solution was chosen: to generate specialized vari\u00adants for all value types during \ncompilation. Ful.lling the bytecode compatibility requirements de\u00adscribed before, for n type parameters \nand full specialization, means the generic class needs to implement 10n methods, of which 10n -2 are \nthen inherited in the specialized subclasses and 2 are overridden by each of the 10n subclasses. This \nmakes the bytecode size proportional to 10n. If the methods were not inherited but de.ned in each subclass, \nthe bytecode size would be proportional to 102n . Still, the generic parent design choice affects inheritance \nbetween specialized classes. Figure 3 shows an example where the design of specialization bumps into \na multiple class inheritance, which is forbidden by Java. In this case, the children inherit from their \ngeneric parent, which is subopti\u00admal, since the specialized variants of MyList cannot use the 3Except \nfor the this type and singleton types in the adapted code.  specialization in ListNode. Experienced \nScala program\u00admers might suggest that MyNode should be a trait, so it can be mixed in [28]. Indeed this \nsolves the multiple inheritance problem, but creates bytecode proportional to 102n, because the compiler \ndesugars the trait into an interface, and each specialized MyList_* class has to implement the methods \nin that interface. Other more technical problems stem from this design choice too, but could be avoided \nby having an ab\u00adstract parent class. For example, .elds from the generic class are inherited by the specialized \nclasses, therefore increasing their memory footprint. Constructors also require more com\u00adplex code because \ninstantiating a specialized class calls the constructor of its parent, the generic class, which needs \nto be prevented from running, such that side effecting operations in the original class constructor are \nnot executed twice. All in all, at the heart of the bytecode explosion problem and thus the other limitations \nof specialization, lies the large number of variants per type parameter: 10. For two type pa\u00adrameters, \nfull specialization with correct inheritance creates 104 times the bytecode. In practice this is not \nacceptable. Therefore a natural question to ask is how can we reduce the number of variants generated \nper type parameter? This is the question that inspired miniboxing. 3. Miniboxing Encoding Constraints \non the bytecode size currently prevent us from extending the use of specialization in the standard library, \nnamely, to tuples of three elements, to the collections hierar\u00adchy and to Function traits, which are \nused in Scala s object oriented representation of functions. Therefore we propose the miniboxing encoding \nand transformation as a solution to reduce bytecode size and allow library specialization. Along with \nthe encoding, we present a transformation based on the principles of specialization, but using the miniboxed \nencod\u00ading (\u00a74) instead of primitive value types. The miniboxing technique relies on a simple insight: \ngrouping different value types reduces the number of vari\u00adants necessary in the heterogeneous translation. \nTo this end, we need to group the value types in the language into dis\u00adjoint sets and for each set designate \na value type, also called a storage type, which can encode any type in that set. Notice that this de.nition \nis not limited to primitive value types, but can also be used for C-like structs. Four conditions need \nto be satis.ed for the miniboxing transformation to work: All of the value types in the language can \nbe encoded into one or more storage types;  The overhead of transforming between any value type and \nits storage type must be limited, ideally a no-op;  The operations available for generic types in the \nlan\u00adguage (inherited from the top of the hierarchy, such as toString, hashCode and equals)must be .xed; \n All the value types need to have boxed representations, to enable compatibility between the miniboxed \nand ho\u00admogeneous translations (\u00a72.4). If the bytecode s common  Figure 3. An example of specialized \nclass inheritance made impossible by the current translation scheme. representation is tagged union, \nthe requirement changes to having tagged union representations. In this case, the heterogeneous translation \nonly needs to generate variants for the storage types and references. Ref\u00aderences are a special storage \ntype, since all value types are also considered to be part of the reference group. During the translation, \nwhenever a type is not known to be miniboxed to one of the storage types, it is automatically assumed \nto be attached to the references group. This allows the opportunis\u00adtic (\u00a72.3) and compatible (\u00a72.4) rewiring \nof the tree: indeed since any value type has a boxed representation, it is always correct (but not optimal) \nto store it as a boxed reference. In the extreme case where all value types are their own storage types, \nwe are back to specialization. The next subsection will present miniboxing in Scala. 3.1 Miniboxing \nin Scala In order to apply the miniboxing encoding to Scala, we de\u00adcided to use the long integer (Long)as \nthe storage type of all other primitive value types. Other sets of storage types could also be implemented \nto improve speci.c scenarios, such as running on 32-bit architectures (32-bit Int and 64\u00adbit Long)or \nusing .oating-point numerics extensively4 (64\u00adbit Double and 64-bit Long). Still, for the rest of the \ndescrip\u00adtion, we will use the long integer as the only storage type, in order to be consistent with the \ncurrent implementation of the miniboxing plugin. The transformation primitives from value types to Long \nand back are implemented in the HotSpot Java Virtual Ma\u00adchine and have direct translations to bytecode4 \nand to pro\u00adcessor instructions [18]. Nevertheless, two concerns need our attention when using miniboxing: \n Packing and unpacking cost;  Memory footprint of the miniboxed encoding.  Packing and unpacking cost. \nBoxing and unboxing ac\u00adcesses the heap memory. The main goal of miniboxing is to eliminate this overhead, \nbut, in doing so, conversions to and from long integers must not slow down program execution signi.cantly \ncompared to monomorphic code. Our bench\u00admarks show that indeed the overhead is negligible (\u00a77). 4The \n.oating point to integer bit-preserving transformations, which are implemented as intrinsics, do incur \na measurable overhead.   Figure 4. An example of miniboxed class inheritance. The suf.xes are: M -miniboxed \nencoding and L -reference type. Compare to the specialized class inheritance in Figure 3. Memory footprint. \nThe miniboxed encoding has a mem\u00adory footprint between that of monomorphic and generic code. Considering \nbyte as the type argument, the memory footprint of the miniboxed encoding is 8 times larger than the \none for monomorphic code, which would store the byte directly. This factor is reduced by specializing \nbulk storage (arrays) and considering the paddings introduced by the vir\u00adtual machine. On the other hand, \nwhen compared to boxing on 64 bit processors, the factor is exactly 1, as both a pointer and a long integer \nhave 8 bytes. And this does not take into account the heap space occupied by the boxed values them\u00adselves. \nTherefore, all things considered, miniboxing has a memory footprint larger than the monomorphic and hetero\u00adgeneous \ntranslations, but smaller than homogeneous transla\u00adtions based on boxing. 4. Miniboxing Transformation \nThe miniboxing transformation, which we developed as a Scala compiler plugin, builds upon specialization, \nwhich has been formalized in [13]. It has the same opportunistic and compatible nature and performs class \nand method duplica\u00adtion in a similar manner. Still, .ve elements set it apart: the different inheritance \nscheme (\u00a74.1)  the type bytes for storing encoded types (\u00a74.2.1, \u00a74.2.4)  the use of a shallow type \ntransformation (\u00a74.2.2)  the use of the .nal peephole transformation (\u00a74.2.3)  the runtime support \nfor miniboxed values (\u00a74.3 and \u00a75)  4.1 Inheritance Miniboxing uses a generic trait as the parent of \nthe special\u00adized classes, therefore avoiding the limitation that miniboxed classes cannot inherit from \neach other (\u00a72.5). Figure 4 shows an example miniboxed class inheritance. As explained in \u00a72.5, for n \nspecialized type parameters, having a trait as the parent increases the bytecode size from 2n to 4n, \nsince each of the 2n miniboxed variants needs to implement all 2n methods. Still, the extra bytecode \nis well spent, for two reasons: Having a trait at the top of the hierarchy means no generic .elds are \ninherited in the specialized variants, as it hap\u00adpens when the homogeneous translation is at the top \nof the hierarchy (\u00a72.5); This inheritance scheme allows specialized classes to in\u00adherit their specialized \nparent, thus achieving better per\u00adformance in deep hierarchies. Since the types assigned to tree nodes \ndo not reference the specialized variants but only the generic interface, this inheritance scheme does \nnot interfere with covariance or contravariance. Indeed, if the type parameter of ListNode is de.ned \nas covariant, ListNode_M[Int] is subtype of ListNode[Int] and, transitively, of ListNode[Any].  4.2 \nMiniboxing Speci.cs This section will work its way from small examples to de\u00adscribing the new elements \nin the miniboxing transformation, as compared to specialization. In order to simplify the pre\u00adsentation, \nwe will use the Long-based encoding for mini\u00adboxing, but the transformation can still be generalized \nto any number of storage types. 4.2.1 Type Bytes Type-encoding bytes (or type bytes for short) record \nthe original type of the miniboxed values. Translating the fol\u00adlowing example shows when type bytes are \nnecessary: def print[@minispec T](value: T): Unit = println(value.toString) Having the type parameter \nT annotated with @minispec will trigger miniboxing, which will duplicate this method for Long-encoded \nvalue types, which we also call miniboxed types. Like specialization, miniboxing produces groups of overloaded \nmethods, with the original method being the all\u00adreference implementation in its group. In our case, only \nthe miniboxed overload needs to be created. To do so, the com\u00adpiler will create another version of print \nfor long integers, which we call print_M: def print_M(value: Long): Unit = println(value.toString) This \nis a very naive translation. Calling print(false), after method rewiring, will transform the boolean \nto a long integer whose value will be printed on the screen instead of the false string. To perform the \ncorrect action, the transla\u00adtion should recover the string representation of the boolean value false \nfrom the Long encoding. This suggests the toString operation should be rewritten to: def print_M(value: \nLong): Unit = println(MBRuntime.toString(value)) The code above shows a less naive implementation, since \nit rewires toString calls on the miniboxed value to a spe\u00adcial runtime support object in order to obtain \nthe string representation. But passing a single miniboxed value isn t enough, as we mentioned miniboxing \ndoes not encode the type with the value as tagged unions do [17]. Therefore, it should have a separate \nparameter to encode the original type:  def print_M(T_Type: Byte, value: Long): Unit = println(MBRuntime.toString(value, \nT_Type)) This is close to the minibox-transformed version of print_M the plugin would output. The T_Type \n.eld only encodes the 9 primitive types in Scala, therefore it does not incur the typical overhead of \nfull rei.ed generics [37]. A call to print(false) will be translated to the following code, where BOOLEAN \nis the type byte for boolean values: print_M(BOOLEAN, MBRuntime.BoolToMinibox(false)) The method call \nabove shows two differences between rewiring in miniboxing and specialization: 1. Calling a miniboxing-transformed \nmethod (or instanti\u00adating a miniboxing-transformed class) requires passing type bytes for all the Long-encoded \ntype arguments; 2. The arguments to minibox-transformed methods need to be explicitly encoded in the \nstorage type.  We will now present exactly how the miniboxing plugin arrives to this transformed code. \nAs the miniboxing transfor\u00admation takes place, it needs to preserve program semantics and type correctness. \nIn order to do so, the transformation for print is actually done in three steps. First, the new signature \nis created, knowing the type pa\u00adrameter T is encoded as Long. The method name is man\u00adgled (mangled names \nare simpli.ed in this presentation) and the type byte for T is added to the signature. Then parame\u00adters \nare added, with all parameters of type T being replaced by parameters of type Long. As this happens, \nthe symbols whose types changed are recorded and treated specially. In this case, the only miniboxed \nparameter is value, which is recorded. It is also recorded that the type byte T_Type cor\u00adresponds to \nthe encoded type T. This yields: (we ll see later why the type parameter T still appears) def print_M[T](T_Type: \nByte, value: Long): Unit = // need to copy and adapt body from print In the second step, the body is \ncopied from the print method. To maintain type correctness, all the symbols previ\u00adously recorded as having \ntheir types changed are now auto\u00admatically boxed back to generic type T, so the newly gener\u00adated code \ntree is consistent in terms of types: def print_M[T](T_Type: Byte, value: Long): Unit = println(MBRuntime.MiniboxToBox[T](value, \nT_Type).toString) In the .nal step, the tree rewrite rules will transform the call to MiniboxToBox followed \nby toString into a single call to the MBRuntime system, which typically yields better performance: def \nprint_M[T](T_Type: Byte, value: Long): Unit = println(MBRuntime.toString(value, T_Type)) The next section \nwill explain why it is necessary to carry the type parameter T.  4.2.2 Shallow and Deep Type Transformations \nTo further understand the miniboxing transformation, let us look at a more complex example, which builds \na linked list with a single element: def list[@minispec T](value: T): ListNode[T] = new ListNode[T](value, \nnull) As explained before, the list method will become the all-reference overload. But the interesting \ntransformation happens in the miniboxed variant. If specialization were to transform this method its \nsignature would be: def list_M[T](value: Long): ListNode[Long] The return type is incorrect, as we expect \nlist(3) to return a ListNode[Int], and yet rewiring list(3) to list_M(...) would return a ListNode[Long]. \nThis ex\u00adposes the difference between the deep type transformation in specialization and the shallow type \ntransformation in mini\u00adboxing: In miniboxing, only values of type T are transformed to Long, but any \ntype referring to T, such as ListNode[T], will remain the same. This explains why the type parameter \nT is carried over to print_M and list_M: it may still be used in the method s signature and code. The \nfull transfor\u00admation for method list_M will be: def list_M[T](T_Type: Byte, value: Long): ListNode[T] \n= new ListNode[T](MiniboxToBox[T](value, T_Type)) The shallow type transformation also changes types \nof local variables from T to Long and recursively transforms all nested methods and classes within the \npiece of code it is adapting. This propagates the storage type representation throughout the code.  \n4.2.3 Peephole Transformation The last transformation to touch the code before it is shipped to the next \nphase is the peephole transformation, which per\u00adforms a .nal sweep over the code to remove redundant \nconversions. To show this phase at work, let us consider what happens if the ListNode class in the last \nexample is also annotated for miniboxing. In this case, the class will have a miniboxed variant, ListNode_M \nto which the instantiation is rewired. Since the head parameter of the ListNode constructor is boxed, \nwhile the head parameter of the ListNode_M constructor is miniboxed, the transfor\u00admation will introduce \na new BoxToMinibox conversion: def list_M[T](T_Type: Byte, value: Long): ListNode[T] = new ListNode_M[T](T_Type, \nBoxToMinibox[T](MiniboxToBox[T](value, ...)), null) Converting from Long to the boxed representation \nand back before creating the list node will certainly affect perfor\u00admance. Such consecutive complementary \nconversions and other suboptimal constructs are automatically removed by the peephole optimization: \n def list_M[T](T_Type: Byte, value: Long): ListNode[T] = new ListNode_M[T](T_Type, value, null) The \ncode produced by the rewiring phase can be opti\u00admized by a single pass of the peephole transformation \nso there is no need to iterate until a .xed point is reached.  4.2.4 Type Bytes in Classes The class \ntranslation is slightly more complex than method translation. For classes, type bytes are also included \nas .elds in the miniboxed variants, to allow the class methods to encode and decode miniboxed values \nas necessary: class ListNode[@minispec T] (val head: T, val tail: ListNode[T]) { def contains(element: \nT): Boolean = ... } The interface resulting after miniboxing will be: trait ListNode[T] { ... // getters \nfor head and tail def contains(element: T): Boolean def contains_M(T_Type_local: Byte, element: Long): \nBoolean } And the miniboxed variant of this class will be: class ListNode_M[T] (T_Type: Byte, head: \nLong, tail: ListNode[T]) extends ListNode[T] { ... // getters for head and tail def contains(element: \nT): Boolean = ... // redirect to this.contains_M def contains_M(T_Type_local: Byte, element: Long): \nBoolean = ... // specialized implementation } ListNode_M has two type tags: T_Type is a class param\u00adeter \nand becomes a .eld of the class while T_Type_local is passed to the contains_M method directly. In the \ncode example, T_Type is used to convert the element param\u00adeter of contains to its miniboxed representation \nwhen redirecting the call to contains_M. But T_Type_local is not used in the ListNode_M class. To understand \nwhen T_Type_local is necessary, we have to look at the refe\u00adrence-carrying variant of the ListNode class: \nclass ListNode_L[T] (head: T, tail: ListNode[T]) extends ListNode[T] { ... // getters for head and tail \ndef contains(element: T): Boolean = ... // generic implementation def contains_M(T_Type_local: Byte, \nelement: Long): Boolean = ... // redirect to this.contains } All instantiations of ListNode where the \ntype argu\u00adment is statically known to be a value type are rewired to ListNode_M. The rest of the instantiations \nare rewired to ListNode_L, either because the type argument is not known statically or because it is \nknown to be a reference type. Therefore, there is no reason for ListNode_L to carry T_Type as a global \n.eld. But, in order to allow contains_M to decode the miniboxed value element into a boxed form and redirect \nthe call contains, a local type byte is neces\u00adsary. Since the ListNode interface and its two implemen\u00adtations, \nListNode_L and ListNode_M need to be compat\u00adible, the local type byte in contains_M is also present for \nListNode_M, although in the miniboxed class it is redun\u00addant.  4.3 Calling the Runtime Support The previous \nexamples have shown how the miniboxing plugin uses the MBRuntime object for conversions between unboxed, \nminiboxed and boxed data representations. But the MBRuntime object is not limited to conversions. In \nScala, any type parameter is assumed to be a subtype of the Any class, so the programmer can invoke methods \nsuch as toString, hashCode and equals on generic values. As shown in \u00a74.2.1, these calls can be translated \nby a con\u00adversion to the boxed representation followed by the call, but are further optimized by calling \nthe implementations in MBRuntime, which work directly on miniboxed values. Aside from conversions and \nimplementations for the methods in the Any class, the miniboxing runtime support contains code to allow \ndirect interaction between arrays and miniboxed values. An example that uses arrays is the ArrayBuffer \nclass: class ArrayBuffer[@minispec T: Manifest] { // fields: private[this] var array = new Array[T](32) \n... // methods: def getElement(idx: Int): T = array(idx) ... } The miniboxed variant ArrayBuffer_M \nis rewritten to call the MBArray object to create and access arrays in the miniboxed format: // ArrayBuffer \nminiboxed variant for primitives: class ArrayBuffer_M[T: Manifest](T_Type: Byte) extends ArrayBuffer[T] \n{ // fields: private[this] var array: Array[T] = MBArray.mbarray_new(32, T_Type) ... // methods: \ndef getElement(idx: Int): T = MiniboxToBox(getElement_M(T_Type, idx), ...) def getElement_M(T_Type_local: \nByte, idx: Int): Long = MBArray.array_get(array, idx, T_Type) ... }  The implementation of the MBArray \nobject is critical to numeric algorithms and performance data structures, as it has to be small enough \nto be inlined by the just-in-time compiler and structured in ways that return the result as fast as possible \nfor any of the primitive types. The following two sections describe the runtime support for arrays and \ngive technical insights into the pitfalls of the implementation. 5. Miniboxing Bulk Storage Optimization \nArrays are Java s bulk storage facility. They can store value types or references to heap objects. This \nis done ef.ciently, as values are stored one after the other in contiguous blocks of memory and access \nis done in constant time. Their charac\u00adteristics make arrays good candidates for internal data struc\u00adtures \nin collections and algorithms. But in order to implement compact storage and constant access overhead, \narrays are monomorphic under the hood, with separate (and incompatible) variants for each of the primitive \nvalue types. What s more, each array type has its own speci.c bytecode instructions to manipulate it. \nThe goal we set forth was to match the performance of monomorphic arrays in the context of miniboxing-encoded \nvalues. To this end, we had two alternatives to implementing arrays for miniboxed values: use arrays \nof long integers to store the encoded values or use monomorphic arrays for each type, and encode or decode \nvalues at each access. Storing encoded values in arrays provides the advantage of uniformity: all the \ncode in the minibox-specialized class uses the Long representation and array access is done in a single \ninstruction. Although this representation wastes heap space, especially for small value types such as \nboolean or byte, this is not the main drawback: it is incompatible with the rest of the Scala code. In \norder to stay compatible with Java, Scala code uses monomorphic arrays for each value type. Therefore \narrays of long integers in miniboxed classes must not be allowed to escape from the transformed class, \notherwise they may crash outside code attempting to read or write them. To main\u00adtain compatibility, we \ncould convert escaping arrays to their monomorphic forms. But the conversion would introduce delays and \nwould break aliasing, as writes from the outside code would not be visible in the miniboxed code and \nvice versa. Since completely prohibiting escaping arrays severely restricts the programs that can use \nminiboxing, this solution becomes unusable in practice. Thus, the only choice left is to use arrays in \ntheir mono\u00admorphic format for each value type, so we maintain com\u00adpatibility with the rest of the Scala \ncode. This decision led to another problem: any array access requires a call to the miniboxing runtime \nsupport which performs a dispatch on the type byte. Depending on the type byte s value, the ar\u00adray is \ncast to its correct type and the corresponding bytecode instruction for accessing it is used. This is \nfollowed by the encoding operation, which converts the read value to a long integer. The following snippet \nshows the array read opera\u00adtion implemented in the miniboxing runtime support code: def array_get[T](array: \nArray[T], idx: Int, tag: Byte): Minibox = tag match { case INT => array.asInstanceOf[Array[Int]](idx).toLong \ncase LONG => array.asInstanceOf[Array[Long]](idx) case DOUBLE => Double.doubleToRawLongBits( array.asInstanceOf[Array[Double]](idx)).toLong \n... } The most complicated and time-consuming part of our work involved rewriting the miniboxing runtime \nsupport to match the performance of specialized code. The next sub\u00adsections present the HotSpot Java \nVirtual Machine execu\u00adtion (\u00a75.1), the main benchmark we used for testing (\u00a75.2) and two implementations \nfor the runtime support: type byte switching (\u00a75.3) and object-oriented dispatching (\u00a75.4). 5.1 HotSpot \nExecution We used benchmarks to guide our implementation of the miniboxing runtime support. In this section \nwe will brie.y present the just-in-time compilation and optimization me\u00adchanisms in the HotSpot Java \nVirtual Machine [21, 32], since they directly in.uenced our design decisions. Although the work is based \non the HotSpot Java Virtual Machine, we high\u00adlight the underlying mechanisms that interfere with minibox\u00ading, \nin hope that our work can be used as the starting point for the analysis on different virtual machines. \nThe HotSpot Java Virtual Machine starts off by interpret\u00ading bytecode. After several executions, a method \nis consid\u00adered hot and the just-in-time compiler is called in to trans\u00adform it into native code. During \ncompilation, aggressive in\u00adlining is done recursively on all the methods that have been both executed \nenough times and are small enough. Typical inlining requirements for the C25 (server) just-in-time com\u00adpiler \nare 10000 executions and size below 35 bytes. When inlining static calls, the code is inlined directly. \nFor virtual and interface calls, however, the code depends on the receiver. To learn which code to inline, \nthe virtual machine will pro.le receiver types during the interpretation phase. Then, if a single receiver \nis seen at runtime, the compiler will inline the method body from that receiver. This inlining may later \nbecome incorrect, if a different class is used as the receiver. For such a case the compiler inserts \na guard: if the runtime type is not the one expected, it jumps back to interpretation mode. The bytecode \nmay be compiled again later if it runs enough times, with both possible method bodies inlined. But if \na third runtime receiver is seen, the call site is marked as megamorphic and inlining is not performed \nanymore, not even for the previous two method bodies. After inlining as much code as feasible, the virtual \nma\u00adchine s just-in-time compiler applies optimizations, which signi.cantly reduce the running time, especially \nfor array operations which are very regular and for which bounds checks can be eliminated. 5We did not \nuse tiered compilation.  Single Context Multi Context generic 20.4 21.5 miniboxed, no inlining 34.5 \n34.4 miniboxed, full switch 2.4 15.1 miniboxed, semi-switch 2.4 17.2 miniboxed, decision tree 24.2 24.1 \nminiboxed, linear 24.3 22.9 miniboxed, dispatcher 2.1 26.4 specialized 2.0 2.4 monomorphic 2.1 N/A Table \n1. The time in milliseconds necessary for reversing an array buffer of 3 million integers. Performance \nvaries based on how many value types have been used before (Single Context vs. Multi Context).  5.2 \nBenchmark We benchmarked the performance on the two examples pre\u00adviously shown in the paper, ListNode \nand ArrayBuffer. Throughout benchmarking, one particular method stood out as the most sensitive to the \nruntime support implementation: the reverse method of the ArrayBuffer class. The rest of this section \nuses the reverse method to explore the perfor\u00admance of different implementations of the runtime support: \ndef reverse_M(T_Type_local: Byte): Unit = { var idx = 0 val xdi = elemCount -1 while (idx < xdi) { val \nel1: Long = getElement_M(T_Type, idx) val el2: Long = getElement_M(T_Type, xdi) setElement_M(T_Type, \nidx, el2) setElement_M(T_Type, xdi, el1) idx += 1 xdi -= 1 } } The running times presented in table \n1 correspond to re\u00adversing an integer array buffer of 3 million elements. To put things into perspective, \nalong with different designs, the ta\u00adble also provides running times for monomorphic code (spe\u00adcialized \nby hand), specialization-annotated code and generic code. Measurements are taken in two scenarios: For \nSingle Context , an array buffer of integers is created and populated and its reverse method is benchmarked. \nIn Multi Con\u00adtext , the array buffer is instantiated, populated and reversed for all primitive value \ntypes .rst. Then, a new array buffer of integers is created, populated and its reverse method is benchmarked. \nThe HotSpot Java Virtual Machine optimiza\u00adtions are in.uenced by the historical paths executed in the \nprogram, so using other type arguments can have a drastic impact on performance, as can be seen from \nthe table, where the times for Single Context and Multi Context are very different: this means the virtual \nmachine gives up some of its optimizations after seeing multiple instantiations with differ\u00adent type \narguments. Multi Context is the likely scenario a library class will be in, as multiple instantiations \nwith differ\u00adent type arguments may be created during execution.  5.3 Type Byte Switching The .rst approach \nwe tried, the simple switch on the type byte, quickly revealed a problem: The array runtime sup\u00adport \nmethods were too large for the just in time compiler to inline at runtime. This corresponds to the miniboxing, \nno inlining in table 1. To solve this problem, we tasked the Scala compiler with inlining runtime support \nmethods in its backend, independently of the virtual machine. But this was not enough: the reverse_M \nmethod calls getElement_M and setElement_M, which also became large after inlin\u00ading the runtime support, \nand were not inlined by the virtual machine. This required us to recursively mark methods for inlining \nbetween the runtime support and the .nal bench\u00admarked method. The forced inlining in the Scala backend \nproduced good results. The measurement, corresponding to the miniboxed, full switch row in the table, \nshows miniboxed code work\u00ading at almost the same speed as specialized and monomor\u00adphic code. This can \nbe explained by the loop unswitching optimization in the just-in-time compiler. With all the code inlined \nby the Scala backend, loop unswitching was able to hoist the type byte switch statement outside the while \nloop. It then duplicated the loop contents for each case in the switch, allowing array-speci.c optimizations \nto bring the running time close to monomorphic code. But using more primitive types as type arguments \ndimin\u00adished the bene.t. We tested the reverse operation in two situations, to check if the optimizations \nstill take place after we use it on array buffers with different type arguments. It is frequently the \ncase that the HotSpot Java Virtual Machine will compile a method with aggressive assumptions about which \npaths the execution may take. For the branches that are not taken, guards are left in place. Then, if \na guard is violated during execution, the native code is interrupted and the program continues in the \ninterpreter. The method may be compiled again later, if it is executed enough times to war\u00adrant compilation \nto native code. Still, upon recompilation, the path that was initially compiled to a stub now becomes \na legitimate path and may preclude some optimizations. We traced this problem to the .oating point encoding, \nspeci.\u00adcally the bit-exact conversion from .oating point numbers to integers, that, once executed, prevents \nloop unswitching. We tried different constructions for the miniboxing run\u00adtime support: splitting the \nmatch into two parts and having an if expression that would select one or the other ( semi\u00adswitch ), \ntransforming the switch into a decision tree ( de\u00adcision tree ) and using a linear set of 9 if statements \n( lin\u00adear ), all of which appear in table 1. These new designs ei\u00adther degraded in the multiple context \nscenario, or provided a bad baseline performance from the beginning. What s more, the fact that the runtime \nremembered the type arguments a class was historically instantiated with made the translation unusable \nin practice, since this history is not only in.uenced by code explicitly called before the benchmark, \nbut transi\u00adtively by all code executed since the virtual machine started.  5.4 Dispatching The results \nobtained with type byte switching showed that we were committing to a type too late in the execution: \nForced inlining had to carry our large methods that covered all types inside the benchmarked method, \nwhere the opti\u00admizer had to hoist the switch outside the loop: while (...) { val el1: Long = T_Type match \n{ ... } val el2: Long = T_Type match { ... } T_Type match { ... } T_Type match { ... } } Ideally, this \nswitch should be done as early as possible, even as soon as class instantiation. This can be done using \nan object-oriented approach: instead of passing a byte value during class instantiation and later switching \non it, we can pass objects which encode the runtime operations for a sin\u00adgle type, much like the where \nobjects in PolyJ [8]. We call this object the dispatcher. The dispatcher for each value type encodes \na common set of operations such as array get and set. For example, IntDispatcher encodes the operations \nfor integers: abstract class Dispatcher { def array_get[T](arr: Array[T], idx: Int): Long def array_update[T](arr: \nArray[T], idx: Int, elt: Long): Unit ... } object IntDispatcher extends Dispatcher { ... } Dispatcher \nobjects are passed to the miniboxed class dur\u00ading instantiation and have .nal semantics. In the reverse \nbenchmark, this would replace the type byte switches by method invocations, which could be inlined. Dispatchers \nmake forced inlining and loop unswitching redundant. With the .nal dispatcher .eld set at construction \ntime, the reverse_M inner loop body can have array access inlined and optimized: ( miniboxed, dispatcher \nin tables 1 and 2) // inlined getElement: val el1: Long = dispatcher.array_get(...) val el2: Long = \ndispatcher.array_get(...) // inlined setElement: dispatcher.array_update(...) dispatcher.array_update(...) \n Despite their clear advantages, in practice dispatchers can be used with at most two different value \ntypes. This happens because the HotSpot Java Virtual Machine inlines the dis\u00adpatcher code at the call \nsite and installs guards that check the object s runtime type. The inline cache works for two re\u00adceivers, \nbut if we try to swap the dispatcher a third time, the callsite becomes megamorphic. In the megamorphic \nstate, the array_get and array_set code is not inlined, hence the disappointing results for the Multi \nContext scenario. Interestingly, specialization performs equally well in both Single Context and Multi \nContext scenarios. The ex\u00adplanation lies in the bytecode duplication: each specialized Table 2. The time \nin milliseconds necessary for reversing an array buffer of 3 million integers. Miniboxing benchmarks \nran with the double factory mechanism and the load-time specialization are marked with LS. Single Context \nMulti Context generic 20.4 21.5 miniboxed, full switch 2.4 15.1 mb. full switch, LS 2.5 2.4 miniboxed, \ndispatcher 2.1 26.4 mb. dispatcher, LS 2.0 2.7 specialized 2.0 2.4 monomorphic 2.1 N/A class contains \na different body for the reverse method, and the pro.les for each method do not interact. Accordingly, \nthe results for integers are not in.uenced by the other value types used. This insight motivated the \nload-time cloning and specialization, which is described in the next section. 6. Miniboxing Load-time \nOptimization The miniboxing runtime support, in both incarnations, us\u00ading switching and dispatching, \nfails to deliver performance in the Multi Context scenario. The reason, in both cases, is that execution \ntakes multiple paths through the code and this prevents the Java Virtual Machine from optimizing. There\u00adfore \nan obvious solution is to duplicate the class bytecode, but instead of duplicating it on the disk, as \nspecialization does, we do it in memory, on-demand and at load-time. The .NET Common Language Runtime \n[5, 20] performs on\u00addemand specialization at load-time, but it does so using more complex transformations \nencoded in the virtual machine. In\u00adstead, we use Java s classloading mechanism. We use a custom classloader \nto clone and specialize miniboxed classes. Similar to the approach in Pizza [29], the classloader takes \nthe name of a class that embeds the type byte value. For example, ListNode_I corresponds to a clone of \nListNode_M with the type byte set to INT. From the name, the classloader infers the miniboxed class name \nand loads it from the classpath. It clones its bytecode and adjusts the constant table [11]. All this \nis done in-memory. Once the bytecode is cloned, the paths taken through the inlined runtime support in \neach class remain .xed during its lifetime, making the performance in Single Context and Multi Context \ncomparable, as can be seen in Table 2. The explanation is that the JVM sees different classes, with separate \ntype pro.les, for each primitive type. Aside from bytecode cloning, the classloader also per\u00adforms class \nspecialization: Replaces the type tag .elds by static .elds (as the class is already dedicated to a \ntype);  Uses constant propagation and dead code elimination to reduce each type tag switch down to a \nsingle case, which can be inlined by the virtual machine, thus eliminating the need for forced inlining; \n  Performs load-time rewiring, which is described in the next section. 6.1 Miniboxing Load-time Rewiring \nWhen rewiring, the miniboxing transformation follows the same rules set forth by specialization (\u00a72.3). \nLoad-time cloning introduces a new layer of rewiring, which needs to take the cloned classes into account. \nThe factory mecha\u00adnism we employ to instantiate cloned and specialized classes (\u00a76.2)is equivalent to \nthe instance rewiring in specialization. The two other rewiring steps in specialization are method rewiring \nand parent class rewiring. Fortunately method rewiring is done during compilation and since methods are \nnot modi.ed, there is no need to rewire them in the class\u00adloader. Parent classes, however, must be rewired \nat load-time to avoid performance degradation. Load-time parent rewiring allows classes to inherit and \nuse miniboxed methods while keeping type pro.les clean. If the parent rewiring is done only at compile-time, \nall classes extending ArrayBuffer_M share the same code for the reverse_M method. But since they may \nuse different type arguments when extending ArrayBuffer, they are back to the Multi Context scenario \nin table 1. To obtain good per\u00adformance, rewiring parent classes is done .rst at compile time, to the \nminiboxed variant of the class, and then at load\u00adtime, to the cloned and specialized class. The following \nsnip\u00adpet shows parent rewiring in the case of dispatcher objects: // user code: class IntBuff extends \nArrayBuffer[Int] // after compile-time rewiring: class IntBuff extends ArrayBuffer_M[Int](IntDispatcher) \n // after load-time rewiring: class IntBuff extends ArrayBuffer_I[Int](IntDispatcher) The load-time \nrewiring of parent classes requires all sub\u00adclasses with miniboxed parents to go through the class\u00adloader \ntransformation. This includes the classes extending miniboxed parents with static type arguments, such \nas the IntBuff class in the code snippet before. This incurs a .rst\u00adinstantiation overhead, which is \nan inconvenience especially for classes that are only used once, such as anonymous clo\u00adsures extending \nFunctionX. But not all classes make use of the miniboxing runtime for arrays, so we can devise an anno\u00adtation \nwhich hints to the compiler which classes need factory instantiation. This would only incur the cloning \nand special\u00adization overhead when the classes use arrays. The annotation could be automatically added \nby the compiler when a class uses array operations and propagated from parent classes to their children: \n@loadtimeSpec class ArrayBuffer[@minispec T] // IntBuff automatically inherits @loadtimeSpec class IntBuff \nextends ArrayBuffer[Int]  6.2 Ef.cient Instantiation Imposing the use of a global classloader is impossible \nin many practical applications. To allow miniboxing to work in such cases, we chose to perform the class \ninstantiation through a factory that loads a local specializing classloader, requests the cloning and \nspecialization of the miniboxed class and instantiates it via re.ection. We benchmarked the approach \nand it introduced signi.cant overhead, as instanti\u00adations using re.ection are very expensive. To counter \nthe cost of re.ective instantiation, we propose a double factory approach that uses a single re.ective \nin\u00adstantiation per cloned class. In this approach each cloned and specialized class has a corresponding \nfactory that instan\u00adtiates it using the new keyword. When instantiating a mini\u00adboxed class with a new \nset of type arguments, its correspond\u00ading factory is specialized by the classloader and instantiated \nvia re.ection. From that point on, any new instance is cre\u00adated by the factory, without the re.ective \ndelay. The follow\u00ading code snippet shows the specialized (or 2nd level) factory: // Factory interface \n abstract class ArrayBufferFactoryInterface { def newArrayBuffer_M[T: Manifest](disp: Dispatcher[T]): \nArrayBuffer[T] } // Factory instance, to be specialized // in the classloader class ArrayBufferFactoryInstance_M \nextends ArrayBufferFactoryInterface { def newArrayBuffer_M[T: Manifest](disp: Dispatcher[T]): ArrayBuffer[T] \n= new ArrayBuffer_M(disp) } 7. Evaluation This section presents the results obtained by the minibox\u00ading \ntransformation. It will .rst present the miniboxing com\u00adpiler plug-in and the miniboxing classloader \n(\u00a77.1). Next, it will present the benchmarking infrastructure (\u00a77.2) and the benchmark targets (\u00a77.3). \nFinally, it will present the results (\u00a77.4 -\u00a77.8) and draw conclusions (\u00a77.9). 7.1 Implementation The \nminiboxing plug-in adds a code transformation phase in the Scala compiler. Like specialization, the miniboxing \nphase is composed of two steps: transforming signatures and transforming trees. As the signatures are \nspecialized, meta\u00addata is stored on exactly how the trees need to be trans\u00adformed. This metadata later \nguides the tree transformation in duplicating and adapting the trees to obtain the miniboxed code. The \nduplication step reuses the infrastructure from specialization, with a second adaptation step which trans\u00adforms \nstorage from generic to miniboxed representation. The plugin performs several transformations: Code duplication \nand adaptation, where values of type T are replaced by long integers and are un-miniboxed back to T at \nuse sites (\u00a74.2.1);  Rewiring methods like toString, hashCode, equals and array operations to use the \nruntime support (\u00a74.3);  Opportunistic rewiring: new instance creation, special\u00adized parent classes \nand method invocations (\u00a72.3);  Peephole minibox/un-minibox reduction (\u00a74.2.3).  The miniboxing classloader \nduplicates classes and per\u00adforms the specialized class rewiring. It uses transformations from an experimental \nScala backend to perform constant propagation and dead code elimination in order to remove switches on \nthe type byte. It supports miniboxed classes gen\u00aderated by the current plug-in and in the current release \nonly works for a single specialized type parameter. Also, the in\u00adfrastructure for the double factory \ninstantiation was written and tuned by hand, and may be integrated in the plug-in in a future release. \nWe did not implement the @loadtimeSpec annotation yet. The project also contains code for testing the \nplug-in and the classloader and performing microbenchmarks, some\u00adthing which turned out to be more dif.cult \nthan expected.  7.2 Benchmarking Infrastructure The miniboxing plug-in produces bytecode which is then \nexecuted by the HotSpot Java Virtual Machine. Although the virtual machine provides useful services to \nthe running program, such as compilation, deoptimization and garbage collection, these operations in.uence \nour microbenchmarks by delaying or even changing the benchmarked code alto\u00adgether. Furthermore, the non-deterministic \nnature of such events make proper benchmarking harder [15]. In order to have reliable results for our \nmicrobenchmarks, we used ScalaMeter [33], a tool speci.cally designed to re\u00adduce benchmarking noise. \nScalaMeter is currently used in performance-testing the Scala standard library. When bench\u00admarking, it \nforks a new virtual machine such that fresh code caches and type pro.les are created. It then warms up \nthe benchmarked code until the virtual machine com\u00adpiles it down to native code using the C2 (server) \n[32] com\u00adpiler. When the code has been compiled and the benchmark reaches a steady state, ScalaMeter \nmeasures several exe\u00adcution runs. The process is repeated several times, 100 in our case, reducing the \nbenchmark noise. For the report, we present the average of the measurements performed. We ran the benchmarks \non an 8-core i7 machine running at 3.40GHz with 16GB of RAM memory. The machine ran a 64 bit version \nof Linux Ubuntu 12.04.2. For the Java Virtual Machine we used the Oracle Java SE Runtime Environment \nbuild 1.7.0_11 using the C2 (server)compiler. The following section will describe the benchmarks we ran. \n 7.3 Benchmark Targets We executed the benchmarks in two scenarios: Single Context corresponds to the \nbenchmark target (ArrayBuffer or ListNode) executed with a single value type, Int; Multi Context corresponds \nto running the benchmark for all value types and only then measuring the execution time for the target \nvalue type, Int; The benchmarks were executed with 7 transformations: generic: the generic version of \nthe code, uses boxing;  mb. switch: miniboxed, using the type byte switching;  mb. dispatcher: miniboxed, \ndispatcher runtime support;  mb. switch + LS: miniboxed, type byte switching, load\u00adtime specialization \nwith the double factory mechanism;  mb. dispatcher + LS: miniboxed, dispatcher, load-time specialization \nwith the double factory mechanism;  specialized: code transformed by specialization;  monomorphic: \ncode specialized by hand, which does not need the redirects generated by specialization.  For the benchmarks, \nwe used the two classes presented in the previous sections: The ArrayBuffer class simulates collections \nand algorithms which make heavy use of bulk storage and the ListNode class simulates collections which \nrequire random heap access. We chose the benchmark meth\u00adods such that each tested a certain feature of \nthe miniboxing transformation. We used very small methods such that any slowdowns can easily be attributed \nto bytecode or can be di\u00adagnosed in a debug build of the virtual machine, using the compilation and deoptimization \noutputs. ArrayBuffer.append creates a new array buffer and appends 3 million elements to it. This benchmark \ntests the array writing operations in isolation, such that they cannot be grouped together and optimized. \nArrayBuffer.reverse reverses a 3 million element array buffer. This benchmark proved the most dif.cult \nin terms of matching the monomorphic code performance. ArrayBuffer.contains checks for the existence \nof elements inside an initialized array buffer. It exercises the equals method rewiring and revealed \nto us that the initial transformation for equals was suboptimal, as we were not using the information \nthat two miniboxed values were of the same type. This benchmark showed a 22x speedup over generic code. \nList construction builds a 3 million element linked list us\u00ading ListNode instances. This benchmark veri.es \nthe speed of miniboxed class instantiation. It was heavily slowed down by the re.ective instantiation, \ntherefore we introduced the double factory for class instantiation using the classloader. List.hashCode \ncomputes the hash code of a list of 3 million elements. We used this benchmark to check the per\u00adformance \nof the hashCode rewiring. It was a surprise to see the hashCode performance for generic code running \nin the interpreter (Table 4). It is almost one order of magnitude faster than specialized code and 5 \ntimes faster than mini\u00adboxing. The explanation is that computing the hash code requires boxing and calling \nthe hashCode method on the boxed object. When the benchmarks are compiled and opti\u00admized, this is avoided \nby inlining and escape analysis, but in the interpreter, the actual object allocation and call to hashCode \ndo happen, making the heterogeneous translation slower.  List.contains tests whether a list contains \nan element, repeated for 3 million elements. It tests random heap access and the performance of the equals \noperator rewiring.  7.4 Benchmark Results Table 3 presents the main results of our benchmarks. The ta\u00adble \nhighlights mb. switch + LS and mb. dispatch + LS , which represent the miniboxing encoding using the \nload\u00adtime specialization invoked with the double factory mech\u00adanism. The miniboxing encoding based on \ntype tag switching, mb. switch + LS , offers steady performance close to that of specialization and monomorphic \ncode, with slowdowns ranging between 0 and 20 percent. The classloader spe\u00adcialization, coupled with \nconstant propagation and dead code elimination, make the type tag switching approach the most stable \nacross multiple executions with differ\u00adent type arguments, with at most 6 percent difference be\u00adtween \nSingle Context and Multi Context , in the case of ArrayBuffer.append. The dispatcher-based encoding, \nmb. dispatch + LS , also offers performance close to specialization and mono\u00admorphic code, with slightly \nbetter performance when travers\u00ading the linked list (benchmarks hashCode and contains), and a lower performance \non List creation. This suggests that passing the dispatcher object on the stack is more ex\u00adpensive than \npassing a type tag. It is worth noting that the dispatcher-based implementa\u00adtion relies on inlining performed \nby the just-in-time com\u00adpiler. Although the load-time cloning mechanism ensures type pro.les remain monomorphic, \nthe burden of inlining falls on the just-in-time compiler. In the case of virtual ma\u00adchines that perform \nahead-of-time compilation, such as Ex-Table 4. Running time for the benchmarks in the HotSpot Java Virtual \nMachine interpreter. The time is measured in seconds as instead of milliseconds as in the other tables. \nSingle context and Multi context have similar results. ArrayBuffer List generic 4.6 2.2 367.0 1.4 0.2 \n16.6 mb. switch + LS 1.6 0.3 25.0 0.8 1.3 4.2 mb. dispatch + LS 2.5 0.7 88.9 1.1 1.5 7.3 specialization \n4.3 0.5 30.7 0.6 1.9 2.2 monomorphic 1.0 0.2 12.7 0.4 1.2 2.2 celsior JET [24], the newly specialized \nclass is compiled to native code without interpretation, thus no type pro.les are available and no inlining \ntakes place for the minibox\u00ading runtime. In contrast to dispatching, type tag switching only requires \nloading-time constant propagation and dead code elimination to remove the overhead of the miniboxing \nruntime. This makes it a better candidate for robust perfor\u00admance across different virtual machines. \nThe next section will present interpreter benchmarks.  7.5 Interpreter Benchmarks Before compiling the \nbytecode to native machine code, the HotSpot Virtual Machine interprets it and gathers pro.les that later \nguide compilation. Table 4 presents results for run\u00adning the same set of benchmarks in the interpreter, \nwithout compilation. It is important that transformations do not vis\u00adibly degrade performance in the \ninterpreter, as this slows down application startup. The data highlights a steady be\u00adhavior for the the \ntype tag switching, while the dispatcher\u00adbased approach suffers from up to 4x slowdowns. The data shows \na consistent slowdown of the tag switch\u00ading approach compared to the monomorphic code in 4 of the 6 experiments. \nThis can most likely be attributed to ArrayBuffer.append ArrayBuffer.reverse ArrayBuffer.contains Single \nContext Multi Context Single Context Multi Context Single Context Multi Context generic 50.1 48.0 20.4 \n21.5 1580.1 3628.8 mb. switch 30.9 35.5 2.5 15.1 161.5 554.3 mb. dispatch 16.5 58.2 2.1 26.5 160.7 2551.6 \nmb. switch + LS 15.6 14.8 2.5 2.4 159.9 161.7 mb. dispatch + LS 15.1 15.9 2.0 2.7 161.8 161.3 specialization \n39.7 38.5 2.0 2.4 155.8 156.3 monomorphic 16.2 N/A List creation 2.1 N/A List.hashCode 157.7 N/A List.contains \nSingle Context Multi Context Single Context Multi Context Single Context Multi Context generic 16.7 1841 \n22.1 20.4 1739.5 2472.4 mb. switch 11.4 11.7 18.3 18.8 1438.2 1443.2 mb. dispatch 11.4 11.5 15.6 21.0 \n1369.1 1753.2 mb. switch + LS 11.5 11.6 16.2 16.1 1434.9 1446.3 mb. dispatch + LS 12.1 12.7 16.1 15.3 \n1364.2 1325.9 specialization 11.4 11.4 14.5 36.4 1341.0 1359.2 monomorphic 10.2 N/A 13.3 N/A 1172.0 N/A \n Table 3. Benchmark running times. The benchmarking setup is presented in \u00a77.2 and the targets are presented \nin \u00a77.3. The time is measured in milliseconds. erasure dispatch switch spec. ArrayBuffer ArrayBuffer \nfactory ListNode ListNode factory 4.4 3.1 19.5 + 9.0 10.9 + 8.7 24.5 + 8.5 11.5 + 8.3 57.6 45.0 Table \n5. Bytecode generated by different translations, in kilobytes. Factories add extra bytecode for the double \nfac\u00adtory mechanism. spec. stands for specialization. the mechanism for invoking object methods, which \nrequires loading a reference to the module from a static .eld and then performing a method call. Even \nafter the method call is in\u00adlined, the Scala backend (and the load-time specializer) do not remove the \nstatic .eld access, thus leaving the redundant but possibly side-effecting instruction in the hot loop. \nIn the native code the .eld access is compiled away by the just-in\u00adtime compiler. This could be improved \nin the Scala backend.  7.6 Bytecode Size Table 5 presents the bytecode generated for ArrayBuffer and \nListNode by 4 transformations: erasure, miniboxing with dispatcher, miniboxing with switching and specializa\u00adtion. \nThe fraction of bytecode created by miniboxing, when compared to specialization, lies between 0.2x to \n0.4x. This is marginally better than the fraction we expected, 0.4x, which corresponds to 4n/10n for \nn = 1. The reason the fraction is 4n/10n instead of 2n/10n is explained in \u00a74.1. The double factory mechanism \nadds a signi.cant bytecode, in the order of 10 kilobytes per class. In order to evaluate the bene.ts \nof using the mini\u00adboxing encoding for real-world software, we developed a specialization-hijacking mode, \nwhere specialization was turned off and all @specialized notations were treated as @minispec, thus triggering \nminiboxing on all methods and classes where specialization was used. For this benchmark we only used \nthe switching-based transformation. The .rst evaluation was performed on Spire [31], a Scala library \nproviding abstractions for numeric types, ranging from boolean algebras to complex number algorithms. \nSpire is the one library in the Scala community which uses special\u00adization the most, and the project \nowner, Erik Osheim, con\u00adtributed numerous bug .xes and enhancements to the Scala compiler in the area \nof specialization. The results, presented in Table 6, show a bytecode reduction of 2.8x and a 1.4x, or \n40%, reduction in the number of specialized classes. The two reductions are not proportional because \nspecialized methods in.ate the code size of classes, but do not increase the class count. The bytecode \nreduction is limited to 2.8x because spe\u00adcialization is used in a directed manner, pointing exactly to \nthe value types which should be specialized. So, instead of generating 10 classes per type parameter, \nit only generates the necessary value types. Nevertheless, even starting from manually directed specialization, \nthe miniboxing transfor\u00admation is able to further reduce the bytecode size. The second evaluation, shown \nin Table 7, is motivated by a common complaint in the Scala community: that the collections in the standard \nlibrary should be specialized. To perform an evaluation on collections, we sliced a part of the library \naround the Vector class and examined the impact of using the specialization and miniboxing transformations. \nOn the approximately 64 Scala classes, traits and objects included in our slice, the bytecode reduction \nobtained by miniboxing compared to specialization is 4.7x. Compared to the generic Vector, the miniboxing \ncode growth is 1.7x, opposed to almost 8x for specialization.  7.7 Load-time Specialization Overhead \nIn this section we will evaluate the overhead of the double factory mechanism. There are three types \nof overhead in\u00advolved: Bytecode overhead, shown in the previous section;  Time spent specializing and \nloading a class;  Heap overhead for the classloader and factory. We will further explore the last two \nsources of overhead.  7.7.1 Time Spent Specializing Table 3, in the List creation column, shows the \nover\u00adhead of the double factory mechanism and class special\u00adization is not statistically noticeable after \nthe mechanism is warmed up. Nevertheless, it is important to understand how the mechanism behaves during \na cold start, as this directly impacts an application s startup time. In this subsection we will examine \nthe overhead for a cold start, coming from two different sources: The runtime class specialization; \n The cold start of the double factory mechanism.  The evaluation checks the two overheads separately: \nin the .rst experiment we only load the classes (using Class.forName) to trigger the runtime class specialization, \nwhile in the second experiment we instantiate the classes, ei\u00adther directly, using the new operator or \nthrough the double bytecode size (KB) classes Spire -specialized (current) 13476 2545 Spire -miniboxed \n4820 1807 Spire -generic 3936 1530 bytecode size (KB) classes Vector -specialized 5691 1434 Vector -miniboxed \n1210 435 Vector -generic (current) 715 223  Table 6. Bytecode generated by using specialization, mini-Table \n7. Bytecode generated by using specialization, mini\u00adboxing and leaving generic code in the Spire numeric \nab-boxing and leaving generic code on the Scala collection li\u00adstractions library. brary slice around \nVector. time in ms classes classpath -just load 182 \u00b1 5 9\u00d7 25 = 225 classloader -warmed up 300 \u00b1 4 225 \nclassloader -cold start 461 \u00b1 9 225 Table 9. Loading time (classpath) and time for cloning and specialization \n(classloader) for the 9 specialized variants of Vector and their transitive dependencies. time in ms \nclasses classpath -new 258 \u00b1 5 9\u00d7 42 = 378 classpath -factory 268 \u00b1 6 378 classloader -factory -warm \n488 \u00b110 378 classloader -factory -cold 655 \u00b1 9 378 Table 10. Instantiation time for the 9 specialized \nvariants of Vector and their transitive dependencies. factory mechanism. In order to evaluate the class \nspecializa\u00adtion, we instrumented the specializing classloader to dump the resulting class .les, such \nthat we can compare the spe\u00adcializing classloader to simply loading the specialized vari\u00adants from the \nclasspath. For the comparison, we use the Vector class described in the previous section. The Vector \nclass mixes in 36 traits [28] which are translated by the Scala compiler as transi\u00adtive dependencies \nof the class. In our experiments, loading the Vector class using Class.forName transitively loaded another \n24 specialized classes for each variant. Instantiating a vector using new further loads another 18 classes, \nmainly specialized trait implementations and internal classes, lead\u00ading to a total of 42 classes loaded \nwith each specialized vari\u00adant of Vector. In each experiment we start the virtual machine, start counting \nthe time, load or instantiate Vector for all 9 value types in Scala, output the elapsed time and exit. \nOnce a class is loaded, its internal representation in the virtual machine remains cached until its classloader \nis garbage collected. In order to perform correct benchmarks, we chose to use a virtual machine to load \nthe 9 specialized variants of Vector only once, and then restart the virtual machine. We repeated the \nprocess 100 times for each measurement. The .rst experiment involves loading the class: this can be done \neither by using the specializing classloader to instan\u00adtiate a template or by loading the class .le dumped \nfrom a previous specialization run. We observed a signi.cant differ\u00adence between cold starting the specializing \nclassloader and warming it up on a different set of classes. This is shown in Table 9: cold starting \nthe specialization classloader incurs a slowdown of 153% while warming it up before leads to a 65% slowdown \nin class loading time. The second experiment involves instantiating the class, either directly (using \nthe new operator)or through the double factory mechanism. Table 10 presents the results. The sur\u00adprising \nresult of this experiment is that the overhead caused by the double factory mechanism is under 4%. As \nbefore, most of the time is spent specializing the template to produce the specialized class, which, \ndepending on whether the class\u00adloader was used before, can lead to a slowdown between 84% and 144%. It \nis important to point out this overhead is a one-time cost, and further instantiations of the specialized \nvariants take on the order of tens of milliseconds.  7.7.2 Heap Overhead In this section we will attempt \nto bound the heap usage of the double factory mechanism. The double factory mechanism consists of a .rst \nlevel factory, which uses re.ection to create second level factories, which, in turn, use the new operator \nto instantiate load-time specialized classes. This mechanism ArrayBuffer.append ArrayBuffer.reverse \nArrayBuffer.contains Single Context Multi Context Single Context Multi Context Single Context Multi Context \ngeneric 78.3 52.3 3.2 20.3 607.6 3146.1 mb. switch 27.6 \u00d7 7.4 \u00d7 844.4 \u00d7 mb. dispatch 27.0 34.8 3.2 10.8 \n844.7 962.7 mb. switch + LS 22.2 14.3 3.8 2.9 725.4 725.2 mb. dispatch + LS 32.9 26.4 3.4 4.0 844.6 845.3 \nspecialization 21.7 13.4 3.5 2.7 488.7 489.4 monomorphic 19.8 N/A List creation 3.1 N/A List.hashCode \n490.4 N/A List.contains Single Context Multi Context Single Context Multi Context Single Context Multi \nContext generic 32.6 23.3 13.4 13.6 1846.5 2168.1 mb. switch 23.7 18.0 11.7 10.9 1420.8 1421.5 mb. dispatch \n20.9 18.3 12.4 11.4 1359.3 1427.5 mb. switch + LS 23.2 17.1 12.2 10.5 1414.8 1459.4 mb. dispatch + LS \n25.0 18.3 12.1 10.5 1390.6 1402.9 specializare 21.7 16.9 12.4 10.6 1463.5 1459.8 monomorphic 19.6 N/A \n11.7 N/A 1249.2 N/A Table 8. Running times on the Graal Virtual Machine. \u00d7 marks benchmarks for which \nthe bytecode generated crashed the Graal just-in-time compiler. The time is measured in milliseconds. \n was imposed in order to avoid the cost of re.ection-based in\u00adstantiation, which we found to be more \nexpensive in terms of overhead. Each second level factory corresponds to a set of pre-determined type \ntags, thus instantiating two specialized variants will require two separate second level factories. The \n.rst level factory mechanism keeps a cache of 10n references pointing to second level factories, which \nis ini\u00adtially empty and .lls up as the different variants are created. The second level factories are \ncompletely stateless and only offer a method for each specialized class constructor. There\u00adfore the maximum \nheap consumption, for a 64 bit system running the HotSpot Virtual Machine, would be 16 bytes for each \nsecond level factory and 8 bytes for its cached refer\u00adence, all times 10n, assuming all variants are \nloaded. This means a total of 24 \u00d7 10n bytes of storage. For a class with a single type parameter, this \nwould mean a heap overhead in the order of hundreds of bytes. Assuming all of spire s specialized classes \nused arrays and required the two factory mechanism, since most take a single type parameter, it would \nmean a heap overhead in the order of tens of kilobytes. However a hidden overhead is also present, consisting \nof the internal class representations for the second level fac\u00adtories inside the virtual machine. To \nbound this overhead, we can compare the factories to the classes themselves: for each specialized variant \nof the class there will be a special\u00adized factory, with a method corresponding to each construc\u00adtor of \nthe class. The factory will therefore always have a strictly smaller internal representation than the \nspecialized class, leading to at most a doubling of the internal class rep\u00adresentation in the virtual \nmachine.  7.8 Extending to Other Virtual Machines In order to asses whether the miniboxing runtime \nsystem provides good performance on other virtual machines, we have evaluated it on Graal [30]. The Graal \nVirtual Machine consists of the same interpreter as the HotSpot Virtual Ma\u00adchine but a completely rewritten \njust-in-time compiler. Since the interpreter is the same, the same type pro.les and hot\u00adness information \nis recorded, but the code is compiled using different transformations and heuristics. The results in \nTa\u00adble 8 exhibit both a much lower variability but also a lower peak performance compared to the C2 compiler \nin HotSpot (in Table 3). With the single exception of ArrayBuffer s contains benchmark, the switching \nruntime support with class loading behaves similarly to specialized code.  7.9 Evaluation Remarks After \nanalyzing the benchmarking results, we believe the miniboxing transformation with type byte switching \nand classloader duplication provides the most stable results and ful.lls our initial goal of providing \nan alternative encod\u00ading for specialization, which produces less bytecode without sacri.cing performance. \nUsing the classloader for duplica\u00adtion and switch elimination, the type byte switching does not require \nforced inlining, making the transformation work without any inlining support from the Scala compiler. \n8. Related Work The work by Sallenave and Ducournau [35] shares the same goals as miniboxing: offering \nunboxed generics without the bytecode explosion. However, the target is different: their Lightweight \nGenerics compiler targets embedded devices and works under a closed world assumption. This allows the \ncompiler to statically analyze the .NET bytecode and con\u00adservatively approximate which generic classes \nwill be in\u00adstantiated at runtime and the type arguments that will be used. This information is used to \nstatically instantiate only the specialized variants that may be used by the program. To further reduce \nthe bytecode size, instantiations are ag\u00adgregated together into three base representations: ref, word \nand dword. This signi.cantly reduces the bytecode size and does not require runtime specialization. At \nthe opposite side of the spectrum, miniboxing works under an open-world as\u00adsumption, and inherits the \nopportunistic and compatible na\u00adture from specialization, which enables it to work under era\u00adsure [10], \nwithout the need for runtime type information. In\u00adstead, type bytes are a lightweight and simple mechanism \nto dispatch operations for encoded value types. According to Morrison et al [27] there are three types \nof polymorphism: textual polymorphism, which corresponds to the heterogeneous translation, uniform polymorphism \nwhich corresponds to the homogeneous translation and tagged polymorphism which creates uniform machine \ncode that can handle non-uniform store representations. In the compiler they develop for the Napier88 \nlanguage, the generated code uses a tagged polymorphism approach with out-of-band sig\u00adnaling, meaning \nthe type information is not encoded in the values themselves but passed as separate values. Their en\u00adcoding \nscheme accommodates surprisingly diverse values: primitives, data structures and abstract types. As opposed \nto the Napier88 compiler, the miniboxing transformation is restricted to primitives. Nevertheless, it \ncan optimize more using the runtime specialization approach, which eliminates the overhead of tagging. \nFurthermore, the miniboxing run\u00adtime support allows the Java Virtual Machine to aggressively optimize \narray instructions, which makes bulk storage oper\u00adations orders of magnitude faster. The initial runtime \nsupport implementations presented in \u00a75 show that it is not possible to have these optimizations in a \npurely compiler-level ap\u00adproach, at least not on the current incarnation of the HotSpot Java Virtual \nMachine. Fixnums in Lisp [44] reserve bits for encoding the type. For example, an implementation may \nuse a 32-bit slot to en\u00adcode both the type, on the .rst 5 bits, and the value, on the last 27 bits. We \ncall this in-band type signaling, as the type is encoded in the same memory slot as the value. Although \nvery ef.cient in terms of space, the .xnum representation has two drawbacks that we avoid in the miniboxing \nencod\u00ading: the ranges of integers and .oating point numbers are re\u00adstricted to only 27 bits, and each \noperation needs to unpack the type, dispatch the correct routine and pack the value back with its type. \nThis requires a non-negligible amount of work for each operation. Out-of-band types are used in Lua [17], \nwhere they are implemented using tagged unions in C. Two differences set miniboxing apart: .rst, .xnums \nand tagged unions are used in homogeneous translations, whereas the miniboxing technique simpli.es heterogeneous \ntranslations. Secondly, miniboxing leverages static type information to eliminate redundant type tags \nthat would be stored in tagged unions. For example, miniboxing uses the static type infor\u00admation that \nall values in an array are of the same type: in such a case, keeping a tag for each element, as would \nbe done with tagged unions, becomes redundant. Therefore, we con\u00adsider miniboxing to be an encoding applicable \nto strongly typed languages, which reduces the bytecode size of hetero\u00adgeneous translations, whereas \n.xnums and tagged unions are encodings best applied to dynamically typed languages and homogeneous translations. \n The .NET Common Language Runtime [5, 20] was a great inspiration for the specializing classloader. \nIt stores generic templates in the bytecode, and instantiates them in the virtual machine for each type \nargument used. Two fea\u00adtures are crucial in enabling this: the global presence of rei\u00ad.ed types and the \ninstantiation mechanism in the virtual ma\u00adchine. Contrarily, the Java Virtual Machine does not store \nrepresentations of the type arguments at runtime [10] and re-introducing them globally is very costly \n[37]. Therefore, miniboxing needs to inherit the opportunistic behavior from specialization. On the other \nhand, the classloading mecha\u00adnism for template instantiation at runtime is very basic, and not really \nsuited to our needs: it is both slow, since it uses re\u00ad.ection, and does not allow us to modify code \nthat is already loaded from the classpath. Consequently we were forced to impose the double factory mechanism \nfor all classes that ex\u00adtend or mix-in miniboxed parents, creating redundant boil\u00aderplate code, imposing \na one-time overhead for class instan\u00adtiation and increasing the heap requirements. The Pizza generics \nsupport [29] inspired us in the use of traits as the base of the specialized hierarchy, also offer\u00ading \ninsights into how class loading can be used to specialize code. The mechanism employed by the classloader \nto sup\u00adport arrays is based on annotations, which mark the bytecode instructions that need to be patched \nto allow reading an array in conformance with its runtime type. In our case there is no need for patching \nthe bytecode instructions, as miniboxing goes the other way around: it includes all the code variants \nin the class and then performs a simple constant propagation and dead code elimination to only keep the \nright instruction. Miniboxing also introduces the double factory mechanism, which pays the re.ective \ninstantiation overhead only once, instead of doing it on each class instantiation. The class gen\u00aderation \nfrom a template was .rst presented in the work of Agesen et al [6]. Around the same time as Pizza, there \nhas been signif\u00adicant research on supporting polymorphism in Java, lead\u00ading to work such as GJ [10], \nNextGen [12] and the poly\u00admorphism translation based on re.ective features of Viroli [43]. NextGen [7, \n12, 36] presents an approach where type parameter-speci.c operations are placed into snippet meth\u00adods, \nwhich are grouped in wrapper classes, one for each polymorphic instantiation. Wrapper classes, in turn, \nextend a base class which contains the common functionality inde\u00adpendent of the type parameters. It also \nimplements a gen\u00aderated interface which gives the subtyping relation between the specialized classes, \nalso supporting covariance and con\u00adtravariance for the type parameters. Taking this approach of grouping \ncommon functionality in base classes, as special\u00adization does, could reduce code duplication in miniboxed \nvariants, at the cost of duplicating all snippet methods from the parent in the children classes. Since \nthe collections hier\u00adarchy in Scala is up to 6 levels deep, the cost of duplicating the same snippet \nmethod 6 times outweighs the bene.t of reducing local duplication in each class. The dispatcher objects \nin miniboxing are specialized and restricted where clauses from PolyJ [8]. Since the methods that operate \non primitive values are .xed and known a priori, unlike PolyJ, we can use dispatcher objects and type \ntags without any change to the virtual machine. Nevertheless it is worth noting that our implementation \ndoes pay the price of carrying dispatcher objects in each instance, which PolyJ avoids by implementing \nvirtual machine support for invoking methods in where clauses. In the context of ML, Leroy presented \nthe idea of mixing boxed and unboxed representations of data and described the mechanism to introduce \ncoercions between the two when\u00adever execution passes from monomorphic to polymorphic code or back [22]. \nMiniboxing introduces similar coercions between the boxed and miniboxed representation, when\u00adever the \nexpected type is generic instead of miniboxed. The peephole optimization in miniboxing could be seen \nas a set of rules similar to the ones given by Jones et al in [19]. The work on passing explicit type \nrepresentations in ML [16, 25, 41, 42] can also be seen as the base of specialization and also miniboxing. \nHowever, since we control rewiring and do it in a conservative fashion, we only use the type tags available, \nthus miniboxing does not need any mechanism for type argument lifting. This paper has systematically \navoided the problem of name mangling, which has been discussed in the context of Scala [13] and more \nrecently of X10 [40]. Finally, minibox\u00ading is not limited to classes and methods, but could also be used \nto reduce bytecode in specialized translations of ran\u00addom code blocks in the program [39]. 9. Conclusions \nWe described miniboxing, an improved specialization trans\u00adformation in Scala, which signi.cantly reduces \nthe bytecode generated. Miniboxing consists of the basic encoding (\u00a73) and code transformation (\u00a74), \nthe runtime support (\u00a75) and the specializing classloader (\u00a76). Together, these techniques were able \nto approach the performance of monomorphic and specialized code and obtain speedups of up to 22x over \nthe homogeneous translation (\u00a77).  Acknowledgments The authors would like to thank Miguel Alfredo Garcia \nGutierrez for his remarks that sparked the idea of minibox\u00ading and for allowing early access to the experimental \nopti\u00admization phases from the new Scala 2.11 backend, which we used in the specializing classloader. \nIulian Dragos pro\u00advided invaluable help in understanding and reusing the spe\u00adcialization infrastructure \nalready existing in the Scala com\u00adpiler. We would also like to thank our anonymous review\u00aders for providing \nvery useful feedback that completely re\u00adshaped two sections of the paper. We are also grateful to Michel \nSchinz, Roland Ducournau, Lukas Rytz, Vera Salvis\u00adberg, Sandro Stucki and Hubert Plociniczak who provided \ndetailed reviews and helped us improve the paper. Last but not least, we are thankful to all the members \nof the Program\u00adming Languages Laboratory at EPFL (LAMP) and the Scala community, who provided a great \nenvironment for devel\u00adoping the miniboxing plugin, with passionate discussions, brainstorming sessions \nand a constant stream of good ques\u00adtions and quality feedback. This research was partially supported \nthrough the Euro\u00adpean Research Council (ERC) grant 587327 DOPPLER . References [1] Ceylon Programming \nLanguage. URL http://ceylon-lang.org/. [2] Kotlin Programming Language. URL http://kotlin.jetbrains.org/. \n[3] Scala Programming Language. URL http://scala-lang.org/. [4] X10 Programming Language. URL http://x10-lang.org/. \n[5] ECMA International, Standard ECMA-335: Common Language Infrastructure, June 2006. [6] O. Agesen, \nS. N. Freund, and J. C. Mitchell. Adding Type Parameterization to the Java Language. In ACM SIGPLAN Notices, \nvolume 32. ACM, 1997. [7] E. Allen, J. Bannet, and R. Cartwright. A First-Class Approach to Genericity. \nIn ACM SIGPLAN Notices, volume 38. ACM, 2003. [8] J. A. Bank, A. C. Myers, and B. Liskov. Parameterized \nTypes for Java. In Proceedings of the 24th ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages (POPL). ACM, 1997. [9] L. Bourdev and J. J\u00e4rvi. Ef.cient Run-Time Dispatching in Generic Program\u00adming \nwith Minimal Code Bloat. Sci. Comput. Program., 76(4), Apr. 2011. ISSN 0167-6423. [10] G. Bracha, M. \nOdersky, D. Stoutamire, and P. Wadler. Making the future safe for the past: Adding Genericity to the \nJava Programming Language. SIGPLAN Notices, 33(10), Oct. 1998. ISSN 0362-1340. [11] A. Buckley. JSR 202: \nJava Class File Speci.cation Update, 2006. URL http: //www.jcp.org/en/jsr/detail?id=202. [12] R. Cartwright \nand G. L. Steele Jr. Compatible Genericity with Run-time Types for the Java Programming Language. In \nACM SIGPLAN Notices, volume 33. ACM, 1998. [13] I. Dragos. Compiling Scala for Performance. PhD thesis, \n\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, 2010. [14] A. Gal, B. Eich, M. Shaver, D. Anderson, D. Mandelin, \nM. R. Haghighat, B. Kaplan, G. Hoare, B. Zbarsky, J. Orendorff, et al. Trace-based Just-in-Time Type \nSpecialization for Dynamic Languages. In ACM SIGPLAN Notices, volume 44. ACM, 2009. [15] A. Georges, \nD. Buytaert, and L. Eeckhout. Statistically Rigorous Java Perfor\u00admance Evaluation. In Proceedings of \nthe 22nd annual ACM SIGPLAN Confer\u00adence on Object-Oriented Programming Systems, Languages and Applications, \nOOPSLA 07, 2007. [16] R. Harper and G. Morrisett. Compiling Polymorphism Using Intensional Type Analysis. \nIn Proceedings of the 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. ACM, \n1995. [17] R. Ierusalimschy, L. H. De Figueiredo, and W. Celes. The Implementation of Lua 5.0. Journal \nof Universal Computer Science, 11(7), 2005. [18] Intel. Intel (R) 64 and IA-32 Architectures Software \nDeveloper s Manual. URL http://www.intel.com/content/www/us/en/processors/ architectures-software-developer-manuals.html. \n[19] S. L. P. Jones and J. Launchbury. Unboxed Values as First Class Citizens in a Non-Strict Functional \nLanguage. In Functional Programming Languages and Computer Architecture. Springer, 1991. [20] A. Kennedy \nand D. Syme. Design and Implementation of Generics for the .NET Common Language Runtime. In Proceedings \nof the ACM SIGPLAN 2001 Conference on Programming Language Design and implementation, PLDI 01, Snowbird, \nUtah, United States, 2001. [21] T. Kotzmann, C. Wimmer, H. M\u00f6ssenb\u00f6ck, T. Rodriguez, K. Russell, and \nD. Cox. Design of the Java HotSpot Client Compiler for Java 6. ACM Transactions on Architecture and Code \nOptimization (TACO), 5(1), 2008. [22] X. Leroy. Unboxed Objects and Polymorphic Typing. In Proceedings \nof the 19th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. ACM, 1992. [23] T. Lindholm \nand F. Yellin. Java Virtual Machine Speci.cation. Addison-Wesley Longman Publishing Co., Inc., 1999. \n[24] V. Mikheev, N. Lipsky, D. Gurchenkov, P. Pavlov, V. Sukharev, A. Markov, S. Kuksenko, S. Fedoseev, \nD. Leskov, and A. Yeryomin. Overview of Excelsior JET, a High Performance Alternative to Java Virtual \nMachines. In Proceedings of the 3rd International Workshop on Software and Performance (WOSP). ACM, 2002. \n[25] Y. Minamide. Full Lifting of Type Parameters. In Proceedings of Second Fuji International Workshop \non Functional and Logic Programming, 1997. [26] A. Moors. Type Constructor Polymorphism for Scala: Theory \nand Practice. PhD thesis, PhD thesis, Katholieke Universiteit Leuven, 2009. [27] R. Morrison, A. Dearle, \nR. C. H. Connor, and A. L. Brown. An Ad Hoc Approach to the Implementation of Polymorphism. ACM Transactions \non Programming Languages and Systems (TOPLAS), 13(3), 1991. [28] M. Odersky and M. Zenger. Scalable Component \nAbstractions. In ACM SIG-PLAN Notices, volume 40. ACM, 2005. [29] M. Odersky, E. Runne, and P. Wadler. \nTwo Ways to Bake Your Pizza-Translating Parameterised Types into Java. Springer, 2000. [30] Oracle. OpenJDK: \nGraal project. URL http://openjdk.java.net/ projects/graal/. [31] E. Osheim. Generic Numeric Programming \nThrough Specialized Type Classes. ScalaDays, 2012. [32] M. Paleczny, C. Vick, and C. Click. The Java \nHotSpot Server Compiler. In Proceedings of the 2001 Symposium on Java Virtual Machine Research and Technology \nSymposium-Volume 1. USENIX Association, 2001. [33] A. Prokopec. ScalaMeter. URL http://axel22.github.com/ \nscalameter/. [34] A. Prokopec, P. Bagwell, T. Rompf, and M. Odersky. A Generic Parallel Collection Framework. \nIn Euro-Par 2011 Parallel Processing. Springer, 2011. [35] O. Sallenave and R. Ducournau. Lightweight \nGenerics in Embedded Systems Through Static Analysis. SIGPLAN Notices, 47(5), June 2012. [36] J. Sasitorn \nand R. Cartwright. Ef.cient First-Class Generics on Stock Java Virtual Machines. In Proceedings of the \n2006 ACM symposium on Applied computing. ACM, 2006. [37] M. Schinz. Compiling Scala for the Java Virtual \nMachine. PhD thesis, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne, 2005. [38] B. Stroustrup. The C++ Programming \nLanguage, Third Edition. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, 3rd edition, 1997. \n[39] N. Stucki and V. Ureche. Bridging islands of specialized code using macros and rei.ed types. In \nProceedings of the 4th Workshop on Scala, SCALA 13, 2013. [40] M. Takeuchi, S. Zakirov, K. Kawachiya, \nand T. Onodera. Fast Method Dispatch and Effective Use of Primitives for Rei.ed Generics in Managed X10. \nIn Proceedings of the 2012 ACM SIGPLAN X10 Workshop. ACM, 2012. [41] D. Tarditi, G. Morrisett, P. Cheng, \nC. Stone, R. Harper, and P. Lee. TIL: A Type-Directed Optimizing Compiler for ML. In Proceedings of the \nACM SIGPLAN 1996 Conference on Programming Language Design and Implementation, 1996. [42] A. Tolmach. \nTag-Free Garbage Collection Using Explicit Type Parameters. In ACM SIGPLAN Lisp Pointers, volume 7. ACM, \n1994. [43] M. Viroli and A. Natali. Parametric Polymorphism in Java: An Approach to Translation Based \non Re.ective Features. In ACM SIGPLAN Notices, volume 35. ACM, 2000. [44] S. Wholey and S. E. Fahlman. \nThe Design of an Instruction Set for Common Lisp. In Proceedings of the 1984 ACM Symposium on LISP and \nFunctional Programming, LFP 84, 1984. [45] T. W\u00fcrthinger, A. W\u00f6\u00df, L. Stadler, G. Duboscq, D. Simon, and \nC. Wimmer. Self-Optimizing AST interpreters. In Proceedings of the 8th Symposium on Dynamic Languages. \nACM, 2012.     \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Parametric polymorphism enables code reuse and type safety. Underneath the uniform interface exposed to programmers, however, its low level implementation has to cope with inherently non-uniform data: value types of different sizes and semantics (bytes, integers, floating point numbers) and reference types (pointers to heap objects). On the Java Virtual Machine, parametric polymorphism is currently translated to bytecode using two competing approaches: homogeneous and heterogeneous. Homogeneous translation requires boxing, and thus introduces indirect access delays. Heterogeneous translation duplicates and adapts code for each value type individually, producing more bytecode. Therefore bytecode speed and size are at odds with each other. This paper proposes a novel translation that significantly reduces the bytecode size without affecting the execution speed. The key insight is that larger value types (such as integers) can hold smaller ones (such as bytes) thus reducing the duplication necessary in heterogeneous translations. In our implementation, on the Scala compiler, we encode all primitive value types in long integers. The resulting bytecode approaches the performance of monomorphic code, matches the performance of the heterogeneous translation and obtains speedups of up to 22x over the homogeneous translation, all with modest increases in size.</p>", "authors": [{"name": "Vlad Ureche", "author_profile_id": "81484655603", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P4290314", "email_address": "vlad.ureche@epfl.ch", "orcid_id": ""}, {"name": "Cristian Talau", "author_profile_id": "83358623257", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P4290315", "email_address": "cristian.talau@epfl.ch", "orcid_id": ""}, {"name": "Martin Odersky", "author_profile_id": "81100056476", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P4290316", "email_address": "martin.odersky@epfl.ch", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509537", "year": "2013", "article_id": "2509537", "conference": "OOPSLA", "title": "Miniboxing: improving the speed to code size tradeoff in parametric polymorphism translations", "url": "http://dl.acm.org/citation.cfm?id=2509537"}