{"article_publication_date": "10-29-2013", "fulltext": "\n Verifying Quantitative Reliability for Programs That Execute on Unreliable Hardware Michael Carbin \nSasa Misailovic Martin C. Rinard MIT CSAIL {mcarbin, misailo, rinard}@csail.mit.edu Abstract Emerging \nhigh-performance architectures are anticipated to contain unreliable components that may exhibit soft \nerrors, which silently corrupt the results of computations. Full de\u00adtection and masking of soft errors \nis challenging, expensive, and, for some applications, unnecessary. For example, ap\u00adproximate computing \napplications (such as multimedia pro\u00adcessing, machine learning, and big data analytics) can often naturally \ntolerate soft errors. We present Rely, a programming language that enables developers to reason about \nthe quantitative reliability of an application namely, the probability that it produces the correct \nresult when executed on unreliable hardware. Rely allows developers to specify the reliability requirements \nfor each value that a function produces. We present a static quantitative reliability analysis that veri.es \nquantitative requirements on the reliability of an ap\u00adplication, enabling a developer to perform sound \nand veri.ed reliability engineering. The analysis takes a Rely program with a reliability speci.cation \nand a hardware speci.cation that characterizes the reliability of the underlying hardware components \nand veri.es that the program satis.es its relia\u00adbility speci.cation when executed on the underlying unreli\u00adable \nhardware platform. We demonstrate the application of quantitative reliability analysis on six computations \nimple\u00admented in Rely. Categories and Subject Descriptors F.3.1 [Logics and Meanings of Programs]: Specifying \nand Verifying and Rea\u00adsoning about Programs General Terms Languages, Reliability, Veri.cation Keywords \nApproximate Computing Permission to make digital or hard copies of part or all of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. Copyrights \nfor third-party components of this work must be honored. For all other uses, contact the owner/author(s). \nOOPSLA 13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright is held by the owner/author(s). \nACM 978-1-4503-2374-1/13/10. http://dx.doi.org/10.1145/2509136.2509546 1. Introduction System reliability \nis a major challenge in the design of emerging architectures. Energy ef.ciency and circuit scal\u00ading are \nbecoming major goals when designing new devices. However, aggressively pursuing these design goals can \noften increase the frequency of soft errors in small [67] and large systems [10] alike. Researchers have \ndeveloped numerous techniques for detecting and masking soft errors in both hardware [23] and software \n[20, 53, 57, 64]. These tech\u00adniques typically come at the price of increased execution time, increased \nenergy consumption, or both. Many computations, however, can tolerate occasional unmasked errors. An \napproximate computation (including many multimedia, .nancial, machine learning, and big data analytics \napplications) can often acceptably tolerate occa\u00adsional errors in its execution and/or the data that \nit manip\u00adulates [16, 44, 59]. A checkable computation can be aug\u00admented with an ef.cient checker that \nveri.es the acceptabil\u00adity of the computation s results [8, 9, 35, 55]. If the checker does detect an \nerror, it can reexecute the computation to obtain an acceptable result. For both approximate and checkable \ncomputations, oper\u00adating without (or with at most selectively applied) mecha\u00adnisms that detect and mask \nsoft errors can produce 1) fast and energy ef.cient execution that 2) delivers acceptably ac\u00adcurate results \noften enough to satisfy the needs of their users despite the presence of unmasked soft errors. 1.1 Background \nResearchers have identi.ed a range of both approximate computations [1, 2, 18, 29, 42 44, 59, 60, 64, \n68, 72] and checkable computations [8, 9, 35, 55]. Their results show that it is possible to exploit \nthese properties for a variety of purposes increased performance, reduced energy con\u00adsumption, increased \nadaptability, and increased fault toler\u00adance. One key aspect of such computations is that they typ\u00adically \ncontain critical regions (which must execute without error) and approximate regions (which can execute \naccept\u00adably even in the presence of occasional errors) [16, 59]. To support such computations, researchers \nhave proposed energy-ef.cient architectures that, because they omit some error detection and correction \nmechanisms, may expose soft errors to the computation [20, 23 25, 64]. A key aspect of these architectures \nis that they contain both reliable and (more ef.cient) unreliable components for executing the critical \nand approximate regions of a computation, respec\u00adtively. The rationale behind this design is that developers \ncan identify and separate the critical regions of the computation (which must execute on reliable hardware) \nfrom the approx\u00adimate regions of the computation (which may execute on more ef.cient unreliable hardware). \n Existing systems, tools, and type systems have focused on helping developers identify, separate, and \nreason about the binary distinction between critical and approximate re\u00adgions [16, 24, 38, 59, 60, 64, \n66]. However, in practice, no computation can tolerate an unbounded accumulation of soft errors to execute \nacceptably, even the approximate re\u00adgions must execute correctly with some minimum reliability.  1.2 \nQuantitative Reliability We present a new programming language, Rely, and an asso\u00adciated program analysis \nthat computes the quantitative reli\u00adability of the computation i.e., the probability with which the \ncomputation produces a correct result when its approxi\u00admate regions execute on unreliable hardware. More \nspeci.\u00adcally, given a hardware speci.cation and a Rely program, the analysis computes, for each value \nthat the computation pro\u00adduces, a conservative probability that the value is computed correctly despite \nthe possibility of soft errors. In contrast to existing approaches, which support only a binary distinction \nbetween critical and approximate regions, quantitative reliability can provide precise static probabilis\u00adtic \nacceptability guarantees for computations that execute on unreliable hardware platforms.  1.3 Rely Rely \nis an imperative language that enables developers to specify and verify quantitative reliability speci.cations \nfor programs that allocate data in unreliable memory regions and incorporate unreliable arithmetic/logical \noperations. Quantitative Reliability Speci.cations. Rely supports quantitative reliability speci.cations \nfor the results that func\u00adtions produce. For example, a developer can declare a func\u00adtion signature int<0.99*R(x, \ny)> f(int x, int y, int z), where 0.99*R(x, y) is the reliability speci.ca\u00adtion for f s return value. \nThe symbolic expression R(x, y) is the joint reliability of x and y namely, the probability that they \nboth simultaneously have the correct value on entry to the function. This speci.cation states that the \nreliability of the return value of f must be at least 99% of x and y s reliability when the function \nwas called. Joint reliabilities serve as an abstraction of a function s input distribution, which enables \nRely s analysis to be both modular and oblivious to the exact shape of the distribu\u00adtions. This is important \nbecause 1) such exact shapes can be dif.cult for developers to identify and specify and 2) known tractable \nclasses of probability distributions are not closed under many operations found in standard program\u00adming \nlanguages, which can complicate attempts to develop compositional program analyses that work with such \nexact shapes [27, 43, 65, 72]. Machine Model. Rely assumes a simple machine model that consists of a \nprocessor (with a register .le and an arith\u00admetic/logic unit) and a main memory. The model includes unreliable \narithmetic/logical operations (which return an in\u00adcorrect value with non-negligible probability [20, \n24, 25, 64]) and unreliable physical memories (in which data may be written or read incorrectly with \nnon-negligible probabil\u00adity [24, 38, 64]). Rely works with a hardware reliability speci.cation that lists \nthe probability with which each op\u00aderation in the machine model executes correctly. Language. Rely is \nan imperative language with integer, logical, and .oating point expressions, arrays, conditionals, while \nloops, and function calls. In addition to these standard language features, Rely also allows a developer \nto allocate data in unreliable memories and write code that uses unre\u00adliable arithmetic/logical operations. \nFor example, the decla\u00adration int x in urel allocates the variable x in an unreli\u00adable memory named urel \nwhere both reads and writes of x may fail with some probability. A developer can also write an expression \na +. b, which is an unreliable addition of the values a and b that may produce an incorrect result. Semantics. \nWe have designed the semantics of Rely to exploit the full availability of unreliable computation in \nan application. Speci.cally, Rely only requires reliable compu\u00adtation at points where doing so ensures \nthat programs are memory safe and exhibit control .ow integrity. Rely s semantics models an abstract \nmachine that con\u00adsists of a heap and a stack. The stack consists of frames that contain references to \nthe locations of each invoked func\u00adtion s variables (which are allocated in the heap). To pro\u00adtect references \nfrom corruption, the stack is allocated in a reliable memory region and stack operations i.e., pushing \nand popping frames execute reliably. To prevent out-of\u00adbounds memory accesses that may occur as a consequence \nof an unreliable array index computation, Rely requires that each array read and write includes a bounds \ncheck; these bounds check computations also execute reliably. Rely does not require a speci.c underlying \nmechanism to execute these operations reliably; one can use any applicable software or hardware technique \n[20, 26, 28, 51, 53, 57, 66, 70]. To prevent the execution from taking control .ow edges that are not \nin the program s static control .ow graph, Rely assumes that 1) instructions are stored, fetched, and \ndecoded reliably (as is supported by existing unreliable processor architectures [24, 64]) and 2) control \n.ow branch targets are computed reliably.  1.4 Quantitative Reliability Analysis Given a Rely program \nand a hardware reliability speci.\u00adcation, Rely s analysis uses a precondition generation ap\u00adproach to \ngenerate a symbolic reliability precondition for each function. A reliability precondition captures a \nset of constraints that is suf.cient to ensure that a function satis.es its reliability speci.cation \nwhen executed on the underlying unreliable hardware platform. The reliability precondition is a conjunction \nof predicates of the form Aout = r \u00b7 R(X), where Aout is a placeholder for a developer-provided reliabil\u00adity \nspeci.cation for an output named out, r is a real number between 0 and 1, and the term R(X) is the joint \nreliability of a set of parameters X. Conceptually, each predicate speci.es that the reliability given \nin the speci.cation (given by Aout) should be less than or equal to the reliability of a path that the \nprogram may take to compute the result (given by r \u00b7 R(X)). The analysis computes the reliability of \na path from the probability that all operations along the path execute reliably. The speci.cation is \nvalid if the probabilities for all paths to computing a result exceed that of the result s speci.cation. \nTo avoid the inherent intractability of considering all pos\u00adsible paths, Rely uses a simpli.cation procedure \nto reduce the precondition to one that characterizes the least reliable path(s) through the function. \nLoops. One of the core challenges in designing Rely and its analysis is dealing with unreliable computation \nwithin loops. The reliability of variables updated within a loop may depend on the number of iterations \nthat the loop executes. Speci.cally, if a variable has a loop-carried dependence and updates to that \nvariable involve unreliable operations, then the variable s reliability is a monotonically decreasing \nfunc\u00adtion of the number of iterations of the loop on each loop iteration the reliability of the variable \ndegrades relative to its previous reliability. If a loop does not have a compile-time bound on the maximum \nnumber of iterations, then the relia\u00adbility of such a variable can, in principle, degrade arbitrarily, \nand the only conservative approximation of the reliability of such a variable is zero. To provide speci.cation \nand veri.cation .exibility, Rely provides two types of loop constructs: statically bounded while loops \nand statically unbounded while loops. Stati\u00adcally bounded while loops allow a developer to provide a \nstatic bound on the maximum number of iterations of a loop. The dynamic semantics of such a loop is to \nexit if the number of executed iterations reaches this bound. This bound allows Rely s analysis to soundly \nconstruct constraints on the relia\u00adbility of variables modi.ed within the loop by unrolling the loop \nfor its maximum bound. Statically unbounded while loops have the same dy\u00adnamic semantics as standard \nwhile loops. In the absence of a static bound on the number of executed loop iterations, however, Rely \ns analysis constructs a dependence graph of the loop s body to identify variables that are reliably updated \n speci.cally, all operations that in.uence these variables values are reliable. Because the execution \nof the loop does not decrease the reliability of these variables, the analysis identi.es that their reliabilities \nare unchanged. For the re\u00admaining, unreliably updated variables, Rely s analysis con\u00adservatively sets \ntheir reliability to zero. Speci.cation Checking. In the last step of the analysis of a function, Rely \nveri.es that the function s speci.cations are consistent with its reliability precondition. Because re\u00adliability \nspeci.cations are also of the form r \u00b7 R(X), the .nal precondition is a conjunction of predicates of \nthe form r1 \u00b7 R(X1) = r2 \u00b7 R(X2), where r1 \u00b7 R(X1) is a reliabil\u00adity speci.cation and r2 \u00b7 R(X2) is a \npath reliability. If these predicates are valid, then the reliability of each computed output is greater \nthan that given by its speci.cation. The validity problem for these predicates has a sound mapping to \nthe conjunction of two simple constraint validity problems: inequalities between real numbers (r1 = r2) \nand set inclusion constraints over .nite sets (X2 . X1). Check\u00ading the validity of a reliability precondition \nis therefore de\u00adcidable and ef.ciently checkable. 1.5 Case Studies We have used Rely to build unreliable \nversions of six build\u00ading block computations for media processing, machine learning, and data analytics \napplications. Our case studies illustrate how quantitative reliability enables a developer to use principled \nreasoning to relax the semantics of both ap\u00adproximate and checkable computations. For approximate computations, \nquantitative reliability al\u00adlows a developer to reify and verify the results of the fault injection and \naccuracy explorations that are typically used to identify the minimum acceptable reliability of a compu\u00adtation \n[43, 44, 68, 72]. For checkable computations, quanti\u00adtative reliability allows a developer to use the \nperformance speci.cations of both the computation and its checker to de\u00adtermine the computation s overall \nperformance given that with some probability it may produce an incorrect answer and therefore needs \nto be reexecuted.  1.6 Contributions This paper presents the following contributions: Quantitative Reliability \nSpeci.cations. We present quan\u00adtitative reliability speci.cations, which characterize the probability \nthat a program executed on unreliable hardware produces the correct result, as a constructive method \nfor developing applications. Quantitative reliability speci.ca\u00adtions enable developers who build applications \nfor unreli\u00adable hardware architectures to perform sound and veri.ed reliability engineering. Language \nand Semantics. We present Rely, a language that allows developers to specify reliability requirements \nfor programs that allocate data in unreliable memory regions and use unreliable arithmetic/logical operations. \n n r x, e a . . . . IntM R Var ArrVar e . Exp b . BExp CExp . n | x | (Exp) | Exp iop Exp . true | false \n| Exp cmp Exp | (BExp) |BExp lop BExp | !BExp | !.BExp . e | a m V RSpec T . MVar . x | a | V, x | V, \na . r | R(V ) | r * R(V ) . int | int<RSpec > D . D0 [in m] F P P0 S . (T | void) ID (P * ) { S }. P0 \n[in m] . int x | T a(n) . D * Ss S? r D0 Ss Sr . int x [= Exp] | int a[n +] . skip | x = Exp | x = a[Exp+] \n| a[Exp+] = Exp |ID(CExp * ) | x = ID(CExp * ) | ife BExp S S | S ; S whilee BExp [: n] S | repeate n \nS . return Exp Figure 1. Rely s Language Syntax We present a dynamic semantics for Rely via a probabilis-1 \n# d e f i n e n b l o c k s 2 0 2 # d e f i n e h e i g h t 1 6 tic small-step operational semantics. \nThis semantics is pa\u00ad 3 # define width 16 rameterized by a hardware reliability speci.cation that char\u00ad \n4 acterizes the probability that an unreliable arithmetic/logical or memory read/write operation executes \ncorrectly. Semantics of Quantitative Reliability. We formalize the semantics of quantitative reliability \nas it relates to the prob\u00adabilistic dynamic semantics of a Rely program. Speci.cally, we de.ne the quantitative \nreliability of a variable as the prob\u00adability that its value in an unreliable execution of the pro\u00adgram \nis the same as that in a fully reliable execution. We also de.ne the semantics of a logical predicate \nlanguage that can characterize the reliability of variables in a program. Quantitative Reliability Analysis. \nWe present a program analysis that veri.es that the dynamic semantics of a Rely program satis.es its \nquantitative reliability speci.cations. For each function in the program, the analysis computes a symbolic \nreliability precondition that characterizes the set of valid speci.cations for the function. The analysis \nthen veri.es that the developer-provided speci.cations are valid according to the reliability precondition. \nCase Studies. We have used our Rely implementation to develop unreliable versions of six building block \ncomputa\u00adtions for media processing, machine learning, and data ana\u00adlytics applications. These case studies \nillustrate how to use quantitative reliability to develop and reason about both ap\u00adproximate and checkable \ncomputations in a principled way. 2. Example Figure 1 presents the syntax of the Rely language. Rely \nis an imperative language for computations over integers, .oats (not presented), and multidimensional \narrays. To illustrate how a developer can use Rely, Figure 2 presents a Rely\u00adbased implementation of \na pixel block search algorithm de\u00adrived from that in the x264 video encoder [71]. The function search_ref \nsearches a region (pblocks) of a previously encoded video frame to .nd the block of pix\u00adels that is most \nsimilar to a given block of pixels (cblock) in the current frame. The motion estimation algorithm uses \nthe results of search_ref to encode cblock as a function of the identi.ed block. 5 i n t < 0 . 9 9 * \nR ( p b l o c k s , c b l o c k ) > s e a r c h _ r e f ( 6 i n t < R ( p b l o c k s ) > p b l o c k \ns ( 3 ) i n u r e l , 7 i n t < R ( c b l o c k ) > c b l o c k ( 2 ) i n u r e l ) 8 { 9 i n t m i n \ns s d = I N T _ M A X , 10 m i n b l o c k = -1 i n u r e l ; 11 i n t s s d , t , t 1 , t 2 i n u r \ne l ; 12 i n t i = 0 , j , k ; 13 14 r e p e a t n b l o c k s { 15 s s d = 0 ; 16 j = 0 ; 17 r e p e \na t h e i g h t { 18 k = 0 ; 19 r e p e a t w i d t h { 20 t 1 = p b l o c k s [ i , j , k ] ; 21 t 2 \n= c b l o c k [ j , k ] ; 22 t = t 1 -. t 2 ; 23 s s d = s s d + . t * . t ; 24 k = k + 1 ; 25 } 26 j \n= j + 1 ; 27 } 28 29 i f ( s s d < . m i n s s d ) { 30 m i n s s d = s s d ; 31 m i n b l o c k = i \n; 32 } 33 34 i = i + 1 ; 35 } 36 r e t u r n m i n b l o c k ; 37 } Figure 2. Rely Code for Motion Estimation \nComputation This is an approximate computation that can trade cor\u00adrectness for more ef.cient execution \nby approximating the search to .nd a block. If search_ref returns a block that is not the most similar, \nthen the encoder may require more bits to encode cblock, potentially decreasing the video s peak signal-to-noise \nratio or increasing its size. However, previ\u00adous studies on soft error injection [20] and more aggressive \ntransformations like loop perforation [44, 68] have demon\u00adstrated that the quality of x264 s .nal result \nis only slightly affected by perturbations of this computation.  2.1 Reliability Speci.cations The function \ndeclaration on Line 5 speci.es the types and reliabilities of search_ref s parameters and return value. \nThe parameters of the function are pblocks(3), a three-dimensional array of pixels, and cblock(2), a \ntwo\u00addimensional array of pixels. In addition to the standard sig\u00adnature, the function declaration contains \nreliability speci.\u00adcations for each result that the function produces. Rely s reliability speci.cations \nexpress the reliability of a function s results when executed on an unreliable hard\u00adware platform as \na function of the reliabilities of its inputs. The speci.cation for the reliability of search_ref s return \nvalue is int<0.99*R(pblocks,cblock)>. This states that the return value is an integer with a reliability \nthat is at least 99% of the joint reliability of the parameters pblocks and cblock (denoted by R(pblocks, \ncblock)). The joint re\u00adliability of a set of parameters is the probability that they all have the correct \nvalue when passed in from the caller. This speci.cation holds for all possible values of the joint relia\u00adbility \nof pblocks and cblock. For instance, if the contents of the arrays pblocks and cblock are fully reliable \n(correct with probability one), then the return value is correct with probability 0.99. In Rely, arrays \nare passed by reference and the execu\u00adtion of a function can, as a side effect, modify an array s contents. \nThe reliability speci.cation of an array therefore allows a developer to constrain the reliability degradation \nof its contents. Here pblocks has an output reliability speci.\u00adcation of R(pblocks) (and similarly for \ncblock), meaning that all of pblock s elements are at least as reliable when the function exits as they \nwere on entry to the function. 2.2 Unreliable Computation Rely targets hardware architectures that expose \nboth reli\u00adable operations (which always execute correctly) and more energy-ef.cient unreliable operations \n(which execute cor\u00adrectly with only some probability). Speci.cally, Rely sup\u00adports reasoning about reads \nand writes of unreliable memory regions and unreliable arithmetic/logical operations. Memory Region Speci.cation. \nEach parameter declara\u00adtion also speci.es the memory region in which the data of the parameter is allocated. \nMemory regions correspond to the physical partitioning of memory at the hardware level into regions of \nvarying reliability. Here pblocks and cblock are allocated in an unreliable memory region named urel. \nLines 9-12 declare the local variables of the function. By default, variables in Rely are allocated in \na default, fully reli\u00adable memory region. However, a developer can also option\u00adally specify a memory \nregion for each local variable. For ex\u00adample, the variables declared on Lines 9-11 reside in urel. Unreliable \nOperations. The operations on Lines 22, 23, and 29 are unreliable arithmetic/logical operations. In Rely, \nevery arithmetic/logical operation has an unreliable counter\u00adpart that is denoted by suf.xing a period \nafter the operation Figure 3. Machine Model Illustration. Gray boxes represent unreliable components \n symbol. For example, -. denotes unreliable subtraction and <. denotes unreliable comparison. Using these \noperations, search_ref s implementation approximately computes the index (minblock) of the most similar \nblock, i.e. the block with the minimum distance from cblock. The repeat statement on line 14, iterates \na constant nblock number of times, enumerating over all previously encoded blocks. For each encoded block, \nthe repeat statements on lines 17 and 19 iterate over the height*width pixels of the block and compute \nthe sum of the squared differences (ssd) between each pixel value and the corresponding pixel value in \nthe current block cblock. Finally, the computation on lines 29 through 32 selects the block that is \napproximately the most similar to cblock.  2.3 Hardware Semantics Figure 3 illustrates the conceptual \nmachine model behind Rely s reliable and unreliable operations; the model consists of a CPU and a memory. \nCPU. The CPU consists of 1) a register .le, 2) arithmetic logical units that perform operations on data \nin registers, and 3) a control unit that manages the program s execution. The arithmetic-logical unit \ncan execute reliably or unre\u00adliably. We have represented this in Figure 3 by physically separate reliable \nand unreliable functional units, but this dis\u00adtinction can be achieved through other mechanisms, such \nas dual-voltage architectures [24]. Unreliable functional units may omit additional checking logic, enabling \nthe unit to ex\u00adecute more ef.ciently but also allowing for soft errors that may occur due to, for example, \npower variations within the ALU s combinatorial circuits or particle strikes. As is pro\u00advided by existing \ncomputer architecture proposals [24, 64], the control unit of the CPU reliably fetches, decodes, and \nschedules instructions; given a virtual address in the appli\u00adcation, the control unit correctly computes \na physical address and operates only on that physical address. Memory. Rely supports machines with memories \nthat consist of an arbitrary number of memory partitions (each potentially of different reliability), \nbut for simplicity Fig\u00adure 3 partitions memory into two regions: reliable and unre\u00adliable. Unreliable \nmemories can, for example, use decreased DRAM refresh rates to reduce power consumption at the expense \nof increased soft error rates [38, 64].  2.4 Reliability Analysis Given a Rely program, Rely s reliability \nanalysis veri.es that each function in the program satis.es its reliability spec\u00adi.cation when executed \non unreliable hardware. Figure 4 presents an overview of Rely s analysis. It takes as input a Rely program \nand a hardware reliability speci.cation. Figure 4. Rely s Analysis Overview The analysis consists of \ntwo components: the precondi\u00adtion generator and the precondition checker. For each func\u00adtion, the precondition \ngenerator produces a precondition that characterizes the reliability of the function s results given \na hardware reliability speci.cation that characterizes the relia\u00adbility of each unreliable operation. \nThe precondition checker then determines if the function s speci.cations satisfy the constraint. If so, \nthen the function satis.es its reliability speci.cation when executed on the underlying unreliable hardware \nin that the reliability of its results exceed their speci.cations. Design. As a key design point, the \nanalysis generates pre\u00adconditions according to a conservative approximation of the semantics of the function. \nSpeci.cally, it characterizes the reliability of a function s result according to the probability that \nthe function computes that result fully reliably. To illustrate the intuition behind this design point, \ncon\u00adsider the evaluation of an integer expression e. The reliabil\u00adity of e is the probability that it \nevaluates to the same value n in an unreliable evaluation as in the fully reliable evaluation. There \nare two ways that an unreliable evaluation can return n: 1) the unreliable evaluation of e encounters \nno faults and 2) the unreliable evaluation possibly encounters faults, but still returns n by chance. \nRely s analysis conservatively approximates the reliabil\u00adity of a computation by only considering the \n.rst scenario. This design point simpli.es our reasoning to the task of com\u00adputing the probability that \na result is reliably computed as opposed to reasoning about a computation s input distribu\u00adtion and the \nprobabilities of all executions that produce the correct result. As a consequence, the analysis requires \nas in\u00adput only a hardware reliability speci.cation that gives the probability with which each arithmetic/logical \noperation and memory operation executes correctly. Our analysis is there\u00adfore oblivious to a computation \ns input distribution and does not require a full model of how soft errors affect its result. 2.4.1 Hardware \nReliability Speci.cation Rely s analysis works with a hardware reliability speci.\u00adcation that speci.es \nthe reliability of arithmetic/logical and memory operations. Figure 5 presents a hardware reliability \nspeci.cation that we have created using results from existing reliability spec { operator (+.) = 1 -10^-7; \noperator (-.) = 1 -10^-7; operator (*.) = 1 -10^-7; operator (<.) = 1 -10^-7; memory rel {rd = 1, wr \n= 1}; memory urel {rd = 1 -10^-7, wr = 1}; } Figure 5. Hardware Reliability Speci.cation computer architecture \nliterature [23, 38]. Each entry spec\u00adi.es the reliability the probability of a correct execution of \narithmetic operations (e.g., +.) and memory read/write operations. For ALU operations, the presented \nreliability speci.ca\u00adtion uses the reliability of an unreliable multiplication opera\u00adtion from [23, Figure \n9]. For memory operations, the speci.\u00adcation uses the probability of a bit .ip in a memory cell from \n[38, Figure 4] with extrapolation to the probability of a bit .ip within a 32-bit word. Note that a memory \nregion speci.\u00adcation includes two reliabilities: the reliability of a read (rd) and the reliability of \na write (wr).  2.4.2 Precondition Generator For each function, Rely s analysis generates a reliability \npre\u00adcondition that conservatively bounds the set of valid spec\u00adi.cations for the function. A reliability \nprecondition is a conjunction of predicates of the form Aout = r \u00b7 R(X), where Aout is a placeholder \nfor a developer-provided relia\u00adbility speci.cation for an output with name out, r is a numer\u00adical value \nbetween 0 and 1, and the term R(X) is the joint reliability of the set of variables X on entry to the \nfunction. The analysis starts at the end of the function from a post\u00adcondition that must be true when \nthe function returns and then works backward to produce a precondition such that if the precondition \nholds before execution of the function, then the postcondition holds at the end of the function. Postcondition. \nThe postcondition for a function is the constraint that the reliability of each array argument ex\u00adceeds \nthat given in its speci.cation. For our example func\u00adtion search_ref, the postcondition Q0 is Q0 = Apblocks \n= R(pblocks) . Acblock = R(cblock), which speci.es that the reliability of the arrays pblocks and cblock \n R(pblocks) and R(cblock) should be at least that speci.ed by the developer Apblocks and Acblock. Precondition \nGeneration. The analysis of the body of the search_ref function starts at the return statement. Given \nthe postcondition Q0, the analysis creates a new precondi\u00adtion Q1 by conjoining to Q0 a predicate that \nstates that reli\u00adability of the return value (r0 \u00b7 R(minblock)) is at least that of its speci.cation \n(Aret ): Q1 = Q0 . Aret = r0 \u00b7 R(minblock).  (3) {Q0 . Aret = r0 4 \u00b7 R(i, ssd, minssd) . Aret = r0 4 \n\u00b7 R(minblock, ssd, minssd)} if (ssd <. minssd) { (2) {Q0 . Aret = r0 \u00b7 R(i, e29)}minssd = ssd;  {Q0 \n. Aret = r0 \u00b7 R(i, e29)} minblock = i; {Q0 . Aret = r0 \u00b7 R(minblock, e29)} } else { (2) {Q0 . Aret = \nr0 \u00b7 R(minblock, e29)}skip; {Q0 . Aret = r0 \u00b7 R(minblock, e29)}} (1) {Q0 . Aret = r0 \u00b7 R(minblock, e29)} \nFigure 6. if Statement Analysis in the Last Loop Iteration The reliability of the return value comes \nfrom our design principle for reliability approximation. Speci.cally, this re\u00adliability is the probability \nof correctly reading minblock from unreliable memory which is r0 = 1 - 10-7 accord\u00ading to the hardware \nreliability speci.cation multiplied by R(minblock), the probability that the preceding computa\u00adtion \ncorrectly computed and stored minblock. Loops. The statement that precedes the return statement is the \nrepeat statement on Line 14. A key dif.culty with reasoning about the reliability of variables modi.ed \nwithin a loop is that if a variable is updated unreliably and has a loop-carried dependence then its \nreliability monotonically decreases as a function of the number of loop iterations. Be\u00adcause the reliability \nof such variables can, in principle, de\u00adcrease arbitrarily in an unbounded loop, Rely provides both an \nunbounded loop statement (with an associated analysis, Section 5.2.3) and an alternative bounded loop \nstatement that lets a developer specify a compile-time bound on the maxi\u00admum number of its iterations \nthat therefore bounds the relia\u00adbility degradation of modi.ed variables. The loop on Line 14 iterates \nnblocks times and therefore decreases the reliabil\u00adity of any modi.ed variables nblocks times. Because \nthe reliability degradation is bounded, Rely s analysis uses un\u00adrolling to reason about the effects of \na bounded loop. Conditionals. The analysis of the body of the loop on Line 14 encounters the if statement \non Line 29.1 This if statement uses an unreliable comparison operation on ssd and minssd, both of which \nreside in unreliable memory. The reliability of minblock when modi.ed on Line 31 therefore also depends \non the reliability of this expression because faults may force the execution down a different path. Figure \n6 presents a Hoare logic style presentation of the analysis of the conditional statement. The analysis \nworks in three steps; the preconditions generated by each step are numbered with the corresponding step. \n1 This happens after encountering the increment of i on Line 34, which does not modify the current precondition \nbecause it does not reference i. Step 1. To capture the implicit dependence of a variable on an unreliable \ncondition, Rely s analysis .rst uses latent con\u00adtrol .ow variables to make these dependencies explicit. \nA control .ow variable is a unique program variable (one for each statement) that records whether the \nconditional evalu\u00adated to true or false. We denote the control .ow variable for the if statement on Line \n29 by \u00a329. To make the control .ow dependence explicit, the analy\u00adsis adds the control .ow variable to \nall joint reliability terms in Q1 that contain variables modi.ed within the body of the if conditional \n(minssd and minblock). Step 2. The analysis next recursively analyses both the then and else branches \nof the conditional, producing one precondition for each branch. As in a standard precondition generator \n(e.g., weakest-preconditions) the assignment of i to minblock in the then branch replaces minblock with \ni in the precondition. Because reads from i and writes to minblock are reliable (according to the speci.cation) \nthe analysis does not introduce any new r0 factors. Step 3. In the .nal step, the analysis leaves the \nscope of the conditional and conjoins the two preconditions for its branches after transforming them \nto include the direct dependence of the control .ow variable on the reliability of the if statement s \ncondition expression. The reliability of the if statement s expression is greater than or equal to the \nproduct of 1) the reliability of the <. operator (r0), 2) the reliability of reading both ssd and 2 minssd \nfrom unreliable memory (r), and 3) the relia\u00ad 0 bility of the computation that produced ssd and minssd \n(R(ssd, minssd)). The analysis therefore transforms each predicate that contains the variable \u00a329, by \nmultiplying the 3 right-hand side of the inequality with rand replacing the 0 variable \u00a329 with ssd and \nminssd. This produces the pre\u00adcondition Q2: 4 Q2 = Q0 . Aret = r0 \u00b7 R(i, ssd, minssd) 4 . Aret = r0 \u00b7 \nR(minblock, ssd, minssd). Simpli.cation. After unrolling a single iteration of the loop that begins at \nLine 14, the analysis produces Q0 .Aret = r0 2564 \u00b7R(pblocks, cblock, i, ssd, minssd) as the precondition \nfor a single iteration of the loop s body. The constant 2564 represents the number of unreliable operations \nwithin a sin\u00adgle loop iteration. Note that there is one less predicate in this precondition than in Q2. \nAs the analysis works backwards through the program, it uses a simpli.cation technique that identi.es \nthat a predicate Aret = r1 \u00b7 R(X1) subsumes another predicate Aret = r2 \u00b7 R(X2). Speci.cally, the analysis \nidenti.es that r1 = r2 and X2 . X1, which together mean that the second predicate is a weaker constraint \non Aret than the .rst and can therefore be removed. This follows from the fact that the joint reliability \nof a set of variables is less than or equal to the joint reliability of any subset of the variables \nregardless of the distribution of their values.  This simpli.cation is how Rely s analysis achieves \nscal\u00adability when there are multiple paths in the program; specif\u00adically a simpli.ed precondition characterizes \nthe least reli\u00adable path(s) through the program. Final Precondition. When the analysis reaches the be\u00adginning \nof the function after fully unrolling the loop on Line 14, it has a precondition that bounds the set \nof valid speci.cations as a function of the reliability of the parame\u00adters of the function. For search \nref, the analysis generates the precondition Aret = 0.994885 \u00b7 R(pblocks, cblock) . Apblocks = R(pblocks) \n. Acblock = R(cblock).  2.4.3 Precondition Checker The .nal precondition is a conjunction of predicates \nof the form Aout = r\u00b7R(X), where Aout is a placeholder for the re\u00adliability speci.cation of an output. \nBecause reliability spec\u00adi.cations are all of the form r \u00b7 R(X) (Figure 1), each pred\u00adicate in the .nal \nprecondition (where each Aout is replaced with its speci.cation) is of the form form r1 \u00b7 R(X1) = r2 \n\u00b7 R(X2), where r1 \u00b7 R(X1) is a reliability speci.cation and r2 \u00b7 R(X2) is computed by the analysis. Similar \nto the analysis s simpli.er (Section 2.4.2), the precondition checker veri.es the validity of each predicate \nby checking that 1) r1 is less than r2 and 2) X2 . X1. For search_ref, the analysis computes the predicates \n0.99\u00b7R(pblocks, cblock) = 0.994885\u00b7R(pblocks, cblock), R(pblocks) = R(pblocks), and R(cblock) = R(cblock). \nBecause these predicates are valid according to our checking procedure, search_ref satis.es its reliability \nspeci.cation when executed on the speci.ed unreliable hardware. 3. Language Semantics Because soft errors \nmay probabilistically change the execu\u00adtion path of a program, we model the semantics of a Rely program \nwith a probabilistic, non-deterministic transition system. Speci.cally, the dynamic semantics de.nes \nproba\u00adbilistic transition rules for each arithmetic/logical operation and each read/write on an unreliable \nmemory region. Over the next several sections we develop a small-step semantics that speci.es the probability \nof each individual transition of an execution. In Section 3.4 we provide big-step de.nitions that specify \nthe probability of an entire execution. 3.1 Preliminaries Rely s semantics models an abstract machine \nthat consists of a heap and a stack. The heap is an abstraction over the phys\u00adical memory of the concrete \nmachine, including its various reliable and unreliable memory regions. Each variable (both scalar and \narray) is allocated in the heap. The stack consists of frames one for each function invocation which \ncontain references to the locations of each allocated variable. Hardware Reliability Speci.cation. A \nhardware reliabil\u00adity speci.cation . . . = (iop+cmp+lop+Mop) . Ris a .nite map from arithmetic/logical \noperations (iop, cmp, lop) and memory region operations (Mop) to reliabilities (i.e., the probability \nthat the operation executes correctly). Arithmetic/logical operations iop, cmp, and lop include both \nreliable and unreliable versions of each integer, com\u00adparison, and logical operation. The reliability \nof each reli\u00adable operation is 1 and the reliability of an unreliable opera\u00adtion is as provided by a \nspeci.cation (Section 2.4.1). The .nite maps rd . M . Mop and wr . M . Mop de.ne memory region operations \nas reads and writes (respectively) on memory regions m . M, where M is the set of all memory regions \nin the reliability speci.cation. The hardware reliability speci.cation 1. denotes the speci.cation for \nfully reliable hardware in which all arith\u00admetic/logical and memory operations have reliability 1. References. \nA reference is a tuple (nb, (n1, . . . , nk), m) . Ref consisting of a base address nb . Loc, a dimension \nde\u00adscriptor (n1, . . . , nk), and a memory region m. The address space Loc is .nite. A base address and \nthe components of a dimension descriptor are machine integers n . IntM, which have .nite bit width and \ntherefore create a .nite set. References describe the location, dimensions, and mem\u00adory region of variables \nin the heap. For scalars, the dimen\u00adsion descriptor is the single-dimension, single-element de\u00adscriptor \n(1). We use the projections pbase and pdim to select the base address and the dimension descriptor of \na reference, respectively. Frames, Stacks, Heaps, and Environments. A frame s . S = Var . Ref is a .nite \nmap from variables to references. A stack d . . ::= s | s :: . is a non-empty list of frames. A heap \nh . H = Loc . IntM is a .nite map from addresses to machine integers. An environment e . E = . \u00d7 H is \na stack and heap pair, (d, h). Memory Allocator. The abstract memory allocator new is a potentially non-deterministic \npartial function that executes reliably. It takes a heap h, a memory region m, and a dimen\u00adsion descriptor \nand returns a fresh address nb that resides in memory region m and a new heap h' that re.ects updates \nto the internal memory allocation data structures. Auxiliary Probability Distributions. Each nondetermin\u00adistic \nchoice in Rely s semantics must have an underlying probability distribution so that the set of possible \ntransitions at any given small step of an execution creates a probabil\u00adity distribution i.e., the sum \nof the probabilities of each possibility is one. In Rely, there are two points at which an execution \ncan make a nondeterministic choice: 1) the result of an incorrect execution of an unreliable operation \nand 2) the result of allocating a new variable in the heap. The discrete probability distribution Pf \n(nf | op, n1, ..., nk) models the manifestation of a soft error during an incorrect execution of an operation. \nSpeci.cally, it gives the prob\u00ad  E-IOP-R1 E-VA R-C E-VAR -F ., p I (nb, (1), m) = s(x) (nb, (1), m) \n= s(x) p = (1 - .(rd(m))) \u00b7 Pf (nf | rd(m), h(nb)) (e1, s, h)-.. e 1 C, .(rd(m)) (F,nf ), p ., pI (x, \ns, h)-.. h(nb) (x, s, h) -.. nf (e1 iop e2, s, h)-.. e1 iop e2 E-IOP-R2 E-IOP-F ., p I E-IO P -C (e2, \ns, h)-.. e p = (1 - .(iop)) \u00b7 Pf (nf | iop, n1, n2) 2 I C, .(iop) (F,nf ), p -.. n iop e (n iop e2, \ns, h) ., p2 (n1 iop n2, s, h) -.. iop(n1, n2) (n1 iop n2, s, h)-.. nf Figure 7. Dynamic Semantics of \nInteger Expressions ability that an incorrect execution of an operation op on operands n1, . . . , nk \nproduces a value nf that is different from the correct result of the operation. This distribution is \ninherently tied to the properties of the underlying hardware. The discrete probability distribution Pm(nb, \nh ' | h, m, d) models the semantics of a nondeterministic memory alloca\u00adtor. It gives the probability \nthat a memory allocator returns a fresh address nb and an updated heap h ' given an initial heap h, a \nmemory region m, and a dimension descriptor d. We de.ne these distributions only to support a precise \nformalization of the dynamic semantics of a program; they do not need to be speci.ed for a given hardware \nplatform or a given memory allocator to use Rely s reliability analysis.  3.2 Semantics of Expressions \nFigure 7 presents a selection of the rules for the dynamic semantics of integer expressions. The labeled \nprobabilis\u00ad ., p ' tic small-step evaluation relation (e, s, h)-.. e states that from a frame s and a \nheap h, an expression e evaluates in one ' step with probability p to an expression e given a hardware \nreliability speci.cation .. The label . . {C, (C, n), (F, nf )}denotes whether the transition corresponds \nto a correct (C or (C, n)) or a faulty ((F, nf )) evaluation of that step. For a correct transition (C, \nn), n . IntM records a nondeterminis\u00adtic choice made for that step. For a faulty transition (F, nf ), \nnf . IntM represents the value that the fault introduced in the semantics of the operation. To illustrate \nthe meaning of the rules, consider the rules for variable reference expressions. A variable reference \nx reads the value stored in the memory address for x. The are two possibilities for the evaluation of \na variable reference: Correct [E-VA R -C]. The variable reference evaluates correctly and successfully \nreturns the integer stored in x. This happens with probability .(rd(m)), where m is the memory region \nin which x allocated. This probability is the reliability of reading from x s memory region.  Faulty \n[E -VA R -F]. The variable reference experiences a fault and returns another integer nf . The probability \nthat the faulty execution returns a speci.c integer nf is (1 - .(rd(m))) \u00b7 Pf (nf | rd(m), h(nb)). Pf \nis the distri\u00adbution that gives the probability that a failed memory read operation returns a value nf \ninstead of the true stored value h(nb) (Section 3.1).  3.3 Semantics of Statements Figure 8 presents \nthe scalar and control .ow fragment of Rely. The labeled probabilistic small-step execution relation \n., p (s, e) -.. (s ' , e ' ) states that execution of the statement s in ' the environment e takes one \nstep yielding a statement s and an environment e ' with probability p under the hardware re\u00adliability \nspeci.cation .. As in the dynamic semantics for ex\u00adpressions, a label . denotes whether the transition \nevaluated correctly (C or (C, n)) or experienced a fault ((F, nf )). The semantics of the statements \nin our language are largely sim\u00adilar to that of traditional presentations except that the state\u00adments \nhave the ability to encounter faults during execution. The semantics we present here and in the Appendix \n[14] (which includes a semantics for arrays and functions) is designed to allow unreliable computation \nat all points in the application subject to the constraint that the application is still memory safe \nand exhibits control .ow integrity. Memory Safety. To protect references that point to mem\u00adory locations \nfrom corruption, the stack is allocated in a re\u00adliable memory region and stack operations i.e., pushing \nand popping frames execute reliably (see Appendix). To prevent out-of-bounds memory accesses that may \noccur due to an unreliable array index computation, Rely requires that each array read and write include \na bounds check. These bounds check computations execute reliably (see Appendix). Control Flow Integrity. \nTo prevent execution from taking control .ow edges that do not exist in the program s static control \n.ow graph, Rely assumes that 1) instructions are stored, fetched, and decoded reliably (as supported \nby exist\u00ading unreliable processor architectures [24, 64]) and 2) targets of control .ow branches are \nreliably computed. These two properties allow for the control .ow transfers in the rules [E-IF-TRU E], \n[E-IF-FA L S E], and [E-SE Q -R2] to execute reli\u00adably with probability 1. We note that the semantics \ndoes not require a speci.c un\u00adderlying mechanism to achieve reliable execution and, there\u00adfore, an implementation \ncan use any applicable software or hardware technique [20, 26, 28, 51, 53, 57, 66, 70].  3.4 Big-step \nNotations We use the following big-step execution relations in the remainder of the paper.  E-DE C L-R \n E-DEC L ., p I (e, s, h)-.. e (nb, hI) = new(h, m, (1)) pm = Pm(nb, hI | h, m, (1)) ., p(C,nb), pm \nI (int x = e in m, (s :: d, h)) -.. (int x = e in m, (s :: d, h)) (int x = n in m, (s :: d, h)) -.. \n(x = n, (s[x . (nb, (1), m)] :: d, hI)) E-AS SI G N -R E-AS S IG N -C ., p I (e, s, h)-.. e (nb, (1), \nm) = s(x) p = .(wr(m)) ., pC, p I (x = e, (s :: d, h)) -.. (x = e , (s :: d, h)) (x = n, (s :: d, h)) \n-.. (skip, (s :: d, h[nb . n])) E-IF E-AS S IG N -F ., p (nb, (1), m) = s(x) p = (1 - .(wr(m))) \u00b7 Pf \n(nf | wr(m), h(nb), n) (b, s, h)-.. bI (F,nf ), p ., p (x = n, (s :: d, h)) -.. (skip, (s :: d, h[nb \n. nf ])) (ife b s1 s2, (s :: d, h)) -.. (ife bI s1 s2, (s :: d, h)) E-SEQ -R1 E-IF-TRU E E-IF-FA LSE \n., pI E-SE Q -R2 (s1, e) -.. (s1, eI)C, 1 C, 1 ., pC, 1 I (ife true s1 s2, e) -.. (s1, e) (ife false \ns1 s2, e) -.. (s2, e) (s1 ; s2, e) -.. (s1 ; s2, eI) (skip ; s2, e) -.. (s2, e) E-WH I L E E-WH I L E \n-BOU N D E D C, 1 C, 1 (whilee b s, e) -.. (ife b {s ; whilee b s} {skip}, e) (whilee b : n s, e) -.. \n(ife b {s ; whilee b : (n - 1) s} {skip}, e) Figure 8. Dynamic Semantics of Statements De.nition 1 (Big-step \nTrace Semantics). t, p .1, p1 .n, pn (s, e) =.. e ' = (s, e) -.. . . . -.. (skip, e ' )where t = .1, \n. . . , .n and p = . pi i The big-step trace semantics is a re.exive transitive clo\u00adsure of the small-step \nexecution relation that records a trace of the execution. A trace t . T ::= \u00b7 | . :: T is a sequence \nof small-step transition labels. The probability of a trace, p, is the product of the probabilities of \neach transition. De.nition 2 (Big-step Aggregate Semantics). p t, pt (s, e) =.. e ' where p =pt such \nthat (s, e) =.. e ' t.T The big-step aggregate semantics enumerates over the set of all .nite length \ntraces and collects the aggregate proba\u00adbility that a statement s evaluates to an environment e ' from \nan environment e given a hardware reliability speci.cation .. The big-step aggregate semantics therefore \ngives the total probability that a statement s starts from an environment e and terminates in an environment \ne ' . 2 Termination and Errors. An unreliable execution of a statement may experience a run-time error \n(due to an out\u00adof-bounds array access) or not terminate at all. The big-step aggregate semantics does \nnot collect such executions. There\u00adfore, the sum of the probabilities of the big-step transitions from \nan environment e may not equal to 1. Speci.cally, let p . E . Rbe a measure for the set of environments \nreach\u00ad p(e) able from e, i.e., .e ' .(s, e) =.. e '. Then p is subprobability measure, i.e., 0 =e..E \np(e ' ) = 1 [32]. 2 The inductive (versus co-inductive) interpretation of T yields a countable set of \n.nite-length traces and therefore the sum over T is well-de.ned. 4. Semantics of Quantitative Reliability \nWe next present the basic de.nitions that give a semantic meaning to the reliability of a Rely program. \n 4.1 Paired Execution The paired execution semantics is the primary execution relation that enables us \nto reason about the reliability of a program. Speci.cally, the relation pairs the semantics of the program \nwhen executed reliably with its semantics when executed unreliably. De.nition 3 (Paired Execution). . \n. F = E . R t,pr (s, (e, .)) .. (e ' , . ' ) such that (s, e) =.1. e ' and pu . ' (e ' ) =.(eu) \u00b7 pu \nwhere (s, eu) =.. e ' u u eu.E The relation states that from a con.guration (e, .) con\u00adsisting of an \nenvironment e and an unreliable environment distribution ., the paired execution of a statement s yields \na new con.guration (e ' , . ' ). The environments e and e ' are related by the fully reliable execution \nof s. Namely, an execution of s from an environ\u00adment e yields e ' under the fully reliable hardware model \n1.. The unreliable environment distributions . and . ' are probability mass functions that map an environment \nto the probability that the unreliable execution of the program is in that environment. In particular, \n. is a distribution on en\u00advironments before the unreliable execution of s whereas . ' is the distribution \non environments after executing s. These distributions specify the probability of reaching a speci.c \nenvironment as a result of faults during the execution. The unreliable environment distributions are \ndiscrete be\u00adcause E is a countable set. Therefore, . ' can be de.ned pointwise: for any environment e \n' . E, the value of . ' (e ' )  u u is the probability that the unreliable execution of the state\u00adment \ns results in the environment e ' given the distribu\u00ad u tion on possible starting environments, ., and \nthe aggregate probability pu of reaching e ' from any starting environment u eu . E according to the \nbig-step aggregate semantics. In general, . ' is a subprobability measure because it is de.ned using \nthe big-step aggregate semantics, which is also a sub\u00adprobability measure (Section 3.4).  4.2 Reliability \nPredicates and Transformers The paired execution semantics enables us to de.ne the se\u00admantics of statements \nas transformers on reliability predi\u00adcates that bound the reliability of program variables. A reli\u00adability \npredicate P is a predicate of the form: P . true | false | R = R | P . P R . r | R(X) | R \u00b7 R A predicate \ncan either be the constant true, the constant false, a comparison between reliability factors (R), or \na conjunction of predicates. A reliability factor is real-valued quantity that is either a constant r \nin the range [0, 1]; a joint reliability factor R(X) that gives the probability that all program variables \nin the set X have the same value in the unreliable execution as they have in the reliable execution; \nor a product of reliability factors, R \u00b7 R. This combination of predicates and reliability factors en\u00adables \nus to specify bounds on the reliability of variables in the program, such as 0.99999 = R({x}), which \nstates the probability that x has the correct value in an unreliable exe\u00adcution is at least 0.99999. \n4.2.1 Semantics of Reliability Predicates. Figure 9 presents the denotational semantics of reliability \npredicates via the semantic function [P ]. The denotation of a reliability predicate is the set con.gurations \nthat satisfy the predicate. We elide a discussion of the semantics of reliability predicates themselves \nbecause they are standard and instead focus on the semantics of joint reliability factors. Joint Reliability \nFactor. A joint reliability factor R(X) represents the probability that an unreliable environment eu \nsampled from the unreliable environment distribution . has the same values for all variables in the set \nX as that in the reliable environment e. To de.ne this probability, we use the function E(X, e), which \ngives the set of environments that have the same values for all variables in X as in the envi\u00adronment \ne. The denotation of a joint reliability factor is then the sum of the probabilities of each of these \nenvironments according to .. Auxiliary De.nitions. We de.ne predicate satisfaction and validity, respectively, \nas follows: (e, .) |= P = (e, .) . [P ]|= P = .e.... (e, .) |= P  4.2.2 Reliability Transformer Given \na semantics for predicates, we can now view the paired execution of a program as a reliability transformer \nnamely, a transformer on reliability predicates that is rem\u00adiniscent of Dijkstra s Predicate Transformer \nSemantics [22]. De.nition 4 (Reliability Transformer). . |= {P } s {Q} = .e.....e ' ... ' . ((e, .) |= \nP . (s, (e, .)) .. (e ' , . ' )) . (e ' , . ' ) |= Q The paired execution of a statement s is a transformer \non reliability predicates, denoted . |= {P } s {Q}. Speci.\u00adcally, the paired execution of s transforms \nP to Q if for all (e, .) that satisfy P and for all (e ' , . ' ) yielded by the paired execution of s \nfrom (e, .), (e ' , . ' ) satis.es Q. The paired execution of s transforms P to Q for any P and Q where \nthis relationship holds. Reliability predicates and reliability transformers allow us to use symbolic \npredicates to characterize and constrain the shape of the unreliable environment distributions before \nand after execution of a statement. This approach provides a well-de.ned domain in which to express Rely \ns reliability analysis as a generator of constraints on the shape of the unreliable environment distributions \nfor which a function satis.es its reliability speci.cation. 5. Reliability Analysis For each function \nin a program, Rely s reliability analysis generates a symbolic reliability precondition with a precon\u00addition \ngenerator style analysis. The reliability precondition is a reliability predicate that constrains the \nset of speci.ca\u00adtions that are valid for the function. Speci.cally, the reliabil\u00ad o ity precondition \nis of the form Ri = Rj where Ri is the i,j reliability factor for a developer-provided speci.cation of \na function output and Rj is a reliability factor that gives a con\u00adservative lower bound on the reliability \nof that output. If the reliability precondition is valid, then the developer-provided speci.cations are \nvalid for the function. 5.1 Preliminaries Transformed Semantics. We formalize Rely s analysis over a \ntransformed semantics of the program that we pro\u00ad duce via a source-to-source transformation function \nT that performs two transformations: Conditional Flattening. Each conditional has a unique control .ow \nvariable \u00a3 associated with it that we use to .atten a conditional of the form ifc (b) {s1} {s2} to the \nsequence \u00a3 = b ; ifc (\u00a3) {s1} {s2}. This transformation rei.es the control .ow variable as an explicit \nprogram variable that records the value of the conditional.  SSA. We transform a Rely program to a SSA \nrenamed version of the program. The f-nodes for a conditional include a reference to the control .ow \nvariable for the   [P ] . P (E \u00d7 F) [true] = E \u00d7 F [false] = \u00d8 [P1 . P2] = [P1] n [P2] [R1 = R2] = \n{(e, .) | [R1](e, .) = [R2](e, .)}  [R] . E \u00d7 F . R [r](e, .) = r [R1 \u00b7 R2](e, .) = [R1](e, .) \u00b7 [R2](e, \n.) [R(X)](e, .) =.(eu) eu.E(X,e) II I E . P (Var + ArrVar) \u00d7 E . P (E) E(X, e) = {e| e. E . .v. v . \nX . equiv(e, e, v)}) II II equiv((s:: d, hI), (s :: d, h), v) = .i . 0 = i < len(v, s) . h(pbase(s(v)) \n+ i) = h(pbase(s(v)) + i)  len(v, s) = let (n0, . . . , nk) = pdim(s(v)) inni 0=i=k Figure 9. Predicate \nSemantics conditional. For example, we transform a sequence of the form \u00a3 = b ; ifc (\u00a3) {x = 1} {x = \n2} to the sequence \u00a3 = b ; ifc (\u00a3) {x1 = 1} {x2 = 2} ; x = f(\u00a3, x1, x2). We rely on standard treatments \nfor the semantics of f\u00adnodes [4]. For arrays, we use a simple Array SSA [31]. We also note that we apply \nthe SSA transformation such that a reference of a parameter at any point in the body of the function \nrefers to its initial value on entry to the func\u00adtion. This property naturally gives a function s reliability \nspeci.cations a semantics that refers to the reliability of variables on entry to the function. These \ntwo transformations together allow us to make ex\u00adplicit the dependence between the reliability of a condi\u00adtional \ns control .ow variable and the reliability of variables modi.ed within. Auxiliary Maps. The map . . Var \n. M is a map from program variables to their declared memory regions. We compute this map by inspecting \nthe parameter and variable declarations in the function. The map G . Var . R is a unique map from the \noutputs of a function namely, the return value and arrays passed as parameters to the reliability factors \n(Section 4.2) for the developer-provided speci.cation of each output. We allocate a fresh variable named \nret that represents the return value of the program. Substitution. A substitution e0[e2/e1] replaces \nall occur\u00adrences of the expression e1 with the expression e2 within the expression e0. Multiple substitution \noperations are applied from left to right. The substitution matches set patterns. For instance, the pattern \nR({x} . X) represents a joint relia\u00adbility factor that contains the variable x, alongside with the remaining \nvariables in the set X. Then, the result of the sub\u00adstitution r1 \u00b7 R({x, z})[r2 \u00b7 R({y} . X)/R({x} . \nX)] is the expression r1 \u00b7 r2 \u00b7 R({y, z}). 5.2 Precondition Generation The analysis generates preconditions \naccording to a con\u00adservative approximation of the paired execution semantics. Speci.cally, it characterizes \nthe reliability of a value in a function according to the probability that the function com\u00adputes that \nvalue including its dependencies fully reliably given a hardware speci.cation. Figure 10 presents a \nselection of Rely s reliability pre\u00adcondition generation rules. The generator takes as input a statement \ns, a postcondition Q, and (implicitly) the maps . and G. The generator produces as output a precondition \nP , such that if P holds before the paired execution of s, then Q holds after. We have crafted the analysis \nso that Q is the constraint over the developer-provided speci.cations that must hold at the end of execution \nof a function. Because arrays are passed by reference in Rely and can therefore be modi.ed, one property \nthat must hold at the end of execution of a function is that each array must be at least as reliable \nas implied by its speci.cation. Our analysis captures this property by setting the initial Q for the \nbody of a function to o ' G(ai) = R(ai) ai ' where ai is the ith array parameter of the function and \na is i an SSA renamed version of the array that contains the appro\u00adpriate value of ai at the end of the \nfunction. This constraint therefore states that the reliability implied by the speci.ca\u00adtions must be \nless than or equal to the actual reliability of each input array at the end of the function. As the precon\u00addition \ngenerator works backwards through the function, it generates a new precondition that if valid at the \nbeginning of the function ensures that Q holds at the end. 5.2.1 Reasoning about Expressions The topmost \npart of Figure 10 .rst presents our rules for rea\u00adsoning about the reliability of evaluating an expression. \nThe reliability of evaluating an expression depends on two fac\u00adtors: 1) the reliability of the operations \nin the expression and 2) the reliability of the variables referenced in the expres\u00adsion. The function \n. . (Exp + BExp) . R\u00d7 P (Var) com\u00adputes the core components of these two factors. It returns a pair consisting \nof 1) the probability of correctly executing all operations in the expression and 2) the set of variables \nref\u00aderenced by the expression. The projections .1 and .2 return each component, respectively. Using these \nprojections, the  . . (Exp + BExp) . R\u00d7 P (Var) .(n) = (1, \u00d8) .(x) = (.(rd(.(x))), {x}) .(e1 iop e2) \n= (.1(e1) \u00b7 .1(e2) \u00b7 .(iop) , .2(e1) . .2(e2)) .1(e) = p1(.(e)) .2(e) = p2(.(e)) RP. RP.(return e, Q) \nRP.(x = e, Q) RP.(x = a[e1, . . . , en], Q) RP.(a[e1, . . . , en] = e, Q) RP.(skip, Q) RP.(s1 ; s2, \nQ) RP.(ife e s1 s2, Q) RP.(x = f(e, x1, x2), Q) RP.(whilee b : 0 s, Q) RP.(whilee b : n s, Q) RP.(int \nx = e in m, Q) RP.(int a[n0, . . . , nk] in m, Q) . S \u00d7 P . P = Q . G(ret) = .1(e) \u00b7 R(.2(e)) = Q [(.1(e) \n\u00b7 .(wr(.(x))) \u00b7 R(.2(e) . X))/R({x} . X)] = Q [ ((.1(ei)) \u00b7 .(rd(.(a))) \u00b7 .(wr(.(x))) \u00b7 R({a} . ( .2(ei)) \n. X))/R({x} . X)] i i = Q [ (.1(e) \u00b7 (.1(ei)) \u00b7 .(wr(.(a))) \u00b7 R(.2(e) . ( .2(ei)) . {a} . X))/ R({a} \n. X)] i i = Q = RP.(s1, RP.(s2, Q)) = RP.(s1, Q) . RP.(s2, Q) = Q [R({e, x1} . X)/R({x} . X)] . Q [R({e, \nx2} . X)/R({x} . X)] = Q = RP.(T (ifen b {s ; whilee b : (n - 1) s} skip), Q) = RP.(x = e, Q) = Q [R(X)/R({a} \n. X)] Figure 10. Reliability Precondition Generation reliability of an expression e given any reliable \nenviron\u00adment and unreliable environment distribution is therefore at least .1(e) \u00b7 R(.2(e)), where R(.2(e)) \nis the joint relia\u00adbility of all the variables referenced in e. We elide the rules for boolean and relational \noperations, but they are de.ned analogously.  5.2.2 Generation Rules for Statements We next present \nthe precondition generation rules for Rely statements. As in a precondition generator, the analysis works \nbackwards from the end of the program towards the beginning. We have therefore structured our discussion \nof the statements starting with function returns. Function Returns. When execution reaches a function \nre\u00adturn, return e, the analysis must verify that the reliability of the return value is greater than \nthe reliability that the de\u00adveloper speci.ed. To verify this, the analysis rule generates the additional \nconstraint G(ret) = .1(e) \u00b7 R(.2(e)). This constrains the reliability of the return value, where G(ret) \nis the reliability speci.cation for the return value. Assignment. For the program to satisfy a predicate \nQ af\u00adter the execution of an assignment statement x = e, then Q must hold given a substitution of the \nreliability of the ex\u00adpression e for the reliability of x. The substitution Q[(.1(e)\u00b7 .(wr(.(x)))\u00b7R(.2(e) \n.X))/R({x} . X)] binds each reli\u00adability factor in which x occurs R({x} . X) and replaces the factor \nwith a new reliability factor R(.2(e) . X) where .2(e) is the set of variables referenced by e. The substitution \nalso multiplies the reliability factor by .1(e)\u00b7.(wr(.(x))), which is the probability that e evaluates \nfully reliably and its value is reliably written to the memory location for x. Array loads and stores. \nThe reliability of a load statement x = a[e1, . . . , en] depends on the reliability of the indices e1, \n. . . , en, the reliability of the values stored in a, and the reliability of reading from a s memory \nregion. The rule s implementation is similar to that for assignment. The reliability of an array store \na[e1, . . . , en] = e de\u00adpends on the reliability of the source expression e, the relia\u00adbility of the \nindices e1, . . . , en , and the reliability of writing to a. Note that the rule preserves the presence \nof a within the reliability term. By doing so, the rule ensures that it tracks the full reliability of \nall the elements within a. Conditional. For the program to satisfy a predicate Q after a conditional \nstatement ifc b s1 s2, each branch must satisfy Q. The rule therefore generates a precondition that is \na conjunction of the results of the analysis of each branch. Phi-nodes. The rule for a f-node x = f(\u00a3, \nx1, x2) captures the implicit dependence of the effects of control .ow on the value of a variable x. \nFor the merged value x, the rule es\u00adtablishes Q by generating a precondition that ensures that Q holds \nindependently for both x1 and x2, given an appropriate substitution. Note that the rule also includes \n\u00a3 in the substi\u00adtution; this explicitly captures x s dependence on \u00a3. The .at\u00adtening statement inserted \nbefore a conditional (Section 5.1), later replaces the reliability of \u00a3 with its dependencies. Bounded \nwhile and repeat. Bounded while loops, whilec b : n s, and repeat loops, repeat n s, execute their bodies \nat most n times. Execution of such a loop there\u00adfore satis.es Q if P holds beforehand, where P is the \nre\u00adsult of invoking the analysis on n sequential copies of the body. The rule implements this approach \nvia a sequence of bounded recursive calls to transformed versions of itself.  Unbounded while. We present \nthe analysis for unbounded while loops in Section 5.2.3. Function calls. The analysis for functions is \nmodular and takes the reliability speci.cation from the function declara\u00adtion and substitutes the reliabilities \nof the function s formal arguments with the reliabilities of the expressions that repre\u00adsent the corresponding \nactual arguments of the function. We present the rule for function calls in the Appendix [14]. Note that \nthis modular approach supports reasoning about recursion. When we analyze a function, if we assume that \nthe speci.cation of a recursive invocation is valid, then the result of the recursive call is no more \nreliable then the speci.cation we re trying to verify. If we perform any unreliable compu\u00adtation on that \nresult, then it is less reliable then our spec\u00adi.cation and therefore cannot be veri.ed unless the given \nspeci.cation is zero. This is consistent with our analysis of unbounded while loops (Section 5.2.3). \n5.2.3 Unbounded while Loops. An unbounded loop, whilec b s, may execute for a num\u00adber of iterations that \nis not bounded statically. The reliabil\u00adity of a variable that is modi.ed unreliably within a loop and \nhas a loop-carried dependence is a monotonically de\u00adcreasing function of the number of loop iterations. \nThe only sound approximation of the reliability of such a variable is therefore zero. However, unbounded \nloops may also update a variable reliably. In this case, the reliability of the variable is the joint \nreliability of its dependencies. We have imple\u00ad mented an analysis for unbounded while loops to distin\u00adguish \nthese two cases as follows: Dependence Graph. Our analysis .rst constructs a depen\u00addence graph for the \nloop. Each node in the dependence graph corresponds to a variable that is read or written within the \ncondition or body of the loop. There is a directed edge from the node for a variable x to the node for \na variable y if the value of y depends on the value of x. We additionally clas\u00adsify each edge as reliable \nor unreliable meaning that a reli\u00adable or unreliable operation creates the dependence. There is an edge \nfrom the node for a variable x to the node for the variable y if one of the following holds: Assignment: \nthere is an assignment to y where x occurs in the expression on the right hand side of the assign\u00adment; \nthis condition captures direct data dependencies. We classify such an edge as reliable if every operation \nin the assignment (i.e., the operations in the expression and the write to memory itself) are reliable. \nOtherwise, we mark the edge as unreliable. The rules for array load and store statements are similar, \nand include dependen\u00adcies induced by the computation of array indices.  Control Flow Side Effects: y \nis assigned within an if statement and the if statement s control .ow variable is named x; this condition \ncaptures control dependencies. We classify each such edge as reliable.  The analysis uses the dependence \ngraph to identify the set of variables in the loop that are reliably updated. A variable x is reliably \nupdated if all simple paths (and simple cycles) to x in the dependence graph contain only reliable edges. \nFixpoint Analysis. Given a set of reliably updated vari\u00adables Xr, the analysis next splits the postcondition \nQ into two parts. For each predicate Ri = r \u00b7 R(X) in Q (where Ri is a developer-provided speci.cation), \nthe analysis checks if the property .x . X . x . modset(s) . x . Xr holds, where modset(s) computes the \nset of variables that may be modi.ed by s. If this holds, then all the variables in X are ei\u00adther modi.ed \nreliably or not modi.ed at all within the body of the loop. The analysis conjoins the set of predicates \nthat satisfy this property to create the postcondition Qr and con\u00adjoins the remaining predicates to create \nQu. The analysis next iterates the function F (A) starting from true, where F (A) = Qr . RP.(T (ifc b \ns skip), A), until it reaches a .xpoint. The resulting predicate Q ' is a trans\u00ad r lation of Qr such \nthe joint reliability of a set of variables is replaced by the joint reliability of its dependencies. \nLemma 1 (Termination). Iteration of F (A) terminates. This follows by induction on iterations, the monotonicity \nof RP and the fact that the range of F (A) (given a simpli\u00ad.er that removes redundant predicates, Section \n5.4) is .nite (together, .nite descending chains). The key intuition is that the set of real-valued constants \nin the precondition before and after an iteration does not change (because all variables are reliably \nupdated) and the set of variables that can occur in a joint reliability factor is .nite. Therefore, there \nare a .\u00adnite number of unique preconditions in the range of F (A). We present a proof sketch in the Appendix \n[14]. Final Precondition. In the last step, the analysis produces a .nal precondition that preserves \nthe reliability of variables that are reliably updated by conjoining Q ' with the predicate r Qu[(Ri \n= 0)/(Ri = Rj )], where Ri and Rj are joint relia\u00adbility factors. The substitution on Qu sets the joint \nreliability factors that contain unreliably updated variables to zero.  5.2.4 Properties Rely s reliability \nanalysis is sound with respect to the trans\u00adformer semantics laid out in Section 4. Theorem 2 (Soundness). \n. |= {RP.(s, Q)} s {Q} This theorem states that if a con.guration (e, .) satis.es a generated precondition \nand the paired execution of s yields a con.guration (e ' , . ' ), then (e ' , . ' ) satis.es Q. Alterna\u00adtively, \ns transforms the precondition generated by our anal\u00adysis to Q. We present a proof sketch in the Appendix \n[14].  5.3 Speci.cation Checking As the last step of the analysis for a function, the analysis checks \nthe developer-provided reliability speci.cations for the function s outputs as captured by the precondition \ngen\u00aderator s .nal precondition. Because each speci.cation has the form r \u00b7 R(X) (Figure 1) the precondition \nis a conjunc\u00adtion of predicates of the form r1 \u00b7 R(X1) = r2 \u00b7 R(X2). While these joint reliability factors \nrepresent arbitrary and potentially complex distributions of the values of X1 and X2, there is simple \nand sound (though not complete) proce\u00addure to check the validity of each predicate in a precondition \nthat follows from the ordering of joint reliability factors.  Proposition 1 (Ordering). For two sets \nof variables X and Y , if X . Y then R(Y ) = R(X). This follows from the fact that the joint reliability \nof a set of variables Y is less than or equal to the joint reliability of any subset of the variables \n regardless of the distribution of their values. As a consequence of the ordering of joint reliability \nfactors, there is a simple and sound method to check the validity of a predicate. Corollary 1 (Predicate \nValidity). If r1 = r2 and X2 . X1 then |= r1 \u00b7 R(X1) = r2 \u00b7 R(X2). The constraint r1 = r2 is a comparison \nof two real numbers and the constraint X2 . X1 is an inclusion of .nite sets. Note that both types of \nconstraints are decidable and ef.ciently checkable. Checking. Because the predicates in the precondition \ngen\u00aderator s output are mutually independent, we can use Corol\u00adlary 1 to check the validity of the full \nprecondition by check\u00ading the validity of each predicate in turn.  5.4 Implementation We implemented \nthe parser for the Rely language, the pre\u00adcondition generator, and the precondition checker in OCaml. \nThe implementation consists of 2500 lines of code. The anal\u00adysis can operate on numerical or symbolic \nhardware reliabil\u00adity speci.cations. Our implementation performs simpli.ca\u00adtion transformations after \neach precondition generator step to simplify numerical expressions and remove predicates that are trivially \nvalid or subsumed by another predicate. Proposition 2 (Predicate Subsumption). A reliability pred\u00adicate \nr1 \u00b7 R(X1) = r2 \u00b7 R(X2) subsumes (i.e., soundly re\u00ad ' '' ''' places) a predicate r1 \u00b7 R(X1) = r2 \u00b7 R(X2) \nif r1\u00b7R(X1) = ' ' r1 \u00b7 R(X1) and r2 \u00b7 R(X2) = r2 \u00b7 R(X2) 6. Case Studies We next discuss six computations \n(three checkable, three approximate) that we implemented and analyzed with Rely. 6.1 Analysis Summary \nTable 11 presents our benchmark computations and Rely s analysis results. For each benchmark, we present \nthe type of the computation (checkable or approximate), its length in lines of code (LOC), the execution \ntime of the analysis, and the number of inequality predicates in the .nal precondition produced by the \nprecondition generator.   Benchmark Type LOC Time (ms) Predicates newton Checkable 21 8 1 secant Checkable \n30 7 2 coord Checkable 36 19 1 search ref Approximate 37 348 3 matvec Approximate 32 110 4 hadamard Approximate \n87 18 3 Figure 11. Benchmark Analysis Summary Benchmarks. We analyze the following six computations: \n newton: This computation searches for a root of a uni\u00advariate function using Newton s Method.  secant: \nThis computation searches for a root of a univari\u00adate function using the Secant Method.  coord: This \ncomputation calculates the Cartesian coordi\u00adnates from the polar coordinates passed as the input.  search \nref: This computation performs a simple motion estimation. We presented this computation in Section 2. \n mat vec: This computation multiplies a matrix and a vector and stores the result in another vector. \n hadamard: This computation takes as input two blocks of 4x4 pixels and computes the sum of differences \nbe\u00adtween the pixels in the frequency domain.  We provide a detailed description of the benchmarks, includ\u00ading \nthe Rely source code, in the Appendix [14]. Analysis Time. We executed Rely s analysis on an Intel Xeon \nE5520 machine with 16 GB of main memory. The analysis times for all benchmarks are under one second. \nNumber of Predicates. We used the hardware reliability speci.cation from Figure 5 to generate a reliability \nprecon\u00addition for each benchmark. The number of predicates in each benchmark s precondition is small \n(all less than .ve) be\u00adcause simpli.cation removes most of the additional predi\u00adcates introduced by the \nrules for conditionals. Speci.cally, these predicates are often subsumed by another predicate.  6.2 \nReliability and Accuracy A developer s choice of reliability speci.cations is typically in.uenced by \nthe perceived effect that the unreliable exe\u00adcution of the computation may have on the accuracy of its \nresult and its execution time and energy consumption. We present two case studies that illustrate how \ndevelopers can use Rely to reason about the tradeoffs between accuracy and performance that are available \nfor checkable computations and approximate computations. 6.2.1 Checkable Computations Checkable computations \nare those that can be augmented with an ef.cient checker that dynamically veri.es the cor\u00adrectness of \nthe computation s result. If the checker detects an error, then it reexecutes the computation or executes \nan alternative reliable implementation. We next present how a developer can use Rely to build and reason \nabout the perfor\u00admance of a checked implementation of Newton s method.  1 # d e f i n e t o l e r a \nn c e 0 . 0 0 0 0 0 1 2 # d e f i n e m a x s t e p s 4 0 3 4 f l o a t < 0 . 9 9 9 9 * R ( x ) > F ( \nf l o a t x i n u r e l ) ; 5 f l o a t < 0 . 9 9 9 9 * R ( x ) > d F ( f l o a t x i n u r e l ) ; 6 \n7 f l o a t < 0 . 9 9 * R ( x s ) > n e w t o n ( f l o a t x s i n u r e l ) { 8 f l o a t x , x p r \ni m i n u r e l ; 9 f l o a t t 1 , t 2 i n u r e l ; 10 11 x = x s ; 12 x p r i m = x s + . 2 * . t \no l e r a n c e ; 13 14 w h i l e ( ( x -. x p r i m > = . t o l e r a n c e ) 15 | | . ( x -. x p r \ni m < = . -t o l e r a n c e ) 16 ) : m a x s t e p s { 17 x p r i m = x ; 18 t 1 = F ( x ) ; 19 t 2 \n= d F ( x ) ; 20 x = x -. t 1 / . t 2 ; 21 } 22 23 i f ( ! . ( ( x -. x p r i m < = . t o l e r a n c \ne ) 24 &#38; &#38; . ( x -. x p r i m > = . -t o l e r a n c e ) ) ) { 25 x = I N F T Y ; 26 } 27 r e \nt u r n x ; 28 } Figure 12. Newton s Method Implementation  Newton s method. Figure 12 presents an unreliable \nim\u00adplementation of Newton s method in Rely. Newton s method searches for a root of a function; given \na differentiable func\u00adtion f(x), its derivative f ' (x), and a starting point xs, it com\u00adputes a value \nx0 such that f(x0) = 0. This is an example of a .xed-point iteration computation that executes a while \nloop at most maxstep steps. The com\u00adputation within each loop iteration of the method can execute unreliably: \neach iteration updates the estimate of the root x by computing the value of the function f and the derivative \nf '. If the computation converges in the maximum number of steps, the function returns the produced value. \nOtherwise it returns the error value (in.nity). The reliability of the com\u00adputation depends on the reliability \nof the starting value xs and the reliability of the functions f and f '. If the reliability speci.cation \nof f is float<0.9999*R(x)> F(float x) (and similar for f '), then the analysis veri.es that the reli\u00adability \nof the whole computation is at least 0.99*R(xs). Checked Implementation. A developer can build a checked \nimplementation of newton with the following code: f l o a t r o o t = n e w t o n ( x s ) ; f l o a t \ne z e r o = f ( r o o t ) ; i f ( e z e r o < -t o l e r a n c e | | e z e r o > t o l e r a n c e ) \nr o o t = n e w t o n _ r ( x s ) ; To check the reliability of the root x0 that newton produces, it \nis suf.cient to evaluate the function f(x0) and check if the result is zero (within some tolerance). \nIf the checker detects that the result is not a zero, then the computation calls newton_r, a fully reliable \nalternative implementation. Reliability/Execution Time Tradeoff. Quantitative relia\u00adbility allows us \nto model the performance of this checked implementation of Newton s method. Let tu be the expected execution \ntime of newton, tum the expected execution time of newton when executed for its maximum number of in\u00adternal \nloop iterations, tc the expected execution time of the checker, and tr the expected execution time of \nnewton r. The expected execution time of the checked computation when newton produces a correct result \nis T1 = tu + tc. In the case when newton produces an incorrect result, the expected execution time is \nat most T2 = tum + tc + tr (i.e., the maximum expected execution time of newton plus the expected execution \ntime of both the checker and the reexecution via newton r). This formula is conservative because it assumes \nthat a fault causes newton to execute for its maximum number of iterations. If we denote the reliability \nof newton by r, then the ex\u00adpected execution time of the checked computation as a whole ' is T = r \u00b7 \nT1 + (1 - r) \u00b7 T2, which produces a projected speedup s, where s = tr/T '. These formulas allow a de\u00adveloper \nto .nd the reliability r that meets the developer s performance improvement goal and can be analogously \nap\u00adplied for alternative resource usage measures, such as energy consumption and throughput. Example. \nAs an illustration, let us assume that the compu\u00adtation executes on unreliable hardware and its reliable \nver\u00adsion is obtained using software-level replication. Using the replication approach proposed in [57], \nthe replicated imple\u00admentation is 40% slower than the unreliable version i.e, tr = 1.4tu. Furthermore, \nlet the reliable Newton s method computation converge on average in a half of the maximum number of steps \n(i.e., tu = tum/2) and let the execution time of the checker be half of the time of a single iteration \nof Newton s method. For a projected speedup of 1.32, the de\u00adveloper can use the previous formulas to \ncompute the target reliability r = 0.99. Rely can verify that newton u executes with this reliability \n(given the hardware reliability speci.ca\u00adtion from Figure 5) when the input xs is fully reliable.  6.2.2 \nApproximate Computations Many applications can tolerate inaccuracies in the results of their internal \napproximate computations, which are compu\u00adtations that can produce a range of results of different quality. \nThe approximate computations in these applications can be transformed to trade accuracy of their results \nfor increased performance by producing lower quality results. In building and transforming approximate \ncomputations, there are two correctness properties that a developer or trans\u00adformation system must consider: \nintegrity which ensures that the computation produces a valid result that the remain\u00ading computation \ncan consume without failing and accuracy which determines the quality of the result itself [12]. In \nthis case study, we present how a developer can use Rely in combination with other techniques to reason \nabout the integrity and accuracy of search_ref (Section 2).  Integrity. Recall that search_ref searches \nfor the index of the block within the array of pixel blocks pblocks that is the minimum distance from \nthe block of pixels cblock. An important integrity property for search_ref is therefore that it returns \na valid index: an index that is at least 0 and at most nblocks -1. However, this property may not hold \nin an unreliable execution because search_ref s unreliable operations may cause it to return an arbitrary \nresult. To guard against this, a developer has several options. One of the developer s options is to \nmodify search_ref to dynamically check that its result is valid and reexecute itself if the check fails. \nThis case is analogous to that for checkable computations (Section 6.2.1), with the distinction that \nthe checker implements a partial correctness speci.cation. Another option is to modify search_ref to \ndynamically check that its result is valid and instead of reexecuting itself if the check fails, it recti.es \n[39, 58] its result by returning a valid index at random. This enables search_ref to produce a valid \n but still approximate result. Alternatively, the developer can place minblock in reli\u00adable memory and \nset its initial value to a .xed, valid index (e.g., 0); this implements a .xed recti.cation approach. \nBe\u00adcause i is also stored in reliable memory, minblock will al\u00adways contain a valid index despite search_ref \ns other un\u00adreliable operations. The developer can establish this fact ei\u00adther informally or formally \nwith relational veri.cation [11]. Accuracy. A computation s reliability bound states how often the computation \nreturns a correct result and therefore also states a conservative bound on the computation s ac\u00adcuracy. \nTo determine an acceptable reliability bound (and therefore an acceptable accuracy), the developer can \nperform local or end-to-end accuracy experiments [29, 44, 68]. As an illustration of an end-to-end experiment, \nwe present a simu\u00adlation approach for search_ref. To estimate the effects of the unreliable execution, \nwe modi.ed a fully reliable version of search_ref (one with\u00adout unreliable operations) to produce the \ncorrect minimum distance block with probability p and produce the maximum distance block with probability \n1 -p. This modi.cation pro\u00advides a conservative estimate of the bound on search_ref s accuracy loss given \na reliability p (when inputs to the com\u00adputation are reliable) and the assumption that a fault causes \nsearch_ref to return the worst possible result. We then implemented two versions of the x264 video encoder \n[71]: one with the reliable version of search_ref and one with the modi.ed version. For several values \nof p, we then compared the videos produced by the reliable and modi.ed encoders on 17 HD video sequences \n(each 200 frames in length) from the Xiph.org Video Test Media repository [40]. We quanti.ed the difference \nbetween the quality of the resulting videos via the Quality Loss Metric (QLM), previously used in [44]. \nThis metric computes a relative difference between the quality scores of the two videos, each of which \nis computed as a weighted sum of the peak signal to noise ratio and the encoded video size. p 0.90 0.95 \n0.97 0.98 QLM 0.041 0.021 0.012 0.009 0.004 Figure 13. search ref Simulation Result Table 13 presents \nthe average QLM as a function of the reliability of search ref. A developer can use the results of the \nsimulation to identify an acceptable amount of qual\u00adity loss for the encoded video. For example, if the \ndeveloper is willing to accept at most 1% quality loss (which corre\u00adsponds to the QLM value 0.01), then \nthe developer can se\u00adlect 0.98 from Table 13 as the target reliability for an unre\u00adliable version of \nsearch_ref. The reliability speci.cation that the developer then writes for an unreliable version is \n0.98*R(pblocks,cblock). As demonstrated in Section 2, Rely s analysis can verify that the presented unreliable \nim\u00adplementation satis.es an even tighter reliability (i.e., 0.99). 7. Extensions In this section we discuss \nseveral extensions to the research we presented in this paper. Unreliable Inputs. Some computations (for \nexample, computations that read unreliable sensors or work with noisy data) may process unreliable inputs. \nA straightforward ex\u00adtension of Rely s analysis would incorporate this additional source of unreliability \ninto the analysis framework. The re\u00adsulting enhanced system would make it possible to reason about how \nunreliable inputs propagate through the program to affect the reliability of the outputs, enabling a \nglobal un\u00adderstanding of the impact of unreliable inputs on the relia\u00adbility of the computation as a \nwhole. Optimization. It is possible to extend Rely to optimize the placement of unreliable operations \nto maximize the perfor\u00admance of the computation subject to a reliability speci.ca\u00adtion. One approach \nwould be to extend the precondition gen\u00aderator to produce a distinct symbolic constant (instead of .(op)) \nfor each location that contains an arithmetic opera\u00adtion. The .nal precondition can be used to select \narithmetic operations that can execute unreliably while still satisfying the given reliability bound. \nThe precondition generator could be similarly extended to generate symbolic expressions for upper bounds \non the number of iterations of bounded loops. These expressions could then help the developer select \nloop bounds to satisfy the reliability speci.cation of a function. Dynamic Reliability Analysis. Rely \ns static analysis pro\u00adduces sound but conservative reliability estimates. A dy\u00adnamic analysis could use \nfault injection to obtain potentially more precise (but unsound) reliability estimates. It would, in \nprinciple, be possible to combine these analyses. One ap\u00adproach would use a dynamic analysis to obtain \nreliability speci.cations for selected functions, then the static analysis for the remaining part of \nthe program. In this scenario the static analysis of callers would use the dynamically derived reliability \nspeci.cations for callees.  Precise Array Analysis. A more precise array analysis can be used to distinguish \nreads and writes to different array elements. We anticipate that existing techniques for analyz\u00ading data \ndependencies in loops can be used to produce more precise sets of array locations that an array read \nor write op\u00aderation can affect. For example, we can extend Rely with ar\u00adray alias speci.cations which \nwould allow for more precise reasoning for loops where elements of one array are unreli\u00adably computed \nfrom elements of a separate array. Reliability Preconditions. The function s reliability spec\u00adi.cations \nthat Rely s analysis veri.es are valid for all calling context. Developers may, however, desire specify \nconditions that are valid only for some calling context, such as a pre\u00adcondition that the function is \ncalled only on fully reliable inputs. We anticipate that Rely s general predicate logic will make it \npossible to extend Rely with preconditions without modifying the underlying analysis domain and structure. \n8. Related Work 8.1 Critical and Approximate Regions Almost all approximate computations have critical \nregions that must execute without error for the computation as a whole to execute acceptably. Dynamic \nCriticality Analysis. One way to identify crit\u00adical and approximate regions is to change different regions \nof the computation or data in some way and observe the effect. To the best of our knowledge, Rinard was \nthe .rst to present a technique (task skipping) designed to iden\u00adtify critical and approximate regions \nin approximate com\u00adputations [59, 60]. Carbin and Rinard subsequently pre\u00adsented a technique that uses \ndirected fuzzing to identify critical and approximate computations, program data, and input data [16]. \nOther techniques use program transforma\u00adtions [44] and input fuzzing [3]. Static Criticality Analysis. \nResearchers have also devel\u00adoped several speci.cation-based static analyses that let the developer identify \nand separate critical and approximate re\u00adgions. Flikker [38] is a set of language extensions with a runtime \nand hardware support to enable more energy ef.\u00adcient execution of programs on inherently unreliable memo\u00adries. \nIt allows a developer to partition data into critical and approximate regions (but does not enforce full \nseparation be\u00adtween the regions). Based on these annotations, the Flikker runtime allocates and stores \ndata in a reliable or unreliable memory. Sampson et al. [64] present EnerJ, a programming language with \nan information-.ow type system that allows a developer to partition program s data into approximate and \ncritical data and ensures that operations on approximate data do not affect critical data or memory safety \nof programs. All of this prior research focuses on the binary distinction between reliable and approximate \ncomputations. In contrast, the research presented in this paper allows a developer to specify and verify \nthat even approximate computations pro\u00adduce the correct result most of the time. Overall, this addi\u00adtional \ninformation can help developers better understand the effects of deploying their computations on unreliable \nhard\u00adware and exploit the bene.ts that unreliable hardware offers. 8.2 Relational Reasoning for Approximate \nPrograms Carbin et al. [11] present a veri.cation system for relaxed approximate programs based on a \nrelational Hoare logic. The system enables rigorous reasoning about the integrity and worst-case accuracy \nproperties of a program s approxi\u00admate regions. Their work builds upon the relational veri.ca\u00adtion techniques \noriginated in Rinard et al. s Credible Com\u00adpilation [63], Pnueli et al. s Translation Validation [54], \nand later by Benton s Relational Hoare Logic [7]. Rely differs from these approaches because of its prob\u00adabilistic \nrelational reasoning: speci.cally, the probability that an unreliable implementation returns the correct \nresult. However, these non-probabilistic approaches are comple\u00admentary to Rely in that they enable reasoning \nabout the non\u00adprobabilistic properties of an approximate computation. 8.3 Accuracy Analysis In addition \nto reasoning about how often a computation may produce a correct result, it may also be desirable to \nreason about the accuracy of the result that the computation pro\u00adduces. Dynamic techniques observe the \naccuracy impact of program transformations [1, 2, 29, 41, 42, 44, 59, 60, 62, 68], or injected soft errors \n[20, 37, 38, 64, 70]. Researchers have developed static techniques that use probabilistic reason\u00ading \nto characterize the accuracy impact of various phenom\u00adena [6, 18, 43, 47, 56, 72]. And of course, the \naccuracy im\u00adpact of the .oating point approximation to real arithmetic has been extensively studied by \nnumerical analysts [17].  8.4 Probabilistic Program Analysis Kozen s work [32] was the .rst to propose \nthe analysis of probabilistic programs as transformers of discrete probabil\u00adity distributions. Researchers \nhave since developed a number of program analyses for probabilistic programs, including those based on \naxiomatic reasoning [5, 6, 46], abstract inter\u00adpretation [19, 21, 45, 69], and symbolic execution [27, \n65]. The language features that introduce probabilistic nonde\u00adterminism in programs that this previous \nresearch studied in\u00adclude probabilistic sampling, x = random() [5, 6, 32, 45], probabilistic choice between \nstatements, s1 .p s2 [19, 21, 46], and speci.cations of the distributions of computation s inputs [69]. \nRely re.nes the probabilistic operators by de.n\u00ading a set of unreliable arithmetic and memory read/write \nop\u00aderations that model faults in the underlying hardware. Morgan et al. [46] propose a weakest-precondition \nstyle analysis for probabilistic programs that treats the programs as expectation transformers. Preconditions \nand postcondi\u00adtions are de.ned as bounds on probabilities that particular logical predicates hold at \na speci.ed location in the program. Rely s analysis, like [46], constructs precondition predicates for \nprogram statements. However, Rely s predicates are re\u00adlational, relating the states of the reliable and \nunreliable ex\u00adecutions of the program. Moreover, Rely s analysis is more precise as it uses direct multiplicative \nlower bounds on reli\u00adability as opposed to additive upper bounds on error.  Barthe et al. [5] de.ne \na probabilistic relational Hoare logic for a simple probabilistic imperative language that is similar \nto Kozen s. The relational predicates are arbitrary conjunctions or disjunctions of relational expressions \nover program variables, each of which is endowed with a prob\u00adability of being true. While general, this \napproach requires manual proofs or an SMT solver to verify the validity of predicates. In comparison, \nRely presents a semantics for re\u00adliability predicates that incorporates joint reliability factors, which \ncreate a simple and ef.cient checking procedure. Reliability Analysis. Analyzing the reliability of complex \nphysical and software systems is a classical research prob\u00adlem [36]. More recently researchers have presented \nsym\u00adbolic execution techniques for checking complex probabilis\u00adtic assertions. Filieri et al. [27] present \nanalysis of .nite-state Java programs. Sankaranarayanan et al. [65] present analysis of computations \nwith linear expressions and potentially un\u00adbounded loops. These techniques require knowledge of the distributions \nof the inputs to the computation. Rely s analy\u00adsis requires only the probability with which each operation \nin the computation executes correctly.  8.5 Fault Tolerance and Resilience Researchers have developed \nvarious software, hardware, or mixed approaches for detection and recovery from speci.c types of soft \nerrors that guarantee a reliable program execu\u00adtion [13, 20, 26, 28, 30, 51 53, 57, 61, 66, 70]. For \nexample, Reis et al. [57] present a compiler that replicates a computa\u00adtion to detect and recover from \nsingle event upset errors. These techniques are complementary to Rely in that each can provide implementations \nof operations that need to be reliable, as either speci.ed by the developer or as required by Rely, to \npreserve memory safety and control .ow integrity.  8.6 Emerging Hardware Architectures Recently researchers \nhave proposed multiple hardware ar\u00adchitectures to trade reliability for additional energy or perfor\u00admance \nsavings. Some of the recent research efforts include probabilistic CMOS chips [50], stochastic processors \n[48], error resilient architecture [34], unreliable memories [33, 38, 49, 64], and the Truf.e architecture \n[24]. These techniques typically use voltage scaling at different granularities. This previous research \ndemonstrated that for speci.c classes of applications, such as multimedia processing and machine learning, \nthe proposed architectures provided en\u00adergy or time savings pro.table to the user. Rely aims to help \ndevelopers better understand and control the behavior of their applications on such platforms. 9. Conclusion \nDriven by hardware technology trends, future computational platforms are projected to contain unreliable \nhardware com\u00adponents. To safely exploit the bene.ts (such as reduced en\u00adergy consumption) that such unreliable \ncomponents may provide, developers need to understand the effect that these components may have on the \noverall reliability of the ap\u00adproximate computations that execute on them. We present a language, Rely, \nfor exploiting unreliable hardware and an associated analysis that provides proba\u00adbilistic reliability \nguarantees for Rely computations execut\u00ading on unreliable hardware. By enabling developers to better \nunderstand the probabilities with which this hardware en\u00adables approximate computations to produce correct \nresults, these guarantees can help developers safely exploit the sig\u00adni.cant bene.ts that unreliable \nhardware platforms offer. Acknowledgements We thank Deokhwan Kim, Hank Hoffmann, Vladimir Kiri\u00adansky, \nStelios Sidiroglou, Rishabh Singh, and the anony\u00admous referees for their useful comments. We also note \nour previous technical report [15]. This research was supported in part by the National Science Foundation \n(Grants CCF-0905244, CCF-1036241, CCF-1138967, CCF-1138967, and IIS-0835652), the United States Department \nof Energy (Grant DE-SC0008923), and DARPA (Grants FA8650-11-C-7192, FA8750-12-2-0110). References [1] \nJ. Ansel, Y. Wong, C. Chan, M. Olszewski, A. Edelman, and S. Ama\u00adrasinghe. Language and compiler support \nfor auto-tuning variable\u00adaccuracy algorithms. CGO, 2011. [2] W. Baek and T. M. Chilimbi. Green: A framework \nfor support\u00ading energy-conscious programming using controlled approximation. PLDI, 2010. [3] T. Bao, \nY. Zheng, and X. Zhang. White box sampling in uncertain data processing enabled by program analysis. \nOOPSLA, 2012. [4] G. Barthe, D. Demange, and D. Pichardie. A formally veri.ed ssa\u00adbased middle-end: Static \nsingle assignment meets compcert. ESOP, 2012. [5] G. Barthe, B. Gr \u00b4egoire, and S. Zanella B \u00b4eguelin. \nFormal certi.cation of code-based cryptographic proofs. POPL, 2009. [6] G. Barthe, B. K \u00a8opf, F. Olmedo, \nand S. Zanella B \u00b4eguelin. Probabilistic reasoning for differential privacy. POPL, 2012. [7] N. Benton. \nSimple relational correctness proofs for static analyses and program transformations. POPL, 2004. [8] \nM. Blum and S. Kanna. Designing programs that check their work. STOC, 1989. [9] M. Blum, M. Luby, and \nR. Rubinfeld. Self-testing/correcting with applications to numerical problems. Journal of computer and \nsystem sciences, 1993. [10] F. Cappello, A. Geist, B. Gropp, L. Kale, B. Kramer, and M. Snir. To\u00adward \nexascale resilience. International Journal of High Performance Computing Applications, 2009. [11] M. \nCarbin, D. Kim, S. Misailovic, and M. Rinard. Proving accept\u00adability properties of relaxed nondeterministic \napproximate programs. PLDI, 2012. [12] M. Carbin, D. Kim, S. Misailovic, and M. Rinard. Veri.ed integrity \nproperties for safe approximate program transformations. PEPM, 2013. [13] M. Carbin, S. Misailovic, M. \nKling, and M. Rinard. Detecting and escaping in.nite loops with Jolt. ECOOP, 2011. [14] M. Carbin, S. \nMisailovic, and M. Rinard. Verifying quantitative reliability of programs that execute on unreliable \nhardware (appendix). http://groups.csail.mit.edu/pac/rely.  [15] M. Carbin, S. Misailovic, and M. Rinard. \nVerifying quantitative reliability of programs that execute on unreliable hardware. Technical Report \nMIT-CSAIL-TR-2013-014, MIT, 2013. [16] M. Carbin and M. Rinard. Automatically identifying critical input \nregions and code in applications. ISSTA, 2010. [17] F. Chaitin-Chatelin and V. Fraysse. Lectures on .nite \nprecision com\u00adputations. 1996. [18] S. Chaudhuri, S. Gulwani, R. Lublinerman, and S. Navidpour. Proving \nprograms robust. FSE, 2011. [19] P. Cousot and M. Monerau. Probabilistic abstract interpretation. ESOP, \n2012. [20] M. de Kruijf, S. Nomura, and K. Sankaralingam. Relax: an architec\u00adtural framework for software \nrecovery of hardware faults. ISCA 10. [21] A. Di Pierro and H. Wiklicky. Concurrent constraint programming: \nTowards probabilistic abstract interpretation. PPDP, 2000. [22] E. W. Dijkstra. Guarded commands, nondeterminacy \nand formal derivation of programs. CACM, 18(8), August 1975. [23] D. Ernst, N. S. Kim, S. Das, S. Pant, \nR. Rao, T. Pham, C. Ziesler, D. Blaauw, T. Austin, K. Flautner, and T. Mudge. Razor: A low-power pipeline \nbased on circuit-level timing speculation. MICRO, 2003. [24] H. Esmaeilzadeh, A. Sampson, L. Ceze, and \nD. Burger. Architecture support for disciplined approximate programming. ASPLOS, 2012. [25] H. Esmaeilzadeh, \nA. Sampson, L. Ceze, and D. Burger. Neural accel\u00aderation for general-purpose approximate programs. MICRO, \n2012. [26] S. Feng, S. Gupta, A. Ansari, and S. Mahlke. Shoestring: probabilistic soft error reliability \non the cheap. ASPLOS, 2010. [27] A. Filieri, C. P .as.areanu, and W. Visser. Reliability analysis in \nsym\u00adbolic path.nder. ICSE, 2013. [28] M. Hiller, A. Jhumka, and N. Suri. On the placement of software \nmechanisms for detection of data errors. DSN, 2002. [29] H. Hoffmann, S. Sidiroglou, M. Carbin, S. Misailovic, \nA. Agarwal, and M. Rinard. Dynamic knobs for responsive power-aware comput\u00ading. ASPLOS, 2011. [30] M. \nKling, S. Misailovic, M. Carbin, and M. Rinard. Bolt: on-demand in.nite loop escape in unmodi.ed binaries. \nOOPSLA, 2012. [31] K. Knobe and V. Sarkar. Array ssa form and its use in parallelization. POPL, 1998. \n[32] D. Kozen. Semantics of probabilistic programs. Journal of Computer and System Sciences, 1981. [33] \nK. Lee, A. Shrivastava, I. Issenin, N. Dutt, and N. Venkatasubrama\u00adnian. Mitigating soft error failures \nfor multimedia applications by se\u00adlective data protection. CASES, 2006. [34] L. Leem, H. Cho, J. Bau, \nQ. Jacobson, and S. Mitra. Ersa: error resilient system architecture for probabilistic applications. \nDATE, 2010. [35] N. Leveson, S. Cha, J. C. Knight, and T. Shimeall. The use of self checks and voting \nin software error detection: An empirical study. IEEE TSE, 1990. [36] N. Leveson and P. Harvey. Software \nfault tree analysis. Journal of Systems and Software, 3(2), 1983. [37] X. Li and D. Yeung. Application-level \ncorrectness and its impact on fault tolerance. HPCA, 2007. [38] S. Liu, K. Pattabiraman, T. Moscibroda, \nand B. Zorn. Flikker: saving dram refresh-power through critical data partitioning. ASPLOS, 2011. [39] \nF. Long, V. Ganesh, M. Carbin, S. Sidiroglou, and Martin Rinard. Automatic input recti.cation. ICSE, \n2012. [40] Xiph.org Video Test Media. http://media.xiph.org/video/derf. [41] J. Meng, A. Raghunathan, \nS. Chakradhar, and S. Byna. Exploiting the forgiving nature of applications for scalable parallel execution. \nIPDPS, 2010. [42] S. Misailovic, D. Kim, and M. Rinard. Parallelizing sequential pro\u00adgrams with statistical \naccuracy tests. ACM TECS Special Issue on Probabilistic Embedded Computing, 2013. [43] S. Misailovic, \nD. Roy, and M. Rinard. Probabilistically accurate program transformations. SAS, 2011. [44] S. Misailovic, \nS. Sidiroglou, H. Hoffmann, and M. Rinard. Quality of service pro.ling. ICSE, 2010. [45] D. Monniaux. \nAbstract interpretation of probabilistic semantics. SAS, 2000. [46] C. Morgan, A. McIver, and K. Seidel. \nProbabilistic predicate trans\u00adformers. TOPLAS, 1996. [47] D. Murta and J. N. Oliveira. Calculating fault \npropagation in func\u00adtional programs. Technical report, Univ. Minho, 2013. [48] S. Narayanan, J. Sartori, \nR. Kumar, and D. Jones. Scalable stochastic processors. DATE, 2010. [49] J. Nelson, A. Sampson, and L. \nCeze. Dense approximate storage in phase-change memory. ASPLOS Ideas &#38; Perspectives, 2011. [50] K. \nPalem. Energy aware computing through probabilistic switching: A study of limits. IEEE Transactions on \nComputers, 2005. [51] K. Pattabiraman, V. Grover, and B. Zorn. Samurai: protecting critical data in unsafe \nlanguages. EuroSys, 2008. [52] J. Perkins, S. Kim, S. Larsen, S. Amarasinghe, J. Bachrach, M. Carbin, \nC. Pacheco, F. Sherwood, S. Sidiroglou, G. Sullivan, W. Wong, Y. Zibin, M. Ernst, and M. Rinard. Automatically \npatching errors in deployed software. SOSP, 2009. [53] F. Perry, L. Mackey, G.A. Reis, J. Ligatti, D.I. \nAugust, and D. Walker. Fault-tolerant typed assembly language. PLDI, 2007. [54] A. Pnueli, M. Siegel, \nand E. Singerman. Translation validation. TACAS, 1998. [55] P. Prata and J. Silva. Algorithm based fault \ntolerance versus result\u00adchecking for matrix computations. FTCS, 1999. [56] J. Reed and B. Pierce. Distance \nmakes the types grow stronger: a calculus for differential privacy. ICFP, 2010. [57] G. Reis, J. Chang, \nN. Vachharajani, R. Rangan, and D. August. Swift: Software implemented fault tolerance. CGO 05, 2005. \n[58] M. Rinard. Acceptability-oriented computing. OOPSLA, 2003. [59] M. Rinard. Probabilistic accuracy \nbounds for fault-tolerant computa\u00adtions that discard tasks. ICS, 2006. [60] M. Rinard. Using early phase \ntermination to eliminate load imbalances at barrier synchronization points. OOPSLA, 2007. [61] M. Rinard, \nC. Cadar, D. Dumitran, D.M. Roy, T. Leu, and W.S. Beebee Jr. Enhancing server availability and security \nthrough failure\u00adoblivious computing. OSDI, 2004. [62] M. Rinard, H. Hoffmann, S. Misailovic, and S. Sidiroglou. \nPatterns and statistical analysis for understanding reduced resource computing. OOPSLA Onwards!, 2010. \n[63] M. Rinard and D. Marinov. Credible compilation with pointers. RTRV, 1999. [64] A. Sampson, W. Dietl, \nE. Fortuna, D. Gnanapragasam, L. Ceze, and D. Grossman. Enerj: Approximate data types for safe and general \nlow-power computation. PLDI, 2011. [65] S. Sankaranarayanan, A. Chakarov, and S. Gulwani. Static analysis \nfor probabilistic programs: inferring whole program properties from .nitely many paths. PLDI, 2013. [66] \nC. Schlesinger, K. Pattabiraman, N. Swamy, D. Walker, and B. Zorn. Yarra: An extension to c for data \nintegrity and partial safety. CSF 11. [67] P. Shivakumar, M. Kistler, S.W. Keckler, D. Burger, and L. \nAlvisi. Modeling the effect of technology trends on the soft error rate of combinational logic. DSN, \n2002. [68] S. Sidiroglou, S. Misailovic, H. Hoffmann, and M. Rinard. Managing performance vs. accuracy \ntrade-offs with loop perforation. FSE, 2011. [69] M. Smith. Probabilistic abstract interpretation of \nimperative programs using truncated normal distributions. Electronic Notes in Theoretical Computer Science, \n2008. [70] A. Thomas and K. Pattabiraman. Error detector placement for soft computation. DSN, 2013. [71] \nx264. http://www.videolan.org/x264.html. [72] Z. Zhu, S. Misailovic, J. Kelner, and M. Rinard. Randomized \naccuracy-aware program transformations for ef.cient approximate computations. POPL, 2012.      \n \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Emerging high-performance architectures are anticipated to contain unreliable components that may exhibit <i>soft errors</i>, which silently corrupt the results of computations. Full detection and masking of soft errors is challenging, expensive, and, for some applications, unnecessary. For example, approximate computing applications (such as multimedia processing, machine learning, and big data analytics) can often naturally tolerate soft errors.</p> <p>We present Rely a programming language that enables developers to reason about the quantitative reliability of an application -- namely, the probability that it produces the correct result when executed on unreliable hardware. Rely allows developers to specify the reliability requirements for each value that a function produces.</p> <p>We present a static quantitative reliability analysis that verifies quantitative requirements on the reliability of an application, enabling a developer to perform sound and verified reliability engineering. The analysis takes a Rely program with a reliability specification and a hardware specification that characterizes the reliability of the underlying hardware components and verifies that the program satisfies its reliability specification when executed on the underlying unreliable hardware platform. We demonstrate the application of quantitative reliability analysis on six computations implemented in Rely.</p>", "authors": [{"name": "Michael Carbin", "author_profile_id": "81319489479", "affiliation": "MIT CSAIL, Cambridge, MA, USA", "person_id": "P4290308", "email_address": "mcarbin@csail.mit.edu", "orcid_id": ""}, {"name": "Sasa Misailovic", "author_profile_id": "81330495396", "affiliation": "MIT CSAIL, Cambridge, MA, USA", "person_id": "P4290309", "email_address": "misailo@csail.mit.edu", "orcid_id": ""}, {"name": "Martin C. Rinard", "author_profile_id": "81100087275", "affiliation": "MIT CSAIL, Cambridge, MA, USA", "person_id": "P4290310", "email_address": "mcarbin@csail.mit.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509546", "year": "2013", "article_id": "2509546", "conference": "OOPSLA", "title": "Verifying quantitative reliability for programs that execute on unreliable hardware", "url": "http://dl.acm.org/citation.cfm?id=2509546"}