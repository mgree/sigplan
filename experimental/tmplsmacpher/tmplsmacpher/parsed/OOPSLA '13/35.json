{"article_publication_date": "10-29-2013", "fulltext": "\n Guided GUI Testing of Android Apps with Minimal Restart and Approximate Learning Wontae Choi George \nNecula Koushik Sen EECS Department EECS Department EECS Department University of California, Berkeley \nUniversity of California, Berkeley University of California, Berkeley wtchoi@cs.berkeley.edu necula@cs.berkeley.edu \nksen@cs.berkeley.edu Abstract Smartphones and tablets with rich graphical user interfaces (GUI) are \nbecoming increasingly popular. Hundreds of thou\u00adsands of specialized applications, called apps, are available \nfor such mobile platforms. Manual testing is the most pop\u00adular technique for testing graphical user interfaces \nof such apps. Manual testing is often tedious and error-prone. In this paper, we propose an automated \ntechnique, called Swift-Hand, for generating sequences of test inputs for Android apps. The technique \nuses machine learning to learn a model of the app during testing, uses the learned model to generate \nuser inputs that visit unexplored states of the app, and uses the execution of the app on the generated \ninputs to re.ne the model. A key feature of the testing algorithm is that it avoids restarting the app, \nwhich is a signi.cantly more expensive operation than executing the app on a sequence of inputs. An important \ninsight behind our testing algorithm is that we do not need to learn a precise model of an app, which \nis of\u00adten computationally intensive, if our goal is to simply guide test execution into unexplored parts \nof the state space. We have implemented our testing algorithm in a publicly avail\u00adable tool for Android \napps written in Java. Our experimental results show that we can achieve signi.cantly better cover\u00adage \nthan traditional random testing and L*-based testing in a given time budget. Our algorithm also reaches \npeak coverage faster than both random and L*-based testing. Categories and Subject Descriptors D.2.5 \n[Software En\u00adgineering]: Testing and Debugging General Terms Algorithm, Design, Experimentation Keywords \nGUI testing, Learning, Automata, Android Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with \ncredit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. 1. Introduction \nSmartphones and tablets with rich graphical user interfaces (GUI) are becoming increasingly popular. \nHundred of thou\u00adsands of specialized applications, called apps, are already available for these mobile \nplatforms. The complexity of these apps lies often in the user interface, with data process\u00ading either \nminor, or delegated to a backend component. A similar situation exists in applications using software-as-a\u00adservice \narchitecture, where the client-side component con\u00adsists mostly of user interface code. Testing such applications \ninvolves predominantly user interface testing. We focus on user interface testing for Android apps, al\u00adthough \nmany of the same challenges exist on other mobile and browser platforms. The only two tools that are \nwidely used in practice for testing Android apps are Monkeyrun\u00adner [2], a framework for manually scripting \nsequences of user inputs in Python, and Monkey [3], a random automatic user input generation tool. A \ntypical use of the Monkey tool involves generating clicks at random positions on the screen, without \nany consideration of what are the actual controls shown on the screen, which ones have already been clicked, \nand what is the sequence of steps taken to arrive at the cur\u00adrent con.guration. It is not surprising \nthen that such testing has trouble exploring enough of the user interface, especially the parts that \nare reached after a speci.c sequence of inputs. In this paper, we consider the problem of automatically \ngenerating sequences of test inputs for Android apps for which we do not have an existing model of the \nGUI. The goal is to achieve code coverage quickly by learning and ex\u00adploring an abstraction of the model \nof the GUI of the app. We discuss further in the paper various techniques that have been explored previously \nto address different aspects of this prob\u00adlem. However, our experience shows that previous learning techniques \nignore a very signi.cant practical constraint in testing apps. All automatic exploration algorithms will \nocca\u00adsionally need to restart the app, in order to explore additional states reachable from the initial \nstate. The only reliable way to restart an app is to remove and reinstall it. This is neces- OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA.. sary, for example, when the application has persistent \ndata. Copyright &#38;#169; 2023 ACM 978-1-4503-2374-1/13/10. . . $15.00. c http://dx.doi.org/10.1145/2509136.2509552 \nOur experiments show that the restart operation takes 30 sec\u00adonds, which is signi.cantly larger than \nthe time required to explore any other transition, such as a user input. Currently, our implementation \nwaits for up to 5 seconds after sending a user input, to ensure that all handlers have .nished. We expect \nthat with more support from the operating system, or a more involved implementation, we could reduce \nthat wait time to well under a second. Since the cost of exploring a transition to the initial state \n(a restart) is an order of magnitude more than the cost of any other transition, we must use an exploration \nand learning algorithm that minimizes the number of restarts. Standard regular language learning algorithms \nare not appropriate in this case. For example, Angluin s L* [10] requires at least O(n2) restarts, where \nn is the number of states in the model of the user interface. Rivest and Schapire s algorithm [38] reduces \nthe number of restarts to O(n), which is still high, by computing homing sequences, and it also increases \nthe runtime by a factor of n, which is again not acceptable when we want to achieve code coverage quickly. \nIn this paper we propose a testing algorithm based on two key observations: 1. It is possible to reduce \nthe use of app restarts, because most user interface screens of an Android app can of\u00adten be reached \nfrom other screens just by triggering a se\u00adquence of user inputs, e.g., using back or home but\u00adtons. \n 2. For the purpose of test generation, we do not need to learn an exact model of the app under test. \nAll we need is an approximate model that could guide the generation of user inputs while maximizing the \ncode coverage. Note that for real-world apps a .nite state model of the GUI may not even exist. Some \napps may require push-down model and others may require more sophisticated in.nite models.  Based on \nthese two observations, we propose a testing al\u00adgorithm, called SwiftHand, that uses execution traces \ngen\u00aderated during the testing process to learn an approximate model of the GUI. SwiftHand then uses the \nlearned model to choose user inputs that would take the app to previously un\u00adexplored states. As SwiftHand \ntriggers the newly generated user inputs and visits new screens, it expands the learned model, and it \nalso re.nes the model when it .nds discrepan\u00adcies between the model learned so far. The interplay between \non-the-.y learning of the model and generation of user in\u00adputs based on the learned model helps SwiftHand \nto quickly visit unexplored states of the app. A key feature of Swift-Hand is that, unlike other learning-based \ntesting algorithms, it minimizes the number of restarts by searching for ways to reach unexplored states \nusing only user inputs. We have implemented SwiftHand for Android apps writ\u00adten in Java. The tool is \npublicly available at https:// github.com/wtchoi/SwiftHand. We applied SwiftHand to several free real-world \nAndroid apps and compared the ef\u00adfectiveness of SwiftHand with that of random testing and L* \u00adbased testing. \nOur results show that SwiftHand can achieve signi.cantly better coverage on these apps within a given \ntime budget. Moreover, SwiftHand achieves branch cover\u00adage at a rate faster than that of random and L*-based \ntesting. We report the results of our investigation in the empirical evaluation section. 2. Overview \nIn this section we introduce a motivating example, which we will use .rst to describe several existing \ntechniques for auto\u00admated user interface testing. Then we describe SwiftHand at a high level in the context \nof the same example. The formal details of the SwiftHand algorithm are in Section 3. We use part of Sanity, \nan Android app in our benchmark\u00adsuite as our running example. Figure 1 shows the .rst four screens of \nthe app. The app starts with three consecutive end\u00aduser license agreement (EULA) screens. To test the \nmain app, an automated testing technique must pass all the three EULA screens. Figure 1: The .rst four \nscreens of Sanity App. The screen names in parentheses are for cross-reference from the mod\u00adels of this \napp discussed later. The .rst and the third EULA screens have four input choices: (a) Yes button to accept \nthe license terms, (b) No button to decline the license terms, (c) ScrollUp the license terms, and (d) \nScrollDown the license terms. Pressing No at any point terminates program. ScrollDown and ScrollUp do \nnot change the non-visual state of the program. The second EULA screen doesn t have the scrolling option. \nPressing Yes button three times leads the user to the main screen of the app. For convenience, in the \nremainder of this paper we are going to use short name q1, q2, q3, and qM , instead of EULA1, EULA2, \nEULA2, and Main. 2.1 Goals and Assumptions We want to develop an algorithm that generates sequences \nof user inputs and feeds them to the app in order to achieve high code coverage quickly. The design of \nour testing algorithm is guided by the following practical assumptions. Testing Interface. We assume \nthat it is possible to dynami\u00adcally inspect the state of the running app to determine the set of user \ninputs enabled on a given app screen. We also assume that our algorithm can restart the app under test, \ncan send a user input to the app, and can wait for the app to become stable after receiving a user input. \n Cost Model. We assume that restarting the app takes sig\u00adni.cantly more time than sending a user input \nand waiting for the app to stabilize. Note that a few complicated tasks are performed when an app is \nrestarted: initializing a virtual ma\u00adchine, loading the app package, verifying the app byte-code, and \nexecuting the app s own initialization code. Testing tools have to wait until the initialization is properly \ndone. Our ex\u00adperiments show that the restart operation takes 30 seconds. Sending a user input is itself \nvery fast, but at the moment our implementation waits for up to 5 seconds for the app to stabilize. We \nexpect that with a more complex implementa\u00adtion, or with some assistance from the mobile platform we \ncan detect when the handlers have actually .nished running. In this case we expect that the ratio of \nrestart cost to the cost of sending a user input will be even higher. User-Interface Model. We assume \nthat an abstract model of the graphical user interface of the app under test is not available a priori \nto our testing algorithm. This is a reason\u00adable assumption if we want to test arbitrary real-world An\u00addroid \napps. When learning a user interface model we have to compare a user interface state with states that \nare already part of the learned model. For this purpose, we consider two user inter\u00adface states equivalent \nif they have the same set of enabled user inputs. An enabled user input is considered according to its \ntype and the bounding box of screen coordinates where it is enabled. This means that we do not care about \nthe ac\u00adtual content of user elements such as colors or text content. This abstraction is similar to the \none proposed by MacHiry et al. [25]. Test Coverage Criteria. We assume that the app under test is predominantly \nconcerned with the user interface, and a signi.cant part of the app state is re.ected in the state of \nthe user interface. Thus we assume that a testing approach that achieves good coverage of the user interface \nstates also achieves good code coverage. This assumption is not always entirely accurate, e.g., for apps \nthat have signi.cant internal application state that is not exposed in the user interface.  2.2 Existing \nApproaches Random Testing. Random testing [25] tests an app by randomly selecting a user input from the \nset of enabled inputs at each state and by executing the selected input. Random testing also restarts \nthe app at each state with some probability. In Sanity case, after a restart, random testing has Figure \n2: A partial model of Android app Sanity. screen are 24 and 7, respectively.1 This will take about 330 \nseconds according to our cost model. In summary, random testing has a hard time in achieving good coverage \nif an interesting screen is reachable only after executing a speci.c sequence of user inputs. This is \ntrue for our example and is common in real-world apps. Note that this analysis, and our experiments, \nuse a ran\u00addom testing technique that is aware of the set of enabled user inputs at each state and can \nmake a decision based on this set. A na\u00a8ive random testing technique, such as the widely\u00adused Monkey \ntester, which touches random coordinates on the screen, will do nothing meaningful because most of the \nscreen coordinates have no associated event handler. Model-based Testing. Model-based testing [8, 11, \n28, 37, 39, 43] is a popular alternative to automatically test GUI pro\u00adgrams and other event-driven programs. \nModel-based testing assumes that a .nite state model of the GUI is provided by the user. The idea behind \nmodel-based testing is to create an ef.cient set of user input sequences from the model of the target \nprogram. The generated test cases could either try to maximize coverage of states or try to maximize \ncoverage of short sequences of user inputs. Figure 2 is a partial model for the Sanity app. The model \ndescribes a .nite state machine. A .nite state machine ab\u00adstracts the in.nite state space of the app \ninto a .nite number of interesting states and describes equivalence classes of user input sequences leading \nto those states. A key advantage of using a .nite state machine (FSM) model is that an optimal set of \ntest cases can be generated based on a given coverage criterion. 1 Let R be the expected number of restarts. \nAt the .rst EULA screen, if No is chosen, the app will terminate and testing should be restarted. This \ncase happens with probability 1 . The expected number of restarts is 4 1 R1 = (1 + R). If either ScrollUp \nor ScrollDown is picked, the app is 4 still in the .rst EULA screen. Therefore, the expected number of \nrestarts 1 in this case is R2 = R. After considering two more cases (Yes,No and 2 Yes,Yes,Scroll*,No), \nwe can construct an equation of R. R = R1 + R2 + R3 + R4 a low probability ( 1 * 1 * 1 = 0.125) of \nreaching the main app 1 11 = (1 + R) + 1 R + (1 + R) + (1 + R) 4 2 8 16 7 15 222 screen. User inputs \nthat do not change the non-visual state, R += 16 16 for example ScrollUp and ScrollDown or clicking outside \nSolving the equation, we have R = 7. We can perform a similar analysis the buttons, do not affect this \nprobability. Expected number to get the expected number of all events including restarts, which is 31. \nThe of user inputs and restarts required to reach the main app number of events except restarts is 24. \n For our running example, if we want to avoid a restart, a model-based testing algorithm could generate \nthe sequence ScrollDown, ScrollUp, Yes, Yes, ScrollDown, ScrollUp, Yes to obtain full coverage of non-terminating \nuser inputs and to lead the test execution to the main screen. Model-based testing can generate optimal \ntest cases for GUI apps, if the model is .nite and accurate. Unfortunately, it is a non-trivial task \nto manually come up with a precise model of the GUI app under test. For several real-world apps, a .nite \nmodel may not even exist. Some apps could require a push-down automaton or a Turing machine as a model. \nMoreover, manually generated models may miss transitions that could be introduced by a programmer by \nmistake. Testing with Active Learning. Testing with model learn\u00ading [18, 34, 35] tries to address the \nlimitations of model\u00adbased testing by learning a model of the app as testing is performed. An active \nlearning algorithm is used in conjunc\u00adtion with a testing engine to learn a model of the GUI app and \nto guide the generation of user input sequences based on the model. A testing engine is used as a teacher \nin active learning. The testing engine executes the app under test to answer two kinds of queries: 1) \nmembership queries whether a se\u00adquence of user inputs is valid from the initial state, i.e. if the user \ninputs in the sequence can be triggered in order, and 2) equivalence queries whether a learned model \nab\u00adstracts the behavior of the app under test. The testing engine resolves equivalence queries by executing \nuntried scenarios until a counter-example is found. An active learning algo\u00adrithm repeatedly asks the \nteacher membership and equiva\u00adlence queries to infer the model of the app. A case study: L* . Angluin \ns L*[10] is the most widely used active learning algorithm for learning .nite state ma\u00adchine. The algorithm \nhas been successfully applied to var\u00adious problem domains from network protocol inference to functional \ncon.rmation testing of circuits. We applied L* to the running example. We observed that L* restarts frequently. \nMoreover, L* made a large number of membership queries to learn a precise and minimal model. Speci.cally, \ntesting with L* required 29 input sequences (i.e. 29 restarts) consisting of 64 user inputs to fully \nlearn the partial model in Figure 2. This translates to spending around 870 seconds to restart the app \nunder test (AUT) and 320 seconds for executing the user inputs. 73% of running time is spent on restarting \nthe app. It is important to note that L* has to learn the partial model completely and precisely before \nit can explore the screens beyond the main screen. We show in the experimental evaluation section that \nL* has similar dif.culties in actual benchmarks.  2.3 Our Testing Algorithm: SwiftHand SwiftHand combines \nactive learning with testing. However, unlike standard learning algorithms such as L* , SwiftHand restarts \nthe app under test sparingly. At each state, instead of restarting, SwiftHand tries to extend the current \nexecution path by selecting a user input enabled at the state. SwiftHand uses the model learned so far \nto select the next user input to be executed. Informally, SwiftHand works as follows. SwiftHand in\u00adstalls \nand launches the app under test and waits for the app to reach a stable state. This is the initial app-state. \nFor each app-state, we compute a model-state based on the set of en\u00adabled user inputs in the app-state \nalong with the bounding boxes of screen coordinates where they are enabled. Initially the model contains \nonly one state, the model-state corre\u00adsponding to the initial app-state. If a model-state has at least \none unexplored outgoing transition, we call it a frontier model-state. At each app\u00adstate s, SwiftHand \nheuristically picks a frontier model-state q that can be reached from the current model-state without \na restart. Case 0. If such a state is not found, SwiftHand restarts the app. This covers both the case \nwhen a frontier-state exists but is not reachable from the current state with user inputs alone, and \nalso the case when there are no frontier-states, in which case our algorithm restarts the app to try \nto .nd inconsistencies in the learned model by exploring new paths. Otherwise, SwiftHand avoids restart \nby heuristically .nd\u00ading a path in the model from the current model-state to q and executes the app along \nthe path. SwiftHand then executes the unexplored enabled input of state q. Three scenarios can arise \nduring this execution. Case 1. The app-state reached by SwiftHand has a corresponding model-state that \nhas not been encountered before. Swift-Hand adds a fresh model-state to the model correspond\u00ading to the \nnewly visited app-state and adds a transition to the model. Case 2. If the reached app-state is equivalent \nbased on enabled user inputs to the screen of a previously encountered app-state, say s', then SwiftHand \nadds a transition from q to the model-state corresponding to s'. This is called state merging. If there \nare multiple model-states whose corresponding app-states have equivalent screens, then SwiftHand picks \none of them heuristically for merging. Case 3. During the execution of the user inputs to reached the \nfrontier state, SwiftHand discovers that an app-state vis\u00adited during the execution of the path does \nnot match the corresponding model-state along the same path in the model. This is likely due to an earlier \nmerging operation that was too aggressive. At this point, SwiftHand runs a passive learning algorithm \nusing the execution traces ob\u00adserved so far, i.e. SwiftHand .nds the smallest model that can explain \nthe app-states and transitions observed so far. Note that SwiftHand applies heuristics at three steps \nin the above algorithm. We discuss these heuristics in Section 3.2.  Figure 3: Progress of learning \nguided testing on Sanity example. A circle with solid line is used to denote a state in the model. The \ninitial state is marked with a short incoming arrow. A circle with double line denotes the current model-state. \nIf the target model is .nite, SwiftHand will be able to learn the target model irrespective of what heuristics \nwe use at the above three decision points. However, our goal is not nec\u00adessarily to learn the exact model, \nbut to achieve coverage quickly. Therefore, we apply heuristics that enable Swift-Hand to explore previously \nunexplored states quickly. In or\u00adder to avoid SwiftHand from getting stuck in some remote part of the \napp, we allow SwiftHand to restart when it has executed a prede.ned number of user inputs from the initial \nstate. Figure 3 illustrates how SwiftHand works on the running example. For this example, we pick user \ninputs carefully in order to keep our illustration short, yet comprehensive. For this reason we do not \npick the No user input to avoid restart. In actual implementation, we use various heuristics to make \nsuch decisions. A model-state with at least one unexplored outgoing transition is called a frontier state. \nA solid line with arrow denotes a transition in the model. The input sequence shown below the diagram \nof a model denotes an input sequence of the current execution from the initial state. Initialization: \nAfter launching the app, we reach the ini\u00adtial app-state, where the enabled inputs on the state are {Yes, \nNo, ScrollUp, ScrollDown}. We abstract this app\u00adstate as the model-state q1 (using the same terminology \nas in Figure 1 and Figure 2). This initial state of the model is shown in Figure 3(a).  1st Iteration: \nStarting from the initial state of the model, SwiftHand .nds that the state q1 is a frontier state and \n chooses to execute a transition for the input Yes. The resulting state has a different set of enabled \ninputs from the initial state. Therefore, according to Case 1 of the algorithm, SwiftHand adds a new \nmodel-state q2 to the model. The modi.ed model is shown in Figure 3(b). 2nd Iteration: The app is now \nat an app-state whose cor\u00adresponding model-state is q2, as shown in Figure 3(b). Both q1 and q2 have \nunexplored outgoing transitions. However, if we want to avoid restart, we can only pick a transition \nfrom q2 because according to the current model there is no sequence of user inputs to get to q1 from \nthe current model state. SwiftHand chooses to execute a tran\u00adsition on Yes, and obtains a new app-state \nfor which it creates a new model-state q3, as shown in Figure 3(c). However, the new app-state has the \nsame set of enabled inputs as the initial app-state q1. Therefore, SwiftHand merges q3 with q1 according \nto Case 2. This results in the model shown in Figure 3(d). If you compare the partial model learned so \nfar with the actual underlying model shown in Figure 2, you will notice that the merging op\u00aderation is \ntoo aggressive. In the actual underlying model the state reached after a sequence of two Yes clicks is \ndif\u00adferent than the initial state. This will become apparent to SwiftHand once it explores the app further. \n 3rd Iteration: The app is now at an app-state whose cor\u00adresponding model-state is q1, as shown in Figure \n3(d). SwiftHand can now pick either q1 or q2 as the next fron\u00adtier state to explore. Assume that SwiftHand \npicks q2 as the frontier state to explore. A path from the cur\u00ad   rent model-state q1 to q2 consists \nof a single transition Yes. After executing this input, however, SwiftHand en\u00adcounters an inconsistency \nthe app has reached the main screen after executing Yes,Yes,Yes sequence from the ini\u00adtialization (see \nFigure 2). In the current model learned so far (Figure 3(d)), the abstract state after the same se\u00adquence \nof events ought to be q2. Yet the set of enabled in\u00adputs associated with this screen (Action1, Action2, \nand Action3) is different from the set of enabled inputs asso\u00adciated with q2. We say that SwiftHand has \ndiscovered that the model learned so far is inconsistent with the app, and the input sequence Yes,Yes,Yes \nis a counter-example showing this inconsistency. Figure 3(e) illustrates this situation; notice that \nthere are two outgoing transitions labeled Yes from state q2. This is Case 3 of the algorithm. The inconsis\u00adtency \nhappened because of the merging decision made at the second iteration. To resolve the inconsistency, \nSwift-Hand abandons the model learned so far and runs an off\u00adthe-shelf passive learning algorithm to \nrebuild the model from scratch using all execution traces observed so far. Figure 3(f) shows the result \nof passive learning. Note that this learning is done using the transitions that we have recorded so far, \nwithout any transition in the actual app. 4th Iteration: SwiftHand is forced to restart the app when \nit has executed a prede.ned number of user inputs from the initial state. Assume that SwiftHand restarts \nthe app in the 4th iteration so that we can illustrate another scenario of SwiftHand. After restart, \nq1 becomes the current state (see Figure 3(g)). SwiftHand now has several options to execute an unexplored \ntransition. Assume that SwiftHand picks ScrollDown transition out of q1 state for execution. Since scrolling \ndoes nothing to the screen state and the set of enabled inputs, we reach an app-state that has the same \nscreen and enabled inputs as q1 and q3. SwiftHand can now merge the new model-state with either q1 or \nq3 (see Figure 3(h)). In practice, we found that the nearest ancestor or the nearest state with the same \nset of enabled inputs works better than other model states. We use this heuristics to pick q1. The resulting \nmodel at this point is shown in Figure 3(i). After the .rst four iterations, SwiftHand will execute 4 \nrestarts and 11 more user inputs, in the worst case, to learn the partial model in Figure 2. The restarts \nare necessary to learn the transitions to the terminal state, End. If SwiftHand wants to explore states \nbeyond the main screen after a restart, it can consult the model and execute the input sequence Yes, \nYes, and Yes to reach the main screen. Random testing will have a hard time reaching the main screen \nthrough random inputs. In terms of our cost model, SwiftHand will spend 190 seconds or 60% of the execution \ntime in restarting the app. The percentage of time spent in restarting drops if the search space becomes \nlarger. In our actual benchmarks, we observed that SwiftHand spent about 10% of the total time in restarting. \n3. Learning guided Testing Algorithm In this sections, we describe formally the SwiftHand al\u00adgorithm. \nWe .rst introduce a few de.nitions that we use in our algorithm. Then we brie.y describe the algorithm. \nSwift-Hand uses a variant of an existing passive learning algorithm to re.ne a model whenever it observes \nany inconsistency be\u00adtween the learned model and the app. We describe this pas\u00adsive learning algorithm \nto keep the paper self-contained. Models as ELTS. We use extended deterministic labeled transition systems \n(ELTS) as models for GUI apps. An ELTS is a deterministic labeled transition system whose states are \nlabeled with a set of enabled transitions (or user inputs). Formally, an ELTS M is a tuple M = (Q, q0, \nS, d, .) where Q is a set of states,  q0 . Q is the initial state,  S is an input alphabet,  d : \nQ \u00d7 S . Q is a partial state transition function,  . : Q . P(S) is a state labeling function. .(q) denotes \nthe set of inputs enabled at state q, and  for any q . Q and a . S, if there exists a p . Q such that \nd(q, a) = p, then a . .(q).  The last condition implies that if there is a transition from q to some \np on input a, then a is an enabled input at state q. a Arrow. We use q -. p to denote d(q, a) = p. l \nArrow* . We say q -. p where l = a1, . . . , an . a1a2 S* if there exists q1, . . . , qn-1 such that \nq -. q1 -. an q2 . . . qn-1 -. p. Trace. An execution trace, or simply a trace, is a sequence of pairs \nof inputs and sets of enabled inputs. Formally, a trace t is an element of (S \u00d7 P(S))* . Trace Projection. \nWe use p(t) to denote the sequence of inputs in the trace t. Formally, if t = (a1, S1), . . . , (an, \nSn), then p(t) = a1, . . . , an. tp(t) Arrow Trace. We say q -. p if q -. p. Consistency. A trace t = \n(a1, S1), . . . , (an, Sn) is consis\u00adtent with a model M = (Q, q0, S, d, .) if and only if ai .q1, . \n. . qn . Q.qi-1 -. qi . .(qi) = Si i.[1,n]  Frontier State. A state in an ELTS is a frontier-state if \nthere is no transition from the state on some input that is enabled on the state. Formally, a state q \nis a frontier-state if a there exists a a . .(q) such that q -. p is not true for any p . Q. Terminal \nState. When the execution of an app terminates, we assume that it reaches a state that has no enabled \ninputs. 3.1 Learning Guided Testing Algorithm Interface with the App under Test. SwiftHand treats the \napp state as a black box. However, it can query the set of enabled inputs on an app state. We assume \nthat .(s) returns the set of enabled inputs on an app state s. SwiftHand can also ask the app to return \na new state and trace after executing a sequence of user inputs from a given app state. Let s be an app \nstate, t be a trace of executing the app from the initial state s0 to s, and l be a sequence of user \ninputs. Then EX E C U T E(s, t, l) returns a pair containing the app state after executing the app from \nstate s on the input sequence l and the trace of the entire execution from the initial state s0 to the \nnew state. Description of the Actual Algorithm. The pseudo-code of the algorithm is shown in Algorithm \n1. The algorithm maintains .ve local variables: 1) s denotes the current app\u00adstate and is initialized \nto s0, the initial state of the app, 2) p denotes the current model-state, 3) t denotes the current trace \nthat is being executed, 4) T denotes the set of traces tested so far, and 5) M denotes the ELTS model \nlearned so far. At each iteration the algorithm tries to explore a new app state. To do so, it .nds a \nfrontier-state q in the model, then .nds a sequence of transitions l that could lead to the frontier-state \nfrom the current model-state, and a transition a enabled at the frontier-state (lines 9 10). It then \nexecutes the app on the input sequence l from the current app state s and obtains a trace of the execution \n(line 11). If the trace is not consistent with the model, then we know that some previous merging operation \nwas incorrect and we re-learn the model using a passive learning algorithm from the set of traces observed \nso far (lines 21 24). On the other hand, if the trace is consistent with the model learned so far, the \nalgo\u00adrithm executes the app on the input a from the latest app state (lines 12 13) . If the set of enabled \ninputs on the new app state matches with the set of enabled inputs on an existing model-state (line 14), \nthen it is possible that we are revisiting an existing model-state from the frontier state q on input \na. The algorithm, therefore, merges the new model-state with the existing model-state (line 15). Note \nthat this is an ap\u00adproximate check of equivalence between two model-states, but it helps to prune the \nsearch space. If the algorithm later discovers that the two states are not equivalent, it will do a passive \nlearning to learn a new model, effectively undoing the merging operation. Nevertheless, this aggressive \nmerg\u00ading strategy is key to prune similar states and guide the app into previously unexplored state space. \nOn the other hand, if the algorithm .nds that the set of enabled inputs on the new app-state is not same \nas the set of enabled inputs of any existing model-state, then we have visited a new app-state. The algorithm \nthen adds a new model-state corresponding to the app-state to the model (line 18). In either case, that \nis whether we merge or we add a new model-state, we update our current model-state to the model-state \ncorresponding to the new app state and repeat the iteration (lines 16 and 19). During the iteration if \nwe fail to .nd a frontier state, we know that our model is complete, i.e. every transition from every \nmodel-state has been explored. However, there is a possibility that some incorrect merging might have \nhappened in the process. We, therefore, need to now con.rm that the model is equivalent to the target \nmodel of the app. This is similar to the equivalence check in the L* algorithm. The algorithm picks a \nsequence of transitions l such that l is not a subsequence of any trace that has been explored so far \n(lines 28 30). Moreover, l should lead to a state q from the current state in the model. If such a l \nis not found, we know that our model is equivalent to the target model (lines 35 36). On the other hand, \nif such an l exists, the algorithm executes the app on l and checks if the resulting trace is consistent \nwith the model (lines 30 31). If an inconsistency is found, the model is re-learned from the set of traces \nexecuted so far (lines 32 33). Otherwise, we continue the re.nement process over any other existing l. \nDuring the iteration, if the algorithm .nds a frontier state, but fails to .nd a path from the current \nstate to the frontier state in the model, it restarts the app (lines 25 26). We observed that in most \nGUI apps, it is often possible to reach most screen states from another screen state after a series of \nuser inputs while avoiding a restart. As such, this kind of restart is rare in SwiftHand. In order to \nmake sure that our testing does not get stuck in some sub-component of an app with a huge number of model \nstates, we do restart the app if the length of a trace exceeds some user de.ned limit (i.e. MAX LENGTH) \n(lines 7 8).  3.2 Heuristics and Decision Points The described algorithm has four decision points and \nwe use the following heuristics to resolve them. We came up with these heuristics after extensive experimentation. \n If there are multiple frontier-states and if there are mul\u00adtiple enabled inputs on those frontier-states \n(lines 9 10), then which frontier-state and enabled transition should we pick? SwiftHand picks a random \nstate from the set of frontier-states and picks a random transition from the frontier state. We found \nthrough experiments that effec\u00adtiveness of SwiftHand does not depend on what heuristics we use for this \ncase.  If there are multiple transition sequences l from the cur\u00adrent model-state to a frontier state \n(line 10), which one should we pick? We found that picking a short sequence   Algorithm 1 SwiftHand: \nLearning Guided Testing 1: procedure TESTI N G(s0) C s0 is the initial state of the app 2: 3: 4: 5: \n6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: \n33: 34: 35: 36: 37: 38: 39: M . ({q0}, q0, S, \u00d8, {q0 . .(s0)}) for some fresh state q0 C M is the current \nmodel T . \u00d8 C T accumulates the set of traces executed so far p . q0 and s . s0 and t . E C p, s, and \nt are the current model-state, app-state, and trace, respectively while \u00actimeout() do if |t| > MAX LENGTH \nthen p . q0 and s . s0 and T . T . {t} and t . E else if there exists a frontier-state q in M then l \n if there exists l . S * and a . S such that p -. q and a . .(q) then (s, t) . EX E CUTE(s, t, l) if \nt is consistent with M then (s, t) . EX E CUTE(s, t, a) if there exists a state r in M such that .(r) \n= .(s) then a add q -. r to M p . r else a add a fresh state r to M such that q -. r and .(r) = .(s) \np . r end if else  T . T . {t} and M . PA S S IV ELE A RN(T ) t p . r where q0 -. r end if C While time \nbudget for testing has not expired C Current trace is longer than a maximum limit C Restart the app C \nModel is not complete yet C Merge with an existing state C Update current model-state C Add a new model-state \nC Update current model-state C Inconsistent model. Need to re-learn the model. C Update current model-state \n else C A frontier-state cannot reached from the current state p . q0 and s . s0 and T . T . {t} and \nt . E C Restart the app end if else if there exists state q in M and l . S * such that p (s, t) . EX \nE CU TE(s, t, l) if t is not consistent with M then T . T . {t} and M . PA S S IV ELE A RN(T ) t p . \nr where q0 -. r end if else return T end if end while return T 40: end procedure l -. q and l is not \na subsequence of p(t) for any t . T then C Model is complete, but may not be equivalent to target model \nC Inconsistent model. Need to re-learn the model. C Update current model-state C Model is complete and \nequivalent to target model C Done with learning is not necessarily the best strategy. Instead, SwiftHand \nselects a sequence of transitions from the current model state to the frontier state so that the sequence \ncontains a previously unexplored sequence of inputs. This helps SwiftHand to learn a more accurate model \nearly in the testing process. If multiple states are available for merging (at line 14), then which one \nshould SwiftHand pick? If we pick the correct model-state, we can avoid re-learning in future. We experimented \nwith a random selection strategy and with a strategy that selects a nearby state. However, we discovered \nafter some experimentation that if we prefer the nearest ancestor to other states, our merge operations \nare often correct. Therefore, SwiftHand uses a heuristics that .rst tries to merge with an ancestor. \nIf an ancestor is not available for merging, SwiftHand picks a random state from the set of candidate \nstates. If there are multiple transition sequences l available for checking equivalence (line 28), which \none should we pick? In this case, we again found that none of the strate\u00adgies we tried make a difference. \nWe therefore use random walk to select such an l.  We set the maximum length of a trace (i.e. MAX LENGTH) \nto 50. We again found this num\u00adber through trial-and-error.  3.3 Rebuilding Model using Passive Learning. \nWe describe the passive learning algorithm that we use for re-learning a model from a set of traces. \nThe algorithm is a variants of Lambeau et al. s [21] state-merging algorithm. We have modi.ed the algorithm \nto learn ELTS. We describe this algorithm to keep the paper self-contained. Pre.x Tree Acceptor. A pre.x \ntree acceptor [9] (or a PTA) is an ELTS whose state transition diagram is a tree with the initial state \nof the ELTS being the root of the tree. Given a set of traces T , we build a pre.x tree acceptor PTAT \nwhose states are the set of all pre.xes of the traces in T . There is a transition with label a from \nt to t ' if t can be extended to t ' using the transition a. The .(t) is S ' if the last element of t \nhas S ' as the second component. Partitioning and Quotient Model. . . P(Q) is a partition of the state \nspace Q if all elements of . are disjoint, all elements of Q are a member of some element of ., and . \nof all elements of a given element of . are the same. An element of . is called a partition and is denoted \nby p. element(p) denotes a random single element of p. M /. is a quotient model of M obtained by merging \nequivalent states with respect to .: p0 is the partition containing q0 d ' def ' ' = {(p, a) . p ' | \n.q . p and .q . p ' .(q, a) . q . d} def .p . ... ' (p) = .(element(p)) M /. = (., p0, S, d ' , . ' ) \nNote that a quotient model can be non-deterministic even though the original model is deterministic. \nThe Algorithm. Algorithm 2 describes the state-merging based learning algorithm. Conceptually, the algorithm \nstarts from a partial tree acceptor PTAT and repeatedly general\u00adizes the model by merging states. The \nmerging procedure .rst checks whether any two states agree on the . function, and then tries to merge \nthem. If merging results in a non\u00addeterministic model, the algorithm tries to eliminate non\u00addeterminism \nby merging target states of non-deterministic transitions provided that the merged states have the same \n.. This is applied recursively until the model is deterministic. If at some point of the procedure, merging \nof two states fails because .s of the states are different, the algorithm unrolls the entire merging \nprocess for the original pair of states. The ChoosePair function decides the order of state merging. \nThe quality of the learned model solely depends on the implementation of this function. Our implementation \nAlgorithm 2 Passive Learning Algorithm 1: procedure REB U I L D(T ) 2: . . {{q} | q . QP T AT } 3: while \n(pi, pj ) . ChoosePair(P T AT ) do 4: Try 5: . . MER G E(., pi, pj ) 6: CatchAndIngnore 7: end while \n8: return P T AT /. 9: end procedure 10: procedure MER G E(., pi, pj ) 11: M . P T AT /. 12: if .M (pi) \n= .M (pj) then 13: throw exception 14: end if 15: ppivot . pi . pj 16: . . (. \\ {pi, pj}) + ppivot 17: \nwhile (pk, pl) . FI N D NO ND E TE R(., ppivot ) do 18: . . ME R G E(., pk, pl) 19: end while 20: return \n. 21: end procedure 22: procedure FI N D NON D E T E R(., p) 23: M . P T AT /. aa 24: S . {(pi, pj ) \n| .a . .M (p).p .M pi . p .M pj . pi= pj } 25: return pick(S) 26: end procedure uses BlueFringe [22] \nordering, which is considered as the best known heuristics. Our algorithm differs from the original algorithm \non two fronts. First, the original algorithm [21] aims to learn DFA with mandatory merging constraints \nand blocking con\u00adstraints. Our algorithm learns an ELTS and only uses the idea of blocking constraints. \nWe use the . function or the set of enabled transitions at any state to avoid illegal merging. Sec\u00adond, \nDFA learning requires both positive and negative exam\u00adples. ELTS has no notion of negative examples. \n4. Implementation We have implemented SwiftHand for Android apps written in Java. The tool is publicly \navailable at https://github. com/wtchoi/SwiftHand. The tool itself is implemented using Java and Scala. \nSwiftHand uses asmdex [31], a Dalvik byte code instrumentation library, axml [4], an xml encoded binary \nmanipulation library, and chimpchat, an Android app remote control library. SwiftHand can test an Android \napp either on an emulator or an Android phone connected to a computer using ADB (Android Debug Bridge). \nSwiftHand expects a target app package .le (Apk), a testing strategy, a time budget for testing, and \na device on which the target app will be tested as input. It currently supports three strategies: Random, \nL*, and SwiftHand. For basic app control, such as restarting, sending a user input, installing and removing \nthe app, SwiftHand uses the chimpchat library. The same library has been used to im\u00adplement the monkeyrunner \nremote testing framework. The chimpchat library is able to control any device connected to the computer \nthrough ADB.  DecorView(1) + LinearLayout(2) {| + ScrollableLayout(3) Touch(6), | | + TextBox(4) Touch(7), \n| | LinearLayout(5) ScrollDown(3), | | + Button[Yes](6) ScrollUp(3) | | | Button[No](7) } (a) EULA screen \n(b) GUI Component Tree (c) Enabled Inputs Figure 4: Enabled Input Inference Example Local clean restart \ncan be implemented by sequentially uninstalling, reinstalling, and starting a target app. Note that the \nAndroid .le system is sandboxed. Removing an app is enough to cleanse the majority of local persistent \ndata. The only exception is an SD card. SD cards are treated as a shared storage. Therefore, removing \nan application will not remove persistent data from SD card. For simplicity, we choose to use devices \nwithout an SD card. A full local cleaning may require checkpointing and restoring the SD card content. \nchimpchat has two limitations. First, chimpchat can only send device level events: to touch a speci.c \ncoordi\u00adnate of the screen, to tilt the device, etc. Thus, SwiftHand has to infer coordinates from screen \ninformation to construct the chimpchat requests. Second, chimpchat can only send events without waiting \nfor the results. It will never tell pro\u00adgrammers that an event-handling is done or an app is termi\u00adnated. \nSwiftHand overcomes these limitations by binary instru\u00admentation. The binary instrumentation basically \ndoes two things: First, an app is modi.ed to record necessary run\u00adtime information throughout its execution. \nSecond, the in\u00adstrumented app launches an observer thread when the app is starting. The observer thread \nperiodically checks recorded information and reports to the SwiftHand tool running on a separate computer. \nThe remainder of this section explains how speci.c infor\u00admation such as the end of a state transition \nis collected through binary instrumentation, how binary instrumentation is implemented, and what limitations \nremain in the current SwiftHand implementation. 4.1 Inferring Enabled Inputs The exact information about \nan app s screen is available at runtime as a GUI component tree. The GUI component tree represents the \nhierarchical structure of GUI components on the screen of the app. Coordinates, size, and type informa\u00adtion \nof each GUI component is also available. SwiftHand instruments the target apps to obtain the GUI component \ntree and computes a representative set of enabled inputs by traversing the tree. If a leaf in the GUI \ncomponent tree has an event handler, a touch input corresponding to the event handler is added to the \nset of enabled inputs. If the GUI component has type EditText, then a user input capable of generating \na pre-de.ned or random string is added to the set of enabled inputs. The input events for a given GUI \ncompo\u00adnent remain the same across executions. For scrollable com\u00adponents, scroll inputs are added to \nthe set of enabled inputs. Inputs corresponding to pressing Back and Menu buttons, are always added to \nthe enabled input set. Figure 4 shows an EULA screen from the Sanity exam\u00adple, its simpli.ed GUI component \ntree, and the correspond\u00ading enabled inputs. In the GUI component tree, each compo\u00adnent is described \nwith its type and identi.er. ScrollableLay\u00adout can be scrolled. Therefore, we add ScrollDown(3) and ScrollUp(3) \nto the enabled input set. TextBox is a view com\u00adponent without any event handler. We add touch events \nfor Buttons with id 6 and 7 to the enabled input set because they are leaf components and each one has \nan associated event handler. To actually .re an event, SwiftHand gets the coordinate and size information \nof the target GUI component from the GUI component tree, and passes this information to the chimpchat \nlibrary.  4.2 App State Detection The SwiftHand algorithm needs to detect the end of a state transition \nand the termination of the app. Detecting App Termination. An Android app is composed of a set of activities. \nEach activity implements a single ap\u00adplication screen and its functionality. When an app is termi\u00adnated, \nall activities are stopped. Thus, the problem of check\u00ading app termination boils down to the problem \nof tracking the status of app s activities. The observer thread has an activity-state tracking mechanism \nand can detect the app ter\u00admination by periodically checking tracked activity states. Our activity state \ntracking mechanism is based on the Android Activity Lifecycle document [1]. According to the Activity \nLifecycle, activities have six conceptual states: created, started, resumed, paused, stopped, and destroyed. \nWhen an activity state is changing, the Android framework triggers a .xed group of event-handlers in \na .xed order. SwiftHand tracks the current activity state using a .nite state machine. There is a separate \nstate machine for each activity\u00adclass instance. Event handlers involved in the Activity Life\u00adcycle are \ninstrumented to trigger a state transition of a corre\u00adsponding state machine. Detecting the End of a \nState Transition. When a state transition happens in Android apps a number of event han\u00addlers are executed \nand the screen content is modi.ed. Af\u00adter sending a command, SwiftHand must wait until all event handler \nexecutions and screen modi.cations are .nished. The end of a single event-handler execution can be de\u00adtected \nby observing when the call-stack of the event handler becomes empty. However, a single event can trigger \nmulti\u00adple event handlers. Therefore, SwiftHand waits a while af\u00adter detecting the end of a single event \nhandler execution to make sure that no event handler is waiting. In experiments we found that 200 milliseconds \nare enough for real phones and 1000 milliseconds are enough for emulators.  In the Android framework \nan event handler could modify the screen content using animation effects. Since SwiftHand needs coordinate \ninformation of GUI components to trigger events through chimpchat, it must wait for the screen coor\u00addinates \nto stabilize. This can be done by periodically check\u00ading coordinate and size information of GUI components. \nIf there is no change for a while, SwiftHand concludes that the screen is stable. In experiments we use \n1100 milliseconds for real phones and 1800 milliseconds for emulators. The overall 5 seconds wait interval \nthat we use in the cost model is the result of having to wait for one or more event handlers and the \nanimations that these handlers may start. With a more complex implementation, and perhaps some as\u00adsistance \nfrom the platform, we could detect more accurately when a user input has been fully processed, without \nsuch long conservative wait intervals.  4.3 Instrumentation To instrument the app byte code, we use \nthe asmdex library. The library provides only parsing and byte code generation functionalities. We implemented \nall other intermediate steps: control-.ow graph (CFG) construction, live register anal\u00adysis, register \ntype inference, and code injection with free\u00adregister acquisition. The CFG construction and live regis\u00adter \nanalysis are standard. Type inference is based on Leroy s Java bytecode veri.cation [24]. Code Injection \nand Free Registers. The Dalvik virtual machine (VM) is a register based machine. The biggest hurdle for \ncode injection is acquiring free registers. Dalvik VM has following policies to manage registers: 1. \nRegisters are identi.ed by a number from 0 to 65535. 2. Each method has to declare the upper bound of \nregister identi.ers. 3. When a method is invoked, arguments are assigned to the higher most registers. \n 4. Each instruction can accept only a limited range of reg\u00adisters as operands; some accept registers \n0 to 15, another accepts 0 to 255, and the others accept the entire range of registers. 5. Registers \nmust have a consistent type through the pro\u00adgram s control .ow, i.e., at every control .ow join point, \na register should have a consistent type.  Code injection requires free registers. Free registers are \nnot always available due to the second policy and the fact that live registers at the target injection \npoint cannot be used. To relax this situation, SwiftHand increases the upper bound of register range \nand adds a few instructions to move the argument registers to their original position. This creates a \nnumber of free registers at the highest position of the regis\u00adter range. Note that the second step is \ncrucial due to the third and the fourth policies: Assume a method with register upper bound 15 and two \narguments. Arguments are originally as\u00adsigned to register 14 and 15. If the upper bound is increased \nby one, the register assigned to the arguments become 15, 16. Unfortunately, some binary instructions \ncannot accept the last argument register. Method call instruction, one of the most frequently used instructions, \nis in this category. Restor\u00ading argument registers to their original position solve this particular problem \nwithout seriously modifying the method body. However, having a free register is not enough due to the \nfourth policy. If a method has the upper bound larger then 15, for example, the acquired free resisters \nare useless for certain classes of instructions. SwiftHand solves this problem by register swapping: \nadding instructions to swap out a content of low registers to free high registers, injecting the main \ncode utilizing the newly acquired free low registers, and adding instructions to restore the original \ncontent of the low registers after after the main code. More interestingly, not all registers can be \nswapped out due to the .fth policy and the exception mechanism. Dalvik VM supports a try/catch style \nexception mechanism. Dalvik VM considers that every instruction in a try block can raise an exception. \nIn terms of program control .ow, the .rst in\u00adstruction of a catch block is a direct successor for all \nin\u00adstructions in the related try block. Thus, registers used in the catch block should have a single \ntype throughout the en\u00adtire try block. They are not reusable. SwiftHand considers that these registers \nare constrained and avoids using them. We modi.ed the live-register analysis to additionally calcu\u00adlate \ngroups of constrained registers. This heuristic will not work if all registers in an instruction s operand \nrange are constrained. Fortunately, we have not faced such an extreme situation. Note that instrumentation \ncan be performed even with a single unconstrained register. This can be done in four steps: (a) Swap \nout the unconstrained register. (b) Move all values to be used into a globally shared data structure \none by one. A static method moving one value at a time, which requires only one low low register to invoke, \ncould be used. (c) Invoke the static method containing the actual instructions to be injected. At the \nbeginning of the method, values should be loaded from the globally shared data structure and casted into \ntheir original types. (d) Restore the original content of the unconstraint register. APK Management. \nAndroid apps are distributed as a sin\u00adgle Apk package .le. To perform instrumentation, SwiftHand needs \nto extract a program binary from a package .le and in\u00adject the modi.ed binary to the package .le. Apk \n.le is a zip archive having a prede.ned direc\u00adtory structure. The archive contains a class.dex .le, a \nManifest.xml .le, a META-INF directory, and other re\u00adsource .les. class.dex .le contains Dalvik byte \ncode to be  executed. Manifest.xml is a binary encoded xml .le de-5.1 Experimental Setup scribing various \nparameters such as main activity and priv\u00adilege setting. META-INF directory includes signature of the \napp. Figure 5: Flow of App Modi.cation Process Figure 5 shows the .ow of the app modi.cation process. \nFirst, the app package is unpacked. Then class.dex and Manifest.xml .les are instrumented. The Manifest.xml \n.le has to be modi.ed because the observer thread in the instrumented app needs network access privilege \nto commu\u00adnicate with the SwiftHand tool. META-INF directory is re\u00admoved since the original signature \ncon.icts with the mod\u00adi.ed package. After the modi.cation, the app is repacked, signed, and delivered \nto experiment devices. For testing pur\u00adpose, the key for the resigned signature does not need to be identical \nto the key for the original signature.  4.4 Limitations The current SwiftHand implementation has three \nlimita\u00adtions. First, the current implementation does not support apps whose main entry routine is native. \nSeveral games fall under this category and we have excluded them from our benchmark-suite. Second, the \ncurrent implementation works correctly only with devices with Android 4.1 or higher versions. Third, \nSwiftHand cannot handle apps that use internet connectivity to store data on a remote server. This is \na limita\u00adtion of our algorithm. Our algorithm needs to reset apps oc\u00adcasionally. However, after a reset \na previously feasible trace could become infeasible depending on the content stored on a remote server. \nThis violates a key requirement of our al\u00adgorithm. This limitation could be eliminated by sandboxing \nthe remote server or by mocking the remote server. 5. Empirical Evaluation In this section, we evaluate \nSwiftHand on several real-world Android apps. We .rst compare SwiftHand with random testing and L*-based \ntesting. Then we discuss the results and shortcomings of SwiftHand. We use branch coverage to compare \nthe effectiveness and performance of the three testing strategies. We use binary instrumentation to obtain \ncode coverage information. Our benchmark-suite consists of 10 apps from F-Droid (https://f-droid.org) \nopen app market. The apps were randomly selected initially, then apps with a native entry routine or \nfrequent internet access are excluded. name category #inst. #method #branch music note whohas explorer \nweight tippy myexpense mininote mileage anymemo sanity educational game lent item tracker JVM status \nlookup weight tracker tip calculator .nance manager text viewer car management .ash card system management \n1345 2366 6034 4283 4475 11472 13892 44631 72145 21946 72 146 252 222 303 482 680 2583 832 1415 245 464 \n885 813 1090 1948 2489 3761 4954 5723 Table 1: Benchmark Apps Table 1 lists these apps. #inst, #method, \nand #branch columns report number of bytecode instructions in the app binary, number of methods, and \nnumber of branches (exclud\u00ading framework libraries), respectively. We performed the experiment on a 8-core \nIntel Xeon 2.0Ghz (E5335) linux machine with 8Gb RAM using 5 em\u00adulators. We use 3 hours as our testing \nbudget per app for ev\u00adery strategy. For unknown reasons, sanity and anymemo did not work smoothly on \nemulators. We used a Google Galaxy Nexus phone to test those apps and used a time budget of 1 hour. Android \nversion 4.1.2 (Jelly Bean) was used on both emulators and phone. In random testing, we restart an app \nwith probability 0.1 and pick an input from a set of enabled inputs uniformly at random. We have implemented \nall three strategies as a part of the SwiftHand implementation. 5.2 Comparison with Random and L*-based \nTesting Table 2 summarizes the results of applying the three testing strategies to the ten Android apps. \nIn the table we use SH to denote SwiftHand. % Branch Coverage column reports branch coverage percentage \nfor each strategy. % Time Spent on Reset columns show percentage of execution time spent on restarting \napp. #Reset/#Input columns report the ratio of the number restarts and the number of inputs generated \nfor each strategy. #Unique Pre.xes columns report the number of unique input pre.xes tried in each testing \nstrategy. # States in Model columns report number of unique states learned within the given time budget. \nFigure 6 and Figure 7 plots percentage branch coverage progress against testing time. In all cases, SwiftHand \nachieves more coverage than both random and L*-based testing. Random and L*-based   App % Branch Coverage \nSH Rand L * % Time Spent on Reset SH Rand L * #Resets / #Inputs Ratio SH Rand L * #Unique Input Pre.xes \nSH Rand L * #State in Model SH L * music note 72.2 68.6 56.7 10.5 36.9 43.0 0.05 0.24 0.33 1419 712 275 \n46 15 whohas 59.3 54.7 54.3 10.0 26.2 41.6 0.05 0.14 0.33 1481 983 289 97 73 explorer 74.0 73.4 65.5 \n2.1 18.2 42.0 0.02 0.11 0.40 1271 1047 305 195 150 weight 62.1 57.6 48.2 8.3 25.6 42.4 0.04 0.12 0.24 \n1341 855 271 109 94 tippy 68.5 60.3 32.9 17.6 49.2 81.6 0.02 0.11 0.54 1435 812 162 71 17 myexpense 41.8 \n38.4 23.6 4.0 32.3 46.1 0.02 0.15 0.25 1740 926 281 149 63 mininote 34.1 27.2 16.8 9.8 25.7 49.1 0.19 \n0.13 0.37 1562 1051 334 169 72 mileage 34.6 28.5 23.3 12.7 32.4 58.8 0.03 0.18 0.55 1109 599 160 130 \n72 anymemo 52.9 37.9 26.6 3.45 34.6 50.8 0.01 0.19 0.33 1366 632 247 169 50 sanity 19.6 17.0 13.1 15.2 \n32.5 46.3 0.06 0.16 0.27 1012 501 230 78 99 Table 2: Summary of Comparison Experiments testing performs \nrelatively well for simple apps such as whohas and explorer. However, for more complex apps, such as \nanymemo, SwiftHand outperforms both random and L*-based testing. For all apps, SwiftHand achieves branch \ncoverage at a rate that is signi.cantly faster than that of random and L*-based testing. For example, \nin explorer SwiftHand reached almost 65% branch coverage within 10 min\u00adutes whereas both random and L*-based \ntesting failed to reach 45% coverage in 10 minutes. This implies that SwiftHand is a far better choice \nwhen the time budget for testing is limited.  Random testing restarts more frequently than SwiftHand. \nAndroid apps, in comparison with desktop apps, have fewer GUI components on screen. Therefore, the prob\u00adability \nof random testing to terminate an app, by acci\u00addentally pushing Back button or some other quit button, \nis relatively high. SwiftHand can avoid this by merging states and by avoiding input sequences that lead \nto a ter\u00adminal state.  Random testing catches up with the coverage of Swift-Hand given an additional \nperiod of time for some small apps; musicnote, explorer, and tippy. However, ran\u00addom testing fails to \nclose the coverage gap for compli\u00adcated apps; mininote, mileage, anymemo, and sanity.  Our experiments \ncon.rm inadequacy of L* as a testing strategy when restart is expensive. L* spent about half of its execution \ntime in restarting the app under test (Table 2). This resulted in the low branch coverage over time (Figure \n6, Figure 7). Furthermore, note that L* has rela\u00adtively small numbers in the #Unique Input Pre.xes col\u00adumn \ncompared to random testing and SwiftHand. This in\u00addicates that L* executes the same pre.x more often \nthan random testing and SwiftHand do.   5.3 What Restricts Test Coverage? We manually analyzed apps \nwith a coverage lower than 40% to understand the cause behind low branch coverage. We discovered the \nfollowing three key reasons behind low branch coverage.  Combinatorial State Explosion. mileage is a \nvehicle management app. The app has a moderately complex GUI. The app provides tab bar for easy context \nswitching be\u00adtween different tasks. In this app, tab bar creates a combina\u00adtorial state-explosion problem. \nConceptually, each tab view is meant for a different independent task. Most actions on a tab view affect \nonly the local state of the tab and few actions may affect the app s global state. SwiftHand does not \nunder\u00adstand this difference. As a result a slight change on one tab view makes all other tabs to treat \nthe change as a new state in the model. The following diagram illustrates this situation: Figure 8: \nModel state explosion with tab view The app sanity has a similar problem. This is an inher\u00adent limitation \nof our current model. We believe that this lim\u00aditation can be overcome by considering a framework where \neach model is a cross-product of several independent mod\u00adels. We can then perform compositional machine \nlearning to learn the sub models independent of each other. However, at this point it is not clear how \nthis compositional learning can be made online. Network Connection. anymemo is a .ash card app with dictionary \nsupport. The user can access a number of reposi\u00adtory to download dictionaries and wordlists. We cannot \ntest this part of the app, because we disable internet connections to ensure clear restart. Inter-app \nCommunication. mininote uses intent to open a text .le from .le system navigation. When an intent is \ncap\u00adtured, the Android system pops a system dialog for con.r\u00admation. The con.rmation dialog box is not \na part of the app. SwiftHand simply waits for the app to wake up and termi\u00adnates the app when it fails \nto respond. As a result the viewer part of the app is never tested. 35% code coverage solely comes from \ntesting .le system navigation part. 6. Related Work Automated GUI Testing. Model-based testing approaches \nare often applied to testing graphical user interfaces [8, 11, 28, 37, 39, 43]. Such approaches model \nthe behavior of the GUI abstractly using a suitable formalism such as event-.ow graphs [28], .nite state \nmachines [32, 39], or Petri nets [37]. The models are then used to automatically generate a set of sequences \nof inputs (or events), called a test-suite. Memon et al. [28, 46] have proposed two-staged auto\u00admatic \nmodel-based GUI testing idea. In the .rst stage, a model of the target application is extracted by dynamically \nanalyzing the target program. In the second stage, test cases are created from the inferred model. This \napproach differs from our work in two ways: (a) in our work, model cre\u00adation and testing forms a feedback \nloop, (b) we use ELTS, a more richer model than Event Flow Graph (EFG). Yuan and Memon [47] also investigated \na way to re.ect runtime information on top of EFG model to guide testing. Crawljax [29] is an online \nmodel-leaning based testing technique for Javascript programs. Crawljax tries to learn a .nite state \nmodel of the program under test using state\u00admerging. State-merging is based on the edit distance be\u00adtween \nDOMs. Crawljax can be seen as a learning-based GUI testing technique that uses ad-hoc criteria for state-merging. \nRecently, a number of model-learning based testing tech\u00adniques targeting Android application [6, 33, \n36, 40, 45] have been proposed. Nguyen et al. s approach [33] combines of\u00ad.ine model-based testing and \ncombinatorial testing to re.ne created test cases. Yang et al. [45] take online model-based testing approach \nsimilar to Crawljax [29]. They use static pre-analysis to re.ne the search space. Takala et al. [40] \nre\u00adport a case study using online model-based testing. Rastogi et al. [36] proposed a testing technique \nbased on learning a behavioral model of target application. All of these tech\u00adniques use some form of \nheuristic state-merging and are not based on formal automata learning. The main difference is in the \ntreatment of counter-examples. Online automata learning techniques try to learn an exact model re.ecting \ncounter\u00adexamples. On the contrary, none of the above approaches learn from counter-examples. Therefore, \nthese techniques may fail to learn the target model even for a .nite state sys\u00adtem, unlike online automata \nlearning techniques. As far as we know, SwiftHand is the .rst GUI testing technique based on automata \nlearning. The Android development kit provides two testing tools, Monkey and MonkeyRunner. Monkey is \nan automated fuzz testing tool creates random inputs without considering appli\u00adcation s state. Hu and \nNeamtiu [20] developed an useful bug .nding and tracing tool based on Monkey. MonkeyRunner is a remote \ntesting framework. A user can control application running on the phone from the computer through USB \ncon\u00adnection. MacHiry et al.[25] suggest a technique to infer represen\u00adtative set of events and perform \nevent aware random testing. The idea is similar to the random strategy used in our exper\u00adiment. Their \nevent inference technique targets a larger event space including tilting and long-touching while our \ntech\u00adnique only considers touching and scrolling events. Also, their tool acquires more runtime information \nby modifying Android framework to prune out more events at the expense of being less portable. On the \ncontrary, SwiftHand modi.es only the target app binary. Finally, they provide comparison with Monkey. \nThe result shows that Monkey needs signi.\u00adcantly more time to tie the branch coverage of event aware \nrandom testings.  Anand et al. [7] applied concolic execution [16] to guide Android app testing. They \nuse state subsumption check to avoid repeatedly visiting equivalent program states. Their technique is \nrelatively sound than our approach, since con\u00adcolic execution engine creates an exact input to .nd new \nbranches in each iteration. In Mirzaei et al. [30] s compiles Android apps to Java byte code and applies \nJPF with a mock Android framework to explore the state-space. Learning Algorithms. Angluin s L* [10] \nis the most well\u00adknown active learning algorithm. The technique has been successfully applied to modular \nveri.cation [13] and API speci.cation inference [5]. These techniques try to learn a precise model of \nthe target application. Passive learning techniques [15, 23] do not assume the presence of a teacher \nand uses positive and negative ex\u00adamples to learn a model. Franc\u00b8ois et al. [14] have intro\u00adduced ideas \nof exploiting domain knowledge to guide pas\u00adsive learning. The technique was subsequently improved by \nLambeau et al. [21]. Learning based Testing. Machine learning has previously been used to make software \ntesting effective and ef.\u00adcient [12, 17 19, 26, 34, 35, 41, 42, 44]. Meinke and Walkin\u00adshaw s survey \n[27] provides a convenient introduction to the topic. In general, classic learning based testing techniques \naim to check whether a program module satis.es a prede\u00ad.ned functional speci.cation. Also, they use a \nspeci.cation and a model checker to resolve equivalence queries. On the contrary, SwiftHand tries to \nmaximize a test coverage and actually executes a target program to resolve equivalence queries. Meinke \nand Sindhu [26] reported that learning algorithms similar to L* are inadequate as a testing guide. They \npro\u00adposed an ef.cient active learning algorithm for testing re\u00adactive systems, which uses a small number \nof membership queries. 7. Conclusion We showed that a straight-forward L*-based testing algo\u00adrithm is \nnot effective for testing GUI applications. This is because L*-based testing requires a lot of expensive \nrestarts and it spends a lot of time in re-exploring the same execution pre.xes. We proposed a novel \nlearning-based testing algo\u00adrithm for Android GUI apps, which tries to maximize branch coverage quickly. \nThe algorithm avoids restarts and aggres\u00adsively merges states in order to quickly prune the state space. \nAggressive state-merging could over-generalize a model and lead to inconsistency between the app and \nthe learned model. Whenever the algorithm discovers such an inconsistency, it applies passive learning \nto rectify the inconsistency. Our ex\u00adperiments show that for complex apps, our algorithm outper\u00adforms \nboth random and L*-based testing. Our algorithm also achieves branch coverage at a much faster rate than \nrandom and L*-based testing. Acknowledgement This research is supported in part by NSF Grants CCF\u00ad1017810, \nCCF-0747390, CCF-1018729, and CCF-1018730, and a gift from Samsung. The last author is supported in part \nby a Sloan Foundation Fellowship. We would like to thank Cindy Rubio Gonzalez and Wonchan Lee for insightful \ncom\u00adments on a previous draft of the paper. References [1] Managing the Activity Lifecycle. http:// developer.android.com/training/basics/ \nactivity-lifecycle/index.html. [2] MonkeyRunner. http://developer.android.com/ tools/help/monkeyrunner_concepts.html. \n[3] UI/Application Exerciser Monkey. http://developer. android.com/tools/help/monkey.html. [4] axml, \nread and write Android binary xml .les. http:// code.google.com/p/axml/, 2012. [5] R. Alur, P. Cern \u00b4y, \nP. Madhusudan, and W. Nam. Synthesis of interface speci.cations for java classes. In POPL, pages 98 109, \n2005. [6] D. Amal.tano, A. R. Fasolino, P. Tramontana, S. D. Carmine, and A. M. Memon. Using GUI ripping \nfor automated testing of Android applications. In ASE, pages 258 261, 2012. [7] S. Anand, M. Naik, M. \nJ. Harrold, and H. Yang. Auto\u00admated concolic testing of smartphone apps. In SIGSOFT FSE, page 59, 2012. \n[8] A. A. Andrews, J. Offutt, and R. T. Alexander. Testing web applications by modeling with FSMs. Software \nand System Modeling, 4(3):326 345, 2005. [9] D. Angluin. Inference of reversible languages. J. ACM, 29(3):741 \n765, July 1982. [10] D. Angluin. Learning regular sets from queries and counterex\u00adamples. Inf. Comput., \n75(2):87 106, 1987. [11] F. Belli. Finite-state testing and analysis of graphical user interfaces. In \n12th International Symposium on Software Reliability Engineering (ISSRE 01), page 34. IEEE Computer Society, \n2001. [12] T. Berg, O. Grinchtein, B. Jonsson, M. Leucker, H. Raffelt, and B. Steffen. On the correspondence \nbetween conformance testing and regular inference. In FASE, pages 175 189, 2005. [13] J. M. Cobleigh, \nD. Giannakopoulou, and C. S. Pasareanu. Learning assumptions for compositional veri.cation. In TACAS, \npages 331 346, 2003. [14] F. Coste, D. Fredouille, C. Kermorvant, and C. de la Higuera. Introducing domain \nand typing bias in automata inference. In ICGI, pages 115 126, 2004. [15] J. N. Departarnento and P. \nGarcia. Identifying regular lan\u00adguages in polynomial. In Advances in Structural and Syntac\u00adtic Pattern \nRecognition, volume 5 of Series in Machine Per\u00ad  ception and Arti.cial Intelligence, pages 99 108. World \nSci\u00adenti.c, 1992. [16] P. Godefroid, N. Klarlund, and K. Sen. DART: directed automated random testing. \nIn PLDI, pages 213 223, 2005. [17] A. Groce, D. Peled, and M. Yannakakis. AMC: An adaptive model checker. \nIn CAV, pages 521 525, 2002. [18] A. Groce, D. Peled, and M. Yannakakis. Adaptive model checking. Logic \nJournal of the IGPL, 14(5):729 744, 2006. [19] R. Groz, M.-N. Irfan, and C. Oriat. Algorithmic improvements \non regular inference of software models and perspectives for security testing. In ISoLA (1), pages 444 \n457, 2012. [20] C. Hu and I. Neamtiu. A GUI bug .nding framework for Android applications. In SAC, pages \n1490 1491, 2011. [21] B. Lambeau, C. Damas, and P. Dupont. State-merging DFA induction algorithms with \nmandatory merge constraints. In ICGI, pages 139 153, 2008. [22] K. Lang, B. Pearlmutter, and R. Price. \nResults of the Ab\u00adbadingo One DFA learning competition and a new evidence\u00addriven state merging algorithm, \n1998. [23] K. J. Lang. Random DFA s can be approximately learned from sparse uniform examples. In COLT, \npages 45 52, 1992. [24] X. Leroy. Java bytecode veri.cation: algorithms and formal\u00adizations. Journal \nof Automated Reasoning, 30(3 4):235 269, 2003. [25] A. MacHiry, R. Tahiliani, and M. Naik. Dynodroid: \nAn input generation system for Android apps. In SIGSOFT FSE, pages 224 235, 2013. [26] K. Meinke and \nM. A. Sindhu. Incremental learning-based testing for reactive systems. In TAP, pages 134 151, 2011. [27] \nK. Meinke and N. Walkinshaw. Model-based testing and model inference. In ISoLA (1), pages 440 443, 2012. \n[28] A. M. Memon. An event-.ow model of GUI-based applica\u00adtions for testing. Softw. Test., Verif. Reliab., \n17(3):137 157, 2007. [29] A. Mesbah, A. van Deursen, and S. Lenselink. Crawling Ajax\u00adbased web applications \nthrough dynamic analysis of user in\u00adterface state changes. TWEB, 6(1):3, 2012. [30] N. Mirzaei, S. Malek, \nC. S. Pasareanu, N. Esfahani, and R. Mahmood. Testing Android apps through symbolic execu\u00adtion. ACM SIGSOFT \nSoftware Engineering Notes, 37(6):1 5, 2012. [31] J. Nevo and P. Cr\u00b4egut. ASMDEX. http://asm.ow2.org/ \nasmdex-index.html, 2012. [32] W. M. Newman. A system for interactive graphical program\u00adming. In Proc. \nof the spring joint computer conference (AFIPS 68 (Spring)), pages 47 54. ACM, 1968. [33] C. D. Nguyen, \nA. Marchetto, and P. Tonella. Combining model-based and combinatorial testing for effective test case \ngeneration. In ISSTA, pages 100 110, 2012. [34] D. Peled, M. Y. Vardi, and M. Yannakakis. Black box check\u00ading. \nIn FORTE, pages 225 240, 1999. [35] H. Raffelt, B. Steffen, and T. Margaria. Dynamic testing via automata \nlearning. In Haifa Veri.cation Conference, pages 136 152, 2007. [36] V. Rastogi, Y. Chen, and W. Enck. \nAppsplayground: auto\u00admatic security analysis of smartphone applications. In CO-DASPY, pages 209 220, \n2013. [37] H. Reza, S. Endapally, and E. Grant. A model-based ap\u00adproach for testing GUI using hierarchical \npredicate transition nets. In International Conference on Information Technology (ITNG 07), pages 366 \n370. IEEE Computer Society, 2007. [38] R. L. Rivest and R. E. Schapire. Inference of .nite automata using \nhoming sequences (extended abstract). In STOC, pages 411 420, 1989. [39] R. K. Shehady and D. P. Siewiorek. \nA methodology to auto\u00admate user interface testing using variable .nite state machines. In FTCS, pages \n80 88, 1997. [40] T. Takala, M. Katara, and J. Harty. Experiences of system\u00adlevel model-based GUI testing \nof an Android application. In ICST, pages 377 386, 2011. [41] N. Walkinshaw, K. Bogdanov, J. Derrick, \nand J. Paris. Increas\u00ading functional coverage by inductive testing: A case study. In ICTSS, pages 126 \n141, 2010. [42] N. Walkinshaw, J. Derrick, and Q. Guo. Iterative re.nement of reverse-engineered models \nby model-based testing. In FM, pages 305 320, 2009. [43] L. White and H. Almezen. Generating test cases \nfor GUI responsibilities using complete interaction sequences. In 11th International Symposium on Software \nReliability Engineering (ISSRE 00), page 110. IEEE Computer Society, 2000. [44] T. Xie and D. Notkin. \nMutually enhancing test generation and speci.cation inference. In FATES, pages 60 69, 2003. [45] W. Yang, \nM. R. Prasad, and T. Xie. A grey-box approach for automated GUI-model generation of mobile applications. \nIn FASE, pages 250 265, 2013. [46] X. Yuan, M. Cohen, and A. M. Memon. Covering array sampling of input \nevent sequences for automated GUI testing. In ASE 07: Proceedings of the twenty-second IEEE/ACM international \nconference on Automated software engineering, pages 405 408, New York, NY, USA, 2007. ACM. [47] X. Yuan \nand A. M. Memon. Iterative execution-feedback model-directed GUI testing. Information &#38; Software \nTech\u00adnology, 52(5):559 575, 2010.    \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Smartphones and tablets with rich graphical user interfaces (GUI) are becoming increasingly popular. Hundreds of thousands of specialized applications, called apps, are available for such mobile platforms. Manual testing is the most popular technique for testing graphical user interfaces of such apps. Manual testing is often tedious and error-prone. In this paper, we propose an automated technique, called <i>Swift-Hand</i>, for generating sequences of test inputs for Android apps. The technique uses machine learning to learn a model of the app during testing, uses the learned model to generate user inputs that visit unexplored states of the app, and uses the execution of the app on the generated inputs to refine the model. A key feature of the testing algorithm is that it avoids restarting the app, which is a significantly more expensive operation than executing the app on a sequence of inputs. An important insight behind our testing algorithm is that we do not need to learn a precise model of an app, which is often computationally intensive, if our goal is to simply guide test execution into unexplored parts of the state space. We have implemented our testing algorithm in a publicly available tool for Android apps written in Java. Our experimental results show that we can achieve significantly better coverage than traditional random testing and L*-based testing in a given time budget. Our algorithm also reaches peak coverage faster than both random and L*-based testing.</p>", "authors": [{"name": "Wontae Choi", "author_profile_id": "83358667257", "affiliation": "University of California, Berkeley, CA, USA", "person_id": "P4290427", "email_address": "wtchoi@cs.berkeley.edu", "orcid_id": ""}, {"name": "George Necula", "author_profile_id": "81100295630", "affiliation": "University of California, Berkeley, CA, USA", "person_id": "P4290428", "email_address": "necula@cs.berkeley.edu", "orcid_id": ""}, {"name": "Koushik Sen", "author_profile_id": "81100399070", "affiliation": "University of California, Berkeley, CA, USA", "person_id": "P4290429", "email_address": "ksen@cs.berkeley.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509552", "year": "2013", "article_id": "2509552", "conference": "OOPSLA", "title": "Guided GUI testing of android apps with minimal restart and approximate learning", "url": "http://dl.acm.org/citation.cfm?id=2509552"}