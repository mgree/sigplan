{"article_publication_date": "10-29-2013", "fulltext": "\n Language Support for Dynamic, Hierarchical Data Partitioning Sean Treichler Michael Bauer Alex Aiken \nStanford University Stanford University Stanford University sjt@cs.stanford.edu mebauer@cs.stanford.edu \naiken@cs.stanford.edu Abstract Applications written for distributed-memory parallel archi\u00adtectures must \npartition their data to enable parallel execu\u00adtion. As memory hierarchies become deeper, it is increas\u00adingly \nnecessary that the data partitioning also be hierarchical to match. Current language proposals perform \nthis hierarchi\u00adcal partitioning statically, which excludes many important applications where the appropriate \npartitioning is itself data dependent and so must be computed dynamically. We de\u00adscribe Legion, a region-based \nprogramming system, where each region may be partitioned into subregions. Partitions are computed dynamically \nand are fully programmable. The division of data need not be disjoint and subregions of a re\u00adgion may \noverlap, or alias one another. Computations use regions with certain privileges (e.g., expressing that \na com\u00adputation uses a region read-only) and data coherence (e.g., expressing that the computation need \nonly be atomic with respect to other operations on the region), which can be con\u00adtrolled on a per-region \n(or subregion) basis. We present the novel aspects of the Legion design, in par\u00adticular the combination \nof static and dynamic checks used to enforce soundness. We give an extended example illustrating how \nLegion can express computations with dynamically de\u00adtermined relationships between computations and data \nparti\u00adtions. We prove the soundness of Legion s type system, and show Legion type checking improves performance \nby up to 71% by eliding provably safe memory checks. In particu\u00adlar, we show that the dynamic checks \nto detect aliasing at runtime at the region granularity have negligible overhead. We report results for \nthree real-world applications running on distributed memory machines, achieving up to 62.5X speedup on \n96 GPUs on the Keeneland supercomputer. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright is held by the owner/author(s). Publication \nrights licensed to ACM. ACM 978-1-4503-2374-1/13/10. . . $15.00. Categories and Subject Descriptors \nD.1.3 [Programming Techniques]: Concurrent Programming; D.3.1 [Program\u00adming Languages]: Formal De.nitions \nand Theory; F.3.2 [Logics and Meanings of Programs]: Semantics of Program\u00adming Languages Keywords Legion; \nregions; type system; independence; aliasing; hierarchical scheduling; data partitioning; coher\u00adence \n1. Introduction In the last decade machine architecture, particularly at the high performance end of \nthe spectrum, has undergone a rev\u00adolution. The latest supercomputers are now composed of het\u00aderogeneous \nprocessors and deep memory hierarchies. Cur\u00adrent programming systems for these machines have elabo\u00adrate \nfeatures for describing parallelism, but few abstractions for describing the organization of data. However, \nhaving the data organized correctly within the machine is becoming ever more important. Current supercomputers \nhave at least six levels of memory, most of which are explicitly man\u00adaged by software; even current commodity \ndesktop and mo\u00adbile computers have at least .ve levels.1 As machines of all scales increase the number \nof processing cores and quantity of available memory, the latency between system compo\u00adnents inevitably \nincreases. For many applications the place\u00adment and movement of data is already the dominant perfor\u00admance \nconsideration, particularly in high-end machines, and this problem will only grow more acute as overall \ntransistor counts and latencies in future machines increase while the total power budget remains relatively \nconstant. To program parallel machines with distributed memory (hierarchically organized or not), data \nmust be partitioned into subsets that are placed in the individual memories. For example, in graph computations \nit is common to subdivide the graph into subgraphs sized to .t in fast memory close to a processor. Note \nthat the term partition does not imply the subdivisions of the data are always disjoint it is desir\u00ad \n1 A typical organization is (1) distributed memory across a physical network of nodes; (2) shared RAM \non chip; (3) one to three levels of cache for each CPU, some shared, some not; (4) GPU global memory; \n(5) GPU shared memory; (6) GPU registers. Only the CPU caches are managed by hardware and only the global \nnetwork is not present in commodity http://dx.doi.org/10.1145/2509136.2509545 consumer machines. able \nto also allow subdivisions that overlap or alias. Con\u00adtinuing with the example, many graph computations \nrequire knowledge of the nodes bordering each subgraph. Some of these ghost nodes for a particular subgraph \nmay also bor\u00adder other subgraphs. In general, the ghost nodes for different subgraphs often alias. In \nmachines with more than two levels of explicitly man\u00adaged memory, data partitioning involves a hierarchy \nwhere the initial partitions of the data are themselves further parti\u00adtioned. Often divide-and-conquer \nstrategies repeatedly sub\u00addivide the data so that the .nest granularity .ts in the small\u00adest, fastest \nmemory closest to a processor where a speci.c computation can access it, which results in complex commu\u00adnication \npatterns as coarser and .ner sets of data are shuf.ed up and down the memory hierarchy [10]. Thus, the \nplace\u00adment and movement of data, and subsets of data, is a .rst\u00adorder programming concern. We adopt a \nregion-based ap\u00adproach that makes these groupings of data explicit in the pro\u00adgram: a logical region \nnames a set of data, a subregion of a logical region names a subset of a logical region s data, and a \npartitioning of a logical region r names a number of (pos\u00adsibly overlapping) subregions of r. We use \nthe term logical region (which we sometimes abbreviate to region) to empha\u00adsize that our language-level \nregions do not imply a physical layout or placement of the logical region s data in the mem\u00adory hierarchy. \nLogical regions are just sets of elements and a subregion is literally a subset of its parent region.2 \nBy making the groupings of data into regions explicit, it becomes possible for the programmer to express \nproperties of the different regions in a program and for the language system to leverage this information \nfor both performance and correctness in ways that would be dif.cult to infer with\u00adout the programmer \ns guidance. In addition to partitioning regions into subregions, we focus on three properties that Legion \nprogrammers can express about regions: Privileges. Computations have privileges specifying how they \ncan use regions: read-only, read-write, and reduce. More computations can execute in parallel using privi\u00adleges \nthan without. For example, regions that alias can still be accessed simultaneously by multiple parallel \ncomputations provided that the computations all access the regions with read-only privileges, or all \naccess the regions to perform reductions using the same associative and commutative reduction operator. \n Coherence. Computations are written in a sequential pro\u00adgram order. By default all computations access \nregions with exclusive coherence, which ensures the compu\u00adtations appear to execute in the sequential \norder, per\u00ad  2 A separate system of physical regions hold concrete copies of the data of logical regions \nat run-time. Physical regions have a speci.c data layout and live in a speci.c memory. The Legion run-time \nsystem may maintain multiple physical copies of a single logical region for performance reasons; for \nexample, read-only may be replicated in multiple physical regions to put it closer to the computations \nthat use it. mitting parallelism only when computations access dis\u00adjoint regions or have non-interfering \nprivileges. However, computations can also request relaxed coherence modes atomic and simultaneous on \nregions. Relaxed coherence modes allow reordering and parallel execution of com\u00adputations that otherwise \nwould execute sequentially due to accessing aliased sets of regions. For example, two computations each \nrequesting atomic coherence on the same region may be re-ordered with respect to the se\u00adquential execution \norder so long as their accesses are serializable. Simultaneous coherence imposes no restric\u00adtions on \nother computations access to a region; one in\u00adstance where simultaneous access is useful is when a programmer \nhas implemented his own, higher-level syn\u00adchronization mechanism. Aliasing. As outlined above, regions \ncan be partitioned into subregions that may be disjoint or may overlap. De\u00adtecting region aliasing is \nnecessary to identify computa\u00adtions that can run in parallel. A central insight of our ap\u00adproach is that \ndetecting region aliasing is both easy and inexpensive when done dynamically at the granularity of logical \nregions instead of individual memory locations. Previous work on hierarchically partitioned data has \nfo\u00adcused on fully static approaches with no runtime overhead. A key feature of these systems is that \nthey disallow all alias\u00ading to make their static analyses tractable. Two recent ex\u00adamples, Sequoia [10] \nand Deterministic Parallel Java (DPJ) [4], each provide a mechanism to statically partition the heap \ninto a tree of collections of data. The two designs are dif\u00adferent in many aspects, but agree that there \nis a single tree\u00adshaped partitioning of data that must be checked statically (see Section 10). Both approaches \nalso include a system of privileges, but have either no or limited coherence systems. Our own experience \nwriting high-performance applica\u00adtions in Sequoia [10] as well as in the current industry stan\u00addard mix \nof MPI, shared-memory threads, and CUDA has taught us that a fully static system is insuf.cient. In many \ncases, the best way to partition data is a function of the data itself the partitions must be dynamically \ncomputed and cannot be statically described. Furthermore, applications often need multiple, simultaneous \npartitions of the same data a single partitioning is not enough. Because data par\u00adtitioning is at the \ncenter of what these applications do, shift\u00ading from fully static partitions to partitions computed at \nrun\u00adtime affects all aspects of the programming model, and in particular the interactions between aliasing, \nprivileges, and coherence. The challenge is to design a system that is both semantically sound and .exible \nin handling partitions, priv\u00adileges and coherence with minimal runtime overhead. In this paper, we present \nstatic and dynamic semantics for Legion [2], a parallel programming model that supports mul\u00adtiple, dynamic \ndata partitions and is able to ef.ciently reason about aliasing, privileges, and coherence. Speci.cally: \n Legion s logical regions are .rst-class values and may be dynamically allocated and stored in data \nstructures.  Logical regions can be dynamically partitioned into sub\u00adregions; partitions are fully programmable. \n A logical region may be dynamically partitioned in mul\u00adtiple different ways; subregions from multiple \npartitions may include the same data.  For each computation, privileges and coherence modes are speci.ed \non a per-region basis, giving the program\u00admer .ne-grained control over how data is accessed. We make \nthe following speci.c contributions:  We present a type system for Core Legion programs that statically \nveri.es the safety of individual pointers and region privileges at call boundaries (Section 4).  We \npresent a novel parallel operational semantics for Core Legion. This semantics is compositional, hierarchi\u00adcal, \nand asynchronous, re.ecting the way such programs actually execute on the hardware (Section 5.3).  We \nprove the soundness of Legion s static type and priv\u00adilege system (Section 6). In particular, we show \nthat Le\u00adgion s very liberal dynamic manipulations of regions can be handled with a combination of static \nand inexpensive dynamic checks.  Using the soundness of the type system, we show that if expressions \ne1 and e2 are non-interfering (can be ex-  SS ecuted in parallel), then subexpressions e1 of e1 and \ne2 of e2 are also non-interfering (Section 8). This result is the basis for Legion s hierarchical, distributed \nscheduler, which is crucial for high performance on the target class of machines. We note that no other \nparallel language or runtime system currently supports distributed scheduling. We give experimental evidence \nthat supports the Le\u00adgion design choices. On three real-world applications, we show that dynamic region \npointer checks would be expensive, justifying checking this aspect of the type system statically. We \nalso show that the cost of region aliasing checks is low, showing that an expressive and dynamic language \nwith aliasing is compatible with both high performance and safety (Section 9). 2. Circuit Example We \nbegin by introducing a circuit simulation written in the Legion programming model that serves as a running \nexam\u00adple throughout the remainder of the paper. In this section we describe how the requirements of the \nsimulation motivate the novel features of Legion. Section 3 introduces the Core Le\u00adgion language by showing \nexamples of code from the circuit simulation. The circuit simulation takes as input an arbitrary graph \nof circuit elements (wires and nodes where the wires con\u00adnect) represented by the two logical regions \nall nodes (a) Node region tree. (b) Circuit piece coloring. (c) Ghost rg0 coloring. (d) Ghost rg1 coloring. \n  Figure 1. Partitions of the all nodes region. and all wires. The simulation iterates for many time \nsteps, performing three computations during each time step: calc new currents, distribute charge, and \n.\u00adnally update voltage. For these computations to be run in parallel, the regions representing the graph \nmust be par\u00adtitioned into pieces that match the simulation s data access patterns. The choice of partitioning \nwill ultimately dictate performance and is therefore the most important decision in any Legion program. \nAn ideal partitioning depends on many factors, includ\u00ading the shape of data structures, the input, and \nthe desired number of partitions (which usually varies with the target machine). Due to the multitude \nof factors that can in.uence partitioning, a critical design decision made in Legion is to provide a \nprogrammable interface whereby the application can compute a partitioning dynamically and communicate \nthat partitioning to the Legion runtime system. This design absolves the Legion implementation of the \nresponsibility for computing an ideal partition for all regions across all appli\u00adcations on any potential \narchitecture. Instead, our approach provides the application with direct control over all parti\u00adtioning \ndecisions that ultimately impact performance. In Legion, partitioning takes place in two steps. First, \nthe programmer assigns a color to each element of the region to be partitioned. The number of colors \nand how they are as\u00adsigned to elements can be the result of an arbitrary computa\u00adtion, giving the programmer \ncomplete control over the col\u00adoring. Second, Legion creates new subregions, one for each color, with \neach region element assigned to the subregion of the appropriate color. Thus, the programmer expresses \nthe desired partitioning of a region, and Legion provides the mechanism to carry out the programmer s \ndirections. To ef.ciently support the circuit simulation s access pat\u00adterns, the region all nodes holding \nall the nodes of the graph is partitioned in two different ways. The desired region tree is shown in \nFigure 1(a). First, there are subregions of T ::= types | bool | int base types | (T1, . . . , Tn) tuple \n| T @(r1, . . . , rn) pointer | coloring(r) region coloring | .r1, . . . , rn.T where O region relationship \n| .r1, . . . , rn.(T1, . . . , Tn), F, Q . Tr functions O ::= {.1, . . . , .n} region constraints . ::= \nr1 = r2 subregion | r1 * r2 disjointness F ::= {f1, . . . , fn} privileges f ::= reads(r) | writes(r) \n| reducesid(r) Q ::= {q1, . . . , qn} coherence modes q ::= atomic(r) | simult(r) v ::= values | bv | \niv base values | (v1, v2) tuple | null | l memory location | {(l, iv), . . .} coloring | ((.1, . . . \n, .n, v)) reg. relation instance bv ::= false | true iv ::= 0 | 1 . . . e ::= expressions ||||||||||||||||| \nbv | iv constants (e1, . . . , en) | e.1 | e.2 | . . . tuple id new T @r | null T @r | isnull(e) upregion(e, \nr1, . . . , rn) downregion(e, r1, . . . , rn) read(e1) | write(e1, e2) memory access reduce(id, e1, e2) \nnewcolor r | color(e1, e2, e3) coloring e1 + e2 integer ops e1 < e2 comparisons let id : T = e1in e2 \nif e1 then e2 else e3 id[r1, . . . , rn](e1, . . . , en) function calls partition rp using e1 as r1, \n. . . , rn in e2 pack e1 as T [r1, . . . , rn] unpack e1 as id : T [r1, . . . , rn] in e2  Figure 2. \nCore Legion all nodes that describe the set of nodes owned by each piece, called rn0, rn1, . . .. Since \neach node is in one piece, this partition is disjoint, which is indicated by a * on the left subtree. \nFigure 1(b) shows one possible partitioning along with the necessary coloring to generate the disjoint \npartition in Figure 1(a). Second, each piece of the circuit needs ac\u00adcess to the ghost nodes on its border. \nThe ghost nodes for two circuit pieces are shown in Figures 1(c) and 1(d); note that two nodes are in \nboth sets. Because a node may neigh\u00adbor more than one other circuit piece, this second partition of all \nnodes is aliased. Thus, there are two sources of alias\u00ading in the region tree: the two distinct partitions \ndivide the all nodes region in different ways, and the ghost node sub\u00adregions are not disjoint. There \nare two alternative approaches to using multiple partitions for the circuit simulation, both of which \navoid introducing aliasing. We could create a single partition with 2n subregions, one for each possible \ncase of sharing, or computations on each piece could use the all nodes region to access their ghost nodes. \nNeither option is attractive: the former signi.cantly complicates programming and the latter greatly \noverestimates the required ghost nodes, increasing runtime data movement as well as limiting parallelism. \nFor simplicity this example has only one level of parti\u00adtioning (although in two different ways). All \nthe semantic issues that concern the results of this paper can be illustrated with one level of partitioning. \nIn general, however, the region tree can have many levels, as subregions are themselves par\u00adtitioned, \nperhaps also in multiple ways. Typically, the num\u00adber of levels and size of partitions depends on both \nthe data and the memory hierarchy of the target machine, allowing regions to be placed in levels of memory \nwhere they .t [2]. Because data is partitioned dynamically in arbitrary ways and because these partitions \nmay not be disjoint, parallelism is necessarily detected dynamically in Legion. Functions that the Legion \nruntime considers for parallel execution are called tasks. Tasks are required to specify the regions \nthat they access as well as the task s privileges and coherence modes on each region; the type system \nintroduced in Sec\u00adtion 4 veri.es that Legion tasks abide by their declared re\u00adgion access privileges. \nThe partitioning of the data, task re\u00adgion privileges, and task coherence modes all contribute to determining \nwhich tasks can be executed in parallel. The Legion task scheduler considers task calls in sequen\u00adtial \nprogram execution order. If a task s region accesses do not con.ict with a previously issued task, the \ntask can be launched as a parallel task, otherwise it is serialized with all of its con.icting tasks. \nOne of our main results is a suf.cient condition for deciding that two tasks do not interfere on their \nregion arguments and can be executed in parallel (Section 7). Subtasks may also be launched within tasks, \ngiving nested parallelism. A second result allows even the scheduling de\u00adcisions to be made in parallel, \nso that scheduling does not become a serial bottleneck (Section 8). 3. Core Legion In this paper, we \nwork with Core Legion, a subset of the full Legion language introduced in [2]. Although equally expressive, \nCore Legion trades programmer convenience for a reduction in the number of constructs, simplifying the \nproofs that follow. We illustrate Core Legion programming through snippets from the circuit simulation. \nThe full Core Legion program for the circuit simulation is in Appendix A. (Line numbers in the code snippets \ncan be used to locate them in the full program.) Figure 2 de.nes Core Legion syntax. The basic types \ninclude booleans, integers, tuples, and pointers. In addition to specifying the type of the value they \npoint to, pointer types in Legion are annotated with one or more logical regions; any non-null pointer \nvalue must point to a location that is contained in at least one of the regions. Pointers are created \nby using the new expression to allocate space within a speci.ed region and may be tested for validity \nwith the isnull expression. To help address the proliferation of types that vary only in their regions, \nthe Core Legion compiler supports type dec\u00adlarations parameterized on logical region names, which are \nexpanded into the syntax of Figure 2 before any analysis is performed. For clarity and conciseness we \npresent the ex\u00adamples using parameterized types, but we omit the transla\u00adtion step to monomorphic Core \nLegion types, which is com\u00adpletely standard. The following code snippet shows the types used to de\u00adscribe \nthe nodes and wires in a circuit. The CircuitWire is parameterized on two regions, with rn intended to \nbe the region of nodes owned by a piece of the circuit, and rg the region of that piece s ghost nodes. \nAn edge has two node endpoints, one of which is in the piece and the other which may be either in the \npiece or a ghost node i.e., the edge is either entirely within the piece or crosses a boundary into another \npiece. 1 -- (voltage,current,charge,capacitance,piece ID) 2 type CircuitNode = (int,int,int,int,int) \n3 -- ( owned node, owned or ghost node, resistance, current) 4 type CircuitWire(rn,rg) = (CircuitNode@rn, \nCircuitNode@(rn,rg),int,int) Core Legion is an expression language, using let expres\u00ad sions to de.ne \nlocal variables. Pointers are manipulated us\u00ading explicit read, write, and reduce expressions as shown \nhere: 95 -- update voltage on a node 96 let node : CircuitNode = read(node ptr) in 97 let voltage : \nint = (node.3 / node.4) in 98 let new node : CircuitNode = ( voltage, node.2, node.3, node.4, node.5 \n) in 99 write(node ptr, new node) As described in Section 2, deciding how to partition re\u00adgions is left \nto the application. In the circuit simulation we use METIS[14], a standard graph-partitioning library. \nBe\u00adcause we need a way to iterate over all the nodes and wires, we de.ne (parameterized) types for lists \nof nodes and wires and then give a prototype for the actual METIS function: 6 type NodeList(rl,rn) = \n( CircuitNode@rn, NodeList(rl,rn)@rl ) 7 type WireList(rl,rw,rn,rg)= ( CircuitWire(rn,rg)@rw, WireList(rl,rw,rn,rg)@rl \n) 8 function extern metis[rl,rn,rw](node list : NodeList(rl,rn)@rl, 9 wires list : WireList(rl,rw,rn,rn)@rl), \nreads(rl,rn,rw), writes(rn) : bool METIS records how the graph is to be partitioned by annotating each \nCircuitNode with a piece ID. Note that both list types use a second region parameter to allow the spine \nof the list to be in a region different than the region where the nodes or wires themselves are placed. \nThere are no global region names in Legion, so functions must be region\u00adpolymorphic, with all region \nnames used in the function s prototype being implicitly universally quanti.ed. In addition to giving \nnames and types of formal parameters and the type of the return value, a Legion function also declares \nthe necessary access privileges. In this case, all three regions are read by extern metis, but only the \nrn region is written (since it contains the piece IDs). A function can be called only if the caller possesses \nall the privileges needed by the called function. Once an application has decided how it wants to parti\u00adtion \na region, that information must be provided to the Le\u00adgion runtime. This is achieved through the use \nof an ob\u00adject of a special coloring type, which maps locations within a speci.ed region to colors . (Core \nLegion uses integers for colors.) A coloring is created by the newcolor expres\u00adsion, and the mapping \nis updated by the color expression. The following code snippet shows how the coloring for the owned nodes \npartition is generated. Similar code for the ghost nodes partition and wires can be found in Appendix \nA. The full Legion language includes a multicoloring type to conveniently describe aliased partitions. \nIn Core Legion, a multicoloring and the corresponding partitioning operation are implemented by performing \na separate coloring and par\u00adtition for each aliased subregion, which soundly captures the aliased nature \nof a multicoloring. 109 function owned node coloring[rl,rn] ( node list: NodeList(rl,rn)@rl ), 110 reads(rl,rn) \n: coloring(rn) = 111 if isnull(node list) then 112 newcolor rn 113 else -- tuple .elds accessed by .(.eld \nnumber) 114 let list elem : NodeList(rl,rn) = read(node list) in 115 let part coloring : coloring(rn) \n= owned node coloring[rl,rn](list elem.2) in 116 let node ptr : CircuitNode@rn = list elem.1 in 117 let \nnode : CircuitNode = read(node ptr) in 118 let piece id from metis: int = node.5 in 119 color(part coloring, \nnode ptr, piece id from metis) Once a coloring has been created, it may be used in a partition expression, \nwhich gives local names to the subregions corresponding to each color used. 27 let owned node map : coloring(rn) \n= owned node coloring[rl,rn](all nodes) in 33 partition rn using owned node map as rn0,rn1 in At run \ntime, the partition operation extends the region tree (recall Figure 1(a)) maintained by the Legion runtime; \nthis data structure, which includes all the allocated dynamic re\u00adgions and their parent-child relationships, \nis used to decide whether computations can run in parallel based on what re\u00adgions they access and with \nwhat privileges[2]. At compile time, the partition operation introduces constraints into the static type \nenvironment describing both the disjointness of subregions (e.g., rn0 * rn1) and the subregion relation\u00adships \n(e.g., rn0 = all nodes). Because subregions are entirely included in the orig\u00adinal parent region, there \nis a subtyping-like relationship between a pointer-into-a-subregion and a pointer-into-the\u00adparent-region. \nHowever, Core Legion provides no automatic conversions between pointer types. A pointer into a subre\u00adgion \nmay be upcast to a pointer to a parent region via the explicit upregion expression, which statically \nveri.es the subregion relationship. The corresponding downcast is available via the downregion expression, \nwhich must perform a run-time check that the pointer does point into the speci.ed subregion. (If it does \nnot, the pointer value is replaced by null, which is de.ned to exist in all regions.) Regions are .rst-class \nentities and may be stored in the heap. This feature is important in many applications; for ex\u00adample, \nin a simple work list algorithm the work list may be a queue of regions to be processed. When a region \nis stored into the heap, however, it escapes the scope of the enclosing partition expressions and the \nregion s relationships to other regions (whether it is a subregion or disjoint from another re\u00adgion) \nare forgotten. To allow these facts to be retained across heap reads and writes of values containing \nregions, Core Legion has region relationships. A region relationship is a bounded existential type that \nallows a programmer to pack one or more regions, a value (whose type may include those regions), and \nsubregion or disjointness constraints, together. The example region relationship below for a CircuitPiece \ninvolves its region of wires rpw, region of nodes rpn, region of ghost nodes rg, and important constraints. \nNote that the names of rpw, rpn, and rg are bound in the region rela\u00adtionship, and the knowledge that \nthey are subregions of free region names rw and rn is captured in the constraints. 11 12 13 type CircuitPiece(rl,rw,rn) \n= rr[rpw,rpn,rg] ( WireList(rl,rpw,rpn,rg)@rl, NodeList(rl,rpn)@rl )where rpn = rn and rg = rn and rpw \n= rw and 14 rn * rw and rl * rn and rl * rw The Core Legion type system statically veri.es the cor\u00ad \nrectness of region relationships as part of a pack expression. The regions and constraints bound in a \nregion relationship can be reintroduced (with fresh names) within the body of an unpack expression. In \nthe circuit simulation given in Ap\u00adpendix A, region relationships are mostly a convenience, al\u00adlowing \nthe programmer to give a name to a collection of regions and constraints that results in simpler function \nin\u00adterfaces. However, in a version of the circuit simulation that partitions the graph into many more \nthan two pieces having a data structure that stores all the pieces with their associated ghost regions \nis essential. In contrast to disjointness and subregion constraints, re\u00adgion access privileges cannot \nbe captured in a region rela\u00adtionship. A function inherits a subset of the privileges of its caller, \nand thus privileges belong to functions. This is a key requirement for soundness of the Legion type system \nthat we return to in Section 4. When a function unpacks a re\u00adgion r from a region relationship, no privileges \nfor r itself are granted. To access r, the function must already hold the needed privileges on some region \nq that is a superset of r (i.e., q is r s parent or another ancestor region), and further\u00ad 1 -- Leaf \nTask Declarations (implementations in appendix) 2 function calc new currents[rl,rw,rn,rg] ( ptr list \n: WireList(rl,rw,rn,rg)@rl ), 3 reads(rl,rw,rn,rg), writes(rw) : bool 4 function distribute charge[rl,rw,rn,rg] \n( ptr list : WireList(rl,rw,rn,rg)@rl ), 5 reads(rl,rw,rn), reduces(reduce charge,rn,rg), atomic(rn,rg) \n: bool 6 function update voltage[rl,rn] ( ptr list : NodeList(rl,rn)@rl ), 7 reads(rl,rn), writes(rn) \n: bool 8 9 -- Reduction function for distribute charge 10 function reduce charge ( node : CircuitNode, \ncurrent : int ) : CircuitNode 11 let new charge : int = node.3 + current in 12 ( node.1,node.2,new charge,node.4) \n13 14 -- Time Step Loop 15 function execute time steps[rl,rw,rn] ( p0 : CircuitPiece(rl,rw,rn), 16 p1 \n: CircuitPiece(rl,rw,rn), steps : int ) , reads(rn,rw,rl), writes(rn,rw) : bool = 17 if steps < 1 then \ntrue else 18 unpack p0 as piece0 : CircuitPiece(rl,rw,rn)[rw0,rn0,rg0] in 19 unpack p1 as piece1 : CircuitPiece(rl,rw,rn)[rw1,rn1,rg1] \nin 20 let : bool = calc new currents[rl,rw0,rn0,rg0](piece0.1) in 21 let : bool = calc new currents[rl,rw1,rn1,rg1](piece1.1) \nin 22 let : bool = distribute charge[rl,rw0,rn0,rg0](piece0.1) in 23 let : bool = distribute charge[rl,rw1,rn1,rg1](piece1.1) \nin 24 let : bool = update voltage[rl,rn0](piece0.2) in 25 let : bool = update voltage[rl,rn1](piece1.2) \nin 26 execute time steps[rl,rw,rn](p0,p1,steps-1) Listing 1. Main Simulation Loop more there must be \nconstraints in the region relationship that prove r = q. The main simulation loop, shown in Listing 1, \nruns for many time steps, each of which performs three com\u00adputations: calculate new currents, distribute \ncharges, and update voltages on the circuit. For simplicity, this exam\u00adple is written for a graph that \nis partitioned into only two pieces. For each time step, the loop (tail recursive func\u00adtion execute time \nsteps, lines 15-26) unpacks the two previously packed circuit pieces, giving new names to the subregions \nintroduced by each region relationship. The execute time steps function will have read/write priv\u00adileges \nfor the newly named regions, such as rn0, because it has read/write privileges for rn and the CircuitPiece \nregion relationship ensures that rn0 = rn. The execute time steps function illustrates the im\u00adportance \nof having different partitions provide multiple views onto the same logical region. The calc new currents \nfunction uses the owned and ghost regions of a piece, which are from different partitions; no single \npartition of the nodes describes this access pattern. In calc new currents these regions only need read \nprivileges, while the only writes are performed to the wires subregion belonging to that piece. Thus, \nboth instances of calc new currents can be run as parallel tasks. Similarly, the update voltage function \n(lines 6-7) modi.es only the disjoint owned regions, while only reading from regions shared with the \nother instance; the two instances of update voltage can also run in parallel. The most interesting function \nis distribute charge (lines 2-5), which uses a reduction privilege for regions rn and rg. A reduction \nnames the reduction operator (which is assumed to be associative and commutative) as the .rst com\u00adponent \nof the privilege. Programmers can write their own re\u00adduction operators, such as the function reduce charge \nin Listing 1. Reductions allow updates to the named regions O . O * ri = rj . O * . ri = ri . O * . \nrj = rj . O * ri = rj . O * . rj = rk . O * . ri = rk . O * ri = rj . O * . rj * rk . O * . ri * rk . \nO * ri * rj . O * . rj * ri . O * F . F * ri = rj . O * . reads(rj ) . F * . reads(ri) . F * ri = rj \n. O * . writes(rj ) . F * . writes(ri) . F * ri = rj . O * . reducesid(rj ) . F * . reducesid(ri) . F \n* reads(r) . F * . writes(r) . F * . reducesid(r) . F * for every function identi.er id Figure 3. Privilege \nand Constraint Closure that are performed with the named reduction operator to be reordered. For example, \nreductions can be performed locally by a task and only the .nal results folded in to the desti\u00adnation \nregion. However, by default, functions with no co\u00adherence annotation have exclusive coherence for their \nregion arguments: reads and writes have the results expected as if the original sequential execution \norder of the program was preserved, unaffected by any concurrently executing tasks. Thus, to fully exploit \nreductions it is important to use a re\u00adlaxed coherence mode, in this case atomic coherence, which permits \nother tasks performing the same reduction operation on the named regions to execute in parallel. The \nmost relaxed coherence mode is simult; simultaneous coherence allows concurrent access to the region \nby all functions that are using the region in a simultaneous mode. The interaction between tasks using \nthe same region with different coherence modes is formalized in Section 7. While associative and commu\u00adtative \nreductions always produce the same result regardless of execution order, in general relaxed coherence \nmodes in\u00adtroduce non-determinism into Legion programs. This non\u00addeterminism is completely under programmer \ncontrol, at the per-region (or subregion) granularity. 4. Type System Core Legion is explicitly typed \nusing judgments of the form G, F, O f e : T Besides a type environment G, type judgments include the \naccess privileges F for the logical regions in the expression e as well as constraints O that must hold \nbetween those logical regions. A representative selection of the type rules is given in Figure 4. Both \nF and O are used in the heap access expres\u00adsions read, write, and reduce. A valid heap access has the \nneeded permission for logical region(s) in the pointer s type. Note the exact region need not be named \nin F if per\u00admissions exist for logical regions that provably contain the pointer s region(s). To simplify \nthis check, Figure 3 de.nes closure operations O * (all constraints implied by O) and F * (all privileges \nimplied by F and O *). Region constraints are introduced into O by the partition expression. The type \nsystem constrains the coloring used in a partitioning to only include pointers into the region be\u00ading \npartitioned. In Core Legion, the subregions that result from a single partition expression are always \ndisjoint. (As a reminder, the aliased subregions that result from a multicol\u00adoring are obtained in Core \nLegion through multiple nested partition expressions.) The pack expression requires the programmer to \nexplic\u00aditly name which regions are expected to satisfy the con\u00adstraints of the region relationship s \ntype. The programmer also provides the new names for regions that result from an unpack, with the constraint \nthat fresh names are chosen. Finally, the type checking rule for the overall Legion pro\u00adgram shows how \neach function is type-checked separately, with no global variables or region constraints. Although the \ncoherence modes (Qi) are part of a function s prototype, they in.uence only the runtime behavior, not \nthe type checking. 5. Operational Semantics Operational semantics for parallel languages are tradition\u00adally \nconstructed using small-step semantics. The state of the system includes the current state of each concurrent \ncompu\u00adtation and a small step allows one of two things to happen: either a single computation makes progress \nor a subset of the computations rendezvous on an explicit synchronization primitive (e.g., a matching \nsend and receive on a channel). Although an operational semantics for Legion can be con\u00adstructed in such \na manner, it is not natural, and certainly Legion programmers do not think about the execution of Legion \nprograms in this way. Nested parallelism (subtasks recursively launching other subtasks) and the absence \nof ex\u00adplicit synchronization constructs encourage programmers to think about programs compositionally, \nas the execution of child tasks in the context of a parent task. To formalize this view of Core Legion \nexecutions, we ex\u00adpress the operational semantics in a big-step style, which captures the hierarchical \nnature of Legion tasks. In addition to being arguably more intuitive to someone trying to un\u00adderstand \nLegion runtime behavior, the preservation of the task hierarchy in our semantics makes it considerably \neas\u00adier to prove the soundness of the type system and the safety of our hierarchical scheduling algorithms. \nFinally, a big-step semantics simpli.es the explanation of Legion s novel treat\u00adment of coherence. Core \nLegion s operational semantics rules have the form M, L, H, S, C f e . v, E and specify that the evaluation \nof expression e yields a value v. The environment includes the standard mapping L of lo\u00adcal variables \nto their values, and an immutable heap typing H assigning types to heap locations. An additional map\u00adping \nM is used to translate logical regions ri to physical re\u00adgions .i, which are sets of concrete memory \nlocations. M is G, F, O f e1 : T @(r1, . . . , rn) .i. reads(ri) . F * [T-Read] G, F, O f read(e1) : \nT G, F, O f e1 : T @(r1, . . . , rn) G, F, O f e2 : T [T-Write] .i. writes(ri) . F * G, F, O f write(e1, \ne2) : T @(r1, . . . , rn) G(id) = (T1, T2), \u00d8, \u00d8 . T1 G, F, O f e1 : T1@(r1, . . . , rn) [T-Reduce] G, \nF, O f e2 : T2 .i. reducesid(ri) . F * G, F, O f reduce(id, e1, e2) : T1@(r1, . . . , rn) G, F, O f \nnew T @r : T @r [T-New] S S G, F, O f e : T @(r1, . . . rk) [T-UpRgn] .i..j, ri S = rj . O * G, F, O \nf upregion(e, r1, . . . , rn) : T @(r1, . . . , rn) S S G, F, O f e : T @(r1, . . . rk) [T-DnRgn] G, \nF, O f downregion(e, r1, . . . , rn) : T @(r1, . . . , rn) G, F, O f newcolor r : coloring(r) [T-NewColor] \nG, F, O f e1 : coloring(r) G, F, O f e2 : T @r [T-Color] G, F, O f e3 : int G, F, O f color(e1, e2, \ne3) : coloring(r) G, F, O f e1 : coloring(rp) OS e e [T-Partition] = O . ri = rp . ri * rj i.[1,k] 1=i<j=k \nG, F, OS f e2 : T {r1, . . . , rk} n regions of (G, T ) = \u00d8 G, F, O f partition rp using e1 as r1, . \n. . , rk in e2 : T S S T1 = .r1, . . . rk. T2 where O1 O1[r1/r1 S , . . . , rk/rk S ] . O * [T-Pack] \nG, F, O f e1 : T2[r1/r1 S , . . . , rk/rk S ] G, F, O f pack e1 as T1[r1, . . . , rk] : T1 S S T1 = \n.r1, . . . , rk. T2 where O1 [T-Unpack] G, F, O f e1 : T1 GS = G[T2[r1/r1 S , . . . , rk/rk S ]/id] \nOS = O . O1[r1/r1 S , . . . , rk/rk S ] GS , F, OS f e2 : T3 {r1, . . . , rk} n regions of (G, T1, T3) \n= \u00d8 G, F, O f unpack e1 as id : T1[r1, . . . , rk] in e2 : T3 S S G(id) = .r1, . . . rk.(T1, . . . , \nTn), FS , QS . Tr [T-Call] G, F, O f ei : Ti[r1/r1 S , . . . , rk/rk S ] FS[r1/r1 S , . . . , rk/rk S \n] . F * S S G, F, O f id[r1, . . . , rk](e1, . . . , en) : Tr[r1/r1, . . . , rk/rk] for 1 = i = p, i \ni ki ni G(idi) = .r1, . . . r .(T1 i , . . . , T i ), Fi , Qi . Tr i i i ni ni Gi = G[a1/T1 i , . . . \n, a /T i ] i Gi , Fi , \u00d8 f e : Tr i [T-Program] 1 111 1 f {function id1[r1, ..., r ](a1 : T1 1 , ..., \na : T 1 ), F1 , Q1 : T 1 : e , k1 n1 n1 r . . . p pp p function idp[r1 , ..., r](a1 : T1 p , ..., a: \nT p ), Fp , Qp : T p : ep} : r kp np np SS M, L, H, S, C f e . l, E = apply(S, E) , SS(l), ifl . C [E-Read] \n v = S v : H(l), otherwise M, L, H, S, C f read(e) . v, E++[read(l, excl, v, 0)] [E-Write] SS M, L, \nH, SS , C f e2 . v, E2 valid interleave(S, C, ES , E1, E2) M, L, H, S, C f e1 . l, E1 = apply(S, E1) \nS M, L, H, S, C f write(e1, e2) . l, E ++[write(l, excl, v, 0)] [E-Reduce] SS M, L, H, SS , C f e2 \n. v, E2 valid interleave(S, C, ES , E1, E2) M, L, H, S, C f e1 . l, E1 = apply(S, E1) S M, L, H, S, \nC f reduce(id, e1, e2) . l, E ++[reduceid(l, excl, v, 0)] l . M(r) l . domain(S) H(l) = M[ T ] [E-New] \nM, L, H, S, C f new T @r . l, [] M, L, H, S, C f e . v, E [E-UpRgn] M, L, H, S, C f upregion(e, r1, \n. . . , rn) . v, E M, L, H, S, C f e . l, E , l, if .i, l . M(ri). [E-DnRgn] v = null, otherwise. M, \nL, H, S, C f downregion(e, r1, . . . , rn) . v, E K = {(l1, iv1), . . . , (lp, ivp)}, where (.i . [1, \np].li . M(r)) . (.i, j . [1, p].li = lj) [E-NewColor] M, L, H, S, C f newcolor r . K, [] M, L, H, S, \nC f e1 . K, E1 SS = apply(S, E1) M, L, H, SS , C f e2 . l, E2 SSS = apply(SS , E2) [E-Color] M, L, H, \nSSS , C f e3 . v, E3 KS = {(l, v)} . {(li, vi) : (li, vi) . K . l = li}valid interleave(S, C, ES , E1, \nE2, E3) S S M, L, H, S, C f color(e1, e2, e3) . K , E [E-Partition] M, L, H, S, C f e1 . K, E1 .i = \n{l : (l, i) . K}, for1 = i = k MS SS = M[.1/r1, . . . , .k/rk] = apply(S, E1) MS , L, H, SS , C f e2 \n. v, E2 valid interleave(S, C, ES , E1, E2) S M, L, H, S, C f partition rp using e1 as r1, . . . , rk \nin e2 . v, E M, L, H, S, C f e1 . v, E .i = M[ri], for1 = i = k S [E-Pack] v = ((.1, . . . , .k, v)) \nS M, L, H, S, C f pack e1 as T1[r1, . . . , rk] . v , E [E-Unpack] M, L, H, S, C f e1 . ((.1, . . . \n, .k, v1)), E1 MS = M[.1/r1, . . . , .k/rk] LS SS = L[v1/id] = apply(S, E1) MS , LS , H, SS , C f e2 \n. v2, E2 valid interleave(S, C, ES , E1, E2) S M, L, H, S, C f unpack e1 as id : T1[r1, . . . , rk]in \ne2 . v2, E M, L, H, S, C f e1 . v1, E1 S1 = apply(S, E1) . . . [E-Call] M, L, H, Sn-1, C f en . vn, En \nSn = apply(Sn-1, En) valid interleave(S, C, ES , E1, . . . , En) S S function id[r 1, . . . , r k](a1 \n: T1, . . . , an : Tn), FS , QS : Tr = en+1 S S MS = {(r1, M(r1)), . . . (rk, M(rk))} LS = {(a1, v1), \n. . . , (an, vn)} SS = apply(S, ES) CS = C . {l : ... l . . . (atomic(.) . MS[ QS] . simult(.) . MS[ \nQS]])} MS , LS , H, SS , CS f en+1 . vn+1, En+1 EnS +1 = mark coherence(En+1, MS[ QS] , taskid) taskid \nfresh valid interleave(S, C, ESS , ES , EnS +1) SS M, L, H, S, C f id[r1, . . . , rk](e1, . . . , en) \n. vn+1, E Figure 4. Type System and Operational Semantics extended in the standard way to map types, \nenvironments, and constraints that refer to logical regions into correspond\u00ading structures that refer \nto physical regions. For example, M[ int@r1] = int@.1. The two unusual components of the operational \nseman\u00adtics are the dynamic memory trace E and the clobber set C. As these are the key to making the Core \nLegion semantics composable, we discuss them in detail in the following two subsections. 5.1 Dynamic \nMemory Traces In a sequential big-step semantics for a language with side effects, evaluation commonly \nbegins in an initial store S and produces a value v and a .nal store SS . In our Core Legion semantics, \ninstead of a .nal store, an explicit list of all memory operations (i.e. reads, writes, reductions) is \nreturned in the form of a dynamic memory trace. When necessary, the dynamic memory trace can be used \nto regenerate the .nal store using the apply(S, E) helper function (see Figure 5). Keeping the list of \nmemory operations performed by the evaluation of an expression serves multiple purposes. First, the proof \nof soundness in Section 6 requires this list. Second, it makes it much easier to describe when and how \ncomputa\u00adtion of subexpressions may be interleaved (i.e. executed in parallel). As a simple example, consider \nthe operational se\u00admantics rule for the addition of two integers: M, L, H, S, C f e1 . v1, E1 SS = apply(S, \nE1) [E-Add] M, L, H, SS , C f e2 . v2, E2 S v = v1 + v2 ES = valid interleave(S, C, ES , E1, E2) M, \nL, H, S, C f e1 + e2 . v S , ES In this rule, the subexpressions e1 and e2 are evaluated, producing memory \ntraces E1 and E2. Our compositional semantics return a single memory trace ES for the parent expression \nby interleaving the individual operations from E1 and E2 according to certain constraints captured in \nthe valid interleave predicate, de.ned in Figure 7. A full expla\u00adnation of these constraints is deferred \nto Section 7, but we describe the four most common cases here: If e1 and e2 access the same region(s) \nwith exclusive co\u00adherence and there are no concurrently executing expres\u00adsions that may modify the locations \naccessed by e1 and e2 (i.e. the locations are not in the clobber set C, see Sec\u00adtion 5.2), then e1 and \ne2 execute in sequential program order, so ES = E1++E2, where ++ is sequence concate\u00adnation.  If e1 \nand e2 include task calls that access the same re\u00adgion(s) with atomic coherence (and there are no concur\u00adrent \nexecutions accessing the same locations), each of e1 and e2 must execute atomically, but the ordering \nof the two executions is not constrained. In this case, ES may  be E1++E2 or E 2++E 1. (E 2 and E 1 \nare used in the sec\u00adond case to emphasize that the actual memory traces are likely to be different depending \non which of e1 or e2 is executed .rst.) If e1 and e2 require exclusive (or atomic) coherence, but access \ndisjoint sets of heap locations, they are non\u00adinterfering computations and may be performed in paral\u00adlel \nwhile still giving the appearance of sequential execu\u00adtion. In this case, ES can be an arbitrary interleaving \nof the memory operations in E1 and E2.  Finally, if e1 and e2 include task calls that access the same \nregion(s) with simultaneous coherence, parallel computa\u00adtion of the subexpressions has been explicitly \nallowed by the programmer. The two computations may access the same locations and see the results of \nthe other s writes and reductions. The resulting memory trace ES will be an interleaving of E 1 and E \n2. (Again, E 1 and E 2 are used instead of E1 to emphasize that the traces are likely to be different \ndue to the interactions through the heap.)   5.2 Clobber Sets As alluded to in the previous discussion, \na composable paral\u00ad lel semantics must account for the unknown, concurrent con\u00ad text in which an expression \nexecutes. In particular, there may be locations read by an expression that are being altered (i.e. clobbered \n) by other concurrently executing expressions. The set of such locations for a given expression is called \nthe clobber set C. When a read is performed to a location that falls in C, the operational semantics \nleave the result of the read unconstrained. Instead, the check that the value of the read is consistent \nwith the preceding writes (or reductions) is deferred to the .rst parent expression that encloses all \nof the computations that may be accessing the same locations. To give a concrete example of how dynamic \nmemory traces and clobber sets work, consider the following Core Legion tasks: 1 function A[r](i : int@r), \nreads(r), writes(r) : int = 2 (B[r](i, 1) + B[r](i, 2)) + B[r](i, 3) 3 4 function B[r](i : int@r, v : \nint), reads(r), writes(r), atomic(r): int= 5 let x: int = read(i) in 6 let : int@r = write(i, v) in 7 \nx There is a single region r in which a single integer has been allocated at location l (and given an \ninitial value of 0), which is stored in pointer variable i. Function A requests exclusive access to r, \nand will return the sum of three calls to function B. Each call to function B performs an exchange on \nthe memory location l, storing the value passed in as an argument and returning the original contents. \nFunction B requests atomic coherence on region r allowing the three sibling task calls to B in A to execute \nin any order while guaranteeing that the individual exchanges are performed atomically. The scope of \na coherence mode on a region for a task call t is always the sibling task calls of t within the  mark \ncoherence([], Q, taskid) = [] apply(S, []) = S = [op(l, cS , v, taskid)]++mark coherence(E, Q ), +E, \nQ, taskid) apply(S, E++[read(l, c, v, t)]) = apply(S, E) simult, if ...l . . . simult(.) . Q apply(S, \nE++[write(l, c, v, t)]) = apply(S, E)[v/l] S apply(S, E++[reduceid(l, c, v, t)]) = SS[id(SS(l), v)/l], \nwhere c atomic, if ...l . . . atomic(.) . Q = mark coherence([op(l, c, v, t)]+ . . . . . where SS = \napply(S, E) excl, otherwise Figure 5. Helper Functions for Type Rules and Operational Semantics parent \ntask. The atomic coherence on region r affects the order of memory operations of the three calls to B \nwithin A, but not, for example, the interleaving of A with a sibling task, which is determined by A s \nexclusive coherence for r. One valid execution for a call to A[r](i) in a parent task would result in: \nM, L, H, S, C f A[r](i) . 5, ES where: M = [ r : {l} ] L = [ i : l] H = [ l : int ] S = [ l . 0 ] C = \n\u00d8 ES = [ read(l, excl, 0, A), write(l, excl, 2, A), read(l, excl, 2, A), write(l, excl, 3, A), read(l, \nexcl, 3, A), write(l, excl, 1, A) ] Recall that a memory trace records the sequence of mem\u00adory operations \nperformed by a task (and all of its subtasks). Each memory operation includes .ve pieces of information: \nthe type of operation (read, write, or reduceid with reduction operator id), the location affected, the \ncoherence mode, the value that is read, written, or reduced (combined with the value already in the memory \nlocation by the reduction op\u00aderator), and the unique identi.er of the task performing the operation. \nHere the call B[r](i, 2) (referred to as B2 below) has executed .rst (reading the initial value 0 of \ni in the store), followed by B[r](i, 3) (B3 below) and .nally B[r](i, 1) (B1). Note that the memory trace \nis coherent with respect to i: each read of i returns the value of the previous write of i or the initial \nvalue of i when there is no previous write. All the memory operations are marked with A s task id and \nwith exclusive coherence, because this is the mode in which A accesses r. The fact that the accesses \noccurred in different subtasks of A (and with different coherence modes) is not visible outside of A. \nTo show how ES was obtained, we will follow the expres\u00adsion hierarchy, beginning at the leaf tasks: B1 \n: M, [i : l, v : 1], H, SB1 , {l} f let x . . . . 3, EB1 B2 : M, [i : l, v : 2], H, SB2 , {l} f let x \n. . . . 0, EB2 B3 : M, [i : l, v : 3], H, SB3 , {l} f let x . . . . 2, EB3 where: EB1 = [ read(l, excl, \n3, 0), write(l, excl, 1, 0) ] EB2 = [ read(l, excl, 0, 0), write(l, excl, 2, 0) ] EB3 = [ read(l, excl, \n2, 0), write(l, excl, 3, 0) ] There are several important points to note here. First, each subtask s \nevaluation includes location l in the clobber set. Because these tasks access region r with atomic coherence, \nall locations in r (i.e. l) are added to the clobber set CS in the [E-Call] rule. This allows the read \noperations performed by the subtasks to return a value other than what is contained in the initial stores \nSB1 , SB2 , and SB3 and allows the resulting dynamic memory traces to be non-coherent with respect to \nthose initial stores. (Note that the stores are only used for the sequential portion of the semantics, \nwhich is the sequence of memory operations on locations with exclusive access that are not also in the \nclobber set. Thus, the stores are threaded through the rules in the usual sequential manner and used \nfor operations on locations that can t be concurrently accessed.) Finally, although these tasks requested \natomic coherence on region r, the memory operations within the task are marked with with the excl coherence \nmode, allowing proper ordering of the operations within each individual atomic subtask. We next consider \nthe function call expressions within the body of function A. M, L, H, SB1 , \u00d8 f B[r](i, 1) . 3, ES B1 \nM, L, H, SB2 , \u00d8 f B[r](i, 2) . 0, ES B2 M, L, H, SB3 , \u00d8 f B[r](i, 3) . 2, ES B3 where: ES = [ read(l, \natomic, 3, B1), write(l, atomic, 1, B1) ] B1 ES = [ read(l, atomic, 0, B2), write(l, atomic, 2, B2) ] \nB2 ES = [ read(l, atomic, 2, B3), write(l, atomic, 3, B3) ] B3 Here we see the result of using the mark \ncoherence helper function (de.ned in Figure 5) to annotate the dynamic mem\u00adory traces of function calls \nwith their coherence modes and unique task id. The next step is to perform the inner addition: M, L, \nH, S, \u00d8 f B[r](i, 1) + B[r](i, 2) . 3, Eint where: Eint = [ read(l, atomic, 0, B2), write(l, atomic, \n2, B2) read(l, atomic, 3, B1), write(l, atomic, 1, B1)] Because all accesses to location l are performed \nwith atomic coherence, either of ES ++ES or ES ++ES is B1 B2 B2 B1 permitted, and we have chosen the \nlatter for our intermediate trace Eint. Note that this trace is not coherent (in particular, the second \nread of l does not return what was written by the previous write). Only sequential consistency of each \nsubtask s accesses is required at this point. The evaluation of the body of A is completed by perform\u00ading \nthe outer addition: M, L, H, S, \u00d8 f (. . .) + B[r](i, 3) . 5, E where: E = [ read(l, atomic, 0, B2), \nwrite(l, atomic, 2, B2) read(l, atomic, 2, B3), write(l, atomic, 3, B3), read(l, atomic, 3, B1), write(l, \natomic, 1, B1) ] The requirements of the valid interleave predicate allow for three possible interleavings \nof Eint and ES , and we B3 have chosen the one that inserts ES in the middle of Eint. B3 The .nal value \nof ES above is attained by applying the mark coherence helper function to E, replacing the task ids and \ncoherence modes of A s subtasks with those of A itself. Now that the accesses to location l are marked \nas excl rather than atomic and l is not in the clobber set, the trace is required to be coherent with \nrespect to l, and this is the point at which any traces with inconsistencies between the choices of values \nread from location l in the calls to function B and the dynamic memory trace interleavings chosen in \nA are disallowed.  5.3 Operational Semantics Rules In addition to the novel construction and interleaving \nof memory traces and clobber sets discussed above, the Core Legion operational semantics include rules \nfor the new con\u00adstructs introduced in the language. These rules are also shown in Figure 4. The new expression \nselects a location that is not currently in use and that also has the correct heap typing from the set \nof locations assigned to the logical region argument. Similarly, downregion checks whether a location \nis within the set assigned to the logical region. If this dynamic check fails, null is returned. The \napplication can use the isnull expression to test for this case and handle it appropriately. As discussed \nabove, the correctness of upregion expressions is checked statically there is no runtime component. The \ncolor expression creates a copy of the input color\u00ading in which the speci.ed location is modi.ed to have \nthe speci.ed color. The behavior of newcolor is subtler. The operational semantics for new requires that \nthe newly allo\u00adcated location already be present in the designated region. To allow allocations to be \nperformed in subregions, addi\u00adtional, unused memory locations are assigned to each sub\u00adregion when it \nis created. Because subregions are created by partitioning an existing region using a coloring, it is \nsim\u00adplest to have newcolor put these extra locations in the initial coloring. Adding extra locations \nto a region cannot cause a computation to fail or alter its output, but it does admit ex\u00adecutions in \nwhich some memory locations are assigned to a region but are never used (never allocated by new). This \nse\u00admantics re.ects the behavior of our implementation, which also preallocates extra space in regions \nthat may never be used, because adding space to a region on a call to new re\u00adquires additional synchronization \nwith users of that region and any containing regions to ensure all agree on the pres\u00adence of the new \nlocation. It is much cheaper to simply add some extra locations when there is only a single user of the \nregion, namely at the point where the region is created. Because the necessary checks are performed \nat compile time, the operational semantics for the pack and unpack expressions are simple. A pack expression \njust uses M to map logical regions to physical regions, while unpack aug\u00adments M with the new logical \nregion names assigned to the physical regions stored in the region relationship. 6. Soundness of Privileges \nOur .rst result shows that a well-typed expression accesses the heap in ways consistent with its static \nprivileges. A judgment E :M F holds if memory operations in memory trace E have types and locations covered \nby privileges F: E :M F . .E . E. (E = read(l, c, v, t) . .r, l . M(r) . reads(r) . F) . (E = write(l, \nc, v, t) . .r, l . M (r) . writes(r) . F) . (E = reduceid(l, c, v, t) . .r, l . M(r) . reducesid(r) . \nF) As usual, the soundness claim is proven assuming the initial type and execution environments are \nconsistent. For our results, three consistency properties are needed: mapping consistency, written M \n~ O, guarantees a re\u00adgion mapping M satis.es the region constraints O  local value consistency, written \nL ~H M[[G]], guarantees local values in L have types consistent with the environ\u00adment G (using M to map \nlogical regions in G to physical regions)  store consistency, written S ~ H, guarantees locations in \nS have values consistent with heap typing H  Two additional properties are proven for each subexpression: \nresult value consistency, written v ~H M[ T ] , guarantees any evaluation of an expression yields a value \nof the right type  memory trace consistency, written E ~ H, guarantees that all writes and reductions \nuse values of the right types  Figure 6 de.nes these properties. Theorem 1. If G, F, O f e : T and \nM, L, H, S, C f e . v, E and M ~ O, L ~H M[[G]] and S ~ H, then v ~H M[ T ] , E ~ H and E :M F. We spare \nthe reader the lengthy proof, which can be found in [19], and merely outline the general strategy, which \nmakes use of a standard induction on the structure of the derivation. For each of the Core Legion expressions, \nwe show that the consistency of the expression s initial execution environment (i.e. mapping, local value, \nand store consistency) guarantees M ~ O . (.ri, rj .ri = rj . O . M(ri) . M(rj )) . E ~ H (.ri, rj \n.ri * rj . O . M(ri) n M(rj ) = \u00d8) L ~H M[[G]] . .(id, v) . L.v ~H M[[G]](id) S ~ H . .(l, v) . S.v ~H \nH(l)  l ~H T @. . bv ~H bool (v1, v2) ~H (T 1, T 2) . iv ~H int ((.1, . . . , .n, v)) ~H rr[r1, . . \n. , rn] T where O .  null ~H T @. K ~H coloring(.) . . (.l, c, v.write(l, c, v, t) . E . v ~H H(l)) \n. (.id, l, v.reduceid(l, c, v, t) . E . (M[[G]](id) = (T 1, T 2), \u00d8, \u00d8 . T 1) . H(l) = T 1 . v ~H T 2) \nl . . . H(l) = T (v1 ~H T1) . (v2 ~H T2) (v ~H T [.1/r1, . . . .n/rn]) . ({(ri, .i)} ~ O) .l1, v1.(l1, \nv1) . K . (l1 . . . .l2, v2.(l2, v2) . K . (l1 = v2)) = l2) . (v1 Figure 6. Consistency Properties \na consistent environment for subexpressions, and the con\u00adsistency of subexpressions results (i.e. result \nvalue consis\u00adtency, memory trace consistency, and containment of heap accesses) results in similar consistency \nfor the enclosing ex\u00adpression s results. Many of the cases are similar, and bene.t from the use of the \nfollowing lemmas (proofs of which can also be found in [19] ). As discussed earlier, apply(S,E), de\u00ad.ned \nin Figure 5, applies the operations in an execution trace E to a store S, the operator ++ is sequence \nconcatenation, and the valid interleave predicate is de.ned in Figure 7. Lemma 1. If S ~ H and E ~ H, \nthen apply(S, E) ~ H. Lemma 2. If E1 ~ H and E2 ~ H, then E1++E2 ~ H. Lemma 3. If E1 ~ H and E2 ~ H and \nvalid interleave(S, C, ES , E1, E2), then ES ~ H. Lemma 4. If E1 :M F and E2 :M F, then E1++E2 :M F. \nLemma 5. If E1 :M F and E2 :M F and valid interleave(S, C, ES , E1, E2), then ES :M F. Lemma 6. M ~ O \n* if and only if M ~ O. Lemma 7. E :M F * if and only if E :M F. Lemma 8. M ~ O if and only if \u00d8 ~ M[[O]]. \nThe interesting cases for each property are summarized here: M ~ O -Three expressions have subexpressions \nthat mod\u00adify M or O and therefore do not trivially satisfy re\u00adgion mapping consistency. For partition, \nthe con\u00adsistency of the coloring preserves region mapping con\u00adsistency with respect to the constraints. \nFor unpack, the consistency of a region relation instance guaran\u00adtees consistency of region mapping. \nFinally, the body of a called function uses an initially-empty set of con\u00adstraints, which are trivially \nsatis.ed. L ~H M[[G]] -Four expressions have subexpressions that modify L, G, or M. For partition, which \nonly modi\u00ad.es M, the requirement that it not reuse existing names ensures that M[[G]] does not change. \nFor let, the value and type of the binding is obviously consistent, while the binding created in an unpack \nis less obviously so, requiring an induction over the type of the unpacked value to show equivalence \nunder the new mapping. The last case is the body of a called function, which requires the same style \nof proof as for unpack for each formal parameter. S ~ H -The heap typing consistency of all stores used \nin subexpressions follows directly from Lemma 1. v ~H M[ T ] -The consistency of upregion is guaranteed \nby the type checking requirement of appropriate sub\u00adregion constraints and the mapping s consistency \nwith those constraints, and downregion s result is consis\u00adtent because of the runtime check. The consistency \nof a read s result is trivial for an address in the clob\u00adber set and uses the consistency of the store \notherwise. The consistency of a color s result depends on the pointer subexpression s consistency and \nthe removal of any previous coloring of that location from the col\u00adoring set. The remaining interesting \ncases arise from changes to the mapping M rather than transformations on the value v. In the case of \npartition and unpack, the type system guarantees that the subexpression s result cannot use the regions \nthat were added to the mapping, allowing the changes to the mapping to be ignored. The last case is again \nthe body of a called function, and the same strategy that was used for the type con\u00adsistency of the formal \nparameters works in reverse for the function s result. E ~ H -The type consistency of the values in an \nex\u00adpression s memory trace follows from Lemma 2 and Lemma 3. New memory operations are added by write \nand reduce expressions, but consistency fol\u00adlows directly from the induction hypothesis. Finally, the \nconsistency of the values in a called function s memory trace is addressed in the same way as the return \nvalue. E :M F -The proof of the crucial property of containment of heap accesses within the available \nprivileges is similar in outline to the previous step. The easy cases are covered by Lemma 3. Straightforward \nproofs cover read, write, reduce, with one .nal special case for function calls. 7. Coherence In our \ncompositional operational semantics, the execution of an expression assumes any concurrent environment \nand there may be many possible execution traces for a given expression. When the semantics of multiple \nsubexpressions are combined in the operational semantics rules, we can restrict the set of execution \ntraces to those that are consistent with the joint behavior of the subexpressions under the given region \ncoherence requirements. Interestingly, however, an insight from the proof of The\u00adorem 1 is that it does \nnot rely on the full de.nition of valid interleave. In fact, soundness of privileges is pre\u00adserved even \nif the valid interleave test is replaced with any interleave (Figure 7), which allows arbitrary interleav\u00adings \nof memory traces from subexpressions. The stronger constraints in valid interleave address the coherence \nof heap accesses, specifying permitted interleavings of memory op\u00aderations for the particular coherence \nmodes on logical re\u00adgions. To determine whether an interleaving of two or more memory traces is valid, \nwe consider three sets of addresses: exclusive locations (l . Lexcl) are those which have at least one \naccess in exclusive mode in the traces and are not in the clobber set. For these locations, we require \nsequential execution semantics all reads to these loca\u00adtions see the effect of previous writes and reductions, \nand the resulting state of the store is as if all writes and reduc\u00adtions were applied from each trace \nin order.  atomic locations (l . Latomic) are those which have at least one access in atomic mode in \nthe traces and are in neither Lexcl nor the clobber set. For these locations, we allow permutations of \nthe original subexpression trace order.  for locations with only access in simult mode or in the clobber \nset, no constraints are enforced. The valid inter\u00adleaving of these accesses is determined within the \ncontext of the closest enclosing task call where the locations are neither in the clobber set nor accessed \nonly with simulta\u00adneous coherence.  7.1 Sequential Execution We now show that a sequential execution \ntrivially satis.es the interleaving criteria required by the operational seman\u00adtics. Our proof of the \nsoundness of parallel scheduling de\u00adpends on this result. Sequential execution ignores the coherence \nmode Q in all function calls, using QS = \u00d8 instead, and interleaves traces by concatenating the subexpressions \ntraces in program or\u00adder. By ignoring the coherence modes, the clobber set re\u00admains empty and the result \nof all read expressions is fully determined. The following lemma and theorem show that the value and \nmemory trace that result from a sequential execu\u00adtion are always valid executions. Lemma 9. Let G, F, \nO f e : T and M, L, H, S, C f e . v, E and S ~ H. If C . CS, then M, L, H, S, C S f e . v, E . Theorem \n2. Let e1, . . . , en be expressions such that M, L, H, Si-1, C f ei . vi, Ei where Si = apply(Si-1, \nEi). If ES = E1++ . . . ++En, then valid interleave(S0, C, ES, E1, . . . , En). 7.2 Parallel Execution \nTo determine when parallel execution is safe, we start from the sequential execution trace and allow \nthe reordering of adjacent heap operations that do not change the behavior of the application. If we \ncan show that it is safe to reorder any pair of operations that come from two different constituent traces, \nthen any interleaving of the constituent traces will be equivalent to a sequential execution and parallel \nexecution is safe. To ef.ciently discover these cases at runtime, we require a test that can determine \nthis property prior to the actual execution of the tasks that create the traces. We show that a test \nbased on the subtask privileges and the current region mapping can soundly predict when this property \nwill hold. We begin by de.ning a non-interference operator on two memory operations E1 = op1(l1, c1, \nv1, t1) and E2 = op2(l2, c2, v2, t2): E1 # E2 .(op1 = read . op2 = read) . (op1 = reduceid1 . op2 = reduceid2 \n. id1 = id2). l1 = l2 Reads have no side effects, and cannot change what another read returns. The safety \nof the second case follows from the requirement that reduction operations be commutative. Fi\u00adnally, accesses \nto different locations cannot affect each other. Therefore, an adjacent pair of non-interfering memory \noper\u00adations in a memory trace can be reordered while preserving the validity of an interleaving. Lemma \n10. Let S be a store, C a clobber set, E1, . . . , En, Ea S , Eb S memory traces, and E1, E2 be two memory \noperations from Ei and Ej (i = j). Then, valid interleave(S, C, Ea S ++[E1, E2]++Eb S , E1, . . . , En) \n. E1#E2 . valid interleave(S, C, E a S ++[E2, E1]++Eb S , E1, . . . , En). Two whole memory traces are \nnon-interfering if no opera\u00adtion from one trace interferes with any from the other: E1#E2 .E1#E2 1 in \nE1, 2 in E2 If whole memory traces are non-interfering, any interleaving can be sorted via pairwise \nswaps to match the sequential memory trace. This gives us a result permitting safe parallel execution: \nLemma 11. Let S be an initial store, C be a clobber set, E1, . . . , En be memory traces such that Ei#Ej \nfor every 1 = i < j = n. Then, any interleave(ES, E1, . . . , En) . valid interleave(S, C, ES, E1, . \n. . , En).  any interleave([], [], . . . , []) = true any interleave([E]++ES , E1, . . . , [E]++Ei, \n. . . , En) = any interleave(ES , E1, . . . , Ei, . . . , En) coherent(S, L1, L2, []) = true valid interleave(S, \nC, ES , E1, . . . , En coherent(S, L1, L2, [E]++E) = . .. . .. ) = any interleave(ES , E1, . . . , En) \n. (l . L2 . S(l) = v) . if E = read(l, c, v, t) coherent(S, Lexcl(ES , C), Lexcl(ES , C), ES coherent(S, \nL1, L2, E), ) . coherent(apply(S, E), L1, L2 . {l}, E), if E = write(l, c, v, t) seq equiv(S, Lexcl(ES \n, C), Lexcl(ES , C), ES , E1, . . . , En) . . . . . and l . L1 .t.seq equiv(S, Latomic(ES , C), \u00d8, ES \n.t, (E1++ . . . ++En) .t) coherent(apply(S, E), L1, L2, E), otherwise Lexcl(E, C) = {l : op(l, excl, \nv, t) in E} \\ C Latomic(E, C) = {l : op(l, atomic, v, t) in E} \\ (C . Lexcl(E, C)) seq equiv(S, L1, L2, \nES , E1, . . . , En) = coherent(S, L1, L2, E1++ . . . ++En) . [] .t = [] .l . L1.apply(S, ES)(l) = apply(S, \nE1++ . . . ++En)(l) (op(l, c, v, tS)++E) .t = , op(l, c, v, tS)++(E .t), if t = tS E .t, otherwise \nFigure 7. Valid Interleaving Test We now use the bounds that static privileges place on run\u00adtime accesses \nto give an ef.cient runtime test for non\u00adinterference. We .rst extend the non-interference operator to \nwork on privileges: priv1(r1)#M priv2(r2) . (priv1 = reads . priv2 = reads) . (priv1 = reducesid1 . priv2 \n= reducesid2 . id1 = id2) . (r1 * r2) . (M(r1) n M(r2) = \u00d8) F1#M F2 . f1#M f2 f1.F1,f2.F2 The cases \nwhere both subtasks have read-only privileges or both subtasks have reduce-only privileges have equivalents \nfor regions, which can be tested statically. Detecting the case where the two sets of memory addresses \nare disjoint is ap\u00adproximated by two tests. The .rst uses (logical) region dis\u00adjointness constraints \nfrom the type system to statically infer non-interference. The second uses the region mapping M to dynamically \ndetermine the disjointness of the two regions. Although a dynamic test, it is performed once per pair \nof re\u00adgions rather than for every pair of memory operations. An algorithm to perform the dynamic test \nef.ciently is given in [2]. As region non-interference is an approximation of mem\u00adory trace non-interference, \nwe must show that it is sound. Lemma 12. Let M be a region mapping and E1 and E2 two memory traces such \nthat E1 :M F1 and E2 :M F2. If F1 and F2 are non-interfering under M, then E1 and E2 must be non-interfering. \nWe now state the theorem that allows the Legion run\u00adtime to perform hierarchical and parallel scheduling \nof non\u00adinterfering tasks. Theorem 3. Let e1, . . . , en be well-typed Legion expres\u00adsions, each with \nits own privileges Fi. Let M be a region mapping, L a local value mapping, H a heap typing, and S be \nan initial store satisfying M ~ O, L ~H M[[G]], and S ~ H. If Fi#M Fj for 1 = i < j = n, then any parallel \nexe\u00adcution of expressions e1, . . . , en results in a valid interleaving of memory operations. The proof \nfollows directly from Lemmas 11 and 12. This result holds even if the clobber set C is non-empty, allow\u00ading \nlocally independent subtasks to run in parallel even if they interact (in a programmer-permitted way) \nwith another subtask. We highlight an important aspect of a Legion implemen\u00adtation that is different \nfrom other systems and relies on the soundness of privileges. Dynamic non-interference of mem\u00adory operations \ncan only be determined after evaluation of an expression is completed, and only at great expense, as \nillustrated by work on transactional memory [13]. At the other extreme are systems like Jade [17] and \nDPJ [5] that check non-interference statically, but must disallow alias\u00ading to do so. In contrast, Legion \ncan verify non-interference of privileges at runtime, which is much simpler and more ef.cient than checking \nnon-interference of dynamic mem\u00adory traces. Even though the privileges themselves are static, the region \nmapping M is dynamic. Dynamically testing non\u00adinterference on the privileges of physical regions allows \npar\u00adallel execution in many more cases than a purely static anal\u00adysis can achieve in the presence of \naliasing. When a dynamic test fails, the Legion runtime is conservative and forces se\u00adquential ordering \nbetween the tasks to guarantee correct be\u00adhavior.  7.3 Atomic Coherence In cases where Legion cannot \nsafely infer non-interference of privileges (perhaps because two tasks actually access the same data \nin aliased regions), relaxation of the constraints on execution order can still be requested by the programmer \nthrough the use of coherence annotations on individual re\u00adgions passed to a task. The atomic coherence \nmode speci.es that although two tasks interfere due to accessing aliased re\u00adgions, they may execute in \neither order, allowing the task issued later in program order to possibly run before the task issued \nearlier in program order. This relaxation only applies 8. Hierarchical Scheduling Because testing non-interference \nof tasks is a pairwise op\u00aderation, scheduling n tasks can require O(n 2) tests. Thus, a scheduler that \nmust globally consider all pairs of tasks will be impractical for large machines and large numbers of \ntasks. The following theorem, however, shows that Legion programs enjoy a locality property that limits \nthe scope of the needed non-interference tests. Theorem 5. Let e1 and e2 be well-typed expressions using \nprivileges F1 and F2 respectively, where F1#M F2. Let e S 1 S be a subexpression of e1 and e be a subexpression \nof e2.if all aliased regions are annotated with atomic coherence. 2 Any memory traces E S 1 of e S 1 \nand E S 2 of e S 2 resulting from To show this is safe, we de.ne a relaxed version of non\u00ad interference \nfor atomic coherence: evaluation of e1 and e2 (with the usual consistent M, L, H, op1(l1, c1, v1, t1) \n#A op2(l2, c2, v2, t2) . op1(l1, c1, v1, t1) # op2(l2, c2, v2, t2) . (c1 = atomic . c2 = atomic . t1 \n= t2) We repeat the steps in Section 7.2 using the #A operator and reach another result used by the Legion \nruntime sched\u00aduler: Theorem 4. Let e1, . . . , en be well-typed Legion expres\u00adsions, each with its own \nprivileges Fi. Let M be a region mapping, L a local value mapping, H a heap typing, and S be an initial \nstore satisfying M ~ O, L ~H M[[G]], and S ~ H. If Fi#A M Fj for 1 = i < j = n, then for any permu\u00adtation \n(p1, . . . , pn) of (1, . . . , n), Ep1 ++ . . . ++Epn is a valid interleaving.  7.4 Simultaneous Coherence \nCoherence also can be relaxed using the simult mode, which allows multiple tasks to access the same region \nconcurrently. The simult coherence mode is appropriate in two important cases: 1. When subtasks are accessing \ndisjoint data, but the dis\u00adjointness is dif.cult to describe (e.g. walking separate linked lists that \nhave been allocated in the same region). 2. When the algorithm is tolerant of non-determinism (e.g. \nin a breadth-.rst search, setting the parent pointer of a node with multiple equally-short paths to the \nroot).  To support the simult coherence, the non-interference test is extended with a #S operator, analogous \nto #A for atomic coherence. Because the rules for valid interleavings exclude locations that are only \naccessed in simult mode, it is straight\u00adforward to extend Theorem 3 to show that parallel execution is \nsafe as long as Fi#S M Fj . It is also possible to have both atomic and simult coher\u00adence modes at the \nsame time for different regions in a task call. In this case the non-interference test #AS uses both \nthe atomic and simult relaxations, and Theorem 4 is extended to allow arbitrary reordering (but not simultaneous \nexecution) of subtasks when Fi#AS Fj . M and S) are non-interfering. The Legion task scheduler uses \nTheorem 5 as follows: sibling function calls (those invoked within the same func\u00adtion body) e1 and e2 \nare checked for non-interference of their (dynamic) privileges. Since e1 and e2 are called on the par\u00adent \ntask s node, no communication is required to perform the non-interference test. If they interfere they \nare executed in program order or serialized depending on their coher\u00adence speci.cations; otherwise they \nare considered for exe\u00adcution as parallel subtasks. If e1 and e2 are determined to be non-interfering \nand are scheduled in parallel on different remote processors then Theorem 5 guarantees that there is \nno communication required between e1 and e2 to perform non-interference tests between their sub-tasks. \nTherefore, the runtime requires no communication for scheduling. 9. Evaluation We evaluate the design \nof Legion s static and dynamic se\u00admantics on four criteria: expressivity (can real applications be written \nSection 9.1), overhead (what are the dynamic checking costs Section 9.2), scalability (can it enable \nhier\u00adarchical scheduling Section 9.3), and performance (does the performance increase from relaxed coherence \nmodes warrant the increased semantic complexity Section 9.4). Our prototype implementation has two components: \na type checker for the language of Section 3 and a C++ runtime library for executing programs written \nin the Legion pro\u00adgramming model. All experiments are conducted on the Keeneland supercomputer[20]. Each \nnode of Keeneland consists of two Xeon 5660 CPUs, three Tesla M2090 GPUs, and 24 GB of DRAM. Nodes are \nconnected by a QDR In\u00ad.niband interconnect. 9.1 Expressivity We evaluate Legion on three real-world applications. \nTo qualitatively gauge the expressivity of Legion, we introduce these applications by describing features \nused in their im\u00adplementations. The Circuit example was already covered in detail in Section 2.  Figure \n8. Overhead in Circuit simulation with 96 pieces. Fluid is a distributed memory version of the fluidanimate \nbenchmark from the PARSEC suite[3]. Fluid simulates the .ow of an incompressible .uid using particles \nthat move within a regular grid of cells. To perform operations in par\u00adallel, the array of cells is partitioned. \nUnlike Circuit, Fluid creates and partitions regions before allocating cells in them. Another difference \nis that Fluid maintains separate regions for ghost cells rather than using multiple partitions of the \nre\u00adgions containing shared data. Region relationships are used to capture which regions are required \nfor each grid. The third application is a Legion port of an adaptive mesh re.nement (AMR) benchmark from \nBoxLib [15]. The al\u00adgorithm solves the two dimensional heat diffusion equation on a grid of cells using \nthree levels of re.nement with sub\u00adre.nements randomly placed on the surface. Every level of re.nement \nuses a separate region, which is partitioned sev\u00aderal ways to support multiple views of the cells. One \npar\u00adtitioning separates cells into pieces that can be updated in parallel. Additional partitions are \ncreated for viewing data from coarser and .ner levels of the simulation. Two types of region relationships \nare created: one describes pieces at each level of re.nement, and another describes relationships be\u00adtween \npieces at different levels of re.nement. The dynamic nature of AMR requires that regions be created and \nparti\u00adtioned at runtime. Dynamically creating and partitioning regions at runtime is crucial to Legion \ns ability to handle applications that make runtime decisions about data organization (AMR). Having multiple \npartitions of regions is necessary for describing the many ways that data can be accessed (Circuit, AMR). \nAll the types of privileges and coherence are needed in some application; region relationships are used \nin all appli\u00adcations. Finally, all applications introduce aliasing of data either through the use of \nmulticolorings or by having mul\u00adtiple partitions. Our implementations of these applications both type \ncheck and execute, proving that our type system is suf.ciently expressive to handle real-world applications. \n 9.2 Checking Overhead The .rst Legion implementation consisted of a C++ library of Legion primitives \n[2] with no checking of region mem\u00adory accesses. When using this system we frequently en-Figure 9. Overhead \nin Fluid simulation with 19200 cells. countered memory corruption due to illegal region accesses caused \nby application bugs. In many cases, this corruption occurred between nodes in the cluster or on GPUs, \nenviron\u00adments for which debugging tools are primitive at best. To locate the application bugs causing \nthese illegal accesses, we initially added dynamic checks on all region accesses for both CPUs and GPUs \nwhich added considerable runtime overhead. In short, the standard bene.ts of type checking (increased \nprogram safety and ef.ciency) are magni.ed in high performance parallel applications, because debugging \nis so dif.cult and ef.ciency considerations are paramount. To preserve the bene.t of checking every access \nwithout the cost of dynamic checks, we implemented the type, privilege, and coherence checker we have \ndescribed. We then rewrote the applications in this language and type checked them, at which point the \ndynamic region access checks could be safely elided. Figures 8, 9, and 10 show the total time spent by \nall CPUs and GPUs in each phase of the application. The top\u00admost component of each bar shows the overhead \nadded by the dynamic checks. In each .gure the problem size stays the same as the number of processors \nincreases (strong scal\u00ading). Figure 10 includes multiple problem sizes to show how overhead is affected \nby changing problem size (weak scal\u00ading). For cases where there is an existing implementation to compare \nagainst we have included a dotted line indicating baseline performance. In a few cases (Figures 9 and \n10(a)), the checking overhead is the difference between better and worse performance than the baseline. \nThe overall perfor\u00admance relative to the baseline implementations is discussed in [2]. In addition to \ntotal processor overhead, we also measured performance gain from eliding checks in terms of wall-clock \ntime. Since most region accesses occur in leaf tasks, the checks parallelize well. Wall-clock performance \ngains from eliding memory checks ranged from 1-10%, 1-15%, and 2\u00ad71% for Circuit, Fluid, and AMR respectively. \nPerformance gains for AMR were larger than the other applications be\u00adcause AMR was already memory bound \nand the additional checks intensi.ed memory pressure. For the GPU kernels in the Circuit application \nchecking required up to 8 addi\u00adtional registers per thread. While the GPU kernels in Cir\u00adcuit were not \nbound by available on-chip memory, kernels that are would be susceptible to extreme performance degra\u00addation \ndue to the extra registers required for checking. We also measured the overhead of the dynamic checks \nassoci\u00adated with checking task call privileges but found them to be negligible, demonstrating that runtime \nnon-interference checks are inexpensive in Legion.    9.3 Scalability Figures 8, 9, and 10 show that \nthe overhead of the Legion runtime is always less than 7% of the total execution time of the applications. \nIn some applications communication does not scale well, but this is a result of the algorithm required \nby the application, not the Legion runtime. Even in the case of the Circuit application, which exhibits \nquadratic increases in communication cost, the Legion runtime is able to achieve a 62.5X speedup on 96 \nGPUs over a hand-coded single GPU implementation[2]. Figure 11. Performance of relaxed coherence modes. \n 9.4 Performance To demonstrate the bene.t of relaxed coherence modes, we modi.ed the circuit example \nfrom Section 2 to use ex\u00adclusive coherence instead of atomic coherence in the dis\u00adtribute charge task \nand compared the performance of the two versions. The results are shown in Figure 11. Slow\u00addowns ranged \nfrom 34% on 48-GPUs to 67% on 96 GPUs and more importantly scaled with node count. This is a di\u00adrect \nconsequence of Amdahl s Law. Even though the dis\u00adtribute charge tasks are a small fraction of the total \nwork, the serialization that results from requiring exclusive ac\u00adcess to the overlapping ghost regions \nlimits the scalability of the application. Relaxed coherence modes will be crucial in achieving scalability \nof applications with aliased data on distributed memory machines. 10. Related Work Legion is most directly \nrelated to Sequoia [1, 10]. Sequoia is a static language with a single uni.ed hierarchy of tasks and \ndata; Legion is more dynamic with separate task and region hierarchies. Deterministic Parallel Java (DPJ) \nis the only other region\u00adbased parallel system of which we are aware[4]. While there are similarities \nbetween DPJ s effects and Legion privileges, there are differences stemming from DPJ s static approach. \nRegions in Legion are .rst-class and can be created, par\u00adtitioned, packed, and unpacked dynamically, \nallowing pro\u00adgrammers to compute data organization at runtime; like Se\u00adquoia, DPJ partitioning schemes \nare static. Legion allows programmers to create multiple partitions of the same region to give different \nviews onto the same data, which is not pos\u00adsible in DPJ. DPJ supports both exclusive and atomic tasks \nwhich are similar to Legion s coherence modes, but only al\u00adlows speci.cation at the coarser granularity \nof tasks and not individual regions. Chapel [7] and X10 [8] also provide some Legion-like facilities. \nChapel s locales and X10 s places provide the pro\u00adgrammer with a mechanism for expressing locality, similar \nto regions in Legion. However, locales and places are not used for independence analysis to discover \nparallelism. In contrast, Jade uses annotations to describe data disjointness, and like Legion leverages \nthe disjointness information to dis\u00adcover parallelism, but lacks a region system to name and or\u00adganize \nunbounded collections of objects [17]. Hierarchical Place Trees (HPT) [21] is a generalization of the \nSequoia and X10 program models. HPT presents hi\u00aderarchical places in which to put data; places are mapped \nonto physical locations in the memory hierarchy. HPT has no equivalent to partitioning in Legion, leaving \nthe burden on the programmer to ensure that data is moved correctly through the place hierarchy and to \nensure the safety of par\u00adallel task execution. Many efforts use static region systems for memory man\u00adagement \n(e.g., [12, 18]). Our system is more closely related to dynamic region systems used for expressing locality \nfor performance [11]. Titanium is an SPMD parallel language with a region system where regions are tightly \nbound to spe\u00adci.c processors [22]. There have been many type and effect systems for owner\u00adship types \n[6] including ones that leverage nested regions for describing relationships (e.g., [9]). However, ownership \ntype and effect systems are primarily used for reasoning about de\u00adterminism in object oriented languages \nand don t capture the range of disjointness properties in Legion. Reasoning about disjoint data is the \nstrong suit of separation logic [16]. While we have borrowed some separation logic notation, we chose \nto use a privileges system because separation logic does not easily support reasoning about the interleaving \nof operations to aliased regions of memory. 11. Conclusion Modern architectures have dramatically increased \nin com\u00adplexity in recent years. To program this class of machines, new programming systems will be required \nthat are capable of reasoning about the structure of data and how it should be partitioned. We have presented \nthe static and dynamic se\u00admantics for the Legion programming system, showing how a combination of static \nand dynamic checks can be used to support region-level privileges and coherence, even in the presence \nof dynamically partitioned and aliased data. We have also given a novel compositional parallel semantics, \npermitting a precise treatment of relaxed coherence modes; in particular we have shown the Legion design \nis sound even with relaxed coherence. These semantics make possi\u00adble a novel hierarchical scheduling \nalgorithm that is crucial for scaling on large distributed machines. Finally, we have demonstrated that \nour system enables static elision of many dynamic checks leading to large performance improvements on \nreal world applications. Acknowledgments This research used resources of the Keeneland Computing Facility \nat the Georgia Institute of Technology, supported by the National Science Foundation under Contract OCI\u00ad0910735. \nSean Treichler and Michael Bauer were supported by grant W911NF-07-2-0027-1 from the Army High Per\u00adformance \nComputing Research Center. Michael Bauer was supported by an NVIDIA Graduate Research Fellowship. References \n[1] M. Bauer, J. Clark, E. Schkufza, and A. Aiken. Programming the memory hierarchy revisited: Supporting \nirregular paral\u00adlelism in Sequoia. In Proceedings of the Symposium on Prin\u00adciples and Practice of Parallel \nProgramming, 2011. [2] M. Bauer, S. Treichler, E. Slaughter, and A. Aiken. Legion: Expressing locality \nand independence with logical regions. In Supercomputing (SC), 2012. [3] C. Bienia. Benchmarking Modern \nMultiprocessors. PhD thesis, Princeton University, January 2011. [4] R. Bocchino et al. A type and effect \nsystem for deterministic parallel Java. In OOPSLA, 2009. [5] R. Bocchino et al. Safe nondeterminism in \na deterministic-by\u00addefault parallel language. In POPL, 2011. [6] C. Boyapati, B. Liskov, and L. Shrira. \nOwnership types for object encapsulation. In POPL, 2003. [7] B.L. Chamberlain et al. Parallel programmability \nand the chapel language. Int l Journal of HPC Applications, 2007. [8] P. Charles et al. X10: An object-oriented \napproach to non\u00aduniform cluster computing. In OOPSLA, 2005. [9] D. Clarke and S. Drossopoulou. Ownership, \nencapsulation and the disjointness of type and effect. In OOPSLA, 2002. [10] K. Fatahalian et al. Sequoia: \nProgramming the Memory Hier\u00adarchy. In Supercomputing, November 2006. [11] D. Gay and A. Aiken. Language \nsupport for regions. In PLDI, 2001. [12] D. Grossman et al. Region-based memory management in cyclone. \nIn PLDI, 2002. [13] T. Harris, S. Marlow, S. Peyton-Jones, and M. Herlihy. Com\u00adposable memory transactions. \nIn PPOPP, 2005. [14] G. Karypis and V. Kumar. A fast and high quality multilevel scheme for partitioning \nirregular graphs. SIAM J. Sci. Com\u00adput., 1998. [15] M. Lijewski, A. Nonaka, and J. Bell. Boxlib. https://ccse. \nlbl.gov/BoxLib/index.html, 2011. [16] J. C. Reynolds. Separation logic: A logic for shared mutable data \nstructures. In IEEE Symposium on Logic in CS, 2002. [17] M. C. Rinard and M. S. Lam. The design, implementation, \nand evaluation of Jade. ACM Trans. Program. Lang. Syst., 1998. [18] M. Tofte and J.P. Talpin. Region-based \nmemory management. In POPL, 1994. [19] S. Treichler, M. Bauer, and A. Aiken. Language sup\u00adport for dynamic, \nhierarchical data partitioning: Extended version. http://theory.stanford.edu/~aiken/ publications/papers/oopsla13a-extended.pdf, \n2013. Technical Report. [20] J.S. Vetter et al. Keeneland: Bringing heterogeneous gpu com\u00adputing to the \ncomputational science community. Computing in Science Engineering, pages 90 95, 2011. [21] Y. Yan, J. \nZhao, Y. Guo, and V. Sarkar. Hierarchical place trees: A portable abstraction for task parallelism and \ndata movement. In Workshop on Languages and Compilers for Parallel Computing, 2009. [22] K. Yelick et \nal. Titanium: A high-performance Java dialect. In Workshop on Java for High-Performance Network Com\u00adputing, \n1998. A. Core Legion Circuit Code -- (voltage,current,charge,capacitance,piece ID) type CircuitNode \n= (int,int,int,int,int) -- ( owned node, owned or ghost node, resistance, current) type CircuitWire(rn,rg) \n= (CircuitNode@rn, CircuitNode@(rn,rg),int,int) type NodeList(rl,rn) = ( CircuitNode@rn, NodeList(rl,rn)@rl \n)type WireList(rl,rw,rn,rg)= ( CircuitWire(rn,rg)@rw, WireList(rl,rw,rn,rg)@rl )function extern metis[rl,rn,rw](node \nlist : NodeList(rl,rn)@rl, wires list : WireList(rl,rw,rn,rn)@rl), reads(rl,rn,rw), writes(rn) : bool \ntype CircuitPiece(rl,rw,rn) = rr[rpw,rpn,rg] ( WireList(rl,rpw,rpn,rg)@rl, NodeList(rl,rpn)@rl ) where \nrpn = rn and rg = rn and rpw = rw and rn * rw and rl * rn and rl * rw -- Multicoloring helper for aliased \npartitions type multicoloring(rn) = ( coloring(rn), coloring(rn) ) -- Simulation initialization and \ninvocation function simulate circuit[rl,rw,rn] ( all nodes : NodeList(rl,rn)@rl, all wires : WireList(rl,rw,rn,rn)@rl, \nsteps : int ), reads(rn,rw,rl), writes(rn,rw,rl) : bool = -- use METIS to decide how to partition circuit \nlet : bool = extern metis[rl,rn,rw](all nodes, all wires) in -- create colorings to describe METIS results \nto Legion let owned node map : coloring(rn) = owned node coloring[rl,rn](all nodes) in let ghost node \nmap : multicoloring(rn) = ghost node coloring[rl,rw,rn,rn](all wires) in let wire map : coloring(rw) \n= wire coloring[rl,rw,rn,rn](all wires) in -- Disjoint partition for the owned nodes of each piece partition \nrn using owned node map as rn0,rn1 in -- Aliased partition for ghost nodes of each piece partition rn \nusing ghost node map.1 as rg0 in partition rn using ghost node map.2 as rg1 in -- Disjoint partition \nfor the owned wires of each piece partition rw using wire map as rw0,rw1 in -- Create region relationships \nfor circuit pieces let lists0 : (WireList(rl,rw0,rn0,rg0)@rl,NodeList(rl,rn0)@rl) = ( build piece wire \nlist[rl,rw,rn,rw0,rn0,rg0](all wires), build piece node list[rl,rn,rn0](all nodes) ) in let piece0 : \nCircuitPiece(rl,rw,rn) = pack lists0 as CircuitPiece(rl,rw,rn)[rw0,rn0,rg0] in let lists1 : (WireList(rl,rw1,rn1,rg1)@rl,NodeList(rl,rn1)@rl) \n= ( build piece wire list[rl,rw,rn,rw1,rn1,rg1](all wires), build piece node list[rl,rn,rn1](all nodes) \n) in let piece1 : CircuitPiece(rl,rw,rn) = pack lists1 as CircuitPiece(rl,rw,rn)[rw1,rn1,rg1] in -- \ndo actual (parallel) simulation execute time steps[rl,rw,rn](piece0,piece1,steps) -- Time Step Loop function \nexecute time steps[rl,rw,rn] ( p0 : CircuitPiece(rl,rw,rn), p1 : CircuitPiece(rl,rw,rn), steps : int \n) , reads(rn,rw,rl), writes(rn,rw) : bool = if steps < 1 then true else unpack p0 as piece0 : CircuitPiece(rl,rw,rn)[rw0,rn0,rg0] \nin unpack p1 as piece1 : CircuitPiece(rl,rw,rn)[rw1,rn1,rg1] in let : bool = calc new currents[rl,rw0,rn0,rg0](piece0.1) \nin let : bool = calc new currents[rl,rw1,rn1,rg1](piece1.1) in let : bool = distribute charge[rl,rw0,rn0,rg0](piece0.1) \nin let : bool = distribute charge[rl,rw1,rn1,rg1](piece1.1) in let : bool = update voltage[rl,rn0](piece0.2) \nin let : bool = update voltage[rl,rn1](piece1.2) in execute time steps[rl,rw,rn](p0,p1,steps-1) Listing \n2. Top-Level Application Code 69 function calc new currents[rl,rw,rn,rg] ( ptr list : WireList(rl,rw,rn,rg)@rl \n), 70 reads(rl,rw,rn,rg), writes(rw) : bool = 71 if isnull(ptr list) then true else 72 let wire node \n: WireList(rl,rw,rn,rg) = read(ptr list) in 73 let wire : CircuitWire(rn,rg) = read(wire node.1) in 74 \nlet in node : CircuitNode = read(wire.1) in 75 let out node : CircuitNode = read(wire.2) in 76 let current \n: int = (in node.1 - out node.1) / wire.3 in 77 let new wire : CircuitWire(rn,rg) = (wire.1,wire.2,wire.3,current) \nin 78 let : CircuitWire(rn,rg)@rw = write(wire node.1, new wire) in 79 calc new currents[rl,rw,rn,rg](wire \nnode.2) 80 81 function distribute charge[rl,rw,rn,rg] ( ptr list : WireList(rl,rw,rn,rg)@rl ), 82 reads(rl,rw,rn), \nreduces(reduce charge,rn,rg), atomic(rn,rg) : bool = 83 if isnull(ptr list) then true else 84 let wire \nnode : WireList(rl,rw,rn,rg) = read(ptr list) in 85 let wire : CircuitWire(rn,rg) = read(wire node.1) \nin 86 let : CircuitNode@rn = reduce(reduce charge, wire.1, wire.4) in 87 let : CircuitNode@(rn,rg) = \nreduce(reduce charge, wire.2, wire.4) in 88 distribute charge[rl,rw,rn,rg](wire node.2) 89 90 function \nupdate voltage[rl,rn] ( ptr list : NodeList(rl,rn)@rl ), 91 reads(rl,rn), writes(rn) : bool = 92 if isnull(ptr \nlist) then true else 93 let node ptr : CircuitNode@rn = read(ptr list).1 in 94 let : CircuitNode@rn = \n 95 -- update voltage on a node 96 let node : CircuitNode = read(node ptr) in 97 let voltage : int = \n(node.3 / node.4) in 98 let new node : CircuitNode = ( voltage, node.2, node.3, node.4, node.5 ) in 99 \nwrite(node ptr, new node) 100 in 101 let next : NodeList(rl,rn)@rl = read(ptr list).2 in 102 update \nvoltage[rl,rn](next) 103 104 -- Reduction function for distribute charge 105 function reduce charge \n( node : CircuitNode, current : int ) : CircuitNode = 106 let new charge : int = node.3 + current in \n107 ( node.1, node.2, new charge, node.4, node.5 ) 108 Listing 3. Leaf Computation Tasks  109 function \nowned node coloring[rl,rn] ( node list: NodeList(rl,rn)@rl ), 110 reads(rl,rn) : coloring(rn) = 111 if \nisnull(node list) then 112 newcolor rn 113 else -- tuple .elds accessed by .(.eld number) 114 let list \nelem : NodeList(rl,rn) = read(node list) in 115 let part coloring : coloring(rn) = owned node coloring[rl,rn](list \nelem.2) in 116 let node ptr : CircuitNode@rn = list elem.1 in 117 let node : CircuitNode = read(node \nptr) in 118 let piece id from metis: int = node.5 in 119 color(part coloring, node ptr, piece id from \nmetis) 120 121 function ghost node coloring[rl,rw,rn,rg] ( wire list: WireList(rl,rw,rn,rg)@rl ), 122 \nreads(rl,rw,rn,rg) : multicoloring(rn) = 123 if isnull(wire list) then 124 ( newcolor rn, newcolor rn \n) 125 else -- tuple .elds accessed by .(.eld number) 126 let list elem : WireList(rl,rw,rn,rg) = read(wire \nlist) in 127 let part coloring : multicoloring(rn) = 128 ghost node coloring[rl,rw,rn,rg](list elem.2) \nin 129 let wire ptr : CircuitWire(rn,rg)@rw = list elem.1 in 130 let wire : CircuitWire(rn,rg) = read(wire \nptr) in 131 let in node : CircuitNode = read(wire.1) in 132 let out node : CircuitNode = read(wire.2) \nin 133 let in piece id : int = in node.5 in 134 let out piece id : int = out node.5 in 135 let id not \nequal : bool = 136 if in piece id ( out piece id then true else 137 if out piece id ( in piece id then \ntrue else false 138 in 139 if id not equal then 140 if in piece id ( 2 then 141 ( color(part coloring.1, \ndownregion(wire.2, rn), 1), part coloring.2 ) 142 else 143 ( part coloring.1, color(part coloring.2, \ndownregion(wire.2, rn), 1) ) 144 else 145 ( part coloring.1, part coloring.2 ) 146 147 function wire \ncoloring[rl,rw,rn,rg] ( wire list: WireList(rl,rw,rn,rg)@rl ), 148 reads(rl,rw,rn) : coloring(rw) = 149 \nif isnull(wire list) then 150 newcolor rw 151 else -- tuple .elds accessed by .(.eld number) 152 let \nlist elem : WireList(rl,rw,rn,rg) = read(wire list) in 153 let part coloring : coloring(rw) = wire coloring[rl,rw,rn,rg](list \nelem.2) in 154 let wire ptr : CircuitWire(rn,rg)@rw = list elem.1 in 155 let wire : CircuitWire(rn,rg) \n= read(wire ptr) in 156 let node ptr : CircuitNode@rn = wire.1 in 157 let node : CircuitNode = read(node \nptr) in 158 let piece id from metis: int = node.5 in 159 color(part coloring, wire ptr, piece id from \nmetis) 160 Listing 4. Coloring Functions 161 function build piece node list[rl,rn,rpn] ( all nodes : \nNodeList(rl,rn)@rl ), 162 reads(rl), writes(rl) : NodeList(rl,rpn)@rl = 163 if isnull(all nodes) then \n164 null NodeList(rl,rpn)@rl 165 else 166 let list elem : NodeList(rl,rn) = read(all nodes) in 167 let \npart list : NodeList(rl,rpn)@rl = 168 build piece node list[rl,rn,rpn](list elem.2) in 169 let node ptr \n: CircuitNode@rpn = downregion(list elem.1, rpn) in 170 if isnull(node ptr) then 171 part list 172 else \n173 let new elem ptr : NodeList(rl,rpn)@rl = new NodeList(rl,rpn)@rl in 174 let new elem : NodeList(rl,rpn) \n= ( node ptr, part list ) in 175 let : NodeList(rl,rpn)@rl = write(new elem ptr, new elem) in 176 new \nelem ptr 177 178 function build piece wire list[rl,rw,rn,rpw,rpn,rpg] 179 ( all wires : WireList(rl,rw,rn,rn)@rl \n), 180 reads(rl,rpw), writes(rl,rpw) : WireList(rl,rpw,rpn,rpg)@rl = 181 if isnull(all wires) then 182 \nnull WireList(rl,rpw,rpn,rpg)@rl 183 else 184 let list elem : WireList(rl,rw,rn,rn) = read(all wires) \nin 185 let part list : WireList(rl,rpw,rpn,rpg)@rl = 186 build piece wire list[rl,rw,rn,rpw,rpn,rpg](list \nelem.2) in 187 let wire ptr : CircuitWire(rn,rn)@rpw = downregion(list elem.1, rpw) in 188 if isnull(wire \nptr) then 189 part list 190 else 191 let old wire : CircuitWire(rn,rn) = read(wire ptr) in 192 let new \nwire ptr : CircuitWire(rpn,rpg)@rpw = 193 new CircuitWire(rpn,rpg)@rpw in 194 let new wire : CircuitWire(rpn,rpg) \n= ( downregion(old wire.1, rpn), 195 downregion(old wire.2, rpn, rpg), 196 old wire.3, old wire.4 ) in \n197 let : CircuitWire(rpn,rpg)@rpw = write(new wire ptr, new wire) in 198 let new elem ptr : WireList(rl,rpw,rpn,rpg)@rl \n= 199 new WireList(rl,rpw,rpn,rpg)@rl in 200 let new elem : WireList(rl,rpw,rpn,rpg) = ( new wire ptr, \npart list ) in 201 let : WireList(rl,rpw,rpn,rpg)@rl = write(new elem ptr, new elem) in 202 new elem \nptr 203 Listing 5. List-Building Helper Functions   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Applications written for distributed-memory parallel architectures must partition their data to enable parallel execution. As memory hierarchies become deeper, it is increasingly necessary that the data partitioning also be hierarchical to match. Current language proposals perform this hierarchical partitioning statically, which excludes many important applications where the appropriate partitioning is itself data dependent and so must be computed dynamically. We describe Legion, a region-based programming system, where each region may be partitioned into subregions. Partitions are computed dynamically and are fully programmable. The division of data need not be disjoint and subregions of a region may overlap, or alias one another. Computations use regions with certain privileges (e.g., expressing that a computation uses a region read-only) and data coherence (e.g., expressing that the computation need only be atomic with respect to other operations on the region), which can be controlled on a per-region (or subregion) basis.</p> <p>We present the novel aspects of the Legion design, in particular the combination of static and dynamic checks used to enforce soundness. We give an extended example illustrating how Legion can express computations with dynamically determined relationships between computations and data partitions. We prove the soundness of Legion's type system, and show Legion type checking improves performance by up to 71% by eliding provably safe memory checks. In particular, we show that the dynamic checks to detect aliasing at runtime at the region granularity have negligible overhead. We report results for three real-world applications running on distributed memory machines, achieving up to 62.5X speedup on 96 GPUs on the Keeneland supercomputer.</p>", "authors": [{"name": "Sean Treichler", "author_profile_id": "81482657726", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4290403", "email_address": "sjt@cs.stanford.edu", "orcid_id": ""}, {"name": "Michael Bauer", "author_profile_id": "81481644464", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4290404", "email_address": "mebauer@cs.stanford.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P4290405", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509545", "year": "2013", "article_id": "2509545", "conference": "OOPSLA", "title": "Language support for dynamic, hierarchical data partitioning", "url": "http://dl.acm.org/citation.cfm?id=2509545"}