{"article_publication_date": "10-29-2013", "fulltext": "\n Barrier Invariants: A Shared State Abstraction for the Analysis of Data-Dependent GPU Kernels * Nathan \nChong Alastair F. Donaldson Shaz Qadeer Paul H.J. Kelly Jeroen Ketema Microsoft Research Imperial College \nLondon qadeer@microsoft.com {nyc04,afd,phjk,jketema}@imperial.ac.uk Abstract Data-dependent GPU kernels, \nwhose data or control .ow are dependent on the input of the program, are dif.cult to ver\u00adify because \nthey require reasoning about shared state manip\u00adulated by many parallel threads. Existing veri.cation \ntech\u00adniques for GPU kernels achieve soundness and scalability by using a two-thread reduction and making \nthe contents of the shared state nondeterministic each time threads synchro\u00adnise at a barrier, to account \nfor all possible thread interac\u00adtions. This coarse abstraction prohibits veri.cation of data\u00addependent \nkernels. We present barrier invariants, a novel abstraction technique which allows key properties about \nthe shared state of a kernel to be preserved across barriers dur\u00ading formal reasoning. We have integrated \nbarrier invariants with the GPUVerify tool, and present a detailed case study showing how they can be \nused to verify three pre.x sum al\u00adgorithms, allowing ef.cient modular veri.cation of a stream compaction \nkernel, a key building block for GPU program\u00adming. This analysis goes signi.cantly beyond what is possi\u00adble \nusing existing veri.cation techniques for GPU kernels. Categories and Subject Descriptors F.3.1 [Logics \nand Meanings of Programs]: Specifying and Verifying and Rea\u00adsoning about Programs Keywords Veri.cation; \nGPUs; concurrency; data races. * This work was supported by the EU FP7 STREP project CARP (project number \n287767) and the EPSRC PSL project (EP/I006761/1). Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting \nwith credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior speci.c permission and/or a fee. Request permissions from permissions@acm.org. OOPSLA \n13, October 29 31, 2013, Indianapolis, Indiana, USA. Copyright c &#38;#169; 2013 ACM 978-1-4503-2374-1/13/10. \n. . $15.00. http://dx.doi.org/10.1145/2509136.2509517 1. Introduction Graphics processing units (GPUs) \nare highly parallel pro\u00adcessors that are now commonly used in the acceleration of a wide range of computationally \nintensive tasks. A GPU con\u00adsists of a large number of processor elements, each equipped with a region \nof private memory, together with an area of shared memory. GPUs are programmed using kernels: a GPU kernel \nis a function parameterised by a thread identi\u00ad.er, to be executed in parallel by many threads such that \neach thread executes on a separate processing element. Thread\u00adlocal data is stored in private memory, \nand data shared be\u00adtween threads is stored in shared memory. In order to com\u00admunicate through shared \nmemory, threads can synchronise using barrier operations. On reaching a barrier a thread stalls until \nall threads have reached the barrier and shared mem\u00adory accesses issued by all threads have completed.1 \nThe two most widely used programming models for writing GPU ker\u00adnels are CUDA [30] and OpenCL [22]. Writing \nGPU kernels is challenging due to concurrency bugs, especially data races, where two threads access the \nsame region of shared memory, at least one of the accesses is a write, and there is no intervening barrier \nsynchronisation. As in traditional concurrent software, data races can lead to nondeterministically occurring \nbugs that are hard to track down and .x. Unlike traditional concurrent software, data races in GPU kernels \nare rarely benign: they are almost always a result of programmer errors. Recently, many techniques for \nformal analysis of GPU kernels have been developed. These techniques have primar\u00adily focused on data \nraces, using formal veri.cation [2, 12, 25], dynamic symbolic execution [11, 26, 27] and a com\u00adbination \nof static and dynamic analysis [24]. The GPUVer\u00adify [2, 12] and PUG [25] tools achieve scalable veri.cation \nbased on the observation that data race-freedom is a pair\u00adwise property: a data race always occurs between \nexactly two threads. Data race analysis can thus be performed by con\u00ad 1 In practice, GPU kernels consist \nof multiple groups of threads. Since barriers, the focus of our paper, allow synchronisation only between \nthreads within a group, we restrict our presentation to the single-group case.  sidering the execution \nof an arbitrary pair of threads, using abstraction to over-approximate the effects of other threads. \nIf a kernel is race-free for an arbitrary pair of threads then it must be race-free for all possible \npairs of threads. We re\u00adfer to this approach as the two-thread reduction. It avoids the need to use quanti.ers \nto reason about all threads exe\u00adcuting a kernel, which is advantageous due to the dif.culty of automated \nreasoning in the presence of quanti.ed formu\u00adlae [14, 16, 29]. The idea of reducing veri.cation complexity \nthrough pairwise reasoning is well-known and has been em\u00adployed for example in model checking of cache \ncoherence protocols [9, 28, 34]. The soundness and precision of the two-thread reduc\u00adtion hinges on the \nmethod used to over-approximate the behaviour of additional threads. The simplest approach is to make \nno assumptions about the behaviour of additional threads, assuming that these threads may update the \nshared state arbitrarily. In veri.cation terms, this can be achieved by making the contents of the shared \nstate nondeterministic (i.e., by havocking the shared state) each time a barrier is reached. Variations \non this adversarial abstraction are em\u00adployed by GPUVerify and PUG. The adversarial abstraction has been \nshown to be effective in the veri.cation of data\u00adindependent GPU kernels: kernels for which control .ow \nand memory access patterns do not depend on the data stored in shared memory. While a large number of \nGPU kernels fall into this category, there are important and interesting kernels that exhibit data-dependence. \nA key family of such kernels use pre.x sum operations [4, 18] to perform compaction of data [3]; we describe \nand study these operations and their use extensively in the paper. Data-dependent kernels are dif.cult \nto verify because they require reasoning about shared state manipulated by many parallel threads. The \naccess pattern of a single thread may depend on the data or control .ow of many other threads, making \nrace checking challenging. To see why data-dependence hinders veri.cation using adversarial abstraction, \nconsider the following simple ker\u00adnel, where A and B are arrays in shared memory, tid denotes the id \nof a thread, and f is a side-effect free procedure which may read from the shared state and ensures that \nfor distinct threads s and t that f(s) f(t) (a larger, real-world exam\u00ad = ple is presented in Section \n2): A[tid] = f(tid); barrier(); B[A[tid]] = tid; The kernel is data-dependent because array B is written \nto at an index which is computed by reading from the shared state. The kernel is clearly race-free. However, \nconsider ex\u00adecution of the kernel fragment with respect to an arbitrary, distinct pair of threads, s \nand t, using the adversarial ab\u00adstraction. Because s = t, execution of A[tid] = f(tid) by both threads \nwill not result in a data race on A, and will lead to a state in which A[s] A[t]. However, at the barrier, \nthe = adversarial abstraction dictates that A and B should be non\u00addeterministically assigned. A possible \nnondeterministic as\u00adsignment yields A[s] = A[t] = 0, which causes the statement B[A[tid]] = tid to result \nin a data race on B at index 0. This simple kernel is not amenable to veri.cation using the two-thread \nreduction with adversarial abstraction: the adver\u00adsarial abstraction is too coarse. In this paper, we \npresent barrier invariants, a novel ab\u00adstraction technique to allow veri.cation of data-dependent GPU \nkernels using the two-thread reduction. The idea is that each barrier in a kernel can be annotated with \na barrier in\u00advariant, stating a property of the shared state that must hold each time the barrier is \nreached. Barrier invariants retain the scalability of the two-thread reduction, but allow the preci\u00adsion \nlost by the adversarial abstraction to be recovered: when considering the execution of a barrier by an \narbitrary pair of threads, instead of setting the shared state to an arbitrary value, the shared state \nis set to an arbitrary value satisfying the barrier invariant. A barrier invariant can be added to the \nabove example to capture the fact that the elements of A are distinct as follows (where x and y range \nover thread ids): A[tid] = f(tid); barrier() invariant(.x= y : A[x]=A[y]); B[A[tid]] = tid; When considering \nthe execution of an arbitrary pair of threads s and t, the invariant is established on entry to the barrier \nby checking that A[s] A[t]: because s and t are = arbitrary, this proves that the invariant holds for \nall pairs of threads. After the barrier, it is legitimate to assume that A[x] A[y] holds for all pairs \nof distinct threads x and y = and, in particular, for the threads s and t under considera\u00adtion, and thus \nthe write to B at index A[tid] is veri.ed to be race-free as required. In fact, after the barrier, it \nis suf\u00ad.cient to assume the invariant only for the pair (s, t). More complex kernels usually require \nthe invariant to be assumed for multiple pairs. However, a small subset of all pairs usu\u00adally suf.ces. \nThis is important for kernels with thousands of threads where assuming a barrier invariant for all pairs \nis not feasible without the use of quanti.ers. In the same way that loop invariants and procedure spec\u00adi.cations \nare fundamental to modular reasoning about se\u00adquential programs, we believe that barrier invariants are \nfun\u00addamental to thread-modular reasoning about data-dependent GPU kernels which, until now, has been \nout-of-scope. Be\u00adcause GPU kernels are executed by large numbers of threads, barrier invariants are still \nnecessary to reason about data\u00addependent properties even for loop-and call-free kernels. In summary, \nthe main contributions of our work are: A theoretical presentation and proof of soundness for bar\u00adrier \ninvariants, which allow precise veri.cation of data\u00addependent GPU kernels;  A case study using barrier \ninvariants to prove speci.ca\u00adtions for three distinct pre.x sum algorithms, and using these for modular \nveri.cation of a stream compaction kernel, a key building block in GPU programming;   // data, out, \nidx: arrays in shared memory procedure compact(data, out) // (i) test each element with predicate p in \nparallel for each thread tid flag[tid] = p(data[tid]) // (ii) compute indices for compaction idx = prescan(flag) \n// (iii) compaction in parallel for each thread tid if (flag[tid]) out[idx[tid]] = data[tid] Figure \n1. Stream compaction program and example (image courtesy M. Harris [18]) An implementation of barrier \ninvariants in the GPUVer\u00adify tool, making it the .rst tool capable of verifying data\u00addependent GPU kernels, \na major step forward from the capabilities of existing veri.ers [2, 24, 25];  Experimental results showing \nthat GPUVerify is capa\u00adble of scalable analysis where existing veri.cation tech\u00adniques cannot be applied, \nand where exhaustive symbolic execution is often infeasible [26, 27].  2. Motivating Example: Stream \nCompaction Figure 1 presents pseudo-code for a stream compaction al\u00adgorithm: a parallel program that \n.lters an input array with respect to a predicate p. This parallel primitive is commonly used for removing \nredundant or dead elements from a data set and has many applications in GPU programming, includ\u00ading in \nparallel breadth .rst tree traversal, ray tracing and col\u00adlision detection [3]. For an input array data, \neach thread t tests its respective element data[t] against p and writes 1 to the temporary array flag \nif the element satis.es p and 0 otherwise. In the compact stage, each thread t, for which p(data[t]) \nholds, must write to an index of output array out such that all elements to be kept are written contigu\u00adously. \nThis index is the sum of the values in flag at indices 0 = i < t. The index can be computed using an \nexclusive pre.x sum operator, also known as a prescan [4, 18]. Given an array [x1, x2, . . . , xn] and \nan associative operator . with identity e, the prescan operator computes the sums of all pre.xes: [e, \nx1, x1 . x2, . . . , x1 . x2 . \u00b7 \u00b7 \u00b7 . xn-1]. For example, taking . and e to be + and 0, respectively, \nthe prescan of the array [3, 1, 7, 0, 4, 1, 6, 3] is [0, 3, 4, 11, 11, 15, 16, 22]. A kernel implementing \nstream compaction is data-depen\u00addent because the access pattern of a thread is determined by the values \nof the input array: not just the element that belongs to a thread, but also the flag result of all pre\u00adceding \nthreads. Determining data race-freedom depends on the correctness of the prescan and, in particular, \nthe con\u00adstraints the operation imposes on the shared array idx: if flag[s] = flag[t] = 1 for distinct \nthreads s and t (so that both s and t will write to the output array), then race-freedom requires that \nidx[s] = idx[t]. Figure 2 presents an OpenCL kernel for stream compac\u00adtion, to be executed by a group \nof n threads. The . an\u00adnotations that follow barrier() statements are barrier in\u00advariants that we will \nuse to prove the prescan speci.ca\u00adtion in Section 4. The id of a thread is denoted tid, and data, out, \nflag and idx are arrays declared in GPU shared memory. The prescan operation is performed inline, im\u00adplemented \nas an algorithm due to Blelloch [4]. Surveying several GPU code repositories (the AMD APP SDK2, the NVIDIA \nCUDA SDK3 and the SHOC,4 Rodinia [7] and Par\u00adboil [33] benchmarks) we found this prescan implementation \nused frequently. Although the algorithm works in-place over the idx ar\u00adray, it is helpful to see that \nthe correctness comes from view\u00ading the array as a tree. If the length n of idx is a power of two, the \narray can be viewed as a balanced binary tree of depth lg n. The example on the right of Figure 2, due \nto Blelloch [4], shows the state of idx as the algorithm pro\u00adceeds: colouring the elements that are updated \nat each iter\u00adation shows the tree traversed by the algorithm. Each itera\u00adtion of the upsweep and downsweep \ncan be identi.ed by the value of offset, which gives the distance between ver\u00adtices at that depth of \nthe tree, and d, which denotes the num\u00adber of active threads. The parallelism of the algorithm is due \nto the vertices at the same depth being updatable simultane\u00adously. The .gure shows the assigned thread \n(T0, T1, T2, T3) per sub-operation of the upsweep and downsweep. Note that the thread-to-element assignment \nchanges at different depths of the tree. a. The upsweep is a reduction working from the leaves of the \ntree up to the root. Pairs of vertices (idx[left], idx[right]) are summed until the root contains the \nfull reduction and other elements form partial sums. b. The downsweep combines the upsweep partial sums \nto give the prescan result. Initially, the identity 0 is inserted into the root, marked clear in step \n(4) of Figure 2. The downsweep then traverses down the tree. At each level of the tree, an active vertex: \n(i) copies its value to its left child idx[left] (the dotted arrow), and (ii) sums its value with the \nold value of its left child temp (this value is a partial sum generated by the upsweep), storing the \nvalue in its right child idx[right] (the black arrows).  x-1 The prescan ensures that idx[x] =flag[i] \nfor i=0 all x. The stream compaction kernel additionally guaran\u00adtees that the input is non-negative: \nflag[x] = 0 for all x. Together these imply that the output satis.es a monotonic property: for all x \n< y we have idx[x] + flag[x] = idx[y]. Hence, for all x = y, if flag[x] > 0 . flag[y] > 0 then idx[x] \n= idx[y], which suf.ces to prove race-freedom. 2 http://developer.amd.com/tools/heterogeneous-computing/ \namd-accelerated-parallel-processing-app-sdk/ 3 https://developer.nvidia.com/gpu-computing-sdk 4 https://github.com/spaffy/shoc/wiki \n // data, out, .ag, idx: arrays in shared memory unsigned offset, d, left, right, temp; // (i) test \neach element with predicate p flag[tid] = p(data[tid]); // (ii) compute indices for compaction barrier(); \n// .load if (tid < n/2) { idx[2*tid] = flag[2*tid]; idx[2*tid + 1] = flag[2*tid + 1]; } // (ii)(a) upsweep \noffset = 1; for (d = n/2; d > 0; d /= 2) {barrier(); // .us if (tid < d) { left = offset * (2 * tid + \n1) - 1; right = offset * (2 * tid + 2) - 1; idx[right] += idx[left]; }offset *= 2; } // code continues \nhere // (ii)(b) downsweep if (tid == 0) idx[n-1] = 0; for (d = 1; d < n; d *= 2) {offset /= 2; barrier(); \n// .ds if (tid < d) { left = offset * (2 * tid + 1) - 1; right = offset * (2 * tid + 2) - 1; temp = idx[left]; \nidx[left] = idx[right]; idx[right] += temp; }} barrier(); // .spec // (iii) compaction if (flag[tid]) \nout[idx[tid]] = data[tid];  Figure 2. Stream compaction as an OpenCL kernel using n threads with the \nprescan inline Blelloch [4] proves that his algorithm satis.es the pre\u00adscan speci.cation using induction \non the pre-order traversal of the tree. Our aim is to prove race-freedom of kernels such as stream compaction \nwhich use the prescan algorithm by direct source code analysis. We are not aware of any veri\u00ad.cation \ntechnology that allows direct veri.cation of source code for massively parallel kernels using induction, \nthus we cannot encode Blelloch s proof of correctness directly. In Sections 4 and 5 we show that the \nprescan speci.cation necessary to prove race-freedom of kernels that use this algorithm can be established \nusing barrier invariants; we also study the application of barrier invariants to two further pre.x sum \nalgorithms. 3. Barrier Invariants We now present our main novel contribution: barrier invari\u00adants, which \nallow precise reasoning about GPU kernels us\u00ading the two-thread reduction. We .rst present a simple ker\u00adnel \nprogramming language with barrier invariants and a con\u00adcrete semantics (Section 3.1). We then present \nan abstract semantics that employs the two-thread reduction, and prove a soundness result: if a kernel \ncan be proved data race-free with respect to the abstract semantics, then the kernel is race\u00adfree with \nrespect to the concrete semantics (Section 3.2). Finally, we discuss practical issues associated with \nimple\u00admenting barrier invariants ef.ciently in the GPUVerify tool (Section 3.3). Our presentation of \nbarrier invariants could be gener\u00adalised to an n-thread reduction for any n = 2. This gen\u00aderalisation \nis conceptually straightforward but notationally cumbersome. In practice we have not found examples where \nit is necessary to consider more than two threads simultane\u00adously to prove data race-freedom of a kernel, \nthus for clarity we make our presentation speci.c to the two-thread case. kernel ::= threads: number; \nmain: stmt; stmt ::= basic stmt | stmt; stmt | barrieriexpr basic stmt ::= name := expr | name := sh[expr] \n| sh[expr] := expr expr ::= constant literal | name | expr op expr name ::= thread identi.er tid | any \nvalid C name iexpr ::= constant literal | sh[iexpr] | name | name | iexpr op iexpr Figure 3. Syntax for \nour kernel programming language 3.1 Concrete operational semantics for GPU kernels In prior work we \nformally presented a GPU kernel program\u00adming language [2]. In order to present barrier invariants in \na self-contained manner, yet with minimal repetition of prior work, we consider a simple language for \nstraight-line GPU kernels exhibiting no control-.ow or procedure calls, ac\u00adcording to the syntax of Figure \n3. For detailed handling of additional language constructs, which is orthogonal to the issues associated \nwith barrier invariants, see [2]; our im\u00adplementation of barrier invariants in GPUVerify handles the \nOpenCL and CUDA languages in full.5 A kernel declares the number of threads that will execute (threads: \nnumber) and a (possibly compound) statement which is the body of the kernel. The set name denotes lo\u00adcal \nvariables of a kernel and includes the special read-only variable tid, the unique thread identi.er of \neach thread. Ker\u00adnel statements allow assignment to local variables, access to shared memory (sh), and \nbarrier synchronisation. Each barrier statement is annotated with an invariant . . iexpr; the meaning \nof which is formalised below. A barrier invariant . is implicitly quanti.ed over all pairs of 5 It is \ninteresting to note that while veri.cation condition generation for straight-line programs is straightforward \n(e.g., using weakest preconditions) and does not require invariant abstractions, the same is not true \nof straight\u00adline GPU kernels. This is because multiple threads are in .ight concurrently, updating shared \nmemory; barrier invariants are required to re.ect these updates when threads synchronise at barriers. \n ((sh, l, R, W ), v := e) . (sh, l[v . e l], R, W ) (T-ASSI G N ) ((sh, l, R, W ), v := sh[e]) . (T-RD) \n(sh, l[v . sh(e l)], R . {e l}, W ) ((sh, l, R, W ), sh[e1] := e2) . ll l (T-WR) (sh[e1 . e2], l, R, \nW . {e1}) Figure 4. Rules for thread execution of basic statements distinct threads, where a local variable \nv appears directly or as v if it refers, respectively, to the v of the .rst or second thread in a pair. \nFor example, if x is a local variable, the barrier invariant sh[x + tid] = sh[x + tid] can be read as \n. s = t : sh[xs + s] = sh[xt + t], where xs and xt refer to the local variable x of threads s and t, \nrespectively, and where tid is replaced by s and tid by t. We do not allow quanti.ers to appear explicitly \nin barrier invariant expressions. In principle we could drop this restric\u00adtion, but as discussed in the \nintroduction a key advantage of the two-thread reduction is that it avoids the need to reason about quanti.ers. \nConcrete semantics Let P be a kernel executed by n threads, and let Word refer to the set of all memory \nwords, which we assume provides a representation for Boolean and bit-vector data. A thread state for \nP is a tuple (sh, l, R, W ) . ThreadStates where: sh : N . Word is the shared memory of the kernel; l \n: name . Word is the storage for the local variables of the thread; R, W . N are read and write sets \nrecording the shared addresses the thread has accessed since the last barrier. The read and write sets \nare used in the operational semantics to perform race checking. A kernel state S for P is a tuple (sh, \n(l0, R0, W0), . . . , (ln-1, Rn-1, Wn-1)) such that lt(tid) = t for all 0 = t < n, and where sh is the \nshared memory of the kernel and (sh, lt, Rt, Wt) is the thread state associated with thread t for all \n0 = t < n. We refer to the shared memory of kernel state S as S.sh and use S(t) to denote the thread \nstate (sh, lt, Rt, Wt) of thread t; we also write S(t).l, S(t).R, and S(t).W to refer to the thread-speci.c \ncomponents of this thread state. A state S is said to be a valid initial state of P if S(t).R = S(t).W \n= \u00d8 for all 0 = t < n. The rules of Figure 4 de.ne the evolution of thread states whilst executing a \nbasic statement. Evaluation of a local expression e given a local store l is denoted el. The rules de.ne \nlocal variable and shared state updates in a standard manner; in addition, T-RD and T-WR record in R \nand W , respectively, the shared locations that are read and written. . 0 = t < n : (S(t), basic stmt) \n. st race(s0, . . . , sn-1) (K-RAC E ) (S, basic stmt; ss) .k error . 0 = t < n : (S(t), basic stmt) \n. st \u00acrace(s0, . . . , sn-1) ' sh = merge(s0, . . . , sn-1) . 0 = t < n : S'(t) = (sh', st.l, st.R, \nst.W ) (K-STE P ) (S, basic stmt; ss) .k (S' , ss) . 0 = s S = t < n : \u00ac[.]s,t (K-BA R-ER R) (S, barrier.; \nss) .k error . 0 = s S ' = t < n : [.]s,t . 0 = t < n : S(t) = (S.sh, S(t).l, \u00d8, \u00d8) (K-BAR -IN V ) (S, \nbarrier.; ss) .k (S' , ss) Figure 5. Rules for lock-step execution of a kernel Figure 5 de.nes the operational \nsemantics of kernels, where error is a designated error state. If execution is guar\u00adanteed to abort when \na data race occurs then for veri.cation purposes it suf.ces to consider a single arbitrary schedule of \nthread execution between a pair of barriers [2, 11, 25, 26]. In Figure 5 threads execute in lock-step: \nall threads exe\u00adcute the .rst statement of the kernel, then all threads execute the second statement, \netc. In rules K-RAC E and K-ST E P, s0, . . . , st-1 are the thread states reached by each thread on \nexecuting the given basic statement according to the rules of Figure 4. The race predicate is used to \ncheck whether exe\u00adcution of this basic statement would cause a data race, indi\u00adcated by a con.ict between \nthe read/write sets of the threads: race(s0, . . . , sn-1) \u00a3 . 0 = s = t < n : (ss.R . ss.W ) n st.W \n= \u00d8 . If a race occurs, rule K-RAC E causes execution to go to error . Otherwise, rule K -S T E P allows \nthe kernel to transi\u00adtion to a new kernel state where the local store and read/write sets for thread \nt are taken from thread state st, and where the shared state is derived by merging the shared state associated \nwith each thread state st according to the write sets: merge(s0, . . . , sn-1)(z) \u00a3 st.sh(z) where t \nis such that z . st.W and t = 0 otherwise. Observe that there is at most one t with z . st.W if there \nwas no data race. On reaching a barrier, K-BA R -E R R and K -BA R -IN V check if the associated barrier \ninvariant holds for all dis\u00adtinct pairs of threads. The valuation of barrier invariant . with respect \nto state S and threads s and t is denoted [.]s,t S , and de.ned as:  [constant literal s,t = constant \nliteral S s,t [sh[iexpr] s,t = S.sh([iexpr]) S S name s,t = S(s).l(name) S s,t [ name = S(t).l(name) \nS s,t s,t s,t [iexpr1op iexpr2 ] = [iexpr1]op [iexpr2] SS S Note that a variable v occurring as v, respectively \nv, is evaluated in the context of thread s, respectively t. Rule K-BAR-ERR causes execution to go to \nerror if the invariant does not hold for some pair of threads. Otherwise, rule K-BAR-INV resets the read/write \nsets for all threads and allows execution to proceed beyond the barrier.  3.2 Two-thread reduction with \nbarrier invariants We now de.ne an abstract semantics for a kernel P with respect to a pair of distinct \nthreads (s, t). Using As,t(P ) to denote P interpreted with respect to this abstract semantics, we show \nthe following: Theorem 3.1 (Soundness). Let P be a kernel executed by n threads. If for every pair 0 \n= s = t < n, no execution of As,t(P ) from a valid initial state leads to error, then no execution of \nP from a valid initial state leads to error. Hence, it suf.ces to prove data race-freedom, and validity \nof barrier invariants, with respect to the abstract semantics. Before describing the abstract semantics \nformally and proving Theorem 3.1, we describe the abstract semantics in an intuitive manner. The semantics \nmodels a pair of dis\u00adtinct threads (s, t) executing the kernel. When executing a sequence of statements \nbetween two barriers, it is as if s and t are the only threads executing the kernel. They perform lo\u00adcal \nupdates and access the shared state, recording all shared locations that are accessed in their read/write \nsets. The read\u00ad/write sets are used to detect data races between s and t: if a data race occurs, execution \naborts. On reaching a barrier, a check is made to determine whether the invariant . associated with the \nbarrier holds for the pair (s, t). This check must take into account possi\u00adble updates to the shared \nstate made by additional threads executing the kernel. We handle this by considering whether a shared \nmemory location v was accessed by s or t since the last barrier. There are two key cases: v was accessed \nby at least one of s, t: We say that v is a known location. In this case it is sound to assume that v \nhas not been modi.ed by any thread r / . {s, t}. This is because our analysis considers all possible \npairs of threads: if r can write to v then because some x . {s, t}accesses v a data race will be detected \nfor the pair (x, r) and the kernel will not be deemed correct.  v was not accessed by s or t: We say \nthat v is an unknown location. In this case we must consider that v could have been modi.ed by some thread \nr /  . {s, t}. A safe approximation is to assume that v contains an arbitrary value when evaluating \nthe barrier invariant. If there exists an assignment to unknown locations yield\u00ading a state in which \n. does not hold for (s, t), execution aborts: there may be a concrete execution that would lead to this \nstate. Thus with the two-thread reduction, user-provided barrier invariants are not taken on trust they \nare checked. If . can be shown to hold for the pair (s, t) for all assign\u00adments to unknown locations \nthen, because (s, t) is arbitrary, it is sound to assume . holds for all pairs of distinct threads. Abstract \nexecution for the pair (s, t) continues after the bar\u00adrier by transitioning to an abstract state in which: \n the local stores for (s, t) and the values of known loca\u00adtions are preserved;  the read and write sets \nfor (s, t) are cleared;  . is assumed to hold for all pairs of distinct threads.  Armed with this intuition, \nwe now formally present the abstract semantics. Abstract semantics A thread state is de.ned as in Sec\u00adtion \n3.1. An abstract kernel state T for P with respect to threads s and t is a tuple (sh, (ls, Rs, Ws), (lt, \nRt, Wt)). An abstract kernel state is similar to a (concrete) kernel state, except that only the states \nof threads s and t are represented. We use T (s) and T (t) to denote the thread states of s and t. We \nwrite knowns,t(T ) for the set of shared locations collec\u00adtively accessed by s and t since the last barrier: \nknowns,t(T ) \u00a3 T (s).R . T (s).W . T (t).R . T (t).W . The key aspects of abstract kernel execution are \nde.ned by the rules of Figure 6; we omit abstract versions of rules K-RACE and K-STEP of Figure 5 which \nare de.ned in the obvious way by restricting these rules to two threads. On reaching a barrier, A-BAR-ERR \nand A-BAR-INV check whether the barrier invariant . holds in all concrete states that agree with the \ncurrent abstract state on the local stores of s and t and on the values of known shared locations. Formally, \nwe de.ne a concretisation operator .s,t yielding the set of such concrete states given an abstract state \nT : .s,t(T ) \u00a3 {S a concrete kernel state | S(s).l = T (s).l . S(t).l = T (t).l  .(S.sh(v) = T.sh(v))} \nv.knowns,t(T ) Note that the state of the read/write sets of s and t are not preserved by .s,t; our semantics \ndoes not require this. Rule A-BAR-ERR aborts if there exists a concrete state S . .s,t(T ) such that \nthe barrier invariant does not hold in S for the pair (s, t). If the barrier invariant holds for (s, \nt) in every state of .s,t(T ) then execution proceeds past the barrier by rule A-BAR-INV. A concrete \nstate S' . .s,t(T ) is chosen such that the barrier invariant . holds in S' for every  S . .s,t s,t \n(T ) S \u00ac[.] (A-BA R-ER R ) (T , barrier.; ss) .k error . S . .s,t s,t . .s,t (T ) : [.](T ) S S ' x,y \n' = as,t ' . 0 = x = y < n : [.]T (S ) S (A-BAR -IN V ) (T , barrier.; ss) .k (T ' , ss) Figure 6. Barrier \nrules for two-threaded execution ' pair of distinct threads (x, y). The abstract state T resulting from \nbarrier synchronisation is obtained by projecting S ' with respect to threads s and t while emptying \nthe read/read sets of s and t and discarding the local stores and read/write sets of all other threads. \nFormally this is described using an abstraction operator as,t: as,t(S) \u00a3 (S.sh, (S(s).l, \u00d8, \u00d8), (S(t).l, \n\u00d8, \u00d8)) . We explain in Section 3.3 how the universal quanti.ers used by rule A-BA R -I N V are eliminated \nin practice. Theorem 3.1 now follows by contradiction assuming that P has a race or violates a barrier \ninvariant while As,t(P ) does not. The argument hinges on the fact that any step of P not ending in error \ncan be mimicked by As,t(P ), due to the lack of data races and barrier invariant violations up to the \nstep. We present a formal proof in Appendix A.  3.3 Integrating barrier invariants with GPUVerify We \nhave implemented support for barrier invariants in GPU-Verify [2], our veri.cation tool for OpenCL and \nCUDA ker\u00adnels which is built on top of the Boogie veri.er [1] and Z3 SMT solver [15]. GPUVerify transforms \na kernel annotated with barrier invariants into a sequential program that models the execution of two \nthreads using the abstract semantics of Section 3.2. Our implementation fully supports loops, condi\u00adtionals \nand procedures using lock-step predicated execution as described in [2, 12]. We describe what we believe \nare the most interesting aspects associated with integrating barrier invariants with GPUVerify. Quanti.er \nelimination Rule A-BA R -IN V of Figure 6 uses quanti.ers to (a) consider all concretisations of the \ncurrent abstract state when checking the barrier invariant, and (b) select a successor state in which \nthe barrier invariant holds for all pairs of distinct threads. To avoid the need to reason directly about \nquanti.ers we now explain how quanti.ers are eliminated in practice. Elimination of the quanti.er for \nconcretisation Because barrier invariant expressions are quanti.er-free, a barrier in\u00advariant . refers \nto a .nite, typically small, number of shared memory locations. Let d be the largest number of syntac\u00adtically \ndistinct shared memory accesses appearing in any single barrier invariant in a given kernel. We introduce \nd auxiliary variables, u1, . . . , ud, which track at most d un\u00adknown shared memory addresses. At the \nstart of execution, and after each barrier, these variables are set nondetermin\u00adistically to re.ect the \nfact that neither thread has accessed any shared memory location since the last barrier. After each shared \nmemory access to a location computed from expres\u00adsion e, a statement assume( = e) is inserted to 1=i=d \nui re.ect the fact that this location is now in the known set. For a barrier invariant ., let e1, . . \n. , ef be the syntactically distinct sub-expressions of . that refer to the shared state, where f = d. \nFor ease of explanation, assume that there is no nesting between these sub-expressions. Let . ' be identi\u00adcal \nto . except that each occurrence of ei is replaced with ite(ei = ui, *, ei), where ite is the if-then-else \noperator this evaluates to ei unless ei is equal to the ith unknown location, in which case evaluation \nis nondeterministic. The formula . ' is then checked for the pair of threads under con\u00adsideration. This \ntransformation ensures that veri.cation only succeeds if the truth of a barrier invariant is independent \nof unknown shared locations, which is what rules A-BA R -E R R and A-BA R -IN V of Figure 6 require. \nWe handle nested sub\u00adexpressions via a straightforward extension of this method. As noted above, this \napproach relies on barrier invariants being quanti.er-free. If a barrier with invariant . follows a loop \nand the loop iterates a large or statically unbounded number of times then, without using quanti.ers, \nit may not be practical or possible, respectively, for . to refer to the set of all shared locations \naccessed during loop execution. We have not thus far found a practical example where this limitation \nis problematic. Elimination of the quanti.er for successor state selection Rule A-BA R -I N V allows \nexecution to continue after a bar\u00adrier by choosing a successor state in which the barrier invari\u00adant \nis assumed to hold for all pairs of distinct threads. Clearly it is sound to weaken this assumption to \nthe case where the invariant holds only for a selected subset of thread pairs that are relevant to the \nstatements following the barrier. However, if too small a subset is chosen, the assumption following \nthe barrier may not be strong enough to prove data race-freedom of statements following the barrier, \nor to establish successive barrier invariants. In practice we have found that it is possible to derive \na small subset of pairs suf.cient for veri.cation to succeed, avoiding the need for quanti.ers. GPUVerify \nrequires a set of instantiation expressions to be provided with each barrier invariant; eliminating the \nneed for quanti.cation. For exam\u00adple, to state that a barrier invariant should be assumed only for the \npair of threads under consideration and the pair con\u00adsisting of their immediate right neighbours (if \nthey exist), we would specify the set {(tid, tid), (tid + 1, tid + 1)}. In Section 4.1 we illustrate \na set of instantiation expressions for one of the Blelloch prescan invariants. Modular and staged veri.cation \nGPUVerify supports rea\u00adsoning about kernels with loops and procedure calls through loop invariants and \nprocedure contracts; we have imple\u00admented front-end support for this using the CLANG/LLVM compiler framework. \nFor data-dependent kernels, it may be necessary for barrier invariant-style expressions of the form iexpr \nto occur in loop invariants, referring to the shared state and variables of the form v. In this case, \nthe loop invariant is established by checking that such an expression holds for the pair of threads (s, \nt) under consideration. However, be\u00adcause threads do not necessarily synchronise at the head of a loop, \nduring loop abstraction such a loop invariant may only be instantiated for the pair (s, t), and not for \nadditional pairs of threads. A similar argument applies when assuming that a post-condition of a procedure \nholds when replacing a procedure with its speci.cation.  GPUVerify also supports staged veri.cation: \nif race\u00adfreedom of a procedure can be established using a simple set of barrier invariants then it is \nsound to prove a post-condition of the procedure using a richer set of barrier invariants un\u00adder the \nassumption of race-freedom. Thus the richer barrier invariants and the post-condition can be checked \nwithout the burden of simultaneously performing data race analysis. In Section 5 we discuss how modular \nand staged analysis are used in verifying race-freedom for the stream compac\u00adtion kernel of Section 2. \n4. Barrier Invariants for Stream Compaction We are now equipped to tackle the data-dependent stream compaction \nkernel of Figure 2. After establishing some no\u00adtation, we explain how barrier invariants are derived \nfor the Blelloch prescan which is at the core of the stream com\u00adpaction kernel (Section 4.1). The required \nbarrier invariants are intricate, capturing the key properties of Blelloch s al\u00adgorithm. We do not envisage \nautomatic inference of such in\u00advariants, and argue that manual speci.cation of invariants for key library \nfunctions such as pre.x sums is worthwhile due to their widespread use. Our veri.cation technique automati\u00adcally \nchecks barrier invariants, thus these complex assertions are not taken on trust by the tool (this is \nin contrast to related work on thread contracts [21], see Section 6). We brie.y discuss two further widely \nused pre.x sum al\u00adgorithms for which we have derived barrier invariants (Sec\u00adtion 4.2), and provide some \ninsights from our experience de\u00adriving barrier invariants for these examples. Preliminaries For presentation \npurposes we separate the idx array used by both the upsweep and downsweep phases into two arrays: a sum \narray used in the tree reduction of the upsweep and a prescan array used by the downsweep that will contain \nthe expected output of the prescan. In our exper\u00adiments (Section 5), veri.cation is performed on unmodi.ed \nsource code, which does not make use of this simpli.cation and instead introduces sum as a ghost variable \n(an auxiliary variable used only for veri.cation): the upsweep and down\u00adsweep work over the same array \nand we take a snapshot of the array in-between the two loops and store it in sum. The speci.cation we \nwill prove is that the output is monotonic: for all threads s < t, prescan[s] + flag[s] = prescan[t]. \nFor unsigned (i.e., non-negative) inputs we can use barrier invariants to prove this speci.cation under \nthe assumption that addition of unsigned integers does not over\u00ad.ow. Without this assumption the speci.cation \ndoes not hold: for suf.ciently large inputs, the additions performed during the prescan will over.ow, \nleading to unexpected re\u00adsults in the idx array, and thus erroneous accesses to the out array. In applications, \nstream compaction is used un\u00adder the implicit assumption that the inputs will not lead to over.ow, thus \nour assumption is pragmatic. In Section 5 we discuss how this assumption is realised in practice for \nour experimental evaluation. As discussed in Section 2, although the prescan algorithm works in-place \nover the same idx array, we can view the upsweep and downsweep as working over a logical tree where we \ncan identify each iteration of the upsweep and downsweep by the value of the offset variable, which gives \nthe distance between vertices at that depth of the tree. In Figure 7(c) we show that a vertex of the \ntree can be speci.ed as an (element, offset)-coordinate (x, .) using the predicate isvertex(x, .) \u00a3 (x+ \n1) mod . = 0. For example, (5, 2) is a vertex as element x = 5 is updated when offset . = 2, while (5, \n4) is not a vertex. We will use the indexing functions ai(tid, .) \u00a3 .(2\u00b7tid+1)-1 and bi(tid, .) \u00a3 .(2\u00b7tid+2)-1, \nwhich give the indices of the left and right child vertices for a thread tid at offset .. For example, \nai(1, 2) = 5 and bi(1, 2) = 7 (see Figures 2(a) and (b)). We note that the thread-private variables d \nand offset are used in both the upsweep and downsweep loops and ad\u00additionally are uniform between threads: \nall threads agree on the value of these variables at each barrier. This prop\u00aderty is easily encoded using \nthe barrier invariant d = d . offset = offset and is important because the barrier in\u00advariants that follow \nuse the offset variable as a measure of progress of the upsweep and downsweep. GPUVerify infers such \nuniform invariants automatically using static analysis. The variable offset is always a power of two \nand ranges from 1 to n in the upsweep loop and back from n to 1 in the downsweep loop. 4.1 Barrier invariants \nfor the Blelloch prescan We use barrier invariants to establish equalities between the shared arrays \nflag, sum and prescan: the .us invariant, in the upsweep loop, gives equalities between elements of sum \nand flag, while the .ds invariant, in the downsweep loop, gives equalities between elements of prescan \nand sum. The tables in Figures 7(a) and (b) give the set of equalities given by the barrier invariants \nfor n = 8 at the end of their re\u00adspective loops. The monotonic speci.cation, expressed as the invariant \n.spec for the .nal barrier, is then a combination of the relevant earlier equalities. We discuss each \nbarrier invariant in turn.  offset 1 2 4 8 sum[0] = flag[0] sum[1] = flag[1] + sum[0] sum[2] = flag[2] \nsum[3] = flag[3] + sum[2] + sum[1] sum[4] = flag[4] sum[5] = flag[5] + sum[4] sum[6] = flag[6] sum[7] \n= flag[7]  + sum[6] + sum[5] + sum[3] (a) Upsweep equalities given by .us prescan[0] = e prescan[4] \n= sum[3]  prescan[1] = sum[0] prescan[5] = sum[3] + sum[4] (c) Tree structure of the algorithm. Right \nchild vertices are shaded. prescan[2] = sum[1] prescan[6] = sum[3] + sum[5] Each vertex (x, .) is labelled \nwith the summations formed as prescan[3] = sum[1] + sum[2] prescan[7] = sum[3] + sum[5] + sum[6] the \ndownsweep proceeds, where a, b, . . . denotes prescan[x] = (b) Downsweep equalities given by .ds sum[a] \n+ sum[b] + \u00b7 \u00b7 \u00b7 and e denotes the identity. Figure 7. Upsweep and downsweep equalities for n = 8 and \nthe tree structure of the algorithm Load invariant The stream compaction kernel is de.ned over n threads, \nhowever, only n/2 threads are required for the prescan. After a parallel test of each element using n \nthreads, we synchronise with a barrier so n/2 threads can continue with the prescan. The state that we \nneed to carry through this barrier is .load \u00a3 sum[tid] = flag[tid] . Upsweep invariant The upsweep is \na reduction working from the leaves of a tree up to its root. At each iteration of the upsweep, each \nparent vertex is given the sum of its left and right child vertices. The upsweep proceeds until the root \nof the tree contains the full reduction and other elements form partial sums: each vertex of the tree \nwill contain the sum of the leaf vertices below it in the tree. In Figure 7(c) we show the tree structure \nfor the concrete case where n = 8 and we shade right child vertices (for the moment, ignore the labels \nof each vertex, which will be used in the downsweep). The upsweep loop has the loop invariant: (d = 4 \n. offset = 1) . (d = 2 . offset = 2) . (d = 1 . offset = 4) . (d = 0 . offset = 8) where each conjunct \ncharacterises a loop iteration (and the last conjunct corresponds to the .nal time the loop head is evaluated \nand the loop exits). By considering the state of the array sum at the start of each iteration we can \nconstruct a barrier invariant .us for the barrier inside the loop body. The columns of Figure 7(a) summarise \nthe per-element equalities formed by the upsweep as the offset changes. Informally, the summations of \nan element x at offset . can be found by traversing the tree from the vertex (x, .) to the leaf vertex \n(x, 1) of the element: if a right child vertex (x, . ' ) is encountered the term sum[x - . ' ] is added \nto the summation. Furthermore, an index x - . ' is a summation term iff isvertex(x, 2. ' ) holds. This \ngives us the following per-element invariant upsweep, which captures the state of any vertex (x, .) of \nthe tree. upsweep(x, .) \u00a3 sum[x] = flag[x] +sum[x - . ' ] . . .{2j |0=j<lg .}isvertex(x,2. ) Using this \nper-element invariant, we de.ne the barrier invariant .us by considering the elements of sum known to \na thread tid in each iteration of the upsweep loop with offset .. At loop entry, when offset = 1, each \nthread tid < n/2 knows about exactly two elements: ai(tid, 1) and bi(tid, 1). In subsequent loop iterations \nwe have an un\u00adeven distribution of elements over threads as more threads become disabled while the upsweep \nproceeds. For each pre\u00advious offset value . ' (< .), each thread tid < n/. ' will continue to know about \nits left child at that depth of the tree, i.e., ai(tid, . ' /2), because this vertex will have no fur\u00adther \nsummation terms. For the current offset value ., each thread tid < n/., i.e., each thread active in the \niteration of the upsweep just completed, will know about its left and right vertices: ai(tid, ./2) and \nbi(tid, ./2). Thus .us is de.ned as tid < n/2 . (upsweep(ai(tid, 1), 1) . upsweep(bi(tid, 1), 1)) in \nthe case . = 1, and as (tid < n/. ' . upsweep(ai(tid, . ' /2), .)) . .{2i|1=i=lg .} . (tid < n/. . upsweep(bi(tid, \n./2), .)) otherwise.  Downsweep invariant The downsweep combines the par\u00adtial sums formed in the upsweep \nto give the expected pre\u00adscan result. For presentation purposes we show the down\u00adsweep operating over \nan array prescan different from the array sum used in the upsweep, where initialisation is such that \nprescan[x] = sum[x] for all elements x. The downsweep traverses the tree from the root to the leaves \nafter clearing the root with the identity element e. At each iteration of the downsweep, each vertex \n(i) copies its value to its left child and (ii) sums its value with the old value of the left child into \nthe right child. This means that a vertex is only summed into when it is a right child; otherwise, it \nreceives the value of its parent vertex. In Figure 7(c) we consider the concrete case where n = 8. We \nshade right child vertices and label each vertex with the summation terms of each vertex after the downsweep \nhas processed that level of the tree. For example, the root (7, 8) is labelled with e to denote that \nit is equal to the identity (i.e., prescan[7] = e), when offset = 8. At the next downsweep iteration, \nwhen offset = 4, only (7, 8) is a vertex. The vertex copies its value e into its left child (3, 4) and \nsums this value with the previous value of its left child, i.e., sum[7] + sum[3] = e + sum[3] = sum[3], \nstoring the result in its right child (7, 4). In the .gure, we write this summation by labelling the \nvertex (7, 4) with 3 . In the next iteration, when offset = 2, vertex (7, 4) copies its value sum[3] \ninto its left child (5, 2) and sums this value with the previous value of its left child, i.e., prescan[7] \n+ sum[5] = sum[3] + sum[5] into its right child (7, 2). Hence, in the .gure, the left (5, 2) and right \n(7, 2) child vertices are respectively labelled 3 and 3, 5 . The remaining vertices are similarly computed. \nNow consider the leaf vertices of the tree, which are the .nal output of the prescan. We note that the \nnumber of terms in the summation for an element x is the number of right child vertices in the path from \nthe root vertex to the leaf vertex. For example, element 5 has two right child vertices on the path from \nthe root vertex (7, 8) to its leaf vertex. Informally, the summations of an element x at offset . can \nbe found by traversing the tree from the vertex (x, .) to the root and gathering terms: if a right child \nvertex (x ' , . ' ) is encountered then add the term sum[x ' - . ' ] to the summa\u00adtion. This observation \nleads to the following invariant, which de.nes the elements of prescan in terms of the offset .: downsweep(x, \n.) \u00a3 prescan[x] . .B(x,.) sum[y(x, . ' )] if isvertex(x, 2.) = sum[x] otherwise where B(x, .) \u00a3 {2i \n| lg . = i < lg n . bit(x, i) = 1} and y(x, . ' ) \u00a3 x - . ' + 2j 0=j<lg . ,bit(x,j)=0 We exploit the \nfact that the binary encoding of x may be interpreted as the path from the root to the element leaf vertex: \nusing 0 to mean left and 1 to mean right as we read from the most-to-least signi.cant bit (see also Figure \n7(c)). Finally, we de.ne the invariant .ds by considering the elements of prescan accessed by the thread \ntid in each iteration of the downsweep loop with offset .: .ds \u00a3 tid < n/2i+1 . . = l2i/2J 0=i<lg n \n . downsweep(ai(tid, 2i), .) . tid < n/2i+1 . . N l2i/2J 0=i<lg n . downsweep(bi(tid, 2i), .) where \nN is de.ned as = if 2i = n/2, and as = otherwise. Speci.cation invariant The result of the prescan is \nex\u00adpressed in the .nal barrier invariant: .spec \u00a3 tid < tid . (prescan[tid] + flag[tid] = prescan[tid]) \nThis is a simple consequence of the equalities formed by .us and .ds, i.e. at . = n for .us and . = 0 \nfor .ds. For example, consider the case tid = 1 and tid = 5. Then we must show prescan[1] + flag[1] = \nprescan[5]. By the downsweep equalities, given in Figure 7(b), we can rewrite this equation as: sum[0] \n+ flag[1] = sum[3] + sum[4]. Then, by the upsweep equalities, given in Figure 7(a), and cancellations, \nthis becomes 0 = sum[2] + flag[3] + sum[4], which holds given unsigned (and thus non-negative) inputs. \nInstantiation expressions As discussed in Section 3.3, GPUVerify requires a set of instantiation expressions \nto be speci.ed for each barrier invariant. As an example, we brie.y discuss instantiation expressions \nfor the upsweep bar\u00adrier invariant, .us. In the upsweep of Figure 2, we see at each iteration that a \nthread tid uses the results produced by threads 2 \u00b7 tid and 2\u00b7tid+1 from the previous iteration. For \nexample, thread 1 at offset 2 uses the sums formed by 2 and 3 when the upsweep was at offset 1. Therefore, \nit suf.ces to instantiate .us for threads 2 \u00b7 tid and 2 \u00b7 tid + 1 after the upsweep barrier; instantiating \nthe invariant for further threads does not add any information that is useful for veri.cation.  4.2 \nOther pre.x sum algorithms We have derived barrier invariants for two further pre.x sum algorithms which, \nthrough our survey of the CUDA and AMD SDKs and SHOC, Rodinia and Parboil benchmarks (see Section 2), \nwe found to be widely used in GPU kernel programming: Brent-Kung [5] and Kogge-Stone [23]. We brie.y \noutline the process of barrier invariant discovery for these algorithms which can also be used to perform \nstream compaction. The source code for these examples, annotated with barrier invariants, is available \nonline.6 6 http://multicore.doc.ic.ac.uk/tools/GPUVerify/OOPSLA13  Figure 8. Circuit for Kogge-Stone \npre.x sum Brent-Kung The Brent-Kung algorithm performs an inclu\u00adsive pre.x sum (also known as a scan) \nmeaning that the output is the sum of all inclusive-pre.xes: i.e., idx[x] = x flag[i] for all x. Like \nthe Blelloch prescan, the Brent\u00ad i=0 Kung scan consists of upsweep and downsweep loops. The barrier invariants \nfor this algorithm follow a similar pattern to those for Blelloch: we establish equalities for the upsweep \nand downsweep loop that can be combined into the mono\u00adtonic speci.cation. The Brent-Kung upsweep matches \nthe Blelloch upsweep so we reuse the same upsweep barrier in\u00advariant. The downsweep then collects partial \nsums to form the .nal result. Unlike the Blelloch downsweep, which op\u00aderates over a logical tree, the \nBrent-Kung downsweep works over a logical forest, which requires signi.cantly different barrier invariants. \nKogge-Stone Figure 8 describes the Kogge-Stone inclu\u00adsive pre.x sum as a circuit diagram for n = 8 elements. \nThere is a wire for each input and data .ows top-down through the diagram. Each node performs the binary \nas\u00adsociative operator on its two inputs and produces an out\u00adput that passes downward and also optionally \nacross the circuit (through a diagonal wire). The algorithm works by summing array elements from successively \nlarger power-of\u00adtwo offsets. In this algorithm we observe that the circuit always adds adjacent summation \nintervals. Initially, at off\u00adset 1, we have for each element x that idx[x] = flag[x] = x flag[i]. After \nthe .rst iteration, at offset 2, each ele\u00ad i=x x ment x = 1 has idx[x] = flag[i]. After the sec\u00ad i=x-1 \nond iteration, at offset 4, each element x = 2 has idx[x] = x flag[i]. For example, element 3 will have \nsummed i=x-3 3 1 idx[3] = flag[i] and idx[1] = flag[i] to be\u00ad i=2 i=0 3 come flag[i]. In general, we \nsee that each add has the i=0 bc c form flag[i] + flag[i] = flag[i]. i=a i=b+1 i=a The GPU kernel implementation \nof Kogge-Stone assigns one thread to each array element. Due to the tightly cou\u00adpled nature of threads, \nwe found it dif.cult to design a com\u00adpact barrier invariant to summarise the state of the idx ar\u00adray, \ncontrary to what we were able to do for the Blelloch and Brent-Kung pre.x sums. However, using the above \nobser\u00advation regarding the summations taking place, we were able to apply abstract interpretation [13] \nto the source code us\u00ading a domain of summation intervals, so that each element idx[x] is represented \nby an abstract element (a, b) denoting b flag[i]. Addition of adjacent intervals is de.ned as i=a (a, \nb) . (b + 1, c) = (a, c). This enables us to prove the scan speci.cation for this algorithm for any associative \noperator using simple barrier invariants.  4.3 Experience The barrier invariants we have derived for \nthe Blelloch, Brent-Kung and Kogge-Stone pre.x sum algorithms are in\u00adtricate. Their derivation was non-trivial \nand in the process we made numerous errors and encountered numerous omis\u00adsions. To identify such problems \nwe relied heavily on the fact that our method does not take user-supplied barrier in\u00advariants on trust, \nbut checks that they do indeed hold. The process of barrier invariant derivation required (a) a thorough \nunderstanding of how these algorithms operate and why they are functionally correct, and (b) a strong \nintuition for writing inductive invariants, gained through signi.cant prior experience using veri.cation \ntools such as Boogie [1]. Due to (b), we speculate that writing barrier invariants for complex algorithms \nis likely to be beyond the scope of gen\u00aderal GPU software developers who do not have a background in \nveri.cation. Nevertheless, we believe there is signi.cant value in applying barrier invariants to the \nveri.cation of im\u00adportant library functions such as pre.x sums, due to the wide use of such functions \nand the fact that the veri.cation effort need only be undertaken once. The speci.cation for the li\u00adbrary \nfunction may be useful in automatic veri.cation of a client application, as we demonstrate for the stream \ncompac\u00adtion example in our experimental evaluation (Section 5). There was some degree of re-use between \nbarrier invari\u00adants in our examples. The upsweep phases of Blelloch and Brent-Kung are identical, allowing \nidentical barrier invari\u00adants, and the process of deriving the Blelloch downsweep in\u00advariant yielded \ninsights which made derivation of the Brent-Kung downsweep invariant relatively straightforward. The \nKogge-Stone algorithm, on the other hand, required a dif\u00adferent approach, as discussed in Section 4.2. \nHaving identi\u00ad.ed a barrier invariant, we found that the necessary instantia\u00adtion expressions (as brie.y \ndiscussed at the end of Section 4) were straightforward to identify. 5. Experimental results We now present \nveri.cation results for analysing the stream compaction kernel and the Blelloch, Brent-Kung and Kogge-Stone \npre.x sum algorithms discussed in Sections 2 and 4. We evaluate the effectiveness of GPUVerify in (a) \nperform\u00ading modular veri.cation of the stream compaction kernel us\u00ading a speci.cation for the pre.x sum \nphase, and (b) the scal\u00adability of verifying the pre.x sums using barrier invariants. We compare GPUVerify \nwith two other GPU kernel analysis tools, GKLEE [26] and GKLEEp [27], which are based on the KLEE dynamic \nsymbolic execution engine [6]. GKLEE searches for bugs in GPU kernels through exhaus\u00adtive path analysis \nusing an explicit representation of threads.  GKLEEp tries to improve on the performance of GKLEE by \nreasoning about representative pairs of threads, introduc\u00ading fresh pairs to the analysis only when necessary. \nWhile the tools are not designed for veri.cation, they can perform brute-force veri.cation via exhaustive \npath exploration. We also attempted to provide a performance compari\u00adson with KLEE-CL [11], another KLEE-based \ntool, but were unable to do so due to bugs in KLEE-CL, which we have reported to the developers. Because \nGKLEE and KLEE-CL share many similarities, we expect that a com\u00adparison with KLEE-CL after bug-.xing \nwill yield similar results. We do not compare with existing veri.cation tools, PUG [25], the tool of \nLeung et al. [24], and GPUVerify prior to this work [2], as they do not support reasoning about data-dependent \nkernels. All experiments were performed on a compute cluster using nodes with Intel Xeon EP-2620 cores \nat 2GHz with 16GB RAM running RedHat Linux 6.3, rev. 2784 of Boogie, and Z3 v4.3.1. We used a timeout \nof 3 hours for each experi\u00adment. Times reported are averages over ten runs. GPUVerify and our benchmarks \nare available online to make our results reproducible. Veri.cation strategy We apply GPUVerify using \na mod\u00adular and staged veri.cation strategy, as discussed in Sec\u00adtion 3.3, exploiting the fact that the \npre.x sum phase of stream compaction is outlined into a procedure. Race-free\u00addom for the stream compaction \nis veri.ed by summaris\u00ading the pre.x sum procedure by its monotonic speci.cation. We consider three implementations \nof pre.x sum, using the Blelloch, Brent-Kung and Kogge-Stone algorithms (see Sec\u00adtion 4). In each case, \nwe verify race-freedom of the pre.x sum using trivial barrier invariants. This is because the pre\u00ad.x \nsums themselves are data-independent: data-dependence arises when the result of a pre.x sum is used for \narray in\u00addexing during stream compaction. Having established race\u00adfreedom for the pre.x sum, we use the \nbarrier invariants de\u00adscribed in Section 4 to prove the monotonic speci.cation, with race checking disabled. \nThe experimental results we present for GPUVerify all use this staged approach; we found that staging \nthese ver\u00adi.cation phases always accelerated veri.cation, yielding speedups of up to 4\u00d7. GKLEE and GKLEEp \nare based on exhaustive path exploration, thus do not support modular and staged analysis. Verifying \nstream compaction using the pre.x sum speci\u00ad.cation We considered verifying race-freedom for stream compaction, \nassuming the pre.x sum speci.cation, for all power-of-two thread counts from 2 to 231. GPUVerify sup\u00adports \nspeci.cation-based reasoning directly. Using GKLEE and GKLEEp a procedure can be summarised through the \nuse of assume commands, one per thread. Figure 9 shows for each thread count up to 215 the time in seconds \ntaken for analysis with GPUVerify, GKLEE and GKLEEp. For each thread count and tool we show the time \ntaken for analysis, in seconds, averaged over 10 runs. Be\u00adlow the time we show the variation observed \nbetween runs. Timeouts are indicated by TO . In all cases analysis with GPUVerify succeeded in less than \ntwo seconds, illustrating the power of modular analysis using the two-thread reduction; we obtained similar \nresults for thread counts up to 231. GKLEE is capable of analysis of up to 8 threads within our resource \nlimits, and GKLEEp scales further to 256 threads. Because GKLEE and GKLEEp use an explicit thread representation \nit is unsurprising that analysis does not scale to larger thread counts. We emphasise that GKLEE and \nGKLEEp were not designed for this sort of analysis; nevertheless, they provide the only source for comparison. \nFor large thread counts, signi.cant variation in analysis time is observed using GKLEEp. We attribute \nthis to various sources of nondeterminism arising from the KLEE execution environment [6]. Verifying \npre.x sum speci.cations For stream compac\u00adtion we are interested in performing a pre.x sum using the \n+ operator with identity 0. Because, as discussed in Section 4, the speci.cation does not hold in the \ncase where addition over.ows, we have added to GPUVerify support for a non\u00adover.owing bit-vector addition \noperator, which performs ad\u00addition of an n-bit bit-vector using n + 1 bits, and then injects an assume \nstatement to restrict analysis to the case where addition did not over.ow, indicated by the top bit of \nthe re\u00adsult. It was not possible to add similar support directly to GKLEE, thus for our experiments with \nGKLEE we model bit-vector addition as saturating. The monotonic speci.cation also holds with equivalent \nbarrier invariants when the operator is changed to max or | (bitwise-or). It is instructive to see how \nthe veri.cation cost varies according to the operator used. We found that GKLEEp could not be used to \nverify the monotonic speci.cation for these kernels: analysis led to false positive reports of the monotonic \npost-condition fail\u00ading. This is because, as mentioned brie.y in [27], the thread reduction employed \nby GKLEEp relies on a form of shared state abstraction similar to the adversarial abstraction dis\u00adcussed \nin Section 1. GKLEE on the other hand does not at\u00adtempt to perform thread reduction and so does not use \nany shared state abstraction, hence we are able to meaningfully compare GPUVerify with GKLEE. The graphs \nof Figure 10 show times for verifying (a) the Blelloch prescan algorithm and (b) the Brent-Kung scan \nal\u00adgorithm for increasing thread counts using GPUVerify (in\u00addicated by circles) and GKLEE (indicated \nby crosses). We consider each of the operators + (ADD), max (MAX) and | (OR), and consider 8-, 16-and \n32-bit integer representa\u00adtions: bvX indicates that integers are represented using X-bit bit-vectors. \nFor GPUVerify, each data-point is the total time to verify the algorithm using our staged veri.cation \nstrategy: i.e., the time to verify the algorithm is race-free (using trivial bar\u00adFigure 9. Experimental \nresults for modular analysis of the stream compaction kernel using GPUVerify, GKLEE and GKLEEp. Analysis \ntimes, in seconds, are averages over 10 runs, and the variation (95% con.dence interval) between runs \nis also shown. Timeouts are indicated by TO .  #Threads 4 8 16 32 64 128 256 512 1024 2048 4096 8192 \n16384 32768 GPUVerify GKLEE GKLEEp 1.47 \u00b10.04 0.80 \u00b10.08 0.57 \u00b10.01 1.43 \u00b10.01 76.95 \u00b10.27 1.03 \u00b10.04 \n1.43 \u00b10.05 TO 3.38 \u00b10.06 1.47 \u00b10.04 TO 9.40 \u00b10.19 1.39 \u00b10.02 TO 39.44 \u00b10.97 1.40 \u00b10.02 TO 220.89 \u00b15.01 \n1.38 \u00b10.02 TO 1838.71 \u00b1110.81 1.38 \u00b10.02 TO TO 1.41 \u00b10.02 TO TO 1.38 \u00b10.02 TO TO 1.39 \u00b10.03 TO TO 1.40 \n\u00b10.02 TO TO 1.41 \u00b10.02 TO TO 1.41 \u00b10.02 TO TO  ADD MAX OR ADD MAX OR (a) Blelloch prescan (b) Brent-Kung \nscan rier invariants) plus the time to verify the monotonic spec\u00adi.cation (with race checking disabled). \nFor GKLEE, each data-point is the time taken for exhaustive path analysis. For each tool, absence of \na data point indicates that a timeout oc\u00adcurred, or that the memory limit of our platform was reached. \nThe results show that for both Blelloch and Brent-Kung with the + and max, the two-thread reduction afforded \nby barrier invariants allows GPUVerify to scale to larger thread counts than with GKLEE, overcoming the \noverhead associ\u00adated with barrier invariants observed for small thread counts. The scalability of GPUVerify \nis particularly noticeable when integers are represented using 8-bits. For larger bit-widths the scalability \nof both tools degrades. For the | operator, which leads to simpler bit-vector rea\u00adsoning, both GPUVerify \nand GKLEE perform signi.cantly better than with + or max for both the Blelloch and Brent-Kung algorithms. \nGKLEE performs extremely well for small thread counts, outperforming GPUVerify by two orders of magnitude. \nAs the thread count increases, this performance gap closes to one order of magnitude or less, and the \nscal\u00adability of GPUVerify appears to be almost linear (note that although the y-axis is plotted to a \nlog scale, the number of threads on the x-axis also grows exponentially). Neverthe\u00adless, in four cases \nGKLEE is able to verify one larger thread con.guration than GPUVerify, before both tools exhaust re\u00adsource \nlimits. Looking at the log .les created by GKLEE, we found that analysis for the | operator required \nconsideration of just a single execution path regardless of the thread count, while analysis using + \nand max required exploring a num\u00adber of paths exponentially proportional to the thread count. This is \nbecause the application of saturating addition, or the max function, leads to a branch in the LLVM intermediate \nrepresentation on which GKLEE operates. In each case the branch is data-dependent on the operands of \nthe function, so each time this function is applied GKLEE must fork ex\u00adecution to consider all successive \nexecution states that can be reached via both branch outcomes. The | function does not lead to a branch, \nso this exponential path explosion is avoided. This is not an issue for GPUVerify, which does not perform \na per-path analysis.  As explained in Section 4.2, we applied source-level ab\u00adstract interpretation \nto the Kogge-Stone pre.x sum algo\u00adrithm. This leads to a description that is independent of the speci.c \nassociative operator that is used. Figure 11 plots ver\u00adi.cation times using GPUVerify and GKLEE for abstract \nKogge-Stone for varying thread counts, where integers are represented using 32-bit bit-vectors. Data \npoints for GPU-Verify and GKLEE are represented using circles and crosses, respectively. The graph shows \nthat veri.cation with GPUVerify scales extremely well: we were able to verify the example for all power-of-two \nthread counts up to 231 (thread counts up to 220 are shown in the .gure), and that analysis appears to \nbe insensitive to the number of threads. Using GKLEE, analysis was possible for up to 64 threads within \nour 3 hour timeout. This demonstrates that scalability that can be achieved by combining the two-thread \nabstraction with additional abstract reasoning. Scaling veri.cation for Blelloch and Brent-Kung using \nan abstract operator Inspired by the excellent scalability re\u00adsults for the abstract Kogge-Stone algorithm \n(Figure 11), we considered the use of abstraction to overcome the scalabil\u00adity limitations associated \nwith bit-vector reasoning for the Blelloch and Brent-Kung algorithms. For non-negative integers x, y \nand z and an operator . . {+, max, |} with identity 0, the barrier invariants for Blel\u00adloch and Brent-Kung, \nand the monotonic speci.cation, rely on the following properties: x . (y . z) = (x . y) . z (as\u00adsociativity), \nx . e = e . x = x (identity), and max(x, y) = x . y (upperbound). Motivated by this, we extended GPU-Verify \nwith an abstract binary operator . over bit-vectors, which maps to a Boogie uninterpreted function together \nwith a set of axioms encoding the associativity, identity and up\u00adperbound properties. Our hypothesis \nwas that by abstracting from the complexity of bit-vector addition we could prove the prescan speci.cation \nwith 32-bit bit vectors for larger thread counts. Unfortunately, specifying these axioms involved the \nuse of quanti.ers, and we found that the quanti.ed associativity axiom led to problems with the quanti.er \ninstantiation ap\u00adproach of Z3, which uses E-matching [14]. If a formula has a large expression that involves \n., the associativity axiom can be instantiated for any sub-tree of the form t1 .(t2 .t3) or t1 . (t2 \n. t3), and instantiation leads to duplication of each of the sub-trees t1, t2 and t3 in the formula. \nThus direct use of quanti.ers in describing properties of . led to rapid memory exhaustion. We used triggers \n[14] to carefully design a set of quan\u00adti.ed axioms for . which allow the barrier invariants for the \nupsweep and downsweep phases of the pre.x sum al\u00adgorithms to be checked. We have not managed to craft \ntrig\u00adgers suitable for proving the .nal monotonic speci.cation; we plan to investigate this further in \nfuture work. Figure 12 shows veri.cation times for checking the bar\u00adrier invariants .load, .us and .ds \n(see Section 4) but not the .nal speci.cation .spec, for {8, 16, 32}-bit bit vectors, with respect to \nconcrete operators +, max, |, and using the ab\u00adstract operator .. The results show that the abstract \nopera\u00adtor scales signi.cantly better than + and max, allowing the (pre)scans to be veri.ed for 32-bit \nbit vectors for up to 64 threads, compared with 8 threads using +. Furthermore, be\u00adcause all assumptions \nmade regarding . hold for the con\u00adcrete operators, veri.cation of the (pre)scans using . alone establishes \nthat these barrier invariants hold for all three op\u00aderators simultaneously. We could not apply GKLEE \nusing the abstract operator since the tool does not support the use of uninterpreted functions and quanti.ers. \n6. Related Work Formal analysis of GPU kernels We have discussed re\u00adlated work on formal analysis for \nGPU kernels [2, 11, 12, 24 27] in the introduction and through experimental com\u00adparison in Section 5. \nAmong this work, the closest to GPU-Verify is PUG [25]. It would be possible in principle to inte\u00adgrate \nbarrier invariants with the PUG veri.cation method in a similar manner to the approach taken here. A \nrecent method for functional veri.cation of GPU ker\u00adnels uses permission-based separation logic [19]. \nHere, a kernel must be annotated (currently manually) with an as\u00adsignment of read or write permissions \nto memory locations on a per-thread basis. Race-freedom is proven by showing that write permissions are \nexclusive. To reason about com\u00admunication at barriers, a barrier speci.cation is required to state how \npermissions are exchanged between threads at a barrier. Barrier speci.cations differ from barrier invariants \nin that they talk only about permissions, not about the data on which the kernel operates. The approach \nof [19] does not  (a) Blelloch prescan (b) Brent-Kung scan employ any thread-reduction abstraction; \ninstead, quanti.ers are used to reason about all threads. This method has not yet been automated, so \na systematic comparison with GPUVer\u00adify for realistic examples is not yet possible. Protocol veri.cation \nA reduction to two processes, similar to the two-thread reduction employed in GPU kernel veri.\u00adcation, \nis at the heart of a method for verifying cache co\u00adherence protocols known as CMP [9], which was inspired \nby the foundational work of McMillan [28]. With CMP, veri.cation of a protocol for an arbitrary number \nof pro\u00adcesses is performed by model checking a system where a small number of processes are explicitly \nrepresented and a highly nondeterministic other process over-approximates the possible behaviours of \nthe remaining processes. The un\u00adconstrained nature of the other process can lead to spu\u00adrious counterexamples, \nwhich must be eliminated either by introducing additional explicit processes, or by adding non\u00adinterference \nlemmas so that the actions of the other process more precisely re.ect the possible actions of processes \nin the concrete system. The CMP method has been extended and generalised with message .ows and message \n.ow in\u00advariants [34], which aid in the automatic derivation of non\u00adinterference lemmas by capturing large \nclasses of permissi\u00adble interactions between processes. Our approach uses the same high-level proof idea \nas the CMP method: we consider a small number of threads (two), and our default adversarial abstraction \nmodels the possible actions of all other threads, analogously to the other pro\u00adcess. The purpose of barrier \ninvariants is to re.ne the ad\u00adversarial abstraction so that the possible behaviours of ad\u00additional threads \nare represented more precisely, thus barrier invariants can be seen as analogues to non-interference \nlem\u00admas. However, the techniques aim to solve different prob\u00adlems: non-interference lemmas in the CMP \nmethod describe interaction sequences between processes in a message-based protocol, while barrier invariants \nfor GPU kernel veri.cation capture properties of the shared state in shared memory par\u00adallel programs, \nand thus there are many technical differences in the manner by which the high-level proof technique is \nap\u00adplied in practice. Other techniques for veri.cation of data-parallel programs A technique for proving \nrace-freedom of data parallel pro\u00adgrams using thread contracts is presented in [21]. With this approach \nthe user speci.es a coordination strategy: a log\u00adical annotation describing how threads will access shared \nmemory when executing a parallel region of code. An SMT solver is used to show that adherence to the \ncoordination strategy guarantees race-freedom. However, the work does not address the problem of proving \nthat a parallel program obeys its coordination strategy; if an erroneous coordina\u00adtion strategy is speci.ed \nthen the associated race-freedom proof cannot be trusted. The authors show how assertions can be generated \nto check coordination strategies at runtime, which is useful but provides no guarantees. Barrier invari\u00adants \ncan be viewed as a kind of coordination strategy tai\u00adlored towards analysis of GPU kernels. A key difference \nbe\u00adtween our contribution and that of [21] is that our veri.ca\u00adtion method checks the validity of barrier \ninvariants as well as using them to prove race-freedom: if erroneous barrier in\u00advariants are provided \nthen the veri.cation attempt will fail. Collective loop invariants are proposed in [32] to allow veri.cation \nof data parallel programs using symbolic execu\u00adtion, with applications to MPI. In the context of MPI, \na col\u00adlective loop invariant is an assertion speci.ed with respect to a set of processes I and a set \nof loop heads L, and is re\u00adquired to hold in any state where every process p . I is at a loop head in \nL. This facilitates reasoning about parallel programs where threads do not necessarily synchronise at \nloop heads. Like barrier invariants, collective loop invariants establish properties over sets of processes/threads, \nbut oth\u00aderwise the techniques are orthogonal: a barrier invariant is designed to capture shared state \nproperties when all threads in a GPU kernel synchronise at the same barrier; collective loop invariants \naim to capture the system state in MPI pro\u00adgrams where processes do not frequently synchronise. We believe \nthere is scope for process-modular reasoning about MPI programs using barrier invariants, complementing \nthe strengths of collective loop invariants.  Thread modular reasoning There has been much work on thread-modular \nreasoning for general-purpose concur\u00adrent programs, notably the Owicki-Gries [31] and rely\u00adguarantee \n[20] techniques. The two-thread reduction can be viewed as a form of thread-modular reasoning: for a \nGPU kernel executed by n threads, each thread is (implicitly) veri.ed with respect to n - 1 environment \nabstractions. In each such environment abstraction, a single additional thread is represented for pur\u00adposes \nof data race analysis, and the actions of further threads are considered by modelling the shared state \nabstractly. Bar\u00adrier invariants re.ne this environment model by providing a more precise representation \nof the shared state. The two-thread reduction with barrier invariants exploits the structure of data-parallel \nprograms where barriers are the only means of synchronisation. This leads to more com\u00adpact speci.cations \nthan are possible using traditional thread\u00admodular techniques which must take account of arbitrary thread \nsynchronisation. Staged veri.cation In Section 3.3 we discussed the use of staged veri.cation in GPUVerify. \nForms of staged analysis are often used in program veri.cation, usually for collabora\u00adtion between techniques. \nFor example, it is common to de\u00adrive simple program invariants using a sound abstract inter\u00adpreter, and \nthen to assume these invariants when applying a more heavy-weight veri.cation method (see, e.g., [17]). \nThe use of assumptions in collaborative veri.cation and testing has been the subject of recent work [10]. \n7. Conclusions and Future Work We have presented barrier invariants, a method for facilitat\u00ading race \nanalysis of data-dependent GPU kernels using the two-thread reduction. We have demonstrated the application \nof barrier invariants through a detailed case-study of stream compaction, and shown that our implementation \nfacilitates practical veri.cation of this important kernel for relatively large thread counts. In future \nwork we plan to apply barrier invariants more widely, verifying other data-dependent kernels such as \nradix sort, collision detection, eigenvalue computation and graph colouring. Based on this experience \nwe will investigate tech\u00adniques for automatically inferring auxiliary barrier invari\u00adants, so that users \ncan focus on the key invariants that are speci.c to the algorithm in hand. We also plan to investi\u00adgate \nthe application of barrier invariants to other data par\u00adallel programming models that employ barrier \nsynchronisa\u00adtion, including OpenMP and MPI, and to consider veri.ca\u00adtion of kernels that use atomic operations \nto avoid barrier synchronisation (building on existing results in this area us\u00ading GKLEE [8]). Our experiments \nshow that employing abstraction to avoid reasoning directly about arithmetic is promising, but our results \nare incomplete due to the challenges associated with quanti.er instantiation. We believe that progress \non this problem could have wide application in software veri.\u00adcation. Acknowledgments We are grateful \nto the following people for their insightful comments on various drafts of this work: Adam Betts, Pe\u00adter \nCollingbourne, Derek Graham, Marieke Huisman, Samin Ishtiaq, Kim Jarvis, Jael Kriener, Rustan Leino, \nCurtis Mad\u00adsen, Alastair Reid, Murali Talupur and John Wickerson. We are also grateful to Ganesh Gopalakrishnan \nand Peng Li for assisting us in using the GKLEE and GKLEEp tools. References [1] M. Barnett et al. Boogie: \nA modular reusable veri.er for object-oriented programs. In FMCO, pages 364 387, 2005. [2] A. Betts, \nN. Chong, A. F. Donaldson, S. Qadeer, and P. Thom\u00adson. GPUVerify: a veri.er for GPU kernels. In OOPSLA, \npages 113 132, 2012. [3] M. Billeter, O. Olsson, and U. Assarsson. Ef.cient stream compaction on wide \nSIMD many-core architectures. In HPG, pages 159 166, 2009. [4] G. E. Blelloch. Pre.x sums and their applications. \nIn J. H. Reif, editor, Synthesis of Parallel Algorithms. Morgan Kauf\u00admann, 1990. [5] R. P. Brent and \nH.-T. Kung. A regular layout for parallel adders. IEEE Trans. Computers, 31(3):260 264, 1982. [6] C. \nCadar, D. Dunbar, and D. R. Engler. KLEE: Unassisted and automatic generation of high-coverage tests \nfor complex systems programs. In OSDI, pages 209 224, 2008. [7] S. Che et al. Rodinia: A benchmark suite \nfor heteroge\u00adneous computing. In Workload Characterization, pages 44 54, 2009. [8] W.-F. Chiang, G. Gopalakrishnan, \nG. Li, and Z. Rakamaric. Formal analysis of GPU programs with atomics via con.ict\u00addirected delay-bounding. \nIn NFM, pages 213 228, 2013. [9] C.-T. Chou, P. K. Mannava, and S. Park. A simple method for parameterized \nveri.cation of cache coherence protocols. In FMCAD, pages 382 398, 2004. [10] M. Christakis, P. M \u00a8uller, \nand V. W \u00a8ustholz. Collaborative veri.cation and testing with explicit assumptions. In FM, pages 132 \n146, 2012.  [11] P. Collingbourne, C. Cadar, and P. H. J. Kelly. Symbolic testing of OpenCL code. In \nHVC, pages 203 218, 2011. [12] P. Collingbourne, A. F. Donaldson, J. Ketema, and S. Qadeer. Interleaving \nand lock-step semantics for analysis and veri.ca\u00adtion of GPU kernels. In ESOP, pages 270 289, 2013. [13] \nP. Cousot and R. Cousot. Abstract interpretation: A uni.ed lattice model for static analysis of programs \nby construction or approximation of .xpoints. In POPL, pages 238 252, 1977. [14] L. M. de Moura and N. \nBj\u00f8rner. Ef.cient E-matching for SMT solvers. In CADE, pages 183 198, 2007. [15] L. M. de Moura and N. \nBj\u00f8rner. Z3: An ef.cient SMT solver. In TACAS, pages 337 340, 2008. [16] D. Detlefs, G. Nelson, and J. \nB. Saxe. Simplify: a theorem prover for program checking. J. ACM, 52(3):365 473, 2005. [17] A. F. Donaldson, \nL. Haller, and D. Kroening. Strengthening induction-based race checking with lightweight static analy\u00adsis. \nIn VMCAI, pages 169 183, 2011. [18] M. Harris, S. Sengupta, and J. D. Owens. Parallel pre.x sum (scan) \nwith CUDA. In H. Nguyen, editor, GPU Gems 3. Addison-Wesley, 2007. [19] M. Huisman and M. Mihel.ci\u00b4c. \nSpeci.cation and veri.cation of GPGPU programs using permission-based separation logic. In BYTECODE, \n2013. [20] C. B. Jones. Tentative steps toward a development method for interfering programs. ACM Trans. \nProgram. Lang. Syst., 5(4): 596 619, 1983. [21] R. Karmani, P. Madhusudan, and B. Moore. Thread contracts \nfor safe parallelism. In PPoPP, pages 125 134, 2011. [22] Khronos OpenCL Working Group. The OpenCL speci.ca\u00adtion, \nversion 1.2, 2012. [23] P. M. Kogge and H. S. Stone. A parallel algorithm for the ef.cient solution of \na general class of recurrence equations. IEEE Trans. Computers, C-22(8):786 793, 1973. [24] A. Leung, \nM. Gupta, Y. Agarwal, et al. Verifying GPU kernels by test ampli.cation. In PLDI, pages 383 394, 2012. \n[25] G. Li and G. Gopalakrishnan. Scalable SMT-based veri.ca\u00adtion of GPU kernel functions. In FSE, pages \n187 196, 2010. [26] G. Li, P. Li, G. Sawaya, G. Gopalakrishnan, I. Ghosh, and S. P. Rajan. GKLEE: Concolic \nveri.cation and test generation for GPUs. In PPoPP, pages 215 224, 2012. [27] P. Li, G. Li, and G. Gopalakrishnan. \nParametric .ows: Auto\u00admated behavior equivalencing for symbolic analysis of races in CUDA programs. In \nSC, pages 29:1 29:10, 2012. [28] K. McMillan. Veri.cation of in.nite state systems by compo\u00adsitional \nmodel checking. In CHARME, pages 219 234, 1999. [29] M. Moskal. Programming with triggers. In SMT, pages \n20 29, 2009. [30] NVIDIA. CUDA C programming guide, version 5.0, 2012. [31] S. S. Owicki and D. Gries. \nAn axiomatic proof technique for parallel programs I. Acta Inf., 6:319 340, 1976. [32] S. F. Siegel and \nT. K. Zirkel. Loop invariant symbolic execu\u00adtion for parallel programs. In VMCAI, pages 412 427, 2012. \n[33] J. Stratton et al. Parboil: A revised benchmark suite for scienti.c and commercial throughput computing. \nTechnical Report IMPACT-12-01, UIUC, 2012. [34] M. Talupur and M. R. Tuttle. Going with the .ow: Parameter\u00adized \nveri.cation using message .ows. In FMCAD, pages 1 8, 2008. A. Proof of Theorem 3.1 De.ne the projection \nps,t(S) as (S.sh, ps(S), pt(S)) where px(S) \u00a3 (S(x).l, S(x).R, S(x).W ). We have: Proof (Theorem 3.1). \nThe proof is by contradiction. Thus, suppose no execution of As,t(P ) for any pair of threads s and t \nleads error , but that P either has a race or violates a barrier invariant. Hence, there is an execution \nof P from an initial state to a state where either (a) K-RAC E or (b) K-BA R -ER R applies. In both cases, \nobserve that this is due to two threads, say s and t. Suppose . = S1, S2, . . . , Sn is the sequence \nof kernel states successively assumed by P during the erroneous exe\u00adcution with exception of the .nal \nK-R AC E or K-BA R -E R R step. We will show by induction that p(.) = ps,t(S1), ps,t(S2), . . . , ps,t(Sn) \nis a sequence of successive kernel states of an execution of As,t(P ) whose execution steps are exactly \nthe execution steps of the problematic execution with each K -BA R -IN V step replaced by A-BA R -IN \nV; from this, it follows that either a K-RAC E or A-BA R -ER R step is possible from ps,t(Sn ), contradicting \nthat no execution of As,t(P ) leads to error . The base case of the induction is trivial. For the succes\u00adsor \ncase, consider K-ST E P and K-BA R -IN V in turn. For each K-ST E P step from Si to Si+1 a K-ST E P step \nis pos\u00adsible from ps,t(Si) to ps,t(Si+1), as no data races occur in . and, hence, the part of the state \nSi employed by s and t is completely captured by ps,t(Si). Similarly, for each K -BA R -IN V step from \nSi to Si+1 it follows that an A-BA R -IN V step is possible from ps,t(Si) to ps,t(Si+1): No execution \nof As,t(P ) ends in error and, hence, for s,t all S . .s,t(T ) it must hold that [.]. Moreover, as S \nSi . .s,t(ps,t(Si)) by de.nition of .s,t and ps,t, it fol\u00adlows by occurrence of K-BA R -I N V in . that \nps,t(Si+1) can ' be used as T (observe here that Si+1 is equal to Si with the read/write sets of all \nthreads cleared).   \n\t\t\t", "proc_id": "2509136", "abstract": "<p>Data-dependent GPU kernels, whose data or control flow are dependent on the input of the program, are difficult to verify because they require reasoning about shared state manipulated by many parallel threads. Existing verification techniques for GPU kernels achieve soundness and scalability by using a two-thread reduction and making the contents of the shared state <i>nondeterministic</i> each time threads synchronise at a barrier, to account for all possible thread interactions. This coarse abstraction prohibits verification of data-dependent kernels. We present <i>barrier invariants</i>, a novel abstraction technique which allows key properties about the shared state of a kernel to be preserved across barriers during formal reasoning. We have integrated barrier invariants with the GPUVerify tool, and present a detailed case study showing how they can be used to verify three <i>prefix sum</i> algorithms, allowing efficient modular verification of a <i>stream compaction</i> kernel, a key building block for GPU programming. This analysis goes significantly beyond what is possible using existing verification techniques for GPU kernels.</p>", "authors": [{"name": "Nathan Chong", "author_profile_id": "81548601456", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P4290422", "email_address": "nyc04@imperial.ac.uk", "orcid_id": ""}, {"name": "Alastair F. Donaldson", "author_profile_id": "81318493370", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P4290423", "email_address": "afd@imperial.ac.uk", "orcid_id": ""}, {"name": "Paul H.J. Kelly", "author_profile_id": "81408594048", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P4290424", "email_address": "phjk@imperial.ac.uk", "orcid_id": ""}, {"name": "Jeroen Ketema", "author_profile_id": "81384591133", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "P4290425", "email_address": "jketema@imperial.ac.uk", "orcid_id": ""}, {"name": "Shaz Qadeer", "author_profile_id": "81100286660", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P4290426", "email_address": "qadeer@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2509136.2509517", "year": "2013", "article_id": "2509517", "conference": "OOPSLA", "title": "Barrier invariants: a shared state abstraction for the analysis of data-dependent GPU kernels", "url": "http://dl.acm.org/citation.cfm?id=2509517"}