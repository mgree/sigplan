{"article_publication_date": "06-11-2012", "fulltext": "\n Input-Sensitive Pro.ling Emilio Coppa Camil Demetrescu Irene Finocchi Dept. ofComputer and System Sciences \nDept. of Computer and System Sciences Dept. of Computer Science Sapienza Universityof Rome Sapienza UniversityofRome \nSapienza UniversityofRome ercoppa@gmail.com demetres@dis.uniroma1.it .nocchi@di.uniroma1.it Abstract \nIn this paper we present a pro.ling methodology and toolkit for helpingdevelopersdiscoverhidden asymptoticinef.cienciesinthe \ncode. From one or more runs of a program, our pro.ler automati\u00adcally measures how the performance of \nindividual routines scales as a function of the input size, yielding clues to their growth rate. The \noutput of the pro.ler is, for each executed routine of the pro\u00adgram, a setoftuplesthat aggregateperformance \ncostsbyinput size. The collected pro.les can be used to produce performance plots and derive trend functions \nby statistical curve .tting or bounding techniques.Akey feature of our methodis the ability to automati\u00adcally \nmeasurethesize oftheinputgivento agenericcodefragment: to this aim, wepropose an effective metricfor \nestimating theinput size of a routineand showhowto computeit ef.ciently.Wediscuss several case studies, \nshowing that our approach can reveal asymp\u00adtoticbottlenecks that otherpro.lers may fail to detect and \ncharac\u00adterize the workload andbehavior ofindividual routinesin the con\u00adtext of real applications.Toprove \nthefeasibility of our techniques, we implemented a Valgrind tool called aprof and performed an extensive \nexperimental evaluation on the SPEC CPU2006 bench\u00admarks. Our experiments show that aprof delivers comparableper\u00adformanceto \notherprominentValgrindtools,and cangenerateinfor\u00admative plots even from single runs on typical workloads \nfor most algorithmically-critical routines. Categories and Subject Descriptors D.2.8[Software Engineer\u00ading]:Metrics \nperformance measures GeneralTerms Algorithms,Measurement,Performance. Keywords Performance pro.ling, \nasymptotic analysis, dynamic program analysis,instrumentation. 1. Introduction Performancepro.lingplays \na crucialrolein softwaredevelopment, allowing programmers to test the ef.ciency of an application and \ndiscoverpossibleperformancebottlenecks.Traditionalpro.lers as\u00adsociate performance metrics to nodes or \npaths of the control .ow or call graph by collecting runtime information on speci.c work-loads[2,19,27,39].These \napproachesprovidevaluableinforma\u00adtion for studying the dynamic behavior of a program and guiding optimizations \nto portions of the code that take most resources on Permission to make digital or hard copies of all \nor part of this work for personal or classroom useisgranted without feeprovided that copies are not made \nordistributed forpro.tor commercial advantage andthat copiesbearthis notice and thefull citation onthe \n.rstpage.Tocopy otherwise,torepublish,topost onserversortoredistribute tolists, requiresprior speci.cpermission \nand/or afee. PLDI 12, June11 16,2012,Beijing,China. Copyright c &#38;#169; 2012ACM978-1-4503-1205-9/12/06. \n. .$10.00 the considered inputs. However, they may fail to characterize how the performance of a program \nscales as a function of the input size, which is crucial for the ef.ciency and reliability of software. \nSeemingly benign fragments of code may be fast on some testing workloads, passing unnoticed in traditional \npro.lers, while all of a sudden they can become major performance bottlenecks when deployed on larger \ninputs. As an anecdotal example, we report a story[8] relatedtotheCOSMOScircuit simulator,originallydevel\u00adopedby \nRandalE.BryantandhiscolleaguesatCMU[10]. When the project was adopted by a major semiconductor manufacturer, \nit underwent a majorperformancetuningphase,including a mod\u00adi.cation to a function in charge of mapping \nsignal names to elec\u00adtrical nodes, which appeared to be especially time-consuming: by justhashingonbounded-length \nnamepre.xes rather than on entire names, the simulator became faster on all benchmarks. However, when \ncircuits later grew larger and adopted hierarchical naming schemes, many signal names ended up sharing \nlong common pre\u00ad.xes, and thus hashed to the same buckets. As a result, the simu\u00adlator startup timebecameintolerable, \ntakinghoursfor what should haverequired afewminutes.Identifying theproblem -introduced severalyearsbeforeinanefforttooptimizetheprogram \n-required several days of analysis. There are many other examples of large softwareprojects where this \nsort ofproblems occurred[9]. Theproblem ofempirically studyingthe asymptoticbehavior of a program has \nbeen the target of extensive research in experimen\u00adtal algorithmics[14,25,30,33].Individualportions of \nalgorithmic code are typically analyzed on ad-hoc testharnesses, which repro\u00adduce real-world scenarios \nby performing multiple runs with dif\u00adferent and determinable input parameters, collecting experimental \ndata for comparing the actual relative performance of algorithms and studying their amenability for use \nin speci.c applications. While this approach provides valuable information for comple\u00admenting theoretical \nanalyses, it has several drawbacks as aperfor\u00admance evaluation method in actual software development. \nFirstly, itistypicallydif.cultandtime-consumingto manually extractpor\u00adtions of code from an application \nand analyze them separately on different input sizes to determine their performance growth rate. Furthermore, \nasymptotic inef.ciencies may be introduced by pro\u00adgrammingerrorsin unexpectedlocations, whosedetection \nrequires automated and systematic pro.ling methodologies, especially on large-scale systems. As another \npoint, by studying performance\u00adcritical routines out oftheir context, we misshowtheyinteract with the \noverall applicationin whichthey aredeployed,including cache effects and branch mispredictions. We also \nnotice that it may be hard to collect real data about typical usage scenarios to be repro\u00adduced in experiments,inparticular \nwhen the workloads ofinterest aregeneratedbyintermediate steps of a computation. Our contributions. Motivatedby \ntheobservationthat critical al\u00adgorithmic routines shouldbe analyzed within the actual context of the \nsoftwareprojectsin which they aredeployed, thispaper makes a .rst step towards bridging the gap between \nthe pro.ling prac\u00adtice andthe methodologies usedin experimental algorithmics.As a main contribution,wedeviseanewautomatedpro.ling \ntechnique forhelpingdevelopersdiscoverhidden asymptoticinef.cienciesin the code. To prove the effectiveness \nof our approach, we devel\u00adoped aValgrind[34] tool called aprof: from one or more runs of aprogram, aprof \nautomatically measureshowtheperformance of individualroutines scales as afunction of theinput size.Using \nsta\u00adtistical curve .tting[11] and curvebounding[31]techniques onthe collected pro.les, developers can \nderive closed form expressions that describe mathematical cost functions for program routines, yielding \nclues to their growth rate. We discuss several case stud\u00adies, showing that our approach can reveal asymptotic \nbottlenecks that otherpro.lers mayfail todetect and characterizethe workload and behavior of individual \nroutines in the context of real applica\u00adtions.Inparticular, wefocus on applicationsincludedinprominent \nLinux distributions as well as in the SPEC CPU2006 suite. A dis\u00adtinguishing feature of our approach is \nthe ability to automatically measurethe size oftheinputgivento ageneric routine.This allows usto explore \na noveldimensionininterproceduralcontext-sensitive pro.lingwhere costs are associated todistinctinput \nsizes of a rou\u00adtine, rather than to paths in the call graph as in traditional contex\u00adtualpro.ling[2,16,46].To \nsupportthis approach, which we call input-sensitivepro.ling,weproposean effectivemetricforestimat\u00ading \nthe input size of a routine by counting the number of distinct memory cells .rst accessed by the routine \nwith a read operation, and showhow to computeit ef.ciently.An extensive experimental evaluation on the \nSPEC CPU2006 benchmarks reveals that aprof is as ef.cient as other prominent Valgrind tools, and can \ngener\u00adate informative plots even from single runs on typical workloads for most algorithmically-critical \nroutines. The tool is available at http://code.google.com/p/aprof/.  The remainder of this paper is \norganized as follows. In Sec\u00adtion2 weintroduce ourpro.ling methodology,discussing relevant properties \nof our approach. In Section 3 we present case studies that demonstrate the utility of input-sensitive \npro.ling. Section 4 proposes an ef.cient pro.ling algorithm, Section 5 describes the most relevant aspect \noftheimplementation of aprof, andSection6 presents our experimentalresults.Related workisdiscussedinSec\u00adtion7 \nand concluding remarks aregiveninSection8. 2. Input-SensitivePro.ling In this section we introduce our \ninput-sensitive pro.ling method\u00adology. Differently from the classical analysis of algorithms based on \ntheoretical cost models, where the input size of a procedure is aparameterknown apriori and clear from \nthe abstract description of the underlying algorithm, a key challenge of an automated ap\u00adproachis the \nabilityto automaticallyinferthe size oftheinputdata on which a function operates. We .rst propose a solution \nto this basicproblem. 2.1 Read MemorySize We introduce a metric, which we call read memory size, for \nesti\u00admating theinput size of a routineinvocation: De.nition 1. The read memory size (RMS) of the execution \nof a routine f is the number of distinct memory cells .rst accessed by f , orby adescendant of f in the \ncall tree, with a read operation. The main idea is that cells that are accessed by a function for the \n.rst time with a read operation contain the input values of the routine. Conversely, if a cell is .rst \nwritten and then read by the routine, the read valueis notpart of theinput asit wasdetermined by the \nroutine itself. We notice that the RMS de.nition, which is basedon tracinglow-level memory accesses madeby \ntheprogram, supports memorydereferencing andpointersin a natural way. Example 1. Consider the following \ntrace of operations. Function g performs three .rst-read oper\u00ad 1. call f ations(lines5 7) andits RMS \nisthus3. 2. read x Function f performs .ve read operations, 3. write y three of which through its subroutine \ng. 4. call g 5. read x However,itsRMS is only2:theread oper\u00ad 6. read y ations at line 5, line 6, and \nline 10 are not 7. read z .rst-reads with respect to f. Indeed, x has  8. write w been already read \natline2 and y and w are 9. return writtenatlines3 and8, respectively,before 10. read w beingread.Hence,onlythereadoperations \n11. return at lines2 and 7 contribute to the computa\u00adtion of the read memory size offunction f. InSection4 \nwe willpropose an ef.cient algorithmfor computing the RMS of each routineinvocationfrom aprogram executiontrace. \n 2.2 Pro.lingMethodology For each routine f , we determine the set Nf = {n1,n2,...}of distinct RMS values \non which f is called during the execution of a program.For eachni . Nf , whichis an estimate of aninput \nsize, we collect a tuple (ni ,ci , maxi , mini , sumi , sqi), where: ci is the number of times the routineis \ncalled oninput size ni ;  maxi and mini arethe maximum and minimum costs required by any execution of \nf oninput size ni , respectively;  sumi and sqi are the sum of the costs required by the execu\u00adtions \nof f on input size ni and the sum of the costs squares, respectively.  We use the generic term cost \nto refer to any performance metric, e.g., time, number of executed basic blocks, etc. The cost of a routine \nexecution is intended as a cumulative cost, i.e., it includes the costs of all the routine sdescendantsin \nthe call tree. Analysis metrics. The value avgi = sumi /ci is the average cost per invocation on input \nsize ni , while sqi is used for com\u00adputing the cost variance vari . The sets of points (ni , maxi ), \n(ni , mini ), and (ni , avgi ) estimate how the worst-case, best\u00adcase, and average-case costs of a routine \ngrow as a function of theinputsize(seeFigure1aforan examplegenerated withour technique). The total cost \nper input size, the variance between ex\u00adecution costs, and the frequency distribution of the input sizes \nare given by points (ni , sumi ), (ni , vari ), and (ni ,ci ), respectively (seeFigures1b,1c, and1d). \nExample2. To exemplifyourpro.ling methodology, we analyze the simple C code frag\u00ad 1: int get(int v[], \nint i) { ment on theleft.Any ex\u00ad 2: return v[i]; ecution of get has RMS 3: } 3. This accounts for the \n4: int mid(int v[], int n) { addresses of lvalues v, 5: int m = get(v, n/2); i, and v[i], which con\u00ad \n6: return m; tain the input values of 7: } thefunction.Routine mid reads 3 distinct addresses as well: \nv and n directly at line 5, and v[n/2] indirectly via a call to get.Variable m is not counted in the \nRMS of mid as it is .rst accessed by a write operation at line 5. In this example, aninput-sensitivepro.ler \nwouldjust collect oneper\u00adformancetupleforget and onefor mid,i.e., |Nget|= |Nmid|=1. Example3. Our next \nexample illustrates the case where the size of theinput may vary at each call: 1: int count_zero(int \nv[], int n) { 2: int i, count = 0; 3: for (i=0; i<n; i++) count += (v[i]==0 ? 1:0); 4: return count; \n5: }  Cost plot (order_moves) Total cost plot (order_moves) Variance plot (order_moves) Frequency plot \n(order_moves) 25 8 7 206 3 5 4 (a)(b)(c)(d) 15 10 2 10 10 3 6 cost \u00d7 100 (executed BB) 5 0 cost \u00d7 106 \n(executed BB)  max average min    5 cost variance \u00d7 10 rms frequency 2 4 1 10 1 2 0 0 0 10 0 5 10 \n15 20 25 30 35 40 45 read memory size \u00d7 10 read memory size \u00d7 10 read memory size \u00d7 10 read memory size \n\u00d7 10 Figure1. Input-sensitiveplots automaticallycomputedby aprof.Allplots are relatedtofunction order \nmoves() of the sjeng chessplaying programincludedin theSPECCPU2006benchmarks[23], executedon a single \nreference workload. The function, which counts the number of0 sin the input array v, has RMS n +2:it \nreads variables v and n, and the .rst n cells of the arraypointed toby v.Aninput-sensitivepro.ler would \ncollect one performancetuplefor eachdistinctvalue of n on which count zero is called during the execution \nof the program. In this case, the number of tuples |Ncount zero| cannot exceed the number of distinctcalls \nof thefunction. Example 4. We now show that even a single invocation of a function made by aprogram canyield \nseveralperformance tuples. Consider a recursive variantof the count zero function: 1: int count_zero(int \nv[], int n) { 2: if (n<1) return 0; 3: return (v[n-1]==0 ? 1:0) + count_zero(v, n-1); 4: } Observe that, \njust like the iterative version, the .rst invocation of function count zero has RMS n1 = n +2. However, \ncalling count zero on parameter n also results in n additional recursive activations of the function \nfor all size values ranging from n - 1 down to 0. Therefore, we collect n +1 performance tuples from \njustone starting activation,i.e., |Ncount zero|= n +1.The read memory size correspondingtothe i-thinvocationis \nni = n -i +3, for each i . [1, n +1].  2.3 CharacterizingAsymptoticBehavior The examples presented in \nSection 2.2 show that our pro.ling approach allows a tool to automatically collectperformance tuples \nfrom an execution run, relatingcost measures to theinput size of a routine. As exempli.ed in Figure 1, \ncollectedpro.les can be used toproduceperformanceplots,providing effective visualizations of the behavior \nof a program. A natural question we address in this section is to which extent this methodology can help \ndevelopers assess the asymptotic cost of apiece of code. Asymptotics vs. .nite experiments. Theultimategoalofthe \ncom\u00adplexityanalysisof algorithmsisto .nd closedformexpressionsfor therunning timeorothermeasures of resourceconsumption(e.g., \nspace usage, cache misses, or number of transmitted bits). Since this may be too dif.cult, it is common \nto estimate the theoretical complexityin an asymptotic sense,i.e.,for arbitrarilylargeinputs. Togetinsightsontheasymptoticbehaviorfrom \n.niteexperiments, itwouldbe necessary to extrapolatetrenddatabeyondthe range of experimentation.Unfortunately, \nnodata analysis methodforinfer\u00adring asymptotic trends can beguaranteed to be correct for all data sets: \nasobservedin[31],forany.nitevectorofproblemsizesthere are functions of arbitrarily high degree that are \nindistinguishable fromthe constantfunction atthose sizes.Throughoutthispaper we will exploit two main \ntechniques: curve .tting and curve bound\u00ading.Statisticalcurve .tting[11] istheprocessofconstructing a \nmathematicalfunctionthathasthebest .ttoaseriesofdatapoints. Regression analysistechniques are widelyusedforthiskind \nofpre\u00addictions:inourexamples,weused thecurve .tting toolsprovided bygnuplot [45]. Given a setofdatapoints \n(Xi,Yi )obtainedfrom an experiment such that Yi = f (Xi )for some unknown function f , curvebounding[31] \nmakesitpossibleto estimate complexity classes O(g1)and O(g2)such that f . O(g1)and f . O(g2). In thispaperwewillperformcurveboundingby \nguessratio ,which works by considering a guess function h and analyzing the trend of ratio f (n)/h(n):the \nratio stabilizesto a nonnegative constantif f . O(h), whileit(eventually)increasesif f . O(h). Number \nofcollectedtuples. Thenumber ofcollectedtuplesplays afundamental rolein characterizingthebehavior of \na routine.This number depends on several factors, including the structure of the routine,howitis usedinthe \ncontext oftheprogram, and ultimately the workload on which the program is tested. We have seen that recursivefunctions \nallowitto collect severaltuples withjust one initial call. In Section 6 we will provide a quantitative \nevaluation of the number of distinct tuples that can be collected for each routine in a variety of applications \non typical workloads. As we willdiscussinSection3, variety ofinputpatternsintest workloads ismoreimportantthantheirsheer \nsize: small workloadscanyield large amounts ofinformation, whilelarge repetitiveinputs maybe of little \nhelp. We also observe that pro.les of the same program collected from different runs can be naturally \nmerged together to generatepro.les withalarger number of tuples. Accuracy of the RMS metric. For functions \nthat read their input entirely at least once, the read memory size approximates within constant factors \nthe actual input size n: hence, if RMS = O(n), the input-sensitive pro.le of a function f correctly estimates \nthe theoretical cost Tf (n) of the algorithm implemented by f . This is not the case for sublinear functions. \nFor instance, consider a binary search over a sorted array, which reads only O(logn)cells of the input \narray: in this case RMS = O(logn), so our approach estimates T (logn)ratherthan T (n).Sublinearfunctions,however, \nare unlikelyto representperformancebottlenecks and aretherefore thelessinterestingfrom apro.lingperspective. \nWe remarkthat the RMS is a measure ofdistinctaccessed mem\u00adory cells. Hence, it fails to characterize \ncomputations whose run\u00adning time is determined by the value of some variable: e.g., the RMS ofthe naive \nalgorithmfor computingthefactorial of a number n wouldbe constant, regardless ofthe value of n.Aggregatingper\u00adformance \nmeasurementsbydistinctvalues offunction argumentsis an alternative approach thatis explored, e.g.,in[28]. \nParameterspassedin registers,which shouldbe regarded aspart of a function s input, are also not counted \nin the RMS. However, they only account for small additive constants, and are therefore negligiblefor \nestimatinggrowth ratesforsuf.cientlylargeinputs. For instance, if parameters v and n of function count \nzero dis\u00adcussed in Section 2.2 are held by the compiler in registers rather than on stack, the RMS wouldbe \nexactly n rather than n+2. 3. CaseStudies Inthissection wedescribe scenarios whereinput-sensitivepro.ling \ncanprovide valuableinformationtotheprogrammer.Our examples are based on the aprof tool described in Section \n5 and use basic block(BB) counts asperformance metric.  index % time self children name [1] 99.9 2.00 \n16.23 main [1] (a) [2] 52.2 5.82 3.70 addword[2] [3] 31.3 2.81 2.90 str tolower [3] [4] 20.3 3.70 0.00 \nhash[4] [1] 100.0 0.21 4.61 main [1] (b) [2] 61.8 0.98 2.00 str tolower [2] [3] 41.4 2.00 0.00 wf tolower \n[3] [4] 32.6 0.07 1.50 addword[4] Figure2. Outputofgprof for applicationwf ontwodifferenttexts: (a)AnnaKarenina;(b)protein \nsequences. Discovering hidden inef.ciencies. As a .rst example, we show that input-sensitive pro.ling \nmakes it possible to discover perfor\u00admance bottlenecks that may not be revealed by standard pro.l\u00ading \nmethods. Our discussion is based on wf-0.41 [15], a simple word frequency counter included in the current \ndevelopment head of Linux Fedora(Fedora 17 Beefy Miracle): wf scans a text .le and counts the frequency \nof wordsin the text. To ef.ciently count repetitions, it adds words to a hash table using a function \ncalled addword, and reimplements a variety of ad-hoc string utility func\u00adtionstohandletheISOLatin-1 representation,including \nafunction (str tolower)to convertall characters tolower case. We analyzedwf withboth gprof [19]and aprof \non atext corpus taken from the Gutenberg Project [22], consideringseveral books from classical literature. \nOn all test sets, gprof ranked addword as the hottest function. For instance, on Tolstoj s Anna Karenina \n, addword took roughly 52%ofthetotaltime, as showninFigure2a. This was not surprising, as it appears \nto be algorithmically more complex than all other basic text management functions. We also pro.led wf \nwith gprof onlarger andlargerinputtextsfrom[22] to study how the performance of individual routines scales \nas a function of the size of theprogram sinput.Collectedpro.les con\u00ad.rmed that addword is consistently \nmore expensive than all other functions.Figure3 left reports theperformance trends of addword and str \ntolower,obtainedby extractingperformance .guresfrom 11 different gprof reports for input .les of different \nsize derived from textsin[22].Both curves seem togrowlinearly as afunction of wf s input size. The tests \nwe made with gprof did not provide any evidence of asymptoticbottlenecksin wf. In contrast,aprof discovereda \nratherdifferent scenario already on the smaller workload, pinpointing a serious inef.ciency in a seemingly \nbenign function: surprisingly, the cost of str tolower turned out togrowquadratically with thelength \nof theinput string (see Figure 3 middle). Conversely, aprof showed that addword scaleslinearly(seeFigure3 \nright).By analyzing the source code of str tolower: 1: void str_tolower(char *str) { 2: int i; 3: for \n(i=0; i < strlen(str); i++) 4: str[i] = wf_tolower(str[i]); 5: } we noticed that strlen is redundantly \ncalled at each iteration, re\u00adsulting in a quadratic running time. Once we discovered the in\u00adef.ciency \nusing aprof, we made a second set of experiments on carefully selected inputs with very long words, where \nthe inef.\u00adciency in str tolower is most likely to impact signi.cantly the overall running time. In particular, \nwe considered a data set con\u00adtaining long protein sequences taken from the Genomics-96 data set of theGenomeInformaticsResearchLab[17].Indeed,onthis \ndata set gprof showed that str tolower accountsfor 61.8%of the total time, while addword drops to32.6%(Figure2b). \nThe inef.ciency found by aprof in wf is rather common[9] and was very easy to .x by loop-invariant code \nmotion. To complete ourinvestigation, we comparedthe wall-clocktime requiredbythe original wf-0.41 code \nincluded in the Fedora RPM, and a new version where strlen(str) was moved out of the loop: .xing the \ncode improved the total running time of wf by 6% on Anna Karenina andby30%onprotein sequences. Discussion. \nThe wf example shows that repeating classic gprof\u00adstyle pro.ling at scale to assess the computational \ncomplexity of individualroutines canyield misleading results: gprof failedto re\u00adveal thequadratic trend \nof str tolower, even making several tests on workloads of different size, because the input of str tolower \nare single words of theinput text, not the entire documentgiven as input to the application. In general, \nthe input of a routine is often producedby other routines(e.g., wf s text tokenizer feeds strings to \nstr tolower)and may be unpredictably related to the input of the overall application.Hence, setting up \na collection of test work\u00adloads for an application to expose the asymptotic behavior of in\u00addividual routines \ncan be rather dif.cult with traditional pro.lers. Also, different input families may be needed to characterize \ndif\u00adferent routines, making this approach largely impractical. Since a typical text contains words of \ndifferent lengths and aprof collects separateperformance measurementsforeachdistinctinputsize,it could \ncorrectly pinpoint the quadratic trend of str tolower s cost function even on a single run on a smallinput. \nWorkload characterization. The most realistic program execu\u00adtions are ondeployed systems.Abene.t of ourpro.ling \nmethodol\u00adogyis thatit cangiveinsights on the typical workloads on which a functionis calledin the context \nof real applications.Thisinforma\u00adtion might be very useful not only for code optimization, but also for \nalgorithmicimprovements, even theoretical,in speci.c scenar\u00adios. For instance, routing algorithms for \nGPS navigation systems are speci.callydesignedtotake advantage ofthe sparsity ofthein\u00adputroad networks \non which they aredeployed.Similarly,if an ap\u00adplication always needsto sortarrays withlessthan16items,it \nmay be convenient to use a non-optimal sorting algorithm with runtime n 2 insteadof an asymptotically \noptimal one with runtime 4n logn. Figure 4 provides a concrete example where the workload of a functioncanbe \nratherdifferentdepending onthespeci.capplica\u00adtion scenario and data set, showing that our pro.ler can \nhighlight such differences. In particular, we ran the word frequency counter wf discussed above on two \ndifferent inputs (Anna Karenina and protein sequences), and analyzed the strlen function used by wf. \nNotice that the read memory size of strlen is an indirect measure of the length of the strings on which \nit is invoked: the maximum RMS is about20forAnnaKarenina andlarger than900forprotein sequences,highlightingthe \nstructuraldifferencebetweenthesetwo data sets.Even more signi.cantisthefactthatthefrequencycurves shown \non the left of Figure 4 have opposite trends: decreasing on Anna Karenina, as we would expect for a natural \nlanguage text, andincreasing onprotein sequences. Sincelongprotein sequences tend to be very frequent, \non thisdata set strlen willlikelyhave a tangibleimpact on the execution cost. Exposing empirical asymptotics. \nInput-sensitive pro.les can characterize the empirical asymptotic behavior of program rou\u00adtines in realistic \nexecution scenarios, often yielding more precise results than theoretical cost models.Forinstance, the \ngg sort rou\u00adtine included in SPEC CPU2006 component gobmk [23] imple\u00adments the combsort algorithm [1], \na variant of bubblesort that compares items at distance larger than 1. Combsort is known to have at least \nquadratic running time in the worst case, but can ri\u00adval fast algorithms like quicksort in many practical \ncases. This is con.rmed by the guess ratio plot of gg sort, which stabilizes to a constant value when \ndivided by the guess function n logn, as  Time plot (addword, str_tolower) Cost plot (str_tolower) Cost \nplot (addword) 25 6 2.5 1.02.21.0 T(n)=0.1 n-0.02 T(n)=0.2 n+17 T(n)=2.2 n-17 5 1.0 20 2 T(n)=0.06 n-0.07 \n average costaverage cost 4 3 15 1.5 addword cost \u00d7 100 (executed BB) cost \u00d7 100 (executed BB) time (secs) \n10 1 0.5 0 2 1 0 5 str_tolower 0 0 50 100 150 200 5 10 15 20 25 30 35 10 20 30 40 50 60 70 80 90 wf \ninput size (MB) read memory size read memory size Figure 3. Pro.ling of wf s functions addword and str \ntolower on texts from classical literature: cost trends derived from the pro.les produced by 11 runs \nof gprof oninput .lesofdifferentsize(left); costtrendsderivedby asinglerunof aprof on the smallest workload \n(middleandright).Regressioncurvesareobtainedbyleast-squares .tting. Frequency plot (strlen - AK) Frequency \nplot (strlen - PR) describingour algorithm, wediscuss a simple-minded approach as 7 6 1010 a warm-upfor \nthe reader. 6 10 frequency frequency 5 10 5 4.1 ASimple-MindedApproach 10 4 10 4 10 Eachactivation of \na routine f hasits own readmemory size, which 3 10 102 103 5 10 15 20 0 200 400 600 800 1000 read memory \nsize read memory size Figure 4. Workload characterization of function strlen used by application wf \non two different texts: Anna Karenina (AK) and protein sequences(PR). ispartofthe set Nf ofinput sizes \non which f isinvokedduringthe execution of the program (see Section 2.2). Computing the RMS of an activation \nof f requires to monitor the set of locations that are read during that activation(not onlyby f itself, \nbut also by its descendantsinthe calltree)beforebeingwritten.A simple-minded approachis to maintain alist \nAf of all memorylocations accessed (bothreadandwritten)duringtheactivationoff .Immediately after entering \nf , this list is empty and the RMS is equal to 0. When f accesses a location w for the .rst time, then \nw is added to Af and, if the access is a read operation, the RMS is increased by 1. Guess ratio (gg_sort) \nCost plot (cSubModIterator::operator++) cost \u00d7 10000 (executed BB) 16 14 The same check and update must \nbe performed for all pending 12 routine activationsin the call stack, which areimplicitly accessing guess \nratio 10 8 6 gg_sort / n log n location w through their descendant f . If we start from the most 4 recent \nactivations, it is possible to stop stack-walking when the 2 2 .rst routine h is encountered such that \nw already belongs to Ah . 0 0 5 10 15 20 25 30 35 40 0 1 2 3 4 5 6 However, h may be the program root \nin the worst case, making read memory size \u00d7 10000 read memory size \u00d7 10 (a) (b) Figure 5. Input-sensitive \npro.les excerpted from two SPEC CPU2006benchmarks:(a) routine gg sort from the SPEC com\u00adponentgobmk (curveboundingisbasedonguessfunction \nn logn); (b)routine cSubModIterator::operator++ fromtheSPECcompo\u00adnent omnetpp. shown in Figure 5(a). \nIn other cases, the asymptotic trends em\u00adpirically observed by aprof can direct the programmer s attention \nto critical routines that may otherwise pass unnoticed. As an ex\u00adample, the chart in Figure 5(b) shows \nthat the cost trend of rou\u00adtine cSubModIterator::operator++ de.ned in SPEC benchmark omnetpp [23] is \nlinear, differently from what one might expect from a ++ operator. Actually, after pro.ling omnetpp with \naprof, we found in cSubModIterator s source code the comment: this shouldbe replaced with somethingfaster \n. 4. Algorithms In this section we describe an ef.cient algorithm for computing the read memory size \nand the input-sensitive pro.le of a routine. An input-sensitive pro.ler is given as input a trace of \nprogram operations, including routine activations (call), routine comple\u00adtions(return), andread/write \nmemory accesses(seeExample1 in Section 2.1). Additional operations might be traced, depending on the \nspeci.c performance metrics to be computed. For each op\u00aderation, thepro.ler must update RMS and costinformation.Before \nupdatesbased on stack-walkingprohibitivelytime-consuming.The space usage of this approach is alsoquitedemanding:in \nthe worst case, each distinct memory location could be storedin alllists Ah ofpending routine activations, \nandin that case the space wouldbe proportionalto the memorysize times the maximum stackdepth.  4.2 TheLatest-AccessAlgorithm \nWe now describe a more space-and time-ef.cient algorithm, sketched in Figure 6. The main idea is to avoid \nmaintaining ex\u00adplicitlythe RMS and thelists Af of accessedlocations,but to store only partial information \nthat can be updated quickly during the computation and from which the RMS can be easily derived upon \nthe completion of a routine. In more detail, for each pending acti\u00advation of a routine f , we store the \nset Lf oflocations whose latest access was done by f (either directlyor by its completed subrou\u00adtines). \nIt is not dif.cult to see that each memory location is stored in exactly one set,i.e., at any timeduring \nthe execution of thepro\u00adgramthe setsLf partitionthelocations accessedsofar.The sets Lf oflatest accesses \nare storedimplicitlyby associatingtimestampsto routines and memorylocations, asdescribedbelow. Data structures. \nThe algorithm uses a shadow run-time stack S, whosetopisindexedby variable top.For each i . [1, top], \nthe i-th stack entry S[i]stores: TheidS[i].rtn of the i-thpending routine activation.  The timestamp \nS[i].ts of the i-th pending activation, i.e., the time at which the routine was entered.  The cumulative \ncostS[i].cost of the activation.  procedurecall(r): procedureread(w): 1: count ++ 1: ifts[w] <S[top].ts \nthen 2: top ++ 2: S[top].rms ++ 3: S[top].rtn . r 3: if ts[w] .0 then = 4: S[top].ts . count 4: let i \nbe the maxindexin S 5: S[top].rms . 0 such that S[i].ts = ts[w] 6: S[top].cost . get cost()  5: S[i].rms \n6: endif procedurereturn(): 7: endif 1: collect(S[top].rtn, 8: ts[w] . count S[top].rms, get cost() \n S[top].cost) procedure write(w): 2: S[top 1].rms += S[top].rms 1: ts[w] . count 3: top Figure6. AlgorithmforRMS \ncomputation andinput-sensitivepro\u00ad.ling:proceduresforprocessing execution trace events. The partial read \nmemory size S[i].rms of the activation, de\u00ad.ned sothatthefollowinginvariantpropertyholdsthroughout the \nexecution: top X .i, 1 = i = top : RMS(i)= S[j].rms (1) j=i where RMS(i)denotesthecurrentRMSvalueofthe \ni-thpending activation on theportion of the execution trace already seen. It can be proved that Invariant \n1 is equivalent to the following equality: S[i].rms = RMS(i)-RMS(i +1) i.e., that thepartial read memory \nsize maintainedby our algorithm is the difference between the RMS of an activation and the RMS of itspendingchild(if \nany).Moreover,itfollowsfromInvariant1that the RMS and thepartial RMS of the topmost routine coincide: \nRMS(top)= S[top].rms (2) Inparticular, thisis true upon completion of a routine, andguaran\u00adtees that \nany algorithm able to maintain Invariant 1 will correctly compute the RMS of any routine activation. \nBesides the shadow stack S, our algorithm also maintains, for each memory location w, a timestamp ts[w]containing \nthe time ofthelatest access(read or write) to w. Memory timestamps are initialized to 0 and time is measured \nby a global counter count, that maintains the total number of routine activations. Algorithm and analysis. \nThe partial read memory size can be maintained more ef.ciently than the RMS, as shown in Figure 6. Toprovethe \ncorrectness ofour approach,inthefollowing we show that allprocedures correctlypreserveInvariant1. When \na routine r isinvoked(seeprocedure call(r)), the time counter is incremented by 1 and a new shadow stack \nentry asso\u00adciated with r is appropriately initialized. Upon completion of the routine,itspartial RMS \nis addedtothepartialRMS ofitsparent(see line2 ofprocedure return). Itis notdif.cult to see that this \noper\u00adation preserves Invariant 1. Performance metrics of the completed routine are also collected(line1):these \nmetrics are associated with the RMS value S[top].rms, which atthispoint coincideswith the true RMS (seeEquation2). \nMemory access operations on a location w update the times\u00adtamp ts[w] with the current counter value (line \n1 of procedure write and line 8 of procedure read). The read operation might also update thepartial memory \nsizes of twodistinct routine activa\u00adtions(lines2 and5).Namely,iflocation w has neverbeen accessed bythetopmostroutine \norbyany ofitscompleteddescendants(con\u00addition tested in line 1), then the current access is a .rst read \nto w with respect to the topmost routine, whosepartial RMS isincreased by1.Wehave now two cases: If \nthe timestamp of location w is still 0 (test at line 3), this is the .rst access to w in the entire execution \nof theprogram, and no other counter needs tobe changed.Notice thatInvariant1is maintainedbythe execution \nofline2, sinceinthis casethe RMS of allpending routinesincreasesby 1.  If the timestamp oflocation w \nis not0,location w hasbeen ac\u00adcessedbeforeduringthe executionand,inviewoftheinequality inline1,thelastaccesshappenedin \nsome ancestor v ofthetop\u00admost routine(orin one of v s completed descendants). In this case,thealgorithm \n.ndsthedeepest ancestorthathasaccessed w (line 4)and decreases its partial RMS by 1: this restores In\u00advariant \n1 for all pending activations j such that 1 = j = i, whose RMS must notbe affectedby the current read \noperation.  Therunning timeof all operationsisconstant,exceptforline4 of procedureread.Sincethetimestamps \nS[i].ts of routine activations on any call path are increasing, line 4 can be implemented with a binary \nsearch and requires O(logd)timeintheworstcase, where d isthe maximum stackdepthduringthe execution oftheprogram. \nWe alsodesigned andimplemented an asymptoticallyfaster al\u00adgorithmbased ondisjoint-setsdata structures[40],butin \nour ex\u00adperimentsit was slower and required more space than the solution wepresentedin this section. 5. \nImplementation In this section we discuss our implementation of aprof based on the Valgrind[34] framework. \nValgrind provides a dynamic instru\u00admentation infrastructure that translates the binary code into an architecture-neutral \nintermediate representation (VEX). Analysis toolsprovide callbacksfor eventsgeneratedby the stream ofVEX \nexecutedinstructions. Instrumentation. While tracing memory accesses is relatively simple in Valgrind, \nreliably instrumenting function calls and re\u00adturns is instead rather complex. aprof uses a similar approach \nas the callgrind pro.ling tool[42]: we maintain a shadow run\u00adtime stack and cover a wide range of exotic \ncases to detect function entry/exit events,includingjumps todifferentELF sec\u00adtions/objects,specialhandlingfordynamicallylinkedfunctions \nvia dl runtime resolve, etc. Our tool takes advantage of the .exi\u00adble infrastructure of Valgrind and \nprovides full support for mul\u00adtithreaded applications by generating separate pro.les for each thread. \nShadow memory. To maintainthetimestampsts of memory cells needed for computing the RMS values, we shadow \neach memory location accessed by the program with a 32-bit counter. Similarly to the memcheck tool[38], \nwe use atwo-levelslookup table.The address spaceisdividedin64K chunks of64KB each(on64\u00adbit machines, \nwe extend address space coverage to 256 GB). So, the primary table indexes all 64 KB chunks. When an \naddress is referenced by a program, if not already done, we allocate a new secondary tablefor coveringthat \nchunk and we update theprimary tablefor future references. The secondary table contains the set of 32-bit \ntimestamps of addresses coveredby the chunk. Memory tracing resolution: space-accuracy tradeoffs. To \nre\u00adduce the space needed by the lookup table, aprof allows users to con.gure the resolution of distinct \nobservable memory objects, trading spacefor accuracy.This canpotentiallyimpact the number ofdistinct \nRMS values observedby aprof,andthereforethenumber of collected performance tuples. We denote by k the \nsize in bytes of the smallest observable objects, which we assume to be aligned to addresses multiple \nof k.For k =1,wehavethe .nest resolution, shadowing the addresses of all accessed individual memory bytes. \nFor k =2, we trace accesses to 2-bytes words aligned at 16-bit  perlbench bzip2 gcc mcf gobmk hmmer \nsjeng libquantum h264ref omnetpp astar xalancbmk bwaves gamess milc gromacs cactusADM leslie3d namd soplex \npovray calculix GemsFDTD tonto lbm wrf sphinx3 secs native 585 852 523 504 645 1,153 798 790 903 517 \n698 341 1,239 1,161 520 727 1.934 745 640 355 298 1,880 610 770 452 996 800 mem check 34.8 11.0 17.5 \n5.8 28.2 10.8 28.4 8.0 28.4 16.1 11.8 35.0 15.7 28.3 13.8 21.1 14.2 16.4 20.3 11.1 41.3 24.1 20.3 31.0 \n25.4 24.6 30.8 TIME slowdown callgrind callgrind base cache 98 178.3 32 90.1 47 97.5 15 35.1 72 135.7 \n23 90.7 94 149.3 31 80.2 85 193.5 45 82.9 26 67.1 95 171.3 17.2 79.1 42.1 147.4 12.3 60.9 14.0 76.0 6.4 \n52.7 12.9 83.2 15.7 89.5 25.6 73.4 81.0 166.0 22.1 104.3 10.4 86.8 48.1 136.7 10.4 85.2 27.9 108.4 39.6 \n109.5 aprof 55.4 28.7 33.8 12.2 44.8 27.6 45.8 21.3 67.5 28.8 22.8 78.3 24.3 40.9 18.5 21.5 17.4 24.3 \n21.5 26.3 55.2 27.0 26.1 42.3 26.6 29.9 38.7 MB native 757 959 488 1,785 138 137 279 171 173 280 444 \n536 982 748 795 153 1,111 231 161 738 118 232 937 147 517 809 159 mem check 2.1 1.3 2.7 1.3 1.8 1.9 1.4 \n1.7 1.7 2.8 1.5 1.8 1.3 1.1 1.3 1.5 1.2 1.5 1.8 1.3 2.7 1.6 1.3 2.0 1.3 1.6 2.0 SPACE overhead callgrind \ncallgrind base cache 1.1 1.1 1.1 1.1 2.1 2.3 1.0 1.0 1.5 1.6 1.4 1.5 1.2 1.2 1.5 1.6 1.4 1.4 1.3 1.2 \n1.2 1.2 1.2 1.2 1.1 1.1 1.1 1.1 1.1 1.1 1.2 1.3 1.1 1.1 1.3 1.3 1.4 1.4 1.2 1.2 1.6 1.6 1.9 1.9 1.1 1.1 \n1.5 1.6 1.1 1.1 1.2 1.2 1.4 1.4 aprof 2.1 2.0 3.9 2.0 2.1 1.9 1.9 2.5 2.0 2.0 2.0 2.3 2.0 1.1 2.0 1.5 \n1.6 1.9 1.8 2.2 1.8 2.5 2.0 2.0 2.0 2.1 2.1 geometric mean 19.1 29.4 97.3 30.6 1.6 1.3 1.3 2.0 Table1. \nPerformance comparison ofaprof and otherValgrind tools on theSPECCPU2006benchmarks. boundaries, halving \nthe universe of timestamps. The larger k, the smaller the RMS accuracy for routines working on small \nobjects (e.g., strings ofcharacters) and the smaller the size of the shadow memory.Bydefault, k =4in \naprof. Optimizations. aprof performs several optimizations at instru\u00admentation time. For instance, we \nreduce the number of traced memory accesses by coalescing into a single event each pair of load/storeevents(causedby \nthe sameguestinstruction) operating on the same object. We also discard all other events related to ob\u00adjects \nalreadyreferenced within the samebasicblock. Aperformance-criticaloperationisthetimestamp search ofline \n4intheread procedure ofFigure6.As we observedinSection4.2, implementing this operation with a binary \nsearch on the shadow stack guarantees a worst-case bound of O(logd), where d is the currentdepth of the \nstack.However, our experiments revealed that a sequential scan tends to be faster in practice, as on \naverage it performs a verysmall number ofiterations. Performance metric. We countbasicblocks asperformance \nmea\u00adsure: we observed that, compared to running time measurements, this adds a light burden to the analysis \ntime overhead, and im\u00adproves accuracy in characterizing asymptotic behavior even on small workloads. \nThe choice of counting basic blocks rather than measuring time for studying asymptotic trends has several \nother advantages, very well motivatedin[18]. Other issues. We noticed that calls to dynamically linked \nli\u00adbraries made via dl runtime resolve generate spurious memory accesses that may introduce some noise \nin the collected perfor\u00admancetuples.Toimprove reliability of RMS computations, wehan\u00addled dl runtime \nresolve as a special casebydisablingitscost and RMS propagation to the ancestors. Finally, to handle \nover.ows of the main routine activations counter (count) in long-running applications, aprof performs \na periodicalglobalrenumbering of timestampsinitsdata structures. 6. ExperimentalEvaluation In this section \nwe discuss the results of an extensive experimen\u00adtal evaluation of aprof on the CPU2006 benchmarks of \nthe Stan\u00addardPerformanceEvaluationCorporation[23].Our experiments aim both at studying the resources \nrequired by our tool compared to other prominent heavyweight dynamic program analysis tools, and at analyzing \nrelevantproperties of theinput-sensitivepro.les generatedfor the consideredbenchmarks. 6.1 ExperimentalSetup \nBenchmarks. Our experiments are mainly based on the SPEC CPU2006v1.1 suite,consideringbothinteger(CINT) \nand.oating\u00adpoint(CFP) components.All of them were run on theSPEC refer\u00adence workloads in 64-bit mode. \nFor CINT, we successfully tested all 12 components. In the case of CFP, we omitted zeusmp due to a knownValgrind \nissue, and dealII as it failed to terminate on all evaluated tools. We could successfully test all remaining \n15 com\u00adponents.Forthe sake ofcompleteness, we alsoincludedin ourtests the wf-0.41 [15]wordfrequency counterdiscussedinSection3 \non a117MBinputtext .leconsisting of theconcatenationof several copies ofAnnaKarenina[22] andprotein sequences[17]. \nPlatform. Experiments were performed on a cluster machine with two nodes, each equipped with eight 64-bit \nIntel Xeon CPU E5520 at 2.27 GHz, with 48 GB of RAM running Linux kernel 2.6.18 withgcc4.1.2 andValgrind3.7.0 \nSVNrev.12129. Evaluation metrics. We collected running times reported by specrun, the execution engine \nof the CPU2006 test harness, and the peak virtual memory space required over all tested workloads for \neach benchmark. We also collected all the pro.ling reports generated by aprof on theCPU2006 suite, measuring \nrelevantpa\u00adrameters such as the number of collected tuples for each routine. Pro.led functions covered \nboth the executable binary and all dy\u00adnamicallylinkedlibraries.  percentage of routines 100 90 80 70 \n60 50 40 30 20 10 0  20 22 24 26 28 210 212 214 216 20 22 24 26 28 210 212 214 216 218 20 25 210 215 \n220 225 230 235 20 25 210 215 220 225 230 235 number of collected tuples number of collected tuples cost \n(executed BB) cost (executed BB) (a) integer benchmarks (b) .oating-point benchmarks (a) integer benchmarks \n(b) .oating-point benchmarks Figure7. Percentage ofroutinesfor which aprof collected atleast a given \nnumber of performance tuples for a representative set of SPECCPU2006benchmarks. Evaluated tools. We compared \nthe performance of aprof to two prominentand widelyusedValgrindtools: memcheck [38],atoolfor detecting \nmemory-related errors, and callgrind [42], a call-graph generatingcache andbranchpredictionpro.ler.We \nconsideredtwo settingsfor callgrind:thebasic call-graphgeneratingtool andthe tool extended with cache \nsimulation, calling them callgrind-base and callgrind-cache, respectively.Although the considered tools \nsolve different analysis problems, all of them share the same in\u00adstrumentationinfrastructureprovidedbyValgrind, \nwhich accounts for a signi.cantfraction of the execution times: memcheck does not tracefunction calls/returns \nand mainly relies on memory read/write events; callgrind-base instrumentsfunction calls/returns,but not \nmemory accesses, while callgrind-cache traces both kinds of events like aprof. In our experiments with \naprof, we also consid\u00adered different values of the memory tracing resolution k discussed inSection5.Unless \notherwise stated, we usedaprof with k =4.  6.2 ExperimentalResults Performance results. Performance \n.gures of our evaluated tools on the SPEC CPU2006 benchmarks are summarized in Table 1. Compared to native \nexecution, aprof s mean slowdown factor is 30.6\u00d7 (31.9\u00d7 on CINT and 27.9\u00d7 on CFP), with a peak slow\u00addown \nof 78.3\u00d7 for xalancbmk, an XSLT engine for XML pro\u00adcessing. callgrind-cache, which is most similar to \naprof from a tracing perspective, is up to 4.2\u00d7 slower than aprof, with a mean slowdown of 3.2\u00d7. callgrind-base \nis 1.5\u00d7 slower than aprof on CINT and 1.4\u00d7 faster on CFP: overall, the two tools deliver comparable performance, \neven if callgrind-base does not trace memory accesses.Compared to memcheck, whichis thelessheavy\u00adweight \nof all considered tools, aprof is about 1.6\u00d7 slower. How\u00adever, memcheck does not tracefunction calls/returns, \nwhichaccount for a signi.cantfraction ofaprof sperformance. The mean memory requirements of aprof are \nwithin afactor of 2 of native execution, with a peak of 3.9\u00d7 for gcc.This is needed for shadowing accessed \nmemory cells with 32-bit timestamps and for maintaining the performance tuples. Compared to memcheck, \nwhich also uses a memory shadowing approach, aprof requires about20% more space.However,the current version \nof aprof does not use any shadow memory compression scheme as memcheck does, and the amount of generated \npro.le data is higher. Both versions of callgrind, which do not use shadow values, require less space \nthan aprof and memcheck (30% overhead compared to nativeexecution).Performance .gureson wf-0.41 are similar, \nwith a slowdown of 43.1\u00d7 and a space overhead of 1.9\u00d7. Analysis of input-sensitive pro.les. Our second \nset of experi\u00adments aims at evaluating some relevant properties of the pro.ling data generated by aprof \non the SPEC CPU2006 benchmarks. A .rst naturalquestionishow manyperformance tuples canbe auto\u00admatically \ncollectedfor each routinefrom a single run of aprogramFigure8. Percentage ofroutines withless than10 \ncollected tuples and atleast agiven maximum costfor a representative set ofSPEC CPU2006benchmarks. on \na typical workload. Charts in Figure 7 plot the percentage of routines that have at least a given number \nof tuples for a repre\u00adsentative set ofprograms and workloadsfromboth theinteger and .oating-point suites.The \nexperiment showsthatthe results can sig\u00adni.cantlyvary acrossdifferentbenchmarks, rangingfromprograms \nwherethefraction of routinesdropsquicklywiththenumberof col\u00adlectedtuples suchas bzip2, whichdoes most \nofthe workin ahand\u00adful ofhot routines, tolarger-scaleprograms containing a wealth of algorithmic-intensivefunctionssuch \nas gcc, with several rich rou\u00adtineshaving manytuples.Forinstance,thefraction of routines with at least \n10 tuples ranges from a minimum of 7.8% for bzip2 to a maximum of 49.1%forgcc, with an average value \nof 18.1%for all benchmarks.This observationdraws aninterestingparallelwiththe well-known Pareto principle, \nwhich accounts most of a program s cost to a small fraction of the routines. Notice that for some func\u00adtionsaprof \ncollectedhundreds ofthousands oftuples.To assessthe potential relevanceof poor routines,forwhich wemay \nnothave enough tuples to produce informative plots, we studied their cost distribution, reported in Figure \n8. The chart shows that the maxi\u00admum cost per invocation of most poor functions with less than 10 tuplesis \nsmall(e.g.,70%ofpoorfunctions never executemorethan 100basicblocksperinvocation), and therefore they \nare unlikelyto be of any asymptotic interest in the context of the application in which they aredeployed. \nSpace-accuracy tradeoffs. As a .nal experiment, we analyzed the impact of the memory tracing resolution \nk (see Section 5) on the performance of aprof and on the pro.les it generated on a representative subset \nof CINT and CFP benchmarks. Our tests, reported in Figure 9, show that the mean running time for the \nconsideredbenchmarksisbarely affectedby varying k, while space usage decreases with k as expected. We \nalso studied how the percentage ofroutineswithatleast10tuplesvaries with k, showing that theloss of accuracyforlower \nresolutionsis small compared to the space savings,henceitis reasonable to use k> 1. 7. RelatedWork Performance \npro.ling. Performance pro.ling has been the sub\u00adject of extensive research since the early70 s[27].In \nearlypro\u00ad.lers,performancemeasurements wereassociated toisolated syn\u00adtactic units of a program, such \nas procedures or statements. The importance ofcontextualinformation was recognized early andpi\u00adoneeredbygprof \ns callgraphpro.les[19]. Since a singlelevel of context sensitivity maybeinaccurate[36,39],Ammons,Ball, \nand Larus introduced the calling context tree (CCT), a compact data structure to associate performance \nmetrics with entire paths through aprogram s callgraph[2].A variety oftechniqueshave laterbeenproposedto \nreducethe slowdownincurredbyCCT-based pro.lers that work by exhaustive instrumentation, including sam\u00adpling[16,21,43] \nandbursting[3,24,46].SincetheCCT canbe  4 3 2 1 0 rather automatically discover hidden asymptotic \ninef.ciencies of slowdown 10 5 in the development of real-time systems also address the prob\u00ad 0 1 2 \n4 8 16 1 2 4 8 16 1 2 4 8 16 kkk Figure 9. Impact of memory tracing resolution k on time, space and accuracy, \nas geometric mean computed over SPEC CPU2006 benchmarksbzip2, gobmk, astar, gcc, sphinx3, and gamess. \nvery large and dif.cult to analyze, a different set of works targets spaceissuesin context-sensitivepro.ling[7,13,20]. \nAt the intraprocedural level, context information is encoded bypathpro.les[4],thatdeterminehow many times \neachpathin the control .ow graph executes. The seminal work of Ball and Larus[4] hasspawned much research \non .ow-sensitivepro.ling: see, e.g.,[2,5,6,26,32,37,41]. All these works on context-sensitive pro.ling \naim at associat\u00adingperformance metricstodistinctpathstraversedeitherinthe call graphorinthe control.owgraphduringaprogram \ns execution,but do not exploreinput-sensitivityissues that are the target of thispa\u00adper.Context-andinput-sensitivityrepresenttwoorthogonalaspects \nof program pro.les and can be naturally combined. The method proposed by Goldsmith,Aiken, andWilkerson[18] \nallowsit mea\u00adsure the empirical computational complexity of a program and is much closer in spirit to \nour work: the program is run on different workloads(possibly spanning several orders of magnitudein size), \nthe performance of its routines is measured, and all the observa\u00adtions are .t to a model that predicts \nperformance as a function of the workload size. Differently from aprof, however, the workload size of \nthe program s routines is not computed automatically, but numerical features characterizing the different \nworkloads must be explicitly speci.edby the user. Performanceprediction. Performancepredictionprovidesmeans \nto estimate the running time of a program on different platforms. Pro.le-based prediction tools, which \nare most closely related to ourwork, .rst runbenchmarkprogramsonceunderlightweightin\u00adstrumentationtoolsin \norder togenerate average statisticsfor apro\u00adgram run, and then feed these statistics to analysis tools \nthat com\u00adpute an estimate of the run time on a speci.c machine. While the instrumentation phase runs \nthe entire program, the analysis phase runsin a time roughlyproportional to the number of staticinstruc\u00adtions, \nwhich is typically several orders of magnitude smaller than the number of instructions actually executed. \nIt has been shown in[35] thatthistechnique can accuratelypredicttheperformance of adetailed out-of-orderissueprocessor \nmodel,largelyimproving over earlier static analysis methods.Differentlyfrom our work,the goalin[35] istopredicttheperformance \nof the samebenchmark on differentplatform models, and not how theperformance scales with theinput size: \nrepeated runs of theinstrumentationphase on differentinputs are necessary topinpoint scalabilityissues. \nThe problem of understanding how an application s perfor\u00admance scalesgivendifferentproblem sizesis addressedin[29],de\u00adscribing \na methodology for constructing semi-automatically mod\u00adels of an application s characteristics parameterized \nby problem size. In [29], data from multiple runs with different and deter\u00adminableinputparametersare \n.rstcollected and thenused tocom\u00adpute a curve parameterized by a parameter related to the problem size. \nThis is quite different from our approach, where the input size of each routine activation is automatically \ncomputed by the pro.ler and the analysis of data extracted from a single program run may be suf.cient \nto determine thegrowth rate of the routines performance:with our methodology, we cannotpredicthow an \nen\u00adtire program will scale with different problem sizes, but we can lem of estimating execution times \ndepending on the input data. In particular, measurement-based methods need to direct input\u00addatagenerationin \nsearchfor worst-case orlongprogram execution times. This is fundamentally different from input-sensitive \npro.l\u00ading, which does not aim at identifying worst-case instances, but rather at understanding how the \nexecution cost scales on speci.c workloads as theinput sizegrows. Experimental algorithmics. Although \nworst-case asymptotic analysis provides a strong mathematical framework for the anal\u00adysis ofalgorithms,predicting \nthe actualbehavior of an algorithmic code may stillbe a verydif.culttask[14,30,33], sincegeneral theoretical \nmodels and techniques do not directly .t to actual ex\u00adistingmachines and to real-worldprobleminstances.Experimental \nalgorithmics complements and reinforces theoretical analyses with empirical studies aimed at discovering \neasy and hard instances for aproblem and measuringpracticalindicators(implementation constant factors, \nlocality of references, cache and communication complexity effects) that may be dif.cult to predict theoretically. \nThe problem of inferring asymptotic bounds from experimental datais ratherdif.cultand thereis no sound \nandgenerally accepted solution. Some researchers have nevertheless proposed heuristics for the empirical \ncurve bounding problem, showing their effec\u00adtivenessfor several synthetic and realdatasets[31]. Performance \npro.lers are especially useful in the experimental analysis of algorithmic code.Inparticular,input-sensitivepro.ling \nconjugates asymptotic algorithmic theory with pro.ling practice, and we believe that it may represent \na useful tool in this domain, providing valuable hints to experimenters about asymptotic inef.\u00adcienciesand \ntypical usagescenarios of critical subroutines.More\u00adover,papersin experimental algorithmics only rarely \nanalyze algo\u00adrithms in situ,i.e., withinthe actual context of applicationsthey are deployedin.Our approachmakes \nin situanalysis viable(and easy), thus exposingpossibleperformance effects(e.g., alarger number of cache \nmisses)due to theinteraction withthe overall application. 8. Conclusions We have proposed a novel approach \nto performance pro.ling in\u00adspired by asymptotic cost models. The running time of an algo\u00adrithm is typically \nestimated by theoretical means as a function re\u00adlatingthe size of theinput to the number of steps of \nthe algorithm. On the other side,pro.lers collect runtimeinformation on a single run on a speci.c input: \nthis kind of information, although useful, does not provide insights on code scalability with respect \nto the input size. Our pro.ler relates the measured cost required by the execution of each code fragment \nto the size of the processed in\u00adputdata. In this way, it can induce an estimatedgrowth rate of the running \ntime,pinpointing scalabilityproblems moreprecisely than traditional codepro.ling.Wehave shownthat our \napproachisboth methodologically sound andpractical. Measuringautomatically the size of theinputgivento \nageneric codefragment raises a variety ofinterestingquestions.Namely,the RMS metric is a measure of distinct \naccessed memory cells, but does not considerdata that are not storedin theprocess memory at thebeginning \nof afunction s execution.Forinstance,data received on-line (e.g., reads from external devices such as \nthe network, keyboard, or timer) or non-deterministic input values read from I/Os(e.g.,inputitemsreadinaloop \novera .leiterator) arenot countedin theinput size.Apossible solution to theseissues could hinge upon \nthe techniques for logging non-determinism described in[12].We regardextending our modeltowardsthisdirection \nas an interestingopenissue.  Acknowledgements We would like to thank Randal Bryant and David O Hallaron \nfor their feedback on the dif.culty of discovering asymptotic inef.\u00adciencies in large software applications, \nand to Matthew Hammer for many usefuldiscussions.We are alsoindebted toUmbertoFer\u00adraro Petrillo for hosting \nour experiments on the Intel Xeon cluster machinedescribedinSection6.1 and toBrunoAleandrifordevel\u00adoping \nan earlier version of aprof. Thiswork was supportedinpartby theItalianMinistry ofEd\u00aducation, University, \nand Research (MIUR) under PRIN 2008TF-BWL4 national research project AlgoDEEP: Algorithmic Chal-lengesforData-IntensiveProcessingonEmergingComputingPlat\u00adforms \n. References [1] An ef.cient variation of bubble sort. Information Processing Letters, 11(1):5 6,1980. \n[2] G. Ammons, T. Ball, and J. R. Larus. Exploiting hardware perfor\u00admance counters with .ow and context \nsensitive pro.ling. SIGPLAN Not.,32(5):85 96,1997. [3] M. Arnold and B. Ryder. A framework for reducing \nthe cost of instrumented code. In PLDI,pages168 179. ACM,2001. [4] T. Ball and J. R. Larus. Ef.cient \npath pro.ling. In MICRO 29: Proceedings of the 29th annual ACM/IEEE international symposium onMicroarchitecture,pages46 \n57,1996. [5] T.Ball,P.Mataga, andM.Sagiv.Edgepro.lingversuspathpro.ling: the showdown. In POPL,pages134 \n148.ACM,1998. [6] M.D.Bond andK.S.McKinley.Practicalpathpro.lingfordynamic optimizers. In CGO,pages205 \n216.IEEEComputerSociety,2005. [7] M.D.Bond andK.S.McKinley. Probabilistic calling context. SIG\u00adPLANNot.(Proc.OOPSLA2007),42(10):97 \n112, 2007. [8] R.E.Bryant.Personal communication,September2011. [9] R. E. Bryant and D. R. O Hallaron. \nComputer Systems: A Program\u00admer sPerspective. PearsonEducation,2010. [10] R. E. Bryant, D. L. Beatty, \nK. S. Brace, K. Cho, and T. J. Shef.er. COSMOS: A Compiled Simulator for MOS Circuits. In DAC, pages \n9 16,1987. [11] J.M.Chambers,W.S.Cleveland,B.Kleiner, andP.A.Tukey. Graph\u00adicalMethodsforDataAnalysis.Chapman \nandHall,NewYork,1983. [12] J.Chow,T.Gar.nkel, andP.M.Chen. Decouplingdynamicprogram analysis from execution \nin virtual environments. In USENIX 2008 Annual TechnicalConference,pages1 14,2008. [13] D. C. D Elia, \nC. Demetrescu, and I. Finocchi. Mining hot calling contextsinsmall space. InM.W.HallandD.A.Padua,editors, \nPLDI, pages516 527.ACM,2011. [14] C.Demetrescu,I.Finocchi, andG.F.Italiano.Algorithm engineering. Bulletinof \ntheEATCS(algorithmics column),79:48 63,2003. [15] Fedora Project. wf: simple word frequency counter. \nhttp:// rpmfind.net//linux/RPM/fedora/devel/rawhide/src/w/wf -0.41-6.fc17.src.html. [16] N. Froyd, J. \nMellor-Crummey, and R. Fowler. Low-overhead call path pro.ling of unmodi.ed, optimized code. In Proc. \n19th Annual International Conf. onSupercomputing,pages81 90.ACM,2005. [17] Genome bioinformatics research \nlaboratory. Resources and datasets. http://genome.crg.es/main/databases.html. [18] S. Goldsmith, A. Aiken, \nand D. S. Wilkerson. Measuring empirical computational complexity. In ESEC/SIGSOFT FSE, pages 395 404, \n2007. [19] S.L.Graham,P.B.Kessler,andM.K.McKusick.gprof: a callgraph executionpro.ler(with retrospective).InK.S.McKinley, \neditor, Best ofPLDI,pages49 57.ACM,1982. [20] R.J.Hall. Callpath re.nementpro.les. IEEE Trans.Softw. \nEng.,21 (6):481 496, 1995. [21] R. J. Hall and A. J. Goldberg. Call path pro.ling of monotonic pro\u00adgram \nresources in UNIX. In Proc. Summer 1993 USENIX Technical Conference,pages1 19.USENIXAssociation,1993. \n[22] M.Hart.GutenbergProject. http://www.gutenberg.org/. [23] J. L. Henning. Spec cpu2006 benchmark descriptions. \nSIGARCH Comput.Archit.News,34:1 17,2006. [24] M. Hirzel and T. Chilimbi. Bursty tracing: A framework \nfor low-overhead temporal pro.ling. In Proc. 4th ACM Workshop on Feedback-Directed andDynamicOptimization,2001. \n[25] D.Johnson. Atheoretician sguidetotheexperimental analysisofal\u00adgorithms. In Data Structures, Near \nNeighbor Searches, and Method\u00adology,pages215 250.AmericanMathematicalSociety,2002. [26] R. Joshi, M. \nD. Bond, and C. Zilles. Targeted path pro.ling: Lower overhead path pro.ling for staged dynamic optimization \nsystems. In CGO,pages239 250. IEEEComputerSociety,2004. [27] D. E. Knuth and F. R. Stevenson. Optimal \nmeasurement points for programfrequencycounts. BIT,13:313 322, 1973. [28] T.K\u00a8ustner, J. Weidendorfer, \nand T. Weinzierl. Argument controlled pro.ling. InEuro-Par 09,pages177 184, 2010. [29] G.Marin andJ.M.Mellor-Crummey.Cross-architectureperformance \npredictionsforscienti.c applications usingparameterized models. In Proc.SIGMETRICS2004,pages2 13,2004. \n[30] C. C. McGeoch. Experimental algorithmics. Communications of the ACM,50(11):27 31, 2007. [31] C. \nC. McGeoch, P. Sanders, R. Fleischer, P. R. Cohen, and D. Pre\u00adcup. Using .nite experiments to study asymptotic \nperformance. In ExperimentalAlgorithmics,LNCS2547,pages93 126,2002. [32] D.MelskiandT.W.Reps.Interproceduralpathpro.ling.In \nProc.8th Int.Conf. onCompilerConstruction,LNCS1575,pages47 62,1999. [33] B. M. E. Moret. Towards a discipline \nof experimental algorithmics. InDataStructures,NearNeighborSearches, andMethodology,pages 197 250.AmericanMathematicalSociety,2002. \n[34] N.Nethercote andJ.Seward. Valgrind: aframeworkforheavyweight dynamicbinaryinstrumentation. In PLDI,pages89 \n100,2007. [35] D. Ofelt and J. L. Hennessy. Ef.cient performance prediction for modern microprocessors. \nIn SIGMETRICS,pages229 239,2000. [36] C.Ponder andR.J.Fateman.Inaccuraciesinprogrampro.lers. Softw., \nPract.Exper.,18(5):459 467, 1988. [37] S.RoyandY.N.Srikant.Pro.lingk-iterationpaths:Ageneralization of \ntheball-laruspro.lingalgorithm.In CGO,pages70 80,2009. [38] J.SewardandN.Nethercote.UsingValgrindtodetectunde.nedvalue \nerrors with bit-precision. In USENIX Annual Technical Conference, pages17 30.USENIX,2005. [39] J.M.Spivey.Fast, \naccurate callgraphpro.ling. Softw.,Pract.Exper., 34(3):249 264, 2004. [40] R.E.Tarjan. Ef.ciency of agoodbutnotlinearsetunion \nalgorithm. J.ACM,22(2):215 225, 1975. [41] K.Vaswani,A.V.Nori, andT.M.Chilimbi. Preferentialpathpro.l\u00ading:compactly \nnumberinginterestingpaths.In POPL,pages351 362. ACM,2007. [42] J. Weidendorfer, M. Kowarschik, and C. \nTrinitis. A tool suite for simulation based analysis of memory access behavior. In Int. Conf. onComputational \nScience,LNCS3038,pages440 447,2004. [43] J. Whaley. A portable sampling-based pro.ler for Java virtual \nma\u00adchines.In Proc.ACM2000Conf. onJavaGrande,pages78 87,2000. [44] R. Wilhelm, J. Engblom, A. Ermedahl, \nN. Holsti, S. Thesing, D. B. Whalley,G.Bernat,C.Ferdinand,R.Heckmann,T.Mitra,F.Mueller, I.Puaut,P.P.Puschner,J.Staschulat,andP.Stenstr\u00a8om.Theworst-case \nexecution-time problem -overview of methods and survey of tools. ACMTrans.EmbeddedComput.Syst.,7(3),2008. \n[45] T. Williams and C. Kelley. Gnuplot: command-driven interactive functionplottingprogram. http://www.gnuplot.info/. \n[46] X. Zhuang, M. J. Serrano, H. W. Cain, and J.-D. Choi. Accurate, ef.cient, and adaptive calling contextpro.ling. \nIn PLDI,pages 263 271,2006.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>In this paper we present a profiling methodology and toolkit for helping developers discover hidden asymptotic inefficiencies in the code. From one or more runs of a program, our profiler automatically measures how the performance of individual routines scales as a function of the input size, yielding clues to their growth rate. The output of the profiler is, for each executed routine of the program, a set of tuples that aggregate performance costs by input size. The collected profiles can be used to produce performance plots and derive trend functions by statistical curve fitting or bounding techniques. A key feature of our method is the ability to automatically measure the size of the input given to a generic code fragment: to this aim, we propose an effective metric for estimating the input size of a routine and show how to compute it efficiently. We discuss several case studies, showing that our approach can reveal asymptotic bottlenecks that other profilers may fail to detect and characterize the workload and behavior of individual routines in the context of real applications. To prove the feasibility of our techniques, we implemented a Valgrind tool called aprof and performed an extensive experimental evaluation on the SPEC CPU2006 benchmarks. Our experiments show that aprof delivers comparable performance to other prominent Valgrind tools, and can generate informative plots even from single runs on typical workloads for most algorithmically-critical routines.</p>", "authors": [{"name": "Emilio Coppa", "author_profile_id": "81502806548", "affiliation": "Sapienza University of Rome, Rome, Italy", "person_id": "P3471159", "email_address": "ercoppa@gmail.com", "orcid_id": ""}, {"name": "Camil Demetrescu", "author_profile_id": "81100357279", "affiliation": "Sapienza University of Rome, Rome, Italy", "person_id": "P3471160", "email_address": "demetres@dis.uniroma1.it", "orcid_id": ""}, {"name": "Irene Finocchi", "author_profile_id": "81502771811", "affiliation": "Sapienza University of Rome, Rome, Italy", "person_id": "P3471161", "email_address": "finocchi@di.uniroma1.it", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254076", "year": "2012", "article_id": "2254076", "conference": "PLDI", "title": "Input-sensitive profiling", "url": "http://dl.acm.org/citation.cfm?id=2254076"}