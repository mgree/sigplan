{"article_publication_date": "06-11-2012", "fulltext": "\n Fast and Precise Hybrid Type Inference for JavaScript Brian Hackett Shu-yu Guo * Mozilla {bhackett,shu}@mozilla.com \nAbstract JavaScript performance is often bound by its dynamically typed na\u00adture. Compilers do not have \naccess to static type information, mak\u00ading generation of ef.cient, type-specialized machine code dif.cult. \nWe seek to solve this problem by inferring types. In this paper we present a hybrid type inference algorithm \nfor JavaScript based on points-to analysis. Our algorithm is fast, in that it pays for itself in the \noptimizations it enables. Our algorithm is also precise, generat\u00ading information that closely re.ects \nthe program s actual behavior even when analyzing polymorphic code, by augmenting static anal\u00adysis with \nrun-time type barriers. We showcase an implementation for Mozilla Firefox s Java-Script engine, demonstrating \nboth performance gains and viability. Through integration with the just-in-time (JIT) compiler in Fire\u00adfox, \nwe have improved performance on major benchmarks and JavaScript-heavy websites by up to 50%. Inference-enabled \ncom\u00adpilation is the default compilation mode as of Firefox 9. Categories and Subject Descriptors D.3.4 \n[Processors]: Com\u00adpilers, optimization Keywords type inference, hybrid, just-in-time compilation 1. The \nNeed for Hybrid Analysis Consider the example JavaScript program in Figure 1. This pro\u00adgram constructs \nan array of Box objects wrapping integer values, then calls a use function which adds up the contents \nof all those Box objects. No types are speci.ed for any of the variables or other val\u00adues used in this \nprogram, in keeping with JavaScript s dynamically\u00adtyped nature. Nevertheless, most operations in this \nprogram inter\u00adact with type information, and knowledge of the involved types is needed to compile ef.cient \ncode. In particular, we are interested in the addition res + v on line 9. In JavaScript, addition coerces \nthe operands into strings or numbers if necessary. String concatenation is performed for the former, \nand numeric addition for the latter. Without static information about the types of res and v, a JIT compiler \nmust emit code to handle all possible combinations of operand types. Moreover, every time values are \ncopied around, the compiler must emit code to keep track of the types of the involved * Work partly done \nat the University of California, Los Angeles, Los Ange\u00adles, CA. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. \nCopyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 1 function Box(v) { 2 this.p = v; \n3 } 4 5 function use(a) { 6 var res = 0; 7 for (var i = 0; i < 1000; i++) { 8 var v = a[i].p; 9 res = \nres + v; 10 } 11 return res; 12 } 13 14 function main() { 15 var a = []; 16 for (var i = 0; i < 1000; \ni++) 17 a[i] = new Box(10); 18 use(a); 19 } Figure 1. Motivating Example values, using either a separate \ntype tag for the value or a specialized marshaling format. This incurs a large runtime overhead on the \ngenerated code, greatly increases the complexity of the compiler, and makes effective implementation \nof important optimizations like register allocation and loop invariant code motion much harder. If we \nknew the types of res and v, we could compile code which performs an integer addition without the need \nto check or to track the types of res and v. With static knowledge of all types involved in the program, \nthe compiler can in many cases generate code similar to that produced for a statically-typed language \nsuch as Java, with similar optimizations. We can statically infer possible types for res and v by reasoning \nabout the effect the program s assignments and operations have on values produced later. This is illustrated \nbelow. For brevity, we do not consider the possibility of Box and use being overwritten. 1. On line 17, \nmain passes an integer when constructing Box ob\u00adjects. On line 2, Box assigns its parameter to the result \ns p prop\u00aderty. Thus, Box objects can have an integer property p. 2. Also on line 17, main assigns a \nBox object to an element of a. On line 15, a is assigned an array literal, so the elements of that literal \ncould be Box objects. 3. On line 18, main passes a to use, so a within use can refer to the array created \non line 15. When use accesses an element of a on line 8, per #2 the result can be a Box object. 4. On \nline 8, property p of a value at a[i] is assigned to v. Per #3 a[i] can be a Box object, and per #1 the \np property can be an integer. Thus, v can be an integer.   5. On line 6, res is assigned an integer. \nSince v can be an integer, res + v can be an integer. When that addition is assigned to res on line 9, \nthe assigned type is consistent with the known possible types of res. This reasoning can be captured \nwith inclusion constraints; we compute sets of possible types for each expression and model the .ow between \nthese sets as subset relationships. To compile correct code, we need to know not just some possible types \nfor variables, but all possible types. In this sense, the static inference above is unsound: it does \nnot account for all possible behaviors of the program. A few such behaviors are described below. The \nread of a[i] may access a hole in the array. Out of bounds array accesses in JavaScript produce the undefined \nvalue if the array s prototype does not have a matching property. Such holes can also be in the middle \nof an array; assigning to just a[0] and a[2] leaves a missing value at a[1].  Similarly, the read of \na[i].p may be accessing a missing prop\u00aderty and may produce the undefined value.  The addition res + \nv may over.ow. JavaScript has a single number type which does not distinguish between integers and doubles. \nHowever, it is extremely important for performance that JavaScript compilers distinguish the two and \ntry to repre\u00adsent numbers as integers wherever possible. An addition of two integers may over.ow and \nproduce a number which can only be represented as a double.  In some cases these behaviors can be proven \nnot to occur, but usually they cannot be ruled out. A standard solution is to capture these behaviors \nstatically, but this is unfruitful. The static analysis must be sound, and to be sound in light of highly \ndynamic behav\u00adiors is to be conservative: many element or property accesses will be marked as possibly \nunde.ned, and many integer operations will be marked as possibly over.owing. The resulting type information \nwould be too imprecise to be useful for optimization. Our solution, and our key technical novelty, is \nto combine un\u00adsound static inference of the types of expressions and heap values with targeted dynamic \ntype updates. Behaviors which are not ac\u00adcounted for statically must be caught dynamically, modifying \nin\u00adferred types to re.ect those new behaviors if caught. If a[i] ac\u00adcesses a hole, the inferred types \nfor the result must be marked as possibly undefined. If res + v over.ows, the inferred types for the \nresult must be marked as possibly a double. With or without analysis, the generated code needs to test \nfor array holes and integer over.ow in order to correctly model the semantics of the language. We call \ndynamic type updates based on these events semantic triggers; they are placed on rarely taken execution \npaths and incur a cost to update the inferred types only the .rst time that path is taken. The presence \nof these triggers illustrates the key invariant our analysis preserves: Inferred types must conservatively \nmodel all types for vari\u00adables and object properties which currently exist and have existed in the past, \nbut not those which could exist in the future. This has important implications: The program can be analyzed \nincrementally. Code is not ana\u00adlyzed until it .rst executes, and code which does not execute need not \nbe analyzed. This is necessary for JavaScript due to dynamic code loading and generation. It is also \nimportant for reducing analysis time on websites, which often load several megabytes of code and only \nexecute a fraction of it. Assumptions about types made by the JIT compiler can be invalidated at almost \nany time. This affects the correctness of the JIT-compiled code, and the virtual machine must be able \nto recompile or discard code at any time, especially when that code is on the stack. Dynamic checks \nand the key invariant are also critical to our handling of polymorphic code within a program. Suppose \nsome\u00adwhere else in the program we have new Box(\"hello!\"). Doing so will cause Box objects to be created \nwhich hold strings, illustrating the use of Box as a polymorphic structure. Our analysis does not distinguish \nBox objects created in different places, and the result of the a[i].p access in use will be regarded \nas potentially producing a string. Naively, solving the constraints produced by the analy\u00adsis will mark \na[i].p, v, res + v, and res as all producing either an integer or a string, even if use s runtime behavior \nis actually monomorphic and only works on Box objects containing integers. This problem of imprecision \nleaking across the program is seri\u00adous: even if a program is mostly monomorphic, analysis precision can \neasily be poisoned by a small amount of polymorphic code. We deal with uses of polymorphic structures \nand functions using runtime checks. At all element and property accesses, we keep track of both the set \nof types which could be observed for the access and the set of types which have been observed. The former \nwill be a superset of the latter, and if the two are different then we insert a runtime check, a type \nbarrier, to check for conformance between the resultant value and the observed type set. Mismatches lead \nto updates of the observed type set. For the example program, a type barrier is required on the a[i].p \naccess on line 8. The barrier will test that the value being read is an integer. If a string shows up \ndue to a call to use outside of main, then the possible types of the a[i].p access will be updated, and \nres and v will be marked as possibly strings by resolving the analysis constraints. Type barriers differ \nfrom the semantic triggers described earlier in that the tests they perform are not required by the language \nand do not need to be performed if our analysis is not being used. We are effectively betting that the \nrequired barriers pay for themselves by enabling generation of better code using more precise type information. \nWe have found this to be the case in practice (\u00a74.1.1, \u00a74.2.5). 1.1 Comparison with other techniques \nThe reader may question, Why not use more sophisticated static analyses that produce more precise results? \nOur choice for the static analysis to not distinguish Box objects created in different places is deliberate. \nTo be useful in a JIT setting, the analysis must be fast, and the time and space used by the analysis \nquickly degrade as complexity increases. Moreover, there is a tremendous variety of polymorphic behavior \nseen in JavaScript code in the wild, and to retain precision even the most sophisticated static analysis \nwould need to fall back to dynamic checks some of the time. Interestingly, less sophisticated static \nanalyses do not fare well either. Uni.cation-based analyses undermine the utility of dynamic checks; \nprecision is unrecoverable despite dynamic monitoring. More dynamic compilation strategies, such as the \ntechnique used by V8 s Crankshaft JavaScript engine, generate type-special\u00adized code based on pro.ling \ninformation, without static knowledge of possible argument or heap types [9, 10]. Such techniques will \nde\u00adtermine the types of expressions with similar precision to our anal\u00adysis, but will always require \ntype checks on function arguments or when reading heap values. By modeling all possible types of heap \nvalues, we only need type checks at accesses with type barriers, a difference which signi.cantly improves \nperformance (\u00a74.1.1). We believe that our partitioning of static and dynamic analysis is a sweet spot \nfor JIT compilation of a highly dynamic language. Our main technical contribution is a hybrid inference \nalgorithm for the entirety of JavaScript, using inclusion constraints to unsoundly in\u00adfer types extended \nwith runtime semantic triggers to generate sound type information, as well as type barriers to ef.ciently \nand precisely handle polymorphic code. Our practical contributions include both an implementation of \nour algorithm and a realistic evaluation. The implementation is integrated with the JIT compiler used \nin Fire\u00adfox and is of production quality. Our evaluation has various metrics showing the effectiveness \nof the analysis and modi.ed compiler on benchmarks as well as popular websites, games, and demos. The \nremainder of the paper is organized as follows. In \u00a72 we describe the static and dynamic aspects of our \nanalysis. In \u00a73 we outline implementation of the analysis as well as integration with the JavaScript \nJIT compiler inside Firefox. In \u00a74 we present empirical results. In \u00a75 we discuss related work, and in \n\u00a76 we conclude. 2. Analysis We present our analysis in two parts, the static may-have-type analysis and \nthe dynamic must-have-type analysis. The algorithm is based on Andersen-style (inclusion based) pointer \nanalysis [6]. The static analysis is intentionally unsound with respect to the se\u00admantics of JavaScript. \nIt does not account for all possible behaviors of expressions and statements and only generates constraints \nthat model a may-have-type relation. All behaviors excluded by the type constraints must be detected \nat runtime and their effects on types in the program dynamically recorded. The analysis runs in the browser \nas functions are trying to execute: code is analyzed function-at-a-time. Inclusion based pointer analysis \nhas a worst-case complexity of O(n3) and is very well studied. It has been shown to perform and scale \nwell despite its cubic worst-case complexity [23]. We reaf.rm this with our evaluation; even when analyzing \nlarge amounts of code the presence of runtime checks keeps type sets small and constraints easy to resolve. \nWe describe constraint generation and checks required for a simpli.ed core of JavaScript expressions \nand statements, shown in Figure 2. We let f ,x, y range over variables, p range over property names, \ni range over integer literals, and s range over string literals. The only control .ow in the core language \nis if, which tests for de.nedness, and via anonymous functions fn. The types over which we are trying \nto infer are also shown in Figure 2. The types can be primitive or an object type o.1 The int type indicates \na number expressible as a signed 32-bit integer and is subsumed by number int is added to all type sets \ncontaining number. There is no arrow type for functions, as functions are treated as objects with special \nargument and return properties, as we show below. Finally, we have sets of types which the analysis computes. \n 2.1 Object Types To reason about the effects of property accesses, we need type information for JavaScript \nobjects and their properties. Each object is immutably assigned an object type o. When o . Te for some \nexpression e, then the possible values for e when it is executed include all objects with type o. For \nthe sake of brevity and ease of exposition, our simpli\u00ad.ed JavaScript core only contains the ability \nto construct Object\u00adprototyped object literals via the {} syntax; two objects have the same type when \nthey were allocated via the same literal. In full JavaScript, types are assigned to objects according \nto their prototype: all objects with the same type have the same proto\u00adtype. Additionally, objects with \nthe same prototype have the same 1 In full JavaScript, we also have the primitive types bool and null. \nv ::= undefined | i | s | {} | fn(x) s ret e values e ::= v | x | x + y | x.p | x[i] | f (x) expressions \ns ::= if(x) s else s | x = e | x.p = e | x[i]= e statements t ::= undefined | int | number | string | \no types T ::= P(t) type sets A ::= t . T | A . A | A . A antecedents C ::= T . T | T .B T | A . C |.o \n. T . C constraints Figure 2. Simpli.ed JavaScript Core, Types, and Constraints type, except for plain \nObject, Array and Function objects. Object and Array objects have the same type if they were allocated \nat the same source location, and Function objects have the same type if they are closures for the same \nscript. Object and Function objects which represent builtin objects such as class prototypes, the Math \nobject and native functions are given unique types, to aid later op\u00adtimizations (\u00a72.5). The type of an \nobject is nominal: it is independent from the properties it has. Objects which are structurally identical \nmay have different types, and objects with the same type may have different structures. This is crucial \nfor ef.cient analysis. JavaScript allows addition or deletion of object properties at any time. Using \nstruc\u00adtural typing would make an object s type a .ow-sensitive property, making precise inference harder \nto achieve. Instead, for each object type we compute the possible properties which objects of that type \ncan have and the possible types of those properties. These are denoted as type sets prop(o, p) and index(o). \nThe set prop(o, p) captures the possible types of a non-integer property p for objects with type o, while \nindex(o) captures the possible types of all integer properties of all objects with type o. These sets \ncover the types of both own properties (those directly held by the object) as well as properties inherited \nfrom the object s prototype. Function objects have two additional type sets arg(o) and ret(o), denoting \nthe possible types that the function s single parameter and return value can have. 2.2 Type Constraints \nThe static portion of our analysis generates constraints modeling the .ow of types through the program. \nWe assign to each expression e a type set representing the set of types it may have at runtime, de\u00adnoted \nTe. These constraints are unsound with respect to JavaScript semantics. Each constraint is augmented \nwith triggers to .ll in the remaining possible behaviors of the operation. For each rule, we informally \ndescribe the required triggers. The grammar of constraints are shown in Figure 2. We have the standard \nsubset constraint, .,a barrier subset constraint, .B, the simple conditional constraint, ., and the universal \nconditional constraint. For two type sets X and Y , X . Y means that all types in Y are propagated to \nX at the time of constraint generation, i.e. dur\u00ading analysis. On the other hand, X .B Y means that if \nY contains types that are not in X, then a type barrier is required which updates the types in X according \nto values which are dynamically assigned to the location X represents (\u00a72.3). Conditional constraints \nan\u00adtecedents are limited to conjunctions and disjunctions of set mem\u00adbership tests. For an antecendent \nA and a constraint C, A . C takes on the meaning of C whenever A holds during propagation. The universal \nconditional constraint .o . T . C is like the sim\u00adple conditional constraint, but is always universally \nquanti.ed over all object types of a type set T . Constraint propagation can happen both during analysis \nand at runtime. Barriers may trigger propa\u00adgation at runtime, and the set membership antecedent is checked \n C : (expression + statement) . P(C) undefinedTundefined .{undefined}(UNDEF) iTi .{int}(INT) sTs .{string}(STR) \n {}T{} .{o}where o fresh (OBJ) fn(x) s ret e .. . Tfn(x) s ret e .{o}, . Tx .B arg(o), where o fresh \n(FUN). ret(o) . Te . x 0/(VAR) x + y .. int . Tx . int . Ty . Tx+y .{int}, .. .. int . Tx . number . \nTy . Tx+y .{number}, (ADD) .number . Tx . int . Ty . Tx+y .{number}, . .. string . Tx . string . Ty . \nTx+y .{string} x.p.o . Tx . Tx.p .B prop(o, p)(PROP) x[i].o . Tx . Tx[i] .B index(o)(INDEX) x = eTx \n. Te(A-VAR) x.p = e.o . Tx . prop(o, p) . Te(A-PROP) x[i]= e.o . Tx . index(o) . Te(A-INDEX) .o . \nTf . arg(o) . x, f (x)(APP) .o . Tf . Tf (x) . ret(o) if(x) s1 else s2 C (s1) . C (s2) (IF) Figure 3. \nConstraint Generation Function C during propagation. This is a radical departure from the traditional \nmodel of generating a constraint graph then immediately solving that graph. Rules for the constraint \ngeneration function C are shown in Figure 3. On the lefthand column is the expression or statement to \nbe analyzed, and on the right hand column is the set of constraints generated. Statically analyzing a \nfunction takes the union of the results from applying C to every statement in the method. The UNDEF, \nINT, STR, and OBJ rules for literals and the VAR rule for variables are straightforward. The FUN rule \nfor anonymous function literals is similar to the object literal rule, but also sets up additional constraints \nfor its argument x and its body e. This rule may be thought of as a read out of f s argument and an assignment \ninto f s return value. The body of the function, s, is not analyzed. Recall that we analyze functions \nlazily, so the body is not processed until it is about to execute. The ADD rule is complex, as addition \nin JavaScript is similarly complex. It is de.ned for any combination of values, can perform either a \nnumeric addition, string concatenation, or even function calls if either of its operands is an object \n(calling their valueOf or toString members, producing a number or string). Using unsound modeling lets \nus cut through this complexity. Additions in actual programs are typically used to add two numbers or \nconcatenate a string with something else. We statically model exactly these cases and use semantic triggers \nto monitor the results produced by other combinations of values, at little runtime cost. Note that even \nthe integer addition rule we have given is unsound: the result will be marked as an integer, ignoring \nthe possibility of over.ow. PROP accesses a named property p from the possible objects referred to by \nx, with the result the union of prop(o, p) for all such objects. This rule is complete only in cases \nwhere the object referred to by x (or its prototype) actually has the p property. Accesses on properties \nwhich are not actually part of an object produce undefined. Accesses on missing properties are rare, \nand yet in many cases we cannot prove that an object de.nitely has some property. In such cases we do \nnot dilute the resulting type sets with undefined. We instead use a trigger on execution paths accessing \na missing property to update the result type of the access with undefined. INDEX is similar to PROP, \nwith the added problem that any property of the object could be accessed. In JavaScript, x[\"p\"] is equivalent \nto x.p. If x has the object type o, an index operation can access a potentially in.nite number of type \nsets prop(o, p). Figuring out exactly which such properties are possible is generally intractable. We \ndo not model such arbitrary accesses at all, and treat all index operations as operating on an integer, \nwhich we collapse into a single type set index(o). In full JavaScript, any indexed access which is on \na non-integer property, or is on an integer property which is missing from an object, must be accounted \nfor with triggers in the same manner as PROP. A-VAR, A-PROP and A-INDEX invert the corresponding read \nexpressions. These rules are complete, except that A-INDEX pre\u00adsumes that an integer property is being \naccessed. Again, in full JavaScript, the effects on prop(o, p) resulting from assignments to a string \nindex x[\"p\"] on some x with object type o must be ac\u00adcounted for with runtime checks. APP for function \napplications may be thought of as an assign\u00adment into f s argument and a read out of f s body, or return \nvalue. In other words, it is analogous to FUN with the polarities reversed. Our analysis is .ow-insensitive, \nso the IF rule is simply the union of the constraints generated by the branches. 2.3 Type Barriers As \ndescribed in \u00a71, type barriers are dynamic type checks inserted to improve analysis precision in the \npresence of polymorphic code. Propagation along an assignment X = Y can be modeled statically as a subset \nconstraint X . Y or dynamically as a barrier constraint X .B Y . It is always safe to use one in place \nof the other; in \u00a74.2.5 we show the effect of always using subset constraints in lieu of barrier constraints. \nFor a barrier constraint X .B Y , a type barrier is required whenever X . Y . The barrier dynamically \nchecks that the type of each value .owing across the assignment is actually in X, and updates X whenever \nvalues of a new type are encountered. Thought of another way, the vanilla subset constraint propagates \nall types during analysis. The barrier subset constraint does not propagate types during analysis but \ndefers with dynamic checks, propagating the types only if necessary at runtime. Type barriers are much \nlike dynamic type casts in Java: assign\u00adments from a more general type to a more speci.c type are possible \nas long as a dynamic test occurs for conformance. However, rather than throw an exception (as in Java) \na tripped type barrier will de\u00adspecialize the target of the assignment. The presence or absence of type \nbarriers for a given barrier con\u00adstraint is not monotonic with respect to the contents of the type sets \nin the program. As new types are discovered, new type barriers may be required, and existing ones may \nbecome unnecessary. However, it is always safe to perform the runtime tests for a given barrier. In the \nconstraint generation rules in Figure 3 we present three rules which employ type barriers: PROP, INDEX, \nand FUN. We use barriers on function argument binding to precisely model polymor\u00adphic call sites where \nonly certain combinations of argument types and callee functions are possible. Barriers could be used \nfor other Tuse . {Use}Ta' .B arg(Use) (1) (2) ret(Use) . Tres (3) Ta . {A}Ttmp . {Box} .o . Ttmp . prop(o,p) \n. {int} .o . Ta . index(o) . Ttmp (4) (5) (6) (7) .o . Tuse . arg(o) . Ta (8) .o . Tuse . Tuse(a) . ret(o) \n(9) .o . Ta' . Ttmp2 .B index(o) (10) .o . Ttmp2 . Tv .B prop(o, p) (11) Figure 4. Motivating Example \nConstraints types of assignments, such as at return sites, but we do not do so. Allowing barriers in \nnew places is unlikely to signi.cantly change the total number of required barriers improving precision \nby adding barriers in one place can make barriers in another place un\u00adnecessary.  2.4 Example Constraints \nThe constraint generation rules as presented are not expressive enough to process the motivating example \nin Figure 1 in full; we must .rst desugar the motivating example to look like the simpli.ed core. In \nthe interest of space, we will instead focus on a few interesting lines and provide the desugaring where \nneeded. In the following walkthrough we show how the variable v on line 8 gets the type int. At line \n5, the declaration of the use function needs to be desug\u00adared to assigning an anonymous function to a \nvariable named use.2 This will match FUN. Let us call the fresh object type for the func\u00adtion Use. We \ngenerate (1), (2), and (3). To avoid confusion with the ' variable a used below, the argument of use \nis renamed to a. At line 15, an empty array is created. The core does not handle arrays, but for the \npurposes of constraint generation it is enough to treat them as objects. Thus, this will match OBJ and \nA-VAR. Let us call the fresh object type for this source location A. Combining both rules, we generate \n(4). At line 17, Box objects are created. Though the core does not handle new constructions, we may desugar \na[i] = new Box(10) to tmp = {}; tmp.p = 10; a[i] = tmp. The .rst desugared assign\u00adment matches OBJ and \nA-VAR. Let us call the fresh object type for this source location Box (this desugaring is approximate; \nin prac\u00adtice for new we assign object types to the new object according to the called function s prototype). \nWe can combine the above two rules to generate (5). The second desugared assignment matches A-PROP and \nINT. Again, we can combine these two rules to gen\u00aderate the conditional (6). The third desugared assignment \nmatches A-INDEX. We generate the conditional (7). At this point we prop\u00adagate Box through (6) and A through \n(7), as we know those set memberships hold during analysis. At line 18, there is a call to use. This \nmatches APP and gen\u00aderates the constraints (8) and (9). Note that (2) employs a barrier constraint. Barrier \nconstraints restrict propagation of types to run\u00adtime, so Ta does not propagate to Ta' even though we \nknow stat\u00adically Use . Tuse . And at the current time, before use is actually called, Ta' . Ta , which \nmeans we need to emit a dynamic check, 2 We do not desugar the Box function in the same fashion as it \nis used as a constructor on line 17. or type barrier, that continues the propagation should new types \nbe observed during runtime. Until main is executed to the point where use is called, no fur\u00adther analysis \nis done. That is, we interleave constraint generation, propagation, and code execution. Propagation, \ndue to barrier con\u00adstraints, can happen during analysis and at runtime. Suppose then that main is executed \nand it calls use. First, the type barrier we in\u00adserted at the argument binding for use is triggered, \nand the types of Ta are propagated to Ta' . That is, Ta'.{A}. Line 8 is where we tie together our extant \nconstraints. First it must be desugared to use a temporary: tmp2 = a'[i]; v = tmp2.p. The .rst desugared \nassignment matches INDEX and A-VAR. We combine the two to generate (10). The second desugared assign\u00adment \nmatches PROP and A-VAR, so we generate (11). Both (10) and (11) emit type barriers, so no propagation \noccurs until line 8 executes. The type barrier required by (10) is triggered and propa\u00adgates Box to Ttmp2 \n. Now that Box . Ttmp2 , the type barrier for (11) triggers and propagates int from prop(Box,p) to Tv \n. At this point, as we have observed all possible types of the property access a'[i].p, no dynamic checks \nare required by the barrier constraints in (10) and (11). But this removal may not be permanent. If we \nanalyze new code which adds new types to the type set prop(Box, p), we will need to re-emit the dynamic \ncheck. For instance, if in the future we were to see new Box(\"hello!\") elsewhere in the code, we would \nneed to re-emit the type barrier. 2.5 Supplemental Analyses Semantic triggers are generally cheap, but \nthey nevertheless incur a cost. These checks should be eliminated in as many cases as possible. Eliminating \nsuch checks requires more detailed analysis information. Rather than build additional complexity into \nthe type analysis itself, we use supplemental analyses which leverage type information but do not modify \nthe set of inferred types. We describe the three most important supplemental analyses below, and our \nimplementation contains several others. Integer Over.ow In the execution of a JavaScript program, the \noverall cost of doing integer over.ow checks is very small. On kernels which do many additions, however, \nthe cost can become signi.cant. We have measured over.ow check overhead at 10-20% of total execution \ntime on microbenchmarks. Using type information, we normally know statically where integers are being \nadded. We use two techniques on those sites to remove over.ow checks. First, for simple additions in \na loop (mainly loop counters) we try to use the loop termination condition to compute a range check which \ncan be hoisted from the loop, a standard technique which can only be performed for JavaScript with type \ninformation available. Second, integer additions which are used as inputs to bitwise operators do not \nneed over.ow checks, as bitwise operators truncate their inputs to 32 bit integers. Packed Arrays Arrays \nare usually constructed by writing to their elements in ascending order, with no gaps; we call these \narrays packed. Packed arrays do not have holes in the middle, and if an access is statically known to \nbe on a packed array then only a bounds check is required. There are a large number of ways packed arrays \ncan be constructed, however, which makes it dif.cult to statically prove an array is packed. Instead, \nwe dynamically detect out-of-order writes on an array, and mark the type of the array object as possibly \nnot packed. If an object type has never been marked as not packed, then all objects with that type are \npacked arrays. The packed status of an object type can change dynamically due to out-of-order writes, \npossibly invalidating JIT code. De.nite Properties JavaScript objects are internally laid out as a map \nfrom property names to slots in an array of values. If a property access can be resolved statically to \na particular slot in the array, then the access is on a de.nite property and can be compiled as a direct \nlookup. This is comparable to .eld accesses in a language with static object layouts, such as Java or \nC++. We identify de.nite property accesses in three ways. First, if the property access is on an object \nwith a unique type, we know the exact JavaScript object being accessed and can use the slot in its property \nmap. Second, object literals allocated in the same place have the same type, and de.nite properties can \nbe picked up from the order the literal adds properties. Third, objects created by calling new on the \nsame function will have the same prototype (unless the function s prototype property is overwritten), \nand we analyze the function s body to identify properties it de.nitely adds before letting the new object \nescape. These techniques are sensitive to properties being deleted or recon.gured, and if such events \nhappen then JIT code will be invalidated in the same way as by packed array or type set changes. 3. Implementation \nWe have implemented this analysis for SpiderMonkey, the Java-Script engine in Firefox. We have also modi.ed \nthe engine s JIT compiler, JaegerMonkey, to use inferred type information when generating code. Without \ntype information, JaegerMonkey gener\u00adates code in a fairly mechanical translation from the original Spi\u00adderMonkey \nbytecode for a script. Using type information, we were able to improve on this in several ways: Values \nwith statically known types can be tracked in JIT\u00adcompiled code using an untyped representation. Encoding \nthe type in a value requires signi.cant memory traf.c or marshaling overhead. An untyped representation \nstores just the data com\u00adponent of a value. Additionally, knowing the type of a value statically eliminates \nmany dynamic type tests.  Several classical compiler optimizations were added, including linear scan \nregister allocation, loop invariant code motion, and function call inlining.  These optimizations could \nbe applied without having static type information. Doing so is, however, far more dif.cult and far less \neffective than in the case where types are known. For example, loop invariant code motion depends on \nknowing whether opera\u00adtions are idempotent (in general, JavaScript operations are not), and register \nallocation requires types to determine whether val\u00adues should be stored in general purpose or .oating \npoint regis\u00adters. In \u00a73.1 we describe how we handle dynamic recompilation in response to type changes, \nand in \u00a73.2 we describe the techniques used to manage analysis memory usage. 3.1 Recompilation As described \nin \u00a71, computed type information can change as a result of runtime checks, newly analyzed code or other \ndynamic behavior. For compiled code to rely on this type information, we must be able to recompile the \ncode in response to changes in types while that code is still running. As each script is compiled, we \nkeep track of all type information queried by the compiler. Afterwards, the dependencies are encoded \nand attached to the relevant type sets, and if those type sets change in the future the script is marked \nfor recompilation. We represent the contents of type sets explicitly and eagerly resolve constraints, \nso that new types immediately trigger recompilation with little overhead. When a script is marked for \nrecompilation, we discard the JIT code for the script, and resume execution in the interpreter. We do \nnot compile scripts until after a certain number of calls or loop back edges are taken, and these counters \nare reset whenever discarding JIT code. Once the script warms back up, it will be recompiled using the \nnew type information in the same manner as its initial compilation.  3.2 Memory Management Two major \ngoals of JIT compilation in a web browser stand in stark contrast to one another: generate code that \nis as fast as possible, and use as little memory as possible. JIT code can consume a large amount of \nmemory, and the type sets and constraints computed by our analysis consume even more. We reconcile this \ncon.ict by observing how browsers are used in practice: to surf the web. The web page being viewed, content \nbeing generated, and JavaScript code being run are constantly changing. The compiler and analysis need \nto not only quickly adapt to new scripts that are running, but also to quickly discard regenerable data \nassociated with old scripts that are no longer running frequently, even if the old scripts are still \nreachable and not subject to garbage collection. We do this with a simple trick: on every garbage collection, \nwe throw away all JIT code and as much analysis information as possible. All inferred types are functionally \ndetermined from a small core of type information: type sets for the properties of objects, function arguments, \nand the observed type sets associated with barrier constraints and the semantic triggers that have been \ntripped. All type constraints and all other type sets are discarded, notably the type sets describing \nthe intermediate expressions in a function without barriers on them. This constitutes the great majority \nof the memory allocated for analysis. Should the involved functions warm back up and require recompilation, \nthey will be reanalyzed. In combination with the retained type information, the complete analysis state \nfor the function is then recovered. In Firefox, garbage collections typically happen every several seconds. \nIf the user is quickly changing pages or tabs, unused JIT code and analysis information will be quickly \ndestroyed. If the user is staying on one page, active scripts may be repeatedly recompiled and reanalyzed, \nbut the timeframe between collections keeps this as a small portion of overall runtime. When many tabs \nare open (the case where memory usage is most important for the browser), analysis information typically \naccounts for less than 2% of the browser s overall memory usage. 4. Evaluation We evaluate the effectiveness \nof our analysis in two ways. In \u00a74.1 we compare the performance on major JavaScript benchmarks of a single \ncompiler with and without use of analyzed type information. In \u00a74.2 we examine the behavior of the analysis \non a selection of JavaScript-heavy websites to gauge the effectiveness of the analysis in practice. 4.1 \nBenchmark Performance As described in \u00a73, we have integrated our analysis into the JaegerMonkey JIT compiler \nused in Firefox. We compare perfor\u00admance of the compiler used both without the analysis (JM) and with \nthe analysis (JM+TI). JM+TI adds several major optimiza\u00adtions to JM, and requires additional compilations \ndue to dynamic type changes (\u00a73.1). Figure 5 shows the effect of these changes on the popular SunSpider3 \nJavaScript benchmark. The compilation sections of Figure 5 show the total amount of time spent compiling \nand the total number of script compilations for both versions of the compiler. For JM+TI, compilation \ntime also includes time spent generating and solving type constraints, which is small: 4ms for the entire \nbenchmark. JM performs 146 compi\u00adlations, while JM+TI performs 224, an increase of 78. The total 3 http://www.webkit.org/perf/sunspider/sunspider.html \n JM Compilation JM+TI Compilation \u00d71 Times (ms) \u00d720 Times (ms) Test Time (ms) # Time (ms) # Ratio JM \nJM+TI Ratio JM JM+TI Ratio 3d-cube 2.68 15 8.21 24 3.06 14.1 16.6 1.18 226.9 138.8 0.61 3d-morph 0.55 \n2 1.59 7 2.89 9.8 10.3 1.05 184.7 174.6 0.95 3d-raytrace 2.25 19 6.04 22 2.68 14.7 15.6 1.06 268.6 152.2 \n0.57 access-binary-trees 0.63 4 1.03 7 1.63 6.1 5.2 0.85 101.4 70.8 0.70 access-fannkuch 0.65 1 2.43 \n4 3.76 15.3 10.1 0.66 289.9 113.7 0.39 access-nbody 1.01 5 1.49 5 1.47 9.9 5.3 0.54 175.6 73.2 0.42 access-nsieve \n0.28 1 0.63 2 2.25 6.9 4.5 0.65 143.1 90.7 0.63 bitops-3bit-bits-in-byte 0.28 2 0.58 3 2.07 1.7 0.8 0.47 \n29.9 10.0 0.33 bitops-bits-in-byte 0.29 2 0.54 3 1.86 7.0 4.8 0.69 139.4 85.4 0.61 bitops-bitwise-and \n0.24 1 0.39 1 1.63 6.1 3.1 0.51 125.2 63.7 0.51 bitops-nsieve-bits 0.35 1 0.73 2 2.09 6.0 3.6 0.60 116.1 \n63.9 0.55 control.ow-recursive 0.38 3 0.65 6 1.71 2.6 2.7 1.04 49.4 42.3 0.86 crypto-aes 2.04 14 6.61 \n23 3.24 9.3 10.9 1.17 162.6 107.7 0.66 crypto-md5 1.81 9 3.42 13 1.89 6.1 6.0 0.98 62.0 27.1 0.44 crypto-sha1 \n0.88 7 2.46 11 2.80 3.1 4.0 1.29 44.2 19.4 0.44 date-format-tofte 0.93 21 2.27 24 2.44 16.4 18.3 1.12 \n316.6 321.8 1.02 date-format-xparb 0.88 7 1.26 6 1.43 11.6 14.8 1.28 219.4 285.1 1.30 math-cordic 0.45 \n3 0.94 5 2.09 7.4 3.4 0.46 141.0 50.3 0.36 math-partial-sums 0.47 1 1.03 3 2.19 14.1 12.4 0.88 278.4 \n232.6 0.84 math-spectral-norm 0.54 5 1.39 9 2.57 5.0 3.4 0.68 92.6 51.2 0.55 regexp-dna 0.00 0 0.00 0 \n0.00 16.3 16.1 0.99 254.5 268.8 1.06 string-base64 0.87 3 1.90 5 2.18 7.8 6.5 0.83 151.9 103.6 0.68 string-fasta \n0.59 4 1.70 9 2.88 10.0 7.3 0.73 124.0 93.4 0.75 string-tagcloud 0.54 4 1.54 6 2.85 21.0 24.3 1.16 372.4 \n433.4 1.17 string-unpack-code 0.89 8 2.65 16 2.98 24.4 26.7 1.09 417.6 442.5 1.06 string-validate-input \n0.58 4 1.65 8 2.84 10.2 9.5 0.93 216.6 184.1 0.85 Total 21.06 146 53.13 224 2.52 261.9 246.4 0.94 4703.6 \n3700.3 0.79 Figure 5. SunSpider-0.9.1 Benchmark Results compilation time for JM+TI is 2.52 times that \nof JM, an increase of 32ms, due a combination of recompilations, type analysis and the extra complexity \nof the added optimizations. Despite the signi.cant extra compilation cost, the type-based optimizations \nperformed by JM+TI quickly pay for themselves. The \u00d71 and \u00d720 sections of Figure 5 show the running times \nof the two versions of the compiler and generated code on the benchmark run once and modi.ed to run twenty \ntimes, respectively. In the single run case JM+TI improves over JM by a factor of 1.06. One run of SunSpider \ncompletes in less than 250ms, which makes it dif.cult to get an optimization to pay for itself on this \nbenchmark. JavaScript-heavy webpages are typically viewed for longer than 1/4 of a second, and longer \nexecution times better show the effect of type based optimizations. When run twenty times, the speedup \ngiven by JM+TI increases to a factor of 1.27. Figures 6 and 7 compare the performance of JM and JM+TI \non two other popular benchmarks, the V84 and Kraken5 suites. These suites run for several seconds each, \nfar longer than SunSpider, and show a larger speedup. V8 scores (which are given as a rate, rather than \na raw time; larger is better) improve by a factor of 1.50, and Kraken scores improve by a factor of 2.69. \nAcross the benchmarks, not all tests improved equally, and some regressed compared to the engine s performance \nwithout the analysis. These include the date-format-xparb and string-tagcloud tests in SunSpider, and \nthe RayTrace and RegExp tests in the V8. These are tests which spend little time in JIT code, and perform \nmany side effects in VM code itself. Changes to objects which happen in the VM due to, e.g., the behavior \nof builtin functions, 4 http://v8.googlecode.com/svn/data/benchmarks/v6/run.html 5 http://krakenbenchmark.mozilla.org \n must be tracked to ensure the correctness of type information for the heap. We are working to reduce \nthe overhead incurred by such side effects. 4.1.1 Performance Cost of Barriers The cost of using type \nbarriers is of crucial importance for two reasons. First, if barriers are very expensive then the effectiveness \nof the compiler on websites which require many barriers (\u00a74.2.2) is greatly reduced. Second, if barriers \nare very cheap then the time and memory spent tracking the types of heap values would be unnecessary. \nTo estimate this cost, we modi.ed the compiler to arti.cially in\u00adtroduce barriers at every indexed and \nproperty access, as if the types of all values in the heap were unknown. For benchmarks, this is a great \nincrease above the baseline barrier frequency (\u00a74.2.2). Fig\u00adure 8 gives times for the modi.ed compiler \non the tracked bench\u00admarks. On a single run of SunSpider, performance was even with the JM compiler. \nIn all other cases, performance was signi.cantly better than the JM compiler and signi.cantly worse than \nthe JM+TI compiler. This indicates that while the compiler will still be able to effec\u00adtively optimize \ncode in cases where types of heap values are not well known, accurately inferring such types and minimizing \nthe barrier count is important for maximizing performance.  4.2 Website Performance In this section \nwe measure the precision of the analysis on a variety of websites. The impact of compiler optimizations \nis dif.cult to accurately measure on websites due to confounding issues like differences in network latency \nand other browser effects. Since Test JM JM+TI Ratio Richards 4497 7152 1.59 DeltaBlue 3250 9087 2.80 \nCrypto 5205 13376 2.57 RayTrace 3733 3217 0.86 EarleyBoyer 4546 6291 1.38 RegExp 1547 1316 0.85 Splay \n4775 7049 1.48 Total 3702 5555 1.50 Figure 6. V8 (version 6) Benchmark Scores (higher is better) Test \nJM (ms) JM+TI (ms) Ratio ai-astar 889.4 137.8 0.15 audio-beat-detection 641.0 374.8 0.58 audio-dft 627.8 \n352.6 0.56 audio-fft 494.0 229.8 0.47 audio-oscillator 518.0 221.2 0.43 imaging-gaussian-blur 4351.4 \n730.0 0.17 imaging-darkroom 699.6 586.8 0.84 imaging-desaturate 821.2 209.2 0.25 json-parse-.nancial \n116.6 119.2 1.02 json-stringify-tinderbox 80.0 78.8 0.99 crypto-aes 201.6 158.0 0.78 crypto-ccm 127.8 \n133.6 1.05 crypto-pbkdf2 454.8 350.2 0.77 crypto-sha256-iterative 153.2 106.2 0.69 Total 10176.4 3778.2 \n0.37 Figure 7. Kraken-1.1 Benchmark Results Suite Time/Score vs. JM vs. JM+TI Sunspider-0.9.1 \u00d71 Sunspider-0.9.1 \n\u00d720 Kraken-1.1 262.2 4044.3 7948.6 1.00 0.86 0.78 1.06 1.09 2.10 V8 (version 6) 4317 1.17 0.78 Figure \n8. Benchmark Results with 100% barriers analysis precision directly ties into the quality of generated \ncode, it makes a good surrogate for optimization effectiveness. We modi.ed Firefox to track several precision \nmetrics, all of which operate at the granularity of individual operations. A brief description of the \nwebsites used is included below. Nine popular websites which use JavaScript extensively. Each site was \nused for several minutes, exercising various features.  The membench50 suite6, a memory testing framework \nwhich loads the front pages of 50 popular websites.  The three benchmark suites described in \u00a74.1. \n Seven games and demos which are bound on JavaScript per\u00adformance. Each was used for several minutes \nor, in the case of non-interactive demos, viewed to completion.  A full description of the tested websites \nand methodology used for each is available in the appendix of the full version of the paper. When developing \nthe analysis and compiler we tuned behavior for the three covered benchmark suites, as well as various \nwebsites. 6 http://gregor-wagner.com/tmp/mem50 Besides the benchmarks, no tuning work has been done \nfor any of the websites described here. We address several questions related to analysis precision, listed \nby section below. The answers to these sometimes differ signi.\u00adcantly across the different categories \nof websites. \u00a74.2.1 How polymorphic are values read at access sites? \u00a74.2.2 How often are type barriers \nrequired? \u00a74.2.3 How polymorphic are performed operations? \u00a74.2.4 How polymorphic are the objects used \nat access sites? \u00a74.2.5 How important are type barriers for precision? 4.2.1 Access Site Polymorphism \nThe degree of polymorphism used in practice is of utmost impor\u00adtance for our analysis. The analysis is \nsound and will always com\u00adpute a lower bound on the possible types that can appear at the var\u00adious points \nin a program, so the precision of the generated type in\u00adformation is limited for access sites and operations \nwhich are poly\u00admorphic in practice. We draw the following distinction for the level of precision obtained: \nMonomorphic Sites that have only ever produced a single kind of value. Two values are of the same kind \nif they are either prim\u00aditives of the same type or both objects with possibly different object types. \nAccess sites containing objects of multiple types can often be optimized just as well as sites containing \nobjects of a single type, as long as all the observed object types share common attributes (\u00a74.2.4). \nDimorphic Sites that have produced either strings or objects (but not both), and also at most one of \nthe undefined, null, or a boolean value. Even though multiple kinds are possible at such sites, an untyped \nrepresentation can still be used, as a sin\u00adgle test on the unboxed form will determine the type. The \nun\u00adtyped representation of objects and strings are pointers, whereas undefined, null, and booleans are \neither 0 or 1. Polymorphic Sites that have produced values of multiple kinds. Compiled code must use \na typed representation which keeps track of the value s kind. The inferred precision section of Figure \n9 shows the fractions of dynamic indexed element and property reads which were at a site inferred as \nproducing monomorphic, dimorphic, or polymorphic sets of values. All these sites have type barriers on \nthem, so the set of inferred types is equivalent to the set of observed types. The category used for \na dynamic access is determined from the types inferred at the time of the access. Since the types inferred \nfor an access site can grow as a program executes, dynamic accesses at the same site can contribute to \ndifferent columns over time. Averaged across pages, 84.7% of reads were at monomorphic sites, and 90.2% \nwere at monomorphic or dimorphic sites. The latter .gure is 85.4% for websites, 97.3% for benchmarks, \nand 94.3% for games and demos; websites are more polymorphic than games and demos, but by and large behave \nin a monomorphic fashion. 4.2.2 Barrier Frequency Examining the frequency with which type barriers are \nrequired gives insight to the precision of the model of the heap constructed by the analysis. The Barrier \ncolumn of Figure 9 shows the frequencies of in\u00addexed and property accesses on sampled pages which required \na barrier. Averaged across pages, barriers were required on 41.4% of such accesses. There is a large \ndisparity between websites and other pages. Websites were fairly homogenous, requiring barriers on be\u00ad \n Inferred Precision (%) Arithmetic (%) Indices (%) Test Mono Di Poly Barrier (%) Int Double Other Unknown \nInt Double Other Unknown gmail googlemaps facebook 78 81 73 5 7 11 17 12 16 47 36 42 62 66 43 9 26 0 \n7 3 40 21 5 16 44 60 62 0 6 0 47 30 32 8 4 6 .ickr 71 19 10 74 61 1 30 8 27 0 70 3 grooveshark meebo \n64 78 15 11 21 10 63 35 65 66 1 9 13 18 21 8 28 17 0 0 56 34 16 49 reddit 71 7 22 51 64 0 29 7 22 0 71 \n7 youtube 280slides 83 79 11 3 6 19 38 64 50 48 27 51 19 1 4 0 33 6 0 0 38 91 29 2 membench50 76 11 13 \n49 65 7 18 10 44 0 47 10 sunspider v8bench 99 86 0 7 1 7 7 26 72 98 21 1 7 0 0 0 95 100 0 0 4 0 1 0 kraken \n100 0 0 3 61 37 2 0 100 0 0 0 angrybirds gameboy bullet 97 88 84 2 0 0 1 12 16 93 16 92 22 54 54 78 36 \n38 0 3 0 0 7 7 88 88 79 8 0 20 0 0 0 5 12 1 lights FOTN 97 98 1 1 2 1 15 20 34 39 66 61 0 0 1 0 95 96 \n0 0 4 3 1 0 monalisa 99 1 0 4 94 3 2 0 100 0 0 0 ztype 91 1 9 52 43 41 8 8 79 9 12 0 Average 84.7 5.7 \n9.8 41.4 58.1 25.7 10.0 6.2 63.2 1.7 27.0 7.7 Figure 9. Website Type Pro.ling Results tween 35% and \n74% of accesses (averaging 50%), while bench-Indexed Acc. (%) Property Acc. (%) marks, games and demos \nwere generally much lower, averaging Test Packed Array Uk Def PIC Uk 17.9% except for two outliers above \n90%. The larger proportion of barriers required for websites indicates gmail 90 4 5 31 5712 that heap \nlayouts and types tend to be more complicated for web\u00ad googlemaps 92 1 7 18 77 5 sites than for games \nand demos. Still, the presence of the type barri\u00ad facebook 16 68 16 41 53 6 ers themselves means that \nwe detect as monomorphic the very large .ickr 27 0 73 33 53 14 proportion of access sites which are, \nwith only a small amount of grooveshark 90 2 8 20 66 14 barrier checking overhead incurred by the more \ncomplicated heaps. meebo 57 04340573 The two outliers requiring a very high proportion of barriers reddit \n97 0 345514 do most of their accesses at a small number of sites; the involved youtube 100 0 0 32 49 \n19 objects have multiple types assigned to their properties, which 280slides 88 12 0 23 56 21 leads to \nbarriers being required. Per \u00a74.1.1, such sites will still see membench50 80 4 16 35 58 6 signi.cant \nperformance improvements but will perform worse than sunspider 93 6 1 81 19 0if the barriers were not \nin place. We are building tools to identify v8bench 7 93 064360hot spots and performance faults in order \nto help developers more kraken 99 0 096 40 easily optimize their code. angrybirds 90 0 10 22 76 2 gameboy \n98 0 2 6940 4.2.3 Operation Precision bullet 4 96 032653 The arithmetic and indices sections of Figure \n9 show the frequency lights 97 3 1 21 78 1 of inferred types for arithmetic operations and the index \noperand FOTN 91 6 3 46 54 0 of indexed accesses, respectively. These are operations for which monalisa \n87 0 13 78 22 0 precise type information is crucial for ef.cient compilation, and ztype 100 0 0 23 76 \n0 give a sense of the precision of type information for operations Average 75.2 14.8 10.1 39.4 55.1 5.5 \nwhich do not have associated type barriers. In the arithmetic section, the Int, Double, Other, and Unknown \nFigure 10. Indexed/Property Access Precision columns indicate, respectively, operations on known integers \nwhich give an integer result, operations on integers or doubles which give a double result, operations \non any other type of known value, and operations where at least one of the operand types is unknown. \nOverall, precise types were found for 93.8% of arithmetic opera-In the indices section, the Int, Double, \nOther, and Unknown tions, including 90.2% of operations performed by websites. Com-columns indicate, \nrespectively, that the type of the index, i.e., the paring websites with other pages, websites tend to \ndo far more type of i in an expression such as a[i], is known to be an integer, a arithmetic on non-numeric \nvalues 27.8% vs. 0.5% and con-double, any other known type, or unknown. Websites tend to have siderably \nless arithmetic on doubles 13.1% vs. 46.1%. more unknown index types than both benchmarks and games. \n Precision Arithmetic Test Poly (%) Ratio Unknown (%) Ratio gmail googlemaps facebook 46 38 48 2.7 3.2 \n3.0 32 23 20 1.5 4.6 1.3 .ickr 61 6.1 39 4.9 grooveshark meebo 58 36 2.8 3.6 30 28 1.4 3.5 reddit 37 \n1.7 13 1.9 youtube 280slides 40 76 6.7 4.0 28 93 7.0 membench50 47 3.6 29 2.9 sunspider v8bench 5 18 \n 2.6 6 1 kraken 2  2 angrybirds gameboy bullet 90 15 62 1.3 3.9 93 7 79 1.0 11.3 lights FOTN 37 28 \n  63 57 monalisa 44  41 ztype 54 6.0 63 7.9 Average 42.1 4.3 37.4 6.0 Figure 11. Type Pro.les Without \nBarriers   4.2.4 Access Site Precision Ef.ciently compiling indexed element and property accesses re\u00adquires \nknowledge of the kind of object being accessed. This infor\u00admation is more speci.c than the monomorphic/polymorphic \ndis\u00adtinction drawn in \u00a74.2.1. Figure 10 shows the fractions of indexed accesses on arrays and of all \nproperty accesses which were opti\u00admized based on static knowledge. In the indexed access section, the \nPacked column shows the fraction of operations known to be on packed arrays (\u00a72.5), while the Array column \nshows the fraction known to be on arrays not known to be packed. Indexed operations behave differently \non ar\u00adrays vs. other objects, and avoiding dynamic array checks achieves some speedup. The Uk column \nis the fraction of dynamic accesses on arrays which are not statically known to be on arrays. Static \ndetection of array operations is very good on all kinds of sites, with an average of 75.2% of accesses \non known packed arrays and an additional 14.8% on known but possibly not packed arrays. A few outlier \nwebsites are responsible for the great majority of accesses in the latter category. For example, the \nV8 Crypto benchmark contains almost all of the benchmark s array accesses, and the arrays used are not \nknown to be packed due to the top down order in which they are initialized. Still, speed improvements \non this benchmark are very large. In the property access section of Figure 10, the Def column shows the \nfraction of operations which were statically resolved as de.nite properties (\u00a72.5), while the PIC column \nshows the fraction which were not resolved statically but were matched using a fall\u00adback mechanism, polymorphic \ninline caches [14]. The Uk column is the fraction of operations which were not resolved either stati\u00adcally \nor with a PIC and required a call into the VM; this includes accesses where objects with many different \nlayouts are used, and accesses on rare kinds of properties such as those with scripted get\u00adters or setters. \nThe Def column gives a measurement of how many times dur\u00ading execution dynamic checks were avoided. Since \nour analysis is hybrid, we cannot (nor does it make sense to) measure how many dynamic checks are statically \nremoved, as a site whose dynamic checks have been removed may have them re-added due to invali\u00addation \nof analysis results. An average of 39.4% of property accesses were resolved as def\u00adinite properties, \nwith a much higher average proportion on bench\u00admarks of 80.3%. The remainder were mostly handled by PICs, \nwith only 5.5% of accesses requiring a VM call. Together, these suggest that objects on websites are \nby and large constructed in a consis\u00adtent fashion, but that our detection of de.nite properties needs \nto be more robust on object construction patterns seen on websites but not on benchmarks. 4.2.5 Precision \nWithout Barriers To test the practical effect of using type barriers to improve preci\u00adsion, we repeated \nthe above website tests using a build of Firefox where subset constraints were used in place of barrier \nconstraints, and type barriers were not used at all (semantic triggers were still used). Some of the \nnumbers from these runs are shown in Figure 11. The precision section shows the fraction of indexed and \nprop\u00aderty accesses which were inferred as polymorphic, and the arith\u00admetic section shows the fraction \nof arithmetic operations where at least one operand type was unknown. Both sections show the ratio of \nthe given fraction to the comparable fraction with type barriers enabled, with entries struck out when \nthe comparable fraction is near zero. Overall, with type barriers disabled 42.1% of accesses are polymorphic \nand 37.4% of arithmetic operations have operands of unknown type; precision is far worse than with type \nbarriers. Benchmarks are affected much less than other kinds of sites, which makes it dif.cult to measure \nthe practical performance im\u00adpact of removing barriers. These benchmarks use polymorphic structures much \nless than the web at large. 5. Related Work There is an enormous literature on points-to analysis, JIT \ncompila\u00adtion, and type inference. We only compare against a few here. The most relevant work on type \ninference for JavaScript to the current work is Logozzo and Venter s work on rapid atomic type analysis \n[16]. Like ours, their analysis is also designed to be used online in the context of JIT compilation \nand must be able to pay for itself. Unlike ours, their analysis is purely static and much more sophisticated, \nutilizing a theory of integers to better infer integral types vs. .oating point types. We eschew sophistication \nin favor of simplicity and speed. Our evaluation shows that even a much sim\u00adpler static analysis, when \ncoupled with dynamic checks, performs very well in the wild . Our analysis is more practical: we have \nimproved handling of what Logozzo and Venter termed havoc statements, such as eval, which make static \nanalysis results im\u00adprecise. As Richards et al. argued in their surveys, real-world use of eval is pervasive, \nbetween 50% and 82% for popular websites [20, 21]. Other works on type inference for JavaScript are more \nformal. The work of Anderson et al. describes a structural object type sys\u00adtem with subtyping over an \nidealized subset of JavaScript [7]. As the properties held by JavaScript objects change dynamically, \nthe structural type of an object is a .ow-sensitive property. Thiemann and Jensen et al. s typing frameworks \napproach this problem by us\u00ading recency types [15, 24]. The work of Jensen et al. is in the con\u00adtext \nof better tooling for JavaScript, and their experiments suggest that the algorithm is not suitable for \nonline use in a JIT compiler. Again, these analyses do not perform well in the presence of stati\u00adcally \nuncomputable builtin functions such as eval. Performing static type inference on dynamic languages has \nbeen proposed at least as early as Aiken and Murphy [4]. More related in spirit to the current work are \nthe works of the the implemen\u00adtors of the Self language [25]. In implementing type inference for JavaScript, \nwe faced many challenges similar to what they faced decades earlier [1, 26]. Agesen outlines the design \nspace for type inference algorithms along the dimensions of ef.ciency and preci\u00adsion. We strived for \nan algorithm that is both fast and ef.cient, at the expense of requiring runtime checks when dealing \nwith com\u00adplex code. Tracing JIT compilers [11, 12] have precise information about the types of expressions, \nbut solely using type feedback limits the optimizations that can be performed. Reaching peak performance \nrequires static knowledge about the possible types of heap values. Agesen and H\u00f6lzle compared the static \napproach of type infer\u00adence with the dynamic approach of type feedback and described the strengths and \nweaknesses of both [2]. Our system tries to achieve the best of both worlds. The greatest dif.culty in \nstatic type in\u00adference for polymorphic dynamic languages, whether functional or object-oriented, is the \nneed to compute both data and control .ow during type inference. We solve this by using runtime information \nwhere static analyses do poorly, e.g. determining the particular .eld of a polymorphic receiver or the \nparticular function bound to a vari\u00adable. Our type barriers may be seen as a type cast in the context \nof Glew and Palsberg s work on method inlining [13]. Framing the type inference problem as a .ow problem \nis a well\u00adknown approach [17, 18]; practical examples include Self s infer\u00adencer [3]. Aiken and Wimmers \npresented general results on type inference using subset constraints [5]. More recently, Rastogi et al. \nhas described a gradual type inference algorithm for ActionScript, an optionally-typed language related \nto JavaScript [19]. They rec\u00adognized that unlike static type systems, hybrid type systems need not guarantee \nthat every use of a variable be safe for every de.\u00adnition of that variable. As such, their type inference \nalgorithm is also encoded as a .ow problem. Though their type system itself is hybrid it admits the \ndynamic type their analysis is static. Other hybrid approaches to typing exist, such as Cartwright and \nFagan s soft typing and Taha and Siek s gradual typing [8, 22]. They have been largely for the purposes \nof correctness and early error detection, though soft typing has been successfully used to eliminate \nruntime checks [27]. We say these approaches are at least partially prescriptive, in that they help enforce \na typing discipline, while ours is entirely descriptive, in that we are inferring types only to help \nJIT compilation. 6. Conclusion and Future Work We have described a hybrid type inference algorithm that \nis both fast and precise using constraint-based static analysis and runtime checks. Our production-quality \nimplementation integrated with the JavaScript JIT compiler inside Firefox has demonstrated the anal\u00adysis \nto be both effective and viable. We have presented compelling empirical results: the analysis enables \ngeneration of much faster code, and infers precise information on both benchmarks and real websites. \nWe hope to look more closely at type barriers in the future with the aim to reduce their frequency without \ndegrading precision. We also hope to look at capturing more formally the hybrid nature of our algorithm. \nAcknowledgements. We thank the Mozilla JavaScript team, Alex Aiken, Dave Herman, Todd Millstein, Jens \nPalsberg, and Sam Tobin-Hochstadt for draft reading and helpful discussion. References [1] O. Agesen. \nConstraint-Based Type Inference and Parametric Poly\u00admorphism, 1994. [2] O. Agesen and U. H\u00f6lzle. Type \nfeedback vs. concrete type infer\u00adence: A comparison of optimization techniques for object-oriented languages. \nIn OOPSLA, pages 91 107, 1995. [3] O. Agesen, J. Palsberg, and M. I. Schwartzbach. Type Inference of \nSelf: Analysis of Objects with Dynamic and Multiple Inheritance. In ECOOP, pages 247 267, 1993. [4] A. \nAiken and B. R. Murphy. Static Type Inference in a Dynamically Typed Language. In POPL, pages 279 290, \n1991. [5] A. Aiken and E. L. Wimmers. Type Inclusion Constraints and Type Inference. In FPCA, pages 31 \n41, 1993. [6] L. O. Andersen. Program Analysis and Specialization for the C Pro\u00adgramming Language. PhD \nthesis, DIKU, University of Copenhagen, 1994. [7] C. Anderson, S. Drossopoulou, and P. Giannini. Towards \nType Infer\u00adence for JavaScript. In ECOOP, pages 428 452, 2005. [8] R. Cartwright and M. Fagan. Soft Typing. \nIn PLDI, pages 278 292, 1991. [9] C. Chambers. The Design and Implementation of the SELF Com\u00adpiler, an \nOptimizing Compiler for Object-Oriented Programming Lan\u00adguages. PhD thesis, Department of Computer Science, \nStanford, 1992. [10] C. Chambers and D. Ungar. Customization: Optimizing Compiler Technology for SELF, \nA Dynamically-Typed Object-Oriented Pro\u00adgramming Language. In PLDI, 1989. [11] M. Chang, E. W. Smith, \nR. Reitmaier, M. Bebenita, A. Gal, C. Wim\u00admer, B. Eich, and M. Franz. Tracing for Web 3.0: Trace Compilation \nfor the Next Generation Web Applications. In VEE, pages 71 80, 2009. [12] A. Gal, B. Eich, M. Shaver, \nD. Anderson, D. Mandelin, M. R. Haghighat, B. Kaplan, G. Hoare, B. Zbarsky, J. Orendorff, J. Ru\u00adderman, \nE. W. Smith, R. Reitmaier, M. Bebenita, M. Chang, and M. Franz. Trace-based just-in-time type specialization \nfor dynamic languages. In PLDI, pages 465 478, 2009. [13] N. Glew and J. Palsberg. Type-Safe Method \nInlining. In ECOOP, pages 525 544, 2002. [14] U. H\u00f6lzle, C. Chambers, and D. Ungar. Optimizing Dynamically-Typed \nObject-Oriented Languages With Polymorphic Inline Caches. In ECOOP, pages 21 38, 1991. [15] S. H. Jensen, \nA. M\u00f8ller, and P. Thiemann. Type Analysis for Java-Script. In SAS, pages 238 255, 2009. [16] F. Logozzo \nand H. Venter. RATA: Rapid Atomic Type Analysis by Abstract Interpretation. Application to JavaScript \nOptimization. In CC, pages 66 83, 2010. [17] N. Oxh\u00f8j, J. Palsberg, and M. I. Schwartzbach. Making Type \nInfer\u00adence Practical. In ECOOP, 1992. [18] J. Palsberg and M. I. Schwartzbach. Object-Oriented Type Inference. \nIn OOPSLA, 1991. [19] A. Rastogi, A. Chaudhuri, and B. Homer. The Ins and Outs of Gradual Type Inference. \nIn POPL, pages 481 494, 2012. [20] G. Richards, S. Lebresne, B. Burg, and J. Vitek. An analysis of the \ndynamic behavior of JavaScript programs. In PLDI, pages 1 12, 2010. [21] G. Richards, C. Hammer, B. Burg, \nand J. Vitek. The Eval That Men Do A Large-Scale Study of the Use of Eval in JavaScript Applications. \nIn ECOOP, pages 52 78, 2011. [22] J. G. Siek and W. Taha. Gradual Typing for Objects. In ECOOP, 2007. \n[23] M. Sridharan and S. J. Fink. The Complexity of Andersen s Analysis in Practice. In SAS, pages 205 \n221, 2009. [24] P. Thiemann. Towards a Type System for Analyzing JavaScript Pro\u00adgrams. In ESOP, pages \n408 422, 2005. [25] D. Ungar and R. B. Smith. Self: The Power of Simplicity. In OOPSLA, pages 227 242, \n1987. [26] D. Ungar, R. B. Smith, C. Chambers, and U. H\u00f6lzle. Object, Message, and Performance: How they \nCoexist in Self. Computer, 25:53 64, October 1992. ISSN 0018-9162. [27] A. K. Wright and R. Cartwright. \nA Practical Soft Type System for Scheme. ACM Trans. Program. Lang. Syst., 19(1):87 152, 1997.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>JavaScript performance is often bound by its dynamically typed nature. Compilers do not have access to static type information, making generation of efficient, type-specialized machine code difficult. We seek to solve this problem by inferring types. In this paper we present a hybrid type inference algorithm for JavaScript based on points-to analysis. Our algorithm is <i>fast</i>, in that it pays for itself in the optimizations it enables. Our algorithm is also <i>precise</i>, generating information that closely reflects the program's actual behavior even when analyzing polymorphic code, by augmenting static analysis with run-time type barriers.</p> <p>We showcase an implementation for Mozilla Firefox's JavaScript engine, demonstrating both performance gains and viability. Through integration with the just-in-time (JIT) compiler in Firefox, we have improved performance on major benchmarks and JavaScript-heavy websites by up to 50%. Inference-enabled compilation is the default compilation mode as of Firefox 9.</p>", "authors": [{"name": "Brian Hackett", "author_profile_id": "81320490544", "affiliation": "Mozilla, Mountain View, CA, USA", "person_id": "P3471212", "email_address": "bhackett@mozilla.com", "orcid_id": ""}, {"name": "Shu-yu Guo", "author_profile_id": "81479643835", "affiliation": "Mozilla &#38; University of California, Los Angeles, Mountain View, CA, USA", "person_id": "P3471213", "email_address": "shu@mozilla.com", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254094", "year": "2012", "article_id": "2254094", "conference": "PLDI", "title": "Fast and precise hybrid type inference for JavaScript", "url": "http://dl.acm.org/citation.cfm?id=2254094"}