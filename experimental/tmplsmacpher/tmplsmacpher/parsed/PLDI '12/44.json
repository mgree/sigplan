{"article_publication_date": "06-11-2012", "fulltext": "\n Logical Inference Techniques for Loop Parallelization Cosmin E. Oancea Lawrence Rauchwerger HIPERFIT, \nDepartment of Computer Science Parasol Lab, Texas A&#38;M University University of Copenhagen, cosmin.oancea@diku.dk \nrwerger@cse.tamu.edu Abstract This paper presents a fully automatic approach to loop paralleliza\u00adtion \nthat integrates the use of static and run-time analysis and thus overcomes many known dif.culties such \nas nonlinear and indi\u00adrect array indexing and complex control .ow. Our hybrid analysis framework validates \nthe parallelization transformation by verify\u00ading the independence of the loop s memory references. To \nthis end it represents array references using the USR (uniform set repre\u00adsentation) language and expresses \nthe independence condition as an equation, S = \u00d8, where S is a set expression representing ar\u00adray indexes. \nUsing a language instead of an array-abstraction rep\u00adresentation for S results in a smaller number of \nconservative ap\u00adproximations but exhibits a potentially-high runtime cost. To alle\u00adviate this cost we \nintroduce a language translation F from the USR set-expression language to an equally rich language of \npredicates (F(S) . S = \u00d8). Loop parallelization is then validated using a novel logic inference algorithm \nthat factorizes the obtained com\u00adplex predicates (F(S)) into a sequence of suf.cient-independence conditions \nthat are evaluated .rst statically and, when needed, dy\u00adnamically, in increasing order of their estimated \ncomplexities. We evaluate our automated solution on 26 benchmarks from PERFECT-CLUB and SPEC suites and \nshow that our approach is effective in parallelizing large, complex loops and obtains much better full \npro\u00adgram speedups than the Intel and IBM Fortran compilers. Categories and Subject Descriptors D.1.3 \n[Concurrent Pro\u00adgramming]: Parallel Programming; D.3.4 [Processors]: Compiler General Terms Performance, \nDesign, Algorithms Keywords auto-parallelization, USR, independence predicates. 1. Introduction Automatic \nloop parallelization requires the analysis of memory references for the purpose of establishing their \ndata independence. For array references, compilers have used data dependence analysis which has historically \ntaken two distinct directions: Static (compile time) analysis and run-time (dynamic) analysis. Static \ndependence analysis, .rst proposed by [2, 4, 12, 22], analyzes an entire loop nest by modeling the data \ndependencies between any pair of read/write accesses. While this technique can also drive powerful code \ntransformations such as loop interchange, skewing, etc., they are typically limited to the af.ne array \nsubscript domain and their effectiveness [21] is limited to relatively-small Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. \nCopyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 loop nests. However, in the more \ngeneral case, there are many obstacles to loop parallelization such as dif.cult to analyze sym\u00adbolic \nconstants, complex control .ow, array-reshaping at call sites, quadratic array indexing, induction variables \nwith no closed-form solutions [6, 13, 20]. Various techniques have been proposed to partially address \nsome of these dif.culties. For example, a class of index-array and stack-like accesses may be disambiguated \nvia access pattern-matching techniques [15, 16] and some monotonic accesses (e.g., quadratic indexing) \ncan be disambiguated via non\u00adaf.ne dependency tests [7]. Similarly, Presburger arithmetic can be extended \nto the non-af.ne domain [24] to solve a class of irregular accesses and control-.ow. The next step has \nbeen to extend analysis to program level by summarizing accesses interprocedurally, where an array abstraction \nis used to represent a (regular) set of memory references either via systems of af.ne constraints [13], \nor linear\u00admemory-access descriptors [20], respectively. Loop independence has been modeled typically \nvia an equation on summaries of shape S = \u00d8. To improve precision, summaries are paired with predi\u00adcates \n[14, 18] that guard (otherwise unsafe) simpli.cations in the array-abstraction domain or predicate the \nsummary existence (i.e., control-.ow conditions). Run-time analysis is necessary and useful when static \nanalysis alone cannot decide whether a loop is independent or not and thus needs to make the conservative \nchoice, i.e., not parallel. Run-time analysis can always resolve static ambiguities because it can use \ninstances of symbols and thus produce accurate results. Histori\u00adcally, dynamic dependence analysis has \nbeen performed by tracing and analyzing a loop s relevant memory references by executing an inspector \nloop (inspector/executor model [26]), or by tracing and analyzing a speculative parallel execution of \nthe loop as it is done in thread-level speculation [25]. Such approaches typically extract maximal parallelism, \nat the cost of signi.cant overhead, usually proportional to the number of traced dynamic memory references. \nIn order to attain our goal of effectively parallelizing a large number of codes automatically we have \ndevised a hybrid compiler technology that can extract maximum static information so that the overhead \nof the dynamic analysis is reduced to a minimum. 1.1 Static Analysis with Light Weight Dynamic Complement \nSummary based static analysis based on the array abstraction has been shown by previous research to be \nmore scalable and useful for loop parallelization. However, from our experience we have found that its \nmain source of inaccuracy lies in the fact that the array abstraction does not form a closed algebra \nunder the required set operations: (recurrence) union, intersection, subtraction, gates, call sites. \nThis shortcoming results in the necessity of conserva\u00adtive approximations during early construction stages, \nboth at the array-abstraction and its associated-predicate levels. Thus either fewer loops are quali.ed \nas parallel or more dynamic analysis is needed. To mitigate this lack of scalability we have adopted \na more expressive, higher level intermediate representation language. We are using the USR(Uniform Set \nRepresentation) [28], a composable language that subsumes both the array abstraction as well as the Figure \n1. Simpli.ed Loop SOLVH DO20 from DYFESM Bench.  SUBROUTINE solvh(HE,XE,IA,IB) SUBROUTINE geteu(XE,SYM,NP) \nDIMENSION HE(32, *), XE(*) DIMENSION XE(16,*) READ(*,*) SYM, NS, NP, N CCC SOLVH_do20 IF (SYM .NE. 1) \nTHEN DO i = 1, N, 1 DO i = 1, NP, 1 DO k = 1, IA(i), 1 DO j = 1, 16, 1 id = IB(i) + k -1 XE(j, i) = ... \nCALL geteu (XE, SYM, NP) ENDDO CALL matmult(HE(1,id),XE,NS) ENDDO CALL solvhe (HE(1,id), NP) ENDIF ENDDO \nEND ENDDO END SUBROUTINE solvhe(HE,NP) SUBROUTINE matmult(HE,XE,NS) DIMENSION HE(8, *) DIMENSION HE(*), \nXE(*) DO j = 1, 3, 1 DO j = 1, NS, 1 DO i = 1, NP, 1 HE(j) = XE(j) HE(j, i)=HE(j, i)+.. XE(j) = ... ENDDO \nENDDO END ENDDO END control .ow. It includes in the language results of operations pre\u00adviously deemed \noutside the array-abstraction domain. Thus we can express the loop data independence summary equation \nas a the set equation S = \u00d8. Sometimes this equation is easy to prove statically. However, usually for \nreal codes, the USR which needs to proven empty is very complex. Furthermore, for outer loops, the set \nex\u00adpressions become very long and cumbersome to deal with during compilation. To deal with this problem \nin a scalable manner, i.e., for outer, program level loops, we de.ne an equally-rich language of pred\u00adicates \n(PDAG) and an effective rule-based translation between the two languages, such that the result predicate \np is a suf.cient con\u00addition for loop independence: p .{S = \u00d8}. This transformation shifts the effort \nof proving loop independence from manipulating set expressions to that of handling logical expressions. \nOur transla\u00adtion scheme is general, allows (later) expansion and builds a less\u00adconstrained predicate \nprogram with fewer conservative approxima\u00adtions than those of related approaches (e.g., p s input symbols \nneed not be read-only). Finally, we have developed a powerful and exten\u00adsible logical inference based \nfactorization algorithm that generates a set of suf.cient conditions for parallelism. Some of these factors \ncan be disambiguated statically and others need to be evaluated dy\u00adnamically if aggressive parallelization \nis to be achieved. The gen\u00aderated suf.cient run-time tests are ordered based on their estimated complexity \nand/or probability of success. Depending on the level of risk desired, the run-time complexity of the \ndynamic tests can be upper bounded during compilation.  1.2 A Simple Code Example Figure 1 shows the \nsimpli.ed version of loop SOLVH DO20 from the dyfesm benchmark. We will now analyze the references to \narrays XE and HE to establish loop independence. If we consider XE and HE as unidimensional arrays, we \ncan observe that XE is written in subroutine geteu on all indexes belonging to interval [1,16*NP] whenever \nSYM.NE.1 holds, and is read in matmult on indexes in [1,NS]. Similarly, HE is written in matmult on all \nindexes in interval [t+1,t+NS], and is read and written in solvhe on a subset of indexes in interval \n[t +1,t +8*NP-5], where t=32*(id-1) is the array offset of parameter HE(1,id). Flow independence of the \noutermost loop is established by showing that the per-iteration read sets of XE and HE are covered by \ntheir respective per-iteration write sets. This corresponds to solving equations [1,NS].[1,16*NP] and \n[t +1,t +8*NP-5].[t +1,t +NS] that result in the independence predicates SYM.NE.1 . NS=16*NP and 8*NP<NS+6 \nfor arrays XE and HE, respectively. Examining output independence, we observe that the per\u00aditeration \nwrite set of array XE is invariant to the outermost loop, hence XE can be privatized and updated with \nthe values written by the last iteration (i.e., static-last value SLV). As discussed in Sec\u00adtion 3.3, \na successful predicate that proves that the per-iteration writes of HE do not overlap across iterations, \nhence HE s output independence, is: .N-1NS=32*(IB(i+1)-IA(i)-IB(i)+1). i=1 Finally, proving solveh do20 \nindependent requires predicates derived from both summary equations, such as NS=16*NP, and control .ow, \nsuch as SYM.NE.1. Also HE s output independence predicate requires a recurrence-based formula. The overhead \nrepre\u00adsented by the dynamic predicate evaluation is negligible: O(1) and O(N), respectively, compared \nwith the loop s O(N2) complexity. Our set to predicate translation and predicate factorization ap\u00adproach \ncan be applied beyond parallelization to optimize prob\u00adlems such as last-value assignment and reduction \nparallelization. Furthermore, our design is open ended because it can readily ac\u00adcept more rules for \ntranslation and factorization. The monotonicity\u00adbased techniques in Section 3.3 represent such an extension \nexam\u00adple which has been well integrated in our framework.  1.3 Main Contributions The main contributions \nof this paper are: A compiler framework that integrates a language translation F from the USR set-expression \nlanguage to a rich language of predicates (F(S) . S = \u00d8).  A novel and extensible logic inference algorithm \nthat factor\u00adizes complex predicates (F(S)) into a sequence of suf.cient\u00adindependence conditions which \ncan be evaluated statically and, when necessary, dynamically.  an experimental evaluation on twenty \nsix Fortran benchmarks from PERFECT-CLUB, SPEC92, 2000, and 2006 suites.  The experimental evaluation \ndemonstrates that: (i) the extracted predicates are successful (accurate) in most cases, (ii) the runtime \noverhead of these predicates is negligible, i.e., under 1% of the parallel runtime, while the other parallelism-enabling \ntechniques show scalable speedup, and that (iii) we obtain speedups as high as 4.5x and 8.4x and on average \n2.4x and 5.4x on four and eight processors, respectively, which are superior to those obtained by INTEL \ns ifort and IBM s xlf r commercial compilers. Compared with results reported in the literature, the evaluation \nshows that (i) our novel factorization scheme parallelized a num\u00adber of previously unreported benchmarks \nvia light-weight predi\u00adcates, (ii) our uni.ed framework solves a class of non-af.ne loops that have been \npreviously analyzed with a number of different techniques, and that (iii) we match previous results obtained \nby SUIF [13] and POLARIS [5] on the statically-analyzable codes. 2. Preliminary Concepts: Summary Construction \nOur solution to automatic, run-time parallelization builds on Rus, Pennings and Rauchwerger s hybrid \nanalysis [28] which we brie.y review. The main idea is to summarize accesses into read-only (RO), write-.rst \n(WF), and read-write (RW) abstract sets [14]. This is achieved by constructing summaries via interprocedural, \nstructural data-.ow analysis and representing them accurately in a scoped, closed-under-composition language, \nnamed uni.ed set reference (USR). In this setting, the loop independence is derived from examining whether \nan summaries equation (S = \u00d8) holds. 2.1 Uni.ed Set Reference (USR) Construction Summaries are constructed \nduring a bottom-up parse of the call and control dependency graphs (CDG) of a structured program in SSA \nrepresentation, where within a CDG region nodes are traversed in  COMPOSE (REG1; REG2) AGGREGATE (REGi, \ni =1,N) (WF1, RO1, RW1) . REG1 (WFi, ROi, RWi) . REGi  (WF2, RO2, RW2) . REG2 N WF =(WFi - i=1 i-1 WF \n= WF1 . (ROk . RWk)) k=1 (WF2 - (RO1 . RW1)) N RO = i=1 ROi - RO =(RO1 - (WF2 . RW2)) N (WFi . RWi). \n(RO2 - (WF1 . RW1)) i=1 N RW = RW1 . (RW2 - WF1) RW = i=1(ROi . RWi) - . (RO1 n WF2)(WF . RO) RETURN \n(W F, RO, RW ) RETURN (W F, RO, RW ) (a) Consecutive Region Composition (b) Loop Aggregation Figure \n2. Data-Flow Equations Used in USR Construction. program order. In this pass, data-.ow equations dictate \nhow sum\u00admaries are initialized at statement level, merged across branches, translated across call sites, \ncomposed between consecutive regions, and aggregated across loops. The latter two cases are illustrated \nin Figures 2(a) and 2(b). For example, the composition of a read-only region S1 with a write-.rst region \nS2 gives RO = S1 - S2 (i.e., S2 cannot contribute to the RO result since it corresponds to a write access), \nand similarly, WF = S2 - S1 and RW = S1 n S2. Summaries use a directed-acyclic-graph (DAG) representation, \nnamed USR, in which leafs are sets of linear memory access de\u00adscriptors [20] (LMAD). LMADs de.ne an algebra \nfor aggregating index sets over quasi-af.ne patterns. For example a sequence of ar\u00adray accesses of stride \nd with offset t and bounded by (span) s leads to the index set {t +i*d| 0 = i*d = s}. In a straightforward \ngen\u00aderalization, an LMAD describes an arbitrary number of such access sequences into an unidimensional \n(uni.ed) set of indexes: {t + i1 * d1 + ... + iM * dM | 0 = ik * dk = sk,k . [1,M]} (1) denoted by [d1, \n..., dM ] . [s1, ..., sM ]+ t, where strides dk and spans sk model virtual multi-dimensional accesses. \nFor example, the WF summaries for the write access to A at each level of the loop nest below are presented \non the right-hand side: Lo DOi=1,N,1 [k,N].[k(M-1),N(N-1)] + k-1 Li DOj=1,M,1 [k].[k(M-1)] + (i-1)N+k-1 \nSt A[i*N+j*k] = ... [].[] + (i-1)*N+jk-1 ENDDO ENDDO At statement St the summary is a point; aggregating \nthe accesses over loop Li, creates an 1D-LMAD of stride dLi = tj.j+1 -t = k and span sLi = tj.M -tj.1 \n= k(M - 1),etc.Notethatan LMAD is transparent to the dimensionality of its corresponding array, and it \ndoes not guarantee that its dimensions do not overlap; this can be ek-1 veri.ed [14] via j=1 sj <dk (i.e., \nN>k(M-1) in our example). We found LMADs well suited for our representation because: (i) they support \ntransparent array reshaping at call sites, as they are by de.nition a set of unidimensional points, and \n(ii) they provide better symbolic support, e.g., symbolic (constant) strides, are not af.ne constraints \nin Presburger arithmetic. USR s internal nodes represent operations that cannot be accu\u00adrately expressed \nin the LMAD domain: (i) irreducible set operations, such as union, intersection, subtraction (., n, -), \nor (ii) control .ow: gates predicating summary s existence, call sites across which summaries cannot \nbe translated, or total (.Ni=1) and partial (.i-1 ) k=1 recurrences that fail exact aggregation. For \nexample, with the USR in Figure 3(c), the set subtraction between LMADs [1].[NS -1]+0 and [1] . [16 * \nNP - 1] + 0 cannot be represented in the symbolic LMAD domain, hence a subtraction USR node (-) was introduced. \nMoreover, the resulting (subtraction) set is part of the result iff gate SYM.NE.1 holds, etc. We note \nthat resorting to conservative approx\u00adimation instead of introducing the subtraction node, would miss \nthe (b) Array HE OIND-USR (a) Array HE FIND-USR (c) Array XE FIND-USR   Figure 3. Flow/Output Independence \nUSRs for HE, XE in Figure 1. Corresponding F/O-IND Predicates are: (c) SYM.NE.1.NS=16NP, (a) 8*NP<NS+6, \n(b) .N-1NS=32*(IB(i+1)-IA(i)-IB(i)+1). i=1 Dotted line points to the subtracted part; dotted .i-1 denotes \npar\u00ad i=1 tial recurrence under a fresh variable that ranges from 1 to i - 1. condition under which the \nloop is provably independent. Similar arguments can be made for the USRs in Figures 3(a) and 3(c).  \n2.2 Loop Independence as USR Equations (IND-USR) Having summarized accesses at loop level, answering \nloop inde\u00adpendence reduces to establishing the satis.ability of an indepen\u00addence equation in the USR \ndomain. Denoting by (WFi, ROi, RWi) the per-iteration write-.rst, read-only and read-write summaries \nof array X in loop L, where L s iterations range from 1 to N, the output independence of array X in L \nis represented via equation: {.Ni=1(WFi n (.i-1 WFk))} = \u00d8 (2) k=1 Equation 2 states that if for any \ni, the write-.rst set of iteration i does not overlap with the write-.rst set of any iteration preceding \ni, then no two different iterations write the same location, hence no cross-iteration dependency exists. \nWe name the left-hand side of equation 2 the output-independence USR (OIND-USR) of X in L. Similarly, \n.ow-anti independence is established via equation: {(.Ni=1WFi) n (.Ni=1WFi) n (.N i=1ROi)}.{(.Ni=1RWi)}. \n{(.Ni=1ROi) n (.Ni=1(RWi n (.i-1  i=1RWi)} . {.N RWk))} = \u00d8 k=1 (3) where we denote the left-hand side \nvia FIND-USR. Figure3 depicts the independence USRs of our running example, where LMAD leafs were seen \nas intervals in Section 1.2. When the independence USR (IND-USR) is de.nitely empty or non-empty the \nloop can be classi.ed statically as independent or dependent, respectively. In numerous cases, however, \nindepen\u00addence is either statically undecidable, for example because certain variables are input dependent, \nor too complex to analyze with the current compiler infrastructure. In principle, a solution would be \nto directly evaluate IND-USR at run-time, and to implement condi\u00adtional loop parallelization based on \nthe independence result. This technique works well in some special instances: (i) when IND-USR has O(1) \nruntime complexity, as with the USR in Fig\u00adure 3(c), or (ii) when its evaluation is amortized over many \nexecu\u00adtions of the loop, i.e., USR evaluation has been safely hoisted out\u00ad  g1 = SYM.NE.1 S1 =[0,NS-1]-[0,16*NP-1] \nA = g1 # S1 g2 = SYM.EQ.1 S2 =[0,NS-1] B = g2 # S2 Translate A . B i.e., A . B = \u00d8.F(A . B) A . B = \u00d8.F(A) \n.F(B) A = \u00d8. g1 .F(S1)= SYM.EQ.1 .F(S1) S1 = \u00d8. [0,NS-1] . [0,16*NP-1] . NS = 16 * NP Hence F(A)= SYM.EQ.1 \n. NS = 16 * NP B = \u00d8. g2 .F(S2)= SYM.NE.1 .F(S2) S2 = \u00d8. [0, NS-1] .\u00d8 . false Hence F(B)= SYM.NE.1 Finally, \nF(A . B)= NS = 16 * NP . SYM.NE.1 Figure 4. Deriving the predicate program corresponding to the simple \nF-IND summary in Figure 3(c) via translation scheme F. side at least one outer loop. In the general case, \nhowever, we have found that runtime USR evaluation generates very high overhead, when compared with our \ntechnique. 3. Summary to Predicate Language Translation IND-USR runtime evaluation is an expensive technique \nbecause it computes all the memory locations involved in potential cross\u00aditeration dependencies, and \nas such, solves a more dif.cult problem than loop independence, which only requires classifying IND-USR \nas empty or non-empty. In contrast, our approach is to de.ne an effective translation scheme F from USR \nto an equally expressive, closed-under-composition language of predicates, named PDAG: F : USR . PDAG, \nF(S) . S = \u00d8. While the predicate program is just a suf.cient condition for loop independence, it is \nless constrained than the .attened pred\u00adicates of related approaches, e.g., it s input symbols need not \nbe read-only, and it is also less conservative as a consequence of being constructed from non-trivial \ninference rules that pattern match the shape of an accurate independence summary (IND-USR). Finally, \nredundancy is removed by hoisting invariant terms outside loop\u00adconjunction nodes, and the simpli.ed predicate \nis factored into a sequence of suf.cient conditions for loop independence, which are tested at runtime \nin the order of their estimated complexity. This section is organized as follows: Section 3.1 presents \nat a high-level the language-translation scheme, implemented via a fac\u00adtorization algorithm. Section \n3.2 describes how leaf predicates are extracted from operations in the LMAD domain. For completeness, \nSection 3.3 brie.y explains more complex translation rules that nat\u00adurally extend and are well integrated \nin our framework. Section 3.4 discusses two USR-reshaping transformations that enhance the re\u00adsulting \npredicate accuracy. Finally, Section 3.5 explains how the result program is separated into a cascade \nof increasingly-complex predicates, and Section 3.6 discusses the asymptotic compile and run time complexity \nand the limitations of our framework. 3.1 Factorization Algorithm Figure 4 demonstrates the gist of \nthe language translation F on the simple IND-USR of Figure 3(c): A suf.cient condition for the input \nUSR, which is a union of two terms, to be empty is that each term is empty. Recursively, term A corresponds \nto a gated node, which exhibits a boolean expression g1 under which summary S1 exists. A suf.cient condition \nfor A to be empty is thus that either g1 does not hold, or S1 is empty. Finally, S1 is the difference \nbetween two LMADS, seen for simplicity as intervals. If the .rst interval is included in the second then \nthe difference is empty. The predicate corresponding to A is thus: SYM.EQ.1 . NS = 16*NP, and B is derived \nin the same manner. We note that, similarly to USR, the PDAG FACTOR(S : USR) PDAG INCLUDED(S1,S2 : USR) \n// Output: P s.t. P . (S = \u00d8) // Output: P s.t. P . (S1 . S2) P = .FALSE. NN IF (S1 = S1 and S2 = S2) \ni=1 ii=1 i CASE S OF: (3) P1 = .N INCLUDED(Si 1 , Si 2) i=1 q#S1: P = q . FACTOR(S1) ELSE P1 =INCLUDED \nH(S1, S2)S1 . S2: P = FACTOR(S1) . P = P1. INCLUDED APP(S1, S2)FACTOR(S2) S1 - S2: P = FACTOR(S1) . PDAG \nINCLUDED H(S, U : USR) INCLUDED(S1,S2) // Output: P s.t. P . (S . U) S1 n S2: P = FACTOR(S1) . FACTOR(S2) \n. P = P1 = P2 = .FALSE. DISJOINT(S1,S2) CASE U OF: N (Si): i=1q#S1: P1 = q . INCLUDED(S, S1) N P =FACTOR(Si) \ni=1 S1 . S2: P1 = INCLUDED(S, S1) . S1 N CallSite: INCLUDED(S, S2) P =FACTOR(S1)N CallSite (4) S1 - S2: \nP1 = INCLUDED(S, S1) . DISJOINT(S,S2) PDAG DISJOINT(S1,S2 : USR) S1 n S2: P1 = INCLUDED(S, S1) . // Output: \nP . (S1 n S2 = \u00d8) INCLUDED(S, S2) N IF ( S1 = (S1) and (5) LMAD(L): P1 = FILLS ARR(L) i=1i (S2 N S2 = \ni=1i ) ) CASE S OF: (Sinv ,Sinv 12 ) . invariant q#S1: P2 = q . INCLUDED(S1, U) overestimates of (Si \n1,Si 2) S1 . S2: P2 = INCLUDED(S1, U) . , Sinv (1) P =DISJOINT(Sinv ) INCLUDED(S2, U) 12 ELSE S1 - S2: \nP2 = INCLUDED(S1, U) P = DISJOINT H(S1, S2) . S1 n S2: P2 = INCLUDED(S1, U) .DISJOINT H(S2, S1) . INCLUDED(S2, \nU) DISJOINT APP(S1, S2) P = P1 . P2 PDAG DISJOINT H(U, S : USR) PDAG INCLUDED APP(C, D : USR) // Output: \nP . (U n S = \u00d8) CASE U OF: ((PC , rC1), (PD, LDj)) . condit. LMAD over/under-estim. of C, D q#S1: P \n= q . DISJOINT(S1, S) P = PC . (PD. S1 . S2: INCLUDED LMADS(rC1, LDj)) P = DISJOINT(S1, S) . DISJOINT(S2, \nS) PDAG DISJOINT APP(C, D : USR) (2) S1 - S2: P = DISJOINT(S1, S) . ((PC , rC1), (PD, rD1)) . condit. \nINCLUDED(S, S2) cond. LMAD overestim. of C, D S1 n S2: = PC . PD. P = DISJOINT(S1, S) . P DISJOINT(S2, \nS) DISJOINT LMADS(rC1, rD1) (a) Factorizing IND-USR. (b) Helper Functions. Figure 5. Logical Inference \nRules of The Factorization Algorithm. predicate language has a DAG representation in which leaves are \nboolean expressions, while internal nodes represent either logical and, or operators (., .), or control-.ow: \nuntranslatable call sites (N Call Site), or irreducible loop-level conjunctions (.Ni=1Pi). Figure 5 presents \nthe factorization algorithm (FACTOR), which implements the translation scheme F. Inference on set-algebra \nproperties guides a recursive construction of a predicate program via a top-down traversal of the input \nsummary. For example, a subtraction between two summaries is empty if the .rst operand is either empty \nor is included in the second operand, and similarly an intersection is empty if any operand is empty \nor the two operands are disjoint. In such cases INCLUDED and DISJOINT are called, respectively, to add \nadd more specialized factors to the result: A summary S is included in the difference of other two sum\u00admaries \nS1 - S2 if, as in rule (4), S is included in S1 and disjoint with S2: S . S1 -S2 .F(S -S1).F(S nS2). \nTwo recurrence\u00adunion summaries over the same loop are in an include relation if, as in rule (3), the \niteration-based summaries are in an include rela\u00adtion: (.iN =1Si 1) . (.iN =1Si 2) ..iN =1 F(Si 1 - Si \n2). Finally, rule (5) extracts via FILLS ARR the predicate under which an LMAD L fully covers the maximal \n(declared) dimension of its corresponding array; this predicate guarantees that any summary is included \nin L.  PDAG DISJOINT LMAD(A, B : LMAD) Similarly, a summary S is disjoint from the difference between \ntwo summaries S1-S2 if, as in rule (2), either S is disjoint with S1 or is included in S2, since in the \nlatter case S cannot be part of the subtraction result: S n (S1 - S2)= \u00d8.F(S n S1) .F(S - S2). However, \ndisjointness of per-iteration summaries of the same loop does not imply that the recurrence-union summaries \nare dis\u00adjoint: (.Ni=1Si 1)n(.Ni=1Si 2)= \u00d8 ..Ni=1F(Si 1 nSi 2). In this case, rule (1) of Figure 5 attempts \nto .nd loop-invariant overestimates for Si 1 and Si 2, denote them S1 inv, for example by .ltering out \ninv and S2 loop-variant gates. The disjointness of the overestimates is now a suf.cient condition for \nthe disjointness of the two recurrence-union summaries1: (.Ni=1Si 1) n (.Ni=1Si 2)= \u00d8.F(S1 inv). inv \nn S2 A powerful inference rule that solves a large class of nonlinear accesses refers to the pattern \n.Ni=1(Si n.i-1 Sk)= \u00d8, which is k=1satis.ed under predicate .Ni=1MONOTON(Si). While this rather com\u00adplex \nrule [19] is brie.y demonstrated in Section 3.3, the intuition is that Si monotonicity, e.g., the maximal \nelement of Si being smaller than the minimal element of Si+1 is a suf.cient condition for the targeted \nsummary equation to hold.  3.2 Extracting Predicates from LMAD Operations When the factorization algorithm \nreaches LMAD leafs or en\u00adcounters summaries of shapes that are not covered by any rules, INCLUDED APP \nand DISJOINT APP conservatively .atten the prob\u00adlem to the LMAD domain. We .rst show how .attening is \nachieved and then describe an algebra under which leaf predicates are ex\u00adtracted from comparing LMADs \nfor inclusion and disjointness; the generalization to sets of LMADs being straightforward2. We have found \nmost useful to represent an overestimate of summary C as a pair (PC , rCl), where PC is a predicate under \nwhich C is empty, while rCl is a set of LMADs that overestimates C. The latter is computed via a recursively \nde.ned operator on the USR domain, which, (i) on the top-down parse disregards node B in terms such as \nC - B, C n B, and (ii) on the bottom-up parse it translates, aggregates and unions the encountered LMAD \nleafs over call site, recurrence, and . nodes, respectively. Similarly, D is underestimated via (PD, \nLDJ), where LDJ is a (maximal) LMAD\u00adset-underestimate of D when predicate PD holds. We recall from Section \n2.1 that an LMAD is denoted by [d1, ..., dM ] . [s1, ..., sM ]+ t and represents the set of indexes {t \n+ i1 *d1 + ... + iM *dM | 0 = ik *dk = sk,k . [1,M]}. Also, under the simplifying assumption that all \nLMAD strides are positive (dk > 0), one can observe that the interval [t, t + s1 + .. + sM ] overestimates \nits corresponding LMAD. We .rst treat the case of uni-dimensional LMADs: Two 1D-LMADs can be proved disjoint \nin two scenarios: (i) they either correspond to an interleaved, but non-overlapping sequence of ac\u00adcesses, \ne.g., LMADs A1 = [2] . [99] + 0 ={0, 2, .., 98} and A2 = [2] . [99] + 1 ={1, 3, .., 99} are disjoint, \nor (ii) they can be over\u00adestimated by disjoint intervals, e.g., A1 = [2] . [49] + 0 . [0, 49] and A2 \n= [2] . [49] + 50 . [50, 99]. Formally, a suf.cient predicate for A1 =[d1]v[s1]+ t1 and A2 =[d2]v[s2]+ \nt2 to be disjoint is: gcd(d1,d2) | (t1 - t2) . (t1 >t2 + s2 . t2 >t1 + s1), where the .rst and second \nterm satisfy the interleaved-access and disjoint-intervals scenarios, respectively. Similarly, using \nthe same notations, one can observe that a suf.cient predicate for A1 to be included in A2 is: (d2 | \nd1) . (d2 | t1 - t2) . (t1 = t2) . (t1 + s1 = t2 + s2). For example, in loop CORREC DO711 from the bdna \nbench\u00admark, loop independence requires to establish that [1].[0]+IX(2)+i-2 and [1].[i-2]+IX(1)-1 are \ndisjoint (the loop index is i .[1,NOP]). 1 This rule solves several important loops from zeusmp, see \nFigure 9(b). 2 The set of LMADs S1 is disjoint from (included in) the set of LMADs S2 if any LMAD in \nS1 is disjoint to any (included in at least one) LMAD in S2. //Input: P s.t. P . (A n B = \u00d8) IF( A and \nB unidimensional LMADs) P = DISJOINT LMAD 1D(A,B) ELSE (A1d ,B1d ) . FLATTEN LMADS(A,B) ,B1d ) Pflat \n= DISJOINT LMAD 1D(A1d (C,D) . UNIFY LMAD DIMS(A,B) (P wf C ,Cin ,Cout ).PROJ OUTER DIM(C) (P wf D ,Din \n,Dout ).PROJ OUTER DIM(D) P out , Dout ) = DISJOINT LMAD 1D(Cout P in (Cin , Din ) d d = DISJOINT LMAD \n. (P wf . P wf . (P out . P in P = Pflat Cd )) Dd (a) Predicate for Disjoint LMADs. SE REDUCE GT 0( expr \n) //Input: an int-type expression //Output: P s.t. P . (expr > 0) (a, b, i, L, U, err) = FIND SYMBOL(expr); \n// expr = a*i+b, L = i = U, i ./b // P= (a = 0 . a*L+b>0) . // (a < 0 . a*U+b>0) IF ( err ) RETURN (expr \n> 0); ELSE RETURN [ REDUCE GT 0(a+1) . REDUCE GT 0(a*L+b) ] . [ REDUCE GT 0(-a) . REDUCE GT 0(a*U+b) \n]; (b) Symbolic Fourier-Motzkin Figure 6. Algorithm for Characterizing LMAD Disjointness. Noting that \nthe interleaved-access term evaluates to false (be\u00adcause 1 divides everything), the rules above yield \npredicate: IX(1)+1-IX(2)-i>0 . IX(1)=IX(2). To eliminate loop index i from term IX(1)+1-IX(2)-i>0, we \nuse a Fourier-Motzkin-like algorithm, depicted in Figure 6(b). The algorithm receives a symbolic expression \nexpr and returns a suf\u00ad.cient predicate for expr> 0 to hold. First, a scalar symbol i of known upper \nand lower bounds is picked from expr, and expr is re-written as a*i+b, where i does not appear in b. \nIf no suitable i is found, the result is expr>0. Finally, when a=0 or a<0, i is replaced with its lower \nor upper bound, respectively, to give suf\u00ad.cient conditions for the inequation expr>0. Note that this \nleads to solving four subproblems of necessarily smaller exponent of i, which ensures that the recursion \neventually terminates (in ex\u00adponential time). In our example, the elimination of i yields term IX(2)+NOP=IX(1), \nand the overall O(1) predicate for loop COR-REC DO711 becomes IX(2)+NOP=IX(1) . IX(1)=IX(2). In general, \nmulti-dimensional LMADs present two dif.culties: .rst, an LMAD dimensions may overlap, and second, since \nthere is no relation between the LMAD dimensionality and that of its corresponding array, we may have \nto compare LMADs that exhibit a different number of dimensions. We address this via a heuristic that \nuni.es the LMADs dimensions, projects and compares a dimension at a time, and joins the result. In addition, \npredicates are extracted to guard the safety of the projection (i.e., the non-overlap invariant). We \ndemonstrate the approach, sketched in Figure 6(a), on bdna s loop CORREC DO900, which requires to prove \nLMADs [M].[2*M]+j-1+2*M and [1,M].[j-2,2*M]+2*M disjoint, where the loop index j is in 1..N. The .rst \nstep is to .atten the input to 1D-LMADs and to extract a predicate Pflat as discussed before. The second \nstep is to unify LMAD dimensions, e.g., the 1D-LMAD is padded with an empty dimension of stride 1 and \nspan 0 giving: [1,M].[0,2*M]+j-1+2*M. Next, if both LMADs have equal outer strides, PROJ OUTER DIM(C) \nseparates the outer dimension of C, returning a well-formedness predicate PC wf , together with LMADs \nCout and Cin that correspond to the last and remaining dimensions. With our example, C=[1,M].[0,2*M]+j-1+2*M \nsplits into Cin P wf and Coutveri.es that the separated dimensions do not overlap i.e. the range of \nthe inner LMAD is less than the outer stride. Fourier-Motzkin simpli.cation gives PC wf =(N=M). Similarly, \nD=[1,M].[j-2,2*M]+ 2*M is split into Din =[1].[j-2]+0, and Dout=[M].[2*M]+2*M, where P wf =[1].[0]+j-1 \n=[M].[2*M]+2*M. C =(j-1<M) D =(N-1=M). Recursively testing disjointness of the inner =Dout and outer \nLMADs gives predicates: Pd out =false, since Cout , and Pd in =true, since [j-1,j-1]n[0,j-2]=\u00d8. The independence \npredicate for CORREC DO900 is thus Pflat . (N = M).  // Estimate .NRI (WFi)= CIV@1 =Q i=1 .NRI DO i \n= 1, N, 1 // ([1].[2]+3*SHIFT(n)) i=1 CIV@2 = .(i.EQ.1, CIV@1,CIV@4) (lower, upper) . (rINT1,LINTj) cond \n= X(i+M).NE.1 .AND. NSP(i).GT.0 REDUCTION(MAX:upper) IF (cond) THEN REDUCTION(MIN:lower) DO j = 1, NSP(i), \n1 PRIVATE(tmp lub,n) IF(0.NE.IA(1,i)) X(j+CIV@2 )=.. DOALL n=1,NRI ENDDO tmp lub(1)=1+3*SHIFT(n) CIV@3 \n= NSP(i) + CIV@2 tmp lub(2)=3+3*SHIFT(n) ENDIF lower = MIN(tmp lub(1), lower) CIV@4 = .(cond, CIV@3, \nCIV@2) upper = MAX(tmp lub(2), upper) ENDDO ENDDOALL CIV@5 = .(N.GE.1, CIV@4, CIV@1) (a) Estimating \nthe size of FSHIFT (b) CIVagg example for loop in gromacs s loop INL DO1130. CORREC DO401 of bdna. Figure \n7. (a) Optimizing Reduction and (b) CIV Aggregation  3.3 Proving Non-Linear Access Independent For completeness, \nthis section sketches how our framework han\u00addles a class of non-linear accesses, such as those exhibiting \narray indexing or induction variables without closed-form solutions. The .rst dif.culty corresponds to \nquadratic or array indexing, which either appears directly in the code, e.g., sparse-matrix im\u00adplementation \nuses index arrays, or as artifacts of transformations such as induction-variable substitution, e.g., \nquadratic indexes. The intuition is that solving such accesses corresponds to a rather-complex rule, \ndescribed in detail elsewhere [19], that trans\u00adlates equations of shape: .N n.i-1 (Sk))=\u00d8 to the pred\u00ad \ni=1(Sik=1icate domain. If Si are seen as intervals, we can observe that if they form a monotonic sequence, \ne.g., the lower bound of the interval corresponding to iteration i +1 is always greater than the upper \nbound of that of iteration i then the summaries Si of any two consecutive iterations do not overlap, \nand by in\u00adduction, any two summaries of distinct iterations do not over\u00adlap, hence the above equation \nis satis.ed. Figure 3(b) shows such an example, where Si =WFi is overestimated to interval [32*(IB(i)-1),32*(IB(i)+IA(i)-2)+NS-1]. \nImposing the above strictly-increasing-monotonicity assumption results in predi\u00adcate .N-1NS=32*(IB(i+1)-IA(i)-IB(i)+1) \nthat veri.es out\u00ad i=1 put independence under O(N)-runtime complexity. The second dif.culty corresponds \nto the use of induction variables that are conditionally incremented (CIV), such as CIV in Figure 7(b). \nThe solution is to devise a .ow-sensitive USR\u00adaggregation technique that succeeds in summarizing CIV \naccesses at iteration and loop level. For example, on the CFG path that takes the if branch (and contains \nthe inner loop), an overesti\u00admate of the per-iteration write access to X, denoted Wi, is in\u00adterval [CIV@2+1,CIV@2+NSP(i)], \nwhich can be rewritten as [CIV@2+1,CIV@4] (since CIV@4=CIV@3=CIV@2+NSP(i)). On the other path, Wi is \nthe empty set, which can also be written as [CIV@2+1,CIV@4], since the interval s upper bound CIV@4=CIV@2 \nis less then its lower bound CIV@2+1. Hence the interval overestimate rWil=[CIV@2+1,CIV@4] holds on all \npaths. One can compute overestimates: r.i-1 Wkl=[Q+1,CIV@2], and k=1 r.Ni=1Wil=[Q+1,CIV@5] in a similar \nfashion. Output indepen\u00addence is proven statically since [Q+1,CIV@2]n[CIV@2+1,CIV@4] gives the empty \ninterval. Observing that the read access to X can be overestimated via interval [M,M+N], the .ow independence \npred\u00adicate Q=M+N . M>CIV@5 is extracted from the requirement that the loop summaries for the write and \nread accesses are disjoint (i.e., [M,M+N]n[Q+1,CIV@5]=\u00d8). Finally, predicate evaluation and par\u00adallel \nexecution of the loop requires that the CIV values at the begin\u00adning of each iteration are precomputed \nvia a loop slice that is also executed in parallel to ensure scalable speedup. Figure 8. USR Reshaping \nTransformations. 3.4 Enabling USR Transformation for Predicate Extraction Predicates are constructed \nby pattern matching the shape of the input summary, and as such, semantically equivalent summaries may \ntranslate to predicates of varying accuracy. Figure 8 depicts two high-level USR reshaping rules that \nwe have found to improve predicates quality in a signi.cant number of (important) loops. The .rst rule \nsays that a repeated (irreducible) subtraction from a summary should be reorganized as one subtraction \nbetween that summary and the union of the subtracted terms3. In Figure 8 we show intuitively that when \nA is included in neither B nor C, per\u00adhaps the union of B and C can simplify to a larger set in the array\u00adabstraction \ndomain which includes A, thus enabling the extraction of a more meaningful predicate. Note that related \napproaches would likely miss this opportunity because in the absence of a language, subtractions are \nperformed in order, and the irreducible A - B would have already been treated conservatively. The second \nrule refers to preserving the shape of a union of mu\u00adtually exclusive gates (UMEG) when subtracting, \nintersecting and uniting summaries of compatible-UMEG shapes, i.e, to distribute the operation inside \neach mutually exclusive gate, where by com\u00adpatible shapes we mean that the gates of one summary are either \na subset or match those of the other summary. The motivation is sim\u00adilar to the one for the .rst rule; \nthe missing step in the .gure being that, before the rule .res, Y is (semantically) normalized to: (C1#(S4.S6)) \n. (C2#(S5.S6)) . (C3#S6) . (C#S6). The .rst transformation was useful in numerous loops, while the second \nwas instrumental in parallelizing the larger SPEC2006 benchmarks ZEUSMP and CALCULIX. Figure 9(b) shows \nthe sim\u00adplest FIND-USR obtained via UMEG-preserving transformations for ZEUSMP s loop TRANX2 DO2100. \nSince Ci inv and Di inv are loop\u00adinvariant overestimates for the two same-loop nodes of index k, ,Dinv \nFACTOR calls DISJOINT(Ci inv i ) which derives the indepen\u00addence predicate that succeeds at runtime: \n(jbeg.EQ.js) . (jbeg.NE.js . jend<133 . M<135), where the last two terms represent well-formedness predicates \ncorresponding to separating the outer dimensions when compar\u00ading multi-dimensional LMADs (see Section \n3.2). Without UMEG\u00adpreserving transformations, the FIND-USR is too large to be pre\u00adsented and none of \nthe extracting predicates succeed at runtime.  3.5 Predicate Simpli.cations The predicate program built \nunder the language-translation scheme can be signi.cantly optimized by (i) hoisting the loop-invariant \nterms outside loop nodes, and (i) by removing redundancy. The 3 This resembles strength-reduction optimization, \nwhere addition is pre\u00adferred to the more inexact subtraction operation.   Figure 9. Predicate-Separation \n&#38; USR-Transformation Examples. former transformation is essential in improving the accuracy of each \nterm of the cascade of (partial) independence predicates. For example invariant hoisting identi.es loop-invariant \nchil\u00addren of a n-ary ./. node and hoists them outside the loop node, i=1(.(Ainv e.g., .N 1 , .., Ar inv \n,B1 var , .., Bp var )) ..(A1 inv , .., Ar inv ) . i=1(.(Bvar .N 1 , .., Bp var )). Hoisting is mainly \nenabled by two trans\u00adformations: First, ./. .attening merges repeated compositions of the same . or . \noperator into one n-ary . or . node, e.g., (A1 . A2) . (A3 . A4) ..(A1,A2,A3,A4), Second, common factor \nextraction, e.g., .(B1 .A, .., Bp .A) ..(B1, .., Bp).A, beside reducing redundancy, would allow now the \nloop-invariant predicate A to be hoisted outside the hypothetical loop node. For example, running our \nalgorithm on FIND-USR of Figure 3(a) identi.es that a suf.cient independence condition is that the bottom \nnode4 -([1,8].[2,8*NP-8]+t)-([1].[NS-1]+t) -is empty, where t=32*(k-2+IB(i)). This reduces to satisfying \nthe inter\u00adval inclusion [t ,t+8*NP-6].[t,t +NS-1], which gives predi\u00adcate Pleaf = 8*NP<NS+6. However, \neven though Pleaf is invari\u00adant to loops of indexes k and i, the algorithm bottom-up pass still IA(i) \nwraps it inside loop nodes, giving .Ni=1(.k=1 8*NP<NS+6). Ap\u00adplying the above simpli.cations moves Pleaf \noutside both loop nodes and removes the now empty loops, giving the O(1) predi\u00adcate 8*NP<NS+6. Finally, \nthe predicate is factored into terms of increasing run\u00adtime complexity, typically O(1) and O(N), where \ncode is gener\u00adated to implement these tests and to cascade them to implement conditional parallelization, \nprivatization, etc. Separating O(1) predicates applies more aggressive rules to extract invariant factors, \ni=1(.(Ainv . Bvar . Bvar e.g., .N 11 , .., Ap inv p )) ..(A1 inv , .., Ap inv ). Separating O(N) predicates \nis obtained by replacing any inner\u00adloop node (i.e., nest depth > 1) in the original predicate via false \nand simplifying the result. Figure 9(a) demonstrates this technique: removing the two while-loop nodes \nin the complete predicate results in the much simpler O(N) counterpart that succeeds at runtime, where \nscalars nope and nope0 are loop variant. 4 Note that a conservative approximation at that point in the \nsummary construction would miss on extracting the relevant independence predicate.  3.6 Asymptotic Complexity \nand Limitations Discussion With respect to the compile-time complexity, we note that the Fourier-Motzkin-like \nelimination is (only) exponential in the num\u00adber of eliminated symbols. The typical case is that we eliminate \nonly the outermost loop index via Fourier-Motzkin. Hence, we ex\u00adpect O(1) overhead, where the inner-loop \nindexes are eliminated by LMAD-level aggregation. Furthermore, while the factorization algorithm has \nworst-case exponential complexity, the typical USR input exhibits a sparse structure in the operations \nthat cause the ex\u00adponential behavior. This means that, in practice, the compile time is dominated by \nthe quadratic time of building USRs [28]. We model a predicate s runtime complexity after the loop-nest \ndepth exhibited by its implementation, where we bound a potential explosion in predicate size via a convenient \nconstant factor. We generate the entire cascade of predicates, noting that we have not yet encountered \na .rst-successful predicate of complexity greater than O(N), where N is the number of iterations of the \ntargeted loop. The last test is always an exact one, i.e., implemented either as USR evaluation or by \nmeans of thread-level speculation. There several possible avenues for future investigation such as: More \naggressive rules to the translation, e.g., nonlinear accesses, currently disambiguated by checking their \nmonotonicity, could be more accurately modeled if existential quanti.ers would be part of the predicate \nlanguage. Furthermore, we could enhance the preci\u00adsion of the LMAD-level comparison, which currently \nresults in only suf.cient conditions even in the case of unidimensional LMADs. 4. Other Uses of the Infrastructure \nThe factorization algorithm extracts predicates that satisfy an ar\u00adbitrary equation in the USR domain, \nand as such, its applicabil\u00adity goes beyond proving loop independence. For example, one can disambiguate \nat runtime whether a symbol requires only static, rather than dynamic last value computation. Static \nlast value (SLV), can be represented via USR equation .Ni=1(WFi) . WFi.N , which says that the whole \nwrite-.rst set of the loop of index i . [1,N] is included in the write-.rst set of the last iter\u00adation. \nArray array psi of loop EMIT DO5 of SPEC89 s nasa7 benchmark is such a case, where the SLV successful \npredicate is: .N-1 i=1 (arrays nwall(i)=arrays nwall(N)). Similarly, we apply runtime tests to optimize \nand extend the ap\u00adplicability of reduction parallelization. Consider the simple loop: DOi =1,N, 1 S1 \n: A(i) = ... S2 : A(B(i)) = A(B(i)) + ... ENDDO First, disregarding statement S1, we observe that S2 \nmatches the reduction pattern and the loop can always be parallelized by com\u00adputing the changes to A \nlocally for each processor, and by merg\u00ading (adding) them in parallel at the end of the loop. However, \nif B is injective, this treatment is unnecessary because each itera\u00adtion reads/writes a distinct element \nof A, and thus every processor can work safely, directly with the shared array A. The predicate5 obtained \nfrom solving equation .Ni=1(RWi n.i-1 (RWk))=\u00d8 is a k=1 suf.cient condition for the access to be independent, \nand thus will guard a conditional application of reduction at runtime (RRED). Second, considering now \nS1, one can observe that while S1 and S2 do not form a reduction group [17] (i.e., S1 is not a reduction), \none can still parallelize the loop by treating S2 as a reduction if A s index sets for statements S1 \nand S2 do not overlap (e.g., predi\u00adcate .Ni=1N<B(i)). We name this case extended reduction (EXT\u00ad 5 With \nour framework we obtain predicate .N-1B(i) <B(i + 1) ex\u00ad i=1 tracted by the monotonicity rule.  RRED), \nwhere we allow A to be written outside reduction statements as long as these writes do not precede, on \nany path, any reduc\u00adtion statement. One can observe that EXT-RRED instances will have non-empty write-.rst \nand read-write sets, corresponding to state\u00adments such as S1 and S2, respectively, and a necessarily \nempty read-only set. Enabling parallelism in this case requires proving (i) .ow independence, i.e., .Ni=1(WFi) \ni=1(RWi)=\u00d8, and ei\u00ad n.N ther (ii) output independence, i.e., .Ni=1(WFi n.i-1 (WFk))=\u00d8 k=1 or (iii) computing \nthe last value of the write-.rst set (e.g., the last value corresponds to the last iteration if .Ni=1(WFi) \n. WFi.N ). Loops MXMULT DO10 and FORMR DO20 that cover almost 55% of dyfesm s sequential runtime, exhibit \nthe EXT-RRED pattern. Finally, in some cases such as gromacs and calculix bench\u00admarks, the reduction \nis statically recognized but the bounds of the reduction array cannot be estimated at compile time because, \nfor example, the array is passed as a parameter of assumed size to a Fortran subroutine called from C. \nOur solution, shown in Figure 7(a) on the simplest example we encountered, is to compute at runtime the \nupper and lower bounds of the array indexes touched in the loop. This is achieved via overestimating \n.Ni=1(RWi) by removing terms B from nodes such as A - B, A n B, etc., such that the resulting USR exhibits \nonly ., callsiteandrecurrencenodes.Incontrastto USR sexactevaluation, our lightweight USR-bounds estimation, \nnamed BOUNDS-COMP, allows parallel evaluation, where the lower and upper bounds are MIN/MAX-reduced across \niterations. On gromacs we are 1.66x faster than IBM s xlf r compiler, which, in the absence of array\u00adbounds \ninformation, appears to parallelize the loop by executing its reduction statements atomically causing \nfrequent cross-processor con.icts. While the example of Figure 7(a) is trivial, a challenging appli\u00adcation \nfor BOUNDS-COMP are arrays such as AUB from calculix s mafillsm do7, where RWi is complex and prohibitively \nexpensive to compute via exact USR evaluation. Still, BOUNDS-COMP s over\u00adhead is less than 9% of the \nparallel runtime and scales well. 5. Code Generation: Putting Everything Together The factorization analysis \nis implemented in a variant of the Polaris research compiler [5] for Fortran77 and is applied on a control-.ow-structured \nprogram under SSA representation. First, accesses are summarized via read-only, write-.rst and read-write \nUSRs, and .ow and anti independence USRs are computed for each symbol in the targeted loop. If IND-USR \nis decidable to be empty for all symbols or non-empty for at least one symbol then the loop is statically \nrecognized independent or dependent, respectively. Ex\u00adceptions are the cases when the symbol access is \nin a reduction pat\u00adtern or when output dependencies can be .xed via the application of privatization \nand static last value. Next, a sequence of .ow and output independence predicates is extracted for each \nunresolved symbol, where a true predicate still classi.es independence stati\u00adcally. Predicate code generation \n.rst extracts the loop slice that cor\u00adresponds to the CDG-transitive closure of all statements that are \nnecessary to compute the symbols appearing in the target predi\u00adcate, where the non read-only symbols \nare privatized and copied in, if necessary. Next, one can observe that the de.nitions of the symbols \nappearing in a leaf node are necessarily on the same CFG path, hence the code for the leaf-node is placed \nimmediately after the de.nition of the last symbol; we denote it the most dominated de.nition (MDD). \nComposition nodes, such as ./. are placed at the immediate common post-dominator of its child nodes, \netc. Predicates of non-constant complexity are evaluated in parallel, where we use and/or reduction to \nmerge boolean results across iterations. Redundancy is further reduced by hoisting the calls to these \npredicates interprocedurally at the highest loop-dominator point where all predicate s input values are \navailable (i.e., the MDD of its input values). It follows that all symbols predicates are called on one \nCFG path, hence predicates can be cascaded (i.e., the .rst successful predicate disables the evaluation \nof the rest). If all predicates fail, then we apply an exact, albeit potentially expensive, runtime test. \nIf we can amortize the cost of the exact test against many execution of the loop (i.e., via hoisting), \nthen we use direct evaluation of IND-USR, otherwise we use TLS [25]. Finally, we optimize parallelism \nby implementing conditional privatization, reduction or static-last value, where loop paralleliza\u00adtion \nuses OpenMP directives. For example, code generation for an array X that requires the EXT-RRED of Section \n4 uses a private copy of X, X1. Prior to (parallel) loop execution the reduction part of X1, i.e., .Ni=1(RWi) \nis zeroed out. Next, each iteration computes its WFi set, and writes back to X the locations in WFi at \niteration s end. Af\u00adter loop termination, the locations involved in reduction statements, i.e., .Ni=1(RWi), \nare reduced across the X1 s copies and X is accord\u00adingly updated. If the access is proven independent \nat runtime, then none of the above are necessary and the code uses shared-array X instead of its private \ncopies X1. 6. Experimental Results This section evaluates our auto-parallelizing approach on 26 bench\u00admarks \nfrom the PERFECT-CLUB, SPEC1992, 2000 and 2006 suites. Tables 1, 2 and 3 characterize each benchmark \nas a whole, named in column one, and several of its representative loops, where columns three and four \nshow the loop name and its contribution to sequen\u00adtial coverage (LSC) as percentage of the sequential \nruntime, respec\u00adtively. Column .ve shows how loops have been classi.ed: whether the loop has been proven \nsequential/parallel statically6(STATIC-PAR/SEQ), or it uses predicates to prove .ow/output independence \n(F/OI), and the complexity of the runtime test (O(1)/O(N)), where N refers to the number of iterations \nof the outermost loop. Finally, column two characterizes the benchmark as a whole: (i) the sequential \ncoverage (SC) and the corresponding number of measured loops (NLsc), but also the total number of ana\u00adlyzed \nloops (NLtot) when different than NLsc, (ii) the sequential coverage of the loops that require runtime \nindependence tests (SCrt ), and the overhead of these tests (RTov ) represented as percentage of the \nparallel runtime, and (iii) the parallelization techniques used: privatization (PRIV), static/dynamic \nlast value (SLV/DLV), static/runtime/extended reduction (SRED/RRED/EXT-RRED/BOUNDS-COMP), as presented \nin Section 4. Additionally, (i) UMEG refers to the transformation of Sec\u00adtion3.4thatpreservesthe USR \nparticularshapeofanunionofmutu\u00adally exclusive gates, (ii) MON signals the use of monotonicity tests, \nand CIVagg and CIV-COMP refer to the summarization re.nement in the presence of conditional induction \nvariables (CIV) and to the parallel pre-computation of their iteration-wise values, as summa\u00adrized in \nSection 3.3, and (iii) HOIST-USR means that independence was proven via runtime USR evaluation, where \nthe USR has been successfully hoisted outside at least one loop, i.e., the overhead was amortized across \nthe many loop executions. Our compiler generates OpenMP-annotated Fortran source code. PERFECT-CLUB and \nSPEC92 benchmarks were compiled (-O2 -ipo) and compared with INTEL s ifort compiler ver\u00adsion 11.1 (-O2 \n-ipo -parallel) on a commodity INTEL quad\u00adcore Q9550@2.83GHz machine with 8Gb memory. The larger SPEC2000/2006 \nbenchmarks were compiled (-O4) and compared with IBM s xlf compiler version 13 (-O4 -qsmp=auto) on a \n8 dual-core POWER5+@1.9GHz, 32Gb-memory machine. Bench\u00admarks were run three times and the average was \ntaken; This was 6 In some cases the static decision refers to extracting a true predicate.  PERFECT \nCLUB Suite BENCH BENCH PROPERTIES SELECTED LOOPS LSC % GR ms PAR/SEQ/ RT TEST FLO52 SC=95%,NLsc=30 SCrt \n=.3%, RTov =0% PRIV,SRED,SLV RRED,NLtot =199 PSMOO do40 DFLUX do30 EFLUX do10 DFLUX do40 19.5% 9.6% 8.2% \n0.3% .04 .08 .02 .01 STATIC-PAR STATIC-PAR STATIC-PAR OI O(1) BDNA SC=94%,NLsc=6 SCrt =0%, RTov =0% PRIV,S/RRED,CIVagg \nNLtot =272 ACTFOR do500 ACTFOR do240 RESTAR do15 ACTFOR do320 59.5% 31.5% 4.8% 1.8% 69 36 28 .1 STATIC-PAR \nCIVagg STATIC-PAR STATIC-PAR ARC2D SC=97%,NLsc=34 SCrt =20%, RTov =.2% PRIV,SLV,MON NLtot =207 STEPFX \ndo210 STEPFX do230 XPENT2 do11,etc FILERX do15 16.3% 11.9% 10.7% 9.0% .8 .6 .002 1.3 STATIC-PAR STATIC-PAR \nFI O(1) FI O(1) DYFESM SC=97%,NLsc=10 SCrt =96%, RTov =.3% PRIV,EXT-RRED HOIST-USR,MON NLtot =195 MXMULT \ndo10 SOLXDD do10/4 SOLVH do20 FORMR do20 43.9% 27.3% 14.2% 10.5% .006 .007 .03 .02 FI HOIST-USR OI O(N) \nOI O(N) F/OI O(1)/O(N) FI HOIST-USR OI O(N) MDG SC=99%,NLsc=12 SCrt =0%, RTov =0% PRIV,RRED,NLtot =59 \nINTERF do1000 POTENG do2000 CORREC do1000 92% 7.2% 0.1% 24 19 .04 STATIC-PAR STATIC-PAR STATIC-PAR TRFD \nSC=99%,NLsc=4 SCrt =34.8%, RTov =0% PRIV,SLV,MON NLtot =44 OLDA do100 OLDA do300 INTGRL do140 INTGRL \ndo20 63.7% 30.9% 3.9% 0.1% 18 9 2 .006 STATIC-PAR FI O(1) OI O(N) STATIC-PAR TRACK SC=97%,NLsc=3 SCrt \n=97%, RTov =47% PRIV,CIVagg ,NLtot =88 EXTEND do400 FPTRAK do300 NLFILT do300$3 49.2% 47.7% 1.2% 117 \n121 3.6 CIV-COMP CIV-COMP TLS SPEC77 SC=76%,NLsc=4 SCrt =11%, RTov =0% PRIV,SRED,SLV GLOOP do1000 GWATER \ndo190 SICDKD do1000 57.1% 16.5% 2.6% 31 9.5 1.3 STATIC-PAR TLS FI O(1) OCEAN SC=65%,NLsc=14 SCrt =45%, \nRTov =.1% PRIV,SLV,MON NLtot =134 FTRVMT do109 CSR do20 SCSC do30/40 RCS do20 45.4% 5.2% 3.8% 1.8% .01 \n.04 .03 .04 FI O(1) STATIC-PAR STATIC-PAR STATIC-PAR QCD SC=99%,NLsc=6 SCrt =1%, RTov =0% NLtot =113 \nUPDATE do1 UPDATE do2 INIT do2 31.9% 31.6% 1% 22 22 1.5 STATIC-SEQ STATIC-SEQ .. OI O(1) Figure 10. Timing \nResults for PERFECT-CLUB Suite.  Table 1. Properties of the PERFECT CLUB suite. The layout of this table \nis explained in the beginning of Section 6 deemed suf.cient because their runtime typically exhibited \nnegli\u00adgible standard deviation. 6.1 Results Summary Examining Figures 10, 11, 12 and 13, one can draw \nseveral high\u00adlevel observations: First, the speedups achieved via our factor\u00adization approach are superior \nto the ones of INTEL s ifort and IBM s xlf r compilers in all but four cases: dyfesm, ocean, hydro2d, \nand qcd. With qcd the results are close, and neither approach extracts signi.cant parallelism. In the \nother cases, our approach successfully parallelizes a number of small-granularity loops which results \nin slowdown compared to sequential execu\u00adtion, while ifort/xlf r fails to prove those loops parallel, \nhence executes them sequentially. Second, the gains mainly re.ect the commercial-compiler in\u00adability \nto parallelize the important loops of the corresponding benchmarks: (i) either because it lacks interprocedural \ndependence analysis, e.g., the benchmarks that were statically parallelized by SUIF a decade ago, or \n(ii) because it lacks extensive use of runtime\u00advalidation of parallelization (conditional parallelization, \ninspec\u00adtor/executor, speculative parallelization). Third, the overhead of our techniques that enable \nparallelism at runtime is negligible in most cases, i.e., less than 1% of the parallel timing; the notable \nexceptions of track, gromacs, calculix, which still exhibit scalable speedup, will be discussed separately. \nFourth, we have classi.ed parallelism exhaustively on a number of benchmarks, indicated via a large NLtot \n, and the results support the feasibility of exploiting nested parallelism (e.g., bdna and apsi exhibit \nmany inner loops that are solved via light predicates). Finally, there are only two loops that require \nthread-level spec\u00adulation (TLS): NLFILT DO300 of track and GWATER DO190 of spec77, and only one notable \nexample where independence is proven via hoistable-USR evaluation: apsi s RUN DO20/40/etc.   6.2 PERFECT \nCLUB Suite Results While being the oldest, PERFECT-CLUB suite is the most dif.\u00adcult to parallelize: arc2d, \ndyfesm, trfd, ocean and bdna all ex\u00adhibit an abundance of .ow and output independence predicates of O(1) \nand O(N) runtime complexity. Loops MXMULT DO10 and FORMR DO20 of dyfesm use the extended treatment for \nreduction of Section 4, while trfd and dyfesm are rich in the monotonicity predicates of Section 3.3. \nFurthermore, bdna, but especially track usethe CIV-aggregation re.nement (CIVagg ) of Section 3.3. In \ntrack s case, two while loops sum up to 97% of the sequential coverage. Parallelizing the two while loops \nrequires (pre)computing the number of iterations oftheloops,togetherwiththeper-iteration CIV values (CIV-COMP). \nThe corresponding loop slice is almost as expensive as the loop, hence the runtime overhead is 47% of \nthe total parallel timing. However, tested on extended datasets, parallelism scales well up to at least \n16 processors ( 7.3x speedup). Figure 10 shows the parallel timings under normal -O2 -ipo compilation. \nUnfortunately, PERFECT-CLUB uses (outdated) small datasets, which, in the cases of flo52, arc2d, dyfesm, \nand ocean results in loop granularities in the range of tens of microseconds, which are simply too small \nto amortize the thread-spawning over\u00adhead. However, extended datasets would likely enable scalable speedups \non all four benchmarks; for example arti.cially increas\u00ading granularity by compiling under option O0 \nresults in speedups: 2.6x, 2.1x, 2.2x and 1.6x, respectively. The modest speedup of ocean is mainly due \na sequential coverage of only 65%, because some parallel loops (in,out) with prohibitively small granularities \n(in the range of one microsecond) have not been considered. We have encountered similar problems in the \nspec77 and qcd codes.  SPEC89 and SPEC92 Suites BENCH BENCH PROPERTIES SELECTED LOOPS LSC % GR ms PAR/SEQ/ \nRT TEST MATRIX300 SC=100%,NLsc=9 SCrt =26% RTov =0% PRIV,RRED SGEMM do160 SGEMM do120 SGEMM do20/40 SGEMM \ndo60/100 30.2% 30% 12.8% 12.8% 160 159 34 34 STATIC-PAR STATIC-PAR OI O(1) OI O(1) SWM256 SC=99%,NLsc=18 \nSCrt =0% PRIV RTov =0% SRED CALC2 do200 CALC3 do300 CALC1 do100 40.6% 29.7% 27.8% .7 .5 .5 STATIC-PAR \nSTATIC-PAR STATIC-PAR ORA SC=100%,NLsc=4 SCrt =0% PRIV, SLV RTov =0% SRED MAIN do9999 MAIN do25 MAIN \ndo412 99.9% 0% 0% 999 0 0 STATIC-PAR STATIC-PAR STATIC-PAR NASA7 SC=90%,NLsc=9 SCrt =43.6% RTov =.03% \nPRIV SLV, SRED, CIVagg GMTTST do120 EMIT do5 BTRTST do120 21.1% 13.2% 9.4% 980 61 436 FI O(1) SLV O(N) \nCIV-COMP FI O(1) TOMCATV SC=100%,NLsc=9 SCrt =0% RTov =0% PRIV,SLV,SRED MAIN do60 MAIN do100$2 MAIN do120$2 \nMAIN do80 37.8% 26.6% 10.9% 10.8% 7 .01 .01 2 STATIC-PAR STATIC-PAR STATIC-PAR STATIC-PAR MDLJDP2 SC=87%,NLsc=6 \nSCrt =0% RTov =0% PRIV,S/RRED FRCUSE do20 POSTFR do20 PREFOR do60 POSTFR do60 82.4% 1.6% 1.5% 1.1% .9 \n.02 .02 .01 STATIC-PAR STATIC-PAR STATIC-PAR STATIC-PAR HYDRO2D SC=92%,NLsc=58 SCrt =0% RTov =0% PRIV \nTISTEP do400 FILTER do300 T1 do10 17.6% 14.2% 7.5% 1.2 .1 .07 STATIC-PAR STATIC-PAR STATIC-PAR Table \n2. Properties of the SPEC89 and SPEC92 suites. The layout of this table is explained in the beginning \nof Section 6   6.3 SPEC89 and SPEC92 Suites Results Figure 11 shows four-processor timings for several \nbenchmarks in SPEC92 suite, fromwhich nasa7 uses independence predicates and CIV aggregation. While predicate \noverheads are negligible, nasa7 obtains only 2.5x speedup because loops GMTTST DO120 and EMIT DO5 have \n2 and 5 iterations, respectively. As with PERFECT-CLUB, we observe that our speedups are superior to \nthe ones of ifort, and our lower speedups correspond to benchmarks that exhibit small loop granularity: \nmdljdp, hydro2d and tomcatv. Tomcatv shows the need for further transformations in or\u00adder to take advantage \nof parallelization. Loops MAIN DO100$2 AND 120$2 exhibit too small granularities, to bene.t from paral\u00adlelization \n(in fact they suffer a slowdown). However the speedup increases to 2 on 8 procesors when the granularity \nis increased via loop interchange (though loosing locality). We expect loop tiling [27] (not implemented) \nto improve matters further.  6.4 SPEC2000 and SPEC2006 Suites Results Figure 12 compares the parallel \nexecution time of our approach against the one xlf r on eight processors and shows that our speed-up \nis superior in most cases. Figure 13 shows our scal\u00adability speedups up to sixteen processors. Benchmark \ngamess, not measured, is notoriously dif.cult to parallelize [3] and we disambiguated only small-granularities \nloops that do not exhibit speedup, while applu exhibits two sequential loops that sum-up to 56% of sequential \ncoverage and contain only sequential or small\u00adgranularity inner loops, which do not contribute to speedup. \nSPEC2000 and SPEC2006 Suites BENCH BENCH PROPERTIES SELECTED LOOPS LSC % GR ms PAR/SEQ/ RT TEST WUPWISE \nSC=93%,NLsc=4 SCrt =93% RTov =0%, PRIV RRED,SLV MULDEO do100 MULDEO do200 MULDOE do100 MULDOE do200 20.6% \n25.8% 20.7% 25.9% 206 258 207 259 F/OI O(1) F/OI O(1) F/OI O(1) F/OI O(1) APSI SC=99%,NLsc=25 SCrt =28% \nRTov =.2% HOIST-USR,PRIV,SRED SLV, NLtot =252 RUN do20/30/40 RUN do50/60/70 WCONT do40 DVDTZ do40 17.6% \n10.4% 11% 10.3% 176 122 330 314 FI HOIST-USR FI HOIST-USR STATIC-PAR STATIC-PAR APPLU SC=98%,NLsc=9 SCrt \n=0% RTov =0% PRIV,S/RRED,SLV BLTS do10 BUTS do1 JACLD do1 JACU do1 28.4% 28.1% 14.1% 10% 119 117 59 314 \nSTATIC-SEQ STATIC-SEQ STATIC-PAR STATIC-PAR MGRID SC=100%,NLsc=12 SCrt =0% RTov =0% PRIV RESID do600 \nPSINV do600 INTERP do800 RPRJ3 do100 51.5% 28.9% 4.9% 4.5% 42 7 2 2 STATIC-PAR STATIC-PAR STATIC-PAR \nSTATIC-PAR SWIM SC=100%,NLsc=18 SCrt =0% RTov =0%, PRIV SRED, NLtot =252 SHALOW do3500 CALC2 do200 CALC1 \ndo100 CALC3 do300 44.8% 20.5% 18% 15.4% 116 53 47 40 STATIC-PAR STATIC-PAR STATIC-PAR STATIC-PAR BWAVES \nSC=100%,NLsc=20 SCrt =0%,PRIV,SLV SRED,NLtot =85 MAT*VEC do1 FLUX do2 SHELL do5$2 75.1% 5.8% 4.2% 206 \n236 509 STATIC-PAR STATIC-PAR STATIC-PAR ZEUSMP SC=99%,NLsc=51 SCrt =10%,RTov =.01% PRIV,SLV UMEG HSMOC \ndo360 MOMX3 do3000 TRANX2/3 do2100 TRANX1 do100 10.3% 5.1% 7.6% 2.4% 783 13 24 26 STATIC-PAR STATIC-PAR \nF/OI O(1) OI O(1) GROMACS SC=90%,NLsc=4 SCrt =90%,RTov =3.4% PRIV,RRED BOUNDS-COMP INL1130 do1 INL1100 \ndo1 INL1000 do1 INL0100 do1 84.8% 2.2% 1.9% 0.8% 33 5 4 1 BOUNDS-COMP BOUNDS-COMP BOUNDS-COMP BOUNDS-COMP \nCALCULIX SC=74%,NLsc=1 SCrt =74%,RTov =8.5% SRED,PRIV,UMEG BOUNDS-COMP MAFILLSM do7 73.7% 14s BOUNDS-COMP \nF/OI O(N) F/OI O(1) GAMESS SC=32%,NLsc=2 SCrt =0% PRIV,RRED DIRFCK do300 GENR70 do170 18% 14.4% .04 .03 \nSTATIC-PAR STATIC-PAR Table 3. Properties of the SPEC2000 and SPEC2006 suites. The layout of this table \nis explained in the beginning of Section 6  We observe that speedups do not scale well between 8 to \n16 pro\u00adcessors; this is likely because the machine has eight dual-core pro\u00adcessors, and executing on \nboth cores decreases the per-core band\u00adwidth. Benchmarks mgrid, swim and bwaves show good speedups, extracted \nstatically. Both calculix and gromacs are written in a mixture of C and Fortran, from which we have analyzed \nand mea\u00adsured only the Fortran part, which shows a sequential coverage of 74% and 90% respectively. We \nremark that our speed-ups are superior to those of IBM s xlf r compiler. Half of the benchmarks use runtime \nparallelization techniques: wupwise, zeusmp and calculix use O(1) and O(N) .ow and output independence \ntests, while apsi proves .ow independence of loops such as RUN DO20 via hoistable-USR evaluation. Both \ngromacs and calculix use reductions, where the target array is allocated in the C part and used in Fortran \nas an assumed-size\u00adarray parameter. Typical reduction implementation (e.g. OpenMP) requires to know the \nupper and lower bounds of the target array. Ourbounds-estimationtechnique, BOUNDS-COMP,describedin Section \n4, is responsible for the overheads RTov =3.4% and 8.5%  of the parallel runtime of gromacs and calculix, \nrespectively. BOUNDS-COMP s overhead (i) slightly increases for gromacs from normalized runtime .01 on \none processor to .02 on 16 processors, due to the small granularity of the BOUNDS-COMP loop, but (ii) \nit scales perfectly with parallelism for calculix: from .16 on one processor to .01 on 16 processors. \nWe note that our technique re\u00adsults in a parallel runtime about 1.66x faster than the one of xlf r on \ngromacs, where xlf r, in the absence of bounds information, appears to be wrapping reduction statements \ninto atomic blocks (another xlf version exhibits slowdown). 7. Related Work Solutions based on Presburger \narithmetic [12, 22] analyze an entire loop nest at a time, albeit in the narrower af.ne domain where \nsub\u00adscripts, loop bounds, if conditions are af.ne expressions of loop indexes. Both the memory dependencies \nand the .ow of values be\u00adtween every pair of read-write accesses are accurately modeled via systems of \naf.ne inequations, which are solved by gaussian-like elimination. These solutions drive powerful code \ntransformations to optimize and enable parallelism [21], but they are most effec\u00adtive when applied on \nrelatively-small, intra-procedural loop nests exhibiting simple control .ow. In comparison, our technique \nis bet\u00adter suited to parallelize larger loops, but is less effective in driving code transformations \nsuch as loop interchange, skewing, tiling, etc. Pugh and Wonnacott are the .rst to show how to interpret \nirre\u00adducible Presburger formulas as simple predicates that can be ver\u00adi.ed either by the user or at runtime. \nThe approach, named con\u00additional dependency analysis [24], existentially quanti.es the vari\u00adables that \ncorrespond to the loop index in the Presburger formula of the .ow dependence and computes the gist of \nthe obtained formula, by removing false-alarm terms. Analysis is extended with uninterpreted-symbol functions \n[23] to model non-af.ne terms and a limited notion of control .ow, where inductive simpli.cation de\u00adrives \nsimpler, (only) suf.cient conditions for independence. To extend analysis to program level, several solutions \nhave been proposed that (i) summarize accesses via an array abstraction, as dictated by (structural) \ndata-.ow analysis, and (ii) model loop in\u00addependence via equations on these summaries. These approaches \ntypically perform conservative approximations to keep the (succes\u00adsive) summary results within the array \nabstraction domain, e.g., they fail to disambiguate some more complicated cases of cou\u00adpled subscripts, \nbut they accommodate better more-complex con\u00adtrol .ow. For example, the simple loop [24]: DOi =1, N,1 \nIF(p[i] > 0) THEN A = ... ELSE A = ... ENDIF ENDDO ... = A produces an inexact .ow-dependence result \nfor scalar A under Pugh and Wonnacott s approach: {[] |.i s.th. 1 = i = n . p(i) > 0}, but the IF-data-.ow \nequation of a summary-based solution would identify that both mutually exclusive branches guard the same \nsum\u00admary, hence the non-af.ne gate (p[i]) can be safely discarded. Hoe.inger et al. use the LMAD [20] \narray abstraction to sum\u00admarize accesses into read-only, write-.rst and read-write sets. The ART test \n[14] builds iteration-level summaries and aggregates them over the targeted loop. If this aggregation \ncreates a new LMAD di\u00admension that does not overlap with the existent dimensions, then a monotonicity-based \nargument establishes that independence holds. Hall et al. organize summaries as systems of af.ne inequations \nand analyze the read, write and exposed-read abstract sets to es\u00adtablish loop independence [13]. In both \napproaches summaries are paired with predicates, typically extracted from control .ow, that guard the \nsummary existence. In addition, Moon and Hall also extract predicates that guard (otherwise unsafe) simpli.cations \nin the array abstraction domain, and use invariants synthetized from branch conditions to enhance the \nprecision of the summary [18]. Adve and Mellor-Crummey present an interesting instance of an equational \nsystem, where summaries are represented via Presburger formula, and summary equations model computation \npartitioning and communication analysis for data-parallel programs [1]. The system de.nes a rich compositional \nalgebra that starts from a set of user-de.ne inputs, such as the alignment of an array with a template \nand the home of certain statements, and computes the set of loca\u00adtions that need to be sent/received \nto/from other processors. This formalism drives several important optimizations, such as mes\u00adsage vectorization, \nin-place communication and in-place splitting. Whenever the result summary falls outside the af.ne domain, \nin\u00adspector/executor techniques are used to verify the desired invariant. In comparison, rather than requiring \nconstraints to be af.ne or resorting to conservative approximations, we build on Rus et al. s language \nof USRs [28], recalled in Section 2, to construct ex\u00adact read-only, read-write, and write-.rst summaries. \nSince runtime evaluation of USRs exhibits in many cases unacceptably-large over\u00adheads, we de.ne a language \ntranslation scheme to an equally ex\u00adpressive language of predicates. The obtained predicate program is \nless constrained than the ones of related solutions, and its construction typically involves less con\u00adservative \napproximations. For example, when predicate symbols are mutable we use a program slice to compute them. \nWe improve both qualitatively and quantitatively on Rus et al. work [28, 29] in that: we de.ne a complex \nsystem of inference rules, which are used more aggressively to derive predicates, and we show that predicates \ncan disambiguate many loops that were either unreported or previ\u00adously solved via the more expensive \nUSR evaluation. Finally, we present the compiler infrastructure for improving predicates accu\u00adracy (e.g., \nUSR reshaping rules), for factorizing and cascading the predicate program into a set of increasingly-complex \nconditions for independence, and for implementing conditional reduction, etc. A signi.cant amount of \nwork was aimed at disambiguating a class of irregular subscripts exhibiting quadratic or array indexing \nor induction variables without closed-form solutions. One direc\u00adtion was to enhance the mathematical \nsupport with more accurate symbolic ranges [8, 10], or more encompassing algebras, such as representing \ninduction-variables via chains of recurrence [9]. For example, Blume and Eigenmann s Range Test [7] uses \nthe extended range support and exploits the monotonicity of read-write accesses to disambiguate a class \nof quadratic or coupled indexing (e.g., loops olda do100/300 from trfd or ocean s ftrvmt do109). Lin \nand Padua extend the library of recognizable access pat\u00adterns to solve: (i) stack/queue-access patterns \n[16] (e.g., loops from bdna, p3m and tree, but not track), and (ii) index-array ac\u00adcesses [15]. The latter \nextends the applicability of the Range Test to cover array indexing, for the cases when the index-array \nvalue properties assumed by pattern recognition can be statically veri.ed (e.g. loops from trfd and dyfesm, \nbut not solvh do20). Pugh and Wonnacott extension of Presburger arithmetic [23] also solves a number \nof loops that would fall outside the tradi\u00adtional af.ne domain: for example some tricky cases of coupled \nsub\u00adscripts, or the indirect array pattern of loop intgrl do540 from trfd or the non-trivial control \n.ow of mdg s interf do1000 and poteng do2000, or the quasi-af.ne pattern of loop filerx do290 from arc2d. \nHowever, loops olda do100/300 from trfd are only recognized to form a monotonic indexing sequence before \ninduction variable substitution, while loops actfor do240 and extend do400 from bdna and track are not \ndisambiguated. Finally, the ART test [14] of Hoe.inger et al. disambiguates a class of coupled subscripts \nand exponential indexing. In comparison, we present a uni.ed framework that parallelizes a large class \nof loops that have been previously analyzed with a  number of different techniques: for example we match \nthe results reported by SUIF [13] on a number of statically analyzable bench\u00admarks (e.g., mdg, ora, swim, \napplu, mgrid, hydro2d etc.), and also solve most of the non-af.ne benchmarks that require condi\u00adtional \nanalysis. The latter corresponds in part to a complex trans\u00adlation rule [19] that exploits the monotonicity \nof a summary of a particular shape (see Section 3.3). Notable exceptions are the stack\u00adaccess pattern \nof benchmark p3m, and several loops exhibiting ex\u00adponential indexing and tricky instances of coupled \nsubscripts. We solve loop run do20 from apsi via expensive USR evaluation, but which still results in \nnegligible overhead due to the use of memoiza\u00adtion. In addition we parallelize a number of previously \nunreported benchmarks using our minimal-weight predicates (e.g., calculix, zeusmp, wupwise, nasa, track, \ngromacs). The other main direction of approaching autoparallelization has been to analyze memory references \nat run-time, either via inspec\u00adtor/executor [26], or via TLS techniques [25], or via faster but less \nscalable techniques [30]. These techniques have overhead propor\u00adtional to the number of the original-loop \naccesses, and hence we use then only as last resort, once all the lighter predicates failed. 8. Conclusions \nIn this paper we presented a fully automatic approach to loop paral\u00adlelization that integrates the use \nof static and run-time analysis and thus overcomes many previously known dif.culties. Starting from our \nrich array reference representation, the USR language, we ex\u00adpressed the independence condition as an \nequation, S = \u00d8, where S is a set expression representing array indexes. We introduced a language translation \nF from the USR set-expression language to a language of predicates (F(S) . S = \u00d8). Loop parallelization \nis then validated using a novel logic inference algorithm that fac\u00adtorizes the obtained complex predicates \n(F(S)) into a sequence of suf.cient-independence conditions which are evaluated .rst stati\u00adcally and, \nwhen necessary, at run-time, in increasing order of their estimated complexities. The experimental evaluation \non 26 bench\u00admarks from PERFECT-CLUB and SPEC suites and show speedups as high as 4.5x and 8.4x and on \naverage 2.4x and 5.4x on four and eight processors, respectively, which are superior to the ones of IN-TEL \ns ifort and IBM s xlf r commercial compilers. Our transla\u00adtion technique from set to predicate language \nand our factorization algorithm are extensible and can be applied to a variety of con\u00additional optimizations. \nIt is powerful because it can make use of dynamic information very ef.ciently. Acknowledgments This work \nwas supported in part by NSF awards CRI-0551685, CCF-0833199, CCF-0830753, IIS-0917266, NSF/DNDO award \n2008-DN-077-ARI018-02, by the DOE NNSA PSAAP grant DE\u00adFC52-08NA28616, IBM, Intel, Oracle/Sun and Award \nKUS-C1\u00ad016-04, made by King Abdullah University of Science and Tech\u00adnology (KAUST). Since November 2011 \nCosmin Oancea was sup\u00adported by Danish Strategic Research Council, for the HIPERFIT research center under \ncontract 10-092299. References [1] V. Adve and J. Mellor-Crummey. Using Integer Sets for Data-Parallel \nProgram Analysis and Optimization. In Procs. Int. Conf. Prog. Lang. Design and Implementation, 1998. \n[2] R. Allen and K. Kennedy. Optimizing Compilers for Modern Architectures. Morgan Kaufmann, 2002. ISBN \n1-55860-286-0. [3] B. Armstrong and R. Eigenmann. Application of Automatic Paral\u00adlelization to Modern \nChallenges of Scienti.c Computing Industries. In Int. Conf. Parallel Proc. , pages 279 286, 2008. [4] \nU. Banerjee. Speedup of Ordinary Programs. Ph.D. Thesis, Univ. of Illinois at Urbana-Champaign, Report \nNo. 79-989, 1988. [5] W. Blume et al. Parallel Programming with Polaris. Computer, 29(12), 1996. [6] \nW. Blume and R. Eigenmann. Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks \nPrograms. IEEE Transactions on Parallel and Distributed Systems, 3:643 656, 1992. [7] W. Blume and R. \nEigenmann. The Range Test: A Dependence Test for Symbolic, Non-Linear Expressions. In Procs. Int. Conf. \non Supercomp, pages 528 537, 1994. [8] W. Blume and R. Eigenmann. Demand-Driven,Symbolic Range Propagation. \nIn Procs. Int. Lang. Comp. Par. Comp. , 1995. [9] R. A. V. Engelen. A uni.ed framework for nonlinear \ndependence testing and symbolic analysis. In Procs. Int. Conf.on Supercomp, pages 106 115, 2004. [10] \nT. Fahringer. Ef.cient Symbolic Analysis for Parallelizing Compilers and Performance Estimator. Journal \nof Supercomp,12:227 252,1997. [11] P. Feautrier. Parametric Integer Programming. Operations Research, \n22(3):243 268, 1988. [12] P. Feautrier. Data.ow Analysis of Array and Scalar References. Int. Journal \nof Par. Prog, 20(1):23 54, 1991. [13] M. W. Hall, S. P. Amarasinghe, B. R. Murphy, S.-W. Liao, and M. \nS. Lam. Interprocedural Parallelization Analysis in SUIF. Trans. on Prog. Lang. and Sys., 27(4):662 731, \n2005. [14] J. Hoe.inger, Y. Paek, and K. Yi. Uni.ed Interprocedural Parallelism Detection. Int. Journal \nof Par. Prog, 29(2):185 215, 2001. [15] Y. Lin and D. Padua. Demand-Driven Interprocedural Array Property \nAnalysis. In Procs. Int. Lang. Comp. Par. Comp., 1999. [16] Y. Lin and D. Padua. Analysis of Irregular \nSingle-Indexed Arrays and its Applications in Compiler Optimizations. In Procs. Int. Conf. on Compiler \nConstruction, pages 202 218, 2000. [17] B. Lu and J. Mellor-Crummey. Compiler Optimization of Implicit \nReductions for Distributed Memory Multiprocessors In Int. Par. Proc. Symp., 1998 [18] S. Moon and M. \nW. Hall. Evaluation of Predicated Array Data-Flow Analysis for Automatic Parallelization. In Proc. of \nPrinciples and Practice of Parallel Programming, pages 84 95, 1999. [19] C. E. Oancea and L. Rauchwerger. \nA Hybrid Approach to Proving Memory Reference Monotonicity. In Procs. Int. Lang. Comp. Par. Comp., 2011. \n[20] Y. Paek, J. Hoe.inger, and D. Padua. Ef.cient and Precise Array Access Analysis. Trans. on Prog. \nLang. and Sys., 24(1):65 109, 2002. [21] L.N. Pouchet, et al. Loop Transformations: Convexity, Pruning \nand Optimization. In Procs. of Princ. of Prog. Lang., 2011. [22] W. Pugh. The Omega Test: a Fast and \nPractical Integer Programming Algorithm for Dependence Analysis. Com. of the ACM, 8:4 13, 1992. [23] \nW. Pugh and D. Wonnacott. Nonlinear Array Dependence Analysis. In Proc. Lang. Comp. Run-Time Support \nScal. Sys., 1995. [24] W. Pugh and D. Wonnacott. Constraint-Based Array Dependence Analysis. In Trans. \non Prog. Lang. and Sys. , 20(3), 635 678, 1998. [25] L. Rauchwerger and D. Padua. The LRPD Test: Speculative \nRun-Time Parallelization of Loops with Privatization and Reduction Parallelization. IEEE Trans. Par. \nDistrib. Sys, 10(2):160 199, 1999. [26] L. Rauchwerger, N. Amato, and D. Padua. A Scalable Method for \nRun Time Loop Parallelization. Int. Journal of Par. Prog,26:26 6,1995. [27] L. Renganarayanan, et. al. \nParameterized Tiled Loops for Free. In Int. Conf. Prog. Lang. Design and Implementation., 2007. [28] \nS. Rus, J. Hoe.inger, and L. Rauchwerger. Hybrid analysis: Static &#38; dynamic memory reference analysis. \nInt. Journal of Par. Prog, 31(3): 251 283, 2003. [29] S. Rus, M. Pennings, and L. Rauchwerger. Sensitivity \nAnalysis for Automatic Parallelization on Multi-Cores. In Procs. Int. Conf. on Supercomp, pages 263 273, \n2007. [30] X. Zhuang, et. al. Exploiting Parallelism with Dependence-Aware Scheduling. In Int. Conf. \nPar. Arch. Compilation Tech., 2009.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>This paper presents a fully automatic approach to loop parallelization that integrates the use of static and run-time analysis and thus overcomes many known difficulties such as nonlinear and indirect array indexing and complex control flow. Our hybrid analysis framework validates the parallelization transformation by verifying the independence of the loop's memory references. To this end it represents array references using the USR (uniform set representation) language and expresses the independence condition as an equation, <i>S</i>=0, where <i>S</i> is a set expression representing array indexes. Using a language instead of an array-abstraction representation for <i>S</i> results in a smaller number of conservative approximations but exhibits a potentially-high runtime cost. To alleviate this cost we introduce a language translation <i>F</i> from the USR set-expression language to an equally rich language of predicates (<i>F</i>(<i>S</i>) ==&gt; <i>S</i> = 0). Loop parallelization is then validated using a novel logic inference algorithm that factorizes the obtained complex predicates (F(<i>S</i>)) into a sequence of sufficient independence conditions that are evaluated first statically and, when needed, dynamically, in increasing order of their estimated complexities. We evaluate our automated solution on 26 benchmarks from PERFECT-Club and SPEC suites and show that our approach is effective in parallelizing large, complex loops and obtains much better full program speedups than the Intel and IBM Fortran compilers.</p>", "authors": [{"name": "Cosmin E. Oancea", "author_profile_id": "81100509575", "affiliation": "University of Copenhagen, Copenhagen, Denmark", "person_id": "P3471311", "email_address": "cosmin.oancea@diku.dk", "orcid_id": ""}, {"name": "Lawrence Rauchwerger", "author_profile_id": "81100504584", "affiliation": "Texas A&#38;M University, College Station, TX, USA", "person_id": "P3471312", "email_address": "rwerger@cse.tamu.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254124", "year": "2012", "article_id": "2254124", "conference": "PLDI", "title": "Logical inference techniques for loop parallelization", "url": "http://dl.acm.org/citation.cfm?id=2254124"}