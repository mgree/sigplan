{"article_publication_date": "06-11-2012", "fulltext": "\n Language-Based Control and Mitigation of Timing Channels Danfeng Zhang Aslan Askarov * Andrew C. Myers \nDepartment of Computer Science School of Engineering and Computer Department of Computer Science Cornell \nUniversity Science, Harvard University Cornell University Ithaca, NY 14853 Cambridge, MA 02138 Ithaca, \nNY 14853 zhangdf@cs.cornell.edu aslan@seas.harvard.edu andru@cs.cornell.edu Abstract We propose a new \nlanguage-based approach to mitigating timing channels. In this language, well-typed programs provably \nleak only a bounded amount of information over time through external tim\u00ading channels. By incorporating \nmechanisms for predictive mitiga\u00adtion of timing channels, this approach also permits a more expres\u00adsive \nprogramming model. Timing channels arising from interaction with underlying hardware features such as \ninstruction caches are controlled. Assumptions about the underlying hardware are explic\u00aditly formalized, \nsupporting the design of hardware that ef.ciently controls timing channels. One such hardware design \nis modeled and used to show that timing channels can be controlled in some simple programs of real-world \nsigni.cance. Categories and Subject Descriptors D.2.4 [Software/Program Veri.cation]: Formal Methods; \nD.4.6 [Security and protection]: Information Flow Controls General Terms Languages, Security Keywords \nTiming channels, mitigation, information .ow 1. Introduction Timing channels have long been a dif.cult \nand important problem for computer security. They can be used by adversaries as side chan\u00adnels or as \ncovert channels to learn private information, including cryptographic keys and passwords [8, 13, 14, \n18, 22, 24, 29, 36]. Timing channels can be categorized as internal or external [28]. Internal timing \nchannels exist when timing channels are converted to storage channels within a system and affect the \nresults computed. External timing channels exist when the adversary can learn some\u00adthing from the time \nat which the system interacts with the outside world. In either case, con.dential information transmitted \nthrough timing channels constitutes timing leakage. Internal timing channels that exploit races between \nthreads have been addressed by enforcing low determinism [16, 37] and by con\u00ad straining thread scheduling \n[26, 28]. The focus of this paper is in\u00ad stead on controlling external timing channels, for which current \nmethods are less satisfactory. Starting with Agat [3], program trans\u00ad formations have been proposed to \nremove external timing channels. However, these methods restrict expressiveness: for example, loop guards \ncan depend only on public information. Further, they do not * This work was done while the author was \nat Cornell University. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00 handle some realistic hardware features. External mitigation is an\u00adother approach to control \nexternal timing channels, by quantitatively limiting how much information leaks via the timing of external \nin\u00adteractions [5, 20, 38]. Since external mitigation treats computation as a black box, it cannot distinguish \nbetween benign timing varia\u00adtions and variations that leak information. When most timing varia\u00adtion is \nbenign, this leads to a signi.cant performance penalty. This work introduces a more complete and effective \nlanguage\u00adbased method for controlling external timing channels, with prov\u00adably bounded leakage. Broadly, \nthe new method improves control of external timing channels in three ways: Unlike methods based on code \ntransformation [3], this method supports more realistic programs and hardware. For example, it can be \nimplemented on hardware with an instruction cache.  Another difference from code-transformation approaches \nis that it offers a fully expressive programming model; in particular, loops with high (con.dential) \nguards are permitted.  The method does not need to be as conservative as external tim\u00ading mitigation \nbecause a program analysis can distinguish be\u00adtween benign timing variations and those carrying con.dential \ninformation, and can distinguish between multiple distinct se\u00adcurity levels. This .ne-grained reasoning \nabout timing channels improves the tradeoff between security and performance.  Timing channels arise \nin general from the interaction of pro\u00adgrams with the underlying implementation of the language in which \nthe programs are written. This language implementation includes not only the compiler or interpreter \nused, but also the underlying hardware. Reasoning accurately about timing channels purely at the source \nlanguage level is essentially impossible because language se\u00admantics, by design, prevent precise reasoning \nabout time. An important contribution of this paper is therefore a system of simple, static annotations \nthat provides just enough information about the underlying language implementation to enable accurate \nreasoning about timing channels. These annotations form a contract between the software (language) level \nand the hardware implemen\u00adtation. A second contribution of this paper is a formalization of this contract. \nUsing this formal contract, implementers may verify that their compiler and architecture designs control \ntiming channels. We illustrate this by sketching the design of a simple memory cache architecture that \navoids timing channels. A third contribution is a new language mechanism that improves expressive power \nachievable while controlling timing channels. It uses predictive timing mitigation [5] to bound the amount \nof infor\u00ad mation that leaks through timing. With this mechanism, algorithms whose timing behavior does \ndepend on con.dential information can be implemented securely; predictive mitigation ensures that total \ntiming leakage is bounded by a programmer-speci.ed function.  To evaluate the correctness and effectiveness \nof our approach, we simulated hardware satisfying the hardware side of the software hardware contract. \nWe evaluate the use of our approach on two applications vulnerable to timing attacks. The results suggest \nthat the combination of language-based mitigation and secure hardware works well, with only modest slowdown. \nWe proceed as follows. Section 2 discusses the problem of con\u00ad trolling timing channels on modern computer \nhardware, and gives an overview of the new method. Section 3 introduces a program\u00ad ming language designed \nto permit precise reasoning about timing channels. Its semantics formalize several constraints that must \nbe satis.ed by a secure implementation. Section 4 sketches how these constraints can be satis.ed by both \nstock and specialized hardware implementations. A type system for the language that soundly con\u00adtrols \ntiming channels is presented in Section 5; its novel multilevel quantitative security guarantees are \nexplored in Section 6. Predictive mitigation of timing channels is discussed in Section 7. Section 8 \npresents performance results from a simulated implementation of language-based predictive mitigation. \nRelated work is covered in Section 9, and Section 10 concludes. 2. Language-level timing mitigation Controlling \ntiming channels is dif.cult because con.dential infor\u00admation can affect timing in many ways, and yet \nwe want to be able to analyze these timing dependencies at the source level. However, language semantics \ndo not and should not de.ne timing precisely. 2.1 Timing dependencies We call timing channels visible \nat the source-language level direct timing dependencies. In this example, control .ow affects timing. \nAssume h holds con.dential data and that sleep (e) suspends execution of the pro\u00ad 1 if (h) 2 sleep (1); \ngram for the amount of time speci.ed by 3 else e. Since line 4 takes longer to execute 4 sleep (10); \nthan line 2, one bit of h is leaked through 5 sleep (h); timing. Attacks on RSA have also used control-.ow-related \ntiming channels [8, 18]. Another source of direct timing dependencies is operations whose execution time \ndepends on parameter values, such as the sleep command at line 5. Modern hardware also creates indirect \ntiming dependencies in which execution time depends on hardware state that has no source\u00adlevel representation. \nThe following code shows that the data cache is one source of indirect dependencies. Suppose only h1 \nand h2 are con.den\u00adtial and that neither l1 nor l2 are cached 1 if (h1) 2 h2:=l1; initially. Even though \nboth branches have 3 else the same instructions and similar memory 4 h2:=l2; access patterns, executing \nthis code frag\u00ad5 l3:=l1; ment is likely to take less time when h1 is not zero: because l1 is cached at \nline 2, line 5 runs faster, and the value of h1 leaks through timing. Some timing attacks [14, 24] also \nexploit data cache timing de\u00ad pendencies to infer AES encryption keys, but indirect dependencies arising \nfrom other hardware components have also been exploited to construct attacks: instruction and data caches \n[1], branch predictors and branch target buffers [2], and shared functional units [34]. We use the term \nmachine environment to refer to all hardware state that is invisible at the language level but that is \nneeded to predict timing. Timing channels relying on indirect dependencies are at best dif.cult to reason \nabout at language level the semantics of languages and even of instruction set architectures (ISAs) hide \ninformation about execution time by abstracting away low-level implementation details. For instance, \nwe do not know that the timing of line 5 depends on h1 without knowing how the data cache works in the \nexample above. It is worth noting that we assume a strong adversary that is par\u00adticularly interesting \nwith the rise of cloud computing: an adversary coresident on the system, controlling concurrent threads \nthat can read low memory locations. The adversary can therefore time when low memory locations change. \nFurther, the adversary can probe tim\u00ading using the shared cache. This is a more powerful adversary than \nthat considered in much previous work on timing channels, includ\u00ading prior attempts to control decryption \nside channels [5, 20, 38]. The prior methods are not effective against this adversary, who can ef.ciently \nlearn secret keys using timing side channels [24].  2.2 Representing indirect timing dependencies abstractly \nRecent work in the architecture community has aimed for a hardware\u00adbased solution to timing channels. \nTheir hardware designs implic\u00aditly rely on assumptions about how software uses the hardware, but these \nassumptions have not been rigorously de.ned. For example, the cache design by Wang and Lee [35] works \nonly under the as\u00ad sumption that the AES lookup table is preloaded into cache and that the load time \nis not observable to the adversary [19]. Timing channels cannot be controlled effectively purely at the \nsource code or the hardware level. Hardware mechanisms can help, but do a poor job of controlling language-level \nleaks such as implicit .ows. The question, then, is how to usefully and accurately charac\u00adterize the \ntiming semantics of code at the source level. Our insight is to combine the language-level and hardware-level \napproaches, by representing the machine environment abstractly at source level. As is standard in information \n.ow type systems [27], we asso\u00ad ciate all information with a security label that in this case describes \nthe con.dentiality of the information. Labels e1 and e2 are ordered, written e1 g e2, if e2 describes \na con.dentiality requirement that is at least as strong as that of e1. It is secure for information to \n.ow from label e1 to label e2. We assume there are at least two distinct labels L (low) and H (high) \nsuch that L g H g L. The label of public information is L; that of secret is H. As is standard, we denote \nby T the most restrictive label, and by ., the least restrictive one. We assume that different components \nof the machine environ\u00adment have security labels as well. For example, different partitions in a partitioned \ncache [35] can be associated with different labels. To track how information .ows into the machine environment, \nbut without concretely representing the hardware state, we associate two labels with each command in \nthe program. The .rst of these labels is the command s read label er. The read label is an upper bound \non the label of hardware state that affects the run time of the command. For example, the run time of \na command with er = L depends only on hardware state with label L or below. The second of these labels \nis the command s write label ew. The write label is a lower bound on the label of hardware state that \nthe command can modify. It ensures that the labels of hardware state re.ect the con.dentiality of information \nthat has .owed into that state. For example, suppose that there is only one (low) data cache, which to \nbe conservative means that anyone can learn from timing whether a given memory location is cached. Therefore \nboth the read and write label of every command must be L. The previous example is then annotated as follows, \nwhere the .rst label in brackets is the read label, and the second, the write label. The example on the \nleft is insecure be\u00ad 1 2 3 if ( h1 )[L,L] h2 := l1 ;[L,L] else cause execution of lines 2 and 4 is con\u00additioned \non the high variable h1. There\u00adfore these lines are in a high context, one 4 5 h2 := l2 ;[L,L] l3 := \nl1 ;[L,L] in which the program counter label [11] is high. If lines 2 and 4 update cache state in the \nusual way, the low write label permits low hardware state to be affected by h1. This insecure information \n.ow is a form of implicit .ow [11], but one in which hardware state with no language-level representation \nis being updated.  Since lines 2 and 4 occur in a high context, the write label of these commands must \nbe H for this program to be secure. Conse\u00adquently, the hardware may not update low parts of the machine \nenvi\u00adronment. One way to avoid modifying the cache is to deactivate it in high contexts. A generalization \nof this idea is to partition the cache into two partitions, low and high. Cache misses in a high context \nthen cause only the high cache partition to be updated. With er and ew abstracting the timing behavior \nof hardware, timing channel security can be statically checked at the language level, according to the \ntype system described in Sec. 5. Moreover, these timing labels could be inferred automatically according \nto the type system, reducing the burden on programmers.  2.3 Language-level mitigation of timing channels \nStrictly disallowing all timing leakage can be done as sketched thus far, but results in an impractically \nrestrictive programming language because execution time is not permitted to depend on con.dential information \nin any way. To increase expressiveness, we introduce a new command mitigate to the language. Command \nmitigate (e,e) c executes the command c while ensuring that timing leakage is bounded. The expression \ne computes an initial prediction for the execution time of c. The label e bounds what information that \ncan learned by ob\u00adserving the timing leakage c. That is, no information at level e' such that e' g e \ncan be learned from c s execution time. This property is enforced by the type system of Sec. 5. Moreover, \nthe type system ensures that timing leakage can be bounded using the variation in the execution time \nof mitigate commands. To provide a strict bound on the execution time of mitigate commands while providing \npractical performance, we introduce the use of predictive timing mitigation [5, 38] as a language-level \nmech\u00ad anism. The idea is that given a prediction of how long executing c will take (the e in mitigate \ncommand), the mitigate command ensures that at least that much time is consumed by simply waiting if \nnecessary. In the case of a misprediction (that is, when the esti\u00admate is too low), a larger prediction \nis generated , and the execution time is padded accordingly. Mispredictions also in.ate the predic\u00adtions \ngenerated by subsequent uses of mitigate. For example, we can use mitigate to limit timing leakage from \nthe command sleep(h), as in this program: mitigate (1,H){ sleep (h)[H,H] } The possible execution times \nof this program will not be arbitrary; they might, for example, be forced by mitigate to be the powers \nof 2. Limiting the possible execution times bounds the timing leakage from sleep. We explore the details \nof the mitigation mechanism more fully in Sec. 7 and evaluate its performance in Sec. 8. Previous work \nhas shown that predictive timing mitigation can bound timing leakage to a function that is sublinear \n(in fact, poly\u00adlogarithmic) in time. But this is the .rst work that provides similar, quantitative bounds \non timing leakage at the language level. 3. A language for controlling timing channels Fig. 1 gives the \nsyntax for a simple imperative language extended with our mechanism. All the novel elements read and \nwrite labels, and the mitigate and sleep commands have already been intro\u00adduced. Notice that the sequential \ncomposition command itself needs no timing labels. As a technical convenience, each mitigate in the source \nhas a unique identi.er .. These identi.ers are mainly used in Sec. 6; they are omitted where they are \nnot essential. We present our semantics in a series of modular steps. We start with a core semantics, \na largely standard semantics for a while\u00adlanguage, which ignores timing. Next, we develop an abstracted \nfull e ::= n | x | e op e c ::= skip[er ,ew ] | (x := e)[er ,ew] | c;c | (while e do c)[er ,ew] | (if \ne then c1 else c2)[er ,ew ] | (mitigate. (e,e) c)[er ,ew ] | (sleep e)[er ,ew] Figure 1: Syntax of the \nlanguage (skip[er ,ew],m).(stop, m)((sleep e)[er ,ew], m).(stop,m) ((mitigate (e,e) c)[er ,ew], m).(c, \nm) ') '')' (c1,m).(stop,m(c1, m).(c1,mc= stop 1 ' (c1;c2,m).(c2,m') (c1; c2,m).(c1;c2,m') (e,m). v ((x \n:= e)[er ,ew], m).(stop,m[x . v]) (e, m). nn = 0 =. i = 1 n = 0 =. i = 2 ((if e then c1 else c2)[er ,ew],m).(ci,m) \n(e, m). nn = 0 ((while e do c)[er ,ew], m).(c;(while e do c)[er ,ew], m) (e, m). nn = 0 ((while e do \nc)[er ,ew ],m).(stop, m) Figure 2: Core semantics of commands (unmitigated) semantics that describes \nthe timing semantics of the language more accurately while abstracting away parameters that depend on \nthe language implementation, including the hardware and the compiler. 3.1 Core semantics For expressions \nwe use a standard big-step evaluation (e,m). v when expression e in memory m evaluates to value v. For \ncommands ' (Fig. 2), we write (c,m).(c,m') for the transition of command c in memory m to command c' \nin memory m'. Note that read and write labels are not used in these rules. The rules use stop as a syntactic \nmarker of the end of computation. We distinguish stop from the command skip[er ,ew] because skip is a \nreal command that may consume some measurable time (e.g., reading from the instruction cache), whereas \nstop is purely syntactic and takes no time at all. For mitigate we give an identity semantics for now: \nmitigate (e,e) c simply evaluates to c. Since time is not part of the core semantics, sleep behaves like \nskip.  3.2 Abstracted full language semantics The core semantics ignores timing; the job of the full \nlanguage semantics is to supply a complete description of timing so that timing channels can be precisely \nidenti.ed. Writing down a full semantics as a set of transition rules would de.ne the complete timing \nbehavior of the language. But this would be useful only for a particular language implementation on particu\u00adlar \nhardware. Instead, we permit any full semantics that satis.es a certain set of properties yet to be described. \nWhat is presented here is therefore a kind of abstracted full semantics in which only the key properties \nare .xed. This approach makes the results more general. These key properties fall into two categories, \nwhich we call faith\u00adfulness requirements and security requirements. The faithfulness re\u00adquirements (Sec. \n3.5) are straightforward; the security requirements (Sec. 3.6) are more subtle.  3.3 Con.gurations \nCon.gurations in the full semantics have the form (c,m,E,G). As in the core semantics, c and m are the \ncurrent program and mem\u00adory. Component E is the machine environment, and G is the global clock. In general \nG can be measured in any units of time, but we in\u00adterpret it as machine clock cycles hereafter. We write \n(c,m, E,G). '' (c ,m ,E' , G') for evaluation transitions. The full semantics of expression evaluation \nobviously also needs to be small-step, but we choose a presentation style that elides the details of \nexpression evaluation. As before, the machine environment E represents hardware state that may affect \ntiming but that is not needed by the core semantics. Hardware components captured by E include the data \ncache and in\u00adstruction cache, the branch prediction buffer, the translation looka\u00adside buffer (TLB), \nand other low-level components. The machine environment might also include hidden state added by the \ncompiler for performance optimization. For example, if one considers only the timing effects of data \ncache and instruction caches, denoted by D and I respectively, E could be a con.guration of the form \nE = (D,I). Note that while both the memory m and the machine environ\u00adment E can affect timing, only the \nmemory affects program control .ow. This is the reason to distinguish them in the semantics. The environment \nE can be completely abstract as long as the properties for the full semantics are satis.ed. This separation \nalso ensures that the core semantics is completely standard. The separation of m and E also clari.es \npossibilities for hard\u00adware design. For instance, it is possible for con.dential data to be stored securely \nin a public partition of E, but not in public memory (cf. Sec. 4.1). 3.4 Threat model To evaluate whether \nthe programming language achieves its secu\u00adrity goals, we need to describe the power of the adversary \nin terms of the semantics. We associate an adversary with a security level eA bounding what information \nthe adversary can observe directly. To represent the con.dentiality of memory, we assume that an environ\u00adment \nG maps variable names to security levels. If a memory location (variable) has security level e that .ows \nto eA (that is, e g eA), the adversary is able to see the contents of that memory location. Re\u00adcall that \nwe are defending against a strong, coresident adversary. Therefore, by monitoring such a memory location \nfor changes, the adversary can also measure the times at which the location is up\u00addated. Two memories \nm1 and m2 are e-equivalent, denoted m1 ~e m2, when they agree on the contents of locations at level e \nand below: m1 ~e m2 . .x . G(x) g e. m1(x)= m2(x) Intuitively, e-equivalence of two memories means that \nan observer at level e cannot distinguish these two memories. Projected equivalence. We de.ne projected \nequivalence on memories to require equivalence of variables with exactly level e: m1 .e m2 . .x . G(x)= \ne. m1(x)= m2(x) We assume there is a corresponding projected equivalence relation on machine environments. \nIf two machine environments E1 and E2 have equivalent e-projections, denoted E1 .e E2, then e-level infor\u00admation \nthat is stored in these environments is indistinguishable. The precise de.nition of projected equivalence \ndepends on the hardware and perhaps the language implementation. For example, for a two\u00adlevel partitioned \ncache containing some entries at level L and some at level H, two caches have equivalent H-projections \nif they contain the same cache entries in the H portion, regardless of the L entries. Using projected \nequivalence it is straightforward to de.ne e\u00adequivalence on machine environments: ' E1 ~e E2 . .e g e. \nE1 .e ' E2 3.5 Faithfulness requirements for the full semantics The faithfulness requirements for the \nfull semantics comprise four properties: adequacy, deterministic execution, sequential composi\u00adtion, \nand accurate sleep duration. Adequacy speci.es that the core semantics and the full seman\u00adtics describe \nthe same executions: for any transition in the core se\u00admantics there is a matching transition in the \nfull semantics and vice versa. ' PROPERTY 1 (Adequacy of core semantics). .m,c,c ,E, G . ' ''' ' (.E \n,G ' . (c,m,E,G).(c ,m , E ,G ')) .(c,m).(c ,m ') We also require that the full semantics be deterministic, \nwhich means that the machine environment E completely captures the possible in.uences on timing. PROPERTY \n2 (Deterministic execution). .m,c, E, G . (c,m,E,G).(c1, m1,E1, G1).(c,m,E, G).(c2,m2,E2,G2) =. E1 = \nE2 . G1 = G2 Since the core semantics is already deterministic, determinism of the machine environment \nand time components suf.ces. Sequential composition must correctly accumulate time and propagate the \nmachine environment. PROPERTY 3 (Sequential composition). 1. .c1, c2,m,E,G . '' '' (c1, m,E, G).(stop,m \n, E,G') . (c1;c2,m,E,G).(c2, m ,E,G') ' 2. .c1, c2,c1,m,E, G such that c' = stop . 1 '' ''' (c1, m,E, \nG).(c1,m ,E' ,G') . (c1;c2,m, E,G).(c1;c2, m , E,G') Finally, the sleep command must take the correct \namount of time because it is used for timing mitigation. When its argument is negative, it is assumed \nto take no time. PROPERTY 4 (Accurate sleep duration). .n, m, E, G,er,ew . ((sleep n)[er ,ew], m, E,G).(stop,m,E \n' ,G ') . G ' = G+max(n, 0) Discussion. The faithfulness requirements are mostly straightfor\u00adward. The \nassumption of determinacy might sound unrealistic for concurrent execution. But if information leaks \nthrough timing be\u00adcause some other thread preempts this one, the problem is in the scheduler or in the \nother thread, not in the current thread. Determin\u00adistic time is realistic if we interpret G as the number \nof clock cycles the current thread has used.  3.6 Security requirements for the full semantics For security, \nthe full semantics also must satisfy certain properties to ensure that read and write labels accurately \ndescribe timing. These properties are speci.ed as constraints on the full semantic con.gura\u00adtions that \nmust hold after each evaluation step. In the formalization of these properties, we quantify over labeled \ncommands with the form c[er ,ew]: that is, all commands except sequential composition. Write labels. \nThe write label ew is the lower bound on the parts of the machine environment that a single evaluation \nstep modi.es. Property 5 in Fig. 3 formalizes the requirements on the machine environment: executing \na labeled command c[er ,ew] cannot modify parts of the environment at levels to which ew does not .ow. \nExample. Consider program sleep(h)[er ,H] under the two-level security lattice L g H. This command is \nannotated with the write label H. The only level e such that ew g e is e = L. In this case, Property \n5 requires that an execution of sleep(h)[er ,H] does not modify L parts of the machine environment. Consider \nprogram sleep(h)[er ,L] which has write label L. Be\u00adcause there is no security level e such that L g \ne, Property 5 does not constrain the machine environment for this command.  ments when H-parts of the \nmachine environment propagate into the PROPERTY 5 (Write label). Given a labeled command c[er ,ew ], \nM-parts. To control such propagation, we introduce Property 7 in and a level e such that ew g e Fig. \n3. Note that here level e is independent of read or write labels. '' .m, E,G . (c[er ,ew ],m,E,G).(c \n,m ,E ' ,G ') =. EE ' e 4. A sketch of secure hardware PROPERTY 6 (Read label). Given any command c[er \n,ew], To illustrate how the requirements for the full language semantics .m1,m2,E1,E2,G . (.x . vars1(c[er \n,ew]) . m1(x)= m2(x)) enable secure hardware design, we sketch two possible ways for a design of cache \nand TLB to realize Properties 5 7. For simplicity, . E1 ~erE2 we assume the two-point label lattice L \ng H throughout this section. ' .(c[er ,ew ],m1,E1,G).(c1,m1,E1' ,G1) We start with a standard single-partition \ndata cache similar to current commodity cache designs and then explore a more sophisti\u00ad ' .(c[er ,ew \n],m2,E2,G).(c2,m2,E2' ,G2) =. G1 = G2 cated partitioned cache similar to that in [35]. PROPERTY 7 (Single-step \nmachine-environment noninterference). 4.1 Choosing machine environments Given any labeled command c[er \n,ew], and any level e, The machine environment does not need to include all hardware .m1, m2,E1,E2,G1,G2 \n. m1 ~e m2 . E1 ~e E2 state. It should be precise enough to ensure that equivalent com\u00ad ' mands always \ntake the same time in equal environments, and no more precise. Including state that has no effect on \ntiming leads to .(c[er ,ew], m1,E1, G1).(c1,m1,E1' ,G ' 1) '' .(c[er ,ew], m2,E2, G2).(c2,m2,E2' ,G ' \n=. E ' E 2) 1 ~e 2 overly conservative security enforcement that hurts performance. For example, consider \na data cache, usually structured as a set Figure 3: Security requirements of cache lines. Each cache \nline contains a tag, a data block and a valid bit. Let us compare two possible ways to describe this \nas a Read labels. The read label er of a command speci.es which machine environment: a more precise modeling \nof all three .elds a parts of the machine environment may affect the time necessary to set of triples \n(tag,data block, valid bit) versus a coarser modeling perform the single next evaluation step. For a \ncompound command of only the tags and valid bits a set of pairs (tag,valid bit). such as if, while, or \nmitigate, this time does not include time The coarse-grained abstraction of data cache state is adequate \nspent in subcommands. to predict execution time, since for most cache implementations, Property 6 in \nFig. 3 formalizes the requirement that read labels the contents of data blocks do not affect access time. \nThe .ne\u00adaccurately capture the in.uences of the machine environment. This grained abstraction does not \nwork as well. For example, consider the formalization uses the vars1 function, which identi.es the part \nof command h := h occurring in a low context. That is, variables h memory that may affect the timing \nof the next evaluation step and h are con.dential, but the fact that the assignment is happening that \nis, a set of variables. We need vars1 because parts of the is not. With the .ne-grained abstraction, \nthe low part of the cache memory can also affect timing, such as e in sleep (e). A simple cannot record \nthe value of h if Property 7 is to hold, because the syntactic de.nition of vars1 conservatively approximates \nthe timing low-equivalent memories m1 and m2 appearing in its de.nition may in.uences of memory, but \na more precise de.nition might depend differ on the value of h . With the coarse-grained abstraction, \nthe on particularities of the hardware implementation. For skip, this location h can be stored in low \ncache, because Property 7 holds set is empty; for x := e and sleep (e), the set consists of x and all \nwithout making the value of h part of the machine environment. variables in expression e; for if e then \nc1 else c2, while e do c, The coarse-grained abstraction shows that high variables can re\u00adand mitigate \n(e,e) c, it contains only variables in e and excludes side in low cache without hurting security in at \nleast some circum\u00adthose in subcommands, since only e is evaluated during the next stances. This is quite \ndifferent from the treatment of memory, be\u00adstep. cause public memory cannot hold con.dential data. Without \nthe for- In the de.nition in Fig. 3, equality of G1 and G2 means that a malization of Property 7, it \nwould be dif.cult to reason about it. Yet single step takes exactly the same time. Both con.gurations \ntake the this insight is important for performance: otherwise, code with a low same time, because m1 \nand m2 must agree on all variables x that are timing label cannot access high variables using cache. \nevaluated in this step. This expresses our assumption that values of 4.2 Realization on standard hardware \nvariables other than those explicitly evaluated in a single step cannot At least some standard CPUs can \nsatisfy the security requirements in.uence its timing. Machine environments E1 and E2 are required (Properties \n5 7). Intel s family of Pentium and Xeon processors has to be er-equivalent, to ensure that parts of \nthe machine environment a no-.ll mode in which accesses are served directly from memory other than those \nat er and below also cannot in.uence its timing. on cache misses, with no evictions from nor .lling of \nthe data cache. Consider command sleep (h)[L,ew] with read-label er = L, with Our approach can be implemented \nby treating the whole cache respect to all possible pairs of memories m1,m2 and machine envi\u00adas low, \nand therefore disallowing cache writes from high contexts. ronments E1,E2. Whenever m1(h) and m2(h) have \ndifferent values, For each block of instructions with ew = H, the compiler inserts a Property 6 places \nno restrictions on the timing of this command re\u00ad no-.ll start instruction before, and a no-.ll exit \ninstruction after. gardless of E1,E2. When m1(h)= m2(h), we require that if E1 and It is easy to verify \nthat Properties 5 7 hold, as follows: E2 are L-equivalent, the resulting time must be the same. To sat\u00adisfy \nsuch a property, the H parts of the machine environment cannot Property 5. For commands with ew = L, \nthis property is vac\u00adaffect the evaluation time. uously true since there is no e such that L g e. Commands \nwith ew = H are executed in no-.ll mode, so the result is trivial. Single-step noninterference. Property \n5 speci.es which parts of the machine environment can be modi.ed. However, it does not say Property 6. \nSince there is only one (L) partition, E1 ~erE2 is anything more about the nature of the modi.cations. \nFor example, equivalent to E1 = E2. The property can be veri.ed for each com\u00adconsider a three-level security \nlattice L g M g H, and a command mand. For instance, consider command sleep (e)[er ,ew]. The condi\u00ad(x \n:= y)[M,M], where both the read label and write label are M. Prop-tion .x . vars1(c[er ,ew]).m1(x)= m2(x) \nensures that m1(e)= m2(e). erty 5 requires that no modi.cations to L parts of the environment Thus, this \ncommand is suspended for the same time. Moreover, are allowed, but modi.cations to the M level are not \nrestricted. This since E1 = E2, cache access time must be the same according to creates possibilities \nfor insecure modi.cations of machine environ-Property 2. So, we have G1 = G2. Property 7. We only need \nto check the L partition, which can be veri.ed for each command. For instance, consider command sleep \n(e)[er ,ew]. When ew = H, the result is true simply because the cache is not modi.ed. Otherwise, the \nsame addresses (variables) are accessed. Since initial cache states are equivalent, identical accesses \nyields equivalent cache states.  4.3 A more ef.cient realization A more ef.cient hardware design might \npartition both the cache(s) and the TLB according to security labels. Let us assume both the cache and \nTLB are equally, statically partitioned into two parts: L and H. The hardware accesses different parts \nas directed by a timing label that is provided from the software level. As discussed in Sec. 8, we have \nimplemented a simulation of this design; here we focus on the correctness of hardware design. One subtle \nissue is consistency, since data can be stored in both the L and the H partitions. We avoid inconsistency \nby keeping only one copy in the cache and TLB. In any CPU pipeline stage that accesses memory when the \ntiming label is H, both H and L partitions are searched. If there is a cache miss, data is installed \nin the H partition. When the timing label is L, only the L partition is searched. However, to preserve \nconsistency, instead of fetching the data from next level or memory, the controller moves the data from \nH partition if it already exists there. To satisfy Property 6, the hardware ensures this process takes \nthe same time as a cache miss. We can informally verify Properties 5 7 for this design as well: Property \n5. When the write label is L, this property holds trivially because there is no label such that L g e. \nWhen the write label is H, 'a new entry is installed only in the H partition, so E ~LE. Property 6. The \npremise of Property 6 ensures that all variables evaluated in a single step have identical values, so \nany variation in execution time is due to the machine environment. When the read label is H, E1 ~HE2 \nensures that the machine environments are identical; therefore, the access time is also identical. When \nthe read label is L, the access time depends only on the existence of the entry in L cache/TLB. Even \nif the data is in the H partition, the load time is the same as if there were an L-partition miss. Property \n7. This requirement requires noninterference for a sin\u00adgle step. Contents of the H partition can affect \nthe L part in the next step only when data is stored in the H partition and the access has a timing label \nL. Since data is installed into the L part regardless of the state of the H partition, this property \nis still satis.ed. Discussion on formal proof and multilevel security. We have discussed ef.cient hardware \nfor a two-level label system. Veri.ca\u00adtion of multilevel security hardware is more challenging. One real\u00adization \nexists in Caisson [21], which enforces a version of nonin\u00ad terference that is both memory and timing-sensitive. \nProperty 7 re\u00ad quires only timing-insensitive noninterference, so Caisson arguably tackles an unnecessarily \ndif.cult problem. A similar implementa\u00adtion that satis.es Property 7 more exactly might be more ef.cient. \n5. A type system for controlling timing channels Next, we present the security type system for our language. \nThis section focuses on the non-quantitative guarantees that the type sys\u00adtem provides, assuming Properties \n1 7 hold. We show that the type system isolates the places where timing needs to be controlled ex\u00adternally. \nThese places are where mitigate commands are needed. 5.1 Security type system Typing rules for expressions \nhave form G f e : e where G is the security environment (a map from variables to security labels), e \nis the expression, and e is the type of the expression. The rules are standard [27] and we omit them \nhere. Typing rules for commands, ' in Fig. 4, have the form G,pc,t f c : t. Here pc is the usual ' program-counter \nlabel [27], t is the timing start-label, and tis the timing end-label. The timing start-and end-labels \nbound the level of information that .ows into timing before and after executing c, '' respectively. We \nwrite G f c to denote G,.,.f c : tfor some t. ' All rules enforce the constraint t g tbecause timing \ndepen\u00addencies accumulate as the program executes. Every rule except (T-MTG) also propagates the timing \nend-labels of subcommands. This can be seen most clearly in the rule for sequential composition (T-SEQ): \nthe end-label from c1 is the start-label for c2. All remaining rules require pc g ew. This restriction, \ntogether with Property 5, ensures that no con.dential information about control .ow leaks to the low \nparts of the machine environment. We do not require t g ew because we assume the adversary cannot directly \nobserve the timing of updates to the machine environment. This assumption is reasonable since the ISA \ngives no way to check whether a given location is in cache. Rule (T-SKIP) takes the read label er into \naccount in its timing end-label. The intuition is that reading from con.dential parts of the machine \nenvironment should be re.ected in the timing end-label. Rule (T-ASGN) for assignments x := e requires \ne U pc U t U er g G(x), where e is the level of the expression. The condition e U pc g G(x) is standard. \nWe also require t U er g G(x), to prevent information from leaking via the timing of the update, from \neither the current time or the machine environment. The timing end-label is set to G(x), bounding all \nsources of timing leaks. Notice that the write label ew is independent of the label on x. The reason \nis that ew is the interface for software to tell hardware which state may be modi.ed. A low write label \non an assignment to a high variable permits the variable to be stored in low cache. Because sleep has \nno memory side effects, rule (T-SLEEP) is slightly simpler than that for assignments; the timing end-label \nconservatively includes all sources of timing information leaks. Rule (T-IF) restricts the environment \nin which branches c1 and c2 are type-checked. As is standard, the program-counter label is raised to \ne U pc. The timing start-labels are also restricted to re.ect the effect of reading from the er-parts \nof the machine environment and of the branching expression. Rule (T-WHILE) imposes similar ' conditions \non end-label t', except that tcan also be used as both start-and end-labels for type-checking the loop \nbody. ' The most interesting rule is (T-MTG). The end-label tfrom ' command c is bounded by mitigation \nlabel e ', but tdoes not propagate to the end-label of the mitigate. Instead, the end-label of the mitigate \ncommand only accounts for the timing of evaluating expression e. This is because the predictive mitigation \nmechanism used at run time controls how c s timing leaks information. We have seen that for security, \nthe write label of a command must be higher than the label of the program counter. There is no corresponding \nrestriction on the read label of a command. The hardware may be able to provide better performance if \na higher read label is chosen. For instance, in most cache designs, reading from the cache changes its \nstate. The cache can only be used when er = ew, so this condition should be satis.ed for best performance. \n 5.2 Machine-environment noninterference An important property of the type system is that it guarantees \nma\u00adchine environment noninterference. This requires execution to pre\u00adserve low-equivalence of memory \nand machine environments. THEOREM 1 (Memory and machine-environment noninterference). .E1, E2,m1, m2, \nG, c,e . G f c . m1 ~e m2 . E1 ~e E2 ' .(c,m1,E1,G). * (stop,m 1,E1' ,G1)' .(c,m2,E2,G). * (stop,m2,E2' \n,G2)'' ' =. m1 ~e m2 . E1 '~e E  pc g ew G f e : e pc g ew e U pc U t U er g G(x) G f e : e pc g ew \nT-SKIP T-ASGN T-SLEEP G,pc, t f skip[er ,ew] : t U er G,pc,t f x := e[er ,ew ] : G(x) G,pc,t f (sleep \n(e))[er ,ew] : t U e U er '' G f e : e pc g ew G,e U pc,e U t U er f ci : tii = 1,2 G f e : e pc g ew \ne U t U er g tG,e U pc,t'f c : t T-WHILE ' G,pc,t f (if e then c1 else c2)[er ,ew] : t1 U t2 T-IF G, \npc,t f (while e do c)[er ,ew ] : t '' ' G,pc,t f c1: t1 G,pc,t1 f c2: t2 G f e : e pc g ew G,pc,t U \ne U er f c : ttg e T-SEQ T-MTG G, pc,t f c1;c2: t2 G,pc,t f (mitigate (e,e ' ) c)[er ,ew] : e U t U er \nFigure 4: Typing rules: commands Theorem 1 guarantees the adversary obtains no information by ob\u00ad serving \npublic parts of the memory and machine environments. Any con.dential information the adversary obtains \nmust be via timing. The proof is provided in the corresponding technical report [39]. Theorem 1 does \nnot guarantee that information is not leaked through timing, that is, by observation of G. However, such \na guar\u00adantee holds if the program contains no mitigate commands. This stronger guarantee is not proved \nhere because it is a corollary of more general results presented in the next section. A note on termination. \nThe de.nition of memory and ma\u00adchine noninterference in Theorem 1 is presented in the batch-style termination-insensitive \nform [4]. Such de.nitions are simple but or\u00ad dinarily limit one s results to programs that eventually \nterminate. Because termination channels are a special case of timing channels, using a batch-style de.nition \nis not fundamentally limiting here. 6. Quantitative properties of the type system The type system identi.es \npotential timing channels in the program. We now introduce a quantitative measure of leakage for multilevel \nsystems, and show that the type system quantitatively bounds leak\u00adage through both timing and storage \nchannels. The main result of this section is that information leakage can be bounded in the terms of \nthe variation in the execution time of mitigate commands alone. 6.1 Adversary observations As discussed \nearlier in Sec. 3.4, an adversary at level eA observes memory, including timing of updates to memory, \nat levels up to eA. The adversary does not directly observe the time of termination of the program, but \nthis is easily simulated by adding a .nal low assign\u00adment to the program. To formally de.ne adversary \nobservations, we re.ne our presentation of the language semantics with observable assignment events. \nObservable assignment events. Let a .{(x,v,t),e} range over observable events, which can be either an \nassignment to variable x of value v at time t, or an empty event e. An event (x,v,G' ) is gener\u00ad ' ated \nby assignment transitions (x := e,m,E, G).(stop,m ,E' ,G'), where (m,e). v, and by all transitions whose \nderivation includes a subderivation of such a transition. We write (c,m,E,G) = (x, v,t) if con.guration \n(c, m, E, G) pro\u00adduces a sequence of events (x,v,t)=(x1, v1,t1)...(xn,vn,tn) and ' '' reaches a .nal \ncon.guration (stop,m ,E' ,G') for some m ,E,G' . eA-observable events. An event (x,v,t) is observable \nto the adver\u00adsary at level eA when G(x) g eA. Given a con.guration (c,m,E,G)such that (c,m,E,G) = (x,v,t), \nwe write (c, m, E, G) =eA (x ' , v ' ,t ' ) for the longest subsequence of (x,v,t) such that for all \nevents (xi,vi,ti) in (x ' ,v ' , t ' ) it holds that G(xi) g eA. For example, for program l1:= l2;h1:= \nl1, the H-adversary observes two assignments: (c,m,E,G) =H (l1,v1,t1), (h1,v2,t2) for some v1,t1, v2 \nand t2. For the L-adversary, we have (c,m,E,G) =L (l1,v1,t1), which does not include the assignment to \nh1. 6.2 Measuring leakage in a multilevel environment Using eA-observable events, we can de.ne a novel \ninformation\u00adtheoretic measure of leakage: leakage from a set of security levels L to an adversary level \neA. We start with an observation on our adversary model and the corresponding auxiliary de.nition. Because \nan adversary observes all levels up to eA, we can exclude these security levels from the ones that give \nnew information. Let LeA be the subset of L that excludes all levels observable to eA, '' that is LeA \n{e | e '. L . e g eA}. For example, for a three-level lattice L g M g H, with eA = M, if L = {M,H} then \nLeA = {H}. Fig. 5a illustrates a general form of this de.nition. The adversary level eA is represented \nby the white point; the levels observable to the adversary correspond to the small rectangular area under \nthe point eA. The set of security levels L is represented by the dashed rectangle (though in general \nthis set does not have to be contiguous). The gray area corresponds to the security levels that are in \nLeA . Leakage from L to eA. We measure the quantitative leakage as the logarithm (base 2) of the number \nof distinguishable observations of the adversary the possible (x,v,t) sequences from indistin\u00adguishable \nmemory and machine environments. As shown in [38], this measure bounds those of Shannon entropy and min-entropy, \nused in the literature [11, 22, 31]. DEFINITION 1 (Quantitative leakage from L to eA ). Given any eA, \nm, and E, the leakage of program c from levels L to level eA, denoted by Q(L ,eA, c,m,E) is de.ned as \nfollows '' ' Q(L ,eA,c,m,E) log(|{(x,v, t) |.m , E . (.e .e ' . LeA . '' m e ' m '. E e ' E ' ) .(c, \nm , E ,0) =eA (x,v,t)}|) This de.nition uses LeA to restrict the quanti.cation of the memory and machine \nenvironments so that we allow variations only in LeA parts of memory and machine environments. This is \nexpressed by requiring projected equivalence (on the second line of the de.nition) ' for all levels e \nnot in LeA . Visually, using Fig. 5a, this captures the .ows from the gray area to the lower rectangle. \nNote that the de.nition distinguishes .ows between different levels. For example, in a three-level security \nlattice L g M g H and a program sleep (h) where h has level H, the leakage from {M} to L is zero even \nthough .ow from {H} to L is not.  6.3 Guarantees of the type system The type system provides an important \nproperty: leakage from L to eA is bounded by the timing variation of the mitigate commands ' whose mitigation \nlevel e is in the upward closure of LeA . Upward closure. In order to correctly approximate leakage from \nlevels in LeA , we need to account for all levels that are as restrictive as the ones in LeA . For example, \nin a three-level lattice L g M g H, let L be the set {M}, and let eA = L; then LeA = {M}. Informa\u00adtion \nfrom M can .ow to H, so in order to account conservatively for leakage from {M}, we must also account \nfor leakage from H. Our de.nitions therefore use the upward closure of LeA , written as  (a) Leakage \nfrom L to eA (b) Variations with L g eA Figure 5: Quantitative leakage ' LeA.{e |.e . LeA . e g e '}. \nIn this example, LeA. = {M,H}. Fig. 5b illustrates the relationship between LeA and its upper clo\u00adsure, \nwhere LeA. includes both shades of gray. Trace and projection of mitigate commands. Next, we focus on \nthe amount of time a mitigate command takes to execute. Recall from Sec. 3 that each mitigate in a program \nsource has an .\u00adidenti.er. For brevity, we refer to the command mitigate. as M. . ' Consider trace (c,m,E,G) \n.* (stop,m ,E' ,G'). We overload the notation for =, by writing (c,m,E,G) = (M,t), where (M,t) is a vector \nof mitigate commands executed in the above trace. The vector consists of the individual tuples (M,t)=(M.1 \n,t1)...(M.n ,tn) where (M.i ,ti) are ordered by the time of completion, and each (M.i ,ti) corresponds \nto a mitigate.i taking time ti to execute. Further, de.ne the projection of mitigate commands (M,t)if \nas the longest subsequence of (M, t) such that each (M. ,t) in the subsequence satis.es predicate f. \nLow-determinism of mitigate commands. Consider the fol\u00adlowing well-typed program that uses mitigate twice. \n1 mi t iga te1 (1 , H ) { 2 if ( high ) 3 then mi t iga te2 (1 , H ) { h := h +1 } 4 else skip ; } Let \nus write pc(M. ) for the value of the pc-label at program point .. It is easy to see that pc(M1)= L, \nand pc(M2)= H. Because M2 is nested within M1, the timing of M2 is accumulated in the timing of M1. Therefore, \nwhen reasoning about the timing of the whole program, it is suf.cient to only reason about the timing \nof M1. In general, given a set of levels L , an adversary level eA, and a vector (M,t), we .lter high \nmitigate commands by the projection (M, t) i pc(M. ).LeA .. This projection consists of all the mitigate \ncommands whose pc-label is in the white area in Fig. 5b. Filtering out high mitigate commands rules out \nunrelated vari\u00adations in the mitigate commands. It turns out that in well-typed programs, the occurrence \nof the remaining low mitigate com\u00admands is deterministic (we call these commands low-deterministic). \nThis result, formalized in the following lemma, is used in the deriva\u00adtion of leakage bounds in Sec. \n7. LEMMA 1. (Low-determinism of mitigate commands). For all programs c such that G f c, adversary levels \neA, sets of security levels L , and memories and environments E1,E2,m1,m2 such that (.e ' . LeA. . E1 \ne ' E2 . m1 e ' m2), we have (c,m1,E1,0) = (M1,t1) .(c,m2,E2,0) = (M2,t2) . M1 ipc(M. ).LeA. = M2 ipc(M. \n).LeA. = Note that there are no constraints on time components t1 and t2. That is, the same mitigate \ncommands may take different times to execute in different traces. The proof is contained in the correspond\u00ading \ntechnical report [39]. Mitigation levels. Per Sec. 3, the argument e in mitigate. (e,e) c is an upper \nbound on the timing leakage of command c. Let lev(M. ) be the label argument of mitigate. command. We \ncall this the mit\u00adigation level of M. . Note that lev(M. ) is unrelated to pc(M. ). For instance, in \nthe example above, pc(M1)= L, because M1 appears in the L-context, but lev(M1)= H. Mitigation levels \nare connected to how much information an adversary at level eA may learn. For example, information at \nlevel e can leak to adversary at level eA (e g eA ) by a command M. only when e g lev(M. ). In general, \ninformation from a set of levels L can be leaked by mitigate commands such that lev(M. ) . LeA.. This \nleads to the de.nition of timing variations. DEFINITION 2 (Timing variations of mitigate commands). Given \na set of security levels L , an adversary level eA, program c, mem\u00adory m, and a machine environment E, \nlet V be the timing variations of mitigate commands: ' V(L ,eA,c,m,E) {t '|. m ,E ' . ' (.e ' . LeA. \n. m ' m '. E ' E ' ) .(c,m ,E ' ,0) = (M,t) ee ..lev(M. ).LeA . (M' ,t ' )=(M,t)ipc(M. ).LeA.} An interesting \ncomponent of this de.nition is the predicate used to project (M,t). In essence, we only focus on the \nmitigate com\u00admands that appear in low contexts and have high mitigation levels, such as the .rst mitigate \nin the example earlier. Also notice that this set counts only the distinct timing components of the mitigate \ncommand projection, ignoring the M' component. This is suf.cient because for well-typed programs the \nM' components of the vectors (M' ,t ' ) are low-deterministic by Lemma 1. In this de.nition, memory and \nmachine environments are quan\u00adti.ed differently from De.nition 1, by considering variations with respect \nto a larger set of security levels LeA.. In Fig. 5b, this corre\u00ad sponds to .ows from both gray areas \nto the area observable by the adversary. Leakage bounds guaranteed by the type system. The type sys\u00adtem \nensures that only the execution time of mitigate commands within certain projections may leak information. \nTHEOREM 2 (Bound on leakage via variations). Given a command c, such that G f c, and an adversary level \neA, we have that for all m, E and L it holds that Q(L ,eA,c, m, E) = log |V(L ,eA,c,m,E)| The proof is \nincluded in the corresponding technical report [39]. An interesting corollary of the theorem is that \nleakage is zero whenever a program c contains no mitigate command, or more generally, when all mitigate \ncommands take .xed time since there is only one timing variation of mitigate commands in this special \ncase. 7. Predictive mitigation Predictive mitigation [5, 38] removes con.dential information from timing \nof public events by delaying them according to prede.ned schedules. We build upon this prior work [5, \n38], but unlike the earlier work, our results improve precision for multilevel security, enabling better \ntradeoffs between security and performance. Instead of delaying public assignments themselves, we delay \nthe completions of mitigate commands that may potentially precede public events. This is suf.cient for \nwell-typed programs, because according to Theorem 2, only timing variations of mitigate com\u00admands carry \nsensitive information. The idea is that as long as the ex\u00adecution time of the mitigate command is no \ngreater than predicted, little information is leaked. Upon a misprediction (when actual ex\u00adecution time \nis longer than predicted), a new schedule is chosen in such a way that future mispredictions are rarer. \n (update(n,e),m,E,G).((while (time - s. = predict(n,e)) do (Miss[e] := Miss[e]+ 1;)[.,.])[.,.], m, E,G) \n(S-UPDATE) (mitigate. (n,e) c,m,E,G).(s. := time[.,.]; c;update(n,e);(sleep (predict(n,e) - time + s. \n))[.,.],m,E,G) (S-MTGPRED) Figure 6: Predictive semantics for mitigate Name # of sets issue block size \nlatency L1 Data Cache 128 4-way 32 byte 1 cycle L2 Data Cache 1024 4-way 64 byte 6 cycles L1 Inst. Cache \n512 1-way 32 byte 1 cycle L2 Inst. Cache 1024 4-way 64 byte 6 cycles Data TLB 16 4-way 4KB 30 cycles \nInstruction TLB 32 4-way 4KB 30 cycles Table 1: Machine environment parameters Mitigating semantics. \nFig. 6 shows the fragment of small-step semantics that implements predictive mitigation. We record mis\u00adpredictions \nin a special array Miss, assuming Miss is initialized to zeros and is otherwise unreferenced in programs. \nExpression time provides the current value of the global clock. Expression predict(n,e)= max(n,1) \u00b7 2Miss[e] \nreturns the current prediction for level e with initial estimate n. This prediction is the fast dou\u00adbling \nscheme [5] with the local penalty policy [38] ; other schemes and penalty policies are possible [5, 38], \nbut are not considered here. In rule (S-MTGPRED), mitigate transitions to a code frag\u00adment that penalizes \nand delays the execution time of c. Variable s. records the time when mitigation has started. If execution \nof c takes less time (time - s. ) than predicted, command update does noth\u00ading; the execution idles until \nthe predicted time. If executing c takes longer than predicted, update increments Miss[e] until the new \nprediction is greater than the time that c has consumed. Leakage analysis of the mitigating semantics. \nNote that all aux\u00adiliary commands in Fig. 6 have labels [., .], ensuring no con.den\u00adtial information \nabout machine environments is leaked when exe\u00adcuting these commands. Moreover, the execution time of \nthe whole mitigated block is at least predict(n,e). Thus, the timing varia\u00adtion of a single mitigate \ncommand is controlled by the variation of possible values of predict(n,e). We can show that leakage is \nat most |LeA.|\u00b7log(K +1)\u00b7(1 + log T ). Here T is the elapsed time and K is the number of relevant mitigate \nstatements in the trace: the ones satisfying pc(M. ) . LeA.. lev(M. ) . LeA.. When mitigate is not used, \nK = 0, so no timing leakage occurs. When K is unknown, it can be conservatively bounded by T , yielding \nan O(log2 T ) leakage bound. Note that the bound is proportional to the size of the set LeA.. A detailed \nanalysis and derivation can be found in the technical report [39]. 8. Implementation We implemented a \nsimulation of the partitioned cache design de\u00adscribed in Sec. 4.3 so we could evaluate our approach on \nreal C programs. As case studies, we chose two applications previously shown to be vulnerable to timing \nattacks. The results suggest the new mechanism is sound and has reasonable performance. 8.1 Hardware \nimplementation We developed a detailed, dynamically scheduled processor model supporting two-level data \nand instruction caches, data and instruc\u00adtion TLBs, and speculative execution. Table 1 summarizes the \nfea\u00ad tures of the machine environment. We implemented this processor design by modifying the SimpleScalar \nsimulator, v.3.0e [9]. A new register is added as an interface to communicate the timing label from the \nsoftware to the hardware. Simply encoding the timing labels into instructions does not work, since labels \nmay be required before the instruction is fetched and decoded: for example, to guide instruction cache \nbehavior. Labels are also propagated along the pipeline to restrict the behavior of hardware. As discussed \nin Sec. 5.1, commodity cache designs require er = ew. In our implementation, we treat this requirement \nas an extra side condition in the type system. 8.2 Compilation We use the gcc compiler in the SimpleScalar \ntool set to run C applications on the simulator. Sensitive data in applications are labeled, and timing \nlabels are then inferred as the least restrictive labels satisfying the typing rules from Fig. 4 (transferring \nthe rules from Sec. 5 to C is straightforward). To inform the hardware of the current timing label, assembly \ncode setting the timing-label register is inserted before and after command blocks. Selecting the initial \nprediction. With the doubling policy, the slowdown of mitigation is at most twice the worst-case time. \nTo improve performance, we can sample the running time of mitigated commands, setting the initial prediction \nto be a little higher than the average. In the experiments, we used 110% of average running time, measured \nwith randomly generated secrets, as the initial prediction.  8.3 Web login case study Web applications \nhave been shown vulnerable to timing channel at\u00adtacks. For example, Bortz and Boneh [7] have shown that \nadver\u00ad saries can probe for valid usernames using a timing channel in the login process. This is unfortunate \nsince usernames can be abused for spam, advertising, and phishing. The pseudo-code for a sim\u00ad ple web-application \nlogin pro\u00adcedure is on the right. The vari\u00adable response and user in\u00adputs user, pass are public to users. \nContents of the preloaded hashmap m (MD5 digests of valid usernames and corre\u00adsponding passwords), password \ndigest hash and the login status state are secrets. The .nal as\u00ad signment to public variable response \nis always 1 on purpose in order to avoid the storage channel arising from the response. How\u00adever, the \ntiming of this assignment might create a timing channel. The leakage is explicit when all con.dential \ndata (m, hash and state) are labeled H. The type system forces line 1 and line 5 10 to have high timing \nlabels, so without a mitigate command, type checking fails at line 11. We secure this code by separately \nmitigating both line 1 and lines 5 10. The code then type-checks. Correctness. In each of our experiments, \nwe measured the time needed to perform a login attempt using 100 different usernames. Since valid usernames \n(the hashmap m) are secrets in this case study, we varied the number of these usernames that were valid \namong 10, 50, and 100. The resulting measurements are shown as three curves in the upper part of Fig. \n7. The horizontal axis shows which login attempt was measured and the vertical axis is time. The data \nfor 10 and 50 valid usernames show that an adversary can easily distinguish invalid and valid usernames \nusing login time. There is also measurable variation in timing even among different valid usernames. \nIt is not clear what a clever adversary could learn from this, but since passwords are used in the computation, \nit seems likely that something about them is leaked too.  Figure 7: Login time with various secrets \nnopar moff mon ave. time (valid) 70618 78610 86132 ave. time (invalid) 39593 43756 86147 overhead (valid) \n1 1.11 1.22 Table 2: Login time with various usernames and options (in clock cycles) The lower part \nof the .gure shows the timing of the same ex\u00adperiments with timing channel mitigation in use. With mitigation \nenabled, execution time does not depend on secrets, and therefore all three curves coincide. This result \nvalidates the soundness of our approach. The roughly 30-cycle timing difference between different requests \ndoes not represent a security vulnerability because it is un\u00adaffected by secrets; it is in.uenced only \nby public information such as the position in the request sequence. Performance. Table 2 shows the execution \ntime of the main loop with various options, including both valid and invalid usernames, hardware with \nno partitions (nopar), and secure hardware both with\u00adout (moff) and with (mon) mitigation. As in Fig. \n7, for unmitigated logins, valid and invalid usernames can be easily distinguished, but mitigation prevents \nthis (we also ver\u00adi.ed that the tiny difference is unaffected by secrets). Table 2 shows that partitioned \nhardware is slower by about 11%. On valid user\u00adnames, language-based mitigation adds 10% slowdown; slowdown \nwith combined software/hardware mitigation is about 22%.  8.4 RSA case study The timing of ef.cient \nRSA implementations depends on the pri\u00advate key, creating a vulnerability to timing attacks [8, 18]. \nUsing the RSA reference implementation, we demonstrate that its timing channels can be mitigated when \ndecrypting a multi-block message. In the pseudo-code on the left, 1 text:=readText() only line 4 uses \ncon.dential 2 for each block b in text 3 ...preprocess ... data. Therefore, source code corresponding \nto this line is la\u00adbeled as high. Both prepro\u00adcess and postprocess include low assignments whose timing \nis observable to the adversary. Correctness. We use 100 encrypted messages and two different private \nkeys to measure whether secrets affect timing. The upper plot in Fig. 8 shows that different private \nkeys have different decryption times, so decryption time does leak information about the private key. \nThe lower plot shows that mitigated time is exactly 32,001,922 cycles regardless of the private key. \nTiming channel leakage is successfully mitigated. Figure 9: Language-level vs. system-level mitigation \nPerformance. To evaluate how mitigation affects decryption time, we use 10 encrypted secret messages \nwhose size ranges from 1 to 10 blocks; the size is treated as public. We also compared the per\u00adformance \nof language-level mitigation with system-level predictive mitigation [5], even though system-level mitigation \nis not effec\u00ad tive against the strong, coresident attacker. To simulate system-level mitigation, the \nentire code body was wrapped in a single mitigate command. The results in Fig. 9 show that .ne-grained \nlanguage\u00ad based mitigation is faster because it does not try to mitigate the timing variation due to \nthe number of decrypted blocks. 9. Related work Control of internal timing channels has been studied \nfrom different perspectives, and several papers have explored a language-based approach. Low observational \ndeterminism [16, 37] can control these channels by eliminating dangerous races. External timing channels \nare harder to control. Much prior language-based work on external timing channels uses simple, implicit \nmodels of timing, and no previous work fully addresses indirect dependencies. Type systems have been \nproposed to pre\u00advent timing channels [33], but are very restrictive. Often (e.g., [28, 30, 32, 33]) timing \nbehavior of the program is assumed to be accurately described by the number of steps taken in an operational \nsemantics. This assumption does not hold even at the machine\u00adlanguage level, unless we fully model the \nhardware implementation in the operational semantics and verify the entire software and hard\u00adware stack \ntogether. Our approach adds a layer of abstraction so software and hardware can be designed and veri.ed \nindependently. Some previous work uses program transformation to remove in\u00addirect dependencies, though \nonly those arising from data cache. The main idea is to equalize the execution time of different branches, \nbut a price is paid in expressiveness, since these languages either rule out loops with con.dential guards \n(as in [3, 6, 15]), or limit the number of loop iterations [10, 23]. These methods do not handle all \nindirect timing dependencies; for example, the instruction cache is not handled, so veri.ed programs \nremain vulnerable to other indi\u00adrect timing attacks [1, 2, 34].  Secure multi-execution [12, 17] provides \ntiming-sensitive nonin\u00ad terference yet is probably less restrictive than the prior approaches discussed \nabove. The security guarantee is weaker than in our ap\u00adproach: that the number of instructions executed, \nrather than the time, leaks no information for incomparable levels. Extra computa\u00adtional work is also \nrequired per security level, hurting performance, and no quantitative bound on leakage is obtained. Though \nsecurity cannot be enforced purely at the hardware level, hardware techniques have been proposed to mitigate \ntiming chan\u00adnels. Targeting cache-based timing attacks, both static [25] and dy\u00ad namic [35] mechanisms, \nbased on the idea of partitioned cache, have been proposed. Such designs are ad hoc and hard to verify \nagainst other attacks. For example, Kong et al. [19] show vulnerabilities in Wang s cache design [35]. \nRecent work by Li et al. [21] intro\u00ad duces a statically veri.able hardware description language for build\u00ading \nhardware that is information-.ow secure by construction. This work could complement our own. 10. Conclusions \nTiming channels have long been considered one of the toughest challenges in computer security. They have \nbecome more of a con\u00adcern as different computing systems are more tightly intermeshed and code from different \ntrust domains is executed on the same hard\u00adware (e.g., cloud computing servers and web browsers). Solving \nthe timing channel problem requires work at both the hardware level and the software level. Neither level \nhas enough in\u00adformation to allow accurate reasoning about timing channels, be\u00adcause timing is a property \nthat crosses abstraction boundaries. The new abstraction of read and write labels makes a useful step \ntoward allowing timing channels to be controlled effectively at the language level. The corresponding \nsecurity properties help guide the design of hardware secure against timing attacks. For programs where \ntiming channels cannot be blocked entirely, predictive mitigation can be incorporated at the language \nlevel. The security guarantees of this language-level enforcement have been proved formally; the performance \ncharacteristics of the enforcement mechanism have been studied experimentally and appear promising. Acknowledgments \nWe thank Owen Arden, Dan Ports, and Nate Foster for their helpful suggestions. This work has been supported \nby a grant from the Of.ce of Naval Research (ONR N000140910652), by two grants from the NSF: 0424422 \n(the TRUST center), and 0964409, and by MURI grant FA9550-12-1-0400, administered by the US Air Force. \nThis research is also sponsored by the Air Force Research Laboratory. References [1] O. Aciic\u00b8mez. Yet \nanother microarchitectural attack: Exploiting I-cache. In Pro\u00ad ceedings of the ACM Workshop on Computer \nSecurity Architecture (CSAW 07), pages 11 18, 2007. [2] O. Aciic\u00b8mez, C. Koc\u00b8, and J. Seifert. On the \npower of simple branch prediction analysis. In ASIACCS, pages 312 320, 2007. [3] J. Agat. Transforming \nout timing leaks. In Proc. 27th ACM Symp. on Principles of Programming Languages (POPL), pages 40 53, \nBoston, MA, January 2000. [4] A. Askarov, S. Hunt, A. Sabelfeld, and D. Sands. Termination-insensitive \nnonin\u00ad terference leaks more than just a bit. In ESORICS, pages 333 348, October 2008. [5] A. Askarov, \nD. Zhang, and A. C. Myers. Predictive black-box mitigation of timing channels. In ACM Conf. on Computer \nand Communications Security (CCS), pages 297 307, October 2010. [6] G. Barthe, T. Rezk, and M. Warnier. \nPreventing timing leaks through transac\u00ad tional branching instructions. Electronic Notes in Theoretical \nComputer Science, 153(2):33 55, 2006. [7] A. Bortz and D. Boneh. Exposing private information by timing \nweb applications. In Proc. 16th Int l World-Wide Web Conf., May 2007. [8] D. Brumley and D. Boneh. Remote \ntiming attacks are practical. Computer Networks, January 2005. [9] D. C. Burger and T. M. Austin. The \nSimpleScalar tool set, version 3.0. Technical Report CS-TR-97-1342, University of Wisconsin, Madison, \nJune 1997. [10] B. Coppens, I. Verbauwhede, K. D. Bosschere, and B. D. Sutter. Practical mit\u00ad igations \nfor timing-based side-channel attacks on modern x86 processors. IEEE Symposium on Security and Privacy, \npages 45 60, 2009. [11] D. E. Denning. Cryptography and Data Security. Addison-Wesley, Reading, Massachusetts, \n1982. [12] D. Devriese and F. Piessens. Noninterference through secure multi-execution. In IEEE Symposium \non Security and Privacy, pages 109 124, May 2010. [13] J. Gif.n, R. Greenstadt, P. Litwack, and R. Tibbetts. \nCovert messaging through TCP timestamps. Privacy Enhancing Technologies, Lecture Notes in Computer Science, \n2482(2003):189 193, 2003. [14] D. Gullasch, E. Bangerter, and S. Krenn. Cache games bringing access-based \ncache attacks on AES to practice. In IEEE Symposium on Security and Privacy, pages 490 505, 2011. [15] \nD. Hedin and D. Sands. Timing aware information .ow security for a JavaCard\u00adlike bytecode. Electronic \nNotes in Theoretical Computer Science, 141(1):163 182, 2005. [16] M. Huisman, P. Worah, and K. Sunesen. \nA temporal logic characterisation of observational determinism. In Proc. 19th IEEE Computer Security \nFoundations Workshop, 2006. [17] V. Kashyap, B. Wiedermann, and B. Hardekopf. Timing-and termination-sensitive \nsecure information .ow: Exploring a new approach. In IEEE Symposium on Security and Privacy, pages 413 \n430, May 2011. [18] P. Kocher. Timing attacks on implementations of Dif.e Hellman, RSA, DSS, and other \nsystems. In Advances in Cryptology CRYPTO 96, August 1996. [19] J. Kong, O. Aciic\u00b8mez, J.-P. Seifert, \nand H. Zhou. Deconstructing new cache designs for thwarting software cache-based side channel attacks. \nIn Proceedings of the 2nd ACM Workshop on Computer Security Architectures, pages 25 34, 2008. [20] B. \nK\u00a8opf and M. D\u00a8urmuth. A provably secure and ef.cient countermeasure against timing attacks. In 2009 \nIEEE Computer Security Foundations, July 2009. [21] X. Li, M. Tiwari, J. Oberg, V. Kashyap, F. Chong, \nT. Sherwood, and B. Hardekopf. Caisson: a hardware description language for secure information .ow. In \nACM SIGPLAN Conference on Programming Language Design and Implementation, pages 109 120, 2011. [22] J. \nK. Millen. Covert channel capacity. In Proc. IEEE Symposium on Security and Privacy, Oakland, CA, April \n1987. [23] D. Molnar, M. Piotrowski, D. Schultz, and D. Wagner. The program counter security model: automatic \ndetection and removal of control-.ow side channel attacks. Cryptology ePrint archive: report 2005/368, \n2005. [24] D. Osvik, A. Shamir, and E. Tromer. Cache attacks and countermeasures: the case of AES. Topics \nin Cryptology CT-RSA 2006, January 2006. [25] D. Page. Partitioned cache architecture as a side-channel \ndefense mechanism. In Cryptology ePrint Archive, Report 2005/280, 2005. [26] A. Russo and A. Sabelfeld. \nSecuring interaction between threads and the scheduler. In Proc. 19th IEEE Computer Security Foundations \nWorkshop, 2006. [27] A. Sabelfeld and A. C. Myers. Language-based information-.ow security. IEEE Journal \non Selected Areas in Communications, 21(1):5 19, January 2003. [28] A. Sabelfeld and D. Sands. Probabilistic \nnoninterference for multi-threaded pro\u00ad grams. In Proc. 13th IEEE Computer Security Foundations Workshop, \npages 200 214. IEEE Computer Society Press, July 2000. [29] S. Sellke, C. Wang, and S. Bagchi. TCP/IP \ntiming channels: Theory to implemen\u00ad tation. In Proc. INFOCOM 2009, pages 2204 2212, January 2009. [30] \nG. Smith. A new type system for secure information .ow. In Proc. 14th IEEE Computer Security Foundations \nWorkshop, pages 115 125, June 2001. [31] G. Smith. On the foundations of quantitative information .ow. \nFoundations of Software Science and Computational Structures, 5504:288 302, 2009. [32] G. Smith and D. \nVolpano. Secure information .ow in a multi-threaded imperative language. In Proc. 25th ACM Symp. on Principles \nof Programming Languages (POPL), pages 355 364, January 1998. [33] D. Volpano and G. Smith. Eliminating \ncovert .ows with minimum typings. In Proc. 10th IEEE Computer Security Foundations Workshop, pages 156 \n168, 1997. [34] Z. Wang and R. Lee. Covert and side channels due to processor architecture. In ACSAC \n06, pages 473 482, 2006. [35] Z. Wang and R. Lee. New cache designs for thwarting software cache-based \nside channel attacks. In Proceedings of the 34th annual international symposium on computer architecture \n(ISCA 07), pages 494 505, 2007. [36] J. C. Wray. An analysis of covert timing channels. In Proc. IEEE \nSymposium on Security and Privacy, pages 2 7, 1991. [37] S. Zdancewic and A. C. Myers. Observational \ndeterminism for concurrent program security. In Proc. 16th IEEE Computer Security Foundations Workshop, \npages 29 43, June 2003. [38] D. Zhang, A. Askarov, and A. C. Myers. Predictive mitigation of timing channels \nin interactive systems. In ACM Conf. on Computer and Communications Security (CCS), pages 563 574, October \n2011. [39] D. Zhang, A. Askarov, and A. C. Myers. Language mechanisms for controlling and mitigating \ntiming channels. Technical report, Cornell University, March 2012. http://hdl.handle.net/1813/28635. \n    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>We propose a new language-based approach to mitigating timing channels. In this language, well-typed programs provably leak only a bounded amount of information over time through external timing channels. By incorporating mechanisms for predictive mitigation of timing channels, this approach also permits a more expressive programming model. Timing channels arising from interaction with underlying hardware features such as instruction caches are controlled. Assumptions about the underlying hardware are explicitly formalized, supporting the design of hardware that efficiently controls timing channels. One such hardware design is modeled and used to show that timing channels can be controlled in some simple programs of real-world significance.</p>", "authors": [{"name": "Danfeng Zhang", "author_profile_id": "81470651200", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P3471163", "email_address": "zhangdf@cs.cornell.edu", "orcid_id": ""}, {"name": "Aslan Askarov", "author_profile_id": "81331488167", "affiliation": "Harvard University, Cambridge, MA, USA", "person_id": "P3471164", "email_address": "aslan@seas.harvard.edu", "orcid_id": ""}, {"name": "Andrew C. Myers", "author_profile_id": "81100011022", "affiliation": "Cornell University, Ithaca, NY, USA", "person_id": "P3471165", "email_address": "andru@cs.cornell.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254078", "year": "2012", "article_id": "2254078", "conference": "PLDI", "title": "Language-based control and mitigation of timing channels", "url": "http://dl.acm.org/citation.cfm?id=2254078"}