{"article_publication_date": "06-11-2012", "fulltext": "\n Understanding and Detecting Real-World Performance Bugs Guoliang Jin Linhai Song Xiaoming Shi Joel \nScherpelz * Shan Lu University of Wisconsin Madison {aliang, songlh, xiaoming, shanlu}@cs.wisc.edu joel.scherpelz@gmail.com \nAbstract Developers frequently use inef.cient code sequences that could be .xed by simple patches. These \ninef.cient code sequences can cause signi.cant performance degradation and resource waste, referred to \nas performance bugs. Meager increases in single threaded per\u00adformance in the multi-core era and increasing \nemphasis on energy ef.ciency call for more effort in tackling performance bugs. This paper conducts a \ncomprehensive study of 109 real-world performance bugs that are randomly sampled from .ve repre\u00adsentative \nsoftware suites (Apache, Chrome, GCC, Mozilla, and MySQL). The .ndings of this study provide guidance \nfor future work to avoid, expose, detect, and .x performance bugs. Guided by our characteristics study, \nef.ciency rules are ex\u00adtracted from 25 patches and are used to detect performance bugs. 332 previously \nunknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, \nin\u00adcluding 219 performance problems found by applying rules across applications. Categories and Subject \nDescriptors D.2.5 [Software Engineer\u00ading]: Testing and Debugging; D.4.8 [Operating Systems]: Perfor\u00admance \nGeneral Terms Languages, Measurement, Performance, Relia\u00adbility Keywords performance bugs, characteristics \nstudy, rule-based bug detection 1. Introduction 1.1 Motivation Slow and inef.cient software can easily \nfrustrate users and cause .\u00adnancial losses. Although researchers have devoted decades to trans\u00adparently \nimproving software performance, performance bugs con\u00adtinue to pervasively degrade performance and waste \ncomputation resources in the .eld [40]. Meanwhile, current support for combat\u00ad ing performance bugs is \npreliminary due to the poor understanding of real-world performance bugs. Following the convention of \ndevelopers and researchers on this topic [5, 26, 40, 50], we refer to performance bugs as software * \nThis work was done when the author was a graduate student at University of Wisconsin. Currently the author \nis working in NVIDIA. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00 defects where relatively simple source-code changes can signi.\u00adcantly speed up software, \nwhile preserving functionality. These de\u00adfects cannot be optimized away by state-of-practice compilers, \nthus bothering end users. Figure 1 shows an example of a real-world per\u00ad formance bug. Apache HTTPD developers \nforgot to change a pa\u00adrameter of API apr stat after an API upgrade. This mistake caused more than ten \ntimes slowdown in Apache servers. Figure 1: A performance bug from Apache-HTTPD ( + and - denote the \ncode added and deleted to .x this bug) Performance bugs exist widely in released software. For exam\u00adple, \nMozilla developers have .xed 5 60 performance bugs reported by users every month over the past 10 years. \nThe prevalence of per\u00adformance bugs is inevitable because little work has been done to help developers \navoid performance-related mistakes. In addition, performance testing mainly relies on ineffective black-box \nrandom testing and manual input design, which allows the majority of per\u00adformance bugs to escape [40]. \nPerformance bugs lead to reduced throughput, increased la\u00adtency, and wasted resources in the .eld. In \nthe past, they have caused several highly publicized failures, causing hundred-million dollar software \nprojects to be abandoned [41, 45]. Worse still, performance problems are costly to diagnose due to their \nnon fail-stop symptoms. Software companies may need sev\u00aderal months of effort by experts to .nd a couple \nof performance bugs that cause a few hundred-millisecond delay in the 99th per\u00adcentile latency of their \nservice [49]. The following trends will make the performance-bug problem more critical in the future: \nHardware: For many years, Moore s law ensured that hardware would make software faster over time with \nno software develop\u00adment effort. In the multi-core era, when each core is unlikely to become faster, \nperformance bugs are particularly harmful. Software: The increasing complexity of software systems and \nrapidly changing workloads provide new opportunities for perfor\u00admance waste and new challenges in diagnosis \n[11]. Energy ef.ciency: Increasing energy costs provide a powerful economic argument for avoiding performance \nbugs. When one is willing to sacri.ce the service quality to reduce energy consump\u00adtion [3, 31], ignoring \nperformance bugs is unforgivable. For exam\u00ad ple, by .xing bugs that have doubled the execution time, \none may potentially halve the carbon footprint of buying and operating com\u00adputers. Performance bugs \nmay not have been reported as often as func\u00adtional bugs, because they do not cause fail-stop failures. \nHowever, considering the preliminary support for combating performance bugs, it is time to pay more attention \nto them when we enter a new resource-constrained computing world.  1.2 Contribution 1: Characteristics \nStudy Many empirical studies [7, 34, 43, 46, 53] have been conducted for traditional bugs that lead to \nincorrect software functionality, referred to as functional bugs. These studies have successfully guided \nthe design of functional software testing, functional bug detection, and failure diagnosis. Poor understanding \nof performance bugs and wrong percep\u00adtions, such as performance is taken care of by compilers and hard\u00adware \n, are partly the causes of today s performance-bug problem [12]. The lack of empirical studies on topics \nlike how performance bugs are introduced , what input conditions are necessary to ex\u00adpose performance \nbugs , what the common root causes of real\u00adworld performance bugs are , and how performance bugs are \n.xed by developers , have severely limited the design of performance\u00adbug avoidance, testing, detection, \nand .xing tools. This paper makes the .rst, to the best of our knowledge, com\u00adprehensive study of real-world \nperformance bugs based on 109 bugs randomly collected from the bug databases of .ve representa\u00adtive open-source \nsoftware suites (Apache, Chrome, GCC, Mozilla, and MySQL). Our study has made the following .ndings. \nGuidance for bug avoidance. Two thirds of the studied bugs are introduced by developers wrong understanding \nof workload or API performance features. More than one quarter of the bugs arise from previously correct \ncode due to workload or API changes. To avoid performance bugs, developers need performance-oriented \nannotation systems and change-impact analysis. (Section 4.2). Guidance for performance testing. Almost \nhalf of the studied bugs require inputs with both special features and large scales to manifest. New \nperformance-testing schemes that combine the input-generation techniques used by functional testing [4, \n17] with a consideration towards large scales will signi.cantly improve the state of the art (Section \n4.3). Guidance for bug detection. Recent works [5, 11, 26, 47, 57, 58] have demonstrated the potential \nof performance-bug detection. Our study found common root-cause and structural patterns of real\u00adworld \nperformance bugs that can help improve the coverage and accuracy of performance-bug detection (Sections \n4.1 and 4.5). Guidance for bug .xing and detection. Almost half of the examined bug patches include reusable \nef.ciency rules that can help detect and .x performance bugs (Section 4.4). Comparison with functional \nbugs. Performance bugs tend to hide for much longer time in software than functional bugs (Section 4.5). \nUnlike functional bugs, performance bugs cannot all be modeled as rare events, because a non-negligible \nportion of them can be triggered by almost all inputs (Section 4.3). General motivation (1) Many performance-bug \npatches are small. The fact that we can achieve signi.cant performance im\u00adprovement through a few lines \nof code change motivates re\u00adsearchers to pay more attention to performance bugs (Section 4.4). (2) A \nnon-negligible portion of performance bugs in multi-threaded software are related to synchronization. \nDevelopers need tool sup\u00adport to avoid over-synchronization traps (Section 4.3).  1.3 Contribution 2: \nBug Detection Rule-based bug detection is effective for detecting functional bugs [6, 21, 30, 42]. Following \nour characteristics study, we hypothesize that (1) ef.ciency-related rules exist; (2) we can extract \nrules from performance-bug patches; and (3) we can use the extracted rules to discover previously unknown \nperformance bugs. To test these hypotheses, we collected rules from 25 Apache, Mozilla, and MySQL bug \npatches and built static checkers to .nd violations to these rules. Our checkers automatically found \n125 potential performance problems (PPPs) in the original buggy versions of Apache, Mozilla, and MySQL. \nProgrammers failed to .x them together with the orig\u00adinal 25 bugs where the rules came from. Our checkers \nalso found 332 previously unknown PPPs in the latest versions of Apache, Mozilla, and MySQL. These include \n219 PPPs found by checking an application using rules extracted from a different application. Our thorough \ncode reviews and unit testings con.rm that each PPP runs signi.cantly slower than its functionality-preserving \nal\u00adternate suggested by the checker. Some of these PPPs are already con.rmed by developers and .xed based \non our report. The main contribution of our bug-detection work is that it con\u00ad.rms the existence and \nvalue of ef.ciency rules: ef.ciency rules in our study are usually violated at more than one place, by \nmore than one developer, and sometimes in more than one program. Our experience motivates future work \nto automatically generate ef.\u00adciency rules, through new patch languages [43], automated patch analysis \n[36], source code analysis, or performance-oriented anno\u00ad tations. Future work can also improve the accuracy \nof performance\u00adbug detection by combining static checking with dynamic analysis and workload monitoring. \n2. Methodology 2.1 Applications Application Suite Description (language) # Bugs Apache Suite HTTPD: Web \nServer (C) TomCat: Web Application Server (Java) Ant: Build management utility (Java) 25 Chromium Suite \nGoogle Chrome browser (C/C++) 10 GCC Suite GCC &#38; G++ Compiler (C/C++) 10 Mozilla Suite Firefox: Web \nBrowser (C++, JavaScript) Thunderbird: Email Client (C++, JavaScript) 36 MySQL Suite Server: Database \nServer (C/C++) Connector: DB Client Libraries (C/C++/Java/.Net) 28 Total 109 Table 1: Applications and \nbugs used in the study We chose .ve open-source software suites to examine: Apache, Chrome, GCC, Mozilla, \nand MySQL. These popular, award\u00adwinning software suites [22] are all large-scale and mature, with millions \nof lines of source code and well maintained bug databases. As shown in Table 1, these .ve suites provide \na good coverage of various types of software, such as interactive GUI applications, server software, \ncommand-line utilities, compilers, and libraries. They are primarily written in C/C++ and Java. Although \nthey are all open-source software, Chrome is backed up by Google and MySQL was acquired by Sun/Oracle \nin 2008. Furthermore, the Chrome browser was .rst released in 2008, while the other four have had 10 \n15 years of bug reporting history. From these applications, we can observe both traditions and new software \ntrends such as web applications. 2.2 Bug Collection GCC, Mozilla, and MySQL developers explicitly mark \ncertain re\u00adports in their bug databases as performance bugs using special tags, which are compile-time-hog, \nperf, and S5 respectively. Apache and Chrome developers do not use any special tag to mark perfor\u00admance \nbugs. Therefore, we searched their bug databases using a set of performance-related keywords ( slow , \nperformance , latency , throughput , etc.). From these sources, we randomly sampled 109 .xed bugs that \nhave suf.cient documentation. The details are shown in Table 1. Among these bugs, 44 were reported after \n2008, 38 were re\u00adported between 2004 and 2007, and 27 were reported before 2004. 41 bugs came from server \napplications and 68 bugs came from client applications.  2.3 Caveats Our .ndings need to be taken with \nthe methodology in mind. The applications in our study cover representative and important soft\u00adware categories, \nworkload, development background, and program\u00adming languages. Of course, there are still uncovered categories, \nsuch as scienti.c computing software and distributed systems. The bugs in our study are collected from \n.ve bug databases without bias. We have followed the decisions made by develop\u00aders about what are performance \nbugs, and have not intentionally ignored any aspect of performance problems in bug databases. Of course, \nsome performance problems may never be reported to the bug databases and some reported problems may never \nbe .xed by developers. Unfortunately, there is no conceivable way to study these unreported or un.xed \nperformance problems. We believe the bugs in our study provide a representative sample of the reported \nand .xed performance bugs in these representative applications. We have spent more than one year to study \nall sources of infor\u00admation related to each bug, including forum discussions, patches, source code repositories, \nand others. Each bug is studied by at least two people and the whole process consists of several rounds \nof bug (re-)study, bug (re-)categorization, cross checking, etc. Finally, we do not emphasize any quantitative \ncharacteristic results, and most of the characteristics we found are consistent across all examined applications. \n3. Case Studies The goal of our study is to improve software ef.ciency by inspir\u00ading better techniques \nto avoid, expose, detect, and .x performance bugs. This section uses four motivating examples from our \nbug set to demonstrate the feasibility and potential of our study. Particu\u00adlarly, we will answer the \nfollowing questions using these examples: (1) Are performance bugs too different from traditional bugs \nto study along the traditional bug-.ghting process (i.e., bug avoid\u00adance, testing, detection, and .xing)? \n(2) If they are not too different, are they too similar to be worthy of a study? (3) If developers were \nmore careful, do we still need research and tool support to combat performance bugs? Transparent Draw \n(Figure 2) Mozilla developers imple\u00admented a procedure nsImage::Draw for .gure scaling, composit\u00ading, \nand rendering, which is a waste of time for transparent .gures. This problem did not catch developers \nattention until two years later when 1 pixel by 1 pixel transparent GIFs became general purpose spacers \nwidely used by Web developers to work around certain idiosyncrasies in HTML 4. The patch of this bug \nskips nsImage::Draw when the function input is a transparent .gure. Intensive GC (Figure 3) Users reported \nthat Firefox cost 10 times more CPU than Safari on some popular Web pages, such as gmail.com. Lengthy \npro.ling and code investigation revealed that Firefox conducted an expensive garbage collection process \nGC at the end of every XMLHttpRequest, which is too frequent. A developer then recalled that GC was added \nthere .ve years ago when XHRs were infrequent and each XHR replaced substantial portions of the DOM in \nJavaScript. However, things have changed Figure 2: A Mozilla bug drawing transparent .gures Figure \n3: A Mozilla bug doing intensive GCs Figure 4: A Mozilla bug with un-batched DB operations Figure 5: \nA MySQL bug with over synchronization in modern Web pages. As a primary feature enabling web 2.0, XHRs \nare much more common than .ve years ago. This bug is .xed by removing the call to GC. Bookmark All (Figure \n4) Users reported that Firefox hung when they clicked bookmark all (tabs) with 20 open tabs. In\u00advestigation \nrevealed that Firefox used N database transactions to bookmark N tabs, which is very time consuming comparing \nwith batching all bookmark tasks into a single transaction. Discussion among developers revealed that \nthe database service library of Fire\u00adfox did not provide interface for aggregating tasks into one trans\u00adaction, \nbecause there was almost no batchable database task in Firefox a few years back. The addition of batchable \nfunctionali\u00adties such as bookmark all (tabs) exposed this inef.ciency prob\u00adlem. After replacing N invocations \nof doTransact with a single doAggregateTransact, the hang disappears. During patch review, developers \nfound two more places with similar problems and .xed them by doAggregateTransact. Slow Fast-Lock (Figure \n5) MySQL synchronization-library developers implemented a fastmutex lock for fast locking. Un\u00adfortunately, \nusers unit test showed that fastmutex lock could be 40 times slower than normal locks. It turns out that \nlibrary func\u00adtion random() actually contains a lock. This lock serializes every threads that invoke \nrandom(). Developers .xed this bug by replac\u00ading random() with a non-synchronized random number generator. \nThese four bugs can help us answer the questions asked earlier. (1) They have similarity with traditional \nbugs. For example, they are all related to usage rules of functions/APIs, a topic well studied by previous \nwork on detecting functional bugs [30, 33]. (2) They also have interesting differences compared to tradi\u00adtional \nbugs. For example, the code snippets in Figure 2 4 turned buggy (or buggier) long after they were written, \nwhich is rare for functional bugs. As another example, testing designed for func\u00adtional bugs cannot effectively \nexpose bugs like Bookmark All. Once the program has tried the bookmark all button with one or two open \ntabs, bookmarking more tabs will not improve the statement or branch coverage and will be skipped by \nfunctional testing. (3) Developers cannot .ght these bugs by themselves. They cannot predict future \nworkload or code changes to avoid bugs like Transparent Draw, Intensive GC, and Bookmark All. Even experts \nwho implemented synchronization libraries could not avoid bugs like Slow Fast-Lock, given opaque APIs \nwith unexpected performance features. Research and tool support are needed here.  Of course, it is premature \nto draw any conclusion based on four bugs. Next, we will comprehensively study 109 performance bugs. \n4. Characteristics Study We will study the following aspects of real-world performance bugs, following \ntheir life stages and different ways to combat them. 1. What are the root causes of performance bugs? \nThis study will provide a basic understanding of real-world performance bugs and give guidance to bug \ndetection research. 2. How are performance bugs introduced? This study will shed light on how to avoid \nintroducing performance bugs. 3. How can performance bugs manifest? This study can help design effective \ntesting techniques to expose performance bugs after they are introduced into software.  4. How are performance \nbugs .xed? Answers to this question will help improve the patching process. The result of this study \nis shown in Table 2. 4.1 Root Causes of Performance Bugs There are a large variety of potential root \ncauses for inef.cient code, such as poorly designed algorithms, non-optimal data struc\u00adtures, cache-unfriendly \ndata layouts, etc. Our goal here is not to discover previously unheard-of root causes, but to check whether \nthere are common root-cause patterns among real-world perfor\u00admance bugs that bug detection can focus \non. Our study shows that the majority of real-world performance bugs in our study are covered by only \na couple of root-cause categories. Common patterns do exist and performance is mostly lost at call sites \nand function boundaries, as follows. Uncoordinated Functions More than a third of the performance bugs \nin our study are caused by inef.cient function-call combina\u00adtions composed of ef.cient individual functions. \nThis occurs to the Bookmark All example shown in Figure 4. Using doTransact to bookmark one URL in one \ndatabase transaction is ef.cient. How\u00adever, bookmarking N URLs using N separate transactions is less \nef\u00ad.cient than calling doAggregateTransact to batch all URL book\u00admarkings in one transaction. Skippable \nFunction More than a quarter of bugs are caused by calling functions that conduct unnecessary work given \nthe calling context, such as calling nsImage::Draw for transparent .gures in the Transparent Draw bug \n(Figure 2) and calling unnecessary GCs in the Intensive GC bug (Figure 3). Synchronization Issues Unnecessary \nsynchronization that in\u00adtensi.es thread competition is also a common cause of performance loss, as shown \nin the Slow Fast-Lock bug (Figure 5). These bugs are especially common in server applications, contributing \nto 4 out of 15 Apache server bugs and 5 out of 26 MySQL server bugs. Others The remaining 23 bugs are \ncaused by a variety of reasons. Some use wrong data structures. Some are related to hardware architecture \nissues. Some are caused by high-level de\u00adsign/algorithm issues, with long propagation chains. For example, \nMySQL39295 occurs when MySQL mistakenly invalidates the query cache for read-only queries. This operation \nitself does not take time, but it causes cache misses and performance losses later. This type of root \ncause is especially common in GCC. 4.2 How Performance Bugs Are Introduced We have studied the discussion \namong developers in bug databases and checked the source code of different software versions to un\u00adderstand \nhow bugs are introduced. Our study has particularly fo\u00adcused on the challenges faced by developers in \nwriting ef.cient software, and features of modern software that affect the introduc\u00adtion of performance \nbugs. Our study shows that developers are in a great need of tools that can help them avoid the following \nmistakes. Workload Mismatch Performance bugs are most frequently in\u00adtroduced when developers workload \nunderstanding does not match with the reality. Our further investigation shows that the following challenges \nare responsible for most workload mismatches. Firstly, the input paradigm could shift after code implemen\u00adtation. \nFor example, the HTML standard change and new trends in web-page content led to Transparent Draw and \nIntensive GC, shown in Figure 2 and Figure 3. Secondly, software workload has become much more diverse \nand complex than before. A single program, such as Mozilla, may face various types of workload issues: \nthe popularity of transparent .gures on web pages led to Transparent Draw in Figure 2; the high frequency \nof XMLHttpRequest led to Intensive GC in Figure 3; users habit of not changing the default con.guration \nsetting led to Mozilla Bug110555. The increasingly dynamic and diverse workload of modern soft\u00adware will \nlead to more performance bugs in the future. API Misunderstanding The second most common reason is that \ndevelopers misunderstand the performance feature of certain func\u00adtions. This occurs for 31 bugs in our \nstudy. Sometimes, the performance of a function is sensitive to the value of a particular parameter, \nand developers happen to use performance-hurting values. Sometimes, developers use a function to perform \ntask i, and are unaware of an irrelevant task j conducted by this function that hurts performance but \nnot functionality. For example, MySQL developers did not know the synchronization inside random and introduced \nthe Slow Fast-Lock bug shown in Figure 5. Code encapsulation in modern software leads to many APIs with \npoorly documented performance features. We have seen de\u00advelopers explicitly complain about this issue \n[51]. It will lead to more performance bugs in the future. Others Apart from workload issues and API \nissues, there are also other reasons behind performance bugs. Interestingly, some performance bugs are \nside-effects of functional bugs. For exam\u00adple, in Mozilla196994, developers forgot to reset a busy-.ag. \nThis semantic bug causes an event handler to be constantly busy. As a result, a performance loss is the \nonly externally visible symptom of this bug. When a bug was not buggy An interesting trend is that 29 \nout of 109 bugs were not born buggy. They became inef.cient long after they were written due to workload \nshift, such as that in Transparent Draw and Intensive GC (Figures 2 and 3), and code changes in other \npart of the software, such as that in Figure 1. Apache Chrome GCC Mozilla MySQL Total Number of bugs \nstudied 25 10 10 36 28 109 Root Causes of Performance Bugs Uncoordinated Functions: function calls take \na detour to generate results 12 4 2 15 9 42 Skippable Function: a function call with un-used results \n6 4 3 14 7 34 Synchronization Issues: inef.cient synchronization among threads 5 1 0 1 5 12 Others: all \nthe bugs not belonging to the above three categories 3 1 5 6 8 23 How Performance Bugs Are Introduced \nWorkload Issues: developers workload assumption is wrong or out-dated 13 2 5 14 7 41 API Issues: misunderstand \nperformance features of functions/APIs 6 3 1 13 8 31 Others: all the bugs not belonging to the above \ntwo categories 6 5 4 10 13 38 How Performance Bugs Are Exposed Always Active: almost every input on every \nplatform can trigger this bug 2 3 0 6 4 15 Special Feature: need special-value inputs to cover speci.c \ncode regions 19 6 10 22 18 75 Special Scale: need large-scale inputs to execute a code region many times \n17 2 10 23 19 71 Feature+Scale: the intersection of Special Feature and Special Scale 13 1 10 15 13 52 \nHow Performance Bugs Are Fixed Change Call Sequence: a sequence of function calls reorganized/replaced \n10 2 3 20 12 47 Change Condition: a condition added or modi.ed to skip certain code 2 7 6 10 10 35 Change \nA Parameter: changing one function/con.guration parameter 5 0 1 4 3 13 Others: all the bugs not belonging \nto the above three categories 9 3 0 4 7 23 Table 2: Categorization for Sections 4.1 4.4 (most categories \nin each section are not exclusive) In Chrome70153, when GPU accelerator became available, some software \nrendering code became inef.cient. Many of these bugs went through regression testing without being caught. \n 4.3 How Performance Bugs Are Exposed We de.ne exposing a performance bug as causing a perceivably negative \nperformance impact, following the convention used in most bug reports. Our study demonstrates several \nunique challenges for perfor\u00admance testing. Always Active Bugs A non-negligible portion of performance \nbugs are almost always active. They are located at the start-up phase, shutdown phase, or other places \nthat are exercised by almost all inputs. They could be very harmful in the long term, because they waste \nperformance at every deployment site during every run of a program. Many of these bugs were caught during \ncomparison with other software (e.g., Chrome vs. Mozilla vs. Safari). Judging whether performance bugs \nhave manifested is a unique challenge in performance testing. Input Feature &#38; Scale Conditions About \ntwo thirds of perfor\u00admance bugs need inputs with special features to manifest. Other\u00adwise, the buggy \ncode units cannot be touched. Unfortunately, this is not what black-box testing is good at. Much manual \neffort will be needed to design test inputs, a problem well studied by past re\u00adsearch in functional testing \n[4, 5]. About two thirds of performance bugs need large-scale inputs to manifest in a perceivable way. \nThese bugs cannot be effectively exposed if software testing executes each buggy code unit only once, \nwhich unfortunately is the goal of most functional testing. Almost half of the bugs need inputs that \nhave special features and large scales to manifest. For example, to trigger the bug shown in Figure 4, \nthe user has to click bookmark all (i.e., special feature), with many open tabs (i.e., large scale). \n 4.4 How Performance Bugs Are Fixed We have manually checked the .nal patches to answer two ques\u00adtions. \nAre there common strategies for .xing performance bugs? How complicated are performance patches? The \nresult of our study is opposite to the intuition that perfor\u00admance patches must be complicated and lack \ncommon patterns. Fixing strategies There are three common strategies in .xing performance bugs, as shown \nin Table 2. The most common one is to change a function-call sequence, referred to as Change Call Sequence. \nIt is used to .x 47 bugs. Many bugs with uncoordinated function calls are .xed by this strategy (e.g., \nBookmark All in Figure 4). Some bugs with skippable function calls are also .xed by this strategy, where \nbuggy function calls are removed or relocated (e.g. Intensive GC in Figure 3). The second most common \nstrategy is Change Condition. It is used in 35 patches, where code units that do not always generate \nuseful results are conditionally skipped. For example, Draw is conditionally skipped to .x Transparent \nDraw (Figure 2). Finally, 13 bugs are .xed by simply changing a parameter in the program. For example, \nthe Apache bug shown in Figure 1 is .xed by changing a parameter of apr stat; MySQL45475 is .xed by changing \nthe con.guration parameter TABLE OPEN CACHE MIN from 64 to 400. Are patches complicated? Most performance \nbugs in our study can be .xed through simple changes. In fact, 42 out of 109 bug patches contain .ve \nor fewer lines of code changes. The median patch size for all examined bugs is 8 lines of code. The small \npatch size is a result of the above .xing strategies. Change A Parameter mostly requires just one line \nof code change. 33 out of the 47 Change Call Sequence patches involve only ex\u00adisting functions, with \nno need to implement new functions. Many Change Condition patches are also small. 4.5 Other Characteristics \nLife Time We chose Mozilla to investigate the life time of perfor\u00admance bugs, due to its convenient CVS \nquery interface. We con\u00adsider a bug s life to have started when its buggy code was .rst writ\u00adten. The \n36 Mozilla bugs in our study took 935 days on average to get discovered, and another 140 days on average \nto be .xed. For comparison, we randomly sampled 36 functional bugs from Mozilla. These bugs took 252 \ndays on average to be discovered, which is much shorter than that of performance bugs in Mozilla. These \nbugs took another 117 days on average to be .xed, which is a similar amount of time with those performance \nbugs. Location For each bug, we studied the location of its minimum unit of inef.ciency. We found that \nover three quarters of bugs are located inside either an input-dependent loop or an input-event han\u00addler. \nFor example, the buggy code in Figure 3 is executed at every XHR completion. The bug in Figure 2 wastes \nperformance for ev\u00ad ery transparent image on a web page. About 40% of buggy code units contain a loop \nwhose number of iterations scales with input. For example, the buggy code unit in Figure 4 contains a \nloop that iterates as many times as the number of open tabs in the browser. In addition, about half performance \nbugs involve I/Os or other time\u00adconsuming system calls. There are a few bugs whose buggy code units only \nexecute once or twice during each program execution. For example, the Mozilla110555 bug wastes performance \nwhile processing exactly two .xed-size default con.guration .les, user\u00adChrome.css and userContent.css, \nduring the startup of a browser. Correlation Among Categories Following previous empirical studies [29], \nwe use a statistical metric lift to study the correlation among characteristic categories. The lift of \ncategory A and category P(AB) B, denoted as lift(AB), is calculated as , where P(AB) is the P(A)P(B) \nprobability of a bug belonging to both categories A and B. When lift(AB) equals 1, category A and category \nB are independent with each other. When lift(AB) is greater than 1, categories A and B are positively \ncorrelated: when a bug belongs to A, it likely also belongs to B. The larger the lift is, the more positively \nA and B are correlated. When lift(AB) is smaller than 1, A and B are negatively correlated: when a bug \nbelongs to A, it likely does not belong to B. The smaller the lift is, the more negatively A and B are \ncorrelated. Among all categories, the Skippable Function root cause and the Change Condition bug-.x strategy \nare the most positively corre\u00adlated with a 2.02 lift. The Workload Issues bug-introducing reason is strongly \ncorrelated with the Change-A-Parameter bug-.x strat\u00adegy with a 1.84 lift. The Uncoordinated Functions \nroot cause and the API Issues bug-introducing reason are the third most positively correlated pair with \na 1.76 lift. On the other hand, the Synchroniza\u00adtion Issues root cause and the Change Condition .x strategy \nare the most negatively correlated categories of different characteristic aspects1. Their lift is only \n0.26. Server Bugs vs. Client Bugs Our study includes 41 bugs from server applications and 68 bugs from \nclient applications. To under\u00adstand whether these two types of bugs have different characteristics, we \napply chi-square test [56] to each category listed in Table 2. We choose 0.01 as the signi.cance level \nof our chi-square test. Under this setting, if we conclude that server and client bugs have differ\u00adent \nprobabilities of falling into a particular characteristic category, this conclusion only has 1% probability \nto be wrong. We .nd that, among all the categories listed in Table 2, only the Synchronization Issues \ncategory is signi.cantly different between server bugs and client bugs Synchronization Issues have caused \n22% of server bugs and only 4.4% of client bugs. 5. Lessons from Our Study Comparison with Functional \nBugs There are several interest\u00ading comparisons between performance and functional bugs. (1) The distribution \nof performance-failure rates over software life time fol\u00adlows neither the bathtub model of hardware errors \nnor the gradually maturing model of functional bugs, because performance bugs have long hiding periods \n(Section 4.5) and can emerge from non-buggy places when software evolves (Section 4.2). (2) Unlike functional \n1 Two categories of the same aspect, such as the Skippable Function root cause and the Uncoordinated \nFunctions root cause, usually have a highly negative correlation. bugs, performance bugs cannot always \nbe modeled as rare events, because some of them are always active (Section 4.3). (3) The per\u00ad centage \nof synchronization problems among performance bugs in our study is higher than the percentage of synchronization \nprob\u00adlems among functional bugs in a previous study for a similar set of applications [29] (Section 4.1). \nBug Detection Our study motivates future research in performance\u00adbug detection: performance bugs cannot \nbe easily avoided (Section 4.2); and, they can escape testing even when the buggy code is exercised (Section \n4.3). Our study provides future bug detectors with common root cause and location patterns (Section 4.1 \nand Section 4.5). Rule-based bug detection [13, 21, 30, 33] are promising for de\u00ad tecting performance \nbugs. It will be discussed in Section 6 in de\u00ad tail. Invariant-based bug detection and delta debugging \n[14, 60] are also promising. Our study shows that the majority of performance bugs require special inputs \nto manifest (Section 4.3). In addition, the same piece of code may behave buggy and non-buggy in differ\u00adent \nsoftware versions (Section 4.2). This provides opportunities for invariant extraction and violation checking. \nAnnotation Systems Annotation systems are used in many software development environments [37, 54]. Unfortunately, \nthey mainly communicate functionality information. Our study calls for performance-aware annotation systems \n[44, 55] that help developers maintain and communicate APIs perfor\u00ad mance features and workload assumptions \n(Section 4.2). Simple support such as warning about the existence of locks in a library function, specifying \nthe complexity of a function, and indicating the desired range of a performance-sensitive parameter can \ngo a long way in avoiding performance bugs. Recent work that auto\u00admatically calculates function complexity \nis also promising [18]. Testing Regression testing and change-impact analysis have to consider workload \nchanges and performance impacts, because new performance bugs may emerge from old code (Section 4.2). \nPerformance testing can be improved if its input design com\u00adbines smart input-generation techniques used \nin functional testing [4, 17] with an emphasis on large scale (Section 4.3). Expressing performance oracles \nand judging whether perfor\u00admance bugs have occurred are critical challenges in performance testing (Section \n4.3). Techniques that can smartly compare perfor\u00ad mance numbers across inputs and automatically discover \nthe exis\u00adtence of performance problems are desired. Diagnosis Pro.ling is frequently used to bootstrap \nperformance diagnosis. Our root-cause study shows that extra analysis is needed to help diagnose performance \nbugs. It is dif.cult to use pro.ling alone to locate the root cause of a performance bug that has a long \npropagation chain or wastes computation time at function boundaries, which is very common (Section 4.1). \nFuture Directions One might argue that performance some\u00adtimes needs to be sacri.ced for better productivity \nand functional correctness. However, the fact that we can often achieve signi.\u00adcant performance improvement \nthrough only a few lines of code change motivates future research to pay more attention to perfor\u00admance \nbugs (Section 4.4). Our study suggests that the workload trend and API features of modern software will \nlead to more per\u00adformance bugs in the future (Section 4.2). In addition, our study ob\u00ad serves a signi.cant \nportion of synchronization-related performance bugs in multi-threaded software. There will be more bugs \nof this type in the multi-core era. Finally, our observations have been consistent across old soft\u00adware \nand new software (Chrome), old bugs (27 pre-2004 bugs) and new bugs (44 post-2008 bugs). Therefore, we \nare con.dent that these lessons will be useful at least for the near future. 6. Rule-Based Performance-Bug \nDetection 6.1 Overview Rule-based detection approach is effective for discovering func\u00adtional bugs and \nsecurity vulnerabilities [6, 15, 21, 30, 42]. Many functional bugs can be identi.ed by comparing against \ncertain function-call sequences that have to be followed in a program for functional correctness and \nsecurity. We hypothesize that rule-based bug detection is useful for de\u00adtecting performance bugs based \non our characteristics study: Ef.ciency rules should exist. Those inef.cient function-call se\u00adquences \nstudied in Section 4.1 could all become rules. For ex\u00ad ample, random() should not be used by concurrent \nthreads, and doTransact() in loops should be replaced by aggregateTransact(). Ef.ciency rules can be \neasily collected from patches, as most patches are small and follow regular .xing strategies (Section \n4.4). Ef.ciency rules could be widely applicable, as a misunderstand\u00ading of an API or workload could \naffect many places and lead to many bugs, considering how bugs are introduced (Section 4.2). This section \nwill test our hypothesis and provide guidance for future work on combating performance bugs. 6.2 Ef.ciency \nRules in Patches Terminology Ef.ciency rules, or rules, include two components: a transformation and \na condition for applying the transformation. Once a code region satis.es the condition, the transformation \ncan be applied to improve performance and preserve functionality. We have manually checked all the 109 \nperformance-bug patches. 50 out of these 109 patches contain ef.ciency rules, coming from all .ve applications. \nThe other 59 do not contain rules, because they either target too speci.c program contexts or are too \ngeneral to be useful for rule-based bug detection. Call Sequence Conditions function C::f() is invoked \nfunction f1 is always followed by f2 function f1 is called once in each iteration of a loop Parameter/Return \nConditions nth parameter of f1 equals K (constant) nth parameter of f1 is the same variable as the return \nof f2 a param. of f1 and a param. of f2 point to the same object the return of f1 is not used later the \nparameter of f1 is not modi.ed within certain scope the input is a long string Calling Context Conditions \nfunction f1 is only called by one thread function f1 can be called simultaneously by multiple threads \nfunction f1 is called many times during the execution Table 3: Typical conditions in function rules \nMost of these 50 rules, according to the lift correlation met\u00adric, are related to the Uncoordinated Functions \nroot cause and the Change Call Sequence .x strategy. The conditions for apply\u00ading these rules are composed \nof conditions on function-call se\u00adquences, parameter/return variables, and calling contexts, as shown \nin Table 3. For example, to apply the Bookmark All patch in Fig\u00adure 4 elsewhere, one needs to .nd places \nthat call doTransact in\u00adside a loop; to apply the patch in Figure 1 elsewhere, one needs to ensure that \ncertain .elds of the object pointed by the .rst pa\u00adrameter of apr stat is not used afterward. There are \nalso non\u00adfunction rules, usually containing Change Condition transforma\u00adtion and other miscellaneous \nalgorithm improvements.  6.3 Building Rule Checkers Selecting Statically Checkable Rules Some rules \napplying con\u00additions are statically checkable, such as function f1 inside a loop; some are dynamically \ncheckable, such as function f1 called by mul\u00adtiple threads at the same time; some are related to workload, \nsuch as having many large input .les. We check three largest application suites in our study: Apache, \nMySQL, and Mozilla. We .nd that 40 bug patches from them contain rules. 25 out of these 40 have applying \nconditions that are mostly statically checkable. Therefore, we have built checkers based on these 25 \nef.ciency rules. Checker Implementation We build 25 checkers in total. 14 of them are built using LLVM \ncompiler infrastructure [27] for rules from C/C++ applications. LLVM works well for C++ software that \ntroubles many other static analysis infrastructure [43]. It also provides suf.cient data type, data .ow, \nand control .ow analysis support for our checking. The other 11 checkers are written in Python for 11 \nrules from Java, JavaScript, and C# applications. The checker implementation is mostly straightforward. \nEach checker goes through software bitcode, in case of LLVM checkers, or source code, in case of Python \ncheckers, looking for places that satisfy the patch-applying condition. We brie.y discuss how our checkers \nexamine typical conditions for function rules in the following. Checking call-sequence conditions, exempli.ed \nin Table 3, in\u00ad volve mainly three tasks: (1) Differentiating functions with the same name but different \nclasses; (2) Collecting loop information (loop-head, loop-exit conditions, loop-body boundaries, etc.); \n(3) Control .ow analysis. LLVM provides suf.cient support for all these tasks. Checkers written in Python \nstruggle from time to time. Checking parameter/return conditions, exempli.ed in Table 3, typically rely \non data-.ow analysis. In our current prototype, LLVM checkers conduct intra-procedural data-.ow analysis. \nThis analysis is scalable, but may lead to false positives and negatives. In practice, it works well \nas shown by our experimental results. Our current Python checkers can extract parameters of particular \nfunction calls, but can only do preliminary data-.ow analysis. 6.4 Rule-Checking Methodology We conduct \nall the experiments on an 8-core Intel Xeon machine running Linux version 2.6.18. We apply every checker \nto the following software: (1) The exact version of the software that the original patch was applied \nto, which is referred to as original version; (2) The latest version of the software that the original \npatch was applied to, which is referred to as original software; (3) The latest versions of software \napplications that are differ\u00adent from the one that the original patch was applied to, which is referred \nto as different software. This was applied to 13 checkers, whose rules are about glibc library functions, \nJava library func\u00adtions, and some general algorithm tricks. We will refer to this as cross-application \nchecking. For example, a C/C++ checker from MySQL will be applied to Mozilla and Apache HTTPD for cross\u00adapplication \nchecking; a Java checker from Apache TomCat server will be applied to the 65 other Java applications \nin the Apache soft\u00adware suite2.  The checking results are categorized into three types: PPPs, bad practices, \nand false positives. As discussed in Section 1.3, a PPP is an inef.cient code region that runs slower \nthan its functionality\u00adpreserving alternate implied by the ef.ciency rule. A bad practice is a region \nprone to becoming inef.cient in the future. We reported some PPPs to developers. Among those reported, \n14 PPPs detected by 6 different checkers have been con.rmed and .xed by the 2 Development teams behind \ndifferent Apache applications are different developers. Other reported PPPs are put on hold due to lack \nof bug-triggering input information, which is unfortunately out of the scope of this work. Finally, we \nhave also changed each checker slightly to report code regions that follow each ef.ciency rule. We refer \nto these regions as good practices, the opposite of PPPs.  6.5 Rule-Checking Results Overall Results \nAs shown in Table 4, 125 PPPs are found in the original version of software. Programmers missed them \nand failed to .x them together with the original bugs. 113 previously unknown PPPs are found in the latest \nversions of the original software, including bugs inherited from the original version and bugs newly \nintroduced. Figure 6 shows an example. 219 previously unknown PPPs are found in the latest versions of \ndifferent software. An example is shown in Figure 6. 14 PPPs in the latest versions of Apache, Mozilla, \nand MySQL are already con.rmed and .xed by developers based on our report. These results con.rm that \nperformance bugs widely exist. Ef.\u00adciency rules exist and are useful for .nding performance problems. \nPPPs In Original Versions 17 out of 25 checkers found new PPPs, 125 in total, in the original versions \nof the buggy software. Some developers clearly tried to .nd all similar bugs when .xing one bug, but \ndid not succeed. For example, in MySQL14637, after two buggy code regions were reported, developers found \nthree more places that were similarly inef.cient and .xed them altogether. Unfortunately, there were \nanother 50 code regions that violated the same ef.ciency rule and skipped developers checking, as shown \nin Table 4. Similarly, MySQL developers found and .xed 3 places that had the inef.ciency pattern shown \nin Figure 6, but missed the other 15 places. 113 out of these 125 PPPs exist in different .les or even \ndif\u00adferent modules where the original bugs exist, which is probably why they were missed by developers. \nThese PPPs end up in several ways: (1) 4 of them were .xed in later versions, which took 14 31 months; \n(2) 20 eventually disappeared, because the functions con\u00adtaining these PPPs were removed or re-implemented; \n(3) 101 still exist in the latest versions of the software, wasting computation re\u00adsources 12 89 months \nafter the original bugs were .xed. Lesson The above results show that developers do need support to systematically \nand automatically .nd similar performance bugs and .x them all at once. PPPs In The Latest Versions 2 \nof the 25 checkers are no longer applicable in the latest versions, because the functions in\u00advolved in \nthese checkers have been removed. The remaining 23 checkers are applied to the latest versions of corresponding \nsoft\u00adware and .nd 113 PPPs. Among them, 101 PPPs were inherited from the original buggy versions. The \nother 12 were introduced later. Lesson Developers cannot completely avoid the mistakes they made and \ncorrected before, which is understandable considering the large number of bugs in software. Speci.cation \nsystems and automated checkers can prevent developers from introducing old bugs into new code. PPPs In \nDifferent Software Applications An exciting result is that 8 out of 13 cross-application checkers have \nsuccessfully found previously unknown PPPs in the latest versions of applications that are different \nfrom where the rules came from. Most of these checkers re.ect common pitfalls in using li\u00adbrary functions. \nFor example, Figure 6 shows a pitfall of using String::indexof(). Apache-Ant developers made this mistake, \nand we found Apache-Struts developers also made a similar mistake. Apache32546 checker presents an interesting \ncase. In the original bug report, developers from Apache-Slide recognized that a small buffer size would \nseverely hurt the performance of java.io.InputStream.read (byte buffer[]) for reasonably large in\u00adput \n(e.g., larger than 50KB). Replacing their original 2KB buffer with a 200KB buffer achieved 80 times throughput \nimprovement in WebDav server. We .rst con.rmed that this rule is still valid. Our checker then found \n135 places in the latest versions of 36 software applications where similar mistakes were made. These \nplaces use small buffers (1KB 4KB) to read images or data .les from disk or web, and are doomed to performance \nlosses. Some checkers re.ect algorithm improvements and are also applicable to many applications. For \nexample, algorithm im\u00adprovements for string operations proposed by MySQL develop\u00aders (MySQL14637 and \nMySQL49491) also apply for Mozilla and Apache HTTPD. Cross-application checking also helps validate ef.ciency \nrules. For example, by comparing how java.util.zip.De.ater.de.ate() is used across applications, we found \nthat Ant developers under\u00adstanding of this API, re.ected by their discussion, was wrong. They .xed Apache45396 \nby coincidence. Lesson The above results show that there exist general inef.\u00adciency patterns that go \nbeyond one application, just like that for functional bugs [21]. Maintaining speci.cations and checkers \nfor these general patterns can signi.cantly save developers effort, and allow them to learn from other \ndevelopers and other software. We can even discover performance bugs in a software where no perfor\u00admance \npatch has ever been .led. Bad Practices Other than PPPs, some code regions identi.ed by the checkers \nare categorized as bad practices. For example, there are code regions very similar to the MySQL PPP shown \nin Figure 6, except that the calculation of end is not completely useless as end is used in places other \nthan the invocation of ismbchar. Clearly this practice is more likely to cause performance problems in \nthe future than directly using mysqlcs.mbmaxlen as the parameter for ismbchar function. Good Practices \nCode regions that have well followed the ef\u00ad.ciency rules are also identi.ed by slightly changed checkers. \nFor example, we found that in 13 places of various applications devel\u00adopers do use InputStream.read (byte \nbuffer[]) in a performance ef.cient way: buffer has a con.gurable size or a large size that suits the \nworkload (e.g., 64K in some Hadoop code). Lesson Violations to ef.ciency rules are not always rare com\u00adparing \nwith good practices. Previous techniques that use statistical analysis to infer functional rules [13, \n30] may not work for ef.\u00ad ciency rules. False Positives Our PPP detection is accurate. On average, the \nfalse-positive-vs-PPP rate is 1:4. The false positives mainly come from three sources. First, Python \ncheckers have no object-type information. There\u00adfore, some rules are applied to functions with right \nfunction names but wrong classes (e.g., Mozilla490742 and Apache32546). This is not a problem in LLVM \ncheckers. Second, some non-function rules are dif.cult to accurately ex\u00adpress and check, which leads \nto false positives in MySQL14637. Third, accurately checking some ef.ciency rules requires run\u00adtime and/or \nworkload information, which inevitably leads to false positives in our static checkers. False positives \nin Apache44408 and Apache48778 mostly belong to this category. These false positives can be largely eliminated \nby run-time checkers. Performance Results Our checkers are ef.cient. Each Python checker .nishes checking \n10 million lines of code within 90 sec\u00adonds. Our LLVM checkers are mainly applied to MySQL, Mozilla Firefox, \nand Apache HTTPD. It takes 4 1270 seconds for one LLVM checker to process one application. We tried \nunit testing on PPPs. The performance difference is signi.cant. For example, for programs that read images \nand .les Table 4: Checking results (BadPr: bad practice; F.P.: false positives; GoodPr: good practices. \nMore detailed de.nitions are presented in Section 6.4. - : not applicable. / : good-practice checker \ndoes not exist. ) ID Orig. Buggy Version Lastest Version of Same Softw. Latest Version of Diff. Softw. \nPPP BadPr F.P. GoodPr PPP BadPr F.P. GoodPr PPP BadPr F.P. GoodPr Mozilla 35294 5 0 10 / - - - / - - \n- / C++ Mozilla103330 2 0 0 117 0 0 0 7 - - - - C++ Mozilla258793 1 0 2 0 0 1 1 2 - - - - C++ Mozilla267506 \n6 0 0 9 3 0 0 19 - - - - C++ Mozilla311566 26 0 7 0 25 0 8 2 - - - - C++ Mozilla104962 0 0 0 1 3 0 0 \n12 0 0 0 0 C# Mozilla124686 0 1 0 14 0 0 0 1 0 0 0 0 C# Mozilla490742 1 0 3 5 0 0 0 4 - - - - JS MySQL14637 \n50 0 11 / 49 0 11 / 46 0 31 / C/C++ MySQL15811 15 20 5 5 16 20 7 7 - - - - C++ MySQL38769 0 0 1 5 - - \n- - - - - - C++ MySQL38941 1 4 0 2 1 4 0 2 3 5 2 0 C/C++ MySQL38968 3 0 1 38 2 0 2 43 - - - - C/C++ MySQL39268 \n7 0 0 4 7 0 0 18 - - - - C++ MySQL49491 1 0 0 0 1 0 0 2 3 0 0 0 C/C++ MySQL26152 0 0 0 0 0 0 0 0 0 0 \n1 4 C# MySQL45699 0 2 0 0 0 0 0 0 9 0 0 45 C#/Java Apache33605 0 2 0 / 0 2 0 / 0 5 0 / C Apache45464 \n3 0 0 47 3 0 0 67 - - - - C Apache19101 1 0 0 1 1 0 0 0 - - - - Java Apache32546 1 0 0 0 1 0 0 0 135 \n24 9 13 Java Apache34464 0 0 0 3 0 0 0 2 1 0 0 12 Java Apache44408 1 0 1 1 0 0 1 2 3 1 2 2 Java Apache45396 \n0 0 0 0 0 0 0 1 0 0 0 1 Java Apache48778 1 0 0 0 1 0 0 0 19 14 1 17 Java Total 125 29 41 252 113 27 30 \n191 219 49 46 94  Figure 6: PPPs we found in latest versions of original and different software (the \ngray area shows how these two PPPs should be .xed) using InputStream.read(byte buffer[]) with a 4KB-buffer \nparam\u00adeter, we can stably get 3 times throughput improvement through a 40K-buffer parameter. When we \nfeed the unit test with a 50MB .le, which is a quite common image-.le workload these days, the .le operation \ntime decreases from 0.87 second to 0.26 second, a de.\u00adnitely perceivable difference. As another example, \nthe Struts code shown in Figure 6 is from a utility function used for processing JSP .les. Our unit testing \nwith a 15K JSP .le shows that the simple patch can decrease latency by 0.1 second, a perceivable difference \nin interactive web applications. Whole system testing turns out to be dif.cult, as suggested by our characteristics \nstudy (Section 4.3). No PPP detected by our checkers belongs to the always-active category. Future performance\u00adoriented \ninput-generation tools will signi.cantly help performance testing and identify truly severe PPPs. Execution \nfrequency infor\u00admation can also help future static performance-bug detectors to rank the severity of \nPPPs.  6.6 Discussions Effectiveness of rule-based performance-bug detection Effort saving Rule-based \ndetection not only identi.es problems, but also suggests alternative implementations with better ef.ciency. \nThese alternative implementations often have small sizes and regu\u00adlar patterns, as shown in Figure 6, \nmaking PPP validation and .xing easy. It is also conceivable to enhance our checkers for automated PPP \n.xing. Improving performance These PPPs showed signi.cant per\u00adformance improvement than their alternative \nimplementations in our unit testing. Without .xing these PPPs, these unit-level perfor\u00admance losses could \naggregate into intolerable performance prob\u00adlems that are dif.cult to diagnose. This is especially signi.cant \nconsidering that many performance bugs are dif.cult to catch using other approaches. Maintaining code \nreadability Like those 109 patches studied earlier, most PPPs detected by us can be .xed through changes \nto a few lines of code, as shown in Figure 6. Even for the few complicated PPPs, wrapper-functions or \nmacros can easily address the patch-readability issue. Other usage Rules and checkers can serve as performance \nspeci.cations for future software development. They can aid in code maintenance when software evolves. \nDevelopers can also save PPPs to an inef.ciency list for future performance diagnosis. Of course, this \nis only a starting point for rule-based performance\u00adbug detection. We expect our experience to motivate \nfuture work on automatically generating rules, checkers, or even patches. Can these problems be detected \nby other tools? Copy-paste detectors Most PPPs that we found are not from copy-paste code regions and \ncannot be detected by text-matching tools [16, 28], as we can see in Figure 6. Rule violations are not \nrare. When developers misunderstand an API, they tend to make mistakes whenever they use this API. As \na result, these mistakes usually go beyond copy-paste code regions. Compiler optimization None of the \nbugs that provided the ef.\u00adciency rules could be optimized away by compilers used in Apache, MySQL, and \nMozilla. Many PPPs involve library functions and al\u00adgorithmic inef.ciency, and are almost impossible \nfor a compiler to optimize (Figure 6). Even for the few cases where compiler opti\u00ad mization might help \n(Figure 1), the required inter-procedural and points-to analyses are not scalable for real-world large \nsoftware. General rule-based bug detectors Ideas for detecting functional bugs can greatly bene.t and \ninspire future research on performance bug detection. However, many approaches cannot be directly ap\u00adplied. \nTools that automatically infer functional correctness rules [13, 30, 33] may not be suitable for ef.ciency \nrules, because rule violations are not rare, as shown in Table 4. In addition, many ef.\u00ad ciency rules \neither involve only one function or discourage multiple functions to be used together, making them unsuitable \nfor tools that focus on function correlations. 7. Related Work High performance computing (HPC) Performance \nis a central topic in the HPC community. Many tools for performance pro.ling and visualization have been \ndeveloped [20, 23, 35, 38]. However, the performance issues encountered there differ from those in main\u00adstream \nsoftware. For example, the inputs of HPC are more regular; it is relatively easy to achieve high code \ncoverage in testing HPC programs; load balancing and parallelism are much more important in HPC world. \nPerformance diagnosis A lot of progress has been made on pro.ling-based performance diagnosis [10, 19, \n25, 39]. A better un\u00ad derstanding of the common root causes, hidden locations, and prop\u00adagation patterns \nof performance bugs can help pro.ling research to save manual effort in performance diagnosis and bug \n.xing. Performance debugging in distributed systems has also received much attention. Many previous works \ntried to isolate bottlenecks in (semi-) blackbox systems [1], correlate system metrics with performance \nbehaviors [8, 48], avoid performance problems at run\u00ad time [52], and diagnose problems through logs [59]. \nIn distributed systems, a performance problem in a single node could spread into the entire system. Of \ncourse, there are also many performance problems unique to distributed environment. Performance testing \nPerformance testing includes load tests, stress tests, and soak tests. Most existing performance-testing \ntools [24] treat software as a blackbox. They mostly rely on devel\u00ad opers to manually design test cases \nand interpret results, or simply conduct stress testing by increasing the number of concurrent users \nand requests. Consequently, testing is costly and ineffective [9]. It is widely acknowledged in the industry \nthat better performance testing techniques are needed [40]. Tools for functional correctness Many ideas \nused for .ghting functional bugs would also work for .ghting performance bugs. The semantic patch project \n[43] provides a special language for de\u00ad velopers to write patches in and automatically pushes each patch \nto multiple locations. We, however, cannot directly use semantic patches for this work, because the existing \nsemantic patch frame\u00adwork does not provide suf.cient data-type information and data\u00ad.ow analysis support \nfor our checkers. It also only works for C pro\u00adgrams at this point. Of course, it is conceivable to extend \nthe idea of semantic patches to cover performance bugs, which coincides with the goal of our work. We \nwant to demonstrate the signi.cance of performance bugs, and explore the challenges and opportunities \nfaced when tackling performance bugs. Our work only looks at one side of performance problems. It complements \nworks on other critical issues, such as code optimiza\u00adtion, architecture design, system resource management, \nmitigating false sharing [32], and con.guration problems [2]. We omit further discussion due to the space \nconstraints. 8. Conclusions Performance bugs have largely been ignored in previous research on software \ndefects. Facing the increasing signi.cance of per\u00adformance bugs, this paper provides one of the .rst \nstudies on real-world performance bugs based on 109 bugs collected from .ve representative software suites. \nThe study covers a wide spec\u00adtrum of characteristics, and provides guidance for future research on performance-bug \navoidance, performance testing, bug detec\u00adtion, etc. Guided by this study, we further explore rule-based \nperformance-bug detection using ef.ciency rules implied by patches, and .nd many previously unknown performance \nproblems. This work is only a starting point for understanding and .ghting perfor\u00admance bugs. We expect \nit to deepen our understanding of per\u00adformance bugs and bring more attention to performance bugs. More \ninformation of this work is available at https://www. cs.wisc.edu/users/shanlu/performance-bugs. Acknowledgments \nWe would like to thank the anonymous reviewers for their invalu\u00adable feedback. We thank Andrea Arpaci-Dusseau, \nRemzi Arpaci-Dusseau, Mark Hill, Ben Liblit, Barton Miller, and Michael Swift for many helpful discussions \nand suggestions. Shan Lu is sup\u00adported by a Claire Boothe Luce faculty fellowship, and her research group \nis supported by NSF under grants CCF-1018180 and CCF\u00ad1054616. References [1] M. K. Aguilera, J. C. Mogul, \nJ. L. Wiener, P. Reynolds, and A. Muthi\u00adtacharoen. Performance debugging for distributed systems of black \nboxes. In SOSP, 2003. [2] M. Attariyan and J. Flinn. Automating con.guration troubleshooting with dynamic \ninformation .ow analysis. In OSDI, 2010. [3] W. Baek and T. M. Chilimbi. Green: a framework for supporting \nenergy-conscious programming using controlled approximation. In PLDI, 2010. [4] C. Cadar, D. Dunbar, \nand D. Engler. Klee: Unassisted and automatic generation of high-coverage tests for complex systems programs. \nIn OSDI, 2008. [5] V. Chipounov, V. Kuznetsov, and G. Candea. S2E: a platform for in\u00advivo multi-path \nanalysis of software systems. In ASPLOS, 2011. [6] A. Chou, B. Chelf, D. Engler, and M. Heinrich. Using \nmeta-level compilation to check FLASH protocol code. In ASPLOS, 2000. [7] A. Chou, J. Yang, B. Chelf, \nS. Hallem, and D. R. Engler. An empirical study of operating system errors. In SOSP, 2001. [8] I. Cohen, \nM. Goldszmidt, T. Kelly, J. Symons, and J. S. Chase. Corre\u00adlating instrumentation data to system states: \na building block for auto\u00admated diagnosis and control. In OSDI, 2004. [9] M. Corporation. Performance \nTesting Guidance for Web Applications. Microsoft Press, 2007. [10] A. Diwan, M. Hauswirth, T. Mytkowicz, \nand P. F. Sweeney. Trace\u00adanalyzer: a system for processing performance traces. Softw. Pract. Exper., \nMarch 2011. [11] B. Dufour, B. G. Ryder, and G. Sevitsky. A scalable technique for characterizing the \nusage of temporaries in framework-intensive java applications. In FSE, 2008. [12] R. F. Dugan. Performance \nlies my professor told me: the case for teaching software performance engineering to undergraduates. \nIn WOSP, 2004. [13] D. Engler, D. Y. Chen, S. Hallem, A. Chou, and B. Chelf. Bugs as deviant behavior: \nA general approach to inferring errors in systems code. In SOSP, 2001. [14] M. Ernst, A. Czeisler, W. \nG. Griswold, and D. Notkin. Quickly detecting relevant program invariants. In ICSE, 2000. [15] Fortify. \nHP Fortify Static Code Analyzer (SCA). https://www.fortify.com/products/hpfssc/source-code-analyzer.html. \n[16] M. Gabel, J. Yang, Y. Yu, M. Goldszmidt, and Z. Su. Scalable and systematic detection of buggy inconsistencies \nin source code. In OOPSLA, 2010. [17] P. Godefroid, N. Klarlund, and K. Sen. Dart: directed automated \nrandom testing. In PLDI, 2005. [18] S. Gulwani, K. K. Mehra, and T. Chilimbi. Speed: precise and ef.cient \nstatic estimation of program computational complexity. In POPL, 2009. [19] M. Hauswirth, A. Diwan, P. \nF. Sweeney, and M. C. Mozer. Automating vertical pro.ling. In OOPSLA, 2005. [20] J. K. Hollingsworth, \nR. B. Irvin, and B. P. Miller. The integration of application and system based metrics in a parallel \nprogram perfor\u00admance tool. In PPOPP, 1991. [21] D. Hovemeyer and W. Pugh. Finding bugs is easy. In OOPSLA, \n2004. [22] InfoWorld. Top 10 open source hall of famers. http://www.infoworld.com/d/open-source/top-10-open-source\u00adhall-famers-848. \n[23] R. B. Irvin and B. P. Miller. Mapping performance data for high-level and data views of parallel \nprogram performance. In ICS, 1996. [24] JMeter. Java desktop application designed for load tests. http://jakarta.apache.org/jmeter. \n[25] M. Jovic, A. Adamoli, and M. Hauswirth. Catch me if you can: performance bug detection in the wild. \nIn OOPSLA, 2011. [26] C. Killian, K. Nagaraj, S. Pervez, R. Braud, J. W. Anderson, and R. Jhala. Finding \nlatent performance bugs in systems implementa\u00adtions. In FSE, 2010. [27] C. Lattner and V. Adve. LLVM: \nA compilation framework for lifelong program analysis &#38; transformation. In CGO, 2004. [28] Z. Li, \nS. Lu, S. Myagmar, and Y. Zhou. CP-Miner: A Tool for Finding Copy-paste and Related Bugs in Operating \nSystem Code. In OSDI, 2004. [29] Z. Li, L. Tan, X. Wang, Y. Zhou, and C. Zhai. Have things changed now?: \nan empirical study of bug characteristics in modern open source software. In ASID, 2006. [30] Z. Li and \nY. Zhou. PR-Miner: Automatically extracting implicit programming rules and detecting violations in large \nsoftware code. In FSE, Sept 2005. [31] S. Liu, K. Pattabiraman, T. Moscibroda, and B. G. Zorn. Flikker: \nsaving DRAM refresh-power through critical data partitioning. In ASPLOS, 2011. [32] T. Liu and E. D. \nBerger. Precise detection and automatic mitigation of false sharing. In OOPSLA, 2011. [33] V. B. Livshits \nand T. Zimmermann. Dynamine: Finding common error patterns by mining software revision histories. In \nFSE, 2005. [34] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from mistakes a comprehensive study of \nreal world concurrency bug characteristics. In ASPLOS, 2008. [35] J. Mellor-Crummey, R. J. Fowler, G. \nMarin, and N. Tallent. Hpcview: A tool for top-down analysis of node performance. J. Supercomput., 23(1):81 \n104, 2002. [36] N. Meng, M. Kim, and K. S. McKinley. Systematic editing: generating program transformations \nfrom an example. In PLDI, 2011. [37] Microsoft. MSDN SAL annotations. http://msdn2.microsoft.com/en\u00adus/library/ms235402.aspx. \n[38] B. P. Miller, M. D. Callaghan, J. M. Cargille, J. K. Hollingsworth, R. B. Irvin, K. L. Karavanic, \nK. Kunchithapadam, and T. Newhall. The paradyn parallel performance measurement tool. Computer, 28(11), \n1995. [39] N. Mitchell. The diary of a datum: an approach to modeling runtime complexity in framework-based \napplications. In ECOOP, 2006. [40] I. Molyneaux. The Art of Application Performance Testing: Help for \nProgrammers and Quality Assurance. O Reilly Media, 2009. [41] G. E. Morris. Lessons from the colorado \nbene.ts management sys\u00adtem disaster. www.ad-mkt-review.com/publichtml/air/ai200411.html, 2004. [42] G. \nMuller, Y. Padioleau, J. L. Lawall, and R. R. Hansen. Semantic patches considered helpful. Operating \nSystems Review, 40(3):90 92, 2006. [43] N. Palix, G. Thomas, S. Saha, C. Calv`es, J. Lawall, and G. Muller. \nFaults in linux: ten years later. In ASPLOS, 2011. [44] S. E. Perl and W. E. Weihl. Performance assertion \nchecking. In SOSP, 1993. [45] T. Richardson. 1901 census site still down after six months. http://www.theregister.co.uk/2002/07/03/ \n1901 census site still down/. [46] C. J. Rossbach, O. S. Hofmann, and E. Witchel. Is transactional programming \nactually easier? In PPoPP, 2010. [47] A. Shankar, M. Arnold, and R. Bodik. Jolt: lightweight dynamic \nanalysis and removal of object churn. In OOPSLA, 2008. [48] K. Shen, C. Stewart, C. Li, and X. Li. Reference-driven \nperformance anomaly identi.cation. In SIGMETRICS, 2009. [49] R. L. Sites. Identifying dark latency. [50] \nC. U. Smith and L. G. Williams. Software performance antipatterns. In Proceedings of the 2nd international \nworkshop on Software and performance, 2000. [51] Stefan Bodewig. Bug 45396: There is no hint in the javadocs. \nhttps://issues.apache.org/ bugzilla/show bug.cgi?id=45396#c4. [52] C. Stewart, K. Shen, A. Iyengar, and \nJ. Yin. Entomomodel: Under\u00adstanding and avoiding performance anomaly manifestations. In MAS-COTS, 2010. \n[53] M. Sullivan and R. Chillarege. A comparison of software defects in database management systems and \noperating systems. In FTCS, 1992. [54] L. Torvalds. Sparse -a semantic parser for c. http://www.kernel.org/pub/software/devel/sparse/. \n[55] J. S. Vetter and P. H. Worley. Asserting performance expectations. In Supercomputing, 2002. [56] \nwikipedia. Chi-squared test. http://en.wikipedia.org/wiki/Chi\u00adsquared test. [57] G. Xu, M. Arnold, N. \nMitchell, A. Rountev, and G. Sevitsky. Go with the .ow: pro.ling copies to .nd runtime bloat. In PLDI \n09, 2009. [58] G. Xu, N. Mitchell, M. Arnold, A. Rountev, E. Schonberg, and G. Se\u00advitsky. Finding low-utility \ndata structures. In PLDI, 2010. [59] W. Xu, L. Huang, A. Fox, D. Patterson, and M. I. Jordan. Detecting \nlarge-scale system problems by mining console logs. In SOSP, 2009. [60] A. Zeller. Isolating cause-effect \nchains from computer programs. In FSE, 2002.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Developers frequently use inefficient code sequences that could be fixed by simple patches. These inefficient code sequences can cause significant performance degradation and resource waste, referred to as performance bugs. Meager increases in single threaded performance in the multi-core era and increasing emphasis on energy efficiency call for more effort in tackling performance bugs.</p> <p>This paper conducts a comprehensive study of 110 real-world performance bugs that are randomly sampled from five representative software suites (Apache, Chrome, GCC, Mozilla, and MySQL). The findings of this study provide guidance for future work to avoid, expose, detect, and fix performance bugs.</p> <p>Guided by our characteristics study, efficiency rules are extracted from 25 patches and are used to detect performance bugs. 332 previously unknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, including 219 performance problems found by applying rules across applications.</p>", "authors": [{"name": "Guoliang Jin", "author_profile_id": "81470644031", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P3471154", "email_address": "aliang@cs.wisc.edu", "orcid_id": ""}, {"name": "Linhai Song", "author_profile_id": "81485652652", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P3471155", "email_address": "songlh@cs.wisc.edu", "orcid_id": ""}, {"name": "Xiaoming Shi", "author_profile_id": "81502726382", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P3471156", "email_address": "xiaoming@cs.wisc.edu", "orcid_id": ""}, {"name": "Joel Scherpelz", "author_profile_id": "81482662204", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P3471157", "email_address": "joel.scherpelz@gmail.com", "orcid_id": ""}, {"name": "Shan Lu", "author_profile_id": "81100052818", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P3471158", "email_address": "shanlu@cs.wisc.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254075", "year": "2012", "article_id": "2254075", "conference": "PLDI", "title": "Understanding and detecting real-world performance bugs", "url": "http://dl.acm.org/citation.cfm?id=2254075"}