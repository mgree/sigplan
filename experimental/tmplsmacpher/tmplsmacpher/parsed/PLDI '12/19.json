{"article_publication_date": "06-11-2012", "fulltext": "\n Parallelizing Top-Down Interprocedural Analyses Aws Albarghouthi * Rahul Kumar Aditya V. Nori University \nof Toronto Microsoft Corporation Microsoft Research India aws@cs.toronto.edu rahulku@microsoft.com adityan@microsoft.com \nSriram K. Rajamani Microsoft Research India sriram@microsoft.com Abstract Modularity is a central theme \nin any scalable program analysis. The core idea in a modular analysis is to build summaries at proce\u00addure \nboundaries, and use the summary of a procedure to analyze the effect of calling it at its calling context. \nThere are two ways to perform a modular program analysis: (1) top-down and (2) bottom\u00adup. A bottom-up \nanalysis proceeds upwards from the leaves of the call graph, and analyzes each procedure in the most \ngeneral calling context and builds its summary. In contrast, a top-down analysis starts from the root \nof the call graph, and proceeds downward, an\u00adalyzing each procedure in its calling context. Top-down \nanalyses have several applications in veri.cation and software model check\u00ading. However, traditionally, \nbottom-up analyses have been easier to scale and parallelize than top-down analyses. In this paper, we \npropose a generic framework, BOLT, which uses MapReduce style parallelism to scale top-down analyses. \nIn particular, we consider top-down analyses that are demand driven, such as the ones used for software \nmodel checking. In such analy\u00adses, each intraprocedural analysis happens in the context of a reach\u00adability \nquery. A query Q over a procedure P results in query tree that consists of sub-queries over the procedures \ncalled by P . The key insight in BOLT is that the query tree can be explored in parallel using MapReduce \nstyle parallelism the map stage can be used to run a set of enabled queries in parallel, and the reduce \nstage can be used to manage inter-dependencies between queries. Iterating the map and reduce stages alternately, \nwe can exploit the parallelism inherent in top-down analyses. Another unique feature of BOLT is that \nit is parameterized by the algorithm used for intraprocedural analysis. Several kinds of analyses, including \nmay-analyses, must\u00adanalyses, and may-must-analyses can be parallelized using BOLT. We have implemented \nthe BOLT framework and instantiated the intraprocedural parameter with a may-must-analysis. We have run \nBOLT on a test suite consisting of 45 Microsoft Windows device drivers and 150 safety properties. Our \nresults demonstrate * This author performed the work reported here during a summer internship at Microsoft \nResearch India. Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00 Figure 1. Black box view of the BOLT framework. an average speedup of 3.71x and a maximum \nspeedup of 7.4x (with 8 cores) over a sequential analysis. Moreover, in several checks where a sequential \nanalysis fails, BOLT is able to successfully complete its analysis. Categories and Subject Descriptors \nD.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation assertion checkers, correct\u00adness proofs, \nformal methods, model checking; D.2.5 [Software Engineering]: Testing and Debugging symbolic execution, \ntest\u00ading tools; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about \nPrograms assertions, pre-and post-conditions General Terms Parallelism, Testing, Veri.cation Keywords \nAbstraction re.nement; Interprocedural analysis; Soft\u00adware model checking 1. Introduction Scalable program \nanalyses work by exploiting the modular struc\u00adture of programs. Almost every interprocedural analysis \nbuilds summaries at procedure boundaries, and uses the summary of a pro\u00adcedure at its calling contexts, \nin order to scale to large programs. Broadly, interprocedural analyses can be classi.ed as either top\u00addown \nor bottom-up, depending on whether the analysis proceeds from callers to callees or vice-versa. Bottom-up \nanalyses have been traditionally easier to scale. They work by processing the call graph of the program \nupwards from the leaves. In a bottom-up analysis, before a procedure Pi is an\u00adalyzed, all the procedures \nPj that are called by Pi are analyzed, and for each Pj its summary SPj is computed, typically without \nconsidering the calling contexts of Pj . Then, during the analysis of Pi the summary SPj is used to calculate \nthe effects of calling Pj , instead of the body of Pj . One of the signi.cant advantages of bottom-up \nanalyses is their decoupling between callers of a proce\u00addure P and the analysis of the body of P , which \nenables paralleliza\u00adtion. For instance, the designers of the SATURN tool [1], a scalable bottom-up analysis, \nsay that another advantage of analyzing pro\u00adcedures separately is that the process is easily parallelized, \nwith parallelism limited only by analysis dependencies between different procedures. We use compute clusters \nof 40-100 cores to run Saturn analyses in parallel and normally achieve 80-90% ef.ciency . In contrast, \na top-down analysis starts from the root of the call graph and proceeds downward, analyzing each procedure \nin its calling context. Top-down analyses have several applications in veri.cation and software model \nchecking [2, 3, 19], as well as dynamic test generation [16, 30]. However, top-down analyses have been \nmore challenging to scale and parallelize. During a top-down analysis, the analysis of a procedure Pi \nis done separately for each calling context, leading to repeated analysis of each procedure, and .ne \ngrained dependencies between analysis instances. Hence, top\u00addown analyses are harder to scale. However, \ntop-down analyses do have precision advantages. Since each analysis of a procedure Pi is done with respect \nto a calling context, the summary built for that context can be more precise. Thus, software model checkers \nsuch as SLAM [4] and BLAST [10], which employ a top-down analysis, are able to be very precise, and have \nvery low false error rates, but only scale to programs of size 100K lines of code [7]. Thus, it is natural \nto ask if we can parallelize and scale top\u00addown analyses. In particular, we consider top-down analyses \nthat are demand driven, such as the ones used for software model checking. In such analyses, each intraprocedural \nanalysis happens in the context of a query. A query Q over a procedure P results in sub-queries over \nprocedures called by P . The query Q is in a blocked state (that is, it cannot make progress) until at \nleast one of these sub-queries can be answered using a summary. When such a summary is available for \none or more of the sub-queries, the parent query Q transitions to a ready state where it can continue \nto execute, and may produce new sub-queries and get blocked again. When the analysis of a query Q is \nconclusive, it moves to a done state. It is important to note that (this will be illustrated later) a \nquery Q can move to a done state, even before all of its sub-queries are done. Therefore, in this situation, \nthe remaining (transitive) sub-queries of q can be stopped and garbage collected. Thus, the dependencies \nbetween query instances are more intricate and detailed during a top-down analysis. In this paper, we \npropose a generic framework, BOLT, which uses MapReduce [13] style parallelism to scale top-down analyses. \nThe key insight in BOLT is that the query tree can be explored in parallel as follows: The map stage \nis used to run a set of ready queries in parallel, which can potentially result in a new set of sub-queries, \nand  the reduce stage is used to manage interdependencies be\u00adtween queries, assess which parent queries \ncan be moved to the ready state from the blocked state, and which queries can be garbage collected because \nthey are no longer necessary for the parents that originated these queries.  By iterating the map and \nreduce stages alternately, BOLT can ex\u00adploit the parallelism inherent in any top-down analysis. BOLT \nis parameterized by an intraprocedural analysis algo\u00adrithm, which we shall henceforth call PUNCH, used \nto analyze a single procedure. BOLT initially receives a program P and a reach\u00adability query Qmain over \nthe main procedure main of P and it em\u00adploys PUNCH to process Qmain. PUNCH explores paths in main ei\u00adther \nforward or backward, or using combination of both. It can use an overapproximate analysis, an underapproximate \nanalysis, or a combination of both. Whenever it encounters a method call Pi, PUNCH formulates a sub-query \nQi for Pi, which it needs to know about Pi in order to answer the query it was asked, namely Qmain. We \nsay that Qi is a child of the parent query Qmain. PUNCH .rst looks for a summary which can answer Qi \nin a database of sum\u00admaries SUMDB. If a suitable summary is found, it answers Qi us\u00ading that summary \nand moves on. If not, PUNCH sets the query Qi to the ready state and adds it to the set of queries it \nwill return, and explores other paths in main, repeating the same strategy to han\u00addle any procedure calls \nit encounters on these paths. The PUNCH call on the query Qmain .nishes when PUNCH cannot perform any \nfurther analysis in main without getting answers to queries it has made to callees of main. At this point, \nit returns all the sub-queries it has generated (which are all in the ready state), and Qmain itself, \nwhich it sets to blocked state. The next map stage applies PUNCH to all the queries that are in the ready \nstate in parallel, and the sub\u00adsequent reduce stage then processes the answers produced by the map stage, \nremoving completed and redundant queries, and inform\u00ading parent queries when their children have produced \nanswers. This process continues until Qmain returns an answer. Figure 1 shows an overview of the BOLT \nframework which uses multiple instances of PUNCH in parallel and a database of procedure summaries SUMDB \nthat stores results produced by PUNCH to avoid recomputing simi\u00adlar queries over the same procedure. \n In our exposition of BOLT, we provide a formal speci.cation of the parameter PUNCH that any correct \ninstantiation of BOLT should respect. PUNCH can be instantiated to capture different kinds of analyses. \nAn instantiation of PUNCH to a may-analysis or an overapproximation-based analysis, using may summaries, \ncan parallelize analyses such as SLAM [4] and BLAST [10]. An instan\u00adtiation of PUNCH to a must-analysis \nor an underapproximation\u00adbased analysis, using must summaries, can parallelize analyses such as DART \n[17] and CUTE [35]. We present an instantiation of PUNCH that uses a may-must-analysis , combining overapproxi\u00admations \nand underapproximations, to parallelize algorithms in the family of DASH [9, 19]. Our contributions are \nsummarized as follows.  BOLT: a generic framework for parallelizing top-down inter\u00adprocedural analyses. \nIn particular, the framework targets demand driven analyses such as the ones used in software model checkers. \n An instantiation of the BOLT framework with an intraprocedu\u00adral analysis algorithm that uses a may-must \nanalysis combining testing (in the style of DART and CUTE) and abstraction (in the style of SLAM and \nBLAST).  A modular implementation of BOLT, where different intrapro\u00adcedural algorithms can be plugged-in \nand parallelized seamlessly; and an extensive experimental evaluation of BOLT, using our may\u00admust-analysis \non a number of Microsoft Windows device drivers and safety properties, that demonstrates an average speedup \nof 3.71x and a maximum speedup of 7.4x using 8 cores (in compari\u00adson with a sequential analysis). Using \nBOLT, we have been able to analyze several driver-property pairs that sequential analyses have been unable \nto analyze previously.  The rest of the paper is organized as follows: Section 2 provides an overview \nof BOLT and motivates its style of parallelism. Section 3 is a formal description of the BOLT framework. \nSection 4 discusses the various instantiations of the intraprocedural parameter PUNCH. Section 5 discusses \nour implementation and evaluation of BOLT. Section 6 compares BOLT with related work. Finally, Section \n7 concludes the paper and outlines directions for future research.  Figure 2. (a) Procedure main of \nexample program, (b) State machine of a query Qi, and (c) Illustration of BOLT on (a). 2. Motivation \nInthissection,weillustratetheoperationof BOLT onatoyprogram and also motivate BOLT s style of parallelism \nby examining a real\u00adworld application. 2.1 Illustration of BOLT on a Toy Program Consider the program \nshown in Figure 2(a) with main procedure main. Procedure main invokes three other procedures, bar, foo, \nand baz, which only have their signatures shown. Our goal is to check if there exists some input to main \nthat violates the assertion assert(y > 0) at the end of the procedure. This check is en\u00adcoded as the \nfollowing query (formally de.ned in Section 3) over the procedure main . ? Qmain = (true =.main y<= 0) \n(1) This query asks the question if there is an execution through the procedure main starting in any \ninput state (denoted by the pre\u00adcondition true) and ending in a state satisfying the error condition \ny <= 0. As shown in Figure 2(c), BOLT alternates between the map and reduce stages in the style of the \nMapReduce framework [13]. Speci.cally, BOLT operates on this example as follows. The .rst stage is the \nmap stage where BOLT applies PUNCH to the main query Qmain , which is initially in the Ready state, i.e., \nready to be processed (the state machine for a lifespan of a query is shown in Figure 2(b)). This results \nin the new queries Qfoo , Qbar and Qbaz , which are children of Qmain , all of which are in the Ready \nstate: ? Qfoo = (true =.foo ret <= -5) (2) ? Qbar = (true =.bar ret <= -5) (3) Qbaz = (p baz <= -10 =.? \nbaz ret <= -5) (4) Here, we assume that the intraprocedural analysis instantiated by PUNCH is able to \nascertain that the assertion assert(y > 0) in the procedure main holds if and only if each of the procedures \nfoo, bar, and baz, return a value greater than -5. Note that Qbaz has the precondition p baz <= -10, \nsince baz is only called with inputs less than or equal to -10. The query Qmain is now in the Blocked \nstate, because it needs an answer from at least one of its child sub-queries before it can make progress. \nThe reduce stage analyzes if any interdependencies between the queries have been resolved, and in this \ncase, none are resolved, so all the queries remain in their respective states (the .rst reduce stage \nis essentially a no-op). The second map stage applies PUNCH, in parallel, to each of the Ready queries \nQfoo , Qbar , and Qbaz . We assume that the .rst two queries, Qfoo and Qbar , complete at this stage \n(perhaps due to foo and bar being leaves in the call-graph) and therefore move to the Done state, and \nQbaz moves to the Blocked state and generates a new query Qroo . The Done state implies that the analysis \nof the query is complete. BOLT stores the results of Done queries as procedure summaries in a summary \ndatabase SUMDB, in order to avoid recomputing them. A summary can be either a must summary, representing \nan underapproximation of the procedure and containing a path to error states, or a not-may summary, representing \nan overapproximation of the procedure and excluding paths to error states [19]. Since Qfoo and Qbar have \nmoved to a Done state, the reduce stage now sets Qmain to a Ready state, and thus enables it to be processed \nby PUNCH in the next map stage, now that some of its  Figure 3. Potential of parallelism illustrated \non a device driver. child sub-queries have returned results (summaries). The reduce stage also deletes \nall Done queries (and their descendants). In this case, it deletes Qbar and Qfoo . In subsequent stages \n(not shown in the .gure), it may so happen that Qmain completes due to the answers it gets from the sub-queries \nQfoo and Qbar . If that happens, the next reduce stage will garbage collect the remaining queries such \nas Qbaz and Qroo , since their answers are no longer required (i.e., we have been able to answer Qmain \neven without requiring the answers to these sub-queries). To summarize, BOLT alternates between two phases: \nthe par\u00adallel map stage, which applies PUNCH in parallel to veri.cation queries, and the reduce stage, \nwhich performs housekeeping, get\u00adting rid of unnecessary queries and reactivating blocked ones. The process \ncontinues until the main query Qmain is answered.  2.2 Opportunity for Parallelism Since our goal is \nto parallelize top-down analyses, it is important to ascertain the amount of parallelism available in \nthe query tree induced by a top-down analysis for real-world programs. Thus, for these programs, it is \nuseful to know the number of Ready queries in each successive stage of the MapReduce process in order \nto understand the level of parallelism that is available. To get a rough idea of the amount of parallelism \navailable in analyzing device drivers, which are our target application, we in\u00adstrumented a sequential \ntop-down analysis and recorded the total number of Ready sub-queries over the lifetime of a query for \na dispatch routine (the main procedure) in a device driver. Figure 3 shows this number plotted against \ntime, for a typical device driver. At its peak, the query has around 50 Ready sub-queries. On this driver, \na sequential top-down analysis would analyze each of these Ready sub-queries one at a time. In contrast, \nBOLT exploits the fact that these Ready queries can be analyzed independently, and, therefore, parallelizes \nexecution of these sub-queries on the avail\u00adable processor cores. Once these sub-queries run in parallel, \nthey will in turn generate more sub-queries with more opportunity for exploiting parallelism. Ultimately, \nthe speedup obtained by BOLT depends on several other factors (see Section 5.1 for a thorough evaluation), \nbut the above methodology is useful to understand the amount of parallelism inherently available in any \ntop-down anal\u00adysis, and the gains we can expect out of parallelizing the analysis using BOLT. 3. The \nBOLT Framework In this section, we describe the BOLT framework together with its intraprocedural parameter \nPUNCH for solving reachability queries. 3.1 Preliminaries Programs A program P is a set of procedures \n{P0,...,Pn}, where P0 is the main procedure (entry point). A procedure Pi is a tuple (Vi,Ni,Ei,n i 0 \n,n ix,.i), where Vi is the disjoint union of the set of local variables ViL of Pi and the set of global \nvariables VG of P.  Ni is the set of control nodes (locations).  Ei : Ni \u00d7 Ni is the set of edges between \ncontrol nodes.  ni 0 ,n ix . Ni are the entry and exit locations, respectively.  .i : Ei -. Stmt is \na labeling function, where Stmt is the set of program statements over Vi. Statements in Stmt are either \nsimple statements or call statements. A simple statement in a procedure Pi is an assignment statement \nx= E or an assume statement assume(Q), where x is a variable in Vi, E is an expression over the variables \nVi, and Q is a Boolean expression over the variables Vi. A call statement to procedure Pj is of the form \ncall Pj .  We assume, w.l.o.g., that communication between procedures is performed via the global variables \nVG, and for each procedure Pi, there does not exist a node n . Ni such that (nix ,n) . Ei. Program Model \nA con.guration of a procedure Pi is a pair (n, s), where n . Ni and the state s is a valuation of variables \nVi of Pi. The set of all states of Pi is denoted by SPi . Every edge e . Ei is a relation Ge . SPi \u00d7 \nSPi de.ned by the standard semantics of the statement .i(e). The initial con.gurations of a procedure \nPi are {(ni 0,s) |s . SPi }. From a con.guration (n, s), P'i can execute a state\u00adment by traversing some \nedge e =(n, n) . Ei and reaching ' a con.guration (n,s'), where (s, s') . Ge. We say that a con\u00ad ' \n.guration (n, s) can reach another con.guration (n,s'), where ' n, n. Ni if and only if there exists \na sequence of edges in ' (n, n1), (n1,n2),..., (nm,n) . Ei which if executed from state s leads to state \ns' . Procedure Summaries For any procedure Pi, let .1 and .2 be formulae representing sets of states \nin 2SPi . Then, we have two types of summaries for Pi, must summaries and not-may sum\u00admaries de.ned as \nfollows [19]. must Must Summary: (.1 =. Pi .2) is a must summary for Pi if x and only if every exit \ncon.guration (ni ,s'), where s' . .2, is reachable from some initial con.guration (ni 0,s), where s . \n.1. \u00acmay Not-may Summary: (. =. Pj .2) is a not-may summary for Pi if and only if every initial con.guration \n(ni 0,s), where x s . .1, cannot reach any exit con.guration (ni ,s'), where s' . .2. Queries A query \nQi over some procedure Pj is de.ned as a 4\u00adtuple (qi,si,pi, Oi), where ? qi is a reachability question \nof the form (.1 =.Pj .2), asking if a procedure Pj starting in a con.guration in {(nj 0,s) | s . .1} \ncan reach a con.guration in {(njx,s) | s . .2}. si .{Ready, Blocked, Done} is the query state. pi is \nthe index of the parent query Qpi of Qi.  Oi is a veri.cation object that maintains the internal state \nof a query. The exact nature of this object depends on the kind of analysis being performed by BOLT (may-analysis/must\u00adanalysis/may-must-analysis). \nWe will formally describe this in Section 4.  A procedure summary S can be used to answer a reachability \nquestion (.1 =.? Pj .2) in either of the following ways: must Answer = yes , if S = (. 1 =. Pj . 2), \nwhere . 1 . .1 and .2 n . 2= \u00d8, \u00acmay Answer = no , if S = (. 1 =. Pj . 2), where .1 . . 1 and .2 . . \n2. Intuitively, a must-summary S answers a reachability question (.1 =.? Pj .2) with a yes, there is \nan execution from a state in .1 to a state in .2 through Pj . On the other hand, if S is a not-may summary, \nthen it answers the reachability question with a no, there are no executions through Pj from any state \nin .1 to any state in .2 . A veri.cation question for a program P is a query Q0 = (q0,s0,p0, O0) over \nits main procedure P0, where q0 =? = (.1 .P0 .2), .2 describes undesirable (error) states, and p0 is \nunde.ned, since the initial query Q0 does not have any parent queries.  3.2 PUNCH: The Intraprocedural \nParameter BOLT is parameterized by an intraprocedural analysis algorithm PUNCH for manipulating queries. \nPUNCH takes a query Qi in the Ready state, and the goal is to either compute a summary that answers the \nreachability question of Qi or produce new queries required to answer Qi. PUNCH stores procedure summaries \nthat it computes in a database SUMDB. PUNCH also queries SUMDB for procedure summaries in order to avoid \nrecomputing answers to queries. The formal speci.cation of PUNCH is described below. Input: Qi =(qi,si,pi, \nOi). Output: Set of queries R. Precondition: si = Ready. Postcondition: R = {Q ' i}. C, where Qi ' =(qi,s \ni' ,pi, O ' ) and 1. (s ' i = Done)=. (C = \u00d8), and 2. (si ' .{Blocked, Ready})=. .(qj ,sj ,pj , Oj ) \n. C \u00b7 pj = i . sj = Ready  PUNCH takes a query Qi =(qi,si,pi, Oi) as input and re\u00adturns a set of queries \nR. If PUNCH successfully analyzes Qi, it returns a copy Q ' i of Qi in a Done state (formula 1 of the \nabove postcondition), and adds a summary that answers qi to SUMDB, as a side effect. Otherwise, it returns \na copy Q ' i of Qi that is Ready or Blocked, and a set of child sub-queries C of Q ' i (for\u00admula 2 of \nthe above postcondition). Every child sub-query Qj = (qj ,sj ,pj , Oj ) . C is uniquely identi.ed by \nits index j. If a query Qi is in the Blocked state, PUNCH cannot make any progress with Qi and can only \ncontinue when one of its children has an answer (i.e., the child reaches Done state and adds a summary \nto SUMDB). If Qi is in the Ready state, PUNCH can perform more processing on Qi. Note that the only side-effect \nof PUNCH is the addition of summaries to SUMDB. BOLT expects PUNCH to operate as follows. First, PUNCH \nat\u00adtempts to answer a query Qi on some procedure Pj by analyzing Pj using the summaries of the procedures \nPj calls that are stored in SUMDB. If it fails to .nd appropriate summaries for those pro\u00adcedures, it \nmoves Qi to a Blocked state and produces a number of new sub-queries C. The query Qi remains Blocked \nuntil one of its sub-queries is Done (and, therefore, has a summary in SUMDB). Alternatively, PUNCH may \ndecide to preempt an ongoing analysis of Qi and return Qi in a Ready state. In Section 4, we present \nsev\u00aderal instantiations of PUNCH. 1: function BOLT(Program P, Query Q0 =(q0,s0,p0, O0)) 2: QSet = {Q0} \n3: while \u00ac.(qi,si,pi, Oi) . QSet \u00b7 si = Done . qi = q0 do MAP: 4: QSet ' .{PUNCH(Qi) | Qi . QSet.si \n= Ready} 5: QSet . QSet ' .{Qi | Qi . QSet . si= Ready} REDUCE: 6: for all Qi =(qi,si,pi, Oi) . QSet \ndo 7: if si = Done then 8: if spi = Blocked then set spi to Ready 9: (* remove subtree rooted at Qi from \nQSet *) 10: QSet . QSet \\ Descendants(Qi) 11: if there exists a must summary for q0 in SUMDB then 12: \nreturn Error Reachable 13: else 14: return Program is Safe Figure 4. BOLT algorithm  3.3 BOLT: The Parallel \nTop-Down Veri.cation Framework BOLT uses MapReduce [13] and is formally described in Figure 4. BOLT takes \nas input a program P and a veri.cation question Q0 over the main procedure P0 of P. The algorithm starts \nwith a set of queries QSet that is initialized to the veri.cation question (line 2). Each iteration (lines \n3 10) is divided into two stages: 1. The MAP stage (lines 4 5): Applies PUNCH, in parallel, to each \nquery Qi . QSet that is in Ready state. QSet ' is then assigned the union of all of the results returned \nby all calls to PUNCH. This is denoted by parallel union symbol . The only resource shared by parallel \ninstances of PUNCH is the summary database SUMDB.  2. The REDUCE stage (lines 6 10): Removes redundant \nand Done queries from QSet. The function Descendants(Qi) is used to denote the image of the transitive \nclosure of the parent\u00adchild relation starting from Qi. For every query Qi s.t. si = Done, all descendants \nof Qi, including Qi, are removed from QSet, since they were added to QSet to help answer Qi, and now \nthat si = Done, they are no longer required. Additionally, the REDUCE stage sets parents of Done queries \nto Ready state, as new results about their child queries have been added to SUMDB by PUNCH, potentially \nenabling parent queries to be Done in the next MAP stage.  The algorithm keeps iterating and executing \nthe MAP and REDUCE stages until q0 is answered. For a query Qi, when si = Done, SUMDB either contains \na must summary or a not-may summary that answers qi (by de.nition of PUNCH). Therefore, when BOLT exits \nthe loop at line 3, we know that there exists a summary that answers the reachability question q0. If \nq0 is answered by a must summary, then BOLT returns Error Reachable , as there is an execution to the \nerror states de.ned in q0. On the other hand, if q0 is answered by a not-may summary, then BOLT returns \nProgram is Safe , since the not-may summary precludes any execution to an error state in q0, Example \n1. Recall the example from Section 2.1. In the second iter\u00adation of BOLT, the MAP stage applies PUNCH \nto the Ready queries in QSet: Qfoo ,Qbar and Qbaz . That is, in the second iteration, QSet is assigned \nas follows: QSet ' . PUNCH(Qfoo ) . PUNCH(Qbar ) . PUNCH(Qbaz ) '' ' = {Qfoo }.{Qbar }.{Qroo ,Q baz }, \nand QSet . QSet ' . Qmain  Note that PUNCH(Qfoo ), PUNCH(Qbar ), and PUNCH(Qbaz ), are computed in parallel. \nSubsequently, the REDUCE stage realizes that Q ' and Q ' are Done and, therefore, sets Qmain to a Ready \nstate foo bar and removes Q ' and Q ' from QSet. foo bar 4. Instantiations of PUNCH In this section, \nwe will describe how any must-analysis, may\u00adanalysis, and may-must-analysis can be suitably modi.ed to \nmeet the speci.cation of PUNCH given in Section 3.2. For a detailed exposition of must-, may-, and may-must-analyses, \nthe reader is referred to [19]. Assume that PUNCH, is given a query Qm =(qm,sm,pm, Om), ? where qm = \n(.1 =.Pi .2) and sm = Ready. We start by de.ning a must-map and a may-map over procedure Pi as follows: \nMust-map: A must-map O: Ni . 2SPi maps locations n . Ni of Pi to sets of states, representing an underapproximation \nof the set of reachable states at that location from states in .1 at ni 0. For each node n . Ni, we use \nOn to denote O(n). Initially, O0 = .1, and for all n . Ni \\{n 0 i }, On = \u00d8. n i May-map: A may-map .: \nNi . 22 SPi maps locations n . Ni of Pi to sets of sets of states (partitions), which together represent \nan overapproximation of the set of states that can reach .2 at that location. For each node n . Ni, we \nuse .n to denote .(n). Initially, .nx = {.2, SPi \\.2}, and for every i n . Ni \\{nix}, .n = {SPi }. For \na node n . Ni, we treat sets of states On and .n . .n as formulas and use the notation OG and .G to denote \nversions of On nn and .n where all local variables are existentially quanti.ed. In what follows, we sketch \nhow different analyses populate these maps to answer the reachability question qm. Must-Analysis A must-analysis \nexplores a subset of the behav\u00adiors, or an underapproximation, of a given program, and is therefore useful \nfor proving the presence of errors. For example, DART [16] and CUTE [35] use a combination of symbolic \nand concrete execu\u00adtions to explore an underapproximation of a program. In a must-analysis, PUNCH progressively \npropagates sets of reachable states along edges of the procedure Pi. If at any point Onx n .2 = \u00d8, then \nthe postcondition .2 of qm is reachable from i a state in .1, and, therefore, a must-summary that answers \nqm can be generated and stored in SUMDB. The veri.cation object Om for a must-analysis is the must-map \nO. The main difference from a typical must-analysis is the way in which PUNCH propagates reachable states \nover call statements. Given an edge e =(n, n ' ) . Ei such that .i(e) is a call state\u00adment call Pj , \nPUNCH encodes reachability over this call as the ? reachability question (OGn =.Pj SPj ), and .rst checks \nwhether a must-summary that answers this question is available in SUMDB. If such a must-summary exists \nin SUMDB, it uses the summary to update the set of reachable states On! at location n ', the des\u00adtination \nlocation of the call-edge e. On the other hand, if a must\u00adsummary is unavailable, PUNCH creates a child \nquery Qk, where = (OG =? ), and adds it to R (the set of sub-queries that PUNCH returns, which contains \nan updated copy of Qm). In contrast, a regular must-analysis would analyze the procedure Pj and compute \nreachability information. If PUNCH successfully computes all reachable states, then it ter\u00adminates analysis \nof Qm. But since a must-analysis is not guaran\u00adteed to converge, PUNCH continues to analyze Qm up to \nsome time limit or an upper-bound on the number of explored paths before it stops analysis and returns \na set of child sub-queries R of Qm. This is to ensure that the MAP stage always terminates. When PUNCH \nqkn .Pj SPj stops its analysis of Qm, the state of PUNCH, which is the must\u00admap O, is saved in Om, so \nthat the next time Qm is processed by PUNCH, it can continue exploration from the saved state Om. May-Analysis \nA may-analysis explores an overapproximation of a program s behaviors, and is therefore used to prove \nabsence of errors. For example, software model checkers such as SLAM [5] and BLAST [10] overapproximate \nthe set of states reachable in a program using predicate abstraction [20]. In our case, the goal of a \nmay-analysis is to prove that no execution can reach a state in .2 at nix from a state in .1 at ni 0 \n. For every edge e =(n, n ' ) . Ei, we assume that there exists an abstract edge between every .n . .n \nand every .n! . .n! (denoted by .n .e .n! ). The may-analysis proceeds by eliminating infeasible abstract \nedges in order to prove that .2 is unreachable. Eliminated abstract edges are stored in the set E, \u00af \n which is initially empty. Suppose that for edge e =(n, n ' ), .i(e) is a simple statement, and that \nthere exists an abstract edge .1 .e .2. A may-analysis checks if .1 can reach a state in .2 by taking \nedge e. In case it cannot, .1 is split into two partitions: .1 . . and .1 .\u00ac., where pre(.i(e),.2) . \n. and pre(.i(e),.2) is the preimage of the set of states .2 w.r.t the statement .i(e). Since no state \nin .1 .\u00ac. can \u00af reach .2, E is updated with the edge (.1 .\u00ac., .2). Intuitively, the partition .1 is \nre.ned into a partition that may reach .2, and another one that may not. Now suppose that .i(e) is a \ncall statement to some procedure ? Pj . Then, PUNCH encodes the reachability question (.1 G =.Pj \u00acmay.2 \nG). If there exists exists a not-may summary (.-1 =. Pj .-2)that answers this reachability question, \nthen we know that there are no executions from .1 to .2. Therefore, PUNCH splits .1 into .1 . . and .1 \n.\u00ac., where . . .-1, and adds (.1 . ., .2) to the \u00af set E. Otherwise, if there does not exist such a \nsummary, PUNCH adds a child query Qk, where qk = (.1 G =.? Pj .2 G), to the set R. As discussed, a may-analysis \nmaintains the map . and the set \u00af of eliminated edges E. Therefore, when PUNCH returns Qm in \u00af a Ready \nor Blocked state, Om is set to (.,E). A may-analysis sets the query Qm to Done when all partitions of \nni 0 intersecting with .1 cannot reach a partition of nix intersecting with .2, where reachability is \nde.ned via abstract edges. As with a must-analysis, for fairness, PUNCH may decide to terminate analysis \nprematurely and store the state of the analysis in Om. May-Must-Analysis Finally, may-must-analyses combine \na must\u00adanalysis with a may-analysis in order to ef.ciently .nd errors as well as prove their absence. \nThe SYNERGY [22] and DASH [9] algorithms are examples of may-must-analyses. Both use testing, symbolic \nexecution and abstraction to check properties of pro\u00adgrams. Interpolation-based software model checking \nalgorithms, such as [2, 24, 31], can also be seen as may-must-analyses. Such algorithms also use symbolic \nexecutions to error locations to .nd bugs and, in case of infeasible executions, use interpolants derived \nfrom refutation proofs to create an abstraction that eliminates a large number of potential counterexamples. \nFor the query Qm, a may-must-analysis maintains ., O, and E. \u00af That is, if PUNCH returns Qm in a Ready \nor Blocked state, it sets Om to (., O,E). \u00af A may-must-analysis only analyzes an abstract transition \n.1 .e .2, where e =(n, n ' ) . Ei and .i(e) is a call to some procedure Pj , if On n .1 = \u00d8 and On! n.2 \n= \u00d8. That is, only abstract transi\u00adtions which have been reached by the must analysis, but not taken, \nare analyzed. These transitions are called frontiers in [9, 22]. A may-must-analysis handles such abstract \ntransitions as fol\u00adlows: let bolt () = while (Q_0.isNotDone()) do QSet := Async.AsParallel [for Q_i \nin QSet -> async {return punch Q_i}]; reduce (); done; ... Figure 5. F# implementation of BOLT s main \nfunction bolt. must \u00ad 1. If there exists a must summary (.-1 =. Pi .2) that answers ? .G the query (OG \n=.Pj 2 ), then we know that there exists an n execution from On to .2 through Pj , and, therefore, PUNCH \nupdates On! to be On! . ., where . . .-2 and . n .2 = \u00d8. \u00acmay \u00ad 2. If there exists a not-may summary \n(.-1 =. Pi .2) that an\u00ad .G swers the query (OG =?2 ), then we know that there n .Pj are no executions \nfrom On to .2, and, therefore, PUNCH splits region .1 into .1 .\u00ac. and .1 . ., where . . .-1 and \u00af \u00ac. \nn On = \u00d8. Thus, the edge (.1 . ., .2) is added to E. 3. Finally, if neither kind of summaries exist, \nthen a child query ? .G Qk, where qk = ((On . .1)G =.Pi 2 ), is added to R. In a may-must-analysis, PUNCH \ncontinues processing a query Qm until a must summary is produced, a not-may summary is pro\u00adduced, or \nall abstract edges have been analyzed and child queries have to be answered to continue processing. Similar \nto may-and must-analysis, PUNCH may decide to terminate analysis prema\u00adturely. In summary, we have shown \nhow PUNCH can be instantiated with various classes of analyses, which encompass a large number of already \npublished algorithms from the literature. In the following section, we discuss a may-must implementation \nof PUNCH, based on DASH [9], and describe our empirical evaluation of BOLT on a number of Microsoft Windows \ndevice drivers. 5. Implementation and Evaluation We have implemented BOLT in the F# programming language \nand integrated it with the Static Driver Veri.er toolkit (SDV) [6] for Mi\u00adcrosoft Windows device drivers. \nOur implementation adopts a plug\u00adgable architecture, where different instantiations of PUNCH that ad\u00adhere \nto the speci.cation in Section 3.2 can be easily integrated into our tool. The main function bolt of \nour implementation is par\u00adtially shown in Figure 5. Our implementation assumes that punch is a pure function \n(except for its communication with SUMDB) that takes a query as input and returns a set of queries. As \na result, the only resource shared between different threads of execution is the summary database SUMDB. \nThis provides a natural framework where program analysis designers can plug in their intraprocedu\u00adral \nanalyses and automatically produce parallelized interprocedural analyses, without having to be faced \nwith the intricacies of paral\u00adlelizing individual analyses. The instance of PUNCH that we have implemented \nis a variant of the may-must DASH algorithm [9] as described in Section 4. Our implementation of PUNCH \nuses the Z3 SMT solver [12] for satis.ability checking and can handle C programs with primitive datatypes, \nstructured types, pointers, and function pointers. Check\u00ading if a summary answers a query is done via \na call to the SMT solver. 5.1 Experiments We now present our experimental setup and results for the \nBOLT algorithm. Statistic Total time taken (sequential) 26 hours Total time taken (parallel) 7 hours \nAverage observed speedup 3.71x Maximum observed speedup 7.41x  Table 2. Cumulative results for BOLT \non 50 checks (#threads=64, #cores=8). Experimental Setup Our goal is to study the scalability of BOLT. \nWe do this by measuring the speedup of BOLT over a sequential may-must top-down analysis. To precisely \nmeasure and study the scalability of the algorithm, we introduce an arti.cial throttle that allows us \nto limit the number of threads that can perform queries in parallel. The arti.cial throttle has the effect \nof limiting the total number of physical cores available to the algorithm, when the number of maximum \nthreads is less than the total physical cores available, which also imposes a bound on the number of \nReady queries processed by PUNCH in the MAP stage. In cases where the number of allowed maximum threads \nis greater than the number of physical cores available, the .NET environment (since BOLT is implemented \nusing F#), thread scheduling, and contention play an important part in determining the .nal overall speedup. \nIt should be noted that the theoretical limit of the speedup that can be achieved on a given test machine \nis N, where N is the total number of available physical cores, unless super linear speedup can be achieved \nby the parallel algorithm eliminating work that the sequential algorithm must perform. We ran our experiments \non an HP workstation with 8 Intel Xeon 2.66 GHz cores and 8 GB of memory. We set our initial number \nof maximum concurrent threads to be 1 (representing a sequential analysis) and doubled it for every subsequent \nrun. The upper limit for the maximum number of concurrent threads is 128. All the experiments were run \nwith resource limitations of 3000 seconds (wall clock time) and 1800 MB of memory. Our evaluation of \nBOLT was over a test suite consisting of 45 Microsoft Windows device drivers and 150 safety properties1. \nFor the purposes of reporting results, we select all checks (that is, driver-property pairs) where the \nsequential version of BOLT re\u00adquires at least 1000 seconds to generate a proof. These are inter\u00adesting \nchecks (total of 50 checks) where a lot of computation is required and, in some cases, the sequential \nanalysis is unable to produce a result. Therefore, they serve as challenge problems for BOLT. It turns \nout that all these checks were cases where the pro\u00adgram satis.es the property (and thus a proof is reported \nby BOLT). For a study of effects of may-must summaries on the ef.ciency of the analysis, we refer the \nreader to [19, 33]. Results The cumulative results are presented in Table 2. For the 50 checks, where \nthe sequential algorithm takes at least 1000 sec\u00adonds for generating a proof, the average observed speedup \nus\u00ading BOLT was 3.71x. The maximum observed speedup was 7.41x. These results were obtained using 8 cores, \nand a maximum of 64 threads. We now analyze a sample of the 50 checks in greater detail. Ta\u00adble 1 shows \nthe detailed results for 6 checks. Each row represents a single check and the time/speedup observed for \ndifferent con.g\u00adurations of the maximum number of concurrent threads allowed. The speedup is calculated \nas the ratio of the time taken by the par\u00adallel version of BOLT (with 2 threads, 4 threads, etc.) to \nthat of the sequential version of BOLT (with 1 thread). 1 A subset of our benchmarks is available as \npart of the SDV-RP toolkit: http://research.microsoft.com/en-us/projects/slam. Property / Max. Number \nof Threads 1 2 4 8 16 32 64 128 Time Time Speedup Time Speedup Time Speedup Time Speedup Time Speedup \nTime Speedup Time Speedup Driver: toastmon (25KLOC) PendedCompletedRequest 1006 328 3.07 345 2.91 373 \n2.70 297 3.39 221 4.55 219 4.59 223 4.51 PnpIrpCompletion 2224 929 2.39 827 2.69 1041 2.14 599 3.71 \n300 7.41 300 7.41 300 7.41 Driver: parport (2KLOC) MarkPowerDown 1821 688 2.65 573 3.18 543 3.35 460 \n3.96 313 5.82 326 5.58 328 5.55 PowerDownFail 1916 673 2.85 566 3.38 524 3.65 398 4.81 305 6.28 315 6.08 \n318 6.03 PowerUpFail 2040 718 2.84 691 2.95 689 2.96 565 3.61 306 6.67 315 6.48 300 6.80 RemoveLockMnSurpriseRemove \n1794 678 2.65 576 3.11 538 3.33 405 4.43 311 5.77 315 5.69 315 5.69 Table 1. Average observed time (in \nseconds) and speedup of parallel BOLT compared to sequential BOLT for varying number of maximum concurrent \nthreads (#cores=8). Result Driver Property Seq Parallel Time daytona IoAllocateFree TO Proof 2800 mouser \nNsRemoveLockMnRemove TO Proof 2743 featured1 ForwardedAtBadIrql TO Proof 2966 incomplete2 RemoveLockForwardDeviceControl \nTO Proof 1205 selsusp IrqlExAllocatePool TO Proof 1951 Table 3. Driver and property combinations where \nBOLT was able to produce a proof (#cores = 8) and the sequential (Seq) version ran out of time (TO). \nFor the toastmon driver and the PnpIrpCompletion2 prop\u00aderty, we see that the speedup achieved with a \nmaximum of 2 con\u00adcurrent threads is 2.39. As the number of threads is increased to 128, the observed \nspeedup of 7.41 reaches close to the theoreti\u00adcal maximum speedup achievable on the test machine (8 cores). \nFor other checks in the table, we see that, in general, the observed speedup increases as the number \nof maximum concurrent threads is increased. Figure 6 illustrates the speedups reported in Table 1. The \nsuper linear speedup observed in some cases is related to the query processing order, which is discussed \nin the latter part of this sec\u00adtion. In general, we .nd that the BOLT algorithm always achieves speedup, \nwith the possibility of providing super linear speedup in some cases. Table 3 shows checks where BOLT \nsuccessfully produces a proof, whereas the sequential analysis runs out of resources. As can be seen \nfrom the table, in many cases, the time taken by BOLT is very close to the timeout limit of 3000 seconds, \nindicating the degree of dif.culty and high amount of computation that is necessary to successfully complete \nthe veri.cation. In general, the speedups achieved by BOLT are due to parallelism, as well as the scheduling \nof queries. That is, sequential implementations tend to have a .xed deterministic exploration strategy \nof the query tree and are therefore unable to discover invariants in many cases. On the other hand, BOLT, \ndue to its inherent parallel exploration, may compute invariants that the sequential version cannot discover. \nIn-depth Analysis To further understand the behavior of BOLT, we make the following measurements in our \nexperiments. The number of queries that are processed as veri.cation pro\u00adgresses. Figure 7 shows the \nnumber of queries processed in parallel for varying numbers of maximum threads (2 128) on the PnpIrpCompletion \nproperty of the toastmon driver. Note that, in Figure 7(f), there are two lines present on the chart, \nbut due to the fact that they are exactly equivalent, it appears as only a single line. From the graphs, \nwe see that when the number of maximum concurrent threads is less than 16, there is always a constant \namount of work that is to be performed and all the threads Figure 6. Measured speedup of the BOLT algorithm \nrelative to the sequential version (#cores=8).  are always close to 100% utilization. When the number \nof maximum threads is increased to more than 16, depending on the nature of the input program, mostly, \nan insuf.cient number of queries are produced. This results in only a few threads being utilized in an \nef.cient and maximal manner, which explains the lower than expected observed speedup. Figures 7(e) and \n(f) clearly illustrate this fact. We can see that the number of queries processed in parallel is very \ninconsistent and never reaches the allowed maximum of 64 or 128. In particular, since a small number \nof queries need to be made, the graph for 64 and 128 is exactly the same. Total number of queries that \nhave to be performed when varying the degree of allowed concurrency/parallelism. As the number of maximum \nconcurrent threads is increased, the order of the queries performed changes. This can have two possible \nout\u00adcomes. First, the order of the queries may impact the veri.ca\u00adtion positively, since an important \nfact can be learned earlier in the veri.cation, which in turn reduces the number of queries that have \nto be performed. The net effect is that in some cases super linear speedup is observed (as seen in Table \n1). Second, the order of the queries may result in an increased number of queries (due to redundancy), \nwhich elongates the veri.cation task and reduces the observed speedup relative to the maxi\u00admum theoretical \nspeedup. Table 4 shows the total number of queries made for different numbers of maximum concurrent threads \nallowed for the two properties listed for the toastmon driver. As we can see in the the case of the PnpIrpCompletion \nproperty, the algorithm performs only 1.2 times as many queries for 128 threads as it did for 2 threads. \nBut for the PendedCompletedRequest property, the algorithm performs 3.5 times as many queries. This fact \ndirectly relates to the lower observed speedup for the PendedCompletedRequest check, in comparison with \nthe PnpIrpCompletion check (shown in 2 Visit http://msdn.microsoft.com/en-us/library/.551714.aspx for \na list of properties. Table 1).  (a) (b) (c) (d) (e) (f) Figure 7. Number of concurrent queries performed \nover time for the driver toastmon and the property PnpIrpCompletion (#cores=8). Maximum number of threads \nfor sub.gures (a), (b), (c), (d) , (e) and (f) are 2, 4, 8, 16, 32, and 64 respectively. For 128 threads, \nthe results are identical to 64 threads. Property PendedCompletedRequest PnpIrpCompletion 2 873 1198 \nMaximum Number of Threads 4 8 16 32 64 1374 2213 2479 2529 3005 1369 1614 1854 1383 1429 128 3078 1429 \n Table 4. Total number of queries performed during veri.cation for various degrees of parallelism on \nthe toastmon driver (#cores=8). Discussion. Our implementation and experiments shows that for may-must-analysis \nBOLT provides an average speedup of 3.7x and maximum speedup of 7.4x with 8 cores. This has enabled us \nto complete veri.cation runs that sequential analyses have been un\u00adable to analyze previously. In particular, \nwe have been able to com\u00adplete every driver-property benchmark we have with BOLT, several of which we \nhave previously found impossible to complete. Fur\u00adthermore, each run of PUNCH needs to load only the \nprocedure under analysis into memory, except for whole program information such as alias analysis, which \ncan be stored in the database. Thus, we get signi.cant advantages in memory savings, in addition to savings \nin time. Our detailed investigation into the amount of parallelism avail\u00adable showed that, for the current \nset of benchmarks we have, in\u00adcreasing thread-level parallelism stops speeding up the analysis af\u00adter \n64 threads, since each MAP stage analyzes small number of queries. To this end, we see two avenues for \nef.ciently utilizing a larger number of cores: (1) getting larger benchmarks with poten\u00adtial for even \nlarger parallelism; and (2) designing a speculative ex\u00adtension to BOLT, where we can speculate on potential \nqueries that will be made and analyze them even before the queries are actually created, thereby, generating \nmore parallelism using speculation. We leave these directions for future work. 6. Related Work Parallel \nand distributed algorithms for static analysis and testing is an active area of research [15, 29, 32, \n34]. The unique feature of the BOLT framework is that it parallelizes interprocedural analyses that work \nin a top-down and demand-driven manner. Also, in contrast to earlier efforts to parallelize static and \ndynamic analysis, BOLT offers a pluggable architecture which inherits the nature of its underlying intraprocedural \nanalysis (as described in Section 4). In this section, we place BOLT in the context of related work. \nSpeci.cally, we compare BOLT with other parallel static analyses, as well as parallel .nite state veri.cation \n(model checking) and exploration techniques. Parallel/Distributed Static Analysis Techniques As discussed \nin Section 1, bottom-up interprocedural analyses are amenable to par\u00adallelization due to the decoupling \nbetween callers and callees. For example, the SATURN software analysis tool [1] employs a bottom\u00adup analysis \nand has been shown to scale to the entire Linux ker\u00adnel, both in the interprocedurally path-insensitive \n[37] and path\u00adsensitive settings [14]. In comparison, BOLT targets demand-driven top-down analyses, and \nis parameterized by the algorithm used for intraprocedural analysis. Therefore, BOLT can be easily applied \nto parallelizing existing software model checking and dynamic test generation techniques, for example, \nas we have shown with our in\u00adstantiation of PUNCH to a DASH-like [9, 19] may-must algorithm. Microsoft \ns Static Driver Veri.er toolkit uses manually created harnesses [7], which specify a set of independent \ndevice driver en\u00adtry points in order to create an embarrassingly parallel workload. On the other hand, \nBOLT is automatically able to exploit paral\u00adlelism that occurs at .ner levels of granularity. We believe \nthat both techniques complement each other and their combination has the potential for greater scalability. \nLopes et al. [29] propose a distributed tree-based [25] software model checking algorithm based on the \nCEGAR [11] framework. The tree unrolling of the control .ow graph of a program, consist\u00ading of a single \nprocedure, is distributed amongst multiple machines. We summarize the differences between [29] and BOLT \nas follows: (1) [29] is intraprocedural and does not reuse procedure summaries as BOLT does. (2) BOLT \nis not restricted to a speci.c intraproce\u00addural analysis. In fact, we believe that the distributed algorithm \nof [29] can be easily adapted to be an implementation of the in\u00adtraprocedural parameter PUNCH. (3) The \ninstantiation of PUNCH we present here does not use predicate abstraction and, therefore, skips the expensive \nstep of computing an abstract post transformer required in [29]. In [32], Monniaux describes a parallel \nimplementation of the Astr\u00b4abstract interpretation-based static analyzer. At certain ee branching locations \n(dispatch points), instead of analyzing pro\u00adgram paths in sequence, they are analyzed in parallel. On \nthree embedded software applications, the experiments showed around 2x speedup on 5 processors. Several \nother parallel static analysis techniques have been re\u00adcently proposed. One prominent example is EigenCFA \n[34], where GPUs are used to parallelize higher-order control-.ow analysis. The authors report up to \n72x speedup, when compared to other se\u00adquential techniques. Parallel/Distributed Exploration Techniques \nIn the .nite state veri.cation arena, several parallel/distributed model checking al\u00adgorithms have been \nproposed, e.g., [8, 21, 36]. At a high level, the methodology adopted by these techniques involves partition\u00ading \nthe state space and distributing the search to several threads or processors. Both [8] and [36], propose \ndistribution strategies for explicit state model checking, where each node is responsible for exploring \na partition of the state space. The authors of [27] propose a load balancing strategy to improve the \nperformance of the state space partitioning algorithms to achieve higher speedup. Similarly, the technique \nin [21] partitions BDDs in symbolic model checking and distributes them to several threads. In [26], \na multi-core algo\u00adrithm along with load balancing techniques geared towards shared memory systems is \nproposed. In [15], a method is proposed for dis\u00adtributed randomized state space search in the context \nof Java Path Finder (JPF) [23]. In contrast to the above techniques, BOLT ex\u00adploits a program s procedural \ndecomposition in order to ef.ciently parallelize top-down demand-driven analyses. 7. Conclusion and Future \nWork Modularity plays a central role in most scalable program analyses. Modular analyses are either top-down, \nwhere analysis starts at the main procedure and descends through the call-graph, or bottom-up, where \nanalysis starts with the leaves of the call-graph and goes up\u00adwards to the main procedure. Top-down analyses \nare extensively used in software model checking and test generation. However, un\u00adlike their bottom-up \ncounterparts, they are hard to parallelize due to the .ne-grained interactions between analysis instances \n(queries). In this paper, we have presented BOLT: a parameterized frame\u00adwork for parallelizing top-down \ninterprocedural analyses. BOLT adopts a MapReduce-like strategy for parallelizing processing of reachability \nqueries over multiple procedures in a top-down analy\u00adsis. We have shown how a number of may-, must-, \nand may-must\u00adanalyses can be parallelized using BOLT. We have also demon\u00adstrated the strength of BOLT \nby parallelizing a may-must analy\u00adsis. Our experimental results on device drivers showed that BOLT scales \nwell on an 8-core machine, and is able to verify several checks where the sequential version runs out \nof resources. We see a number of opportunities for continuing research in this direction: Distributed \nBOLT: BOLT s MapReduce architecture permits it to be easily implemented as a distributed application \nusing pro\u00adgramming models for large scale distributed systems [38]. As noted in [1], the limiting factor \nfor scaling any analysis to very large programs is memory and not time, and this forms the pri\u00admary motivation \nfor performing a bottom-up analysis. Indeed, our experience with BOLT also indicates that its memory \nusage is signi.cantly smaller than that the corresponding sequential top-down analysis. Therefore, we \nbelieve that we can distribute the parallelism that BOLT exposes across large-scale clusters of machines \nand, thereby, scale top-down analysis to very large programs.  Deriving value out of larger number of \ncores: Our investiga\u00adtion showed that, on our set of benchmarks, increasing thread\u00adlevel parallelism \nstops speeding up the analysis beyond 64 threads, due to limitations in the number of queries available \nfor the MAP stage. Thus, we see two directions for ef.ciently utilizing a larger number of cores and, \npotentially, distributing the analysis: (1) getting larger benchmarks with potential for even larger \nparallelism, and (2) designing a speculative exten\u00ad  sion to BOLT, where we can predict what queries \nwill be made and analyze them even before the queries are actually created, thereby, generating more \nparallelism using speculation. Other instantiations of BOLT: We would also like to experi\u00adment with \nother instantiations of PUNCH. For example, it would be interesting to apply BOLT for fuzz testing large \nscale sys\u00adtems in the style of SAGE [18]. Another interesting application for BOLT is the veri.cation \nof concurrent programs. Recent re\u00adsearch on reducing concurrent analysis to a sequential analy\u00adsis [28] \nmakes it possible to instantiate PUNCH so that BOLT is able to verify properties of concurrent programs. \nAcknowledgments We thank Tony Hoare, Vlad Levin, Jakob Lichtenberg and Robert Simmons for many useful \ndiscussions that shaped our work and the anonymous reviewers for their insightful comments and sug\u00adgestions. \nReferences [1] A. Aiken, S. Bugrara, I. Dillig, T. Dillig, B. Hackett, and P. Hawkins. An overview of \nthe Saturn project. In Program Analysis for Software Tools and Engineering (PASTE 2007), pages 43 48, \n2007. [2] A. Albarghouthi, A. Gur.nkel, and M. Chechik. Whale: An interpolation-based algorithm for interprocedural \nveri.cation. In Veri\u00ad.cation, Model Checking, and Abstract Interpretation (VMCAI 2012), 2012. [3] T. \nBall and S. K. Rajamani. Bebop: A symbolic model checker for boolean programs. In SPIN Workshop on Model \nChecking of Software (SPIN 2000), pages 113 130, 2000. [4] T. Ball and S. K. Rajamani. The SLAM toolkit. \nIn Computer Aided Veri.cation (CAV 2001), pages 260 264, 2001. [5] T. Ball, R. Majumdar, T. D. Millstein, \nand S. K. Rajamani. Automatic Predicate Abstraction of C Programs. In Programming language design and \nimplementation (PLDI 01), pages 203 213. ACM, 2001. [6] T. Ball, B. Cook, V. Levin, and S. K. Rajamani. \nSLAM and Static Driver Veri.er: Technology transfer of formal methods inside Mi\u00adcrosoft. In Integrated \nFormal Methods (IFM 2004), pages 1 20, 2004. [7] T. Ball, V. Levin, and S. K. Rajamani. A decade of software \nmodel checking with SLAM. Communications of the ACM, 54:68 76, July 2011. [8] J. Barnat, L. Brim, and \nJ. Stribrna. Distributed LTL model-checking in SPIN. In SPIN Workshop on Model Checking of Software (SPIN \n2001), pages 200 216, 2001. [9] N. E. Beckman, A. V. Nori, S. K. Rajamani, and R. J. Simmons. Proofs \nfrom tests. In International Symposium on Software Testing and Analysis (ISSTA 2008), pages 3 14, 2008. \n[10] D. Beyer, T. A. Henzinger, R. Jhala, and R. Majumdar. The Software Model Checker BLAST . STTT: International \nJournal on Software Tools for Technology Transfer, 9(5-6):505 525, 2007. [11] E. Clarke, O. Grumberg, \nS. Jha, Y. Lu, and H. Veith. Counterexample-Guided Abstraction Re.nement. In Computer Aided Veri.cation \n(CAV 2000), pages 154 169, 2000. [12] L. de Moura and N. Bj\u00f8rner. Z3: An ef.cient SMT solver. In Tools \nand Algorithms for the Construction and Analysis of Systems (TACAS 2008), pages 337 340, 2008. [13] J. \nDean and S. Ghemawat. MapReduce: Simpli.ed data processing on large clusters. In Operating Systems Design \nand Implementation (OSDI 2004), pages 137 150, 2004. [14] I. Dillig, T. Dillig, and A. Aiken. Sound, \ncomplete and scalable path\u00adsensitive analysis. In Programming Language Design and Implemen\u00adtation (PLDI \n2008), pages 270 280, 2008. [15] M. B. Dwyer, S. G. Elbaum, S. Person, and R. Purandare. Parallel ran\u00addomized \nstate-space search. In International Conference on Software Engineering (ICSE 2007), pages 3 12, 2007. \n [16] P. Godefroid. Compositional dynamic test generation. In Principles of Programming Languages (POPL \n2007), pages 47 54, 2007. [17] P. Godefroid, N. Klarlund, and K. Sen. DART: Directed automated random \ntesting. In Programming Language Design and Implementa\u00adtion (PLDI 2005), pages 213 223, 2005. [18] P. \nGodefroid, M. Y. Levin, and D. A. Molnar. Automated whitebox fuzz testing. In Network and Distributed \nSystem Security Symposium (NDSS 2008), 2008. [19] P. Godefroid, A. Nori, S. Rajamani, and S. Tetali. \nCompositional may-must program analysis: Unleashing the power of alternation. In Principles of Programming \nLanguages (POPL 2010), pages 43 56, 2010. [20] S. Graf and H. Sa\u00a8idi. Construction of abstract state \ngraphs with PVS. In Computer Aided Veri.cation (CAV 1997), pages 72 83, 1997. [21] O. Grumberg, T. Heyman, \nN. Ifergan, and A. Schuster. Achieving speedups in distributed symbolic reachability analysis through \nasyn\u00adchronous computation. In Correct Hardware Design and Veri.cation Methods (CHARME 1995), pages 129 \n145, 2005. [22] B. Gulavani, T. Henzinger, Y. Kannan, A. V. Nori, and S. K. Rajamani. SYNERGY: A new \nalgorithm for property checking. In Foundations of Software Engineering (FSE 2006), pages 117 127, 2006. \n[23] K. Havelund and T. Pressburger. Model Checking Java Programs Using Java Path.nder. International \nJournal on Software Tools for Technology Transfer, 1999. [24] M. Heizmann, J. Hoenicke, and A. Podelski. \nNested interpolants. In Principles of Programming Languages (POPL 2010), pages 471 482, 2010. [25] T. \nHenzinger, R. Jhala, R. Majumdar, and G. Sutre. Lazy abstraction. In Principles of Programming Languages \n(POPL 2002), pages 58 70, 2002. [26] G. J. Holzmann and D. Bosnacki. The design of a multicore extension \nof the SPIN model checker. IEEE Transactions on Software Engineer\u00ading, 33:659 674, October 2007. [27] \nR. Kumar and E. G. Mercer. Load balancing parallel explicit state model checking. Electronic Notes in \nTheoretical Computer Science, 128:19 34, 2005. [28] A. Lal and T. W. Reps. Reducing concurrent analysis \nunder a context bound to sequential analysis. Formal Methods in System Design (FMSD), 35(1):73 97, 2009. \n[29] N. P. Lopes and A. Rybalchenko. Distributed and predictable soft\u00adware model checking. In Veri.cation, \nModel Checking, and Abstract Interpretation (VMCAI 2011), pages 340 355, 2011. [30] K. McMillan. Lazy \nannotation for program testing and veri.cation. In Computer Aided Veri.cation (CAV 2010), pages 104 118, \n2010. [31] K. L. McMillan. Lazy abstraction with interpolants. In Computer Aided Veri.cation (CAV 2006), \npages 123 136, 2006. [32] D. Monniaux. The parallel implementation of the astr\u00b4ee static ana\u00adlyzer. In \nK. Yi, editor, APLAS, volume 3780 of Lecture Notes in Com\u00adputer Science, pages 86 96. Springer, 2005. \nISBN 3-540-29735-9. [33] A. V. Nori and S. K. Rajamani. An empirical study of optimizations in Yogi. \nIn International Conference on Software Engineering (ICSE 2010), pages 355 364, 2010. [34] T. Prabhu, \nS. Ramalingam, M. Might, and M. Hall. EigenCFA: accelerating .ow analysis with GPUs. In Principles of \nProgramming Languages (POPL 2011), pages 511 522, 2011. [35] K. Sen, D. Marinov, and G. Agha. CUTE: A \nconcolic unit testing engine for C. In Foundations of Software Engineering (ESEC-FSE 2005), pages 263 \n272, 2005. [36] U. Stern and D. L. Dill. Parallelizing the Murphi Veri.er. In Computer Aided Veri.cation \n(CAV 1997), pages 256 278, 1997. [37] Y. Xie and A. Aiken. Scalable error detection using Boolean satis.a\u00adbility. \nIn Principles of Programming Languages (POPL 2005), pages 351 363, 2005. [38] Y. Yu, M. Isard, D. Fetterly, \nM. Budiu, U. Erlingsson, P. K. Gunda, and J. Currey. DryadLINQ: a system for general-purpose distributed \ndata\u00adparallel computing using a high-level language. In Operating Systems Design and Implementation (OSDI \n2008), pages 1 14, 2008.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Modularity is a central theme in any scalable program analysis. The core idea in a modular analysis is to build summaries at procedure boundaries, and use the summary of a procedure to analyze the effect of calling it at its calling context. There are two ways to perform a modular program analysis: (1) top-down and (2) bottomup. A bottom-up analysis proceeds upwards from the leaves of the call graph, and analyzes each procedure in the most general calling context and builds its summary. In contrast, a top-down analysis starts from the root of the call graph, and proceeds downward, analyzing each procedure in its calling context. Top-down analyses have several applications in verification and software model checking. However, traditionally, bottom-up analyses have been easier to scale and parallelize than top-down analyses.</p> <p>In this paper, we propose a generic framework, BOLT, which uses MapReduce style parallelism to scale top-down analyses. In particular, we consider top-down analyses that are demand driven, such as the ones used for software model checking. In such analyses, each intraprocedural analysis happens in the context of a reachability query. A query <i>Q</i> over a procedure <i>P</i> results in query tree that consists of sub-queries over the procedures called by <i>P</i>. The key insight in BOLT is that the query tree can be explored in parallel using MapReduce style parallelism -- the map stage can be used to run a set of enabled queries in parallel, and the reduce stage can be used to manage inter-dependencies between queries. Iterating the map and reduce stages alternately, we can exploit the parallelism inherent in top-down analyses. Another unique feature of BOLT is that it is parameterized by the algorithm used for intraprocedural analysis. Several kinds of analyses, including may analyses, mustanalyses, and may-must-analyses can be parallelized using BOLT.</p> <p>We have implemented the BOLT framework and instantiated the intraprocedural parameter with a may-must-analysis. We have run BOLT on a test suite consisting of 45 Microsoft Windows device drivers and 150 safety properties. Our results demonstrate an average speedup of 3.71x and a maximum speedup of 7.4x (with 8 cores) over a sequential analysis. Moreover, in several checks where a sequential analysis fails, BOLT is able to successfully complete its analysis.</p>", "authors": [{"name": "Aws Albarghouthi", "author_profile_id": "81498660436", "affiliation": "University of Toronto, Toronto, Canada", "person_id": "P3471202", "email_address": "aws@cs.toronto.edu", "orcid_id": ""}, {"name": "Rahul Kumar", "author_profile_id": "81539094956", "affiliation": "Microsoft Corporation, Redmond, WA, USA", "person_id": "P3471203", "email_address": "rahulku@microsoft.com", "orcid_id": ""}, {"name": "Aditya V. Nori", "author_profile_id": "81320493380", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P3471204", "email_address": "adityan@microsoft.com", "orcid_id": ""}, {"name": "Sriram K. Rajamani", "author_profile_id": "81100468626", "affiliation": "Microsoft Research, Bangalore, India", "person_id": "P3471205", "email_address": "sriram@microsoft.com", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254091", "year": "2012", "article_id": "2254091", "conference": "PLDI", "title": "Parallelizing top-down interprocedural analyses", "url": "http://dl.acm.org/citation.cfm?id=2254091"}