{"article_publication_date": "06-11-2012", "fulltext": "\n Reagents: Expressing and Composing Fine-grained Concurrency Aaron Turon Northeastern University turon@ccs.neu.edu \nAbstract Ef.cient communication and synchronization is crucial for .ne\u00adgrained parallelism. Libraries \nproviding such features, while indis\u00adpensable, are dif.cult to write, and often cannot be tailored or \ncom\u00adposed to meet the needs of speci.c users. We introduce reagents, a set of combinators for concisely \nexpressing concurrency algo\u00adrithms. Reagents scale as well as their hand-coded counterparts, while providing \nthe composability existing libraries lack. Categories and Subject Descriptors D.1.3 [Programming tech\u00adniques]: \nConcurrent programming; D.3.3 [Language constructs and features]: Concurrent programming structures General \nTerms Design, Algorithms, Languages, Performance Keywords .ne-grained concurrency, nonblocking algorithms, \nmonads, arrows, compositional concurrency 1. Introduction Programs are what happens between cache misses. \nThe problem Amdahl s law tells us that sequential bottlenecks fundamentally limit our pro.t from parallelism. \nIn practice, the effect is ampli.ed by another factor: interprocessor communication, often in the form \nof cache coherence. When one thread waits on another, the program pays the cost of lost parallelism and \nan extra cache miss. The extra misses can easily accumulate to yield parallel slowdown, more than negating \nthe bene.ts of the remaining parallelism. Cache, as ever, is king. The easy answer is: avoid communication. \nIn other words, par\u00adallelize at a coarse grain, giving threads large chunks of indepen\u00addent work. But \nsome work doesn t easily factor into large chunks, or equal-size chunks. Fine-grained parallelism is \neasier to .nd, and easier to sprinkle throughout existing sequential code. Another answer is: communicate \nef.ciently. The past two decades have produced a sizable collection of algorithms for syn\u00adchronization, \ncommunication, and shared storage which mini\u00admize the use of memory bandwidth and avoid unnecessary wait\u00ading. \nThis research effort has led to industrial-strength libraries java.util.concurrent (JUC) the most prominent \noffering a wide range of primitives appropriate for .ne-grained parallelism. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. \nCopyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 Such libraries are an enormous undertaking \nand one that must be repeated for new platforms. They tend to be conservative, im\u00adplementing only those \ndata structures and primitives likely to ful.ll common needs, and it is generally not possible to safely \ncombine the facilities of the library. For example, JUC provides queues, sets and maps, but not stacks \nor bags. Its queues come in both blocking and nonblocking forms, while its sets and maps are nonblocking \nonly. Although the queues provide atomic (thread-safe) dequeuing and sets provide atomic insertion, it \nis not possible to combine these into a single atomic operation that moves an element from a queue into \na set. In short, libraries for .ne-grained concurrency are indispens\u00adable, but hard to write, hard to \nextend by composition, and hard to tailor to the needs of particular users. Our contribution We have \ndeveloped reagents, which abstractly represent .ne\u00adgrained concurrent operations. Reagents are expressive \nand com\u00adposable, but when invoked retain the scalability and performance of existing algorithms: Expressive \nReagents provide a basic set of building blocks for writing concurrent data structures and synchronizers. \nThe building blocks include isolated atomic updates to shared state, and inter\u00adactive synchronous communication \nthrough message passing. This blend of isolation and interaction is essential for expressing the full \nrange of .ne-grained concurrent algorithms (\u00a73.3). The building blocks also bake in many common concurrency \npatterns, like op\u00adtimistic retry loops, backoff schemes, and blocking and signaling. Using reagents, \nit is possible to express sophisticated concurrency algorithms at a higher level of abstraction, while \nretaining the per\u00adformance and scalability of direct, hand-written implementations. Composable Reagents \nalso provide a set of composition opera\u00adtors, including choice, sequencing and pairing. Using these oper\u00adators, \nclients can extend and tailor concurrency primitives without knowledge of the underlying algorithms. \nFor example, if a reagent provides only a nonblocking version of an operation like dequeue, a user can \neasily tailor it to a version that blocks if the queue is empty; this extension will work regardless \nof how dequeue is de\u00ad.ned, and will continue to work even if the dequeue implementa\u00adtion is changed. \nSimilarly, clients of reagent libraries can sequence operations together to provide atomic move operations \nbetween ar\u00adbitrary concurrent collections again, without access to or knowl\u00adedge of the implementations. \nCompositionality is also useful to al\u00adgorithm designers, since many sophisticated algorithms can be un\u00adderstood \nas compositions of simpler ones (\u00a73.3, \u00a73.4). We begin in \u00a72 by illustrating, through examples, some \nof the common patterns, tradeoffs and concerns in designing .ne-grained concurrent algorithms. The two \nsubsequent sections contain our core technical contributions:  class TreiberStack [A] { The design of \nthe reagent combinators is given in \u00a73. Each com\u00ad private val head = new AtomicReference[List[A]](Nil) \nbinator is motivated by speci.c needs and patterns in concurrent def push(a: A) {the algorithms described \nin \u00a72 concisely and at a higher-than\u00adalgorithms; the section shows in particular how to write all of \nval backo. = new Backo. while (true) {val cur = head.get() usual level of abstraction. The implementation \nof reagents is detailed in \u00a75, via both if (head.cas(cur, a :: cur)) return high-level discussion and \nexcerpts from our code in Scala. backo. .once() It reveals the extent to which reagents turn patterns \nof .ne\u00ad } grained concurrency into a general algorithmic framework. It } also shows how certain choices \nin the design of reagents enable def tryPop(): Option[A] = { important optimizations in their implementation. \nval backo. = new Backo. while (true) { Reagents have a clear cost model that provides an important guar\u00ad \nval cur = head.get() antee: they do not impose extra overhead on the atomic updates cur match { performed \nby individual concurrent algorithms. That is, a reagent\u00ad case Nil . return None based algorithm manipulates \nshared memory in exactly the same case a :: tail . way as its hand-written counterpart, even if it reads \nand writes if (head.cas(cur, tail )) return Some(a) many disparate locations. This is a guarantee not \ngenerally pro\u00ad } vided by concurrency abstractions like software transactional mem\u00ad backo. .once() ory \n(STM), which employ redo or undo logs or other mechanisms } to support composable atomicity. Reagents \nonly impose overheads } when multiple algorithms are sequenced into a large atomic block, } e.g. in an \natomic transfer between collections. In principle, then, reagents offer a strictly better situation \nthan with current libraries: when used to express the algorithms pro-Figure 1. Treiber s stack (in Scala) \nvided by current libraries, reagents provide a higher level of ab\u00adstraction yet impose negligible overhead; \nnothing is lost. But unlike with current libraries, the algorithms can then be extended, tailored, The \nprevailing wisdom1 is that such coarse-grained locking is in\u00adand combined. Extra costs are only paid \nwhen the new atomic com-herently unscalable: positions are used. It forces operations on the data structure \nto be serialized, even We test this principle empirically in \u00a76 by comparing multi\u00adwhen they could be \nperformed in parallel. ple reagent-based collections to their hand-written counterparts, as well as to \nlock-based and STM-based implementations. The bench\u00ad It adds extra cache-coherence traf.c, since each \ncore must ac\u00admarks include both single operations used in isolation (where little quire the same lock \ns cache line in exclusive mode before oper\u00adoverhead for reagents is expected) and combined atomic transfers. \nating. For .ne-grained communication, that means at least one We compare at both high and low levels \nof contention. Reagents cache miss per operation. perform universally better than the lock-and STM-based \nimple\u00ad It is susceptible to preemptions or stalls of a thread holding a mentations, and are indeed competitive \nwith hand-written lock-free lock, which prevents other threads from making progress. implementations. \nTo achieve scalability, data structures employ .ner-grained Finally, it is worth noting in passing that \nreagents generalize locking, or eschew locking altogether. Fine-grained locking as\u00adboth Concurrent ML \n[22] and Transactional Events [4], yielding sociates locks with small, independent parts of a data structure, \nthe .rst lock-free implementation for both. We give a thorough allowing those parts to be manipulated \nin parallel. Lockless (or discussion of this and other related work especially STM in \u00a77. nonblocking) \ndata structures instead perform updates directly, using A prototype implementation of reagents, together \nwith the hardware-level operations (like compare-and-set) to ensure atomic\u00adbenchmarks and benchmarking \nharness we used, can be found at ity. Doing the updates directly means that there is no extra commu\u00ad \n https://github.com/aturon/ChemistrySet nication or contention for locks, but it also generally means \nthat the entire update must consist of changing a single word of memory a constraint which is often quite \nchallenging to meet. 2. Background Fig. 1 gives a classic example of a .ne-grained concurrent data structure: \nTreiber s lock-free stack [28]. The stack is represented Broadly, we are interested in data structures \nand algorithms for as an immutable linked list. Mutation occurs solely through the communication, synchronization, \nor both. This section gives a brief head pointer, represented here as an AtomicReference. The head survey \nof the most important techniques for communication and is mutated using compareAndSet (here abbreviated \nas cas), which synchronization in a .ne-grained setting exactly the techniques takes an expected value \nand a new value, and atomically updates the that reagents abstract and generalize. Readers already familiar \nwith reference if it has the expected value. It returns true iff the update Treiber stacks [28], elimination-backoff \nstacks [9], dual stacks [24], was successful. and MCS locks [19] can safely skip to \u00a73. A word about \nScala: the syntax Given our target of cache-coherent, shared-memory architec\u00ad exp match { case pat1 . \nbody1 ... case patN . bodyN } tures, the most direct way of communicating between threads is modifying \nshared memory. The challenge is to provide both atom-denotes pattern matching, where the value of exp \nis matched in icity and scalability: communications must happen concurrently order against pat1 up to \npatN. In Fig. 1, we use the two list both without corruption and without clogging the limited memory \nconstructors Nil and :: (an inline cons) in patterns. bandwidth, even when many cores are communicating. \nA simple way to provide atomicity is to associate a lock with each shared 1 With some dissenting opinions, \nas in the recent work on .at combin\u00addata structure, acquiring the lock before performing any operation. \ning [10]. The push and tryPop operations are implemented in a typical optimistic style: they take a \nsnapshot of the head, perform some local computation with it, and then attempt to update it accordingly. \nThe computation is optimistic because it is performed without holding a lock. The head might concurrently \nchange, invalidating the cas intended to update it. To account for possible interference, the snapshot-compute-update \ncode executes within a retry loop. While the loop may retry forever, it can only do so by repeatedly \nfailing to cas, which in turn means that an unlimited number of other operations on the stack are succeeding. \nIn other words, push and tryPop are formally lock free. To successfully cas, a core must acquire the \nrelevant cache line in exclusive mode, or something conceptually equivalent. When several cores attempt \nto cas a common reference, they must co\u00adordinate at the hardware level, using precious memory bandwidth \nand wasting processor cycles. A failed cas is evidence that there is contention over a common location. \nTo increase the cas success rate and conserve memory bandwidth, .ne-grained concurrent al\u00adgorithms employ \na backoff scheme, which here we have abstracted into a lightweight Backo. class. The class encapsulates \nthe sim\u00adplest scheme: busy-wait for a random amount of time that grows exponentially each time once is \ninvoked. Treiber s stack scales better than a lock-based stack mainly be\u00adcause it decreases the amount \nof shared data that must be updated per operation: instead of acquiring a shared lock, altering a shared \nstack pointer, and releasing the shared lock, Treiber s stack does a single CAS to update the shared \npointer directly. However, that pointer is still a centralized source of contention. While exponential \nbackoff helps relieve the contention, we can do better by paralleliz\u00ading stack operations which is counterintuitive, \ngiven that they all involve modifying the top of the stack. Parallelization requires a change of perspective. \nNormally, we view concurrent operations on the same part of a data structure as competing to atomically \nupdate that data structure; we want the operations to be isolated. On the other hand, sometimes operations \ncan help each other: a push and a tryPop effectively cancel each other out. This insight leads to a scheme \ncalled elimination backoff [9, 26]. Operations .rst try the usual cas-based code. If the cas fails, rather \nthan busy-waiting, the operations advertise their presence on a side-channel, reducing the contention \nfor the head pointer. If a push and tryPop detect their mutual presence, the push can pass its argument \ndirectly to tryPop through the side-channel, and no change to the head pointer is necessary. Atomicity \nis not violated, because had the push and pop executed in sequence, the head pointer would have been \nreturned to its original value anyway. On the other hand, if no dual operation is detected during backoff, \nthe operation withdraws its offer and tries once more to cas the head. Both push and tryPop are total \noperations: they can succeed no matter what state the stack is in, and fail (and retry) only due to active \ninterference from concurrent threads. A true pop operation, on the other hand, is partial: it is unde.ned \nwhen the stack is empty. Often this is taken to mean that the operation should block until another thread \nmoves the stack into a state on which the operation is de.ned. Partial operations introduce considerable \ncomplexity, because all operations on the data structure must potentially signal blocked threads, depending \non the changes being performed. In some cases, it is possible to cleverly treat signaling in essentially \nthe same way as atomic updates [24]. Synchronization and signaling is also subject to cache and mem\u00adory \nbandwidth concerns, but it would take us too far a.eld to dis\u00adcuss these in depth. Mellor-Crummey and \nScott pioneered the now\u00adcommon techniques for grappling with these concerns [19], and we apply their \ntechniques in implementing the blocking and signaling protocols for reagents (\u00a75). // Shared state upd: \nRef[A] . (A \u00d7 B -A \u00d7 C) . Reagent[B,C] // Message passing swap: Endpoint[A,B] . Reagent[A,B] // Composition \n+ : Reagent[A,B] \u00d7 Reagent[A,B] . Reagent[A,B] >> : Reagent[A,B] \u00d7 Reagent[B,C] . Reagent[A,C] * : Reagent[A,B] \n\u00d7 Reagent[A,C] . Reagent[A, B \u00d7 C] // Post-commit actions postCommit: (A . Unit) . Reagent[A,A] Figure \n2. The core reagent combinators 3. Reagents: the core combinators Reagents are a new instance of an old \nidea: representing computa\u00adtions as data. The computations being represented are .ne-grained concurrent \noperations, so a value of type Reagent[A,B] represents a function from A to B that internally interacts \nwith a concurrent data structure through mutation, synchronization, or both. Because the computations \nare data, however, they can be combined in ways that go beyond simple function composition. Each way \nof com\u00adbining reagents corresponds to a way of combining their internal interactions with concurrent \ndata structures. Existing reagents for example, those built by a concurrency expert can be composed by \nlibrary users, without those users knowing their internal implemen\u00adtation. This way of balancing abstraction \nand composition was pio\u00adneered with Concurrent ML [22], and is now associated with mon\u00adads [30] and arrows \n[14]. Our contribution is giving a set of combi\u00adnators appropriate for expressing and composing .ne-grained \ncon\u00adcurrent algorithms, with a clear cost semantics and implementation story (\u00a75). The reagent combinators \nencompass a blend of duals: Shared state versus Message passing Given the goal of using reagents to express \nalgorithms like Treiber s stack (\u00a72), it is not surprising that reagents include primitives from shared-state \nconcurrency. But what about message passing? Under\u00adlying the shared state/message passing duality is \na deeper one: Isolation versus Interaction Operations on shared state are generally required to be atomic, \nwhich means in particular that they are isolated from one another; concurrent shared-state operations \nappear to be linearizable [13] into a series of nonoverlapping, sequential operations. Synchronous message \npassing is just the opposite: rather than appearing to not overlap, sends and receives are required to \noverlap. Fine-grained concurrent operations straddle these two extremes, appearing to be isolated while \ninternally tolerating (or exploiting) interaction. Elimination backoff (\u00a72) provides a striking example \nof this phe\u00adnomenon, and reagents can capture it concisely (\u00a73.3). There is also duality in reagent composition: \nDisjunction versus Conjunction In particular, we can combine two reagents by requiring exactly one of \nthem to take effect (choice), or by requiring both of them to take effect in a single atomic step (join). \nFinally, reagents, like functions, are inert. To be useful, they must be invoked. Reagents offer two \nmeans of invocation: Active (reactants) versus Passive (catalysts)  In chemistry, a reagent is a participant \nin a reaction, and reagents are subdivided into reactants, which are consumed during reaction, and catalysts, \nwhich enable reactions but are not consumed by them. Similarly for us. Invoking a reagent as a reactant \nis akin to calling it as a function: its internal operations are performed once, yielding a result or \nblocking until it is possible to do so. Invoking it as a catalyst instead makes it passively available \nas a participant in reactions. Because catalysts are not used up, they can participate in many reactions \nin parallel. In the rest of this section, we introduce the core reagent combi\u00adnators and use them to \nbuild a series of increasingly complex con\u00adcurrent algorithms. By the end, we will have seen how to implement \nall of the algorithms described in \u00a72, and several more besides. 3.1 Atomic updates on Refs Memory is \nshared between reagents using the type Ref[A] of atomically-updatable references. The upd combinator \n(Fig. 2) rep\u00adresents atomic updates to references. It takes an update function, which tells how to transform \na snapshot of the reference cell and some input into an updated value for the cell and some output. Using \nupd, we can rewrite TreiberStack in a more readable and concise way: class TreiberStack [A] { private \nval head = new Ref[List[A]]( Nil) val push: Reagent[A, Unit] = upd(head) { case (xs, x) . (x::xs, ()) \n} val tryPop: Reagent[Unit, Option[A]] = upd(head) { case (x::xs, ()) . (xs, Some(x)) case (Nil , ()) \n. (Nil, None) }} In Scala, anonymous partial functions are written as a series of cases enclosed in braces. \nFor push and tryPop, the case analyses are exhaustive, so the update functions are in fact total. Unit \nis akin to void: it is a type with a single member, written (). Being reagents, push and tryPop are inert \nvalues. They can be invoked as reactants using the ! method, which is pronounced react. For a Reagent[A,B] \nthe ! method takes an A and returns a B. Scala permits in.x notation for methods, so we can use a TreiberStack \ns by writing s.push ! 42. The key point is that when we invoke these reagents, we are executing exactly \nthe same algorithms written by hand in \u00a72, including the retry loop with exponential backoff; reagents \nsystematize and internalize common patterns of .ne-grained concurrency. By exposing push and tryPop as \nreagents rather than methods, we allow further composition and tailoring by a user of the data structure \n(\u00a73.3, \u00a73.4). While tryPop handles both empty and nonempty stacks, we can write a variant that drops \nthe empty case: val pop: Reagent[Unit, A] = upd(head) { case (x::xs, ()) . (xs, x) } Now our update function \nis partial. An invocation s.pop ! () will block the calling thread unless or until the stack is nonempty. \nIn general, there are two ways a reagent can fail to react: tran\u00adsiently or permanently. Transient failures \narise when a reagent loses a race to CAS a location; they can only be caused by active inter\u00adference \nfrom another thread. A reagent that has failed transiently should retry, rather than block, following \nthe concurrency patterns laid out in \u00a72. Permanent failures arise when a reagent places re\u00adquirements \non its environment such as the requirement, with pop above, that the head reference yield a nonempty \nlist. Such failures are permanent in the sense that only activity by another thread can enable the reagent \nto proceed. When faced with a permanent fail\u00adure, a reagent should block until signaled that the underlying \nstate has changed. Blocking and signaling are entirely handled by the reagent implementation; there is \ntherefore no risk of lost wakeups. The reagent upd(f) reagent can fail permanently only for those inputs \non which f is unde.ned. 3.2 Synchronization: how reagents react With reagents, updates to shared memory \nare isolated, so they cannot be used for interaction in which the parties are mutually aware. Reagents \ninteract instead through synchronous swap chan\u00adnels, which consist of two complementary endpoints. The \nmethod mkChan[A,B] returns a pair of type (Endpoint[A,B], Endpoint[B,A]) The combinator for communication \nis swap (see Fig. 2), which lifts an Endpoint[A,B] to a Reagent[A,B]. When two reagents communicate on \nopposite endpoints, they provide messages of complementary type (A and B, for example) and receive each \nother s messages. We call a successful communication a reaction between reagents. On the other hand, \nif no complementary message is available, swap will block until a reaction can take place a permanent \nfailure.  3.3 Disjunction of reagents: choice If r and s are two reagents of the same type, their choice \nr + s will behave like one of them, nondeterministically, when invoked. The most straightforward use \nof choice is waiting on several signals simultaneously, while consuming only one of them. For example, \nif c and d are endpoints of the same type, swap(c) + swap(d) is a reagent that will accept exactly one \nmessage, either from c or from d. If neither endpoint has a message available, the reagent will block \nuntil one of them does. A more interesting use of choice is adding backoff strategies (of the kind described \nin \u00a72). For example, we can build an elimination backoff stack as follows: class EliminationStack [A] \n{private val s= new TreiberStack[A] private val (elimPop, elimPush) = mkChan[Unit,A] val push: Reagent[A,Unit] \n= s.push + swap(elimPush) val pop: Reagent[Unit,A] = s.pop + swap(elimPop) } Choice is left-biased, so \nwhen push is invoked, it will .rst attempt to push onto the underlying Treiber stack. If the underlying \npush fails (due to a lost CAS race), push will attempt to send a message along elimPush, i.e., to synchronize \nwith a concurrent popper. If it succeeds, the push reagent completes without ever having modi.ed its \nunderlying stack. For choice, failure depends on the underlying reagents. A choice fails permanently \nonly when both of its underlying reagents have failed permanently. If either fails transiently, the choice \nreagent has failed transiently and should therefore retry. Reasoning along these lines, we deduce that \npush never blocks, since the underly\u00ading s.push can only fail transiently. On the other hand, pop can \nblock because s.pop can fail permanently on an empty stack and swap(elimPop) can fail permanently if \nthere are no offers from pushers. When push or pop retry, they will spinwait brie.y for another thread \nto accept their message along elimPush or elimPop; the length of the wait grows exponentially, as part \nof the exponential backoff logic. Once the waiting time is up, the communication attempt is canceled, \nand the whole reagent is retried. This protocol is elaborated in \u00a75.  3.4 Conjunction of reagents: \nsequencing and pairing Choice offers a kind of disjunction on reagents. There are also two ways of conjoining \ntwo reagents, so that the composed reagent has the effect of both underlying reagents: End-to-end composition, \nvia sequencing: if r : Reagent[A,B] and s: Reagent[B,C] then r >> s: Reagent[A,C].  Side-by-side composition, \nvia pairing: if r : Reagent[A,B] and  s: Reagent[A,C] then r * s: Reagent[A,(B,C)]. These combinators \ndiffer only in information .ow. Each guarantees that the atomic actions of both underlying reagents become \na sin\u00adgle atomic action for the composition. For example, if s1 and s2 are both stacks, then s1.pop >> \ns2.push is a reagent that will atomi\u00adcally transfer an element from the top of one to the top of the \nother. The reagent will block if s1 is empty. Similarly, s1.pop * s2.pop will pop, in one atomic action, \nthe top elements of both stacks, or block if either is empty. Here we begin to see the bene.ts of the \nreagent abstraction. Both of the example combinations work regardless of how the underlying stacks are \nimplemented. If both stacks use elimination backoff, the conjoined operations will potentially use elimination \non both simultaneously. This behavior is entirely emergent; it does not require any code on the part \nof the stack author, and it does not require the stack user to know anything about the implementation. \nReagents can be composed in unanticipated ways. Conjunctions provide a solution to the Dining Philosophers \nproblem: to consume two resources atomically, one simply con\u00adjoins two reagents that each consume a single \nresource. For ex\u00adample, if c and d are endpoints of type Unit to A and B respec\u00adtively, then swap(c) \n* swap(d) is a reagent that receives messages on both endpoints simultaneously and atomically. There \nis no risk of introducing a deadlock through inconsistent acquisition order\u00ading, because the reagents \nimplementation is responsible for the ul\u00adtimately acquisition order, and will ensure that this order \nis globally consistent. The failure behavior of conjunctions is dual to that of disjunc\u00adtions: if either \nconjunct fails permanently, the entire conjunction fails permanently. The implementation details for \nconjunctions are discussed later (\u00a75), but a key point is that the performance cost is pay as you go. \nSingle atomic reagents like push and pop execute a sin\u00adgle CAS just like the standard nonblocking algorithms \nthey are meant to implement even though these operations can be com\u00adbined into larger atomic operations. \nThe cost of conjunction is only incurred when a conjoined reagent is actually used. This is a crucial \ndifference from STM, which generally incurs overheads regardless of the size of the atomic blocks; see \n\u00a77 for more discussion. 3.5 Catalysts: persistent reagents The ! operator invokes a reagent as a reactant: \nthe invocation lasts for a single reaction, and any messages sent by the reagent are con\u00adsumed by the \nreaction. But sometimes it is useful for reagent invo\u00adcations to persist beyond a single reaction, i.e., \nto act as catalysts. For example, the following function creates a catalyst that merges input from two \nendpoints and sends the resulting pairs to another endpoint: def zip(in1: Endpoint[Unit, A], in2: Endpoint[Unit, \nB], out: Endpoint[(A,B), Unit]) = dissolve ((swap(in1) * swap(in2)) >> swap(out)) The dissolve function \ntakes a Reagent[Unit, Unit] and intro\u00adduces it as a catalyst.2 Operationally, in this example, that just \n2 For simplicity, we have not given a way to cancel catalysts after they have been introduced, but cancellation \nis easy to add. means sending messages along in1 and in2 that are marked as cat\u00adalyzing messages, and \nhence are not consumed during reaction. The upshot is that senders along in1 will see the catalyzing \nmessages, look for messages along in2 to pair with, and ultimately send mes\u00adsages along out (and similarly \nin the other order). Catalysts could instead be expressed using a thread that re\u00adpeatedly invokes a reagent \nas a reactant. Allowing direct expres\u00adsion through dissolve is more ef.cient (since it does not tie up \na thread) and allows greater parallelism (since, as with the zip exam\u00adple above, multiple reagents can \nreact with it in parallel). Catalysts are not limited to message passing. The zip example above could \nbe rephrased in terms of arbitrary reagents rather than just endpoints. 3.6 Post-commit actions Reagents \nsupport post commit actions , which comprise code to be run after a reaction has successfully taken place, \ne.g. for signaling or spawning another thread after an operation completes. The postCommit combinator \n(Fig. 2) takes a function from A to Unit and produces a Reagent[A,A]. The post-commit action will be \nrecorded along with the input of type A, which is passed along unchanged. Once the reaction completes, \nthe action will be invoked on the stored input. The combinator is meant to be used in sequence with other \ncombinators that will produce the appropriate input. For example, the reagent pop >> postCommit(println) \nwill print the popped element from a stack after the pop has completed. 3.7 Case study: the join calculus \nFournet and Gonthier s join calculus [6] provides an interesting example of the expressive power of reagents. \nThe join calculus is based on message passing, but instead of a simple receive prim\u00aditive on channels, \nprogrammers write join patterns. The join pat\u00adterns for a set of channels say, once and for all, how \nto react to mes\u00adsages along those channels. Each pattern c1(x1)&#38; \u00b7\u00b7\u00b7 &#38;cn(xn) rb consists of a \nsequence of channels ci with names xi for the mes\u00adsages along those channels, and a body b in which the \nnames xi are bound. A join pattern matches if messages are available on each of the listed channels, \nand it .res by atomically consuming all of the messages and then running the body. Since several patterns \nmay be given for the same channels, the choice of which pattern to .re may be nondeterministic. The join \npattern c1(x1)&#38; \u00b7\u00b7\u00b7 &#38; cn(xn) rb can be interpreted directly as a catalyst. The join operator \n&#38; is interpreted as a con\u00adjunction * , the channel names as appropriate swap instances, and the body \nb as a post-commit action. Altogether, the reagent corre\u00adsponding to the pattern is (swap(c1) * \u00b7\u00b7\u00b7 * \nswap(cn)) >> postCommit(b) A set of join patterns governing a set of channels can all be writ\u00adten in \nthis way and dissolved as catalysts, which is equivalent to dissolving the choice of all the patterns. \nThus, reagents provide a scalable implementation of the join calculus, along the lines of the one developed \nin previous work with Russo [29].  3.8 Atomicity guarantees Because conjunction distributes over disjunction, \nevery reagent built using the core combinators (Fig. 2) can be viewed as a dis\u00adjunction of conjunctions, \nwhere each conjunction contains some combination of updates and swaps. For such a reagent, reactions \natomically execute all of the conjuncts within exactly one of the disjuncts. This STM-like guarantee \nis too strong for algorithms which read shared memory without requiring the reads to be visi\u00adble (i.e., \nto participate in an atomic transaction). The next section will introduce computed reagents which allow \ninvisible reads and writes, trading weaker guarantees for better performance.  // Low-level shared state \ncombinators class MSQueue[A] {read: Ref[A] . Reagent[Unit, A] private case class Node(data: A, next: \nRef[Node]) cas: Ref[A] \u00d7 A \u00d7 A . Reagent[Unit, Unit] private val head = new Ref(new Node(null)) // sentinel \n private val tail = new Ref(read(head) ! ()) // sentinel // Computational combinators val tryDeq: Reagent[Unit, \nOption[A]] = upd(head) {ret: A . Reagent[Unit,A] case (Node( , Ref(n@Node(x, ))), ()) . (n, Some(x)) \ncomputed: (A -Reagent[Unit, B]) . Reagent[A,B] case (emp, ()) . (emp, None) }private def .ndAndEnq(n: \nNode): Reagent[Unit,Unit] = read( tail ) ! () match {case ov@Node( , r@Ref(null)) . // found true tail \ncas(r, null, n) >> postCommit { cas(tail, ov, n)? ! () }case ov@Node( , Ref(nv)) . // not the true tail \ncas( tail , ov, nv)? ! (); // catch up tail ref .ndAndEnq(n) }val enq: Reagent[A, Unit] = computed { \n(x: A) . .ndAndEnq(new Node(x, new Ref(null))) }} Figure 4. The Michael-Scott queue, using reagents for \nworking directly on Ref values. Together with the computed combinator described in \u00a74.1, read and cas \nsuf.ce to build upd. The read combinator is straightforward: if r has type Ref[A], then read(r) has type \nReagent[Unit, A] and, when invoked, re\u00adturns a snapshot of r. The cas combinator takes a Ref[A] and two \nA arguments, giving the expected and updated values, respectively. Unlike its counterpart for AtomicReference,a \ncas reagent does not yield a boolean result. A failure to CAS is transient, and therefore results in \na retry. 4.3 Constant and tentative reagents The ret combinator (Fig. 2) always succeeds, immediately \nreturn\u00ading the given value. Because choice is left-biased, it can be used together with the remaining \ncombinators to express tentative reagents: if r is a Reagent[A,B] then r? is a Reagent[A,Option[B]] that \n.rst tries r (wrapping its output with Some) and then tries ret (None), which always succeeds. This allows \na reaction to be attempted, without retrying it when it fails.  4.4 Case study: the Michael-Scott queue \nTo illustrate the use of computed, we now show how to implement the classic Michael-Scott lock-free queue \n[20]. Unlike a stack, in which all activity focuses on the head, queues have two loci of updates. That \nmeans, in particular, that the Refs used by its reagents may vary depending on the current state of the \nqueue. The strategy we employ to implement it readily scales to more complicated examples, such as concurrent \nskiplists or the lazy, lock-free set algorithm [11]. With any of these examples, we reap the usual bene.ts: \na concise, composable and extensible exposition of the algorithm. Here is a brief overview of the Michael-Scott \nalgorithm. The queue is represented as a mutable linked list, with a sentinel node at the head (front) \nof the queue. The head pointer always points to the current sentinel node; nodes are dequeued by a CAS \nto this pointer, just like Treiber stacks (but lagged by one node). The true tail of the queue is the \nunique node, reachable from the head pointer, with a null next pointer; thanks to the sentinel, such \na node is guaranteed to exist. If the queue is empty, the same node will be the head (sentinel) and tail. \nFinally, as an optimization for enqueing, a tail Figure 3. The low-level and computational combinators \nWhen reagents interact through message passing, their atomic\u00adity becomes intertwined: they must react \ntogether in a single atomic step. This requirement raises an important but subtle question: what should \nhappen when isolation and interaction con.ict? Consider two reagents that interact over a channel, but \nalso each update the same shared reference. The atomicity semantics demands that both reagents involved \nin the reaction atomically commit, but the isola\u00adtion on references demands that the updates be performed \nin sepa\u00adrate atomic steps. For both simplicity and performance, we consider such situa\u00adtions to be illegal, \nand throw an exception in such cases. Ideally, the static types of reagents would somehow track the necessary \ninformation to determine whether compositions are safe, but we leave such a type discipline for future \nwork. In practice, this rules out only compositions of certain operations within the same data structure, \nwhich are much less common than compositions across data structures. It is also straightforward to adopt \nan alternative ap\u00adproach, e.g. the one taken by Communicating Transactions (\u00a77), which treats isolation/interaction \ncon.icts as transient failures. 4. Low-level and computational combinators 4.1 Computed reagents The \ncombinators introduced in \u00a73 are powerful, but they impose a strict phase separation: reagents are constructed \nprior to, and in\u00addependently from, the data that .ows through them. Phase separa\u00adtion is useful, because \nit allows reagent execution to be optimized based on complete knowledge of the computation to be performed \n(see \u00a75). But in many cases the choice of computation to be per\u00adformed depends on the input or other \ndynamic data. The computed combinator (Fig. 3) expresses such cases. It takes a partial func\u00adtion from \nA to Reagent[Unit,B] and yields a Reagent[A,B]. When the reagent computed(f) is invoked, it is given \nan argument value of type A, to which it applies the function f. If f is not de.ned for that input, the \ncomputed reagent issues a permanent (blocking) failure, similarly to the upd function. Otherwise, the \napplication of f will yield another, dynamically-computed reagent, which is then invoked with (), the \nunit value. In functional programming terms, the core reagent combinators of \u00a73 can be viewed in terms \nof arrows [14], which are abstract, composable computations whose structure is statically determined. \nWith the addition of computed, reagents can also be viewed in terms of monads [30], which extend arrows \nwith dynamic deter\u00admination of computational structure. In the remainder of this section, we introduce \na handful of lower-level combinators which are useful in connection with com\u00adputed reagents. We close \nwith a case study: Michael and Scott s lock-free queue [20].  4.2 Shared state: read and cas Although \nthe upd combinator is convenient, it is sometimes neces\u00adsary to work with shared state with a greater \ndegree of control. To this end, we include two combinators, read and cas (see Fig. 2), pointer is maintained \nwith the invariant that the true tail node is always reachable from it. The tail pointer may lag behind \nthe true tail node, however, which allows the algorithm to work using only single-word CAS instructions. \n Our reagent-based implementation of the Michael-Scott queue is shown in Fig. 4. The node representation \nis given as an inner case class. In Scala, case classes provide two features we take advantage of. First, \nthe parameters to their constructors (here data and next) are automatically added as .nal .elds to the \nclass, which are initial\u00adized to the constructor argument values. Second, they extend pat\u00adtern matching \nthrough case so that instances can be deconstructed. A pattern like case Node(d, n) matches any instance \nof the node class, binding d to its data .eld and n to its next .eld. The head and tail references of \nthe queue are initialized to the same sentinel node. Here we use the read combinator (\u00a74.2) to extract \nthe sentinel value from the head when constructing the tail. The read reagent in tail s initializing \nexpression is immediately executed during construction of an MSQueue instance. The tryDeq reagent is \nvery similar to the tryPop reagent in TreiberStack, modulo the sentinel node. The reagent pattern matches \non the sentinel node, ignoring its data .eld by using , the wildcard. The next .eld is then matched to \na nested pattern, Ref(n@Node(x, )). This pattern immediately reads the current value of the reference \nstored in next, binds that value to n, and then matches the pattern Node(x, ) against n. If the pattern \nmatches which it will any time the next .eld of the sentinel is non-null the node n becomes the new head \n(and hence the new sentinel). Since the location of the tail node is determined dynamically by the data \nin the queue, the enq reagent must itself be determined dy\u00adnamically. For enq, we compute a dynamic reagent \nby .rst taking the given input x, creating a node with that data, and then calling a private function \n.ndAndEnq that will locate the tail of the queue and yield a reagent to update it to the new node. Since \n.ndAndEnq is private and tail-recursive, Scala will compile it to a loop. The .ndAndEnq function searches \nfor the true tail node (whose next .eld is null ) starting from the tail pointer, which may lag. To perform \nthe search, .ndAndEnq must read the tail pointer, which it does using the read combinator. There is a \nsubtle but important point here: this read occurs while the .nal reagent is being computed. That means, \nin particular, that the read is not part of the computed reagent; it is a side-effect of computing the \nreagent. The distinction is important: such a read is effectively invisible to the outer reagent being \ncomputed, and thus is not guaranteed to happen atomically with it. Invisible reads and writes are useful \nfor avoiding compound atomic updates, but must be employed carefully to ensure that the computed reagent \nprovides appropriate atomicity guarantees. Once the tail pointer has been read, its value is pattern-matched \nto determine whether it points to the true tail. If it does, .ndAndEnq yields a cas reagent (\u00a74.2) that \nwill update the next .eld of the tail node from null to the new node. The attached post-commit action \nattempts to catch up the tail pointer through a cas. Since the cas fails only if further nodes have been \nenqueued by other concur\u00adrent threads, we perform it tentatively (\u00a74.3); it is not necessary or desirable \nto retry on failure. If, on the other hand, the tail pointer is lagging, .ndAndEnq performs an invisible \ncas to update it. Since it may be racing with other enqueuers to catch up the tail, a failure to CAS \nis ignored here. Regardless of the outcome of the cas, the .ndAndEnq func\u00adtion will restart from a freshly-read \ntail pointer. Notice that in this case, an entire iteration of .ndAndEnq is executed with no visi\u00adble \nimpact or record on the .nal computed reagent there is no extended redo log or compound atomic transaction. \n5. Implementation Having seen the design and application of reagents, we now turn to their implementation. \nWe begin with a high-level overview (\u00a75.1) introducing the key techniques and data structures we use. \nWe then delve into the core code of our Scala implementation, beginning with the primary entry point \n(the ! method, \u00a75.4) and continuing with the key combinators. For space reasons, we do not discuss the \nimplementation of catalysts, which is fairly straightforward. 5.1 The basic approach When invoked, reagents \nattempt to react, which is conceptually a two phase process: .rst, the desired reaction is built up; \nsecond, the reaction is atomically committed. We emphasize conceptually because, as discussed in the \nintroduction, reagents are designed to avoid this kind of overhead in the common case. We .rst discuss \nthe general case (which imposes overhead) but return momentarily to the common (no overhead) case. An \nattempt to react can fail during either phase. A failure dur\u00ading the .rst phase, i.e. a failure to build \nup the desired reaction, is always a permanent failure (\u00a73.3). Permanent failures indicate that the reagent \ncannot proceed given current conditions, and should therefore block until another thread intervenes and \ncauses condi\u00adtions to change. On the other hand, a failure during the second phase, i.e. a failure to \ncommit, is always a transient failure (\u00a73.3). Transient failures indicate that the reagent should retry, \nsince the reaction was halted due to active interference from another thread. In general, a reaction \nencompasses three lists: the CASes to be per\u00adformed, the messages to be consumed, and the actions to \nbe per\u00adformed after committing. It thus resembles the redo log used in some STM implementations [15]. \nIn the common case that a reagent performs only one visi\u00adble (\u00a74.1) CAS or message swap, those components \nof the reaction are not necessary and hence are not used. Instead, the CAS or swap is performed immediately, \ncompressing the two phases of reaction. Aside from avoiding extra allocations, this key optimization \nmeans that in the common case a cas or upd in a reagent leads to exactly one executed CAS during reaction, \nwith no extra overhead. When a reaction encompasses multiple visible CASes, a costlier kCAS protocol \nmust be used to ensure atomicity. We discuss the kCAS protocol in \u00a75.5, and the common case single CAS \nin \u00a75.6. In the implementation, Reagent[A,B] is an abstract class all of whose subclasses are hidden. \nThe subclasses roughly correspond to the combinator functions (which are responsible for instantiating \nthem), and instances of the subclasses store the arguments given to their corresponding combinator. Each \nsubclass provides an imple\u00admentation of the tryReact method, which is an abstract method of Reagent[A,B] \nwith the following signature: def tryReact(a: A, rx: Reaction, o.er : O.er[B]): Any The Any type in Scala \nlies at the top of the subtyping hierarchy, akin to Object in Java. Here we are using Any to represent \na union of the type B with the type Failure , where the latter has just two singleton instances, Block \nand Retry, corresponding to permanent and transient failures. In other words, tryReact takes the input \n(type A) to the reagent and the reaction built up so far, and either completes the reaction, returning \na result (type B), or fails, returning one of the failure singletons (Block or Retry). The remaining \nargument, o.er , is used for synchronization and communication between reagents, which we explain next. \n 5.2 Offers Message passing between reagents is synchronous, meaning that both reagents take part in \na single, common reaction. In the im\u00adplementation, this works by one reagent placing an offer to react \n def !(a: A): B = {val backo. = new Backo. def withoutO.er (): B = tryReact(a, empty, null) match { \ncase Block . withO.er() case Retry . backo. .once() if (maySync) withO.er() else withoutO.er () case \nans . ans.asInstanceOf[B] } def withO.er (): B = { val o.er = new O.er[B] tryReact(a, empty, o.er ) match \n{ case (f : Failure ) . if (f == Block) park() else backo..once(o.er ) if ( o.er . rescind ) withO.er \n() else o.er .answer case ans . ans.asInstanceOf[B] } } withoutO.er() } Figure 5. The ! method, de.ned \nin Reagent[A,B] in a location visible to the other. The reagent making the offer ei\u00adther spinwaits or \nblocks until the offer is ful.lled; if it spinwaits, it may later decide to withdraw the offer. The reagent \naccepting the offer combines the accumulated reactions of both reagents, and at\u00adtempts to commit them \ntogether. Ful.lling the offer means, in par\u00adticular, providing a .nal answer value that should be returned \nby the reagent that made the offer. Each offer includes a status .eld, which is either Pending, Rescinded, \nor a .nal answer. Hence, the O.er class is parameterized by the answer type; a Reagent[A,B] will use \nO.er [B]. When ful.lling an offer, a reagent CASes its status from Pending to the desired .nal answer. \nIn addition to providing a basic means of synchronization, the offer data structure is used to resolve \nexternal choices. For example, the reagent swap(ep1) + swap(ep2) may resolve its choices inter\u00adnally \nby ful.lling an existing offer on ep1 or ep2; but if no offers are available, the reagent will post a \nsingle offer to both endpoints, allowing the choice to be resolved externally. Reagents attempting to \nconsume that offer will race to change a single, shared status .eld, thereby ensuring that such choices \nare resolved atomically. Offers are made as part of the same tryReact process that builds and commits \nreactions. The o.er argument to tryReact is non-null whenever an offer is to be made.  5.3 Continuations \nFor implementing backtracking choice and message passing, it is necessary for each reagent to know and \nhave control over the reagents that are sequenced after it. Thus we do not represent the reagent sequencing \ncombinator >> with its own class. In\u00adstead, each reagent records its own continuation, which is another \nreagent. Thus, for example, while the cas combinator produces a reagent of type Reagent[Unit,Unit], the \nCAS class has a parame\u00adter k of type Reagent[Unit,R], and CAS extends Reagent[Unit,R] rather than Reagent[Unit,Unit]. \nThe R stands for (.nal) result. The combinator functions are responsible for mapping from the user-facing \nAPI, which does not use continuations, to the internal reagent subclasses, which do. Each reagent initially \nbegins with the empty continuation, called Commit, the behavior of which is explained in \u00a75.5. The sequencing \ncombinator then merely plumbs together the continuation arguments of its parameters. 5.4 The entry point: \nreacting The code for performing a reaction is given in the ! method def\u00adinition for Reagent[A,B], shown \nin Fig. 5. This method provides two generalized versions of the optimistic retry loops we described in \n\u00a72. The retry loops are written as a local, tail-recursive functions, which Scala compiles down to loops. \nThe .rst retry loop, withoutO.er, attempts to perform the re\u00adaction without making visible offers to \nother reagents. It may, how\u00adever, .nd and consume offers from other reagents as necessary for message \npassing. To initiate the reaction, withoutO.er calls the abstract tryReact method with the input a, an \nempty reaction to start with, and no offer. If the reaction fails in the .rst phase (a permanent failure, \nrepresented by Block), the next attempt must be made with an offer, to set up the blocking/signaling \nprotocol. If the reaction fails in the second phase (a transient failure, rep\u00adresented by Retry), there \nis likely contention over shared data. To reduce the contention, withoutO.er performs one cycle of expo\u00adnential \nbackoff before retrying. If the reagent includes communica\u00adtion attempts, the retry is performed with \nan offer, since doing so increases chances of elimination (\u00a73.3) without further contention. Finally, \nif both phases of the reaction succeed, the .nal answer is returned. The second retry loop, withO.er, \nis similar, but begins by al\u00adlocating an O.er object to make visible to other reagents. Once the offer \nhad been made, the reagent can actually block when faced with a permanent failure; the offer will ensure \nthat the attempted re\u00adaction is visible to other reagents, which may complete it. Blocking is performed \nby the park method provided by Java s LockSupport class. On a transient failure, the reagent spinwaits, \nchecking the of\u00adfer s status. In either case, once the reagent has .nished waiting it attempts to rescind \nthe offer, which will fail if another reagent has ful.lled the offer.3 Initially, the reaction is attempted \nusing withoutO.er, repre\u00adsenting optimism that the reaction can be completed without mak\u00ading a visible \noffer.  5.5 The exit point: committing As mentioned in \u00a75.2, the initial continuation for reagents is \nthe Commit continuation, shown in Fig. 6. The tryReact method of Commit makes the transition from building \nup a Reaction object to actually committing it. If the reagent has made an offer, but has also completed \nthe .rst phase of reaction, the offer must be rescinded before the commit phase is attempted otherwise, \nthe reaction could complete twice. As with the ! method, the attempt to rescind the offer is in a race \nwith other reagents that may be completing the offer. If Commit loses the race, it returns the answer \nprovided by the offer. Other\u00adwise, it attempts to commit the reaction, and if successful simply returns \nits input, which is the .nal answer for the reaction. Committing a reaction requires a kCAS operation: \nk compare and sets must be performed atomically. This operation, which forms the basis of STM, is in \ngeneral expensive and not available through hardware. There are several software implementations that \nprovide nonblocking progress guarantees [1, 7, 18]. Reagents that perform a multiword CAS will inherit \nthe progress properties of the chosen implementation. For our prototype implementation, we have opted \nto use an ex\u00adtremely simple implementation that replaces each location to be CASed with a sentinel value, \nessentially locking the location. As the Reaction object is assembled, locations are kept in address \nor\u00adder, which guarantees a consistent global order and hence avoids dead-and live-lock within the kCAS \nimplementation. The advan\u00ad 3 Even if the reagent had blocked, it is still necessary to check the status \nof its offer, because park allows spurious wakeups.  class Commit[A] extends Reagent[A,A] {def tryReact(a: \nA, rx : Reaction, o.er : O.er [A]) = if ( o.er != null &#38;&#38; !o.er. rescind ) o.er .answer else \nif (rx .commit) a else Retry } class CAS[A,R](ref: Ref[A], ov: A, nv: A, k: Reagent[A,R]) extends Reagent[Unit,R] \n{def tryReact(u: Unit, rx : Reaction, o.er : O.er [R]) = if (! rx .hasCAS &#38;&#38; !k.hasCAS) if ( \nref .cas(ov, nv)) k.tryReact ((), rx , o.er ) else Retry else k.tryReact ((), rx .withCAS(ref, ov, nv), \no.er ) } class Choice[A,B](r1: Reagent[A,B], r2 : Reagent[A,B]) extends Reagent[A,B] {def tryReact(a: \nA, rx : Reaction, o.er : O.er [B]) = r1 . tryReact(a, rx , o.er ) match {case Retry . r2.tryReact(a, \nrx , o.er ) match {case ( : Failure ) . Retry // must retry r1 case ans . ans }case Block . r2.tryReact(a, \nrx , o.er ) case ans . ans }} class Computed[A,B](c: A -Reagent[Unit,B]) extends Reagent[A,B] {def tryReact(a: \nA, rx : Reaction, o.er : O.er [B]) = if (c. isDe.nedAt(a)) c(a). tryReact ((), rx , o.er ) else Block \n} class Swap[A,B,R](ep: Endpoint[A,B], k: Reagent[B, R]) extends Reagent[A,R] {// NB: this code glosses \nover some important details // discussed in the Channels subsection def tryReact(a: A, rx : Reaction, \no.er : O.er [R]) = {if ( o.er != null ) // send message if so requested ep.put(new Message(a, rx, k, \no.er )) def tryFrom(cur: Cursor, failMode: Failure ): Any = cur .getNext match {case Some(msg, next) \n. msg.exchange(k).tryReact(a, rx , o.er ) match {case Retry . tryFrom(next, Retry) case Block . tryFrom(next, \nfailMode) case ans . ans }case None . failMode }tryFrom(ep.dual. cursor , Retry) // attempt reaction \n} Figure 6. Excerpts from the reagent subclasses tage of this implementation, other than its simplicity, \nis that is has no impact on the performance of single-word CASes to references, which we expect to be \nthe common case; such CASes can be per\u00adformed directly, without any awareness of the kCAS protocol. Our \nexperimental results in \u00a76 indicate that even this simple kCAS im\u00adplementation provides reasonable performance \nmuch better than STM or coarse-grained locking but a more sophisticated kCAS would likely do even better. \n 5.6 Shared state The implementation of the cas combinator is given by the CAS class, shown in Fig. \n6. Its tryReact method is fairly simple, but it illustrates a key optimization we have mentioned several \ntimes: if neither the reaction so far nor the continuation of the cas are per\u00adforming a CAS, then the \nentire reagent is performing a single CAS, and can thus attempt the CAS immediately. This optimization \nelim\u00adinates the overhead of creating a new Reaction object and employ\u00ading the kCAS protocol, and it means \nthat lock-free algorithms like TreiberStack and MSQueue behave just like their hand-written counterparts. \nIf, on the other hand, the reagent may perform a kCAS, then the current cas is recorded into a new Reaction \nobject, which is passed to the continuation. In either case, the continuation is invoked with the unit \nvalue as its argument. 5.7 Choice The implementation of choice (Fig. 6) is pleasantly simple. It attempts \na reaction with either arm of the choice, going in left to right order. As explained in \u00a73.3, a permanent \nfailure of choice can only result from a permanent failure of both arms. Also, note that the right arm \nis tried even if the left arm has only failed transiently. 5.8 Computed reagents The implementation \nof computed reagents (Fig. 6) is exactly as described in \u00a74.1: attempt to execute the stored computation \nc on the argument a to the reagent, and invoke the resulting reagent with a unit value. If c is not de.ned \nat a, the computed reagent issues a permanent failure. The implementation makes clear that the invisible \nreads and writes performed within the computation c do not even have access to the Reaction object, and \nso cannot enlarge the atomic update performed when it is committed.  5.9 Channels We represent each \nendpoint of a channel as a lock-free bag (which can itself be built using reagents). The lock-freedom \nallows mul\u00adtiple reagents to interact with the bag in parallel; the fact that it is a bag rather than \na queue trades a weaker ordering guarantee for increased parallelism, but any lock-free collection would \nsuf.ce. The endpoint bags store messages, which wrap offers with ad\u00additional data from the sender: case \nclass Message[A,B,C]( payload: A, // sender s actual message data senderRx: Reaction, // sender s checkpointed \nreaction senderK: Reagent[B,C], // sender s reagent continuation o.er : O.er[C]) // sender s o.er Each \nmessage is essentially a checkpoint of a reaction in progress, where the reaction is blocked until the \npayload (of type A) can be swapped for a dual payload (of type B). Hence the stored sender continuation \ntakes a B for input; it returns a C, which matches the .nal answer type of the sender s offer. The core \nimplementation of swap is shown in the Swap class in Fig. 6. If an offer is being made, it must be posted \nin a new mes\u00adsage on the endpoint before any attempt is made to react with exist\u00ading offers. This ordering \nguarantees that there are no lost wakeups: each reagent is responsible only for those messages posted \nprior to it posting its own message.4 Once the offer (if any) is posted, tryReact peruses messages on \nthe dual endpoint using the tail-recursive loop, tryFrom. The 4 Our earlier work [29] with Russo on scalable \njoin patterns gives a more detailed explanation of this protocol and its liveness properties.  loop \nnavigates through the dual endpoint s bag using a simple cursor, which will reveal at least those messages \npresent prior to the reagent s own message being posted to its endpoint. If a message is found, tryFrom \nattempts to complete the reaction; the exchange method combines the reaction and continuation of the \nlocated message with those of the reagent executing it, and actually performs the payload swap. If the \nreaction is successful, the .nal result is returned (and the result for the other reagent is separately \nwritten to its offer status). If the reaction fails, tryFrom continues to look for other messages. If \nno messages remain, swap behaves as if it were a disjunction: it fails permanently only if all messages \nit encountered led to permanent failures. The code sketch we have given for channels glosses over several \ndetails of the real implementation, including avoiding ful.lling one s own offer or a single offer multiple \ntimes, and allowing multiple interactions between two reagents within a single reaction. 6. Performance \nFine-grained concurrent data structures are usually evaluated by targeted microbenchmarking, with focus \non contention effects and .ne-grained parallel speedup [2, 7, 9, 10, 12, 19, 20, 24]. In ad\u00addition to \nthose basic aims, we wish to evaluate (1) the extent to which reagent-based algorithms can compete with \ntheir hand-built counterparts and (2) whether reagent composition is a plausible ap\u00adproach for scalable \natomic transfers. To this end, we designed a se\u00adries of benchmarks focusing on simple lock-free collections, \nwhere overhead from reagents is easy to gauge. Each benchmark consists of n threads running a loop, where \nin each iteration they apply one or more atomic operations on a shared data structure and then sim\u00adulate \na workload by spinning for a short time. For a high contention simulation, the spinning lasts for 0.25\u00b5s \non average, while for a low contention simulation, we spin for 2.5\u00b5s. In the PushPop benchmark, all of \nthe threads alternate push\u00ading and popping data to a single, shared stack. In the StackTrans\u00adfer benchmark, \nthere are two shared stacks, and each thread pushes to one stack, atomically transfers an element from \nthat stack to the other stack, and then pops an element from the second stack; the direction of movement \nis chosen randomly. The EnqDeq and QueueTransfer benchmarks are analogous, but work with queues instead. \nThe stack benchmarks compare our reagent-based TreiberStack to (1) a hand-built Treiber stack, (2) a \nmutable stack protected by a single lock, and (3) a stack using STM. The queue benchmarks compare our \nreagent-based MSQueue to (1) a hand\u00adbuilt Michael-Scott queue, (2) a mutable queue protected by a lock, \nand (3) a queue using STM. For the transfer benchmarks, the hand\u00adbuilt data structures are dropped, since \nthey do not support atomic transfer; for the lock-based data structures, we acquire both locks in a .xed \norder before performing the transfer. We used the Multiverse STM, a sophisticated open-source im\u00adplementation \nof Transaction Locking II [3] which is distributed as part of the Akka package for Scala.5 Our benchmarks \nwere run on a 3.46Ghz Intel Xeon X5677 (Westmere) with 32GB RAM and 12MB of shared L3 cache. The machine \nhas two physical proces\u00adsors with four hyperthreaded cores each, for a total of 16 hardware threads. \nL1 and L2 caches are per-core. The software environment includes Ubuntu 10.04.3 and the Hotspot JVM 6u27. \nThe results are shown in Fig. 7; the x-axes show thread counts, while the y-axes show throughput (iterations/\u00b5s, \nso larger numbers are better). The reagent-based data structures perform universally better than the \nlock-or STM-based data structures. The results show that reagents can plausibly compete with hand-built \nconcur\u00ad 5 Although TL2 is intended for more complex data structures than the ones tested here, it is \nby far the most readily available production STM in the Java/Scala ecosystem. rent data structures, while \nproviding scalable composed operations that are rarely provided for such data structures. 7. Related \nwork 7.1 Concurrent ML Concurrent ML [22] was designed to resolve an apparent ten\u00adsion between abstraction \nand choice: if protocols are represented abstractly as functions, it is impossible to express the choice \nof two abstract protocols. The solution is higher-order concurrency, a code-as-data approach in which \nsynchronous message-passing protocols are represented abstractly as events. CML s events are built up \nfrom combinators, including a choice combinator, commu\u00adnication combinators, and combinators for arbitrary \ncomputations not involving communication. Reagents are clearly in.uenced by the design of CML s events, \nand include variants of CML s core event combinators (communication and choice). But where CML is aimed \nsquarely at capturing synchronous communication pro\u00adtocols, reagents are designed for writing and tailoring \n.ne-grained concurrent data structures and synchronization primitives. This dif\u00adference in motivation \nled us to include a number of additional com\u00adbinators, including those dealing directly with shared state. \nOriginally, CML was focused on managing concurrency rather than pro.ting from parallelism, and this focus \nwas re.ected in its implementation. More recently, a parallel implementation of CML was proposed [23]. \nThe key challenge is resolving uses of choice both consistently and scalably. It is addressed by sharing: \nan event making a choice is enrolled as offering a communication corresponding to each possible choice, \nand when a communica\u00adtion is accepted, the (single, shared) event is atomically marked as consumed. We \nfollow a similar strategy in dealing with message passing, but where Parallel CML uses lock-based queues \nto store messages, we show how to use lock-free bags for increased par\u00adallelism (\u00a75). We also show how \nto incorporate choice resolution with shared-state updates and our conjunction combinators.  7.2 Software \ntransactional memory Software transactional memory was originally intended to provide a general highly \nconcurrent method for translating sequential object implementations into non-blocking ones [25]. This \nambitious goal has led to a remarkable research literature, which has been summa\u00adrized in textbook form \n[15]. Much of the research is devoted to achieving scalability on multiprocessors or multicores, sometimes \nby relaxing consistency guarantees or only providing obstruction\u00adfreedom rather than lock-freemdom [12]. \nReagents, on the other hand, are aimed at a less ambitious goal: enabling the concise expression, user \ntailoring, and composition of .ne-grained concurrent algorithms. That is, unlike STM, reagents do not \nattempt to provide a universal .ne-grained concurrent algo\u00adrithm. Instead, they assist in writing and \nusing speci.c algorithms. There is a clear tradeoff. Using STM, one can implement a concurrent stack \nor queue by simply wrapping a sequential version in an atomic block, which requires no algorithmic insight \nand is simpler than the stack or queue we give in \u00a73. But even with a very clever STM, these implementations \nare unlikely to scale as well as our elimination stack or Michael-Scott queue; some evidence for that \nis shown in \u00a76. Reagents carve out a middle ground between completely hand\u00adwritten algorithms and the \ncompletely automatic atomic blocks of STM. When used in isolation, reagents are guaranteed to perform \nonly the CASes that the hand-written algorithm would, so they in\u00adtroduce no overhead on shared-memory \noperations; by recoding an algorithm use reagents, you lose nothing. Yet unlike hand-written algorithms, \nreagents can be composed using choice, tailored with new blocking behavior, or combined into larger atomic \nblocks.  Threads (on 16-way machine) Figure 7. Benchmarking results Haskell s STM [8] was inspirational \nin showing that transac\u00adtions can be represented via monads [21], explicitly composed, and combined with \nblocking and choice operators. Reagents also form a monad, but we have chosen an interface closer to \narrows [14], to encourage static reagent layout wherever possible (\u00a74.1). Like orElse in Haskell s STM, \nour choice operator is left-biased. But unlike orElse , our choice operator will attempt the right-hand \nside even when the left-hand side has only failed transiently (rather than permanently).6 While the distinction \nappears technical, it is crucial for supporting examples like the elimination backoff stack (\u00a73.3). \n 7.3 Transactions that communicate A central tenet of transactions is isolation: transactions should \nnot be aware of concurrent changes to memory from other transactions. But sometimes it is desirable for \nconcurrent transactions to coor\u00addinate or otherwise communicate while executing. Recent papers have proposed \nmechanisms for incorporating communication with STM, in the form of Communicating Transactions [16], \nTransac\u00adtion Communicators [17], and Transactions with Isolation and Co\u00adoperation (TIC, [27]). A key \nquestion in this line of work is how the expected isolation of shared memory can safely coexist with \ncon\u00adcurrent communication. Communicating Transactions use explicit, asynchronous message passing to communicate; \nthe mechanism is entirely separate from shared memory, which retains isolation. On the other hand, Transaction \nCommunicators and TIC allow isola\u00adtion to be weakened in a controlled way. Our mixture of message-passing \nand shared-state combinators most closely resembles the work on Communicating Transactions. Of course, \nthe most important difference is in the way we deal with shared state discussed in \u00a77.2. We also believe \nthat syn\u00ad 6 Note that retry in Haskell s STM signals a permanent failure, rather than an optimistic retry. \nchronous communication is better for expressing patterns like elim\u00adination (\u00a73.3), since they rely on \nparticipants being mutually-aware. There has also been work treating pure message-passing in a transactional \nway. Transactional Events [4] combines CML with an atomic sequencing operator. Previously, Transactional \nEvents were implemented on top of Haskell s STM, relied on search threads7 for matching communications, \nand used an STM-based represen\u00adtation of channels. However, Transactional Events are expressible using \nreagents, through the combination of swap and the conjunc\u00adtion combinators. Doing so yields a new implementation \nthat does not require search threads, performs parallel matching for commu\u00adnication, and represents channels \nas lock-free bags. We are not in a position to do a head-to-head comparison, but based on the results \nin \u00a76, we expect the reagent-based implementation to scale better on .ne-grained workloads. Another message-passing \nsystem with a transactional .avor is the Join Calculus [6], discussed in \u00a73.7. The treatment of conjunc\u00adtion, \nmessage passing, and catalysis is essentially inherited from our previous work giving a scalable implementation \nof the join cal\u00adculus [29].  7.4 Composing .ne-grained concurrent data structures Most of the literature \non .ne-grained concurrent data structures is focused on within-object atomicity, for example developing \nalgorithms for inserting or removing elements into a collection atomically. However, there has been some \nrecent work studying the problem of transferring data atomically between such .ne-grained data structures \n[2]. The basic approach relies on a kCAS operation in much the same way that reagent sequencing does. \nThe transfer methods must be de.ned manually, in advance, and with access to 7 The implementation can \nbe made to work with a single search thread at the cost of lost parallelism.  the internals of the relevant \ndata structures, whereas reagents allow arbitrary new compositions, without manual de.nition, and without \naccess to the code or internals of the involved data structures. Nevertheless, the implementation of \nreagent composition yields an algorithm very similar to the manually-written transfer methods. It is \nalso possible to go in the other direction: start from STM, which provides composition, and add an escape \nhatch for writing arbitrary .ne-grained concurrent algorithms within the scope of a transaction. The \nescape hatch can be provided through unlogged reads and/or writes to memory locations being used by transactions, \nas in early release [12] or elastic transactions [5]. As we discussed above (\u00a77.2), we favor an approach \nwhere the focus is foremost on writing .ne-grained algorithms, with guarantees about the perfor\u00admance \nand shared-memory semantics of those algorithms. Provid\u00ading such guarantees via an escape hatch mechanism \nmay be dif.cult or impossible, depending on the details of the STM implementa\u00adtion. As we showed in \u00a73, \nit is also very useful to have combinators for choice, message-passing, and blocking, if one wishes to \ncapture the full range of .ne-grained concurrent algorithms. 8. Conclusion and planned work Reagents \nblend together message passing and shared state concur\u00adrency to provide an abstract, compositional way \nof writing .ne\u00adgrained concurrent algorithms. We see this work as serving the needs of two distinct groups: \nconcurrency experts and concurrency users. Using reagents, experts can write libraries more easily, be\u00adcause \ncommon patterns are expressible as abstractions and many are built-in. Users can then extend, tailor \nand compose the result\u00ading library without detailed knowledge of the algorithms involved. There is signi.cant \nremaining work for elucidating both the the\u00adory and practice of reagents. On the theoretical side, developing \na formal operational semantics would help to clarify the interactions possible between shared state and \nmessage passing, as well as the atomicity guarantees that reagents provide. On the practical side, developing \na serious concurrency library using reagents would go a long way toward demonstrating their usability. \nIn future work, we plan to pursue both of these goals. In addition, we plan to ex\u00adpand the scope of reagents \nto include .ne-grained locking as well as non-blocking data structures. Acknowledgments Thanks to Claudio \nRusso, Vincent St-Amour, Sam Tobin-Hochstadt, Jesse Tov and Mitchell Wand for insightful discussions \nduring the development of this work. Pete Manolios provided access to bench\u00admarking equipment. Thanks \ngo to Neil Torronto for the develop\u00adment of the new Racket plot library. The anonymous reviewers provided \nthorough reviews which led to several improvements to the presentation. Finally, I greatly appreciate \nthe generous .nancial support of Microsoft Research. References [1] H. Attiya and E. Hillel. Highly-concurrent \nmulti-word synchroniza\u00adtion. In ICDCN, 2008. [2] D. Cederman and P. Tsigas. Supporting lock-free composition \nof concurrent data objects. In CF, 2010. [3] D. Dice, O. Shalev, and N. Shavit. Transactional locking \nII. DISC, 2006. [4] K. Donnelly and M. Fluet. Transactional events. JFP, 18(5 &#38; 6):649 706, Oct. \n2008. [5] P. Felber, V. Gramoli, and R. Guerraoui. Elastic transactions. In DISC, 2009. [6] C. Fournet \nand G. Gonthier. The re.exive chemical abstract machine and the join-calculus. In POPL, 1996. [7] K. \nFraser and T. Harris. Concurrent programming without locks. TOCS, 25(2), May 2007. [8] T. Harris, S. \nMarlow, S. Peyton-Jones, and M. Herlihy. Composable memory transactions. In PPOPP, Aug. 2005. [9] D. \nHendler, N. Shavit, and L. Yerushalmi. A scalable lock-free stack algorithm. In SPAA, Jan. 2004. [10] \nD. Hendler, I. Incze, N. Shavit, and M. Tzafrir. Flat combining and the synchronization-parallelism tradeoff. \nIn SPAA, 2010. [11] M. Herlihy and N. Shavit. The Art of Multiprocessor Programming. Morgan Kaufmann, \n2008. [12] M. Herlihy, V. Luchangco, M. Moir, and W. Scherer III. Software transactional memory for dynamic-sized \ndata structures. In PODC, 2003. ISBN 1581137087. [13] M. P. Herlihy and J. M. Wing. Linearizability: \na correctness condition for concurrent objects. TOPLAS, 12(3):463 492, 1990. [14] J. Hughes. Generalising \nmonads to arrows. Science of computer programming, 37(1-3):67 111, May 2000. [15] J. Larus and R. Rajwar. \nTransactional memory. Morgan and Claypool, 2006. [16] M. Lesani and J. Palsberg. Communicating memory \ntransactions. In PPOPP, 2011. [17] V. Luchangco and V. Marathe. Transaction communicators: enabling cooperation \namong concurrent transactions. In PPOPP, 2011. [18] V. Luchangco, M. Moir, and N. Shavit. Nonblocking \nk-compare\u00adsingle-swap. In SPAA, Dec. 2003. [19] J. Mellor-Crummey and M. Scott. Algorithms for scalable \nsynchro\u00adnization on shared-memory multiprocessors. TOCS, 9(1):21 65, 1991. [20] M. M. Michael and M. \nL. Scott. Simple, fast, and practical non\u00adblocking and blocking concurrent queue algorithms. In PODC, \n1996. [21] S. Peyton Jones and P. Wadler. Imperative functional programming. In POPL, 1993. [22] J. Reppy. \nCML: a higher order concurrent language. In PLDI, 1991. [23] J. Reppy, C. Russo, and Y. Xiao. Parallel \nconcurrent ML. In ICFP, Aug. 2009. [24] W. Scherer III and M. Scott. Nonblocking concurrent data structures \nwith condition synchronization. In DISC. Springer, 2004. [25] N. Shavit and D. Touitou. Software transactional \nmemory. Distributed Computing, 10(2):99 116, Feb. 1997. [26] N. Shavit and D. Touitou. Elimination trees \nand the construction of pools and stacks. Theory of Computing Systems, 30:645 670, 1997. [27] Y. Smaragdakis, \nA. Kay, R. Behrends, and M. Young. Transactions with isolation and cooperation. In OOPSLA, 2007. [28] \nR. Treiber. Systems programming: Coping with parallelism. Technical report, IBM Almaden Research Center, \n1986. [29] A. Turon and C. V. Russo. Scalable Join Patterns. In OOPSLA, 2011. [30] P. Wadler. The essence \nof functional programming. In POPL, 1992.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Efficient communication and synchronization is crucial for fine grained parallelism. Libraries providing such features, while indispensable, are difficult to write, and often cannot be tailored or composed to meet the needs of specific users. We introduce <i>reagents</i>, a set of combinators for concisely expressing concurrency algorithms. Reagents scale as well as their hand-coded counterparts, while providing the composability existing libraries lack.</p>", "authors": [{"name": "Aaron Turon", "author_profile_id": "81418594363", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P3471183", "email_address": "turon@ccs.neu.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254084", "year": "2012", "article_id": "2254084", "conference": "PLDI", "title": "Reagents: expressing and composing fine-grained concurrency", "url": "http://dl.acm.org/citation.cfm?id=2254084"}