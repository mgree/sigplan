{"article_publication_date": "06-11-2012", "fulltext": "\n Synthesising Graphics Card Programs from DSLs Luke Cartey Rune Lyngsoe Oege de Moor University of Oxford \nUniversity of Oxford University of Oxford luke.cartey@cs.ox.ac.uk lyngsoe@stats.ox.ac.uk oege.de.moor@cs.ox.ac.uk \n Abstract Over the last .ve years, graphics cards have become a tempting tar\u00adget for scienti.c computing, \nthanks to unrivaled peak performance, often producing a runtime speed-up of x10 to x25 over comparable \nCPU solutions. However, this increase can be di.cult to achieve, and doing so often requires a fundamental \nrethink. This is especially problematic in scienti.c computing, where experts do not want to learn yet \nanother architecture. In this paper we develop a method for automatically parallelis\u00ading recursive functions \nof the sort found in scienti.c papers. Using a static analysis of the function dependencies we identify \nsets partitions of independent elements, which we use to synthesise an e.cient GPU implementation using \npolyhedral code generation techniques. We then augment our language with DSL extensions to support a \nwider variety of applications, and demonstrate the ef\u00adfectiveness of this with three case studies, showing \nsigni.cant per\u00adformance improvement over equivalent CPU methods, and similar e.ciency to hand-tuned GPU \nimplementations. Categories and Subject Descriptors I.2.2 [Automatic Program\u00adming]: Program Synthesis; \nD.1.3 [Concurrent Programming]: Parallel Programming General Terms Language, Performance Keywords gpu, \nscienti.c applications, program synthesis, dy\u00adnamic programming 1. Introduction Recent advances in graphics \ncard design have embraced the rapid pace of change predicted by Moore s law to unlock a mass market massively-parallel \narchitecture, with phenomenally high potential peak performance. Unfortunately, this great power also \ncomes at a cost: an architecture that is often di.cult to program e.ectively and e.ciently. The problem \nis twofold we must identify a suitable parallel algorithm and we must use the resources of the device \ne.ciently. Whilst we have a comprehensive understanding of how to map graphical problems to massively-parallel \ndevices, for many other classes of problems it is less clear what parallel algorithm might be suitable. \nMuch of the work so far has pragmatically focused on data-parallelism; applying the same problem to a \nlarge data-set by Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright &#38;#169;c2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00  assigning a single problem to each thread. We want to focus on those problems which require \na more nuanced algorithm to achieve peak performance on the GPU. With the area in an embryonic stage, \nlanguage support has often been aimed at extending existing languages with parallel primitive operations. \nHowever, a growing number of people are approaching from a di.erent angle by providing high-level languages \nor speci.cations which are automatically parallelised (Liszt &#38; OptiML[2], FLAME[15], Nikola[18] and \nObsidian[7]) In this paper we propose a system for synthesising graphics card programs from recursion \nequations. We develop a simple language for describing such recursions, then augment those equations \nwith domain-speci.c notations. This allows us to support a wide-range of recursive problems in many di.erent \ndomains. We build on existing work to develop an automatically parallelising end-to-end compiler that \ngenerates appropriate target code for NVIDIA GPUs. 1.1 Graphics Cards Modern graphics cards are powered \nby massively parallel proces\u00adsors. These typically consist of a large number of cores hun\u00addreds to thousands \n arranged in a series of multi-processors. Each multi-processor contains a number of cores that work \nsyn\u00adchronously to achieve parallelisation, and each multi-processor acts independently of any other \nthere are no global synchronisation mechanisms. Indeed, this is a deliberate design decision to reduce \nthe complexity of the hardware. In practice, this forces us to use the GPU in one of two ways: an inter-task \nparallel system where each thread solves one problem, and so all threads on a multiprocessor compute \nthe same instruction for di.erent problems in sync, and an intra-task parallel system, where blocks of \nthreads work co-operatively on the same problem. Inter-parallel systems tend to be more straight forward, \nand focused on data-parallelism. We will focus on problems suited to intra\u00adparallel solutions in this \npaper. 1.2 Domain Speci.c Languages Domain Speci.c Languages (DSLs) di.er from general-purpose languages \nin that they may restrict the language or make as\u00adsumptions about the input, in order to produce optimised \noutput code[19]. We can use the knowledge derived from the concise in\u00adput to provide both static and \ndynamic optimisations. Domain users also gain bene.ts from such a language an interface for describ\u00ading \ntheir problems in the way they understand them, which will often lead to better code and more opportunities \nfor reuse. Our ap\u00adproach in this paper is to allow scientists to describe their problems in the same \nway they describe them in papers and other literature. 1.3 Scienti.c Problems Some of the most demanding \nusers of high-performance hardware are from the scienti.c community. In this paper, we will use case \nstudies from bioinformatics to motivate our approach. It is an ideal match typical problems require \nfeatures with obvious parallel potential, such as large data-sets and extensive search spaces. Com\u00admon \napplications include sequence alignment[3], gene .nding[10], homology search[3] and structure prediction \napplications. Almost every practical application listed uses a form of recursive equation to de.ne and \ndescribe the problem, often with an associated data model, such as a matrix, Hidden Markov Model or graph \nstructure. Our choice of bioinformatics as a case study does not represent a fundamental limit to our \napproach, and we expect that our frame\u00adwork could support many problems in other scienti.c domains and \nbeyond.  1.4 Contributions In this paper we develop a minimal language for describing recur\u00adsive functions \nthat can then be automatically parallelised. Further\u00admore, we show how we can ful.ll this promise by \nadding domain speci.c extensions. Our contributions are: We design a simple host language for recursive \nproblems (Sec\u00adtion 4). One of our main contributions is to describe how this simple, but nonetheless \nexpressive, language can be used to au\u00adtomatically synthesise a program targeting a massively paral\u00adlel \nprocessor such as a GPU. The synthesis takes the following form: We use existing techniques[9, 11] to \nframe the parallelisa\u00adtion problem as one of partitioning the call-graph into an ordered list of groups, \nwhere all members of a group are in\u00addependent. We use a scheduling function which determines, for each \ncall, to which partition it belongs (Section 4.4). We show how the scheduling function can be used to \nsyn\u00adthesise a graphics card program using a polyhedral code\u00adgeneration tool, CLooG[1](Section 4.3). We \nde.ne what criteria a scheduling function must satisfy in order to be valid for a given recursive equation. \nThis is used as the foundation for an analysis that will verify user\u00adprovided schedules (Section 4.5). \nA recursive equation will usually be amenable to multiple partition schemes. We use the identi.ed criteria \nto frame the search for a schedule. We develop a technique for automat\u00adically .nding valid and minimal \nschedules (Section 4.6). Many scienti.c problems can be written as recursive problems over complex data \nstructures. We show how limited extensions to our host language can be made, and, importantly, what do\u00admain \nspeci.c extensions can be made without losing the advan\u00adtages of the automatic parallelisation (Section \n5).  We evaluate our approach for a variety of applications in our target domain, comparing against \nstandard CPU approaches as well as hand-optimized GPU solutions (Section 6).  2. Anatomy of a scienti.c \nrecursive problem The edit-distance problem is a straight-forward example of a recur\u00adsive problem. It \nasks us to compare two strings to .nd the minimum cost of making a series of edit or match operations \nto transform one string to another. It is one that is relevant in scienti.c domains such as bioinformatics, \nwhere the Smith-Waterman algorithm is an edit distance algorithm for aligning two biological sequences. \nThe Principle of Optimality allows us to frame the problem as a combination of a series of sub-problems, \nand so we can provide a recursive de.nition to determine the solution. For the edit-distance algorithm, \nas for many optimisation algorithms, this provides a natural and intuitive way of describing the solution \n(see Figure 1). This form of declaration has an implicit method of evaluation, a recursive function, \nwhich is most naturally solved in a serial fashion, one call at a time. However, for many problems, this \n. x if y = 0 d(x, y) = . . y d(x - 1, y - 1). d(x - 1, y), . if x = 0 if s[x] = t[y] . . min . . d(x, \ny - 1), d(x - 1, y - 1) . . + 1 otherwise Figure 1. The edit distance recursion on strings s and t. \n is not the most e.cient solution. For example, we may repeat computations unnecessarily when two di.erent \ncalls depend on the same computed value. Dynamic programming permits us to store those repeated computations, \nor even tabulate the result bottom up d(0, 0) .rst, followed by dependent computations. This leads to \nanother example of redundancy in the edit-distance problem once we have computed d(0, 0), it does not \nmatter whether we compute d(1, 0) or d(0, 1) as they are independent that is, they do not depend on \neach others values. This opens up the possibility of concurrently executing both d(1, 0) and d(0, 1). \nSimilarly, we can observe that any set of cells on an x + y diagonal line are independent, and can be \ncomputed concurrently provided that all prior dependencies are solved. The edit-distance problem illustrates \nhow recursive problems often have a useful form of concurrency, and whilst edit-distance problems have \nfrequently been parallelised in this way, many sim\u00adilar recursive problems have not, and new ones constantly \nemerge. Given that these problems speci.cally in bioinformatics are often large or long running, it \nwould be ideal if we could exploit any available concurrency using massively-parallel processors of the \nsort available in graphics cards. The question is, can we develop a technique for mapping a recursive \nde.nition to a parallel environment? Such an approach would permit us to use recursions as a host language \nfor describing parallel problems. This paper seeks to answer that question. 2.1 Dependent computations \nFor this, and further sections, we will assume that we have a single recursive problem that is to be \ncomputed in a bottom-up fashion using dynamic programming. The .rst step to answering our question is \nto acknowledge that it is the recursive dependencies that will determine what, if any, form of concurrency \nwill be available to us. Any parallel scheme we choose will need to respect the dependencies in the problem. \nd0,0 i1 d1,0 d0,1 i2  d2,0 d1,1 d0,2 i3    d2,1 d1,2 i4    d2,2 i5 (a) Edit distance (b) \nFibonacci Figure 2. An example of the di.erence between a linear (2b) and a wide dependency graph (2a) \n. Partition 0 Partition 1 Partition 2 Partition 3 Partition 4 Figure 3. A valid diagonal schedule for \nthe 3x3 edit distance problem. This schedule has .ve partitions. Figure 2 contains the contrasting dependency \ngraphs for the edit distance algorithm on a 3x3 problem and a straight-forward recur\u00adsive de.nition of \nthe Fibonacci sequence for .b(5). It is clear that the Fibonacci recursion permits no parallel solution \n analysing each sub-problem only unlocks one other sub-problem. In compar\u00adison, sub-problems in the edit \ndistance recursion can clearly un\u00adlock multiple other sub-problems once we compute d0,0, we can compute \neither d1,0 or d0,1. We say that the computation of d1,0 is independent of d0,1, and therefore the problem \nprovides no con\u00adstraints on the order of evaluation between the two. Consequently, we can evaluate them \none after another, or even evaluate them si\u00admultaneously. It is this independence property within problems \nthat we hope to exploit when identifying a parallel solution.  2.2 Scheduling partitions Any form of \nconcurrency requires that we identify a number of computations that can be executed simultaneously. This \nis espe\u00adcially important when we are developing an intra-multiprocessor application for a graphics card, \nwhere all threads must work in sync with each other. In a .nite recursive problem, such as the edit distance \nproblem, we might think of this as dividing the elements in the domain that is, the recursive calls \nin the dependency graph into sets of computations that can be concurrently executed. Our intuitive understanding \nof can be concurrently executed here should be that the computations are independent, that is, the elements \nin the set should not depend on any other elements in the set, either directly or indirectly. It is clear \nthat a single problem may permit many di.erent partitions, and that partitions may even overlap. Each \npartition may also require some dependencies to have been computed before the partition itself can be \nevaluated. To this end we de.ne a schedule, a group of partitions that together cover the entire domain \nand do not overlap. As implied by the term schedule, this de.nes a total order between partitions in \nthe domain, providing a .xed sequence of execution and simplifying the dependency graph. Figure 3 illustrates \na valid scheduling of the 3x3 edit distance problem. We can think of each partition as a time\u00adstep of \nthe computation. Typically a schedule for a function f is de.ned as a map\u00adping or function from the original \ndomain to the partitions of the schedule[9]. We denote this schedule as Sf the scheduling func\u00adtion \nfor f . For example, if we have two sets of arguments to f , x\u00afand y\u00af, then if Sf (\u00afx) = Sf (\u00afy) we know \nthat we can independently evaluate the results f (\u00afx) and f (\u00afy). In our edit distance example, if Sd(0, \n1) = Sd(1, 0) then we can deduce that d(0, 1) and d(1, 0) can be computed independently of each other. \n We note that any technique that de.nes Sf explicitly as a direct enumeration of the cells of each partition \nis doomed to fail to do so would be both memory intensive and di.cult to determine e.ciently a priori \non problems of varying sizes. Instead, we make the analysis tractable and the implementation possible \nby restricting the schedule to be an a.ne function. A.ne functions allow us to describe a wide-variety \nof regular partitions of the dependency graph with a single function, which can then be used to synthesise \na parallel solution. In the edit distance example (Figure 3) it is clear that an ap\u00adpropriate scheduling \nfunction is Sd = x + y the parallel lines which represent independent values are described by the equation \nx + y = c, where c is the current partition. Figure 4 illustrates some schedules commonly found in scienti.c \nproblems. 2.3 Choosing a schedule It is clear that not all schedules are created equal. We must therefore \nde.ne some criteria for choosing an appropriate schedule such that it is both valid and e.cient. As we \npreviously observed, the validity of a schedule is deter\u00admined by the dependencies described in the recursive \nfunction. In other words, a valid schedule is one that ensures that any depen\u00addency between two elements \nis ful.lled by executing the depen\u00addent before the dependee. We can do this in an inductive fashion, \nby proving any direct dependencies of an element are ful.lled be\u00adfore evaluating that element. What are \nthe dependencies of f ? The recursive calls of f ; each di.erent set of arguments to a recursive call \nconstitutes a potential dependency. We will formalise this choice by identifying from each recursive \ncall a criterion on valid schedules. The key condition which we wish to maintain is that the partition \n e.g time-step of the recursive call is less than that of the current call. Take, for example, the edit \ndistance algorithm in Figure 1. It has three recursive calls d(x - 1, y), d(x, y - 1) and d(x - 1, y \n- 1). We will therefore need to prove that the following equations hold: Sd (x, y) > Sd(x - 1, y), Sd(x, \ny) > Sd(x, y - 1) Sd(x, y) > Sd(x - 1, y - 1) If, as before, we select Sd (x, y) = x + y, it is clear \nthat all three are true, therefore this is a valid schedule. An e.cient schedule is one which will make \nbest use of the re\u00adsources available. In an ideal world, we would take in to considera\u00adtion the exact \ncon.guration of the hardware available including the number of cores and their purpose. However, it \nmay not be prac\u00adtical to determine this precisely. We will instead aim to minimise the number of partitions \nwe need to evaluate, as that will provide a reasonable proxy for an e.cient solution. This maximises \nthe av\u00aderage size of each partition. Again, if we consider the edit distance example, an equally valid \nschedule would be Sd(x, y) = 2x + y, however this results in more partitions and would therefore be considered \na less e.cient schedule. There are very few occasions where a schedule with more partitions will be more \ne.cient. These arguments are further developed in Section 4. 3. A hosted language environment for massively \nparallel architectures In the previous section we have outlined how we might identify a parallel solution \nfor a language that is su.cient to describe recursively de.ned problems at a high-level. However, the \nmajority yx 0 1 2 3 4 5 6 yx 0 1 2 3 4 5 6 yx 0 1 2 3 4 5 6 0 0 0 1 1 1 2 2 2 3 3 3 4 4 4 5 5 5 (a) \nSf = x (b) Sf = y (c) Sf = x + y Figure 4. An example of three parallel strategies for the two dimensional \ncase. Each case highlights the partition where Sf = 4. Gene .nder Sequence Alignment HMM Matrix Applications \nDSLs Language Framework Hardware  Figure 5. The levels of a hosted language environment for mas\u00adsively \nparallel architectures of problems require more than a pure recursive function to describe. In bioinformatics, \nproblems often refer to models, data matrices or other structures or systems. In addition, a simple recursive \nlanguage may not provide the necessary terseness or clarity in the language to aid the domain-focused \nuser. To this end, we will integrate our simple recursive language into a wider framework for hosting \ndomain speci.c languages targeting GPUs. Our system will be designed to match, as closely as is practical, \nthe descriptions of algorithms that scientists typically provide in papers. Figure 5 describes the architecture \nof our system. Each DSL can be written as an extension to our base language, which is a DSL in itself \nfor describing parallel computations.. This approach has a number of bene.ts. It takes the common elements \nfrom many domains, and uses a uni.ed language to de\u00adscribe them. Our work in e.ectively parallelising \nthis host language is therefore shared amongst many di.erent domains, and provides a common language \nbased on how these problems are typically de\u00adscribed. At the same time, the extensions we write for speci.c \ndo\u00admains allows implementors to concentrate on how to parallelise the elements unique to that domain, \nsuch as how to layout a particular model in memory. We later (Section 5) show how our base lan\u00adguage \ncan permit a wide range of extensions without breaking the parallelisation rules. Our language is designed \nto mimic the style of a scripting lan\u00adguage, providing basic operations in this case recursive func\u00adtions \n and allowing users to potentially combine di.erent DSLs. We will provide statements in the language \nfor function de.nition, .le loading, printing and function execution, including a map prim\u00aditive, allowing \nus to map a function to a sequence of arguments. This map primitive supports the high-level inter-multiprocessor \nparallelisation. Func . f uncname varname* Expr Expr . | Expr + Expr | Expr - Expr | Expr \u00d7 Expr | Expr/Expr \nExpr < Expr | Expr > Expr | Expr = Expr | Expr! = Expr | Expr min Expr | Expr max Expr || (Expr) Var[Expr] \n| Var | f uncname(Expr*) || integer if Expr then Expr else Expr Var . varname  Figure 6. The grammar \nfor the domain speci.c language describ\u00ading the permitted operations 3.1 Function de.nition We provide \nthe function de.nition section of our grammar in Fig\u00adure 6. An example of a function for the edit-distance \nproblem is given in Figure 7. We have based it on pure function de.nitions, with common arithmetic operations \nand recursive calls. The recur\u00adsive function calls are only to named functions no higher order or .rst \nclass functions are allowed, and so no pointer analysis is required. This is a key restriction for a \ntractable analysis. In addi\u00adtion, we only support single recursive functions, as the analysis of mutually \nrecursive functions is signi.cantly more di.cult. Recursive calls are also restricted by limiting the \ndescent func\u00adtions that is, the function that is applied to the original argu\u00adments to get the recursive \narguments. Like the scheduling function, and for similar reasons, we will enforce that the descent functions \nare a.ne functions on the parameters. Without restriction, it will be di.cult to enforce that the recursive \ncalls only refer to partitions of a lower value. The majority of applications in bioinformatics are based \non analysis of sequences of characters, and so we provide a sequence primitive which can be queried by \nindex. No further operations are provided on sequences sequences are immutable. In particular, recursion \non sub-sequences is only supported through indices. The only conditional operation we will allow is a \nbranching if expression. To allow branching to occur, we must allow con\u00additional operators, which will \nconsist of the numerical comparison operators, and an equality comparison for characters. Although the \nbase language provides no looping construct, it is an operation that we could add with a domain extension \n e.g looping over a data model. By constraining the allowed functions we permit a tractable analysis \n we do not preclude an analysis of a more comprehen\u00adsive de.nition. We simply take a pragmatic approach \nto support the most commonly found forms for scienti.c applications. 3.2 Type System Our base language \nhas a simple type system which includes the fol\u00adlowing primitive types: integers, characters, sequences, \nindices on sequences, .oats, probabilities, booleans and alphabets. We justify the introduction of a \nprobability type as well as a general .oating point type by the frequency with which bioinformatics applications \nuse probabilities. By introducing a high-level type, we can deter\u00admine an appropriate low-level representation. \nFor example, a fre\u00adquent problem is under.ow when dealing with multiplying small probabilities using \n.oating points. We can counter this by convert\u00ading probabilities to log space, where we can use addition \ninstead of multiplication. Alternatively, we could introduce a custom ex\u00adtended exponent implementation. \n Alphabets de.ne a set of characters; sequence and character types are speci.ed with reference to a particular \nalphabet. In the edit distance example (Figure 7), sequence s and t refer to the English alphabet, denoted \nby en. Indices specify the sequence to which they belong; this is necessary so we can determine the dimensions \nof the recursive problem for analysis and tabulation. In the edit distance example, index i references \nstring s in this way, as does j with string t. To support recursive functions with some invariant parameters, \nwe introduce two classi.cations calling and recursive. These classi.cations are baked in to the compiler \n with each type belonging to either, neither or both. For a type to occur in the parameter declaration \nof a function, it must be either a calling or recursive type. The classi.cations are: Calling type \na type is a calling type when it must be instan\u00adtiated before the problem can begin, and remains constant \nover a single recursive run.  Recursive type a type is a recursive type when it must be speci.ed each \ntime we recurse -it is therefore varying. The prototype for a recursive call is simply all the parameters \nwhose types are annotated as recursive.  Sequences are a calling type, so in the edit distance example \nthe strings (the sequences) s and t are de.ned once and remain constant over a single recursion, while \nwe vary the indices i and j. We have included two types in the framework that can act as recursive parameters \n integers and explicit indices on sequences. Types may be both calling and recursive for example, integer \ntypes, where the initial value determines the size of the domain. This is because the domain of all parameters \nmust be .nite for the method to be applicable, so we must specify an initial value for the integer domain. \nIn the case of indices, they are naturally bound by the size of the sequence they reference. We use this \n.nite nature to our advantage to state that all recur\u00adsive types must de.ne a mapping between elements \nin their domain and the natural numbers. We can therefore assume in our analysis that parameters can \nbe considered as natural numbers.  3.3 Intermediate Representation The target of our high-level framework \nis a low-level intermediate representation we have devised to abstract away the details of a par\u00adticular \nmassively-parallel environment. This allows us to separate out the construction of the overall parallel \nframework from the ex\u00ad 1 int d(seq[en] s, index[s] i, seq[en] t, index[t] j) = 2 if i == 0 then 3 j \n4 else if j == 0 then 5 i 6 else if s[i -1] == t[j -1] then 7 d(i -1, j -1) 8 else 9 (d(i -1, j) min \nd(i, j -1) min d(i -1, j -1)) + 1 Figure 7. The source code of a simple edit distance recursion parfor \nthreads t in 0..tn for p in 0..max_partition: for elements of p assigned to t x0,...,xn = I(p, t) farr[x0,...,xn] \n= f(x0,...,xn) sync Figure 8. The program synthesis template act environment. We only target NVIDIA \nCUDA graphics cards at this stage, but in principle our abstract representation would allow us to support \nother vendors in the future. 4. Parallelising Recursive Functions 4.1 Overview In this section we describe \nour process for synthesising a graphics card program from a recursive de.nition. Our method takes the \nfollowing steps: 1. We .rst encode the dependencies of the recursion as a series of criteria on the scheduling \nfunction (Section 4.5). 2. We use these criteria and a suitable goal function to automati\u00adcally .nd, \nusing a constraint solver, the coe.cients of a valid scheduling function, which is optimal with respect \nto the num\u00adber of partitions required (Section 4.6). 3. Given a scheduling function, we synthesise a \nmassively parallel program using a polyhedral code generation tool, CLooG[1] (Section 4.3). 4. Finally, \nwe discuss the issue of evaluating multiple problems on the same GPU (Section 4.7).  4.2 Scheduling \nFunctions Thus far we have only described the scheduling function infor\u00admally. It maps the elements in \nthe domain of our function f the possible calls to f to partitions. A partition is an integer value, \nthus providing a total ordering over partitions. There may be many valid schedules for each function; \ndi.erent schemes of parallelising the problem are described by di.erent schedules. We will de.ne a schedule \nfor function f to be an a.ne function, named Sf with integer coe.cients: Sf = a1 x1 + ... + anxn Where \na1,..., an . Z and the recursive or source domain of f is de.ned as X = X1,..., Xn, where \u00afx . X when \n\u00afx = (x0,..., xn) and xi . Xi. The constraints on the function ensure that the analysis is tractable \nand the implementation e.cient; they are not, however, fundamental limits and the extent to which they \ncan be lifted will be discussed later in the section. 4.3 Program Synthesis Scheduling functions provide \na succinct way to describe the de\u00adpendencies of a problem. However, to have any practical utility, we \nmust be able to use it to synthesise a practical and e.cient pro\u00adgram. We start by tying this knot as \na way to motivate and guide the search for a schedule. Figure 8 outlines our approach: we will loop over \nthe time-step partitions of the schedule in the domain, computing each partition entirely before synchronising \nand continuing on. Each thread will be allocated a number of elements to compute. For each partition, \neach thread will loop over all assigned elements. The computation of each element proceeds by calling \nsome function I to determine the indices of x associated with the current for (p=0;p<=m+n;p++) { for \n(i=max(0,p-m);i<=min(n,p);i++) { S1(i,p-i); } } Figure 9. CLooG output for the edit distance problem \nwith a scheduling (scattering) function of Sd(x, y) = x + y. parfor threads t in 0..tn { for (p=0;p<=m+n;p++) \n{ for (i=t+max(0,p-m);i<=min(n,p);i+=tn) { x0,x1 = i, (p -i); farr[x0,x1] = f(x0,x1); } sync } } Figure \n10. Our conversion of the CLooG loop. partition, thread and thread-step. We can then calculate the value \nand store it in the dynamic programming array, possibly for future use. To formalise our approach, we \nwill use the polyhedral model[12]. We can consider the original domain of our recursive function a polyhedron \n a convex polyhedron representing the elements of X1,..., Xn that the recursion can visits Our scheduling \nfunction then becomes an a.ne transformation of that source polyhedron to a target polyhedron, and I \nis the inverse of that transformation. The polyhedral model has been extensively researched in relation \nto loop parallelisation, however the principal is the same for recur\u00adsions. Using a scheduling function \nalone as an a.ne transformation creates a mapping from the recursive domain to a single dimension the \ntime-step. We still need iterate through all elements in the domain, assigning them to threads a set \nof space dimensions. CLooG[1] is a code-generation tool that can determine itera\u00adtions over dimensions \nin this way. It takes, as input, the source poly\u00adhedron and the schedule or scattering function, as \nit is described in CLooG. The output of this is a series of nested loops that iterate over the entire \ndomain. In our case, the .rst, outer, loop will be over our partition time-steps. Each further nested \nloop represents a new space dimension of the target polyhedron, and the collection of all those inner \nnested loops iterates over all elements in the partition. This transformation should ensure that each \nelement of the source  recursive domain has an equivalent element in the target do\u00admain. Typically \nthe number of dimensions in the target polyhedron will match that of the source polyhedron.  The code \ngenerated by CLooG will be a set of nested loops for a single thread. We must take one of the space dimension \nloops to use to map to our threads. We choose the outer loop. Given tn threads, with the thread designated \nby t, we can convert this loop: for (int v = <start>; v < <end>; v+=<inc>) by computing the entire range \nin groups of tn threads, like so: for (int v = <start> + t; v < <end>; v+=(<inc>+tn)) Consider the edit \ndistance example, where Sf (x, y) = x + y. Figure 9 describes the output given by CLooG. Figure 10 gives \nour parallel version of the loop.  4.4 Formalising function dependencies The derivation of a suitable \nschedule is the .rst step in our compila\u00adtion process. For that, we will .rst need to formalise the description \nof the dependencies between function calls. Recall that f is a pure function, so we know that the result \nof the function is dependent only on the arguments in the domain. We can therefore de.ne a call, c, as \nthe list of the arguments, y0, ...yn, to the function f , where each yi is a value in the domain of the \nequivalent parameter of f . Let G(C, E) be the call graph for f , where C is the set of possible calls \nand E is a set of edge pairs (c1, c2). The edge (c1, c2) represents the case when c1 may call c2. We \ndo no branch analysis based on conditionals instead, we consider all recursive calls in the function \nto be part of E. We will say c1 . c2 when (c1, c2) . E. Using E we can de.ne the relation the transitive \nclosure of E. We can think of this as the transitive closure of the dependency graph. Validation of \nSf can then be de.ned in terms of the transitive closure: c1 c2 . Sf (c1) > Sf (c2) (1) This is the \npartition ordering condition. The de.nition encom\u00adpasses two important properties. The .rst is that there \nis an implicit ordering over partitions, such that if we evaluate the lowest time\u00adstep partition followed \nby the next time-step partition and so on, we are guaranteed to maintain the order of dependencies required \nby . The second is that two elements in a single partition must be in\u00addependent. A pair of calls, c1, \nc2, are then described as independent i. there is no call path between them, that is c1 P c2. If two \ncalls are independent, they can be computed without reference to each other, and therefore can be computed \nsynchronously. The partition ordering condition implies just that: Sf (c1) = Sf (c2) . c1 P c2 (2) By \n.nding integer coe.cients a1,..., an of Sf = a1 x1 + ... + anxn such that it adheres to 1, we will use \nthis implication of independence to generate a parallel implementation. 4.5 Validity of a schedule We \nhave now de.ned the properties of a valid schedule. However, as formulated in (1), it is di.cult to e.ciently \napply directly to a schedule. In this section, we derive a set of criteria that valid schedules adhere \nto. This has two practical consequences .rstly, users can specify a schedule that we can then verify. \nSecondly, we will use the validity criteria to automatically determine a schedule (Section 4.6). For \na Sf to be valid, it must adhere to the implication in (1). We will show this inductively, by considering \nthe direct dependencies of the call. Recall that is the transitive closure of . (c1 c2 . c1 .* c2). We \ncan thus satisfy our condition by proving that for all c1 . c2 the following holds: c1 . c2 . Sf (c1) \n> Sf (c2) (3) So far we have simply stated that . is the set of all direct depen\u00addencies of calls. We \nwill need to de.ne . in terms of the recursive calls of the function. For a function f (x1,..., xn), \na recursive call will consist of f (xr1,..., xrn), where xri are linear combinations of the initial parameters, \ne.g xri = bi,1 x1 + ... + bi,nxn + ci . These xr are the descent functions of the recursive call. Each \nrecursive call therefore represents a set of dependencies, described by: {(x1,..., xn) . (xr1,..., xrn)|.x1,..., \nxn}  For each recursive call site, we will need to verify that this set satis.es (3). Substituting in \nour Sf equations: Sf (x1,..., xn) - Sf (xr1,..., xrn) > 0, .x1,..., xn Recalling our pre-condition that \nSf is a.ne, and evaluating by variable substitution, this is equivalent to the following condition: a1(x1 \n- xr1) + ... + an(xn - xrn) > 0, .x1,..., xn This equation denotes the criteria on a valid schedule. \nWe derive one criterion for each recursive call in the function, and con.rm that a Sf is valid by con.rming \nit satis.es all the criteria. When the descent function xri is uniform (e.g of the form xi +ci) the majority \nof practical cases this equation will simplify to (-a1c1) + ... + (-ancn) > 0, which is straight-forward \nto analyse. For any descent function that is a.ne (e.g of the form Ax\u00af+ c\u00af), the criteria will require \nthe runtime range of x to determine the validity.  4.6 Automatically determining a schedule In the previous \nsection, we derived a series of criteria which we used to verify a pre-existing schedule, perhaps provided \nby a user. In this section, we will use the same criteria to derive a schedule on behalf of the user. \nThis is fully automatic no further user input beyond the recursion is required. In this way, we can \nsupport both automatic and user-speci.ed parallelisation schemes. Our aim is to describe a Constraint \nSatisfaction Problem (CSP) that combines the criteria of the previous section which will enforce the \nvalidity of the scheduling function with a selection measure which will help choose an e.cient schedule. \nWe can then pass these conditions and goal into a suitable CSP solver to .nd the coe.cients a1,..., an \nof a solution Sf . By necessity, the selection measure will need to be a heuristic the various factors \nthat a.ect the execution time are di.cult to predict prior to execution, and may include which memory \noperations occur and when, the number of instructions, the size of the problem and the exact hardware \nused. As such, the heuristic we have chosen is the number of partitions required to evaluate the entire \ntable. By minimising the number of partitions, we are maximising the average size of a partition, and \ntherefore maximising the ideal amount of parallelisation provided by an ideal device with in.nite synchronous \ncores. In practice, we have a .xed number of cores, so we will have to evaluate partitions in blocks, \nregardless of the size of the partition. If we have a schedule, Sf (\u00afx) = a1 x1 + ... + anxn, where x\u00af= \nx1,..., xn, then we can compute the minimum number of partitions by minimising the di.erence between \nthe largest and the smallest partition in the range, using the following formula: mina1,...,an (maxx\u00af(Sf \n(\u00afx)) - minx\u00af(Sf (\u00afx))) (4) Our intention is to construct this as a CSP over a1,..., an. However, it \nis clear that equation (4) is not a linear problem we are trying to solve for both a and x. How can \nwe encode this as a CSP problem if it is not linear? The solution to this apparent problem comes from \nthe observa\u00adtion that Sf is linear, and therefore the maximum value of Sf occurs when each component \nof Sf is maximised. How do we maximise aixi? As we run our algorithm at runtime, we allow ourselves the \nluxury of knowing the range of xi, e.g 0 = xi < ni. We can there\u00adfore maximise the component by maximising \nxi, if ai is positive, and minimising xi if ai is negative. We can do the reverse to min\u00adimise the component, \nand thus .nd the minimum expression With this approach we will need to evaluate up to 2n di.erent constraint \nproblems to .nd the solution. This is practical for two reasons: the majority of applications are between \ntwo or three dimensions, and in most of cases, the criteria may predispose us to eliminate a subset \nof these problems for example, if we already know ai > 0 then xi = ni - 1, as it is the largest value \nin the range. For each constraint problem, we specify the maximum and minimum values of x1,..., xn, use \nequation (4) as the goal we wish to minimise, and add a criteria for each recursive call, as discussed \nin the previous section. If a result is found, we store the minimum goal value found, and the .rst set \nof solution coe.cients, and continue to the next problem, until we have checked or eliminated all dimensions. \n 4.7 Multiple problems Our technique so far discusses how to map a single problem to a single multiprocessor \n rather than employing all multiprocessors in a GPU. Typically a single problem is tiled across the multipro\u00adcessors, \nhowever bioinformatics often deals with large numbers of small problems. The compilation algorithm above \nmakes use of the precise bounds of each domain to determine the best Sf and af . Di.erent problem sizes \nmay therefore require di.erent schedules to run e.ciently. What we want is to evaluate the problems simul\u00adtaneously \non di.erent multiprocessors, so we must synthesise a so\u00adlution that can apply di.erent schedules to each \nproblem. To this end, we have developed a conditional parallelisation technique, where we adapt the CSP \nto derive multiple valid sched\u00adules. Considering all selected schedules, we then derive conditions under \nwhich each particular schedule is minimal, which we then evaluate at runtime on a problem-by-problem \nbasis to determine which generated parallelisation code to use. For example, given a function f (x, y) \n= ... f (x - 1, y - 1) ..., the minimal schedule de\u00adpends on the length when nx < ny then Sf (x, y) \n= x otherwise Sf (x, y) = y. Our solution proceeds by adapting the CSP for the schedule to remove the \ngoal value. Instead of deriving a single, minimal, solution, we instead propagate the constraints to \nderive the valid range of each variable. The descent functions must be uniform, as we can no longer consider \nruntime ranges. Given the ranges of values found by the CSP, we could loop over all the possible coe.cients \nfor the scheduling function. However, we would identify many schedules that are valid but never minimal \nfor any value in the domain. e.g, in our previous example we might .nd the following non-minimal schedules \n(2, 1),(2, 2),(3, 3), when it is clear that the only minimal schedules are (1, 0) and (0, 1). Formally, \na schedule is minimal i. the coe.cients a of the schedule \u00af satisfy .x\u00af. X : .b\u00af. P : a\u00afx\u00af< bx\u00af, where \nP is the set of valid schedules. Identifying the complete set of minimal schedules a priori is tricky. \nInstead, we determine a subset of the minimal schedules with positive coe.cients using the following \nmethod: 1. Create all n! permutations of the domains e.g in two dimensions 1, 2 and 2, 1 2. For each \npermutation, we minimise each dimension in turn, propagating the constraints until we .nd a solution \nwhich we add to the set. We therefore .nd the .rst lexicographical solu\u00adtion with respect to the permutation \nof dimensions. Each de\u00ad.ned solution is minimal for some \u00afx. We note that in practice the majority of \nproblems have a single schedule, so the set size is far smaller than n!.  It may seem that this method \nis potentially expensive. To limit this e.ect, we restrict the coe.cients to a small .xed number (10) \nthat is customisable by the end user and note that n is usually small. Additionally, unlike the runtime \nanalysis described for single problems, this analysis can be performed during compile time. For each \nschedule found, we .nd use CLooG to generate the appropriate set of nested loops to iterate over the \nspace. We use the conditions to determine which of the generated code paths to take. This does not lead \nto any unnecessary code branching on the GPU, as a single problem will be alllocated to a single multiprocessor, \nand each thread on that multiprocessor will choose the same schedule.  4.8 Sliding window optimisation \nIn a traditional dynamic programming algorithm all prior results are stored until the computation is \ncomplete. One of the bene.ts of partitioning the dependency graph is that we can determine the range \nof previous results required, with respect to the number of partitions. We de.ne this as the sliding \nwindow over the domain. When the recursive descent functions are uniform, we can use the precise value \nof each criteria to determine the size of the sliding window, in terms of the number of previous partitions \nrequired. When the descent functions are a.ne, this is not possible. This optimisation is especially \nrelevant for a GPU, since it will allow us to store a much smaller table for the intermediate values. \nThis smaller table will typically .t in a cache that is much closer to the processor, such as shared \nmemory on a NVIDIA GPU, almost eliminating the signi.cant latency to global memory.  4.9 Limitations \nof the Approach We have made it clear that we are only considering a.ne schedul\u00ading and descent functions \n that is, linear combinations of the co\u00ade.cients. We justify this limitation in three ways: 1. Goods \nsolvers are available which can determine solutions in linear cases if the recursive parameters or the \nscheduling function were not linear, we would not be able to use CSP or linear programming techniques. \n 2. We can synthesise good target code in the linear case; it is not clear how a non-linear function \nwould be synthesised because it would be di.cult to .nd the inverse transformation. 3. Problems in bioinformatics, \nour target domain, infrequently require non-linear cases.  In addition, for our technique in Section \n4.7, we require uniform descent functions. This restriction is necessary as an a.ne descent would require \nknowledge of the runtime range to evaluate. Our approach is optimal with respect to the number of partitions \nwe evaluate. However, this does not guarantee a minimal execution time. In particular, we will typically \nevaluate a partition in blocks of threads. Doing so can lead to wasted execution on the GPU, where a \ntheoretically worse schedule might, in fact do better. 5. Domain Extensions In Section 3, we described \nour proposed architecture in which our simple base language can be expanded by domain speci.c exten\u00adsions. \nThe intention of such DSLs is not that they relax any of the limitations set out in the paper, but instead \nbe used to extend the data-structure and traversal operations to support speci.c applica\u00adtions. The following \nfeatures can be de.ned as language extensions without modi.cation to the described compilation algorithm: \n The introduction of any branching expression;  Create new recursive data-types on the condition there \nis a de.ned mapping to the natural numbers;  Create new looping expressions, where the extension can \nde.ne the range of the loops at runtime, and can therefore derive solvable criteria on recursions within \nthe loop.  New extensions should cover areas with wide applicability, so that the cost of a cross-specialist \ndeveloping the extension is amor\u00adtised across a wide array of applications. For the purpose of our evaluation, \nwe have produced two such extensions Hidden Markov Models and Substitution Matrices . Both are widely \nused in bioinformatics, and extend our interesting, but neutered, language to something which can tackle \nreal problems in that do\u00admain. 5.1 Substitution Matrices Substitution Matrices represent a simple data-extension \nto our lan\u00adguage. They allow the user to describe the cost of substituting one character in an alphabet \nfor another. We introduce a new statement to our language for de.ning the matrix with a straight-forward \nta\u00adble. This is represented by a new type, matrix, that can be passed to functions, and a new expression \nto load the substition value. Expr . ... | Var[Expr, Expr] The introduction of this expression has no \na.ect on the recur\u00adsion analysis. The generated load code for this expression will de\u00adpend where we locate \nthe table; in shared memory for small tables or global memory for large. 5.2 Hidden Markov Models A \nHidden Markov Model (HMM) is a statistical model for random stochastic processes, constructed as a probabilistic \n.nite automata. Models consist of a set of states, where transitions between states are associated with \na probability. Each transition only depends on the current state -the markov property. The hidden part \nof the name derives from the fact that we cannot observe the states -instead each state contains a distribution \nover some emission alphabet. As with the substitution matrix, we introduce a new statement to our language \nfor de.ning the HMM model. This allows the user to de.ne states, transitions and emissions. With it we \nintroduce some new types -a HMM calling type, to pass the model to the function, and state and transitions \nas recursive types. To be able to tabulate recursions using states or transitions we provide an arbitrary \ntotal ordering over each one, providing a mapping to the natural numbers (see Section 3.2). The ordering \nis arbitrary because no recursion depends on the position of the states. We add the following new expressions \nfor exploring the model: Expr . ... | Var.start | Var.end | Var.isstart | Var.isend | Var.emission[Expr] \n| Var.transitionsto | Var.transitions f rom | sum(var in Expr : Expr)  start and end load the states \nassociated with a transition. isstart and isend determine whether the state is the last or the .rst. \nemission[Expr] is the probability of a state emitting the Expr value. The .nal expressions transitionsto \nand transitionsfrom are for identifying the sets of transitions to and from a state. We provide a summation \nstatement (amongst others such as max and min), which can iterate over these sets of transitions. Figure \n11 compares the mathematical description of one of the canonical HMM algorithms to the implementation \nin our language. We recurse over s, the states in the HMM, and i, the indices into the sequence x. The \nonly new expression that a.ects the recursion analysis is the summation expression. The expression evaluated \nfor each element may include a recursive call that depends on the newly created variable. For example, \nthe forward algorithm has a recursive call using the start state of the transition within the sum (Line \n8). For the purpose of the analysis, we assume that the newly created variable -in this case t -can vary \nover all possible values and thus t.start, in this case, varies over any state. We conclude that our \nschedule can only be S f orward(s, i) = i. F(0, 0) = 1 F(s, 0) = 0 for s > 0 F(s, i) = es,x[i] s p:tp,s>0 \ntp,s F(p, i - 1) (a) The recursive equation, where F(s, i) is the likelihood we are in state s at position \ni in the sequence x. ts,p is the probability of transitioning from state s to state p 1 2 3 4 5 6 7 8 \nprob forward(hmm h, state[h] s, seq[*] x, index[x] i) if i == 0 then if s.isstart then 1.0 else 0.0 else \n// The end state is silent (if s.isend then 1.0 else s.emission[x[i-1]]) * sum(t in s.transitionsto : \nt.prob * forward(t.start, i -1)) (b) The implementation in our extension = Figure 11. The forward algorithm, \nmathematical description com\u00adpared to the implementation  6. Evaluation There are two factors that will \nin.uence performance using our framework. The .rst is the choice of schedule, the second is the low-level \nimplementation. In this paper, we have concentrated on selecting the former. We have chosen our schedule \nto be minimal with respect to the number of partitions. This choice was justi.ed in Section 4.6, however \nwe will note, for those that desire some\u00adthing more practical, that quantitative justi.cation would be \nhard to achieve beyond the observation that in the given examples edit distance, HMM algorithms it \nproduces the expected result. As this paper has described the construction of a DSL frame\u00adwork our aim \nin this section is to demonstrate that we can use it to build e.cient DSLs. For this reason we choose \nto analyse a num\u00adber of DSLs rather than a speci.c DSL in detail. Each of our three case studies uses \nthe base language described here, plus an exten\u00adsion, and is compared to existing CPU and, where possible, \nGPU applications to show that our framework is up to scratch. These case studies are real applications \nin bioinformatics, using real genome data on existing models to provide a realistic compar\u00adison. In all \ncases our framework automatically derived the paral\u00adlelisation schemes no schedules were speci.ed by \nthe user. In each case we compare against a hand-coded GPU application, our tool has succesfully derived \nthe same parallel strategy as the hand\u00adcoded application. All quoted results are inclusive of setup time, \nsuch as memory allocation and copying to/from device. Our framework provides a runtime environment. Consequently, \nthe times for our software are inclusive of scanning and parsing the input .les. We have not, however, \nincluded the code generation time -we cache the compiled code for each function. The code generation \noverhead is typically around 1 second, primarily due to ine.ciencys in the way in which we call ClooG \nfrom Java. We expect to be able to reduce this overhead drastically in future releases. We note that \nprograms written in DSLs of this type are rarely large enough to increase this compilation overhead. \nResults were obtained on a NVIDIA GTX 480 with 1.5GB of RAM, and CPU results using Intel Xeon E5520 CPU \nwith 4GB of RAM. Each test was performed three times, and the results Execution time (Seconds) 40 30 \n20 10 0  200 400 600 800 Query Sequence Size averaged. Results across runs were observed to be regular, \nand so we have not provided error bars. 6.1 Smith-Waterman The Smith-Waterman algorithm is an implementation \nof the local edit distance problem, used for sequence alignment. The typical application will compare \none query sequence against a database of other sequences, to identify high-scoring alignments. We imple\u00adment \nthe Smith-Waterman algorithm using the substitution matrix extension to determine the cost of substituting \ncharacters. The ex\u00adpected parallelisation is along the diagonal x + y, as with other edit distance algorithms. \nFor comparison, we use CUDASW++ 2.0[13] a GPU imple\u00admentation of Smith-Waterman using the NVIDIA CUDA \nframe\u00adwork and the ssearch tool in Fasta[16], compiled without SSE2 vector instructions. CUDASW++ 2.0 \nprovides two methods of par\u00adallelisation intra-task parallelisation, which uses parallel diago\u00adnals \nacross the table in the same way as our recursion, and an inter\u00adtask parallelisation, with a database \nsequence allocated per thread. Best performance is achieved by a hybrid approach, where smaller sequences \nare computed with inter-task and larger with intra-task. We provide both the hybrid and the intra-task \nresults for CUD-ASW++ for comparison. Our results (Figure 12) are very similar to the intra-task CU-DASW++, \nand comfortably beat Fasta, demonstrating that we are reaching similar performance to the hand-coded, \nhand-tuned CU-DASW++ for this type of parallelisation. As expected, the best overall performance is achieved \nby using the hybrid parallelisa\u00adtion approach. For this paper we have deliberately focused on auto\u00admatic \nparallelisation in an intra-task manner. This is because inter\u00adtask parallelisation is algorithmically \ntrivial. Although we have not covered the techniques in this paper, generation of a sequence-per\u00adthread \nkernel as well as a intra-task kernel is straight-forward from our DSL code, and we would expect to achieve \nresults on par with the hybrid CUDASW++. 6.2 Gene-.nding Hidden Markov Models Gene .nding or gene prediction \nis the process of identifying genes in regions of DNA. They are often described using Hidden Markov Models, \nwhich are trained to recognise statistical features of genes, and used for likelihood estimation and \nprediction. We build a simple gene-.nder using our HMM extension lan\u00adguage. We compare our results against \nHMMoC 1.3[14], a code\u00ad 200 150 100  Execution time (Seconds) Execution time (Seconds) 100 50 50 00 \n\u00b7104 Number of Sequences Model Size (Number of Positions) Figure 13. Gene-.nding performance on varying \nsequence sizes. Figure 15. Performance on a dataset of 13,355 sequences, on models of a varying size. \n12 a series of .lters to remove low-scoring sequences at low cost 10 by evaluating simpli.ed models \nand algorithms, such as the MSV (Multiple Segment Viterbi), before bringing the full forward algo\u00ad rithm \nto bear on a much smaller set of problems. Whilst it would be both viable and bene.cial to translate \nsuch techniques into our language, it would be a signi.cant undertaking. To that end, we set the max \n.ag to turn o. all such .ltering and provide a fair com\u00adparison between a best of breed full forward \nalgorithm for pro.le HMMs on the CPU to our GPU implementation. Our results (Figures 14 and 15) show \nan expected large increase in performance over HMMoC for the GPU techniques. Our runtime performance \nis on par with GHMMeR. Any di.erence in perfor- Execution time (Seconds) 8 6 4 2 0 \u00b7104 Sequence Size \nFigure 14. Performance on the TK model of 10 positions, with varying numbers of sequences. generation \ntool for HMMs on CPUs. Our results (Figure 13) show a signi.cant performance increase in line with the \nexpected re\u00adsults over a CPU technique on this application at larger database sizes, when we are using \nthe GPU to its full extent, the performance increase is about x60. HMMoC is single-threaded application; \ncon\u00adsidering the relative strengths of the CPU and GPU this speed-up is in line with what we might expect. \nAs far as we are aware, there are no existing equivalent tools for GPUs.  6.3 Pro.le Hidden Markov Models \nPro.le HMMs are a type of HMM designed for representing a family of sequences. Typical applications include \ntraining models and database searching, where we apply a database of sequences to the pro.le model in \norder to determine new sequences that belong to a given family. We perform a database search using the \nforward algorithm on various sequence and pro.le models. Once again, we provide comparison to HMMoC, \nas a general purpose HMM tool, as well as to HMMeR[4], a special purpose tool for pro.le HMMs. In addition, \nwe compare to a GPU port of HMMeR 2.0, GPU\u00adHMMeR[21]. HMMeR is a high-performance, widely used tool \nhand\u00adcoded and hand-tuned over 15 years to provide optimal perfor\u00admance on pro.le models. The latest \nversion, HMMeR 3.0, uses mance is primarily due to the overhead of our runtime framework, and is smoothed \nout on larger sequence sets. However, all three are beaten by the most recently released version of HMMeR, \n3.0. Whilst it may at .rst this might be surprising, it must be remem\u00adbered that HMMeR can make a number \nof assumptions about the nature of the problem that we are unable to do in a general HMM tool. HMMeR \nis also well optimised, using multiple threads and vectorised code on the CPU to maximise performance. \nHowever, this was a signi.cant undertaking for the developers what we provide is access to this type \nof performance across a much wider range of problems without the need to put in 15 years of optimisa\u00adtion \ninto each one. 7. Related Work Schedules were .rst proposed as a means of parallel analysis of recursive \nfunctions in the seminal paper by Karp et. al. [9], and further developed in [20]. Much of the further \nwork in this area has been focused on loop parallelization[11], and in particular the polyhedral model[12] \nwhere it has been pro.tably used for code generation[1] for loop schedules. There is a wealth of literature \non parallel implementations of fully .edged functional programming languages, and a recent tuto\u00adrial \nin the context of Haskell is [8]. We have consciously restricted our input language to enable more aggressive \noptimisations. Elliott [5] shows how functional programs can be used to generate e.\u00adcient code for graphics \noperations on graphics processors. There is an overlap with this paper in that we use a restricted functional \nin\u00adput language, but Elliott does not address the problem of mapping complex dynamic programming recursions \nto a GPU architecture. DSLs are a growing trend in the graphics card world. Our approach of using a host \nlanguage with extensions is very similar to the concept of Language Virtualization provided by the Delite \nframework[2]. Where our approaches di.er is that we develop a host language that provides automatic parallelisation, \nwhere they make use of the use of the DSL capabilities of Scala to support the creation of extension \nwhich can provide parallelisation. Examples include Liszt for physics simulations and OptiML for machine \nlearning. Other examples of graphics card DSLs include Nikola[15] and Obisidian[18], both embedded DSLs \nfor array computations and FLAME[7] a DSL for dense linear algebra. Algebraic Dynamic Programming[6] \nis an example of a DSL designed speci.cally for Dynamic Programming problems in bioinformatics using \nstrings, and has recently been extended to GPUs[17]. Whilst similar to our approach, it is currently \ndeals only in diagonal parallelisation with one input sequence per problem. 8. Conclusion In this paper \nwe have shown that scientists can automatically syn\u00adthesise e.cient GPU code from speci.cations that \nread like the al\u00adgorithm descriptions in their papers. Our preliminary results show that performance \nof the synthesised code is on a par with hand\u00adtuned code for several important real world examples. However, \nthe advantage of our system is that it can support domains far beyond these single application hand-tuned \nimplementations. Our technique is to partition the domain of a recursion into sets of values that can \nbe computed concurrently. We adopt the construction of scheduling functions to achieve this, and have \nde\u00adscribed how we can select such a function and subsequently de\u00adrive a graphics card implementation. \nWe apply insights from loop parallelisation using polyhedra to recursively de.ned problems on modern \nsynchronous processors. We have shown that such an approach can produce e.cient and e.ective results, \nwith minimal work on the part of the domain\u00adfocused user, and without needing to know the architectural \ncom\u00adplexities of the graphics card bringing high-level language ap\u00adproaches close to hand-optimised \nCPU and GPU performance. 9. Further Work We would like to extend our work to support mutually recursive \nfunctions, by deriving multiple scheduling functions, one for each function, whose partition time-step \nvalues are compatible. The same principle holds as for a single scheduling function; if Sf (\u00afx) < Sg(\u00afy) \nthen f (\u00afx) must be computed before g(\u00afy). This would allow us to support more complicated applications, \nsuch as RNA secondary structure prediction. In Section 3.2 we provided a simpli.ed view of the translation \nof complex parameters the need for them to map to natural numbers. In practice, the ordering of the \nmodel elements may be important to achieve high performance di.erent orderings may permit or restrict \nthe available schedules. References [1] C. Bastoul. Code generation in the polyhedral model is easier \nthan you think. In PACT 13 IEEE International Conference on Parallel Architecture and Compilation Techniques, \npages 7 16, Juan-les-Pins, France, September 2004. [2] H. Cha., Z. DeVito, A. Moors, T. Rompf, A. K. \nSujeeth, P. Han\u00adrahan, M. Odersky, and K. Olukotun. Language virtualization for heterogeneous parallel \ncomputing. In OOPSLA 10: Proceedings of the ACM international conference on Object oriented programming \nsystems languages and applications, pages 835 847, New York, NY, USA, 2010. ACM. ISBN 978-1-4503-0203-6. \n[3] R. Durbin, S. R. Eddy, A. Krogh, and G. Mitchison. Biological Se\u00adquence Analysis: Probabilistic Models \nof Proteins and Nucleic Acids. Cambridge University Press, July 1999. ISBN 0521629713. [4] S. Eddy. HMMer \nWebsite, including User Manual. http://hmmer.wustl.edu. [5] C. Elliott. Programming graphics processors \nfunctionally. In Pro\u00adceedings of the 2004 ACM SIGPLAN workshop on Haskell, Haskell 04, pages 45 56, New \nYork, NY, USA, 2004. ACM. ISBN 1-58113\u00ad850-4. doi: http://doi.acm.org/10.1145/1017472.1017482. [6] R. \nGiegerich and C. Meyer. Algebraic dynamic programming. In AMAST 02: Proceedings of the 9th International \nConference on Al\u00adgebraic Methodology and Software Technology, pages 349 364, Lon\u00addon, UK, 2002. Springer-Verlag. \nISBN 3-540-44144-1. [7] J. A. Gunnels, F. G. Gustavson, G. M. Henry, and R. A. van de Geijn. FLAME: Formal \nLinear Algebra Methods Environment. ACM Transactions on Mathematical Soft\u00adware, 27(4):422 455, Dec. 2001. \nISSN 0098-3500. URL http://doi.acm.org/10.1145/504210.504213. [8] S. L. P. Jones and S. Singh. A tutorial \non parallel and concurrent programming in haskell. In P. W. M. Koopman, R. Plasmeijer, and S. D. Swierstra, \neditors, Advanced Functional Programming, volume 5832 of Lecture Notes in Computer Science, pages 267 \n305. Springer, 2008. ISBN 978-3-642-04651-3. [9] R. M. Karp and M. Held. Finite-state processes and dynamic \nprogram\u00adming. SIAM Journal on Applied Mathematics, 15(3):693 718, 1967. doi: 10.1137/0115060. [10] A. \nKrogh, I. S. Mian, and D. Haussler. A hidden markov model that .nds genes in e.coli dna. Nucleic Acids \nResearch, 22(22):4768 4778, 1994. doi: 10.1093/nar/22.22.4768. [11] L. Lamport. The parallel execution \nof do loops. Communications of The ACM, 17:83 93, February 1974. doi: 10.1145/360827.360844. [12] C. \nLengauer. Loop parallelization in the polytope model. In CON-CUR 93, Lecture Notes in Computer Science \n715, pages 398 416. Springer-Verlag, 1993. [13] Y. Liu, B. Schmidt, and D. Maskell. Cudasw++2.0: enhanced \nsmith\u00adwaterman protein database search on cuda-enabled gpus based on simt and virtualized simd abstractions. \nBMC Research Notes, 3(1): 93, 2010. ISSN 1756-0500. doi: 10.1186/1756-0500-3-93. [14] G. Lunter. HMMoC \na compiler for hidden Markov models. Bioinfor\u00admatics, 23(18):2485 2487, September 2007. doi: 10.1093/bioinfor\u00admatics/btm350. \n[15] G. Mainland and G. Morrisett. Nikola: embedding compiled gpu functions in haskell. In Haskell 10: \nProceedings of the third ACM Haskell symposium on Haskell, pages 67 78, New York, NY, USA, 2010. ACM. \nISBN 978-1-4503-0252-4. [16] W. R. Pearson and D. J. Lipman. Improved tools for biological se\u00adquence \ncomparison. Proceedings of The National Academy of Sci\u00adences, 85:2444 2448, 1988. [17] P. Ste.en, R. \nGiegerich, and M. Giraud. Gpu paralleliza\u00adtion of algebraic dynamic programming. 2009. URL HAL:http://hal.archives-ouvertes.fr/inria-00438219/en/. \n[18] J. Svensson, K. Claessen, and M. Sheeran. Gpgpu kernel im\u00adplementation and re.nement using obsidian. \nProcedia Computer Science, 1(1):2065 2074, 2010. ISSN 1877-0509. doi: DOI: 10.1016/j.procs.2010.04.231. \nICCS 2010. [19] A. van Deursen, P. Klint, and J. Visser. Domain-speci.c languages: an annotated bibliography. \nSIGPLAN Not., 35:26 36, June 2000. [20] H. Verge, C. Mauras, and P. Quinton. The alpha lan\u00adguage and \nits use for the design of systolic arrays. The Journal of VLSI Signal Processing, 3:173 182, 1991. ISSN \n0922-5773. URL http://dx.doi.org/10.1007/BF00925828. 10.1007/BF00925828. [21] J. P. Walters, V. Balu, \nS. Kompalli, and V. Chaudhary. Evaluating the use of gpus in liver image segmentation and hmmer database \nsearches. In IPDPS 09: Proceedings of the 2009 IEEE International Sympo\u00adsium on Parallel&#38;Distributed \nProcessing, pages 1 12, Washington, DC, USA, 2009. IEEE Computer Society. ISBN 978-1-4244-3751-1.  \n  \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Over the last five years, graphics cards have become a tempting target for scientific computing, thanks to unrivaled peak performance, often producing a runtime speed-up of x10 to x25 over comparable CPU solutions.</p> <p>However, this increase can be difficult to achieve, and doing so often requires a fundamental rethink. This is especially problematic in scientific computing, where experts do not want to learn yet another architecture.</p> <p>In this paper we develop a method for automatically parallelising recursive functions of the sort found in scientific papers. Using a static analysis of the function dependencies we identify sets - partitions - of independent elements, which we use to synthesise an efficient GPU implementation using polyhedral code generation techniques. We then augment our language with DSL extensions to support a wider variety of applications, and demonstrate the effectiveness of this with three case studies, showing significant performance improvement over equivalent CPU methods, and similar efficiency to hand-tuned GPU implementations.</p>", "authors": [{"name": "Luke Cartey", "author_profile_id": "81502795941", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P3471171", "email_address": "luke.cartey@cs.ox.ac.uk", "orcid_id": ""}, {"name": "Rune Lyngs&#248;", "author_profile_id": "81100448064", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P3471172", "email_address": "lyngsoe@stats.ox.ac.uk", "orcid_id": ""}, {"name": "Oege de Moor", "author_profile_id": "81100198102", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P3471173", "email_address": "oege.de.moor@cs.ox.ac.uk", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254080", "year": "2012", "article_id": "2254080", "conference": "PLDI", "title": "Synthesising graphics card programs from DSLs", "url": "http://dl.acm.org/citation.cfm?id=2254080"}