{"article_publication_date": "06-11-2012", "fulltext": "\n A Compiler Framework for Extracting Superword Level Parallelism Jun Liu,Yuanrui Zhang, Ohyoung Jang,Wei \nDing, Mahmut Kandemir The Pennsylvania State University, UniversityPark,PA 16802, USA {jxl1036, yuazhang, \noyj5007, wzd109, kandemir}@cse.psu.edu Abstract SIMD (single-instruction multiple-data) instruction set \nextensions are quite common today in both high performance and embedded microprocessors, and enable the \nexploitation of a speci.c type of data parallelism called SLP (Superword Level Parallelism). While prior \nresearch shows that signi.cant performance savings are pos\u00adsible when SLP is exploited, placing SIMD \ninstructions in an ap\u00adplication code manually can be very dif.cult and error prone. In this paper, we \npropose a novel automated compiler framework for improving superword level parallelism exploitation. \nThe key part of our framework consists of two stages: superword statement gen\u00aderation and data layout \noptimization. The .rst stage is our main contribution and has two phases, statement grouping and state\u00adment \nscheduling, of which the primary goals are to increase SIMD parallelism and, more importantly, capture \nmore superword reuses among the superword statements through global data access and reuse pattern analysis. \nFurther, as a complementary optimization, our data layout optimization organizes data in memory space \nsuch that the price of memory operations for SLP is minimized. The re\u00adsults from our compiler implementation \nand tests on two systems indicate performance improvements as high as 15.2% over a state\u00adof-the-art SLP \noptimization algorithm. Categories and Subject Descriptors D.3.4 [Processors]: Code Generation,Compilers, \nOptimization General Terms Design, Algorithms, Languages, Experimenta\u00adtion, Performance Keywords SLP, \nSIMD, Scheduling, Data Layout,Compiler 1. Introduction As a response to demands from the application \nside, many mi\u00adcroprocessors today employmultimedia extensions/support. While the implementation details \nof this support vary from one architec\u00adture to another, it generally comes in form of vector/SIMD (single\u00adinstruction \nmultiple-data) instructions, which provide a mechanism to accelerate the performance of various application \nprograms. The most popular commercial multimedia extensions include Intel s MMX/SSE/SSE2/SSE3/SSE4 [1, \n14], AMD s 3DNow! [23], and IBM s VMX/Altivec [13]. Permission to make digital or hard copies of all \nor part of this work for personal or classroomuseisgrantedwithout feeprovidedthat copies arenot madeordistributed \nforpro.torcommercialadvantage andthatcopiesbearthisnoticeandthefullcitation onthe .rstpage.To copy otherwise,to \nrepublish,topostonserversorto redistribute tolists, requirespriorspeci.cpermission and/ora fee. PLDI \n12, June11 16,2012, Beijing, China. Copyright c &#38;#169; 2012ACM978-1-4503-1205-9/12/06. . .$10.00 \nWhile earlier multimedia extensions supported only small data types, these newer extensions can operate \non 128-bit superwords, leading to a new type of data parallelism, called the Superword Level Parallelism \n(SLP) [17]. Speci.cally, data can be packed in superwords and operated using SIMD instructions, as illustrated \nin Figure 1. It needs to be noted that SLP is different from well\u00adknown vector level parallelism [9, \n21, 22, 29] in that the latter can only be applied to certain array-based codes where large amounts of \nparallelism exists. By contrast, SLP can be applicable even if small to moderate levels of parallelism \nis available in the application code. While a knowledgeable programmer can exploit SLP by manually transforming \nhis/her code to short SIMD form, this is not a very desirable option due to its dif.culty and error-proneness. \nMore speci.cally, many data access patterns do not easily lend themselves to this transformation, and, \nin most cases, performing such code modi.cations requires an in-depth understanding of the data dependences \nand data reuse patterns. In this work, we propose and evaluate a novel compiler support for improving \nSLP exploitation. In contrast to the prior efforts on this topic [17, 25 28, 30, 31], we employ a holistic \napproach from two perspectives. First, instead of depending on local heuristics that can result in poor \nsolutions in terms of parallelism and superword access overheads, we take a more global view of the target \napplica\u00adtion code when capturing the data reuse patterns before committing to optimization decisions. \nSecond, we combine our main optimiza\u00adtion, i.e., superword statementgeneration, with a data layout opti\u00admization \nstage to achieve further improvements. Speci.cally, this paper makes the following contributions: We \npropose a compiler framework for SLP exploitation that accommodates two stages, namely, superword statement \ngenera\u00adtion and data layout optimization, to achieve auto-detection and op\u00adtimization of SLP. The superword \nstatement generation is our main contribution and can be divided into two phases, i.e., statement grouping \nand statement scheduling. The .rst phase determines how statements are grouped for short SIMD operations, \nwith the goal of increasing SIMD parallelism and, more importantly, capturing more superword reuses among \nthe groups. The second phase de\u00adcides the execution sequence of the groups, as well as the order of the \nstatements within each individual group, in order to reduce the vector register permutation overheads. \nFurther, as a complementary optimization, our data layout optimization analyzes the data access patterns \nafter the .rst stage, and re-organizes data in memory space such that the price of memory operations \nfor SLP is minimized.  We implemented our framework in a compiler [33] and per\u00adformed experiments using \n16 application programs on two differ\u00adent commercial systems. Our experimental results indicate that \nthe proposed SLP framework performs better than an existing SLP scheduling algorithm [17], and generates \nas much as 15.2% im\u00adprovement over it.   The remainder of this paper is organized as follows. The next \nsection explains SLP, goes over the prior work, and introduces the motivation for this work. Section \n3 gives an overview of our compiler framework. Section 4 presents our main optimization, i.e., superword \nstatement generation. Section 5 describes our data layout optimization strategy. Section6 usesanexampleto \nillustrate how our approach is applied. Section 7 discusses the experimental results, and Section8 concludes \nthe paper. 2. Superword LevelParallelism Most modern commercial microprocessors add support for multi\u00admediaextensions[1,13,14,23]to \nmeetthe demandsof computation\u00adintensive multimedia applications. The core of these extensions is a set \nof SIMD instructions that can operate on aggregate data objects larger than a machine word, i.e., superwords, \nin the vector registers in parallel. The newer extensions can now operate on superwords of 128 bits, \nand the width of the SIMD data path is expected to keep increasing. One critical issue regarding the \nef.cient utilization of multime\u00addia extensions is the compiler support to release the programmer from \nthe burden of inserting short SIMD inline assembly routines manually. However, traditional compiler techniques \nfor automatic parallelization on vector machines [6, 8 10, 12, 15, 18, 19, 24, 32, 34] can be ef.cient \nonly when the applications expose large amounts of data parallelism. They also rely on complex high-level \nloop transformations and cannot deal with applications that are not vectorizable, e.g., codes that have \nmostly scalar data. To solve the above problem, Larsen and Amarasinghe [17] pro\u00adposeda new typeof parallelism \ncalled the SuperwordLevelParal\u00adlelism (SLP), targeting multimedia extensions. Different from vec\u00adtor \nparallelism, SLP exploits .ne-grained parallelism from basic blocks rather than from loop nests. In [17], \nthey.rst try to identify isomorphic statements, which are statements with the same opera\u00adtions in corresponding \npositions. The operations in all isomorphic statements should be in the same order, and the operands \nin the corresponding positions should have the same data type. Theythen group the isomorphic statements \ntogether as a superword statement for concurrent execution. In Figure 1 for example, statements S1 and \nS2 are isomorphic and can be put together as a superword state\u00adment <S1 ,S2 > for SIMD operation. An \nimportant advantage of SLP is that it enables ef.cient utilization of multimedia extensions even if the \nparallelism available in the application code is small or moderate. Shin et al [26, 27] developed a strategy \nto manage the vector register .le as a compiler-controlled cache in order to improve data locality when \nexploiting SLP [17]. In addition, they [25, 28] derived large basic blocks using instruction prediction \nin the presence of control .ow to identify more superword level par\u00adallelism. Their studies are orthogonal \nto our approach proposed in this paper.Tenlladoetal[30,31] presented techniquestoef.ciently exploit SLP \nin applications where inner loops carry dependences. Nuzman et al [22] investigated a compiler technique \nthat supports effective vectorization in the presence of interleaved data. Nuzman and Zaks [21] also \nrevisited outer-loop vectorization techniques for short SIMD architectures. Barik et al [5] proposed \nto enable auto\u00admatic vectorization on a low-level IR closer to the machine-level, and targeted at achieving \ncompile time ef.ciency during dynamic compilation. As opposed to the prior work, which are built upon \nthe original SLP algorithm [17], we propose an entirely different strategy for extracting SLP. A critical \nrequirement in exploiting SLP is that the operands in the superword statement need to be put together \nin a desired order as superwords in vector registers for short SIMD operations. If, however, the source \noperands in a superword are not available in the vector register, expensive memory accesses and additional \nvector register reshuf.ing/permutation instructions are needed to bring the data from memory and to rearrange \nthem on demand in thevectorregister.This processis referredtoas superwordpacking. Similar overheads can \nbe incurred during the process of scattering the target operands in the superword for later uses, which \nis referred to as superword unpacking. Prior studies [17, 25] show that the superword packing/unpacking \noverheads can be so high that can evenoffset the potential performancegains broughtby SLP. Therefore, \nit is of great importance to reduce the packing/unpacking overheads as much as possible when exploit\u00ading \nSLP. One important observation is that, if the superword used in a superword statement already exists \nin the vector register, ac\u00adcessing its operands is almost free, without the need for expensive memory \naccesses or any register reshuf.ing/permutation instruc\u00adtion. For example, in Figure 1, the superword \n<V1 ,V2 > used in the superword statement <S3 ,S4 > can be obtained by directly reusing it from <S1 ,S2 \n>. Or, in some other cases, even if a direct reuse of superword is not possible but two superwords access \nthe same data with different orderings, we can still save memory ac\u00adcess overhead by only introducing \nthe vector register permutation instructions. For example, in Figure 1, the superword <V2 ,V1 > used \nin <S5 ,S6 > can reuse the operands in <V1 ,V2 > of <S3 ,S4 > by interchanging the positions of V1 and \nV2 . Hence, it is critical to locate and expose such reuses as much as possible when generating SIMD \ncodes. The greedy algorithm proposed by Larsen and Amarasinghe [17] and also employed in other related \nwork [28, 31] tries to achieve superword reuses based on local heuristics. The algorithm targets basic \nblocks and starts by identifying isomorphic statement pairs with adjacent memory accesses as the seed \nsuperword state\u00adments. It then groups more isomorphic statements by following the def-use and use-def \nchains so that reuses may be caught between the newly generated superwords and the existing ones. The \nmain prob\u00adlem with this algorithm is that the decision made at each step to generate new superword statements \nhighly depends on the existing grouping decisions and the def-use/use-def chains, being oblivious to \nthe rest of the statements. In other words, this approach does not have a global perspective. As a result, \nit is unable to fully utilize the potential superword reuse opportunities available in the input basic \nblocks. To address this problem, we introduce a new SLP algorithm, of which the basic idea is to keep \na global (an entire basic block wide) view of all the statements regarding data access and reuse patterns \nat each step when constructing new superword statements. Thus, the grouping decisions made to increase \nsuper\u00adword reuses in our algorithm are based on a larger scope. To our knowledge, this is the .rst work \nto introduce the concept of global superword reuse optimization for exploiting SLP. Though ef.cient exploitation \nof superword reuses is our main optimization and can greatly reduce the number of packing/unpacking operations \nas will be shown later, sometimes these operations are still required, e.g., when a superword is re\u00adferred \nfor the .rst time in a basic block. The mandatory packing operations can be costly and may reduce the \npotential performance bene.ts. To alleviate this problem, it is desirable to have those operands stored \nin an aligned and consecutive fashion in memory space so that the number of memory operations could be \nminimized during packing/unpacking. Prior work [17, 28]addressed this issue by choosing some statements \nwith contiguous memory accesses as the seed superword statements. However, this can lead to poor solutions \nin terms of superword reuses. In addition, this strategy highly depends on the existing data layout of \nthe data accessed. By contrast, we believe that the data layout optimization .ts better when solving \nthis issue in the sense that it has much less negative effect on the exploration of superword reuses \nand, above all, it can derive better access patterns from an SLP perspective. Henretty et al [11] proposed \na data layout transformation to avoid stream align\u00ad  S1: V1=V3; S2: V2=V5; S3:  = V\u00d7 ; S3: V5=V7; \nS4:  = V\u00d7 ; S: V=V+ V; 4311S5:  ; S: V=V+ V; 5525 S6: ; Figure 1. An example exploit-Figure 2. An example \nbasic ing superword level parallelism. block. ment con.ict on processors with SIMD capabilities. However, \nthey speci.cally target stencil computations. In our compiler frame\u00adwork, weexplorea much more general \ndata layout modi.cation as a post optimization togain further bene.ts.To our knowledge, this is also \nthe .rst work that combines superword statement generation with general data layout optimization to improve \nSLP exploitation. 3. Framework Overview The input to our compiler framework is a set of basic blocks \nof a program. For loop-intensive applications, loop unrolling can be used to reveal more opportunities \nfor short SIMD operations and to fully utilize the superword datapath available in the underlying architecture. \nOverall, there are three main goals that we want to achieve in order to exploit SLP more ef.ciently: \n(1) increasing par\u00adallelism by generating more superword statements; (2) reducing the number of superword \npacking/unpacking operations by achieving more superword reuses; and (3) reducing the overhead of manda\u00adtory \npacking/unpacking operations by employing data layout opti\u00admization. Figure 3 gives an overview of the \nproposed compiler frame\u00adwork, which mainly accommodates three modules: pre-processing, holistic SLP optimizer, \nand post-processing. Taking the program source code as input, the pre-processing module .rst applies \ntrans\u00adformations including loop unrolling and alignment analysis, with the objective of exposing more \nopportunities for SLP exploitation. Next, our holistic SLP optimizer performs a set of SLP optimiza\u00adtions \nand generates vectorized code. Finally, the post-processing module performs register allocation and other \nlow-level optimiza\u00adtions, and outputs executable. As the main contribution of this work, the holistic \nSLP optimizer adopts a two-stage strategy: super\u00adword statement generation and data layout optimization. \nAt the .rst stage, the statement grouping determines how to group statements together for superword operations \nwithout .xing the ordering of the statements in each superword statement, aiming at identifying more \n(statement) groups as well as increasing the superword reuses among the groups. On the other hand, the \nstatement scheduling de\u00adcides the scheduling sequence of the groups and the order of the statements within \neach group, in order to minimize the number of vector register reshuf.ing/permutation instructions. To \nfurther in\u00adcrease improvements, at the second stage, the data layout optimiza\u00adtion changes the memory \nlayout of the data accessed in the super\u00adword statements, in an attempt to reduce the overhead of mandatory \npacking/unpacking operations. It is to be noted that while the .rst stage focuses on reducing the occurrences \nof packing/unpacking operations, the second stage aims at reducing the number of mem\u00adory operations and \nvector register reshuf.ing/permutation instruc\u00adtions involved in these operations. 4. Superword Statement \nGeneration As shown in Figure 3, the superword statement generation includes two major phases: grouping \nand scheduling. Before going into the Program Source Code  Figure 3. Overview of our framework. detailed \ndescriptions of these two phases, we .rst give a formal de.nition of the problem we are trying to solve \nin this section.  4.1 Problem De.nition Our SLP optimizer takes a set of basic blocks of an application \nprogram code as the input. Each basic block consists of a se\u00adquence of statements S =<S1 ,S2 ,S3 , ..., \nSn >, where Si rep\u00adresents a single statement. Given an SIMD datapath width sup\u00adported by the target \narchitecture, our goal at this stage is to .nd a scheduling of these statements for each basic block, \nrepresented by D =<D1 ,D2 ,D3 , ..., Dm >, such that the performance of the basic block is maximized. \nHere, Di denotes either a single state\u00adment from S, or a superword statement that contains multiple state\u00adments.A \nscheduling D is said to be valid, if and only if it satis.es the following four constraints: 1. There \nis no dependence between the statements in each su\u00adperword statement: .Di, if Di is a superword statement, \nthen .Sp,Sq . Di, Sp and Sq are dependence free. 2. The dependences between the statements in the sequence \nS are preserved in D: if Sp dSq , and Sp . Di, Sq . Dj , then Di dDj , where d stands for a dependence \nrelationship between a source (single statement or superword statement, i.e., Sp or Di), and a target \n(single statement or superword statement, i.e., Sq or Dj ). 3. The statements within each superword \nstatement are isomor\u00adphic, i.e., theycontain the same operations in the same order. 4. The data width \nof the potential superword operation should not exceed the datapath width supported by the underlying \narchitecture.  These constraints are necessary to guarantee the correctness of pro\u00adgram execution (i.e., \npreserve the original semantics) when exploit\u00ading SIMD opportunities in the basic blocks. We want to \nemphasize that, two schedulings D and D ' for a basic block are identical, if and only if (1) they contain \nthe same set of SIMD groups (the concept of SIMD group is similar to that of superword statement except \nthat its statements are not ordered), (2) the sequence of the single statements and SIMD groups in both \nschedulings are the same, and (3) the ordering of the statements in their corresponding superword statements \nare the same. The .rst condition is crucial in that it determines how many SIMD instructions are produced \nand more importantly, how many superword reuses are generated. The second condition is used to help maintain \nthe dependences in the original program code and bring superword reuses as closer as possible. The third \ncondition can affect the number of vector register permutation instructions needed to reuse the superwords. \n  2/3  1/1 1/2    Figure 4. The variable pack con.icting Figure 5. The statement grouping graph \nof Figure 6. The auxiliary graph for calculating graph of the example code in Figure 2. the example code \nin Figure 2. the weight of the candidate group {S4 ,S5 }. 4.2 Grouping We employan iterative process \nto perform statement grouping. This approach .rst uses a basic grouping algorithm to .nd groups of size \ntwo, and then treats the groups already found as atomic statements, and applies the basic grouping algorithm \nagain to obtain groups of larger sizes, until the target SIMD datapath is fully utilized. Thus, we are \nable to cope with processors with wider SIMD devices. In particular, in the basic grouping algorithm, \nwe identify all possible statement groups and evaluate their quality (i.e., how much ben\u00ade.t a statement \ngroup can potentially bring to the the .nal SIMD code generated for an entire basic block) in order to \ndecide which groups will be chosen. In other words, each group decision is made based on the global data \naccess and reuse pattern analysis regarding the superword reuses, instead of employing local/greedy heuristics. \nNote that, in this phase, a superword is said to be reused, if the data it contains are used more than \nonce in the code, even for the case with different orderings. The reason why we consider the latter case \nalso as a reuse is because it does not require expensive memory op\u00aderations between two usages, though \nit still needs vector register permutation instructions. 4.2.1 The Basic Grouping Algorithm Tofacilitate \nourexplanation, we use the .rst-round grouping, to\u00adgether with an example (a basic block), shown in Figure \n2, to de\u00adscribe the basic grouping algorithm, where we try to obtain SIMD groups of size 2 (i.e., two \nstatements). The .rst step of our basic grouping algorithm is to identify the candidate groups. A candidate \ngroup refers to a potential SIMD group that contains two isomorphic statements {Si,Sj }, where Si,Sj \n. S. According to the scheduling constraints listed above, there should be no dependence between these \ntwo statements, and the superword sizeof the candidate group should notexceed the tar\u00adget SIMD datapath \nwidth. Note that, there is no ordering between Si and Sj in the candidate group. In other words, the \nordering of the statements inside each group is ignored at this phase. Let C = {C1 ,C2 , \u00b7\u00b7\u00b7 ,Ct} denote \nthe set of all t candidate groups identi\u00ad.ed in the basic block. Two candidate groups C1 and C2 from \nC are said to be con.icting with each other, if they have a common statement, e.g., C1 = {Sp,Sq } and \nC2 = {Sp,Sr }, or there exists a dependence cycle between these two groups, e.g., C1 dC2 and C2 dC1 , \nbecause of the dependences between their member state\u00adments.We can see that, in either case, con.icting \ncandidate groups cannot coexist; otherwise,itwould leadto incorrectexecution.For example, the candidate \ngroup set for the code shown in Figure2 is C = {{S1 ,S2 }, {S1 ,S3 }, {S4 ,S5 }}. Based on the candidate \ngroup set, the second step builds a vari\u00adable pack con.icting graph.A variable pack refers to a set of \nvariables coming from the same position of different isomorphic statements in a candidate group, e.g., \n{V1 ,V2 } and {V3 ,V5 } from candidate group {S1 ,S2 } in the example code (Figure 2), which are expected \nto form superwords. Note that the variable packs are brought about by the statement grouping. Similar \nto statement grouping, we do not consider the ordering of the variables in a vari\u00adable pack at this step. \nThe purpose of building the variable pack con.icting graph is to capture all the con.icts between the \ncandi\u00addate groups in C,but at a .ner granularity using variable packs. In this way, we can later refer \nto this graph for an accurate analysis of the reuse information of variable packs (or superwords). The \nvari\u00adable pack con.icting graph VP =(V, T ) is constructed as follows: we go over all the candidate groups \nin C one-by-one; for each can\u00addidate group {Sp,Sq }, we create a new set of nodes representing all the \nvariable packs generated from its statements. In addition, we insert edges between these newly-built \nnodes and the nodes already in the graph which were generated from candidate groups that con\u00ad.ict with \n{Sp,Sq }. It is to be noted that each node with variable pack {Vi,Vj } is also tagged with its associated \ncandidate group in\u00adformation, i.e., {Vi,Vj } {Sp ,Sq }. Therefore, there may exist mul\u00adtiple nodes containing \nthe same set of variables, but they are gen\u00aderated from different candidate groups. And, we distinguish \nthem from one another in the graph. Most importantly, if such nodes do not have any edge among them, \nit means that the corresponding variable packs can coexist in the transformed code with SIMD op\u00aderations. \nThus, the numberof such nodesinfactgives us the reuse information of the corresponding superword, e.g., \n{Vi,Vj }. Figure 4 illustrates a variable pack con.icting graph built from the candi\u00addate group set of \nthe example code shown in Figure 2. Our third step is to build a statement grouping graph SG = (V ' ,T \n' ), using both the candidate groups identi.ed earlier and the variable pack con.icting graph obtained \nin the previous step. Each node(. V ' )in this graph denotes a statement Sp from the basic block, and \neach edge(. T ' ) represents a candidate group between two statements. That is, if two statements belong \nto the same candidate group, there is an edge connecting them, e.g., S1 and S2 as shown in Figure 5. \nMoreover, the statement grouping graphisa weighted graph, where the weightof each edge represents an \nestimation of the bene.t , i.e., the variable pack (superword) reuses, that each candidate group can \npotentially bring to the entire code. To calculate the weight of an edge between two statements Sp and \nSq from the candidate group {Sp,Sq}, we .rst construct an auxiliary graph by extracting all packs (nodes) \nfrom the variable packing graph VP , which are the same as those variable packs appearing in {Sp,Sq } \nbut do not con.ict with them. Also, all the edges among the extracted nodes in VP are maintained in SG. \nFor example, suppose we are calculating the weight of the edge between statements S4 and S5 in Figure \n5. The auxiliary graph we construct is depicted in Figure 6. It includes all nodes in Figure 4 that are \nthe same as the variable packs of {S4 ,S5 } and can also coexist with them. However, in the auxiliary \ngraph, there may still exist edges among the nodes, which indicates that the extracted nodes cannot be \nused together in any scheduling. We introduce a greedy strategy to eliminate these con.icts: at each \nstep, we select a node that has the highest degree (i.e., one with the largest number of edges connected \nto it), and remove Figure 7. The updated auxiliary graph af-Figure 8. The updated statement graph Figure \n9. The updated variable pack con.icting ter con.ict elimination from Figure 6. after the .rst grouping \ndecision {S1 ,S2 }. graph after the .rst grouping decision {S1 ,S2 }.    this node as well as all \nthe edges associated with it. We repeat this procedure until no edge is left in the auxiliary graph, \ne.g., as shown in Figure 7. Now, by combining the remaining nodes in the auxiliary graph, e.g., {V1 ,V2 \n} {S1 ,S2 } and {V3 ,V5 } {S1 ,S2 }, and the variable packs from {Sp,Sq } in VP , e.g., {V3 ,V5 } {S4 \n,S5 }, {V1 ,V2 } {S4 ,S5 } and {V1 ,V5 } {S4 ,S5 }, we can obtain an average reuse of the variable packs \n(superwords) from {Sp,Sq } in the entire basic block, e.g., 2/3 for {S4 ,S5 }. This reuse value is assigned \nas the weight of the edge between statements Sp and Sq in SG. Therefore, the weight between two statements \nprovides an estimate of the potential bene.t (in terms of superword reuses) for the entire basic block \n(global effect), brought by grouping them together as a superword statement in the .nal code. The fourth \nstep makes grouping decisions based on the state\u00adment grouping graph SG constructed in the previous step. \nSpeci.\u00adcally, we .rst sort all the edges in SG in a non-increasing order ac\u00adcording to their weights, \nand then select an edge that has the highest weight. The two statements connected by this edge are then \ndecided to be grouped as a superword statement, e.g., {S1 ,S2 } in Figure 5. Note that, if two edges \nhave the same weight, we randomly choose one. Once a decision is made, we update the two graphs, namely, \nSG and VP , as follows. In SG, we delete the nodes that repre\u00adsent the two statements in the SIMD group \njust found, as well as all the nodes connected to them (i.e., the con.icting statements), e.g., as shown \nin Figure 8. Similarly, in VP , we delete the nodes that represent the variable packs generated from \nthe newly decided SIMD group, as well as all the nodes connected to them (i.e., the con.icting variable \npacks), e.g., as displayed in Figure 9. We then recalculate the weights of all retained edges in SG.We \nwant to em\u00adphasize that, at this point, when we evaluate the global bene.ts (su\u00adperword reuses) of a \ncandidate group, we take into account all the variable packs that are the same as those of the SIMD groups \ndeter\u00admined sofar.Forexample, when calculating the weight for the can\u00addidate group {S4 ,S5 } in Figure \n8, we need to consider {S4 ,S5 } and the already-decided group {S1 ,S2 } together whenbuilding the auxiliary \ngraph. This process continues until no edge is left in the statement graph SG, which means that we have \nexploited all the opportunities for SIMD operations. As stated earlier in Section 2, the essential difference \nbetween our work and the original SLP algorithm [17] is that we take a global (an entire basic block \nwide) view when exploiting super\u00adword reuses during grouping, instead of employing a local/greedy heuristic. \nThis is clearly re.ected in the four steps involved in the basic grouping algorithm described above. \nEspecially, the .rst step (identifying the candidate groups) provides all the grouping possi\u00adbilities \nbefore making any decision; the second step (building the variable pack con.icting graph) retrieves information \non the whole set of possible superwords resulting from grouping, taking into ac\u00adcount the con.icts among \nthem; the third step (building the state\u00adment grouping graph) then evaluates the bene.ts brought by each \ncandidate group to the whole basic block, by extracting data access and reuse patterns from the variable \npack con.icting graph; .nally, the fourth step (making grouping decisions) selects the most bene\u00ad.cial \none as the current grouping decision, which increases super\u00adword reuses across the basic block.  4.2.2 \nIterative Grouping In the basic grouping algorithm described above, the size of a gen\u00aderated SIMD group \n(superword statement) is two. If we simply use the output of the .rst-round grouping to generate the \n.nal super\u00adword statements, the SIMD datapath width available in the under\u00adlying architecture may be \nunderutilized.To solve this problem, we extend our basic grouping algorithm using an iterative process \nto obtain SIMD groups with larger sizes. The basic idea is that, af\u00adter the .rst-round grouping, we treat \neach SIMD group {Sp,Sq } as a new single statement, and each variable pack as a new sin\u00adgle variable. \nThe original statement set of the input basic block is then updated by adding these new statements. Next \nwe apply the basic grouping algorithm on the input basic block again, but with the updated statement \nset. Note that some SIMD groups may not be considered for further grouping because they already can .ll \nthe SIMD datapath width.We iteratively employthis strategy until the .nally generated superword statements \nare able to exploit the SIMD datapath width to the largest extent possible. Therefore, our framework \nwill be able to ef.ciently utilize the SIMD devices even when/if datapath widths increase in the future. \n 4.3 Scheduling The .rst phase (grouping) described above solves the problem of grouping statements \ntogether as superword statements, and we fo\u00adcus on reducing expensive memory operations between two usages \nby achieving more superword reuses. However, additional vector register permutation instructions may \nstill be needed if the same data are arranged in different sequences in two superwords. In this phase \n(scheduling), we try to solve two issues: (1) obtain a valid execution sequence for all statements (including \nsingle statements and superword statements) in the basic block and bring the super\u00adword reuses in the \nsuperword statements as close as possible; (2) .x the ordering of the statements within each superword \nstatement to reduce the number of permutation operations as much as possible. In particular, we are more \nconcerned about the ordering between superword statements and the statement ordering within each su\u00adperword \nstatement, than the sequence between single statements or between single statements and superword statements, \nas the former has an impact on the numberofvector reshuf.ing/permutation op\u00aderations needed, while the \nlatter can be handled and optimized by instruction scheduling to be invoked later. We start by building \na dependence graph among the gener\u00adated superword statements, based upon the execution sequence of the \noriginal single statements in the input basic block. In this graph, each node denotes a superword statement, \nand each directed edge indicates a dependence between the two connected superword statements. Since the \ngrouping phase already excludes the exis\u00adtence of any cyclic dependence, we are guaranteed to have at \nleast one valid scheduling for all superword statements, without elimi\u00adnating anyof them.For this reason, \nas compared to [17], our frame\u00adwork can retain more superword statements for SIMD operations. Note that, \nin reality, this dependence graph can provide more than one possibility of scheduling the superword statements. \nThus we are able to take advantage of this .exibility to bring more bene.ts. Based on the dependence \ngraph, we schedule the superword statements and decide the ordering of the statements within each of \nthem.We de.nea live superword set as a set of superwords that are most likely in vector registers currently. \nNote that, in the live superword set, the ordering of the statements of each superword is determined. \nInitially,thelive superwordsetissetasempty.Wethen fetch from the dependence graph the ready superword \nstatements, of which all the dependencies have been resolved. These statements are the candidates to \nbe considered to be added into the target state\u00adment execution sequence. Next, we calculate the number \nof reuses between the superwords in each candidate superword statement and the ones in the live superword \nset. The candidate statement with the largest number of reuses is then selected as the next one to run, \nre\u00adsulting in closer superword reuses and higher probability of reuses in the vector registers. We next \ndecide an ordering for the state\u00adments of the superword statement just chosen. Given a superword statement \nof size N , there could be N ! different orderings.Yet we can reduce this number by testing only those \norderings that lead to at least one direct superword reuse (i.e., reuse without any permu\u00adtation operation). \nIn other words, to improve ef.ciency, we don t employ exhaustive search across all valid orderings. Among \nthe tested orderings, we choose the one that needs the smallest number of permutation operations for \nall the superword reuses in the con\u00adsidered superword statement. Meanwhile, we update the live su\u00adperword \nset by inserting into it the newly ordered superwords and removing from it those existing superwords \nthat access the same data.We then continue with our schedulingby fetching new ready superword statements \nand repeating the above steps until all the nodes in the dependence graph are processed.  It is to be \nnoted that, by isolating the determination of the order\u00ading of the statements in each superword statement \nfrom the group\u00ading phase and postponing it to scheduling phase, our framework is able to fully exploit \nboth the direct and indirect superword reuses. The latter is crucial as it can save expensive memory \noperations by introducing only register permutation instructions, which is ne\u00adglected in the original \nSLP algorithm [17]. In addition, when deter\u00admining the sequence for all statements (including single \nstatements and superword statements) in the basic block, we not only obtain a valid scheduling,but also \ntry to bring the superword reuses as close as possible, which helps transform the reuses into data locality \nin the vector register .le. After performing grouping and scheduling on the basic block, we employ a \nsimilar cost model used in [16] to estimate the poten\u00adtial speed-ups brought by the transformed code, \ntaking into account all the importantfactors, e.g., the number of SIMD instructions, the number of memory \noperations and the number of vector register reshuf.ing/permutation instructions. If we realize that \nour transfor\u00admation could potentially degrade the performance, we choose not to apply it. Speci.cally, \neven though we are able to increase par\u00adallelism by introducing SIMD operations, the overheads (memory \naccess latencies/instructions, vector register instructions) brought by packing/unpacking operations, \ncould be so high that it may take longer for the transformed code to .nish. In this case, we skip the \ncurrent basic block and move on to the next one.  4.4 Pseudo Code The pseudo-code of our basic grouping \nalgorithm is given in Fig\u00adure 10. In this algorithm, line1 identi.es the candidate statement groups. \nLines 2-11build thevariable pack con.icting graph, while lines 12-18 initialize the statement grouping \ngraph. As the key part of our algorithm, Lines 21-42 make grouping decisions one by one. More speci.cally, \nlines 22-30 construct an auxiliary graph for weight calculation, based on current candidate group and \nthe decided groups. The con.icts in the auxiliary graph are resolved in line 31, after which lines 32-38 \ncalculate the weight (average super\u00adword reuse). Lines 40-41 choose the edge with the largest weight \nas the current grouping decision, and update the variable pack con\u00ad.icting graph as well as the statement \ngrouping graph. Lines 21-42 repeat until all groups are identi.ed. The complexity of our basic grouping \nalgorithm is O(E2 SG \u00d7 NVP ), where ESG is the number of edges in the statement grouping graph and NVP \nis the number of nodes in the variable packing con.ict graph. The second phase of our algorithm (scheduling), \nis given in Fig\u00adure 11. Speci.cally, lines 1-9 form the dependence graph using the detected groups, from \nwhich lines 10-13 initialize the set of ready superword statements. Lines 15-18 select the superword \nstatement that has the highest number of superword reuses with the current live superword set, as the \nnext one in the statement execution se\u00adquence. Lines 19-27 determine the ordering of the statements in \nthe selected superword statement, aiming at reducing permutation operations as much as possible. Finally, \nlines 28-35 update the live superword set, the dependence graph, and the set of ready super\u00adword statements. \nThe algorithm then moves on to decide the next superword statement to put into the statement execution \nsequence. 5. Data Layout Optimization The optimizations applied in previous section can reduce the num\u00adber \nof packing/unpacking operations but cannot completely elim\u00adinate them. In fact, once a packing operation \nis required, its over\u00adhead can be signi.cant if the data it accesses are not aligned and storedin memoryina \ncontiguousfashion.Thepreviously proposed SLP algorithm [17] handles this issue by grouping together some \nstatements with contiguous memory accesses. However, this ap\u00adproach can lead to poor solutions in terms \nof superword reuses as we have discussed in the previous sections, and more importantly, it highly relies \non the existing data layout. For this reason, in this section, we introduce an additional step, called \ndata layout opti\u00admization, to further our bene.ts by reducing the number of mem\u00adory operations and register \ninstructions involved in the mandatory packing/unpacking operations. The basic idea behind our data layout \noptimization is to orga\u00adnize the superwords in the memory in a way such that the overhead involved in \nloading them into vector registers (or writing them back to memory fromvectorregister) couldbe minimized.Forexample, \nin Figure 13, assume the width of the SIMD datapath can hold two basic data types and we decide to group \nstatements S1 and S2 to\u00adgether after the .rst stage which yields two superwords: < a,b > and <A[4i],A[4i \n+ 3] >. Assume further that these two super\u00adwords are currently not in vector registers during execution. \nThus, theyneed to be packed/unpacked into/from the vector registers be\u00adfore/after the SIMD operation. \nIn this case, if variables a and b are already contiguous and aligned in the memory, we only need one \nmemory operation to write < a,b > back. For the same reason, it is also desirable to make the data accessed \nby A[4i] and A[4i + 3] be contiguous and aligned in the memory space. In this work, we mainly target \nat optimizing the data layout for two classes of super\u00adwords, namely, scalar superword and array reference \nsuperword. In the scalar superword, all involved variables are scalar, whereas the array reference superword \ncontains only array references. We treat these two classes of superwords separately when applying our \ndata layout optimization. Speci.cally, we employ address assign\u00adment for scalar superwords, while applying \narray transformation and replication for the array reference superwords. 5.1 Optimizationfor Scalar \nSuperword We try to solve the problem of layout optimization for scalar super\u00adwords by applying a similar \nstrategy used in the offset assignment problem in DSP code generation [20]. However, the difference is \nthat, in our case, the desired layout of the variables is determined by our statement scheduling for \nSLP (i.e., superword statement gen\u00aderation) in the .rst stage. Speci.cally, our algorithm is an iterative \nprocess andworks as follows.We .rst sort all the scalar superwords  Input: A sequence of statements \nof a basic block, S =< Input: A set of decided statement groups D Input: A set of superwords, P S1 ,S2 \n,S3 , ..., Sn > Output: The ordering of thegroupsin D and the sequence Output: Data layout for P ' .P \nOutput: A set of decided statement groups D Q of the statements in each group D .D 1: Initialize C;//C: \nset of scalar superwords; 1: Identify candidate statement group set G; 2: Initialize Y;//Y: set of array \nreference superwords; 1: Initialize DG;//DG:groupdependencegraph DG. 2: Initialize VP ;//VP : variable \npack con.icting graph. 3: for P .P do2: for D .D do3: for {Si,Sj }.G do 4: if .V . P, V is scalar then \n 3: GDG.V. GDG.V.{D}; 4: for {Vi,Vj },Vi . Si.V and Vj . Sj .V do 5: C .C.{P }; 4: end for 5: V P.V. \nV P.V .{{Vi,Vj } {Si,Sj } }; 6: else if .V . P, V is read-only reference to the same 6: for {Vq ,Vr } \n{Sq,Sr } and {Sq,Sr }n{Si,Sj } = \u00d8 do 5: for Di,Dj . GDG.V do array then 7: V P.E. V P.E. 6: if Di depends \non Dj then 7: Y .Y.{P }; 8: {{Vi,Vj } {Si,Sj } , {Vq,Vr } {Sq,Sr} }; 7: GDG.E. GDG.E. <Di,Dj >; 8: \nend if9: end for 8: end if 9: end for10: end for 9: end for 10: //Layout optimization for scalar superwords. \n11: end for 10: Initialize LP and RD;//LP: setoflivepacks. RD: set 11: while C \u00d8 do = 12: Initialize \nSG;//SG: statement grouping graph. of ready groups. 12: Sort C based on the occurences; 13: for {Si,Sj \n}.G do 11: for Di . GDG.V and not. <Dj ,Di >. GDG.E do 13: Select C .C with the largest number of occurences; \n14: for S .{Si,Sj } and S/. SG.V do 15: SG.V. SG.V .{S}; 12: RD . RD .{Di}; 14: Assign aligned memory \nslots to variables in C in the 13: end for same order; 16: end for 17: SG.E. SG.E .{{Si,Sj }}; 14: while \nRD = \u00d8 do 15: for C ' .C do ' 18: end for 15: for D . RD do 16: if C n C = \u00d8 then 19: D.\u00d8; 16: Nr = \nNumberOfReuses(D, LP); 17: C.C-{C ' }; 20: while SG.E \u00d8 do = 17: end for 18: end if 21: for {Si,Sj }. \nSG.E do 18: Choose D with largest Nr; 19: end for 22: Initialize AG;//AG: auxiliary graph. 19: //Decide \nthe sequence for statements inDi. 20: C.C-{C}; 23: for {Vi,Vj } {Sq } . V P.V do '' ,Sr 21: P .P .{C}; \n20: Q.\u00d8; Q: the candidate sequence set for D. 24: if {Vi,Vj } {Si,Sj } . V P.V and ,Sr } {Sq = 22: end \nwhile 21: for each sequence Q of D, such that D can directly {Si,Sj } and {{Vi,Vj } {Sq,Sr} , {Vi,Vj \n} {Si,Sj } }./ 23: //Layout optimization for array reference superwords. reuse one pack in LP do V P.V \nthen 24: while Y \u00d8 do = 25: AG.V. AG.V .{{Vi,Vj } {Sq,Sr} }; 22: Q.Q.{Q}; 25: Sort Y based on the occurrences; \n26: end if 23: end for 26: Select Y .Y with the largest number of occurences; 27: end for 24: for Q .Q \ndo 27: R is a reference in Y ; 28: for Pm,Pn . AG.V and {Pm,Pn}. V P.E do 25: Nr = NumberOfP ermutations(D, \nQ, LP); 28: for R . Y do 29: AG.E. AG.E .{{Pm,Pn}}; 26: end for 29: 6r is the memory access vector of \nR; 30: end for 27: Choose Q of D with smallest Np; 30: 6i + 6 r . Q6O; 31: Resolve con.icts in AG; \n28: for each pack <Vi,Vj > from D in the order Q do 31: //Apply the .rst layout transformation for spatial32: \n//Calculate reuses for{Si,Sj }. 29: LP .LP. <Vi,Vj >; locality. 33: r . 0; 30: if <Vq ,Vr >. LP and \n{Vq ,Vr } = {Vi,Vj } then 34: for {Vi,Vj } appear in AG.V or D . {{Si,Sj }} do 32: Obtain a transformation \nmatrix M; 31: LP .LP-{<Vq ,Vr >}; 6i + 6M6 33: r1 O1 ;//Q1 MQ and 6 35: r . r +(N {Vi,Vj } - 1);//N \n{Vi,Vj } : the number 6. Q1= O1 = O. 32: end if of occurence of {Vi,Vj }. 34: //Apply mapping/replication \nof data access by d6 36: end for 33: end for from A to B. 37: = r/Nt;//Nt: the number of pack types \nqm,n-O ' -f ' (d ' )(qn,1 ,...,qn,m-1 )T W {Si,Sj } 34: Update GDG; ' n 35: f(d6) . (f (d6' ),L + in \nD .{{Si,Sj }}. 35: Update RD; qn,m p);L: the number ofreferences in Y ; p: theposition 38: SG.W. SG.W.{W \n{Si,Sj } }; 36: end while of R in Y ; 39: end for 40: D.D.{Si,Sj };//W {Si,Sj } = MAX(SG.W). 36: end \nfor 41: Update VP and SG; 37: Y .Y-{Y }; 42: SG.W.\u00d8; 38: P ' .P ' .{Y }; 43: end while 39: end while \nFigure 10. Pseudo code for the basic Figure 11. Pseudo code for the scheduling Figure 12. Pseudo code \nfor the data layout grouping phase. phase. optimization. S1: S2: a = A[4i]; b = A[4i+3]; Figure 13. \nAn example that bene.ts from data layout optimiza\u00adtion. by their occurrences, followed by selecting the \nscalar superword with the largest number of occurrences as the current one to ap\u00adply layout optimization. \nThe scalars in the selected superword are then assigned consecutive memory locations in which the variables \nare organized in the same order as they appear in the superword. After that, we skip all the scalar superwords \nthat share variable(s) with the current one and thus have con.icting layout requirements. We repeat the \nabove process until all scalar superwords are pro\u00adcessed. It needs to be noted that, our data layout \noptimization may not be able to handle all scalar superwords because of the existing con.icts among them. \nHowever, we ensure that those with higher access frequencies are handled with priority, as they are likely \nto incur more packing/unpacking operations.  5.2 Optimizationfor Array Reference Superword Within a \nloop nest, an array reference can access different data el\u00adements in different iterations, which prevents \nus from treating it simply as a scalar when applying layout optimization. One alterna\u00adtive is to take \nadvantage of the access patterns of array references to achieve the desired data layout. In this work, \nwe focus on loop nests in which the loop bounds and array references are af.ne func\u00adtions of the enclosing \nloop indices and loop independent variables. As a result, we are able to utilize existing polyhedral \nmodel [7] for our data layout optimization. In the polyhedral model, the mem\u00adory access pattern of an \narray reference is represented as a memory access vector ar: r = Q i +O, (1) in which ai is the iteration \nvector, Q is the memory access ma\u00adtrix of size m \u00d7 n, and aoffset vector. Based on O is the access the \naccess patterns of the variables in the array reference super\u00adwords, we combine two different optimizations \ntogether when solv\u00ading the data layout optimization problem, i.e., af.ne transforma\u00adtion and selective \nmapping/replication. It is to be noted that two constraints need to be satis.ed here. First, all references \nin the su\u00adperword are intra-array references, i.e., accessing the same array. Second, since we use data \nreplication and a given data element may appear in two different memory locations, it can only be ap\u00adplied \nto read-only array references. The problem of data layout op\u00adtimization for array reference superword \ncan then be expressed as follows. Given an array reference superword PA =<A(a 1 ), ..., A(a k), ..., \nA(aan) > that accesses only array A, our goal is to ob\u00adtain a new array B from A such that the references \nin PA now access array B and the data accessed are arranged contiguously in the memory in the same order \nas theyappear in PA. Figure 14 gives an example that illustrates the basic idea of this optimization. \nIt assumes a SIMD datapath width of 2, and the array reference superword is <A[4i],A[4i + 3] >.We can \nsee that the access patterns of references A[4i] and A[4i+3] on array A require two register loads and \nadditional register reshuf.ing/permutation operations to pack them in a superword. By contrast, if we \ncan map the elements of array A to array B as illustrated in the example, and at the same time, change \nthe original references to <B[2i],B[2i+ 1] >, we need only one register load to pack them. As a whole, \nthe two references in the superword can now access the new array B in an interleavedfashion.  To put \nit in a more general way, after the optimization, the access pattern of each individual reference to \nthe new array is a strided pattern. The stride lengths of all the references are the same as the size \nof the superword, though the starting offsets of the references can vary depending on their respective \npositions within the superword. Therefore, the data transformation and replication for all the references \ncan be conducted in a uniform fashion. We assume that the access pattern of a reference R to array A \ncan be described as an access vector ar, as shown in Expression (1). Besides, we assume that the default \nlayout Ldefault adopted by the compiler is row major. Our optimization then consists of two steps: (1) \naf.ne transformation and (2) mapping to new array. The .rst step is similar to data layout transformation \nfor achiev\u00ading spatial locality. Speci.cally, we start by determining the data layout Lopt for reference \nR when considering only spatial local\u00adity in the loop nest, which largely depends on the innermost loop \nindex. In other words, Lopt is a data layout which achieves con\u00adtiguous data accesses in successive iterations \nof the innermost loop. Based on that, we obtain a transformation matrix M for array A by solving the \nfollowing equation: LdefaultM = Lopt. (2) After the transformation, the new reference R1 will access \nneigh\u00adboring data elements (may not be direct neighbors) of the trans\u00adformed array A in the innermost \nloop. The new memory access vector r1 for R1 is: r1 = Q1 i + O1 , (3) a where Q1 = MQ and O1 = MOa. Since \nthe default layout is assumed to be row major, the last column of the new memory ac\u00adcess matrix Q1 that \ndenotes the new access pattern in the innermost loop will be can = (0, 0, ..., 0,qm,n. )T In the second \nstep, our goal is to map/replicate the data in the transformed array A accessed by R1 to a new array \nB, using non\u00adaf.ne transformations, so that the data elements accessed by R1 are stored in a strided \nmanner within array B. The stride length is equal to the length of the superword L, and the starting \noffset in B is equal to p, which is the offset of R1 in the array reference superword. Considering a \nsimple case, if A is a one dimensional array and R1 = A[ai + b], we map the data in index d accessed \nby R1 in A to array B using the following function: d - b f (d)= L + p. (4) a It is straightforward to \nprove that the above formula can be used to express the mapping in the example in Figure 14. On the other \nhand, if A is a two dimensional array, let us assume that Q1 = S: S: a =A[i]; 1 S: c = a \u00d7B[4i]; S4: \n S: g = q \u00d7B[4i-2];  1 2 3 S: c = a \u00d7 B[4i]; 2 S: b = A[i+1];  S: d =b \u00d7B[4i+4]; 4 5 S: d =b \u00d7B[4i+4]; \n 5 S: g = q \u00d7B[4i-2]; 3 S: h = r \u00d7B[4i+2]; 6 S6: h = r \u00d7 B[4i+2]; S: A[2i] = d + a \u00d7 c; 7  : A[2i] \n= d + a \u00d7c;S: A[2i+2] = g + r \u00d7h;S7 8 S: A[2i+2] = g + r \u00d7h; (a) 8  (b)  a = A[i]; b = A[i+1]; c = \na \u00d7 D[2i]; h = r \u00d7 D[2i+1];  A[2i] = d + a \u00d7c; A[2i+2] = g + r \u00d7 h; (c)  (d) \u00bb q11 0 and aO1 = (o1 \n, o2 )T . For a data access index ad = q21 q22 (d1 ,d2 ) in array A accessed by R1 , we can apply the \nfollowing mapping function: d1 -o1 d2 - o2 - q21 f (d)=( , q11 L + p). (5) q11 q22 d1 - o1 We now discuss \nthe general mapping/replication function for an array A of N dimensions. We .rst obtain a new equation \nfrom Equation (3) by removing the last dimension of vectors ra1 ,i and aa O1 , as well as the last row \nand the last column of Q1 : ' Q ' i ' + O ' r = 1 , (6) 1 1 where Q ' 1 is nonsingular. Given an array \nindex da=(d1 ,d2 , ..., dN ) in A accessed by R1 , we .rst obtain a mapping function for a d' =(d1 ,d2 \n, ..., dN -1 ): '-1 f ' (d ')= Q(d ' - O ' ). (7) 11 1 a Next we obtain the .nal mapping/replication \nfunction for d to ensure strided accesses to the new array B in the innermost loop: qm,n - O ' - f ' \n(d ')(qn,1 , ..., qn,m-1 )T ' n f (d)=(f (d ' ),L+p). (8) qn,m As stated before, our data layout optimization \nfor array refer\u00adence superwords are limited to intra-array and read-only references in the af.ne loop \nnests. In addition, since we employ data map\u00adping/replication, more memory space are needed to hold the \nrepli\u00adcated data. In case the input data sizes retrieved by static analysis are too large and/or the \nphysical memory available is too small, we can skip the layout transformation for array reference superwords, \nand only perform layout optimization for scalar superwords. The pseudo-code of our data layout optimization \nalgorithm is given in Figure 12. Lines 1-9 retrieve the scalar superwords and array reference superwords, \non which our algorithm is applicable. Lines 10-22 and 23-39 then apply layout transformation to the scalar \nsuperwords and array reference superwords, respectively.  6. An Example In the .gure, the .rst transformation \nis performed by the orig\u00adinal SLP algorithm. It starts by identifying the set of isomorphic statements \nwith contiguous memory accesses as the seed groups, which are {<S1 ,S4 >}. It then follows the def-use/use-def \nchains to obtain additional superword statements, i.e., <S2 ,S5 >, while achieving superword reuses, \n< a,b >. The .nal set of generated superword statements is {<S1 ,S4 >, <S2 ,S5 >, <S3 ,S6 >, <S7 ,S8 \n>}, as shown in Figure 15 (b).We can see that, in the op\u00adtimized code, there is only one superword reuse \nthat can help save one packing operation. That is, packing of < a,b > in <S2 ,S5 > can be saved by reusing \nit in <S1 ,S4 >. The second transformation in the example applies our .rst opti\u00admization (superword statement \ngeneration) to the input code. Fol\u00adlowing the strategy described in Section 4 step by step, we obtain \nthe optimized code shown in Figure 15 (c), with the set of gen\u00aderated superword statements {<S1 ,S4 >,< \nS5 ,S3 >,< S2 , S6 >,< S7 ,S8 >}. The differences between Figure 15 (b) and Figure 15 (c) mainly result \nfrom the grouping decisions for state\u00adments {S2 ,S3 ,S5 ,S6 }. Our framework chooses to group them as \n{<S5 ,S3 >,< S2 ,S6 >}, instead of {<S2 ,S5 >,< S3 ,S6 > }, as the former can bring more superword reuses \nto the whole basic block. This is achieved by taking a global view whenever a group\u00ading decision is made. \nAs a result, in the transformed code (Figure 15 (c)), we obtain three superword reuses (< d,g >, < c,h \n>, and < a,r >), compared to only one in Figure 15 (b). Thus, our approach can bring more packing/unpacking \nreductions. The third transformation shown performs data layout trans\u00adformation on the code in Figure \n15 (c). Note that more pack\u00ading/unpacking savings are obtained by optimizing the data layout for both \nscalar superwords(< a,b >, < d,g > and < c,h >), and array reference superword(<B[4i + 4],B[4i - 2] > \nand <B[4i],B[4i + 2] >), using the strategy discussed in Section 5. The generated code is given in Figure \n15 (d), in which arrays C and D are constructed using data replication and renaming. The boxes that appear \nin (d) but not in (c) indicate additional bene.ts from layout optimization. For example, when applying \nSIMD to S1 and S4, (c) will introduce unpacking overhead for scalar references a and b. But (d) will \nnot, since a and b are already adjacent in mem\u00adory after the layout transformation and can be written \nback together from one vector register. 7. Experimental Evaluation   7.1 Implementation and Setup We \nevaluated our compiler framework on two systems using 16 benchmarks. We implemented our SLP framework \non top of the SUIF 2.0 compiler infrastructure [4, 33]. We also implemented an alternate algorithm proposed \nin [17] against which we compare our proposed optimizations. In our implementation of the algo\u00adrithm \nin [17], we tried to optimize its performance as much as we could. In both the implementations (ours \nand [17]), we included pre-processing steps which perform alignment analysis and loop\u00adunrolling, with \nthe purpose of exposing more superword level par\u00adallelism to the compiler. In other words, both the implementations \nuse exactly the same pre-processing steps. In addition, for both the implementations, we map the register \nreshuf.ing/permutation operations to native shuf.ing instruction set supported by the un\u00adderlying architecture, \nrather than loading/storing from/to physical memory. In the following discussion, we refer to the .rst \nstage opti\u00admization in our SLP framework (superword statement generation) Table 1. Characteristics of \nthe Intel Dunnington based machine. Figure 15 gives an example that illustrates our SLP optimization \non a basic block, and compares it with the original SLP algorithm [17]. We assume that one superword \ncan hold two variables. The original input code is shown in Figure 15 (a). Number of Cores 12 cores \n(2 sockets) core Type Xeon CPU E7450(clocked at 2.40GHz) L1 Data 32KB/core; 8-way; 64-byte line size \nL2 total 18MB (6 \u00d7 3MB); 12-way; 64-byte line size L3 total 24MB (2 \u00d7 12MB); 12-way, 64-byte line size \n Number of Cores 4 cores Core Type AMD Phenom II \u00d74 (clocked at 3.00GHz) L1 Data 64KB/core; 2-way; 64-byte \nline size L2 total 2MB (4 \u00d7 512KB); 16-way; 64-byte line size L3 total 6MB (1 \u00d7 6MB); 48-way, 64-byte \nline size Table 2. Characteristics of the AMD Phenom II based machine. Table 3. Benchmark description. \nSPEC2006 [3] cactusADM soplex lbm milc povray gromacs calculix dealII wrf namd Solving the Einstein evolution \nequations Liner programming solver using simplex algorithm Lattice Boltzmann method Simulations of 3-D \nSU(3)lattic gauge theory Ray-tracing: a rendering technique Performing molecular dynamics Setting up.nite \nelement equations and solves them Object oriented .nite elementsoftware library Weather research and \nforecasting Simulation oflarge biomolecular systems NAS [2] ua ft bt sp mg cg Unstructured adaptive 3-D \nFast fourier transform (FFT) Block tridiagonal Scalar pentadiagonal Multigrid to solve the 3-D possion \nPDE Conjugate gradient as Global, and the SLP algorithm proposed in [17] as SLP. The version obtained \nby combining Global with our data layout opti\u00admization (discussed in Section 5), is referred to as Global+Layout. \nFinally, on both the systems, the native compiler-generated version when SLP optimization is enabled \nis denoted as Native. In our ex\u00adperiments, we compared these four schemes with each other as well as \nthe original applications that do not employ any SLP speci.c optimization (scalar code). In terms of \ncompilation overhead, com\u00adpared to the SLP version, our approach increased compilation time by 27% on \naverage. Both the systems used in our experiments, Intel Dunnington based machine and AMD Phenom II based \nmachine, support the SSE/SSE2 instruction set. Theycontain a set of 128-bit vector reg\u00adisters that enable \ntwo 64-bit, four 32-bit, eight 16-bit, or sixteen 8-bit operands to be processed concurrently. In particular, \nthe Intel Dunnington based machine has a dual hexa-core with Intel Xeon CPU E7450 clocking at 2.40GHz, \nwhereas the AMD Phenom II based machine has a quad-core with AMD Phenom II X4 945 op\u00aderating at 3.00GHz.Table1 \nandTable2give the detailed con.gu\u00adrations of these two commercial machines. The set of benchmark programs \nused in this study are listed in Table 3. We used all C and C++ .oating-point benchmarks in SPEC2006[3]aswellassixNAS \nbenchmarks[2].Our applications exhibit diversity in terms of the types of inherent parallelism they contain. \nFor each benchmark, we used the largest input size avail\u00adable. The results reported below represent average \nvalues across multiple runs with the same con.guration, and collected using one core by default (we present \nmulticore results later).  7.2 Results Our .rst set of results present the execution time reductions \n(on the Intel architecture) brought by Global, SLP and Native, all nor\u00admalized, for each benchmark, to \nthe execution time observed when no SLP optimization is enabled (scalar code). In Figure 16, bench\u00admarks \non the x-axis are ordered from the one for which Global generates the least improvement, to the one it \ngenerates the high\u00adest improvement.Based on the improvements achievedby Global, we can divide our benchmarks \ninto three categories (each category is marked by its own box in Figure 16). We further observe that \nour approach (Global) and SLP generate the same results in three of all the benchmarks tested; however, \nin all the other applications, Global consistently outperforms SLP. Our Global version performs better \nover SLP when there are more valid scheduling candidates and more potential superword reuses in a basic \nblock. In addition, SLP and Native result in the same output code and performance in four applications. \n  To explain the performance difference between our approach (Global) and SLP, we present in Figure \n17 the reductions brought by Global over SLP in terms of the dynamic instructions exe\u00adcuted (excluding \nthe packing/unpacking instructions) and the pack\u00ading/unpacking overheads. It can be seen that our approach \ncuts dy\u00adnamic instructionsand packing/unpacking operationsofSLPonav\u00aderage by 14.5% and 43.5%, respectively. \nIn particular, we observe that Global achieves signi.cant packing/unpacking instruction re\u00adductions for \nmost benchmarks tested, which shows the advantage of our global strategy of extracting superword reuses. \nWe next study how close the results generated by our approach (Global) are to the optimal potential improvements, \ni.e., if super\u00adword level parallelism could be fully exploited. Figure 18 plots the percentage of dynamic \ninstructions eliminated by Global over the scalar code for a variety of hypothetical datapath widths. \nRecall that both of our machines use superwords of 128 bits. When we look at the results of 128-bit superword, \nwe observe that, on aver\u00adage, nearly 49.1% of the dynamic instructions of the original appli\u00adcations \n(without any SLP optimization) are eliminated by Global. Further, when we increase the datapath width \nto 1024 bits, this value climbs to 54.5%. Considering that future architectures may employlonger datapath \nwidths, our approach can be more effective in the future. Figure 19 gives the results (execution time \nreductions over the scalar code) of Global+Layout (results of Global are reproduced here for ease of \ncomparison). We can observe that our data lay\u00adout optimization brings additional bene.ts in seven of \nour bench\u00admarks (those benchmarks are marked using rectangles). The reason why the layout optimization \ndoes not bring any bene.t in the re- Figure 19. Performance improvements in terms of execution times \nwith Global+Layout on the Intel Dunnington based machine. maining benchmarks is because its application \nis restricted by cer\u00adtain constraints as discussed in Section 5. In particular, we employ data replication \nto optimize layout for the array reference super\u00adwords, which has a negative impact on the cache behavior. \nIn order to achieve improvement, the bene.t of layout optimization has to outweigh the cost. Otherwise, \nin our implementation, we skip the data optimization phase. It is to be noted that, when considering \nthe results of the SLP version in Figure 16, the highest performance improvement Global+Layout brings \nover SLP is about 15.2%. We also present results collected on our AMD system. Fig\u00adure 20 plots the execution \ntime reductions brought by Global and Global+Layout over the scalar code, respectively. We see that, \non average, Global and Global+Layout bring 10.8% and 14.1% im\u00adprovements, which are similar to the average \nimprovements we obtained on the Intel machine (12% and 14.9% for Global and Global+Layout, respectively).We \nbelieve that, in cases where sav\u00adings are lower (compared to the Intel machine) on the AMD ma\u00adchine, \nthe mainfactor is the higher packing/unpacking costs. Finally, we evaluate the effectiveness of our compiler \n(both Global and Global+Layout) on multithreaded applications.For this evaluation, we focus on the NAS \nbenchmarks in our experimen\u00adtal suite, and present the execution time reductions over the scalar code \nin Figure 21. These experiments are performed in our Intel Dunnington based machine (with a total core \ncount of 12, dis\u00adtributed over two sockets). Note that, in these plots, the y-axis gives the execution \ntime reductions brought by our approach over the original application, with both of them running on the \nsame num\u00adber of cores. We clearly see that both of our approaches, Global in Figure 21(a) and Global+Layout \nin Figure 21(b), bring consis\u00ad   Execution Time Improvement 33% 23% 13% 3% -7% Core Count (b) Global+Layout. \n         1 2 4 6 8 10 12 Figure 21. Performance improvements in terms of execution times with \ndifferent core counts (x-axis). tent improvements across different core counts. The results become slightly \nbetter when we increase the number of cores, mostly due to the less-than-perfect scalability of the original \napplications. 8. Concluding Remarks The main contribution of this paper is an automated compiler framework \nthat detects and exploits superword level parallelism in application programs. We apply a two-stage strategy, \nnamely, superword statement generation and data layout optimization, to increase SIMD level parallelism, \nreduce the number of super\u00adword packing/unpacking operations by extracting more superword reuses, and \nreduce the overhead of mandatory packing/unpacking operations by reorganizing data in the memory. The \nresults col\u00adlected on two commercial systems (Intel and AMD) indicate that the proposed SLP framework \nperforms better than an existing SLP scheduling algorithm, bringing as much as 15.2% performance im\u00adprovement. \nAcknowledgments This research is supported in part by NSF grants #1147388, #1152479, #1017882, #0963839, \n#0720645, #0811687, #0702519 and a grant from Microsoft Corporation. References [1] Pentium processor \nwith MMX technology. http://edc. intel.com/Platforms/Previous/Processors/ Pentium-MMX/. [2] NAS parallel \nbenchmark suite. http://www.nas.nasa.gov/ Resources/Software/npb.html. [3] Spec cpu2006. http://www.spec.org/cpu2006/. \n[4] The SUIF 2 compiler system. http://suif.stanford.edu/ suif/suif2/. [5] R. Barik,J. Zhao, andV. Sarkar. \nEf.cient selectionofvector instruc\u00adtions using dynamic programming. MICRO, 2010. [6] A. Bik, M. Girkar, \nP. Grey, and X. Tian. Automatic intra-register vectorization for the intel architecture. IJPP, 2002. \n[7] U. Bondhugula, A. Hartono, J. Ramanujam, and P. Sadayappan. A practical automatic polyhedral parallelizer \nand locality optimizer. Proc. of PLDI, 2008. [8] D. DeVries. A vectorizing SUIF compiler: Implementation \nand per\u00adformance. Master s Thesis, 1997. [9] A. Eichenberger, P. Wu, and K. O Brien. Vectorization for \nSIMD architectures with alignment constraints. PLDI, 2004. [10] R. Hanxleden and K. Kennedy. Relaxing \nSIMD control .ow con\u00adstraints using loop transformations. PLDI, 1992. [11] T. Henretty, K. Stock, L. \nPouchet, F. Franchetti, J. Ramanujam, and P. Sadayappan. Data layout transformation for stencil computations \non short-vector SIMD architectures. CC, 2011. [12] M. Hohenauer, F. Engel, R. Leupers, G. Ascheid, and \nH. Meyr. A SIMD optimization framework for retargetable compilers. TACO, 2009. [13] IBM. PowerPC microprocessorfamily:Vector/SIMD \nmultimedia ex\u00adtension technology programming environments manual. IBM Systems andTechnology Group, 2005. \n[14] Intel. IA-32 intel architecture optimization reference manual. 2005. [15] A. Krall and S. Lelait. \nCompilation techniques for multimedia proces\u00adsors. IJPP, 2000. [16] S. Larsen. Compilation techniques \nfor short-vector instructions. PhD Thesis, 2006. [17] S. Larsen and S. Amarasinghe. Exploiting superword \nlevel parallelism with multimedia instruction sets. PLDI, 2000. [18] C. Lee and D. DeVries. Initial results \non the performance and cost of vector microprocessors. Micro, 1997. [19] C. Lee and M. Stoodley. Simple \nvector microprocessors for multime\u00addia applications. Micro, 1998. [20] R. Leupers andF.David.A uniform \noptimization technique foroffset assignment problems. ISSS, 1998. [21] D. Nuzman and A. Zaks. Outer-loop \nvectorization -revisited for short SIMD architectures. PACT, 2008. [22] D. Nuzman, I. Rosen, and A. Zaks. \nAuto-vectorization of interleaved data for SIMD. PLDI, 2006. [23] S. Oberman, G. Favor, and F. Weber. \nAmd 3dnow! technology: Architecture and implementations. IEEE MICRO, 1999. [24] G. Ren,P.Wu, andD.Padua. \nOptimizing data permutations forsimd devices. PLDI, 2006. [25] J. Shin. Compiler optimizations for architectures \nsupporting superword-level parallelism. PhD Thesis, 2005. [26] J. Shin, J. Chame, and M. Hall. Compiler-controlled \ncaching in superword register .les for multimedia extension architectures. PACT, 2002. [27]J. Shin,J. \nChame, andM. Hall. Exploitingsuperword-level localityin multimedia extension architectures. JILP, 2003. \n[28] J. Shin, M. Hall, and J. Chame. Superword-level parallelism in the presence of control .ow. CGO, \n2005. [29] N. Sreraman and R. Govindarajan. A vectorizing compiler for multi\u00admedia extensions. IJPP, \n2000. [30] C.Tenllado,L. Pinuel,M. Prieto, andF. Catthoor.Pack transposition: Enhancing superword level \nparallelism exploitation. PARCO, 2005. [31] C. Tenllado, L. P. M. Prieto, F. Tirado, and F. Catthoor. \nIm\u00adproving superword level parallelism support in modern compilers. CODES+ISSS, 2005. [32] M.Weiss. Strip-mining \non SIMD architectures. ICS, 1991. [33] R. Wilson, R. French, C. Wilson, S. Amarasinghe, J. Anderson, \nS. Tjiang,S. Liao,C.Tseng,M. Hall,M. Lam, andJ. Hennessy. SUIF: An infrastructure for research on parallelizing \nand optimizing compil\u00aders. ACM SIGPLAN Notices, 1994. [34]P.Wu,A. Eichenberger,andA.Wang.Ef.cientSIMDcode \ngeneration for runtime alignment and length conversion. CGO, 2005.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>SIMD (single-instruction multiple-data) instruction set extensions are quite common today in both high performance and embedded microprocessors, and enable the exploitation of a specific type of data parallelism called SLP (Superword Level Parallelism). While prior research shows that significant performance savings are possible when SLP is exploited, placing SIMD instructions in an application code manually can be very difficult and error prone. In this paper, we propose a novel automated compiler framework for improving superword level parallelism exploitation. The key part of our framework consists of two stages: superword statement generation and data layout optimization. The first stage is our main contribution and has two phases, statement grouping and statement scheduling, of which the primary goals are to increase SIMD parallelism and, more importantly, capture more superword reuses among the superword statements through global data access and reuse pattern analysis. Further, as a complementary optimization, our data layout optimization organizes data in memory space such that the price of memory operations for SLP is minimized. The results from our compiler implementation and tests on two systems indicate performance improvements as high as 15.2% over a state-of-the-art SLP optimization algorithm.</p>", "authors": [{"name": "Jun Liu", "author_profile_id": "81474658649", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P3471249", "email_address": "jxl1036@cse.psu.edu", "orcid_id": ""}, {"name": "Yuanrui Zhang", "author_profile_id": "81444604154", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P3471250", "email_address": "yuazhang@cse.psu.edu", "orcid_id": ""}, {"name": "Ohyoung Jang", "author_profile_id": "81498649799", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P3471251", "email_address": "oyj5007@cse.psu.edu", "orcid_id": ""}, {"name": "Wei Ding", "author_profile_id": "81496659745", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P3471252", "email_address": "wzd109@cse.psu.edu", "orcid_id": ""}, {"name": "Mahmut Kandemir", "author_profile_id": "81100186744", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P3471253", "email_address": "kandemir@cse.psu.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254106", "year": "2012", "article_id": "2254106", "conference": "PLDI", "title": "A compiler framework for extracting superword level parallelism", "url": "http://dl.acm.org/citation.cfm?id=2254106"}