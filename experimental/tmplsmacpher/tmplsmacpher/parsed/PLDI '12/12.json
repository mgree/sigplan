{"article_publication_date": "06-11-2012", "fulltext": "\n Parcae: A System for Flexible Parallel Execution Arun Raman Ayal Zaks Intel Research Intel Corporation \nSanta Clara, CA, USA Haifa, Israel arun.a.raman@intel.com ayal.zaks@intel.com Abstract Workload, platform, \nand available resources constitute a parallel program s execution environment. Most parallelization efforts \nstat\u00adically target an anticipated range of environments, but performance generally degrades outside that \nrange. Existing approaches address this problem with dynamic tuning but do not optimize a multipro\u00adgrammed \nsystem holistically. Further, they either require manual programming effort or are limited to array-based \ndata-parallel pro\u00adgrams. This paper presents Parcae, a generally applicable automatic system for platform-wide \ndynamic tuning. Parcae includes (i) the Nona compiler, which creates .exible parallel programs whose \ntasks can be ef.ciently recon.gured during execution; (ii) the Dec\u00adima monitor, which measures resource \navailability and system per\u00adformance to detect change in the environment; and (iii) the Morta executor, \nwhich cuts short the life of executing tasks, replacing them with other functionally equivalent tasks \nbetter suited to the current environment. Parallel programs made .exible by Parcae outperform original \nparallel implementations in many interesting scenarios. Categories and Subject Descriptors D.1.3 [Software]: \nConcur\u00adrent Programming Parallel Programming; D.3.4 [Programming Languages]: Processors Compilers, Run-time \nenvironments General Terms Design, Performance Keywords automatic parallelization, code generation, compiler, \n.exible, multicore, parallel, performance portability, run-time, adaptivity, tuning 1. Introduction The \nemergence of general-purpose multicore processors has re\u00adsulted in a spurt of activity in parallel programming \nmodels. Justi.\u00adably, the primary focus of these programming models has been par\u00adallelism extraction. \nHowever, parallelism extraction is just one part of the problem of synthesizing well-performing programs \nwhich execute ef.ciently in a variety of execution environments. The other, equally important part is \nthe tuning of the extracted par\u00adallelism [29, 30]. In the absence of intelligent tuning, a paral\u00adlel \nprogram may perform worse than the original sequential pro\u00adgram [23, 40]. Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. \nCopyright &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 Jae W. Lee David I. August Sungkyunkwan \nUniversity Princeton University Suwon, Korea Princeton, NJ, USA jaewlee@skku.edu august@princeton.edu \nParallel program performance depends on several environmen\u00adtal run-time factors. Synchronization and \ncommunication over\u00adheads are often dif.cult to predict and may erode the bene.ts of parallelism [40]. \nParallel resources available to the program may vary, including number of cores and memory bandwidth \n[9, 23]. In addition, application workload can change during execution. Most parallel programs are produced \nby programmers or compilers with a single static parallelism con.guration encoded at development or compile \ntime. Any single program con.guration is likely to become suboptimal with changes in the execution environment \n[33, 40]. Libraries such as OpenMP and Intel Threading Building Blocks allow programmers to encode multiple \nprogram con.gurations im\u00adplicitly, and have an appropriate con.guration chosen at run-time based on the \nexecution environment [33, 40]. These libraries, how\u00adever, require programmers to expend considerable \nefforts (re-) writ\u00ading the parallel program. Furthermore, the run-time systems of these libraries optimize \nthe execution of individual programs, with\u00adout exploring platform-wide implications and opportunities. \nIdeally, compilers should automatically generate .exible par\u00adallel programs which adapt to changes in \ntheir execution environ\u00adment. Existing compiler and run-time systems tune parameters such as the degree \nof parallelism of data-parallel (DOALL) loops and block size of loops, either statically or dynamically, \nto match the execution environment [3, 5, 14, 17, 41, 44]. These systems are limited to optimizing array-based \nprograms with communication\u00adfree data-parallelism, where the performance impact of those pa\u00adrameters \ncan be relatively easily modeled. However, for general\u00adpurpose programs with complex dependency patterns, \nparallelism is typically non-uniform and involves explicit synchronization or communication. This signi.cantly \ncomplicates performance mod\u00adeling, often resulting in a large space of possibly effective paral\u00adlelism \ncon.gurations. Existing compiler-based parallelization al\u00adgorithms for such general-purpose programs \nselect a single con.g\u00aduration, typically one deemed most suitable for an unloaded plat\u00adform [34, 43, \n47]. This paper presents Parcae, a compiler and run-time software system that delivers performance portability \nfor both array-based programs and general-purpose programs, extending the applicabil\u00adity of prior work. \nThe Parcae compiler, Nona, creates .exible par\u00adallel programs by (i) extracting multiple types of parallelism \nfrom a program region; (ii) creating code to ef.ciently pause, recon.gure, and resume its execution; \nand (iii) inserting pro.ling hooks for a run-time system to monitor its performance. The Parcae run-time \nsystem, comprising the Decima monitor and the Morta executor, maximizes overall system performance by \n(i) monitoring program performance and system events, such as launches of new programs, to detect change \nin the execution environment, and (ii) determining the best con.gurations of all .exible parallel programs \nexecuting concurrently for the new environment. We evaluate Parcae on two real platforms with 8 and 24 \ncores. Flexible parallel execution enabled by Parcae is compared with conventional parallel execution \nof programs generated by a com\u00adpiler applying high quality parallelizing transforms: PS-DSWP [34, 43] \nand DOANY [31, 46], targeting 8 and 24 threads with OS load balancing. Across seven benchmarks, Parcae \nreduces execu\u00adtion time by -1.4% (a small slowdown) to 41.9% with concomitant estimated processor energy \nsavings of 23.9% to 83.9%. Platform\u00adwide, Parcae reduces execution time of an entire workload by 12.8% \nto 53.5% with concomitant estimated processor energy sav\u00adings of 22.4% to 76.3%.  In summary, the main \ncontributions of this work are: 1. a compilation framework that is suf.ciently powerful to auto\u00admatically \ntransform both array-based and general-purpose se\u00adquential programs into .exible parallel programs (Section \n3), 2. a lightweight monitoring and execution framework to determine and enforce platform-wide optimal \nparallelism con.gurations of multiple concurrently executing programs (Section 4), and 3. a prototype \nimplementation and its evaluation on seven bench\u00admarks demonstrating its effectiveness in delivering \nrobust portable performance (Section 6).  2. Run-time Variability and Implications During program execution, \nthe workload processed by the program may change phase or the parallel resources available to the program \nmay vary. As an example, consider the problem of determining an ideal program con.guration while the \nnumber of cores allocated to a program by the OS varies. This is a common scenario in platforms that \nare shared by multiple programs. Figure 1 shows optimized performance of the blackscholes option pricing \nbenchmark from the PARSEC benchmark suite [7]. The compiler parallelized the outermost loop using a three-stage \npipeline parallelization scheme (sequential-parallel-sequential). The program was executed on two different \nplatforms, processing three different reference inputs, and making use of different number of cores. \nObserve that: On both platforms, the parallel version performs worse than sequential if three cores \nor fewer are employed.  On Platform 1, the parallel version scales to 8 cores on Input 1, plateaus at \n6 cores on Input 2, and is only slightly better than sequential on Input 3.  On Platform 2, the parallel \nversion scales to 10 cores but de\u00adgrades afterwards on Input 1, plateaus at 4 cores on Input 2 (with \n34% speedup), and performs worse than sequential on In\u00adput 3.  The reason for poor performance on Input \n3 is the signi.cantly greater communication bandwidth requirement, which overwhelms the bene.ts of parallelization. \nFrom the example, we infer that: The optimal con.guration depends in general on both platform and workload \ncharacteristics.  Employing more cores may not result in better performance, and may even degrade it. \nMoreover, parallel versions may even be slower than the sequential version.  If an optimal con.guration \nuses fewer cores than the total avail\u00adable, energy can be saved by switching off unused cores.  The \nrange of scenarios and varying concerns lay undue burden  on programmers to ensure performance portability. \nThis work presents a system that optimizes and executes programs in a way that .exibly adapts them to \nnew execution environments. Based on the above inferences, the proposed system: 1. prepares multiple \nparallel versions for each parallel region, as well as a sequential baseline version; 2. optimizes thread-level-parallelism \nleveraging both data-and pipeline-parallelism schemes; 3. generates parallel code that can be ef.ciently \ntuned and re\u00adplaced online as the parallel region executes; 4. periodically optimizes and tunes multiple \nsimultaneously exe\u00adcuting programs for the duration of their execution; and  (a) Platform 1: 8-core, \n8 GB RAM (b) Platform 2: 24-core, 24 GB RAM  Figure 1. Performance variability running blackscholes \noption pricing across 3 workloads and 2 parallel platforms. 5. performs all of the above automatically, \neasing the burden on the programmer. Figure 2 shows an example of the proposed execution model on a hypothetical \n.ve-core machine. Each parallel region consists of a set of concurrently executing tasks. (The inscription \ninside each box indicates the task and iteration number of a region; e.g., M5 represents the .fth iteration \nof task M.) At time t0, program P1 is launched with a pipeline parallel con.guration (PS-DSWP1) having \nthree stages corresponding to tasks A, B, and C. A and C are executed sequentially whereas B is executed \nin parallel by 3 cores as determined by the run-time system. At time t1, another program P2 is launched \non the same machine. In response, the run\u00adtime system signals P1 to pause at the end of its current iteration \n(iteration 5). The core receiving this signal (Core 1) acknowledges the signal at time t2 and propagates \nthe pause signal to the other cores. At time t3, P1 reaches a known consistent state, following which \nthe run-time system determines a new allocation of resources to programs P1 and P2, say 2 cores to P1 \nand 3 cores to P2. At time t4, the run-time system launches DOANY2 execution of both programs P1 and \nP2. For P1, task K and the synchronization block implement the same functionality as tasks A, B, and \nC. 3. Compilation for Flexible Parallel Execution The Nona compiler identi.es parallelizable regions \nin a sequential program and applies multiple parallelizing transforms to each re\u00adgion, generating multiple \nversions of .exible code. The generated .exible code can be paused during its sequential or parallel \nexe\u00adcution, recon.gured, and ef.ciently resumed by the Morta task ex\u00adecutor. Nona also inserts pro.ling \nhooks into the generated code for the Decima monitor to observe program behavior. The parallelizing 1 \nThe PS-DSWP transformation splits a loop body across stages and sched\u00adules them for concurrent execution. \nIt enforces dependencies through inter\u00adstage communication channels. 2 The DOANY transformation schedules \nloop iterations for parallel execu\u00adtion while synchronizing shared data accesses by means of critical \nsections. \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Workload, platform, and available resources constitute a parallel program's execution environment. Most parallelization efforts statically target an anticipated range of environments, but performance generally degrades outside that range. Existing approaches address this problem with dynamic tuning but do not optimize a multiprogrammed system holistically. Further, they either require manual programming effort or are limited to array-based data-parallel programs.</p> <p>This paper presents Parcae, a generally applicable automatic system for platform-wide dynamic tuning. Parcae includes (i) the Nona compiler, which creates flexible parallel programs whose tasks can be efficiently reconfigured during execution; (ii) the Decima monitor, which measures resource availability and system performance to detect change in the environment; and (iii) the Morta executor, which cuts short the life of executing tasks, replacing them with other functionally equivalent tasks better suited to the current environment. Parallel programs made flexible by Parcae outperform original parallel implementations in many interesting scenarios.</p>", "authors": [{"name": "Arun Raman", "author_profile_id": "81100001392", "affiliation": "Intel Research, Santa Clara, CA, USA", "person_id": "P3471175", "email_address": "arun.a.raman@intel.com", "orcid_id": ""}, {"name": "Ayal Zaks", "author_profile_id": "81100166351", "affiliation": "Intel Corporation, Haifa, Israel", "person_id": "P3471176", "email_address": "ayal.zaks@intel.com", "orcid_id": ""}, {"name": "Jae W. Lee", "author_profile_id": "81479656211", "affiliation": "Sungkyunkwan University, Suwon, South Korea", "person_id": "P3471177", "email_address": "jaewlee@skku.edu", "orcid_id": ""}, {"name": "David I. August", "author_profile_id": "81100388492", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P3471178", "email_address": "august@princeton.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254082", "year": "2012", "article_id": "2254082", "conference": "PLDI", "title": "Parcae: a system for flexible parallel execution", "url": "http://dl.acm.org/citation.cfm?id=2254082"}