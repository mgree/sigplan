{"article_publication_date": "06-11-2012", "fulltext": "\n Chimera: Hybrid Program Analysis for Determinism Dongyoon Lee Peter M. Chen Jason Flinn Satish Narayanasamy \nUniversity of Michigan {dongyoon,pmchen,j.inn,nsatish}@umich.edu Abstract Chimera1 uses a new hybrid \nprogram analysis to provide determin\u00adistic replay for commodity multiprocessor systems. Chimera lever\u00adages \nthe insight that it is easy to provide deterministic multipro\u00adcessor replay for data-race-free programs \n(one can just record non\u00addeterministic inputs and the order of synchronization operations), so if we \ncan somehow transform an arbitrary program to be data\u00adrace-free, then we can provide deterministic replay \ncheaply for that program. To perform this transformation, Chimera uses a sound static data-race detector \nto .nd all potential data-races. It then in\u00adstruments pairs of potentially racing instructions with a \nweak-lock, which provides suf.cient guarantees to allow deterministic replay but does not guarantee mutual \nexclusion. Unsurprisingly, a large fraction of data-races found by the static tool are false data-races, \nand instrumenting them each of them with a weak-lock results in prohibitively high overhead. Chimera \ndras\u00adtically reduces this cost from 53x to 1.39x by increasing the gran\u00adularity of weak-locks without \nsigni.cantly compromising on par\u00adallelism. This is achieved by employing a combination of pro.ling and \nsymbolic analysis techniques that target the sources of impreci\u00adsion in the static data-race detector. \nWe .nd that performance over\u00adhead for deterministic recording is 2.4% on average for Apache and desktop \napplications and about 86% for scienti.c applications. Categories and Subject Descriptors D.2.5 [Software \nEngineer\u00ading]: Testing and Debugging; D.4.5 [Operating Systems]: Relia\u00adbility General Terms Design, Performance, \nReliability Keywords Determinism, Replay, Data-race detection, Static anal\u00adysis, Pro.ling, Symbolic range \nanalysis 1. Introduction A shared-memory multithreaded program is not guaranteed to pro\u00adduce the same \noutput across different executions even if the input is guaranteed to be the same. Lack of determinism \nsigni.cantly impairs a programmer s ability to reason about an execution and understand the root causes \nof program failure. This problem can 1 Chimera is a mythological hybrid animal composed of parts of a \nlion, a goat and a snake. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. \n. . $10.00 be addressed by constructing multiprocessor systems that guaran\u00adtee that every execution for \na given input produces the same out\u00adput [14]. Another approach is to record the non-deterministic thread \ninterleavings and enable a programmer to deterministically replay and understand an execution [28]. In \nthis paper, we focus on solv\u00ading the latter replay problem for multithreaded programs, but the principles \ndiscussed here could be applied to build a deterministic multiprocessor system as well. The ability to \nfaithfully reproduce an execution has proven useful in many areas, including debugging [27, 47], fault \ntoler\u00adance [11], computer forensics [16], dynamic analysis [13, 36], and workload capture [34]. However, \npast solutions to deterministic re\u00adplay for shared-memory multiprocessor systems have been unsat\u00adisfactory \neither due to performance costs [17, 29, 49], reliance on custom hardware [24, 32, 33, 54], or lack of \nsuf.ciently strong de\u00adterminism guarantees [2, 38, 51, 56]. Uniprocessor replay is relatively easy. During \na recording phase, non-deterministic events (e.g., interrupts and data read from input devices) are logged. \nIf the program is multithreaded, then thread schedules (e.g., the instructions at which each thread is \npre\u00adempted) must also be logged and replayed [43]. Previous studies have shown that logging these non-deterministic \nevents adds little overhead [55]. Ef.cient multiprocessor replay, however, remains an open prob\u00adlem. \nThe fundamental challenge comes from recording the non\u00addeterministic interleaving among threads, a property \nthat is neces\u00adsary to deterministically replay programs that contain data-races. Recording and replaying \nthe frequent interactions among thread accesses to shared data can slow execution by an order of magni\u00adtude \nor more. However, if one could somehow statically guarantee that a program is data-race-free, then it \nis not necessary to record thread interactions past research has shown that recording and replaying \nthe happens-before order of synchronization operations is suf.cient to ensure deterministic replay [40]. \nIn most applications, synchronization operations are relatively infrequent compared to memory accesses, \nand therefore logging them is relatively cheap. Many uses of deterministic replay, including program \ndebugging, reproducing errors encountered in the .eld in a test environment, replication for fault tolerance, \nand forensics following a computer intrusion, are especially useful for programs with bugs, so working \nonly for bug-free programs (i.e., programs without data-races) is not a viable option. Unfortunately, \nstatically proving the absence of data-races in a program without rejecting data-race-free programs is \nhard. If there is a chance that a program contains a data-race, then one must record the order of potentially \nracing operations in order to guar\u00adantee that the recorded program can be replayed deterministically. \nOne could discover such operations with a dynamic data-race de\u00adtector. However, despite signi.cant advances, \ndynamic data-race detection in software slows program execution by nearly 8x [20] for state-of-the-art \ndetectors. Thus, logging the order of potentially racing instructions is no less of a problem than detecting \na data race.  In this paper we discuss Chimera, a deterministic replay system that employs a new hybrid \nprogram analysis to handle programs with data races. Chimera combines static data race analysis with \noff-line pro.ling and targeted, dynamic checks to provide deter\u00administic replay ef.ciently. Chimera instruments \na program to log all non-deterministic in\u00adputs (e.g., system call results), the thread schedule on each \nproces\u00adsor core, and the happens-before relationships due to synchroniza\u00adtion operations. This information \nis suf.cient to guarantee that the program can later be replayed deterministically, provided the pro\u00adgram \ncontains no data-races. To provide replay for racy programs, Chimera uses a sound but imprecise static \ndata-race detector (RELAY [50]) to .nd potential data-races. Every memory instruction that potentially \nraces with another instruction is placed inside a code region protected by a weak-lock. Chimera records \nall happens-before relationships due to weak-locks in addition to the relationships due to the original \nprogram synchronization. Thus, Chimera guarantees deterministic replay for all programs. We use weak-locks \ninstead of traditional locks in order to be conservative and avoid introducing arti.cial deadlocks. A \nweak\u00adlock is essentially a time-out lock, where mutual exclusion is com\u00adpromised if the weak-lock is \nnot acquired in reasonable amount of time. In the rare case when a weak-lock times out, Chimera deter\u00administically \npreempts the thread that currently holds the weak-lock and forces it to yield the weak-lock to the thread \nthat timed out on the weak-lock; the original holder of the weak-lock must reacquire the weak-lock before \nresuming its execution. This approach splits the code region protected by the weak-lock into two regions \nacross the preemption. Because this timeout mechanism enables Chimera to preserve the invariant that \nonly one thread holds a given weak\u00adlock at any given time, Chimera can support deterministic replay by \nreproducing the order of weak-locks at the same preemption point. Unsurprisingly, we .nd that a sound \ndata-race detector reports a large number of false data-races, and thus adding a weak-lock for every \nreported data-race results in prohibitively high overhead. Chimera employs two critical optimizations \nto drastically reduce this cost. Both optimizations attempt to increase the granularity of a weak-lock, \nin terms of the size of the locked code region and the amount of data the lock protects. Coarser weak-locks \nreduce the cost of instrumentation but may serialize threads unnecessarily and compromise parallelism. \nChimera s optimizations navigate this performance trade-off by targeting the main sources of imprecision \nin a static data-race detector [50]. The .rst optimization is based on the observation that a large fraction \nof false data-race reports are due to the inability of the static data-race detector to account for the \nhappens-before relations due to synchronization operations other than locks. One example of this is that \na number of data-races are reported between initial\u00adization code and the rest of the program because \nthe static tool does not account for the happens-before relation due to fork-join synchronization. To \naddress this problem, we pro.le the program of.ine over a variety of inputs. If the code regions containing \npo\u00adtentially racing instructions are non-concurrent in all pro.le runs, Chimera increases the granularity \nof the weak-lock to protect the entire code region instead of just one instruction. Chimera cur\u00adrently \ntreats functions as code regions. This optimization reduces the number of times weak-locks are acquired \nand released during a function s execution. The second optimization pertains to the remaining set of \nfalse racy pairs that are part of function pairs that ran concurrently in at least one pro.le run. This \noptimization targets the inaccuracy caused by the conservative points-to analysis [3, 45] on which RE-LAY \nis based. Due to this analysis, RELAY overestimates the set of shared objects that could be accessed \nby a memory instruction and also underestimates the set of locks that could be acquired. We observe that \nwhile the numeric values for the address bounds of an object accessed by a memory instruction are generally \nhard to determine precisely during static analysis, one can often estimate reasonable bounds in the form \nof a symbolic expression [42]. Therefore, in our implementation, we compute symbolic ad\u00address bounds \nof objects that can be accessed by a racing instruction within a loop. Using this information, we increase \nthe granularity of the weak-lock to the entire loop containing the race, such that it protects the loop \nfor the data variables speci.ed by the loop s sym\u00adbolic address bounds. This avoids the cost of instrumentation \nfor every iteration of the loop. Our evaluation shows that Chimera is more ef.cient than the state-of-the-art \nsoftware solutions that guarantee multiprocessor replay [49]. We show that recording a set of server \n(e.g., Apache) and desktop (e.g., pbzip, aget) applications incurs only about 2.4% performance overhead, \nand recording a set of memory-intensive scienti.c applications (SPLASH [53] incurs about 86%. Replay \noverhead is also similar to that of recording. We .nd that our two optimizations play a signi.cant role \nin bringing the average overhead from 53x (when all races are naively instrumented) to 1.39x. Programs \ntransformed by Chimera are data-race-free under the new set of synchronization operations. Though our \nimmediate motive for this transformation is to provide deterministic record and replay, we envision that \nfuture work may be able to leverage the data-race-freedom provided by Chimera to provide stronger guarantees \nsuch as sequential consistency and deterministic execu\u00adtion [37], since these properties are much easier \nto guarantee in the absence of data-races. The primary contributions of this paper are as follows: We \ndiscuss Chimera, a new deterministic replay system for commodity multiprocessors based on a static data-race \ndetector.  We discuss two optimizations that employ pro.ling and sym\u00adbolic bounds analysis to drastically \nreduce the overhead of a naive method that instruments all false data-races.  Our experimental study \nshows that the performance overhead is about 40% on average, which is less than the state-of-the-art \nsoftware solutions for multiprocessor replay.  2. Design Overview This section provides a design overview \nof the Chimera multipro\u00adcessor replay system. 2.1 Background A program is said to be data-race-free \nif none of its executions exhibit a data-race. Two memory instructions are said to be racy if at least \none of them is a write, and there is at least one execution where the two are executed in different threads \nand not ordered by any happens-before relation due to synchronization operations. For clarity, we de.ne \na few terms that we use in this paper. A race\u00adpair is a pair of static memory instructions that are racy. \nThe two functions (or loops) that contains the race-pair are referred to as a racy-function-pair (or \na racy-loop-pair). Chimera records non-deterministic input (e.g., interrupts and .le reads) and happens-before \nrelations due to synchronization ac\u00adcesses in a program. This is suf.cient to later provide deterministic \nreplay for data-race-free programs because all memory instructions are ordered by some happens-before \nrelation [40]. However, it is  rac StaticRace Detector (RELAY)  Figure 1. Chimera Overview insuf.cient \nto later provide deterministic replay for programs that contain data-races.  2.2 Design Overview Figure \n1 presents an overview of how Chimera transforms a po\u00adtentially racy program to a data-race-free program \nby adding ad\u00additional synchronization and runtime constraints. Chimera s trans\u00adformation does not attempt \nto correct a given program, it simply makes it easier to deterministically record and replay the program \ns execution. Chimera analyzes a given program using the RELAY [50] static data-race detector. RELAY is \nsound, except for two corner cases (assembly instructions and pointer arithmetic). However, the un\u00adsoundness \nis modularized and can be addressed using additional analysis [5, 52] (Section 3.2). In the simplest \nimplementation of Chimera, each race-pair is placed inside a code section protected by an unique weak-lock \nw. Recording and replaying the happens-before relation due to weak\u00adlocks enables Chimera to record and \nreplay the order of all racy accesses and thus guarantee deterministic replay for all programs. A sound \nstatic data-race detector is imprecise as it has to make very conservative assumptions. This results \nin a huge number of false data-races, and naively recording all those races results in high overhead. \nThe insight of this paper is that by employing a combination of pro.ling, symbolic address bounds analysis \nand dynamic checks, the overhead is signi.cantly reduced to the point where deterministic record and \nreplay is viable even for production systems. We discuss two speci.c optimizations. The .rst optimization \nis based on our observation that for many false race-pairs, the code regions containing them are almost \nnever executed concurrently. One main cause for this imprecision is the static data-race de\u00adtector s \ninability to account for non-mutex synchronization oper\u00adations. Chimera learns which code regions are \nalmost always non\u00adconcurrent by pro.ling executions with a set of representative in\u00adputs. It uses pro.le \ninformation to increase the granularity of weak\u00adlocks, both in terms of size of the code region and the \namount of shared objects they protect, which reduces the number of weak\u00adlock operations at runtime. Chimera \ns pro.ler treats every function as a code region, though other granularities could be considered. As \nshown in Figure 1, racy-pairs in non-concurrent functions are han\u00addled using weak-locks instrumented \nat the granularity of a function (referred to as function-locks). Not all false data-races are part of \nnon-concurrent code regions. Two code regions can overlap in time, but still may not exhibit a data race \nif they access different sets of shared objects. However, a static data-race detector may not always \nbe able to prove that the set of shared objects accessed in concurrently executed code regions are disjoint \ndue to imprecise pointer analysis. While it is hard to accurately compute the numeric values for address \nbounds statically for a code region, it is often possible to derive a symbolic expression for the upper \nand lower bounds of an object that will be accessed within a code region [42]. For data-races that are \nnot found to be part of non-concurrent functions, Chimera checks if they are part of a loop. If a data-race \nis not part of any loop, then Chimera simply instruments a weak-lock at the granularity of a basic block \n(referred to as a basic block lock). In case the basic block has a function call, Chimera instruments \na weak-lock at the granularity of an instruction (referred to as an instruction lock). If a data-race \nis part of a loop, Chimera derives a symbolic ad\u00address bound for the range of addresses that a racy instruction \ncan access within the loop. A race-pair is then guarded by instrument\u00ading a loop-lock. The loop-lock \nis also a weak-lock, but it protects a range of addresses, which are computed at runtime using the sym\u00adbolic \nexpression derived statically. If the symbolic bounds expres\u00adsion is too imprecise (e.g., one of the \nbounds is in.nity), and if the loop body is reasonably large in size, then Chimera instruments at the \ngranularity of a basic block. In this manner, Chimera avoids the risk of over-serializing the execution \nof loops.  2.3 Weak-Lock Design Chimera ensures that the instrumented weak-locks do not introduce a \ndeadlock. Chimera orders the set of weak-locks constructed for each granularity of a code region (basic \nblock, loop, and function) and ensures that they are always acquired in the same order. When a program \nhas nested code regions (e.g., a function calling a function, a loop calling a function, etc.), an outer \nregion releases all its weak\u00adlocks before starting the inner region, and acquires the weak-locks back \nafter exiting the inner region. The order in which weak-locks of different granularities are acquired \nis also consistent. Function\u00adlocks are always acquired before loop and basic-block locks. Loop\u00adlocks \nare always acquired before basic-block locks. Hence, there cannot be a deadlock between weak-locks. Chimera \navoids deadlocks that may happen when a weak-lock protected code region contains a programmer speci.ed \nsynchro\u00adnization wait. The weak part of the weak-lock is meant for han\u00addling such deadlocks. If a weak-lock \nis stalled for more than a threshold period of time, the stalled weak-lock invokes a special system call \nto handle the potential deadlock. The system call han\u00addler identi.es the thread that currently owns the \nstalled weak-lock by examining the log .les used to record the order of weak-lock acquires and releases. \nThe kernel preempts the current owner, and forces it to release and reacquire the weak-lock that timed-out. \nThis allows the stalled thread to acquire the weak-lock and proceed with its execution. Though the above \nmechanism may compromise the atomicity of a weak-lock protected code region, we always preserve the in\u00advariant \nthat only one thread holds a given weak-lock at any given time. Thus, recording and replaying the exact \norder of forced weak\u00adlock release and reacquire operations with respect to instrumented weak-lock operations \nis suf.cient to guarantee deterministic replay.  This requires that Chimera record and replay the exact \ninstance when a thread is preempted and forced to release its weak-locks. For this purpose, we plan to \nuse a mechanism from the Double-Play replay system [49] in which the kernel records the instruction pointer \nand the branch count (measured via hardware performance counters) at the point of preemption. We have \nnot yet ported this implementation to the Chimera infrastructure as none of our bench\u00admarks have exhibited \na weak-lock timeout.  2.4 Discussion Any data-race that exists in the original program can manifest \nin the transformed program. However, Chimera now records the order between the racing instructions. Increasing \nthe granularity of weak\u00adlock (e.g., to a basic-block) would make it less likely for instruc\u00adtions from \ntwo racy basic-blocks to interleave. If there is only one race between two racy basic-blocks, then all \nthread interleavings in the original program can manifest at approximately the same prob\u00adability in the \ntransformed program. However, if there is more than one race between two basic blocks, then Chimera s \nweak-locks will try to serialize them. While preventing .ne-grained interleaving of smaller code regions \nmay be bene.cial for masking certain atom\u00adicity violations in production systems [31], a programmer trying \nto record and debug a test run might consider this to be a limitation of Chimera s optimizations. 3. \nStatic Data-Race Detection Chimera uses the RELAY [50] static data-race detector to iden\u00adtify potential \ndata-races. In this section, we brie.y summarize the RELAY detection algorithm, and then we discuss soundness \nand completeness of RELAY. 3.1 RELAY RELAY is a lockset-based static race detection tool that scales \nto millions lines of code. A lockset for a program point is the set of locks held at that point. A lockset-based \nanalysis assumes that for every shared object there is at least one common lock that is held whenever \nthat object is accessed. The tool reports a race if a pair of memory accesses in different threads could \naccess the same shared object, the intersection of their locksets is empty, and at least one of the accesses \nis a write. We brie.y summarize RELAY s analysis, but details can be found in the original paper [50]. \nRELAY starts by analyzing every leaf function in the static call graph ignoring the calling context. \nFor each leaf function, it computes a summary. A function s summary soundly approximates the effect of \nthe function on the set of locks held before the function execution. Also, it includes a summary of the \nset of shared objects accessed in the function and the lockset held during each of its accesses. For \nexample, a summary of a function bar(void *b) may say that a write to the .eld b->bob can happen while \nholding a lock b->lock, and that the function releases the lock b->lock before returning. RELAY composes \nfunction summaries in a bottom-up manner over the call graph by plugging in the summaries of the callee \nfunctions to compute the summaries of the callers. Thus, RELAY performs a bottom-up calling-context-sensitive \nanalysis on the call graph to compute the access summaries for all functions that are thread entry points. \nThis is done using a combi\u00adnation of .ow-insensitive points-to [3, 45] and symbolic analysis. 3.2 Soundness \nChimera only instruments data-races found by the static data-race detector. Therefore, its deterministic \nreplay guarantees are based on the soundness of the static data-race detector it uses. RELAY has three \npotential sources of unsoundness, but they are modularized and each one can be addressed separately using \nknown techniques. First, RELAY ignores memory operations that occur inside blocks of assembly code when \ncalculating lockset summaries. However, this issue could be addressed with additional engineering that \nintegrates memory access analysis for assembly instructions [5] with RELAY, or via manual annotations. \nSecond, the points-to analysis [3, 45] used by RELAY does not handle pointer arithmetic. RELAY s pointer \nanalysis assumes that after any arithmetic operation on a pointer, the pointer still points to the same \nobject. When this assumption does not hold true, the pointer analysis is not guaranteed to be sound. \nAs a result, we can guarantee replay for an execution only until the .rst buffer over.ow. However, this \ndoes not fundamentally affect Chimera s analysis. Enhancing pointer analysis to handle pointer arithmetic \n[52], or ensuring language safety would address this problem. Finally, RELAY post-processes data-race \nwarnings using un\u00adsound .lters, but we do not use them.  3.3 False Positives To provide soundness, RELAY \nmakes conservative assumptions, resulting in its reporting a high number of false data-races. In\u00adstrumenting \nweak-locks for every false data-race results in pro\u00adhibitively high overhead. There are two main sources \nof false positives. First, RELAY accounts for lock synchronizations, but ignores happens-before relationships \ndue to non-mutex synchronization operations such as fork/join, barriers, and conditional variables. As \na result, RELAY may report a data-race between memory operations that can never execute concurrently. \nThe second main source of false positives is due to the conservative pointer analysis it uses [3, 45]. \nConservative pointer analysis would cause RELAY to underestimate the lockset held by a code region and \noverestimate the variables that could be accessed by a memory instruction. Our experimental results in \nSection 7 show that RELAY re\u00adported data-race warnings on about 14% of memory operations in a dynamic \nexecution. Instrumenting them with weak-locks to record the order of those potential data-races incurs \nan approximately 53x slowdown. We next discuss two important optimizations based on pro.ling (Section \n4) and symbolic bounds analysis (Section 5) that signi.cantly reduce this cost. 4. Pro.ling Non-Concurrent \nFunctions A static data-race detector may report races between code regions that are never executed concurrently. \nOne reason for this is the inad\u00adequacy of the static analysis in accounting for non-mutex synchro\u00adnization \noperations. To address this issue, Chimera uses a pro.le\u00adguided analysis to determine code regions that \nare likely to never execute concurrently and use that information to increase the gran\u00adularity of weak-locks \nwithout compromising an application s par\u00adallelism. 4.1 Overview One important limitation of lockset \nbased static data-race detec\u00adtors, including RELAY, is that they account only for locks, but ig\u00adnore \nhappens-before relations due to non-mutex synchronization operations. Many false data-races may be reported \ndue to this lim\u00aditation. Figure 2(a) illustrates a false data-race reported for water. The data-race \nis false because the two supposedly racy functions are never executed concurrently due to a barrier synchronization. \nWe also .nd that a number of false data-races are reported between initialization code and the rest of \nthe code regions, as RELAY does not account for fork-join synchronization. Another source of false data-races, \nunrelated to non-mutex synchronizations, is the lack of static knowledge of control dependencies. For \nexample, we found  (a) T1 T2 interf() interf() bndry() bndry()  (b)voidinterf(){ voidinterf(){ voidbndry(){ \nvoidbndry(){ . W EAK-LOCK(f1) . W EAK-LOCK(f1) . . W EAK-LOCK(b1)  W EAK-LOCK(b1) p=FORCE p=FORCE \np=BOX p=BOX W EAK-UNLOCK(b1) . W EAK-UNLOCK(b1) . . . W EAK-UNLOCK(f1) W EAK-UNLOCK(f1) } }}} Figure \n2. (a) A false data-race reported for water application from the SPLASH benchmark [53]. Functions bndry() \nand interf() are never executed concurrently due to the barrier synchronization, which is not accounted \nfor in RELAY. (b) The granularity of weak-locks is increased to function level in the two potentially \nracy functions because Chimera s pro.ler .nds them to be non-concurrent. instances where a set of code \nregions are executed in only one thread, but RELAY reported false races among them. In all these cases, \nthe two code regions containing the race-pair reported by RELAY are never executed concurrently. We observe \nthat such cases can be determined by pro.ling with a set of representative inputs. If a pair of potentially \nracy code regions are never executed concurrently in any of the pro.le runs, then there is suf.cient \ncon.dence that they are likely to be non\u00adconcurrent in another execution. Pro.ling cannot guarantee that \nthey will be non-concurrent in all executions. Nevertheless, we can take advantage of pro.led information \nto increase the granularity of weak-locks to larger code regions and reduce the dynamic number of weak-lock \noperations. If a pair of code regions containing a potential race-pair is likely to be non-concurrent, \nthen Chimera increases the granularity of the weak-lock to protect the entire code region instead of \njust the basic blocks containing the race-pair. Figure 2(b) shows how this optimization affects the weak-lock \ninstrumented to handle the false data-race that we discussed for water (Figure 2(a)). In this study, \nwe consider functions as code regions while performing non\u00adconcurrent region pro.ling, but our method \ncould be applied for other region granularities as well. We refer to a weak-lock that protects a function \nas a function-lock. By increasing the granularity of the weak-lock to the function\u00adlevel, Chimera reduces \nthe dynamic number of operations on that lock. Increasing the granularity in terms of the code region \nsize for a weak-lock also creates the opportunity to use a single weak-lock to guard multiple potential \ndata-races. The next section discusses an optimization that exploits this opportunity.  4.2 Clique analysis \nWe propose a clique analysis to determine which racy function\u00adpairs can share the same function-lock. \nSharing a function-lock reduces the cost of instrumentation. Figure 3(a) shows a graph with a node for \nevery function that contains at least one potential data-race. A dotted edge connects a pair of functions \nthat could potentially race. A solid edge connects a pair of functions that are found to be non-concurrent \nin all of the pro.le runs. For example, alice is potentially racy and non\u00adconcurrent with bob and carol. \nFunctions bob and carol are non-concurrent, but are proven to be race-free with each other. Functions \nbob and dave are racy and have also been found to be concurrent in some pro.le run. One simple algorithm \nwould be to assign a unique weak-lock for every racy-function-pair. If the race-function pair is also \nnon\u00adconcurrent, then we can use a function-level lock as shown in Figure 3(a). Note that bob and dave \ncould run concurrently, and so we do not use function-level weak-locks to guard potential races between \nthem, as that could serialize those the two concurrent functions and compromise on parallelism. Instead, \na weak-lock is instrumented at the basic-block granularity. The above algorithm requires that alice acquires \nand releases two function-level weak-locks (f1 and f2) every time it is ex\u00adecuted. However, alice, bob, \nand carol are potentially non\u00adconcurrent with each other. Therefore, the two potential races could be \nguarded using a single function-lock f0 as shown in Figure 3(b). This optimization would reduce the number \nof weak-lock opera\u00adtions. To identify a group of functions which are mutually non\u00adconcurrent, we construct \nmaximal cliques using a greedy algo\u00adrithm in a graph of potentially non-concurrent functions (deter\u00admined \nthrough pro.ling). A clique of an undirected graph is a subset of nodes where every node is connected \nto every other node. A maximal clique is a clique that cannot be extended by including one more adjacent \nnode. Figure 3(c) shows a graph of potentially non-concurrent functions with two cliques, {alice,bob,carol}and \n{carol,dave}. Once cliques are identi.ed in a graph of non-concurrent func\u00adtions, Chimera assigns function-locks \nas follows. For each race\u00adpair, it checks if its racy functions are non-concurrent. If they are, then \nit .nds the clique that the racy-function-pair is part of in the graph of non-concurrent functions. Chimera \nassigns the function-lock corresponding to that clique to both racy functions. For example in Figure \n3(b), racy-function pairs {alice,carol}and {alice,bob} are both assigned a single function-lock f0. No\u00adtice \nthat this weak-lock assignment is ef.cient for alice as it now has to acquire only one weak-lock as opposed \nto two. However, bob and carol are unnecessarily serialized (as they do not race with each other), which \nis still acceptable as they are also found to be non-concurrent during pro.ling. It is possible that \na racy-function-pair is part of two cliques. In that case, we use a greedy algorithm that chooses the \nweak-lock corresponding to the clique that contains the most number of racy\u00adfunction-pairs. 5. Symbolic \nBounds Analysis for Loops Chimera s second optimization targets race-pairs that remain af\u00adter applying \nthe pro.le-based analysis described in the previous section. This optimization is based on symbolic address \nbounds analysis. It addresses the imprecision of the conservative but sound pointer analysis used in \na static data-race detector. 5.1 Overview Static data-race detectors [26, 50] use pointer analysis to \ndetermine the set of objects a memory instruction can access and also to de\u00adtermine the lockset at a \nprogram point. RELAY uses a combination of Steensgaard [45] and Andersen [3] .ow-insensitive and context\u00adinsensitive \npointer analysis, which are used in many static tools because they scale well to large programs. However, \nbecause these analyses are very conservative, RELAY overestimates the range of addresses that a memory \ninstruction can access and underestimates the set of locks held at a program point, both of which cause \nit to report a number of false data races.  Function Race Non-concur ent f Function-lock b BB-lock \nClique Clique0 Clique (b)  (c) (a) Clique1  Figure 3. (a) One weak-lock is instrumented for each \nrace-pair. If a racy function-pair is non-concurrent, a function-level weak-lock (f1, f2) is used. Otherwise, \na basic-block level weak-lock is used (b0). (b) Two potential data-races in a clique in a graph of non-concurrent \nfunction share one function-lock (f0). (c) Cliques in a graph representing non-concurrent functions. \n1: for(i=0;i<max_digits;i++){ 2: WEAK-LOCK(&#38;rank[0]to&#38;rank[radix-1]); 3: for(j=0;j<radix;j++){ \n4: rank[j]=0; 5: } 6: WEAK-UNLOCK(&#38;rank[0]to&#38;rank[radix-1]); 7: 8: WEAK-LOCK(-INFto+INF); 9: \nfor(j=start;j<stop;j++){ 10: my_key=key_from[j]&#38;bb; 11: rank[my_key]++; 12: } 13: WEAK-UNLOCK(-INFto+INF); \n14: } Figure 4. Instrumenting weak-locks for a loop in the function slave sort in radix using symbolic \nbounds. For example, we .nd a number of false data-races between two functions executed concurrently \nin different threads. This often happens when a programmer partitions work between threads, but the static \nanalysis is unable to determine that the function will ac\u00adcess different parts of a data structure. Figure \n4 shows an example. RELAY reports a false data-race on the rank array in line 4 and 11, and also on key \nfrom array in line 10. However, radix divides a large array into multiple portions and assigns different \nportions to concurrent threads to process them in parallel. Therefore, the base address of rank and key \nfrom are different for each worker thread, and hence the threads do not access the same entry in those \narrays concurrently. It is hard to statically determine the absolute values of address bounds of an object \naccessed by memory operations in a code re\u00adgion. However, it has been shown that the lower and upper \nbounds in the form of a symbolic expression can often be derived stati\u00adcally [41, 42] with much better \naccuracy. Chimera uses this in\u00adformation to increase the granularity of weak-locks that it must instrument \nfor race-pairs in concurrent code regions. The weak\u00adlock is constructed in such a way that it protects \na code region for a range of addresses speci.ed by a symbolic expression. Thus, two potentially racy \ncode regions can execute concurrently (pro\u00advided our symbolic bounds are accurate enough). At the same \ntime, Chimera can protect the regions with weak-locks instrumented at larger granularities to reduce \nthe number of operations. Figure 4 shows an example. RELAY reports that line 4 could race with itself. \nInstrumenting a weak-lock inside the loop would be very expensive. Instead, Chimera instruments a weak-lock \nthat provides mutual exclusion for the entire loop (lines 3-5) only for the address range from &#38;rank[0] \nto &#38;rank[radix-1]. This range is computed by a sound static symbolic address bounds analysis, which \nwe discuss in the next section.   5.2 Symbolic Bounds Analysis We implemented our symbolic bounds \nanalysis based on the algo\u00adrithm proposed by Rugina and Rinard [41, 42]. The goal of this analysis is \nto determine the symbolic expressions that specify the upper and lower bounds for a pointer or array \nindex variable at a program point that is found to be potentially racy by the static data race detector. \nFor the example in Figure 4, the analysis determines that the symbolic lower bound of j of the .rst inner \nloop (line 4-7) is 0 and the upper bound is the initial value of radix radix0 - 1. It also .nds that \nline 4 can access a memory region from &#38;rank[0] to &#38;rank[radix-1]. Details about the algorithm \nare discussed by Rugina and Rinard [42]. The effectiveness of our optimization depends on the accuracy \nof the lower and upper bounds. The analysis we use is sound, but imprecise. If the bounds are too conservative, \nwe may serialize concurrent code regions unnecessarily. There are two main sources of imprecision. The \n.rst case is when the address of the racy object is based on the value of a variable that cannot be determined \noutside the code region. For example, precise symbolic bounds for the rank array accesses in the second \ninner loop (line 9-12) cannot be determined. The value of the index variable my key cannot be computed \noutside the loop as it depends on the value read from another array key from inside the loop (line 11). \nHowever, we can derive the symbolic bounds for the array key from accurately. The second source of inaccuracy \nis when the racy object s bounds depends on an arithmetic operation (e.g., the modulo operation or logical \nAND/OR) not supported in the analysis.  5.3 Choosing the Granularity for Code Region Rugina and Rinard \ns analysis [41, 42] describes a generic algorithm for larger code regions including inter-procedural \nanalysis, but, as a .rst step, we applied their technique only for loops with no function calls in the \nloop body. As a result, our current implementation may not exploit all opportunities for optimization. \nIf the symbolic bounds are too imprecise, care must be taken to ensure that we do not over-serialize \nloops. If the derived symbolic expression for an address range is from negative in.nity to positive in.nity, \nwe consider it to be too imprecise to be useful. Otherwise, we consider it to be precise enough. In that \ncase, we balance the number of weak-lock operations with the loop serialization cost. If the symbolic \nbounds of a racy loop is precise enough, we assign a weak-lock at the loop granularity (the .rst inner \nloop in Figure 4). If the bounds are too imprecise, we estimate via pro.ling the average number of instructions \nexecuted by a loop iteration. If the estimate is less than a loop-body-threshold, we still instrument \nat the loop granularity because the cost of operations on the weak-lock does not warrant exposing parallelism \nin the loop. Otherwise, we instrument a basic-block lock inside the loop body.   If a loop is nested, \nwe select the outermost loop with precise enough bounds. 6. Implementation This section presents the \nimplementation details of the Chimera record and replay system. 6.1 Analysis, Instrumentation, and Runtime \nSystem Our analysis and instrumentation framework is implemented in OCaml, using CIL [35] as a front \nend. To pro.le concurrent func\u00adtion pairs (Section 4), we instrumented the entry and exit of each function \nusing CIL s source-to-source translation. To statically de\u00adrive symbolic bounds of racy loops (Section \n5), we also performed data .ow analysis on a racy loop and produced linear programming constraints using \nCIL. Then, we used lpsolve [1], a mixed inte\u00adger linear programming solver, to .nd a solution for static \nbounds that a racy loop may access. Finally, based on the results of the above static analysis, we used \nCIL to instrument weak-locks at the function, loop, basic block, or instruction granularity. We modi.ed \nthe Linux kernel to record and replay non\u00addeterministic input from system calls and signals. We also \nmod\u00adi.ed GNU pthread library version 2.5.1 to log the happens-before order of the original synchronization \noperations and the weak-locks added by Chimera.  6.2 Static Analysis and Source code We used RELAY [50] \nto perform pointer analysis and to collect a set of potential data-races. We applied Andersen s inclusion-based \npointer analysis [3] to resolve function pointers, and Steensgaard s uni.cation-based approach [45] to \nperform alias analysis between lvalues. While performing pointer analysis, RELAY .rst translates function \nlocal arrays and address-taken variables to heap variables (making them global) in order to derive pointer \nconstraints in a uni.ed manner. RELAY performs static analysis on this modi.ed source code. This can \nlead to unnecessary false data-races on lo\u00adcal variables. To resolve this, we .ltered out race warnings \non a heapi.ed local variable that did not escape its function. To perform sound static analysis, we made \nsure that all library source code (except for apache and pbzip2) are included in our static analysis. \nFor the standard C library, we used uClibc [48] which is smaller and easier to analyze than the GNU glibc \nlibrary, as it is developed for embedded Linux systems. The uClibc library involves all the necessary \nfunctions such as libc and libm. For apache, we did not include libraries such as gdbm, sqlite3, etc., \nbecause they do not contain code that gets executed for the in\u00adput we use in our study. It is possible \nthat the source code of a third party library may not be available for static analysis. When any part \nof the source code of a library used by a program is not analyzed, the soundness of static analysis may \nbe compromised. One solution is to ask library builders to provide annotation (lockset summaries) for \ntheir library functions so that it can be fed into RELAY to perform a sound data-race analysis. Developing \nsuch annotations would be an one-time cost for library builders, and it would not place any burden on \nsoftware developers that use those libraries. Another possible solution is to assume that a library function \nwill only access the set of objects pointed to by the parameters passed as function arguments without \nacquring any new locks. However, this approach is not guaranteed to be sound, because a library could \nretain pointers passed to previous calls to the same library. Also, instructions in a library s function \ncan have a data-race on some shared-variable that is internal to the library. We employed the lat\u00adter \napproach for pbzip2 (we excluded the libbz2 library used by pbzip2) We also converted the C++ pbzip2 \nprogram into ANSI-C code by replacing the vector STL container with a linked-list-based C library, because \nour instrumentation framework, CIL [35], can only handle C programs, but not C++ constructs. 7. Results \nThis section evaluates Chimera s recording and replaying overhead and demonstrates the effectiveness \nof the pro.ling and symbolic bounds optimizations. 7.1 Methodology We evaluated our system using three \nsets of benchmarks which are listed in Table 1. The .rst set consists of three desktop applica\u00adtions: \naget, pfscan, and pbzip2. The second set has two web sever programs: knot and apache, which are evaluated \nusing the ApacheBench (ab) client. The .nal set contains four scienti.c pro\u00adgrams from SPLASH-2 [53]: \nocean, water, fft, and radix. To collect a set of concurrent function pairs for clique analysis (Sec\u00adtion \n4.2), we pro.led each program 20 times with various inputs. The inputs used for pro.ling are signi.cantly \ndifferent from the input used for our performance evaluation. Chimera is scalable to large programs. \nIt is built on RE-LAY [50], which has been shown to scale to very large programs (e.g., Linux with 4.5 \nmillion lines of code). Chimera also uses static analysis to derive symbolic bounds, but it is a scalable \nintra\u00adprocedural analysis. Our benchmark set includes some fairly large programs. Table 1 provides the \nnumber of lines (LOC) of our benchmarks in their CIL representation. It does not account for the size \nof library code: libc(41.7K) and libm(3.6K). Presence of assembly code and buffer over.ow may compro\u00admise \nthe soundness of static data-race analysis (Section 3.2). How\u00adever, the programs we evaluated do not \ncontain assembly code, except for a few library functions. Also, we are not aware of any buffer over.ow \nbugs in our benchmarks. Also, we did not observe any weak-lock timeouts (Section 2.3) in any of our experiments. \nWe ran our experiments on a 2.66 GHz 8-core Xeon processor with 4 GB of RAM running CentOS Linux version \n5.3. We mod\u00adi.ed Linux 2.6.26 kernel and GNU pthread library version 2.5.1 to support Chimera s record \nand replay features. All results are the mean of .ve trials with 4 worker threads (excluding main or \ncontrol threads). Section 7.2 presents scalability results for which we used 2, 4, and 8 threads.  7.2 \nRecord and replay performance Table 2 shows Chimera s record and replay performance when all the optimizations \n(function, loop, and basic-block level weak\u00adlock optimizations) are enabled. The .rst set of columns \nquanti.es the number of logs generated for recording program input (read through systems calls) and the \nhappens-before order of synchro\u00adnization operations. These logs are suf.cient to guarantee replay for \ndata-race-free (DRF) programs. The second set of columns quan\u00adti.es the number of logs due to various \ntypes of weak-locks. The next set of columns presents the performance overhead. The last set of columns \nquanti.es the gzip compressed log sizes for recording the program input and the order of all synchronization \noperations (including weak-locks). Chimera incurs negligible overhead for desktop and server ap\u00adplications. \nFor scienti.c applications (with high frequency of ac\u00adcesses to shared variables) the overhead is relatively \nhigh. On av\u00aderage, our system incurs 40% performance overhead to record an execution with four worker \nthreads. Replay overhead is similar to that of recording overhead for most applications, except for I/O \nin\u00adtensive applications. Network intensive applications such as aget, knot, and apache replay much faster \nas we feed the recorded input directly to the replayed process without waiting for the network re\u00adsponse. \nChimera s performance overhead is an order of magnitude Table 1. Benchmarks and input used for pro.ling \nand evaluating Chimera. The number of lines in the source program (LOC) is measured for the CIL representation. \nIt does not include the size of library code: libc(41.7K) and libm(3.6K).  application LOC desktop aget \n1.2K pfscan 2.1K pbzip2 4.8K server knot 1.3K apache 99K scienti.c ocean 5.3K water 2.5K fft 1.4K radix \n1.3K pro.le environment evaluation environment 2 workers, download a 29KB .le from local network 2,4,8 \nworkers, download a 10MB .le from http://ftp.gnu.org 2 workers, scan 236 KB of small 22 .les 2,4,8 workers, \nscan 952 MB of 8 log .les 2 workers, compress a 219 KB .le, output to stdout 2,4,8 workers, compress \n16 MB .le, output to .le 2 workers, 4 clients, 100 requests, 29KB .le 2,4,8 workers, 16 clients, 1000 \nrequests, 390KB .le 2 workers, 4 clients, 100 requests, 29KB .le 2,4,8 workers, 16 clients, 1000 requests, \n390KB .le 2 workers, 130*130 grid, 1e-01 error tolerance 2,4,8 workers, 1026*1026 grid, 1e-07 error tolerance \n2 workers, 64 molecules, 5 steps 2,4,8 workers, 1000 molecules, 10 steps 2 workers, 24 matrix , no inverse \nFFT check 2,4,8 workers, 220 matrix, with inverse FFT check 2 workers, 28 keys , no sanity check 2,4,8 \nworkers, 214 keys, with sanity check application desktop aget pfscan pbzip2 server knot apache scienti.c \nocean water fft radix system calls DRF Logs 16604 8424 109 879 592 2491 8056 32 18301 36812 2750 9978 \n10295 67202 113 193 102 312 synch. ops. instr. log 28876 8 2621 5136 798891 6237 21838 1843 3 basic blk. \nlog 5191 0 81 0 266956 8233 1409884 38 13 loop log logging order of potential data-races 15939 32416 \n39 347 1177 1540 251 2257 func. log 565863 1123337 287642 37655 198993 1112798 49718 11595 344 393 original \ntime(ms) 5058 848 1343 7137 18668 2328 1665 586 1599 record time(ms) 5114 881 1371 7176 19376 5585 2820 \n1249 1939 recording overhead performance 1.01 1.04 1.02 1.01 1.04 2.40 1.69 2.13 1.21 replay overhead \n0.06 1.02 1.03 0.01 0.02 2.24 1.75 2.23 1.20 input log(KB) 20072 2 1989 84 178 16 101 2 1 order log (KB) \nlog size 361 3 26 23 6469 727 12744 107 3 Table 2. Chimera record and replay performance. The results \nare the mean of .ve trials with 4 worker threads. improvement over the state-of-the art software solutions \nthat guar\u00adantee multiprocessor replay [49]. Log sizes of Chimera are within acceptable limits for various \nuses of replay. aget produces large logs because the contents of all the downloaded .les are in the log. \nwater also produces a large log size because of frequent user speci.ed synchronizations and weak-locks. \n 7.3 Effectiveness of Optimizations We analyzed the effect of different optimizations on recorder s \noverhead. Fine grained weak-locks (instruction and basic-block level weak-locks) enable higher concurrency, \nbut they increase the number of program points instrumented resulting in higher performance and log size \noverhead. The opposite is true for coarser grained weak-locks such as function and loop level weak-locks. \nWe use function-level weak-locks if two functions are likely to be non\u00adconcurrent (Section 4). We use \nloop-level weak-locks with runtime bounds checks if our static analysis can derive precise enough symbolic \nbounds (Section 5). Figure 5 shows the performance overhead of Chimera s recorder with different sets \nof optimizations normalized to native execu\u00adtion time. As expected, instrumenting every potential data-race \nat the granularity of a source line (labeled as instr ) incurs 53x slowdown. However, when we apply the \npro.le-based optimiza\u00adtion to increase the granularity of some weak-locks to function level ( inst+func \n) the overhead drops to 27x. If we use only sym\u00adbolic analysis to coarsen the granularity of some weak-locks \nto loop level results in 33x overhead. However, when we employ all the op\u00adtimizations together ( inst+bb+loop+func \n), including basic block level weak-locks, the average overhead drops signi.cantly to 1.39x. Applications \nsuch as pfscan and water bene.t signi.cantly from function-level locks. In these applications, most data-races \nare in function-pairs ordered by some non-mutex synchronization operations that our static analysis could \nnot account for. For ap\u00adplications such as apache, ocean, fft, and radix, loop-level locks reduce the \nrecording overhead drastically. For example, in apache, RELAY reports a false data-race between memory \nopera\u00adtions within a hot loop in the memset library function that iterates approximately over 6 million \ntimes in our experiments. Function\u00adlevel weak-lock is ineffective in this case, because two threads may \nexecute the memset concurrently. However, our static analysis de\u00adtermines the bounds of addresses accessed \nwithin the hot loops of memset fairly accurately, which enabled us to use loop-level weak\u00adlocks effectively. \nWe also observe noticeable bene.ts in coarsening weak-locks from instruction-level to basic-blocks (e.g., \nwater). Finally, for network applications like aget, knot, and apache, recording cost overlaps with I/O \nwait resulting in negligible over\u00adhead. Chimera could be used even in production systems for such applications. \nFigure 6 shows the proportion of dynamic number of weak-lock operations with respect to the total number \nof dynamic memory op\u00aderations. A naive dynamic data-race detector would have to instru\u00adment 100% of memory \noperations. This result shows the advantage of static data-race analysis and our optimizations in terms \nof reduc\u00ading the number of instrumented points in the program. In general, results in Figure 6 for different \noptimizations are consistent with the recording overhead in Figure 5. This indicates that the savings \nobtained from coarser weak-locks was not overshadowed by any loss in parallelism. On average, naively \nmonitoring all data-races reported by the static data-race detector requires us to instrument about 14% \nof all dynamic memory operations. By increasing the weak-lock gran\u00adularity to function, loop, and basic \nblock levels, we can reduce the proportion of weak-lock operations with respect to memory ac\u00adcesses down \nto 0.02% on average. In general, this result shows that  ' ob . . .EQ z0\" \" <. 1 1 1  Figure 5. Normalized \nrecording overhead for Chimera with different sets of optimizations t t r rff  t r r  t r r r rff \n t r r r r r rff ___ 5 0 5 0 5 0  Figure 6. Proportion of instrumentation points for different logging \nschemes increasing the granularity of weak-locks reduces the instrumenta\u00adtion cost. However, in some \nfairly rare cases, increasing the locking gran\u00adularity from instruction to loop-level may increase the \nfrequency of weak-lock operations. The reason is due to control .ow depen\u00addencies. In pfscan, there is \na racy instruction inside a hot loop that is guarded by an if statement. If we use loop-level weak\u00adlock, \nChimera has to always perform weak-lock operations when the loop is executed. But if we use instruction-level \nweak-lock, the instrumented code will be executed only when the if condition gets satis.ed. In apache \nthe number of weak-lock operations increase when we go from instruction-level to function-level granularity. \nThe rea\u00adson for this is behavior is best explained using a contrived example. Assume that RELAY .nds \na data-race between each of the func\u00adtions foo, bar, and qux. Also, assume that all these functions are \nnon-concurrent with each other, except for the function-pair bar and qux. For this example, Chimera will \nassign two different func\u00adtion level weak-locks (one for foo-bar and another for foo-qux). This allows \nbar and qux to run concurrently. As a result, foo is instrumented with two function-level locks, which \nmay be more costlier than using one instruction-level lock if there is only one racy instruction inside \nfoo. We also studied the sensitivity of our pro.le-based non-concurrent function analysis to the number \nof pro.le runs. We did this study only for pfscan and water-nsq, because other applications shows little \nperformance bene.t from function-level logging (Figure 5). For these two applications, the number of \nconcurrent function pairs observed quickly saturates after a small number of pro.le runs (.ve for pfscan \nand three for water-nsq). 7.4 Sources of Overhead and Scalability Figure 7 provides a breakdown of the \nremaining sources of perfor\u00admance overhead in the Chimera recorder that incorporates all of our optimizations \n( inst+bb+loop+func ). The results are normalized to the native execution time. We measure the performance \nof our sys\u00adtem by instrumenting each type of weak-lock one by one. The per\u00adformance overhead due to a \nweak-lock type is further broken down into the cost of logging the weak-lock operations and the cost \ndue to weak-lock contention. To measure the time lost due to weak\u00adlock contention, we subtracted the \nexecution time of a program ex\u00adecution in which a weak-lock acquire operation always succeeds without \nwaiting from the execution time of a program execution in which the weak-locks semantics are obeyed. \nContention for loop-level weak-locks dominate the overhead for scienti.c applications such as ocean and \nfft. The reason is that our static symbolic bounds analysis is not very precise for some performance \ncritical loops in these programs because they tend to execute irregular array accesses and unmodeled \narithmetic operations (Section 5.2). As a result, the bounds checks performed as part of loop-lock acquire \noperation over-serializes the execution. We also fail to use loop-level lock and resort to instruction-level \nlogging (e.g., water), if the loop body contains a function call, because our symbolic analysis in intra-procedural. \nContention between loop-level weak-locks is the for increase in performance overhead as the number of \nthreads increases for some applications (Figure 8). We believe that source-level inlining for small functions \nor inter-procedural symbolic bounds analysis could help reduce this overhead. Nevertheless, as we discussed \near\u00adlier (Section 7.3), our current analysis already provides signi.cant bene.ts with loop-level locks \nfor many applications (e.g., ocean).  Z 8E.\" ! - 88- . . 8. .- 8 E.\" . - 8 - \" 8; . .\u00ad yy  y te \n g y t   g  y t  a t g a t f y g f y a t 2 . 1 .  2 1  Figure 7. Sources of recording \noverhead    3 . 3 2 . 2 1 . 1 0 . 0   Figure 8. Scalability results on 2,4,and 8 processor executions \n8. Related Work Chimera is related to three distinct areas of prior work: determinis\u00adtic record-and-replay, \ndeterministic execution, and hybrid data-race detection. Deterministic Replay. Support for checkpointing \nand logging non-deterministic input (interrupts, DMA, I/O, etc.) is suf.cient to guarantee deterministic \nreplay for single-threaded programs [44]. Deterministic replay for multithreaded programs running on \na uniprocessor also can be provided at low cost by recording thread schedules [16]. However, multiprocessor \nreplay remains an open challenge due to dif.culties in detecting and logging shared mem\u00adory dependencies. \nThe earliest work in this area either incurred prohibitive performance overhead due to the cost of monitoring \nmemory operations [8, 17, 34], supported deterministic record and replay only for data-race-free programs \n[40], or relied on custom hardware support [33, 54]. State-of-the-art software solutions mainly take \ntwo approaches: search or redundancy. PRES [38] and ODR [2] are of.ine search based replay systems. Instead \nof detecting and recording shared memory dependencies at runtime, they perform of.ine search to re\u00adconstruct \nthread interleavings. This class of systems show notable performance improvement during recording, but \nthe of.ine search is not guaranteed to succeed in a bounded amount of time (failing to .nd a solution \nin some experiments). In some cases, the .rst deter\u00administic replay may take prohibitively long time \nwhen the search does not scale. However, subsequent replays have low overhead, because a solution has \nbeen previously found. Alternatively, Re\u00adspec [29] and DoublePlay [49] use redundant execution to detect \ndata-races during recording that could compromise deterministic replay. While the record and replay latency \noverhead is low when spare cores are available to run an additional replica, these systems impose a minimum \nof a 2x CPU throughput overhead to run an additional replica. LEAP [25] uses static escape analysis to \nprovide ef.cient mul\u00adtiprocessor replay. LEAP improves the ef.ciency of a recorder by instrumenting accesses \nto only shared variables that are determined using a static escape analysis. LEAP also ignores accesses \nto vari\u00adables that are immutable after initialization to improve ef.ciency. Monitoring and logging accesses \nto all mutable shared variables determined using a conservative static analysis can be quite expen\u00adsive \nat runtime. LEAP can slowdown a program by more than 2x in the average case and 6x in the worst case \n[25]. In contrast to these prior systems, Chimera uses a sound static data race analysis and a series \nof optimizations to build the most ef.cient software replay solution for commodity multiprocessors to \ndate. Deterministic Execution. Deterministic execution systems help programmers reproduce a multiprocessor \nexecution by en\u00adsuring that the thread interleaving observed is always the same for a given program and \nan input [14]. This approach obviates the need for recording the order of shared-memory accesses, but \nmust still record any non-deterministic program input to provide deter\u00administic replay. While deterministic \nexecution can be supported fairly ef.ciently for programs without data-races [9, 37], current software-only \nsolutions for racy programs incur high overhead [6]. Ef.ciency can be improved by using custom hardware \n[14, 15, 23], by restricting the class of programs supported to fork-join par\u00adallelism [7] or shared-nothing \naddress spaces [4], or by allowing executions that are not consistent with the processor s memory con\u00adsistency \nmodel [30]. Chimera transforms a program into an equiv\u00adalent data-race-free program under the new set \nof synchronization operations. Future work can leverage this property to design an ef.cient software \nonly solution for deterministic execution.  Data-Race Detection. There is a large body of work that \nuses locksets to perform static data-race detection for C/C++ pro\u00adgrams [19, 26, 39, 46]. Type systems \nhave been used to improve static data-race analysis [10, 21, 22]. We used lockset based RE-LAY [50] to \nbuild Chimera, but future advancements in this area could help us further reduce the overhead of our \nreplay system. Perhaps the most closely related study is the work on hybrid data-race detectors that \nused static analysis to eliminate runtime checks for memory operations that are proven to be data-race\u00adfree \n[12, 18]. Unlike Chimera, they check all suspected racy ac\u00adcesses at the instruction granularity, which \nwe show could lead to a high runtime overhead. To reduce this overhead, Choi et al. [12] discuss unsound \noptimizations that may not .nd more than one data-race per memory location. Such weaker guarantees may \nbe acceptable for detecting concurrency bugs, but are not suf.cient to guarantee deterministic replay. \n9. Conclusion Non-determinism has been one of the thorny issues in shared\u00admemory multithreaded programming. \nAn ef.cient deterministic re\u00adplay system can help solve this problem by empowering program\u00admers with \nthe ability to reproduce and understand a program s exe\u00adcution. This is critical in many stages of the \nsoftware development process, including debugging, testing, reproducing problems from the .eld, and forensic \nanalysis. Chimera is the .rst software system for multiprocessors that leverages a static data-race detector \ntool to provide a low overhead replay solution. However, an ef.cient solution would not have been possible \nwithout the two critical optimizations that we employed to drastically reduce the overhead of recording \nall the data-races reported by a conservative, but sound static analysis tool. Chimera s transformations \nensure that the resultant code is data\u00adrace-free when instrumented with the new set of synchronization \noperations. We believe that this technique could also prove quite useful for enabling stronger semantics \nfor concurrent languages such as sequential consistency and for enabling deterministic ex\u00adecution. Acknowledgments \nThis work was funded in part by NSF with grants CCF-0916770, CNS-0905149, a Microsoft gift and an equipment \ngrant from Intel. References [1] lpsolve, mixed integer linear programming solver. http://lpsolve.sourceforge.net/5.5. \n[2] G. Altekar and I. Stoica. ODR: Output-deterministic replay for mul\u00adticore debugging. In Proceedings \nof the 22nd ACM Symposium on Operating Systems Principles, pages 193 206, October 2009. [3] L. O. Andersen. \nProgram analysis and specialization for the c pro\u00adgramming language. In PhD thesis, DIKU, University \nof Copenhagen, 1994. [4] A. Aviram, S.-C. Weng, S. Hu, and B. Ford. Ef.cient system-enforced deterministic \nparallelism. In Proceedings of the 9th Symposium on Operating Systems Design and Implementation, Vancouver, \nBC, 2010. [5] G. Balakrishnan and T. Reps. Analyzing memory accesses in x86 executables. In In CC, pages \n5 23. Springer-Verlag, 2004. [6] T. Bergan, O. Anderson, J. Devietti, L. Ceze, and D. Grossman. Core\u00addet: \na compiler and runtime system for deterministic multithreaded execution. In Proceedings of the 15th International \nConference on Ar\u00adchitectural Support for Programming Languages and Operating Sys\u00adtems, pages 53 64, Pittsburgh, \nPA, 2010. [7] E. D. Berger, T. Yang, T. Liu, and G. Novark. Grace: Safe multi\u00adthreaded programming for \nC/C++. In Proceedings of the International Conference on Object Oriented Programming Systems, Languages, \nand Applications, pages 81 96, Orlando, FL, October 2009. [8] S. Bhansali, W. Chen, S. de Jong, A. Edwards, \nand M. Drinic. Frame\u00adwork for instruction-level tracing and analysis of programs. In Second International \nConference on Virtual Execution Environments, pages 154 163, June 2006. [9] R. L. Bocchino, Jr., V. S. \nAdve, D. Dig, S. V. Adve, S. Heumann, R. Komuravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian. \nA type and effect system for deterministic parallel Java. In Proceedings of the International Conference \non Object Oriented Programming Systems, Languages, and Applications, pages 97 116, Orlando, FL, October \n2009. [10] C. Boyapati and M. Rinard. A parameterized type system for race\u00adfree java programs. In Proceedings \nof the 16th ACM SIGPLAN con\u00adference on Object-oriented programming, systems, languages, and ap\u00adplications, \nOOPSLA 01, pages 56 69, New York, NY, USA, 2001. [11] T. C. Bressoud and F. B. Schneider. Hypervisor-based \nfault toler\u00adance. ACM Transactions on Computer Systems, 14(1):80 107, Febru\u00adary 1996. [12] J.-D. Choi, \nK. Lee, A. Loginov, R. O Callahan, V. Sarkar, and M. Srid\u00adharan. Ef.cient and precise datarace detection \nfor multithreaded object-oriented programs. In Proceedings of the ACM SIGPLAN 2002 Conference on Programming \nLanguage Design and Implementation, Berlin, Germany, June 2002. [13] J. Chow, T. Gar.nkel, and P. M. \nChen. Decoupling dynamic program analysis from execution in virtual environments. In Proceedings of the \n2008 USENIX Technical Conference, pages 1 14, June 2008. [14] J. Devietti, B. Lucia, L. Ceze, and M. \nOskin. DMP: Deterministic shared memory multiprocessing. In Proceedings of the 2009 Inter\u00adnational Conference \non Architectural Support for Programming Lan\u00adguages and Operating Systems (ASPLOS), pages 85 96, March \n2009. [15] J. Devietti, J. Nelson, T. Bergan, L. Ceze, and D. Grossman. Rcdc: a relaxed consistency deterministic \ncomputer. In Proceedings of the sixteenth international conference on Architectural support for pro\u00adgramming \nlanguages and operating systems, pages 67 78, 2011. [16] G. W. Dunlap, S. T. King, S. Cinar, M. A. Basrai, \nand P. M. Chen. Re-Virt: Enabling intrusion analysis through virtual-machine logging and replay. In Proceedings \nof the 5th Symposium on Operating Systems Design and Implementation, pages 211 224, Boston, MA, December \n2002. [17] G. W. Dunlap, D. G. Lucchetti, M. Fetterman, and P. M. Chen. Exe\u00adcution replay on multiprocessor \nvirtual machines. In Proceedings of the 2008 ACM SIGPLAN/SIGOPS International Conference on Vir\u00adtual \nExecution Environments (VEE), pages 121 130, March 2008. [18] T. Elmas, S. Qadeer, and S. Tasiran. Goldilocks: \nA race and transaction-aware Java runtime. In PLDI, pages 245 255, 2007. [19] D. Engler and K. Ashcraft. \nRacerX: Ef.cient static detection of race conditions and deadlocks. In Proceedings of the 19th ACM Symposium \non Operating Systems Principles, pages 237 252, Bolton Landing, NY, 2003. [20] C. Flanagan and S. Freund. \nFastTrack: Ef.cient and precise dynamic race detection. In Proceedings of the ACM SIGPLAN 2009 Conference \non Programming Language Design and Implementation, pages 121 133, Dublin, Ireland, June 2009. [21] C. \nFlanagan and S. N. Freund. Type-based race detection for java. In Proceedings of the ACM SIGPLAN conference \non Programming lan\u00adguage design and implementation, pages 219 232, Vancouver, British Columbia, Canada, \n2000. [22] D. Grossman. Type-safe multithreading in cyclone. In Proceed\u00adings of the 2003 ACM SIGPLAN \ninternational workshop on Types in languages design and implementation, TLDI 03, pages 13 25, New York, \nNY, USA, 2003. [23] D. Hower, P. Dudnik, M. D. Hill, and D. A. Wood. Calvin: Determin\u00adistic or not? free \nwill to choose. In 17th International Conference on High-Performance Computer Architecture, February \n. [24] D. R. Hower and M. D. Hill. Rerun: Exploiting episodes for lightweight memory race recording. \nIn Proceedings of the 2008 Inter\u00ad  national Symposium on Computer Architecture, pages 265 276, June \n2008. [25] R. Huang, D. Y. Den, and G. E. Suh. Orthrus: Ef.cient software in\u00adtegrity protection on multi-cores. \nIn Proceedings of the 15th Inter\u00adnational Conference on Architectural Support for Programming Lan\u00adguages \nand Operating Systems, pages 371 383, Pittsburgh, PA, March 2010. [26] V. Kahlon, N. Sinha, E. Kruus, \nand Y. Zhang. Static data race detection for concurrent programs with asynchronous calls. In Proceedings \nof the the 7th joint meeting of the European software engineering conference and the ACM SIGSOFT symposium \non The foundations of software engineering, ESEC/FSE 09, pages 13 22, New York, NY, USA, 2009. [27] S. \nT. King, G. W. Dunlap, and P. M. Chen. Debugging operating systems with time-traveling virtual machines. \nIn Proceedings of the 2005 USENIX Technical Conference, pages 1 15, April 2005. [28] T. J. LeBlanc and \nJ. M. Mellor-Crummey. Debugging parallel pro\u00adgrams with instant replay. IEEE Transaction on Computers, \n36(4): 471 482, 1987. [29] D. Lee, B. Wester, K. Veeraraghavan, P. M. Chen, J. Flinn, and S. Narayanasamy. \nRespec: Ef.cient online multiprocessor replay via speculation and external determinism. In Proceedings \nof the 15th International Conference on Architectural Support for Program\u00adming Languages and Operating \nSystems, pages 77 89, Pittsburgh, PA, March 2010. [30] T. Liu, C. Curtsinger, and E. D. Berger. Dthreads: \nef.cient determin\u00adistic multithreading. In Proceedings of the Twenty-Third ACM Sympo\u00adsium on Operating \nSystems Principles, pages 327 336, 2011. [31] B. Lucia, J. Devietti, K. Strauss, and L. Ceze. Atom-aid: \nDetecting and surviving atomicity violations. In Proceedings of the 35th Annual International Symposium \non Computer Architecture, pages 277 288, Beijing, China, 2008. [32] P. Montesinos, L. Ceze, and J. Torrellas. \nDeLorean: Recording and deterministically replaying shared-memory multiprocessor execution ef.ciently. \nIn Proceedings of the 2008 International Symposium on Computer Architecture, pages 289 300, June 2008. \n[33] S. Narayanasamy, C. Pereira, and B. Calder. Recording shared mem\u00adory dependencies using Strata. \nIn ASPLOS-XII: Proceedings of the 12th International Conference on Architectural Support for Program\u00adming \nLanguages and Operating Systems, pages 229 240, 2006. [34] S. Narayanasamy, C. Pereira, H. Patil, R. \nCohn, and B. Calder. Au\u00adtomatic logging of operating system effects to guide application-level architecture \nsimulation. In International Conference on Measurements and Modeling of Computer Systems (SIGMETRICS), \npages 216 227, June 2006. [35] G. C. Necula, S. McPeak, S. P. Rahul, and W. Weimer. Cil: Inter\u00admediate \nlanguage and tools for analysis and transformation of c pro\u00adgrams. In Proceedings of the 11th International \nConference on Com\u00adpiler Construction, CC 02, pages 213 228, 2002. [36] E. B. Nightingale, D. Peek, P. \nM. Chen, and J. Flinn. Paralleliz\u00ading security checks on commodity hardware. In Proceedings of the 13th \nInternational Conference on Architectural Support for Program\u00adming Languages and Operating Systems, pages \n308 318, Seattle, WA, March 2008. [37] M. Olszewski, J. Ansel, and S. Amarasinghe. Kendo: ef.cient de\u00adterministic \nmultithreading in software. In Proceedings of the 2009 International Conference on Architectural Support \nfor Programming Languages and Operating Systems (ASPLOS), pages 97 108, March 2009. [38] S. Park, Y. \nZhou, W. Xiong, Z. Yin, R. Kaushik, K. H. Lee, and S. Lu. PRES: Probabilistic replay with execution sketching \non multiproces\u00adsors. In Proceedings of the 22nd SOSP, pages 177 191, October 2009. [39] P. Pratikakis, \nJ. S. Foster, and M. Hicks. Locksmith: Practical static race detection for c. ACM Trans. Program. Lang. \nSyst., 33:3:1 3:55, January 2011. [40] M. Ronsse and K. De Bosschere. RecPlay: A fully integrated practical \nrecord/replay system. ACM Transactions on Computer Systems, 17 (2):133 152, May 1999. [41] R. Rugina \nand M. Rinard. Symbolic bounds analysis of pointers, array indices, and accessed memory regions. In Proceedings \nof the ACM SIGPLAN 2000 conference on Programming language design and implementation, PLDI 00, pages \n182 195, New York, NY, USA, 2000. [42] R. Rugina and M. C. Rinard. Symbolic bounds analysis of pointers, \narray indices, and accessed memory regions. ACM Trans. Program. Lang. Syst., 27:185 235, March 2005. \n[43] M. Russinovich and B. Cogswell. Replay for concurrent non\u00addeterministic shared-memory applications. \nIn Proceedings of the ACM SIGPLAN 1996 Conference on Programming Language Design and Implementation, \npages 258 266, 1996. [44] S. Srinivasan, C. Andrews, S. Kandula, and Y. Zhou. Flashback: A light-weight \nextension for rollback and deterministic replay for software debugging. In Proceedings of the 2004 USENIX \nTechnical Conference, pages 29 44, Boston, MA, June 2004. [45] B. Steensgaard. Points-to analysis in \nalmost linear time. In Proceed\u00adings of the 23rd ACM SIGPLAN-SIGACT symposium on Principles of programming \nlanguages, POPL 96, pages 32 41, New York, NY, USA, 1996. [46] N. Sterling. Warlock: A static data race \nanalysis tool. pages 97 106, 1993. [47] J. Tucek, S. Lu, C. Huang, S. Xanthos, and Y. Zhou. Triage: Diag\u00adnosing \nproduction run failures at the user s site. In Proceedings of the 21st ACM Symposium on Operating Systems \nPrinciples, pages 131 144, October 2007. [48] uclib.org. uClibc, a C library for embedded Linux. http://uClibc.org. \n[49] K. Veeraraghavan, D. Lee, B. Wester, J. Ouyang, P. M. Chen, J. Flinn, and S. Narayanasamy. DoublePlay: \nParallelizing sequential logging and replay. In Proceedings of the 16th International Conference on Architectural \nSupport for Programming Languages and Operating Systems, Long Beach, CA, March 2011. [50] J. W. Voung, \nR. Jhala, and S. Lerner. Relay: static race detection on millions of lines of code. In Proceedings of \nthe the 6th joint meeting of the European software engineering conference and the ACM SIG-SOFT symposium \non The foundations of software engineering, pages 205 214, Dubrovnik, Croatia, 2007. [51] D. Weeratunge, \nX. Zhang, and S. Jagannathan. Analyzing multicore dumps to facilitate concurrency bug reproduction. In \nProceedings of the 2010 International Conference on Architectural Support for Programming Languages and \nOperating Systems (ASPLOS), pages 155 166, March 2010. [52] R. P. Wilson and M. S. Lam. Ef.cient context-sensitive \npointer analysis for c programs. In Proceedings of the ACM SIGPLAN 1995 conference on Programming language \ndesign and implementation, PLDI 95, pages 1 12, New York, NY, USA, 1995. [53] S. C. Woo, M. Ohara, E. \nTorrie, J. P. Singh, and A. Gupta. The SPLASH-2 programs: Characterization and methodological consider\u00adations. \nIn Proceedings of the 22nd International Symposium on Com\u00adputer Architecture, pages 24 36, June 1995. \n[54] M. Xu, R. Bodik, and M. D. Hill. A .ight data recorder for enabling full-system multiprocessor deterministic \nreplay. In Proceedings of the 2003 International Symposium on Computer Architecture, pages 122 135, June \n2003. [55] M. Xu, V. Malyugin, J. Sheldon, G. Venkitachalam, and B. Weissman. ReTrace: Collecting execution \ntrace with virtual machine determinis\u00adtic replay. In Proceedings of the 2007 Workshop on Modeling, Bench\u00admarking \nand Simulation (MoBS), June 2007. [56] C. Zam.r and G. Candea. Execution synthesis: A technique for auto\u00admated \nsoftware debugging. In Proceedings of the European Confer\u00adence on Computer Systems, pages 321 334, April \n2010.     \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Chimera uses a new hybrid program analysis to provide deterministic replay for commodity multiprocessor systems. Chimera leverages the insight that it is easy to provide deterministic multiprocessor replay for data-race-free programs (one can just record non-deterministic inputs and the order of synchronization operations), so if we can somehow transform an arbitrary program to be data-race-free, then we can provide deterministic replay cheaply for that program. To perform this transformation, Chimera uses a sound static data-race detector to find all potential data-races. It then instruments pairs of potentially racing instructions with a weak-lock, which provides sufficient guarantees to allow deterministic replay but does not guarantee mutual exclusion.</p> <p>Unsurprisingly, a large fraction of data-races found by the static tool are false data-races, and instrumenting them each of them with a weak-lock results in prohibitively high overhead. Chimera drastically reduces this cost from 53x to 1.39x by increasing the granularity of weak-locks without significantly compromising on parallelism. This is achieved by employing a combination of profiling and symbolic analysis techniques that target the sources of imprecision in the static data-race detector. We find that performance overhead for deterministic recording is 2.4% on average for Apache and desktop applications and about 86% for scientific applications.</p>", "authors": [{"name": "Dongyoon Lee", "author_profile_id": "81448592586", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3471299", "email_address": "dongyoon@umich.edu", "orcid_id": ""}, {"name": "Peter M. Chen", "author_profile_id": "81406595818", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3471300", "email_address": "pmchen@umich.edu", "orcid_id": ""}, {"name": "Jason Flinn", "author_profile_id": "81100027306", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3471301", "email_address": "jflinn@umich.edu", "orcid_id": ""}, {"name": "Satish Narayanasamy", "author_profile_id": "81100556410", "affiliation": "University of Michigan, Ann Arbor, MI, USA", "person_id": "P3471302", "email_address": "nsatish@umich.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254119", "year": "2012", "article_id": "2254119", "conference": "PLDI", "title": "Chimera: hybrid program analysis for determinism", "url": "http://dl.acm.org/citation.cfm?id=2254119"}