{"article_publication_date": "06-11-2012", "fulltext": "\n And Then ThereWere None: AStall-Free Real-Time Garbage Collectorfor Recon.gurable Hardware DavidF. \nBacon PerryCheng Sunil Shukla IBM Research {dfb,perry,skshukla}@us.ibm.com Abstract Programmers are \nturning to radical architectures such as recon.g\u00adurable hardware (FPGAs) to achieve performance. But \nsuch sys\u00adtems, programmed at a very low level in languages with impover\u00adished abstractions, are orders \nof magnitude more complex to use than conventional CPUs. The continued exponential increase in transistors, \ncombined with the desire to implement ever more so\u00adphisticated algorithms, makes it imperative that such \nsystems be programmed at much higher levels of abstraction. One of the fun\u00addamental high-level language \nfeatures is automatic memory man\u00adagementin the formofgarbage collection. We present the .rst implementationofa \ncompletegarbage col\u00adlector in hardware (as opposed to previous hardware-assist tech\u00adniques), using an \nFPGA and its on-chip memory. Using a com\u00adpletely concurrent snapshot algorithm, it provides single-cycle \nac\u00adcesstotheheap,andneverstallsthe mutatorforevenasinglecycle, achieving a deterministic mutator utilization \n(MMU) of 100%. We have synthesized the collector to hardware and show that it never consumes more than \n1% of the logic resources of a high-end FPGA.For comparison we also implementedexplicit (malloc/free) \nmemory management, and show that real-time collection is about 4% to 17% slower than malloc, with comparable \nenergy consump\u00adtion. Surprisingly, in hardware real-time collection is superior to stop-the-world collection \non every performance axis, and even for stressful micro-benchmarks can achieve100% MMU with heaps as \nsmall as 1.01 to 1.4 times the absolute minimum. Categories and Subject Descriptors B.3.3[Memory Structures]: \nWorst-case analysis; B.5.1[Register-Transfer-Level Implementa\u00adtion]: Memory design; B.7.1[Integrated \nCircuits]: Gate arrays; C.3[Special-Purpose and Application-Based Systems]: Real-time and embedded systems; \nD.3.3[Programming Languages]: Lan\u00adguage Constructs and Features; D.3.4[Programming Languages]: Memory \nmanagement(garbage collection) General Terms Design, Languages, Experimentation, Perfor\u00admance Keywords \nBlock RAM, FPGA, High Level Synthesis, Garbage Collection, RealTime Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, Beijing, China. \nCopyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 1. Introduction The end of frequencyscaling \nhas driven architects and developers to parallelism in search of performance. However, general-purpose \nMIMD parallelism is inef.cient and power-hungry, with power rapidly becoming the limiting factor. This \nhas led the search for performance to non-traditional chip architectures like GPUs and other more radical \narchitectures. The mostradical general-purpose computing platform of all is recon.gurable hardware, in \nthe form of Field-Programmable Gate Arrays (FPGAs). FPGAs are now available with over 1 million programmable \nlogiccellsand8MBofon-chip blockRAM ,providingamassive amount of bit-level parallelism combined with single-cycle \naccess to the memory capacity of a VAX-11/780. Furthermore, because that memory is distributed throughout \nthe chip in 18 Kbit units, algorithms with huge data bandwidth can also be implemented. However, programming \nmethodology for FPGAs has laggedfar behind their capacity, which in turn has greatly reduced their appli\u00adcation \nto general-purpose computing. The most common languages for FPGA programming are still hardware description \nlanguages (VHDLandVerilog)inwhichtheonly abstractionsarebits, arrays of bits, registers, wires, and so \non. The entire approach to program\u00adming them is oriented around the synthesis of a chip that happens \ntobe recon.gurable,as opposedto programmingageneral-purpose device. Recent research has focused on raising \nthe level of abstraction and programmability to that of high-level software-based program\u00adming languages, \nin particular, the Kiwi project[14]which uses C#, and the Liquid Metal project, which has developed the \nLime lan\u00adguage[4]based onJava. However, up until now, whether programmers are writing in low-level HDLs \nor high-level languages like Kiwi and Lime, use of dynamic memory management has only just begun to be \nexplored[11, 25], and use ofgarbage collection has been non\u00adexistent. In this paper we presentagarbage \ncollector synthesized entirely into hardware, capable of collecting a heap of uniform objects completely \nconcurrently.We call such a heap of uniform objects a miniheap. By uniform we mean that the shape of \nthe objects (the size of the data .elds and the location of pointers) is .xed. Thus we tradeadegreeof \n.exibilityin the memory layout for largegainsin collector performance. In the FPGA domain this makes \nsense: due to the distributed nature of the memory it is common to build pipelined designs where each \nstage of the pipeline maintains its own internal data structures that are able to access their local \nblock RAM in parallel with other pipeline stages. Furthermore, .xed data layouts can provide order-of-magnitude \nbetter performance because theyallow designs which deterministically process one operation per clock \ncycle.  For instance, the Lime language provides the capability to map a graph of compute tasks, each \nwith its own private heap memory, onto the FPGA. Thus it is more important to handle many small heaps \nwith a high degree of ef.ciency, than to handle a high degree of heterogeneity in the data layout. Historically, \nit is also interesting to note that McCarthy s inven\u00adtionofgarbage collection for LISP[18]wasin the contextof \nan IBM 704 with only 4K 36-bit words, essentially used as a .xed\u00adformat heap of S-expressions. Algorithmically, \nour collectorisafairly straightforwardYuasa\u00adstyle snapshot-at-the-beginning concurrent collector[35], \nwith a linear sweep phase. However, by taking advantage of hardware structures like dual-ported memories, \nthe ability to simultaneously read and write a register in a single cycle, and to atomically dis\u00adtribute \na control signal across the entire system, we are able to de\u00advelop a collector that never interferes \nwith the mutator. Furthermore, the mutator has single-cycle access to memory, and the design can actually \nsupport multiple simultaneous memory operations percycle. Arbitration circuits delay somecollector oper\u00adationsby \nonecycleinfavorof mutator operations,butthe collector cankeepup witha mutatoreven whenit performsa memory \noper\u00adation every cycle (allocations are limited to one every other cycle). The collector we describe \ncan be used either directly with pro\u00adgrams hand-written in hardware description languages (which we explore \nthis paper) or as part of a hardware run-time system tar\u00adgeted by a compiler for a C-to-gates[11, 25]or \nhigh-level lan\u00adguage[4, 14]system including dynamic memory allocation. The latter is left to future work, \nand we concentrate in this paper on exploring the design, analysis, and limits of the hardware collector. \nThe contributions of this paper are: the .rst implementation of an entiregarbage collector in hard\u00adware \n(as opposed to hardware-assist or microcode), including both stop-the-world and fully concurrent variants; \n the .rstgarbage collector to entirely eliminate mutator interfer\u00adencebythe collector(byevenasinglecycle), \nthereby achieving minimum mutator utilization (MMU[7]) of 100%;  an implementation of the collector \nin Verilog along with de\u00admanding hardware-based client applications performing up to one memory allocation \nevery 14 cycles;  a performanceevaluation showing the costofgarbage collec\u00adtion in absolute terms and \nrelative to malloc/free, including dy\u00adnamic measurement of throughput and energy;  analytic closed-form \nworst-case bounds (in cycles) for collec\u00adtion time and minimum heap size required for 0-stall real-time \nbehavior,alongwithexperimentalevidenceof safety,and tight\u00adness within 2-6% for time and 3% for space; \nand  an exploration of the design space showing that in hardware, real-time collection simultaneously \nachieves higher throughput, and lower latency,memory usage, and energy consumption than stop-the-world \ncollection.  2. FPGA Background Field Programmable Gate Arrays (FPGAs) are programmable logic devices \nconsisting of 4-or 6-input look-up tables (LUTs) which can be used to implement combinational logic, \nand .ip-.ops which can be used to implement sequential logic. On the Xilinx FPGAs which we use in this \nwork, several LUTs and .ip-.ops are combined together to form a unit called a slice, which is the standard \nunit in which resource consumption is reported for FPGAs (Altera FPGAs use a slightly different architecture). \nFPGAs also include a clock distribution network for propagat\u00ading a globally synchronized clock to allow \nfor the use of conven\u00adtional clocked digital logic. Our collector takes advantage of this global clock \nin a number of ways, in particular to implement an ef.cient single-cycle atomic root snapshot. The FPGA \nalso contains a large amount of con.gurable routing resources for connecting the slices, based on the \ndata .ow in the hardware description language program. The routing resources are usedby the place-and-route(PAR) \ntool during hardware synthesis. 2.1 Memory Structures on FPGAs Particularly important to this work are \nthe memories available on the FPGA. Block RAMs (BRAMs) are specialized memory struc\u00adtures embedded within \nthe FPGA for resource-ef.cient implemen\u00adtation of large random-and sequential-access memories. The XilinxVirtex-5 \nLX330T[32]device that we use in this paper (oneof the largestin thatfamily) hasa BRAM capacityof 1.5MB;the \nlatest generationof Xilinxdevices,theVirtex-7,have as much as8MBof BRAM. AsingleBRAMinaVirtex-5FPGA can \nstoreupto36 Kilobits (Kb) of memory. An important feature of BRAM is that it can be organized in various \nform factors (analogous to word sizes on a CPU).OntheVirtex-5, formfactorsof1,2,4,9,18,36,72, and so \nonare supported.A36KbBRAMcanalsobeusedastwologically separate 18 Kb BRAMs. Moreover, a larger memory \nstructure can bebuiltby cascading multiple BRAMs horizontally,vertically or in a hybrid manner. Any memory \nstructure in the design which is smaller than 18 Kb would lead to quantization (or, in memory system \nparlance, fragmentation ). The quantization effect can be considerable depending on the logical memory \nstructure in the design (and is explored in Sec-tion7).ABRAM canbe used asa true dual ported (TDP) RAM \nproviding two fully independent read-write ports. Furthermore, each port supports either read, write, \nread-before-write, or read\u00adafter-write operations. Our collector makes signi.cant use of read\u00adbefore-write \nfor things like theYuasa-style write barrier[35]. BRAMs can also be con.gured for use as FIFO queues \nrather than as random access memories; we make use of this feature for implementing the mark queues in \nthe tracing phase of the collector. FPGAs are typically packaged on boards with dedicated off\u00adchip DRAM \nand/or SRAM which can be accessed via a memory controller synthesized for the FPGA. Such memory could \nbe used to implement much larger heap structures. However, we do not consider use of DRAM or SRAM in \nthis paper because we are focusing on high-performance designs with highly deterministic (single cycle) \nbehavior. 3. Memory Architecture The memory architecture that is, the way in which object .elds are \nlaid out in memory,and the free list is maintained is common to our support of both malloc/free andgarbage-collected \nabstrac\u00adtions. In this section we describe our memory architecture as well as some of the alternatives, \nand discuss the tradeoffs qualitatively. Some tradeoffs areexplored quantitativelyin Section 7. Since \nmemory structures within an FPGA are typically and of necessityfar more uniform thanina conventional \nsoftware heap, we organize memory into one or more miniheaps, in which objects have a .xed size and shape \nin terms of division between pointer and data .elds. This is essentially the same design as the big bag \nof pages (BIBOP) style in conventional software memory allocator design, in which the metadata for the \nobjects is implicit in the page in which theyreside[28]. The .xed shape of the miniheaps is a natural \nmatch to the FPGA, which must use .xed-width data paths to transmit the ob\u00adjects and .elds across the \nrouting network to achieve high perfor\u00admance.  ing a single .eld of pointer type (for heap size N 8 \nand pointer width log N =3). Block RAMs are in yellow, with the dual ports (A/B) shown.For each port, \nthe data line is above and the address select line is below. Ovals designate 3-bit wide pointer .elds; \nthose in blue are in use. 3.1 Miniheap Interface Each miniheap has an interface allowing objects to be \nallocated (and freed when using explicit memory management), and opera\u00adtions allowing individual data \n.elds to be read or written. In this paper we will consider miniheaps with one or twopointer .elds and \none or twodata .elds. This is suf.cient for implementing many stack, list, queue, and tree data structures, \nas well as S\u00adexpressions. FPGA modules for common applications like packet processing, compression, etc. \nare coveredby such structures. Our design allows an arbitrary number of data .elds. Increas\u00ading the number \nof pointer .elds is straightforward for malloc-style memory. However, for garbage collected memory, the \nextension would require additional logic.Webelievethisis relatively straight\u00adforward to implement (and \ninclude details below)but theexperi\u00admental results in this paper are con.ned to one-and two-pointer objects. \n 3.2 Miniheap with Malloc/Free There are many ways in which the interface in Section 3.1 can be implemented. \nFundamentally, these represent a time/space (and sometimes power) trade-offbetween the number of available \npar\u00adallel operations, and the amount of hardware resources consumed. For FPGAs, one speci.esalogical \nmemory block withadesired data width and number of entries, and the synthesis tools attempt to allocate \nthe required number of individual Block RAMs as ef\u00ad.ciently as possible, usingvarious packing strategies.We \nrefer to the BRAMs for such a logical memory block as a BRAM set. In our design we use one BRAM set for \neach .eld in the object. For example, if there are two pointer .elds and one data .eld, then there are \nthree BRAM sets. The non-pointer .eld hasanatural width associated with its data type (for instance 32 \nbits). However, for a miniheap of size N, the pointer .elds must only be llog2 Nl bits wide. Because \ndata widths on the FPGA are completely customizable, we use precisely the required number of bits. Thus \na larger miniheap will increase in size not only because of the number of entries,but because the pointer \n.elds themselves become larger. Asin software,the pointervalue0is reservedto mean null , so a miniheap \nof size N can really only store N - 1 objects. A high-level block diagram of the memory manager is shown \nin Figure 1. It shows the primary data and control .elds of the memory module, although manyof the signals \nhave been elided to simplify the diagram.For clarityof presentationit showsa single object .eld,of pointer \ntype(Pointer Memory), which is stored in a single BRAM set.Asecond BRAM set(Free Stack)is used to store \na stack of free objects. For an object withf .elds, there would be f BRAM sets with associated interfaces \nfor the write and read values (but not an additional address port). And of course there is only a single \nfree stack, regardless of how many.elds the object has. The Alloc signal is a one-bit signal used to \nimplement the malloc operation. A register is used to hold the value of the stack top. Assuming it is \nnon-zero, it is decremented and then presented on portBof the Free Stack BRAM set,in read mode. The resulting \npointer to a free .eld is then returned(Addr Alloc d), but is also fed to port B of the Pointer Memory, \nin write mode with the write value hard-wired to 000 (or null ). To free an object, the pointer is presented \nto the memory man-ager(Addr to Free). The Stack Top register is used as the address for the Free Stack \nBRAM set on port B, in write mode, with the data value Addr to Free. Then the Stack Top register is incre\u00admented. \nThis causes the pointer to the freed object to be pushed onto the Free Stack. In order to read or write \na .eld in the Pointer Memory, the Addr to Read/Write is presented, and, if writing, a Pointer to Write. \nThis uses portAof the BRAM setin either read or write mode, returning a value on the Pointer Value port \nin the former case. Note that this design, by taking advantage of dual-porting the BRAMs, can allow a \nread or write to proceed in parallel with an allocate or free.  3.3 Fragmentation and OtherTrade-Offs \nTheBRAMquantization describedin Section 2.1canplayasignif\u00adicant rolein theef.ciencyof BRAM utilization.For \ninstance, fora miniheap of size N = 256, pointers are8bits wide, soa single18 Kb BRAM con.gured as9bits \nwidewouldbe used. Thiswastes1 bitperentry,butalsowastes1.75K entries,sinceonly256(0.25K) entries are \nneeded.The1bitwastedper .eldisa formof internal fragmentation and the 1.75K wasted .elds are a form of \nexternal fragmentation. To reduce external fragmentation, multiple .elds of the same size couldbe implemented \nwithasingle BRAM set.However,since BRAMs are dual-ported, supporting more than two .elds would result \nin a loss of parallelism in terms of .eld access. Furthermore, since we use one BRAM port for initialization \nof .elds when they are allocated, this effect comes into play even with two .elds. The opposite approach \nis also possible: multiple .elds can be implemented with a single BRAM set, resulting in a wider data \nwidth. In principle this can reduce internal fragmentation. However, in practice we .nd that this can \nactually result in poorer resource allocation.Awider data width also means that updatesto individual \n.elds mustbe performed witharead/modify/write sequence, which requires two cycles. Furthermore, the read/modify/write \ncan not be pipelined, so in addition to higher latency, throughput can be halved. Threaded Free List. \nAcommon software optimization would beto representthefree objectsnotasa stackof pointers,butasa linked \nlist threaded through the unused objects (that is,a linked list through the .rst pointer .eld). Since \nthe set of allocated and free objects are mutually exclusive, this optimization is essentially free modulo \ncache locality effects. However, in hardware, this causes resource contention on the BRAM set containing \nthe .rst pointer (since it is doing double duty). Thus parallelism is reduced: read or write operations \non the .rst pointer can not be performed in the same cycle as malloc or free, and the latter require \ntwo cycles rather than one.  Figure 2. Single-Cycle Atomic Root Snapshot Engine 4. Garbage Collector \nDesign Wenowdescribetheimplementationofbothastop-the-worldanda fully concurrent collector in hardware. \nIn software, the architecture of these two styles of collector are radically different. In hardware, \nthe differences are much smaller, as the same fundamental struc\u00adtures and interfaces are used. The concurrent \ncollector has a few extra data structures (im\u00adplemented with BRAMs) and also requires more careful alloca\u00adtionof \nBRAM portstoavoid contention,but these featuresdo not negatively affect the use of the design in the \nstop-the-world col\u00adlector. Therefore, we will present the concurrent collector design, and merely mention \nhere that the stop-the-world variant omits the shadow register(s) from the root engine, the write barrier \nregister and logic from the trace engine, and the used map and logic from the sweep engine. Our collector \ncomprises three separate components, which han\u00addle the atomic root snapshot, tracing, and sweeping. 4.1 \nBackground:Yuasa s Snapshot Algorithm Before delving into the details of our implementation, we describe \nYuasa s snapshot algorithm[35]which is the basis of our imple\u00ad mentation. While the mechanics in hardware \nare quite different, it is interesting to note that implementing in hardware allows us to achieveahigherdegreeof \nconcurrencyand determinismthan state\u00adof-the-art software algorithms,but without incorporating more so\u00adphisticated \nalgorithmic techniques developed in the interim. The fundamental principle of the snapshot algorithm \nis that when collectionis initiated,a logical snapshot of the heap is taken. The collector then runs \nin this logical snapshot, and collects every\u00adthing thatwasgarbageat snapshot time. InYuasa s original \nalgorithm,the snapshot consistedofthereg\u00adisters, stacks, and globalvariables. This setof pointerswasgath\u00adered \nsynchronously (since then, much research has been devoted to avoiding the need for anyglobal synchronization \nat snapshot time or during phase transitions[3,22]). Once the roots have beengathered, the mutator is \nallowed to proceed and the collector runs concurrently, marking the transitive closure of the roots. \nIf the mutator concurrently modi.es the heap, its only obliga\u00adtion is to make sure that the collector \ncan still .nd all of the objects that existed in the heap at snapshot time. This is accomplished by the \nuseofa write barrier: before anypointer is over-written, it is recordedinabufferand treatedasa rootforthe \npurposesof collec\u00adtion. Objects that are freshly allocated during a collection are not eligible for collection \n(they are allocated black in the parlance of collector literature). The advantage of the snapshot algorithm \nis its simplicity and determinism. Since it operates in a logical snapshot at an instant in time, the \ninvariants are easy to describe. In addition, termination is simple and deterministic, since the amount \nof work is bounded at the instant that collection begins. This is in contrast to the incremental update \nstyle algorithms of Steele[27]and Dijkstra[12](and numerous successors), which attemptto chase objects \nthat are freshly allocated during collec\u00adtion.  4.2 Root Snapshot The concurrent collector uses the \nsnapshot-at-the-beginning algo\u00adrithm described above.Yuasa soriginal algorithm requiredaglobal pause \nwhile the snapshot was taken by recording the roots; since then real-time collectors have endeavored \nto reduce the pause re\u00adquiredbytheroot snapshot.Inhardware,weareableto completely eliminatethe snapshot \npausebytakingadvantageofthe parallelism and synchronization available in the hardware. The snapshot must \ntake twotypes of roots into account: those in registers, and those on the stack. Figure 2shows the root \nsnapshot module, simpli.ed to show a single stack and a single register. The snapshot is controlled by \nthe GC input signal, which goes high for one clock cycle at the beginning of collection. The snap\u00adshot \nis de.ned as the state of the memory at the beginning of the next cycle after the GC signal goes high. \nThis allows some setup time and reduces synchronization requirements. The register snapshot is obtained \nby using a shadow register. In the cycle after the GC signal goes high, the value of the register is \ncopiedintothe shadowregister.This can happeneveniftheregister isalso writtenbythe mutatorinthe samecycle,sincethenewvalue \nwill not be latched until the end of the cycle. The stack snapshot is obtained by having another register \nin addition to the Stack Top register, called the Scan Pointer. In the samecycle that the GC signal goeshigh,thevalueofthe \nStack Top pointer minus one is written into the Scan Pointer (because the Stack Top points to the entry \nabove the actual top value). Beginning in the following cycle, the Scan Pointer is used as the source \naddresstoportBoftheBRAMset containingthestack,and the pointer is read out, going through the MUX and \nemerging on the Root to Add port from the snapshot module. The Scan Pointer is also decremented in preparation \nfor the following cycle. Note that the mutator can continue to use the stack via portA of the BRAM set, \nwhile the snapshot uses port B. And since the mutatorcannotpopvaluesoffthestackfasterthanthe collectorcan \nread them out, the property is preserved that the snapshot contains exactly those rootsthatexistedinthecyclefollowingthe \nGC signal. A detail omitted from the diagram is that a state machine is required to sequence the values \nfrom the stack and the shadow register(s) through the MUX to the Root to Add port. Note that the values \nfrom the stack must be processed .rst, because the stack snapshot technique relies on staying ahead of \nthe mutator without anyexplicit synchronization. If multiple stacks were desired, then a shadow stack \nwould be required to hold values as theywere read out before the mutator couldoverwritethem,whichcouldthenbe \nsequencedontothe Root to Add port. As will be seen in Section 4.4, collection is triggered (only) by \nan allocation that causes free space to drop below a threshold. Therefore the generation of root snapshot \nlogic only needs to con\u00adsider those hardware states in which this might occur. Any register or stack \nnot live in those states can be safely ignored.  Figure 3. Tracing Engine and a Single Pointer Memory \n 4.3 Tracing The tracing engine, along with a single pointer memory (corre\u00adspondingtoa single pointer \n.eldinan object)isshownin Figure 3. It provides the same mutator interface as the malloc/free style mem\u00adory \nmanagerof Figure 1:Addr to Read/Write, Pointer to Write, and Pointer Value except that the external \ninterface Addr to Free is replaced by the internal interface (denoted in red) Addr to Clear, whichis \ngeneratedbytheSweep module (describedin Section 4.4). The only additional interface is the Root to Add \nport which takes its inputs from the output port of the same name of the Root Engine in Figure 2. As \nit executes, there are three sources of pointers for the engine to trace:externally added roots from \nthe snapshot, internally traced roots from the pointer memory, and over-written pointers from the pointer \nmemory (captured with a Yuasa-style barrier to maintain the snapshot property). The different pointer \nsources .ow through a MUX, and on each cycle a pointer can be presented to the Mark Map, which contains \none bit for each of the N memory locations. Using the BRAM read-before-write mode, the old mark value \nis read, and then the mark value is unconditionally set to 1. If the old mark value is 0, this pointer \nhas not yet been traversed, sothenegationoftheoldmarkvalue (indicatedbythebubble)is used to control whether \nthe pointer is added to the Mark Queue (note that this means that all values in the Mark Queue have been \n.ltered, so at most N - 1 values can .ow through the queue). The Mark Queue isaBRAM usedin FIFO (rather \nthan random access) mode. Pointers from the Mark Queue are presented as a read address on port B of the \nPointer Memory, and if the fetched values are non-null are fed through the MUX and thence to the marking \nstep. The write barrieris implementedbyusingportAofthe Pointer Memory BRAM in read-before-write mode. \nWhen the mutator writes a pointer, the old value is read out .rst and placed into the Barrier Reg. This \nis subsequently fed through the MUX and marked (the timing and arbitration is discussed below). Given \nthe three BRAMs involved in the marking process, pro\u00adcessing one pointer requires 3 cycles. However, \nthe marking en\u00adgine is implemented as a 3-stage pipeline, so it is able to sustain a throughput of one \npointer per cycle. 4.3.1 Trace EnginePairing For objects with twopointers, twotrace engines are paired \ntogether to maximize resource usage (this is not shown in the .gure). Since Figure 4. Free Stack and \nSweeping Engine each trace engine only uses one port of the mark map, both engines can mark concurrently. \nFurthermore, the two mark queues are MUXed together and the next item to mark is always taken from the \nlonger queue. When there is only one item to enqueue, it is placed on the shorter queue. Using this design, \nwe provision each of the2queues to be of size 3N/8+ R (where R is the maximum number of roots), which \nguarantees that the queues will neverover.ow.Fora formal argument, see Appendix A. On each cycle, one \npointer is removed from the queues, and the two pointers in the object retrieved are examined and potentially \nmarked and enqueued. The .nal optimization is that since there are now two write barrier registers and \ntwo mark queues, the write barrier values are not processed until there are two of them. This means that \nthe mark engines canmake progressevery othercycleevenifthe application is performing one write per cycle. \n 4.3.2 TraceTermination and WCET Effects The termination protocol for marking is simple: once the last \nitem from the mark queues is popped (both mark queues become empty), it takes 2 or 3 cycles for the trace \nengine to .nish the current pipeline. If the two pointers returned by the heap are null, then the mark \nprocessis terminatedinthe secondcycleas thereisno need to read the mark bits in this case. Otherwise \nthe mark bit for the non-null pointers are read to ensure that both pointers are marked, in which case \nthe mark phase is terminated in the third cycle. Write barrier values arriving after the .rst cycle of \ntermination can be ignored, since by the snapshot property they would either have to be newly allocated \nor else discovered by tracing the heap. However, note that some (realistic) data structures, in particular \nlinked lists, will cause a pathological behavior, in which a pointer is marked, removed from the queue, \nwhich will appear empty, and then 2 cycles later the next pointer from the linked list will be enqueued.So \nwhilethepipeline can sustain marking one objectper cycle, pipelinebubbles will occur which reduce that \nthroughput. We are currently investigating speculative pointer forwarding optimizationsbut they have \nnot been implemented so we merely note that it may be possible to remove at least one if not both of \nthesebubblesata modest costin additional logic.  4.4 Sweeping Once tracing is complete, the sweep phase \nbegins, in which mem\u00adory is reclaimed. The high-level design is shown in Figure 4. The sweep engine also \nhandles allocation requests and maintains the stack of pointers to free memory(Free Stack). The Mark \nMap here is the same Mark Map asin Figure 3.  When an Alloc request arrives from the mutator, the Stack \nTop register is used to remove a pointer to a free object from the Free Stack, and the stack pointer \nis decremented. If the stack pointer falls belowa certainlevel(we typically use 25%), thenagarbage collection \nis triggered by raising the GC signal which is connected to the root snapshot engine (Figure 2). The \naddress popped from the Free Stack is returned to the mutator on the Addr Alloc d port. It is also used \nto set the object s entry in the Used Map, to 01, meaning freshly allocated (and thus black ).Avalueof \n00 means free , in which case the object is on the Free Stack. When tracing is completed, sweeping begins \nin the next ma\u00adchine cycle. Sweeping is a simple linear scan. The Sweep Pointer is initialized to1(since \nslot0is reserved for null), and on every cycle (except when pre-empted by allocation) the sweep pointer \nis presented to both the Mark Map and the Used Map. If an object is marked, its Used Map entry is set \nto 10. If an object is not marked and its used map entry is 10 (the and gate in the .gure) then the used \nmap entry is reset to 00. Although only 3 states are used, the particular choice of bit pattern is based \non avoiding unneededlogic.Theresultingsignalisalsousedto control whether the current Sweep Pointer address \nis going to be freed. If so,itis pushed ontothe Free Stack and also output on the Addr to Clear port, \nwhich is connected to the mark engine so that the data values being freed are zeroed out. Note that since \nclearing only occurs during sweeping, there is no contention for the Pointer Memory port in the trace \nengine between clearing and marking. Furthermore, an allocation and a free may happen in the same cycle: \nthe top-of-stack is accessed using read-before-write mode and returned as the Addr Alloc d,and then the \nnewly freed object is pushed back. When an object is allocated, its entry in the Mark Map is not set \n(otherwise an extra interlock would be required). This means that the tracing engine may encounter newly \nallocated objectsin its marking pipeline (via newly installed pointers in the heap), albeit at most once \nsince theywill then be marked. This also affects WCET analysis, as we will see in the next section. 5. \nAnalysis of Real-Time Behavior First of all, we note that since the design of our real-time collector \nallows mutation and collection to occur unconditionally together in a singlecycle, the minimum mutator \nutilization(or MMU[7]),is 100% unless insuf.cient resources are dedicated to the heap. Furthermore, unlike \nsoftware-based collectors[5, 16], the sys\u00ad tem is fully deterministic because we can analyze the worst \ncase behavior down to the (machine) cycle. Given R is the maximum number of roots, N is the size of the \nheap,thentheworst-casetime(incycles)forgarbage collectionis T = TR + TM + TW + TX + TS + TA (1) where \nTR is the time to snapshot the roots, TM is the time (in cycles) to mark, TS is the time to sweep, and \nTW is the time lost to write barriers during marking, TX is the time lost to blackening newly allocated \nobjects during marking, and TA is time lost to allocations during sweeping. In the worst case, without \nanyknowledge of the application, TR = R +2 TM =3N +3 TW =0 TX =0 TS = N The reasoning for these quantities \nfollows. During the snapshot phase, we can place one root into the mark queue every cycle, plus onecycleto \nstartand .nishthe phase, accountingfor R+2. During marking, there could be N objects in the heap, con.gured \nas a linkedlist which causedthemark pipelineto stallfortwocycleson each object,plus3 cyclesto terminate. \nSweepingis unaffectedby application characteristics, and always takes N cycles. Pre-emption of the collectorby \nmutator write barriers(TW )does notfactor into theworst-case analysis becausethe write barrierworkisoverlapped \nwith the collector stalls. Extra mark operations to blacken newly allocated objects(TX )also simply .ll \nstall cycles. Our design allows an allocation operation in every cycle, but allocation pre-empts the \nsweep phase, meaning that such an alloca\u00adtion rate can only be sustained in shortbursts. The largest \nsustain\u00adable allocation rate is 0.5 otherwise the heap would be exhausted before sweeping completed. \nThus TA = N and Tworst = R +5N +5 (2) 5.1 Application-Speci.c Analysis Real-time analysis typically takes \nadvantage of at least some application-speci.c knowledge. This is likely to be particularly true of hardware-based \nsystems.Fortunately, the structure of such systemsmakesit morelikelythatsuchfactorscanbe quanti.edto \na high degree of precision, e.g. by looking at operations per clock cycle in the synthesized design. \nLet \u00b5 be theaverage numberof mutations percycle(\u00b5 = 1), a betheaverage numberof allocationspercycle(a< \n0.5), and m be the maximum number of live data objects in the heap at anyone time(m<N). Then we can more \nprecisely estimate '''' \u00b5 ' a T=3m +3 T= aTT = mT = N M XMW A 2 - \u00b5 1 - a Note that both a and \u00b5 can \nonly be averaged over a time window guaranteed to be less than or equal to the phases which theyin.u\u00adence; \nm is a safe window size. The largest inaccuracyis still due to pipeline stalls during mark\u00ading, for which \nworst-and average-case behavior can be very dif\u00adferent.We therefore let B be the number of pipeline stalls(0 \n= B = 2m), so an even more precise bound on marking is T''= M m + B +3 (and also improving T''= aT'' \nXM ). For a linked list,B =2m;for three linked lists each with its own root, B =0.Wehypothesizethatfortheheap \nconsideredasa forest without back-edges, B is boundedbythe numberoflevelsof width1plus the number of \nlevels of width2(when the width is3 or greater, there is enough parallelism tokeep the 3-stage pipeline \nfull and avoid stalls). Using these application-speci.c estimates, we then are able to bound the worst-case \nexecution time (WCET) of collection as \u00ab \u00ab 12 N Tmax = R + B +5+ m + (3) 1 - a 2 - \u00b5 1 - a  5.2 Minimum \nHeap Size Once the worst-case execution time for collection is known, we can solve for the minimum heap \nsize in which the collector can run with real-time behavior (zero stalls). Obviously m objects must be \navailable for the live data. While a collection taking time Tmax takes place, another aTmax objects can \nbe allocated. However, there may also be aTmax .oatinggarbage from the previouscycle when a collection \nstarts. Thus the minimum heap size is Nmin = m +2aTmax (4) and if we denote the non-size-dependent portion \nof Tmax from equation(3)by \u00ab \u00ab 12  then we can solve for Nmin = m +2aTmax \u00ab Nmin = m +2aK + (1 - a)2 \n(1 - a)2(m +2aK) Nmin = (5) 1 - 4a + a2 6. Experimental Methodology Since we have implemented the .rst \ncollector of this kind, we can not simply use a standard set of benchmarks to evaluate it. Therefore, \nwe have implemented two micro-benchmarks intended to be representative of the types of structures that \nmight be used in an FPGA: a doubly-ended queue (deque), which is common in packet processing, and a binary \ntree, which is common for algorithms like compression. It is important to note that because of the very \nhigh degree of determinism in the hardware, and in our collector implementation, such micro-benchmarks \ncan provideafar more accurate pictureof performance than in typical evaluations of CPU-based collectors \nrunning in software. There are no cache effects, no time-slicing, and no interrupts. Because there are \nno higher order effects, the performance behavior presented to the mutator by the collector and vice \nversa is completely captured by the memory management APIatacycle-accuratelevel.Wevalidate thisexperimentallyby \nshowing that the estimates for collection time and minimum real\u00adtime heap size (from Section 5.1)are \nhighly accurate. Agiven micro-benchmarkcan be paired with one of the three memory management implementations \n(Malloc, stop-the-world GC, and real-time GC). Furthermore, these are parameterized by the size of the \nminiheap, and for the collectors, the trigger at which to start collection (although for most purposes, \nwe simply trigger when free spacefalls below 25%).We call these design points. There are manyFPGA product \nlines and manydifferent con.g\u00adurations within each line. Our experiments are performed using a XilinxVirtex-5 \nLX330T[32], which is the largest chip within the Virtex5 LXT product line (that is now two generations \nold). Given that the motivation to use dynamic memory management will go up with complexity, and complexity \nwill go up with larger chips, we believe that this is a good representative design. The LX330T has 51,840 \nslices and 11,664 Kb (1.4 MB) of Block RAM.Fabricated in 65nm technology, the chip is theoret\u00adically \ncapableofbeing clockedatupto550MHz,but realisticde\u00adsigns generally run between 100 and 300 MHz. For each \ndesign point, we perform complete synthesis, includ\u00ading place-and-route(PAR), for the LX330T chip.PARis \nthe .nal physical synthesis stage and it reports the highest clock frequency the design can be clocked \nat, as well as the device resource utiliza\u00adtion such as slices and BRAM.We used Xilinx ISE 13.4 tool \nfor synthesis. 6.1 Description of Benchmarks Our .rst benchmark is a binary search tree which is standard \nmem\u00adberofafamilyof binary tree data structures includingvariants like red-black trees, splay trees, and \nheaps. Though all the standard op\u00aderations are implemented, the benchmark, for simplicity, exports only \nthree operations: insert, delete, and traverse. The benchmark can be run against a workload containing \na sequence of such oper\u00adations.Ourworkload generatoris con.guredtokeepthe maximum number of live nodes \nto 8192 while bursts of inserts and deletes can cause the instantaneous amount of live nodes tofall to \n7/8 of that. Theburstiness of the benchmark necessitates measuring the allocation rate dynamically through \ninstrumentationbutprovidesa more realistic and challenging test for our collector.Traversal op- Figure \n5. FPGA Logic Resource (slice) Usage  Figure 7. Synthesized Clock Frequency erations are included to \ncon.rm that our collector is not corrupting anydata as the heap size is reduced. The allocation rate \nof the bi\u00adnary tree is proportional to the tree depth and could be characterized as intermediatefor micro-benchmarks.Inthe \ncontextofacomplete program, the .nal allocation rateis potentiallyevenlower.Asfar as pipeline stalls, \nthere will only be B =3 stalls since thefanout of the tree .lls the pipeline after the .rst two levels \nhave been pro\u00adcessed. The second benchmark is a deque (double-ended queue). The doubly-linked list can \nbe modi.ed by pushes and pops to either the front or back. As before, our workload consists of a random \nsequence of such operations whilekeeping the maximum amount of live data to 8192. To contrast with the \nprevious benchmark, there are no deliberatebursts which makes the allocation rate more consistentbutalsokeepsthe \namountoflivedataalwaysquite close to the maximum. Because there is no traversal or computation, the allocation \nrate is much higher and stresses the collector much more. Asfarastraversal,a doubly-linkedlistisequivalenttotwo \nsingly\u00adlinked lists each of half the length. Thus, there will be B = m/2 stalls in the marking pipeline. \n7. Evaluation Webeginbyexamining the cost,in termsof static resources,of the 3memory managers malloc/free \n( Malloc ), stop-the-world col\u00adlection ( STW ), and real-time concurrent collection ( RTGC ).  (a) \nExecution durationin cyclesof BinaryTree (b) Execution durationin cyclesof Deque  (c) Execution timein \nmillisecondsof BinaryTree (d) Execution timein millisecondsof Deque Figure 8. Throughput measurementsforthe \nBinaryTreeandDeque Microbenchmarks. Because energy consumptionis dominatedby static power, which is virtually \nconstant, graphs (c) and (d) also show energy in millijoules; the curves are identical. For these purposes \nwe synthesize the memory manager in the ab\u00adsence of anyapplication. This provides insight into the cost \nof the memory management itself, and also provides an upper bound on the performance of actual applications \n(since they can only use more resources or cause the clock frequencyto decline). We evaluate design points \nat heap sizes (in objects) from 1K to 64K in powers of 2. For these purposes we use an object layout \nof two pointers and one 32-bit data .eld. The results are shown in Figures5to7.Figure 5showsthe utilizationof \nnon-memorylogic resources(in slices).Asexpected,garbage collection requires more logic than Malloc. Between \nthe two collectors, RTGC requires between4%to39% more slicesthanSTW.WhileRTGC consumes upto4times more \nslices than Mallocin relative terms,in absolute terms it uses less than 0.7% of the total slices even \nfor the largest heap size so logic consumption for all3 schemesiseffectivelya non-issue. Figure6shows \nBRAM consumption. Because we have chosen powers of 2 for heap sizes, the largest heap size only uses \n60% of the BRAM resources (one is of course free to choose other sizes).Atthe smallerheap sizes,garbage \ncollectorsconsumeupto 80% more BRAMs than Malloc. However, at realistic heap sizes, the .gure drops to \n24%. In addition,RTGC requires about 2-12% more memory than STW since it requires the additional 2-bit \nwide Used Map to cope with concurrent allocation. Fragmentation is noticeablebut nota majorfactor, ranging \nfrom 11-31% for Malloc and 11-53%forgarbagecollection.Asbefore,atlargerheapsizes, the fragmentation decreases. \nSome wastage can be avoided by choosingheapsizesmorecarefully,not necessarilyapowerof2,by noting that \nBRAMs are available in 18Kb blocks. However, some fragmentation loss is inherent in the quantization \nof BRAMs as they are chained together to form larger memories. Finally, Figure 7shows the synthesized \nclock frequencyat dif\u00adferentdesignpoints.Hereweseea signi.canteffectfromthe more complex logic forgarbage \ncollection:even thoughit consumes rel\u00adatively little area, clock frequencyforgarbage collectionis notice\u00adably \nslower (15-39%) than Malloc across all design points. On the other hand, the difference between STW andRTGC \nis small with RTGC often faster. Regardless of the form of memory manage\u00adment, clock frequencydeclines \nas the heap becomes larger. How\u00adever, the overall clock rate may very well be constrained by the application \nlogic rather than the collector logic, as we will see be\u00adlow. 7.1 Dynamic Measurements So far we have \ndiscussed the costs of memory management in the absence of applications; we now consider what happens \nwhen the memory manager is linked to the microbenchmarks from Section 6.1. Unlike the previous section, \nwhere we concentrated on theeffectsofawiderangeofmemorysizesonstaticchip resources, herewefocusonasmallerrangeofsizesusingatracewithasingle \nmaximum live data set of m = 8192 as described previously.We thenvary the heap size N from m to 2m at \nfractional increments of m/10. As we make memory scarce, the resolution is also increased to m/100 toshowhowthe \nsystembehavesatverytight conditions. Each design point requires a full synthesis of the hardware design \nwhich can affect the frequency, power, and execution time. 7.1.1 Throughput Figure 8 shows the throughput \nof the benchmarks as the heap size varies for all 3 schemes. To understand the interaction of various \neffects, we not only examine the throughput both in cycle  (a) Dynamic Energyin milli-joulesof BinaryTree \n(b) Dynamic Energyin milli-joulesof Deque Figure 9. Dynamic Energy duration(graphs(a)and(b)),butalso,sincethe \nsynthesizableclock frequenciesvary,inphysicaltime (graphs(c)and(d)). The Binary Tree benchmark goes through \nphases that are allocation-and mutation-intensive, and those that are not. As a result its allocation \nrate a is 0.009 objects/cycle, and its mutation rate \u00b5 is 0.02 pointer writes/cycle, when consideredovera \nwindow size of m cycles. Because of these relatively low rates, the duration incyclesin Figure 8(a)of \nboth Malloc andRTGC stays constant from 2m all the way down to 1.1m. RTGC actually consumes slightly \nfewer cycles since it does not need to issue explicit free operations. Because STW pauses the mutator, \neach collection in\u00adcreases the total number of cycles required. As the heap gets tight, the duration \nin cycles for STW rises quickly. However, when we obtain the physical time by dividing total duration \nin cycles by synthesized clock frequency, as shown in Figure 8(c), things become less cut and dried. \nAlthough Malloc alone can be synthesized at considerably higher frequencies than STW or RTGC (Figure \n7), it is often the application rather than the memory manager that becomes the limiting factor on clock \nspeed. Therefore, though the differences between the three memory managers is small, there are many chaotic \nvariations. These are primarilyexplainedbyexpected randomvariationsinthe synthesis tool which uses simulated \nannealing to achieve a good placement and routing.To maintainfairness, we have used same settings for \nall the synthesis jobs and avoided explicit manual optimizations. The Deque benchmark showsa different \nbehavior.With much higher allocation and mutation rates(a =0.07 and \u00b5 =0.13),it is much more sensitive \nto collector activity. As seen in Figure 8(b), even at heap size N =2m, STW consumes noticeably more \ncycles, risingto almost doublethecyclesatN =1.1m.Bycontrast RTGC consumesslightly fewer cycles than Malloc \nuntil it begins to experience stall cycles (non-real-time behavior) at N =1.4m becauseit cannotkeepup \nwith the mutator. The Deque benchmarkis considerably simplerthan BinaryTree in terms of logic, so it \nhas a correspondingly lower impact on syn\u00adthesized clock frequency.Theeffectis seen clearlyin Figure \n8(d): Malloc synthesizes at a higher frequency, allowing it to make up RTGC s slight advantage in cycles \nand consume 25% less time on an average. STW suffers even more from the combined effect of a lower clock \nfrequencyand additional cycles due to synchronous collection.Onaverage,RTGCisfaster than STWby 14% andof \ncourse does not interrupt the application at all. These measurements reveal some surprising trends that \nare completely contrary to the expected trade-offs for software col\u00adlectors:RTGC is actually faster, \nmore deterministic, and requires less heap space than STW! There seems to be no reason to use STW because \nthe natural advantage of implementing concurrency in hardware completely supersedes the traditional latency \nversus bandwidth tradeoffin software. Furthermore, RTGC allows applications to run at far lower multiples \nof the maximum live set m than possible for either real\u00adtime or stop-the-world collectors in software. \nRTGC is also only moderatelyslowerthanMalloc,meaningthatthecostof abstraction is considerably more palatable.As \npredicted, this performancegap only decreases with more complex applications.  7.1.2 Energy The energy \n(or power-delay product) is the product of average power dissipationandphysicaltime.Itisabetter metricthanpower \nalone since it takes into the account the actual time needed to .nish agivenworkload.Forexample, dynamicpower \nconsumption can be reducedbylowering the clock frequencybutit does not mean that the energy consumed \nto .nish the workload is also reduced. To calculate the energy we need to have knowledge of the power \nconsumption and the total time taken to .nish the workload. We calculated thepower, whichisa sumof static \nand dynamicpower, using the Xpower tool from Xilinx[33]. The Xpower tool provides an approximate estimate \nof the power consumed by a design, for a particular FPGA device, by taking several factors into account \nsuch as simulation data, clock frequency, supply voltages, ambient and junction temperature. The accuracyof \nthe result depends most importantly on the simulation data which is a representative of the target application.Based \non the switching activity of the nets em\u00adbedded in the simulation data, the dynamic power can be calculated \nvery accurately. Staticpower consumption(thepower consumedby the chip simply when it is on) for an FPGA \nchip is independent of the design.We have found that the static power contributes more than 90% of the \ntotal power for all the cases we investigated. We synthesized the Binary Tree and Deque benchmarks for \neach heap sizes shown in Figure 8to get the clock frequencyand generate a fully placed-and-routed netlist. \nThen we simulated each design using the ModelSim simulator to generate the toggle activity of each net \nin the design and the result is stored in the VCD format [19]. We fed the synthesized netlist, the VCD \n.le and the clock frequency constraint to the Xpower tool which provides detailed information about the \nstatic and dynamic power consumption. The power consumption is then multiplied by the execution time \n(in seconds) to calculate the total energy consumption. The total energy consumption for Binary Tree \nand Deque is shown in Figure 8(c) and Figure 8(d) respectively (using the sec\u00ad ondary right-hand axis). \nBecause static power is so much larger than the dynamic power, the total power tracks the execution time \nwithin 1% and it is shown on the same graph.To demonstrate the differences in energy due to the design \nand benchmarks, the dy\u00adnamic energy consumption for the BinaryTree and Deque bench\u00admarksis shownin Figure \n9. For the BinaryTree benchmark, theaverage energy consump\u00adtion(averagedover all the design points) forRTGCislower \nthan  (a) BinaryTree (b) Deque STWby6% and higher than Mallocby 8%.For the Deque bench\u00admark, onaverageRTGC \nconsumes 14% less and 34% more energy than STW and Malloc respectively. The dynamic energy provides additional \ninsight into the nature of benchmarks.For BinaryTree, RTGC consumes 3% and 30% more dynamic energy than \nSTW and Malloc respectively.ForDeque,RTGC consumes8%lessand63% more dynamic energy than STW and Malloc \nrespectively. The analysis shows that the energy consumption is highly application-dependent. For both \nthe benchmarks we considered itis safetosay thatRTGCisa better choice thanSTWasfaras energy consumption \nis considered. The average total energy con\u00adsumptionof Mallocis smallerthanRTGCforboththe benchmarks. \nHowever, as the complexity and size of benchmark increases the energy consumptiongap betweenRTGC and \nMalloc diminishes.  7.2 Validation of Real-time Bounds Because the design and analysis of our concurrent \ncollector is intended to be cycle-accurate, we can validate the time and space boundsofthe collectorwithexpectationthattheywillbefully \nmet. Figure 10 shows the actual time spent in garbage collection and the analytic upper bound(Tmax from \nequation 3). Note that we show bothaverageas wellasthe maximum time spentingarbage collections. The heap \nsize is chosen to be much tighter than in earlier graphs as our focus here is how the collector behaves \nwhen it is under stress (near Nmin from equation 4). For convenience, we express heap size as a fraction(N/m)of \nthe maximum amount of live data since our bounds are almost linear when considered in terms of m and \nN. Average time spent in collection is always less than the pre\u00addicted worst case with an actual difference \nof about 10% for both programs.We also show the amountof stallsexperiencedby the benchmark as a fraction \nof total time. At larger heap sizes, there are no stalls. As the heap size is reduced, there will come \na point when the collector cannotkeepup and the mutator s allocation re\u00adquestwillbeblocked.ForBinaryTreethis \noccurswhentheheapis a mere 1.01 \u00d7 m while the more allocation-intensive Dequefails at 1.43 \u00d7 m. Our predicted \nNmin values of 1.037 and 1.457 are correctly above the actualfailure points. Because the average collection \ntime includes multiple phases of a program, it can be signi.cantly lower than the maximum collection \ntime.We see that thegapbetween Tmax and collection time shrinks from 10% to about 2% and 6% when one \nconsiders maximum rather thanaverage collectiontime.For space, Nmin has only a worst-case .avor as there \nis adequate heap space only if the heap is suf.cient at every collection. The space bound is within 3% \nof when stalls begin. Our time and space bounds are not only empiricallyvalidatedbut are tight. In general, \ntime spent fora single collectionfalls as the heap size is decreased since the sweep phase will take \nless time. It may seem surprising that this happens even when the heap size is taken below Nmin.However,falling \nbelowthis safe point causes mutator stalls but does not penalize the collector at all. In fact, because \nthe mutator is stalled, it can no longer interfere with the collector which will additionally, though \nvery slightly, speed up collection. Of course, sincetheoverallgoalistoavoid mutator stalls, operating \nin this regime is inadvisable. 8. RelatedWork There has been very little work on supporting high-level \nmemory abstractions in recon.gurable hardware, and none ongarbage col\u00adlection. Simsa, Singh,etal.[11,25]haveexplored \ncompilationof C subprograms that use malloc/free into VHDL or Bluespec for synthesis to FPGAs. LEAP scratchpads[1]provide \nanexpandable memory abstrac\u00ad tion which presentsa BRAM interface,but usesoff-chip RAMif the structure \nis too large to .t, and transparently uses the on-chip BRAM as a cache. Such a system could be coupled \nwith ours in order to provide a larger, virtualized memory, albeit at the expense of determinism and \nthroughput. Faesetal.[13]havebuiltan FPGA-aware collector, witha completely different goal from ours: \nallowing the FPGA to main\u00adtain references into the CPU s main program heap. Thisfacilitates co-processingby \nthe FPGA. 8.1 Micro-coded Collectors Meyer[20]hasbuilta special-purpose processor and an associated \ngarbage collection co-processor (using Baker s semi-space algo\u00adrithm[6]), and realizedthem on an Altera \nAPEX FPGA.However, the design and the goals were very different. Meyer s collector is for a general-purpose \nheap allocated in DRAM, and for a program operating on what is for the most part a conventional CPU. \nThe collector is implemented with a microcoded co-processor, and the CPU is modi.ed with a special pointer \nregister set, and pointer op\u00aderations include support for read and write barriers. Some of the control \nlogic also runs in software on the main CPU. By contrast, we have created fully custom logic that is \nmuch more tightly inte\u00adgrated with the memory, for programs that are also synthesized into hardware, \nand with deterministic single-cycle memory access. Maximum collector pauses in Meyer s system are 500 \ncycles (2.5 \u00b5s at 200 MHz), due to root scanning. Read barriers and interlocks also cause pauses up to \n200 cycles. By contrast our collector pauses for0clockcycles.Meyer s collector also requires heapsof2timesthe \nmaximumlivedatainordertoavoid non-real\u00adtime behavior, considerably more than our collector. Of course, \nour performance comes at a cost in .exibility: Meyer s collector handles arbitrary heap shapes and larger \nmemories.  The Intel iAPX 432, the apotheosis of CISC architecture, did not provide explicit frees, \nand microcoded the marking portion of Dijkstra s on-the-.y collector (the rest was implemented in the \niMAX-432 operating system). The system object table contained bits for the tri-color marking abstraction[31]. \nSchmidt and Nilsen[23]studied the design (using the DLX sim\u00ad ulator) of a garbage collected memory module \n(GCMM), in which a memory module plugged intoa conventionalbus included paired DRAMs (for the collector \nsemispaces) and an extra processor that ran Baker s algorithm[6]. The interface to the CPU for opera\u00adtions \nlike allocation, root descriptors, and so on are implemented as memorymappedports.Theyreportdelaysof0.5msto \ninitiatea collection and1 \u00b5s for allocate, fetch, and store.  8.2 Hardware-assisted Collection Most \nhardware support forgarbage collection has beenin the form of specialized memory barriers (particularly \nread barriers) because of their high throughput cost. The Symbolics Lisp Machine[21] introduced this \nkind of hardware barrier to implement Baker s al\u00adgorithm[6].Both the Lisp Machine and SOAR[30]also introduced \nhardware support for generational write barriers. Frustration with the availability and acceptance of \ncustom pro\u00adcessor designs to supportgarbage collectiongave rise toa stock hardware line of research, \nusing conventional CPU features to sup\u00adport collector operations[2,8]. The pendulum swung back with the \nthe AzulVegaprocessor and its associated collector[10].Hard\u00ad ware support for read barriers,fast user-mode \ntrap handlers (4-10 cycles), cooperative pre-emption, and special TLB support enabled a collector capable \nof handling very large heaps and large numbers of processors. The collector achieved an MMU of 21% at \n50 ms (the authors hypothesize due largely to read barrier trap storms), and 52% at 200 ms. Ironically, \nthe Azul collector has recently been portedtoa stock hardware x86 platform runningona Linuxker\u00adnel augmented \nwith page remapping operations[29]. Hardware support has also been proposed for reference count\u00ading.Joaoetal.[17]describeaprocessorextendedwitha \nreference count coalescingbuffer whichis ableto .lterover 96%of refer\u00adence counttraf.c. Srisa-anandLo[26]proposea \nco-processorthat performs saturating reference counting (using 2-or 3-bit counts) backedbya conventional \ntracing collector.Yu[34]proposesasys\u00ad tem with a reference-counted nursery and an incrementally traced \nmature space, using hardware support for allocation and write bar\u00adriers and a specialized cache architecture. \nHeil and Smith[15]apply hardware support for pro.ling to the garbage collection problem,by treatingitasaformof \ninstrumenta\u00adtion of store operations. The instrumented operations are then han\u00addledbyservice threads,and \nthereisan additional threadto perform marking and sweeping. Schoeberl[24]extends the JOPJava processor \nwith hardware support for non-blocking object copies in the memory controller. This allows compaction \noperations to be pre-empted at the granu\u00adlarity of a single write. 9. Conclusion We have described our \ndesign, implementation, and evaluation of the .rst garbage collectors to be completely synthesized into \nhardware. The real-time version causes zero cycles of interference with the mutator, achieving for the \n.rst time a minimum mutator utilization of 100%. Careful implementation allows a closed-form analytic \nsolution forworst-caseexecutiontime(WCET)ofthe collector,andalower bound on heap size to achievereal-time \nbehavior. These bounds are also cycle-accurate. In software there are large trade-offs between stop-the-world \nand real-time collection in terms of throughput, latency, and space. Our measurements show that in hardware \nthe real-time collector isfaster, has lower (zero) latency, and can run effectively in less space. In \naddition, it consumes less overall energy because it can run to completionfaster. This performance and \ndeterminism is not without cost: our collector only supports a single .xed object layout. Supporting \nlarger objects with more pointers is a relatively straightforward extension to our design; supporting \nmultiple object layouts is more challengingbut we believe canbe achieved without sacri.cing the fundamental \nadvantages. Compared toexplicit memory management, hardwaregarbage collection still sacri.ces some throughput \nin exchange for a higher level of abstraction. It may be possible to narrow thisgap through more aggressive \npipelining. However, thegap in space needed to achievegood performance is substantially smaller than \nin software. Forthe.rsttime,garbage collectionof programs synthesizedto hardware is practical and realizable. \nA. Proof of Over.ow Freedom As describedin Section 4.3.1,the queuesforapairof trace engines are each \nsized to have 3N/8+ R entries. Here we prove that neither queue will over.ow. We begin by reviewing various \nGC operations that affect the queue. During the root phase, R roots are inserted into queue 0 while write \nbarrier entries are inserted into queue 1. Thus, queue1 can have up to R entries at the start of the \ntracing phase. Each trace step in the tracing phase pops from the fuller queue and then inserts up to2pointers \n(one in each queue). Interleaved with the traced steps are write barrier insertions which can also insert \nup to2 pointers (one in each queue). If the write barrier step inserts only one pointer, then it will \ninsert into the emptier queue. We note that the operations in the mark phase are symmetric with respect \nto the two queues. Speci.cally, there is no bias in either the pushes and pops nor in the graph traversal \nsince the role of left and right pointers are interchangeable. However, there is a difference in the \nroot phase in that queue 0 is always equally or more full than queue 1. Thus, if there were to be over.ow \nat all, it would occur in queue 0. At the end of the root phase, the queues have up to R items so we \nmust show that the queue will not grow by more than 3N/8 during the tracing phase. The tracing phase \ncan be considered to bea sequenceof tracing steps and write barrier insertions.Tracing stepsalwayspopanitemandcanpush0,1,or2items.Writebarrier \ninsertions perform no popsbut can push0,1, or2items.Awrite barrier operation that pushes0 items does \nnot change the state at all and need not be considered. However, a trace step that pushes 0 items cannot \nbe so easily dismissed since the total number of trace steps is bounded by N. Thus, limiting anytype \nof trace steps potentially affects the overall outcome. We take full advantage of the balanced push and \npop by noting thatno operation increases queue imbalancebeyondadifferenceof 1between the queues since \nboth the trace step pops from the fuller queue and the write barrier pushes onto the emptier queue for \nthe case when it pushes only one item. If there were a pathological sequence of operations that causes \neither queue to over.ow, then in the step just prior, the soon-to-over.ow queue must be full and the \nother queue must be nearly full. Further, we need not consider write barrier operations that perform \na single-push as we can conceptually rearrange them into half as many double-push operations which runfaster.If \nthere weretobeanover.ow,they would occur even faster with this rearrangement. Thus, we need only bound \nthe total queue growth to N/2.We label the3typesof tracing steps T0, T1, and T2 based on the number of \npushes they perform. Note that the net effect on the queue size is one less than the index.Similarly, \nwe have W2 which stands for the number of 2-push write barrier operations.  We are tryingto maximize \ntotal occupancywhichisgivenby -T0 + T2 +2W2 subject to the usual non-negativity constraints as well as \n3 addi\u00adtional constraints. The .rst constraint expresses the fact that we schedule at most one write \nbarrier operation for each tracing opera\u00adtion. The second and third constraints require that the total \nnumber of pushes and pops cannot exceed the total number of objects. T0 + T1 + T2 = W2 T1 +2T2 +2W2 = \nN T0 + T1 + T2 = N With a slight rearrangement, these equations can be put in the standard form of a \nlinear programming problem[9]. Although the quantities are constrained to be integral (making this in \nreality an integer linear programming problem), we are safe in droppping the integrality constraints \nas that only increases the feasible region. The over-approximation of the objective is not a soundness \nissue since we are establishing an upper bound. Conversely, the bound is also rather tight by noting \nthe total size of the coef.cients in the objective function. The problem is easily solved with the simplex \nmethod and standard tableau techniques show that the problem is feasible and bounded with the objective \nmaximized at 3N/4 and the free vari\u00adables T0 and T1 at zero while T2 and W2 are at N/4. Since the queues \nalways maintain balance, we arrive at the .nal individual queue size by halving the 3N/4 and including \nthe capacity needed for the root phase to arrive at 3N/8+ R. We omit the actual tableaus as theyare uninteresting \nand shed less insight thanbyexaminingafewkeypointsin the space. From the objective function, it is intuitively \ndesirable to maximize W2. If we allow only W2 and T0 to be non-zero, then we will have both at N/2 with \na total occupancy of N/2. Similarly, allowing only W2 and T1 into play at N/3 will achieve 2N/3. Finally, \nW2 and T2 both at N/4 achieves the maximum of 3N/4. If we were to leave out W2 entirely, T2 increases \nto N/2 but the objective actually decreases to N/2. The changes in these values con.rm our intutition \nthat trace operations that perform no pushes do not stress the queues and that maximizing the write barrier \noperations will cause the greatest occupancy. Acknowledgments We thank the members of the Liquid Metal \nteam at IBM Research who contributed insights, infrastructure, and a stimulating working environment: \nJoshua Auerbach, Stephen Fink, and Rodric Rabbah. Wealso thank team members and the reviewers for their \ncorrections and insightful comments which helped to improve the paper. References [1] M. Adler et al. \nLeap scratchpads: automatic memory and cache management for recon.gurable logic. In FPGA, pp. 25 28, \n2011. [2]A.W.Appel,J.R.Ellis,andK.Li. Real-time concurrent collectionon stock multiprocessors. In PLDI, \npp. 11 20, June 1988. [3] J. Auerbach, D.F. Bacon,P. Cheng, D. Grove, B. Biron, C. Gracie, B. McCloskey,A. \nMicic, andR. Sciampacone. Tax-and-spend: demo\u00ad cratic scheduling for real-timegarbage collection. In \nEMSOFT, pp. 245 254, 2008. [4]J. Auerbach,D.F. Bacon,P. Cheng,andR. Rabbah. Lime:aJava\u00ad compatible and \nsynthesizable language for heterogeneous architec\u00ad tures. In OOPSLA, pp. 89 108, Oct. 2010. [5] D.F. \nBacon,P. Cheng, andV.T. Rajan. Areal-timegarbage collector with low overhead and consistent utilization. \nIn POPL, pp. 285 298, Jan. 2003. [6] H. G. Baker. List processing in real-time on a serial computer. \nCom\u00admun. ACM, 21(4):280 294, Apr. 1978. [7] G. E. Blelloch andP. Cheng. On bounding time and space for \nmulti\u00ad processorgarbage collection. In PLDI, pp. 104 117, June 1999. [8] R. A. Brooks. Trading data space \nfor reduced time and code space in real-timegarbage collection on stock hardware. In LFP, pp. 256 262, \nAug. 1984. [9] V. Chvatal. Linear Programming.W. H. Freeman and Company,1983. [10]C.Click,G.Tene,andM.Wolf. \nThe pauselessGC algorithm.In VEE, pp. 46 56, 2005. [11] B. Cook et al. Finding heap-bounds for hardware \nsynthesis. In FMCAD, pp. 205 212, Nov. 2009. [12]E.W. Dijkstra,L. Lamport,A.J. Martin,C.S. Scholten,andE.F.M. \nSteffens. On-the-.ygarbage collection: an exercise in cooperation. Commun. ACM, 21(11):966 975, 1978. \n[13]P.Faes,M. Christiaens,D. Buytaert,andD. Stroobandt. FPGA-aware garbage collection in Java. In FPL, \npp. 675 680, 2005. [14] D. Greaves and S. Singh. Kiwi: Synthesis of FPGA circuits from parallel programs. \nIn FCCM, 2008. [15] T. H. Heil and J. E. Smith. Concurrent garbage collection using hardware-assisted \npro.ling. In ISMM, pp. 80 93, 2000. [16] R. Henriksson. Scheduling Garbage Collection in Embedded Systems. \nPhD thesis, Lund InstituteofTechnology,July 1998. [17]J.A.Joao,O.Mutlu,andY.N.Patt. Flexible reference-counting-based \nhardware acceleration forgarbage collection. In ISCA, pp. 418 428, 2009. [18] J. McCarthy. Recursive \nfunctions of symbolic expressions and their computationby machine. Commun. ACM, 3(4):184 195, 1960. [19] \nMentor Graphics. ModelSim SE Users Manual. Version 10.0c. [20] M. Meyer. An on-chipgarbage collection \ncoprocessor for embedded real-time systems. In RTCSA, pp. 517 524, 2005. [21]D.A.Moon. Garbage collectioninalargeLISPsystem.In \nLFP, Aug. 1984. [22] F. Pizlo, D. Frampton, E. Petrank, and B. Steensgaard. Stopless: a real-timegarbage \ncollector for multiprocessors. In ISMM, pp. 159 172, 2007. [23] W. J. Schmidt and K. D. Nilsen. Performance \nof a hardware-assisted real-timegarbage collector. In ASPLOS, pp. 76 85, 1994. [24] M. Schoeberl andW. \nPuf.tsch. Nonblocking real-timegarbage col\u00ad lection. ACM Trans. Embedded Comput. Sys., 10:1 28, 2010. \n[25] J. Simsa and S. Singh. Designing hardware with dynamic memory abstraction. In FPGA, pp. 69 72, 2010. \n[26] W. Srisa-an, C.-T. D. Lo, and J. M. Chang. Active memory processor: A hardware garbage collector \nfor real-time Java embedded devices. IEEE Trans. Mob. Comput., 2(2):89 101, 2003. [27] G. L. Steele, \nJr. Multiprocessing compactifyinggarbage collection. Commun. ACM, 18(9):495 508, Sept. 1975. [28] G.L. \nSteele,Jr. Data representationin PDP-10MACLISP.Tech. rep., MIT, 1977. AI Memo 420. [29] G.Tene, B. Iyengar, \nand M.Wolf. C4: the continuously concurrent compacting collector. In ISMM, pp. 79 88, 2011. [30]D.Ungaretal. \nArchitectureofSOAR:SmalltalkonaRISC.In ISCA, pp. 188 197, 1984. [31] Wikipedia. Intel iAPX 432, Nov. \n2011. [32] Xilinx. Virtex-5familyoverview.Tech.Rep. DS100,Feb. 2009. [33] Xilinx. Power methodology guide. \nTech. Rep. DS786, Mar. 2011. [34]W.S.Yu. Hardware concurrentgarbage collection for object-oriented processor. \nMaster s thesis,CityUniversityofHongKong, 2005. [35]T.Yuasa. Real-timegarbage collection on general-purpose \nmachines. J. Systems and Software, 11(3):181 198, Mar. 1990.     \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Programmers are turning to radical architectures such as reconfigurable hardware (FPGAs) to achieve performance. But such systems, programmed at a very low level in languages with impoverished abstractions, are orders of magnitude more complex to use than conventional CPUs. The continued exponential increase in transistors, combined with the desire to implement ever more sophisticated algorithms, makes it imperative that such systems be programmed at much higher levels of abstraction. One of the fundamental high-level language features is automatic memory management in the form of garbage collection.</p> <p>We present the first implementation of a complete garbage collector in hardware (as opposed to previous \"hardware-assist\" techniques), using an FPGA and its on-chip memory. Using a completely concurrent snapshot algorithm, it provides single-cycle access to the heap, and never stalls the mutator for even a single cycle, achieving a deterministic mutator utilization (MMU) of 100%.</p> <p>We have synthesized the collector to hardware and show that it never consumes more than 1% of the logic resources of a high-end FPGA. For comparison we also implemented explicit (malloc/free) memory management, and show that real-time collection is about 4% to 17% slower than malloc, with comparable energy consumption. Surprisingly, in hardware real-time collection is superior to stop-the-world collection on every performance axis, and even for stressful micro-benchmarks can achieve 100% MMU with heaps as small as 1.01 to 1.4 times the absolute minimum.</p>", "authors": [{"name": "David F. Bacon", "author_profile_id": "81100628167", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P3471135", "email_address": "dfb@us.ibm.com", "orcid_id": ""}, {"name": "Perry Cheng", "author_profile_id": "81451593218", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P3471136", "email_address": "perry@us.ibm.com", "orcid_id": ""}, {"name": "Sunil Shukla", "author_profile_id": "81488643003", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P3471137", "email_address": "skshukla@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254068", "year": "2012", "article_id": "2254068", "conference": "PLDI", "title": "And then there were none: a stall-free real-time garbage collector for reconfigurable hardware", "url": "http://dl.acm.org/citation.cfm?id=2254068"}