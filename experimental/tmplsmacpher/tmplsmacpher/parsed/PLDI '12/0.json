{"article_publication_date": "06-11-2012", "fulltext": "\n Compiling a High-Level Language for GPUs (via Language Support for Architectures and Compilers) Christophe \nDubach1,2 Perry Cheng2 Rodric Rabbah2 David F. Bacon2 Stephen J. Fink2 1University of Edinburgh 2IBM \nResearch christophe.dubach@ed.ac.uk {perry,rabbah,dfb,sj.nk}@us.ibm.com Abstract non-uniform memory hierarchy \nmay degrade performance by a fac\u00adtor of ten or more. Languages such as OpenCL and CUDA offer a standard \ninterface Experience shows that programming in a high-level language for general-purpose programming \nof GPUs. However, with these is more productive, portable, and less error-prone. Ideally, a pro\u00adlanguages, \nprogrammers must explicitly manage numerous low\u00adgrammer should express a program using high-level constructs \nthat level details involving communication and synchronization. This are architecture independent, and \nhave the compiler automatically burden makes programming GPUs dif.cult and error-prone, ren\u00adgenerate \ndevice-speci.c code that is competitive with low-level dering these powerful devices inaccessible to \nmost programmers. hand-written code. Indeed, programmers have enjoyed these bene-We desire a higher-level \nprogramming model that makes GPUs .ts with general purpose programming languages on general pur\u00admore \naccessible while also effectively exploiting their computa\u00adpose CPUs for several decades. In this paper, \nwe address challenges tional power. This paper presents features of Lime, a new Java\u00adin delivering similar \nbene.ts for programs running with GPUs. compatible language targeting heterogeneous systems, that allow \nThis paper presents details of a GPU programming model in a an optimizing compiler to generate high quality \nGPU code. The new programming language called Lime. As presented earlier [2],key insight is that the \nlanguage type system enforces isolation and Lime is a Java-compatible object-oriented language which \ntargets immutability invariants that allow the compiler to optimize for a heterogeneous systems with \ngeneral purpose processors, FPGAs, GPU without heroic compiler analysis. and GPUs. The Lime methodology \nallows a programmer to gently Our compiler attains GPU speedups between 75% and 140% of refactor a suitable \nJava program into a pattern amenable for hetero\u00adthe performance of native OpenCL code. geneous parallel \ndevices. We present the design and evaluation of Categories and Subject Descriptors D.3.3 [Programming \nLan-the Lime compiler and runtime subsystems speci.c to GPUs. guages]: Language Constructs and Features \nThe Lime language exposes parallelism and computation ex\u00ad plicitly with high level abstractions [2]. \nNotably, the type system General Terms Design, Languages, Performance for these abstractions enforces \nkey invariants regarding isolation and immutability. The optimizing compiler leverages these invari-Keywords \nGPU, OpenCL, Java, Lime, Streaming, Map, Reduce ants to generate ef.cient parallel code for multicores \nand GPUs, without relying on deep program analysis. 1. Introduction This paper shows how a Lime programmer \ncan exploit a GPU In response to increasing challenges with frequency scaling, hard-without writing complex \nlow-level code required with mainstream ware designers have turned to architectures with increasing degrees \napproaches (OpenCL or CUDA). The compiler and runtime system of explicit parallelism. Today s hardware \nofferings range from gen-coordinate to automatically orchestrate communication and com\u00aderal purpose chips \nwith a few cores (e.g., Intel Core i7), to spe-putation, map data to the GPU memory hierarchy, and tune \nthe ker\u00adcialized distributed-memory multiple-SIMD platforms (e.g.,IBM nel code to deliver robust end-to-end \nperformance. The contribu-Cell), to graphics processors (GPUs) that support large-scale data tions of \nthis paper are: parallel computations. Additionally, several efforts underway at- A design and implementation \nof an optimizing compiler to gen\u00adtempt to exploit recon.gurable hardware (FPGAs), with massively erate \nhigh quality GPU code from high-level language abstrac\u00ad bit-parallel execution, for general-purpose computation. \ntions including isolated parallel tasks, communication opera- OpenCL [9]and CUDA [15] have emerged as \nmainstream lan-tors, value types to express immutable data structures, and .ne\u00adguages for programming \nGPUs and multicore systems. These pop-grained map-and-reduce operations (Sections 3-4). ular languages \nprovide APIs that expose low-level details of the de\u00ad A set of automatic optimizations for GPU architectures \nthat vice architecture. The programmer must manually tune low-level may be applied without sophisticated \nalias analysis or data de\u00adcode for a speci.c device in order to fully exploit its processing pendence \nanalysis. These include memory optimizations that resources. For example, a sub-optimal mapping of data \nto a GPU s improve locality, reduce bank con.icts, and permit vectoriza\u00adtion (Section 4.2). A detailed \nempirical performance evaluation comparing the generated code to hand-tuned OpenCL programs. The perfor- \n Permission to make digital or hard copies of all or part of this work for personal or classroom use \nis granted without fee provided that copies are not made or distributed mance of the generated code lies \nbetween 75% to 140% of for pro.t or commercial advantage and that copies bear this notice and the full \ncitation hand-written and tuned native OpenCL code (Section 5). on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. The performance results show that across a suite of 9 bench- PLDI 12, June 11 16, Beijing, China. \nmarks, our compiler delivers a speedup ranging from 12x - 430x Copyright cfor the NVidia GeForce GTX580 \narchitecture (Fermi) and 12x - . 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00  416x for the AMD Radeon \nHD5970. Further, using the OpenCL multicore runtime, we report a performance gain between 4.8x - 32.5x \nfor the Intel Core i7 architecture.  2. OpenCL Background OpenCL and CUDA represent the de facto standards \nfor general purpose programming on GPUs. OpenCL, in particular, was de\u00adsigned to provide an industry-standard \nAPI for systems with het\u00aderogeneous devices. Although OpenCL provides a portable API, the API presents \na low-level interface to the underlying hardware. The OpenCL pro\u00adgrammer must explicitly manage many \nlow-level details to map data to appropriate address spaces, enable vectorization, schedule data transfers \nbetween the host and device, and manage synchro\u00adnization among queues connecting the host and devices. \nIn this section, we brie.y review the structure of a small OpenCL N-Body application. This example motivates \nthe need for high-level abstractions and introduces tuning issues germane to GPU performance. Figure \n1 illustrates an N-Body simulation expressed using a common OpenCL programming pattern. Lines 1-16 embody \na data\u00adparallel kernel that represents a n 2 force calculation. The kernel may run on a GPU or a multicore \nCPU. Address Space Quali.ers The kernel declaration (line 1) in\u00adcludes address space quali.ers that map \nthe respective data to the GPU memory hierarchy. OpenCL presents a non-uniform mem\u00adory hierarchy with \n.ve types of memories: private, local, global, constant and image.The private memory rep\u00adresents a fast, \nsmall memory private to each computational thread. The local memory represents a shared memory, used \nto coher\u00adently share data between a small group of related threads called a work group.The global memory \nprovides a shared address space for all threads on a device, with no implicit coherency guarantees across \nwork groups. The constant and image memories rep\u00adresent specialized read-only storage. The former, typically \na small space, usually holds constants referenced by a computational ker\u00adnel. Graphics applications typically \nstore texture objects for ren\u00addering a scene in the image memory. We inspected a number of OpenCL benchmarks \nand found that the kernels often use private, constant, and global quali.ers in the kernel signatures. \nVectorization The kernel code features the use of the OpenCL float4 data type (lines 5, 6, 12). The code \nrepresents the forces as an array of tuple elements each consisting of four .oating-point values, even \nthough each force value has only three components. This decision allows the device to vectorize the memory \naccesses. OpenCL 1.0 only supports vectors of size 2, 4, 8, and 16. Kernel Tuning The force calculation \nillustrates a typical data parallel pattern where multiple instances (work items)ofthe same kernel operate \non disjoint data sets. Lines 2-4 show some man\u00adual calculations to determine the working set for a running \nwork item. The working set depends on the number of concurrent kernel instances, a user-determined property \nthat is typically determined through trial and error to suit the computational power of the target device. \nFor N-Body, each working set represents the subset of par\u00adticles for which the work item computes forces. \nThe loop at lines 8\u00ad15 iterates through the kernel working set and computes the forces for the corresponding \nparticles. Lines 9, 10, and 15 access local memory and perform synchronizations between kernel instances \nto ensure correct access to local memory. Orchestrating Execution The second half of Figure 1 shows a \nfraction of the host code necessary to orchestrate the execution of the kernel code. A typical execution \npattern applies the following steps: (1) discover and initialize the device and compile the kernel code, \n(2) create a command queue, (3) create the kernel, (4) create read and write buffers, (5) enqueue commands \nto transfer the read buffer, invoke kernel, and transfer the write buffer. The programmer is responsible \nfor scheduling the data transfers and overlapping these with kernel computation. This process uses at \nleast a dozen OpenCL procedures based on our inspection of several hand-tuned OpenCL programs. We omitted \nan additional 182 lines of code which deal with step (1) alone.  3. Lime Programming Language The previous \nexample shows that OpenCL allows .ne-grain control of the host and kernel code, but the low-level details \nimpose a sig\u00adni.cant burden on the programmer. Similar points hold for CUDA. In contrast, Lime [2] provides \na high-level object-oriented language offering task, data, and pipeline parallelism. It extends Java \nwith several constructs designed for programming heterogeneous archi\u00adtectures with GPU and FPGA accelerators. \nHere, we brie.y review the Lime constructs which the compiler relies on to ef.ciently of\u00ad.oad computation \nto a GPU. Figure 2 represents a Lime implementation of the N-Body ex\u00adample. Since the language is Java-compatible \nand interoperable, it provides a gentle migration path from Java. The .gure illustrates this process \nwith white and black squares: white squares pre.x original Java code and black ones pre.x new Lime code. \nLine 3 sets up the main computation, as embodied in a task graph data structure. A task graph is a directed \ngraph of compu\u00adtations, where values .ow between tasks over edges in the graph. The nbody task graph \nfrom line 3 is illustrated in the .gure below. Source Filter Sink .oat[[][4]] .oat[[][3]] positions \nforces A particle generator task emits an initial set of particles, stored as an array of 4-element \ntuples: three elements for the position and one for the mass. A force computation task computes the force \nacting on each particle using a simple n 2 algorithm. This task produces an array of 3-element tuples \nrepresenting the forces acting on each particle. The force accumulator task consumes these forces, and \ncomputes a new position for each particle. The task graph just described represents a single simulation \nstep. Typically the algorithm runs for a large number of simulation steps. Two noteworthy Lime operators \nappear on line 3. First is the task operator, which creates a computational unit equivalent to an OpenCL \nkernel. Second is the => (connect) operator, which rep\u00adresents the .ow of data between tasks. The finish \non line 4 initi\u00adates the computation and forces completion. This is necessary since Lime decouples the \ncreation of task graphs from their execution. 3.1 Task and Connect The task operator is used to create \ntasks (line 3). A task repeat\u00adedly applies a worker method as long as input data is presented to the \ntask via an input port, and enqueues its output (the result of the method application) to an output stream. \nThe operator binds the method speci.ed after the dot to the task worker method. The worker method may \nbe static (NBody.computeForces)or an in\u00adstance method (e.g., NBody().particleGen). In the latter, the \ntask operator creates an instance of the type NBody and binds its instance method to the worker. The \ndistinction between the two cases is that static worker methods are essentially pure functions and the \ninstance methods may be stateful. Methods in lime are task\u00adagnostic, meaning they may be invoked as conventional \nstatic or in\u00ad  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 \n33 34 35 36 update position(positions , forces ); }} Figure 1. Parts of the OpenCL kernel and host \ncode for N-Body stance methods and only become worker methods by applying the task operator. The language \nruntime repeatedly invokes the worker method as long as data items are available on the input port. A \nspecial case is the source task (lines 6-8) that emits data until interrupted by an Underflow Exception \nthat can be thrown by any task to notify that the computation is .nished. Tasks in Lime are either isolated \nor non-isolated. An isolated task, also known as a .lter, has its own address space and may not access \nmutable global state (e.g., non-.nal statics .elds in Java). Lime achieves isolation using a combination \nof local methods, and value types. The worker method of an isolated task must be declared local (lines \n10 and 13). A local method may only call other local meth\u00adods, and may not access global mutable .elds. \nThe worker methods input immutable (value types) arguments (if any) and must return values (if any). \nThis ensures that data exchanged between tasks does not mutate in .ight, and provides the compiler and \nruntime greater opportunities for optimizing communication between tasks without imposing undue burden \non the compiler to infer invariants involving aliasing. A value type represents a deeply immutable object \ntype (e.g., data structure or array) declared using the value modi.er on a type. A value array is indicated \nusing double brackets, so for example float[[][4]] is a two dimensional array of .oats, where the outer \ndimension is unbounded, the inner dimension is bounded to size four, and the entire array is immutable. \nValue arrays must be initialized at construction time (lines 8 and 17). The task operator encapsulates \ncomputation whereas the => (connect) operator encapsulates communication between tasks (line 3). This \noperator is used to connect two tasks when the output type of the upstream task (left of the operator) \nmatches the input type of the downstream task (right of the operator). Lime exposes the communication \nbetween tasks explicitly using the connect op\u00aderator so that the compiler and runtime can optimize the \nI/O and synchronization between tasks automatically and without program\u00admer intervention, in contrast \nto OpenCL (see Figure 1 lines 20-35).  3.2 Map and Reduce Lime also offers a map and reduce model for \n.ne-grained data parallelism. This model suits the thread-level parallelism available in GPUs, and also \nshort-vector SIMD instructions available in many general purpose ISAs. A map operation applies a (logical) \nfunction to each element of some aggregate data structure, producing another aggregate data structure. \nThe reduce operation combines values from an aggregate data structure using a combinator function. These \nabstractions are well-known in traditional functional languages. The map operator is represented by the \n@ token; see line 11 of Figure 2. It applies the function computeParticleForces to each element of the \nparticles array, and returns the resultant array. Each application of the map function computes the force \ninteractions between a particle p and all other particles (lines 13-17). The example omits the core force \ncomputation since it is similar in all implementations (OpenCL, Java and Lime).  1 2 3 4 5 6 7 8 9 10 \n11 12 13 14 15 16 17 18 19 20 21 Figure 2. Lime version of force calculation of NBody. The code marked \nwith . is original Java Source. A reduction in Lime is expressed using an operator or method followed \nby ! to indicate the operator or method should be treated as a combinator. The language permits instance \nor static methods as well as certain arithmetic operators to serve as reduction operators as long as \nthey apply to two arguments of the same type and produce a result of that type.  4. Compilation Methodology \nThe explicit separation of computation and communication in Lime via the task and connect operators relieves \nthe programmer from the burden of orchestrating the execution of tasks between host and device. This \nresponsibility now falls onto the Lime compiler and runtime. Similarly, because Lime programs do not \nforce the programmer to make an explicit distinction between the kernel and host codes, the compiler \nmust determine a partitioning of the program between host (CPU) and device (GPU). Our compilation methodology \nis illustrated in Figure 3.The compiler partitions the source code into host and device code, and compiles \neach partition to native code. The Lime compiler gener\u00adates a mix of Java bytecodes and OpenCL. The bytecodes \nrun in an unmodi.ed Java virtual machine. The generated OpenCL code encompasses both the compiler-tuned \nkernels and the coordina\u00adtion and scheduling code for managing the buffers, scheduling data transfers \nand executing the kernels. 4.1 Kernel Identi.cation In the N-Body example, the main computation workload \nlies in the n 2 force calculation. Thus, a natural partition of the N-Body example runs the particleGen \nand forceAccumulator tasks on the host and the computeForces task on the GPU. To allow of.oading, the \ncompiler requires the computeForces task to be an isolated task, also known as a .lter. The language \nsemantics for a .lter guarantee that it does not perform globally side-effecting operations because it \nis isolated (i.e., local) and its arguments are immutable (i.e., values). As a result, our compiler recognizes \n.lter task creations, and treats each .lter as the unit of computation to of.oad. Note that the system \nmay freely move .lters between device and host without concern for data-races and non-determinism. Within \neach .lter, the compiler scans for map and reduce op\u00aderations to identify opportunities for kernel-level \ndata-parallelism. Figure 3. Starting from a Lime program, our compiler produces the application code \nthat runs in the JVM, the C code that handle data exchange and the calls to the OpenCL API and .nally \nthe OpenCL kernel code. The compiler detects data-parallel maps and generates correspond\u00ading parallel \nimplementations without deep dependence analysis. In\u00adstead, it checks for the following invariants on \nmap expressions: (a) the map function is static and local, and (b) the map function argu\u00adments are value \ntypes (includes primitive types). The type system guarantees that a static local method does not access \nglobally muta\u00adble data, and because the value arguments are immutable, the map function is pure and side-effect \nfree. In a similar way, the compiler may infer a parallel reduction. Following kernel identi.cation, \nthe compiler performs kernel optimization (Section 4.2) with an emphasis on the GPU memory hierarchy. \nLastly, the compiler generates appropriate glue code to orchestrate both the data transfers and the kernel \ninvocations (Section 4.3). The glue code is implied by the connect operator which identi.es the communication \nrequired between host and device. Table 1 summarizes the differences between programming in OpenCL and \ncompiling Lime code for GPUs. The salient observa\u00adtion is that Lime provides a much higher level of abstraction \nand does not expose low-level architectural details.  Table 1. GPU programming in OpenCL vs. Lime. OpenCL \nLime of.oad unit kernel .lter communication API => operator data parallelism manual map &#38; reduce \n memory quali.ers manual compiler synchronization manual compiler scheduling manual compiler  4.2 Kernel \nOptimizations The mechanics of optimizing and generating an OpenCL kernel from a Lime .lter center on \nthe exploitation of map and reduce op\u00aderations for thread and SIMD parallelism, and applying a set of \nlo\u00adcality enhancing optimizations that take advantage of the OpenCL memory organization when applicable \n(e.g., a GPU). Figure 4 shows the code generated by the Lime compiler for the Lime code snippet shown \nearlier. The compiler takes advantages of the semantic information available at the source level to determine \nthat the map operation (Figure 2 line 11) is data-parallel, and it exploits the immutable and bounded-size \nnature of individual particles to perform memory optimizations and vectorization. The generated kernel \ncode will adapt to any number of threads started by the Lime runtime or as requested by the user. In \nFigure 4 line 9, the kernel loop iterates over the array of particles with each thread assigned an element \ni at line 10. This generated code is more robust than the hand-written OpenCL kernels we inspected because \nit executes correctly independent of the number of threads. In addition to the obvious kernel input and \noutput arguments, the compiler also generates a structure to contain runtime book\u00adkeeping information \nand data values needed by the kernel code (see Figure 4(b)). Examples of the latter include array lengths \nthat are used explicitly at line 15 and implicitly at line 11. This record is passed to the kernel as \na parameter on line 2. 4.2.1 Memory Optimizations Once the kernel is identi.ed, a key optimization maps \nnon-scalar (e.g., array) data to the different memory structures in the device. This section describes \nhow the compiler optimizes memory ac\u00adcesses for the OpenCL memory hierarchy (common to GPUs). The compiler \npermits for any of the optimizations to be enabled and disabled so that it is possible to perform an \nautomated exploration of the memory mapping and layout. The compiler drives memory optimizations using \na relatively simple pattern matching algorithm. It scans the intermediate repre\u00adsentation for common \nmemory access idioms and applies the corre\u00adsponding transformation when a pattern is encountered. In \ncontrast to much previous work, our memory optimizer does not require sophisticated alias analysis or \ndata dependence analysis. Instead, our compiler exploits the strong type system in Lime to infer nec\u00adessary \ninvariants without deep analysis. For example, immutable value types in the source language provide the \nkey invariants that memory locations are read only and cannot be reassigned. We claim that enabling a \nrelatively simple memory optimizer is a strength of our approach, as compared to more unconstrained input \nlanguages that necessitate heroic program analysis. Figure 5 illustrates some of the idioms recognized \nby the Lime compiler. In the .gure and the text that follows, a parallel loop corresponds to a data-parallel \nmap operator that the compiler has already inferred. Global Memory Mapping data to the global memory \nis the de\u00adfault behavior of the optimizer when no other mapping is possible.  (b) Generated structure \nfor runtime information Figure 4. Lime to OpenCL code generation Private Memory The compiler attempts \nto map all the arrays that are not shared across threads to the fast private memory. Due to the extremely \nsmall capacity of this type of memory, the compiler only considers arrays whose size can be determined \nstatically and does not exceed a certain threshold value. An array is not shared when it is allocated \nwithin the inner most parallel loop since each thread will execute its own instance of the loop body. \nFigure 5(a-b) show a simple example where an array variable arr is mapped to the private memory in OpenCL. \nLocal Memory In most OpenCL application, the local memory is used as a type of scratch pad for shared \ndata. This memory is the second fastest type of memory. It is typically used when the data is reused \namong several threads running on the same core. A typical example where this happens is in the case of \na double nested loop as showninFigures 5(c-d). The parallel inner most loop reuses the value v. Since \nthis inner loop is parallelized into many threads, it is possible to store parts of the arg array in \nthe local memory and thus increase data reuse. The compiler performs a code transformation similar to \nloop tiling. When the size of the local memory cannot be determined statically our system dynamically \nallocates memory at runtime depending on the number of parallel threads. The local memory is typically \norganized in different banks, with consecutive words assigned to different banks. Once the optimizer \nhas decided to map an array into the local memory it determines whether padding is necessary in order \nto avoid bank con.icts. This is a common optimization although often done manually [17]in OpenCL or CUDA \nkernels. In Lime, it is relatively easier to auto\u00admatically apply this optimization using the type information \navail\u00adable and the fact that the code is virtually free from pointers as opposed to lower-level pointer-rich \nprograms. The Lime compiler  3 3 4 4  (a) Private memory candidate (allocation within parallel region) \n(b) OpenCL using private array allocation } (c) Local memory candidate (nested loop with data reuse) \n... (e) Image memory candidate (accesses to v static) 6 (d) OpenCL using local memory (synchronization \nomitted) ... (f) OpenCL using image memory (g) Constant memory candidate (x invariant in the loop) (h) \nOpenCL using constant memory   Figure 5. Example of code patterns that our optimizer is looking for \nand the corresponding generated OpenCL in pseudo-code. Readers familiar with writing OpenCL code may \nrecognize here some typical code pattern often encountered in OpenCL kernels. detects the size of the \narray elements and adds padding accordingly. This ensures that each consecutive thread reads data from \na differ\u00adent bank, thus increasing memory throughput. Image Memory The compiler tries to map read-only \narrays into the image memory. Since OpenCL 1.0 only supports access to the image memory by groups of \n4 words, the compiler limits the scope to arrays whose last dimension is either 2 or 4. The compiler \nadopts a packed representation in the case where the last array dimension is of length 2. In addition, \nit prevents the optimizer from assigning to image memory arrays whose last dimensions elements are not \naccessed contemporaneously, in order to ensure good performance. Figure 5(e-f) shows a typical candidate \n(arg) for this optimization. The Lime array access expressions are converted into the appropri\u00adate OpenCL \nimage access functions. Since the OpenCL does not support 1D images, the compiler maps the index x to \nthe 2D coor\u00addinate (x,0) 1. Constant Memory The constant memory is reserved for values that are broadcast \nto all the threads. That is, all the threads read the same address. In this case, the compiler identi.es \narray accesses within a parallel loop that are accessed using a loop-invariant index as shown in Figure \n5(g-h). 1 The compiler implementation is more complex since the compiler may perform modulo operations \nwhen the index is greater than the maximum width supported by the image format.  4.2.2 Vectorization \nFollowing the memory optimizations, the compiler vectorizes memory accesses for multidimensional arrays. \nAn innermost array dimension is a vectorization candidate if it is of length 2, 4, 8 or 16. This optimization \nis only applied for arrays that are read-only and whose access to the last dimension is known statically. \nVectorizing the memory accesses usually reduces the total number of memory accesses and thus improves \nbandwidth utilization and performance. This optimization is applied for data mapped to the global, local \nor constant memory (the image memory optimization is intrinsically already vectorized). Once again, the \nbene.t of using a high-level language that allows for pointer-free programming makes this type of analysis \nsimple.  4.3 Orchestrating Communication Although the data and code isolation of Lime tasks makes compu\u00adtation \nof.oading possible, the runtime system must still ef.ciently transfer data to and from the main system \nmemory to the device. Because Lime targets devices that include GPUs and FPGAs, the runtime implementation \nadopts a universal wire format that relies only on sending a byte stream as shown in Figure 6. The communication \nsteps between the host JVM and the na\u00adtive device entail (1) serializing a Lime value to a byte array, \n(2) crossing the JNI boundary, and (3) converting this byte array into a C-style value. The particular \nrepresentation of a value for use in OpenCL is speci.c to our code generator; the C deserializer does \nnot necessarily convert to a standard C format. The return path is a mirror image in which we convert \nthe OpenCL data structure to a  Lime Task Figure 6. Data transfer between Java and the OpenCL device. \nThis example shows a task that takes a .oat array as input and returns an int array. byte array, return \nfrom the JNI call, and then deserialize from the byte array back into a heap-resident Lime value. If \nnot optimized, high communication costs can cancel out any performance gains from exploiting a GPU. Our \ninitial implementa\u00adtion was simple and used Lime s internal runtime type informa\u00adtion to serialize and \ndeserialize. Unfortunately, the performance was so poor that more than 90% of the time was spent marshaling \ndata to or from a byte array. Performance was greatly improved by writing custom serialization routines \nfor the most common types primitives and (nested) arrays of primitives. During the initializa\u00adtion process \nof migrating a task to a native device, the runtime will .nd a custom serializer based on the data type. \nBecause the default marshaller is written recursively, we modi.ed it to use a specialized marshaller \nrecursively when available. For instance, if the data type is a tuple of integer arrays, then although \nthere is no specialization at the tuple level, the lowest level integer arrays (where most of the data \nactually is) will still be optimized. Finally, because Lime ar\u00adrays can express bounds (e.g., sub-rectangular \narrays are possible), the runtime system can sometimes determine the exact size of the target byte array \nup-front. Marshaling on the C side is similar but more specialized. Be\u00adcause our OpenCL backend only \nhandles rectangular arrays of primitives, the data is generally densely packed. The layout must take \nalignment and vectorization into consideration, making the marshaller more specialized though less comprehensive. \nBecause the serialization is primarily memory-bound, we simply use mal\u00adloc/free rather than implement \nour own memory manager to lower costs. Our current communication implementation could be further optimized \nsince it entails repeated serialization in the same address space. However, our current design affords \na common format as a starting point for a communication subsystem that supports het\u00aderogeneous devices. \nOne might further optimize the protocol by creating speci.c communication channels so that the sender \nand receiver are aware of the data format the other party desires. Going even further, one might be able \nto avoid a low-level memory copy by pinning memory pages and managing memory explicitly. How\u00adever, these \nchanges come at the cost of OS and JVM portability.  5. Evaluation We present an empirical evaluation \nof the Lime system to answer three questions: 1. End-to-end Speedup. Can the Lime programmer effectively \nexploit a GPU to improve performance? That is, can the system deliver high performance, including all \ncommunication costs and runtime overhead? 2. Comparison to hand-tuned OpenCL. What is the quality of \nthe OpenCL code generated by the Lime compiler as compared to hand-tuned native OpenCL implementations? \n 3. Computation vs. Communication. How much overhead does the system introduce to communicate between \nthe host and device?  Table 3 reports the set of benchmarks used in this study. The set includes three \nbenchmarks from Parboil [1] and two benchmarks from JavaGrande (JG) [13]; we selected the benchmarks \nthat were easiest to port to Lime. We expect other benchmarks which can be expressed using task graphs \nand map and reduce to bene.t in the same way. In addition we include two benchmarks, N-Body and Mosaic, \nthat we wrote from scratch. Previous sections reviewed N-Body in detail. Mosaic features a map-and-reduce \nalgorithm to compare tiles from a reference image to tiles from an image library to .nd the best-matched \ntiles using a scoring function. Some of the benchmarks predominantly exercise .oating-point arithmetic. \nALUs in modern processors perform .oating-point arithmetic in at least double-precision, whereas GPU \nALU building blocks are single-precision. For GPUs, single-precision operations run faster than double-precision \nones. Because this paper focuses on compilation rather than numerical stability issues, we present re\u00adsults \nfor both single-and double-precision variants in cases where precision strongly affects performance. \nTable 2 lists the hardware platforms evaluated. We measure per\u00adformance on four platforms. In each case, \nLime tasks are compiled to OpenCL and run natively using the OpenCL runtime, while the remaining application \ncode runs in bytecode. The Intel Core i7 sys\u00adtem runs 64-bit Ubuntu Linux 10.10 with the 2.6.35-28 kernel. \nThe NVidia cards represent two generations of GPU architectures: a re\u00adcent GeForce GTX580 (Fermi) and \na 2006 GeForce GTX8800. The latter is used to compare the Parboil benchmarks since they are speci.cally \nhand-optimized for this card [17]. The GeForce archi\u00adtecture evolved substantially between these generations. \nNotably, the Fermi architecture adds caches in addition to the local mem\u00adory. The NVidia GPUs use CUDA \n4.0.13 with device driver ver\u00adsion 270.40. The AMD GPU uses driver version 11.9 and AMD OpenCL SDK 2.5-RC2. \n 5.1 End-to-end Speedup Figure 7 shows the bottom-line, end-to-end performance results including all \nsystem overheads. The .gure represents performance results of the Lime code compiled to OpenCL, running \npartially in bytecode and partially in the native OpenCL runtime for the CPU (top) and the GPU (bottom). \nThe .gure reports speedup based on wall-clock execution times, measured after a preliminary warmup phase \nto ensure JIT optimizations occur. The .gure normalizes speed as compared to Lime code running entirely \nin bytecode. The baseline Lime bytecode performance achieves 95-98% of the performance of the original \npure Java implementations for N-Body, Mosaic and JG-Series. In the worst case, the performance of JG-Crypt \nis half as fast when running Lime compiled to bytecode, as compared to the original Java. This slowdown \nis an artifact of our methodology in porting from Java, since we only ported the dominant computational \nkernels to Lime. As a result, the Java to Lime interoperability introduces some overhead with respect \nto array conversion, and further the cost of byte-array accesses in Lime are more expensive than in Java. \nHowever, note that the acceleration gained by compiling the kernels to OpenCL more than compensates for \nthe slowdown due to this interoperation. Since the end-to-end measurements include both computation and \ncommunication costs, we only show the results for the Core i7, the faster NVidia GTX580, and the AMD \nHD5970. This is because the overheads are proportionately larger with greater acceleration.  Table 2. \nEvaluation platforms. Note, the number of GPU cores represents the number of Streaming Multiprocessors. \nType Model Cores FP units per core Const. mem Local mem L1 cache L2 cache L3 cache CPU Intel Core i7-990X \n6 4 single (4 double) 6x64KB 6x256KB 12MB GPU NVidia GeForce GTX 8800 16 8 single 64KB 16x16KB GPU NVidia \nGeForce GTX 580 16 32 single (16 double) 64KB 16x48KB 16x16KB 768KB GPU AMD Radeon HD 5970 20 80 single \n64KB 20x32KB Table 3. Benchmarks used in the evaluation. Name Description Input size Output size Data \nType N-Body N-Body simulation 64KB / 128KB 48KB / 128KB Float / Double Mosaic Mosaic image application \n600KB 5MB Integer Parboil-CP Coulombic Potential 62KB 1MB Float Parboil-MRIQ Magnetic Resonance Imaging \n432KB 256KB Float Parboil-RPES Rys Polynomial Equation Solver 13MB 4MB Float JG-Crypt IDEA encryption \n3MB 3MB Byte JG-Series Fourier coef.cient analysis 780KB / 1560KB 780KB / 1560KB Float / Double  N-Body \n(Single) Mosaic Parboil-MRIQ JG-Crypt JG-Series (Double) N-Body (Double) Parboil-CP Parboil-RPES JG-Series \n(Single) (a) CPU (Core i7) N-Body (Single) Mosaic Parboil-MRIQ JG-Crypt JG-Series (Double) N-Body (Double) \nParboil-CP Parboil-RPES JG-Series (Single) (b) GPU Figure 7. End-to-end speedup (includes overhead). \nFigure 7(a) shows the speedups (higher is better) when running on 1 or 6 cores. Since the CPU supports \nhyperthreading, running on a single core runs two threads, one each for the JVM and OpenCL kernel. The \n1-core performance is generally the same as the baseline, with a 10% degradation in the worst case because \nof high marshaling costs (see Figure 9). The gains for Parboil-MRIQ, Parboil-RPES, and JG-Series result \nfrom a faster implementation of the transcendental functions in OpenCL compared to Java. The performance \nscales as the number of cores is increased, with .ve benchmarks showing a speedup of 4.8 - 5.7x. The \nfour remaining benchmarks show super-linear speedups of 13.6-32.5x using 6-cores. This is attributed \nto hyper-threading (permitting two OpenCL threads to run per core) and cache effects. Figure 7(b) shows \nthe speedups resulting from co-execution between the JVM and the GPU. The speedups vary signi.cantly \ndepending on the benchmark and platform, ranging from 12x to 431x. The benchmarks which do not use .oating-point \n(JG-Crypt and Mosaic), or which use simple .oating-point operations (N-Body) see the lowest end-to-end \nspeedups. These benchmarks also have high communication to computation ratios, as shown in later results. \nThe largest performance gains manifest for applications using transcendental functions. The results also \nshow that double-precision computation on the GTX580 is approximately 2 -3x slower than single-precision, \nand 1.5x slower on the HD5970. Overall, the results demonstrate that the Lime system delivers substantial \nend-to-end speedups as compared to the original Java programs, for all the benchmarks and platforms considered. \n 5.2 Comparison to hand-tuned OpenCL Next, we evaluate the code quality of the generated OpenCL code, \nas well as the different memory optimizations described earlier. For this purpose, we wrote and hand-tuned \nOpenCL versions of N-Body and Mosaic, and converted three existing Parboil benchmarks originally written \nin CUDA to OpenCL. We made our best effort to optimize N-Body and Mosaic for the GTX580 GPU. We also \ninclude results for the GTX8800 GPU because the Parboil bench\u00admarks were optimized speci.cally for this \ncard by another research group [17]. OpenCL requires the programmer to select the number of threads to \nrun and how these threads map to cores. These tuning parameters can have a strong impact on performance. \nTo control for these variables, we conducted an exhaustive systematic of.ine exploration of the tuning \nparameters and use the best settings for each experiment. For example, the hand-tuned versions of the \nPar\u00adboil benchmarks are optimized for the GTX8800, but those settings are not competitive on the GTX580. \nA system could perform this auto-tuning automatically ahead of time or at runtime, but such tuning falls \noutside the scope of this paper. To evaluate code quality, we measure only time spent in com\u00adputational \nkernels on the GPU, and exclude time spent on the host and time spent in explicit communication between \nhost and GPU. Figure 8 shows the relative performance of computational kernels for compiled Lime code \nas compared to hand-tuned OpenCL. A speedup greater than one indicates performance better than hand\u00adtuned \ncode. A speedup less than one indicates a slow down and an opportunity for further improvements in the \nLime compiler. For each benchmark, the Figure shows results using the vari\u00adous memory optimizations applied \nby the Lime compiler including  Global Local Texture Global+Vector Local+Conflicts removed Constant \nLocal+Conflicts removed+Vector Constant+Vector Speedup relativ e to hand-tuned Speedup relativ e to \nhand-tuned Speedup relativ e to hand-tuned 0.0 0.5 1.0 1.5 0.0 0.5 1.0 1.5 0.5 1.0  N-Body Parboil-CP \nParboil-RPES Mosaic Parboil-MRIQ (a) NVidia GTX8800 N-Body Parboil-CP Parboil-RPES Mosaic Parboil-MRIQ \n (b) NVidia GTX580 (Fermi) N-Body Parboil-CP Parboil-RPES Mosaic Parboil-MRIQ (c) AMD Radeon HD5970 \nFigure 8. Lime vs. hand-tuned OpenCL kernel-times and effects of optimizations. vectorization. The Figure \nshows 8 bars per benchmark, represent\u00ading each of the memory optimizations covered in Sections 4.2. \nOverall, the results show that with the best optimization choices, the Lime compiler delivers competitive \nperformance, attaining be\u00adtween 75% and 140% of the hand-coded performance. Exceed\u00ading hand-coded performance \nindicates cases where the human pro\u00adgrammer was imperfect we discuss speci.c issues below. The results \nshow that using the global memory generally yields the worst performance, even when using vectorization. \nIn the worse cases, the slowdown is up to 10x compared to hand-tuned for the GTX8800, up to 60% for the \nHD5970, and 20% for the GTX580. On the other hand, the compiler can often use the local memo\u00adries effectively. \nNote in particular that the compiled code surpris\u00adingly outperforms the hand-tuned versions for the Mosaic \nbench\u00admark. After further investigation, we discovered that the compiled code is more effective at reducing \nmemory bank con.icts. The compiler-generated code for Parboil-MRIQ also slightly outper\u00adforms the hand-tuned \nkernel, when using constant memory. The Parboil-RPES benchmark bene.ts signi.cantly from the use of tex\u00adture \nmemory on the GTX8800 because it is equipped with a hard\u00adware cache, and this benchmark exhibits good \nspatial locality. JG-Series (Double) JG-Series (Single) JG-Crypt Parboil-RPES Parboil-MRIQ Parboil-CP \nMosaic N-Body (Double) N-Body (Single) 0 20 % of total execution time 40 60 80 100 (a) CPU (Core i7) \n  (b) GPU (GTX580) Figure 9. Computation and communication costs. The GTX580 architecture differs from \nthe other GPUs by plac\u00ading a cache between the device memory and the cores. As a di\u00adrect consequence, \nthe performance is less sensitive to memory opti\u00admizations, as shown in Figure 8(b). Using global memory \ndelivers performance relatively close to the hand-tuned version, however, optimizations are necessary \nto recover the last 10-20% of perfor\u00admance in some benchmarks. We conclude that it is possible for a \nLime compiler to achieve performance competitive with typical hand-tuned code for the plat\u00adforms and \nbenchmarks considered. The results also demonstrate the sensitivity of performance to the GPU memory \narchitecture. We claim these results further demonstrate the need to lift the level of programming abstraction \naway from low-level GPU de\u00adtails. Clearly, writing a portable, high-performance OpenCL code for multiple \ndevices imposes a substantial burden on a human pro\u00adgrammer.  5.3 Communication vs. Computation The \nend-to-end speedups shown earlier include all runtime over\u00adhead, including communication between host \nand device. In our system, of.oading the computation involves moving the data from Java to C and also \nfrom C back to Java (refer to Figure 6). In addi\u00adtion, there are costs attributed to the OpenCL API, \nand PCIe trans\u00adfer costs to move data from the host to the device. Figure 9 shows the breakdown of computation \n(kernel time) and communication costs (everything else). When running on a multicore, shared memory obviates \nthe need for memory transfers; as a rule, computation dominates the execution time (Figure 9(a)). JG-Crypt \nprovides an exception to the rule, since its computation ratio per byte is particularly low. In contrast, \nthe communication costs on a GPU are relatively higher due to its greater computational power. We show \nthe results for the GTX580 in Figure 9(b); the computation to communication ratios are comparable for \nthe other GPUs. Most of the overhead (30%) comes from data marshaling (both Java and C). Marshaling objects \nin Java suffers from signi.cant overheads due to arrays bounds checking and object allocation. Setting \nup OpenCL data structures is relatively fast (typically 5%), except for JG-RPES (40%). We are investigating \nthe cause of this anomaly.  The raw data transfer from host memory to device memory does not play a \nmajor role in communication costs. We expect this trend to continue with PCIe3.0 and tighter integration \nof GPU and CPU. Overall, the combined overhead due to all communication aver\u00adages 40%. Although this \noverhead is high, the tremendous com\u00adputational power of the GPU still allows impressive end-to-end speedups. \nWe conclude that communication costs, while not yet crippling, leave much room for improvement. In future \nwork, we plan to pur\u00adsue various strategies to reduce communication overhead. To avoid extraneous copying, \nthe Java marshaling code should marshal di\u00adrectly to a format as required for device memory. This would \nap\u00adproximately halve the marshaling overhead. More generally, the communication costs can be hidden by \nwell-known pipelining tech\u00adniques that overlap communication and computation; these tech\u00adniques lie beyond \nthe scope of this paper.  6. Related Work Recent years have seen many projects targeting general purpose \nlanguages to exploit multicores and GPUs. The closest related work is Sponge [7], a compiler which generates \nCUDA code from the StreamIt [19] programming language. Udupa et al. [20]also target StreamIt for GPUs. \nThis work focuses on the problem of scheduling the tasks to the GPUs. Similar to our work, Sponge schedules \ndifferent tasks onto GPUs and optimizes the mapping to the different type of memory. In contrast to Sponge, \nour system generates OpenCL code which can target multicore platforms as well as GPUs. Sponge supports \nonly coarse-grained parallelism, whereas Lime includes constructs that support .ne-grained data parallel \nas well. Data parallel operations make it easier to exploit thousands of threads on a GPU. Further, the \nStreamIt programming model is much simpler compared to Lime as it does not permit object allocation or \nunbounded arrays, requires the task graph to be fully resolved at compile time, and does not support \nobject-oriented programming features. Lime on the other hand does not have any of these limitations. \nFurther, because Lime is Java-compatible, it permits a gentle migration of existing Java code. 6.1 GPU \nProgramming and Optimization There are many other task-oriented languages that are suitable for GPU programming. \nCg [12] was among the .rst languages to be developed to program GPUs. Then Brook [3] was intro\u00adduced \nfeaturing the use of a streaming programming model. Ac\u00adcelerator [18] was later developed as a C# API \nlibrary that uses a data-parallel model based on parallel array to program GPUs. Today, general purpose \ncomputations on GPUs is dominated by OpenCL [9]and CUDA [15]. Some researchers are investigat\u00ading automatically \ntranslating OpenMP source code to CUDA [10] while others apply directives to sequential C code to convert \nthem into CUDA programs with the hiCUDA [6] framework. Earlier parts of this paper address the primary \ndifferences between Lime and OpenCL. These advantages are the same compared to CUDA. Another related \nnew language is the IBM X10 language. It pro\u00advides abstractions for programming distributed memory parallel \ncomputers (e.g., clusters) with a globally shared, partitioned ad\u00address space. X10 considers the GPU \nas a shared-memory parallel computer (X10 place ), where threads ( activities ) communicate and synchronize \nthrough shared memory. Their work [4] to incor\u00adporate CUDA abstractions does not describe compiler optimiza\u00adtions \nto map data structures to the GPU memory hierarchy. Instead it provides language constructs for the programmer \nto manage this mapping explicitly. Yang et al. [23] contribute a compiler framework to optimize GPU code. \nThis compiler takes a simple unoptimized kernel as input and applies optimizations as discussed in [17]. \nSimilarly, CUDA-Lite [21] coalesces memory accesses of existing kernels. A compiler for a high level \nlanguage (HLL) can apply similar opti\u00admizations after translating to a low level representation. However, \na HLL compiler can perform more aggressive transformations by ex\u00adploiting higher-level semantic information \nembodied in the source code. We demonstrated several techniques whereby the Lime com\u00adpiler exploits high-level \ninformation exposed by the type system to realize aggressive parallelization, prove isolation, and optimize \nthe mapping to the memory hierarchy. It is unclear whether a low-level approach can in general recover \nthis level of semantic information, due to dif.culties inherent to sound whole program static analysis \nof object-oriented languages. 6.2 Multicores Programming Gordon et al. [5] developed a compiler that \nmaps the StreamIt language to multicore architectures. Intel s array building block (ArBB) [14] consists \nof a virtual machine and a C++ API that de.nes new parallel types such as collections. These collections \nare treated like values and the JIT optimizes these and extract thread and vector (SIMD) parallelism. \nOur work differs in that we start directly with a streaming computational model, making it easier to \ndecompose programs for heterogeneous platforms. In contrast to ArBB, where the programmer has to deal \nspeci.cally with data transformation between the C data types and the parallel collections (using the \nbind function), the programmer simply uses the standard array types provided by Lime. Finally, our system \nworks with GPUs as well as multicores without changes to the program. 6.3 Runtime for Heterogeneous \nPlatforms SoCC [16] is an extension to C that allows the programmer to man\u00adage distributed memory, express \npipeline parallelism and map the different tasks to resources. EXOCHI [22] is an effort from In\u00adtel that \nfocuses on providing a runtime for integrating accelerators with general purpose processor. It provides \nshared memory and dy\u00adnamic mapping of tasks to accelerators. The Quilin [11]system is composed of a C \nAPI that is used to write parallel programs and an adaptable runtime that dynamically maps computations \nto pro\u00adcessing elements in a CPU+GPU system. Jablin et al. [8] proposed a new runtime management system \nthat frees the programmer from explicitly managing data movement between the CPU and GPU on the host \nside. It determines which data are required by a GPU kernel and also copies the data to the GPU memory. \nThe Lime model intrinsically provides this functionality via the task graph, and our compilation methodology \nleverages the language seman\u00adtics and type system to automatically partition the code between host and \ndevice, generate the corresponding code, and coordinate the overall execution without programmer intervention. \n  7. Conclusion This paper reviewed how a compiler for Lime, a high-level Java\u00adcompatible language, \ncan exploit computational resources on a GPU. Exploiting invariants enforced by the type system, the \ncom\u00adpiler and runtime system implement transformations to exploit massively parallel GPU devices with \nnon-uniform memory hier\u00adarchies. Bene.ting from language and compiler co-design, the sys\u00adtem achieves \nthese goals without ambitious program analysis. Ex\u00adperimental results show that for the cases considered, \nthe system delivers impressive speedups as compared to a Java implementa\u00adtion, and generates code quality \nin the same ballpark as hand-tuned code. Although this paper has focused on GPUs, Lime supports a va\u00adriety \nof architectures including specialized multicores and FPGAs. The results from this paper indicate that \nthe Lime approach remains promising for GPUs, as part of a larger vision for programming heterogeneous \nsystem. We remain encouraged that this language\u00adbased approach may help bring the computational power \nof hetero\u00adgeneous architectures to mainstream programmers.  Acknowledgments We thank Joshua Auerbach \nand the members of the Liquid Metal team who contributed infrastructure and a stimulating working environment. \nChristophe Dubach was partially supported by the Royal Academy of Engineering and EPSRC.  References \n[1] Parboil Benchmark Suite. http://impact.crhc.illinois.edu/parboil.php, 2011. [2] J. Auerbach, D. F. \nBacon, P. Cheng, and R. Rabbah. Lime: a Java\u00adcompatible and synthesizable language for heterogeneous \narchitec\u00adtures. In OOPSLA, 2010. [3] I. Buck, T. Foley, D. Horn, J. Sugerman, K. Fatahalian, M. Houston, \nand P. Hanrahan. Brook for GPUs: stream computing on graphics hardware. In SIGGRAPH, 2004. [4] D. Cunningham, \nR. Bordewekar, and V. Saraswat. GPU programming in a high level language: Compiling X10 to CUDA. In X10 \nWorksop, 2011. [5] M. I. Gordon, W. Thies, and S. Amarasinghe. Exploiting coarse\u00adgrained task, data, \nand pipeline parallelism in stream programs. In ASPLOS, 2006. [6] T. D. Han and T. S. Abdelrahman. hiCUDA: \nHigh-level GPGPU programming. IEEE Trans. Parallel Distrib. Syst., 22, Jan 2011. [7] A. H. Hormati, M. \nSamadi, M. Woh, T. Mudge, and S. Mahlke. Sponge: portable stream programming on graphics engines. In \nAS-PLOS, 2011. [8] T. B. Jablin, P. Prabhu, J. A. Jablin, N. P. Johnson, S. R. Beard, and D. I. August. \nAutomatic CPU-GPU communication management and optimization. In PLDI, 2011. [9] Khronos OpenCL Working \nGroup. The OpenCL Speci.cation. [10] S. Lee, S.-J. Min, and R. Eigenmann. OpenMP to GPGPU: a compiler \nframework for automatic translation and optimization. In PPoPP, 2009. [11] C.-K. Luk, S. Hong, and H. \nKim. Qilin: exploiting parallelism on heterogeneous multiprocessors with adaptive mapping. In MICRO, \n2009. [12] W. R. Mark, R. S. Glanville, K. Akeley, and M. J. Kilgard. Cg: a system for programming graphics \nhardware in a C-like language. In SIGGRAPH, 2003. [13] J. A. Mathew, P. D. Coddington, and K. A. Hawick. \nAnalysis and development of Java Grande benchmarks. In Proceedings of the ACM 1999 conference on Java \nGrande, JAVA 99, pp. 72 80, New York, NY, USA, 1999. ACM. [14] C. Newburn, B. So, Z. Liu, M. McCool, \nA. Ghuloum, S. Toit, Z. G. Wang, Z. H. Du, Y. Chen, G. Wu, P. Guo, Z. Liu, and D. Zhang. Intel s Array \nBuilding Blocks: A retargetable, dynamic compiler and embedded language. In CGO, 2011. [15] NVIDIA Corporation. \nThe CUDA Speci.cation. [16] A. D. Reid, K. Flautner, E. Grimley-Evans, and Y. Lin. SoC-C: ef.\u00adcient programming \nabstractions for heterogeneous multicore systems on chip. In CASES, 2008. [17] S. Ryoo, C. I. Rodrigues, \nS. S. Baghsorkhi, S. S. Stone, D. B. Kirk, and W.-m. W. Hwu. Optimization principles and application \nperformance evaluation of a multithreaded GPU using CUDA. In PPoPP, 2008. [18] D. Tarditi, S. Puri, and \nJ. Oglesby. Accelerator: using data parallelism to program GPUs for general-purpose uses. In ASPLOS, \n2006. [19] W. Thies, M. Karczmarek, and S. P. Amarasinghe. StreamIt: A lan\u00adguage for streaming applications. \nIn CC, 2002. [20] A. Udupa, R. Govindarajan, and M. J. Thazhuthaveetil. Software pipelined execution \nof stream programs on GPUs. In CGO, 2009. [21] S.-Z. Ueng, M. Lathara, S. S. Baghsorkhi, and W.-M. W. \nHwu. Lan\u00adguages and compilers for parallel computing. In LCPC, 2008. [22] P. H. Wang, J. D. Collins, \nG. N. Chinya, H. Jiang, X. Tian, M. Girkar, N. Y. Yang, G.-Y. Lueh, and H. Wang. EXOCHI: architecture \nand pro\u00adgramming environment for a heterogeneous multi-core multithreaded system. In PLDI, 2007. [23] \nY. Yang, P. Xiang, J. Kong, and H. Zhou. A GPGPU compiler for memory optimization and parallelism management. \nIn PLDI, 2010.   \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Languages such as OpenCL and CUDA offer a standard interface for general-purpose programming of GPUs. However, with these languages, programmers must explicitly manage numerous low-level details involving communication and synchronization. This burden makes programming GPUs difficult and error-prone, rendering these powerful devices inaccessible to most programmers.</p> <p>We desire a higher-level programming model that makes GPUs more accessible while also effectively exploiting their computational power. This paper presents features of Lime, a new Java-compatible language targeting heterogeneous systems, that allow an optimizing compiler to generate high quality GPU code. The key insight is that the language type system enforces isolation and immutability invariants that allow the compiler to optimize for a GPU without heroic compiler analysis.</p> <p>Our compiler attains GPU speedups between 75% and 140% of the performance of native OpenCL code.</p>", "authors": [{"name": "Christophe Dubach", "author_profile_id": "81319491350", "affiliation": "University of Edinburgh, Edinburgh, Scotland Uk &#38; IBM Research, NY, USA", "person_id": "P3471125", "email_address": "christophe.dubach@ed.ac.uk", "orcid_id": ""}, {"name": "Perry Cheng", "author_profile_id": "81451593218", "affiliation": "IBM Research, NY, USA", "person_id": "P3471126", "email_address": "perry@us.ibm.com", "orcid_id": ""}, {"name": "Rodric Rabbah", "author_profile_id": "81100434259", "affiliation": "IBM Research, NY, USA", "person_id": "P3471127", "email_address": "rabbah@us.ibm.com", "orcid_id": ""}, {"name": "David F. Bacon", "author_profile_id": "81100628167", "affiliation": "IBM Research, NY, USA", "person_id": "P3471128", "email_address": "dfb@us.ibm.com", "orcid_id": ""}, {"name": "Stephen J. Fink", "author_profile_id": "81100118324", "affiliation": "IBM Research, NY, USA", "person_id": "P3471129", "email_address": "sjfink@us.ibm.com", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254066", "year": "2012", "article_id": "2254066", "conference": "PLDI", "title": "Compiling a high-level language for GPUs: (via language support for architectures and compilers)", "url": "http://dl.acm.org/citation.cfm?id=2254066"}