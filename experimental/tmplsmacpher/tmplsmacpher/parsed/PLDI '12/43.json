{"article_publication_date": "06-11-2012", "fulltext": "\n Polyhedra Scanning Revisited Chun Chen University ofUtah Abstract ThispaperpresentsanewpolyhedrascanningsystemcalledCode-Gen+ \nto address the challenge of generating high-performance code for complex iteration spaces resulting from \ncompiler opti\u00admization and autotuning systems. The strength of our approach lies in two new algorithms. \nFirst, a loop overhead removal algo\u00adrithm provides precise control of trade-offs between loop over\u00adhead \nand code size based on actual loop nesting depth. Second, an if-statement simpli.cation algorithm further \nreduces the num\u00adber of comparisons in the code. These algorithms combined with the expressive power of \nPresburger arithmetic enable CodeGen+ to support complex optimization strategies expressed in iteration \nspaces. We compare with the state-of-the-art polyhedra scanning tool CLooG on .ve loop nest computations, \ndemonstrating that CodeGen+ generates code thatis simpler andup to1.15xfaster. Categories and Subject \nDescriptors D.3.4 [Processors]: Code generation, Compilers,Optimization General Terms Performance, Algorithms \nKeywords polyhedra scanning, polyhedraltransformations 1. Introduction The polyhedral model has long \nbeen considered a powerful tool in loop nest optimization. In contrast, ad-hoc loop transformation techniques \ntypically apply one loop transformation at a time and have to switch back to a loop s syntactic form \nagain for the next loop transformation. Subsequent transformations have to deal with increasingly complex \nloop structures, severely limiting the opti\u00admization strategy a compiler can apply. A polyhedral model \nrep\u00adresentseachstatement sexecutionintheloopnestasalatticepoint in the space constrained by loop bounds, \nknown as the iteration space. Then a loop transformation can be simply viewed as map\u00adping from one iteration \nspace to another, and various transforma\u00adtions can be composed. This enables reasoning about complex \nop\u00adtimization strategies. In a polyhedral model, polyhedra scanning is used to generate optimized code. \nThe polyhedra representing the iteration spaces of an optimized loop nest are scanned from the .rst dimension \nto the lasttogeneratethecorrespondingoutputnestedloopstructure.The quality of generated code directly \naffects the transformed code s performance. For example, unnecessary control .ow in an inner- Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed forpro.torcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requiresprior \nspeci.c permissionand/or a fee. most loop would most likely severely degrade performance on to\u00adday s \narchitectures. Expensive arithmetic operations such as mod\u00adulo that result from polyhedra scanning can \nalso degrade perfor\u00admance. Polyhedra scanning has been studied by researchers since the early 1990s. \nGenerating code from unimodular and non\u00adunimodulartransformations[6,14,20,28 30]canbethoughtofas a polyhedra \nscanning problem. Ancourt and Irigoin [1] are among the .rst researchers to propose an algorithm to systematically \nscan polyhedra to generate loop code. However this method only deals withasinglepolyhedron.Inageneralpolyhedralframework,state\u00admentsmayhavedifferentiterationspacesfromeachother. \nKellyet al.[12]describeapolyhedrascanningsystemforasetofpolyhedra. Theiralgorithmsremoveloopoverheadfrominnerloopsiteratively \nfrom the most compact code generated. Quillere et al. [18] argue thattheiterative algorithms in [12] \nareinef.cient, andinsteadsplit overlapping polyhedra at each dimension during the scanning pro\u00adcess without \niterating. However, the method in [18] still requires backtracking to remove dead code. In practice, \nremovingoverhead indiscriminately might unnecessarily cause code explosion with little bene.t in performance. \nThis might be a particular concern when generating codes for embedded systems where limiting code sizeisalso \nimportant.Sometimesremoving overheadfromjustthe innermost loop may be a better choice in balancing code \nsize and performance, as it is typically the structure of the inner loop body that directlyaffectsinstruction \nscheduling. Morerecently,CLooG[2,26]triestoovercometheabovemen\u00adtioned problem in Quillere et al. s method. \nConceptually, its algo\u00adrithm can be thought of as the reverse of Kelly et al. s method by reducing code \nsize from a starting point of maximal overhead re\u00admoval. However, we observe that CLooG s code compacting \npro\u00adcess does not follow the lexicographical order of iteration spaces. One could argue that this reordering \nmight be bene.cial to pro\u00adviding .exibility in overhead removal, but it also might result in incorrect \ncode when there is a data dependence preventing such statement reordering. CodeGen+usesthesamebasicconceptofKellyetal. \nsmethod, but its algorithms are completely reworked. Our goal is to pro\u00adduce high-performance code that \nis competitive with manually\u00adtuned code, so ef.ciency of generated code is paramount. As con\u00adtext for \nthis work, CodeGen+ has been used extensively as part of an autotuning compiler system, whereby the compiler \ngenerates a set of parameterized variants of a computation, and employs em\u00adpirical techniques to .nd \nthe best variant and associated set of op\u00adtimization parameter values [25]. The variants represent different \noptimization strategies, while the optimization parameters are dis\u00adcrete values that govern code generation \nsuch as tile size or un\u00ad *This research was funded in part by the U.S. Government. The views and conclusions \ncontained in this document are those of the authors and should PLDI 12, June11 16,2012,Beijing,China. \nnot be interpreted as representing the of.cial policies, either expressed or Copyright cimplied, of the \nU.S. Government. . 2012ACM 978-1-4503-1205-9/12/06...$10.00  roll factor. Due to its mathematical foundations, \nthe polyhedral model is a powerfulunderlying technology foran autotuning com\u00adpiler because it can generate \ncorrect code under different param\u00adeter values and complex optimization strategies. The code result\u00ading \nfrom this system has been shown to achieve performance that is comparable and sometimes better than manually-tuned \nlibraries suchasATLAS,Goto,ACML,CrayScienti.cLibrary,PETScand CUBLAS [3, 13, 19, 21, 23], and manually-tuned \ncode written by programmers targeting supercomputers [19, 22, 24]. Considering the optimization strategies \nused in hand-tuned codes such as, for example, BLAS linear algebra libraries, to achieve comparable performance \npolyhedral frameworks must be abletocomposealargecollectionoftransformations,suchastiling, permutation,iteration-spacesplitting,shifting,andunroll-and-jam. \nSuch complex optimization strategies result in correspondingly complexiterationspaces.Parallelcodeintroducesadditionaltrans\u00adformations \nsuch as strip mining across threads, resulting in non\u00adconvex iteration spaces, which further increases \ncomplexity. Auto\u00adtuning additionally introduces non-trivial parameter combinations during the search \nthat lead to signi.cant clean-up code and elabo\u00adrate loop bound calculations. To summarize, the requirements \nfor polyhedral compilers tar\u00adgeting modern memory hierarchies, multi-core and heterogeneous systems pose \nsigni.cant challenges for polyhedra scanning algo\u00adrithms, particularly when the goal is to achieve very \nhigh levels of performance. We describe in this paper algorithms in CodeGen+ thatplayanimportantroleinmeetingthisgoal.Themaincontribu\u00adtions \nof this paper are: (1) a new loop overhead removal algorithm that can handle complex constraint relationships \nfrom Presburger arithmeticamongdifferentpolyhedra,andprovidesprecisecontrol of trade-offs between loop \noverhead and code size based on actual loop nesting depth; and, (2) a new algorithm to further simplify \nif\u00adstatements for the code after trade-offs. Since CLooG has recently been extended to use fullPresburger \narithmetic byintegrating with ISL[27],wewillprovidedirectcomparisonson.veloopnestcom\u00adputations for identical \niteration spaces. These experiments demon\u00adstrate that our bottom-up algorithms built with mathematical \nrigor provide a better control of trade-offs between loop overhead and code size andare capable of generating \nbetter quality code. This paper is organized as follows. Section 2 introduces the polyhedral model and \nthe underlying mathematical model. Sec\u00adtion3describesourpolyhedrascanningalgorithmsandcodegener\u00adationprocess.InSection4,weexplainthedesignfeaturesandcode \nquality improvements under the perspective of polyhedral frame\u00adwork. Finally, Section 5 summarizes the \nrelated work, and Sec\u00adtion 6 concludesthepaper.  2. Background We .rst give a brief introduction of \nthe polyhedral model for loop transformations. Polyhedra scanning is part of this complete solu\u00adtion, \nand its various design decisions must be considered in this greater context. As a prerequisite for the \npolyhedral model, we as\u00adsumethatloopnestshaveaf.neloopbounds,andthereisnocontrol dependenceforstatementsinsidetheloopnest.Wealsoexplaines\u00adsentialfunctionsfromtheunderlyingmathematicalsystemthatare \ncritical to understand the polyhedra scanning algorithms described in thispaper. 2.1 Polyhedral Model \nStatements inside a loop nest can be viewed as lattice points in a polyhedron. Executing the loop nest \nsequentially is equivalent to enumeratingthoselatticepointsinlexicographicalorder. Thiscon\u00adceptualizationallowsthecompilertoreasonaboutlooptransforma\u00adtions \nas manipulating iteration spaces of loop nests. For example, consider the following loop nest: for (i=0;i<n;i++) \nfor (j=0;j<i;j++) s0: a[i][j]=b[i][j] Statement s0 siteration space is {[i, j]:0 = i<n . 0 = j<i}. Dimension \ni in this iteration space corresponds to outer loop i in the above loop nest and dimension j corresponds \nto inner loop j. A reordering of a statement s execution order is valid as long as it preserves all data \ndependences, i.e., the source is always executed beforethesinkaftertransformationforanydatadependence.View\u00ading \na loop nest by its iteration spaces give us a powerful abstrac\u00adtiontotransformaloopnestwithoutbeingrestrictedtotheoriginal \nloopstructure.Thus,alooptransformationcanberepresentedbya mappingfunctionfromonespacetotheotherandtheirdimension\u00adality \ndoes not need to be the same. A mapping must be invertible toguarantee thatthesameamountofworkisdone \nbefore and after transformation, i.e., a reordering transformation. For example, ap\u00adplying mapping function \n{[i, j] . [i,j]: i. = j . j. = i} to the above iterationspace would resultin {[i,j]:0 = j<i<n}, which \nrepresents the iterationspace of the interchangedloop nest. Thus, after a loop nest has been transformed \nin the polyhedral model, the new iteration spaces (set of polyhedra) must be con\u00adverted back to their \nsyntactic format as nested loops to produce code.Thiscodegenerationisperformedusingpolyhedrascanning. \nForasinglepolyhedron,itsimplyscansfromthe.rstdimensionto the lastwhile generating the loop bounds and \nstep size for each di\u00admension.Anyotherconstraintsthatcannotbeenforcedbyloopsare represented by if-statements, \ne.g., a guard to only execute a state\u00adment on a subset of iterations of a loop. For example, the above \niteration space after loop interchange generates the following code from polyhedra scanning: for (t1=0;t1<=n-2;t1++) \nfor (t2=t1+1;t2<=n-1;t2++) s0: a[t2][t1]=b[t2][t1] Further challenges come from scanning a set of polyhedra \nand allowing more complex constraints such as modulo constraints to be described in iteration spaces. \nDifferent polyhedra may overlap with each other with nontrivial constraint relationships. Polyhedra scanning \nneeds to generate concise loop bounds and optimally place additional guard conditions, taking into account \ntrade-offs betweenloop overhead and code size.  2.2 IntegerLinearArithmetic CodeGen+ depends on Omega+ \n(Presburger arithmetic) to manip\u00adulate a system of integer equations and inequalities. Omega+ is an updated \nOmega library[11]whichusesenhanced Fourier-Motzkin elimination as its core algorithm [15 17]. Omega+ \nfurther extends the Omega library to better support integer modulo constraints in various functions. \nThis enables CodeGen+ to generate ef.cient codes when iteration spaces do not have unit stride. In this \nsection, we describe a few high-level functions that provide key underly\u00ading support in designing polyhedra \nscanning algorithms. These are Project, Gist and Hull. Project To project a variable from a polyhedron, \nthe variable is eliminated from all equations and inequalities from a linear system.For example, Project({1 \n= y = x = 100},x)= {1 = y = 100}.  In some cases, Project will generate additional constraints if necessary: \nProject({1 = x = 100 . y =2x},x) = {2 = y = 200 ..a(y =2a)}. Gist Gist is a unique function implemented \nin the Omega library. It takes two relations and the result must satisfy the following condition: Gist(A, \nB) . B = A . B. The function can be interpreted as this: given that we know B, whatistheextraknowledgein \nA thathasnotbeenknownin B. The following illustratesthe behavior of thisfunction. Gist({i> 10 . j> 10}, \n{j> 10})= {i> 10}, Gist({1 = i = 100}, {i> 10})= {i = 100}. A new enhancement in Omega+ enables Gist \nto reduce the strengthofmoduloconstraints.For example, Gist({.a(i =6a)}, {.a(i =2a)})= {.a(i =3a)}. The \nabove result can be proved correct using the Chinese re\u00admainder theorem. Intuitively speaking, if we \nalready know i is an even number, the extra information included in the fact that i isamultiple of 6 \nis that i mustbe amultiple of 3. Hull Hull of a set of polyhedra returns a single polyhedron that must \ninclude all points in the original set of polyhedra. Al\u00adgebraically, the result of Hull must be a conjunction \nof con\u00adstraints. Althougha convex hullis theminimal polyhedronthat encloses all input polyhedra, it can \nbe very expensive to com\u00adpute in some cases. Our experience so far is that absolute tight bounds are \nnot necessary when dealing with real application code.Instead,Hullisanapproximationalgorithmin.ndingthe \nenclosing polyhedron. A new enhancement in Omega+ makes Hull handle stride conditions and .nd the lattice \nwhen input polyhedrahavedifferentmoduloconstraints. Belowisasimple example illustratingHull s behavior: \nHull({1 = i, j = 100 ..a(j = i +4a)}. {1 = i = 50 . 1 = j = 200 ..a(j = i +6a)}) = {1 = i = 100 . 1 = \nj = 200 ..a(j = i +2a)}. Itshouldbepointedoutthatdifferentmathematicalframeworks other than Omega+ can \nbe used as long as the same functionality is provided. The generated code quality may differ due to different \ncapabilityimplemented in eachframework.  3. Polyhedra Scanning and Code Generation In this section \nwe describe our improved polyhedra scanning al\u00adgorithms, including the ones for the loop overhead removal \nwith code size trade-off control and the subsequent if-statement simpli\u00ad.cation. These optimizations \nare becoming increasingly important forgeneratinghighqualitycode,asoptimizationstrategiesbecome more \nsophisticated in response to the growing complexity of mi\u00adcroprocessorsand many-core architectures. To \nsimplify the discussion, we assume that each polyhedron to be scanned is the end result of applying a \nmapping function to the original iteration space. The mapping function only affects the variablesubstitution \nduring the code generation process, whichwe assume will be properly handled by the system. Also keep \nin mind that CodeGen+ treats every dimension in the polyhedra during scanning the same way without exception. \nThis allows a clean and consistent strategy for loop overhead optimization and provides a predictable \nbehavior to higher-level polyhedral transformation frameworks. split:node set active; (relation, node*) \n(restriction, body)[];  loop:node leaf:node set active; set active; int level; relation known; relation \nknown; relation guards[]; relation restriction; relation bounds; relation guard; node* body; Figure \n1. AST node structure.  3.1 AST Structure CodeGen+ relies on an abstract syntax tree (AST) to track \nthe breakdown of a conjunction of unordered constraints in iteration spacesintolevelsofconstraints,whereconstraintsateachlevelcan \nonlyuse loop variables ofthis level and the levels above. Our AST isthesameas[12]withonlyminorreinterpretation.Therearethree \ntypes of nodes: split, loop and leaf. Figure 1 shows the structures of these node types. Note the common \nactive .eld which includes statements that will be executed within the node or its subtree. In addition, \nall the .elds with an Omega+ relation type must be a single conjunct, which is guaranteed after disjoint \niteration spaces are preprocessed to be split into separate ones. Below are detailed descriptions ofeach \nnode type. Split Asplitnodehasasits.eldsanarrayofrestrictionsandasso\u00adciatedsubtrees(calledbody).Theorderofsubtreesisimportant \nasitrepresents thelexicographicalorderof the scanning result. For each body node, the corresponding restriction, \nas its name implies, restricts the iteration space of any node in the subtree. Unlike the other two node \ntypes, a split node does not corre\u00adspond to any actual code when converting from the AST to ac\u00adtual code. \nIt only helps to separate disjoint iteration spaces at the designated level. Loop The loop node is the \ncore part of the AST. Each loop node corresponds to one loop level. Its known and restriction .elds are \nfor bookkeeping information calculated from its ancestor nodes. Known refers to what constraints have \nbeen enforced by the code enclosing this loop, and restriction indicates the restricted iteration space \nfor this node. They are not necessar\u00adily the same since not all constraints in the restricted iteration \nspaces can be enforced by loop nodes seen so far from the top down. Thus, known must be a subset of restriction, \na prop\u00adertymaintainedthroughoutASTrestructuring.Twoother.elds, bounds and guard,correspondtoactualcodethatwillbegener\u00adated. \nBounds is the condition that can be represented by a loop structure, namely lower and upper bounds for \nthis loop vari\u00adable and one stride condition for a constant step size. Guard is a conjunction of those \nextra constraints that cannot be repre\u00adsentedbyaloopstructureastheyaretoocomplicatedandmust be enforced \nby a separate if-statement. The logical relationship here is that guard is placed outside the loop structure \nthat en\u00adforcesbounds,andsoconstraintsinguardcannotreferencethis loopvariable. Leaf A leaf node representsthose \nstatements that will be executed attheiterationspacerestrictedbyallitsparentsplitnodes.Each statement \nhas an array guards .eld which represents the re\u00admainingconstraintsintherestrictediterationspacesthatcannot \nbe enforced by any parent loop nodes and require additional if-statements to enforce. Unlike the split \nnode, there is no or\u00ad  initAST(level, active, restriction) input: level: current looplevel; active:set \nof activestatements; restriction: conditionto restrict polyhedra; output: rootofAST if (level > max level) \nnew leaf node with active; return leaf node; if (singlestatement in active) child = initAST(level +1, \nactive, restriction); new loop node with level and body = child; return loop node; for (each statement \ns . active) /* In thefollowingstatement, ISs istheiterationspaceofthetransformedstatement. */ /* Approximate \nisan existing Omega operationtosimplifythe iterationspacerepresentation. */ Rs =Approximate(restriction \nn Project(ISs, l(level+1) \u00b7\u00b7\u00b7 lmax level));/* no existentials*/ for (each statement s . active) for (each \nconstraint c . Rs involvingloop variable llevel) if (c splits Rs.active intodisjointsets active1 and \nactive2) WLOG assume llevel is smaller in the partition constrained by c thanthe oneconstrainedby c child1 \n= initAST(level, active1, restriction n c); child2 = initAST(level, active2, restriction n c); new split \nnode with (c, child1)and(c, child2); return split node; child = initAST(level +1, active, restriction); \nnew loop node with level and body = child; return loop node; Figure2. Algorithm to initialize AST. der \nrequirementforstatementsactive atthislocationasthey all have the samelexicographicalorder.  3.2 ManipulatingtheAST \nWe .rst build an initial AST from a set of polyhedra as the basis for further optimization. As a preprocessing \nstep, each statement s iteration space is split into a disjoint set of polyhedra. In addition, all polyhedra \nare extended to the same dimensionality of the max\u00adimum one, withconstantvalues foradditionaldimensions. \nTheex\u00adtradimensionsareconvenientlyignoredwhengeneratingcode,and since they are constant values no extra \ncomplexity is introduced. For completeness of discussion, Figure 2 shows the algorithm to build the initial \nAST, which is essentially the same as the one used by [12]. It is invoked initially with initAST(1, {all \nstmts}, {.l1, TRUE .,..., .lmax level, TRUE .}), which invokes the algorithm on all statements in the \nloop nest, starting at the outermost loop and with no restrictions on the loop indices. The algorithm \nuses a simple strategy in that at any dimension, if there is an overlap between polyhedra, they are enclosed \nin the same loop node. Oth\u00aderwise,severalloopnodeswithdisjointspacestoacommonparent split node are created. \nThis approach represents minimal code size from polyhedrascanning. 3.2.1 Computing Node Properties After \nexecuting the algorithm in Figure 2, we still must initialize other .elds such as bounds and guards for \nloop nodes and guards forleafnodes.Figure3showsthealgorithmtocomputethesenode properties, starting at \nthe root of the AST. The algorithm will also be invoked whenever the AST is updated due to a newly inserted \nsplit node. There are several key points in the algorithm that are important to overhead optimization \nand code generation. First, the bounds selected for a loop node are represented directly in a loop node.recompute(parent \nactive, known, restriction) input: parent active: setof active statements in parent node; known: knowncondition \nfor this node; restriction: restriction condition forthis node; output: rootofnew AST active = active \nn parent active; node.known = known; node.restrictions = restrictions; if (node issplit node) for (eachrestrictionand \nchildpair (r, c) in node) c = c.recompute(active, known, restriction n r); elseif (node is loopnode) \nfor (everystatement s . active) Rs = Project(ISs, l(level+1) \u00b7\u00b7\u00b7 lmax level) n restriction; if (Rs == \n\u00d8) active = active -{s}; hull = Hull(Rs.active); let i bethe indexforthe loop at level if (i is degenerate) \n bounds = i sequality constraintin hull; guard = TRUE; else bounds = i s lowerand upper bounds andsingle \nmodulo constraint with unit coef.cient in hull; guard = Gist(Project(hull, i), known n bounds); endif \nbody = body.recompute(active, known n bounds n guard, restriction n bounds n guard); if (body == NULL) \ndelete node; return NULL; else return node; elseif (node is leaf node) for (eachstatement s . active) \nguards[s] =Gist(ISs n restriction, known); if (guards[s] == FALSE) active = active -{s}; if (active \n== \u00d8) delete node; return NULL; else return node; Figure 3. Algorithmto compute AST node properties. \nstructure.Thisguaranteesthatoverheadoptimizationdecisionsde\u00adscribed in the next algorithm are indeed \nbased on the actual output loop nesting structure. Second, for a loop level with a single itera\u00adtion \n(called degenerate loops), its guard condition is always set to true. This strategy postpones the extra \nconstraints that cannot be enforced at this loop level into lower level (inner loop) nodes. Be\u00adcause \nthere is an inherent comparison operation for each loop, by pushing the constraints inward, we avoid \nredundant checking by the guard at this level and an inner loop nest encountered in the subtree.  3.2.2 \nLoop Overhead Removal Figure 4 shows the algorithm to lift overheads out of inner loops byduplicatingthecode.Thelevelofcodeduplicationiscontrolled \nby parameter d which is the loop nesting depth. The loop nesting depth can be calculated from the previous \ncomputed AST using a recursive function. The leaf node has a nesting depth zero. The nesting depth for \nother nodes is based on the maximum nesting depth, max depth, among its children. For non-degenerate \nloop nodes, nesting depth is max depth+1, and it is max depth for all othernodetypes.The liftOverhead \nalgorithmalsotakesanother parameter propagate up as a .ag to tell whether it is currently working on \na node that is inside the subloop with a nesting depth = d.  Loop overheads arise from non-tautology \nguard conditions in loop and leaf nodes. They correspond to if-statements in a loop nest. In our algorithm, \na single overhead condition selected to be lifted out of the current position corresponds to a constraint \nor a combinationofconstraintswhosecomplementisasingleconjunct. Forexample, i = 5 canbe one selectedoverhead \ncondition.And .a(5a = i = 5a +2) canalsobe lifted out as a whole sinceits complement .a(5a +3 = i = 5a \n+4) is a single conjunct. This requirement allows a single split node to partition the space into two \ndisjoint subspaces each constrained by a single conjunct. The algorithm begins at the root of the AST \nwith propagate up set to FALSE, and descends the AST recursively. The main part of the algorithm deals \nwith loop nodes with a nesting depth = d,and thus it will be explained in more detail. When a suitable \noverhead tobeoptimizedislocated,itispropagatedupthetreetothehighest level possible. The highest level \nis the loop node such that one of the following conditions is satis.ed: (1) the required nesting depthisreached;(2)forconstraint(s)withexistentialvariables,the \nmaximumlooplevelofvariablesappearinginsideisthislooplevel minus one; or, (3) for constraints without \nexistential variables, the maximumlooplevelofvariablesappearinginsideisthislooplevel. Thereasonfor(1)isstraightforwardsincethatistheintendedplace \nto lift the overhead. Further up will cause more code duplication thanrequired.Thereasonfor(2)and(3)isthatthisoverheadcannot \nbe propagated higher since otherwise it will cause a referenced variabletobe projectedaway. At this point, \nthe algorithm has identi.ed the location in the AST for inserting a split node for the overhead condition \nto be lifted. A new split node is inserted at this location and the orig\u00adinal subAST is duplicated and \nreinserted as two children to this newly created split node, one constrained by satisfying the over\u00adheadconditionandtheotherbyitscomplement.Theorderofthese \ntwo subtrees follows the lexicographical order. Both subASTs are then recomputed for node properties \nsince they are under different restrictedspacesnow. The algorithm liftOverhead is then recur\u00adsively invoked \non this new split node. This process stops when all candidate overheadconditionshave beenlifted. Currentlywedonottreatmin/maxboundsinaloopnestasover\u00adheadsincewehavenotfoundtherun-timecostsofsuchoperations \nto be signi.cant. Nevertheless, if desired, min/max bounds can be easily added into the algorithm and \ncontrolled by a different nest\u00adingdepthparameterifneeded.Theconstrainttypeismuchsimpler than those that \nwould appear in the guard condition, and thus the implementation isstraightforward.  3.2.3 If-statement \nSimpli.cation Our previous algorithm for lifting loop overhead only deals with trade-offs between loop \noverhead and code size. It is possible that guard conditions from loop nodes of the same level at different \nsubASTs use the same constraint or they contradict each other. Thisgivesustheopportunitytofurtherreducethecontroloverhead \nwithout any negative impact on code size by constructing if-then\u00adelse subtrees(represented byIFnodesin \nthe algorithm) andavoid\u00adingunnecessaryconditionals.Thisalgorithmisinvokedforasetof adjacentnodesatthesamenestingdepth,withpostponed \nguard set node.liftOverhead(d, propagate up) input: d:loop nestingdepth counted frominnermost; propagate \nup:.ag whetherinside a loopnest of nesting depth = d; output: (overhead constraint, root of newAST) if \n(node is split node) for (eachrestrictionand childpair(r, c)in node) (r2, c2)= c.liftOverhead(d, propagate \nup); (r, c)=(r, c2); if (r2 . = TRUE) return (r2, node); return (TRUE, node); elseif (node is loopnode) \nif (nestingDepth(node) >d) (r, c)= body.liftOverhead(d, FALSE); body = c; return (TRUE, node); else \nif (propagate up) pick one guard r with single conjunctcomplement; if (r . = TRUE) return (r, node); \nif (propagate up || bounds isnot an equality) (r, c)= body.liftOverhead(d, TRUE); else (r, c)= body.liftOverhead(d, \nFALSE); body = c; if (r == TRUE) return (TRUE, node); if (bounds isan equality) substitute llevel in \nr using the equation; if (!propagate up || (r hasexistential variables and the maximum level of loopvariables \nusedis level - 1) || (r has no existentialvariables and themaximumlevelof loop variablesused is level)) \nWLOG assume llevel issmallerin the partitionsatisfying r; new split node with (r, node)and (r, node); \nsplit node.recompute(active, known, restriction); return split node.liftOverhead(d, propagate up); else \nreturn (r, node); elseif (node is leaf node) for (eachstatement s . active) if (guards[s] . = \u00d8) pick \noneconstraint r from guards[s] withsingle conjunctcomplement; if (r . = TRUE) return (r, node); return \n(TRUE, node); Figure 4. Algorithm to lift loop overhead. to NULL, and known guard representing the context \nin which the nodes are nested. Therearetwoscenariosforif-statementsimpli.cation.Forloop nodes, only guard \nconditions from neighboring nodes can be con\u00adsidered for merging since the generated code must follow \nthe lexi\u00adcographical order. On theotherhand, forstatements ina leafnode, since they all have the exact \nsame lexicographical order, simplify\u00ading if-statements can be done in any order and this provides more \n.exibility in reducing the number of comparisons. Figure 5 shows the algorithm mergeIfInOrder to merge \nif-conditions in order. Merging if-conditions out-of-order can be done in a similar fash\u00adion, and itis \nomitted here due to space limitations. ThealgorithminFigure5takestheinputofanarrayofcontigu\u00adousloopnodesofthesamelevel.Italsotakestwoadditionalparam\u00adeters:onetracksthoseconstraintsyettobeenforced,andonetracks \nconstraints already generated. The basic idea of the algorithm is to testifaconstraintfromtheguardconditioninthe.rstnodematches \ntheconstraintappearingintheimmediatelyfollowingones.Forthe remaining nodes, the algorithm tests if the \nguards contradict. This partitionsthenodesintothreecontiguouspartswithregardtotheif\u00adstatementforthisparticularconstraint:the.rstandsecondpartare \nthe then and else part of the if-statement respectively, and the third partfollowsthisif-statement.Thenthealgorithmrecursivelyworks \non each group until all constraints are processed. Note that simply comparingtheconstraintsinisolationisnotoptimalinreducingthe \nnumber of comparisons. These constraints have additional known conditions guarding execution of the code \nat this point, and there is no need to retest these conditions. Instead, the algorithm tests whether \nthey are equivalent and complementary given the known condition using the Gist function. For example, \nas shown later in Figure 8(f), if we already know variable i is an even number from its enclosing loops, \nthen the constraint .a(i =4a) is the exact complement of constraint .a(i =4a +2). We can then put the \nstatements guarded by .a(i =4a +2) into the else part of the if-statement with condition (mod(i, 4) == \n0).  However, recall that in the algorithm to compute node proper\u00adties (Figure 3), we postpone the computation \nof guard conditions for those loop nodes which are assignments (degenerate loops). This will cause a \nproblem here in that now the guard conditions at the different loop levels in the AST might actually \nbe translated to neighboring if-statements in the generated code if intermediate loop nodes are all degenerate. \nAs a preprocessing step, we con\u00adceptually propagate guard conditions in the child loop nodes into upper \nlevel loop nodes if loop nodes between them are all degen\u00aderate, with variable substitutions according \nto those assignments. Such propagation may involve multiple levels of degenerate loop nodes or multiple \nsplit nodes if necessary. Figure 6 illustrates this behavior using one AST snippet as an example. Guard \nconditions in nodes C, D, E and F are translated to neighboring if-statements intheactualcode.Thusinourpreprocessingstep,theguardcondi\u00adtion \nin node C is propagated to node A, and the guard condition in node F is propagated to node E. Finally, \nnode A, D, E and G with updated guard conditions are the .rst parameters to the algorithm in Figure 5. \n3.3 Code Generation Once the AST is constructed and subsequently optimized, it corre\u00adsponds to a compiler \ns high-level intermediate representation (IR) AST if all split nodes are ignored. Thus code generation \nis a straightforwardprocess thatfollowsfrom scanning theAST. Some details of converting constraintsto \ncode are presented here. For a loop node with unit stride, lower and upper bounds come from inequalities \nin bounds conditions, with min/max added if therearemultiplesuchinequalities.Fornon-unitstride,CodeGen+ \nmust know whether the lower bounds always satisfy the stride condition. Suppose K is the intersection \nof the known condition and bounds condition for this loop node. First, we create a relation with a modulo \nconstraint for this loop variable with the same stride but starting at a lower bound and test whether \nthere is new knowledge in this relation given K. If not, we further test whether there is new knowledge \nin the stride condition when this loop variable is set to the lower bound in K. Both tests are done by \nthe Gist function and if both results are a tautology, we can safely use thislowerbound;otherwise,wegeneratearemainderexpressionto \nbeaddedtothe lower bound. For degenerate loop nodes (single iteration loops), we either generate assignment \ncode directly or substitute every appearance of this loop variable in its child AST with a substitution \nexpres\u00adsion. The choice depends on how complex the expression is, and whetheritcanbeadjustedbyaparameter.Additionalconsideration \nis needed when the coef.cient c for this loop variable is not a unit; mergeIfInOrder(nodes[], postponed \nguard, known guard) input: nodes[]: neighboringnodes for theirguard conditions; postponed guard: guard \nconditions yet to beenforced; known guard:guard conditionsalreadyconsidered; output: if-then-else tree \n/* forconvenience of illustration, assume parameter nodes always indexedfrom 1 to n */ if (nodes == \u00d8) \nreturn NULL; r =Gist(nodes[1].guard, known guard); if ((nodes[1].known n known guard) . r) for (i in \n2..n) if (Gist(nodes[i].guard, known guard) . = TRUE) break; s1 = code for nodes[1..(i - 1)] without \ntheir guard condition; s2 = mergeIfInOrder(nodes[i..n], NULL, known guard); s3 = concatenate s1 and s2 \nin thisorder; if (postponed guard . = \u00d8) return new IF(postponed guard, s3, NULL); else return s3; else \nc =selectaconstraint from nodes[1].guard that maximizes thecontiguous region of nodesstartingfrom 2 to \nsatisfy it; Let c partition nodes intothree contiguous regions nodes1, nodes2 and nodes3 in this orderwhere \nnodes1 satis.es c, nodes2 satis.es c and nodes3 remaining nodesin theend; if (nodes2 == \u00d8. nodes3 == \n\u00d8) return mergeIfInOrder(nodes1, postponed guard n c, known guard n c); elseif (nodes2 == \u00d8) s1 = mergeIfInOrder(nodes1, \nc, known guard n c); s2 = mergeIfInOrder(nodes3, NULL, known guard); s3 =concatenate s1 and s2 inthis \norder; if (postponed guard . = \u00d8) return new IF(postponed guard, s3, NULL); else return s3; else s1 \n= mergeIfInOrder(nodes1, NULL, known guard n c); s2 = mergeIfInOrder(nodes2, NULL, known guard n c); \ns3 = mergeIfInOrder(nodes3, NULL, known guard); s4 = new IF(c, s1, s2); s5 =concatenate s4 and s3 inthis \norder; if (postponed guard . = \u00d8) return new IF(postponed guard, s5, NULL); else return s5; Figure \n5. Algorithm to merge neighboring if-conditionsin order. that is, when |c|.We must test if the right \nhand side of the as\u00ad =1. signmentwithoutdividingbythec partisalwaysamultipleofc us\u00adingtheGistfunctiongiventheknownconditionwhencodereaches \nhere. If not, an if-statement with condition (mod(RHS, |c|)==0) mustbecreatedtoguardtheassignmentandthecodefromthechild \nAST, where RHS is the right hand side of the assignment without dividing by the c part. Here are a few \nexamples where constraints in the guard condi\u00adtions have existential variables: .a(4i =5j +4a) . mod(4 \n* i - 5 * j), 4) == 0, .a(4a = i = 5a) . ceil(i, 5) <= floor(i, 4). Our implementation also tries to \nrecognize .oor de.nitions from constraints and generate clean code whenever possible. For exam\u00ad  ....... \n. ....... ....... . ....... ....... ..... ..... ....... ....... Figure 6. Propagate guard conditions \nup through degenerate loop nodes (marked by = ). ple, with a de.ned by . m . inthefollowing relation, \n4 {[i]: .a(m - 4 < 4a<= m . 4a<= i<= n)}, the generated lower bound for i is 4 * floor(m, 4). Thus there \nis no if-statement needed for the generated loop code from this polyhedron. 4. Experiments With precise \ncontrol of trade-offs between loop overhead and code sizeandreduction ofif-conditions,while atthesametimepreserv\u00ading \nthe lexicographical order among statements represented in the input iteration spaces, CodeGen+ provides \na clean interface and predictable behavior for high-level polyhedral loop transformation frameworks. \nIn thissection, we examine the quality of code gener\u00adated by CodeGen+ after composing a sequence of transformations \nin a polyhedral framework for a set of loop nest computations. We will compare CodeGen+ with the state-of-the-art \npolyhedra scan\u00adningtoolCLooGusingexamplestoillustratetheimprovementsre\u00adsulting from ournew algorithms. \nFor the generated codes in this section, we use CLooG 0.16.3 and CodeGen+ 2.2.3. For code compilation \nand performance, we use gcc 4.6.1 with -O3 .ag. For performance measurements, the target architecture \nis a single core of a 2.8GHz Intel Core i7 930 with 2GBytes memory. We .rst consider a set of simple \ncode examples shown in Figures 7 and 8 to illustrate the differences resultingfromthealgorithmsintheprevioussection.Subsequently, \nwe examine a set of kernel computations, shown in Table 1. The code inputs are standard hand written \nkernels (e.g., a simple 3\u00addeep loop nest for gemm) to which transformations are applied using the CHiLL \ntransformation and code generation system [4]. WhileCHiLL can invoke CodeGen+ internally, weinstead capture \nthe iteration spaces of the transformed statements. Then, identical iteration spaces are input to both \nCodegen+ and to CLooG (via the iscc frontend to ISL in Barvinok) [27]. The output code from both are \ncompared to producetheresults in the table. 4.1 ControlofTrade-offs Precise control of trade-offs between \nloop overhead and code size is important in tailoring the output of polyhedra scanning. For ex\u00adample, \nin optimizing for high performance, code generation of an innermost loop impacts instruction-level parallelism \nor SIMD exe\u00adcution in multimedia extensions, which are critical for overall per\u00adformance. Thus removing \nloop overhead from innermost loops is usually the best choice in balancing loop overhead and code size. \nForothersituations,adifferentchoicemightbeselected.Asexam\u00adples,wemightnotwanttoliftanyoverheadtoavoidcodegrowthin \nan embeddedsystem,orwemightwant toremoveasmuch control .owaspossible from loopsforthehighest performance. \nFigure7(b-d)showsthethreevariationsofgeneratedcodefrom iteration spaces in (a) with different trade-offs \nof removing loop overheadfromsubloopsofdepth0,1and2,respectively. Notethat statement s0 is enclosed in \nloop t1 which itself is a loop nest of depth 2. Thus when removing loop overhead from nesting depth 0 \nand 1, its enclosing if-condition (n>=2) is not moved out of the t1 loop. Only when removing loop overhead \nfrom all subloops of depth 2 is it moved out of the t1 loop; no more if-conditions re\u00admain inside any \nloop in the generated code (Figure 7(d)). CLooG generates almost identical code to Figure 7(d), which \nfollows the lexicographical order. However, it does not provide such a guaran\u00adtee when generating codes \nfor other trade-off points using the -f or -l .ags, which move conditions based on the .rst or last loop \ns nesting depth, respectively. 4.2 If-condition Overhead The algorithms presented in this paper can reduce \nthe number of unnecessary if-conditions in the generated codes, especially in some dif.cult situations. \nFigure 8 shows two examples to com\u00adpare number of if-conditions in the codes generated from CLooG and \nCodeGen+. For the iteration space in Figure 8(a), the output of CLooG in Figure 8(b) contains redundant \nmodulo checking in the innermost loop, but this is removed by CodeGen+ as shown in Figure 8(c). Consider \nthe iteration space of Figure 8(d) and the output of CLooG in Figure 8(e). Given that c1 is even, based \non loopbounds,thecondition ((c1 + 2)%4 == 0) canbedetermined statically to be the complement of condition \n(c1%4 == 0). Thus, the mod operation occurs just once in the CodeGen+ code of Fig\u00adure8(f),andtheoutermostconditiononvariable \nn isnotgenerated sincethe.rstloopencounteredwillcheckthesameconditionagain as partofits standardbounds \nchecking. 4.3 ApplicationtoCodeOptimization We now demonstrate thattheimprovements in the polyhedra scan\u00adning \nalgorithms indeed make a signi.cant impact on generating high-quality codes when complex optimization \nstrategies are ap\u00adplied in polyhedral frameworks. Table 1 shows our comparison re\u00adsult with CLooG using \nthe same iteration spaces generated from CHiLL [4] for three programs generated from scripts in CHiLL \ns example directory, matrix-vector multiply (gemv), matrix-matrix multiply (gemm) and LU factorization \n(lu), and two codes qr and swim taken from CLooG [26]. These examples illustrate the effect of complexity \nof iteration spaces on generated code. The kernel gemv is the simplest, followed by qr, swim and gemm. \nIteration spacesfor lu areby far the most complex. For gemv s simple unroll-and-jam optimization, both \nCLooG and CodeGen+ take roughly the same amount of time generating the codes, and the resulting codes \nhave the same performance al\u00adthoughthecodegeneratedbyCLooGissomewhatlargerbecauseit generates extra if-conditions \ncorresponding to modulo constraints. The optimization strategy for qr involves peeling, shifting and \nfu\u00adsion;theresultingcodeislargerusingCLooGwhichincreasescode generation and compile time, but performance \nis comparable for the two systems. For swim, the optimization strategy is very close to that of [8], \nemploying peeling, shifting, and fusion. While the transformations are similar to what was used in qr, \nthe iteration space complexity grows because there are three loop nests that all must be peeled andshifteddifferentamountsto \nenable fusion. The  s0: {[i]:1 = i = 100 . n> 1}; s1: {[i, j]:1 = i, j = 100 . n> 1}; s2: {[i, j]:1 \n= i, j = 100}; (a) iteration spaces for (t1=1;t1<=100;t1++) if (n>=2) s0(t1); for (t2=1;t2<=100;t2++) \ns2(t1,t2); if (n>=2) s1(t1,t2); (b)noloop overhead removal (depth 0) for (t1=1;t1<=100;t1++) if (n>=2) \ns0(t1); for (t2=1;t2<=100;t2++) s1(t1,t2); s2(t1,t2); else for (t2=1;t2<=100;t2++) s2(t1,t2); (c)removeoverheadfrom \ndepth1 loops (default) if (n>=2) for(t1=1;t1<=100;t1++) s0(t1); for (t2=1;t2<=100;t2++) s1(t1,t2); s2(t1,t2); \nelse for (t1=1;t1<=100;t1++) for (t2=1;t2<=100;t2++) s2(t1,t2); (d) removeoverheadfromdepth2loops Figure \n7. Different trade-offs between loop overhead and code size in CodeGen+. s0: {[i, j]:1 = i = n . i = \nj = n ..a, \u00df(i =1+4a . j = i +3\u00df)}; (a) single iteration space with complex stride conditions if (n>=1) \nfor (i=1;i<=n;i+=4) for (t1=1;t1<=n;t1+=4) for (j=i;j<=n;j+=3) for (t2=t1;t2<=n;t2+=3) if ((11*i+4*j+9)%12 \n== 0) s0(t1,t2); s0(i,j); (b)CLooGoutputfrom(a) (c)CodeGen+outputfrom(a) s0: {[i]:1 = i = n ..a(i =4a)}; \ns1: {[i]:1 = i = n ..a(i =4a +2)}; (d) two iteration spaceswith complexrelationship witheach other if \n(n>=2) for (t1=2;t1<=n;t1+=2) for (c1=2;c1<=n;c1+=2) if (intMod(t1,4)==0) if (c1%4==0) s0(t1); s0(c1); \nelse if ((c1+2)%4==0) s1(t1); s1(c1); (e)CLooGoutputfrom(d) (f)CodeGen+outputfrom(d) Figure 8. Comparisonof \nnumber of if-conditionsinthegeneratedcodes. code generated by CLooG is more than four times larger than \nthat generated by CodeGen+, and code generation time and compile timeareconsequently increased. Swim \nachieves CodeGen+ s high\u00adest execution-time speedup over CLooG of 1.15x. To consider a verydifferentoptimizationstrategy,both \ngemm and lu requiremul\u00adtiple levels of tiling and result in many statements generated for the .nal set \nof iteration spaces. In addition, lu requires extensive splitting of the iteration spaces, for example \nto separate the com\u00adputation into a mini-LU, triangular solve and matrix-matrix multi\u00adply as in highly-tuned \nimplementations [10]. CodeGen+ generates signi.cantly less lines of code for gemm and lu as compared \nwith CLooG, a reduction of over 8x and 24x, respectively, while the re\u00adsultingcodeperforms1.12xfasterthantheCLooGoutputforboth. \nMoreover,codegenerationtimeandcompilationtimehaveuptoan order of magnitude speedup; for example, 25x \nand 34x reductions, respectively,for lu. Overall,thecombinedeffectofliftingcontroloverheadfromin\u00adnermostloopsand \nif simpli.cationinCodeGen+results insimpler code with less branching than that generated by CLooG for \nidenti\u00adcal iteration spaces. This simpler code takes less time to generate and compile. For the more \ncomplex examples, where tiling or un\u00adrollingisusedresultinginmoduloconstraints,orwherepeelingand shiftingcausesadditionalcontrol.ow,thecodegeneratedbyCode-Gen+ \nalso performs better, up to 1.15x faster. This performance difference is very signi.cant in the high-performance \ncomputing community, where programmers are willing to expend signi.cant extra effort to increase utilization \nof precious supercomputing re\u00adsources. As future architectures become more complex, we antic\u00adipate the \ncompiler transformation strategies must also increase in complexity, so that this gap in generated code \nquality is likely to grow. 5. Related Work Early work on polyhedra scanning focuses on how to generate \ncode for a single polyhedron. Ancourt and Irigoin [1] use Fourier-Motzkin elimination to .nd loop bounds \nfor unimodular trans\u00adformations. Their method is improved later by Le Fur; however onlytransformed iterationspaceswithoutholesareconsidered[7]. \nOther research [6, 14, 20, 30] expands the mapping function to allow non-unimodular transformations. \nHermite Normal Form is used to .nd loop strides. Besides using Fourier-Motzkin elimina\u00adtion to extract \nloop bounds from a system of linear inequalities, different mathematical solutions are pursued. Collard \net al. [5] use a dual simplex method.Chernikova salgorithmis anothergeomet\u00adric approach to simplify a \nset of linear inequalities. It is used by Quillereet al.[18]which will be mentioned laterinthis section. \nKelly et al. [12] are among the .rst to address the problem of code generation for a set of polyhedra. \nMoreover, their system is built on a systematic approach to integer linear systems, imple\u00admented as part \nof the Omega library [15 17]. However, loop over\u00adhead trade-off decisions are based on static loop levels, \nwithout taking into account that modern polyhedral frameworks often rely onadditionalauxiliarydimensions,someofwhichareconstants,to \nmanagecomplicatedoptimizationstrategies.Moreover,.awsinits codegenerationlogicwillcausesuboptimalorincorrectcodetobe \ngenerated in some dif.cult cases. Griebl et al. [9] also present their code generation solution for multiplepolyhedra,whichallowsnon-unimodulartransformations. \n lines of generated code code generation time gcc compile time code performance CLooG CodeGen+ Reduction \nCLooG CodeGen+ Speedup CLooG CodeGen+ Speedup CLooG CodeGen+ Speedup gemv 39 24 1.70x 0.151s 0.132s 1.14x \n0.095s 0.071s 1.34x 0.091s 0.091s 1.00x qr 88 35 2.51x 0.316s 0.104s 3.04x 0.115s 0.057s 2.02x 4.255s \n4.254s 1.00x swim 501 107 4.68x 0.622s 0.251s 2.49x 2.767s 0.590s 4.69x 0.599s 0.522s 1.15x gemm 373 \n42 8.88x 6.552s 1.082s 6.06x 0.364s 0.086s 4.23x 136.667s 121.736s 1.12x lu 2635 106 24.86x 106.984s \n4.170s 25.66x 7.360s 0.211s 34.88x 49.973s 44.551s 1.12x Table 1. Comparison of code generation performance \nusing iterationspaces representing real optimization strategies. Their approach can generate two .avors \nof code from polyhedra scanning: a run-time solution and a compile-time solution, which looselycorrespondtominimalcodesizeandminimaloverheadver\u00adsions,respectively.Theirmethodinreducingoverheadwouldresult \nin N ! possibilities in loop bounds for N parameters, which can be handled by Omega code generation [12] \nwith proper min/max bounds with signi.cantly fewer loops. One innovation of Griebl et al. s approach \nis that it allows non-invertible mapping by expand\u00ading the matrix to an invertible one, .lling in unit \nvectors, during polyhedra scanning. However, this may not be necessary if higher\u00adlevel tools can select \nmissing dimensions using better knowledge of intended transformations. Quillere et al. [18] propose that \nby splitting overlapping itera\u00adtionspacesatanydimensionduringtheprocessofscanningpolyhe\u00addra,theycangenerateminimumloopoverheadcodewithouttheit\u00aderativemethodusedby[12].However,thismethodcaneasilycause \ncode explosion and provides no control of trade-offs between loop overhead and code size. CLooG [2, 26] \nuses the same approach of Quillereetal.,butittriestoovercomeitscodeexplosionlimitation. Afterinitialprocessing,thealgorithmwilltrytoreducethenumber \nof polyhedra previously split. However, CLooGdoes not provide a guarantee of generating code respecting \nthe lexicographical order initerationspacesduringitstrade-offsotherthanthedefaultchoice. 6. Conclusion \nThis paper describes a polyhedra scanning system that can meet the challenge of supporting sophisticated \npolyhedral frameworks combinedwithautotuningtogeneratehigh-performancecode.The algorithms are based on \na strict mathematical foundation and are designedtohandlecomplexconstraintrelationshipsamongasetof polyhedra,withprecisecontroloftrade-offsbetweenloopoverhead \nandcodesize.Wecomparewiththestate-of-the-artpolyhedrascan\u00adning tool CLooG on .ve loop nest computations, \ndemonstrating thatCodeGen+generatescodethatissimplerandupto1.15xfaster. Webelievethatsuchasystemiscriticaltoexpandtheapplicability \nof polyhedral frameworks and enable compilers to close or signif\u00adicantly narrow the performance gap between \ncompiler-optimized loop nests and hand-tuned code, including domain-speci.c perfor\u00admance librariessuch \nas BLAS. Editorial Note. AuthorChunChenpassedawayshortlyaftersub\u00admissionofthismanuscript.AnandVenkat,ProtonuBasuandMary \nHall made minor editing changes to the document to respond to reviewer requests and improve readability, \nand most signi.cantly, added experimental results for two additional benchmarks, qr and swim. Questions \nabout the contents of the paper can be directed to {anandv,protonu,mhall}@cs.utah.edu. Acknowledgments \nThis research was funded in part by DARPA contract HR0011\u00ad10-9-0008,NationalScienceFoundationawardCCF-1018881,and \nDepartment of Energy Of.ce of Science awards DE-SC0003777 and DE-SC0003520. References [1] Corinne Ancourt \nand Franc\u00b8ois Irigoin. Scanning polyhedra with DO loops. In Proceedings of ACM SIGPLAN Symposium on Principles \nand Practice of Parallel Programming, April 1991. [2] C\u00b4edricBastoul. Codegenerationinthepolyhedralmodeliseasierthan \nyouthink. In Proceedings of the International Conference on Parallel Architectures and Compilation Techniques, \nOctober2004. [3] C. Chen, J. Shin, S. Kintali, J. Chame, and M. Hall. Model-guided empiricaloptimizationformultimediaextensionarchitectures:Acase \nstudy. In Proceedings of the Workshop on Performance Optimization for High-Level Languages and Libraries \n(POHLL 2007),May 2007. [4] Chun Chen, Jacqueline Chame, and Mary W. Hall. CHiLL: A frame\u00adworkforcomposinghigh-levellooptransformations.TechnicalReport \n08-897, University of Southern California, Jun 2008. [5] Jean-Franc\u00b8ois Collard, Tanguy Risset, and Paul \nFeautrier. Construc\u00adtionofDOloopsfromsystemsofaf.neconstraints. Parallel Process\u00ading Letters, 5(3):421 \n436, 1995. [6] Agust\u00b4inFern\u00b4andez,Jos\u00b4eM.Llaber\u00b4ia,andMiguelValero-Garc\u00b4ia.Loop transformationusingnonumimodularmatrices. \nIEEE Transactions on Parallel and Distributed Systems, 6(8):832 840, August1995. [7] Marc Le Fur. Scanning \nparameterized polyhedron using Fourier-Motzkin elimination. Concurrency: Practice and Experience, 8(6):445 \n460,1996. [8] Sylvain Girbal, Nicolas Vasilache, C\u00b4 edric Bastoul, Albert Cohen, DavidParello,MarcSigler,andOlivierTemam. \nSemi-automaticcom\u00adposition of loop transformations for deep parallelism and memory hi\u00aderarchies. International \nJournal of Parallel Programming,34(3):261 317,June 2006. [9] Martin Griebl, Christian Lengauer, and Sabine \nWetzel. Code genera\u00adtion in the polytope model. In Proceedings of the International Con\u00adference on Parallel \nArchitectures and Compilation Techniques, Octo\u00adber 1998. [10] MaryHall,JacquelineChame,JaewookShin,ChunChen,GabeRudy, \nand Malik Murtaza Khan. Loop transformation recipes for code generationand auto-tuning. In LCPC, October,2009. \n[11] Wayne Kelly, Vadim Maslov, William Pugh, Evan Rosser, Tatiana Shpeisman, and David Wonnacott. The \nOmega Library interface guide. Technical Report CS-TR-3445, University of Maryland at College Park, March \n1995. [12] Wayne Kelly, William Pugh, and Evan Rosser. Code generation for multiple mappings. In Proceedings \nof the 5th Symposium on the Frontiers of Massively Parallel Computation,February 1995. [13] Malik Khan. \nAutotuning, code generation and optimizing compiler technology for GPUs. PhD thesis, University of Southern \nCalifornia, May 2012. [14] WeiLiandKeshavPingali.Asingularlooptransformationframework based on non-singular \nmatrices. In Proceedings of the 5th Interna\u00adtional Workshop on Languages and Compilers for Parallel Comput\u00ading, \nAugust 1992. [15] William Pugh. The Omega test: A practical algorithm for exact array dependence analysis. \nCommunications of the ACM, 35(8):102 114, August 1992. [16] William Pugh and David Wonnacott. An exact \nmethod for analysis of value-based array data dependences. In Proceedings of the 6th International Workshop \non Languages and Compilers for Parallel Computing, August1993.  [17] William Pugh and David Wonnacott. \nExperiences with constraint\u00adbased array dependence analysis. In Proceedings of the Second Inter\u00adnational \nWorkshop on Principles and Practice of Constraint Program\u00adming, May1994. [18] Fabien Quiller\u00b4e, Sanjay \nRajopadhye, and Doran Wilde. Generation of ef.cient nested loops from polyhedra. International Journal \nof Parallel Programming, 28(5):469 498, October2000. [19] Shreyas Ramalingam, Mary Hall, and Chun Chen. \nImproving high\u00adperformance sparse libraries using compiler-assisted specialization : A PETSc case study. \nIn Proceedings of the Workshop on High-Level Parallel Programming Models and Supportive Environments \n(HIPS 2012), May2012. [20] J. Ramanujam. Beyond unimodular transformations. The Journal of Supercomputing, \n9(4):365 389, February 1995. [21] Gabe Rudy, Malik Murtaza Khan, Mary Hall, Chun Chen, and Cha Jacqueline. \nA programming language interface to describe transfor\u00admationsandcodegeneration. In Proceedings of the \n23rd international conference on Languages and compilers for parallel computing,pages 136 150.Springer-Verlag, \n2011. [22] Jaewook Shin, Mary W. Hall, Jacqueline Chame, Chun Chen, Paul F. Fischer, and Paul D. Hovland. \nSpeeding up nek5000 with autotuning and specialization. In Proceedings of the 24th ACM International \nConference on Supercomputing,ICS 10, pages 253 262,2010. [23] Jaewook Shin, Mary W. Hall, Jacqueline \nChame, Chun Chen, and Paul D. Hovland. Transformation recipes for code generation and auto-tuning. In \nThe Fourth International Workshop on Automatic Performance Tuning, October2009. [24] A. Tiwari, J. K. \nHollingsworth, C. Chen, M. Hall, C. Liao, D. J. Quinlan, and J. Chame. Auto-tuning full applications: \nA case study. International Journal of High Performance Computing Applications, pages286 294,August 2011. \n[25] Ananta Tiwari, Chun Chen, Jacqueline Chame, Mary Hall, and Jef\u00adfreyK.Hollingsworth. Ascalableautotuningframeworkforcompiler \noptimization. In IPDPS,Rome,Italy, May2009. [26] Nicolas Vasilache, C\u00b4edric Bastoul, and Albert Cohen. \nPolyhedral code generation inthereal world. In Proceedings of the International Conference on Compiler \nConstruction, March 2006. [27] Sven Verdoolaege. isl: An integer set library for the polyhedral model. \nIn International Congress on Mathematical Software,Septem\u00adber 2010. [28] MichaelE.WolfandMonicaS.Lam.Adatalocalityoptimizingalgo\u00adrithm. \nInProceedings of ACM SIGPLAN Conference on Programming Language Design and Implementation, June 1991. \n[29] Michael E. Wolf and Monica S. Lam. A loop transformation theory and an algorithm to maximize parallelism. \nIEEE Transactions on Parallel and Distributed Systems, 2(4):452 471, October1991. [30] Jingling Xue. \nAutomating non-unimodular loop transformations for massive parallelism. Parallel Computing, 20(5):711 \n728,May 1994.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>This paper presents a new polyhedra scanning system called CodeGen+ to address the challenge of generating high-performance code for complex iteration spaces resulting from compiler optimization and autotuning systems. The strength of our approach lies in two new algorithms. First, a loop overhead removal algorithm provides precise control of trade-offs between loop overhead and code size based on actual loop nesting depth. Second, an if-statement simplification algorithm further reduces the number of comparisons in the code. These algorithms combined with the expressive power of Presburger arithmetic enable CodeGen+ to support complex optimization strategies expressed in iteration spaces. We compare with the state-of-the-art polyhedra scanning tool CLooG on five loop nest computations, demonstrating that CodeGen+ generates code that is simpler and up to 1.15x faster.</p>", "authors": [{"name": "Chun Chen", "author_profile_id": "81100119614", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P3471310", "email_address": "mhall@cs.utah.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254123", "year": "2012", "article_id": "2254123", "conference": "PLDI", "title": "Polyhedra scanning revisited", "url": "http://dl.acm.org/citation.cfm?id=2254123"}