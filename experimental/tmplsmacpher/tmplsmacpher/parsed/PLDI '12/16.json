{"article_publication_date": "06-11-2012", "fulltext": "\n Automated Error Diagnosis Using Abductive Inference * Isil Dillig Thomas Dillig Alex Aiken Department \nof Computer Science Department of Computer Science Department of Computer Science College of William \n&#38; Mary College of William &#38; Mary Stanford University idillig@cs.wm.edu tdillig@cs.wm.edu aiken@cs.stanford.edu \n Abstract When program veri.cation tools fail to verify a program, either the program is buggy or the \nreport is a false alarm. In this situation, the burden is on the user to manually classify the report, \nbut this task is time-consuming, error-prone, and does not utilize facts already proven by the analysis. \nWe present a new technique for assisting users in classifying error reports. Our technique computes small, \nrelevant queries presented to a user that capture exactly the infor\u00admation the analysis is missing to \neither discharge or validate the er\u00adror. Our insight is that identifying these missing facts is an instance \nof the abductive inference problem in logic, and we present a new algorithm for computing the smallest \nand most general abductions in this setting. We perform the .rst user study to rigorously evaluate the \naccuracy and effort involved in manual classi.cation of error re\u00adports. Our study demonstrates that our \nnew technique is very useful for improving both the speed and accuracy of error report classi.\u00adcation. \nSpeci.cally, our approach improves classi.cation accuracy from 33% to 90% and reduces the time programmers \ntake to clas\u00adsify error reports from approximately 5 minutes to under 1 minute. Categories and Subject \nDescriptors F.3.1 [Logics and Meaning of Programs]: Specifying and Verifying and Reasoning about Pro\u00adgrams \nGeneral Terms Languages, Veri.cation, Algorithms, Experimen\u00adtation Keywords Error diagnosis, abductive \ninference, static analysis 1. Introduction Automated software veri.cation systems perform sophisticated \nreasoning to prove the correctness of program properties, such as validity of assertions, memory safety, \nand lack of run-time excep\u00adtions. If the tool concludes that the program satis.es the property, then \nall is well. On the other hand, if the tool claims that the pro\u00adgram may violate the property, there \nare two possibilities: either the program is indeed buggy, or  the report generated by the tool is \na false alarm  * This work was supported by NSF grants CCF-0915766 and CCF\u00ad0702681. Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 12, June 11 16, 2012, \nBeijing, China. Copyright c . 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 Since program veri.cation \nis, in general, undecidable, false alarms are inevitable no matter how sophisticated the reasoning performed \nby the analysis tool. Thus, when a static analysis fails to verify the correctness of a program, the \nburden is on the user to decide whether the report generated by the tool is a genuine bug or a false \nalarm. This situation is undesirable for multiple reasons: Manual report classi.cation is a very time-consuming \ntask that requires expertise, patience, and understanding of the program.  Even if the cause of the \nreport is very simple (e.g., the analysis could not establish a single, local fact), there is no bound \non the amount of work the user must do to discover this cause. Effectively, the user must repeat all \nof the successful reasoning the tool performed just to discover where it became stuck.  Manual classi.cation \nof error reports is error-prone: As corrob\u00adorated by our experimental results, users often misclassify \nfalse alarms as genuine bugs, or much more problematically, genuine bugs as false alarms.  We believe \nthat the dif.culty of manual report classi.cation is a major remaining impediment to the wide-spread \nadoption of even the most precise and state-of-the-art static analysis tools. As reported in a recent \narticle by Bessey et al., the overwhelming experience in the .eld is that users who do not understand \nerror reports become frustrated and ignore those reports [1]. In this paper, we propose a new technique \nfor assisting users in classifying error reports when automated static analyses fail to verify a program. \nOur technique allows veri.cation tools to interact with users by computing small, relevant queries that \ncapture exactly the facts that the analysis is missing to either verify the program or prove the existence \nof a real error. These queries are then presented to a user who decides whether the answer to the query \nis yes or no. The .rst kind of query we compute is a proof obligation query, which asks the user whether \na property P is a program invariant. These queries have the key characteristic that, if P is indeed an \ninvariant, then the program is error-free. Furthermore, proof obli\u00adgation queries are as simple and as \ngeneral as possible: they capture exactly, and only, the information the analysis is missing to verify \nthe program. Our analysis also computes a second kind of query, called a failure witness query, which \nasks the user whether a prop\u00aderty P can arise in some execution. The key feature of a witness query is \nthat, if P can indeed occur in some execution, the program must have an error. Furthermore, our technique \ncomputes simplest and most general failure witness queries: they capture precisely the facts the analysis \nis missing to be certain that the program is buggy. Our approach to error report classi.cation is semi-automatic: \nIt computes simple and relevant queries to resolve the error report, but trusts a user to correctly answer \nthem. Since this technique is only useful when an automated analysis has been unable to verify the program, \nit is inevitable to ask the user for some help. In fact, the current best practice asks the user to do \nall the work, as manual report classi.cation is 100% trusted user information. Our approach minimizes \n-in a precise sense-the amount of trusted information the user must supply, both simplifying and speeding \nup her job while also increasing the accuracy of the classi.cation. There are four salient features \nunderlying our approach: We describe a static analysis algorithm that makes explicit all possible sources \nof incompleteness in static analyzers and uses facts inferred by any other program analysis technique. \n This representation allows the analysis to not only prove the absence of errors but also to prove their \npresence, i.e., in some cases, the analysis can decide that the program must be buggy.  When the analysis \ncan neither discharge nor validate a potential error, it computes small, relevant queries to be presented \nto the user, called proof obligations and failure witnesses, that ultimately allow the analysis to discharge \nor validate an error.  If the user s answer to a proof obligation query is yes , the analysis can discharge \nthe potential error. Similarly, if the user s answer to a failure witness query is yes , the analysis \ncan prove the existence of an error. Otherwise, our technique com\u00adputes new proof obligation and failure \nwitness queries, until the error can either be discharged or validated. Queries are posed to the user \nin order of increasing cost, meaning that the user is asked the questions that should be the easiest \nto answer .rst. Since state-of-the-art static analyzers typically lose information in few places, it \nis easier for the user to answer simple queries about a handful of missing analysis facts than to manually \nin\u00adspect and understand an entire program.  1.1 Informal Overview We illustrate key features of our \napproach with a simple example: void foo(int flag, unsigned int n) { 1: intk=1; 2: if(flag) k = n*n; \n3: inti=0,j=0; 4: while(i <= n) { 5: i++; 6: j+=i; 7: } 8: intz=k+i+j; 9: assert(z > 2*n); } This code \nsnippet contains an assertion at line 9 which is guar\u00adanteed to hold 1. Assume that a static analysis \nfails to verify this program and reports a potential assertion failure for line 9. Of course, just because \nthe tool reports a false alarm does not mean it has inferred no useful information about the program. \nFor instance, the analysis may have inferred that k s value after line 2 is at least 0 (as n*n is always \nnon-negative), and that the value of i after the loop is at least 0 and greater than n. Our goal is to \nutilize these facts inferred by the analysis to assist the user in classifying this report. The .rst \nkey idea underlying our technique is to model poten\u00adtial sources of incompleteness in static analyses \nusing abstraction variables. In this example, since the exact values of i and j are unknown after the \nloop, our technique introduces abstraction vari\u00adables ai and aj to model the unknown values of i and \nj at line 7. Similarly, assuming the analysis does not reason precisely about non-linear arithmetic, \nour technique introduces abstraction variable an*n to model the unknown result of the multiplication \nat line 2. Next, our technique utilizes invariants obtained by any static analysis to infer restrictions \non abstraction variables. For example, 1 In this example, we assume there are no integer over.ows. since \nan existing static analyzer has inferred that n*n is always non\u00adnegative, we have the side condition: \nan*n = 0 Similarly, since we know that the value of i after the loop is greater than n and at least \n0, the abstraction variable ai must satisfy: ai = 0 . ai >n Thus, for line 9, our technique obtains \nthe following invariant I: I =(an*n = 0 . ai = 0 . ai >n . n = 0) Here, the .rst three predicates in \nI express invariants obtained from an existing static analysis, and the last predicate n = 0 expresses \nthat unsigned variables are non-negative. Now that we can explicitly name each potential source of im\u00adprecision, \nour technique computes an exact symbolic value set for each expression in the program. For instance, \nthe value of variable k at line 2 is represented by the symbolic value set: {(1, \u00ac.ag), (an*n, .ag)} \n meaning that k has value 1 under constraint \u00ac.ag and the unknown value represented by an*n under condition \n.ag. Similarly, the values of variables i and j at line 7 after the loop are represented by the singleton \nvalue sets {(ai, true)} and {(aj , true)}. Since z is obtained by adding k, i, and j, we can use symbolic \nvalue sets of k, i, and j to compute the symbolic value of z, given by: {(1 + ai + aj , \u00ac.ag), (an*n \n+ ai + aj , .ag)} Since the assertion at line 9 succeeds if the value of z is greater than 2*n, we can \nuse the symbolic value set for z to compute the condition f under which the assertion holds: f = (1+ai+aj \n> 2*n .\u00ac.ag) . (an*n+ai+aj > 2*n . .ag) Observe that the assertion is veri.ed if the invariants I entail \nthe success condition f of the assertion, i.e.,: I|= f Similarly, we know that the assertion is guaranteed \nto fail if: I|= \u00acf In this example, since neither I|= f nor I|= \u00acf, the analysis cannot discharge or \nvalidate the potential assertion failure. Thus, our goal is to identify the possible facts the analysis \nis missing to either discharge or validate the error and ask the user whether these facts indeed hold. \nOur insight is that this problem is an instance of the abductive inference problem in logic, where the \ngoal is to .nd an explanatory hypothesis for a desired outcome. For\u00admally, given known facts F and a \ndesired outcome O, an abductive inference problem is to .nd an explanation E such that: F . E |= O and \nSAT(F . E) In other words, the abduction E is consistent with known facts F , and together with F , \nis suf.cient to explain O. In our setting, the desired outcome is to either prove the absence of an error \nor to validate its existence. Thus, we solve two abductive inference problems, one to infer the missing \ninformation necessary to verify the program, and one to .nd the missing facts to validate the presence \nof an error. Speci.cally, to prove the absence of an error, we compute a proof obligation G by solving \nthe following abductive inference problem: I. G |= f and SAT(I. G) In other words, a proof obligation \nG is consistent with program invariants I and, along with I, is suf.cient to discharge the error. Furthermore, \nwe are not interested in just any solution to the abduc\u00adtive inference problem; what we want is a weakest \nminimum proof obligation so that the queries presented to the user are as small and as general as possible. \nIn this example, using the techniques of Sec\u00adtion 4, we compute a weakest minimum proof obligation as: \n aj = n Thus, if the user can show that j >= n always holds at line 7, the analysis can discharge the \npotential error. Dually, to prove the presence of an error, we compute a failure witness . by solving \na second abductive inference problem: I. . |= \u00acf and SAT(I. .) In other words, a failure witness is also \nconsistent with program invariants, and if . holds in some execution, we know that the program must have \nan error. Again, we are not interested in any solution to the abductive inference problem; instead, we \nwant to .nd a weakest minimum failure witness to ensure that the queries we compute are as small and \nas general as possible. For our running example, techniques described in Section 4 yield the following \nweakest minimum failure witness: \u00ac.ag . ai + aj < 0 Thus, if the user can show that i+j< 0 is possible \nat line 7 in an execution where !flag holds, the analysis can validate the error. After computing weakest \nminimum proof obligations and fail\u00adure witnesses, our technique then decides whether it is more promising \nto try to discharge the error or to validate it by com\u00adparing the costs of the proof obligation and failure \nwitness. In this example, our technique decides that it is more promising to try to discharge the error \nand therefore queries the user whether j>=n is a program invariant at line 7. Since it is easy to show \nthat j >= n always holds at line 7, the analysis can immediately discharge the error. Observe that, although \nthe assertion condition in this example requires reasoning about values of multiple variables i, j, k, \nand z, our technique can take advantage of facts already established by the analysis to compute a simple \nand intuitive query involving only variable j.  1.2 Organization and Contributions The rest of this \npaper is organized as follows: Section 2 de.nes a simple language in which we formalize our technique. \nSection 3 describes a static analysis that makes explicit potential sources of imprecision and performs \nsymbolic value propagation. Section 4 de.nes weakest minimum proof obligations and failure witnesses, \ndescribes a technique for computing them, and presents an itera\u00adtive algorithm for validating or discharging \nerror reports. Section 5 describes our implementation; Section 6 presents experimental re\u00adsults. Finally, \nSection 7 surveys related work, and Section 8 con\u00adcludes. In summary, this paper makes the following \ncontributions: We present a new technique for semi-automatic report classi.\u00adcation when static analyzers \nare unable to verify the program.  We de.ne weakest minimum proof obligations and failure wit\u00adnesses \nas a technical characterization of simple, relevant facts useful for resolving error reports.  We present \nthe problem of computing proof obligations and failure witnesses as an abductive inference problem and \ngive a new algorithm for computing abductions in this setting.  We show how proof obligations and failure \nwitnesses can be used to interact with users until a potential error is resolved.  We perform a user \nstudy to evaluate our technique. Our results show that the new technique is very useful both for improving \nthe time required to classify error reports as well as for dramat\u00adically improving the accuracy of report \nclassi.cation. Speci.\u00adcally, our approach improves classi.cation accuracy from 33%  ..{+, -, *}S . e1 \n: c1 S . e2 : c2 S . v : S(v) S . c : cS . e1 . e2 : c1 . c2 S . e1 : c1 S . e2 : c2 true if c1 . c2 \nlop . {., .}b = false otherwise S . p1 : b1 S . p2 : b2 S . e1 . e2 : bS . p1 lop p2 : b1 lop b2 S . \np : bS . e : c S .\u00acp : \u00acbS . v = e : S[c/v] S . skip : S S . p : true S . s1 : S1 S . p : false S . s2 \n: S2 S . if(p) then s1 else s2 : S1 S . if(p) then s1 else s2 : S2 S . p : true S . s : S. S . s1 : S1 \nS1 . s2 : S2 S. . loop.(p){s} : S.. S . s1; s2 : S2 S . loop.(p){s} : S.. S . loop.(p){s} : S. S. . p. \n: true S . p : false S . while.(p){s}[@p.]: S. S . loop.(p){s} : S S =[c1/a1,...,ck/ak][0/v1,..., 0/vn] \nS . s : S. S. . p : b . ..a.(let .v in (s; check(p)))(c1,...ck): b Figure 1. Operational semantics of \nthe language from Section 2 to 90% and reduces the time programmers take to classify error reports from \napproximately 5 minutes to under 1 minute. 2. Language In this section, we present a simple programming \nlanguage that we use to formalize our technique: Program P := ..a. (let .v in (s; check(p))) Statement \ns := v = e | skip | s1; s2 | if(p) then s1 else s2 | while.(p){s}[@p .]? Expression e := v | c | c * \ne | e1 . e2 (..{+, -}) Predicate p := e1 . e2 (..{<, >, =}) | p1 . p2 | p1 . p2 |\u00acp In this language, \na program with inputs .a and local variables .v consists of a statement s and a check(p) statement, which \nchecks whether predicate p evaluates to true. The program evaluates to true if predicate p holds, and \nto false otherwise. We say that an execution of a program P is error-free if P evaluates to true in this \nexecution, and buggy otherwise. Similarly, we say that program P is error-free if P evaluates to true \nin all possible executions, and buggy if P evaluates to false in some execution. Statements in this language \nconsist of assignments (v = e, where v is bound in the let statement) , skip statements, sequenc\u00ading \n(s1; s2), if statements, and while loops labeled with unique identi.ers .. Observe that while loops can \nbe optionally tagged with annotations of the form @p ., where predicate p . corresponds to invariants \nthat hold after the loop. For the purposes of this paper, these invariants may be obtained from any automatic \nsound static analysis technique, such as abstract interpretation or predicate ab\u00adstraction. We say that \na program P is analyzed if all while loops are annotated with sound post-conditions using a static analyzer. \nExpressions include integer variables v, integer constants c, and arithmetic operations. This language \nhas an expressive family of predicates which include comparisons between expressions, as well as conjunction, \ndisjunction, and negation. The operational semantics is given in Figure 1. We omit function calls from \nthis .1 = {(p1,f1),..., (pk,fk)}.2 = {(p. ,f. ),..., (p. ,f. )} 11nn . = ((pi . p. ), (fi . f. )) ij \njj . .1 . .2 : . .1 = {(p1,f1),..., (pk,fk)}.2 = {(p. ,f. ),..., (p. ,f. )} . 11nn f = ((pi . p. ) . \nfi . f. ) ijj j . .1 . .2 : f .. =(pi, (fi . f)) (pi,fi).. . . . f : .. Figure 2. Operations on symbolic \nvalue sets ..{+, -, *}S . e1 : .1 S . e2 : .2 S . v : S(v) S . c :(c, true) S . e1 . e2 : .1 . .2 Figure \n3. Symbolic evaluation rules for expressions language, as the issues raised by function calls are orthogonal \nand not necessary for understanding our technique. However, our implementation is an interprocedural \nanalysis (see Section 5). 3. Analysis In this section, we describe a static analysis that is performed \nafter a veri.cation tool already analyzed the program, inferring the @p . annotations on while loops \nand reporting a potential error. Our analysis, which is a prerequisite for computing relevant queries \nto classify the error report, has the following key characteristics: Values of program variables are \nrepresented by symbolic ex\u00adpressions consisting of constants and analysis variables.  There are two \nkinds of analysis variables: input variables . rep\u00adresent unknown values of program inputs, and abstraction \nvari\u00adables a model unknown values of variables due to an impreci\u00adsion in the analysis. For instance, \nabstraction variables represent values that may be unknown after loops.  The analysis uses facts inferred \nby other analyzers, which are annotated using the @p . construct on loops. These invariants are used \nto constrain values of abstraction variables.  The only source of imprecision in this analysis is loops; \nit performs exact symbolic value propagation on loop-free code.  Our static analysis is described in \nFigures 2, 3, 4, and 5. Values of program variables are represented as symbolic expressions p: p := . \n| a | c | p1 + p2 | p1 - p2 | c * p Besides input variables . and abstraction variables a, symbolic ex\u00adpressions \nare integer constants c, addition or subtraction of sym\u00adbolic expressions, and linear multiplication. \nSince the only impre\u00adcision of the static analysis for the simple language from Section 2 is due to loops, \nabstraction variables are only used to model the (potentially) unknown values of program variables after \nloops. In the analysis, environment S maps program variables to sym\u00adbolic value sets .: (p,f) . := 2 \nwhere p is a symbolic expression and f is a constraint. Since variables may have different symbolic values \non different program paths, the constraint f allows the analysis to keep values on differ\u00adent paths separate. \nFor concreteness, constraints in this paper are in the theory of of linear arithmetic over integers. \nS . e1 : .1 lop . {., .}S . p1 : f1 S . e2 : .2 S . p2 : f2 S . p : f S . e1 . e2 : .1 . .2 S . p1 lop \np2 : f1 lop f2 S . \u00acp : \u00acf Figure 4. Symbolic evaluation rules for predicates S . e : . S, I. s1 : S1, \nI1 S. = S[./v] S1, I1 . s2 : S2, I2 S, I. v = e : S. , I S, I. skip : S, I S, I. s1; s2 : S2, I2 S . \np : f S, I. s1 : S1, I1 S, I. s2 : S2, I2 S. =(S1 . f) . (S2 .\u00acf) I. = ((f .I1) . (\u00acf .I2)) S, I. if(p) \nthen s1 else s2 : S. , I. S. .. = S[(a1, true)/v1,..., (ak, true)/vk])(.v modi.ed in s) S, I. loop.(p){s} \n: S. , I S, I. loop.(p){s} : S. , I S. . p: f S, I. while.(p){s}[@p.]: S. , I. f S = [(.1, true)/a1,..., \n(.k, true)/ak] S. = S[(0, true)/v1,..., (0, true)/vn] S. S.. . p : f , true . s : S.. , I . ..a.(let \n.v in (s; check(p))) : I,f Figure 5. Transformers for the static analysis Figure 2 de.nes some useful \noperations on symbolic value sets. The .rst rule .1 . .2 describes how to perform arithmetic operations \non symbolic value sets, where . ranges over +, -, * and where . is the symbolic value set representing \nthe result of the arithmetic operation. The second rule .1 . .2 (where . is <, >, or =) describes how \nto compare value sets .1 and .2. The result is a constraint f, which describes the condition under which \n.1 is less than, greater than, or equal to .2. Finally, the last rule in this .gure de.nes what it means \nto conjoin a constraint f with a value set .. Figures 3 and 4 describe symbolic evaluation of expressions \nand predicates, and are direct analogues of the corresponding oper\u00adational semantics rules in Figure \n1, with integer constants replaced by symbolic value sets and boolean constants with constraints. The \n.rst six rules in Figure 5 describe the transformers for statements and derive judgements of the form: \nS, I. s : S. , I. Since statements may modify values of program variables, each statement may modify \nSand produce a new symbolic store S.. The constraints I and I. describe invariants about abstraction \nvariables obtained from annotations on while loops. The .rst three rules in Figure 5 are self-explanatory \nand are straightforward analogues of their concrete counterparts from Fig\u00adure 1. In the rule for if statements, \nfacts that are obtained by ana\u00adlyzing the then branch s1 (resp. else branch s2) only hold under the conditional \np (resp. \u00acp). Therefore, we .rst compute the sym\u00adbolic evaluation of conditional p as f and conjoin f \nto all facts obtained in the then branch and \u00acf to facts obtained in the else branch. In this rule, conjunction \non symbolic stores is de.ned as: .v . dom(S). (S. f)(v)= {(pj ,fj . f) | (pj ,fj ) . S(v)} This rule \nalso uses an (exact) join operation . on symbolic stores, de.ned as: (p, f) . S1(v) . (p, f.) . S2(v) \n. (p, f . f.) . (S1 . S2)(v) (p, f) . Si(v) . (p, ) .. Sj (v) . (p, f) . (S1 . S2)(v)  Thus, in the \nrule for if statements, the symbolic store S. is obtained by conjoining f to all bindings in S1, \u00acf to \nall bindings in S2, and then combining the resulting symbolic stores. Similarly, observe that a new invariant \nI. is obtained as: (f .I1) . (\u00acf .I2) because invariants obtained from the then branch only hold under \nf, while invariants obtained in the else branch hold under \u00acf. The rule for while loops does not infer \nany loop invariants, but instead uses loop postconditions already inferred by other analyses annotated \nusing the @p . construct. For this reason, we .rst bind the values of all variables v1,...,vk modi.ed \nin the loop body to fresh abstraction variables a.,...,a. in the loop.(p){s} helper rule. 1k Now, to \nutilize any known invariants on the values of v1,...,vk, we symbolically evaluate the annotated invariant \n@p . as constraint f and obtain a new invariant I. f after analyzing the while loop. Observe that since \nthe annotation @p . is symbolically evaluated under store S. and since loop modi.ed variables are bound \nto their corresponding abstraction variables a in S., the new invariant f constrains values of abstraction \nvariables introduced in this loop. The last rule in Figure 5 describes the analysis of the whole program. \nThe initial store is obtained by binding the arguments a1,...,ak to analysis variables .1,...,.k and \ninitializing local variables to 0, as speci.ed by the concrete semantics. The result of the analysis \nis a pair of constraints I,f where I represents all known invariants on the abstraction variables and \nf represents the condition under which the program evaluates to true. LEMMA 1. Let P be any program such \nthat . P : I,f. If I|= f, then P is error-free (i.e., evaluates to true in all executions). PROOF 1. \nGiven in the extended version of this paper [2]. LEMMA 2. Let P be any program such that . P : I,f. If \nI|= \u00acf, then P must be buggy. PROOF 2. Given in the extended version of this paper [2]. At .rst glance, \nthe condition in Lemma 2 may seem too strong as the satis.ability of the formula I.\u00acf is suf.cient to \nestablish that the program may be buggy. However, here, we are interested in proving that the program \nmust be buggy, which we can only guarantee if I|= \u00acf. EXAMPLE 1. Consider the following program with \nannotated in\u00advariants obtained from a static analysis: .a1,a2.(let k, i, j, z in ( if(a2 > 0) then k \n= a2 else k = 1; while1(i<a2 + 1){ i = i + 1; j = j + i; }@[i> -1 . i>a2] if(a1 > 0) then z = k + i \n+ j else z =2 * a2 + 1; check(z> 2 * a2) )) Applying the analysis rules described in this section, we \nobtain: I = a1 i = 0 . a1 i >.2 and success condition f: (.2 + a1 i + aj 1 > 2.2 . .2 > 0 . .1 > 0) . \n(1 + a1 i + aj 1 > 2.2 . .2 = 0 . .1 > 0) . (2.2 +1 > 2.2 . .1 = 0) Since neither I|= f nor I|= \u00acf, the \npotential error can neither be discharged nor validated. 4. Query Guided Error Diagnosis As Lemmas 1 \nand 2 from Section 3 show, our analysis can some\u00adtimes prove that a program is error-free or de.nitely \nbuggy. Unfor\u00adtunately, in many cases, the outcome of the analysis is not de.nite: It can neither discharge \nthe potential error nor prove that the pro\u00adgram is buggy. In this case, the error report must be inspected \nby a human who classi.es the report as a genuine bug or false alarm. As mentioned in Section 1, this \nsituation is problematic because manual classi.cation is time-consuming, dif.cult, and error-prone. In \nthis section, we describe a novel technique for computing small, relevant queries that systematically \nallow de.nite classi.ca\u00adtion of error reports. More technically, our goal is to identify rele\u00advant facts \nsuch that, along with these facts, either Lemma 1 applies (in which case, the report is de.nitely a false \nalarm) or Lemma 2 holds (in which case the report is con.rmed to be a genuine bug). 4.1 Weakest Minimum \nQueries to Discharge Error Suppose that, for a given program P , the analysis from Section 3 derives \nthe judgment . P : I,f but neither Lemma 1 nor Lemma 2 applies. To be able to discharge the error, what \nwe need to do is to .nd a formula G such that, along with I, G implies the success condition f. Speci.cally, \nwe need to solve the following abductive inference problem: DEFINITION 1. (Proof Obligation) Given known \nfacts I and suc\u00adcess condition f,a proof obligation is a formula G such that: G .I|= f and SAT(G .I) \n Thus, a proof obligation G is consistent with known program invariants I, and together with I allows \nus to prove the success condition f. Observe that there is always a trivial proof obligation, namely \nG= f. In other words, a trivial way to discharge the error is simply to ask the user to show the success \ncondition f itself! Therefore, we are not interested in just any proof obligation, but simple, intuitive \nproof obligations that are local and easy for the user to decide. To make precise what we mean by simple \nproof obligations, it is necessary to assign a cost to each formula G. In this paper, we assign costs \nto proof obligations as follows: DEFINITION 2. (Cost of Proof Obligation) Let G be a proof obli\u00adgation \nquery for I,f, and let .p be a mapping from variables to costs such that .p(a)=1 for abstraction variable \na and .p(.)= |Vars(f) . Vars(I)| for input variable .. Then: Cost(G)= .p(v) v.Vars(G) The above cost \nfunction assigns cost 1 to each abstraction vari\u00adable a and cost |Vars(f) . Vars(I)| to each input variable \n.. Of course, it is impossible to exactly capture the intuitiveness of a pro\u00adgram fact with a mathematical \nformula, and any de.nition of query cost necessarily approximates the human effort required to decide \nthat query. We choose to approximate the simplicity of a query as given by De.nition 2 because it captures \nthe following intuitions: Local facts are generally easier to decide for humans: Our cost function assigns \na lower cost to formulas with fewer abstraction variables, because queries involving fewer variables \nrequire the user to reason about fewer sources of analysis imprecision.  In general, it is undesirable \nto make assumptions about the pro\u00adgram s execution environment (i.e., inputs in our language): For this \nreason, the cost function assigns a higher cost to formulas containing input variables .. In fact, the \nintuition behind assign\u00ading cost |Vars(f).Vars(I)| to input variables . is to ensure that it is more \nexpensive to constrain a single input vs. all sources   of imprecision. In other words, if there is \nany way to discharge the error without constraining the program s execution environ\u00adment, the analysis \nwill ask those queries .rst. While this cost de.nition could be re.ned, we found it to cap\u00adture simplicity \nwell in practice (see Section 6). Furthermore, the techniques we describe are agnostic to the de.nition \nof query cost; we only require that cost is a function of variables in the formula. Given a function \nthat assigns costs to queries, we can now de.ne a special kind of proof obligation that we are interested \nin computing: DEFINITION 3. (Weakest Minimum Proof Obligation) Given facts I and success condition f,a \nweakest minimum proof obliga\u00adtion is a formula G such that: 1. G .I |= f and SAT(G .I) 2. For any other \nG. that satis.es (1), either Cost(G) < Cost(G.) or Cost(G) = Cost(G.) . (G .. G. . G . G.)  Thus, a \nweakest minimum proof obligation G has cost less than or equal to any other proof obligation, and G is \nno stronger than other proof obligations with the same minimum cost as G. As argued earlier, the condition \nof minimality captures the intuitive notion of simplicity. Furthermore, the requirement that G is no \nstronger than other proof obligations with the same cost expresses generality. In particular, we do not \nwant to ask the user about invariants that are stronger than necessary to discharge the error. 4.1.1 \nComputing Weakest Minimum Proof Obligations We now turn to the problem of how to compute weakest minimum \nproof obligations. First, observe that we can rewrite G .I |= f as: G |= I. f Thus, G is a formula with \nminimum cost that entails I. f. Our insight is that we can make use of a minimum partial satisfying assignment \nof I. f to compute the weakest G with minimum cost. DEFINITION 4. (Cost of Partial Assignment) Let s \nbe a partial assignment for a formula f and let . be a mapping from variables in f to non-negative integers. \nThe cost of partial assignment s, written Cost(s) is .(v) v.Vars(s) DEFINITION 5. (Minimum Satisfying \nAssignment) Given map\u00adping . from variables to costs, a minimum satisfying assignment of formula . is \na partial assignment s to a subset of the variables in . such that: s(.) = true  .s. such that s.(.) \n= true, Cost(s) = Cost(s.)  In this paper, we do not address the problem of computing minimum satisfying \nassignments for formulas. An algorithm for computing minimum satisfying assignments in theories that \nadmit quanti.er elimination is described in [3]. Since the theory of linear arithmetic over integers \nused in this paper admits quanti.er elimi\u00adnation, the algorithm of [3] is directly applicable. Minimum \nsatisfying assignments are useful because they allow us to determine the minimum set of variables that \nany proof obli\u00adgation G must contain. However, we are not interested in any min\u00adimum satisfying assignment \nto I. f since some minimum satis\u00adfying assignments make I. f true by falsifying I. Such assign\u00adments \nare not interesting in this context because they violate known invariants I about the program. Thus, \nwhat we want is a minimum satisfying assignment to I. f that is consistent with I. DEFINITION 6. (Consistent \nMinimum Satisfying Assignment) A minimum satisfying assignment s of . is consistent with .. if s(..) \nis satis.able. Suppose s is a minimum satisfying assignment of I. f consistent with I. Now, if we interpret \ns as a logical formula Fs (i.e., a conjunction of equalities between variables and constants), Fs is \nin fact a proof obligation, since, by de.nition: Fs |= I. f and SAT(Fs .I) Furthermore, Fs also has \nminimum cost, but, it is not the weakest proof obligation. More speci.cally, since Fs assigns each variable \nto a concrete value, it is in fact a strongest proof obligation with minimum cost and is not very likely \nto be a valid program invariant. What we are really after, then, is a formula G containing only the variables \nin s but that is also the weakest formula containing these variables that still entails I. f. In other \nwords, what we want is the weakest suf.cient condition of I. f containing only the variables in s. LEMMA \n3. Let V be the set of variables in a minimum satisfying assignment of I. f consistent with I, and let \nV be the set of variables in I. f but not in V . We can obtain a weakest minimum proof obligation by \neliminating the quanti.ers from the formula: .V. (I. f) PROOF 3. The fact that this formula is a minimum \nproof obligation follows from De.nition 6. The fact that this formula is the weakest such one follows \nfrom the well-known result that the weakest suf.\u00adcient condition of a formula f containing only variables \nV is given by .V .f and from the fact that linear integer arithmetic admits quanti.er elimination. Remark: \nWhile the formula computed as described in Lemma 3 yields a weakest minimum proof obligation, this formula \nmay be redundant. In other words, parts of this formula may already be implied by the known invariants \nI. Thus, to avoid unnecessary queries, we simplify the formula computed as in Lemma 3 with respect to \nI. This can be achieved, for instance, by using the sim\u00adpli.cation algorithm of [4], with I as the initial \ncritical constraint. EXAMPLE 2. We now illustrate the computation of the proof obli\u00adgation G on Example \n1, where we already computed I and f. Here, a minimum satisfying assignment to I. f that is consistent \nwith I is a1 j =0. Now, we eliminate the quanti.ers from the formula: ..1,.2,ai 1 . (I. f) which, after \nsimpli.cation, yields a1 j = 0. Thus, the assertion in Example 1 can be proven if the user shows that \nj = 0 after the loop, which is indeed the case. Observe that the query we pose to the user is much simpler \ncompared to f. 4.1.2 Deciding Proof Obligation Queries As discussed earlier, a proof obligation G is \na query presented to a user, and, if true, proves the absence of an error in the program. Formally, we \nde.ne a valid answer to a proof obligation query: DEFINITION 7. (Valid Answer to Proof Obligation Query) \nWe say that the answer to a proof obligation query G is valid iff: The answer is either yes or no  \nIf the answer if yes, then G holds on all program executions (i.e., G is a program invariant)  If the \nanswer is no, then there is at least one execution in which G is violated  In practice, a user may \nnot always be able to give a de.nite yes/no answer to a proof obligation query; thus it is reasonable \nto accept I don t know as an answer. We discuss how to extend our technique to handle I don t know answers \nto queries in Section 5. In this paper, we do not address the problem of verifying that the user s answer \nto a query is correct. Since our technique only applies in cases where automated veri.cation fails, it \nis dif.cult, and sometimes even impossible, to completely eliminate trusted user information. Thus, our \ngoal is to minimize the amount of trusted user information required to verify the program, making the \njob of the user much easier and improving classi.cation accuracy. LEMMA 4. Let G be a proof obligation \nquery, and suppose yes is a valid answer to this query. Then, the program is error-free. PROOF 4. This \nfollows immediately from the de.nition of proof obligation and validity of answer to the query.  4.2 \nWeakest Minimum Queries to Validate An Error In this section, we now turn to the complementary problem \nof validating the existence of a real error in the program. Recall from Lemma 2 that the condition I|= \n\u00acf guarantees that the program must contain a real error. Now, to validate the existence of an error, \nwe need to solve the same kind of abductive inference problem we considered in the previous section. \nMore speci.cally, we need to .nd a formula ., called a failure witness, de.ned as follows: DEFINITION \n8. (Failure Witness) Given facts I and success con\u00addition f of a program, a failure witness is a formula \n. such that: I. . |= \u00acf and SAT(. .I) As in the case of proof obligations, we are not interested in any \nfailure witness, as some witnesses may be more complicated than necessary or overly restrictive. Thus, \nto be able to ask the user simple and general witness queries, we want to obtain a weakest minimum failure \nwitness. For this purpose, we de.ne cost of failure witnesses as follows: DEFINITION 9. (Cost of Failure \nWitness) Let . be a failure wit\u00adness for I,f, and let .w be a mapping from variables to costs such that \n.w(.)=1 for input variable . and .w(a)= |Vars(f) . Vars(I)| for abstraction variable a. Then: Cost(.)= \n.w(v) v.Vars(.) Similar to that for proof obligations, this cost function is bi\u00adased in favor of queries \ninvolving local facts, as it penalizes wit\u00adness queries involving more abstraction variables. But in \ncontrast to proof obligations, we prefer witness queries involving only in\u00adputs to those involving abstraction \nvariables. Since it is in general undesirable to make assumptions about the program s inputs, it is likely \nthat witness queries involving input variables are easy to an\u00adswer af.rmatively, and are thus assigned \na lower cost. While this cost de.nition merely approximates the simplicity and usefulness of a witness \nquery, we found it to work well in practice. We can now de.ne weakest minimum failure witness: DEFINITION \n10. (Weakest Minimum Failure Witness) Given known facts I about a program P and success condition f,a \nweakest minimum failure witness of P is a formula . such that: 1. . .I |= \u00acf and SAT(. .I) 2. For any \nother .. that satis.es (1), either Cost(.) < Cost(..) or Cost(.) = Cost(..) . (. .. .. . . . ..) where \nCost is de.ned according to De.nition 9.  The computation of weakest minimum failure witnesses is anal\u00adogous \nto the computation of weakest minimum proof obligations: LEMMA 5. Let V be the set of variables in a \nminimum satisfying assignment of I.\u00acf consistent with I, and let V be the set of variables in I . \u00acf \nbut not in V . Then, a weakest minimum failure witness is obtained by eliminating the quanti.ers from \nthe formula: .V. (I.\u00acf) 4.2.1 Deciding Failure Witness Queries A failure witness is a query such that \nif the user answers yes , then there exists at least one execution in which the program fails. Thus, \na valid answer to a witness query is de.ned as follows: DEFINITION 11. (Valid Answer to Witness Query) \nWe say that the answer to a failure witness query . is valid iff: The answer is either yes or no  If \nthe answer is yes, then there exists at least one program execution where . holds  If the answer is \nno, then there is no execution in which . holds,  i.e. \u00ac. is a program invariant. Observe that answering \nyes to a witness query has very different semantics than answering yes to a proof obligation query: In \nthe former case, . needs to hold in only one execution (existential se\u00admantics), whereas in the latter \ncase, G must hold in all executions (universal semantics). Furthermore, observe that answering no to \na witness query . is equivalent to answering yes to a proof obli\u00adgation query \u00ac.. Similarly, answering \nno to a proof obligation query G is equivalent to answering yes to a witness query \u00acG. LEMMA 6. Let . \nbe a failure witness query, and suppose yes is a valid answer to this query. Then, the program exhibits \nan error in at least one execution. PROOF 5. This follows from the de.nition of failure witness and validity \nof answer to the witness query.  4.3 The Full Algorithm We now describe the algorithm, presented in \nFigure 6, for system\u00adatically generating a sequence of queries until the error report is resolved. This \nalgorithm takes as input the known program invari\u00adants I and the success condition f. The set W is a \nset of witnesses, i.e., conditions known to hold in some execution of the program. In lines 5-6 of the \nalgorithm, we .rst compute proof obligation G as described in Section 4.1. Here, compute msa(., S, .p) \ncom\u00adputes the set of variables in the minimum satisfying assignment of . consistent with the set of formulas \ngiven by S with respect to cost mapping .p (recall De.nition 2). As discussed in Section 4.1, the minimum \nsatisfying assignment needs to be consistent with I, since we do not want to issue queries about facts \nthat violate known program invariants. Furthermore, we want the minimum satisfying assignment to be consistent \nwith all learned witnesses in W , since we do not want to ask the user queries about facts for whose \nviola\u00adtion we already have witnesses. In line 4, the notation V1 denotes the set of variables that are \nin I. f but that are not part of the minimum satisfying assignment. As stated by Lemma 3, we can compute \nthe weakest minimum proof obligation G by eliminating the universal quanti.ers from the formula .V1. \nI. f. Dually, lines 7-8 compute a weakest minimum failure witness . as described in Section 4.2: We .rst \ncompute the minimum satisfying assignment of I. \u00acf consistent with I with respect to cost function .w \n(recall De.nition 9). Again, it is important that the minimum satisfying assignment is consistent with \nI, as we do not want to ask queries that we know cannot hold in any execution. However, in this case, \nit is not necessary that the assignment is consistent with W since the failure witness needs to hold \nfor only function diagnose error(I,f): 1: set W = \u00d8; 2: while(true){ 19 : } 3 : if(VALID(I . f)) return \nERROR DISCHARGED 4 : if(.. . W. UNSAT(I . . . f)) return ERROR VALIDATED 5 : V1 = compute msa(I . f, \nW . I, .p) 6 : G = elim quantifier(.V1.(I . f)) 7 : V2 = compute msa(I . \u00acf, I, .w) 8 : . = elim quantifier(.V2.(I \n. \u00acf)) 9 : 10 : if(Cost(G) = Cost(.)){Q1 = form invariant query(G) 11 : if(answer to Q1= YES) return \nERROR DISCHARGED 12 : W := W . (\u00acG) 13 : } 14 : 15 : else{Q2 = form witness query(.) 16 : if(answer to \nQ2= YES) return ERROR VALIDATED 17 : I := I . (\u00ac.) 18 : } Figure 6. Algorithm for Diagnosing Error Reports \none, rather than for all, executions. Finally, as shown by Lemma 5, we compute the weakest minimum failure \nwitness by eliminating quanti.ers from the formula .V2. I.\u00acf, where V2 is the set of variables in I.\u00acf, \nbut not in V2. At line 9, we compare the costs of the proof obligation G and the failure witness . to \ndecide whether it is more promising to try to discharge the error or to validate it. If the cost of G \nis cheaper, then at line 10, we formulate the proof obligation G as an invariant query (discussed further \nin Section 4.4). If the user answers the invariant query positively, we have discharged the error. On \nthe other hand, if the user decides that G is not an invariant, then we have learned a new witness: We \nnow know there exists at least one execution where G is violated; thus we add \u00acG to the set of witness \nformulas. On the other hand, if the error validation strategy has lower cost than the error discharge \nstrategy (lines 14-18), we formulate . as a witness query (again, see Section 4.4). If the answer to \nthis query is yes , we have validated the error. On the other hand, if the answer to the witness query \nis no , this means there is no execution in which . holds. In this case, we have learned that \u00ac. is an \ninvariant, and we conjoin it with the existing invariants I. Observe that at lines 3-4, we check whether \nthe error can be discharged or validated with the current invariants I and witnesses W . This check is \nnecessary because even when the user answers no to a query, we still learn additional facts as a result. \nAlso, observe that the queries computed by our algorithm may increasingly become more dif.cult over time. \nAs the analysis pro\u00adgresses, I becomes stronger and more witnesses are acquired; thus, the minimum satisfying \nassignment needs to be consistent with more facts and may therefore contain more variables. In fact, \nin the unlikely case where the user answers no to all queries, the .nal query may degenerate into the \nsuccess condition itself.  4.4 From Formulas to Queries So far, we focused on computing small, relevant \nformulas that are useful for classifying error reports. We now discuss how to present these formulas \nin a way that can be easily understood by humans. The .rst challenge is to translate analysis variables \ninto program expressions that can be understood without knowledge of internal analysis details. This \nproblem is easily solved in our representation since there is a straightforward mapping between analysis \nvariables and program expressions: Input variables .i used in the analysis represent values of program \ninputs ai, and abstraction variables ai. represent values of program variables vi at program point .. \nThe second challenge is that query formulas presented to the user may be arbitrary boolean combinations \nof program expres\u00adsions. Since it is unrealistic to expect programmers not well-versed in logic to easily \nreason about complicated boolean expressions, we need to decompose queries with complex boolean structure \nto a series of simpler queries. To achieve this goal, we observe that invariant queries distribute over \nconjuncts, while witness queries distribute over disjuncts. More speci.cally, if f1 . f2 is an invari\u00adant, \nit is necessary that f1 and f2 are independently also invariants. Similarly, if f1 . f2 is a witness, \nit is necessary that f1 is possible in some execution or f2 is possible in some execution. Thus, we .rst \nconvert invariant queries to conjunctive normal form (CNF) and treat each clause as a separate, independent \nquery. Dually, we convert witness queries to disjunctive normal form (DNF) and rea\u00adson about each clause \nindependently. We also further decompose clauses into simpler queries. For witness queries, observe that \nif f1 and f2 are individually wit\u00adnesses, this does not mean f1 . f2 is also a witness, since f1 and \nf2 may be possible in distinct executions, but never possible in the same execution. In this case, we \ndecompose a conjunct f1 . f2 by .rst querying whether f1 is possible in some execution and then ask whether \nf2 is also possible in an execution where f1 holds. For invariant queries, clauses contain disjunctions, \nwhich hu\u00admans typically .nd dif.cult to reason about. Thus, for an invariant query f1 . f2, we .rst ask \nwhether either f1 or f2 are indepen\u00addently invariants, which is often the case. In cases where a truly \ndisjunctive invariant is required, observe that for f1 . f2 to be an invariant, \u00acf1 .\u00acf2 should not be \na witness. Thus, to avoid dis\u00adjunctions, we convert the invariant query f1 . f2 into the witness query \n\u00acf1 .\u00acf2 and use the same technique described in the pre\u00advious paragraph to decompose conjunctive witness \nqueries. In addition to making queries more understandable, decompos\u00ading queries into simpler, independent \nsubqueries also has another advantage: We can learn additional facts for every subquery even when the \nentire query may not be shown. For example, although f1 .f2 is not an invariant, f1 could still be an \ninvariant. It is easy to integrate this additional information learned from subqueries into the algorithm, \nand our implementation uses this optimization. 5. Implementation We have implemented the proposed technique \non top of the Com\u00adpass analysis framework for C programs [5 7]. While the simple language from Section \n2 contains only integer variables and no function calls, our implementation extends the technique to \ndeal with other features of the C language supported in Compass, such as pointers, arrays, and function \ncalls. In particular, Compass rea\u00adsons about heap objects and arrays using the techniques described in \n[5, 6] and uses a summary-based technique for interprocedural analysis [7]. Besides imprecise loop invariants, \nCompass also has other sources of imprecision, such as non-linear arithmetic or in\u00adline assembly, which \nare also modeled using abstraction variables. In addition, side effects of library functions that we \nchoose not to analyze or whose source code is unavailable are also modeled using abstraction variables. \nIn the algorithm from Section 4.3, we require users to give only yes/no answers to queries, but, in practice, \nthis requirement is un\u00adrealistic, as programmers cannot always answer a query. Thus, we allow users to \nanswer I don t know and extend the algorithm Manual classi.cation New technique LOC Kind Classi.cation \n% correct % wrong % ? Avg. time % correct % wrong % ? Avg. time Problem 1 88 synthetic false alarm 43.5 \n% 34.8 % 21.7% 297 s 92.3 % 3.9% 3.9 % 57 s Problem 2 352 real false alarm 30.8 % 50.0 % 19.2 % 269 s \n87.0 % 8.7% 4.4 % 40 s Problem 3 66 synthetic false alarm 46.2 % 38.5 % 15.4 % 266 s 79.2 % 20.8% 0.0% \n58 s Problem 4 278 real real bug 37.5 % 45.8 % 16.7 % 265 s 92.3 % 7.7 % 0.0% 53 s Problem 5 363 real \nfalse alarm 32.0 % 48.0 % 20.0 % 289 s 100.0 % 0.0 % 0.0 % 46 s Problem 6 173 real false alarm 25.0% \n54.2 % 20.8% 339 s 92.0 % 8.0 % 0.0 % 54 s Problem 7 326 real real bug 40.0 % 56.0 % 4.0% 233 s 79.2 \n% 8.3 % 12.5 % 55 s Problem 8 97 synthetic false alarm 16.7 % 70.8 % 12.5 % 271 s 92.0% 8.0% 0.0% 58 \ns Problem 9 116 synthetic real bug 25.0 % 58.3 % 16.7 % 308 s 92.0 % 4.0% 4.0% 62 s Problem 10 72 synthetic \nreal bug 24.0 % 60.0 % 16.0 % 455 s 95.8 % 4.2% 0.0 % 68 s Problem 11 118 synthetic real bug 41.7 % 45.8% \n12.5% 235 s 84.0 % 16.0% 0.0% 50 s Average 186 n/a n/a 32.9 % 51.1 % 16.0 % 293 s 89.6 % 7.3 % 2.3 % \n55 s Figure 7. Results from our user study from Section 4.3 to deal with such uncertainty. Speci.cally, \nour algorithm maintains a set of potential witnesses and potential in\u00advariants. If G is an invariant \nquery that the user cannot answer, we add G as a potential invariant and \u00acG as a potential witness. Sim\u00adilarly, \nif . is a witness query that the user cannot decide, we add . as a potential witness and \u00ac. as a potential \ninvariant. To avoid repeating the same queries, these potential witnesses and invari\u00adants are taken into \naccount when computing minimum satisfying assignments: Minimum satisfying assignments in the proof obli\u00adgation \ncase must be consistent with potential invariants and wit\u00adnesses, while the minimum satisfying assignments \nfor the witness case must be consistent with potential invariants. Our technique utilizes minimum satisfying \nassignments for computing abductions, but no off-the-shelf SMT solver we know of provides this functionality. \nThus, we have integrated the required functionality for providing minimum satisfying assignments into \nour own Mistral SMT solver; the algorithm we implemented for this purpose is described in [3]. 6. Experiments \nTo evaluate the proposed technique, we performed a user study in which we asked professional programmers \nto classify error reports as genuine bugs or false alarms. Our study consists of 11 problems, .ve of \nwhich are code fragments taken directly or slightly modi.ed from real open source C programs (e.g., Unix \ncoreutils, OpenSSH), and six of which are benchmarks from our program analysis test suite. These benchmarks \nare available from http://www.cs.wm.edu/ tdillig/pldi12-benchmarks.tar.gz. All benchmarks contain between \n66-363 lines of code and one asser\u00adtion. For examples drawn from real applications, we only included \nthose parts of the code that are relevant to deciding the validity of the assertion (i.e., a manual slice). \nFive of our benchmark prob\u00adlems contain real errors while the remaining six are error-free. The analysis \nwe performed initially reports potential, but not certain, errors on all eleven benchmarks. The causes \nof these uncertain er\u00adror reports are diverse, including imprecise loop invariants, missing annotations \non library functions, non-linear arithmetic, and missing facts about the program s execution environment \n(e.g., relationship between argc and argv). For a given problem, each participant was randomly assigned \nto classify an error report either manually or using the proposed tech\u00adnique. Therefore, each participant \nclassi.ed approximately half the benchmarks manually and half of them using the new technique. In the \nmanual classi.cation case, the participants were asked to decide whether the report is a false alarm, \na real bug, or whether they can\u00adnot decide. When using our new technique, the participants were asked \nto give a Yes , No , or I don t know answer to a se\u00adries of questions generated by our tool, ranging \nfrom one to three questions on these benchmarks. The time for query computation is negligible; in all \ncases, the computation time is below 0.1s. For our user study, we recruited 56 professional programmers \nthrough ODesk and paid them $40 each for participating in the study. Each participant was required to \nhave at least one year of C programming experience and to have ODesk s systems program\u00adming certi.cation \n. In addition to our eleven benchmark problems, the study also included three simple diagnostic questions \nthat were not identi.ed as such to the participants. We excluded participants who gave the wrong answer \nto any of the diagnostic questions, re\u00adsulting in 49 valid participants for our study. Thus, each benchmark \nproblem was classi.ed manually by about 24 people and classi.ed using our technique by approximately \nthe same number of people. Figure 7 summarizes the results of our user study. The .rst three columns \nin the table give details about each benchmark. The column labeled LOC gives the lines of code presented \nto participants; the column Kind identi.es whether the benchmark is taken from a real application or \nfrom our program analysis test suite (i.e., synthetic). Finally, the column Classi.cation indicates the \ncorrect classi.cation of the report (false alarm or real bug). The next four columns in the table ( Manual \nclassi.cation ) give statistics about accuracy and speed of manual classi.cation. The column % correct \nindicates the percentage of participants who correctly classi.ed the report; the column % wrong gives \nthe percentage of participants who chose the wrong classi.cation. The column labeled % ? gives the percentage \nof participants who answered I don t know . Finally, the column Avg. time shows the average time participants \ntook to classify the report. The next four columns in the table (marked New Technique ) give the same \nstatistics about accuracy of classi.cation and timing using the new technique presented in this paper. \nAs the results in Figure 7 demonstrate, the average accuracy of manual report classi.cation is surprisingly \nlow: On average, 32.9% of the participants are able to classify an error report correctly, while 51.1% \nof the participants incorrectly classify a real bug as a false alarm or a false alarm as a real bug. \nAlso, observe that 16% of participants are unable to classify the error report despite spending signi.cant \ntime. (Our instructions speci.cally mentioned that par\u00adticipants should only choose I don t know after \nspending at least 8 minutes on each benchmark.) Finally, observe that programmers take an average of \napproximately 5 minutes to manually classify error reports despite the fact that their classi.cation \nis often wrong. These results indicate that manual classi.cation is extremely unreliable and time-consuming \neven on our small benchmark ex\u00adamples. In fact, the group of programmers who participated in our study \ndo not seem to outperform randomly assigning a classi.ca\u00adtion to each report. We believe this is strong \nevidence that the use\u00adfulness of static analysis techniques can be greatly improved by as\u00adsisting programmers \nin classifying and understanding error reports. As the results in Figure 7 also demonstrate, the accuracy \nand speed of report classi.cation dramatically improve when program\u00admers use our new technique. While \nonly 32.9% of participants cor\u00adrectly classify an error report using manual classi.cation, 89.6% of participants \ngive the correct answer using our technique. Similarly, the percentage of participants who give the wrong \nanswer drops from 51.1% to 7.3%. In addition to dramatically improving classi\u00ad.cation accuracy, our technique \nalso substantially reduces the time programmers need to classify reports: The average classi.cation time \ndrops from approximately 5 minutes to under 1 minute. As standard in user studies, we performed a t-test \nto evaluate the statistical signi.cance of our results. The p-value of a two-tailed t-test (assuming \npotentially unequal variance) comparing manual classi.cation accuracy vs. our technique is 5 \u00d7 10-8. \nThis means the probability that our technique has no in.uence on classi.cation accuracy is less than \n1 in 10,000,000. Similarly, the p-value for a t-test comparing classi.cation times is 1.2 \u00d7 10-28, indicating \nour results are statistically signi.cant. To give the reader a .avor of how our technique makes report \nclassi.cation much easier, we brie.y describe the reasoning re\u00adquired to classify Problem 6, based on \nthe chroot utility from the Unix coreutils. In this program, the value of variable optind is cor\u00adrelated \nwith four different return values of function getopt long. To prove the assertion in this example, we \nhave to remember all four return values of getopt long along with the intricate path conditions associated \nwith each of these return points and mentally evaluate several conditionals, all of which are relevant \nto proving the assertion. In fact, an author of this submission spent approxi\u00admately half an hour to \ndecide that the report is indeed a false alarm. In contrast, if we use the technique proposed in this \npaper, the user only needs to answer one simple query asking whether the value of optind is always greater \nthan zero after a while loop. This query is easy to decide by inspecting only three lines of code immediately \npreceding the program point associated with the query. 7. Related Work Explaining Error Traces in Model \nChecking Several papers fo\u00adcus on explaining error traces identi.ed by model checkers [8 13]. Ball et \nal. describe an algorithm for localizing the likely cause of an error given a trace produced by a model \nchecker [8]. Groce et al. present an error localization technique that computes the minimum distance \nbetween an error trace and a successful execu\u00adtion [9]. Ravi and Somenzi present an algorithm for giving \nsuccinct explanations for error traces, and like our technique, they employ minimum satisfying assignments \nto derive these explanations [13]. Jose and Majumdar give an algorithm for fault localization using maximum \nsatis.ability [10]. This technique computes the maxi\u00admum satis.able set of clauses in an extended trace \nformula, and the complement of this set is identi.ed as a potential cause of the error. In this work, \nwe share the goal of making error reports eas\u00adier to understand for programmers. However, our technique \ndoes not require a counterexample produced by model checkers and can be gainfully combined with any static \nanalysis. Furthermore, our technique is useful for classifying and understanding false alarms, which \nare not addressed by these techniques. Counterexample-Guided Abstraction Re.nement (CEGAR) Our technique \nis similar to CEGAR in that both approaches in\u00adfer relevant conditions that are useful for eliminating \nfalse alarms [14 16]. The main difference is that CEGAR-based approaches learn new predicates from one \nconcrete counterexample trace. Thus, while the learned predicate may be suf.cient to prevent this particular \ncounterexample trace, it may not be suf.cient to elimi\u00adnate other spurious traces reaching the same error. \nIn contrast, proof obligations we compute are, if valid, guaranteed to rule out the false alarm entirely. \nFurthermore, while CEGAR-based approaches have the advantage of being fully automatic and not requiring \nassistance from the user, they are not guaranteed to terminate. Techniques for Error Report Understanding \nManevich at al. present a post-mortem backwards data.ow analysis for construct\u00ading failing execution \ntraces from a failure point [17]. This tech\u00adnique can sometimes show that the report is a false alarm. \nLe and Soffa describe a fault correlation technique that can be helpful for diagnosing error reports \n[18]. Their technique groups error reports according to their root cause, which may aid in understanding \nthese reports. The approach of [19] computes a patch, which is a modi\u00ad.ed version of the program not \ncontaining the error, and this patch is included in the bug report. While these techniques may some\u00adtimes \nbe insightful for understanding a report, they do not allow the classi.cation of an error report as a \nreal bug or false alarm. Program slicing [20 22] is useful for identifying statements in the source code \nthat may be relevant to the potential failure. How\u00adever, program slices are typically large and do not \ntake advantage of facts already shown by the analysis. Unlike our technique, a slice also does not provide \nany semantic clues about whether the report corresponds to a genuine bug or false alarm. Explaining Type \nErrors There has been much work on explain\u00ading errors that arise in type inference [23 26]. Since errors \nduring polymorphic type checking result from uni.cation failure, the lo\u00adcation associated with a report \nis often not useful for understanding its cause. The techniques of [23 25] augment type inference with \ninformation useful for explaining the chain of inferences that led to the error. The algorithm described \nin [26] does not modify the orig\u00adinal compiler, but searches for a program close to the original one \nthat does type check. We share the goal of making static reason\u00ading transparent to users; however, the \nafore-mentioned approaches specialize in explaining type inference errors while our technique assists \nusers in classifying reports generated by veri.cation tools. Combining May and Must Information There \nhas been recent work on combining may and must information to prove both the presence and absence of \nerrors. Must information has been har\u00advested both from dynamic analysis [27 29] and computed purely statically \n[30]. While these approaches are useful for showing the presence of errors, they do not help when errors \ncan neither be dis\u00adcharged nor proven, which is our focus. However, we believe must information obtained \nfrom dynamic analysis could be useful for automatically deciding some of our failure witness queries. \n8. Conclusion and Future Work We have presented a new technique for assisting users with classi\u00adfying \nerror reports generated by static analyses. Our technique can be gainfully combined with any static analysis \nto make their results more understandable for users. We have evaluated this technique with a user study; \nour results indicate that the new technique im\u00adproves both the time and accuracy of report classi.cation. \nWe believe that the approach described in this paper can be fur\u00adther improved by making use of underapproximations. \nIn particu\u00adlar, the analysis described in Section 3 can only prove the program is buggy if I|= \u00acf. However, \nsince I is an invariant (i.e., an over\u00adapproximation of program behavior), I|= \u00acf indicates that every \nexecution of the program must be buggy. By making use of under\u00adapproximations obtained through static \nor dynamic techniques, the analysis can prove that some executions of the program must be buggy without \nrequiring the user s help. We believe dynamic anal\u00adysis could also be very useful for automatically discharging \nsome of the failure witness queries. References [1] Bessey, A., Block, K., Chelf, B., Chou, A., Fulton, \nB., Hallem, S., Henri-Gros, C., Kamsky, A., McPeak, S., Engler, D.: A few billion lines of code later: \nusing static analysis to .nd bugs in the real world. Communications of the ACM 53(2) (2010) 66 75 [2] \nDillig, I., Dillig, T., Aiken, A.: Automated error di\u00adagnosis using abductive inference (extended version). \nhttp://www.cs.wm.edu/ idillig/pldi12-extended.pdf [3] Dillig, I., Dillig, T., McMillan, K., Aiken, A.: \nMinimum satisfying assignments for SMT. In: To appear in CAV. (2012) [4] Dillig, I., Dillig, T., Aiken, \nA.: Small formulas for large programs: On\u00adline constraint simpli.cation in scalable static analysis. \nStatic Analysis Symposium (SAS) (2010) 236 252 [5] Dillig, I., Dillig, T., Aiken, A.: Fluid updates: \nBeyond strong vs. weak updates. ESOP (2010) 246 266 [6] Dillig, I., Dillig, T., Aiken, A.: Precise reasoning \nfor programs using containers. POPL (2011) 187 200 [7] Dillig, I., Dillig, T., Aiken, A., Sagiv, M.: \nPrecise and compact modular procedure summaries for heap manipulating programs. In: PLDI. (2011) 567 \n577 [8] Ball, T., Naik, M., Rajamani, S.: From symptom to cause: localizing errors in counterexample \ntraces. POPL 38(1) (2003) 97 105 [9] Groce, A.: Error explanation with distance metrics. TACAS (2004) \n108 122 [10] Jose, M., Majumdar, R.: Cause clue clauses: error localization using maximum satis.ability. \nIn: PLDI, ACM (2011) 437 446 [11] Griesmayer, A., Staber, S., Bloem, R.: Automated fault localization \nfor c programs. Theoretical Computer Science 174(4) (2007) 95 111 [12] Renieres, M., Reiss, S.: Fault \nlocalization with nearest neighbor queries. In: ASE. (2003) 30 39 [13] Ravi, K., Somenzi, F.: Minimal \nassignments for bounded model checking. TACAS (2004) 31 45 [14] Henzinger, T., Jhala, R., Majumdar, R., \nSutre, G.: Software veri.ca\u00adtion with blast. Model Checking Software (2003) 624 624 [15] Ball, T., Rajamani, \nS.: The SLAM project: debugging system software via static analysis. In: POPL, NY, USA (2002) 1 3 [16] \nHenzinger, T., Jhala, R., Majumdar, R., McMillan, K.: Abstractions from proofs. POPL 39(1) (2004) 232 \n244 [17] Manevich, R., Sridharan, M., Adams, S., Das, M., Yang, Z.: PSE: explaining program failures \nvia postmortem static analysis. In: FSE. Volume 29., ACM (2004) 63 72 [18] Le, W., Soffa, M.: Path-based \nfault correlations. In: FSE. (2010) 307 316 [19] Weimer, W.: Patches as better bug reports. In: GPCE. \n(2006) 181 190 [20] Weiser, M.: Program slicing. In: ICSE, IEEE Press (1981) 439 449 [21] Korel, B., \nLaski, J.: Dynamic program slicing. Information Processing Letters 29(3) (1988) 155 163 [22] Sridharan, \nM., Fink, S., Bodik, R.: Thin slicing. In: PLDI, ACM (2007) 112 122 [23] Beaven, M., Stansifer, R.: Explaining \ntype errors in polymorphic languages. LOPLAS 2(1-4) (1993) 17 30 [24] Haack, C., Wells, J.: Type error \nslicing in implicitly typed higher-order languages. Programming Languages and Systems (2003) 284 301 \n[25] Yang, J.: Explaining type errors by .nding the source of a type con.ict. Trends in Functional Programming \n(1999) 49 57 [26] Lerner, B., Flower, M., Grossman, D., Chambers, C.: Searching for type-error messages. \nIn: PLDI, ACM (2007) 425 434 [27] Nori, A., Rajamani, S., Tetali, S., Thakur, A.: The yogi project: Software \nproperty checking via static analysis and testing. TACAS (2009) 178 181 [28] Godefroid, P., Nori, A., \nRajamani, S., Tetali, S.: Compositional may\u00admust program analysis: unleashing the power of alternation. \nIn: POPL. Volume 45., ACM (2010) 43 56 [29] Yorsh, G., Ball, T., Sagiv, M.: Testing, abstraction, theorem \nproving: better together! In: ISSTA, ACM (2006) 145 156 [30] Dillig, I., Dillig, T., Aiken, A.: Sound, \ncomplete and scalable path\u00adsensitive analysis. In: PLDI, ACM (2008) 270 280  \n\t\t\t", "proc_id": "2254064", "abstract": "<p>When program verification tools fail to verify a program, either the program is buggy or the report is a false alarm. In this situation, the burden is on the user to manually classify the report, but this task is time-consuming, error-prone, and does not utilize facts already proven by the analysis. We present a new technique for assisting users in classifying error reports. Our technique computes small, relevant queries presented to a user that capture exactly the information the analysis is missing to either discharge or validate the error. Our insight is that identifying these missing facts is an instance of the <i>abductive inference problem</i> in logic, and we present a new algorithm for computing the smallest and most general abductions in this setting. We perform the first user study to rigorously evaluate the accuracy and effort involved in manual classification of error reports. Our study demonstrates that our new technique is very useful for improving both the speed and accuracy of error report classification. Specifically, our approach improves classification accuracy from 33% to 90% and reduces the time programmers take to classify error reports from approximately 5 minutes to under 1 minute.</p>", "authors": [{"name": "Isil Dillig", "author_profile_id": "81331491247", "affiliation": "College of William &#38; Mary, Williamsburg, VA, USA", "person_id": "P3471189", "email_address": "idillig@cs.wm.edu", "orcid_id": ""}, {"name": "Thomas Dillig", "author_profile_id": "81331491149", "affiliation": "College of William &#38; Mary, Williamsburg, VA, USA", "person_id": "P3471190", "email_address": "tdillig@cs.wm.edu", "orcid_id": ""}, {"name": "Alex Aiken", "author_profile_id": "81100399954", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P3471191", "email_address": "aiken@cs.stanford.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254087", "year": "2012", "article_id": "2254087", "conference": "PLDI", "title": "Automated error diagnosis using abductive inference", "url": "http://dl.acm.org/citation.cfm?id=2254087"}