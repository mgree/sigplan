{"article_publication_date": "06-11-2012", "fulltext": "\n Effective Parallelization of Loops in the Presence of I/O Operations Min Feng Rajiv Gupta Iulian Neamtiu \nDepartment of Computer Science and Engineering University of California, Riverside {mfeng,gupta,neamtiu}@cs.ucr.edu \nAbstract Software-based thread-level parallelization has been widely stud\u00adied for exploiting data parallelism \nin purely computational loops to improve program performance on multiprocessors. However, none of the \nprevious efforts deal with ef.cient parallelization of hybrid loops, i.e., loops that contain a mix of \ncomputation and I/O oper\u00adations. In this paper, we propose a set of techniques for ef.ciently parallelizing \nhybrid loops. Our techniques apply DOALL paral\u00adlelism to hybrid loops by breaking the cross-iteration \ndependences caused by I/O operations. We also support speculative execution of I/O operations to enable \nspeculative parallelization of hybrid loops. Helper threading is used to reduce the I/O bus contention \ncaused by the improved parallelism. We provide an easy-to-use programming model for exploiting parallelism \nin loops with I/O operations. Par\u00adallelizing hybrid loops using our model requires few modi.cations to \nthe code. We have developed a prototype implementation of our programming model. We have evaluated our \nimplementation on a 24-core machine using eight applications, including a widely-used genomic sequence \nassembler and a multi-player game server, and others from PARSEC and SPEC CPU2000 benchmark suites. The \nhybrid loops in these applications take 23% 99% of the total exe\u00adcution time on our 24-core machine. \nThe parallelized applications achieve speedups of 3.0x 12.8x with hybrid loop parallelization over the \nsequential versions of the same applications. Compared to the versions of applications where only computation \nloops are parallelized, hybrid loop parallelization improves the application performance by 68% on average. \nCategories and Subject Descriptors D.3.2 [Programming Lan\u00adguages]: Language Constructs and Features \nConcurrent program\u00adming structures General Terms Languages, Performance Keywords DOALL Parallelization, \nSpeculative Parallelization, Helper Threading, I/O contention 1. Introduction Uncovering parallelism \nis crucial for improving an application s performance on shared memory multiprocessors. Software-based \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n12, June 11 16, 2012, Beijing, China. Copyright c &#38;#169; 2012 ACM 978-1-4503-1205-9/12/06. . . $10.00 \nthread-level parallelization has been widely studied for exploiting parallelism on multiprocessors. Many \nparallel programming mod\u00adels have been proposed to facilitate the development of parallel ap\u00adplications \non shared memory multiprocessors. The shared-memory paradigm used by these programming models makes them \neasy-to\u00aduse for the programmer. A majority of the parallel programming models (e.g., Thread\u00ading Building \nBlocks [20], OpenMP [8], Galois [14], SpiceC [10]) focus on exploiting data parallelism in loops including \nDOALL, DOACROSS, pipelined, and speculatively parallelized loops. How\u00adever, these programming models \nonly target loops that contain pure computations, i.e., they are free of I/O operations. Since many ap\u00adplications \ncontain loops with I/O operations, they fail to yield much speedup due to these loops still being sequential. \nTherefore, it is highly desirable to support parallel programming models which al\u00adlow parallel execution \nof hybrid loops, i.e., loops with both compu\u00adtation and I/O operations. For example, Velvet [30] is a \npopular bioinformatics application. In its version 1.1 (i.e., the latest version when we conducted this \nwork), 18 pure computation loops have been parallelized using OpenMP. However, since OpenMP lacks support \nfor parallelizing loops with I/O operations, none of the hy\u00adbrid loops in Velvet have been parallelized. \nActually, Velvet has 39 hybrid loops, out of which 26 loops can be parallelized by the approach presented \nin this paper. These hybrid loops take a sig\u00adni.cant portion (27% 53%) of the total execution time in \nour ex\u00adperiments on a 24-core machine. Therefore, to further improve the performance of Velvet, developers \nmust exploit the parallelism in hybrid loops. Interestingly, while we were writing this paper, the developers \nof Velvet were working independently on manually parallelizing the hybrid loops, which only con.rms our \nobservation of the need to parallelize hybrid loops. In a few of the previous efforts [10, 26] where \nhybrid loops are parallelized, DOACROSS parallelism is used (i.e., synchroniza\u00adtion is imposed to deal \nwith cross-iteration dependences) even if the computation part of the loop can be performed using DOALL \n(i.e., no cross-iteration dependence in the computation part). How\u00adever, DOACROSS execution is not as \nef.cient as DOALL. Because DOACROSS execution assigns only one iteration per scheduling step, it results \nin signi.cant synchronization and scheduling over\u00adhead. In addition, the speculation-based loop parallelization \ntech\u00adniques proposed in these works are not applicable when the loop contains I/O operations. While the \nwork on parallel I/O [22, 24, 25] provides a set of low-level techniques that change the I/O subsys\u00adtem \nto improve the performance of multiple simultaneous I/O op\u00aderations, it does not provide any means for \nprogrammers to paral\u00adlelize hybrid loops. In this paper, we propose compiler techniques for ef.ciently \nparallelizing hybrid loops, i.e., loops that contain I/O operations in addition to computation. Our techniques \nbreak the cross-iteration dependences involving I/O operations. Therefore, we are able to employ DOALL \nparallelism whenever there is no cross-iteration dependence in the computation part of the loop. Our \ntechniques also support speculative execution of I/O operations to enable spec\u00adulative parallelization \nof hybrid loops. We observe increased I/O bus contention with aggressive parallelization of hybrid loops. \nFor performance scalability, we propose the use of helper threading to reduce the I/O bus contention. \nThe helper thread enables the work\u00ading threads to re.ll their input buffers and .ush their output buffers \nso that the I/O traf.c is spread out and not bursty. Unlike Parallel I/O work, our techniques do not \nrequire changes in the I/O subsys\u00adtem.  We provide an easy-to-use programming model for exploiting parallelism \nin hybrid loops. The programming model contains two I/O related pragmas. Programmers can parallelize \nhybrid loops by merely inserting the pragmas preceding the loops in sequen\u00adtial code. Similarly, to enable \nspeculative parallelization of hybrid loops, programmers just need to use one of the proposed pragmas \nat the beginning of the loop. Employing helper threading in parallel loops simply requires calling helper \nthreading APIs preceding the loops. We have developed a prototype implementation of our program\u00adming \nmodel. The core components of the implementation con\u00adsist of a source-to-source translator and a user-level \nruntime li\u00adbrary. We evaluate our implementation on a 24-core Dell Pow\u00aderEdge R905 server using eight \nbenchmarks from PARSEC [3] and SPEC CPU2000 benchmark suites, and two real applications. These benchmarks \nare parallelized via DOALL and speculative parallelism. Our implementation achieves 3.0x 12.8x speedup \nin these benchmarks. In comparison to the parallelized versions of benchmarks without hybrid loop parallelization, \nour technique im\u00adproves the performance by 30% to 272%. The rest of the paper is organized as follows. \nSection 2 illus\u00adtrates our approach to parallelizing hybrid loops. Section 3 presents our programming \nmodel and its implementation. Section 4 presents the evaluation. Section 5 discusses related work and \nSection 6 con\u00adincurs high scheduling and synchronization overhead see Figure 1(b) for DOACROSS loop execution. \n Figure 1. Execution of the loop example. To fully exploit the parallelism in the loop, we need to .nd \na way to break the cross-iteration dependences due to I/O operations. In this paper we develop solutions \nto enable DOALL parallelization which leads to the execution shown in Figure 1(c). With DOALL parallelization \nwe can eliminate the synchronization between con\u00adsecutive iterations and employ more ef.cient scheduling \npolices such as Guided Self Scheduling [17]. Figure 2 compares the perfor\u00admance of DOACROSS with the \nperformance of DOALL on a mi\u00adcrobenchmark constructed based on the example loop type. In the microbenchmark, \nthe compute function is composed of a loop. We can adjust the ratio of I/O workload vs. computation workload \nby varying the loop size. The .gure shows the performance compar\u00adison for both I/O-dominant workload \nand computation-dominant workload. In the I/O-dominant workload, the I/O calls take around 75% of the \nexecution time of the sequential loop while in the computation-dominant workload the I/O calls take only \n25% of the loop execution time. In both cases, DOALL performs better than DOACROSS, especially with large \nnumber of parallel threads. Therefore, the .rst challenge addressed in this work is to ef.ciently parallelize \nhybrid loops by enabling DOALL parallelization. cludes this paper. 14 13 12 11 2. Parallelizing Hybrid \nLoops We begin by discussing the challenges in parallelizing hybrid loops and then describe our approach \nto overcoming these challenges. Speedup 10 9 8 7 6 5 First, we discuss why existing techniques for \nDOALL paralleliza\u00ad tion and speculative loop parallelization cannot be directly applied 4 3 if a loop \nalso contains I/O operations. Our goal is to generalize these techniques so that they can be applied \nto hybrid loops. Sec\u00adond, we show that parallelization of hybrid loops can lead to I/O contention which \nmust be effectively handled to realize the full bene.ts of parallelization. Enabling DOALL Parallelization \nof Hybrid Loops. Figure 1(a) shows a typical loop with I/O operations that can be found in many applications \n(e.g., bzip2, parser [12], and stream encoder/decoder). Each loop iteration .rst checks if the end of \nthe input .le has been reached. If not, data is read from the .le, computation on the data is performed, \nand .nally results are written to the output .le. When the computation within a loop does not involve \ncross\u00aditeration dependences, maximum parallelism can be exploited via DOALL parallelization where all \nloop iterations can be executed in parallel. However, in a hybrid loop, even when the computa\u00adtion does \nnot involve cross-iteration dependences, DOALL paral\u00adlelization is not possible because the .le read/write \noperations in\u00adtroduce cross-iteration dependences due to the movement (in our example, advancement) of \nthe .le pointer. In prior work, e.g., [10], such loops are parallelized using DOACROSS parallelism which \n2 1 1 4 8 12 16 20 24 Number of parallel threads Figure 2. Performance comparison of DOACROSS and DOALL \non the example loop. Performing Speculative Parallelization of Hybrid Loops. Now let us consider the \nsituation in which cross-iteration dependences exist in the computation part of the loop. We cannot perform \nDOALL parallelization of such loops even if we break the de\u00adpendences introduced by I/O operations. Recent \nworks have shown that an effective approach to handling cross-iteration dependences in the computation \npart is to employ speculative paralleliza\u00adtion of loops. This approach is very effective when the cross\u00aditeration \ndependences manifest themselves infrequently. Previous works [9, 26] have shown that speculative parallelization \nworks better than non-speculative DOACROSS parallelization; however, these works also assume that the \nloops do not contain I/O oper\u00adations. To apply speculative parallelization to the loops with I/O operations, \nwe need to enable the speculative execution of the I/O  Speedup 4 3 2 1 0  1 4 8 12 16 20 24 Number \nof parallel I/O requests Figure 3. Performance of reading a 1 GB .le using different num\u00adbers of parallel \nI/O requests. operations. Therefore, the second challenge of ef.ciently paralleliz\u00ading hybrid loops is \nto develop solutions for speculative execution of I/O operations. I/O Contention due to Parallelization. \nOnce DOALL and spec\u00adulative parallelization of hybrid loops have been achieved, we are faced with yet \nanother challenge. By increasing parallelism we also increase I/O bus contention. Figure 3 shows the \nperformance of a microbenchmark that reads a 1 GB .le from the disk. The mi\u00adcrobenchmark creates multiple \nparallel threads, each of which then sends an I/O request to read a portion of the .le. We varied the \nnumber of parallel threads (i.e., number of parallel I/O requests) to examine the impact of I/O bus contention. \nWe can see that the I/O performance degrades quickly with more than 7 parallel I/O re\u00adquests due to the \nI/O bus contention. With more than 16 parallel I/O requests, we actually get a slowdown compared to the \nperformance with just one I/O request. Figure 2 also shows that the DOALL performance of the example \nloop degrades slightly with large num\u00adber of parallel threads. Therefore, to effectively parallelize \nhybrid loops, we must develop techniques for reducing I/O bus contention. 2.1 DOALL Parallelization of \nHybrid Loops To apply DOALL parallelization to a hybrid loop we need to break the cross-iteration dependences \nintroduced by the I/O operations in the loop. Our dependence-breaking strategies for input operations \ndiffer from those for output operations. Thus, we discuss them separately below. Cross-iteration dependences \ncaused by the input operations. These dependences arise because the starting .le position for an it\u00aderation \ndepends on the input operations performed in the previous iteration. The DOALL parallelization assigns \neach parallel thread a chunk of consecutive iterations for execution. Therefore to break the dependences \nwe identify the starting .le position corresponding to each parallel thread by directly calculating it \nbefore the execution of the loop. The method for computing the starting .le positions de\u00adpends upon the \n.le access pattern used in the loop. Our examination of Velvet [30] and the programs in two benchmark \nsuites, SPEC CPU 2000 and PARSEC [3], has identi.ed three commonly-used .le access patterns which are \ndescribed next. FSB: Fixed Size Blocks. The .rst access pattern, shown in Fig\u00adure 4(a), is called the \n.xed-size blocks pattern as in this pattern each loop iteration reads a .xed-size block of data. The \nstride of the .le pointer is equal to the size of the data block read by each iteration. Since we know \nthe stride of the .le pointer before en\u00adtering the loop, we can easily calculate the size of data to \nbe read by each parallel thread. Thus the starting .le position of a parallel thread can be calculated \nby summing up the size of data to be read by previous parallel threads. Given the starting .le position \nof each parallel thread, the time complexity of moving the .le pointer to the starting .le position via \na seek operation is O(1). FLS: Fixed Loop Size. In the second pattern, as shown in Figure 4(b), the loop \n.rst reads the total number of blocks from the .le and then accesses one delimited block during each \nloop iteration. Since the blocks are of variable size, delimiters are used to separate them (e.g., in \na text .le, the delimiter is \\n ). From the total number of blocks, we can calculate the number of blocks, \nn, to be read by each parallel thread. Using a scan operation, we can locate the starting .le position \nof parallel thread i by skipping n * i occurrences of the delimiter. The time complexity is O(N), where \nN is the .le size. This strategy is slow because it requires a scan as opposed to a seek. FCS: Fixed \nCumulative Size. In the third pattern, shown in Figure 4(c), each loop iteration accesses one delimited \nblock that is encountered after skipping data blocks whose cumulative size reaches a given number. Because \nthe data blocks are of variable size, delimiters are used to separate them. Since we know the total size \nof data to be read by the loop and the total number of parallel threads (T ), we can locate the starting \n.le position of parallel thread i by .rst skipping the i/T fraction of the data and then looking for \nthe .rst occurrence of the delimiter. In this case, we actually assign equal amount of data instead of \nequal number of iterations to each parallel thread, which is different from typical DOALL parallelization. \nThis strategy requires two operations a seek and a scan.This scan is faster than the scan performed in \nFLS since it only scans to the .rst occurrence of the delimiter. The time complexity of this strategy \nis O(L), where L is the maximum length of a data block. This time complexity is much lower than that \nof assigning equal number of iterations to every thread O(N), where N is the .le size. Cross-iteration \ndependences caused by the output opera\u00adtions. Simply computing the starting .le position cannot help \nbreak cross-iteration dependences caused by output operations. This is because the calculated .le position \ndoes not exist until all previous output operations have completed. Therefore, to break these depen\u00addences, \nwe propose a different approach. We create an output buffer for each parallel thread. Each thread writes \nthe outputs into its out\u00adput buffer during the parallel execution of the loop. As a result, the output \noperations in one thread no longer depend on those in other threads. Flushing these buffers can be performed \nin parallel with the sequential code following the loop, as shown in Figure 5(a).  2.2 Speculative Parallelization \nof Hybrid Loops Speculative parallelization is a way to ef.ciently exploit potential, but not guaranteed, \nparallelism in loops. Software speculative exe\u00adcution of non-I/O code has been studied in previous work \n[9, 26]. However, I/O operations cannot be executed speculatively in the same way because they use system \ncalls. The code executed by the system calls is hidden from the compiler and runtime library, which thus \ncannot monitor the execution of system calls. Moreover, the results of I/O operations cannot be simply \nreversed once they are done. Therefore, to ef.ciently parallelize loops with I/O operations using speculative \nparallelism, we need support for speculative exe\u00adcution of I/O operations. Speculative execution of input \noperations. Speculative execu\u00adtion of the input operations in each iteration is enabled by creating a \ncopy of the .le pointer at the start of each iteration and then using the copy to perform all input operations \nin the iteration. If specu\u00adlation succeeds, the original .le pointer is discarded and the copy is used \nin the subsequent iterations. If speculation fails, we simply discard the copy. One way of creating the \ncopy is to instantiate a new .le pointer and then seek to the current .le position. Speculative execution \nof output operations. Speculative ex\u00adecution of a loop iteration that contains output operations should \nsatisfy the atomicity semantics, i.e., either all output operations oc\u00adcur or none occur. Therefore, \nthe output operations in the loop it\u00aderation should not actually write to the .le since that cannot be \n Loop index o 1 2 o1 N o1  posbegin posbegin posend Strategy Threadi seeks to posi and then reads, \nwhere posi =posbegin+s*n*i, s = stride of file access, n= # of iterations for each thread. Threadi scans \nto the (n*i)-th delimiter and then reads, where n= # of iterations assigned to each thread. Threadi seeks \nto posi , then scans to the first delimiter, and finally reads, where posi =posbegin+L*i/T, T= # of threads. \nComplexity O(1) O( size(file) ) O( size...(block) ) (a) Fixed size blocks (b) Fixed loop size (C) Fixed \ncumulative size Figure 4. Commonly used .le access patterns of loops and strategies for locating the \nstarting .le position for each parallel thread. Programming constructs Description #pragma SpiceC parallel \ndoall|doacross|pipelining specify a parallel loop and the form of parallelism in which the loop is to \nbe executed #pragma SpiceC subregion [regionname] [after(iteration, region)] specify a subregion in the \nparallel loop that is executed under the order speci.cation #pragma SpiceC commit [atomicity] [after(iteration, \nregion)] perform a commit operation with speci.ed atomicity check and execution order Table 1. SpiceC \nprogramming constructs. Figure 5. Strategies for .ushing the output buffer. reversed. Speculative execution \nof the output operations in an it\u00aderation can be realized by using the output buffering described in \nthe previous section. If speculation succeeds, we keep the buffered output. If speculation fails, we \nsimply discard the buffered output. We .ush the output buffer at the end of each iteration as shown in \nFigure 5(b). Flushing the buffer at the end of each iteration requires small amount of memory since the \nbuffer does not need to hold the outputs from the entire loop, but it incurs synchronization overhead \nbecause the .ush operations need to be performed sequentially. The .ush operation in an iteration needs \nto wait for the completion of the .ush operation in the previous iteration as shown in Figure 5(b). \n 2.3 I/O Contention Reduction via Helper Threading Parallelizing hybrid loops can lead to increased contention \non the I/O bus. We propose the use of helper threading to reduce this I/O contention. Before entering \na hybrid loop, we create a helper thread which monitors the .le buffers of the associated .le pointers \nand performs the following tasks. For an input .le pointer, the helper thread re.lls the .le buffer when \nthe size of remaining data in the buffer is less than a prede.ned threshold. For an output .le pointer, \nit .ushes the .le buffer when the size of buffered output is larger than a prede.ned threshold. The use \nof a helper thread actually causes the I/O requests sent to the I/O bus to be serialized. Therefore, \nit eliminates the slowdown caused by the bursty nature of I/O requests. Moreover, since the parallel \nthreads only access the buffer in the memory instead of the .le on the disk, the I/O latency is also \nhidden by the helper thread. The helper threading can also be used in sequential loops to reduce I/O \nlatency. 3. Our System for Parallelizing Hybrid Loops The strategies described to enable parallelization \nof hybrid loops have been incorporated into the SpiceC [10] system. We chose SpiceC because it already \nsupports programming constructs, in the form of compiler directives, that can be used to express DOALL, \nDOACROSS, pipelining, and speculative parallelism. We have ex\u00adtended the SpiceC programming model to \nallow parallelization of hybrid loops. This section .rst presents our approach to program\u00adming parallel \nloops in the presence of I/O and illustrate it using several examples from real applications. Next we \ndescribe our im\u00adplementation of the programming model. 3.1 Programming Parallel Hybrid Loops Let us \n.rst consider our programming model for parallelizing hy\u00adbrid loops. We show how parallel hybrid loops \nare programmed and describe the use of I/O helper threads to boost the I/O performance on multicores. \nWe use the C standard I/O library (stdio) for illus\u00adtrating our programming model. All the proposed APIs \ncan be used for other I/O libraries, e.g., the C++ I/O library (iostream). The SpiceC [10] programming \nconstructs used to express parallelism in this section are summarized in Table 1. 3.1.1 Parallelizing \nLoops with Input Operations To enable DOALL parallelization of loops with input operations, we introduce \nthe pinput clause: #pragma SpiceC parallel doall \\ pinput(file, stride, start, end) The pinput clause \nis designed to be used with the SpiceC DOALL construct. To parallelize a loop with input operations, \nprogrammers just need to insert the DOALL construct combined with the pinput clause. The pinput clause \nhas four parameters: .le, stride, start, and end. Parameter .le is the input .le pointer that causes \ncross\u00aditeration dependences. Parameter stride gives the stride of .le in the loop; it can be either the \nsize of data block read by each iteration or the delimiter between data blocks. We distinguish between \nthem based on the parameter s type. Parameter start is the initial position of .le at the beginning of \nthe loop. By default (i.e., when this parameter is empty), the current position pointed by .le is used \nas the initial position. Parameter end is the ending position that .le will reach at the end of the loop. \nThe end should be left empty if the ending position of .le is unknown before entering the loop (e.g., \nthe FLS .le access pattern shown in Figure 4(b)). Given these parameters, the compiler can then calculate \nthe starting .le position for each parallel thread using the strategies described in Section 2.1.  Pattern \nPragma Example FSB pinput(.le, size, 0, EOF) FCS pinput(.le, delimiter, 0, EOF) FSB pinput(.le, size) \nFLS pinput(.le, delimiter) Table 2. Examples of the pinput clause. Table 2 shows four examples of the \npinput clause covering four different .le input patterns. In the .rst two examples, the loop reads the \nwhole .le. We use 0 as the initial .le position so that the compiler knows that the .le pointer starts \nfrom the beginning of the .le. The use of EOF as the ending .le position tells the compiler that the \n.le pointer will reach the end of the .le. In the last two examples, the loop reads a .xed number of \ndata blocks from the current .le position. The initial .le position is not given in the pinput clause \nsince the current .le position is used as the initial position. The ending .le position is left empty \nbecause it is unknown. The .rst and third examples exhibit the FSB pattern (as shown in Figure 4(a)) \nsince they use a constant block size as the stride of the .le pointer. In the second example, a delimiter \nis used as the stride and the total data size read by the loop can be calculated by the initial and ending \n.le position. Therefore, it exhibits the FCS pattern (as shown in Figure 4(c)). The last example exhibits \nthe FLS pattern (as shown in Figure 4(b)) since a delimiter is used as the stride and the loop size is \n.xed. .le=fopen( input , r ); fscanf(.le, %d , &#38;ntuples); #pragma SpiceC parallel doall pinput(.le, \n\\n ) { for( i=0; i<ntuples; i++) { fgets(line, maxsize, .le); read line(line, &#38;index, &#38;x, &#38;y); \ntuples[index] = create tuple(x,y,0); }} Figure 6. An input loop of benchmark DelaunayRe.nement. Figure \n6 shows a real example of DOALL input loop that is similar to the fourth case given in Table 2. The original \ninput loop is from the DelaunayRe.nement benchmark [16]. Although the computation part of this benchmark \nhas been parallelized in various ways [14, 21], its input loop, which contains I/O, has never been parallelized. \nThe input loop reads an array of ntuples tuples from the .le. Each iteration reads a line from the .le \nand then creates a tuple structure from the input. In the example, the pragmas inserted to parallelize \nthe loop are highlighted in bold. The SpiceC DOALL construct is used to identify the parallel region \nand type of parallelism. The pinput clause is used to specify the .le input pattern. Since each iteration \nreads one line from the .le, we give \\n as the delimiter in the pinput clause.  3.1.2 Parallelizing \nLoops with Output Operations To parallelize a loop with output operations using DOALL paral\u00adlelism, we \nneed to buffer the output of each iteration. Program\u00admers can achieve this by using the boutput clause \nwith the SpiceC DOALL construct as follows. #pragma SpiceC parallel doall \\ boutput(file, isparallel) \nThe boutput clause tells the compiler that the output to .le in the loop is written into its buffer. \nThe buffer will not be .ushed until the end of the loop, as shown in Figure 5(a). The boutput clause \nhas two parameters: .le and isparallel. Parameter .le is the .le pointer whose output needs to be buffered. \nParameter isparallel speci.es whether buffer .ushing is performed in parallel with the computation threads \nor in a sequential fashion. #pragma SpiceC parallel doall boutput(out.le, true) {for (index = 0; index<length; \nindex++) {nucleotide = getNucleotide(descriptor, index); switch (nucleotide) {case ADENINE: fprintf(out.le, \nA ); break; case CYTOSINE: fprintf(out.le, C ); break; ... }}} Figure 7. An output loop of bioinformatics \napplication Velvet. Figure 7 shows an output loop of Velvet [30], a widely\u00adused bioinformatics application \n(genomic sequence assembler). Although the computationally-intensive part of Velvet has re\u00adcently been \nparallelized with OpenMP, its hybrid loops have not been parallelized. In the original output loop, each \niteration of the loop gets a nucleotide from the descriptor and outputs a character based on the type \nof the nucleotide. The pragmas used to paral\u00adlelize the loop are highlighted in bold. The SpiceC pragma \nis used to mark the parallel region and the boutput clause is used to buffer all outputs of fprintf so \nthat the output operations do not cause any cross-iteration dependence on the .le pointer. Buffer .ushing \nis programmed to be performed in parallel with the sequential code after the loop. The boutput clause \ncan also be used to program DOACROSS loops with output operations. For DOACROSS parallelism, the buffer \nis .ushed at the end of each iteration, as shown in Figure 5(b). Figure 8 shows the kernel of the benchmark \nParser. Each iter\u00adation .rst reads a line from stdin and then parses the line. Because of the cross-iteration \ndependences in the parse portion of the loop, the loop cannot be parallelized using DOALL parallelism. \nHow\u00adever, since these dependences rarely manifest themselves at run\u00adtime, the loop can be parallelized \nusing speculative DOACROSS parallelism. Because the parse portion of the loop calls printf to output \nthe results, programmers need the boutput clause when ap\u00adplying speculative execution to the parse portion. \nIn the .gure, the pragmas inserted to speculatively parallelize the loop are high\u00adlighted in bold. The \n.rst SpiceC pragma is used to identify the par\u00adallel region and type of parallelism. The loop is divided \ninto two subregions by the rest of the SpiceC pragmas. The .rst subregion, READ, performs the input operations. \nThe pinput clause is used at the beginning of the loop to tell the compiler the .le input pattern.  \n#pragma SpiceC parallel doacross \\ pinput(in.le, \\n , 0, EOF) boutput(stdout, false) {for(index=0; !feof(in.le); \nindex++) { #pragma SpiceC subregion READ { fgets(line, max line, in.le); } #pragma SpiceC subregion PARSE \n{ if ( special command(line) ) continue; .rst prepare to parse(1); while ( !success ) { /* parser code \nhere */ printf( Linkage %d , index+1); /* parser code here */ }#pragma SpiceC commit atomicity \\ after(ITER-1, \nPARSE) }}} Figure 8. Speculative parallelization of a kernel from Parser. The compiler can then calculate \nthe starting position of in.le for each iteration and break the cross-iteration dependences introduced \nby fgets. The second subregion, PARSE, parses the input; it is ex\u00adecuted speculatively as speci.ed by \nthe commit pragma at the end of the PARSE subregion. Since printf cannot be executed specula\u00adtively, \nthe boutput clause is used with the DOACROSS construct to buffer the outputs to stdout in the loop. This \nenables speculative ex\u00adecution. Buffer .ushing is performed at the end of each iteration in sequential \norder, as speci.ed by parameter isparallel in the boutput clause.  3.1.3 Programming I/O Helper Threads \nWe use helper threading to reduce the I/O contention caused by the hybrid loop parallelization. Table \n3 summarizes our API for programming I/O helper threads. Function inithelper is used to create a new \nI/O helper thread; it returns the handle of the created helper thread which can then be bound to a .le \npointer using the function sethelper. Once bound to a .le pointer, the helper thread continues to monitor \nthe buffer corresponding to that .le pointer. .le=fopen( input , r ); fscanf(.le, %d , &#38;ntuples); \nhelper = inithelper(); sethelper(.le, helper); #pragma SpiceC parallel doall pinput(.le, \\n ) { for( \ni=0; i<ntuples; i++) { fgets(line, maxsize, .le); read line(line, &#38;index, &#38;x, &#38;y); tuples[index] \n= create tuple(x,y,0); }} Figure 9. Example of using I/O helper threads. #  $  \"   $ $  \" % \n$ '  \" \" \"  Figure 10. Implementation overview. consist of a source-to-source translator and a user-level \nruntime library. The translator analyzes the hybrid loops parallelized with SpiceC directives and our \nAPIs and translates them into explicitly parallel C/C++ code. We implemented the analysis by extending \nROSE [18], a compiler infrastructure to build source-to-source code translators. The explicitly parallel \ncode is compiled by the GCC compiler and linked with our runtime library. The runtime library implements \noutput buffering and helper threading. Next, we describe the code transformation performed by the source\u00adto-source \ntranslator and then elaborate how output buffering and helper threading are implemented. 3.2.1 Loop Transformation \nAPI Description inithelper() initialize a I/O helper thread sethelper(.le, helper) bind a helper thread \nto a .le pointer Table 3. APIs for programming I/O helper thread. Figure 9 shows an example of using \nI/O helper threading in a parallel loop taken from DelaunayRe.nement as shown in Figure 6. We create \na helper thread and bind it to the input .le pointer before entering the loop. Upon entering the parallel \nthread, the helper thread will automatically monitor the buffer corresponding to the .le pointer copy \nin each parallel thread. Programmers do not need to code the binding of the helper thread to each copy \nof the .le pointer. I/O helper threading can also be used in sequential loops to re\u00adduce the I/O latency. \nSimilar to the example of parallel loop, use of I/O helper thread in sequential loops is straightforward: \nprogram\u00admers just need to call inithelper and sethelper before entering the loop.  3.2 Implementation \nFigure 10 presents the overview of our implementation of the programming model. The core components of \nthe implementation API bwrite(index, data, size, .le) bputs(index, string, .le) bprintf(index, .le, format, \n...) b.ush(.le, ALL/index, ispar) Description write data of size into the buffer of .le in iteration \nindex write string into the buffer of .le in iteration index write formatted data into the buffer of \n.le in iteration index .ush the buffer of .le using a separate thread or not Table 4. Low-level functions \nfor buffering outputs. The code transformation from DOALL hybrid loops to C/C++ code is done automatically \nin our implementation. Figure 11 shows an example of the code transformation. Figure 11(a) shows a DOALL \nhybrid loop parallelized by our extended SpiceC direc\u00adtives. Each iteration of the loop reads 100 bytes \nfrom the input .le, then processes them, and .nally outputs them into the out\u00adput .le. Figure 11(b) shows \nthe transformed main program. We insert initialization of the parallel threads at the beginning of the \nprogram and close the parallel threads at the end of the program. The DOALL loop is outlined into function \nwrapper. All variables used in the DOALL loop are wrapped into a structure which is then passed as a \nparameter to the outlined loop. We call func\u00adtion start doall to execute the outlined loop in the parallel \nthreads. Function join doall is a synchronization method that waits until all parallel threads .nish \ntheir work. Function b.ush is called after the loop to .ush the buffer of .le pointer out. Figure 11(c) \nshows the outlined loop. Before the loop, three functions are called to prepare the workload for the \ncurrent thread. Function get thread id is used to get the ID of the current thread. Function local start \nis called to calculate the starting .le position of the current thread. Function local count is called \nto calculate the number of iterations to be per\u00adformed in the current thread. Function fputs is replaced \nwith our function bputs for buffering the outputs. Table 4 lists our substi\u00adtute for the C standard I/O \nfunctions. They are designed to buffer and .ush the outputs for breaking the cross-iteration dependences \nintroduced by the output operations.  ... in = fopen( input , r ); init parallel threads(); ... void \nwrapper(void* args) {in = args->in; out = fopen( output , w ); #pragma SpiceC parallel doall \\ pinput(in, \n100, 0, EOF) boutput(out, true) {for( ; !feof(in); ) fread(buf, 1, 100, in); process(buf); fputs(buf, \nout); }... in = fopen( input , r ); out = fopen( output , w ); args->in = in; args->out = out; start \ndoall(wrapper, args); join doall(); b.ush(out, ALL, true); ... close parallel threads(); out = args->out; \ntid = get thread id(); f = local start(tid, in, 100, 0, EOF); c = local count(tid, in, 100, 0, EOF); \nfor( i=0; i<c; i++ ) fread(buf, 1, 100, f); process(buf); bputs(i, buf, out); } (a) Original code with \nSpiceC pragmas (b) Transformed main program (c) Transformed parallel loop Figure 11. Example of code \ntransformation.  3.2.2 Output Buffering To enable output buffering (e.g., bputs, b.ush), we create an \noutput buffer for each parallel thread. The structure of each output buffer is a linked list, as shown \nin Figure 12. Each node in the linked list is a buffer of prede.ned length which can typically hold the \noutput from several iterations. Once a node is full, a new node is created and appended to the linked \nlist. Flushing the output buffers takes O(n) time, where n is the total size of all output buffers. Figure \n12(a) shows the data layout of the output buffers for a DOALL loop with two parallel threads. The output \nbuffer of thread 1 stores the output from iteration 1 to 6 and the output buffer of thread 2 stores the \nrest. It is straightforward to .ush the output buffers with this data layout. We can .ush the output \nbuffers starting from the .rst thread and ending with the last thread, which takes O(n) time. Figure \n12(b) shows the data layout of the output buffers for a DOACROSS loop. The output buffer of thread 1 \nstores the output from odd iterations and the output buffer of thread 1 stores the output from even iterations. \nIn this case, we can .ush the output buffers in a round-robin manner, which also takes O(n) time. The \noutput of the current iteration in a DOACROSS loop can be .ushed ef.ciently (as shown in Figure 8) since \nthe output of the current iteration is always pointed to by the tail pointer of the output buffer.  \n 3.2.3 I/O Helper Threading To enable I/O helper threading, our runtime library implements an extended \nversion of the standard .le pointer by dividing the .le buffer into two parts of equal size: the f-buffer \nand the h-buffer, where the f-buffer is used as the .le buffer directly accessed by the I/O operations \nand the h-buffer is the helper buffer used by the helper thread. For input .le pointers, all input operations \nread data from the f-buffer. Once the f-buffer is empty, it is switched with the h-buffer. The helper \nthread keeps monitoring the h-buffer. If the h-buffer is empty, it re.lls it by calling I/O system calls. \nOutput .le pointers work in a similar way. In our implementation, we typically do not put the helper \nthread to sleep to minimize the re.lling latency. However, when the num\u00adber of parallel threads is equal \nto the number of processor cores in the system, the helper thread will compete with the parallel threads \nfor CPU resources. Therefore in this case, instead of busy idling the helper thread, we put the helper \nthread to sleep when it does not .nd any buffer that needs re.lling. The helper thread is then woken \nwhen an f-buffer is switched with the h-buffer in a buffer pair. 4. Evaluation This section evaluates \nthe prototype implementation of our pro\u00adgramming model. The experiments were conducted on a 24-core DELL \nPowerEdge R905 machine. Table 5 lists the machine details. Processors 4\u00d76-core 64-bit AMD Opteron 8431 \nProcessor (2.4GHz) L1 cache Private, 128KB for each core L2 cache Private, 512KB for each core L3 cache \nShared among 6 cores, 6144KB Memory 32GB RAM OS Ubuntu server, Linux kernel version 2.6.32 Table 5. \nDell PowerEdge R905 machine details. 4.1 Benchmarks Our programming model was applied to eight applications. \nTwo are real-world applications, while the others are from the PARSEC [3] and SPEC CPU2000 suites. We \nselected these applications using the following criteria: (1) the applications must have at least one \nhybrid loop that can be ef.ciently parallelized (i.e., there is no frequent cross-iteration dependence \nin the computation part); and (2) the hybrid loop(s) must take a signi.cant portion of execution time. \nApplying our techniques on applications that do not satisfy these criteria would diminish our capacity \nto measure and evaluate our approach. We applied DOALL parallelism to the hybrid loops Table 6. Benchmark \nsummary. From left to right: benchmark name, source of the benchmark, number of parallelized hybrid loops, \ninput .le access pattern, whether output buffering is used, whether speculative parallelization is used, \nwhether helper threading is used, percentage of total execution time taken by the hybrid loops, number \nof statements added or modi.ed for parallelization.  Name Source Loops Input Output? Speculation? Helper? \n% runtime # stmts velveth real application 8 FCS Yes No Yes 53% 18 velvetg real application 18 FCS Yes \nNo Yes 27% 46 spacetyrant real application 4  Yes No No 95% 8 DelaunayRe.nement lonestar 3 FLS No No \nYes 23% 7 bzip2 SPEC CPU2000 1 FSB Yes No Yes 99% 13 parser SPEC CPU2000 1 FCS Yes Yes No 99% 8 blackscholes \nPARSEC 1 FLS No No Yes 45% 6 .uidanimate PARSEC 3 FLS Yes No Yes 36% 10 in seven applications except \nparser (speculative parallelism was required to parallelize parser). Table 6 shows the details of the \nbenchmarks. Velvet [30] is a popular genomic sequence assembler. It con\u00adtains two applications velveth \nand velvetg. Velveth con\u00adstructs the dataset and calculates what each input sequence rep\u00adresents. Velvetg \nmanipulates the de Bruijn graph that is built on the dataset. 18 computation loops in velveth and velvetg \nhave already been parallelized using OpenMP. In the experiment, we parallelized the hybrid loops in them. \nAll parallelized input loops have the FCS pattern. We use output buffering to parallelize the loops that \ncontain output operations. We used nucleotide se\u00adquence SRR027005 [1] as input. SpaceTyrant [2] is an \nonline multi-player game server. We parallelized its backup thread which executes the backupdata function \nto backup game data. The back\u00adupdata function has 4 output loops for storing different types of data. \nOutput buffering was used to parallelize these loops. In the experiments, we assume that every data block \nis dirty and needs to be written to the .le. DelaunayRefinement [16] is a mesh\u00ading algorithm for two-dimensional \nquality mesh generation, orig\u00adinally written in JAVA; we ported it to C++. Its computation loop has been \nparallelized in previous work [14]. We parallelized the three input loops in the read function. The three \nloops read dif\u00adferent aspects of the input graph. They all have the FLS pattern. We appliedDOALLparallelismtothembybreakingtheI/Ode\u00adpendences. \nBzip2 is a tool used for data compression and de\u00adcompression. In the experiments, we parallelized its \ncompression loop using DOALL. There are many super.uous cross-iteration dependences on global variables \nin bzip2. To remove these de\u00adpendences, we replicated buffers for each iteration and made many global \nvariables local to each iteration. Some of the local variables are summarized into global variables after \nthe loop. Parser is a syntactic parser for English. We used speculative parallelism to parallelize its \nbatch process function which reads and parses the sentences in the input .led. The function contains \na FCS loop. We broke the I/O dependences by calculating the starting .le position for each iteration \nand using output buffering. We need to speculate on dependences for control variables which may be altered \nby the special commands in the input .le. Blackscholes is a compu\u00adtational .nance application. We parallelized \nthe input loop in the main function. The loop is a FLS loop that contains only input operations. Fluidanimate \nis designed to simulate an incompress\u00adible .uid in parallel. In its original Parsec version, the number \nof threads supplied by users must be a power of 2. We modi.ed the workload partitioning to enable an \narbitrary number of threads. For the PARSEC benchmarks used in the experiments, we use their pthread-based \nparallel versions.  4.2 Performance Figure 13 shows the absolute speedup of the parallelized applica\u00adtions \nover their sequential versions for varying number of paral\u00adlel threads. Figure 13(a) shows the speedup \nwhen applying our hybrid loop parallelization techniques we achieve 3.0x 12.8x speedup. On average, we \nimprove the performance of these ap\u00adplications by 6.6x on the 24-core machine. For some benchmarks, the \nperformance degrades with 24 parallel threads. This is caused by the contention between the helper thread \nand parallel threads. Fluidanimate has unstable performance across varying the num\u00adber of parallel threads \nbecause its workload cannot be evenly partitioned with certain numbers of threads. The performance of \nSpaceTyrant goes down with larger number of threads. This is caused by the dynamic memory allocation \nfor output buffer\u00ading. For comparison, Figure 13(b) shows the speedup of these parallelized applications \nwithout hybrid loop parallelization. The speedup of SpaceTyrant is always 1 since its backup thread can\u00adnot \nbe parallelized without hybrid loop parallelization. The speedup of these applications without hybrid \nloop parallelization is between 2.3x 8.8x which is signi.cantly lower than the speedups with hy\u00adbrid \nloop parallelization. Figure 14(a) shows the relative speedup of hybrid loops with parallelization vs. \nwithout parallelization. For seven applications, hybrid loop parallelization improves the hybrid loop \nperformance by factors greater than 5x. On average, hybrid loop parallelization improves the loop performance \nby a factor of 7.54x. Figure 14(b) shows the relative parallelized full application speedups with hy\u00adbrid \nloop parallelization vs. without hybrid loop parallelization. On average, hybrid loop parallelization \nimproves the application per\u00adformance by 68%.  4.3 Impact of Helper Threading Figure 15 shows the impact \nof I/O helper threading. On average, I/O helper threading improves the performance of parallelized hybrid \nloops by 11.9%. I/O helper threading usually provides more bene.t with larger number of threads except \n24 parallel threads where the I/O parallel thread competes with the parallel threads for processing resources. \nFigure 16 shows the impact of the buffer size of the helper thread in two applications velveth and DelaunayRefinement. \nI/O helper threading achieves higher speedup with larger buffer sizes. The buffer size is a critical \nfactor that determines whether a helper thread can ef.ciently load data for multiple threads. We use \nthe following example to show how we set the proper buffer size for a helper thread. Figure 17 compares \nthe computation time with the data load time for different numbers of iterations in DelaunayRe\u00ad.nement. \nThe trend of the curves is similar for hybrid loops in other applications. We de.ne tck as the computation \ntime of k iterations and tlk as the time of loading data for k iterations. From the .gure, tck increases \nmuch more quickly than tlk with the increase of k.  14 14 velveth velvetg 12 12  spacetyrant delaunayrefinement \n10 bzip2 10    Speedup Speedup Speedup parser blackscholes 8 8 6 fluidanimate 6 4 4 2 \n 2  1 Number of parallel threadsNumber of parallel threads 1  (a) With Hybrid Loop Parallelization \n(b) Without Hybrid Loop Parallelization Figure 13. Absolute parallelized application speedup over sequential \nprograms. 4 velveth 20 velvetg  3.5 spacetyrant delaunayrefinement bzip2  3 15    Speedup \n parser blackscholes  2.5 fluidanimate 10  2 5  1.5 1 1 Number of parallel threadsNumber of parallel \nthreads (a) Parallelized hybrid loops speedup (b) Parallelized full application speedup Figure 14. Relative \nspeedup: with hybrid loop parallelization vs. without hybrid loop parallelization. synchronization overhead \nwas introduced. Parser has the highest  synchronization overhead since it is parallelized speculatively. \nI/O operations take a higher percentage of execution time with larger number of parallel threads due \nto I/O bus contention. Figure 19 shows the memory overhead incurred by hybrid loop paralleliza\u00adtion. \nFor 5 out of 8 benchmarks, the memory overhead is smaller than 10MB. Velveth and velvetg have high memory \noverhead since their output, which needs to be buffered in the memory dur\u00ading loop execution, is large. \n5. Related Work Benchmarks For a helper thread that loads data for p parallel threads, its buffer for \neach parallel thread should be able to hold data for n iterations where tcn >p * tln since loading data \nshould be .nished before the data in the buffer is used up. Since tck increases much more quickly than \ntlk, there always exists n satisfying tcn >p * tln.  4.4 Overhead Figure 18 shows the breakdown of the \nhybrid loop execution time for the parallelized applications. The time is divided into four cat\u00adegories: \ncomputation, I/O, speculation, and synchronization. For 5 out of 8 applications, most time is spent on \nthe computation. SpaceTyrant and Blackscholes spend most time on I/O opera\u00adtions since their hybrid loops \ncontain very little computation. Since most loops are parallelized using DOALL parallelism, very little \nParallel programming models. Many programming models have been proposed to enable exploitation of data \nparallelism in sequen\u00adtial programs on shared memory multiprocessors. OpenMP [8] is a widely used programming \nmodel that provides a set of com\u00adpiler directives for parallelizing sequential programs on shared\u00admemory \nsystems. Threading Building Blocks (TBB) [20] is a pro\u00adgramming model that provides a set of thread-safe \ncontainers and algorithms for expressing parallelism on shared-memory systems. Single Program Multiple \nData (SPMD) is another category of pro\u00adgramming models and the message passing interface (MPI) [11] is \ncurrently the de facto standard for SPMD. Partitioned global ad\u00address space (PGAS) [6, 7] is a set of \nparallel programming models which aim to combine the performance advantage of MPI with the programmability \nof a shared-memory model. Galois [14, 15] in\u00adtroduces a programming model to exploit the data parallelism \nin irregular applications. SpiceC [10] is a recently proposed parallel programming model for both multicores \nand manycores. SpiceC can be used to express multiple forms of parallelisms, including DOALL, DOACROSS, \npipelining and speculative parallelism. Fi\u00ad  Number of parallel threads  velveth velvetg spacetyrant \ndelaunay bzip2 parser blackscholes fluidanimate 400000 Time (clock cycles) 350000 300000 250000 200000 \n150000 100000 50000 0  Figure 17. Computation time vs. data load time of benchmark DelaunayRe.nement. \nBenchmarks Figure 19. Memory overhead. Parallel I/O. Parallel I/O has been proposed to improve the per\u00adformance \nof multiple I/O operations at the same time. It is mostly designed to deal with massive amounts of data \non distributed sys\u00adtems. Research work in parallel I/O can be mainly divided into two different groups: \nparallel .le systems and parallel I/O libraries. Par\u00adallel .le systems [22] usually spread data over \nmultiple servers for high performance. They allow shared accesses to .les from multi\u00adple processes. Parallel \nI/O libraries such as ROMIO [24] are APIs designed to access parallel .le systems. Collective I/O [25] \nhas been proposed to optimize non-contiguous I/O requests from mul\u00ad tiple processes; it coordinates accesses \nto .les by a group of pro\u00adcesses in which collective I/O functions are called. Our techniques are orthogonal \nto parallel I/O. Parallel I/O pro\u00advides lower-level programming constructs designed to improve the I/O \nthroughput of large-scale systems. However, when using par\u00adallel I/O, programmers must be highly skilled \nin order to express, and make ef.cient use of, parallel I/O operations. We make it easy for programmers \nto parallelize hybrid loops by providing a higher\u00adlevel programming model. Our compiler techniques are \ndesigned to support our programming model and optimize the performance of hybrid loops written in our \nmodel. More speci.cally, our approach differs in two ways. First, to parallelize a loop with operations \nusing MPI I/O, programmers 2824 2824 2824 2824 2824 2824 2824 2824 must write code to calculate the \nstarting and ending offset for each thread and explicitly set the offset using the MPI I/O APIs. Programmers \nalso need to take care of synchronization, scheduling, load balancing, etc. Using our programming model, \nprogrammers just need to insert a few pragmas. Second, Parallel I/O, such as MPI I/O, does not provide \nany support for speculative parallelization, Figure 18. Breakdown of hybrid loop execution time. nally, \nsoftware-based thread level speculation (TLS) techniques have been proposed for automatically parallelizing \nsequential pro\u00adgrams [9, 13, 26 28]. They are all based on state separation, i.e., the results of speculative \ncomputations are stored in a separate space from the non-speculative state space. Speculative Decoupled \nSoft\u00adware Pipelining (Spec-DSWP) [29] is another series of TLS works. Software multithreaded transactional \nmemory system [19] has been developed to optimize the performance of Spec-DSWP. None of the above programming \nmodels and techniques pro\u00advide support for ef.ciently parallelizing hybrid loops. They can\u00adnot break \nthe cross-iteration dependences caused by I/O operations. Therefore, they cannot perform the I/O part \nof loops in parallel. In addition, the TLS techniques that focus on speculative execution of data computation \ndo not provide any rollback mechanism for I/O operations in case of misspeculation. while our techniques \ndo. I/O support for transactional memory. Unrestricted transac\u00adtional memory [4] is a hardware transactional \nmemory technique that has been proposed to support I/O calls in transactions. Transac\u00adtions usually cannot \ncontain I/O calls because these operations can\u00adnot easily be rolled back. Unrestricted transactional \nmemory gives up some concurrency in exchange for gaining the ability to perform I/O calls within transactions \nby allowing only a single over.owed transaction per application. I/O prefetching. Helper threading has \nbeen used in software\u00adguided prefetching to hide I/O latency [5, 23]. To minimize I/O latency, these \ntechniques require timely prefetching. They rely on the pro.ler or operating system to insert prefetching \ncalls. Our helper threading technique is designed to reduce contention on the I/O bus instead of hiding \nthe I/O latency. Therefore, we only require data residing in main memories (i.e., off-chip memories) \ninstead of caches (i.e., on-chip memories) when they are read. Since main memories have very large capacity \nnowadays, we do not require very timely prefetching. Moreover, our helper threading is specially designed \nfor loops with contiguous I/O accesses. Therefore, we do not require any support from a pro.ler or the \noperating system.  6. Conclusions In this paper, we identi.ed the opportunity to parallelize hybrid \nloops, i.e., loops with computation and I/O operations. We pre\u00adsented several techniques for ef.ciently \nparallelizing hybrid loops. We proposed an easy-to-use programming model for exploiting parallelism in \nhybrid loops. Parallelizing hybrid loops using our model requires few modi.cations to the code. We developed \na pro\u00adtotype implementation of our programming model. The implemen\u00adtation was evaluated on a 24-core \nmachine using eight applications, from PARSEC and SPEC CPU2000 benchmark suites, and real\u00adworld applications. \nThe applications with hybrid loop paralleliza\u00adtion achieve 3.0x 12.8x speedup while in comparison 2.3x \n8.8x speedup was observed without hybrid loop parallelization. Acknowledgments We thank Stefano Lonardi \nfor suggesting Velvet as an applica\u00adtion. This research is supported by the National Science Founda\u00adtion \ngrants CCF-0963996 and CCF-0905509 to the University of California, Riverside. References [1] DDBJ sequence \nread archive. http://trace.ddbj.nig.ac.jp/dra/index e.shtml. [2] Space tyrant. http://spacetyrant.com/st.c. \n[3] C. Bienia, S. Kumar, J. P. Singh, and K. Li. The PARSEC benchmark suite: Characterization and architectural \nimplications. In Proceedings of the International Conference on Parallel Architectures and Compi\u00adlation \nTechniques (PACT), pages 72 81, 2008. [4] C. Blundell, E. C. Lewis, and M. M. K. Martin. Unrestricted \ntransac\u00adtional memory: Supporting I/O and system calls within transactions. Technical Report TR-CIS-06-09, \nUniversity of Pennsylvania, 2006. [5] A. D. Brown, T. C. Mowry, and O. Krieger. Compiler-based I/O prefetching \nfor out-of-core applications. ACM Transactions on Com\u00adputer Systems, 19:111 170, May 2001. [6] P. Charles, \nC. Grothoff, V. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. X10: an object-oriented \napproach to non-uniform cluster computing. In Proceedings of the ACM SIGPLAN Conference on Object-Oriented \nProgramming, Sys\u00adtems, Languages &#38; Applications (OOPSLA), pages 519 538, 2005. [7] U. Consortium. \nUPC language speci.cations, v1.2. Berkeley Lab Technical Report LBNL-59208, 2005. [8] L. Dagum and R. \nMenon. Openmp: An industry-standard api for shared-memory programming. IEEE computational science &#38; \nengi\u00adneering, 5(1):46 55, 1998. [9] C. Ding, X. Shen, K. Kelsey, C. Tice, R. Huang, and C. Zhang. Software \nbehavior oriented parallelization. In Proceedings of the ACM SIGPLAN conference on Programming Language \nDesign and Implementation (PLDI), pages 223 234, 2007. [10] M. Feng, R. Gupta, and Y. Hu. SpiceC: scalable \nparallelism via im\u00adplicit copying and explicit commit. In Proceedings of the ACM SIG-PLAN Symposium on \nPrinciples and Practice of Parallel Program\u00adming (PPoPP), pages 69 80, 2011. [11] W. Gropp, E. Lusk, \nand A. Skjellum. Using MPI: Portable Parallel Programming with the Message Passing Interface. The MIT \nPress, 1994. [12] J. L. Henning. SPEC CPU2000: Measuring cpu performance in the new millennium. Computer, \n33:28 35, July 2000. [13] K. Kelsey, T. Bai, C. Ding, and C. Zhang. Fast track: A software sys\u00adtem for \nspeculative program optimization. In Proceedings of the In\u00adternational Symposium on Code Generation and \nOptimization (CGO), pages 157 168, 2009. [14] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, K. \nBala, and L. P. Chew. Optimistic parallelism requires abstractions. In Proceed\u00adings of the ACM SIGPLAN \nconference on Programming Language De\u00adsign and Implementation (PLDI), pages 211 222, 2007. [15] M. Kulkarni, \nK. Pingali, G. Ramanarayanan, B. Walter, K. Bala, and L. P. Chew. Optimistic parallelism bene.ts from \ndata partitioning. In Proceedings of the International Conference on Architectural Support for Programming \nLanguages and Operating Systems (ASPLOS), pages 233 243, 2008. [16] M. Kulkarni, M. Burtscher, C. Cascaval, \nand K. Pingali. Lonestar: A suite of parallel irregular programs. In Proceedings of the IEEE International \nSymposium on Performance Analysis of Systems and Software (ISPASS), pages 65 76, 2009. [17] C. D. Polychronopoulos \nand D. J. Kuck. Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. IEEE \nTransactions on Computers, 36:1425 1439, 1987. [18] D. Quinlan. Rose: Compiler support for object-oriented \nframework. In Proceedings of the Workshop on Compilers for Parallel Computers (CPC), 2000. [19] A. Raman, \nH. Kim, T. R. Mason, T. B. Jablin, and D. I. August. Spec\u00adulative parallelization using software multi-threaded \ntransactions. In Proceedings of the International Conference on Architectural Support for Programming \nLanguages and Operating Systems (ASPLOS), pages 65 76, 2010. [20] J. Reinders. Intel threading building \nblocks. O Reilly Media, 2007. [21] M. Scott, M. F. Spear, L. Dalessandro, and V. J. Marathe. De\u00adlaunay \ntriangulation with transactions and barriers. In Proceedings of the IEEE International Symposium on Workload \nCharacterization (IISWC), 2007. [22] A. Silberschatz, P. B. Galvin, and G. Gagne. Operating System Con\u00adcepts. \nWiley Publishing, 2008. [23] S. W. Son, S. P. Muralidhara, O. Ozturk, M. Kandemir, I. Kolcu, and M. Karakoy. \nPro.ler and compiler assisted adaptive I/O prefetching for shared storage caches. In Proceedings of the \nInternational Confer\u00adence on Parallel Architectures and Compilation Techniques (PACT), pages 112 121, \n2008. [24] R. Thakur, W. Gropp, and E. Lusk. An abstract-device interface for implementing portable parallel-I/O \ninterfaces. In Proceedings of the Symposium on the Frontiers of Massively Parallel Computation (FRONTIERS), \npages 180 187, 1996. [25] R. Thakur, W. Gropp, and E. Lusk. Data sieving and collective I/O in ROMIO. \nIn Proceedings of the Symposium on the Frontiers of Massively Parallel Computation (FRONTIERS), pages \n182 191, 1999. [26] C. Tian, M. Feng, and R. Gupta. Copy or discard execution model for speculative parallelization \non multicores. In Proceedings of the Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), \npages 330 341, 2008. [27] C. Tian, M. Feng, and R. Gupta. Supporting speculative parallelization in the \npresence of dynamic data structures. In Proceedings of the ACM SIGPLAN conference on Programming Language \nDesign and Implementation (PLDI), pages 62 73, 2010. [28] C. Tian, C. Lin, M. Feng, and R. Gupta. Enhanced \nspeculative paral\u00adlelization via incremental recovery. In Proceedings of the ACM SIG-PLAN Symposium on \nPrinciples and Practice of Parallel Program\u00adming (PPoPP), pages 189 200, 2011. [29] N. Vachharajani, \nR. Rangan, E. Raman, M. J. Bridges, G. Ottoni, and D. I. August. Speculative decoupled software pipelining. \nIn Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT), \npages 49 59, 2007. [30] D. R. Zerbino and E. Birney. Velvet: algorithms for de novo short read assembly \nusing de bruijn graphs. Genome Research, 18:821 829, 2008.    \n\t\t\t", "proc_id": "2254064", "abstract": "<p>Software-based thread-level parallelization has been widely studied for exploiting data parallelism in purely computational loops to improve program performance on multiprocessors. However, none of the previous efforts deal with efficient parallelization of <i>hybrid loops</i>, i.e., loops that contain a mix of computation and I/O operations. In this paper, we propose a set of techniques for efficiently parallelizing hybrid loops. Our techniques apply DOALL parallelism to hybrid loops by breaking the cross-iteration dependences caused by I/O operations. We also support speculative execution of I/O operations to enable speculative parallelization of hybrid loops. Helper threading is used to reduce the I/O bus contention caused by the improved parallelism. We provide an easy-to-use programming model for exploiting parallelism in loops with I/O operations. Parallelizing hybrid loops using our model requires few modifications to the code. We have developed a prototype implementation of our programming model. We have evaluated our implementation on a 24-core machine using eight applications, including a widely-used genomic sequence assembler and a multi-player game server, and others from PARSEC and SPEC CPU2000 benchmark suites. The hybrid loops in these applications take 23%-99% of the total execution time on our 24-core machine. The parallelized applications achieve speedups of 3.0x-12.8x with hybrid loop parallelization over the sequential versions of the same applications. Compared to the versions of applications where only computation loops are parallelized, hybrid loop parallelization improves the application performance by 68% on average.</p>", "authors": [{"name": "Min Feng", "author_profile_id": "81365596375", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P3471307", "email_address": "mfeng@cs.ucr.edu", "orcid_id": ""}, {"name": "Rajiv Gupta", "author_profile_id": "81100027751", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P3471308", "email_address": "gupta@cs.ucr.edu", "orcid_id": ""}, {"name": "Iulian Neamtiu", "author_profile_id": "81100589658", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P3471309", "email_address": "neamtiu@cs.ucr.edu", "orcid_id": ""}], "doi_number": "10.1145/2254064.2254122", "year": "2012", "article_id": "2254122", "conference": "PLDI", "title": "Effective parallelization of loops in the presence of I/O operations", "url": "http://dl.acm.org/citation.cfm?id=2254122"}